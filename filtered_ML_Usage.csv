,Title,Description,URL,Keyword
0,programmatically press an enter key after starting .exe file in Matlab,"
In Matlab I can start external .exe files that sometime have a pop up that requires an enter key pressed. For example:
system('C:\Program Files (x86)\WinZip\WINZIP32.EXE')

will start Winzip, and then in order to use it you need to pass the ""buy now"" pop up window by pressing enter. 
Now my problem is not with winzip, I only gave it as an example (i use winrar anyway :).
How can I programmatically press an enter key in Matlab in such cases ? (I use win 7)
Can an event listener be used to solve that?
EDIT: The java.awt.Robot class indeed works on explorer, but not on any software that has a pop up window with an OK button that needs to be pressed. I don't know why it doesn't work for that. I gave the winzip example because I assume everybody has winzip/winrar installed in their machine. The actual software I have is different and irrelevant for the question. 
",https://stackoverflow.com/questions/27933270/programmatically-press-an-enter-key-after-starting-exe-file-in-matlab,program
1,Run the current application as Single Instance and show the previous instance,"
I just implemented this code that is guarding the Single Instance of the Application, in order to not run the application twice.
Now I am wondering how I can show the original Application process that is already running.
Here is my code in the program class:
static class Program
{
    [STAThread]
    static void Main()
    {
        const string appName = ""MyappName"";
        bool createdNew;
        mutex = new Mutex(true, appName, out createdNew);

        Application.EnableVisualStyles();
        Application.SetCompatibleTextRenderingDefault(false);
        Form form = new Form1();

        if (!createdNew)
        {
            form.Show();  <<=========================== NOT WORKING
            form.Visible = true; <<===================== None
            form.TopMost = true; <<===================== of
            form.BringToFront(); <<===================== these working!
            form.WindowState = FormWindowState.Maximized;
            return;
        }
        Application.Run(form);
    }        private static Mutex mutex = null;
}

",https://stackoverflow.com/questions/50552592/run-the-current-application-as-single-instance-and-show-the-previous-instance,program
2,Run event when any Form loads,"
I'm trying to create a Popularity Contest for Forms in our primary front end. There are many items that are no longer used, but getting details on which are used and which are no longer used is proving to be difficult.
So I came up with the idea of logging a form when it is loaded and then in a year or so I'll run a group by and get an idea of which forms are used, how often, and by who. Now the issue is that I don't want to add a line to every forms InitializeComponent block. Instead I would like to put this in the Program.cs file and some how intercept all Form loads so I can log them.
Is this possible?
Edit
Using @Jimi's comment I was able to come up with the following.
using CrashReporterDotNET;
using System;
using System.Diagnostics;
using System.Linq;
using System.Threading;
using System.Windows.Automation;
using System.Windows.Forms;

namespace Linnabary
{
    static class Program
    {
        /// <summary>
        /// The main entry point for the application.
        /// </summary>
        [STAThread]
        static void Main()
        {
            //This keeps the user from opening multiple copies of the program
            string[] clArgs = Environment.GetCommandLineArgs();
            if (PriorProcess() != null && clArgs.Count() == 1)
            {
                MessageBox.Show(""Another instance of the WOTC-FE application is already running."");
                return;
            }

            //Error Reporting Engine Setup
            Application.ThreadException += ApplicationThreadException;
            AppDomain.CurrentDomain.UnhandledException += CurrentDomainOnUnhandledException;


            Application.EnableVisualStyles();
            Application.SetCompatibleTextRenderingDefault(false);

            //This is the SyncFusion License Key.
            Syncfusion.Licensing.SyncfusionLicenseProvider.RegisterLicense(""<Removed>"");

            //Popularity Contest
            Automation.AddAutomationEventHandler(WindowPattern.WindowOpenedEvent,
                         AutomationElement.RootElement, TreeScope.Subtree, (UIElm, evt) =>
                          {
                              try
                              {
                                  AutomationElement element = UIElm as AutomationElement;
                                  string AppText = element.Current.Name;
                                  if (element.Current.ProcessId == Process.GetCurrentProcess().Id)
                                  {
                                      Classes.Common.PopularityContest(AppText);
                                  }
                              }
                              catch (Exception)
                              {
                                  //throw;
                              }
                          });


            Application.Run(new Forms.frmMain());
        }

        private static void CurrentDomainOnUnhandledException(object sender, UnhandledExceptionEventArgs unhandledExceptionEventArgs)
        {
            ReportCrash((Exception)unhandledExceptionEventArgs.ExceptionObject);
            Environment.Exit(0);
        }

        private static void ApplicationThreadException(object sender, ThreadExceptionEventArgs e)
        {
            ReportCrash(e.Exception);
        }

        public static void ReportCrash(Exception exception, string developerMessage = """")
        {
            var reportCrash = new ReportCrash(""<Removed>"")
            {
                CaptureScreen = true,
                DeveloperMessage = Environment.UserName,
                ToEmail = ""<Removed>""
            };
            reportCrash.Send(exception);
        }

        public static Process PriorProcess()
        {
            Process curr = Process.GetCurrentProcess();
            Process[] procs = Process.GetProcessesByName(curr.ProcessName);
            foreach (Process p in procs)
            {
                if ((p.Id != curr.Id) && (p.MainModule.FileName == curr.MainModule.FileName))
                {
                    return p;
                }
            }
            return null;
        }
    }
}

However, I wonder if there is a way to get the name of the form instead of it's Text. Since this is accessing ALL windows and is therefor outside of the managed space, I doubt it. Still, it works and I'll post this as an answer tomorrow if no one else does so.
",https://stackoverflow.com/questions/55955331/run-event-when-any-form-loads,program
3,UIAutomation Memory Issue,"
I have a simple WPF program that just has a single button with no event handling logic.  I then use the UIAutomation framework to click that button many times in a row.  Finally, I look at the memory used by the WPF program and it seems to grow and grow.
Anyone know why this is the case and how I can prevent this from happening?
Here is the simple WPF program (nothing in the code behind):
<Window x:Class=""SimpleApplication.MainWindow""
        xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation""
        xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml""
        Title=""Simple Application""
        AutomationProperties.AutomationId=""Simple Application""
        Height=""350"" Width=""525"">
    <Grid>
        <Button AutomationProperties.AutomationId=""button"" Height=""50"" Width=""100"">Click Me</Button>
    </Grid>
</Window>

Here is the UIAutomation test program:
class Program
{
    static void Main(string[] args)
    {
        string appPath = @""..\..\..\SimpleApplication\bin\Debug\SimpleApplication.exe"";
        string winAutoId = ""Simple Application"";
        string buttonAutoId = ""button"";

        using (Process process = Process.Start(new ProcessStartInfo(appPath)))
        {
            Thread.Sleep(TimeSpan.FromSeconds(1));

            AutomationElement winElement = AutomationElement.RootElement.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.AutomationIdProperty, winAutoId));

            for (int i = 0; i < 1001; i++)
            {
                AutomationElement buttonElement = winElement.FindFirst(TreeScope.Descendants, new PropertyCondition(AutomationElement.AutomationIdProperty, buttonAutoId));

                InvokePattern invokePattern = (InvokePattern)buttonElement.GetCurrentPattern(InvokePattern.Pattern);
                invokePattern.Invoke();

                process.Refresh();
                long totalMemory = process.WorkingSet64 + process.PagedMemorySize64;

                if (i % 100 == 0)
                {
                    Console.WriteLine(""Memory = {0} MB"", ((double)totalMemory) / (1024 * 1024));
                }
            }

            WindowPattern windowPattern = (WindowPattern)winElement.GetCurrentPattern(WindowPattern.Pattern);
            windowPattern.Close();
        }

        Console.WriteLine();
        Console.WriteLine(""Press Enter to Continue..."");
        Console.ReadLine();
    }
}

Here are the results from the program on my machine:
Memory = 38.20703125 MB
Memory = 42.9296875 MB
Memory = 45.00390625 MB
Memory = 47.04296875 MB
Memory = 51.9296875 MB
Memory = 52.2890625 MB
Memory = 52.41015625 MB
Memory = 55.70703125 MB
Memory = 55.70703125 MB
Memory = 57.21484375 MB
Memory = 59.09375 MB

Looking at it with the .NET Memory Profiler, the new objects that are appearing in the WPF application are from the System.Threading namespace.  When I run the WPF program by itself and click the button with the mouse these objects do no appear.
UPDATE:
I tried doing a similar test using Visual Studio's CodedUI, and the same 8 objects appeared to leak in that situation as well.  The objects that appear to leak are:
System.Threading.CancellationTokenSource
System.Threading.TimerQueueTimer
System.Threading.SparselyPopulatedArray<CancellationCallbackInfo>[]
System.Threading.Timer
System.Threading.TimerHolder
System.Threading.SparselyPopulatedArray<CancellationCallbackInfo>
System.Threading.SparselyPopulatedArrayFragment<CancellationCallbackInfo>
System.Threading.CancellationCallbackInfo[]

I have also submitted a bug to Microsoft:
http://connect.microsoft.com/VisualStudio/feedback/details/801209/uiautomation-memory-issue
",https://stackoverflow.com/questions/18832122/uiautomation-memory-issue,program
4,Interacting with multiple instances of an application in Coded UI,"
The scenario that I am facing is that I am trying to write a single test which will use Coded UI to interact with multiple instances of the same application, in this case Microsoft Excel. In other words, there will be multiple Excel workbooks open in multiple windows, and I need to be able to direct Coded UI to interact with a specific instance programatically. I initially thought this type of instance management would be a function of the ApplicationUnderTest class, but it is not obvious how this class would achieve this.
The interactions will involve the same UIMap for all instances (in fact, each instance will probably need multiple UIMaps, but for the sake of simplicity that can be ignored for this question unless it is significant to the answer).
A couple of solution approaches I'm already aware of:

Minimize and maximize the instances so only the one currently being used is visible at any given time. Ideally I'd like to avoid this. For one thing, it may eventually become a requirement that two windows are visible simultaneously during the tests.
Dynamically modify the search properties to always include some unique identifier every time the UI Map is accessed. I'm not sure what the best candidate for a search property would be here. 

Ideally I would like something more integrated into Coded UI than either of these options, though the latter would probably suffice if necessary. I would appreciate any direction on whether there are any other possible approaches.
",https://stackoverflow.com/questions/23522114/interacting-with-multiple-instances-of-an-application-in-coded-ui,program
5,Is it possible to activate a tab in another program using an IntPtr?,"
Thanks in advance.
Is it possible to activate a tab in another program using an IntPtr? If so, how? 
SendKeys is not an option.
Perhaps what I need is a fishing lesson. I have exhausted Google and my lead developer. 
I would appreciate an outright solution OR a recommendation to continue my Google efforts.
basic process is:
I drag a shortcut icon to the launcher

This opens the target application (Notepad++) and grabs IntPtr, etc.
I would like to programmatically select various items in Notepad++ such as Edit, menu items under Edit, or a doc tab.

The basic code I am running is:

the 'blob'

item 1: IntPtr of item
item 2: IntPtr of itemsChild
item 3: control text of item 1
item 4: is rectangle parameters of item 1


root contains similar info:

",https://stackoverflow.com/questions/29951432/is-it-possible-to-activate-a-tab-in-another-program-using-an-intptr,program
6,How can we fetch multiple data stored in <div> table in karate?,"
I am trying to fetch set of data from a web table, stored/made using the <div> tags, not like the traditional html data tables. For example:
    <div class=""tabulator-cell"" role=""gridcell"" tabulator-field=""program_name"" title="""" style=""width: 135px; text-align: left; height: 30px;"">
        <span style=""color: #00def; font-weight: 500;"">Consumer xyz</span>
    </div>
    <div class=""tabulator-cell"" role=""gridcell"" tabulator-field=""business_val"" title="""" style=""width: 119px; text-align: center; height: 00px;"">
        11898
        <div class=""tabulator-col-resize-handle""></div>
        <div class=""tabulator-col-resize-handle prev""></div>
    </div>

So, I am using scriptAll() method for this and need data just for 'Consumer xyz' here but unable to do so.
    * def list = scriptAll('div div', '_.textContent', function(x){ return x.contains('Consumer xyz') })
    * delay(3000)
    * print list 

Any help on this would be appreciated. Thanks in advance.
",https://stackoverflow.com/questions/65844733/how-can-we-fetch-multiple-data-stored-in-div-table-in-karate,program
8,AutomationElement shows up using Inspect.exe but does show not up when using UIAutomationCore.dll or System.Windows.Automation,"
TL;DR: What am I doing wrong that is causing the workspace pane to show up in Inspect Objects but not show up in my custom code?

I am trying to write some UI automation to a 3rd party program. I am using Inspect.exe that came with the Windows SDK, and I have tried both System.Windows.Automation and direct COM Calls (using the wrapper library from UIA Verify).
Process[] processes = Process.GetProcessesByName(""Redacted Client"");
if (processes.Length == 0) throw new Exception(""Could not find \""Redacted Client\"" process"");

PropertyCondition parentFileCond = new PropertyCondition(AutomationElement.ProcessIdProperty, processes[0].Id);
PropertyCondition workspaceCond = new PropertyCondition(AutomationElement.NameProperty, ""Workspace"", PropertyConditionFlags.IgnoreCase);
PropertyCondition documentCond = new PropertyCondition(AutomationElement.NameProperty, ""Untitled3"", PropertyConditionFlags.IgnoreCase);

var parentElement = AutomationElement.RootElement.FindFirst(TreeScope.Children, parentFileCond);
var workspaceElement = parentElement.FindFirst(TreeScope.Children, workspaceCond); //Also does not work with TreeScope.Descendants
var documentElement = workspaceElement.FindFirst(TreeScope.Children, documentCond);

When I try the above code, parentElement does have the correct reference to the main program window, but workspaceElement is null.

A temporary workaround:
If I change my documentElement code to:
var documentElement = parentElement.FindFirst(TreeScope.Descendants, documentCond);

I will get the correct element returned. I can use this as a workaround as the document window is the one I really wanted anyway, but I would like to know why the Workspace pane would not show up so I can improve my skills in case I run into this in the future with a situation I cannot work around.

UPDATE: I tried MrGomez's suggestions
PropertyCondition parentFileCond = new PropertyCondition(AutomationElement.ProcessIdProperty, 5872);
PropertyCondition panelCond = new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Pane);

var parentElement = AutomationElement.RootElement.FindFirst(TreeScope.Children, parentFileCond);
var panels = parentElement.FindAll(TreeScope.Children, panelCond);


I get 3 results, unfortunately, I have 4 panels, and the one that did not show up was the panel named Workspace.
I also tried to use a TreeWalker
PropertyCondition parentFileCond = new PropertyCondition(AutomationElement.ProcessIdProperty, 5872);
PropertyCondition workspaceCond= new PropertyCondition(AutomationElement.NameProperty, ""Workspace"");

var walker = new TreeWalker(workspaceCond);
var parentElement = AutomationElement.RootElement.FindFirst(TreeScope.Children, parentFileCond);
var workspaceElement = walker.Normalize(parentElement);

but that also returns null for workspaceElement
Finally, in desperation, I tried the current value of ""NativeWindowHandle"" from Inspect and started the walking from the root node.
PropertyCondition workspaceCond = new PropertyCondition(AutomationElement.NativeWindowHandleProperty, 0x110906);
var walker = new TreeWalker(workspaceCond);
var workspaceElement = walker.Normalize(AutomationElement.RootElement);

Workspace element is STILL null.

Result Found
I finally did get Workspace to show up, but I had to perform 
PropertyCondition workspaceCond = new PropertyCondition(AutomationElement.NativeWindowHandleProperty, 0x110906);
var test = AutomationElement.RootElement.FindFirst(TreeScope.Subtree, workspaceCond); 

and it took quite a while to run.
Old Screen Captures
Here is screenshots from Inspect.exe showing the tree view.

Here are the properties of the main window of the program.
How found:  Selected from tree...
RuntimeId:  ""[42.2557552]""
BoundingRectangle:  {l:75 t:1 r:1311 b:1003}
ProcessId:  8160
ControlType:    UIA_WindowControlTypeId (0xC370)
LocalizedControlType:   ""window""
Name:   ""Redacted""
AccessKey:  """"
HasKeyboardFocus:   false
IsKeyboardFocusable:    true
IsEnabled:  true
ClassName:  ""C:\Program Files (x86)\RedactedProgramFiles7\RedactedClientFolder""
HelpText:   """"
IsPassword: false
NativeWindowHandle: 0x270670
IsOffscreen:    false
FrameworkId:    ""Win32""
ProviderDescription:    ""[pid:4000,hwnd:0x270670 Main:Nested [pid:8160,hwnd:0x270670 Annotation(parent link):Microsoft: Annotation Proxy (unmanaged:uiautomationcore.dll); Main:Microsoft: MSAA Proxy (unmanaged:uiautomationcore.dll)]; Nonclient:Microsoft: Non-Client Proxy (unmanaged:uiautomationcore.dll); Hwnd(parent link):Microsoft: HWND Proxy (unmanaged:uiautomationcore.dll)]""
Window.CanMaximize: true
Window.CanMinimize: true
Window.WindowVisualState:   Normal (0)
Window.WindowInteractionState:  ReadyForUserInteraction (2)
Window.IsModal: false
Window.IsTopmost:   false
Transform.CanMove:  true
Transform.CanResize:    true
Transform.CanRotate:    false
LegacyIAccessible.ChildId:  0
LegacyIAccessible.DefaultAction:    """"
LegacyIAccessible.Description:  """"
LegacyIAccessible.Help: """"
LegacyIAccessible.KeyboardShortcut: """"
LegacyIAccessible.Name: ""Redacted""
LegacyIAccessible.Role: client (0xA)
LegacyIAccessible.State:    focusable (0x100000)
LegacyIAccessible.Value:    """"
IsDockPatternAvailable: false
IsExpandCollapsePatternAvailable:   false
IsGridItemPatternAvailable: false
IsGridPatternAvailable: false
IsInvokePatternAvailable:   false
IsLegacyIAccessiblePatternAvailable:    true
IsMultipleViewPatternAvailable: false
IsRangeValuePatternAvailable:   false
IsScrollPatternAvailable:   false
IsScrollItemPatternAvailable:   false
IsSelectionItemPatternAvailable:    false
IsSelectionPatternAvailable:    false
IsTablePatternAvailable:    false
IsTableItemPatternAvailable:    false
IsTextPatternAvailable: false
IsTogglePatternAvailable:   false
IsTransformPatternAvailable:    true
IsValuePatternAvailable:    false
IsWindowPatternAvailable:   true
IsItemContainerPatternAvailable:    false
IsVirtualizedItemPatternAvailable:  false
IsSynchronizedInputPatternAvailable:    false
FirstChild: ""Workspace"" pane
LastChild:  ""Application"" menu bar
Next:   ""Inspect  (HWND: 0x01700F06)"" window
Previous:   ""Sandbox Console (Debugging) - Microsoft Visual Studio (Administrator)"" window
Other Props:    Object has no additional properties
Children:   ""Workspace"" pane
    (null) title bar
    ""Application"" menu bar
Ancestors:  ""Desktop"" pane
    [ No Parent ]

Here are the properties of the problem ""Workspace"" pane.
How found:  Selected from tree...
RuntimeId:  ""[42.34146524]""
BoundingRectangle:  {l:83 t:51 r:1303 b:995}
ProcessId:  8160
ControlType:    UIA_PaneControlTypeId (0xC371)
LocalizedControlType:   ""pane""
Name:   ""Workspace""
AccessKey:  """"
HasKeyboardFocus:   false
IsKeyboardFocusable:    true
IsEnabled:  true
ClassName:  ""MDIClient""
HelpText:   """"
IsPassword: false
NativeWindowHandle: 0x20908DC
IsOffscreen:    false
FrameworkId:    ""Win32""
ProviderDescription:    ""[pid:4000,hwnd:0x20908DC Main:Nested [pid:8160,hwnd:0x20908DC Annotation(parent link):Microsoft: Annotation Proxy (unmanaged:uiautomationcore.dll); Main:Microsoft: MSAA Proxy (unmanaged:uiautomationcore.dll)]; Hwnd(parent link):Microsoft: HWND Proxy (unmanaged:uiautomationcore.dll)]""
LegacyIAccessible.ChildId:  0
LegacyIAccessible.DefaultAction:    """"
LegacyIAccessible.Description:  """"
LegacyIAccessible.Help: """"
LegacyIAccessible.KeyboardShortcut: """"
LegacyIAccessible.Name: ""Workspace""
LegacyIAccessible.Role: client (0xA)
LegacyIAccessible.State:    focusable (0x100000)
LegacyIAccessible.Value:    """"
IsDockPatternAvailable: false
IsExpandCollapsePatternAvailable:   false
IsGridItemPatternAvailable: false
IsGridPatternAvailable: false
IsInvokePatternAvailable:   false
IsLegacyIAccessiblePatternAvailable:    true
IsMultipleViewPatternAvailable: false
IsRangeValuePatternAvailable:   false
IsScrollPatternAvailable:   false
IsScrollItemPatternAvailable:   false
IsSelectionItemPatternAvailable:    false
IsSelectionPatternAvailable:    false
IsTablePatternAvailable:    false
IsTableItemPatternAvailable:    false
IsTextPatternAvailable: false
IsTogglePatternAvailable:   false
IsTransformPatternAvailable:    false
IsValuePatternAvailable:    false
IsWindowPatternAvailable:   false
IsItemContainerPatternAvailable:    false
IsVirtualizedItemPatternAvailable:  false
IsSynchronizedInputPatternAvailable:    false
FirstChild: ""Untitled3"" window
LastChild:  ""Letters (32638 of 32638):"" window
Next:   (null) title bar
Previous:   [null]
Other Props:    Object has no additional properties
Children:   ""Untitled3"" window
    ""Letters (32638 of 32638):"" window
Ancestors:  ""Redacted"" window
    ""Desktop"" pane
    [ No Parent ]

Here are the properties of the ""Working"" document window.
How found:  Selected from tree...
RuntimeId:  ""[42.9505096]""
BoundingRectangle:  {l:85 t:53 r:651 b:491}
ProcessId:  8160
ControlType:    UIA_WindowControlTypeId (0xC370)
LocalizedControlType:   ""window""
Name:   ""Untitled3""
AccessKey:  """"
HasKeyboardFocus:   false
IsKeyboardFocusable:    true
IsEnabled:  true
AutomationId:   ""10""
ClassName:  ""ProToolsSubMDIWndClass""
HelpText:   """"
IsPassword: false
NativeWindowHandle: 0x910948
IsOffscreen:    false
FrameworkId:    ""Win32""
ProviderDescription:    ""[pid:4000,hwnd:0x910948 Main:Nested [pid:8160,hwnd:0x910948 Annotation(parent link):Microsoft: Annotation Proxy (unmanaged:uiautomationcore.dll); Main:Microsoft: MSAA Proxy (unmanaged:uiautomationcore.dll)]; Nonclient:Microsoft: Non-Client Proxy (unmanaged:uiautomationcore.dll); Hwnd(parent link):Microsoft: HWND Proxy (unmanaged:uiautomationcore.dll)]""
Window.CanMaximize: true
Window.CanMinimize: true
Window.WindowVisualState:   Normal (0)
Window.WindowInteractionState:  ReadyForUserInteraction (2)
Window.IsModal: false
Window.IsTopmost:   false
Transform.CanMove:  true
Transform.CanResize:    true
Transform.CanRotate:    false
LegacyIAccessible.ChildId:  0
LegacyIAccessible.DefaultAction:    """"
LegacyIAccessible.Description:  """"
LegacyIAccessible.Help: """"
LegacyIAccessible.KeyboardShortcut: """"
LegacyIAccessible.Name: ""Untitled3""
LegacyIAccessible.Role: client (0xA)
LegacyIAccessible.State:    focusable (0x100000)
LegacyIAccessible.Value:    """"
IsDockPatternAvailable: false
IsExpandCollapsePatternAvailable:   false
IsGridItemPatternAvailable: false
IsGridPatternAvailable: false
IsInvokePatternAvailable:   false
IsLegacyIAccessiblePatternAvailable:    true
IsMultipleViewPatternAvailable: false
IsRangeValuePatternAvailable:   false
IsScrollPatternAvailable:   false
IsScrollItemPatternAvailable:   false
IsSelectionItemPatternAvailable:    false
IsSelectionPatternAvailable:    false
IsTablePatternAvailable:    false
IsTableItemPatternAvailable:    false
IsTextPatternAvailable: false
IsTogglePatternAvailable:   false
IsTransformPatternAvailable:    true
IsValuePatternAvailable:    false
IsWindowPatternAvailable:   true
IsItemContainerPatternAvailable:    false
IsVirtualizedItemPatternAvailable:  false
IsSynchronizedInputPatternAvailable:    false
FirstChild: """" thumb
LastChild:  (null) title bar
Next:   ""Letters (32638 of 32638):"" window
Previous:   [null]
Other Props:    Object has no additional properties
Children:   """" thumb
    (null) title bar
Ancestors:  ""Workspace"" pane
    ""Redacted"" window
    ""Desktop"" pane
    [ No Parent ]

",https://stackoverflow.com/questions/9282275/automationelement-shows-up-using-inspect-exe-but-does-show-not-up-when-using-uia,program
9,Open file from windows file dialog with python automatically,"
I do automated testing and get a file dialog. I want to choose a file from the windows open file dialog with python or selenium.
NOTE: The dialog is given by an other program. I don't want to create it with Tkinter.
The Window looks like:
.
How to do this?
",https://stackoverflow.com/questions/37027644/open-file-from-windows-file-dialog-with-python-automatically,program
10,Windows UI Automation not showing all child elements?,"
I have a TreeView control on my form, and I'm recursively going through the elements of another window starting with the window itself. I'm  using this to find the elements:
getRecursiveElements(AutomationElement parent)
{
  children = parent.FindAll(TreeScope.Children, Condition.TrueCondition);

  foreach (AutomationElement child in children)
  {
    addToTreeView(child);
    getRecursiveElements(child);
  }
}

Generally speaking, the code works quite well in most cases. The tree is populated and I have a bit of other supporting code allowing me to double click, for example, an element in the tree-view and it will highlight that element on the target form.
The issue I'm having is that, while it generates an awesome tree, there are still some elements missing for certain target programs.
What possible reason could there be for this, and is there any way to get around it?
If I call EnumChildWindows() from user32.dll will that have the same problem?
",https://stackoverflow.com/questions/7238883/windows-ui-automation-not-showing-all-child-elements,program
11,Programatically interact with the IE browser to fill in forms and navigate etc,"
I'd like to use C# to interact with the IE browser. 
I have a feeling that shdocvw.dll will be involved, but there are so many classes in there that I don't know where to start, and maybe it's not even necessary to use it.
The goal here is to interact with a website, visiting it's pages and ""warming it up,"" not unlike as described here by Kenneth Scott.  The thing is, javascript is getting executed as you interact with a website, so it would be nice just to be able to login / submit forms exactly as you would on the website itself.
Plus it would be nice to be able to create a program that records my actions in IE, and then be able to slightly automate and slightly modify them.
Additionally, it would be nice if it could do all this in the background, without having to display the webpage at all.
I'm not looking for third party solutions, I want to do this myself (with your advice of course.)  
Thanks.
",https://stackoverflow.com/questions/8438782/programatically-interact-with-the-ie-browser-to-fill-in-forms-and-navigate-etc,program
12,Cross platform solution for automating ncurses-type telnet sessions,"
Background
Part of my work in networking and telco involves automating telnet sessions when legacy hardware doesn't offer easy solutions in other interfaces. Many older pieces of equipment can only be accessed via craft ports (RS-232 serial ports), SNMP, or telnet. Sometimes telnet is the only way to access specific information, however telnet is designed as a human interface and thus requires screen scraping. In addition, there is also the issue of scraping screens where only portions are updated in order to save bandwidth (see ncurses). In my work I have used ActiveState Expect and the Python telnet library.
Question
Which languages and libraries are able to automate telnet sessions and have the following requirements:

Suitable for large projects (e.g. Tcl
doesn't seem to scale as well as
Python in my experience and seems outdated)
Cross Platform (e.g. Pexpect does not work on Windows and Activestate
Expect behaves differently on
Windows plus requires DEP on newer
machines to be turned off)
Able to screen scrape sessions that repaint portions of the screen
(similar to the behavior of ncurses in command-line programs)
Free as in beer!

A preferable solution would also include the following:

Easily redistributable (e.g. Does not
require some huge runtime to be installed on a machine.)
Also works for SSH, serial connections, and other command-line interfaces.

",https://stackoverflow.com/questions/2060420/cross-platform-solution-for-automating-ncurses-type-telnet-sessions,program
13,Chrome Executable getting overriden while running the Karate test on a docker container,"
Below you can see that the Karate Driver is being configured as below.After that it is trying to use the user-data-dir and then the location.How do I disable the process using --user-data-dir for Chrome executable path:
build-env_1  | 12:10:42.702 [ForkJoinPool-1-worker-1] INFO  com.intuit.karate - Karate Driver config:
build-env_1  | {
build-env_1  |   ""type"": ""chrome"",
build-env_1  |   ""executable"": ""/usr/bin/karate_chrome_driver"",
build-env_1  |   ""port"": 9515,
build-env_1  |   ""httpConfig"": {
build-env_1  |     ""readTimeout"": 120000
build-env_1  |   }
build-env_1  | }
build-env_1  | 12:10:42.727 [ForkJoinPool-1-worker-1] WARN  com.intuit.karate - type was null, defaulting to 'chrome'
build-env_1  | 12:10:42.754 [ForkJoinPool-1-worker-1] DEBUG com.intuit.karate.shell.Command - found / verified free local port: 9222
build-env_1  | 12:10:42.759 [chrome_1603973442746] DEBUG c.i.k.driver.chrome_1603973442746 - command: [/usr/bin/google-chrome, --remote-debugging-port=9222, --no-first-run, --user-data-dir=/usr/regression/target/chrome_1603973442746, --disable-popup-blocking]
build-env_1  | 12:10:42.762 [ForkJoinPool-1-worker-1] DEBUG c.i.k.driver.chrome_1603973442746 - poll attempt #0 for port to be ready - localhost:9222
build-env_1  | 12:10:42.762 [chrome_1603973442746] ERROR com.intuit.karate.shell.Command - command error: [/usr/bin/google-chrome, --remote-debugging-port=9222, --no-first-run, --user-data-dir=/usr/regression/target/chrome_1603973442746, --disable-popup-blocking] - Cannot run program ""/usr/bin/google-chrome"" (in directory ""target/chrome_1603973442746""): error=2, No such file or directory.

",https://stackoverflow.com/questions/64591040/chrome-executable-getting-overriden-while-running-the-karate-test-on-a-docker-co,program
14,Automatic login script for a website on windows machine?,"
I saw some guy had a file (I guess a batch file). On clicking of the batch file he was able to log in to multiple sites. (Perhaps it was done using VB.)
I looked for such a script on Google but didn't find anything useful.
I know a bit of C++ and UNIX (also some HTML and JavaScript). I don't know if it can be done on a windows machine using these languages, but even if it could be done I think it would be difficult compared to VB or C## or some other high level languages.
I learned how to open multiple sites using basic windows batch commands enclosed in a batch file like:
start http://www.gmail.com
start http://stackoverflow.com

But still I can't figure out how actually clicking on the batch file would help me to log in to the sites without even typing the username and password.
Do I need to start learning Visual Basic, .NET, or windows batch programming to do this?
One more thing: can I also use it to log in to remote desktops?
",https://stackoverflow.com/questions/6248679/automatic-login-script-for-a-website-on-windows-machine,program
15,Scrape multiple urls using QWebPage,"
I'm using Qt's QWebPage to render a page that uses javascript to update its content dynamically - so a library that just downloads a static version of the page (such as urllib2) won't work.
My problem is, when I render a second page, about 99% of the time the program just crashes. At other times, it will work three times before crashing. I've also gotten a few segfaults, but it is all very random.
My guess is the object I'm using to render isn't getting deleted properly, so trying to reuse it is possibly causing some problems for myself. I've looked all over and no one really seems to be having this same issue.
Here's the code I'm using. The program downloads web pages from steam's community market so I can create a database of all the items. I need to call the getItemsFromPage function multiple times to get all of the items, as they are broken up into pages (showing results 1-10 out of X amount).
import csv
import re
import sys
from string import replace
from bs4 import BeautifulSoup
from PyQt4.QtGui import *
from PyQt4.QtCore import *
from PyQt4.QtWebKit import *

class Item:
    __slots__ = (""name"", ""count"", ""price"", ""game"")

    def __repr__(self):
        return self.name + ""("" + str(self.count) + "")""

    def __str__(self):
        return self.name + "", "" + str(self.count) + "", $"" + str(self.price)

class Render(QWebPage):  
    def __init__(self, url):
        self.app = QApplication(sys.argv)
        QWebPage.__init__(self)
        self.loadFinished.connect(self._loadFinished)
        self.mainFrame().load(QUrl(url))
        self.app.exec_()

    def _loadFinished(self, result):
        self.frame = self.mainFrame()
        self.app.quit()
        self.deleteLater()

def getItemsFromPage(appid, page=1):

    r = Render(""http://steamcommunity.com/market/search?q=appid:"" + str(appid) + ""#p"" + str(page))

    soup = BeautifulSoup(str(r.frame.toHtml().toUtf8()))

    itemLst = soup.find_all(""div"", ""market_listing_row market_recent_listing_row"")

    items = []

    for k in itemLst:
        i = Item()

        i.name = k.find(""span"", ""market_listing_item_name"").string
        i.count = int(replace(k.find(""span"", ""market_listing_num_listings_qty"").string, "","", """"))
        i.price = float(re.search(r'\$([0-9]+\.[0-9]+)', str(k)).group(1))
        i.game = appid

        items.append(i)

    return items

if __name__ == ""__main__"":

    print ""Updating market items to dota2.csv ...""

    i = 1

    with open(""dota2.csv"", ""w"") as f:
        writer = csv.writer(f)

        r = None

        while True:
            print ""Page "" + str(i)

            items = getItemsFromPage(570)

            if len(items) == 0:
                print ""No items found, stopping...""
                break

            for k in items:
                writer.writerow((k.name, k.count, k.price, k.game))

            i += 1

    print ""Done.""

Calling getItemsFromPage once works fine. Subsequent calls give me my problem. The output of the program is typically
Updating market items to dota2.csv ...
Page 1
Page 2

and then it crashes. It should go on for over 700 pages. 
",https://stackoverflow.com/questions/21274865/scrape-multiple-urls-using-qwebpage,program
16,"How to ""scan"" a website (or page) for info, and bring it into my program?","
Well, I'm pretty much trying to figure out how to pull information from a webpage, and bring it into my program (in Java). 
For example, if I know the exact page I want info from, for the sake of simplicity a Best Buy item page, how would I get the appropriate info I need off of that page? Like the title, price, description? 
What would this process even be called? I have no idea were to even begin researching this.
Edit:
Okay, I'm running a test for the JSoup(the one posted by BalusC), but I keep getting this error:
Exception in thread ""main"" java.lang.NoSuchMethodError: java.util.LinkedList.peekFirst()Ljava/lang/Object;
at org.jsoup.parser.TokenQueue.consumeWord(TokenQueue.java:209)
at org.jsoup.parser.Parser.parseStartTag(Parser.java:117)
at org.jsoup.parser.Parser.parse(Parser.java:76)
at org.jsoup.parser.Parser.parse(Parser.java:51)
at org.jsoup.Jsoup.parse(Jsoup.java:28)
at org.jsoup.Jsoup.parse(Jsoup.java:56)
at test.main(test.java:12)

I do have Apache Commons
",https://stackoverflow.com/questions/2835505/how-to-scan-a-website-or-page-for-info-and-bring-it-into-my-program,program
17,Scrapy Very Basic Example,"
Hi I have Python Scrapy installed on my mac and I was trying to follow the very first example on their web. 
They were trying to run the command:
scrapy crawl mininova.org -o scraped_data.json -t json

I don't quite understand what does this mean? looks like scrapy turns out to be a separate program. And I don't think they have a command called crawl. In the example, they have a paragraph of code, which is the definition of the class MininovaSpider and the TorrentItem. I don't know where these two classes should go to, go to the same file and what is the name of this python file? 
",https://stackoverflow.com/questions/18838494/scrapy-very-basic-example,program
18,Web scraping program cannot find element which I can see in the browser,"
I am trying to get the titles of the streams on https://www.twitch.tv/directory/game/Dota%202, using Requests and BeautifulSoup. I know that my search criteria are correct, yet my program does not find the elements I need.
Here is a screenshot showing the relevant part of the source code in the browser:

The HTML source as text:


<div class=""tw-media-card-meta__title"">
  <div class=""tw-c-text-alt"">
    <a class=""tw-full-width tw-interactive tw-link tw-link--button tw-link--hover-underline-none tw-link--inherit"" data-a-target=""preview-card-title-link"" href=""/weplayesport_en"">
      <div class=""tw-align-items-start tw-flex"">
        <h3 class=""tw-ellipsis tw-font-size-5"" title=""NAVI vs HellRaisers | BO5 | ODPixel &amp; S4 | WeSave! Charity Play"">NAVI vs HellRaisers | BO5 | ODPixel &amp; S4 | WeSave! Charity Play</h3>
      </div>
    </a>
  </div>
</div>


Here is my code:
import requests
from bs4 import BeautifulSoup

req = requests.get(""https://www.twitch.tv/directory/game/Dota%202"")

soup = BeautifulSoup(req.content, ""lxml"")

title_elems = soup.find_all(""h3"", attrs={""title"": True})

print(title_elems)

When I run it, title_elems is just the empty list ([]).
Why is my program not finding the elements?
",https://stackoverflow.com/questions/60904786/web-scraping-program-cannot-find-element-which-i-can-see-in-the-browser,program
19,"""SSL: certificate_verify_failed"" error when scraping https://www.thenewboston.com/","
So I started learning Python recently using ""The New Boston's"" videos on youtube, everything was going great until I got to his tutorial of making a simple web crawler. While I understood it with no problem, when I run the code I get errors all seemingly based around ""SSL: CERTIFICATE_VERIFY_FAILED."" I've been searching for an answer since last night trying to figure out how to fix it, it seems no one else in the comments on the video or on his website are having the same problem as me and even using someone elses code from his website I get the same results. I'll post the code from the one I got from the website as it's giving me the same error and the one I coded is a mess right now.
import requests
from bs4 import BeautifulSoup

def trade_spider(max_pages):
    page = 1
    while page <= max_pages:
        url = ""https://www.thenewboston.com/forum/category.php?id=15&orderby=recent&page="" + str(page) #this is page of popular posts
        source_code = requests.get(url)
        # just get the code, no headers or anything
        plain_text = source_code.text
        # BeautifulSoup objects can be sorted through easy
        for link in soup.findAll('a', {'class': 'index_singleListingTitles'}): #all links, which contains """" class='index_singleListingTitles' """" in it.
            href = ""https://www.thenewboston.com/"" + link.get('href')
            title = link.string # just the text, not the HTML
            print(href)
            print(title)
            # get_single_item_data(href)
    page += 1
trade_spider(1)

The full error is: ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:645)
I apologize if this is  a dumb question, I'm still new to programming but I seriously can't figure this out, I was thinking about just skipping this tutorial but it's bothering me not being able to fix this, thanks!
",https://stackoverflow.com/questions/34503206/ssl-certificate-verify-failed-error-when-scraping-https-www-thenewboston-co,program
20,Scraping a dynamic ecommerce page with infinite scroll,"
I'm using rvest in R to do some scraping. I know some HTML and CSS.
I want to get the prices of every product of a URI:
http://www.linio.com.co/tecnologia/celulares-telefonia-gps/
The new items load as you go down on the page (as you do some scrolling).
What I've done so far:
Linio_Celulares <- html(""http://www.linio.com.co/celulares-telefonia-gps/"")

Linio_Celulares %>%
  html_nodes("".product-itm-price-new"") %>%
  html_text()

And i get what i need, but just for the 25 first elements (those load for default). 
 [1] ""$ 1.999.900"" ""$ 1.999.900"" ""$ 1.999.900"" ""$ 2.299.900"" ""$ 2.279.900""
 [6] ""$ 2.279.900"" ""$ 1.159.900"" ""$ 1.749.900"" ""$ 1.879.900"" ""$ 189.900""  
[11] ""$ 2.299.900"" ""$ 2.499.900"" ""$ 2.499.900"" ""$ 2.799.000"" ""$ 529.900""  
[16] ""$ 2.699.900"" ""$ 2.149.900"" ""$ 189.900""   ""$ 2.549.900"" ""$ 1.395.900""
[21] ""$ 249.900""   ""$ 41.900""    ""$ 319.900""   ""$ 149.900"" 

Question: How to get all the elements of this dynamic section? 
I guess, I could scroll the page until all elements are loaded and then use html(URL). But this seems like a lot of work (i'm planning of doing this on different sections). There should be a programmatic work around.
",https://stackoverflow.com/questions/29861117/scraping-a-dynamic-ecommerce-page-with-infinite-scroll,program
21,How to click a link by text with No Text in Python,"
I am trying to scrape a Wine data from vivino.com and using selenium to automate it and scrape as many data as possible. My code looks like this:
import time 
from selenium import webdriver

browser = webdriver.Chrome('C:\Program Files (x86)\chromedriver.exe')

browser.get('https://www.vivino.com/explore?e=eJwFwbEOQDAUBdC_uaNoMN7NZhQLEXmqmiZaUk3x987xkVXRwLtAVcLLy7qE_tiN0Bz6FhcV7M4s0ZkkB86VUZIL9l4kmyjW4ORmbo0nTTPVDxlkGvg%3D&cart_item_source=nav-explore') # Vivino Website with 5 wines for now (simple example). Plan to scrape around 10,000 wines 

lenOfPage = browser.execute_script(""window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;"")

match=False
while(match==False):
    lastCount = lenOfPage
    time.sleep(7)
    lenOfPage = browser.execute_script(""window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;"")
    if lastCount==lenOfPage:
        match=True

That opens a website with 5 wines and scrolls down. Now I want to click to hyperlink of the wine one by one to scrape information about its price, wine grapes sort, etc. So, basically my script will try scroll down which allows to have as many wines displayed on the page and then click to a first hyperlink, get additional information and go back. Then, the process will repeat. I don't think that's an efficient strategy but that's what I came up so far.
The problem I have is with hyperlink in the vivino website. There is no text near the href link which allows me to use find_element_by_link_text function:
<a class=""anchor__anchor--2QZvA"" href=""/weingut-r-a-pfaffl-austrian-cherry-zweigelt/w/1261542?year=2018&amp;price_id=23409078&amp;cart_item_source=direct-explore"" target=""_blank"">

Could you please suggest the way how click for a wine with Selenium that has not text after the hyperlink? I haven't found proper answer during my web search. Thanks in advance
",https://stackoverflow.com/questions/65585597/how-to-click-a-link-by-text-with-no-text-in-python,program
22,R web scraping across multiple pages,"
I am working on a web scraping program to search for specific wines and return a list of local wines of that variety. The problem I am having is multiple page results. The code below is a basic example of what I am working with 
url2 <- ""http://www.winemag.com/?s=washington+merlot&search_type=reviews""
htmlpage2 <- read_html(url2)
names2 <- html_nodes(htmlpage2, "".review-listing .title"")
Wines2 <- html_text(names2)

For this specific search there are 39 pages of results. I know the url changes to http://www.winemag.com/?s=washington%20merlot&drink_type=wine&page=2, but is there an easy way to make the code loop through all the returned pages and compile the results from all 39 pages into a single list? I know I can manually do all the urls, but that seems like overkill. 
",https://stackoverflow.com/questions/36683510/r-web-scraping-across-multiple-pages,program
23,Running script upon login in mac OS X [closed],"






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.


Closed 10 years ago.


This post was edited and submitted for review 9 months ago and failed to reopen the post:

Original close reason(s) were not resolved






                        Improve this question
                    



I am wondering if anyone is able to help me out with getting a shell (.sh) program to automatically run whenever I log in to my account on my computer. I am running Mac OS X 10.6.7.
I have a file ""Example.sh"" that I want to run when I log onto my computer. I do not have a problem running it when I am already logged in, but I want this to run automatically.
",https://stackoverflow.com/questions/6442364/running-script-upon-login-in-mac-os-x,program
24,"Exception in thread ""main"" org.openqa.selenium.NoSuchElementException: Unable to locate element: //*[@id='login-email']","
I had to re-test the xpath, Previously it was working fine, But now it gives me an error. 
I tried with different locators as well, Like id, name. but still get the same error.
package staging;

import org.openqa.selenium.By;
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.firefox.FirefoxDriver;

public class login {

    public static void main (String[]args){
        System.setProperty(""webdriver.gecko.driver"",""C:\\Program Files\\geckodriver.exe"");
        WebDriver driver = new FirefoxDriver();

        //opening the browser
        driver.get(""https://staging.keela.co/login"");

        //logging
        driver.findElement(By.xpath(""//*[@id='login-email']"")).sendKeys(""bandanakeela@yopmail.com"");
        driver.findElement(By.xpath(""//*[@id='login-password']"")).sendKeys(""keela"");
        driver.findElement(By.xpath(""//*[@id='login-form']/div[3]/div/button"")).click();       
 }
}

",https://stackoverflow.com/questions/46202283/exception-in-thread-main-org-openqa-selenium-nosuchelementexception-unable-to,program
25,Have bash script answer interactive prompts [duplicate],"






This question already has answers here:
                        
                    



Passing arguments to an interactive program non-interactively

                                (5 answers)
                            

Closed 2 years ago.
The community reviewed whether to reopen this question 1 year ago and left it closed:

Original close reason(s) were not resolved




Is it possible to have a bash script automatically handle prompts that would normally be presented to the user with default actions?  Currently I am using a bash script to call an in-house tool that will display prompts to the user (prompting for Y/N) to complete actions, however the script I'm writing needs to be completely ""hands-off"", so I need a way to send Y|N to the prompt to allow the program to continue execution.  Is this possible?
",https://stackoverflow.com/questions/3804577/have-bash-script-answer-interactive-prompts,program
26,Automating running command on Linux from Windows using PuTTY,"
I have a scenario where I need to run a linux shell command frequently (with different filenames) from windows. I am using PuTTY and WinSCP to do that (requires login name and password).  The file is copied to a predefined folder in the linux machine through WinSCP and then the command is run from PuTTY. Is there a way by which I can automate this through a program. Ideally I would like to right click the file from windows and issue the command which would copy the file to remote machine and run the predefined command (in PuTTy) with the filename as argument.
",https://stackoverflow.com/questions/6147203/automating-running-command-on-linux-from-windows-using-putty,program
27,Disposing of Microsoft.Office.Interop.Word.Application,"
(Somewhat of a follow on from the post (which remains unanswered): https://stackoverflow.com/q/6197829/314661)
Using the following code
Application app = new Application();
_Document doc = app.Documents.Open(""myDocPath.docx"", false, false, false);
doc.PrintOut(false);
doc.Close();

I am attempting to open and print a file programmatically.
The problem is each time I run the above code a new WINWORD.exe process is started and obviously this quickly eats up all the memory.
The application class doesn't seem to contain a dispose/close or similar method.
After a bit of research I (realized) and changed the code to the following.
 Application app = new Application();
 _Document doc = app.Documents.Open(fullFilePath + "".doc"", false, false, false);
 doc.PrintOut(false);
 doc.Close();
 int res = System.Runtime.InteropServices.Marshal.ReleaseComObject(doc);
 int res1 = System.Runtime.InteropServices.Marshal.ReleaseComObject(app);

And I can see the remaining reference count is zero but the processes remain? 
PS: I'm using Version 14 of the Microsoft.Office.Interop library.
",https://stackoverflow.com/questions/6777422/disposing-of-microsoft-office-interop-word-application,program
28,How to print every executed line in GDB automatically until a given breakpoint is reached?,"
I would like to be able to set a breakpoint in GDB, and have it run to that point - and in the process, print out lines it has ""stepped through"".
Here is an example, based on this simple file with a main and a function, and two breakpoints for each: 
$ cat > test.c <<EOF
#include ""stdio.h""

int count=0;

void doFunction(void) {
  // two steps forward
  count += 2;
  // one step back
  count--;
}

int main(void) {
  // some pointless init commands;
  count = 1;
  count += 2;
  count = 0;
  //main loop
  while(1) {
    doFunction();
    printf(""%d\n"", count);
  }
}
EOF

$ gcc -g -Wall test.c -o test.exe
$ chmod +x test.exe
$ gdb -se test.exe
...
Reading symbols from /path/to/test.exe...done.
(gdb) b main
Breakpoint 1 at 0x80483ec: file test.c, line 14.
(gdb) b doFunction
Breakpoint 2 at 0x80483c7: file test.c, line 7.

To start the session, I need to run (r) the program, which will then stop at first breakpoint (main):
(gdb) r
Starting program: /path/to/test.exe 

Breakpoint 1, main () at test.c:14
14    count = 1;
(gdb) 

At this point - I can, for instance, hit continue (c); and the process will run through, not outputing anything, and break at the requested line: 
(gdb) c
Continuing.

Breakpoint 2, doFunction () at test.c:7
7     count += 2;
(gdb)

On the other hand, instead of continue - I can go line by line, either by using step (s) or next (n); for instance:
14    count = 1;
(gdb) n
15    count += 2;
(gdb) s
16    count = 0;
(gdb) s
19      doFunction();
(gdb) s

Breakpoint 2, doFunction () at test.c:7
7     count += 2;
(gdb) s
9     count--;
(gdb) s
10  }
(gdb) s
main () at test.c:20
20      printf(""%d\n"", count);
(gdb) s
...
(gdb) s
_IO_vfprintf_internal (s=Cannot access memory at address 0xe5853361
) at vfprintf.c:210
210 vfprintf.c: No such file or directory.
    in vfprintf.c
(gdb) s
245 in vfprintf.c
(gdb) s
210 in vfprintf.c
(gdb) n
245 in vfprintf.c
...
(gdb) n
2006    in vfprintf.c
(gdb) n
__printf (format=0x80484f0 ""%d\n"") at printf.c:39
39  printf.c: No such file or directory.
    in printf.c
(gdb) n
main () at test.c:21
21    }
(gdb) n
19      doFunction();
(gdb) n

Breakpoint 2, doFunction () at test.c:7
7     count += 2;
(gdb) 

Anyways, I am aware that I can keep Enter pressed, and the last entered command (step or next) will repeat (left a bit longer session in the second case, to show that 'next' remains on same level, 'step' steps inside the functions being called). However, as it can be seen, depending on whether step or next runs, it may take a while until a result is reached - and so, I don't want to sit for 10 minutes with my hand stuck on the Enter button :) 
So, my question is - can I somehow instruct gdb to run to 'breakpoint 2' without further user intervention - while printing out the lines it goes through, as if step (or next) was pressed?
",https://stackoverflow.com/questions/6947389/how-to-print-every-executed-line-in-gdb-automatically-until-a-given-breakpoint-i,program
29,How to use WebBrowser control DocumentCompleted event in C#?,"
Before starting writing this question, i was trying to solve following
// 1. navigate to page
// 2. wait until page is downloaded
// 3. read and write some data from/to iframe 
// 4. submit (post) form

The problem was, that if a iframe exists on a web page, DocumentCompleted event would get fired more then once (after each document has been completed). It was highly likely that program would have tried to read data from DOM that was not completed and naturally - fail.
But suddenly while writing this question 'What if' monster inspired me, and i fix'ed the problem, that i was trying to solve. As i failed Google'ing this, i thought it would be nice to post it here.
    private int iframe_counter = 1; // needs to be 1, to pass DCF test
    public bool isLazyMan = default(bool);

    /// <summary>
    /// LOCK to stop inspecting DOM before DCF
    /// </summary>
    public void waitPolice() {
        while (isLazyMan) Application.DoEvents();
    }

    private void webBrowser1_Navigating(object sender, WebBrowserNavigatingEventArgs e) {
        if(!e.TargetFrameName.Equals(""""))
            iframe_counter --;
        isLazyMan = true;
    }

    private void webBrowser1_DocumentCompleted(object sender, WebBrowserDocumentCompletedEventArgs e) {
        if (!((WebBrowser)sender).Document.Url.Equals(e.Url))
            iframe_counter++;
        if (((WebBrowser)sender).Document.Window.Frames.Count <= iframe_counter) {//DCF test
            DocumentCompletedFully((WebBrowser)sender,e);
            isLazyMan = false; 
        }
    }

    private void DocumentCompletedFully(WebBrowser sender, WebBrowserDocumentCompletedEventArgs e){
        //code here
    }

For now at least, my 5m hack seems to be working fine. 
Maybe i am really failing at querying google or MSDN, but i can not find:
""How to use webbrowser control DocumentCompleted event in C# ?""
Remark: After learning a lot about webcontrol, I found that it does FuNKY stuff. 
Even if you detect that the document has completed, in most cases it wont stay like that forever. Page update can be done in several ways - frame refresh, ajax like request or server side push (you need to have some control that supports asynchronous communication and has html or JavaScript interop). Also some iframes will never load, so it's not best idea to wait for them forever. 
I ended up using:
if (e.Url != wb.Url)

",https://stackoverflow.com/questions/840813/how-to-use-webbrowser-control-documentcompleted-event-in-c,program
30,Interact with other programs using Python,"
I'm having the idea of writing a program using Python which shall find a lyric of a song whose name I provided. I think the whole process should boil down to couple of things below. These are what I want the program to do when I run it:

prompt me to enter a name of a song
copy that name
open a web browser (google chrome for example)
paste that name in the address bar and find information about the song
open a page that contains the lyrics
copy that lyrics
run a text editor (like Microsoft Word for instance)
paste the lyrics
save the new text file with the name of the song

I am not asking for code, of course. I just want to know the concepts or ideas about how to use python to interact with other programs
To be more specific, I think I want to know, fox example, just how we point out where is the address bar in Google Chrome and tell python to paste the name there. Or how we tell python how to copy the lyrics as well as paste it into the Microsof Word's sheet then save it.
I've been reading (I'm still reading) several books on Python: Byte of python, Learn python the hard way, Python for dummies, Beginning Game Development with Python and Pygame. However, I found out that it seems like I only (or almost only) learn to creat programs that work on itself (I can't tell my program to do things I want with other programs that are already installed on my computer)
I know that my question somehow sounds rather silly, but I really want to know how it works, the way we tell Python to regconize that this part of the Google chrome browser is the address bar and that it should paste the name of the song in it. The whole idea of making python interact with another program is really really vague to me and I just 
extremely want to grasp that.
Thank you everyone, whoever spend their time reading my so-long question.
ttriet204
",https://stackoverflow.com/questions/14288177/interact-with-other-programs-using-python,program
31,Schedule automatic daily upload with FileZilla [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.


Closed 7 years ago.


The community reviewed whether to reopen this question 1 year ago and left it closed:

Original close reason(s) were not resolved






                        Improve this question
                    



I would like to use FileZilla to automatically upload PDFs to my GoDaddy hosted site daily, replacing the previous day's sheets. Is there any way to do this? I read online that batch files might work, could someone post a sample version of a batch file that would do the trick?
",https://stackoverflow.com/questions/24945709/schedule-automatic-daily-upload-with-filezilla,program
32,Programmatically add trusted sites to Internet Explorer,"
I'm doing an IE automation project using WatiN. 
When a file to be downloaded is clicked, I get the following in the Internet Explorer Information bar:

To help protect your security,
  Internet Explorer has blocked this
  site from downloading files to you
  computer.

In order to download the report, I can manually add the site to Internet Explorer's list of trusted sites, but I would prefer to check programmatically in .NET to see if the site is trusted and add it to the list if it is not. 
FYI, I'm currently using IE7.
",https://stackoverflow.com/questions/972345/programmatically-add-trusted-sites-to-internet-explorer,program
33,Accessing Excel Custom Document Properties programmatically,"
I'm trying to add custom properties to a workbook I have created programmatically.  I have a method in place for getting and setting properties, but the problem is the workbook is returning null for the CustomDocumentProperties property.  I cannot figure out how to initialize this property so that I can add and retrieve properties from the workbook.  Microsoft.Office.Core.DocumentProperties is an interface, so I cant go and do the following
if(workbook.CustomDocumentProperties == null)
    workbook.CustomDocumentProperties = new DocumentProperties;

Here is the code I have to get and set the properties:
     private object GetDocumentProperty(string propertyName, MsoDocProperties type)
    {
        object returnVal = null;

        Microsoft.Office.Core.DocumentProperties properties;
        properties = (Microsoft.Office.Core.DocumentProperties)workBk.CustomDocumentProperties;

        foreach (Microsoft.Office.Core.DocumentProperty property in properties)
        {
            if (property.Name == propertyName && property.Type == type)
            {
                returnVal = property.Value;
            }
            DisposeComObject(property);
        }

        DisposeComObject(properties);

        return returnVal;
    }

    protected void SetDocumentProperty(string propertyName, string propertyValue)
    {
        DocumentProperties properties;
        properties = workBk.CustomDocumentProperties as DocumentProperties;

        bool propertyExists = false;
        foreach (DocumentProperty prop in properties)
        {
            if (prop.Name == propertyName)
            {
                prop.Value = propertyValue;
                propertyExists = true;
            }
            DisposeComObject(prop);

            if(propertyExists) break;
        }

        if (!propertyExists)
        {
            properties.Add(propertyName, false, MsoDocProperties.msoPropertyTypeString, propertyValue, Type.Missing);
        }

        DisposeComObject(propertyExists);

    }

The line
    properties = workBk.CustomDocumentProperties as DocumentProperties;
always set properties to null.
This is using Microsoft.Office.Core v12.0.0.0 and Microsoft.Office.Interop.Excell v12.0.0.0 (Office 2007)
",https://stackoverflow.com/questions/1137763/accessing-excel-custom-document-properties-programmatically,program
34,How to send input to the console as if the user is typing?,"
This is my problem. I have a program that has to run in a TTY, cygwin provides this TTY. When I redirect stdIn the program fails because it does not have a TTY. I cannot modify this program, and need some way of automating it. 
How can I grab the cmd.exe window and send it data and make it think the user is typing it? 
I'm using C#, I believe there is a way to do it with java.awt.Robot but I have to use C# for other reasons.
",https://stackoverflow.com/questions/451228/how-to-send-input-to-the-console-as-if-the-user-is-typing,program
35,VBA:IE-How to assign pathname to file input tag without popup file upload form?,"
I am currently doing automaiton for file uploading
Below is HTML tag for input file tag:
 <input name=""file"" title=""Type the path of the file or click the Browse button to find the file."" id=""file"" type=""file"" size=""20"">

And below is button HTML Tag:
<input name=""Attach"" title=""Attach File (New Window)"" class=""btn"" id=""Attach"" onclick=""javascript:setLastMousePosition(event); window.openPopup('/widg/uploadwaiting.jsp', 'uploadWaiting', 400, 130, 'width=400,height=130,resizable=no,toolbar=no,status=no,scrollbars=no,menubar=no,directories=no,location=no,dependant=no', true);"" type=""submit"" value=""Attach File"">

My VBA coding is:
Dim filee As Object
Set filee = mydoc.getElementById(""file"")
filee.Value = filenamepath

Set attach = mydoc.getElementsByName(""Attach"")
attach(0).Click

When I am running this coding, input filepath box not assign path name so i am getting chose file path.
Find attach screenshot.
 
Finally i have tried below code but that send key not executing  
Dim filee As Object
    Set filee = mydoc.getElementById(""file"")
    filee.Click

obj.SetText filename
obj.PutInClipboard
SendKeys ""^v""
SendKeys ""{ENTER}""

Set attach = mydoc.getElementsByName(""Attach"")
    attach(0).Click

Set finall = mydoc.getElementsByName(""cancel"")
    finall(0).Click

Kindly tell me the windows API program to assign my file name directory in fine name: input box on opened Choose File to Open explorer and click the open button. 
",https://stackoverflow.com/questions/34803863/vbaie-how-to-assign-pathname-to-file-input-tag-without-popup-file-upload-form,program
36,Programmatically building htpasswd,"
Is there a programmatic way to build htpasswd files, without depending on OS specific functions (i.e. exec(), passthru())?
",https://stackoverflow.com/questions/39916/programmatically-building-htpasswd,program
37,Clicking a button on a page using a Greasemonkey/userscript in Chrome,"
I'm going to be as absolutely verbose here as possible as I've run into a few solutions that didn't end up panning out. Please keep in mind that I don't know Javascript. I know basic HTML and CSS. I don't have any actual programming background but I'm trying to learn bit by bit by researching basic tasks like this. Please talk to me like I'm an idiot. Any lingo I throw around in this post I learned while researching this specific issue. I'm writing this userscript as a personal project and to share with some friends.
What I'm trying to do.
I'm trying to write a userscript for Chrome/Greasemonkey (Chrome is my target browser) that will click the Refresh button on the Battlefield 3 server browser. For those of you that don't know, Battlefield 3 uses a web site paired with a browser plugin for VOIP and actually launching the game via a server browser. The bulk of it seems to be fairly straight forward HTML arranged in tables.
The idea is that when viewing the main page for a server that is full, the script will click the Refresh button every three seconds or so until the page reports an open spot on the server, then stop the refresh loop and click the join server button. I've already got the part of the script running that polls the server current and maximum players then assigns them to their own variables.
At this point I'm trying to get a click to work in the console so I can actually put it to some use in my script and am having zero luck.
The code I'm trying to manipulate.
This is the div for the button that I'm trying to click pulled from the Chrome dev tools:
<div class=""serverguide-header-refresh-button alternate show""> 
<div type=""reset"" class=""common-button-medium-grey"">
<p style=""position: relative;"">
<a href=""/bf3/servers/show/c7088bdc-2806-4758-bf93-2106792b34d8/"">Refresh </a>
</p>
</div>
</div>

(That link is not static. It's a link to a specific server page)
What I've tried.
To actually find the button I'm using getElementsByClassName. It doesn't have a unique ID but the class is unique to that element on this particular page so getElementsByClassName(""serverguide-header-refresh-button"")[0] is pulling the proper div each time. It's getting the script to perform any actual action on the button that's the problem.
document.getElementsByClassName(""serverguide-header-refresh-button"")[0].click();

I now realize this didn't work because it's not a conventional submit button. I don't understand the specifics of the standard here but I get that it doesn't support the .click() method.
function addJQuery(callback) {
  var script = document.createElement(""script"");
  script.setAttribute(""src"", ""http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js"");
  script.addEventListener('load', function() {
    var script = document.createElement(""script"");
    script.textContent = ""("" + callback.toString() + "")();"";
    document.body.appendChild(script);
  }, false);
  document.body.appendChild(script);
}

// the guts of this userscript
function main() {
  unsafeWindow.jQuery('.serverguide-header-refresh-button')[0].click();
}

// load jQuery and execute the main function
addJQuery(main);

This is simply unsafeWindow.jQuery('.serverguide-header-refresh-button').click(); wrapped in some code to load jQuery for userscripts. It was a bit I picked up elsewhere but was told it would only work if jQuery was loaded on the page. I figured it was worth a try. This is one of those I have no idea what I'm doing shots in the dark and it didn't work. I tried the same thing below with another snippet of jQuery code I picked up:
function addJQuery(callback) {
  var script = document.createElement(""script"");
  script.setAttribute(""src"", ""http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js"");
  script.addEventListener('load', function() {
    var script = document.createElement(""script"");
    script.textContent = ""("" + callback.toString() + "")();"";
    document.body.appendChild(script);
  }, false);
  document.body.appendChild(script);
}

// the guts of this userscript
function main() {
   var evObj = document.createEvent('Events');
      evObj.initEvent(""click"", true, false);
      document.getElementsByClassName('serverguide-header-refresh-button')[0].dispatchEvent(evObj);
}

    // load jQuery and execute the main function
addJQuery(main);

Both of these return Undefined in the Chrome and Firebug consoles.
So, would anyone be so kind as to help me create a bit of code for this script to press the Refresh button on this page?
",https://stackoverflow.com/questions/8192126/clicking-a-button-on-a-page-using-a-greasemonkey-userscript-in-chrome,program
38,Windows Console Application Getting Stuck (Needs Key Press) [duplicate],"






This question already has answers here:
                        
                    



How and why does QuickEdit mode in Command Prompt freeze applications?

                                (2 answers)
                            

Closed 6 years ago.



I have a console program that has different components that run like this:  
void start() {
while(true){
     DoSomething();
     Thread.Sleep(1000*5);
}
}

My main entry point looks like [pseudo-ish code]
Thread.Start(Componenet1.Start);
Thread.Start(Componenet2.Start);

while(true){
     Console.Writeline(""running"");
     Thread.Sleep(1000*5);
}

There are no Console.Reads anywhere. My problem is SOMETIMES the application will be running great but then stop and if I press any key on the window it will start working again. This happens fairly infrequently but I have this program deployed on 100+ VM's running 24/7 in an automated environment.
Also on the computer I have some AHK scripts and other stuff that manipulate the mouse but not sure if that has anything to do with it.
Also note that sometimes the CPU can really be running at 100% on the machines so maybe thread priority is an issue?
SOLUTION: You need to disable quick edit mode. Here is working C# code to do this:
 // http://msdn.microsoft.com/en-us/library/ms686033(VS.85).aspx
    [DllImport(""kernel32.dll"")]
    public static extern bool SetConsoleMode(IntPtr hConsoleHandle, uint dwMode);

    private const uint ENABLE_EXTENDED_FLAGS = 0x0080;

    static void Main(string[] args)
    {
         IntPtr handle = Process.GetCurrentProcess().MainWindowHandle;
         SetConsoleMode(handle, ENABLE_EXTENDED_FLAGS);

",https://stackoverflow.com/questions/4453692/windows-console-application-getting-stuck-needs-key-press,program
39,Headless Browser for Python (Javascript support REQUIRED!) [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 Questions asking us to recommend or find a tool, library or favorite off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, describe the problem and what has been done so far to solve it.


Closed 8 years ago.







                        Improve this question
                    



I need a headless browser which is fairly easy to use (I am still fairly new to Python and programming in general) which will allow me to navigate to a page, log into a form that requires Javascript, and then scrape the resulting web page by searching for results matching certain criteria, clicking check boxes, and clicking to download files. All of this requires Javascript.
I hear a headless browser is what I want - requirements/preferences are that I be able to run it from Python, and preferably that the resultant script will be compilable by py2exe (I am writing this program for other users).
So far Windmill looks like it MIGHT be what I want, but I am not sure.
Any ideas appreciated!
",https://stackoverflow.com/questions/6025082/headless-browser-for-python-javascript-support-required,program
40,How I can get web page's content and save it into the string variable,"
How I can get the content of the web page using ASP.NET?  I need to write a program to get the HTML of a webpage and store it into a string variable.
",https://stackoverflow.com/questions/4510212/how-i-can-get-web-pages-content-and-save-it-into-the-string-variable,program
41,Screen Scraping from a web page with a lot of Javascript [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



I have been asked to write an app which screen scrapes info from an intranet web page and presents the certain info from it in a nice easy to view format. The web page is a real mess and requires the user to click on half a dozen icons to discover if an ordered item has arrived or has been receipted. As you can imagine users find this irritating to say the least and it would be nice to have an app anyone can use that lists the state of their orders in a single screen.
Yes I know a better solution would be to re-write the web app but that would involve calling in the vendor and would cost us as small fortune.
Anyway while looking into this I discovered the web page I want to scrape is mostly Javascript (although it doesn't use any AJAX techniques). Does anyone know if a library or program exists which I could feed with the Javascript and which would then spit out the DOM for my app to parse ? 
I can pretty much write the app in any language but my preference would be JavaFX just so I could have a play with it.
Thanks for your time.
Ian
",https://stackoverflow.com/questions/857515/screen-scraping-from-a-web-page-with-a-lot-of-javascript,program
42,How to use the WebClient.DownloadDataAsync() method in this context?,"
My plan is to have a user write down a movie title in my program and my program will pull the appropiate information asynchronously so the UI doesn't freeze up.
Here's the code:
public class IMDB
    {
        WebClient WebClientX = new WebClient();
        byte[] Buffer = null;


        public string[] SearchForMovie(string SearchParameter)
        {
            //Format the search parameter so it forms a valid IMDB *SEARCH* url.
            //From within the search website we're going to pull the actual movie
            //link.
            string sitesearchURL = FindURL(SearchParameter);

            //Have a method download asynchronously the ENTIRE source code of the
            //IMDB *search* website.
            Buffer = WebClientX.DownloadDataAsync(sitesearchURL);


            //Pass the IMDB source code to method findInformation().

            //string [] lol = findInformation();

            //????

            //Profit.

            string[] lol = null;
            return lol;
        }

My actual problem lies in the WebClientX.DownloadDataAsync() method. I can't use a string URL for it. How can I use that built in function to download the bytes of the site (for later use I will convert this to string, I know how to do this) and without freezing up my GUI?
Perhaps a clear cut example of the DownloadDataAsync so I can learn how to use it?
Thanks SO, you're always such a great resource.
",https://stackoverflow.com/questions/1585985/how-to-use-the-webclient-downloaddataasync-method-in-this-context,program
43,Text Extraction from HTML Java,"
I'm working on a program that downloads HTML pages and then selects some of the information and write it to another file.
I want to extract the information which is intbetween the paragraph tags, but i can only get one line of the paragraph. My code is as follows;
FileReader fileReader = new FileReader(file);
BufferedReader buffRd = new BufferedReader(fileReader);
BufferedWriter out = new BufferedWriter(new FileWriter(newFile.txt));
String s;

while ((s = br.readLine()) !=null) {
    if(s.contains(""<p>"")) {
        try {
            out.write(s);
        } catch (IOException e) {
        }
    }
}

i was trying to add another while loop, which would tell the program to keep writing to file until the line contains the </p> tag, by saying;
while ((s = br.readLine()) !=null) {
    if(s.contains(""<p>"")) {
        while(!s.contains(""</p>"") {
            try {
                out.write(s);
            } catch (IOException e) {
            }
        }
    }
}

But this doesn't work. Could someone please help.
",https://stackoverflow.com/questions/1386107/text-extraction-from-html-java,program
44,What's the best way of scraping data from a website? [closed],"






Closed. This question is opinion-based. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 8 years ago.







                        Improve this question
                    



I need to extract contents from a website, but the application doesn鈥檛 provide any application programming interface or another mechanism to access that data programmatically.
I found a useful third-party tool called Import.io that provides click and go functionality for scraping web pages and building data sets, the only thing is I want to keep my data locally and I don't want to subscribe to any subscription plans.
What kind of technique does this company use for scraping the web pages and building their datasets? I found some web scraping frameworks pjscrape & Scrapy could they provide such a feature
",https://stackoverflow.com/questions/22168883/whats-the-best-way-of-scraping-data-from-a-website,program
45,Scraping Real Time Visitors from Google Analytics,"
I have a lot of sites and want to build a dashboard showing the number of real time visitors on each of them on a single page. (would anyone else want this?) Right now the only way to view this information is to open a new tab for each site.
Google doesn't have a real-time API, so I'm wondering if it is possible to scrape this data. Eduardo Cereto found out that Google transfers the real-time data over the realtime/bind network request. Anyone more savvy have an idea of how I should start? Here's what I'm thinking:

Figure out how to authenticate programmatically

Inspect all of the realtime/bind requests to see how they change. Does each request have a unique key? Where does that come from? Below is my breakdown of the request:
https://www.google.com/analytics/realtime/bind?VER=8
&key= [What is this? Where does it come from? 21 character lowercase alphanumeric, stays the same each request]
&ds= [What is this? Where does it come from? 21 character lowercase alphanumeric, stays the same each request]
&pageId=rt-standard%2Frt-overview
&q=t%3A0%7C%3A1%3A0%3A%2Ct%3A11%7C%3A1%3A5%3A%2Cot%3A0%3A0%3A4%2Cot%3A0%3A0%3A3%2Ct%3A7%7C%3A1%3A10%3A6%3D%3DREFERRAL%3B%2Ct%3A10%7C%3A1%3A10%3A%2Ct%3A18%7C%3A1%3A10%3A%2Ct%3A4%7C5%7C2%7C%3A1%3A10%3A2!%3Dzz%3B%2C&f
The q variable URI decodes to this (what the?):
t:0|:1:0:,t:11|:1:5:,ot:0:0:4,ot:0:0:3,t:7|:1:10:6==REFERRAL;,t:10|:1:10:,t:18|:1:10:,t:4|5|2|:1:10:2!=zz;,&f
&RID=rpc
&SID= [What is this? Where does it come from? 16 character uppercase alphanumeric, stays the same each request]
&CI=0
&AID= [What is this? Where does it come from? integer, starts at 1, increments weirdly to 150 and then 298]
&TYPE=xmlhttp
&zx= [What is this? Where does it come from? 12 character lowercase alphanumeric, changes each request]
&t=1

Inspect all of the realtime/bind responses to see how they change. How does the data come in? It looks like some altered JSON. How many times do I need to connect to get the data? Where is the active visitors on site number in there? Here is a dump of sample data:


19
[[151,[""noop""]
]
]
388
[[152,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[49,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[0,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3,2,0],""name"":""Total""}]}}]]]
]
388
[[153,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[52,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[2,1,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3,2],""name"":""Total""}]}}]]]
]
388
[[154,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[53,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[0,3,1,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3],""name"":""Total""}]}}]]]
]

Let me know if you can help with any of the items above!

",https://stackoverflow.com/questions/11021554/scraping-real-time-visitors-from-google-analytics,program
46,Programmatic Python Browser with JavaScript,"
I want to screen-scrape a web-site that uses JavaScript. 
There is mechanize, the programmatic web browser for Python. However, it (understandably) doesn't interpret javascript. Is there any programmatic browser for Python which does? If not, is there any JavaScript implementation in Python that I could use to attempt to create one?
",https://stackoverflow.com/questions/1916711/programmatic-python-browser-with-javascript,program
47,OpenGL/D3D: How do I get a screen grab of a game running full screen in Windows?,"
Suppose I have an OpenGL game running full screen (Left 4 Dead 2). I'd like to programmatically get a screen grab of it and then write it to a video file. 
I've tried GDI, D3D, and OpenGL methods (eg glReadPixels) and either receive a blank screen or flickering in the capture stream.
Any ideas?
For what it's worth, a canonical example of something similar to what I'm trying to achieve is Fraps.
",https://stackoverflow.com/questions/3486729/opengl-d3d-how-do-i-get-a-screen-grab-of-a-game-running-full-screen-in-windows,program
48,CasperJS click event having AJAX call,"
I am trying to fetch data from a site by simulating events using CasperJS with phantomJS 1.7.0.
I am able to simulate normal click events and select events. But my code fails in following scenario:
When I click on button / anchor etc on remote page, the click on remote page  initiates an AJAX call / JS call(depending on how that page is implemented by programmer.). 
In case of JS call, my code works and I get changed data. But for clicks where is AJAX call is initiated, I do not get updated data.
For debugging, I tried to get the page source of the element container(before and after), but I see no change in code. 
I tried to set wait time from 10 sec to 1 ms range, but that to does not reflect any changes in behavior. 
Below is my piece of code for clicking. I am using an array of CSS Paths, which represents which element(s) to click.
/*Click on array of clickable elements using CSS Paths.*/
fn_click = function(){
casper.each(G_TAGS,function(casper, cssPath, count1) 
                    {
                            casper.then ( function() {
                            casper.click(cssPath);

                            this.echo('DEBUG AFTER CLICKING -START HTML ');
                            //this.echo(this.getHTML(""CONTAINER WHERE DETAILS CHANGE""));
                            this.echo('DEBUG AFTER CLICKING -START HTML');
                            casper.wait(5000, function() 
                                                    {   

                                                        casper.then(fn_getData);
                                                    } 
                                    );
                            });     
                    });
};

UPDATE:
I tried to use remote-debug option from phantomJS, to debug above script. 
It is not working. I am on windows. I will try to run remote debugging on Ubuntu as well. 
Please help me. I would appreciate any help on this. 
UPDATE:
Please have a look at following code as a sample. 
https://gist.github.com/4441570

Content before click and after click are same. 
I am clicking on sorting options provided under tag (votes / activity etc.). 
",https://stackoverflow.com/questions/14098483/casperjs-click-event-having-ajax-call,program
49,BeautifulSoup: How do I extract all the <li>s from a list of <ul>s that contains some nested <ul>s?,"
I'm a newbie programmer trying to jump in to Python by building a script that scrapes http://en.wikipedia.org/wiki/2000s_in_film and extracts a list of ""Movie Title (Year)"".
My HTML source looks like:
<h3>Header3 (Start here)</h3>
<ul>
    <li>List items</li>
    <li>Etc...</li>
</ul>
<h3>Header 3</h3>
<ul>
    <li>List items</li>
    <ul>
        <li>Nested list items</li>
        <li>Nested list items</li></ul>
    <li>List items</li>
</ul>
<h2>Header 2 (end here)</h2>

I'd like all the li tags following the first h3 tag and stopping at the next h2 tag, including all nested li tags.
firstH3 = soup.find('h3')

...correctly finds the place I'd like to start.
firstH3 = soup.find('h3') # Start here
uls = []
for nextSibling in firstH3.findNextSiblings():
    if nextSibling.name == 'h2':
        break
    if nextSibling.name == 'ul':
        uls.append(nextSibling)

...gives me a list uls, each with li contents that I need.
Excerpt of the uls list:
<ul>
...
    <li><i><a href=""/wiki/Agent_Cody_Banks"" title=""Agent Cody Banks"">Agent Cody Banks</a></i> (2003)</li>
    <li><i><a href=""/wiki/Agent_Cody_Banks_2:_Destination_London"" title=""Agent Cody Banks 2: Destination London"">Agent Cody Banks 2: Destination London</a></i> (2004)</li>
    <li>Air Bud series:
        <ul>
            <li><i><a href=""/wiki/Air_Bud:_World_Pup"" title=""Air Bud: World Pup"">Air Bud: World Pup</a></i> (2000)</li>
            <li><i><a href=""/wiki/Air_Bud:_Seventh_Inning_Fetch"" title=""Air Bud: Seventh Inning Fetch"">Air Bud: Seventh Inning Fetch</a></i> (2002)</li>
            <li><i><a href=""/wiki/Air_Bud:_Spikes_Back"" title=""Air Bud: Spikes Back"">Air Bud: Spikes Back</a></i> (2003)</li>
            <li><i><a href=""/wiki/Air_Buddies"" title=""Air Buddies"">Air Buddies</a></i> (2006)</li>
        </ul>
    </li>
    <li><i><a href=""/wiki/Akeelah_and_the_Bee"" title=""Akeelah and the Bee"">Akeelah and the Bee</a></i> (2006)</li>
...
</ul>

But I'm unsure of where to go from here.

Update:
Final Code:
lis = []
    for ul in uls:
        for li in ul.findAll('li'):
            if li.find('ul'):
                break
            lis.append(li)

    for li in lis:
        print li.text.encode(""utf-8"")

The if...break throws out the LI's that contain UL's since the nested LI's are now duplicated.
Print output is now:


102 Dalmatians(2000)
10th & Wolf(2006)
11:14(2006)
12:08 East of Bucharest(2006)
13 Going on 30(2004)
1408(2007)
...


",https://stackoverflow.com/questions/4362981/beautifulsoup-how-do-i-extract-all-the-lis-from-a-list-of-uls-that-contains,program
50,Screen-scraping a windows application in c# [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 2 years ago.







                        Improve this question
                    



I need to scrape data from a windows application to run a query in another program. Does anyone know of a good starting point for me to do this in .NET?
",https://stackoverflow.com/questions/375117/screen-scraping-a-windows-application-in-c-sharp,program
51,Detecting 'stealth' web-crawlers,"
What options are there to detect web-crawlers that do not want to be detected?
(I know that listing detection techniques will allow the smart stealth-crawler programmer to make a better spider, but I do not think that we will ever be able to block smart stealth-crawlers anyway, only the ones that make mistakes.)
I'm not talking about the nice crawlers such as Googlebot and Yahoo! Slurp.
I consider a bot nice if it:

identifies itself as a bot in the user agent string
reads robots.txt (and obeys it)

I'm talking about the bad crawlers, hiding behind common user agents, using my bandwidth and never giving me anything in return.
There are some trapdoors that can be constructed updated list (thanks Chris, gs):

Adding a directory only listed (marked as disallow) in the robots.txt,
Adding invisible links (possibly marked as rel=""nofollow""?),

style=""display: none;"" on link or parent container
placed underneath another element with higher z-index


detect who doesn't understand CaPiTaLiSaTioN,
detect who tries to post replies but always fail the Captcha.
detect GET requests to POST-only resources
detect interval between requests
detect order of pages requested
detect who (consistently) requests HTTPS resources over HTTP
detect who does not request image file (this in combination with a list of user-agents of known image capable browsers works surprisingly nice)

Some traps would be triggered by both 'good' and 'bad' bots.
you could combine those with a whitelist:

It trigger a trap
It request robots.txt?
It doest not trigger another trap because it obeyed robots.txt

One other important thing here is:
Please consider blind people using a screen readers: give people a way to contact you, or solve a (non-image) Captcha to continue browsing.
What methods are there to automatically detect the web crawlers trying to mask themselves as normal human visitors.
The question is not: How do I catch every crawler. The question is: How can I maximize the chance of detecting a crawler.
Some spiders are really good, and actually parse and understand HTML, xhtml, CSS JavaScript, VBScript etc...
I have no illusions: I won't be able to beat them.
You would however be surprised how stupid some crawlers are. With the best example  of stupidity (in my opinion) being: cast all URLs to lower case before requesting them.
And then there is a whole bunch of crawlers that are just 'not good enough' to avoid the various trapdoors.
",https://stackoverflow.com/questions/233192/detecting-stealth-web-crawlers,program
52,Web crawler that can interpret JavaScript [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



I want to write a web crawler that can interpret JavaScript. Basically its a program in Java or PHP that takes a URL as input and outputs the DOM tree which is similar to the output in Firebug HTML window. The best example is Kayak.com where you can not see the resulting DOM displayed on the browser when you 'view source' but can save the resulting HTML though Firebug. 
How would I go about doing this? What tools exist that would help me?
",https://stackoverflow.com/questions/2670082/web-crawler-that-can-interpret-javascript,program
53,How to get a web page's source code from Java [duplicate],"






This question already has answers here:
                        
                    



How do you Programmatically Download a Webpage in Java

                                (11 answers)
                            

Closed 7 years ago.



I just want to retrieve any web page's source code from Java. I found lots of solutions so far, but I couldn't find any code that works for all the links below: 

http://www.cumhuriyet.com.tr?hn=298710
http://www.fotomac.com.tr/Yazarlar/Olcay%20%C3%87ak%C4%B1r/2011/11/23/hesap-makinesi 
http://www.sabah.com.tr/Gundem/2011/12/23/basbakan-konferansta-konusuyor#

The main problem for me is that some codes retrieve web page source code, but with missing ones. For example the code below does not work for the first link.
InputStream is = fURL.openStream(); //fURL can be one of the links above
BufferedReader buffer = null;
buffer = new BufferedReader(new InputStreamReader(is, ""iso-8859-9""));

int byteRead;
while ((byteRead = buffer.read()) != -1) {
    builder.append((char) byteRead);
}
buffer.close();
System.out.println(builder.toString());

",https://stackoverflow.com/questions/8616781/how-to-get-a-web-pages-source-code-from-java,program
54,How to request Google to re-crawl my website? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 This question does not appear to be about programming within the scope defined in the help center.


Closed 7 years ago.







                        Improve this question
                    



Does someone know a way to request Google to re-crawl a website? If possible, this shouldn't last months. My site is showing an old title in Google's search results. How can I show it with the correct title and description? 
",https://stackoverflow.com/questions/9466360/how-to-request-google-to-re-crawl-my-website,program
55,"Save complete web page (incl css, images) using python/selenium","
I am using Python/Selenium to submit genetic sequences to an online database, and want to save the full page of results I get back. Below is the code that gets me to the results I want:
from selenium import webdriver

URL = 'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome'
SEQUENCE = 'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA' #'GAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGA'
CHROME_WEBDRIVER_LOCATION = '/home/max/Downloads/chromedriver' # update this for your machine

# open page with selenium
# (first need to download Chrome webdriver, or a firefox webdriver, etc)
driver = webdriver.Chrome(executable_path=CHROME_WEBDRIVER_LOCATION)
driver.get(URL)
time.sleep(5)

# enter sequence into the query field and hit 'blast' button to search
seq_query_field = driver.find_element_by_id(""seq"")
seq_query_field.send_keys(SEQUENCE)

blast_button = driver.find_element_by_id(""b1"")
blast_button.click()
time.sleep(60)

At that point I have a page that I can manually click ""save as,"" and get a local file (with a corresponding folder of image/js assets) that lets me view the whole returned page locally (minus content which is generated dynamically from scrolling down the page, which is fine). I assumed there would be a simple way to mimic this 'save as' function in python/selenium but haven't found one. The code to save the page below just saves html, and does not leave me with a local file that looks like it does in the web browser, with images, etc.
content = driver.page_source
with open('webpage.html', 'w') as f:
    f.write(content)

I've also found this question/answer on SO, but the accepted answer just brings up the 'save as' box, and does not provide a way to click it (as two commenters point out)
Is there a simple way to 'save [full page] as' using python? Ideally I'd prefer an answer using selenium since selenium makes the crawling part so straightforward, but I'm open to using another library if there's a better tool for this job. Or maybe I just need to specify all of the images/tables I want to download in code, and there is no shortcut to emulating the right-click 'save as' functionality?
UPDATE - Follow up question for James' answer
So I ran James' code to generate a page.html (and associated files) and compared it to the html file I got from manually clicking save-as. The page.html saved via James' script is great and has everything I need, but when opened in a browser it also shows a lot of extra formatting text that's hidden in the manually save'd page. See attached screenshot (manually saved page on the left, script-saved page with extra formatting text shown on right). 

This is especially surprising to me because the raw html of the page saved by James' script seems to indicate those fields should still be hidden. See e.g. the html below, which appears the same in both files, but the text at issue only appears in the browser-rendered page on the one saved by James' script:
<p class=""helpbox ui-ncbitoggler-slave ui-ncbitoggler"" id=""hlp1"" aria-hidden=""true"">
These options control formatting of alignments in results pages. The
default is HTML, but other formats (including plain text) are available.
PSSM and PssmWithParameters are representations of Position Specific Scoring Matrices and are only available for PSI-BLAST. 
The Advanced view option allows the database descriptions to be sorted by various indices in a table.
</p>

Any idea why this is happening?
",https://stackoverflow.com/questions/53729201/save-complete-web-page-incl-css-images-using-python-selenium,program
56,Node.JS: How to pass variables to asynchronous callbacks? [duplicate],"






This question already has answers here:
                        
                    



JavaScript closure inside loops 鈥?simple practical example

                                (44 answers)
                            

Closed 6 years ago.



I'm sure my problem is based on a lack of understanding of asynch programming in node.js but here goes.
For example: I have a list of links I want to crawl. When each asynch request returns I want to know which URL it is for. But, presumably because of race conditions, each request returns with the URL set to the last value in the list.
var links = ['http://google.com', 'http://yahoo.com'];
for (link in links) {
    var url = links[link];
    require('request')(url, function() {
        console.log(url);
    });
}

Expected output:
http://google.com
http://yahoo.com

Actual output:
http://yahoo.com
http://yahoo.com

So my question is either:

How do I pass url (by value) to the call back function? OR
What is the proper way of chaining the HTTP requests so they run sequentially? OR
Something else I'm missing?

PS: For 1. I don't want a solution which examines the callback's parameters but a general way of a callback knowing about variables 'from above'.
",https://stackoverflow.com/questions/13221769/node-js-how-to-pass-variables-to-asynchronous-callbacks,program
57,Java Web Crawler Libraries,"
I wanted to make a Java based web crawler for an experiment. I heard that making a Web Crawler in Java was the way to go if this is your first time. However, I have two important questions.

How will my program 'visit' or 'connect' to web pages? Please give a brief explanation. (I understand the basics of the layers of abstraction from the hardware up to the software, here I am interested in the Java abstractions)
What libraries should I use? I would assume I need a library for connecting to web pages, a library for HTTP/HTTPS protocol, and a library for HTML parsing.

",https://stackoverflow.com/questions/11282503/java-web-crawler-libraries,program
58,Running UIAutomation scripts from Xcode,"
Did anyone succeed in setting up automated UIAutomation tests in Xcode?
I'm trying to set up a target in my Xcode project that should run all the UIAutomation scripts I prepared. Currently, the only Build Phase of this target is this Run Script block:
TEMPLATE=""/Applications/Xcode.app/Contents/Applications/Instruments.app/Contents/PlugIns/AutomationInstrument.bundle/Contents/Resources/Automation.tracetemplate""
MY_APP=""/Users/Me/Library/Application Support/iPhone Simulator/6.0/Applications/564ED15A-A435-422B-82C4-5AE7DBBC27DD/MyApp.app""
RESULTS=""/Users/Me/Projects/MyApp/Tests/UI/Traces/Automation.trace""
SCRIPT=""/Users/Me/Projects/MyApp/Tests/UI/SomeTest.js""
instruments -t $TEMPLATE $MY_APP -e UIASCRIPT $SCRIPT -e UIARESULTSPATH $RESULTS

When I build this target it succeeds after a few seconds, but the script didn't actually run. In the build log I get these errors:
instruments[7222:707] Failed to load Mobile Device Locator plugin
instruments[7222:707] Failed to load Simulator Local Device Locator plugin
instruments[7222:707] Automation Instrument ran into an exception while trying to run the script.  UIATargetHasGoneAWOLException
+0000 Fail: An error occurred while trying to run the script.
Instruments Trace Complete (Duration : 1.077379s; Output : /Users/Me/Projects/MyApp/Tests/UI/Traces/Automation.trace)

I am pretty sure, that my javascript and my run script are both correct, because if I run the exact same instruments command in bash it works as expected. 
Could this be a bug in Xcode?
",https://stackoverflow.com/questions/13923272/running-uiautomation-scripts-from-xcode,content
59,C# System.Windows.Automation get element text,"
I am trying to get text/labels from application controls with Automation in C#.
So far I am able to obtain AutomationElement tree of application (for example Notepad) with this function:
    private void WalkControlElements(AutomationElement rootElement, TreeNode treeNode)
    {
        AutomationElement elementNode = TreeWalker.ContentViewWalker.GetFirstChild(rootElement);;

        while (elementNode != null)
        {
            TreeNode childTreeNode = treeNode.Nodes.Add(elementNode.Current.ControlType.LocalizedControlType);

            // here I want to get text from 'elementNode'

            WalkControlElements(elementNode, childTreeNode);
            elementNode = TreeWalker.ControlViewWalker.GetNextSibling(elementNode);
        }
    }

I tried to follow this article http://msdn.microsoft.com/en-us/library/ms788751(v=vs.110).aspx but it only can get text attributes as font name, font weight and so on.
Could anybody point me to the right procedure how to get element text with Automation?
",https://stackoverflow.com/questions/23850176/c-sharp-system-windows-automation-get-element-text,content
60,Click a button element on page load,"
I'm trying to auto-click a button to initiate a function when the html page loads. I've tried document.getElementById('watchButton').click and it doesn't seem to work, the button is not clicked. Any suggestions? 
<div class=""content"">
        <div class=""action-area ch30"">
            <button class=""button dh"" id=""watchButton"">Start Geolocation Watch</button>
            <button class=""button dh"" id=""refreshButton"" >Refresh Geolocation</button>
        </div>

The javascript:
    run:function() {
    var that = this;
    document.getElementById(""watchButton"").addEventListener(""click"", function() {
        that._handleWatch.apply(that, arguments);
    }, false); 
    document.getElementById(""refreshButton"").addEventListener(""click"", function() {
        that._handleRefresh.apply(that, arguments);
    }, false);
},

Thanks!
",https://stackoverflow.com/questions/21418915/click-a-button-element-on-page-load,content
61,AutomationProperties.LiveSetting not working in WPF in .NET Framework 4.7.1,"
I have a TextBlock and I want to track that control from Screen reader and whenever a new value is set to the control in code, the screen reader should readout the new text. This is available in WPF from .NET framework 4.7.1 which is mentioned in the MSDN LINK. 
But I am always getting null for the AutomationPeer value. What am I missing in the code? Am I doing it in the right way? Please help.
XMAL 
      <Window x:Class=""WPFAccessibility.MainWindow""
                xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation""
                xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml""
                xmlns:d=""http://schemas.microsoft.com/expression/blend/2008""
                xmlns:mc=""http://schemas.openxmlformats.org/markup-compatibility/2006""
                xmlns:local=""clr-namespace:WPFAccessibility""
                mc:Ignorable=""d""
                Title=""WPFAccessibility"" Height=""450"" Width=""800"">
            <Grid>

                <TextBlock Name=""MyTextBlock"" AutomationProperties.LiveSetting=""Assertive"">My initial text</TextBlock>

                <Button Name=""Save"" Content=""Save"" HorizontalAlignment=""Left"" VerticalAlignment=""Top"" Width=""75"" Margin=""50,321,0,0"" Height=""49"" Click=""Save_Click""/>   

            </Grid>
        </Window>

Code
 private void Save_Click(object sender, RoutedEventArgs e)
        {
            // Setting the MyTextBlock text to some other value and screen 
            // reader should notify to the user
            MyTextBlock.Text = ""My changed text"";
            var peer = UIElementAutomationPeer.FromElement(MyTextBlock); 
           // I am always getting peer value null 
            peer.RaiseAutomationEvent(AutomationEvents.LiveRegionChanged);
        }

",https://stackoverflow.com/questions/52699145/automationproperties-livesetting-not-working-in-wpf-in-net-framework-4-7-1,content
64,Use AppleScript to list the names of all UI elements in a window (GUI scripting),"
One line summary - I'm looking for a way to get AppleScript itself to reveal the name it expects a specific piece of window content (UI element) to be referred to as in a ""tell"" statement.
How do I get AppleScript to list the name it wants me to use to refer to a window's contents?
for example I can say tell current application to tell its front window's list 1 to ... 
I'm trying to find out the term like ""list 1"" for all of the window's contents so I can cross-reference it with the list from Accessibility Inspector.. 
I tried this code but the first line generates an error saying ""error ""Can鈥檛 make names of 芦class ects禄 of window 1 of 芦class prcs禄 \""iTunes\"" of application \""System Events\"" into type string."" number -1700 from names of 芦class ects禄 of window 1 of 芦class prcs禄 ""iTunes"" to string""
tell application ""System Events"" to tell process ""iTunes"" to set elementNames to the names of the entire contents of its front window as string
tell application ""TextEdit""
    activate
    make new document at the front
    set the text of the front document to elementNames
    set WrapToWindow to text 2 thru -1 of (localized string ""&Wrap to Window"")
end tell

",https://stackoverflow.com/questions/42231133/use-applescript-to-list-the-names-of-all-ui-elements-in-a-window-gui-scripting,content
65,How do I use UI Automation on a WPF ItemsControl that groups items?,"
I am using Microsoft UI Automation (i.e. AutomationElement) to run automated acceptance tests against my application. This has gone well, but I've hit a situation that doesn't appear to be exposed to the automation framework.
I have an ItemsControl (although I could be using one of its derived controls, e.g. ListBox) and I am using CollectionViewSource to group items. Here is a complete window to demonstrate:
<Window x:Class=""GroupAutomation.Window1"" xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation"" xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml"" Title=""Orchestra"">
    <Window.Resources>

        <!-- Take some simple data -->
        <XmlDataProvider x:Key=""SampleData"" XPath=""Orchestra/Instrument"">
            <x:XData>
                <Orchestra xmlns="""">
                    <Instrument Name=""Flute"" Category=""Woodwind"" />
                    <Instrument Name=""Trombone"" Category=""Brass"" />
                    <Instrument Name=""French horn"" Category=""Brass"" />
                </Orchestra>
            </x:XData>
        </XmlDataProvider>

        <!-- Add grouping -->
        <CollectionViewSource Source=""{Binding Source={StaticResource SampleData}}"" x:Key=""GroupedView"">
            <CollectionViewSource.GroupDescriptions>
                <PropertyGroupDescription PropertyName=""@Category"" />
            </CollectionViewSource.GroupDescriptions>
        </CollectionViewSource>
    </Window.Resources>

    <!-- Show it in an ItemsControl -->
    <ItemsControl ItemsSource=""{Binding Source={StaticResource GroupedView}}"" HorizontalAlignment=""Left"" Margin=""4"">
        <ItemsControl.GroupStyle>
            <GroupStyle>
                <GroupStyle.HeaderTemplate>
                    <DataTemplate>
                        <TextBlock Text=""{Binding Path=Name}"" FontWeight=""Bold"" />
                    </DataTemplate>
                </GroupStyle.HeaderTemplate>
            </GroupStyle>
        </ItemsControl.GroupStyle>
        <ItemsControl.ItemTemplate>
            <DataTemplate>
                <Border Padding=""4"" Margin=""4"" Background=""#FFDEDEDE"">
                    <StackPanel>
                        <Label Content=""{Binding XPath=@Name}"" />
                        <Button Content=""Play"" />
                    </StackPanel>
                </Border>
            </DataTemplate>
        </ItemsControl.ItemTemplate>
    </ItemsControl>
</Window>

This produces a window containing the items grouped into their categories, and each item has a button that I'd like to click with UI Automation:

(source: brizzly.com) 
However, if I look in UISpy.exe (or navigate with AutomationElement) I only see the groups (even in the Raw view):

(source: brizzly.com) 
As you can see, the groups are there but they contain no items, so there is nowhere to look for the buttons. I have tried this in both WPF 3.5 SP1 and WPF 4.0 and get the same result.
Is it possible to use UI Automation on items that are grouped, and if so, how?
",https://stackoverflow.com/questions/2772071/how-do-i-use-ui-automation-on-a-wpf-itemscontrol-that-groups-items,content
66,"powershell: how to click a ""submit type"" input","
used powershell to do web ui automation.  came up an exception: invoke method failed, because [System.__ComObject] does not contain 鈥渃lick鈥?method.
can submit type input be clicked?
i used getElementsByTagName getElementsByClassName getElementsByName , does not work.
anyone can help me on this?
powershell code is below:
# open the specified web site and commit the key
$ie = new-object -com ""InternetExplorer.Application""
$ie.navigate(""http://gitlab.alibaba-inc.com/keys/new"")
$ie.visible = $true
while($ie.busy) {sleep 1}

$doc = $ie.document

# commit the button
$commit = $doc.getElementsByTagName(""commit"")

if($commit) 
{$commit.click()}

the html source is as below:
<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'>
<title>
Profile | 
GitLab
</title>
<link href=""/assets/favicon-4b751da746de7855d7eb8123072388ed.ico"" rel=""shortcut icon""    type=""image/vnd.microsoft.icon"" />
<link href=""/assets/application-a9eac7f5b0c3b922de8997ae9ad74ab0.css"" media=""screen"" rel=""stylesheet"" type=""text/css"" />
<script src=""/assets/application-61398d184a36e6ae900134f123d5d649.js"" type=""text/javascript""></script>
<meta content=""authenticity_token"" name=""csrf-param"" />
<meta content=""9SLFk6AwlsN2FoyO8xPY+M1hEbKfqlLTQ4CSDVc4efE="" name=""csrf-token"" />
<script type=""text/javascript"">
//<![CDATA[
window.gon =   {};gon.default_issues_tracker=""gitlab"";gon.api_version=""v3"";gon.api_token=""xkMg31Ssva322SDF cgxY"";gon.gravatar_url=""http://www.gravatar.com/avatar/%{hash}?s=% {size}&d=mm"";gon.relative_url_root="""";
//]]>
</script>

</head>

<body class='ui_basic profile' data-page='keys:new'>
<header class='navbar navbar-static-top navbar-gitlab'>
<div class='navbar-inner'>
<div class='container'>
<div class='app_logo'>
<span class='separator'></span>
<a href=""/"" class=""home has_bottom_tooltip"" title=""Dashboard""><h1>GITLAB</h1>
</a><span class='separator'></span>
</div>
<h1 class='project_name'>Profile</h1>
<ul class='nav'>
<li>
<a>
<div class='hide turbolink-spinner'>
<i class='icon-refresh icon-spin'></i>
Loading...
</div>
</a>
</li>
<li>
<div class='search'>
<form accept-charset=""UTF-8"" action=""/search"" class=""navbar-form pull-left""  method=""get""><div style=""margin:0;padding:0;display:inline""><input name=""utf8""  type=""hidden"" value=""&#x2713;"" /></div>
<input class=""search-input"" id=""search"" name=""search"" placeholder=""Search"" type=""text""   />
<input id=""group_id"" name=""group_id"" type=""hidden"" />
<input id=""repository_ref"" name=""repository_ref"" type=""hidden"" />

<div class='search-autocomplete-json hide' data-autocomplete-opts='[{""label"":""project:  kelude2"",""url"":""/kelude2""},{""label"":""My Profile"",""url"":""/profile""},{""label"":""My SSH  Keys"",""url"":""/keys""},{""label"":""My Dashboard"",""url"":""/""},{""label"":""Admin  Section"",""url"":""/admin""},{""label"":""help: API Help"",""url"":""/help/api""},{""label"":""help:  Markdown Help"",""url"":""/help/markdown""},{""label"":""help: Permissions  Help"",""url"":""/help/permissions""},{""label"":""help: Public Access  Help"",""url"":""/help/public_access""},{""label"":""help: Rake Tasks  Help"",""url"":""/help/raketasks""},{""label"":""help: SSH Keys Help"",""url"":""/help/ssh""}, {""label"":""help: System Hooks Help"",""url"":""/help/system_hooks""},{""label"":""help: Web Hooks  Help"",""url"":""/help/web_hooks""},{""label"":""help: Workflow Help"",""url"":""/help/workflow""}]'>   </div>
</form>

</div>

</li>
<li>
<a href=""/public"" class=""has_bottom_tooltip"" data-original-title=""Public area""    title=""Public area""><i class='icon-globe'></i>
</a></li>
<li>
<a href=""/s/heyun"" class=""has_bottom_tooltip"" data-original-title=""Public area""    title=""My snippets""><i class='icon-paste'></i>
</a></li>
<li>
<a href=""/projects/new"" class=""has_bottom_tooltip"" data-original-title=""New project""    title=""Create New Project""><i class='icon-plus'></i>
</a></li>
<li>
<a href=""/profile"" class=""has_bottom_tooltip"" data-original-title=""Your profile""    title=""My Profile""><i class='icon-user'></i>
</a></li>
<li>
<a href=""/users/sign_out"" class=""has_bottom_tooltip"" data-method=""delete"" data-original-title=""Logout"" rel=""nofollow"" title=""Logout""><i class='icon-signout'></i>  
</a></li>
<li>
<a href=""/u/heyun"" class=""profile-pic""><img alt=""F3ea5164088694b48e4980e52d831927? s=26&amp;d=mm"" src=""http://www.gravatar.com/avatar/f3ea5164088694b48e4980e52d831927? s=26&amp;d=mm"" />
</a></li>
</ul>
</div>
</div>
</header>

<div class='flash-container'>
</div>

<nav class='main-nav'>
<div class='container'><ul>
<li class=""home""><a href=""/profile"" title=""Profile""><i class='icon-home'></i>
</a></li><li class=""""><a href=""/profile/account"">Account</a>
</li><li class=""""><a href=""/profile/notifications"">Notifications</a>
</li><li class=""active""><a href=""/keys"">SSH Keys
<span class='count'>1</span>
</a></li><li class=""""><a href=""/profile/design"">Design</a>
</li><li class=""""><a href=""/profile/history"">History</a>
</li></ul>
</div>
</nav>
<div class='container'>
<div class='content'><h3 class='page_title'>Add an SSH Key</h3>
<hr>
<div>
<form accept-charset=""UTF-8"" action=""/keys"" class=""new_key"" id=""new_key"" method=""post"">   <div style=""margin:0;padding:0;display:inline""><input name=""utf8"" type=""hidden""   value=""&#x2713;"" /><input name=""authenticity_token"" type=""hidden""   value=""9SLFk6AwlsN2FoyO8xPY+M1hEbKfqlLTQ4CSDVc4efE="" /></div><div class='clearfix'>
<label for=""key_title"">Title</label>
<div class='input'><input id=""key_title"" name=""key[title]"" size=""30"" type=""text"" />   </div>
</div>
<div class='clearfix'>
<label for=""key_key"">Key</label>
<div class='input'>
<textarea class=""xxlarge thin_area"" cols=""40"" id=""key_key"" name=""key[key]"" rows=""20"">
</textarea>
<p class='hint'>
Paste your public key here. Read more about how generate it
<a href=""/help/ssh"">here</a>
</p>
</div>
</div>
<div class='actions'>
<input class=""btn btn-save"" name=""commit"" type=""submit"" value=""Save"" />
<a href=""/keys"" class=""btn btn-cancel"">Cancel</a>
</div>
</form>

</div>

<script>
  $('#key_key').on('keyup', function(){
    var title = $('#key_title'),
        val      = $('#key_key').val(),
        key_mail = val.match(/([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+|\.[a-zA-Z0-9._-]+)/gi);

    if( key_mail && key_mail.length > 0 && title.val() == '' ){
      $('#key_title').val( key_mail );
    }
  });
</script>
</div>
</div>
</body>
</html>

",https://stackoverflow.com/questions/17721295/powershell-how-to-click-a-submit-type-input,content
67,Powershell commands to find and close Windows File Explorer dialogs,"
All windows file explorer windows show up in the Task manager in type of sub-heading all under explorer.exe, however, I would like to close individual file explorer windows by the name on the content of the window.  I am trying to close windows that show connectivity errors after connecting to a VPN such as the one below:

When I use the following command I get info about it but PS gives no indication as to the name of the window (e.g. ""Restoring Network Connections"").  How do I get and use Window info?

Get-Process -name explorer

I also tried the following command, however, it only shows 1 of the window objects when I have over 10 open that match the criteria ""*HFS*""

(New-Object -ComObject Shell.Application).Windows() | Where-Object{$_.LocationName -like ""*HFS*"" }


",https://stackoverflow.com/questions/73534710/powershell-commands-to-find-and-close-windows-file-explorer-dialogs,content
68,Why doesn't UI Automation condition find element by UIA_IsScrollPatternAvailablePropertyId?,"
I wanted to find the element within a main window handle that allows scrolling.  So instead of finding scrollbars and then the owner of the scrollbars I wanted to just return the items that allow scrolling via a ScrollPattern so I setup the condition on that but nothing is found. if I search for scrollbar owner window then get the ScrollPattern it works.  Why can't I just find the elements that have a scroll pattern available?
Here's the common code:
BOOL CUIAutomateScroller::FindWindow(HWND hwnd, IUIAutomationElement **windowelement)
{
  BOOL result=FALSE;
  // make sure init completed
  if (m_pClientUIA) {
    // get window element
    HRESULT hr=m_pClientUIA->ElementFromHandle(hwnd, windowelement);
    // check result
    result=SUCCEEDED(hr);
    // output debug info
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""ElementFromHandle error: %d\n""), hr);
    }
    else {
      _ASSERT(*windowelement!=NULL);
    }
  }
  return result;
}

BOOL CUIAutomateScroller::FindContainerWindowElement(const long controltype, IUIAutomationElement **pelement)
{
  // Create search condition
  VARIANT varprop;
  varprop.vt=VT_I4;
  varprop.uintVal=controltype;

  CComPtr<IUIAutomationCondition> pcondition;
  HRESULT hr=m_pClientUIA->CreatePropertyCondition(UIA_ControlTypePropertyId, varprop, &pcondition);
  if (FAILED(hr)) {
    CDebugPrint::DebugPrint(_T(""CreatePropertyCondition error: %d\n""), hr);
    return NULL;
  }

  // find the control based on condition
  CComPtr<IUIAutomationElementArray> pcontrolelementarr;
  hr=m_pWindowElement->FindAll(TreeScope_Subtree, pcondition, &pcontrolelementarr);
  if (FAILED(hr)) {
    CDebugPrint::DebugPrint(_T(""CreatePropertyCondition error: %d\n""), hr);
    return NULL;
  }

  // get number of controls found
  int numfound;
  pcontrolelementarr->get_Length(&numfound);
  CDebugPrint::DebugPrint(_T(""Controls Found: %d\n""), numfound);

  // process controls found, but really we exit earily if container window found
  for (int i=0; i < numfound; i++) {
    // get individual control element
    CComPtr<IUIAutomationElement> pcontrolelement;
    hr=pcontrolelementarr->GetElement(i, &pcontrolelement);
    if (FAILED(hr)) {
      // skip element unable to be retreived
      CDebugPrint::DebugPrint(_T(""GetElement error: %d\n""), hr);
      continue;
    }

    // output debug information
    CComBSTR name;
    hr=pcontrolelement->get_CurrentName(&name);
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""GetCurrentName error: %d\n""), hr);
    }
    CDebugPrint::DebugPrint(_T(""Control Name: %s\n""), name);
    name.Empty();

    hr=pcontrolelement->get_CurrentClassName(&name);
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""GetCurrentClass error: %d\n""), hr);
    }
    CDebugPrint::DebugPrint(_T(""Class Name: %s\n""), name);
    name.Empty();

    CComPtr<IUIAutomationTreeWalker> pcontentwalker=NULL;
    hr=m_pClientUIA->get_ContentViewWalker(&pcontentwalker);
    if (pcontentwalker == NULL) {
      return NULL;
    }

    // Get ancestor element nearest to the scrollbar UI Automation element in the tree view
    hr=pcontentwalker->NormalizeElement(pcontrolelement, pelement);
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""NormalizeElement error: %d\n""), hr);
      return NULL;
    }

    // output debug information
    hr=(*pelement)->get_CurrentName(&name);
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""get_CurrentName error: %d\n""), hr);
    }
    CDebugPrint::DebugPrint(_T(""Ancestor Name: %s\n""), name);
    name.Empty();

    return TRUE;
  }

  return FALSE;
}

This does NOT work (It doesn't find anything):
  // get main window
  if (FindWindow(hwnd, &m_pWindowElement)) {
    HRESULT hr;
    VARIANT varprop;
      
    // create condition for elements that have UIA_IsScrollPatternAvailablePropertyId available
    CComPtr<IUIAutomationCondition> pscrollpatterncondition;
    varprop.vt=VT_BOOL;
    varprop.boolVal=TRUE;
    hr=m_pClientUIA->CreatePropertyCondition(UIA_IsScrollPatternAvailablePropertyId, varprop, &pscrollpatterncondition);
    // check result
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""CreatePropertyCondition for ScrollPattern Error: %d\n""), hr);
    }
    else {
      // find the matching element
      CComPtr<IUIAutomationElementArray> pscrollpatternarr;
      hr=m_pWindowElement->FindAll(TreeScope_Subtree, pscrollpatterncondition, &pscrollpatternarr);
      // check result (normal is success with empty array if not found)
      if (FAILED(hr)) {
        CDebugPrint::DebugPrint(_T(""FindAll Error: %d\n""), hr);
      }
      else {
        // get number of elements in array
        int numfound=0;
        pscrollpatternarr->get_Length(&numfound);
        // make sure we only get one scrollable area - in the future we could figure out the rect
        // **numfound is 0**

This DOES work:
  // get main window
  if (FindWindow(hwnd, &m_pWindowElement)) {
    // get scrollable window element based on scrollbar
    if (FindContainerWindowElement(UIA_ScrollBarControlTypeId, &m_pScrollableElement)) {
      HRESULT hr;
      // get the scroll pattern
      hr=m_pScrollableElement->GetCurrentPattern(UIA_ScrollPatternId, (IUnknown**) &m_pScrollPattern);
      if (FAILED(hr)) {
        CDebugPrint::DebugPrint(_T(""GetCurrentPattern for Scroll Pattern Error %d:\n""), hr);
      }
      else if (m_pScrollPattern!=NULL) {
        // **we're good!!**

",https://stackoverflow.com/questions/63063225/why-doesnt-ui-automation-condition-find-element-by-uia-isscrollpatternavailable,content
69,How to select elements within an iframe element in Puppeteer,"
Since ESPN does not provide an API, I am trying to use Puppeteer to scrape data about my fantasy football league. However, I am having a hard time trying to login using puppeteer due to the login form being nested with an iframe element.
I have gone to http://www.espn.com/login and selected the iframe. I can't seem to select any of the elements within the iframe except for the main section by doing
    frame.$('.main')

This is the code that seems to get the iframe with the login form.
    const browser = await puppeteer.launch({headless:false});
    const page = await browser.newPage();

    await page.goto('http://www.espn.com/login')
    await page.waitForSelector(""iframe"");

    const elementHandle = await page.$('div#disneyid-wrapper iframe');
    const frame = await elementHandle.contentFrame();
    await browser.close()

I want to be able to access the username field, password field, and the login button within the iframe element. Whenever I try to access these fields, I get a return of null.
",https://stackoverflow.com/questions/56420047/how-to-select-elements-within-an-iframe-element-in-puppeteer,content
70,"Using Playwright for Python, how do I select (or find) an element?","
I'm trying to learn the Python version of Playwright. See here
I would like to learn how to locate an element, so that I can do
things with it. Like printing the inner HTML, clicking on it and such.
The example below loads a page and prints the HTML
from playwright import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(headless=False)
    page = browser.newPage()
    page.goto('http://whatsmyuseragent.org/')
    print(page.innerHTML(""*""))
    browser.close()

This page contains an element
<div class=""user-agent"">
    <p class=""intro-text"">Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4238.0 Safari/537.36</p>
</div>

Using Selenium, I could locate the element and print it's content like this
elem = driver.find_element_by_class_name(""user-agent"")
print(elem)
print(elem.get_attribute(""innerHTML""))

How can I do the same in Playwright?
#UPDATE# - Note if you want to run this in 2021+ that current versions of playwright have changed the syntax from CamelCase to snake_case.
",https://stackoverflow.com/questions/64303326/using-playwright-for-python-how-do-i-select-or-find-an-element,content
72,Using internetexplorer object what is the correct way to wait for an ajax response?,"
I tried to upload a file to a sharepoint library, my code fails to properly detect  if ie is still waiting for an ajax response or not. What is the proper way to do this ?
[void] [System.Reflection.Assembly]::LoadWithPartialName(""'Microsoft.VisualBasic"")
[void] [System.Reflection.Assembly]::LoadWithPartialName(""'System.Windows.Forms"")

function wait4IE($ie=$global:ie){
    while ($ie.busy -or $ie.readystate -lt 4){start-sleep -milliseconds 200}
}

$global:ie=new-object -com ""internetexplorer.application""
$ie.visible=$true
[Microsoft.VisualBasic.Interaction]::AppActivate(""internet explorer"")

# open EDM
$ie.navigate(""https://xxx.sharepoint.com/sites/site1/Forms/AllItems.aspx"")
wait4IE

# click on  the button to display the form
$ie.Document.getElementById(""QCB1_Button2"").click()

wait4IE

the rest of the code is executed, but the uploading form is not shown yet.
How to wait for the display of the form  ?
I also tried this (should wait untill a button of the upload form is not find), but it never ends ...
while( $ie.document.getElementById(""ctl00_PlaceHolderMain_UploadDocumentSection_ctl05_InputFile"") -eq $null){
        echo ""waiting ...""
        wait4IE
}


Update :
I think I've found the problem : the form is open in an iframe :
<iframe id=""DlgFrame0be35d71-22cb-47bd-bbf0-44c97db61fd6"" class=""ms-dlgFrame"" src=""https://.../Upload.aspx?List={45085FA0-3AE3-4410-88AD-3E80A218FC0C}&amp;RootFolder=&amp;IsDlg=1"" frameborder=""0"" style=""width: 592px; height: 335px;""></iframe>

But now, How to get the good frame number ?
PS>($ie.Document.frames.Item(4).document.body.getElementsbytagname(""input"") |?{$_.type -eq 'file'}).id
ctl00_PlaceHolderMain_UploadDocumentSection_ctl05_InputFile

moreover it seems i can access the frame content with getElementsByTagName, but not with getElementById ....?I still don't understand why .:
PS>$ie.Document.frames.Item(4).document.body.getElementById('ctl00_PlaceHolderMain_UploadDocumentSection_ctl05_InputFile
    ')
    脡chec lors de l'appel de la m茅thode, car [System.__ComObject] ne contient pas de m茅thode nomm茅e 芦聽getElementById聽禄.
    Au caract猫re Ligne:1 : 1
    + $ie.Document.frames.Item(4).document.body.getElementById('ctl00_PlaceHolderMain_ ...
    + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        + CategoryInfo          : InvalidOperation : (getElementById:String) [], RuntimeException
        + FullyQualifiedErrorId : MethodNotFound

",https://stackoverflow.com/questions/29209227/using-internetexplorer-object-what-is-the-correct-way-to-wait-for-an-ajax-respon,content
73,How to use installed version of chrome in Playwright?,"
I want to use chrome instead of chromium. I can achieve the same in puppeteer by providing executable path. In playwright it doesn't work as browser type argument supports only 'chromium, webkit, firefox'


const { chromium } = require('playwright');
(async () => {
    const browser = await chromium.launch({
        headless: false,
        executablePath: '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome',
    });
    const context = await browser.newContext();
    const page = await context.newPage();
    await page.goto('http://whatsmyuseragent.org/');
    await page.screenshot({ path: `example-${browserType}.png` });
})();


",https://stackoverflow.com/questions/62281859/how-to-use-installed-version-of-chrome-in-playwright,content
74,Puppeteer Get all data attribute values,"
My html doc is
<div class=""inner-column"">
 <div data-thing=""abc1""></div>
 <div data-thing=""abc2""></div>
 <div data-thing=""abc3""></div>
</div>

How can I get all ""data-thing"" value (eg. [""abc1"", ""abc2"", ""abc3""]) inside div with class .inner-column?
const puppeteer = require('puppeteer');
const fs = require('fs');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  page.setViewport({width: 1440, height: 1200})
  await page.goto('https://www.example.com')

  const data = await page.content();

  await browser.close();
})();

",https://stackoverflow.com/questions/55797082/puppeteer-get-all-data-attribute-values,content
75,Selenium Chrome WebDriver how to scroll horizontally,"
Chrome web driver has a limitation that only loads webpage content that is in view. I have a website that has long horizontal table. I have Xpath that extracts column headers of a table that stretches more than the screen width. In chrome dev tool console if I run the xpath $x(myxpathgoeshere) I get all the headers including the ones that are not in view(the one that makes you scroll to see all). So I know my xpath is correct. But in code, when I access it by using selenium webdriver it only gives header names that are in current view. I came across various posts on chrome webdriver google group page, users mentioning this limitation and answer to it was to not fix it. So anyways, now I am trying to make it work using javascript to scroll horizontally and then do the findelement by xpath again to see if the elements to the right are loaded. But for some strange reason I cannot seem to get the scrolling horizontally to work. I am using C# Javascript executor.
IJavaScriptExecutor js = (IJavaScriptExecutor) Driver;
js.ExecuteScript(""scrollTo(3000,0);""); // whatever X value I use chrome is not scrolling to the right. 

I have also tried scrollX and no luck. Is there something wrong with my code?
Edited: forgot that I was using X for horizontal not Y
",https://stackoverflow.com/questions/26104952/selenium-chrome-webdriver-how-to-scroll-horizontally,content
76,How to get a specific frame in a web page and retrieve its content,"
I wanted to access the translation results of the following url 

http://translate.google.com/translate?hl=en&sl=en&tl=ar&u=http%3A%2F%2Fwww.saltycrane.com%2Fblog%2F2008%2F10%2Fhow-escape-percent-encode-url-python%2F

the translation is displayed in the bottom  content frame out of the two frames. I am interested in retrieving only the bottom content frame to get the translations 
selenium for python allows us to fetch page contents via web automation:
browser.get('http://translate.google.com/#en/ar/'+hurl)

The required frame is an iframe :
<div id=""contentframe"" style=""top:160px""><iframe   src=""/translate_p?hl=en&am... name=c frameborder=""0"" style=""height:100%;width:100%;position:absolute;top:0px;bottom:0px;""></div></iframe>

but how to get the bottom content frame element to retrieve the translations using web automation?
Came to know that PyQuery also allows us to browse the contents using the JQuery formalism
Update:
An answer mentioned that Selenium provides a method where you can do that.
frame = browser.find_element_by_tag_name('iframe')
browser.switch_to_frame(frame)
# get page source
browser.page_source

but it does not work in the above example. It returns an empty page .
",https://stackoverflow.com/questions/15785920/how-to-get-a-specific-frame-in-a-web-page-and-retrieve-its-content,content
77,How to use Python requests to fake a browser visit a.k.a and generate User Agent?,"
I want to get the content from this website.
If I use a browser like Firefox or Chrome I could get the real website page I want, but if I use the Python requests package (or wget command) to get it, it returns a totally different HTML page.
I thought the developer of the website had made some blocks for this.
Question
How do I fake a browser visit by using python requests or command wget?
",https://stackoverflow.com/questions/27652543/how-to-use-python-requests-to-fake-a-browser-visit-a-k-a-and-generate-user-agent,content
78,Is it ok to scrape data from Google results? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 5 years ago.







                        Improve this question
                    



I'd like to fetch results from Google using curl to detect potential duplicate content.
Is there a high risk of being banned by Google?
",https://stackoverflow.com/questions/22657548/is-it-ok-to-scrape-data-from-google-results,content
80,Web scraping with Python [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I'd like to grab daily sunrise/sunset times from a web site. Is it possible to scrape web content with Python? what are the modules used? Is there any tutorial available?
",https://stackoverflow.com/questions/2081586/web-scraping-with-python,content
81,Headless Browser and scraping - solutions [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



I'm trying to put list of possible solutions for browser automatic tests suits and headless browser platforms capable of scraping.

BROWSER TESTING / SCRAPING:

Selenium - polyglot flagship in browser automation, bindings for Python, Ruby,  JavaScript, C#, Haskell and more, IDE for Firefox (as an extension) for faster test deployment. Can act as a Server and has tons of features.

JAVASCRIPT

PhantomJS - JavaScript, headless testing with screen capture and automation, uses Webkit. As of version 1.8 Selenium's WebDriver API is implemented, so you can use any WebDriver binding and tests will be compatible with Selenium
SlimerJS - similar to PhantomJS, uses Gecko (Firefox) instead of WebKit
CasperJS - JavaScript, build on both PhantomJS and SlimerJS, has extra features
Ghost Driver - JavaScript implementation of the WebDriver Wire Protocol for PhantomJS.
new PhantomCSS - CSS regression testing. A CasperJS module for automating visual regression testing with PhantomJS and Resemble.js.
new WebdriverCSS - plugin for Webdriver.io for automating visual regression testing
new PhantomFlow - Describe and visualize user flows through tests. An experimental approach to Web user interface testing.
new trifleJS - ports the PhantomJS API to use the Internet Explorer engine.
new CasperJS IDE (commercial)

NODE.JS

Node-phantom - bridges the gap between PhantomJS and node.js
WebDriverJs - Selenium WebDriver bindings for node.js by Selenium Team
WD.js - node module for WebDriver/Selenium 2
yiewd - WD.js wrapper using latest Harmony generators! Get rid of the callback pyramid with yield
ZombieJs - Insanely fast, headless full-stack testing using node.js
NightwatchJs - Node JS based testing solution using Selenium Webdriver
Chimera - Chimera: can do everything what phantomJS does, but in a full JS environment
Dalek.js - Automated cross browser testing with JavaScript through Selenium Webdriver
Webdriver.io - better implementation of WebDriver bindings with predefined 50+ actions
Nightmare - Electron bridge with a high-level API.
jsdom - Tailored towards web scraping. A very lightweight DOM implemented in Node.js, it supports pages with javascript.
new Puppeteer - Node library which provides a high-level API to control Chrome or Chromium. Puppeteer runs headless by default.

WEB SCRAPING / MINING

Scrapy - Python, mainly a scraper/miner - fast, well documented and, can be linked with Django Dynamic Scraper for nice mining deployments, or Scrapy Cloud for PaaS (server-less) deployment, works in terminal or an server stand-alone proces, can be used with Celery, built on top of Twisted
Snailer - node.js module, untested yet.
Node-Crawler - node.js module, untested yet.

ONLINE TOOLS

new Web Scraping Language - Simple syntax to crawl the web

new Online HTTP client - Dedicated SO answer

dead CasperBox - Run CasperJS scripts online


Android TOOLS for Automation

new Mechanica Browser App


RELATED LINKS & RESOURCES

Comparsion of Webscraping software
new Resemble.js : Image analysis and comparison

Questions:

Any pure Node.js solution or Nodejs to PhanthomJS/CasperJS module that actually works and is documented?

Answer: Chimera seems to go in that direction, checkout Chimera

Other solutions capable of easier JavaScript injection than Selenium?

Do you know any pure ruby solutions?


Answer: Checkout the list created by rjk with ruby based solutions

Do you know any related tech or solution?

Feel free to edit this question and add content as you wish! Thank you for your contributions!
",https://stackoverflow.com/questions/18539491/headless-browser-and-scraping-solutions,content
82,How to scrape a website which requires login using python and beautifulsoup?,"
If I want to scrape a website that requires login with password first, how can I start scraping it with python using beautifulsoup4 library? Below is what I do for websites that do not require login. 
from bs4 import BeautifulSoup    
import urllib2 
url = urllib2.urlopen(""http://www.python.org"")    
content = url.read()    
soup = BeautifulSoup(content)

How should the code be changed to accommodate login? Assume that the website I want to scrape is a forum that requires login. An example is http://forum.arduino.cc/index.php
",https://stackoverflow.com/questions/23102833/how-to-scrape-a-website-which-requires-login-using-python-and-beautifulsoup,content
83,Can bs4 get the dynamic content of a webpage if requests can't?,"
So I've tried Selenium previously and now wanted to test out bs4. I tried running the following code but recieved None as an output.
res_pewdiepie = requests.get(
    'https://www.youtube.com/user/PewDiePie')
soup = bs4.BeautifulSoup(res_pewdiepie.content, ""lxml"")
subs = soup.find(id=""sub-count"")
print(subs)

After researching for a while, I found out that requests doesn't load dynamic content like the subcount on YouTube or Socialblade.  Is there a way to get this information with bs4 or if do I have to switch back to something like Selenium?
Thanks in advance!
",https://stackoverflow.com/questions/65265321/can-bs4-get-the-dynamic-content-of-a-webpage-if-requests-cant,content
84,Scraping Google Finance (BeautifulSoup),"
I'm trying to scrape Google Finance, and get the ""Related Stocks"" table, which has id ""cc-table"" and class ""gf-table"" based on the webpage inspector in Chrome. (Sample Link: https://www.google.com/finance?q=tsla)
But when I run .find(""table"") or .findAll(""table""), this table does not come up. I can find JSON-looking objects with the table's contents in the HTML content in Python, but do not know how to get it. Any ideas?
",https://stackoverflow.com/questions/45259232/scraping-google-finance-beautifulsoup,content
85,How to run Puppeteer code in any web browser?,"
I'm trying to do some web scraping with Puppeteer and I need to retrieve the value into a Website I'm building.
I have tried to load the Puppeteer file in the html file as if it was a JavaScript file but I keep getting an error. However, if I run it in a cmd window it works well.

Scraper.js:

getPrice();
function getPrice() {
    const puppeteer = require('puppeteer');
    void (async () => {
        try {
            const browser = await puppeteer.launch()
            const page = await browser.newPage()              
            await page.goto('http://example.com') 
            await page.setViewport({ width: 1920, height: 938 })        
            await page.waitForSelector('.m-hotel-info > .l-container > .l-header-section > .l-m-col-2 > .m-button')
            await page.click('.m-hotel-info > .l-container > .l-header-section > .l-m-col-2 > .m-button')
            await page.waitForSelector('.modal-content')
            await page.click('.tile-hsearch-hws > .m-search-tabs > #edit-search-panel > .l-em-reset > .m-field-wrap > .l-xs-col-4 > .analytics-click')
            await page.waitForNavigation();
            await page.waitForSelector('.tile-search-filter > .l-display-none')
            const innerText = await page.evaluate(() => document.querySelector('.tile-search-filter > .l-display-none').innerText);
            console.log(innerText)
        } catch (error) {
            console.log(error)
        }

    })()
}


index.html:

<html>
  <head></head>
  <body>
    <script src=""../js/scraper.js"" type=""text/javascript""></script>
  </body>
</html>

The expected result should be this one in the console of Chrome:

But I'm getting this error instead:


What am I doing wrong?
",https://stackoverflow.com/questions/54647694/how-to-run-puppeteer-code-in-any-web-browser,content
86,Accessing object in iframe using VBA,"
To the point:
I have successfully used VBA to do the following:

Login to a website using getElementsByName
Select parameters for the report that will be generated (using getelementsby...)
generating the report after selecting parameters which renders the resulting dataset into an iframe on the same page

Important to note - The website is client-side
The above was the simple part, the difficult part is as below:

clicking on a gif image within the iframe that exports the dataset to a csv

I have tried the following:
Dim idoc As HTMLDocument
Dim iframe As HTMLFrameElement
Dim iframe2 As HTMLDocument

Set idoc = objIE.document
Set iframe = idoc.all(""iframename"")
Set iframe2 = iframe.contentDocument

    Do Until InStr(1, objIE.document.all(""iframename"").contentDocument.innerHTML, ""img.gif"", vbTextCompare) = 0
        DoEvents
    Loop

To give some context to the logic above -

I accessed the main frame
i accessed the iframe by its name element
i accessed the content within the iframe
I attempted to find the gif image that needs to be clicked to export to csv

It is at this line that it trips up saying ""Object doesn't support this property or method""
Also tried accessing the iframe gif by the a element and href attribute but this totally failed. I also tried grabbing the image from its source URL but all this does it take me to the page the image is from.
note: the iframe does not have an ID and strangely the gif image does not have an ""onclick"" element/event

Final consideration - attempted scraping the iframe using R

accessing the HTML node of the iframe was simple, however trying to access the attributes of the iframe and subsequently the nodes of the table proved unsuccessful. All it returned was ""Character(0)""
library(rvest)
library(magrittr)

Blah <-read_html(""web address redacted"") %>%
  html_nodes(""#iframe"")%>%
  html_nodes(""#img"")%>%
  html_attr(""#src"")%>%
  #read_html()%>%
  head()
Blah

As soon as a i include read_html the following error returns on the script:
Error in if (grepl(""<|>"", x)) { : argument is of length zero
I suspect this is referring to the Character(0) 
Appreciate any guidance here!
Many Thanks,

HTML

<div align=""center""> 
    <table id=""table1"" style=""border-collapse: collapse"" width=""700"" cellspacing=""0"" cellpadding=""0"" border=""0""> 
        <tbody>
            <tr>
                <td colspan=""6""> &nbsp;</td>
            </tr> 
            <tr> 
                <td colspan=""6""> 
                    <a href=""href redacted"">
                        <img src=""img.gif"" width=""38"" height=""38"" border=""0"" align=""right"">
                    </a>
                    <strong>x - </strong>
                </td>
            </tr> 
        </tbody>
    </table>
</div>

",https://stackoverflow.com/questions/44902558/accessing-object-in-iframe-using-vba,content
87,Scrape web page contents,"
I am developing a project, for which I want to scrape the contents of a website in the background and get some limited content from that scraped website. For example, in my page I have ""userid"" and ""password"" fields, by using those I will access my mail and scrape my inbox contents and display it in my page.
I done the above by using javascript alone. But when I click the sign in button the URL of my page (http://localhost/web/Login.html) is changed to the URL (http://mail.in.com/mails/inbox.php?nomail=....) which I am scraped. But I scrap the details without changing my url.
",https://stackoverflow.com/questions/584826/scrape-web-page-contents,content
89,Wait page to load before getting data with requests.get in python 3,"
I have a page that i need to get the source to use with BS4, but the middle of the page takes 1 second(maybe less) to load the content, and requests.get catches the source of the page before the section loads, how can I wait a second before getting the data?
r = requests.get(URL + self.search, headers=USER_AGENT, timeout=5 )
    soup = BeautifulSoup(r.content, 'html.parser')
    a = soup.find_all('section', 'wrapper')

The page
<section class=""wrapper"" id=""resultado_busca"">

",https://stackoverflow.com/questions/45448994/wait-page-to-load-before-getting-data-with-requests-get-in-python-3,content
90,Reading dynamically generated web pages using python,"
I am trying to scrape a web site using python and beautiful soup. I encountered that in some sites, the image links although seen on the browser is cannot be seen in the source code. However on using Chrome Inspect or Fiddler, we can see the the corresponding codes. 
What I see in the source code is:
<div id=""cntnt""></div>

But on Chrome Inspect, I can see a whole bunch of HTML\CSS code generated within this div class. Is there a way to load the generated content also within python? I am using the regular urllib in python and I am able to get the source but without the generated part.
I am not a web developer hence I am not able to express the behaviour in better terms. Please feel free to clarify if my question seems vague !
",https://stackoverflow.com/questions/13960567/reading-dynamically-generated-web-pages-using-python,content
91,Python selenium multiprocessing,"
I've written a script in python in combination with selenium to scrape the links of different posts from its landing page and finally get the title of each post by tracking the url leading to its inner page. Although the content I parsed here are static ones, I used selenium to see how it works in multiprocessing. 
However, my intention is to do the scraping using multiprocessing. So far I know that selenium doesn't support multiprocessing but it seems I was wrong.
My question: how can I reduce the execution time using selenium when it is made to run using multiprocessing?
This is my try (it's a working one):
import requests
from urllib.parse import urljoin
from multiprocessing.pool import ThreadPool
from bs4 import BeautifulSoup
from selenium import webdriver

def get_links(link):
  res = requests.get(link)
  soup = BeautifulSoup(res.text,""lxml"")
  titles = [urljoin(url,items.get(""href"")) for items in soup.select("".summary .question-hyperlink"")]
  return titles

def get_title(url):
  chromeOptions = webdriver.ChromeOptions()
  chromeOptions.add_argument(""--headless"")
  driver = webdriver.Chrome(chrome_options=chromeOptions)
  driver.get(url)
  sauce = BeautifulSoup(driver.page_source,""lxml"")
  item = sauce.select_one(""h1 a"").text
  print(item)

if __name__ == '__main__':
  url = ""https://stackoverflow.com/questions/tagged/web-scraping""
  ThreadPool(5).map(get_title,get_links(url))

",https://stackoverflow.com/questions/53475578/python-selenium-multiprocessing,content
92,Scraping ajax pages using python,"
I've already seen this question about scraping ajax, but python isn't mentioned there. I considered using scrapy, i believe they have some docs on that subject, but as you can see the website is down. So i don't know what to do. I want to do the following:
I only have one url, example.com you go from page to page by clicking submit, the url doesn't change since they're using ajax to display the content. I want to scrape the content of each page, how to do it? 
Lets say that i want to scrape only the numbers, is there anything other than scrapy that would do it? If not, would you give me a snippet on how to do it, just because their website is down so i can't reach the docs.
",https://stackoverflow.com/questions/16390257/scraping-ajax-pages-using-python,content
93,How to scroll down with Phantomjs to load dynamic content,"
I am trying to scrape links from a page that generates content dynamically as the user scroll down to the bottom (infinite scrolling). I have tried doing different things with Phantomjs but not able to gather links beyond first page. Let say the element at the bottom which loads content has class .has-more-items. It is available until final content is loaded while scrolling and then becomes unavailable in DOM (display:none). Here are the things I have tried-

Setting viewportSize to a large height right after var page = require('webpage').create();


page.viewportSize = {             width: 1600,            height: 10000,
          };


Using page.scrollPosition = { top: 10000, left: 0 } inside page.open but have no effect like-


page.open('http://example.com/?q=houston', function(status) {
   if (status == ""success"") {
      page.scrollPosition = { top: 10000, left: 0 };  
   }
});



Also tried putting it inside page.evaluate function but that gives 


Reference error: Can't find variable page


Tried using jQuery and JS code inside page.evaluate and page.open but to no avail-


$(""html, body"").animate({ scrollTop: $(document).height() }, 10,
  function() {
          //console.log('check for execution');
      });

as it is and also inside document.ready. Similarly for JS code-
window.scrollBy(0,10000)

as it is and also inside window.onload
I am really struck on it for 2 days now and not able to find a way. Any help or hint would be appreciated.
Update
I have found a helpful piece of code at https://groups.google.com/forum/?fromgroups=#!topic/phantomjs/8LrWRW8ZrA0
var hitRockBottom = false; while (!hitRockBottom) {
    // Scroll the page (not sure if this is the best way to do so...)
    page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };

    // Check if we've hit the bottom
    hitRockBottom = page.evaluate(function() {
        return document.querySelector("".has-more-items"") === null;
    }); }

Where .has-more-items is the element class I want to access which is available at the bottom of the page initially and as we scroll down, it moves further down until all data is loaded and then becomes unavailable.
However, when I tested it is clear that it is running into infinite loops without scrolling down (I render pictures to check). I have tried to replace page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 }; with codes from below as well (one at a time)
window.document.body.scrollTop = '1000';
location.href = "".has-more-items"";
page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };
document.location.href="".has-more-items"";

But nothing seems to work.
",https://stackoverflow.com/questions/16561582/how-to-scroll-down-with-phantomjs-to-load-dynamic-content,content
94,Android Web Scraping with a Headless Browser [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I have spent a day on researching a library that can be used to accomplish the following:

Retrieve the full contents of a webpage like in the background without rendering result to a view.
The lib should support pages that fires off ajax requests to load some additional result data after the initial HTML has loaded for example.
From the resulting html I need to grab elements in xpath or css selector form.
In future I also possibly need to navigate to a next page (fire off events, submitting buttons/links etc)

Here is what I have tried without success:

Jsoup: Works great but no support for javascript/ajax (so it does not load full page)
Android built in HttpEntity: same problem with javascript/ajax as jsoup
HtmlUnit: Looks exactly what I need but after hours cannot get it to work on Android (Other users failed by trying to load the 12MB+ worth of jar files.  I myself loaded the full source code and referenced it as a project library only to find that things such as Applets and java.awt (used by HtmlUnit) does not exist in Android).
Rhino - I find this very confusing and don't know how to get it working in Android and even if it is what I am looking for.
Selenium Driver: Looks like it can work but you don't have an straightforward way to implement it in a headless way so that you don't have the actual html displayed to a view.

I really want HtmlUnit to work as it seems the best suited for my solution.  Is there any way or at least another library I have missed that is suitable for my needs?
I am currently using Android Studio 0.1.7 and can move to Ellipse if needed.
Thanks in advance!
",https://stackoverflow.com/questions/17399055/android-web-scraping-with-a-headless-browser,content
95,Jsoup Cookies for HTTPS scraping,"
I am experimenting with this site to gather my username on the welcome page to learn Jsoup and Android.  Using the following code
Connection.Response res = Jsoup.connect(""http://www.mikeportnoy.com/forum/login.aspx"")
    .data(""ctl00$ContentPlaceHolder1$ctl00$Login1$UserName"", ""username"", ""ctl00$ContentPlaceHolder1$ctl00$Login1$Password"", ""password"")
    .method(Method.POST)
    .execute();
String sessionId = res.cookie("".ASPXAUTH"");

Document doc2 = Jsoup.connect(""http://www.mikeportnoy.com/forum/default.aspx"")
.cookie("".ASPXAUTH"", sessionId)
.get();

My cookie (.ASPXAUTH) always ends up NULL.  If I delete this cookie in a webbrowser, I lose my connection.  So I am sure it is the correct cookie.  In addition, if I change the code
.cookie("".ASPXAUTH"", ""jkaldfjjfasldjf"")  Using the correct values of course

I am able to scrape my login name from this page.  This also makes me think I have the correct cookie.  So, how come my cookie comes up Null?  Are my username and password name fields incorrect?  Something else?  
Thanks.
",https://stackoverflow.com/questions/7139178/jsoup-cookies-for-https-scraping,content
96,Scraping webpage generated by JavaScript with C#,"
I have a web browser, and a label in Visual Studio, and basically what I'm trying to do is grab a section from another webpage.
I tried using WebClient.DownloadString and WebClient.DownloadFile, and both of them give me the source code of the web page before the JavaScript loads the content.  My next idea was to use a web browser tool and just call webBrowser.DocumentText after the page loaded and that did not work, it still gives me the original source of the page.
Is there a way I can grab the page post JavaScript load?
",https://stackoverflow.com/questions/24288726/scraping-webpage-generated-by-javascript-with-c-sharp,content
97,web scraping dynamic content with python,"
I'd like to use Python to scrape the contents of the ""Were you looking for these authors:"" box on web pages like this one: http://academic.research.microsoft.com/Search?query=lander
Unfortunately the contents of the box get loaded dynamically by JavaScript. Usually in this situation I can read the Javascript to figure out what's going on, or I can use an browser extension like Firebug to figure out where the dynamic content is coming from. No such luck this time...the Javascript is pretty convoluted and Firebug doesn't give many clues about how to get at the content.
Are there any tricks that will make this task easy? 
",https://stackoverflow.com/questions/17608572/web-scraping-dynamic-content-with-python,content
98,How to get text from span tag in BeautifulSoup,"
I have links looks like this
<div class=""systemRequirementsMainBox"">
<div class=""systemRequirementsRamContent"">
<span title=""000 Plus Minimum RAM Requirement"">1 GB</span> </div>

I'm trying to get 1 GB from there. I tried
tt  = [a['title'] for a in soup.select("".systemRequirementsRamContent span"")]
for ram in tt:
    if ""RAM"" in ram.split():
        print (soup.string)

It outputs None.
I tried a['text'] but it gives me KeyError. How can I fix this and what is my mistake?
",https://stackoverflow.com/questions/38133759/how-to-get-text-from-span-tag-in-beautifulsoup,content
99,Is there a PHP equivalent of Perl's WWW::Mechanize?,"
I'm looking for a library that has functionality similar to Perl's WWW::Mechanize, but for PHP. Basically, it should allow me to submit HTTP GET and POST requests with a simple syntax, and then parse the resulting page and return in a simple format all forms and their fields, along with all links on the page.
I know about CURL, but it's a little too barebones, and the syntax is pretty ugly (tons of curl_foo($curl_handle, ...) statements
Clarification:
I want something more high-level than the answers so far. For example, in Perl, you could do something like:
# navigate to the main page
$mech->get( 'http://www.somesite.com/' ); 

# follow a link that contains the text 'download this'
$mech->follow_link( text_regex => qr/download this/i );

# submit a POST form, to log into the site
$mech->submit_form(
    with_fields      => {
        username    => 'mungo',
        password    => 'lost-and-alone',
    }
);

# save the results as a file
$mech->save_content('somefile.zip');

To do the same thing using HTTP_Client or wget or CURL would be a lot of work, I'd have to manually parse the pages to find the links, find the form URL, extract all the hidden fields, and so on. The reason I'm asking for a PHP solution is that I have no experience with Perl, and I could probably build what I need with a lot of work, but it would be much quicker if I could do the above in PHP.
",https://stackoverflow.com/questions/199045/is-there-a-php-equivalent-of-perls-wwwmechanize,content
100,How to put the WebBrowser control into IE9 into standards?,"
i am using automation (i.e. COM automation) to display some HTML in Internet Explorer (9):
ie = CoInternetExplorer.Create;
ie.Navigate2(""about:blank"");
webDocument = ie.Document;
webDocument.Write(szSourceHTML);
webDocument.Close();
ie.Visible = True;

Internet Explorer appears, showing my html, which starts off as:
<!DOCTYPE html>
<HTML>
<HEAD>
   ...


Note: the html5 standards-mode opt-in doctype html

Except that the document is not in ie9 standards mode; it's in ie8 standards mode:


If i save the html to my computer first:

and then view that html document, IE is put into standards mode:

My question is how update my SpawnIEWithSource(String html) function to throw the browser into standards mode?
void SpawnIEWithSource(String html)
{
   Variant ie = CoInternetExplorer.Create();
   ie.Navigate2(""about:blank"");
   webDocument = ie.Document;
   webDocument.Write(html);
   webDocument.Close();
   ie.Visible = true;
}


Edit: A more verbose, less understandable or readable code sample, that doesn't help further the question might be:
IWebBrowser2 ie;
CoCreateInstance(CLASS_InternetExplorer, null, CLSCTX_INPROC_SERVER | CLSCTX_LOCAL_SERVER, IID_WebBrowser2, ie);
ie.AddRef();
ie.Navigate2(""about:blank"");

IHtmlDocument doc;
dispDoc = ie.Document;
dispDoc.AddRef();
dispDoc.QueryInterface(IHTMLDocument2, doc);
dispDoc.Release()
doc.Write(html); 
doc.Close();
doc.Release();
ie.Visible = true;
ie.Release();


Update
Commenter asked on the ieblog entry Testing sites with Browser Mode vs. Doc Mode:

Can we get a description of how the document mode is determined when the HTML content is within an embedded webcontrol? Seems to be that the document mode is choosen differently - maybe for compatibility reasons?

MarkSil [MSFT] responded:

@Thomas: Thanks for raising that question. The WebBrowser Control determines the doc mode the same way that IE does because it contains the same web platform (e.g. there is one shared mshtml.dll across IE and WebBrowser Control hosts). The WebBrowser Control does default to the Compatibility View browser mode, which means that the default doc mode is IE7. Here is a blog post with more detail on this: blogs.msdn.com/.../more-ie8-extensibility-improvements.aspx.

To which Thomas responded:

@MarcSil (re: WebBrowser Control)
The problem with using registry entries to select document mode for WebControl is that it applies to the application as a whole. I write plugins for Google SketchUp where you have WebDialog windows to create UIs - it's just a WebBrowser control in a window. But that leads to problems as I want to force a document mode for my instance of the WebBrowser control, not for all of SU's WebBrowser controls as a whole.
So, my question is: how do you control the document mode per instance for a WebBrowser control?

",https://stackoverflow.com/questions/4097593/how-to-put-the-webbrowser-control-into-ie9-into-standards,content
101,VBA Internet Explorer wait for web page to load,"
I know questions like this have been asked before, but mine is a bit different and has been fairly troubling.  What I'm dealing with is a web page with a form with a few events that load more of the page when certain items in input boxes are filled out.  When these events fire the page loads again, but remains at the same URL with the same nameprop.  I've been using the following types of methods both seperately and strung together to handle waiting for the page to load, but sometimes the VBA still manages to continue executing and set the HTMLDocument variable to a page without the appropriate information on it causing the macro to debug.  Here are the kinds of things I've been trying so far:
While IE.Busy
    DoEvents
Wend

Do Until IE.statusText = ""Done""
    DoEvents
Loop

Do Until IE.readyState = 4
    DoEvents
Loop

I've even attempted to place these events into a loop like the following, but it didn't quite work because the lastModified property only returns a value down to the second and the macro spins through the fields fast enough that it is returning a new page in the same second:
Do Until IE.statusText = ""Done"" And IE.Busy = False And IE.ReadyState = 4 _
And IE.document.lastModified > LastModified ----or---- IE.document.nameprop = _
""some known and expected name prop here""
    While IE.Busy
        DoEvents
    Wend
    Do Until IE.statusText = ""Done""
        DoEvents
    Loop
    Do Until IE.readyState = 4
        DoEvents
    Loop
Loop

Even that fails to wait long enough to set the HTMLDocument object leading to a debug.  I've contemplated setting the next input element and checking that for nothing to further the code, but even that wouldn't be successful 100% of the time because generally the Input elements exist in the HTML but are hidden until the appropriate event is fired, which wouldn't be a problem but they don't load their possible selections until after the event is fired.  It might be an odd page.
Anyway... not sure what else to add.  If there is something else that might be helpful to see just ask.  I guess what I'm looking for is a way to get VBA to wait until IE knows another page isn't on it's way.  It seems to load a few times before it is completely done.
So... Anyone have any ideas?
EDIT:  Found a few new things to try.  Still, no dice.  It was suggested that I add these attempts.  Here is the code, for some reason the VBE and excel instance become non-responsive when using this approach after firing an event that should populate the options on the select element... thinking about trying xml... here is the code:
intCounter = 0
Do until intCounter > 2
    Do Until IE.Busy = False: DoEvents: Loop
    Do Until IE.ReadyState = 4: DoEvents: Loop
    Set HTMLDoc = IE.Document
    Do Until HTMLDoc.ReadyState = ""complete""
    Set HTMLSelect = HTMLDoc.getElementById(""ctl00$ctl00$MainContent$ChildMainContent$ddlEmployeeBranchCodes"")
    intCounter = 0
    For each Opt in HTMLSelect
        intCounter = intCounter + 1
    Next Opt
Loop

Based on what I can see happening on the web page, I know that it is somewhere in this loop that the VBE and Excel become non-responsive.  
Hope that helps... I know it didn't help me... Drats.
EDIT:  Just thought I'd add this.  When it comes to automating a web page, for the most part, I no longer use IE.  I've found it's much better, and sidesteps this issue of async stuff entirely, to simply perform the posts and gets yourself.  May not be the best solution depending on what you're trying to do, but it works pretty reliably if you look at the traffic closely and parameterize things well.
",https://stackoverflow.com/questions/19933313/vba-internet-explorer-wait-for-web-page-to-load,content
103,Can scrapy be used to scrape dynamic content from websites that are using AJAX?,"
I have recently been learning Python and am dipping my hand into building a web-scraper.  It's nothing fancy at all; its only purpose is to get the data off of a betting website and have this data put into Excel.
Most of the issues are solvable and I'm having a good little mess around. However I'm hitting a massive hurdle over one issue. If a site loads a table of horses and lists current betting prices this information is not in any source file. The clue is that this data is live sometimes, with the numbers being updated obviously from some remote server. The HTML on my PC simply has a hole where their servers are pushing through all the interesting data that I need.
Now my experience with dynamic web content is low, so this thing is something I'm having trouble getting my head around. 
I think Java or Javascript is a key, this pops up often. 
The scraper is simply a odds comparison engine.  Some sites have APIs but I need this for those that don't. I'm using the scrapy library with Python 2.7
I do apologize if this question is too open-ended. In short, my question is: how can scrapy be used to scrape this dynamic data so that I can use it?  So that I can scrape this betting odds data in real-time?
",https://stackoverflow.com/questions/8550114/can-scrapy-be-used-to-scrape-dynamic-content-from-websites-that-are-using-ajax,content
108,Simple Screen Scraping using jQuery,"
I have been playing with the idea of using a simple screen-scraper using jQuery and I am wondering if the following is possible.
I have simple HTML page and am making an attempt (if this is possible) to grab the contents of all of the list items from another page, like so:
Main Page:
<!-- jQuery -->
<script type='text/javascript'>
$(document).ready(function(){
$.getJSON(""[URL to other page]"",
  function(data){

    //Iterate through the <li> inside of the URL's data
    $.each(data.items, function(item){
      $(""<li/>"").value().appendTo(""#data"");
    });

  });
});
</script>

<!-- HTML -->
<html>
    <body>
       <div id='data'></div>
    </body>
</html>

Other Page:
//Html
<body>
    <p><b>Items to Scrape</b></p>   
    <ul>
        <li>I want to scrape what is here</li>
        <li>and what is here</li>
        <li>and here as well</li>
        <li>and append it in the main page</li>
    </ul>
</body>

So, is it possible using jQuery to pull all of the list item contents from an external page and append them inside of a div?
",https://stackoverflow.com/questions/5667880/simple-screen-scraping-using-jquery,content
111,file_get_contents() give me 403 Forbidden,"
I have a partner that has created some content for me to scrape.
I can access the page with my browser, but when trying to user file_get_contents, I get a 403 forbidden.
I've tried using stream_context_create, but that's not helping - it might be because I don't know what should go in there.
1) Is there any way for me to scrape the data?
2) If no, and if partner is not allowed to configure server to allow me access, what can I do then?
The code I've tried using:
$opts = array(
  'http'=>array(
    'user_agent' => 'My company name',
    'method'=>""GET"",
    'header'=> implode(""\r\n"", array(
      'Content-type: text/plain;'
    ))
  )
);

$context = stream_context_create($opts);

//Get header content
$_header = file_get_contents($partner_url,false, $context);

",https://stackoverflow.com/questions/11680709/file-get-contents-give-me-403-forbidden,content
112,Scrape a dynamic website,"
What is the best method to scrape a dynamic website where most of the content is generated by what appears to be ajax requests?  I have previous experience with a Mechanize, BeautifulSoup, and python combo, but I am up for something new.
--Edit--
For more detail: I'm trying to scrape the CNN primary database.  There is a wealth of information there, but there doesn't appear to be an api.
",https://stackoverflow.com/questions/206855/scrape-a-dynamic-website,content
113,Scrape web pages in real time with Node.js,"
What's a good was to scrape website content using Node.js. I'd like to build something very, very fast that can execute searches in the style of kayak.com, where one query is dispatched to several different sites, the results scraped, and returned to the client as they become available.
Let's assume that this script should just provide the results in JSON format, and we can process them either directly in the browser or in another web application.
A few starting points:
Using node.js and jquery to scrape websites
Anybody have any ideas?
",https://stackoverflow.com/questions/5211486/scrape-web-pages-in-real-time-with-node-js,content
114,"Nokogiri, open-uri, and Unicode Characters","
I'm using Nokogiri and open-uri to grab the contents of the title tag on a webpage, but am having trouble with accented characters.  What's the best way to deal with these?  Here's what I'm doing:
require 'open-uri'
require 'nokogiri'

doc = Nokogiri::HTML(open(link))
title = doc.at_css(""title"")

At this point, the title looks like this:

Rag\303\271

Instead of:

Rag霉

How can I have nokogiri return the proper character (e.g. 霉 in this case)?
Here's an example URL:
http://www.epicurious.com/recipes/food/views/Tagliatelle-with-Duck-Ragu-242037
",https://stackoverflow.com/questions/2572396/nokogiri-open-uri-and-unicode-characters,content
115,Click on a javascript link within python?,"
I am navigating a site using python's mechanize module and having trouble clicking on a javascript link for next page.  I did a bit of reading and people suggested I need python-spidermonkey and DOMforms.  I managed to get them installed by I am not sure of the syntax to actually click on the link.
I can identify the code on the page as: 
<a href=""javascript:__doPostBack('ctl00$MainContent$gvSearchResults','Page$2')"">2</a>

Does anyone know how to click on it? or if perhaps there's another tool.
Thanks
",https://stackoverflow.com/questions/5207948/click-on-a-javascript-link-within-python,content
116,How can I use Perl to grab text from a web page that is dynamically generated with JavaScript?,"
There is a website I am trying to pull information from in Perl, however the section of the page I need is being generated using javascript so all you see in the source is:
<div id=""results""></div>

I need to somehow pull out the contents of that div and save it to a file using Perl/proxies/whatever. e.g. the information I want to save would be
document.getElementById('results').innerHTML;

I am not sure if this is possible or if anyone had any ideas or a way to do this.
I was using a lynx source dump for other pages but since I cant straight forward screen scrape this page I came here to ask about it!
If anyone is interested, the page is http://downloadcenter.trendmicro.com/index.php?clk=left_nav&clkval=pattern_file&regs=NABU and the info I am trying to get is the row about the ConsumerOPR
",https://stackoverflow.com/questions/2655034/how-can-i-use-perl-to-grab-text-from-a-web-page-that-is-dynamically-generated-wi,content
117,How can i grab CData out of BeautifulSoup,"
I have a website that I'm scraping that has a similar structure the following. I'd like to be able to grab the info out of the CData block. 
I'm using BeautifulSoup to pull other info off the page, so if the solution can work with that, it would help keep my learning curve down as I'm a python novice.
Specifically, I want to get at the two different types of data hidden in the CData statement. the first which is just text I'm pretty sure I can throw a regex at it and get what I need. For the second type, if i could drop the data that has html elements into it's own beautifulsoup, I can parse that. 
I'm just learning python and beautifulsoup, so I'm struggling to find the magical incantation that will give me just the CData by itself.
<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN""   ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">
<html xmlns=""http://www.w3.org/1999/xhtml"">  
<head>  
<title>
   Cows and Sheep
  </title>
</head>
<body>
 <div id=""main"">
  <div id=""main-precontents"">
   <div id=""main-contents"" class=""main-contents"">
    <script type=""text/javascript"">
       //<![CDATA[var _ = g_cow;_[7654]={cowname_enus:'cows rule!',leather_quality:99,icon:'cow_level_23'};_[37357]={sheepname_enus:'baa breath',wool_quality:75,icon:'sheep_level_23'};_[39654].cowmeat_enus = '<table><tr><td><b class=""q4"">cows rule!</b><br></br>
       <!--ts-->
       get it now<table width=""100%""><tr><td>NOW</td><th>NOW</th></tr></table><span>244 Cows</span><br></br>67 leather<br></br>68 Brains
       <!--yy-->
       <span class=""q0"">Cow Bonus: +9 Cow Power</span><br></br>Sheep Power 60 / 60<br></br>Sheep 88<br></br>Cow Level 555</td></tr></table>
       <!--?5695:5:40:45-->
       ';
        //]]>
      </script>
     </div>
     </div>
    </div>
 </body>
</html>

",https://stackoverflow.com/questions/2032172/how-can-i-grab-cdata-out-of-beautifulsoup,content
118,php - Fastest way to check presence of text in many domains (above 1000),"
I have a php script running and using cURL to retrieve the content of webpages on which I would like to check for the presence of some text.
Right now it looks like this:
for( $i = 0; $i < $num_target; $i++ ) {
    $ch = curl_init();
    $timeout = 10;
    curl_setopt ($ch, CURLOPT_URL,$target[$i]);
    curl_setopt ($ch, CURLOPT_RETURNTRANSFER, true);
    curl_setopt ($ch, CURLOPT_FORBID_REUSE, true);
    curl_setopt ($ch, CURLOPT_CONNECTTIMEOUT, $timeout);
    $url = curl_exec ($ch);
    curl_close($ch);

    if (preg_match($text,$url,$match)) {
        $match[$i] = $match;
        echo ""text"" . $text . "" found in URL: "" . $url . "": "" . $match .;

        } else {
        $match[$i] = $match;
        echo ""text"" . $text . "" not found in URL: "" . $url . "": no match"";
        }
}

I was wondering if I could use a special cURL setup that makes it faster ( I looked in the php manual chose the options that seemed the best to me but I may have neglected some that could increase the speed and performance of the script).
I was then wondering if using cgi, Perl or python (or another solution) could be faster than php.
Thank you in advance for any help / advice / suggestion.
",https://stackoverflow.com/questions/12891689/php-fastest-way-to-check-presence-of-text-in-many-domains-above-1000,content
120,How to fetch HTML in Java,"
Without the use of any external library, what is the simplest way to fetch a website's HTML content into a String?
",https://stackoverflow.com/questions/31462/how-to-fetch-html-in-java,content
121,What's the best approach for parsing XML/'screen scraping' in iOS? UIWebview or NSXMLParser?,"
I am creating an iOS app that needs to get some data from a web page. My first though was to use NSXMLParser initWithContentsOfURL: and parse the HTML with the NSXMLParser delegate. However this approach seems like it could quickly become painful (if, for example, the HTML changed I would have to rewrite the parsing code which could be awkward). 
Seeing as I'm loading a web page I took take a look at UIWebView too. It looks like UIWebView may be the way to go. stringByEvaluatingJavaScriptFromString: seems like a very handy way to extract the data and would allow the javascript to be stored in a separate file that would be easy to edit if the HTML changed. However, using UIWebView seems a bit hacky (seeing as UIWebView is a UIView subclass it may block the main thread, and the docs say that the javascript has a limit of 10MB).
Does anyone have any advice regarding parsing XML/HTML before I get stuck in?
UPDATE:
I wrote a blog post about my solution:HTML parsing/screen scraping in iOS
",https://stackoverflow.com/questions/3541615/whats-the-best-approach-for-parsing-xml-screen-scraping-in-ios-uiwebview-or,content
122,How can I read and parse the contents of a webpage in R,"
I'd like to read the contents of a URL (e.q., http://www.haaretz.com/) in R. I am wondering how I can do it
",https://stackoverflow.com/questions/1844829/how-can-i-read-and-parse-the-contents-of-a-webpage-in-r,content
123,Extracting table contents from html with python and BeautifulSoup,"
I want to extract certain information out of an html document. E.g. it contains a table 
(among other tables with other contents) like this:
    <table class=""details"">
            <tr>
                    <th>Advisory:</th>
                    <td>RHBA-2013:0947-1</td>
            </tr>
            <tr>    
                    <th>Type:</th>
                    <td>Bug Fix Advisory</td>
            </tr>
            <tr>
                    <th>Severity:</th>
                    <td>N/A</td>
            </tr>
            <tr>    
                    <th>Issued on:</th>
                    <td>2013-06-13</td>
            </tr>
            <tr>    
                    <th>Last updated on:</th>
                    <td>2013-06-13</td>
            </tr>

            <tr>
                    <th valign=""top"">Affected Products:</th>
                    <td><a href=""#Red Hat Enterprise Linux ELS (v. 4)"">Red Hat Enterprise Linux ELS (v. 4)</a></td>
            </tr>


    </table>

I want to extract Information like the date of ""Issued on:"". It looks like BeautifulSoup4
could do this easyly, but somehow I don't manage to get it right.
My code so far:
    from bs4 import BeautifulSoup
    soup=BeautifulSoup(unicodestring_containing_the_entire_htlm_doc)
    table_tag=soup.table
    if table_tag['class'] == ['details']:
            print table_tag.tr.th.get_text() + "" "" + table_tag.tr.td.get_text()
            a=table_tag.next_sibling
            print  unicode(a)
            print table_tag.contents

This gets me the contents of the first table row, and also a listing of the contents. 
But the next sibling thing is not working right, I guess I am just using it wrong.
Of course I could just parse the contents thingy, but it seems to me that beautiful soup
was designed to prevent us from doing exactly this (if I start parsing myself, I might as
well parse the whole doc ...). If someone could enlighten me on how to acomplish this, I 
would be gratefull. If there is a better way then BeautifulSoup, I would be interested to 
hear about it.
",https://stackoverflow.com/questions/17196018/extracting-table-contents-from-html-with-python-and-beautifulsoup,content
124,How to replace words with span tag using jsoup?,"
Assume I have the following html:
<html>
<head>
</head>
<body>
    <div id=""wrapper"" >
         <div class=""s2"">I am going <a title=""some title"" href="""">by flying</a>
           <p>mr tt</p>
         </div> 
    </div>
</body>    
</html>

Any words in the text nodes that are equal to or greater than 4 characters for example the word 'going' is replaced with html content (not text) <span>going<span> in the original html without changing anything else.
If I try do something like element.html(replacement), the problem is if lets the current element is <div class=""s2""> it will also wipe off <a title=""some title"" 
",https://stackoverflow.com/questions/6527876/how-to-replace-words-with-span-tag-using-jsoup,content
127,Python web scraping involving HTML tags with attributes,"
I'm trying to make a web scraper that will parse a web-page of publications and extract the authors. The skeletal structure of the web-page is the following:
<html>
<body>
<div id=""container"">
<div id=""contents"">
<table>
<tbody>
<tr>
<td class=""author"">####I want whatever is located here ###</td>
</tr>
</tbody>
</table>
</div>
</div>
</body>
</html>

I've been trying to use BeautifulSoup and lxml thus far to accomplish this task, but I'm not sure how to handle the two div tags and td tag because they have attributes. In addition to this, I'm not sure whether I should rely more on BeautifulSoup or lxml or a combination of both. What should I do?
At the moment, my code looks like what is below:
    import re
    import urllib2,sys
    import lxml
    from lxml import etree
    from lxml.html.soupparser import fromstring
    from lxml.etree import tostring
    from lxml.cssselect import CSSSelector
    from BeautifulSoup import BeautifulSoup, NavigableString

    address='http://www.example.com/'
    html = urllib2.urlopen(address).read()
    soup = BeautifulSoup(html)
    html=soup.prettify()
    html=html.replace('&nbsp', '&#160')
    html=html.replace('&iacute','&#237')
    root=fromstring(html)

I realize that a lot of the import statements may be redundant, but I just copied whatever I currently had in more source file.
EDIT: I suppose that I didn't make this quite clear, but I have multiple  tags in page that I want to scrape. 
",https://stackoverflow.com/questions/1391657/python-web-scraping-involving-html-tags-with-attributes,content
128,How can I screen scrape with Perl?,"
I need to display some values that are stored in a website, for that I need to scrape the website and fetch the content from the table. Any ideas?
",https://stackoverflow.com/questions/713827/how-can-i-screen-scrape-with-perl,content
129,Using Nokogiri to Split Content on BR tags,"
I have a snippet of code im trying to parse with nokogiri that looks like this:
<td class=""j"">
    <a title=""title text1"" href=""http://link1.com"">Link 1</a> (info1), Blah 1,<br>
    <a title=""title text2"" href=""http://link2.com"">Link 2</a> (info1), Blah 1,<br>
    <a title=""title text2"" href=""http://link3.com"">Link 3</a> (info2), Blah 1 Foo 2,<br>
</td>

I have access to the source of the td.j using something like this:
data_items = doc.css(""td.j"")
My goal is to split each of those lines up into an array of hashes.  The only logical splitting point i can see is to split on the BRs and then use some regex on the string.  
I was wondering if there's a Better way to do this maybe using nokogiri only?  Even if i could use nokogiri to suck out the 3 line items it would make things easier for me as i could just do some regex parsing on the .content result. 
Not sure how to use Nokogiri to grab lines ending with br though -- should i be using xpaths? any direction is appreciated! thank you
",https://stackoverflow.com/questions/7058922/using-nokogiri-to-split-content-on-br-tags,content
130,Convert a relative URL to an absolute URL with Simple HTML DOM?,"
When I'm scraping content from some pages, the script gives a relative URL. Is it possible to get a absolute URL with Simple HTML DOM?
",https://stackoverflow.com/questions/3329499/convert-a-relative-url-to-an-absolute-url-with-simple-html-dom,content
131,Get instagram followers,"
I want to parse a website's followers count with BeautifulSoup. This is what I have so far:
username_extract = 'lazada_my'

url = 'https://www.instagram.com/'+ username_extract
r = requests.get(url)
soup = BeautifulSoup(r.content,'lxml')
f = soup.find('head', attrs={'class':'count'})

This is the part I want to parse:

Something within my soup.find() function is wrong, but I can't wrap my head around it. When returning f, it is empty. Any idea what I am doing wrong?
",https://stackoverflow.com/questions/49043857/get-instagram-followers,content
132,Screen scraping web page after delay,"
I'm trying to scrape a web page using C#, however after the page loads, it executes some JavaScript which loads more elements into the DOM which I need to scrape. A standard scraper simply grabs the html of the page on load and doesn't pick up the DOM changes made via JavaScript. How do I put in some sort of functionality to wait for a second or two and then grab the source?
Here is my current code:
private string ScrapeWebpage(string url, DateTime? updateDate)
{
    HttpWebRequest request = null;
    HttpWebResponse response = null;
    Stream responseStream = null;
    StreamReader reader = null;
    string html = null;
    try
    {
        //create request (which supports http compression)
        request = (HttpWebRequest)WebRequest.Create(url);
        request.Pipelined = true;
        request.Headers.Add(HttpRequestHeader.AcceptEncoding, ""gzip,deflate"");
        if (updateDate != null)
            request.IfModifiedSince = updateDate.Value;
        //get response.
        response = (HttpWebResponse)request.GetResponse();
        responseStream = response.GetResponseStream();
        if (response.ContentEncoding.ToLower().Contains(""gzip""))
            responseStream = new GZipStream(responseStream,
                CompressionMode.Decompress);
        else if (response.ContentEncoding.ToLower().Contains(""deflate""))
            responseStream = new DeflateStream(responseStream,
                CompressionMode.Decompress);
        //read html.
        reader = new StreamReader(responseStream, Encoding.Default);
        html = reader.ReadToEnd();
    }
    catch
    {
        throw;
    }
    finally
    {
        //dispose of objects.
        request = null;
        if (response != null)
        {
            response.Close();
            response = null;
        }
        if (responseStream != null)
        {
            responseStream.Close();
            responseStream.Dispose();
        }
        if (reader != null)
        {
            reader.Close();
            reader.Dispose();
        }
    }
    return html;
}

Here's a sample URL:
http://www.realtor.com/realestateandhomes-search/geneva_ny#listingType-any/pg-4
You'll see when the page first loads it says 134 listings found, then after a second it says 187 properties found.
",https://stackoverflow.com/questions/5636921/screen-scraping-web-page-after-delay,content
133,How can I scrape pages with dynamic content using node.js?,"
I am trying to scrape a website but I don't get some of the elements, because these elements are dynamically created.
I use the cheerio in node.js and My code is below.
var request = require('request');
var cheerio = require('cheerio');
var url = ""http://www.bdtong.co.kr/index.php?c_category=C02"";

request(url, function (err, res, html) {
    var $ = cheerio.load(html);
    $('.listMain > li').each(function () {
        console.log($(this).find('a').attr('href'));
    });
});

This code returns empty response, because when the page is loaded, the <ul id=""store_list"" class=""listMain""> is empty. 
The content has not been appended yet. 
How can I get these elements using node.js? How can I scrape pages with dynamic content?
",https://stackoverflow.com/questions/28739098/how-can-i-scrape-pages-with-dynamic-content-using-node-js,content
134,Parse HTML content in VBA,"
I have a question relating to HTML parsing. I have a website with some products and I would like to catch text within page into my current spreadsheet. This spreadsheet is quite big but contains ItemNbr in 3rd column, I expect the text in the 14th column and one row corresponds to one product (item).
My idea is to fetch the 'Material' on the webpage which is inside the Innertext after  tag. The id number changes from one page to page (sometimes ).
Here is the structure of the website:
<div style=""position:relative;"">
    <div></div>
    <table id=""list-table"" width=""100%"" tabindex=""1"" cellspacing=""0"" cellpadding=""0"" border=""0"" role=""grid"" aria-multiselectable=""false"" aria-labelledby=""gbox_list-table"" class=""ui-jqgrid-btable"" style=""width: 930px;"">
        <tbody>
            <tr class=""jqgfirstrow"" role=""row"" style=""height:auto"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""1"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""2"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""3"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""4"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""5"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""6"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""7"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td role=""gridcell"" style=""padding-left:10px"" title=""Material"" aria-describedby=""list-table_"">Material</td>
                <td role=""gridcell"" style="""" title=""600D polyester."" aria-describedby=""list-table_"">600D polyester.</td>
            </tr>           
            <tr ...>
            </tr>
        </tbody>
    </table> </div>

I would like to get ""600D Polyester"" as a result.
My (not working) code snippet is as is:
Sub ParseMaterial()

    Dim Cell As Integer
    Dim ItemNbr As String

    Dim AElement As Object
    Dim AElements As IHTMLElementCollection
Dim IE As MSXML2.XMLHTTP60
Set IE = New MSXML2.XMLHTTP60

Dim HTMLDoc As MSHTML.HTMLDocument
Dim HTMLBody As MSHTML.HTMLBody

Set HTMLDoc = New MSHTML.HTMLDocument
Set HTMLBody = HTMLDoc.body

For Cell = 1 To 5                            'I iterate through the file row by row

    ItemNbr = Cells(Cell, 3).Value           'ItemNbr isin the 3rd Column of my spreadsheet

    IE.Open ""GET"", ""http://www.example.com/?item="" & ItemNbr, False
    IE.send

    While IE.ReadyState <> 4
        DoEvents
    Wend

    HTMLBody.innerHTML = IE.responseText

    Set AElements = HTMLDoc.getElementById(""list-table"").getElementsByTagName(""tr"")
    For Each AElement In AElements
        If AElement.Title = ""Material"" Then
            Cells(Cell, 14) = AElement.nextNode.value     'I write the material in the 14th column
        End If
    Next AElement

        Application.Wait (Now + TimeValue(""0:00:2""))

Next Cell

Thanks for your help !
",https://stackoverflow.com/questions/25488687/parse-html-content-in-vba,content
135,Do Google's crawlers interpret Javascript? What if I load a page through AJAX? [closed],"






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.


Closed 10 years ago.







                        Improve this question
                    



When a user enters my page, I have to make another AJAX call...to load data inside a div.
That's just how my application works.
The problem is...when I view the source of this code, it does not contain the source of that AJAX.  Of course, when I do wget URL ...it also does not show the AJAX HTML. Makes sense.
But what about Google? Will Google be able to crawl the content, as if it's a browser?  How do I allow Google to crawl my page just like a user would see it?
",https://stackoverflow.com/questions/2061844/do-googles-crawlers-interpret-javascript-what-if-i-load-a-page-through-ajax,content
136,Spider a Website and Return URLs Only,"
I'm looking for a way to pseudo-spider a website. The key is that I don't actually want the content, but rather a simple list of URIs. I can get reasonably close to this idea with Wget using the --spider option, but when piping that output through a grep, I can't seem to find the right magic to make it work:
wget --spider --force-html -r -l1 http://somesite.com | grep 'Saving to:'

The grep filter seems to have absolutely no affect on the wget output. Have I got something wrong or is there another tool I should try that's more geared towards providing this kind of limited result set?
UPDATE
So I just found out offline that, by default, wget writes to stderr. I missed that in the man pages (in fact, I still haven't found it if it's in there). Once I piped the return to stdout, I got closer to what I need:
wget --spider --force-html -r -l1 http://somesite.com 2>&1 | grep 'Saving to:'

I'd still be interested in other/better means for doing this kind of thing, if any exist.
",https://stackoverflow.com/questions/2804467/spider-a-website-and-return-urls-only,content
137,Selenium wait for Ajax content to load - universal approach,"
Is there a universal approach for Selenium to wait till all ajax content has loaded? (not tied to a specific website - so it works for every ajax website)
",https://stackoverflow.com/questions/33348600/selenium-wait-for-ajax-content-to-load-universal-approach,content
138,find a word on a website and get its page link,"
I want to scrape a few websites and see if the word ""katalog"" is present there. If yes, I want to retrieve the link of all the tabs/sub pages where that word is present. Is it possible to do so?
I tried following this tutorial but the wordlist.csv I get at the end is empty even though the word catalog does exist on the website.
https://www.phooky.com/blog/find-specific-words-on-web-pages-with-scrapy/
        wordlist = [
            ""katalog"",
            ""downloads"",
            ""download""
            ]

def find_all_substrings(string, sub):
    starts = [match.start() for match in re.finditer(re.escape(sub), string)]
    return starts

class WebsiteSpider(CrawlSpider):

    name = ""webcrawler""
    allowed_domains = [""www.reichelt.com/""]
    start_urls = [""https://www.reichelt.com/""]
    rules = [Rule(LinkExtractor(), follow=True, callback=""check_buzzwords"")]

    crawl_count = 0
    words_found = 0                                 

    def check_buzzwords(self, response):

        self.__class__.crawl_count += 1

        crawl_count = self.__class__.crawl_count

        url = response.url
        contenttype = response.headers.get(""content-type"", """").decode('utf-8').lower()
        data = response.body.decode('utf-8')

        for word in wordlist:
                substrings = find_all_substrings(data, word)
                print(""substrings"", substrings)
                for pos in substrings:
                        ok = False
                        if not ok:
                                self.__class__.words_found += 1
                                print(word + "";"" + url + "";"")
        return Item()

    def _requests_to_follow(self, response):
        if getattr(response, ""encoding"", None) != None:
                return CrawlSpider._requests_to_follow(self, response)
        else:
                return []

How can I find all instances of a word on a website and obtain the link of the page where the word is founded?
",https://stackoverflow.com/questions/68193300/find-a-word-on-a-website-and-get-its-page-link,content
139,Send Post Request in Scrapy,"
I am trying to crawl the latest reviews from google play store and to get that I need to make a post request.
With the Postman, it works and I get desired response.

but a post request in terminal gives me a server error
For ex: this page https://play.google.com/store/apps/details?id=com.supercell.boombeach
curl -H ""Content-Type: application/json"" -X POST -d '{""id"": ""com.supercell.boombeach"", ""reviewType"": '0', ""reviewSortOrder"": '0', ""pageNum"":'0'}' https://play.google.com/store/getreviews

gives a server error and
Scrapy just ignores this line:
frmdata = {""id"": ""com.supercell.boombeach"", ""reviewType"": 0, ""reviewSortOrder"": 0, ""pageNum"":0}
        url = ""https://play.google.com/store/getreviews""
        yield Request(url, callback=self.parse, method=""POST"", body=urllib.urlencode(frmdata))

",https://stackoverflow.com/questions/30342243/send-post-request-in-scrapy,content
140,how do web crawlers handle javascript,"
Today a lot of content on Internet is generated using JavaScript (specifically by background AJAX calls). I was wondering how web crawlers like Google handle them. Are they aware of JavaScript? Do they have a built-in JavaScript engine? Or do they simple ignore all JavaScript generated content in the page (I guess quite unlikely). Do people use specific techniques for getting their content indexed which would otherwise be available through background AJAX requests to a normal Internet user? 
",https://stackoverflow.com/questions/1785083/how-do-web-crawlers-handle-javascript,content
141,HtmlUnit Only Displays Host HTML Page for GWT App,"
I am using HtmlUnit API to add crawler support to my GWT app as follows:
PrintWriter out = null;
try {
    resp.setCharacterEncoding(CHAR_ENCODING);
    resp.setContentType(""text/html"");

    url = buildUrl(req);
    out = resp.getWriter();

    WebClient webClient = webClientProvider.get();

    // set options
    WebClientOptions options = webClient.getOptions();
    options.setCssEnabled(false);
    options.setThrowExceptionOnScriptError(false);
    options.setThrowExceptionOnFailingStatusCode(false);
    options.setRedirectEnabled(true);
    options.setJavaScriptEnabled(true);

    // set timeouts
    webClient.setJavaScriptTimeout(0);
    webClient.waitForBackgroundJavaScript(20000);

    // ajax controller
    webClient.setAjaxController(new NicelyResynchronizingAjaxController());

    // render page
    HtmlPage page = webClient.getPage(url);

    webClient.getJavaScriptEngine().pumpEventLoop(timeoutMillis);

    out.println(page.asXml());

    webClient.closeAllWindows();
}
...

However; only the bare HTML host page for my GWT app is produced and sent to the client.

UPDATE: Here is the output from Chrome DevTools:
Request URL:http://127.0.0.1:8888/MyApp.html?gwt.codesvr=127.0.0.1:9997&_escaped_fragment_=myobject%3Bid%3D507ac730e4b0e3b7a73b1b81
Request Method:GET
Status Code:200 OK
Request Headersview source
Accept:text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Charset:ISO-8859-1,utf-8;q=0.7,*;q=0.3
Accept-Encoding:gzip,deflate,sdch
Accept-Language:en-GB,en-US;q=0.8,en;q=0.6
Cache-Control:max-age=0
Connection:keep-alive
Cookie:__utma=96992031.428505342.1351707614.1351707614.1356355174.2; __utmb=96992031.1.10.1356355174; __utmc=96992031; __utmz=96992031.1351707614.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)
Host:127.0.0.1:8888
User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11
Query String Parametersview URL encoded
gwt.codesvr:127.0.0.1:9997
_escaped_fragment_:myobject;id=507ac730e4b0e3b7a73b1b81
Response Headersview source
Content-Type:text/html; charset=utf-8
Server:Jetty(6.1.x)
Transfer-Encoding:chunked

Why isn't the GWT code being executed?
",https://stackoverflow.com/questions/13997424/htmlunit-only-displays-host-html-page-for-gwt-app,content
142,selenium implicitly wait doesn't work,"
This is the first time I use selenium and headless browser as I want to crawl some web page using ajax tech.
The effect is great, but for some case it takes too much time to load the whole page(especially when some resource is unavailable),so I have to set a time out for the selenium.
First of all I tried set_page_load_timeout() and set_script_timeout(),but when I set these timeouts, I won't get any page source if the page doesn't load completely, as the codes below:
driver = webdriver.Chrome(chrome_options=options)
driver.set_page_load_timeout(5)
driver.set_script_timeout(5)
try:
    driver.get(url)
except Exception:
    driver.execute_script('window.stop()')

print driver.page_source.encode('utf-8')  # raise TimeoutException this line.

so I try to using Implicitly Wait and Conditional Wait, like this:
driver = webdriver.Firefox(firefox_options=options, executable_path=path)
print(""Firefox Headless Browser Invoked"")
wait = WebDriverWait(driver, timeout=10)
driver.implicitly_wait(2)
start = time.time()
driver.get(url)
end = time.time()
print 'time used: %s s' % str(end - start)
try:
    WebDriverWait(driver, 2, 0.5).until(expected.presence_of_element_located((By.TAG_NAME, 'body')))
    print driver.find_element_by_tag_name('body').text
except Exception:
    driver.execute_script('window.stop()')

This time I got the content that I want.However,it takes a very long time(40+ seconds),that means the timeout I set for 2 seconds doesn't work at all.
In my view, it seems like the driver.get() call ends until the browser stop loading the page, only after that the codes below can work, and you can not kill the get() call or you'll get nothing.
But this is very different from the selenium docs, I REALLY wonder where is the mistake.
environment: OSX 10.12, selenium 3.0.9 with FireFox & GoogleChrome Headless(both latest version.)
--- update ----
Thanks for help.I change the code as below, using WebDriverWait() alone, but there still exist cases that the call last for a very long time, far more than the timeout that I set.
Wonder if I can stop the page load immediately as the time is out?
driver = webdriver.Firefox(firefox_options=options, executable_path=path)
print(""Firefox Headless Browser Invoked"")
start = time.time()
driver.get('url')
end = time.time()
print 'time used: %s s' % str(end - start)
try:
    WebDriverWait(driver, 2, 0.5).until(expected.presence_of_element_located((By.TAG_NAME, 'body')))
    print driver.find_element_by_tag_name('body').text
except Exception:
    driver.execute_script('window.stop()')
driver.quit()

Here is a terminal output in test:
Firefox Headless Browser Invoked
time used: 44.6049938202 s

according to the code this means the driver.get() call takes 44 seconds to finish call, which is unexpected,I wonder if I misunderstood the behavior of the headless browsers?
",https://stackoverflow.com/questions/48989984/selenium-implicitly-wait-doesnt-work,content
143,How to write a crawler?,"
I have had thoughts of trying to write a simple crawler that might crawl and produce a list of its findings for our NPO's websites and content.
Does anybody have any thoughts on how to do this? Where do you point the crawler to get started? How does it send back its findings and still keep crawling? How does it know what it finds, etc,etc.
",https://stackoverflow.com/questions/102631/how-to-write-a-crawler,content
144,I need a Powerful Web Scraper library [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



I need a powerful web scraper library  for mining contents from web. That can be paid or free both will be fine for me. Please suggest me a library or better way for mining the data  and store in my preferred database. I have searched but i didn't find any good solution for this. I need a good suggestion from experts. Please help me out.
",https://stackoverflow.com/questions/4377355/i-need-a-powerful-web-scraper-library,content
145,scrapy- how to stop Redirect (302),"
I'm trying to crawl a url using Scrapy. But it redirects me to page that doesn't exist. 
Redirecting (302) to <GET http://www.shop.inonit.in/mobile/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/1275197> from <GET http://www.shop.inonit.in/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/pid-1275197.aspx>

The problem is http://www.shop.inonit.in/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/pid-1275197.aspx exists, but http://www.shop.inonit.in/mobile/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/1275197 doesn't, so the crawler cant find this. I've crawled many other websites as well but didn't have this problem anywhere else. Is there a way I can stop this redirect?
Any help would be much appreciated. Thanks.
Update: This is my spider class
class Inon_Spider(BaseSpider):
name = 'Inon'
allowed_domains = ['www.shop.inonit.in']

start_urls = ['http://www.shop.inonit.in/Products/Inonit-Gadget-Accessories-Mobile-Covers/-The-Red-Tag/Samsung-Note-2-Dead-Mau/pid-2656465.aspx']

def parse(self, response):

    item = DealspiderItem()
    hxs = HtmlXPathSelector(response)

    title = hxs.select('//div[@class=""aboutproduct""]/div[@class=""container9""]/div[@class=""ctl_aboutbrand""]/h1/text()').extract()
    price = hxs.select('//span[@id=""ctl00_ContentPlaceHolder1_Price_ctl00_spnWebPrice""]/span[@class=""offer""]/span[@id=""ctl00_ContentPlaceHolder1_Price_ctl00_lblOfferPrice""]/text()').extract()
    prc = price[0].replace(""Rs.  "","""")
    description = []

    item['price'] = prc
    item['title'] = title
    item['description'] = description
    item['url'] = response.url

    return item

",https://stackoverflow.com/questions/15476587/scrapy-how-to-stop-redirect-302,content
146,Creating a generic scrapy spider,"
My question is really how to do the same thing as a previous question, but in Scrapy 0.14.
Using one Scrapy spider for several websites
Basically, I have GUI that takes parameters like domain, keywords, tag names, etc. and I want to create a generic spider to crawl those domains for those keywords in those tags.  I've read conflicting things, using older versions of scrapy, by either overriding the spider manager class or by dynamically creating a spider.  Which method is preferred and how do I implement and invoke the proper solution?  Thanks in advance.
Here is the code that I want to make generic.  It also uses BeautifulSoup.  I paired it down so hopefully didn't remove anything crucial to understand it.
class MySpider(CrawlSpider):

name = 'MySpider'
allowed_domains = ['somedomain.com', 'sub.somedomain.com']
start_urls = ['http://www.somedomain.com']

rules = (
    Rule(SgmlLinkExtractor(allow=('/pages/', ), deny=('', ))),

    Rule(SgmlLinkExtractor(allow=('/2012/03/')), callback='parse_item'),
)

def parse_item(self, response):
    contentTags = []

    soup = BeautifulSoup(response.body)

    contentTags = soup.findAll('p', itemprop=""myProp"")

    for contentTag in contentTags:
        matchedResult = re.search('Keyword1|Keyword2', contentTag.text)
        if matchedResult:
            print('URL Found: ' + response.url)

    pass

",https://stackoverflow.com/questions/9814827/creating-a-generic-scrapy-spider,content
147,Difference between find and filter in jquery,"
I'm working on fetching data from wiki pages. I'm using a combination of php and jquery to do this. First I am using curl in php to fetch page contents and echoing the content. The filename is content.php:
$url = $_GET['url'];
$url = trim($url,"" "");
$url = urldecode($url);
$url = str_replace("" "",""%20"",$url);

echo ""<a class='urlmax'>"".$_GET['title'].""</a>"";
echo crawl($url);

Then jQuery is used to find the matched elements. 
$.get(""content.php"",{url:""http://en.wikipedia.org/w/index.php?action=render&title=""+str_replace("" "",""_"",data[x]),title:str_replace("" "",""_"",data[x])},function(hdata){
                        var imgs = $(hdata).find('a.image img');
                        var ent = $(hdata).filter('a.urlmax');


                        ent = $(ent[0]).text();


});

I was able to successfully get images but for the variable ent when I use find instead of filter, it's returning an empty array. Only filter is working. Why is this?
Edit: I know the basic difference between find and filter. Here both the a.image img and a.urlmax are descendats of the hdata. Then why find does not work on a.urlmax. Not a.urlmax alone it's not working on any other class or id
",https://stackoverflow.com/questions/10378757/difference-between-find-and-filter-in-jquery,content
148,Simple web crawler in C#,"
I have created a simple web crawler but I want to add the recursion function so that every page that is opened I can get the URLs in this page, but I have no idea how I can do that and I want also to include threads to make it faster.
Here is my code
namespace Crawler
{
    public partial class Form1 : Form
    {
        String Rstring;

        public Form1()
        {
            InitializeComponent();
        }

        private void button1_Click(object sender, EventArgs e)
        {
            
            WebRequest myWebRequest;
            WebResponse myWebResponse;
            String URL = textBox1.Text;

            myWebRequest =  WebRequest.Create(URL);
            myWebResponse = myWebRequest.GetResponse();//Returns a response from an Internet resource

            Stream streamResponse = myWebResponse.GetResponseStream();//return the data stream from the internet
                                                                       //and save it in the stream

            StreamReader sreader = new StreamReader(streamResponse);//reads the data stream
            Rstring = sreader.ReadToEnd();//reads it to the end
            String Links = GetContent(Rstring);//gets the links only
            
            textBox2.Text = Rstring;
            textBox3.Text = Links;
            streamResponse.Close();
            sreader.Close();
            myWebResponse.Close();




        }

        private String GetContent(String Rstring)
        {
            String sString="""";
            HTMLDocument d = new HTMLDocument();
            IHTMLDocument2 doc = (IHTMLDocument2)d;
            doc.write(Rstring);
            
            IHTMLElementCollection L = doc.links;
           
            foreach (IHTMLElement links in  L)
            {
                sString += links.getAttribute(""href"", 0);
                sString += ""/n"";
            }
            return sString;
        }

",https://stackoverflow.com/questions/10452749/simple-web-crawler-in-c-sharp,content
149,Wildcards in robots.txt,"
If in WordPress website I have categories in this order:
-Parent
--Child
---Subchild

I have permalinks set to:
%category%/%postname%
Let use an example.
I create post with post name ""Sport game"".
It's tag is sport-game.
It's full url is: domain.com/parent/child/subchild/sport-game
Why I use this kind of permalinks is exactly to block some content easier in robots.txt.
And now this is the part I have question for.
In robots.txt:
User-agent: Googlebot
Disallow: /parent/*
Disallow: /parent/*/*
Disallow: /parent/*/*/*

Disallow: /parent/* Is meaning of this rule that it's blocking domain.com/parent/child but not domain.com/parent/child/subchild and not domain.com/parent/?
Disallow: /parent/*/*/* Is meaning of this that it's blocking domain.com/parent/child/subchild/, that it's blocking only subchild, not child, not parent, and not posts under subchild?
",https://stackoverflow.com/questions/22134608/wildcards-in-robots-txt,content
151,Apache HTTPClient throws java.net.SocketException: Connection reset for many domains,"
I'm creating a (well behaved) web spider and I notice that some servers are causing Apache HttpClient to give me a SocketException -- specifically:
java.net.SocketException: Connection reset

The code that causes this is:
// Execute the request
HttpResponse response; 
try {
    response = httpclient.execute(httpget); //httpclient is of type HttpClient
} catch (NullPointerException e) {
    return;//deep down in apache http sometimes throws a null pointer...  
}

For most servers it's just fine.  But for others, it immediately throws a SocketException.
Example of site that causes immediate SocketException: http://www.bhphotovideo.com/
Works great (as do most websites): http://www.google.com/
Now, as you can see, www.bhphotovideo.com loads fine in a web browser.  It also loads fine when I don't use Apache's HTTP Client.  (Code like this:)
 HttpURLConnection c = (HttpURLConnection)url.openConnection();  
 BufferedInputStream in = new BufferedInputStream(c.getInputStream());  
 Reader r = new InputStreamReader(in);     

 int i;  
 while ((i = r.read()) != -1) {  
      source.append((char) i);  
 }  

So, why don't I just use this code instead?  Well there are some key features in Apache's HTTP Client that I need to use.
Does anyone know what causes some servers to cause this exception?
Research so far:

Problem occurs on my local Mac dev machines AND an AWS EC2 Instance, so it's not a local firewall.
It seems the error isn't caused by the remote machine because the exception doesn't say ""by peer""
This stack overflow seems relavent java.net.SocketException: Connection reset but the answers don't show why this would happen only from Apache HTTP Client and not other approaches.

Bonus question: I'm doing a fair amount of crawling with this system.  Is there generally a better Java class for this other than Apache HTTP Client?  I've found a number of issues (such as the NullPointerException I have to catch in the code above).  It seems that HTTPClient is very picky about server communications -- more picky than I'd like for a crawler that can't just break when a server doesn't behave.
Thanks all! 
Solution
Honestly, I don't have a perfect solution, but it works, so that's good enough for me.
As pointed out by oleg below, Bixo has created a crawler that customizes HttpClient to be more forgiving to servers.  To ""get around"" the issue more than fix it, I just used SimpleHttpFetcher provided by Bixo here:
(linked removed - SO thinks I'm a spammer, so you'll have to google it yourself)
SimpleHttpFetcher fetch = new SimpleHttpFetcher(new UserAgent(""botname"",""contact@yourcompany.com"",""ENTER URL""));
try {
    FetchedResult result = fetch.fetch(""ENTER URL"");
    System.out.println(new String(result.getContent()));
} catch (BaseFetchException e) {
    e.printStackTrace();
}

The down side to this solution is that there are a lot of dependencies for Bixo -- so this may not be a good work around for everyone.  However, you can always just work through their use of DefaultHttpClient and see how they instantiated it to get it to work.  I decided to use the whole class because it handles some things for me, like automatic redirect following (and reporting the final destination url) that are helpful.
Thanks for the help all.
Edit: TinyBixo
Hi all.  So, I loved how Bixo worked, but didn't like that it had so many dependencies (including all of Hadoop).  So, I created a vastly simplified Bixo, without all the dependencies.  If you're running into the problems above, I would recommend using it (and feel free to make pull requests if you'd like to update it!)
It's available here: https://github.com/juliuss/TinyBixo 
",https://stackoverflow.com/questions/5280577/apache-httpclient-throws-java-net-socketexception-connection-reset-for-many-dom,content
152,Web Crawling (Ajax/JavaScript enabled pages) using java,"
I am very new to this web crawling. I am using crawler4j to crawl the websites. I am collecting the required information by crawling these sites. My problem here is I was unable to crawl the content for the following site. http://www.sciencedirect.com/science/article/pii/S1568494612005741. I want to crawl the following information from the aforementioned site (Please take a look at the attached screenshot).

If you observe the attached screenshot it has three names (Highlighted in red boxes). If you click one of the link you will see a popup and that popup contains the whole information about that author. I want to crawl the information which are there in that popup.
I am using the following code to crawl the content.
public class WebContentDownloader {

private Parser parser;
private PageFetcher pageFetcher;

public WebContentDownloader() {
    CrawlConfig config = new CrawlConfig();
    parser = new Parser(config);
    pageFetcher = new PageFetcher(config);
}

private Page download(String url) {
    WebURL curURL = new WebURL();
    curURL.setURL(url);
    PageFetchResult fetchResult = null;
    try {
        fetchResult = pageFetcher.fetchHeader(curURL);
        if (fetchResult.getStatusCode() == HttpStatus.SC_OK) {
            try {
                Page page = new Page(curURL);
                fetchResult.fetchContent(page);
                if (parser.parse(page, curURL.getURL())) {
                    return page;
                }
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    } finally {
        if (fetchResult != null) {
            fetchResult.discardContentIfNotConsumed();
        }
    }
    return null;
}

private String processUrl(String url) {
    System.out.println(""Processing: "" + url);
    Page page = download(url);
    if (page != null) {
        ParseData parseData = page.getParseData();
        if (parseData != null) {
            if (parseData instanceof HtmlParseData) {
                HtmlParseData htmlParseData = (HtmlParseData) parseData;
                return htmlParseData.getHtml();
            }
        } else {
            System.out.println(""Couldn't parse the content of the page."");
        }
    } else {
        System.out.println(""Couldn't fetch the content of the page."");
    }
    return null;
}

public String getHtmlContent(String argUrl) {
    return this.processUrl(argUrl);
}
}

I was able to crawl the content from the aforementioned link/site. But it doesn't have the information what I marked in the red boxes. I think those are the dynamic links.

My question is how can I crawl the content from the aforementioned link/website...???
How to crawl the content from Ajax/JavaScript based websites...???

Please can anyone help me on this.
Thanks & Regards,
Amar
",https://stackoverflow.com/questions/24365154/web-crawling-ajax-javascript-enabled-pages-using-java,content
153,How to follow all links in CasperJS?,"
I'm having trouble clicking all JavaScript based links in a DOM and saving the
output. The links have the form 
<a id=""html"" href=""javascript:void(0);"" onclick=""goToHtml();"">HTML</a>

the following code works great:
var casper = require('casper').create();

var fs = require('fs');

var firstUrl = 'http://www.testurl.com/test.html';

var css_selector = '#jan_html';

casper.start(firstUrl);

casper.thenClick(css_selector, function(){
    console.log(""whoop"");
});

casper.waitFor(function check() {
    return this.getCurrentUrl() != firstUrl;
}, function then() {
    console.log(this.getCurrentUrl());
    var file_title = this.getTitle().split(' ').join('_') + '.html';
    fs.write(file_title, this.getPageContent());
});

casper.run();

However, how can I get this to work with a selector of ""a"", clicking all
available links and saving content? I'm not sure how to get the clickWhileSelector to remove nodes from the selector as is done here: Click on all links matching a selector
",https://stackoverflow.com/questions/20224687/how-to-follow-all-links-in-casperjs,content
154,Are Robots.txt and metadata tags enough to stop search engines to index dynamic pages that are dependent of $_GET variables?,"
I created a php page that is only accessible by means of token/pass received through $_GET
Therefore if you go to the following url you'll get a generic or blank page

http://fakepage11.com/secret_page.php

However if you used the link with the token it shows you special content

http://fakepage11.com/secret_page.php?token=344ee833bde0d8fa008de206606769e4

Of course this is not as safe as a login page, but my only concern is to create a dynamic page that is not indexable and only accessed through the provided link.
Are dynamic pages that are dependent of $_GET variables indexed by google and other search engines?
If so, will include the following be enough to hide it?

Robots.txt User-agent: * Disallow: /

metadata: <META NAME=""ROBOTS"" CONTENT=""NOINDEX"">


Even if I type into google:

site:fakepage11.com/

Thank you!
",https://stackoverflow.com/questions/35504252/are-robots-txt-and-metadata-tags-enough-to-stop-search-engines-to-index-dynamic,content
155,Puppeteer not giving accurate HTML code for page with shadow roots,"
I am trying to download the HTML code for the website intersight.com/help/. But puppeteer is not returning the HTML code with hrefs as we can see in the page (example https://intersight.com/help/getting_started is not present in the downloaded HTML). On inspecting the HTML in browser I came to know that all the missing HTML is present inside the <an-hulk></an-hulk> tags. I don't know what these tags mean.
const puppeteer = require('puppeteer');
const fs = require('fs');
(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  const data = await page.goto('https://intersight.com/help/', { waitUntil: 'domcontentloaded' });
  // Tried all the below lines, neither worked
  // await page.waitForSelector('.helplet-links')
  // document.querySelector(""#app > an-hulk"").shadowRoot.querySelector(""#content"").shadowRoot.querySelector(""#main > div > div > div > an-hulk-home"").shadowRoot.querySelector(""div > div > div:nth-child(1) > div:nth-child(1) > div.helplet-links > ul > li:nth-child(1) > a > span"")
  // await page.evaluateHandle(`document.querySelector(""#app > an-hulk"").shadowRoot.querySelector(""#content"").shadowRoot.querySelector(""#main > div > div > div > an-hulk-home"")`);
  await page.evaluateHandle(`document.querySelector(""an-hulk"").shadowRoot.querySelector(""#aside"").shadowRoot.querySelectorAll("".item"")`)
  const result = await page.content()
  fs.writeFile('./intersight.html', result, (err) => {
    if (err) console.log(err)
    else console.log('done!!')
  })
  // console.log(result)
  await browser.close();
})();

",https://stackoverflow.com/questions/68525115/puppeteer-not-giving-accurate-html-code-for-page-with-shadow-roots,content
156,Get a list of URLs from a site [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 7 years ago.







                        Improve this question
                    



I'm deploying a replacement site for a client but they don't want all their old pages to end in 404s. Keeping the old URL structure wasn't possible because it was hideous.
So I'm writing a 404 handler that should look for an old page being requested and do a permanent redirect to the new page. Problem is, I need a list of all the old page URLs.
I could do this manually, but I'd be interested if there are any apps that would provide me a list of relative (eg: /page/path, not http:/.../page/path) URLs just given the home page. Like a spider but one that doesn't care about the content other than to find deeper pages.
",https://stackoverflow.com/questions/857653/get-a-list-of-urls-from-a-site,content
157,How to switch to new window in Selenium for Python?,"
I am working on selenium automation project using Python.
I am facing an issue, which is handling multiple browser windows.
Scenario is as follows. When I click a link on the home page, a new window opens. In the newly opened window I cannot perform any actions, because the focus is still on the home page web driver.
Can anybody show me how to change focus from the background window to the newly opened window?
A possible solution is driver.switch_to.window(), but it requires the window's name. How to find out the window's name? If this is a wrong way to do this, can anybody give some code examples to perform this action?
",https://stackoverflow.com/questions/10629815/how-to-switch-to-new-window-in-selenium-for-python,auto
158,Add an event to all Forms in a Project,"
If I want to display the size of every Form in my Project in the Form's Title what will be the best approach?
I don't want to manually put a event handler in every Form.
I want the process to be automatic.Something like a overloaded Load() event that adds a handler on the resize event.
",https://stackoverflow.com/questions/51491566/add-an-event-to-all-forms-in-a-project,auto
159,Python - Control window with pywinauto while the window is minimized or hidden,"
What I'm trying to do:
I'm trying to create a script in python with pywinauto to automatically install notepad++ in the background (hidden or minimized), notepad++ is just an example since I will edit it to work with other software.
Problem:
The problem is that I want to do it while the installer is hidden or minimized, but if I move my mouse the script will stop working.
Question:
How can I execute this script and make it work, while the notepad++ installer is hidden or minimized.
This is my code so far:
import sys, os, pywinauto

pwa_app = pywinauto.application.Application()

app = pywinauto.Application().Start(r'npp.6.8.3.Installer.exe')

Wizard = app['Installer Language']

Wizard.NextButton.Click()

Wizard = app['Notepad++ v6.8.3 Setup']

Wizard.Wait('visible')

Wizard['Welcome to the Notepad++ v6.8.3 Setup'].Wait('ready')
Wizard.NextButton.Click()

Wizard['License Agreement'].Wait('ready')
Wizard['I &Agree'].Click()

Wizard['Choose Install Location'].Wait('ready')
Wizard.Button2.Click()

Wizard['Choose Components'].Wait('ready')
Wizard.Button2.Click()

Wizard['Create Shortcut on Desktop'].Wait('enabled').CheckByClick()
Wizard.Install.Click()

Wizard['Completing the Notepad++ v6.8.3 Setup'].Wait('ready', timeout=30)
Wizard['CheckBox'].Wait('enabled').Click()
Wizard.Finish.Click()
Wizard.WaitNot('visible')

",https://stackoverflow.com/questions/32846550/python-control-window-with-pywinauto-while-the-window-is-minimized-or-hidden,auto
160,Selenium: Drag and Drop from file system to WebDriver?,"
I have to test a web-application which contains a drag and drop area for uploading files from the local file system. My test environment is based on C#.
For the automation testing I have used Selenium, but it is not possible to drag files from the file system. The upload area is a div tag (no input tag). So what's the best way to do it? AutoIt (is it possible to drop in a web browser)? Sikuli?
",https://stackoverflow.com/questions/38829153/selenium-drag-and-drop-from-file-system-to-webdriver,auto
161,Python code to automate desktop activities in windows,"
I want to automate desktop activities in Windows environment using Python. How it can be done? Some examples will also be helpful.
By desktop activities, I mean actions such as taking control over mouse and keyboard, access active windows properties, double-click on an icon on the desktop, minimize and maximize windows, enter data to an input popup window through keyboard, etc.
",https://stackoverflow.com/questions/11825322/python-code-to-automate-desktop-activities-in-windows,auto
162,System.Windows.Automation is extremely slow,"
System.Windows.Automation is EXTREMELY slow. 
I execute:
element.FindAll(TreeScope.Children, Condition.TrueCondition);

Obtaining only 30 child elements may take 1000ms on a very fast computer. 
I have even seen it hanging forever while getting the child elements of a Tree in a QT application.
Is this a known problem?
I cannot find any usefull answer after googling a lot.
",https://stackoverflow.com/questions/41768046/system-windows-automation-is-extremely-slow,auto
163,"What's a good, if any, .NET Windows automation library?","
I'm looking for a library that can be used in native .NET code, just like any .NET assembly. The purpose of the library must be to automate Windows (push a button, select a window, send keys, record & playback, that sort of thing).
So: the library is supposed to be used natively in .NET, but the automation itself must be able to target any native or .NET Windows application that can receive user input.

Suggestions so far:

benPearce suggested AutoIt. It has a DLL, which is native Win32 but not native .NET and cannot be used without use of .NET Interop.
Chris Dunaway suggested Global Mouse Keyboard Lib. This came closest, but is not an automation lib. It just helps setting up keyboard and mouse hooks.
pm100 suggested Microsoft's WPF UI Automation. This one is pretty good, albeit that it's not available if you develop in .NET 2.0 and it requires the WPF to be installed on the system. It can, however, automate everything from Win32 apps to HTML in a browser.
JasonTrue suggested WebAI from ArtOfTest. This is a testing framework mainly geared towards browsers and web applications. It is unfortunately not well suitable for use for Windows automation.

If nothing else appears available, I'll probably choose Microsoft's UI Automation and upgrade any projects that require it that are still in .NET 2.0 to .NET 3.5, if possible. But I hope for a more widely applicable automation framework (.NET prior to 2.0 does not need to be supported).
",https://stackoverflow.com/questions/2052915/whats-a-good-if-any-net-windows-automation-library,auto
164,UI Automation events stop being received after a while monitoring an application and then restart after some time,"
We are using Microsoft's UIAutomation framework to develop a client that monitors events of a specific application and responds to them in different ways. We've started with the managed version of the framework, but due to delay issues, moved to the native version wrapped in UIACOMWrapper. After more issues with performance inside our (massive) WPF application, we decided to move it to a separate terminal application (transfer the events to our WPF app through UDP) which seemed to fix all the performance issues. The only problem is that it seems that every several minutes, the events for TabSelection, StructureChanged, WindowOpened and WindowClosed stop being captured for a few minutes. Surprisingly PropertyChanged events are still received and handled while this happens. I will post the relevant code of our event monitor, but this is probably irrelevant as we have seen similar behavior when using Microsoft's own AccEvent utility. I can't post the code of the monitored application as it is proprietary and confidential as well, I can say that it is a WinForms application that hosts WPF windows and also quite massive.
Has anyone seen this sort of behavior while working with the UI Automation framework?
Thank you for your time.
Here's the monitor code (I know the event handling is on the UI Automation threads here but moving it to a dedicated thread did not change anything):
        public void registerHandlers()
    {
        //Register on structure changed and window opened events 
        System.Windows.Automation.Automation.AddStructureChangedEventHandler(
            this.getMsAutomationElement(), System.Windows.Automation.TreeScope.Subtree, this.handleStructureChanged);
        System.Windows.Automation.Automation.AddAutomationEventHandler(
            System.Windows.Automation.WindowPattern.WindowOpenedEvent,
            this.getMsAutomationElement(),
            System.Windows.Automation.TreeScope.Subtree,
            this.handleWindowOpened);
        System.Windows.Automation.Automation.AddAutomationEventHandler(
            System.Windows.Automation.WindowPattern.WindowClosedEvent,
            System.Windows.Automation.AutomationElement.RootElement,
            System.Windows.Automation.TreeScope.Subtree,
            this.handleWindowClosed);

        this.registerValueChanged();
        this.registerTextNameChange();
        this.registerTabSelected();
        this.registerRangeValueChanged();
    }

    private void registerRangeValueChanged()
    {
        if (this.getMsAutomationElement() != null)
        {
            System.Windows.Automation.Automation.AddAutomationPropertyChangedEventHandler(
                    this.getMsAutomationElement(),
                    System.Windows.Automation.TreeScope.Subtree, this.handlePropertyChange,
                    System.Windows.Automation.RangeValuePattern.ValueProperty);
        }
    }

    private void unregisterRangeValueChanged()
    {
        System.Windows.Automation.Automation.RemoveAutomationPropertyChangedEventHandler(
                this.getMsAutomationElement(),
                this.handlePropertyChange);
    }

    private void registerValueChanged()
    {
        if (this.getMsAutomationElement() != null)
        {
            System.Windows.Automation.Automation.AddAutomationPropertyChangedEventHandler(
                this.getMsAutomationElement(),
                System.Windows.Automation.TreeScope.Subtree, this.handlePropertyChange,
                System.Windows.Automation.ValuePattern.ValueProperty);
        }
    }

    private void unregisterValueChanged()
    {
        System.Windows.Automation.Automation.RemoveAutomationPropertyChangedEventHandler(
                            this.getMsAutomationElement(),
                            this.handlePropertyChange);
    }

    private void registerTextNameChange()
    {
        if (this.getMsAutomationElement() != null)
        {
            System.Windows.Automation.Automation.AddAutomationPropertyChangedEventHandler(
            this.getMsAutomationElement(),
            System.Windows.Automation.TreeScope.Subtree, this.handlePropertyChange,
                System.Windows.Automation.AutomationElement.NameProperty);
        }
    }

    private void unregisterTextNameChange()
    {
        System.Windows.Automation.Automation.RemoveAutomationPropertyChangedEventHandler(
        this.getMsAutomationElement(),
        this.handlePropertyChange);
    }
    private void handleWindowOpened(object src, System.Windows.Automation.AutomationEventArgs e)
    {
        Console.ForegroundColor = ConsoleColor.Magenta;
        Console.WriteLine(DateTime.Now.ToShortTimeString() + "" "" + ""Window opened:"" + "" "" + 
            (src as System.Windows.Automation.AutomationElement).Current.Name);

        System.Windows.Automation.AutomationElement element = src as System.Windows.Automation.AutomationElement;
        //this.sendEventToPluginQueue(src, e, element.GetRuntimeId(), this.getAutomationParent(element).GetRuntimeId());
        //Fill out the fields of the control added message
        int[] parentId = this.getAutomationParent(element).GetRuntimeId();
        this.copyToIcdArray(parentId,
            this.protocol.getMessageSet().outgoing.ControlAddedMessage.Data.controlAdded.parentRuntimeId);
        this.copyToIcdArray(element.GetRuntimeId(),
            this.protocol.getMessageSet().outgoing.ControlAddedMessage.Data.controlAdded.runtimeId);
        //Send the message using the protocol
        this.protocol.send(this.protocol.getMessageSet().outgoing.ControlAddedMessage);
    }

    private void copyToIcdArray(int[] runtimeId, ICD.UI_AUTOMATION.RuntimeId icdRuntimeId)
    {
        icdRuntimeId.runtimeIdNumberOfItems.setVal((byte)runtimeId.Count());
        for (int i = 0; i < runtimeId.Count(); i++)
        {
            icdRuntimeId.runtimeIdArray.getElement(i).setVal(runtimeId[i]);
        }
    }

    private void handleWindowClosed(object src, System.Windows.Automation.AutomationEventArgs e)
    {
        if (src != null)
        {
            Console.ForegroundColor = ConsoleColor.Cyan;
            Console.WriteLine(DateTime.Now.ToShortTimeString() + "" "" + ""Window closed:"" + "" "" +
                (src as System.Windows.Automation.AutomationElement).GetRuntimeId().ToString());

            System.Windows.Automation.AutomationElement element = src as System.Windows.Automation.AutomationElement;
            this.copyToIcdArray(element.GetRuntimeId(),
                this.protocol.getMessageSet().outgoing.ControlRemovedMessage.Data.controlRemoved.runtimeId);
            //Send the message using the protocol
            this.protocol.send(this.protocol.getMessageSet().outgoing.ControlRemovedMessage);

            //this.sendEventToPluginQueue(src, e, element.GetRuntimeId());
        }
    }

EDIT: 
I forgot to mention that I strongly suspect that the issue is that one of the UI-Automation event handler threads gets stuck somehow. The reason I believe this, is that when the problem occurred in my monitor, I started an instance of AccEvent and it received all the missing events that my monitor was not getting. This means that the events are being fired but not passed to my monitor.
EDIT2:
I forgot to mention that this happens running in Windows 8 with the specific target application, I have not seen this phenomenon on my own Windows 7 machine with other applications. Another interesting thing is that it seems to happen periodically more or less, but regardless of when I subscribe to events, i.e. it can happen almost immediately after subscribing but then it takes several minutes to reoccur. 
",https://stackoverflow.com/questions/32347734/ui-automation-events-stop-being-received-after-a-while-monitoring-an-application,auto
165,How to get selected text from ANY window (using UI Automation) - C#,"
I have a small tray application which registers a system-wide hotkey. When the user selects a text anywhere in any application and presses this hotkey I want to be able to capture the selected text. I'm currently doing this using AutomationElements:
//Using FocusedElement (since the focused element should be the control with the selected text?)
AutomationElement ae = AutomationElement.FocusedElement;        
AutomationElement txtElement = ae.FindFirst(TreeScope.Subtree,Condition.TrueCondition);
if(txtElement == null)
    return;

TextPattern tp;

try
{
    tp = txtElement.GetCurrentPattern(TextPattern.Pattern) as TextPattern;
}
catch(Exception ex)
{
    return;
}

TextPatternRange[] trs;

if (tp.SupportedTextSelection == SupportedTextSelection.None)
{
    return;
            }
else
{
    trs = tp.GetSelection();
    string selectedText = trs[0].GetText(-1);
    MessageBox.Show(selectedText );

}

This works for some apps (such as notepad, visual studios edit boxes and such) but not for all (such as Word, FireFox, Chrome, and so on.)
Anyone here with any ideas of how to be able to retreive the selected text in ANY application?
",https://stackoverflow.com/questions/4243944/how-to-get-selected-text-from-any-window-using-ui-automation-c-sharp,auto
166,Capture Button Click event inside a MessageBox in another application,"
I want to capture the OK Button's Click event on a MessageBox shown by another WinForms application.
I want to achieve this using UI Automation. After some research, I have found that IUIAutomation::AddAutomationEventHandler will do the work for me.  
Though, I can capture the Click event of any other button, I'm unable to capture a Click event of the MessageBox.  
My code is as follows:  
var FindDialogButton = appElement.FindFirst(TreeScope.Descendants, new PropertyCondition(AutomationElement.NameProperty, ""OK""));

if (FindDialogButton != null)
{
    if (FindDialogButton.GetSupportedPatterns().Any(p => p.Equals(InvokePattern.Pattern)))
    {
        Automation.AddAutomationEventHandler(InvokePattern.InvokedEvent, FindDialogButton, TreeScope.Element, new AutomationEventHandler(DialogHandler));
    }
}

private void DialogHandler(object sender, AutomationEventArgs e)
{
    MessageBox.Show(""Dialog Button clicked at : "" + DateTime.Now);
}


EDIT: 
My Complete code is as follows:  
private void DialogButtonHandle()
{
    AutomationElement rootElement = AutomationElement.RootElement;
    if (rootElement != null)
    {
        System.Windows.Automation.Condition condition = new PropertyCondition
     (AutomationElement.NameProperty, ""Windows Application""); //This part gets the handle of the Windows application that has the MessageBox

        AutomationElement appElement = rootElement.FindFirst(TreeScope.Children, condition);

        var FindDialogButton = appElement.FindFirst(TreeScope.Descendants, new PropertyCondition(AutomationElement.NameProperty, ""OK"")); // This part gets the handle of the button inside the messagebox
        if (FindDialogButton != null)
        {
            if (FindDialogButton.GetSupportedPatterns().Any(p => p.Equals(InvokePattern.Pattern)))
            {
                Automation.AddAutomationEventHandler(InvokePattern.InvokedEvent, FindDialogButton, TreeScope.Element, new AutomationEventHandler(DialogHandler)); //Here I am trying to catch the click of ""OK"" button inside the MessageBox
            }
        }
    }
}

private void DialogHandler(object sender, AutomationEventArgs e)
{
    //On Button click I am trying to display a message that the button has been clicked
    MessageBox.Show(""MessageBox Button Clicked"");
}

",https://stackoverflow.com/questions/58184953/capture-button-click-event-inside-a-messagebox-in-another-application,auto
167,Changing Android Device orientation with ADB,"
I'm using Android 4.4 on a real device and I want to set the device orientation via adb.  I don't want it done with uiautomator since it won't last after the termination of the uiautomator code.
How can I do this?
",https://stackoverflow.com/questions/25864385/changing-android-device-orientation-with-adb,auto
168,Python Get Screen Pixel Value in OS X,"
I'm in the process of building an automated game bot in Python on OS X 10.8.2 and in the process of researching Python GUI automation I discovered autopy. The mouse manipulation API is great, but it seems that the screen capture methods rely on deprecated OpenGL methods...
Are there any efficient ways of getting the color value of a pixel in OS X? The only way I can think of now is to use os.system(""screencapture foo.png"") but the process seems to have unneeded overhead as I'll be polling very quickly.
",https://stackoverflow.com/questions/12978846/python-get-screen-pixel-value-in-os-x,auto
169,UIAutomation won't retrieve children of an element,"
I can see that an element with specific Automation ID has children in the Inspect tool:

But when I try to retrieve them like this:
AutomationElement aPane = mainWindow.FindFirst(TreeScope.Subtree, new PropertyCondition(AutomationElement.AutomationIdProperty, ""8264""));
AutomationElementCollection theChildren = aPane.FindAll(TreeScope.Subtree, Condition.TrueCondition);

The aPane element is retrieved correctly, but theChildren element is empty. Any ideas what went wrong?
",https://stackoverflow.com/questions/14187110/uiautomation-wont-retrieve-children-of-an-element,auto
171,UI automation with excel,"
I am new to UI Automation. In my current organisation I was tasked with making an automated tool using GUI(Graphics User Interface) screen reading, but it is not working perfectly with other my colleague's machine because of a difference in screen resolution. 
I watched this link on you-tube to try and understand UI Automation with excel, but I can't find much on this topic anywhere else.
Can anyone direct me toward resources on UI Automation? I Would like to know where I can learn it, read about it, and how to implement it with Excel.
Thanks in advance I really appreciate if anyone could help me. 
",https://stackoverflow.com/questions/44756042/ui-automation-with-excel,auto
173,AutomationProperties.Name VS x:Name,"
There is no difference for the ""CodedUI test builder"" between the AutomationProperties.Name and x:Name. But the first one can override the second one.
Also the AtomationProperties.Name supports data binding, x:Name of course doesn't.
As we know if you are using the MVVM pattern it is best to only use x:Name when needed.
So should AutomationProperties.Name be preferred to x:Name?
",https://stackoverflow.com/questions/4605777/automationproperties-name-vs-xname,auto
174,set text on textfield / textbox with the automation framework and get the change event,"
I want to set a text on a textfield / textbox element with the Mircosoft UI Automation framework, that means on a AutomationElement from the ControlType.Edit or ControlType.Document.
At the moment i'm using the TextPattern to get the text from one of these AutomationElements:
TextPattern tp = (TextPattern)element.GetCurrentPattern(TextPattern.Pattern);
string text = tp.DocumentRange.GetText(-1).Trim();

But now I want to set a new text in the AutomationElement. I can't find a method for this in the TextPattern class. So I'm trying to use the ValuePattern but I'm not sure if that's the right way to do it:
ValuePattern value = element.GetCurrentPattern(ValuePattern.Pattern) as ValuePattern;
value.SetValue(insertText);

Is there an other way to set the text value?
An other question is how can I get an event when the text was changed on a Edit / Document element? I tried to use the TextChangedEvent but i don't get any events fired when changing the text:
AutomationEventHandler ehTextChanged = new AutomationEventHandler(text_event);
Automation.AddAutomationEventHandler(TextPattern.TextChangedEvent, element, TreeScope.Element, ehTextChanged);

private void text_event(object sender, AutomationEventArgs e)
{
    Console.WriteLine(""Text changed"");
}

",https://stackoverflow.com/questions/10720162/set-text-on-textfield-textbox-with-the-automation-framework-and-get-the-change,auto
176,Automation of Android APK with Espresso,"
I am trying to automate some UI of my Android application(I do not have source code so I am using the APK file) .
I have gone through tutorial provided here and also some tutorial available at Google but all of them require source code.
If anyone have some idea how to automate the UI with Espresso without source code, please help.
I am using IntelliJ as IDE and app android version 5.0.2.
",https://stackoverflow.com/questions/32393159/automation-of-android-apk-with-espresso,auto
177,Is there a way to set the AutomationID of an object without using XAML?,"
I need to automate a Winform application.  How do I set the AutomationID (or AutomationName) like the the XAML in this article  does?
From this stack overflow article the answer seems to be no, unless I switch the application to a WPF application (so I can use XAML to define the controls).
I have tried this na茂ve approach:
  AutomationElement formAutomation = AutomationElement.FromHandle(this.Handle);
  formAutomation.Current.Name = ""SandboxResponseDialogName"";
  formAutomation.Current.ClassName = ""SandboxResponseDialogClassName"";
  formAutomation.Current.AutomationId = ""SandboxResponseDialogID;

But at this point in the constructor for the control, these Automation properties have getters only; no setters.
",https://stackoverflow.com/questions/14617061/is-there-a-way-to-set-the-automationid-of-an-object-without-using-xaml,auto
179,C# : How to detect if screen reader is running?,"
How to detect if screen reader is running (JAWS)?
As I understand in .NET 4 we can use AutomationInteropProvider.ClientsAreListening from System.Windows.Automation.Provider namespace, but what if I have to do it for .NET 2.0?
I tried to inspect ClientsAreListening source code, it calls external RawUiaClientsAreListening method from UIAutomationCore.dll library.
Do you have any ideas how to implement JAWS detection in .NET 2.0?
",https://stackoverflow.com/questions/8079716/c-sharp-how-to-detect-if-screen-reader-is-running,auto
180,Get TitleBar Caption of any application using Microsoft UI Automation?,"
In C# or else VB.Net, how I could use Microsoft UI Automation to retrieve the text of any control that contains text?.
I've been researching in the MSDN Docs, but I don't get it.
Obtain Text Attributes Using UI Automation
Then, for example, with the code below I'm trying to retrieve the text of the Window titlebar by giving the hwnd of that window, but I don't know exactlly how to follow the titlebar to find the child control (label?) that really contains the text.
Imports System.Windows.Automation
Imports System.Windows.Automation.Text

.
Dim hwnd As IntPtr = Process.GetProcessesByName(""notepad"").First.MainWindowHandle

Dim targetApp As AutomationElement = AutomationElement.FromHandle(hwnd)

' The control type we're looking for; in this case 'TitleBar' 
Dim cond1 As New PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.TitleBar)

Dim targetTextElement As AutomationElement =
    targetApp.FindFirst(TreeScope.Descendants, cond1)

Debug.WriteLine(targetTextElement Is Nothing)

In the example above I'm trying with the titlebar, but just I would like to do it with any other control that contains text ...like a titlebar.
PS: I'm aware of P/Invoking GetWindowText API.
",https://stackoverflow.com/questions/30875408/get-titlebar-caption-of-any-application-using-microsoft-ui-automation,auto
182,Using Instruments to test an iOS app without having source code to the application,"
I would like to use UIAutomation via Instruments in Xcode to test an app on my iOS device. Is it possible to do so without having to build the source code? The reason for this is that our team will have testers who will be writing automation scripts to test the apps on our devices, but we don't want them to all go through syncing to latest builds and compiling it through Xcode. Does anyone know if this is possible through UIAutomation or possibly through a 3rd party application?
Thanks.
",https://stackoverflow.com/questions/12045621/using-instruments-to-test-an-ios-app-without-having-source-code-to-the-applicati,auto
183,"Does Microsoft UI Automation Framework work with Chrome, Python and Java Apps?","
I am working on an automation project, in which I need to capture the activities [ application launched, data entered, input type etc.] user performs on a desktop. I came across Microsoft UI Automation framework which so far works well for native windows based applications like MS Office, .NET apps etc. However I did not find any useful information / samples of capturing the information from different web browsers [Chrome is a must], Python apps, Java Apps etc. Can someone please confirm whether MS UI Automation Framework supports such apps. Any working example to extract user activities from these apps would be highly appreciated. Thanks.
",https://stackoverflow.com/questions/47216824/does-microsoft-ui-automation-framework-work-with-chrome-python-and-java-apps,auto
184,How to pass POINT structure to ElementFromPoint method in Python?,"
I'm trying to use method IUIAutomation::ElementFromPoint in Python using comtypes package. There are many examples how to use it in C++, but not in Python. This simple code reproduces the problem on 64-bit Windows 10 (Python 2.7 32-bit):
import comtypes.client

UIA_dll = comtypes.client.GetModule('UIAutomationCore.dll')
UIA_dll.IUIAutomation().ElementFromPoint(10, 10)

I get the following error:
TypeError: Expected a COM this pointer as first argument

Creating the POINT structure this way doesn't help as well:
from ctypes import Structure, c_long

class POINT(Structure):
    _pack_ = 4
    _fields_ = [
        ('x', c_long),
        ('y', c_long),
    ]

point = POINT(10, 10)
UIA_dll.IUIAutomation().ElementFromPoint(point) # raises the same exception

",https://stackoverflow.com/questions/44826285/how-to-pass-point-structure-to-elementfrompoint-method-in-python,auto
186,Reading out Edge Browser Title & Url with System.Windows.Automation,"
I'm trying to read out the TITLE & URL from the Microsoft EDGE Browser.
Doing this with System.Windows.Automation most preferably since the code base already uses this for other problems.

Is it possible with System.Windows.Automation?
How to access the URL?

I'm currently this far:
AutomationId ""TitleBar""
ClassName ""ApplicationFrameWindow""
Name = [string]
=> Reading out this element gives me the TITLE

=> Walking it's children, I find the item ""addressEditBox"":
   AutomationId ""addressEditBox""
   ClassName ""RichEditBox""
   Name ""Search or enter web address""
   => I always get back the string ""Search or enter web address""
   => This is the control where the url is in, though it isn't updated as the user goes to a website, it always returns a fixed string.

In code:
   var digger1 = AutomationElement.FromHandle(process.MainWindowHandle).RootElement.FindAll(TreeScope.Children, Condition.TrueCondition);

       foreach(AutomationElement d1 in digger1 {
          if(d1.Current.ClassName.Equals(""ApplicationFrameWindow"")) {
             var digger2 = d1.FindAll(TreeScope.Children, Condition.TrueCondition);
             foreach(AutomationElement d2 in digger2) {
                if(d2.Current.ClassName.Equals(""Windows.Ui.Core.CoreWindow"")) {
                   var digger3 = d2.FindAll(TreeScope.Children, Condition.TrueCondition);
                   foreach(AutomationElement d3 in digger3) {
                      if(d3.Current.AutomationId.Equals(""addressEditBox"")) {
                          var url = d3.Current.Name;
                          return url;
                      }
                   }
                }
             }
          }
       }

",https://stackoverflow.com/questions/32204961/reading-out-edge-browser-title-url-with-system-windows-automation,auto
187,How to refresh UIMap object in CodedUI,"
Can i refresh UIMap object ? 
Problem is I change the location of UI element on and I again try to get the AutomationElement at that time I get AutomationELment but its BoundingRectanle is infinity. 
So i am assuming that it is not refreshing the UIMap object.
Can anyone please help me on this ?
",https://stackoverflow.com/questions/10848757/how-to-refresh-uimap-object-in-codedui,auto
189,What's the difference of UISpy.exe and Inspect.exe? (From Microsoft Windows SDK),"
I really want to know, how Inspect.exe gets it's UI-Elements, because it gets by far more elements than UISpy (Both available in Microsoft Windows SDK 7).
1) I think UISpy gets it's elements with UIAutomation library, right?
(Tried it with UIAutomation and got exactly the same elements, that UISpy displayed).
2) Which library does Inspect.exe use?
Because it shows some UI-Elements of a Application with MacromediaFlashPlayerActiveX for example, which I need to get in my own UI-Automation-Application, hope somebody knows something about it.
EDIT: Inspect also have a ""UI Automation"" Mode, does it also use UIAutomation library? The strange thing about it is, that in Inspect it also shows many more elements than UISpy.
",https://stackoverflow.com/questions/40496048/whats-the-difference-of-uispy-exe-and-inspect-exe-from-microsoft-windows-sdk,auto
190,"In UI automator viewer Error Obtaining Device screenshot, Reason : Error Unable to connect to adb. Check if adb is installed correctly","
When I click on UI Automator viewer --> Device screenshot throws Error Unable to connect to adb. Check if adb is installed correctly.
I am trying to run it Appium. I am able to load the apk in the emulator, stuck on the UI Automator viewer due to the adb connection error.
",https://stackoverflow.com/questions/42696158/in-ui-automator-viewer-error-obtaining-device-screenshot-reason-error-unable,auto
191,Is there Anyone who Successfully Implement UI Test Automation Regime using Microsoft UI Automation?,"
I am looking for an Automated UI test framework/ software tool. In the past I have been using TestComplete, and although it's a good piece of software, but the concept of GUI test automation was deemed to be sufficiently difficult that I wrote a few posts to complain about it. 
One of the problems with third party test automation tool is that you have to learn new language in order to be productive on it, not to mention that the tooling support is poor. I am now planning to look into Microsoft UI Automation that comes with .Net 3.0 and the White Framework. But before I do that, I want to know what's the outcome there.
Anyone has any experience to share on this? Have you create a sustainable and successful test suite using UI automation on your application?
Edit: This seems like a very hard question. I would setup bounty for this if I don't receive any answers within these few days. 
",https://stackoverflow.com/questions/1249041/is-there-anyone-who-successfully-implement-ui-test-automation-regime-using-micro,auto
192,Python with Selenium: Drag and Drop from file system to webdriver?,"
I have to automate a web-application, which contains a drag and drop area for uploading files from the local file system. My test environment is developed using Python. For the automation tests I have used Selenium, but it is not possible to drag files from the file system, once the upload area is a div tag (No input tag - this way I know it would be easy).
I read a lot of different articles, but by the moment none worked for me. It's important to highlight that I'm not interested in using AutoIT, only native python with selenium.
I found this Selenium: Drag and Drop from file system to webdriver? what looks really promising, however I do not know to adapt to Python.
Thank you a lot in advance!
",https://stackoverflow.com/questions/43382447/python-with-selenium-drag-and-drop-from-file-system-to-webdriver,auto
193,How can I tell if a process has a graphical interface?,"
I'm using automation to test an application, but sometimes I want to start the application via a batch file. When I run ""process.WaitForInputIdle(100)"" I get an error:
""WaitForInputIdle failed.  This could be because the process does not have a graphical interface.""
How can I tell if the process has a graphical interface or not?
",https://stackoverflow.com/questions/3785698/how-can-i-tell-if-a-process-has-a-graphical-interface,auto
196,"When I try to use UI Automation for PowerPoint 2013, I can only get the first character/word when I use RangeFromPoint","
The code works for Word and Outlook but fails with PowerPoint in that only the first character or first word of the textbox ever gets selected. Is this a bug? Is there any workaround? Try this on a simple PowerPoint slide in PowerPoint 2013.
private static async Task<string> getText(double x, double y)
{
    string result = null;

    try
    {
        var location = new System.Windows.Point(x, y);
        AutomationElement element = AutomationElement.FromPoint(location);

        object patternObj;
        if (element.TryGetCurrentPattern(TextPattern.Pattern, out patternObj))
        {
            var textPattern = (TextPattern)patternObj;

            var range = textPattern.RangeFromPoint(location);
            range.ExpandToEnclosingUnit(TextUnit.Word);
            range.Select();

            var text = range.GetText(-1).TrimEnd('\r');
            return text.Trim();
        }
        else
        {
            return ""no text found"";
        }
    }
    catch (Exception ex)
    {
        return ex.Message;
    }
}

You cannot see it from the screenshot, but the mouse is on ""first"" not ""stuck"", but regardless of where the mouse is placed, it always is stuck. Maybe this is fixed in PowerPoint 2016?

When I look at the bounding box for the range it is always the whole element, rather than the selected word. That could be part of the problem of why RangeToPoint is not working.
Original posted in MSDN but no response...
Update. If I use 
text = printRange(range, text);
while (range.Move(TextUnit.Word, 1) > 0)
{
    text += Environment.NewLine;
    text = printRange(range, text);
}

I get

",https://stackoverflow.com/questions/32540442/when-i-try-to-use-ui-automation-for-powerpoint-2013-i-can-only-get-the-first-ch,auto
197,Programmatically turn on/off wifi on real iOS device with UI Automation,"
I have already read once or twice that turn on/off wifi on a real iOS device (iPad in my case) with a UI Automation script seems not possible.
I've also read that you can create a script with the target ""Settings"" but it seems that it's only for simulators, am I right ?
Do you have any ideas or solutions for me ?
Regards, 
",https://stackoverflow.com/questions/21828552/programmatically-turn-on-off-wifi-on-real-ios-device-with-ui-automation,auto
198,How to inject click event with Android UiAutomation.injectInputEvent,"
I'm automating the testing of a flow in my app where I install a device administrator.  To activate a device administrator on most devices (let's assume here I don't have some enterprise API that lets me do this like what Samsung offers) the system displays a popup to the user who then has to click the ""Activate"" button.
I'm using Robotium and Android JUnit to drive my tests.  In a normal testing case one can only interact with the app and process under test and not any system activities that come up.
The UiAutomation claims to allow you to interact with other applications by leveraging the Accessibility Framework, and then allowing one to inject arbitrary input events.
So - here's what I'm trying to do:
public class AbcTests extends ActivityInstrumentationTestCase2<AbcActivity> {

    private Solo mSolo

    @Override
    public void setUp() {
        mSolo = new Solo(getInstrumentation(), getActivity());

    }

    ...

    public void testAbc(){
    
        final UiAutomation automation = getInstrumentation().getUiAutomation();         
        
        MotionEvent motionDown = MotionEvent.obtain(SystemClock.uptimeMillis(), SystemClock.uptimeMillis(), KeyEvent.ACTION_DOWN,
                100,  100, 0);

        automation.injectInputEvent(motionDown, true)
        MotionEvent motionUp = MotionEvent.obtain(SystemClock.uptimeMillis(), SystemClock.uptimeMillis(), KeyEvent.ACTION_UP,
                100, 100, 0);

        automation.injectInputEvent(motionUp, true)
        motionUp.recycle();
        motionDown.recycle();
     }
    
 }

When this test is run the System popup to ""Activate"" the device administrator is active, and I want to just click on the screen.  I've hardcoded in 100,100 as the position for clicks for the purposes of this question but realistically I'll click in the bottom right corner of the screen so I can hit the button.
I do not get any click events occurring on the screen.  Does anyone have experience with this?   Are there any alternatives to do what I want to do?  From my understanding there are very few tools that do this.
Thanks.
Update
Added setSource for right answer
",https://stackoverflow.com/questions/23159265/how-to-inject-click-event-with-android-uiautomation-injectinputevent,auto
199,How to check element properties in iOS gui automation?,"
All UI Automation examples I've seen uses standard components whose state can be inspected with the JavaScript API using the value() method. This is a bit limiting. Lets say you want to check the color or alpha value and whatnot.
How can I inspect the properties of a view?
An example: a tap on a certain element should make it ""selected"". I'd like to perform a tap on it and then verify that isSelected is TRUE.
Update:
I found the withPredicate() method which should do it in theory, except it seems to only trigger on name properties:
element.withPredicate(""isSelected == YES"")          // always fails
element.withPredicate(""name matches 'my element'"")  // works

",https://stackoverflow.com/questions/6504358/how-to-check-element-properties-in-ios-gui-automation,auto
201,How to add UIAutomationClient.dll and UIAutomationTypes.dll to .Net Core 5.0 project?,"
How to use UIAutomationClient.dll and UIAutomationTypes.dll in .NET 5.0 project since there is no nuget package available!
I'm trying to convert a .NET Framework 4.8 project to .NET 5.0
<Project Sdk=""Microsoft.NET.Sdk.WindowsDesktop"">
    <PropertyGroup>
        <OutputType>WinExe</OutputType>
        <TargetFrameworks>net48;net5.0-windows</TargetFrameworks>
        <UseWindowsForms>true</UseWindowsForms>
        <LangVersion>9.0</LangVersion>
    </PropertyGroup>
</Project>

",https://stackoverflow.com/questions/67355538/how-to-add-uiautomationclient-dll-and-uiautomationtypes-dll-to-net-core-5-0-pro,auto
203,Can we use UI Automation tools with the iPhone Simulator?,"
I鈥檝e been using the new UI automation tools with Instruments and the iPhone SDK 4.0, but so far I haven鈥檛 been able to get it to run under the iPhone Simulator. I鈥檝e tried setting the target to every location possible鈥攎y build folder, the app folder in ~/Library/Application Support/iPhone Simulator, etc.鈥攂ut I get an error message when I try to run it:

Unexpected error in -[UIATarget_0x5a1e3b0 frontMostApp], /SourceCache/UIAutomation_Sim/UIAutomation-37/Framework/UIATargetElements.m line 437,

Has anyone gotten this to work?
",https://stackoverflow.com/questions/3397733/can-we-use-ui-automation-tools-with-the-iphone-simulator,auto
204,Getting full contents of a Datagrid using UIAutomation,"
I have need to retrieve all of the items in a Datagrid from an external application using UIAutomation. Currently, I can only retrieve (and view in UISpy) the visible items. Is there a way to cache all of the items in the Datagrid and then pull them? Here's the code:
static public ObservableCollection<Login> GetLogins()
    {

        ObservableCollection<Login> returnLogins = new ObservableCollection<Login>();

        var id = System.Diagnostics.Process.GetProcessesByName(""<Name here>"")[0].Id;
        var desktop = AutomationElement.RootElement;

        var bw = AutomationElement.RootElement.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ProcessIdProperty, id));

        var datagrid = bw.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.AutomationIdProperty, ""lv""));

        var loginLines = datagrid.FindAll(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.DataItem));

        foreach (AutomationElement loginLine in loginLines)
        {
            var loginInstance = new Login { IP = new IP() };

            var loginLinesDetails = loginLine.FindAll(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Custom));

            for (var i = 0; i < loginLinesDetails.Count; i++)
            {
                var cacheRequest = new CacheRequest 
                { 
                    AutomationElementMode = AutomationElementMode.None,
                    TreeFilter = Automation.RawViewCondition
                };

                cacheRequest.Add(AutomationElement.NameProperty);
                cacheRequest.Add(AutomationElement.AutomationIdProperty);

                cacheRequest.Push();

                var targetText = loginLinesDetails[i].FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ClassNameProperty, ""TextBlock""));

                cacheRequest.Pop();

                var myString = targetText.Cached.Name;

                #region Determine data and write to return object
                //Removed private information
                #endregion
                }

            }

            returnLogins.Add(loginInstance);
        }

        return returnLogins;
    }

",https://stackoverflow.com/questions/12129592/getting-full-contents-of-a-datagrid-using-uiautomation,auto
205,How to trigger a click on a chrome extension button?,"
I'm building an automated test suite using Selenium Web Driver. At a certain point I must test how the page works by having a Chrome extension turn on or off. Think of it as you would want to click on the Adblock extension and then click disable for this site. Then, turn it on again. 
I searched all over the Internet and there is no way to implement this using just Selenium. Do you know how could I perform such an action? (from Java ideally)
",https://stackoverflow.com/questions/47042409/how-to-trigger-a-click-on-a-chrome-extension-button,auto
206,Need to store attribute values of a table for assertion in Karate,"
I have a case where locator doesn't have a text value but it's attribute named title has a text value that I need to assert. While writing custom locator for it I can only get the text value which is """" and not specific attribute value say title = ""#abcdd"".
Example:
<div class=""table-cell"" role=""cell"" table-field= ""risk"" title=""high"">high</div>

Has high as value which I can get
Whereas,
<div class=""table-cell"" role=""cell"" table-field= ""colour"" title=""#abcdd""></div>

Doesn't have any text value but need to get title attribute value #abcdd in this case.
Need a generic code to get all such title attribute values present inside this table.
Where are the things going wrong? Any way I can handle this? Or that text value needs to be included in html?
Using karate as Automation test tool.
",https://stackoverflow.com/questions/66718167/need-to-store-attribute-values-of-a-table-for-assertion-in-karate,auto
208,read data from a database or text file in instruments javascript script,"
I have a script like so:
var target = UIATarget.localTarget();
var mainWindow = target.frontMostApp().mainWindow();

var element = mainWindow.textFields()[""UserID""];
element.setValue(""Hello World"");

UIALogger.logStart(""Logging element tree ..."");
target.logElementTree();
UIALogger.logPass();

What I want to do is read a text file or database connection, so I can replace the ""Hello World"" with either a value from a text file or a database query. Is this possible in the Instruments application with using javascript to control UI Automation for the iphone simulator?
",https://stackoverflow.com/questions/19008544/read-data-from-a-database-or-text-file-in-instruments-javascript-script,auto
210,arguments[0].click() not working for select option in selenium,"
I am using selenium for the web application automation.
 I stuck in one point,I am using .ExecuteScript() to perform some action like to click on a link and for that am using :-
((IJavaScriptExecutor)driver).ExecuteScript(""arguments[0].click()"", driver.FindElement(By.XPath(""//a[contains(text(),'Login to the Demo')]"")));

[Note : for every click-able element am using ,this approach because click-able element may be hidden or not visible in web page]
But this approach is not working for  <select> <option>item<option> .. </select>
I am using below code clicking on one of the select option :
((IJavaScriptExecutor)driver).ExecuteScript(""arguments[0].click()"", driver.FindElement(By.XPath(""//select[@id='form_switcher']/option[5]"")));

but nothing is happening nor giving any error/exception.
--Edit start--
But if I use without ExecuteScript() then its work fine:
driver.FindElement(By.XPath(""//select[@id='form_switcher']/option[5]"")).Click();

--Edit end--
[Note : I am using click to select options so that it fire the change event.]
So can anyone please explain me how to click on the select option using ((IJavaScriptExecutor)driver).ExecuteScript


Thanks in advance.
",https://stackoverflow.com/questions/25290100/arguments0-click-not-working-for-select-option-in-selenium,auto
211,How do I get access to a MessageBox through WPF Automation API?,"
How do I get access to MessageBox using the low level WPF Automation API?
I have searched all over but there seems to be very little documentation for this. I would rather not use White as I need more control than it gives.
Thanks
",https://stackoverflow.com/questions/24480596/how-do-i-get-access-to-a-messagebox-through-wpf-automation-api,auto
212,iOS/UI Automation: UIAActionSheet does not have possibilities to manipulate with buttons,"
My question is related to UI Automation template from XCode's Instruments tool. How does UI Automation support UIActionSheet testing? I know that there is a UIAActionSheet element and I was able to obtain it in my application. But I do not know how to get and manipulate with buttons from the action sheet. UI Automation does not provide any elements for these buttons. The UI Automation documentation does not have any info on the matter either. See the link below. It looks like this control does not use UIButton class for the buttons and renders them in some specific way. Could you give me some clue how to reach the buttons from UIAActionSheet? Thank you.
http://developer.apple.com/library/ios/#documentation/ToolsLanguages/Reference/UIAActionSheetClassReference/UIAActionSheet/UIAActionSheet.html#//apple_ref/doc/uid/TP40009895
",https://stackoverflow.com/questions/5250201/ios-ui-automation-uiaactionsheet-does-not-have-possibilities-to-manipulate-with,auto
213,How get current url address on mains browsers using UIAutomation?,"
I have a source that promises to get the active url from any browser using UIAutomation, but I have difficulty about how to call the main function and show result in a ListBox for example. Then, how would it? 
Here is my code:
uses
UIAutomationClient_TLB, activeX;

var
Firefox_quebrou: boolean;

function GetURL(hTargetWnd: HWND): string;
  function Enumerar(pParent: IUIAutomationElement; Scope: TreeScope; pCondition: IUIAutomationCondition): String;
  var
    found    : IUIAutomationElementArray;
    ALen     : Integer;
    i        : Integer;
    iElement : IUIAutomationElement;

    retorno: integer;
    value : WideString;
    iInter: IInterface;
    ValPattern  : IUIAutomationValuePattern;
  begin
    Result := '';
    Firefox_quebrou := false;
    if pParent = nil then
      Exit;
    pParent.FindAll(Scope, pCondition, found);
    found.Get_Length(ALen);
    for i := 1 to ALen - 1 do
    begin
      found.GetElement(i, iElement);
      iElement.Get_CurrentControlType(retorno);
      if (
          (retorno = UIA_EditControlTypeId) or
          (retorno = UIA_GroupControlTypeId)
         ) then //UIA_DocumentControlTypeId
      begin
        iElement.GetCurrentPattern(UIA_ValuePatternId, iInter);
        if Assigned(iInter) then
        begin
          if iInter.QueryInterface(IID_IUIAutomationValuePattern, ValPattern) = S_OK then
          begin
            ValPattern.Get_CurrentValue(value);
            Result := trim(value);
            Firefox_quebrou := true;
            Break;
          end;
        end;
      end;
      if not Firefox_quebrou then
      begin
        Result := Enumerar(iElement, Scope, pCondition);
      end;
    end;

  end;
var
  UIAuto      : IUIAutomation;
  Ret         : Integer;
  RootElement : IUIAutomationElement;
  Scope       : TreeScope;
  varProp     : OleVariant;
  pCondition  : IUIAutomationCondition;
begin
  Result := '';
  try
    UIAuto := CoCUIAutomation.Create;
    if Succeeded(UIAuto.ElementFromHandle(hTargetWnd, RootElement)) then
    begin
      TVariantArg(varProp).vt    := VT_BOOL;
      TVariantArg(varProp).vbool := True;
      UIAuto.CreatePropertyCondition(UIA_IsControlElementPropertyId,
                                     varProp,
                                     pCondition);
      Scope := TreeScope_Element or TreeScope_Children;
      Result := Enumerar(RootElement, Scope, pCondition);
    end;
  except
    Result := '';
  end;
end;

",https://stackoverflow.com/questions/25437652/how-get-current-url-address-on-mains-browsers-using-uiautomation,auto
214,Read cell Items from data grid in SysListView32 of another application using C#,"
I am trying to read data grid items in SysListView32 of another process using C# .net ui-automation and winapi
C# code using ui-automation
http://pastebin.com/6x7rXMiW
C# code using winapi
http://pastebin.com/61RjXZuK
using this code you just have to place your Mouse pointer on SysListView32 on screen and press Enter.
now both code returns empty on the cell item which have following properties
pastebin.com/Rw9FGkYC

but both code works on following properties
pastebin.com/L51T4PLu

the only difference i noted that the name property contains the same data as in cell but problem occurs when name property is empty.
Is there any other way to read the cell ? or any changes I can make, Please elaborate.
",https://stackoverflow.com/questions/10799757/read-cell-items-from-data-grid-in-syslistview32-of-another-application-using-c-s,auto
215,Get url from all open tabs in Google Chrome using VB .Net and UI Automation,"
Hello I have this code working to get current url on Chrome, but only get active tab url. I need to get url from all open tabs using UI Automation.
My working code:
Function GetChromeUrl(ByVal proc As Process) As String
    If proc.MainWindowHandle = IntPtr.Zero Then
    Return Nothing
End If

Dim element As System.Windows.Automation.AutomationElement = AutomationElement.FromHandle(proc.MainWindowHandle)
If element Is Nothing Then
    Return Nothing
End If

Dim edit As System.Windows.Automation.AutomationElement = element.FindFirst(TreeScope.Children, New PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Edit))
Return (edit.GetCurrentPattern(ValuePattern.Pattern)).Current.Value.ToString
End Function

and call it using this code in Form Load event:
For Each proc As Process In Process.GetProcessesByName(""chrome"")
    MsgBox(proc.MainWindowTitle + "" "" + GetChromeUrl(proc))
Next

",https://stackoverflow.com/questions/16305238/get-url-from-all-open-tabs-in-google-chrome-using-vb-net-and-ui-automation,auto
216,Is it possible to set the value of Style property of an element by nightwatch.js ? if yes then how?,"
I am working with nightwatch.js and i am quite new into this automation testing, i want to set the value into the style property of an element by nightwatch.js, so i am asking, is it possible ? if it is possible then how can we implement it.
I can access the style property values and can check by following nightwatch api command but i couldn't find any way to set the style's value to an element using nightwatch.js
browser.expect.element('#main').to.have.css('display').which.equals('block');

",https://stackoverflow.com/questions/34648278/is-it-possible-to-set-the-value-of-style-property-of-an-element-by-nightwatch-js,auto
217,How to get playwrightUrl of Docker container for Playwright (needed to integrate Karate scripts)?,"
I am trying to execute the Karate script in mcr.microsoft.com/playwright:bionic Docker container.
I have exposed the port 5900 as shown below but not sure how to get the playwrightUrl for the container. Do I need to execute the node server.js inside it to get websocket endpoint?
docker run --name playwright -it --rm --ipc=host --cap-add=SYS_ADMIN -u root -p 5900:5900  -v $(pwd):/src -v /home/Automation/:/root/.m2 mcr.microsoft.com/playwright:bionic &

",https://stackoverflow.com/questions/68978017/how-to-get-playwrighturl-of-docker-container-for-playwright-needed-to-integrate,auto
218,c# Getting Chrome URL's from all tab,"
hi i want to get URL from browsers and for chrome i used these and the is not working getting null exception  i think chrome has changed something.. getting error on   elm4 == null.
using UIAutomation i searched more and all the example are not working ...
refrences:- https://stackoverflow.com/a/21799588/5096993
https://social.msdn.microsoft.com/Forums/vstudio/en-US/39bf60a8-2bdc-4aa0-96fb-08dca49cdb06/c-get-all-chrome-urls-opened?forum=csharpgeneral
else if (browser == BrowserType.Chrome)
            {
                //""Chrome_WidgetWin_1""

                Process[] procsChrome = Process.GetProcessesByName(""chrome"");
                foreach (Process chrome in procsChrome)
                {
                    // the chrome process must have a window
                    if (chrome.MainWindowHandle == IntPtr.Zero)
                    {
                        continue;
                    }
                    //AutomationElement elm = AutomationElement.RootElement.FindFirst(TreeScope.Children,
                    //         new PropertyCondition(AutomationElement.ClassNameProperty, ""Chrome_WidgetWin_1""));
                    // find the automation element
                    AutomationElement elm = AutomationElement.FromHandle(chrome.MainWindowHandle);

                    // manually walk through the tree, searching using TreeScope.Descendants is too slow (even if it's more reliable)
                    AutomationElement elmUrlBar = null;
                    try
                    {
                        // walking path found using inspect.exe (Windows SDK) for Chrome 29.0.1547.76 m (currently the latest stable)
                        var elm1 = elm.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.NameProperty, ""Google Chrome""));
                        var elm2 = TreeWalker.ControlViewWalker.GetLastChild(elm1); // I don't know a Condition for this for finding :(
                        var elm3 = elm2.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.NameProperty, """"));
                        var elm4 = elm3.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.ToolBar));
                        elmUrlBar = elm4.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.NameProperty, ""Address and search bar""));
                    }
                    catch
                    {
                        // Chrome has probably changed something, and above walking needs to be modified. :(
                        // put an assertion here or something to make sure you don't miss it
                        continue;
                    }

                    // make sure it's valid
                    if (elmUrlBar == null)
                    {
                        // it's not..
                        continue;
                    }

                    // elmUrlBar is now the URL bar element. we have to make sure that it's out of keyboard focus if we want to get a valid URL
                    if ((bool)elmUrlBar.GetCurrentPropertyValue(AutomationElement.HasKeyboardFocusProperty))
                    {
                        continue;
                    }

                    // there might not be a valid pattern to use, so we have to make sure we have one
                    AutomationPattern[] patterns = elmUrlBar.GetSupportedPatterns();
                    if (patterns.Length == 1)
                    {
                        string ret = """";
                        try
                        {
                            ret = ((ValuePattern)elmUrlBar.GetCurrentPattern(patterns[0])).Current.Value;
                        }
                        catch { }
                        if (ret != """")
                        {
                            // must match a domain name (and possibly ""https://"" in front)
                            if (Regex.IsMatch(ret, @""^(https:\/\/)?[a-zA-Z0-9\-\.]+(\.[a-zA-Z]{2,4}).*$""))
                            {
                                // prepend http:// to the url, because Chrome hides it if it's not SSL
                                if (!ret.StartsWith(""http""))
                                {
                                    ret = ""http://"" + ret;
                                }
                                return ret;
                            }
                        }
                        continue;
                    }
                }

            }

",https://stackoverflow.com/questions/36516260/c-sharp-getting-chrome-urls-from-all-tab,auto
219,"How to locate the x, y coordinates of text on the screen?","
I am trying to find the x, y coordinates of a web element (part of the web page that's open on screen) and some automated tests using the robotframework are being run on it.
I'd like to provide the function with the text string, and get (x, y) coordinates returned.
I am not sure if I can do this in pyautogui.
Environment: Chrome / OS X
EDIT:
I am wondering if I can use locateOnScreen() function in this library to locate text, (but it seems it's only for images according to the documentation)?
",https://stackoverflow.com/questions/42859908/how-to-locate-the-x-y-coordinates-of-text-on-the-screen,auto
220,Getting error while generating test cases for espresso android,"
I am following these two links 1 and 2 for espresso test report but getting error while running ./gradlew createDebugCoverageReport. The error will be shown in image given below. Please help me, I am not able to generated report for espresso and ui automation test cases. Now trying to use jacoco but not able to find any solution. 
Build.Gradle
apply plugin: 'com.android.application'
android {
    compileSdkVersion 24
    buildToolsVersion '25.0.0'
    defaultConfig {
        applicationId ""com.example.project""
        minSdkVersion 18
        targetSdkVersion 24
        versionCode 27
        versionName ""1.5""
        testInstrumentationRunner ""android.support.test.runner.AndroidJUnitRunner""
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
            testCoverageEnabled true
        }
        debug{
            testCoverageEnabled true
        }
    }
}
dependencies {
    compile fileTree(dir: 'libs', include: ['*.jar'])
    androidTestCompile('com.android.support.test.espresso:espresso-core:2.2.2', {
        exclude group: 'com.android.support', module: 'support-annotations'
    })
    compile 'com.github.barteksc:android-pdf-viewer:2.0.3'
    compile 'com.google.code.gson:gson:2.8.0'
    compile 'com.android.support:appcompat-v7:24.2.1'
    compile 'com.android.support:design:24.2.1'
    compile 'com.android.support:recyclerview-v7:24.2.1'
    compile 'com.android.support:cardview-v7:24.2.1'
    compile 'com.android.volley:volley:1.0.0'
    compile 'com.android.support:support-v4:24.2.1'
    compile 'com.google.firebase:firebase-messaging:10.0.1'
    compile 'com.google.firebase:firebase-core:10.0.1'
    compile 'com.google.firebase:firebase-crash:10.0.1'
    testCompile 'junit:junit:4.12'
    compile 'com.android.support:support-annotations:24.2.0'
    androidTestCompile 'com.android.support.test:runner:0.2'
    androidTestCompile 'com.android.support.test:rules:0.2'
    androidTestCompile 'com.android.support.test.uiautomator:uiautomator-v18:2.1.0'
    androidTestCompile 'com.android.support:support-annotations:24.2.1'
    androidTestCompile('com.android.support.test.espresso:espresso-contrib:2.2') {
        exclude group: 'com.android.support', module: 'appcompat'
        exclude group: 'com.android.support', module: 'support-v4'
        exclude group: 'com.android.support', module: 'support-v7'
        exclude group: 'com.android.support', module: 'design'
        exclude module: 'support-annotations'
        exclude module: 'recyclerview-v7'



    }
    androidTestCompile 'com.android.support.test.espresso:espresso-core:2.2.1'
    androidTestCompile ""com.android.support.test.espresso:espresso-intents:2.2.2""
}

apply plugin: 'com.google.gms.google-services'
apply plugin: 'jacoco'

task jacocoTestReport(type: JacocoReport, dependsOn: ['testDebugUnitTest', 'createDebugCoverageReport']) {

    reports {
        xml.enabled = true
        html.enabled = true
    }

    def fileFilter = ['**/R.class', '**/R$*.class', '**/BuildConfig.*', '**/Manifest*.*', '**/*Test*.*', 'android/**/*.*']
    def debugTree = fileTree(dir: ""${buildDir}/intermediates/classes/debug"", excludes: fileFilter)
    def mainSrc = ""${project.projectDir}/src/main/java""

    sourceDirectories = files([mainSrc])
    classDirectories = files([debugTree])
    executionData = fileTree(dir: ""$buildDir"", includes: [
            ""jacoco/testDebugUnitTest.exec"",
            ""outputs/code-coverage/connected/*coverage.ec""
    ])
}

",https://stackoverflow.com/questions/43094105/getting-error-while-generating-test-cases-for-espresso-android,auto
222,How can I login to a website with Python?,"
How can I do it? 
I was trying to enter some specified link (with urllib), but to do it, I need to log in.
I have this source from the site:
<form id=""login-form"" action=""auth/login"" method=""post"">
    <div>
    <!--label for=""rememberme"">Remember me</label><input type=""checkbox"" class=""remember"" checked=""checked"" name=""remember me"" /-->
    <label for=""email"" id=""email-label"" class=""no-js"">Email</label>
    <input id=""email-email"" type=""text"" name=""handle"" value="""" autocomplete=""off"" />
    <label for=""combination"" id=""combo-label"" class=""no-js"">Combination</label>
    <input id=""password-clear"" type=""text"" value=""Combination"" autocomplete=""off"" />
    <input id=""password-password"" type=""password"" name=""password"" value="""" autocomplete=""off"" />
    <input id=""sumbitLogin"" class=""signin"" type=""submit"" value=""Sign In"" />

Is this possible?
",https://stackoverflow.com/questions/2910221/how-can-i-login-to-a-website-with-python,auto
223,Textbox events?,"
I am using Kantu to automate filling out some forms. There is a textbox that when a persons id number is entered and you click into another box or tab out of the textbox it will load that persons vcard. I can try to expound if you need more clarity. 
I don't know much but i'm guessing me clicking into another box is activiating some kind of event to load this vcard. I can't seem to simulate this. Does anyone know of a way to do so?
",https://stackoverflow.com/questions/55977388/textbox-events,auto
224,Web automation from C++,"
We need to do some fairly complex web automation from C++ application (log into application, do some actions, logout), but performance is really important so we are looking at options.

Is there a way to drive WebKit or other headless engine directly from C++, without the need for few more layers in between (like selenium+webdriver+network communication+...)? Chromedriver perhaps?
If option 1 is not possible, what is the most optimal way to run WebDriver (with real browser) from C++?

",https://stackoverflow.com/questions/17345551/web-automation-from-c,auto
225,Execute javascript trough Internet Explorer's com interface using PowerShell,"
I am writing some Internet Explorer automation scripts using PowerShell. Here is how I start the IE com object:
$ie = New-Object -com ""InternetExplorer.Application""
$ie.Navigate(""about:blank"")
$ie.visible = $true

$doc = $ie.Document

So, what I would like to do is to execute some javascript on the $doc object. For example, I have an item on the page that has an onclick event which executes submitCommand('lookup'), so I'd like to run that directly on the $doc instead of having to find the object on the page and then calling the Click() method on it.
It would be easier as the object has no name nor id, making it very sensible to change as I can only rely on it's position on the page (eg: the 11th span item on the page).
Alternatively, how would you select elements based on their class? That would help a lot as the ""button"" has it's own class.
Thanks
",https://stackoverflow.com/questions/1444330/execute-javascript-trough-internet-explorers-com-interface-using-powershell,auto
226,How to code vba to open internet explorer in new session?,"
I am struggling to get this done since months, how to code VBA to open internet explorer in new session i have an application with many logins  i need to open them simultaneously using automation , i have used 
  set ie=new InternetExplorer  

but it opens the ie within the old session, i want to open new session for each and every login please help me, i googled a lot for it but ended up with out any solution.
 this is my code
 Function GetIE() As InternetExplorer

  Dim WScript
Dim objShellWindows

 Set objShell = CreateObject(""Shell.Application"")
 Set objShellWindows = objShell.Windows
 Set WScript = CreateObject(""WScript.Shell"")


 Dim ieStarted
 ieStarted = False

  Dim ieError
  ieError = False

    Dim seconds
      seconds = 0

  While (Not ieStarted) And (Not ieError) And (seconds < 30)

If (Not objShellWindows Is Nothing) Then
    Dim objIE As InternetExplorer
    Dim IE


    For Each objIE In objShellWindows

        If (Not objIE Is Nothing) Then

            If IsObject(objIE.Document) Then
                Set IE = objIE.Document

                If VarType(IE) = 8 Then

                    If IE.Title = EmptyTitle Then
                        If Err.Number = 0 Then
                            IE.Write LoadingMessage

                            objIE.navigate Sheet1.Login.Text
                        ieStarted = True
                        Set GetIE = objIE


                      Else

                       MsgBox ErrorMessage
                            Err.Clear
                            ieError = True

                            Exit For
                        End If
                    End If
                End If
            End If
        End If

        Set IE = Nothing
        Set objIE = Nothing
    Next
End If

Application.Wait Now + TimeValue(""00:00:1"")
seconds = seconds + 1
Wend

 Set objShellWindows = Nothing
 Set objShell = Nothing



   End Function

with this code im able to open the browser but sadly my webpage is opening in outlook which is already opened pls help
",https://stackoverflow.com/questions/14184340/how-to-code-vba-to-open-internet-explorer-in-new-session,auto
227,Selenium Webdriver vs Mechanize,"
I am interested in automating repetitive data entry in some forms for a website I frequent. So far the tools I've looked up that would provide support for this in a headless fashion could be Selenium WebDriver and Mechanize. 
My question is, is there a fundamental technical difference in using once versus the other? Selenium is mostly used for testing. I've also noticed some folks use it for doing exactly what I'm looking for, and that's automating data entry. Testing becomes a second benefit in that case. 
Is there reasons to not use Selenium for what I want to do over Mechanize? Does it not matter and both of these tools will work? 
I'm not asking which is better, I'm asking which is the right tool for the job. Perhaps I'm not understanding the premise behind the purpose of each tool.
",https://stackoverflow.com/questions/31530335/selenium-webdriver-vs-mechanize,auto
229,Puppeteer does not change selector,"
I'm trying to automate the task of querying for data on this site using Puppeteer. So I need to select the dataset (Daily Summaries, 1st option), then select location type (State, 3rd option), then select state (Alaska, 2nd option). The problem is my code does not change to the next table. So instead of selecting the 3rd option (State) after selecting the 1st option in dataset (Daily Summaries), it just selects the 3rd option but in dataset table again! I am new to Puppeteer so I don't really know what to do with this. Any help is appreciated.
Below is my code:


const puppeteer = require('puppeteer');
(async () => {
  const browser = await puppeteer.launch({headless:false})
  const page = await browser.newPage()

  const navigationPromise = page.waitForNavigation()

  await page.goto('https://www.ncdc.noaa.gov/cdo-web/datatools/selectlocation')

  await page.waitForSelector('.selectLocationFilters > .datasetContainer > .slideElement > #datasetSelect > option:nth-child(1)')
  await page.click('.selectLocationFilters > .datasetContainer > .slideElement > #datasetSelect > option:nth-child(1)')

  await page.select('.inset #locationCategorySelect', '')

  await page.waitForSelector('.selectLocationFilters > .locationCategoryContainer > .locationCategoryFilter > #locationCategorySelect > option:nth-child(3)')
  await page.click('.selectLocationFilters > .locationCategoryContainer > .locationCategoryFilter > #locationCategorySelect > option:nth-child(3)')

  await page.select('.inset #selectedState', '')

  await page.waitForSelector('.selectLocationFilters > .locationContainer > .stateFilter > #selectedState > option:nth-child(2)')
  await page.click('.selectLocationFilters > .locationContainer > .stateFilter > #selectedState > option:nth-child(2)')

  await browser.close()
})()


This is what I want. Dataset -> Location type -> State Alaska. Instead the code keeps selecting only in the Dataset table.

",https://stackoverflow.com/questions/61647401/puppeteer-does-not-change-selector,auto
230,Wait for a particular URL in selenium,"
I have the requirement of waiting for a particular URL in website automation using Selenium in Chrome browser. 
The user will be doing online payment on our website. Fro our website user is redirected to the payment gateway. When the user completes the payment, the gateway will redirect to our website. I want to get notified redirection from gateway to our site. 
I got an example which waits for 鈥淧articular Id鈥?in the web page, here is vb.net code
driver.Url = ""http://gmail.com""
   Dim wait As New WebDriverWait(driver, TimeSpan.FromSeconds(10))
                wait.Until(Of IWebElement)(Function(d) d.FindElement(By.Id(""next"")))

This navigates to 鈥済mail.com鈥?and waits for ID 鈥渘ext鈥?on that page. Instead, I want to continue the code only when particular URL loads. 
How can I do this?
Please help me.
",https://stackoverflow.com/questions/37570322/wait-for-a-particular-url-in-selenium,auto
231,Controlling a web browser using Excel VBA,"
I have been assigned the task of automating a web based task ( for a HTTPS website). The users currently are filling in the Excel sheet with the data, they now want to automate excel in such a way that it directly controls the browser and fills in the data.
I found the iMacros Scripting edition as a possible solution for doing this, I wanted to know if there are any other similar tools which can be used for controlling the browser and filling in data.
I also had a look at the Selenium Client Driver, but I am not sure on how to use it in Excel VBA.
Any help would be appreciated.
Thanks,
",https://stackoverflow.com/questions/7489418/controlling-a-web-browser-using-excel-vba,auto
232,How to add wait / Delay until web page is fully loaded in Automation Anywhere?,"
I want to know 'How to add wait or Delay until webpage is fully loaded,' in automations anywhere,
I used 

wait for screen change

But it hold the process until some time specified by the developer , but I want to add delay until the web page fully loaded, 
Is there anyone can help me?
sorry for the bad English.
",https://stackoverflow.com/questions/46641179/how-to-add-wait-delay-until-web-page-is-fully-loaded-in-automation-anywhere,auto
233,Multi-threaded C# Selenium WebDriver automation with Uris not known beforehand,"
I need to perform some simultaneous webdrivers manipulation, but I am uncertain as to how to do this.
What I am asking here is: 

What is the correct way to achieve this ?
What is the reason for the exception I am getting (revealed below)

After some research I ended up with:
1. The way I see people doing this (and the one I ended up using after playing with the API, before searching) is to loop over the window handles my WebDriver has at hand, and perform a switch to and out of the window handle I want to process, closing it when I am finished.
2. Selenium Grid does not seem like an option fore me - am I wrong or it is intended for parallel processing ? Since am running everything in a single computer, it will be of no use for me.

In trying the 1st option, I have the following scenario (a code sample is available below, I skipped stuff that is not relevant/repeat itself (where ever I added 3 dots:
I have a html page, with several submit buttons, stacked.
Clicking each of them will open a new browser/tab (interestingly enough, using ChromeDriver opens tabs, while FirefoxDriver opens separate windows for each.)
As a side note: I can't determine the uris of each submit beforehand (they must be determined by javascript, and at this point, let's just assume I want to handle everything knowing nothing about the client code.
Now, after looping over all the submit buttons, and issuing webElement.Click() on the corresponding elements, the tabs/windows open. The code flows to create a list of tasks to be executed, one for each new tab/window.
The problem is: since all tasks all depend upon the same instance of webdriver to switch to the window handles, seems I will need to add resource sharing locks/control. I am uncertain as whether I am correct, since I saw no mention of locks/resource access control in searching for multi-threaded web driver examples.
On the other hand, if I am able to determine the tabs/windows uris beforehand, I would be able to skip all the automation steps needed to reach this point, and then creating a webDriver instance for each thread, via Navigate().GoToUrl() would be straightforward. But this looks like a deadlock! I don't see webDriver's API providing any access to the newly opened tab/window without performing a switch. And I only want to switch if I do not have to repeat all the automation steps that lead me to the current window !
...
In any case, I keep getting the exception:
Element belongs to a different frame than the current one - switch to its containing frame to use it
at 
IWebElement element = cell.FindElement

inside the ToDictionary() block.
I obviously checked that all my selectors are returning results, in chrome's console. 
foreach (WebElement resultSet in resultSets)
    resultSet.Click();


foreach(string windowHandle in webDriver.WindowHandles.Skip(1))
{
    dataCollectionTasks.Add(Task.Factory.StartNew<List<DataTable>>(obj =>
    {
        List<DataTable> collectedData = new List<DataTable>();
        string window = obj as string;

        if (window != null)
        {
            webDriver.SwitchTo().Window(windowHandle);
            List<WebElement> dataSets = webDriver.FindElements(By.JQuerySelector(utils.GetAppSetting(""selectors.ResultSetData""))).ToList();

            DataTable data = null;

            for (int i = 0; i < dataSets.Count; i += 2)
            {
                data = new DataTable();

                data.Columns.Add(""Col1"", typeof(string));
                data.Columns.Add(""Col2"", typeof(string));
                data.Columns.Add(""Col3"", typeof(string));

                ///...

                //data set header
                if (i % 2 != 0)
                {
                    IWebElement headerElement = dataSets[i].FindElement(OpenQA.Selenium.By.CssSelector(utils.GetAppSetting(""selectors.ResultSetDataHeader"")));
                    data.TableName = string.Join("" "", headerElement.Text.Split().Take(3));
                }
                //data set records
                else
                {
                    Dictionary<string, string> cells = dataSets[i]
                        .FindElements(OpenQA.Selenium.By.CssSelector(utils.GetAppSetting(""selectors.ResultSetDataCell"")))
                        .ToDictionary(
                            cell =>
                            {
                                IWebElement element = cell.FindElement(OpenQA.Selenium.By.CssSelector(utils.GetAppSetting(""selectors.ResultSetDataHeaderColumn"")));
                                return element == null ? string.Empty : element.Text;
                            },
                            cell =>
                            {
                                return cell == null ? string.Empty : cell.Text;
                            });

                    string col1Value, col2Value, col3Value; //...
                    cells.TryGetValue(""Col1"", out col1Value);
                    cells.TryGetValue(""Col2"", out col2Value);
                    cells.TryGetValue(""Col3"", out col3Value);
                    //...

                    data.Rows.Add(col1Value, col2Value, col3Value /*...*/);
                }
            }

            collectedData.Add(data);
        }

        webDriver.SwitchTo().Window(mainWindow);
        webDriver.Close();

        return collectedData;
    }, windowHandle));
} //foreach

Task.WaitAll(dataCollectionTasks.ToArray());
foreach (Task<List<DataTable>> dataCollectionTask in dataCollectionTasks)
{
    results.AddRange(dataCollectionTask.Result);
}

return results;

",https://stackoverflow.com/questions/31654380/multi-threaded-c-sharp-selenium-webdriver-automation-with-uris-not-known-beforeh,auto
235,how to instantiate the webdriver object from the custom library when doing web automation using robot framework,"
while defining user keywords in custom library for web automation,which library should be imported?selenium2library or importing webdriver from selenium.How to use the webdriver to click on some elements.Kindly explain with an example 
",https://stackoverflow.com/questions/35308330/how-to-instantiate-the-webdriver-object-from-the-custom-library-when-doing-web-a,auto
236,can't convert webbot script to an executable,"
I am trying to convert a python script which uses the webbot library for web automation.
As I tried to convert my running Python (3.6.5) script to an .exe file using pyinstaller I was getting an error that the path of the webbot module could not be found.
In order to overcome this problem I tried to specify the path of the module in the spec file, without success. An easier workaround suggests copying the downloaded folder webbot in the same folder where the .exe file is.
Its a very handy tool to use , i don't wanna ditch it .
",https://stackoverflow.com/questions/53138328/cant-convert-webbot-script-to-an-executable,auto
237,IE Web Automation - How to auto select value from combo box using Excel VBA/XML Macro,"
I'm a beginner in VBA and I've failed to select country name automatically in web Combo box or list box from my Excel spreadsheet. My code is entering country name only, but not selecting it. 
How can I change this code so it can pick country name from my Excel spreadsheet and select the same in web combo box as a loop. Passport number, DOB and Nationality are correct on my code. If you'll use manually then you can find the work permit number which I need to capture in my spreadsheet. Chrome Inspect Element screenshot is attached herewith.

My code is as follows:
Sub MOL()
    Dim IE As New SHDocVw.InternetExplorer
    Dim Doc As MSHTML.HTMLDocument
    Dim Buttons As MSHTML.IHTMLElementCollection
    Dim Button As MSHTML.IHTMLElement
    Dim HTMLInput As MSHTML.IHTMLElement
    Dim Tags As MSHTML.IHTMLElement
    Dim HTMLTables As MSHTML.IHTMLElementCollection
    Dim HTMLTable As MSHTML.IHTMLElement
    Dim HTMLRow As MSHTML.IHTMLElement
    Dim HTMLCell As MSHTML.IHTMLElement
    Dim Alltext As IHTMLElementCollection

Application.ScreenUpdating = False
'Application.Calculation = xlCalculationManual
'Application.EnableEvents = False

On Error Resume Next

    IE.Visible = True
    IE.navigate ""https://eservices.mol.gov.ae/SmartTasheel/Complain/IndexLogin?lang=en-gb""

Do While IE.readyState <> READYSTATE_COMPLETE: Loop

Set Doc = IE.document
Set Buttons = Doc.getElementsByTagName(""Button"")
Buttons(2).Click
Do While IE.readyState <> READYSTATE_INTERACTIVE = 3: Loop
Set HTMLInputs = Doc.getElementsByTagName(""Input"")
    HTMLInputs(46).Value = ""somevalue""
    HTMLInputs(48).Value = ""24/02/1990""
    HTMLInputs(47).Value = ""India""
Buttons(21).Click
End Sub

",https://stackoverflow.com/questions/50086005/ie-web-automation-how-to-auto-select-value-from-combo-box-using-excel-vba-xml,auto
238,How can I efficiently parse HTML with Java?,"
I do a lot of HTML parsing in my line of work. Up until now, I was using the HtmlUnit headless browser for parsing and browser automation.
Now, I want to separate both the tasks.
I want to use a light HTML parser because it takes much time in HtmlUnit to first load a page, then get the source and then parse it.
I want to know which HTML parser can parse HTML efficiently. I need

Speed
Ease to locate any HtmlElement by its ""id"" or ""name"" or ""tag type"".

It would be ok for me if it doesn't clean the dirty HTML code. I don't need to clean any HTML source. I just need an easiest way to move across HtmlElements and harvest data from them.
",https://stackoverflow.com/questions/2168610/how-can-i-efficiently-parse-html-with-java,auto
240,How can I catch and process the data from the XHR responses using casperjs?,"
The data on the webpage is displayed dynamically and it seems that checking for every change in the html and extracting the data is a very daunting task and also needs me to use very unreliable XPaths. So I would want to be able to extract the data from the XHR packets. 
I hope to be able to extract information from XHR packets as well as generate 'XHR' packets to be sent to the server. 
The extracting information part is more important for me because the sending of information can be handled easily by automatically triggering html elements using casperjs.
I'm attaching a screenshot of what I mean.
The text in the response tab is the data I need to process afterwards. (This XHR response has been received from the server.)
",https://stackoverflow.com/questions/24555370/how-can-i-catch-and-process-the-data-from-the-xhr-responses-using-casperjs,auto
242,How to scrape a public tableau dashboard? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



Every day I need to downlaod the data available on a public Tableau dashboard. After defining the parameters of interest (time series frequency, time series interval, etc) the dashboard allows you to download the series. 
My life would be reasonably easier if I could automate the download of these series to a database using Python or R. I've already tried to analyze the requests made on the page but I couldn't get much further. Is there any way to automate this process?
The dashboard: https://tableau.ons.org.br/t/ONS_Publico/views/DemandaMxima/HistricoDemandaMxima?:embed=y&:showAppBanner=false&:showShareOptions=true&:display_count=no&:showVizHome=no
",https://stackoverflow.com/questions/62095206/how-to-scrape-a-public-tableau-dashboard,auto
243,"Excel VBA ""Method 'Document' of object 'IWebBrowser2' failed""","
I'm trying to automate a form submission in Excel for work, and In have trouble with the basics. I keep getting the error message:

""Method 'Document' of object 'IWebBrowser2' failed""

With the code as is, and if I include the Or part in the waiting check, I get the error

""Automation Error The object invoked has disconnected from its clients.""

I'm not sure what to do here, I've searched all over for solutions. This code is intended to eventually do more than this, but it keeps failing on the first try to getElementsByTagName. 
Sub GoToWebsiteTest()
Dim appIE As Object 'Internet Explorer
Set appIE = Nothing
Dim objElement As Object
Dim objCollection As Object

If appIE Is Nothing Then Set appIE = CreateObject(""InternetExplorer.Application"")
sURL = *link*
With appIE
    .Visible = True
    .Navigate sURL
End With

Do While appIE.Busy ' Or appIE.ReadyState <> 4
    DoEvents
Loop

Set objCollection = appIE.Document.getElementsByTagName(""input"")

Set appIE = Nothing
End Sub

",https://stackoverflow.com/questions/30086425/excel-vba-method-document-of-object-iwebbrowser2-failed,auto
244,automatically execute an Excel macro on a cell change,"
How can I automatically execute an Excel macro each time a value in a particular cell changes?
Right now, my working code is:
Private Sub Worksheet_Change(ByVal Target As Range)
    If Not Intersect(Target, Range(""H5"")) Is Nothing Then Macro
End Sub

where ""H5"" is the particular cell being monitored and Macro is the name of the macro.
Is there a better way?
",https://stackoverflow.com/questions/409434/automatically-execute-an-excel-macro-on-a-cell-change,auto
245,Switch tabs using Selenium WebDriver with Java,"
Using Selenium WebDriver with Java.
I am trying to automate a functionality where I have to open a new tab do some operations there and come back to previous tab (Parent).
I used switch handle but it's not working.
And one strange thing the two tabs are having same window handle due to which I am not able to switch between tabs.
However when I am trying with different Firefox windows it works, but for tab it's not working.
How can I switch tabs?
Or, how can I switch tabs without using window handle as window handle is same of both tabs in my case?
(I have observed that when you open different tabs in same window, window handle remains same)
",https://stackoverflow.com/questions/12729265/switch-tabs-using-selenium-webdriver-with-java,auto
246,.doc to pdf using python,"
I'am tasked with converting tons of .doc files to .pdf. And the only way my supervisor wants me to do this is through MSWord 2010. I know I should be able to automate this with python COM automation. Only problem is I dont know how and where to start. I tried searching for some tutorials but was not able to find any (May be I might have, but I don't know what I'm looking for). 
Right now I'm reading through this. Dont know how useful this is going to be.
",https://stackoverflow.com/questions/6011115/doc-to-pdf-using-python,auto
247,Stored procedure that Automatically delete rows older than 7 days in MYSQL,"
I would like to know if is possible to create a stored procedure that automatically, every day at 00:00, deletes every row of every table that is over 7 days.
I have seen few solutions but not sure if its what I am looking for, and would be nice if someone has any good example. I know this could be done with simple scripts in python and php, but I would like something more automated by MySQL.
Any help would be really appreciate.
Thanks!
",https://stackoverflow.com/questions/32507258/stored-procedure-that-automatically-delete-rows-older-than-7-days-in-mysql,auto
248,How to Automatically Start a Download in PHP?,"
What code do you need to add in PHP to automatically have the browser download a file to the local machine when a link is visited?
I am specifically thinking of functionality similar to that of download sites that prompt the user to save a file to disk once you click on the name of the software?
",https://stackoverflow.com/questions/40943/how-to-automatically-start-a-download-in-php,auto
249,How can I use powershell to run through an installer?,"
I am trying to install a piece of software that when done manually has configuration options you can choose from when going through the process. I am trying to figure out a way to automate this using powershell but am stuck as to how I can set those configuration options. I believe I would need to run the start-process command on the installer .exe but I don't know where to go from there. Can I use the parameters on the start-process command to pass in the configurations I want? 
",https://stackoverflow.com/questions/46221983/how-can-i-use-powershell-to-run-through-an-installer,auto
254,"C# WebBrowser Control - Form Submit Not Working using InvokeMember(""Click"")","
I am working on automated testing script and am using the WebBrowser control. I am trying to submit the following HTML and testing when the user accepts the terms of service:
    <form action=""http://post.dev.dealerconnextion/k/6hRbDTwn4xGVl2MHITQsBw/hrshq"" method=""post"">
        <input name=""StepCheck"" value=""U2FsdGVkX18zMTk5MzE5OUgFyFgD3V5yf5Rwbtfhf3gjdH4KSx4hqj4vkrw7K6e-"" type=""hidden"">
        <button type=""submit"" name=""continue"" value=""y"">ACCEPT the terms of use</button>
        <button type=""submit"" name=""continue"" value=""n"">DECLINE the terms of use</button>
    </form>

    // Terms of Use Information

    <form action=""http://post.dev.dealerconnextion/k/6hRbDTwn4xGVl2MHITQsBw/hrshq"" method=""post"">
        <input name=""StepCheck"" value=""U2FsdGVkX18zMTk5MzE5OUgFyFgD3V5yf5Rwbtfhf3gjdH4KSx4hqj4vkrw7K6e-"" type=""hidden"">
        <button type=""submit"" name=""continue"" value=""y"">ACCEPT the terms of use</button>
        <button type=""submit"" name=""continue"" value=""n"">DECLINE the terms of use</button>
    </form>

Here is the code in C#, but does not submit the form.
            HtmlElementCollection el = webBrowser.Document.GetElementsByTagName(""button"");
            foreach (HtmlElement btn in el)
            {
                if (btn.InnerText == ""ACCEPT the terms of use"")
                {
                    btn.InvokeMember(""Click"");
                }
            }

Any help would be much appreciated. Thanks.
",https://stackoverflow.com/questions/19044659/c-sharp-webbrowser-control-form-submit-not-working-using-invokememberclick,auto
255,webdriver.FirefoxProfile(): Is it possible to use a profile without making a copy of it?,"
As the documentation states, you can call webdriver.FirefoxProfile() with the optional argument of profile_directory to point to the directory of a specific profile you want the browser to use. I noticed it was taking a long time to run this command, so when I looked into the code, it looked like it was copying the specified profile Problem is, it takes an extremely long time for the profile to copy (something like >30 minutes, didn't have the patience to wait for it to finish.)
I'm using a hybrid of userscripts and selenium to do some automation for me, so to setup a new profile every single time I want to test out my code would be burdensome.
Is the only way to change this behaviour to edit the firefox_profile.py itself (if so, what would be the best way to go about it?)?
",https://stackoverflow.com/questions/49356081/webdriver-firefoxprofile-is-it-possible-to-use-a-profile-without-making-a-cop,auto
256,Handling Browser Authentication using Selenium,"
Does anyone know about handling Browser Authentication using Selenium or any other tool during automation? 
",https://stackoverflow.com/questions/10395462/handling-browser-authentication-using-selenium,auto
257,How to get the range of occupied cells in excel sheet,"
I use C# to automate an excel file. I was able to get the workbook and the sheets it contains.
If for example I have in sheet1 two cols and 5 rows. I wanted o get the range for the occupied cells as A1:B5. I tried the following code but it did not give the correct result.
the columns # and row # were much bigger and the cells were empty as well. 
     Excel.Range xlRange = excelWorksheet.UsedRange;
     int col = xlRange.Columns.Count;
     int row = xlRange.Rows.Count;

Is there another way I can use to get that range?
",https://stackoverflow.com/questions/1284388/how-to-get-the-range-of-occupied-cells-in-excel-sheet,auto
258,Selenium Webdriver + Java - Eclipse: java.lang.NoClassDefFoundError,"
I installed JDK from here: http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
(This version for windows x64: Java SE Development Kit 8u151)
I downloaded eclipse from here:
http://www.eclipse.org/downloads/packages/eclipse-ide-java-developers/oxygenr
(Windows 64-bit)
I opened a new project in eclipse: File->New->Java Project
Then I downloaded Selenium Java Jars from here:
http://www.seleniumhq.org/download/ ---> java language
Then in eclipse I click on my project -> properties ->Java Build Path -> Libraries tab -> Add External JARs... -> I go to ""SeleniumDrivers\Java"" library (there I saved all the JARS that I downloaded) -> I checked all the files there:
these files
I clicked on ""ok"" and created a new class in eclipse
Then I downloaded chromedriver from here: http://www.seleniumhq.org/download
I unzipped it and saved it here: C:\Selenium\Drivers
This is my script:
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.chrome.ChromeDriver;

public class MainClass {

    public static void main(String[] args) {
        // TODO Auto-generated method stub
        System.out.println(""hi there\n"");

        System.setProperty(""webdriver.chrome.driver"", 
        ""C:/Selenium/Drivers/chromedriver.exe"");
        WebDriver driver = new ChromeDriver();
        driver.get(""https://www.facebook.com"");
    }

}

As you can see, this is a very basic script which opens chrome browser and navigate to facebook.
I run this script and got this error:
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/http/config/RegistryBuilder
    at org.openqa.selenium.remote.internal.HttpClientFactory.getClientConnectionManager(HttpClientFactory.java:69)
    at org.openqa.selenium.remote.internal.HttpClientFactory.<init>(HttpClientFactory.java:57)
    at org.openqa.selenium.remote.internal.HttpClientFactory.<init>(HttpClientFactory.java:60)
    at org.openqa.selenium.remote.internal.ApacheHttpClient$Factory.getDefaultHttpClientFactory(ApacheHttpClient.java:242)
    at org.openqa.selenium.remote.internal.ApacheHttpClient$Factory.<init>(ApacheHttpClient.java:219)
    at org.openqa.selenium.remote.HttpCommandExecutor.getDefaultClientFactory(HttpCommandExecutor.java:93)
    at org.openqa.selenium.remote.HttpCommandExecutor.<init>(HttpCommandExecutor.java:72)
    at org.openqa.selenium.remote.service.DriverCommandExecutor.<init>(DriverCommandExecutor.java:63)
    at org.openqa.selenium.chrome.ChromeDriverCommandExecutor.<init>(ChromeDriverCommandExecutor.java:36)
    at org.openqa.selenium.chrome.ChromeDriver.<init>(ChromeDriver.java:181)
    at org.openqa.selenium.chrome.ChromeDriver.<init>(ChromeDriver.java:168)
    at org.openqa.selenium.chrome.ChromeDriver.<init>(ChromeDriver.java:123)
    at MainClass.main(MainClass.java:11)
Caused by: java.lang.ClassNotFoundException: org.apache.http.config.RegistryBuilder
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    ... 13 more

I don't know how to resolve this issue, can you please help to solve it so that I will be able to run my basic script?
",https://stackoverflow.com/questions/47104058/selenium-webdriver-java-eclipse-java-lang-noclassdeffounderror,auto
259,How to use ADB to send touch events to device using sendevent command?,"
I am trying to send touch events to a device using AndroidDebugBridge, so that I can do some basic automation for UI tests. I have followed the discussion in LINK. I am able to use sendevent to simulate touch on emulators, but unable to do the same on a device. 
Like in above link the emulator seems to send out 6 events for each touch ( xcoord, ycoord, 2 for press,2 for release) and it was easy to use this information to sendevents, but a getevent for the touchscreen for a device seems to generate far too many events.
Has somebody managed to send touch from ADB to a device? Could you please share the solution.
",https://stackoverflow.com/questions/3437686/how-to-use-adb-to-send-touch-events-to-device-using-sendevent-command,auto
261,Selenium Webdriver: How to Download a PDF File with Python?,"
I am using selenium webdriver to automate downloading several PDF files. I get the PDF preview window (see below), and now I would like to download the file. How can I accomplish this using Google Chrome as the browser?  

",https://stackoverflow.com/questions/43149534/selenium-webdriver-how-to-download-a-pdf-file-with-python,auto
262,"How can I automate the ""generate scripts"" task in SQL Server Management Studio 2008?","
I'd like to automate the script generation in SQL Server Management Studio 2008.
Right now what I do is :

Right click on my database, Tasks, ""Generate Scripts...""
manually select all the export options I need, and hit select all on the ""select object"" tab
Select the export folder
Eventually hit the ""Finish"" button

Is there a way to automate this task?
Edit : I want to generate creation scripts, not change scripts.
",https://stackoverflow.com/questions/483568/how-can-i-automate-the-generate-scripts-task-in-sql-server-management-studio-2,auto
263,R command for setting working directory to source file location in Rstudio,"
I am working out some tutorials in R. Each R code is contained in a specific folder. There are data files and other files in there. I want to open the .r file and source it such that I do not have to change the working directory in Rstudio as shown below:

Is there a way to specify my working directory automatically in R.
",https://stackoverflow.com/questions/13672720/r-command-for-setting-working-directory-to-source-file-location-in-rstudio,auto
264,Automating telnet session using Bash scripts,"
I am working on automating some telnet related tasks, using Bash scripts.
Once automated, there will be no interaction of the user with telnet (that is, the script will be totally automated).
The scripts looks something like this:
# execute some commands on the local system
# access a remote system with an IP address: 10.1.1.1 (for example)

telnet 10.1.1.1

# execute some commands on the remote system
# log all the activity (in a file) on the local system
# exit telnet
# continue with executing the rest of the script

There are two problems I am facing here:

How to execute the commands on the remote system from the script (without human interaction)?
From my experience with some test code, I was able to deduce that when telnet 10.1.1.1 is executed, telnet goes into an interactive session and the subsequent lines of code in the script are executed on the local system. How can I run the lines of code on the remote system rather than on the local one?

I am unable to get a log file for the activity in the telnet session on the local system. The stdout redirect I used makes a copy on the remote system (I do not want to perform a copy operation to copy the log to the local system). How can I achieve this functionality?


",https://stackoverflow.com/questions/7013137/automating-telnet-session-using-bash-scripts,auto
265,Automate saveas dialogue for IE9 (vba),"
I am trying to download an excel sheet from a website. I have thus far achieved until clicking the download button automatically (web scraping). Now ie9 is popping a save as screen. How do i automate that?
",https://stackoverflow.com/questions/26038165/automate-saveas-dialogue-for-ie9-vba,auto
266,Dead code detection in legacy C/C++ project [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



How would you go about dead code detection in C/C++ code? I have a pretty large code base to work with and at least 10-15% is dead code. Is there any Unix based tool to identify this areas? Some pieces of code still use a lot of preprocessor, can automated process handle that?
",https://stackoverflow.com/questions/229069/dead-code-detection-in-legacy-c-c-project,auto
270,How to delete mysql row after time passes?,"
I have no idea where to start with this one:
I have a database that stores postID and Date.
What I want to do is have my website auto delete all rows where Date is less than today. This script can't have any user input at all.  No button clicks, nothing. The script must run every day at midnight.
I've been looking all over the place for something that does this and I've found absolutely nothing.
",https://stackoverflow.com/questions/9865393/how-to-delete-mysql-row-after-time-passes,auto
271,Why should I ever use CSS selectors as opposed to XPath for automated testing?,"
Please help me understand why using CSS selectors are even an option for automated testing.  I've been using the tool Ghost Inspector some in my workplace for creating lots of automated tests for our stuff.  This tool gives you the option of using CSS selectors intead of XPath. Why?
XPath is SO much more durable than CSS.  The CSS on any given UI is subject to change almost weekly on some projects/features.  This make the tests extremely brittle and prone to being broken regularly.
Is it because most new test writers don't want to learn about anything XPath and wish to stick to the basics? CSS selectors look prettier than XPath syntax? Please convince me. thanks.
",https://stackoverflow.com/questions/51936193/why-should-i-ever-use-css-selectors-as-opposed-to-xpath-for-automated-testing,auto
272,"VBA Internet Explorer Automation - How to Select ""Open"" When Downloading a File","
This is my first question ever here on stackoverflow!
I've been searching for a solution to this problem for a while and haven't found any help. I may just be using the wrong keywords in my searches, but so far I've had no luck. Here's the question:
In VBA, how can I select the ""Open"" option from the file download dialog in Internet Explorer?
Just for extra clarification, I'm talking about the yellow-orange bar that pops up across the bottom of the screen in IE9 when a file is downloaded.
I'm doing some VBA automation to download hundreds of PDFs from the web using Internet Explorer, but there is an intermediate step where a .fdf file has to be opened before I get to the actual PDF. So I first need to select the ""Open"" option so that I can move on to the next step of the automation. Like I said earlier, I've done a lot of searching and had no luck so far.
I've tried using SendKeys in hopes that hitting Enter would work, and that was a last ditch effort that didn't work.
Thanks in advance for the help!
",https://stackoverflow.com/questions/10400795/vba-internet-explorer-automation-how-to-select-open-when-downloading-a-file,auto
273,Selenium can't open a second page,"
I am using Selenium to open different pages of a site. Have tried multiple times but the browser does not open a second webpage after the initial GET call. Have tried on both Chrome and Safari. Here is my code:
driver = webdriver.Chrome()
driver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-2"")
driver.set_page_load_timeout(30)
driver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-3"")

Here is the error I get for the second call:

The info from Network logs is Error 504, but I have verified that it works perfectly when done on another window of the browser, without automation
",https://stackoverflow.com/questions/65994908/selenium-cant-open-a-second-page,auto
274,Upload file to SFTP using PowerShell,"
We were asked to set up an automated upload from one of our servers to an SFTP site. There will be a file that is exported from a database to a filer every Monday morning and they want the file to be uploaded to SFTP on Tuesday. The current authentication method we are using is username and password (I believe there was an option to have key file as well but username/password option was chosen).
The way I am envisioning this is to have a script sitting on a server that will be triggered by Windows Task scheduler to run at a specific time (Tuesday) that will grab the file in question upload it to the SFTP and then move it to a different location for backup purposes.
For example:

Local Directory: C:\FileDump
SFTP Directory: /Outbox/
Backup Directory: C:\Backup

I tried few things at this point WinSCP being one of them as well as SFTP PowerShell Snap-In but nothing has worked for me so far. 
This will be running on Windows Server 2012R2.
When I run Get-Host my console host version is 4.0.
Thanks.
",https://stackoverflow.com/questions/38732025/upload-file-to-sftp-using-powershell,auto
275,Using conditional statements inside 'expect',"
I need to automate logging into a TELNET session using expect, but I need to take care of multiple passwords for the same username.
Here's the flow I need to create:

Open TELNET session to an IP
Send user-name
Send password
Wrong password? Send the same user-name again, then a different password
Should have successfully logged-in at this point...

For what it's worth, here's what I've got so far:
#!/usr/bin/expect
spawn telnet 192.168.40.100
expect ""login:""
send ""spongebob\r""
expect ""password:""
send ""squarepants\r""
expect ""login incorrect"" {
  expect ""login:""
  send ""spongebob\r""
  expect ""password:""
  send ""rhombuspants\r""
}
expect ""prompt\>"" {
  send_user ""success!\r""
}
send ""blah...blah...blah\r""

Needless to say this doesn't work, and nor does it look very pretty. From my adventures with Google expect seems to be something of a dark-art. Thanks in advance to anyone for assistance in the matter!
",https://stackoverflow.com/questions/1538444/using-conditional-statements-inside-expect,auto
276,c# and excel automation - ending the running instance,"
I'm attempting Excel automation through C#. I have followed all the instructions from Microsoft on how to go about this, but I'm still struggling to discard the final reference(s) to Excel for it to close and to enable the GC to collect it.
A code sample follows. When I comment out the code block that contains lines similar to:
Sheet.Cells[iRowCount, 1] = data[""fullname""].ToString();

then the file saves and Excel quits. Otherwise the file saves but Excel is left running as a process. The next time this code runs it creates a new instance and they eventually build up.
Any help is appreciated. Thanks.
This is the barebones of my code:
        Excel.Application xl = null;
        Excel._Workbook wBook = null;
        Excel._Worksheet wSheet = null;
        Excel.Range range = null;

        object m_objOpt = System.Reflection.Missing.Value;

        try
        {
            // open the template
            xl = new Excel.Application();
            wBook = (Excel._Workbook)xl.Workbooks.Open(excelTemplatePath + _report.ExcelTemplate, false, false, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt);
            wSheet = (Excel._Worksheet)wBook.ActiveSheet;

            int iRowCount = 2;

            // enumerate and drop the values straight into the Excel file
            while (data.Read())
            {

                wSheet.Cells[iRowCount, 1] = data[""fullname""].ToString();
                wSheet.Cells[iRowCount, 2] = data[""brand""].ToString();
                wSheet.Cells[iRowCount, 3] = data[""agency""].ToString();
                wSheet.Cells[iRowCount, 4] = data[""advertiser""].ToString();
                wSheet.Cells[iRowCount, 5] = data[""product""].ToString();
                wSheet.Cells[iRowCount, 6] = data[""comment""].ToString();
                wSheet.Cells[iRowCount, 7] = data[""brief""].ToString();
                wSheet.Cells[iRowCount, 8] = data[""responseDate""].ToString();
                wSheet.Cells[iRowCount, 9] = data[""share""].ToString();
                wSheet.Cells[iRowCount, 10] = data[""status""].ToString();
                wSheet.Cells[iRowCount, 11] = data[""startDate""].ToString();
                wSheet.Cells[iRowCount, 12] = data[""value""].ToString();

                iRowCount++;
            }

            DirectoryInfo saveTo = Directory.CreateDirectory(excelTemplatePath + _report.FolderGuid.ToString() + ""\\"");
            _report.ReportLocation = saveTo.FullName + _report.ExcelTemplate;
            wBook.Close(true, _report.ReportLocation, m_objOpt);
            wBook = null;

        }
        catch (Exception ex)
        {
            LogException.HandleException(ex);
        }
        finally
        {
            NAR(wSheet);
            if (wBook != null)
                wBook.Close(false, m_objOpt, m_objOpt);
            NAR(wBook);
            xl.Quit();
            NAR(xl);
            GC.Collect();
        }

private void NAR(object o)
{
    try
    {
        System.Runtime.InteropServices.Marshal.ReleaseComObject(o);
    }
    catch { }
    finally
    {
        o = null;
    }
}


Update
No matter what I try, the 'clean' method or the 'ugly' method (see answers below), the excel instance still hangs around as soon as this line is hit:
wSheet.Cells[iRowCount, 1] = data[""fullname""].ToString();

If I comment that line out (and the other similar ones below it, obviously) the Excel app exits gracefully. As soon as one line per above is uncommented, Excel sticks around.
I think I'm going to have to check if there's a running instance prior to assigning the xl variable and hook into that instead. I forgot to mention that this is a windows service, but that shouldn't matter, should it?

",https://stackoverflow.com/questions/1041266/c-sharp-and-excel-automation-ending-the-running-instance,auto
277,"PsExec Throws Error Messages, but works without any problems","
So we are using PsExec a lot in our automations to install virtual machines, as we can't use ps remote sessions with our windows 2003 machines. Everything works great and there are no Problems, but PsExec keeps throwing errors, even every command is being carried out without correctly. 
For example:
D:\tools\pstools\psexec.exe $guestIP -u $global:default_user -p $global:default_pwd -d -i C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -command ""Enable-PSRemoting -Force""

Enables the PsRemoting on the guest, but also throws this error message:
psexec.exe : 
Bei D:\Scripts\VMware\VMware_Module5.ps1:489 Zeichen:29
+     D:\tools\pstools\psexec.exe <<<<  $guestIP -u $global:default_user -p $global:default_pwd -d -i C:\Windows\System32\WindowsPowerShell\
v1.0\powershell.exe -command ""Enable-PSRemoting -Force""
+ CategoryInfo          : NotSpecified: (:String) [], RemoteException
+ FullyQualifiedErrorId : NativeCommandError

PsExec v1.98 - Execute processes remotely
Copyright (C) 2001-2010 Mark Russinovich
Sysinternals - www.sysinternals.com


Connecting to 172.17.23.95...Starting PsExec service on 172.17.23.95...Connecting with PsExec service on 172.17.23.95...Starting C:\Windows\
System32\WindowsPowerShell\v1.0\powershell.exe on 172.17.23.95...
C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe started on 172.17.23.95 with process ID 2600.

These kinds of error messages apear ALWAYS no matter how i use psexec, like with quotes, with vriables/fixed values, other flags, etc. Does anybody has an idea how i could fix this? It is not a real problem, but it makes finding errors a pain in the ass, because the ""errors"" are everywhere. Disabling the error messages of psexec at all would also help...
",https://stackoverflow.com/questions/18380227/psexec-throws-error-messages-but-works-without-any-problems,auto
278,"IE9, Automation server can't create object error while using CertEnroll.dll","
In my web page, a JS block like this:
var classFactory = new ActiveXObject(""X509Enrollment.CX509EnrollmentWebClassFactory"");

// Other initialize CertEnroll Objects

It works fine in windows7(32bit or 64bit) with IE8(32bit), as long as I change the IE8 secure setting, enable Initializing and Script ActiveX controls not marked as safe.
But when use IE9(32bit), I try anything I can find on web, it reports error ""Automation server can't create object.""
I even created a static html file, save it in my hard disk, and then open it with IE9(32bit), it worked fine. Then I put the html file on my web site, visit the html file with url, then it came up with the error message again.
I have worked on this problem for 4 days, any suggestion would be appreciated.
3Q. I hope you can read my words as I'm not an native English speaker.
",https://stackoverflow.com/questions/15686040/ie9-automation-server-cant-create-object-error-while-using-certenroll-dll,auto
280,IE11 Frame Notification Bar Save button,"
On a 64-bit system with MS Excel 2010 and IE11 I'm using this code to automate download process from a website:  
hWnd = FindWindowEx(IE.hWnd, 0, ""Frame Notification Bar"", vbNullString)

If hWnd Then
    hWnd = FindWindowEx(hWnd, 0&, ""Button"", ""Save"")
End If

If hWnd Then
    SetForegroundWindow (hWnd)
    Sleep 600
    SendMessage hWnd, BM_CLICK, 0, 0
End If

Everything goes OK until the Frame Notification Bar appears. I'm getting the HWND of this window, but can't get the HWND of the ""Save"" button, so that I can send click to it.
",https://stackoverflow.com/questions/31489801/ie11-frame-notification-bar-save-button,auto
282,How can you automatically remove trailing whitespace in vim,"
I am getting 'trailing whitespace' errors trying to commit some files in Git.
I want to remove these trailing whitespace characters automatically right before I save Python files.
Can you configure Vim to do this? If so, how?
",https://stackoverflow.com/questions/356126/how-can-you-automatically-remove-trailing-whitespace-in-vim,auto
283,WatiN or Selenium? [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



I'm going to start coding some automated tests of our presentation soon. It seems that everyone recommends WatiN and Selenium. Which do you prefer for automated testing of ASP.NET web forms? Which of these products work better for you?
As a side note, I noticed that WatiN 2.0 has been in CTP since March 2008, is that something to be concerned about?
",https://stackoverflow.com/questions/417380/watin-or-selenium,auto
284,What is the best automated website UI testing framework [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 9 years ago.



What are the good automated web UI testing tools?
I want to be able to use it in the .Net world - but it doesn't have to written in .net.
Features such as a record mode, integration into build process\ continuous integration would be nice.
Im going to look at:

Watir
Selenium

Are there any others I should look at?
",https://stackoverflow.com/questions/805910/what-is-the-best-automated-website-ui-testing-framework,auto
285,What are the shortcut to Auto-generating toString Method in Eclipse?,"
Is it good or bad practice auto-generating toString methods for some simple classes?
I was thinking of generating something like below where it takes the variable names and produces a toString method that prints the name followed by its value.
private String name;
private int age;
private double height;

public String toString(){
   return String.format(""Name: %s Age: %d Height %f"", name, age, height);
}

",https://stackoverflow.com/questions/2653268/what-are-the-shortcut-to-auto-generating-tostring-method-in-eclipse,auto
286,How to add application to Azure AD programmatically?,"
I want to automate the creation of my application in Azure AD and get back the client id generated by Azure AD.
Are there PowerShell commandlets to do this? Is there some other means, like an API of doing this besides the management console?
Can you point me to an example?
Thanks!
",https://stackoverflow.com/questions/31684821/how-to-add-application-to-azure-ad-programmatically,auto
287,"Automation Google login with python and selenium shows """"This browser or app may be not secure""""","
I've tried login with Gmail or any Google services but it shows the following ""This browser or app may not be secure"" message:

I also tried to do options like enable less secure app in my acc but it didn't work.
then I made a new google account and it worked with me. but not with my old acc.

how can i solve this ? 
How can i open selenium in the normal chrome browser not the one controlled by automated software
?

This is my code

    from selenium.webdriver import Chrome
    from selenium.webdriver.chrome.options import Options


    browser = webdriver.Chrome()
    browser.get('https://accounts.google.com/servicelogin')
    search_form = browser.find_element_by_id(""identifierId"")
    search_form.send_keys('mygmail')
    nextButton = browser.find_elements_by_xpath('//*[@id =""identifierNext""]') 
    search_form.send_keys('password')
    nextButton[0].click() 

",https://stackoverflow.com/questions/66209119/automation-google-login-with-python-and-selenium-shows-this-browser-or-app-may,auto
288,Learning and Understanding the Xcode Build System,"
Alright, I'm curious about the build process with Xcode. Setting up multiple Targets, how to automate versioning and generally understanding the system so I can manipulate it to do what I want. 
Does anyone have any books or can point me to some documentation somewhere so that I can figure all of this out? 
Thanks a ton.
Another thing, if anyone actually sees this since changing it.
But any books anyone is aware of that will focus on Xcode 4? There's Xcode 3 Unleashed but I'd be real curious if there are any books that focus heavily on Xcode 4. 
",https://stackoverflow.com/questions/5490048/learning-and-understanding-the-xcode-build-system,auto
289,Create Outlook email draft using PowerShell,"
I'm creating a PowerShell script to automate a process at work.  This process requires an email to be filled in and sent to someone else.  The email will always roughly follow the same sort of template however it will probably never be the same every time so I want to create an email draft in Outlook and open the email window so the extra details can be filled in before sending.
I've done a bit of searching online but all I can find is some code to send email silently. The code is as follows:
$ol = New-Object -comObject Outlook.Application  
$mail = $ol.CreateItem(0)  
$Mail.Recipients.Add(""XXX@YYY.ZZZ"")  
$Mail.Subject = ""PS1 Script TestMail""  
$Mail.Body = ""  
Test Mail  
""  
$Mail.Send() 

In short, does anyone have any idea how to create and save a new Outlook email draft and immediately open that draft for editing?
",https://stackoverflow.com/questions/1453723/create-outlook-email-draft-using-powershell,auto
290,GMail is blocking login via Automation (Selenium),"
I am using selenium to automate a mail verification process in a web application. I have a script already in place to login to gmail and read an activation mail received on the account. The script was perfectly working till yesterday but today I am facing a problem.

Additional Screenshot of issue

Gmail is not allowing sign in if the browser is launched with selenium. Says, 

You're using a browser that Google doesn't recognize or that's setup in a way that we don't support.


I have tried upgrading chromedriver version to 76.0.0 as I am using
chrome version 76.0.3809.100(64 bit). (Previously used chromedriver
2.45) Still, the problem persists.
Verified that this issue occurs even if I use Firefox instead of Chrome for automation.
Verified that Javascript is enabled in the browser
Gmail is not asking for any OTP or recovery mail. It is simply
blocking my attempt to login via automation. However I am able to
login to the same account manually.


Software used:  ""webdriverio"": ""^4.14.1"", ""wdio-cucumber-framework"":
  ""^2.2.8""

Any help is appreciated.
",https://stackoverflow.com/questions/57602974/gmail-is-blocking-login-via-automation-selenium,auto
291,Can a Perl script install its own CPAN dependencies?,"
I have a Perl script that has two dependencies that exist in CPAN. What I'd like to do is have the script itself prompt the user to install the necessary dependencies so the script will run properly. If the user needs to enter in some kind of authentication to install the dependencies that's fine: what I'm trying to avoid is the following workflow:

Run script -> Watch it fail -> Scour CPAN aimlessly -> Lynch the script writer

Instead I'm hoping for something like:

Run script -> Auto-download script dependencies (authenticating as necessary) -> Script succeeds -> Buy the script writer a beer

Can this be done?
",https://stackoverflow.com/questions/7664829/can-a-perl-script-install-its-own-cpan-dependencies,auto
292,"UI Automation ""Selected text""","
Anyone knows how to get selected text from other application using UI Automation and .Net?
http://msdn.microsoft.com/en-us/library/ms745158.aspx
",https://stackoverflow.com/questions/517694/ui-automation-selected-text,auto
294,How to connect to existing instance of Excel from PowerShell?,"
All examples that automate Excel through PowerShell start with this line:
PS> $Excel = New-Object -Com Excel.Application

This seems to be handling a new instance of Excel, e.g. running $Excel.Visible = $true will show an empty, blank Excel window, not switch to the existing workbook.
If there is already an instance of Excel running, is there a way to connect to it?
",https://stackoverflow.com/questions/11081317/how-to-connect-to-existing-instance-of-excel-from-powershell,auto
295,reusing Internet Explorer COM Automation Object,"
I am using VBScript macros to utilize the InternetExplorer.Application COM automation object and I am struggling with reusing an existing instance of this object.
From what I have read, I should be able to use the GetObject() method in vbscript to grab a hold of an existing instance of this object.
When I execute the following code I get an ""Object creation failed - moniker syntax error"".
Is my issue really syntax? 
Is my issue how I am trying to use this object? 
or can what I am trying to accomplish just not be done?
Code:
Dim IEObject as object

Sub Main  
  Set IEObject =  GetObject( ""InternetExplorer.Application"" )

  'Set the window visable
  IEObject.Visible = True

  'Navigate to www.google.com
  IEObject.Navigate( ""www.google.com"" )
End Sub

Also, I have no problem running the CreateObject() which opens up a new internet explorer window and navigates where i want to, but i would rather not have the macro open up multiple instances of Internet Explorer.
",https://stackoverflow.com/questions/941767/reusing-internet-explorer-com-automation-object,auto
296,"Auto-reload browser when I save changes to html file, in Chrome?","
I'm editing an HTML file in Vim and I want the browser to refresh whenever the file underneath changes. 
Is there a plugin for Google Chrome that will listen for changes to the file and auto refresh the page every time I save a change to the file? I know there's XRefresh for Firefox but I could not get XRefresh to run at all.
How hard would it be to write a script to do this myself?
",https://stackoverflow.com/questions/5588658/auto-reload-browser-when-i-save-changes-to-html-file-in-chrome,auto
298,What's a good tool to screen-scrape with Javascript support? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



Is there a good test suite or tool set that can automate website navigation -- with Javascript support -- and collect the HTML from the pages?
Of course I can scrape straight HTML with BeautifulSoup.  But this does me no good for sites that require Javascript. :)
",https://stackoverflow.com/questions/125177/whats-a-good-tool-to-screen-scrape-with-javascript-support,auto
299,Using Python and Mechanize to submit form data and authenticate,"
I want to submit login to the website Reddit.com, navigate to a particular area of the page, and submit a comment.  I don't see what's wrong with this code, but it is not working in that no change is reflected on the Reddit site.
import mechanize
import cookielib


def main():

#Browser
br = mechanize.Browser()


# Cookie Jar
cj = cookielib.LWPCookieJar()
br.set_cookiejar(cj)

# Browser options
br.set_handle_equiv(True)
br.set_handle_gzip(True)
br.set_handle_redirect(True)
br.set_handle_referer(True)
br.set_handle_robots(False)

# Follows refresh 0 but not hangs on refresh > 0
br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)

#Opens the site to be navigated
r= br.open('http://www.reddit.com')
html = r.read()

# Select the second (index one) form
br.select_form(nr=1)

# User credentials
br.form['user'] = 'DUMMYUSERNAME'
br.form['passwd'] = 'DUMMYPASSWORD'

# Login
br.submit()

#Open up comment page
r= br.open('http://www.reddit.com/r/PoopSandwiches/comments/f47f8/testing/')
html = r.read()

#Text box is the 8th form on the page (which, I believe, is the text area)
br.select_form(nr=7)

#Change 'text' value to a testing string
br.form['text']= ""this is an automated test""

#Submit the information  
br.submit()

What's wrong with this?
",https://stackoverflow.com/questions/4720470/using-python-and-mechanize-to-submit-form-data-and-authenticate,auto
300,"Headless, scriptable Firefox/Webkit on linux? [closed]","






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I'm looking to automate some web interactions, namely periodic download of files from a secure website. This basically involves entering my username/password and navigating to the appropriate URL.
I tried simple scripting in Python, followed by more sophisticated scripting, only to discover this particular website is using some obnoxious javascript and flash based mechanism for login, rendering my methods useless. 
I then tried HTMLUnit, but that doesn't seem to want to work either. I suspect use of Flash is the issue.
I don't really want to think about it any more, so I'm leaning towards scripting an actual browser to log in and grab the file I need. 
Requirements are:

Run on linux server (ie. no X running). If I really need to have X I can make that happen, but I won't be happy.
Be reliable. I want to start this thing and never think about it again.
Be scriptable. Nothing too sophisticated, but I should be able to tell the browser the various steps to take and pages to visit.

Are there any good toolkits for a headless, X-less scriptable browser? Have you tried something like this and if so do you have any words of wisdom?
",https://stackoverflow.com/questions/2073481/headless-scriptable-firefox-webkit-on-linux,auto
301,Webbrowser behaviour issues,"
I am trying to automate Webbrowser with .NET C#. The issue is that the control or should I say IE browser behaves strange on different computers. For example, I am clickin on link and fillup a Ajax popup form on 1st computer like this, without any error:
private void btn_Start_Click(object sender, RoutedEventArgs e)
{
    webbrowserIE.Navigate(""http://www.test.com/"");
    webbrowserIE.DocumentCompleted += fillup_LoadCompleted; 
}

void fillup_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{
    System.Windows.Forms.HtmlElement ele = web_BrowserIE.Document.GetElementById(""login"");
    if (ele != null)
        ele.InvokeMember(""Click"");

    if (this.web_BrowserIE.ReadyState == System.Windows.Forms.WebBrowserReadyState.Complete)
    {
        web_BrowserIE.Document.GetElementById(""login"").SetAttribute(""value"", myUserName);
        web_BrowserIE.Document.GetElementById(""password"").SetAttribute(""value"", myPassword);

        foreach (System.Windows.Forms.HtmlElement el in web_BrowserIE.Document.GetElementsByTagName(""button""))
        {
            if (el.InnerText == ""Login"")
            {
                el.InvokeMember(""click"");
            }
        }

        web_BrowserIE.DocumentCompleted -= fillup_LoadCompleted;        
    }
}

However, the above code wont work on 2nd pc and the only way to click is like this:
private void btn_Start_Click(object sender, RoutedEventArgs e)
{
    webbrowserIE.DocumentCompleted += click_LoadCompleted;
    webbrowserIE.Navigate(""http://www.test.com/""); 
}

void click_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{
    if (this.webbrowserIE.ReadyState == System.Windows.Forms.WebBrowserReadyState.Complete)
    {
        System.Windows.Forms.HtmlElement ele = webbrowserIE.Document.GetElementById(""login"");
        if (ele != null)
            ele.InvokeMember(""Click"");

        webbrowserIE.DocumentCompleted -= click_LoadCompleted;
        webbrowserIE.DocumentCompleted += fillup_LoadCompleted;
    }
}

void click_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{

        webbrowserIE.Document.GetElementById(""login_login"").SetAttribute(""value"", myUserName);
        webbrowserIE.Document.GetElementById(""login_password"").SetAttribute(""value"", myPassword);

        //If you know the ID of the form you would like to submit:
        foreach (System.Windows.Forms.HtmlElement el in webbrowserIE.Document.GetElementsByTagName(""button""))
        {
            if (el.InnerText == ""Login"")
            {
                el.InvokeMember(""click"");
            }
        }

        webbrowserIE.DocumentCompleted -= click_LoadCompleted;      
}

So, in second solution I have to call two Load Completed Chains. Could someone advise on how should I can handle this issue? Also, a proposal for more robust approach would be very helpfull. Thank you in advance 
",https://stackoverflow.com/questions/18572635/webbrowser-behaviour-issues,auto
302,Evaluate javascript on a local html file (without browser),"
This is part of a project I am working on for work.
I want to automate a Sharepoint site, specifically to pull data out of a database that I and my coworkers only have front-end access to.
I FINALLY managed to get mechanize (in python) to accomplish this using Python-NTLM, and by patching part of it's source code to fix a reoccurring error.
Now, I am at what I would hope is my final roadblock: Part of the form I need to submit seems to be output of a JavaScript function :| and lo and behold... Mechanize does not support javascript. I don't want to emulate the javascript functionality myself in python because I would ideally like a reusable solution...
So, does anyone know how I could evaluate the javascript on the local html I download from sharepoint? I just want to run the javascript somehow (to complete the loading of the page), but without a browser.
I have already looked into selenium, but it's pretty slow for the amount of work I need to get done... I am currently looking into PyV8 to try and evaluate the javascript myself... but surely there must be an app or library (or anything) that can do this??
",https://stackoverflow.com/questions/16375251/evaluate-javascript-on-a-local-html-file-without-browser,auto
303,web scraping to fill out (and retrieve) search forms?,"
I was wondering if it is possible to ""automate"" the task of typing in entries to search forms and extracting matches from the results. For instance, I have a list of journal articles for which I would like to get DOI's (digital object identifier); manually for this I would go to the journal articles search page (e.g., http://pubs.acs.org/search/advanced), type in the authors/title/volume (etc.) and then find the article out of its list of returned results, and pick out the DOI and paste that into my reference list. I use R and Python for data analysis regularly (I was inspired by a post on RCurl) but don't know much about web protocols... is such a thing possible (for instance using something like Python's BeautifulSoup?). Are there any good references for doing anything remotely similar to this task? I'm just as much interested in learning about web scraping and tools for web scraping in general as much as getting this particular task done... Thanks for your time!
",https://stackoverflow.com/questions/1170120/web-scraping-to-fill-out-and-retrieve-search-forms,auto
304,Parse a .Net Page with Postbacks,"
I need to read data from an online database that's displayed using an aspx page from the UN. I've done HTML parsing before, but it was always by manipulating query-string values. In this case, the site uses asp.net postbacks. So, you click on a value in box one, then box two shows, click on a value in box 2 and click a button to get your results.
Does anybody know how I could automate that process? 
Thanks,
Mike
",https://stackoverflow.com/questions/1245782/parse-a-net-page-with-postbacks,auto
305,TypeError: can't use a string pattern on a bytes-like object in re.findall(),"
I am trying to learn how to automatically fetch urls from a page. In the following code I am trying to get the title of the webpage:
import urllib.request
import re

url = ""http://www.google.com""
regex = r'<title>(,+?)</title>'
pattern  = re.compile(regex)

with urllib.request.urlopen(url) as response:
   html = response.read()

title = re.findall(pattern, html)
print(title)

And I get this unexpected error:
Traceback (most recent call last):
  File ""path\to\file\Crawler.py"", line 11, in <module>
    title = re.findall(pattern, html)
  File ""C:\Python33\lib\re.py"", line 201, in findall
    return _compile(pattern, flags).findall(string)
TypeError: can't use a string pattern on a bytes-like object

What am I doing wrong?
",https://stackoverflow.com/questions/31019854/typeerror-cant-use-a-string-pattern-on-a-bytes-like-object-in-re-findall,auto
306,"Pulling data from a webpage, parsing it for specific pieces, and displaying it","
I've been using this site for a long time to find answers to my questions, but I wasn't able to find the answer on this one.
I am working with a small group on a class project. We're to build a small ""game trading"" website that allows people to register, put in a game they have they want to trade, and accept trades from others or request a trade.
We have the site functioning long ahead of schedule so we're trying to add more to the site. One thing I want to do myself is to link the games that are put in to Metacritic.
Here's what I need to do. I need to (using asp and c# in visual studio 2012) get the correct game page on metacritic, pull its data, parse it for specific parts, and then display the data on our page. 
Essentially when you choose a game you want to trade for we want a small div to display with the game's information and rating. I'm wanting to do it this way to learn more and get something out of this project I didn't have to start with. 
I was wondering if anyone could tell me where to start. I don't know how to pull data from a page. I'm still trying to figure out if I need to try and write something to automatically search for the game's title and find the page that way or if I can find some way to go straight to the game's page. And once I've gotten the data, I don't know how to pull the specific information I need from it.
One of the things that doesn't make this easy is that I'm learning c++ along with c# and asp so I keep getting my wires crossed. If someone could point me in the right direction it would be a big help. Thanks
",https://stackoverflow.com/questions/18065526/pulling-data-from-a-webpage-parsing-it-for-specific-pieces-and-displaying-it,auto
308,Fetch contents(loaded through AJAX call) of a web page,"
I am a beginner to crawling. I have a requirement to fetch the posts and comments from a link. I want to automate this process. I considered using webcrawler and jsoup for this but was told that webcrawlers are mostly used for websites with greater depth. 
Sample for a page: Jive community website
For this page, when I view the source of the page, I can see only the post and not the comments. Think this is because comments are fetched through an AJAX call to the server. 
Hence, when I use jsoup, it doesn't fetch the comments. 
So how can I automate the process of fetching posts and comments?
",https://stackoverflow.com/questions/20633294/fetch-contentsloaded-through-ajax-call-of-a-web-page,auto
310,HtmlAgilityPack HtmlWeb.Load returning empty Document,"
I have been using HtmlAgilityPack for the last 2 months in a Web Crawler Application with no issues loading a webpage.
Now when I try to load a this particular webpage, the document OuterHtml is empty, so this test fails
var url = ""http://www.prettygreen.com/"";
var htmlWeb = new HtmlWeb();
var htmlDoc = htmlWeb.Load(url);
var outerHtml = htmlDoc.DocumentNode.OuterHtml;
Assert.AreNotEqual("""", pageHtml);

I can load another page from the site with no problems, such as setting
url = ""http://www.prettygreen.com/news/"";

In the past I once had an issue with encodings, I played around with htmlWeb.OverrideEncoding and htmlWeb.AutoDetectEncoding with no luck.  I have no idea what could be the issue here with this webpage.
",https://stackoverflow.com/questions/13400493/htmlagilitypack-htmlweb-load-returning-empty-document,auto
312,How can I use EnumWindows to find windows with a specific caption/title?,"
I am working on an application that will eventually be an api for driving UI Tests for a WPF application.
At one point of the initial test we are working on, we get 2 Windows security popups.
We have some code that loops 10 times, it gets the handle of one of the popups using the FindWindowByCaption method and enters the information and clicks ok.
9 times out of 10 this works just fine, however we are occasionally seeing what looks to be a race condition. My suspicion is that the loop starts when only one of the windows is open and while its entering the information the second one opens and steals focus; after this it just hangs indefinitely.
What I'm wondering is if there is any method to get all of the window handles for a given caption, so that we can wait until there are 2 before starting the loop.
",https://stackoverflow.com/questions/19867402/how-can-i-use-enumwindows-to-find-windows-with-a-specific-caption-title,AI
314,Access a new window - cypress.io,"
The question is as simple as that. In Cypress, how can I access a new window that opens up when running the test.
Steps to recreate :


Run the test. After some action, new window pops up (the url is dynamic in nature).
Fill in the fields in the new window, and click a few buttons.
After required actions are completed in the new Window, close the new window and move back to the main window.
Continue execution with the main window.


Point of interest: the focus should be
main window -> new window -> main window

I have read few things that relate to use of iframe and confirmation box, but here its none of those. Relates to accessing a whole new window. Something like Window Handlers in Selenium. Unfortunately could not find anything related to it.
",https://stackoverflow.com/questions/47749956/access-a-new-window-cypress-io,AI
319,"Is Selenium slow, or is my code wrong?","
So I'm trying to login to Quora using Python and then scrape some stuff.
I'm using Selenium to login to the site. Here's my code:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys

driver = webdriver.Firefox()
driver.get('http://www.quora.com/')

username = driver.find_element_by_name('email')
password = driver.find_element_by_name('password')

username.send_keys('email')
password.send_keys('password')
password.send_keys(Keys.RETURN)

driver.close()

Now the questions:

It took ~4 minutes to find and fill the login form, which painfully slow. Is there something I can do to speed up the process?
When it did login, how do I make sure there were no errors? In other words, how do I check the response code?
How do I save cookies with selenium so I can continue scraping once I login?
If there is no way to make selenium faster, is there any other alternative for logging in? (Quora doesn't have an API)

",https://stackoverflow.com/questions/17462884/is-selenium-slow-or-is-my-code-wrong,AI
320,How to get webDriver to wait for page to load (C# Selenium project),"
I've started a Selenium project in C#. Trying to wait for page to finish loading up and only afterwards proceed to next action.
My code looks like this:
 loginPage.GoToLoginPage();
        loginPage.LoginAs(TestCase.Username, TestCase.Password);
        loginPage.SelectRole(TestCase.Orgunit);
        loginPage.AcceptRole();

inside loginPage.SelectRole(TestCase.Orgunit):
 RoleHierachyLabel = CommonsBasePage.Driver.FindElement(By.XPath(""//span[contains(text(), "" + role + "")]""));
 RoleHierachyLabel.Click();
 RoleLoginButton.Click();

I search for element RoleHierachyLabel. I've been trying to use multiple ways to wait for page to load or search for an element property allowing for some timeout:
1. _browserInstance.Manage().Timeouts().ImplicitlyWait(TimeSpan.FromSeconds(5));


2. public static bool WaitUntilElementIsPresent(RemoteWebDriver driver, By by, int timeout = 5)
    {
        for (var i = 0; i < timeout; i++)
        {
            if (driver.ElementExists(by)) return true;
        }
        return false;
    }

How would you tackle this obstacle?
",https://stackoverflow.com/questions/43203243/how-to-get-webdriver-to-wait-for-page-to-load-c-selenium-project,AI
321,Access element whose parent is hidden - cypress.io,"
The question is as given in the title, ie, to access element whose parent is hidden. The problem is that, as per the cypress.io docs  :

An element is considered hidden if:

Its width or height is 0.
Its CSS property (or ancestors) is visibility: hidden.
Its CSS property (or ancestors) is display: none.
Its CSS property is position: fixed and it芒鈧劉s offscreen or covered up.


But the code that I am working with requires me to click on an element whose parent is hidden, while the element itself is visible.
So each time I try to click on the element, it throws up an error reading :

CypressError: Timed out retrying: expected
  '< mdc-select-item#mdc-select-item-4.mdc-list-item>' to be 'visible'
This element '< mdc-select-item#mdc-select-item-4.mdc-list-item>' is
  not visible because its parent
  '< mdc-select-menu.mdc-simple-menu.mdc-select__menu>' has CSS property:
  'display: none'


The element I am working with is a dropdown item, which is written in pug. The element is a component defined in angular-mdc-web, which uses the mdc-select for the dropdown menu and mdc-select-item for its elements (items) which is what I have to access.
A sample code of similar structure :
//pug
mdc-select(placeholder=""installation type""
            '[closeOnScroll]'=""true"")
    mdc-select-item(value=""false"") ITEM1
    mdc-select-item(value=""true"") ITEM2

In the above, ITEM1 is the element I have to access. This I do in cypress.io as follows :
//cypress.io
// click on the dropdown menu to show the dropdown (items)
cy.get(""mdc-select"").contains(""installation type"").click();
// try to access ITEM1
cy.get('mdc-select-item').contains(""ITEM1"").should('be.visible').click();

Have tried with {force:true} to force the item click, but no luck. Have tried to select the items using {enter} keypress on the parent mdc-select, but again no luck as it throws : 

CypressError: cy.type() can only be called on textarea or :text. Your
  subject is a: < mdc-select-label
  class=""mdc-select__selected-text"">Select ...< /mdc-select-label>

Also tried using the select command, but its not possible because the Cypress engine is not able to identify the element as a select element (because its not, inner workings are different). It throws :

CypressError: cy.select() can only be called on a . Your
  subject is a: < mdc-select-label
  class=""mdc-select__selected-text"">Select ...< /mdc-select-label>

The problem is that the mdc-select-menu that is the parent for the mdc-select-item has a property of display:none by some internal computations upon opening of the drop-down items.

This property is overwritten to display:flex, but this does not help.

All out of ideas. This works in Selenium, but does not with cypress.io. Any clue what might be a possible hack for the situation other than shifting to other frameworks, or changing the UI code?
",https://stackoverflow.com/questions/47551639/access-element-whose-parent-is-hidden-cypress-io,AI
324,Run tests with Karate-Chrome (Connection refused exception),"
Karate UI-based tests run successfully locally with karate-chrome (steps here) which starts a container with an exposed port.
Now, I am trying to run the tests within the karate-chrome container in CI. I have started the karate-chrome container with KARATE_SOCAT_START=trueand then executed the java -jar (standalone jar) command to run the tests. Non-UI based tests pass but the UI tests are throwing the following exception:
ERROR com.intuit.karate - http request failed: 
15:26:09 DOCKER: org.apache.http.conn.HttpHostConnectException: Connect to localhost:9222 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused (Connection refused)

Note that driverTarget with docker has not been configured. Only the following driver's configuration:
  * configure driver = { type: 'chrome', start: false, showDriverLog: true, port:9222 ,pollAttempts: 5}

Is it possible to make it work this way or should a custom docker image be set up?
",https://stackoverflow.com/questions/66273843/run-tests-with-karate-chrome-connection-refused-exception,AI
325,"Chrome opens with ""Data;"" with selenium","
I am a newbie to Selenium and trying to open localhost:3000 page from Chrome via selenium driver. 
The code is : 
import com.google.common.base.Function;
import org.openqa.selenium.By;
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.WebDriverException;
import org.openqa.selenium.chrome.ChromeDriver;
import org.openqa.selenium.firefox.FirefoxDriver;
import org.openqa.selenium.firefox.FirefoxDriver;
public class SeleniumTests {

    public static void main(String[] args) {


        System.setProperty(""webdriver.chrome.driver"", ""C://chromedriver_win32//chromedriver.exe"");
        WebDriver driver = new ChromeDriver();              
        driver.get(""localhost:3000"");
    }

}

However, this opens my chrome window with a ""data;"" . 
The chrome version is 50.0.2661.94
Any idea what is the exact issue?
",https://stackoverflow.com/questions/37159684/chrome-opens-with-data-with-selenium,AI
330,Multiple Instances of Firefox during Selenium Webdriver Testing not handling focus correctly.,"
I have noticed that while running multiple selenium firefox tests in parallel on a grid that the focus event handling is not working correctly. I have confirmed that when each of my tests is run individually and given focus of the OS the tests pass 100% of the time. I have also run the tests in parallel on the grid with Chrome and not seen the issue present. 
I have found the following thread on google groups which suggests launching each browser in a separate instance of xvfb may be a viable solution. 
https://groups.google.com/forum/?fromgroups#!topic/selenium-developers/1cAmsYCp2ho%5B1-25%5D
The portion of the test is failing is due to a jquery date picker which is used in the project. The date picker launches on a focus event and since there are multiple selenium tests executing at the same time the webdriver test executes the .click() command but focus does not remain long enough for the date picker widget to appear. 
.focus(function(){ $input.trigger(""focus""); });

jQuery timepicker addon
By: Trent Richardson [http://trentrichardson.com]

My question is if anyone has seen this before and solved it through some firefox profile settings. I have tried loading the following property which had no affect on the issue. 
profile.setAlwaysLoadNoFocusLib(true);

The test fails in the same way as it did before with that property enabled and loaded in the Remote Driver Firefox Profile. 
I need a way ensure the focus event is triggered 100% of the time or to solve the issue of multiple firefox browsers competing for focus. Considering Chrome displays none of these issues I wonder if it may also be considered a bug in firefox.
Thanks! 
",https://stackoverflow.com/questions/11974538/multiple-instances-of-firefox-during-selenium-webdriver-testing-not-handling-foc,AI
332,WPF UI Automation issue,"
This Thread belong to this
I am asking where do I need to insert the workaround from this
I have a WPF application which has performance issue on some clients with Windows 7. On Windows XP all is working fast. The application has a MainShell and some Child-Windows. The MainShell hangs sometimes on some machines, and so do the child windows. Now, do I have to insert the workaround from the thread from the above link in all windows?
Are there still other workaround about this?
",https://stackoverflow.com/questions/6362367/wpf-ui-automation-issue,AI
333,Using javascript to set text for a web element - Selenium Web driver,"
How can text of a web element be set using java script executor? Or is there any other way to do this?
<a class=""selectBox selectBox-dropdown selectBox-menuShowing selectBox-active"" style=""width: 52px; display: inline-block; -moz-user-select: none;"" title="""" tabindex=""0"">
<span class=""selectBox-label"" style=""width: 13px;"">10</span>
<span class=""selectBox-arrow""/>
</a>

There are two span elements under the  tag - which is a drop down. User clicks on span[2] and a list is shown which contains data like 10, 20, 30, 40, etc.. User clicks on the number(element) and that is set as the text of span[1] (In this case, 10 is selected). How should I go about solving this?
I tried Action builder and it is not working. Any others suggestions?
",https://stackoverflow.com/questions/21695714/using-javascript-to-set-text-for-a-web-element-selenium-web-driver,AI
352,Android Espresso: Make assertion while button is kept pressed,"
I'm quite new to Espresso on Android and I am running into the following problem:
I want Espresso to perform a longclick(or something..) on a button, and while the button is kept pressed down, I want to check the state of a different View.
In (more or less) pseudocode:
onView(withId(button_id)).perform(pressButtonDown());
onView(withId(textBox_id)).check(matches(withText(""Button is pressed"")));
onView(withId(button_id)).perform(releaseButton());

I tried writing 2 custom Taps, PRESS and RELEASE, with MotionEvents.sendDown() and .sendUp(), but did not get it to work.
If this is the right path, I can post the code I've got so far.
Edit:
public class PressViewActions
{
    public static ViewAction pressDown(){
        return new GeneralClickAction(HoldTap.DOWN, GeneralLocation.CENTER, Press.THUMB);
    }

    public static ViewAction release() {
        return new GeneralClickAction(HoldTap.UP, GeneralLocation.CENTER, Press.THUMB);
    }
}

And the code for the Tappers:
public enum HoldTap implements Tapper {

    DOWN {
        @Override
        public Tapper.Status sendTap(UiController uiController, float[] coordinates, float[] precision)
        {
            checkNotNull(uiController);
            checkNotNull(coordinates);
            checkNotNull(precision);

            DownResultHolder res = MotionEvents.sendDown(uiController, coordinates, precision);

            ResultHolder.setController(uiController);
            ResultHolder.setResult(res);

            return Status.SUCCESS;
        }
    },

    UP{
        @Override
        public Tapper.Status sendTap(UiController uiController, float[] coordinates, float[] precision)
        {
            DownResultHolder res = ResultHolder.getResult();
            UiController controller = ResultHolder.getController();

            try {
                if(!MotionEvents.sendUp(controller, res.down))
                {
                    MotionEvents.sendCancel(controller, res.down);
                    return Status.FAILURE;
                }

            }
            finally {
                //res.down.recycle();
            }

            return Status.SUCCESS;
        }
    }
}

The error I get is the following:
android.support.test.espresso.PerformException: Error performing 'up click' on view 'with id: de.test.app:id/key_ptt'.
...
...
Caused by: java.lang.RuntimeException: Couldn't click at: 281.5,1117.5 precision: 25.0, 25.0 . Tapper: UP coordinate provider: CENTER precision describer: THUMB. Tried 3 times. With Rollback? false

I hope, this helps.
Any help and ideas will be appreciated!
Tanks a lot in advance!
",https://stackoverflow.com/questions/32010927/android-espresso-make-assertion-while-button-is-kept-pressed,AI
353,Filling web form via PowerShell does not recognize the values entered,"
Working as a QA I need to fill in a lot of applications through a web form.
Idea is to have the personal data in some xls/txt/whatever file, read the file and use Powershell to feed data to the browser.
When I use the code below to fill in the form in IE, even though it seems to work fine, I get an error when submitting the form that no data was entered.
Any ideas or suggestions how to get past this would be much appreciated
Sadly my resources are limited to Powershell 2.0. Selenium or any other ""more sophisticated"" tools are out of question at least for now.
validation error here
$ie = New-Object -com InternetExplorer.Application
$ie.Navigate(""MyURL"")
$ie.visible = $true

while ($ie.ReadyState -ne 4){sleep -m 100}

Function ClickById($id) {
    $ie.document.getElementById($id).Click()
}

### Z谩kladn铆 煤daje
$FnId = 'personalData.firstName'
$LnId = 'personalData.lastName'
$PhoneId = 'personalData.mobilePhone'
$EmailId = 'personalData.email'
$DataAgreementCheckBox = 'application.personalDataAgreement'
$SubmitfwdId = 'forward'


$Values = ""Ublala"", ""Pung"", ""222333444"", ""ublala@pung.com""
$Ds1Elements = $FnId, $LnId, $PhoneId, $EmailId

$j = 0
foreach ($El in $Ds1Elements) {
    $ie.document.getElementById($El).value = $values[$j]
    $j++
}

ClickById $DataAgreementCheckBox
ClickById $SubmitfwdId

",https://stackoverflow.com/questions/33216366/filling-web-form-via-powershell-does-not-recognize-the-values-entered,AI
357,How to convert Katalon Script in Selenium Java?,"
As we know Katalon has now become a paid tool so my Katalon scripts need to be converted into Selenium and Java script.
Katalon scripts are in Groovy, and it's written using Katalon Built-in libraries, objects are saved in .rs(.xml) fie on Object repository and user-defined Keywords are also in Groovy.
So please suggest the best way(time-saving) to convert scripts into selenium.
",https://stackoverflow.com/questions/58836655/how-to-convert-katalon-script-in-selenium-java,AI
366,Performing wildcard operations on table containing duplicate elements in karate? [duplicate],"






This question already has an answer here:
                        
                    



Karate UI: How to click a specific checkbox with same class name

                                (1 answer)
                            

Closed 1 year ago.



I am stuck with a case where the need is to click on an icon after asserting inputs from the user. In case there were some unique identifiers, the thing was pretty simple like the use of:  rightOf('{}UniqueIdentifier').find('i').click() served the purpose.
Also working fine with: scroll('{}UniqueIdentifier').parent.children[4].click()
But in case the table contains repeated values nothing could be found unique to search for and click. For which the thought was to match entire row text where the last element is that icon which needs to be clicked OR any other method which suits this?
Table looks like this:-

Need to click on triple dot icon for- A2,P2,2,resolved. How can this be achieved using wildcard locators? I tried creating a list of elements and match it with user input list but failed doing so.
Any help would be appreciated. Thanks!
",https://stackoverflow.com/questions/66898591/performing-wildcard-operations-on-table-containing-duplicate-elements-in-karate,AI
371,Execute Coded UI Tests in multiple environments,"
Right now my Coded UI Tests use their app.config to determine the domain they execute in, which has a 1-1 relationship with environment. To simplify it:

www.test.com
www.UAT.com
www.prod.com

and in App.config I have something like:
<configuration>
    <appSettings>
        <add key=""EnvironmentURLMod"" value =""test""/>

and to run the test in a different environment, I manually change the value between runs. For instance the I open the browser like this:
browserWindow.NavigateToUrl(new Uri(""http://www.""
                + ConfigurationManager.AppSettings.Get(""EnvironmentURLMod"")
                + "".com""));

Clearly this is inelegant. I suppose I had a vision where we'd drop in a new app.config for each run, but as a spoiler this test will be run in ~10 environments, not 3, and which environments it may run may change.
I know I could decouple these environment URL modifications to yet another XML file, and make the tests access them sequentially in a data-driven scenario. But even this seems like it's not quite what I need, since if one environment fails then the whole test collapses. I've seen Environment Variables as a suggestion, but this would require creating a test agent for each environment, modifying their registries, and running the tests on each of them. If that's what it takes then sure, but it seems like an enormous amount of VM bandwidth to be used for what's a collection of strings.
In an ideal world, I would like to tie these URL mods to something like Test Settings, MTM environments, or builds. I want to execute the suite of tests for each domain and report separately.
In short, what's the best way to parameterize these tests? Is there a way that doesn't involve queuing new builds, or dropping config files? Is Data Driven Testing the answer? Have I structured my solution incorrectly? This seems like it should be such a common scenario, yet my googling doesn't quite get me there.
Any and all help appreciated.
",https://stackoverflow.com/questions/33239523/execute-coded-ui-tests-in-multiple-environments,AI
373,element not interactable exception in selenium web automation,"
In the below code i cannot send password keys in the password field, i tried clicking the field, clearing the field and sending the keys. But now working in any of the method. But its working if i debug and test
  public class TestMail {
   protected static WebDriver driver;

   protected static String result;

   @BeforeClass

   public static void setup()  {
              System.setProperty(""webdriver.gecko.driver"",""D:\\geckodriver.exe"");

   driver = new FirefoxDriver();

   driver.manage().timeouts().implicitlyWait(60, TimeUnit.SECONDS);

  }

   @Test

 void Testcase1() {

   driver.get(""http://mail.google.com"");

   WebElement loginfield = driver.findElement(By.name(""Email""));
   if(loginfield.isDisplayed()){
       loginfield.sendKeys(""ragesh@gmail.in"");
   }
   else{
  WebElement newloginfield = driver.findElemnt(By.cssSelector(""#identifierId""));                                      
       newloginfield.sendKeys(""ragesh@gmail.in"");
      // System.out.println(""This is new login"");
   }


    driver.findElement(By.name(""signIn"")).click();

  // driver.findElement(By.cssSelector("".RveJvd"")).click();

   driver.manage().timeouts().implicitlyWait(15, TimeUnit.SECONDS);
 // WebElement pwd = driver.findElement(By.name(""Passwd""));
  WebElement pwd = driver.findElement(By.cssSelector(""#Passwd""));

  pwd.click();
  pwd.clear();
 // pwd.sendKeys(""123"");
 if(pwd.isEnabled()){
     pwd.sendKeys(""123"");
 }
 else{
     System.out.println(""Not Enabled"");
 }

",https://stackoverflow.com/questions/45183797/element-not-interactable-exception-in-selenium-web-automation,AI
376,puppeteer wait for page/DOM updates - respond to new items that are added after initial loading,"
I want to use Puppeteer to respond to page updates.
The page shows items and when I leave the page open new items can appear over time.
E.g. every 10 seconds a new item is added.
I can use the following to wait for an item on the initial load of the page:
await page.waitFor("".item"");
console.log(""the initial items have been loaded"")

How can I wait for / catch future items?
I would like to achieve something like this (pseudo code):
await page.goto('http://mysite');
await page.waitFor("".item"");
// check items (=these initial items)

// event when receiving new items:
// check item(s) (= the additional [or all] items)

",https://stackoverflow.com/questions/54109078/puppeteer-wait-for-page-dom-updates-respond-to-new-items-that-are-added-after,AI
380,How to login to a website with python and mechanize,"
i'm trying to log in to the website http://www.magickartenmarkt.de and do some analyzing in the member-area (https://www.magickartenmarkt.de/?mainPage=showWants). I saw other examples for this, but i don't get why my approaches didn't work. I identified the right forms for the first approach, but it's not clear if it worked.
In the second approach the returing webpage shows me that i don't have access to the member area. 
I would by glad for any help.
import urllib2
import cookielib
import urllib
import requests
import mechanize
from mechanize._opener import urlopen
from mechanize._form import ParseResponse

USERNAME = 'Test'
PASSWORD = 'bla123'
URL      = ""http://www.magickartenmarkt.de""

# first approach
request = mechanize.Request(URL)
response = mechanize.urlopen(request)
forms = mechanize.ParseResponse(response, backwards_compat=False)
# I don't want to close?!
#response.close()

# Username and Password are stored in this form
form = forms[1]

form[""username""] = USERNAME
form[""userPassword""] = PASSWORD

#proof entering data has worked
user = form[""username""]  # a string, NOT a Control instance
print user
pw = form[""userPassword""]  # a string, NOT a Control instance
print pw
#is this the page where I will redirected after login?
print urlopen(form.click()).read () 

#second approach
cj = cookielib.CookieJar()
opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
login_data = urllib.urlencode({'username' : USERNAME, 'userPassword': PASSWORD})

#login
response_web = opener.open(URL, login_data)

#did it work? for me not....
resp = opener.open('https://www.magickartenmarkt.de/?mainPage=showWants')
print resp.read()

",https://stackoverflow.com/questions/16598145/how-to-login-to-a-website-with-python-and-mechanize,AI
381,Click a checkbox with selenium-webdriver,"
I'm testing my app with tumblr and I have to log in and out as I go through procedures. While doing so, I'm having trouble clicking a checkbox that keeps popping up. How can I use selenium-webriver in python to click it?
I've tried selecting xpaths, ...by_ids, and by_classes, they won't work, so now I'm trying to use the mouse's coordinates to physically click the item. (This is on the tumblr login page, fyi)
 
Above is the html of the item I'm trying to select.
(EDIT:)
I've the following selectors:
#checkbox = driver.find_element_by_id(""recaptcha-anchor"")
#checkbox = driver.find_element_by_id(""g-recaptcha"") 
#driver.find_element_by_xpath(""//*[@id='recaptcha-token']"")
#driver.find_element_by_css_selector(""#recaptcha-anchor"")
#driver.find_element_by_xpath(""//*[@id='recaptcha-anchor']"")
#driver.find_element_by_id(""recaptcha-token"").click()
#driver.find_element_by_class_name('rc-anchor-center-container')
#checkbox = driver.find_element_by_id(""recaptcha-anchor"")

",https://stackoverflow.com/questions/32446151/click-a-checkbox-with-selenium-webdriver,AI
383,How to find_element_by_link_text while having: NoSuchElement Exception?,"
This question has been asked over and over again - and in-spite of trying all the hacks I still can't seem to figure out what's wrong.
I tried increasing the implicitly_wait to 30 (and even increased it upto 100) - yet it did not work. 
Use case -: I am trying to create a list that wil populate all the items in the page here, as a base case - and I intend to bind this to a mini-module that I already have with scrapy which has all (pages with similar web elements) crawled links - so essentially will be building the whole pipeline, post I am done with this.
###My source code - generated via Selenium IDE, exported to a Python webdriver and manipulated a little later ###

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.support.wait import WebDriverWait
import unittest, time, re

class Einstein(unittest.TestCase):
    def setUp(self):
        self.driver = webdriver.Firefox()
        self.driver.implicitly_wait(30)
        self.base_url = ""http://shopap.lenovo.com/in/en/laptops/""
        self.verificationErrors = []
        self.accept_next_alert = True

    def test_einstein(self):
        driver = self.driver
        driver.get(self.base_url)
        print driver.title
        driver.find_element_by_link_text(""T430"").click()
        print driver.title
#       driver.find_element_by_xpath(""id('facetedBrowseWrapper')/div/div/div[1]/div[2]/ul[1]/li[1]/a"").click()
        driver.find_element_by_xpath(""//div[@id='subseries']/div[2]/div/p[3]/a"").click()
        print driver.title
       # driver.find_element_by_xpath(""//div[@id='subseries']/div[2]/div/p[3]/a"").click()
        try: self.assertEqual(""Thinkpad Edge E530 (Black)"", driver.find_element_by_link_text(""Thinkpad Edge E530 (Black)"").text)
        except AssertionError as e: self.verificationErrors.append(str(e))
       # Everything ok till here


        #**THE CODE FAILS HERE**#
        laptop1 = driver.find_element_by_link_text(""Thinkpad Edge E530 (Black)"").text
        print laptop1
        price1 = driver.find_element_by_css_selector(""span.price"").text
        print price1
        detail1 = self.is_element_present(By.CSS_SELECTOR, ""div.desc.std"")
        print detail1

            def is_element_present(self, how, what):
        try: self.driver.find_element(by=how, value=what)
        except NoSuchElementException, e: return False
        return True

    def is_alert_present(self):
        try: self.driver.switch_to_alert()
        except NoAlertPresentException, e: return False
        return True

    def close_alert_and_get_its_text(self):
        try:
            alert = self.driver.switch_to_alert()
            alert_text = alert.text
            if self.accept_next_alert:
                alert.accept()
            else:
                alert.dismiss()
            return alert_text
        finally: self.accept_next_alert = True

    def tearDown(self):
        self.driver.quit()
        self.assertEqual([], self.verificationErrors)

if __name__ == ""__main__"":
    unittest.main()


Errors & output :
ekta@ekta-VirtualBox:~$ python einstein.py
Laptops & Ultrabooks | Lenovo (IN)
ThinkPad T430 Laptop PC for Business Computing | Lenovo (IN)
Buy Lenovo Thinkpad Laptops | Lenovo Thinkpad Laptops Price India
E
======================================================================
ERROR: test_einstein (__main__.Einstein)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""einstein.py"", line 27, in test_einstein
    try: self.assertEqual(""Thinkpad Edge E530 (Black)"", driver.find_element_by_link_text(""Thinkpad Edge E530 (Black)"").text)
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/webdriver.py"", line 246, in find_element_by_link_text
    return self.find_element(by=By.LINK_TEXT, value=link_text)
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/webdriver.py"", line 680, in find_element
    {'using': by, 'value': value})['value']
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/webdriver.py"", line 165, in execute
    self.error_handler.check_response(response)
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/errorhandler.py"", line 158, in check_response
    raise exception_class(message, screen, stacktrace)
NoSuchElementException: Message: u'Unable to locate element: {""method"":""link text"",""selector"":""Thinkpad Edge E530 (Black)""}' ; Stacktrace: 
    at FirefoxDriver.prototype.findElementInternal_ (file:///tmp/tmphli5Jg/extensions/fxdriver@googlecode.com/components/driver_component.js:8444)
    at fxdriver.Timer.prototype.setTimeout/<.notify (file:///tmp/tmphli5Jg/extensions/fxdriver@googlecode.com/components/driver_component.js:386) 

----------------------------------------------------------------------
Ran 1 test in 79.348s

FAILED (errors=1)

Questions & comments: 

If you are answering this question - please mention why this specific ""find_element_by_link_text"" does not work. 
(Very Basic) In the GUI of my selenium IDE -> Show all available commands - why dont I see the css (find_element_by_css_selector) for all the web elements - is there a way to force feed an element to be read as a CSS selector ?
In case you suggest using some other locator - please mention if that will be consistent way to fetch elements, given (1) 
Does assert work to capture the exceptions and ""move on"" - since even after trying ""verify"" , ""assert"" loops, I still cant fetch this  ""find_element_by_link_text""
I tried using Xpath to build this ""element"" , but in the view Xpath (in firefox) - I see nothing, to clue why that happens (Of course I removed the namespace "":x"" )

Other things I tried apart from implicity_wait(30):
find_element_by_partial_link(鈥淭hinkpad鈥? and appending Unicode to this (wasn鈥檛 sure if it was reading the brackets ( , driver.find_element_by_link_text(u""Thinkpad Edge E530 (Black)"").text, still did not work.


Related questions:

How to use find_element_by_link_text() properly to not raise NoSuchElementException?
NoSuchElement Exception using find_element_by_link_text when implicitly_wait doesn't work?

",https://stackoverflow.com/questions/18023678/how-to-find-element-by-link-text-while-having-nosuchelement-exception,AI
386,How to get body / json response from XHR request with Puppeteer [duplicate],"






This question already has answers here:
                        
                    



Puppeteer: How to listen to a specific response?

                                (5 answers)
                            

Closed 4 months ago.



I want to get the JSON data from a website I'm scraping with Puppeteer, but I can't figure how to get the body of the request back. Here's what I've tried:
const puppeteer = require('puppeteer')
const results = [];
(async () => {
    const browser = await puppeteer.launch({
        headless: false
    })
    const page = await browser.newPage()
    await page.goto(""https://capuk.org/i-want-help/courses/cap-money-course/introduction"", {
        waitUntil: 'networkidle2'
    });

    await page.type('#search-form > input[type=""text""]', 'bd14ew')  
    await page.click('#search-form > input[type=""submit""]')

    await page.on('response', response => {    
        if (response.url() == ""https://capuk.org/ajax_search/capmoneycourses""){
            console.log('XHR response received'); 
            console.log(response.json()); 
        } 
    }); 
})()

This just returns a promise pending function. Any help would be great.
",https://stackoverflow.com/questions/56689420/how-to-get-body-json-response-from-xhr-request-with-puppeteer,AI
387,How to get children of elements by Puppeteer,"
I understand that puppeteer get its own handles rather than standard DOM elements, but I don't understand why I cannot continue the same query by found elements as
const els = await page.$$('div.parent');

for (let i = 0; i < els.length; i++) {
    const img = await els[i].$('img').getAttribute('src');
    console.log(img);
    const link = await els[i].$('a').getAttribute('href');
    console.log(link);
}

",https://stackoverflow.com/questions/55659097/how-to-get-children-of-elements-by-puppeteer,AI
389,How to open the new tab using Playwright (ex. click the button to open the new section in a new tab),"
I am looking for a simpler solution to a current situation. For example, you open the google (any another website) and you want BY CLICK on the button (ex. Gmail) - open this page in the new tab using Playwright.
let browser, page, context;
describe('Check the main page view', function () {
    before(async () => {
        for (const browserType of ['chromium']) {
            browser = await playwright[browserType].launch({headless: false});
            context = await browser.newContext();
            page = await context.newPage();
            await page.goto(baseUrl);
        }
    });
    after(async function () {
        browser.close();
    });
    
        await page.click(tax);
        const taxPage = await page.getAttribute(taxAccount, 'href');

        const [newPage] = await Promise.all([
        context.waitForEvent('page'),
        page.evaluate((taxPage) => window.open(taxPage, '_blank'), taxPage)]);

        await newPage.waitForLoadState();
        console.log(await newPage.title());

",https://stackoverflow.com/questions/64277178/how-to-open-the-new-tab-using-playwright-ex-click-the-button-to-open-the-new-s,AI
390,How to scroll down in an instagram pop-up frame with Selenium,"
I have a python script using selenium to go to a given Instagram profile and iterate over the user's followers. On the instagram website when one clicks to see the list of followers, a pop-up opens with the accounts listed (here's a screenshot of the site)
However both visually and in the html, only 12 accounts are shown. In order to see more one has to scroll down, so I tried doing this with the Keys.PAGE_DOWN input.
from selenium import webdriver
from selenium.common.exceptions         import TimeoutException
from selenium.webdriver.support.ui      import WebDriverWait 
from selenium.webdriver.support         import expected_conditions as EC
from selenium.webdriver.chrome.options  import Options
from selenium.webdriver.common.keys     import Keys
import time 

...
username = 'Username'
password = 'Password'
message  = 'blahblah'
tryTime  = 2

#create driver and log in
driver = webdriver.Chrome()
logIn(driver, username, password, tryTime)

#gets rid of preference pop-up
a = driver.find_elements_by_class_name(""HoLwm"")
a[0].click()

#go to profile
driver.get(""https://www.instagram.com/{}/"".format(username))

#go to followers list
followers = driver.find_element_by_xpath(""//a[@href='/{}/followers/']"".format(username))
followers.click()
time.sleep(tryTime) 

#find all li elements in list
fBody  = driver.find_element_by_xpath(""//div[@role='dialog']"")
fBody.send_keys(Keys.PAGE_DOWN) 

fList  = fBody.find_elements_by_tag(""li"")
print(""fList len is {}"".format(len(fList)))

time.sleep(tryTime)

print(""ended"")
driver.quit()

When I try to run this I get the following error:
Message: unknown error: cannot focus element

I know this is probably because I'm using the wrong element for fBody, but I don't know which would be the right one. Does anybody know which element I should send the PAGE_DOWN key to, or if there is another way to load  the accounts? 
Any help is much appreciated!
",https://stackoverflow.com/questions/54173603/how-to-scroll-down-in-an-instagram-pop-up-frame-with-selenium,AI
392,Why is switch_to_window() method not working for selenium webdriver in Python?,"
I am trying to switch to a newly opened window using the Python selenium webdriver. The code worked fine before but now it is showing error. Surprisingly, the switch_to_window() method is not being recognized by Python and has no declaration to go to.
def process_ebl_statements(self, account_number):

    current_window = self.driver.current_window_handle
    all_windows = self.driver.window_handles

    print(""Current window: "", current_window)
    print(""All windows: "", all_windows)
    number_of_windows = len(all_windows)
    self.driver.switch_to_window(all_windows[number_of_windows - 1])

Error details:
'WebDriver' object has no attribute 'switch_to_window'


",https://stackoverflow.com/questions/70360072/why-is-switch-to-window-method-not-working-for-selenium-webdriver-in-python,AI
394,Selenium 3.0.1 -interactive gives ParameterException: Unknown option: -interactive,"
How to run selenum standanlone jar in interactive mode so that we can trigger commands from terminal. When I run as shown below exception occurs
java -jar selenium-server-standalone-3.0.1.jar -interactive
Exception in thread ""main"" com.beust.jcommander.ParameterException: Unknown option: -interactive
    at com.beust.jcommander.JCommander.parseValues(JCommander.java:742)
    at com.beust.jcommander.JCommander.parse(JCommander.java:282)
    at com.beust.jcommander.JCommander.parse(JCommander.java:265)
    at com.beust.jcommander.JCommander.<init>(JCommander.java:210)
    at org.openqa.grid.selenium.GridLauncherV3$1.setConfiguration(GridLauncherV3.java:219)
    at org.openqa.grid.selenium.GridLauncherV3.buildLauncher(GridLauncherV3.java:147)
    at org.openqa.grid.selenium.GridLauncherV3.main(GridLauncherV3.java:73)

However I am able to run version 2.48.2 in the above manner and it works fine.
",https://stackoverflow.com/questions/41275503/selenium-3-0-1-interactive-gives-parameterexception-unknown-option-interacti,AI
395,how to use edgeDriver with qaf,"
I use below properties to run edge browser, but it doesn't work.
webdriver.edge.driver=src/main/resources/common/msedgedriver.exe
driver.name=edgeDriver

How can I use edge browser?
",https://stackoverflow.com/questions/62354345/how-to-use-edgedriver-with-qaf,AI
398,Unable to capture response.json() in playwright,"
I am trying to capture json response using playwright. I keep on getting Promise pending. However under headless:false mode i can see the data is being received and populated on the browser. I have just started playing with Playwright and also not very familiar with ""Promise"".
What i have tried is as below:
(async () => {
        let browser = await firefox.launch({headless: true, userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0'});
        let page = await browser.newPage();
        page.waitForResponse(async(response) => {
            if (response.url().includes('/abcd') && response.status() == 200) {
                let resp = await response.json();
                console.log(resp);
            }
        });
        await page.goto('https://myurl.com', {waitUntil: 'networkidle', timeout: 30000});
        await page.waitForTimeout(20000);
        await browser.close();
})

What am i doing wrong? I have tried increasing timeout. Doesnot help.
",https://stackoverflow.com/questions/67019344/unable-to-capture-response-json-in-playwright,AI
400,StaleElement exception error when asserting data in a table,"
I am trying to add data to a table and then asserting that data is added by collecting table data in a list but every time it throws me a StaleElement exception error, now I guess it is happening because the list is getting refreshed, so I am not sure how do I handle it.
Here is my implementation
    private static List<WebElement> listOfJobs = 
    driver.findElements(By.xpath((""//*[@id='resultTable']//tbody/tr//a"")));

    public static List<WebElement> getListOfJobs() 
        {
            try 
            {
                return listOfJobs;
            } 
            catch (Exception e) 
            {
                e.printStackTrace();
            }
            return null;
        }

    public static String generateName()
        {
            String AlphaNumericString = ""abcdefghijklmnopqrstuvxyz"";
            StringBuilder sb = new StringBuilder(9);
    
            for (int i = 0; i < 9; i++) 
            {
                int index = (int)(AlphaNumericString.length() * Math.random());
                sb.append(AlphaNumericString.charAt(index));
            }
            return sb.toString()+""digi"";
        }
    
    @SuppressWarnings({ ""null"" })
        public static List<String> listOfJobs()
        {
            List<String> jobs = null;
            for(int i=0; i < OrangeHRMAddJobCategories.getListOfJobs().size(); i++)
            {
                jobs.add(OrangeHRMAddJobCategories.getListOfJobs().get(i).getText());
            }
            return jobs;
        }

    OrangeHRMAddJobCategories jobCategories = new OrangeHRMAddJobCategories();
            jobCategories.clickJobTab().clickJobCategoires().clickAdd().setJobCategoryName(UsefulFunctionUtils.generateName()).saveJobCategory();   

    Assertions.assertThat(UsefulFunctionUtils.listOfJobs().contains(""digi""));

I feel that the listOfJobs should be reinjected somewhere but not sure where exactly do I put it because refreshing the page did not work.
Complete stacktrace

    org.openqa.selenium.StaleElementReferenceException: stale element reference: element is not attached to the page document
      (Session info: chrome=94.0.4606.61)
    For documentation on this error, please visit: https://www.seleniumhq.org/exceptions/stale_element_reference.html
    Build info: version: '3.141.59', revision: 'e82be7d358', time: '2018-11-14T08:17:03'
    System info: host: 'DESKTOP-R3JT7MO', ip: '192.168.0.103', os.name: 'Windows 10', os.arch: 'amd64', os.version: '10.0', java.version: '16.0.1'
    Driver info: org.openqa.selenium.chrome.ChromeDriver
    Capabilities {acceptInsecureCerts: false, browserName: chrome, browserVersion: 94.0.4606.61, chrome: {chromedriverVersion: 94.0.4606.61 (418b78f5838ed..., userDataDir: C:\Users\CHINMA~1\AppData\L...}, goog:chromeOptions: {debuggerAddress: localhost:62489}, javascriptEnabled: true, networkConnectionEnabled: false, pageLoadStrategy: normal, platform: WINDOWS, platformName: WINDOWS, proxy: Proxy(), setWindowRect: true, strictFileInteractability: false, timeouts: {implicit: 0, pageLoad: 300000, script: 30000}, unhandledPromptBehavior: dismiss and notify, webauthn:extension:credBlob: true, webauthn:extension:largeBlob: true, webauthn:virtualAuthenticators: true}
    Session ID: 8b68238ee73ae8190f250fa15fbb41f1
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:78)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
        at org.openqa.selenium.remote.http.W3CHttpResponseCodec.createException(W3CHttpResponseCodec.java:187)
        at org.openqa.selenium.remote.http.W3CHttpResponseCodec.decode(W3CHttpResponseCodec.java:122)
        at org.openqa.selenium.remote.http.W3CHttpResponseCodec.decode(W3CHttpResponseCodec.java:49)
        at org.openqa.selenium.remote.HttpCommandExecutor.execute(HttpCommandExecutor.java:158)
        at org.openqa.selenium.remote.service.DriverCommandExecutor.execute(DriverCommandExecutor.java:83)
        at org.openqa.selenium.remote.RemoteWebDriver.execute(RemoteWebDriver.java:552)
        at org.openqa.selenium.remote.RemoteWebElement.execute(RemoteWebElement.java:285)
        at org.openqa.selenium.remote.RemoteWebElement.getText(RemoteWebElement.java:166)
        at com.digicorp.utils.UsefulFunctionUtils.listOfJobs(UsefulFunctionUtils.java:45)
        at com.digicorp.testcases.TC_AddJobCategory.testAddJobCategory(TC_AddJobCategory.java:26)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:567)
        at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:133)
        at org.testng.internal.TestInvoker.invokeMethod(TestInvoker.java:598)
        at org.testng.internal.TestInvoker.invokeTestMethod(TestInvoker.java:173)
        at org.testng.internal.MethodRunner.runInSequence(MethodRunner.java:46)
        at org.testng.internal.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:824)
        at org.testng.internal.TestInvoker.invokeTestMethods(TestInvoker.java:146)
        at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)
        at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:128)
        at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
        at org.testng.TestRunner.privateRun(TestRunner.java:794)
        at org.testng.TestRunner.run(TestRunner.java:596)
        at org.testng.SuiteRunner.runTest(SuiteRunner.java:377)
        at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:371)
        at org.testng.SuiteRunner.privateRun(SuiteRunner.java:332)
        at org.testng.SuiteRunner.run(SuiteRunner.java:276)
        at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:53)
        at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:96)
        at org.testng.TestNG.runSuitesSequentially(TestNG.java:1212)
        at org.testng.TestNG.runSuitesLocally(TestNG.java:1134)
        at org.testng.TestNG.runSuites(TestNG.java:1063)
        at org.testng.TestNG.run(TestNG.java:1031)
        at org.testng.remote.AbstractRemoteTestNG.run(AbstractRemoteTestNG.java:115)
        at org.testng.remote.RemoteTestNG.initAndRun(RemoteTestNG.java:251)
        at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:77)


",https://stackoverflow.com/questions/69431495/staleelement-exception-error-when-asserting-data-in-a-table,AI
403,WebDriverWait not working as expected,"
I am working with selenium to scrape some data.
There is button on the page that I am clicking say ""custom_cols"". This button opens up a window for me where I can select my columns. 
This new window sometimes takes some time to open (around 5 seconds). So to handle this I have used 
WebDriverWait 

with delay as 20 seconds. But some times it fails to select find elements on new window, even if the element is visible. This happens only once in ten times for rest of time it works properly.
I have used same function(WebDriverWait) on other places also and it is works as expected. I mean it waits till the elements gets visible and then clicks it at the moment it finds it.
My question is why elements on new window is not visible even though I am waiting for element to get visible. To add here I have tried to increase delay time but still I get that error once in a while.
My code is here 
def wait_for_elem_xpath(self, delay = None, xpath = """"):
    if delay is None:
        delay = self.delay

    try:
        myElem = WebDriverWait(self.browser, delay).until(EC.presence_of_element_located((By.XPATH , xpath)))
    except TimeoutException:
        print (""xpath: Loading took too much time!"")
    return myElem
select_all_performance = '//*[@id=""mks""]/body/div[7]/div[2]/div/div/div/div/div[2]/div/div[2]/div[2]/div/div[1]/div[1]/section/header/div'
self.wait_for_elem_xpath(xpath = select_all_performance).click()

",https://stackoverflow.com/questions/49775502/webdriverwait-not-working-as-expected,AI
404,Web-scraping JavaScript page with Python,"
I'm trying to develop a simple web scraper. I want to extract text without the HTML code. It works on plain HTML, but not in some pages where JavaScript code adds text.
For example, if some JavaScript code adds some text, I can't see it, because when I call:
response = urllib2.urlopen(request)

I get the original text without the added one (because JavaScript is executed in the client).
So, I'm looking for some ideas to solve this problem.
",https://stackoverflow.com/questions/8049520/web-scraping-javascript-page-with-python,AI
405,Scraping data to Google Sheets from a website that uses JavaScript,"
I am trying to import data from the following website to Google Sheets. I want to import all the matches for the day.
https://www.tournamentsoftware.com/tournament/b731fdcd-a0c8-4558-9344-2a14c267ee8b/Matches
I have tried importxml and importhtml, but it seems this does not work as the website uses JavaScript. I have also tried to use Apipheny without any success.
When using Apipheny, the error message is

'Failed to fetch data - please verify your API Request: {DNS error'

",https://stackoverflow.com/questions/74237688/scraping-data-to-google-sheets-from-a-website-that-uses-javascript,AI
406,retrieve links from web page using python and BeautifulSoup [closed],"






Closed. This question needs details or clarity. It is not currently accepting answers.
                        
                    










Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



How can I retrieve the links of a webpage and copy the url address of the links using Python?
",https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup,AI
407,How can I pass variable into an evaluate function?,"
I'm trying to pass a variable into a page.evaluate() function in Puppeteer, but when I use the following very simplified example, the variable evalVar is undefined.
I can't find any examples to build on, so I need help passing that variable into the page.evaluate() function so I can use it inside.
const puppeteer = require('puppeteer');

(async() => {

  const browser = await puppeteer.launch({headless: false});
  const page = await browser.newPage();

  const evalVar = 'WHUT??';

  try {

    await page.goto('https://www.google.com.au');
    await page.waitForSelector('#fbar');
    const links = await page.evaluate((evalVar) => {

      console.log('evalVar:', evalVar); // appears undefined

      const urls = [];
      hrefs = document.querySelectorAll('#fbar #fsl a');
      hrefs.forEach(function(el) {
        urls.push(el.href);
      });
      return urls;
    })
    console.log('links:', links);

  } catch (err) {

    console.log('ERR:', err.message);

  } finally {

    // browser.close();

  }

})();

",https://stackoverflow.com/questions/46088351/how-can-i-pass-variable-into-an-evaluate-function,AI
408,selenium with scrapy for dynamic page,"
I'm trying to scrape product information from a webpage, using scrapy. My to-be-scraped webpage looks like this:

starts with a product_list page with 10 products
a click on ""next""  button loads the next 10 products (url doesn't change between the two pages)
i use LinkExtractor to follow each product link into the product page, and get all the information I need

I tried to replicate the next-button-ajax-call but can't get working, so I'm giving selenium a try. I can run selenium's webdriver in a separate script, but I don't know how to integrate with scrapy. Where shall I put the selenium part in my scrapy spider? 
My spider is pretty standard, like the following:
class ProductSpider(CrawlSpider):
    name = ""product_spider""
    allowed_domains = ['example.com']
    start_urls = ['http://example.com/shanghai']
    rules = [
        Rule(SgmlLinkExtractor(restrict_xpaths='//div[@id=""productList""]//dl[@class=""t2""]//dt'), callback='parse_product'),
        ]

    def parse_product(self, response):
        self.log(""parsing product %s"" %response.url, level=INFO)
        hxs = HtmlXPathSelector(response)
        # actual data follows

Any idea is appreciated. Thank you!
",https://stackoverflow.com/questions/17975471/selenium-with-scrapy-for-dynamic-page,AI
409,How to scrape only visible webpage text with BeautifulSoup?,"
Basically, I want to use BeautifulSoup to grab strictly the visible text on a webpage. For instance, this webpage is my test case. And I mainly want to just get the body text (article) and maybe even a few tab names here and there. I have tried the suggestion in this SO question that returns lots of <script> tags and html comments which I don't want. I can't figure out the arguments I need for the function findAll() in order to just get the visible texts on a webpage.
So, how should I find all visible text excluding scripts, comments, css etc.?
",https://stackoverflow.com/questions/1936466/how-to-scrape-only-visible-webpage-text-with-beautifulsoup,AI
410,Scraping html tables into R data frames using the XML package,"
How do I scrape html tables using the XML package?
Take, for example, this wikipedia page on the Brazilian soccer team. I would like to read it in R and get the ""list of all matches Brazil have played against FIFA recognised teams"" table as a data.frame. How can I do this?
",https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package,AI
411,How does reCAPTCHA 3 know I'm using Selenium/chromedriver?,"
I'm curious how reCAPTCHA v3 works. Specifically the browser fingerprinting.
When I launch an instance of Chrome through Selenium/chromedriver and test against reCAPTCHA 3 (https://recaptcha-demo.appspot.com/recaptcha-v3-request-scores.php) I always get a score of 0.1 when using Selenium/chromedriver.
When using incognito with a normal instance, I get 0.3.
I've beaten other detection systems by injecting JavaScript and modifying the web driver object and recompiling webdriver from source and modifying the $cdc_ variables.
I can see what looks like some obfuscated POST back to the server, so I'm going to start digging there.
What might it be looking for to determine if I'm running Selenium/chromedriver?
",https://stackoverflow.com/questions/55501524/how-does-recaptcha-3-know-im-using-selenium-chromedriver,AI
412,CasperJS/PhantomJS doesn't load https page,"
I know there are certain web pages PhantomJS/CasperJS can't open, and I was wondering if this one was one of them: https://maizepages.umich.edu. CasperJS gives an error: PhantomJS failed to open page status=fail.
I tried ignoring-ssl-errors and changing my user agent but I'm not sure how to determine which ones to use.
All I'm doing right now is the basic casper setup with casper.start(url, function () { ... }) where url=https://maizepages.umich.edu;
",https://stackoverflow.com/questions/26415188/casperjs-phantomjs-doesnt-load-https-page,AI
415,Scraping: SSL: CERTIFICATE_VERIFY_FAILED error for http://en.wikipedia.org,"
I'm practicing the code from 'Web Scraping with Python', and I keep having this certificate problem:
from urllib.request import urlopen 
from bs4 import BeautifulSoup 
import re

pages = set()
def getLinks(pageUrl):
    global pages
    html = urlopen(""http://en.wikipedia.org""+pageUrl)
    bsObj = BeautifulSoup(html)
    for link in bsObj.findAll(""a"", href=re.compile(""^(/wiki/)"")):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                #We have encountered a new page
                newPage = link.attrs['href'] 
                print(newPage) 
                pages.add(newPage) 
                getLinks(newPage)
getLinks("""")

The error is:
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 1319, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1049)>

Btw,I was also practicing scrapy, but kept getting the problem: command not found: scrapy (I tried all sorts of solutions online but none works... really frustrating)
",https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org,AI
417,Java HTML Parsing [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 11 years ago.



I'm working on an app which scrapes data from a website and I was wondering how I should go about getting the data.  Specifically I need data contained in a number of div tags which use a specific CSS class - Currently (for testing purposes) I'm just checking for 
div class = ""classname""

in each line of HTML - This works, but I can't help but feel there is a better solution out there.  
Is there any nice way where I could give a class a line of HTML and have some nice methods like:
boolean usesClass(String CSSClassname);
String getText();
String getLink();

",https://stackoverflow.com/questions/238036/java-html-parsing,AI
418,Scraping dynamic content using python-Scrapy,"
Disclaimer: I've seen numerous other similar posts on StackOverflow and tried to do it the same way but was they don't seem to work on this website.
I'm using Python-Scrapy for getting data from koovs.com. 
However, I'm not able to get the product size, which is dynamically generated. Specifically, if someone could guide me a little on getting the 'Not available' size tag from the drop-down menu on this link, I'd be grateful. 
I am able to get the size list statically, but doing that I only get the list of sizes but not which of them are available.
",https://stackoverflow.com/questions/30345623/scraping-dynamic-content-using-python-scrapy,AI
420,Problem HTTP error 403 in Python 3 Web Scraping,"
I was trying to scrape a website for practice, but I kept on getting the HTTP Error 403 (does it think I'm a bot)?
Here is my code:
#import requests
import urllib.request
from bs4 import BeautifulSoup
#from urllib import urlopen
import re

webpage = urllib.request.urlopen('http://www.cmegroup.com/trading/products/#sortField=oi&sortAsc=false&venues=3&page=1&cleared=1&group=1').read
findrows = re.compile('<tr class=""- banding(?:On|Off)>(.*?)</tr>')
findlink = re.compile('<a href ="">(.*)</a>')

row_array = re.findall(findrows, webpage)
links = re.finall(findlink, webpate)

print(len(row_array))

iterator = []

The error I get is:
 File ""C:\Python33\lib\urllib\request.py"", line 160, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Python33\lib\urllib\request.py"", line 479, in open
    response = meth(req, response)
  File ""C:\Python33\lib\urllib\request.py"", line 591, in http_response
    'http', request, response, code, msg, hdrs)
  File ""C:\Python33\lib\urllib\request.py"", line 517, in error
    return self._call_chain(*args)
  File ""C:\Python33\lib\urllib\request.py"", line 451, in _call_chain
    result = func(*args)
  File ""C:\Python33\lib\urllib\request.py"", line 599, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden

",https://stackoverflow.com/questions/16627227/problem-http-error-403-in-python-3-web-scraping,AI
421,Using BeautifulSoup to extract text without tags,"
My webpage looks like this:
<p>
  <strong class=""offender"">YOB:</strong> 1987<br/>
  <strong class=""offender"">RACE:</strong> WHITE<br/>
  <strong class=""offender"">GENDER:</strong> FEMALE<br/>
  <strong class=""offender"">HEIGHT:</strong> 5'05''<br/>
  <strong class=""offender"">WEIGHT:</strong> 118<br/>
  <strong class=""offender"">EYE COLOR:</strong> GREEN<br/>
  <strong class=""offender"">HAIR COLOR:</strong> BROWN<br/>
</p>

I want to extract the info for each individual and get YOB:1987, RACE:WHITE, etc...
What I tried is:
subc = soup.find_all('p')
subc1 = subc[1]
subc2 = subc1.find_all('strong')

But this gives me only the values of YOB:, RACE:, etc...
Is there a way that I can get the data in YOB:1987, RACE:WHITE format?
",https://stackoverflow.com/questions/23380171/using-beautifulsoup-to-extract-text-without-tags,AI
422,How to save an image locally using Python whose URL address I already know?,"
I know the URL of an image on Internet.
e.g. http://www.digimouth.com/news/media/2011/09/google-logo.jpg, which contains the logo of Google.
Now, how can I download this image using Python without actually opening the URL in a browser and saving the file manually.
",https://stackoverflow.com/questions/8286352/how-to-save-an-image-locally-using-python-whose-url-address-i-already-know,AI
423,How to run Scrapy from within a Python script,"
I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:
http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/
http://snipplr.com/view/67006/using-scrapy-from-a-script/
I can't figure out where I should put my spider code and how to call it from the main function. Please help. This is the example code:
# This snippet can be used to run scrapy spiders independent of scrapyd or the scrapy command line tool and use it from a script. 
# 
# The multiprocessing library is used in order to work around a bug in Twisted, in which you cannot restart an already running reactor or in this case a scrapy instance.
# 
# [Here](http://groups.google.com/group/scrapy-users/browse_thread/thread/f332fc5b749d401a) is the mailing-list discussion for this snippet. 

#!/usr/bin/python
import os
os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'project.settings') #Must be at the top before other imports

from scrapy import log, signals, project
from scrapy.xlib.pydispatch import dispatcher
from scrapy.conf import settings
from scrapy.crawler import CrawlerProcess
from multiprocessing import Process, Queue

class CrawlerScript():

    def __init__(self):
        self.crawler = CrawlerProcess(settings)
        if not hasattr(project, 'crawler'):
            self.crawler.install()
        self.crawler.configure()
        self.items = []
        dispatcher.connect(self._item_passed, signals.item_passed)

    def _item_passed(self, item):
        self.items.append(item)

    def _crawl(self, queue, spider_name):
        spider = self.crawler.spiders.create(spider_name)
        if spider:
            self.crawler.queue.append_spider(spider)
        self.crawler.start()
        self.crawler.stop()
        queue.put(self.items)

    def crawl(self, spider):
        queue = Queue()
        p = Process(target=self._crawl, args=(queue, spider,))
        p.start()
        p.join()
        return queue.get(True)

# Usage
if __name__ == ""__main__"":
    log.start()

    """"""
    This example runs spider1 and then spider2 three times. 
    """"""
    items = list()
    crawler = CrawlerScript()
    items.append(crawler.crawl('spider1'))
    for i in range(3):
        items.append(crawler.crawl('spider2'))
    print items

# Snippet imported from snippets.scrapy.org (which no longer works)
# author: joehillen
# date  : Oct 24, 2010

Thank you.
",https://stackoverflow.com/questions/13437402/how-to-run-scrapy-from-within-a-python-script,AI
427,Why does headless need to be false for Puppeteer to work?,"
I'm creating a web api that scrapes a given url and sends that back. I am using Puppeteer to do this. I asked this question: Puppeteer not behaving like in Developer Console
and recieved an answer that suggested it would only work if headless was set to be false. I don't want to be constantly opening up a browser UI i don't need (I just the need the data!) so I'm looking for why headless has to be false and can I get a fix that lets headless = true.
Here's my code:
express()
  .get(""/*"", (req, res) => {
    global.notBaseURL = req.params[0];
    (async () => {
      const browser = await puppet.launch({ headless: false }); // Line of Interest
      const page = await browser.newPage();
      console.log(req.params[0]);
      await page.goto(req.params[0], { waitUntil: ""networkidle2"" }); //this is the url
      title = await page.$eval(""title"", (el) => el.innerText);

      browser.close();

      res.send({
        title: title,
      });
    })();
  })
  .listen(PORT, () => console.log(`Listening on ${PORT}`));

This is the page I'm trying to scrape: https://www.nordstrom.com/s/zella-high-waist-studio-pocket-7-8-leggings/5460106?origin=coordinating-5460106-0-1-FTR-recbot-recently_viewed_snowplow_mvp&recs_placement=FTR&recs_strategy=recently_viewed_snowplow_mvp&recs_source=recbot&recs_page_type=category&recs_seed=0&color=BLACK
",https://stackoverflow.com/questions/63818869/why-does-headless-need-to-be-false-for-puppeteer-to-work,AI
428,"How to import a table from web page (with ""div class"") to excel?","
I'm trying to import to Excel a list of exhibitors and countries from this webpage and I'm not getting it.
Can Someone help me?
I have tried the methods listed in this forum and doesn't work.
Sub test()

    Dim objIE As Object
    Dim hmtl As HTMLDocument

    Dim elements As IHTMLElementCollection

    Set objIE = New InternetExplorer
    objIE.Visible = True

    objIE.navigate ""https://sps.mesago.com/events/en/exhibitors_products/exhibitor-list.html""

    Application.StatusBar = ""Loading, Please wait...""

    While objIE.Busy
        DoEvents
    Wend
    Do
    Loop Until objIE.readyState = READYSTATE_COMPLETE

    Application.StatusBar = ""Importing data...""

    Set html = objIE.document

    'I try differents types and name - ByClassName(""...""), ByTagName(""...""), ...
    Set elements = html.getElementsByClassName(""list"") 

    For i = 0 To elements.Length - 1
         Sheet1.Range(""A"" & (i + 1)) = elements(i).innerText
    Next i

    objIE.Quit
    Set objIE = Nothing

    Application.StatusBar = """"

End Sub

Sorry about my English.
",https://stackoverflow.com/questions/56277464/how-to-import-a-table-from-web-page-with-div-class-to-excel,AI
429,How would I import YouTube Likes and Dislikes and a ratio from YouTube onto Google Sheets? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 3 years ago.


This post was edited and submitted for review 5 months ago and failed to reopen the post:

Original close reason(s) were not resolved






                        Improve this question
                    



What would be the correct xpath to get  YouTube likes and dislikes from a video?
",https://stackoverflow.com/questions/55060363/how-would-i-import-youtube-likes-and-dislikes-and-a-ratio-from-youtube-onto-goog,AI
430,How do I prevent site scraping? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



I have a fairly large music website with a large artist database.  I've been noticing other music sites scraping our site's data (I enter dummy Artist names here and there and then do google searches for them).  
How can I prevent screen scraping?  Is it even possible?
",,AI
431,Periodically refresh IMPORTXML() spreadsheet function,"
I have a large sheet with around 30 importxml functions that obtain data from a website that updates usually twice a day.
I would like to run the importxml function on a timely basis (every 8 hours) for my Google Spreadsheet to save the data in another sheet. The saving already works, however the updating does not!
I read in Google Spreadsheet row update that it might run every 2 hours, however I do not believe that this is true, because since I added it to my sheet nothing has changed or updated, when the spreadsheet is NOT opened.
How can I ""trigger"" the importxml function in my Google Spreadsheet in an easy way, as I have a lot of importxml functions in it?
",https://stackoverflow.com/questions/33872967/periodically-refresh-importxml-spreadsheet-function,AI
432,'list' object has no attribute 'get_attribute' while iterating through WebElements,"
I'm trying to use Python and Selenium to scrape multiple links on a web page. I'm using find_elements_by_xpath and I'm able to locate a list of elements but I'm having trouble changing the list that is returned to the actual href links. I know find_element_by_xpath works, but that only works for one element.
Here is my code:
path_to_chromedriver = 'path to chromedriver location'
browser = webdriver.Chrome(executable_path = path_to_chromedriver)

browser.get(""file:///path to html file"")

all_trails = []

#finds all elements with the class 'text-truncate trail-name' then 
#retrieve the a element
#this seems to be just giving us the element location but not the 
#actual location

find_href = browser.find_elements_by_xpath('//div[@class=""text truncate trail-name""]/a[1]')
all_trails.append(find_href)

print all_trails

This code is returning:
<selenium.webdriver.remote.webelement.WebElement 
(session=""dd178d79c66b747696c5d3750ea8cb17"", 
element=""0.5700549730549636-1663"")>, 
<selenium.webdriver.remote.webelement.WebElement 
(session=""dd178d79c66b747696c5d3750ea8cb17"", 
element=""0.5700549730549636-1664"")>,

I expect the all_trails array to be a list of links like: www.google.com, www.yahoo.com, www.bing.com.
I've tried looping through the all_trails list and running the get_attribute('href') method on the list but I get the error:

Does anyone have any idea how to convert the selenium WebElement's to href links?
Any help would be greatly appreciated :)
",https://stackoverflow.com/questions/47735375/list-object-has-no-attribute-get-attribute-while-iterating-through-webelemen,AI
434,Python - Download Images from google Image search?,"
I want to download all Images of google image search using python . The code I am using seems to have some problem some times .My code is 
import os
import sys
import time
from urllib import FancyURLopener
import urllib2
import simplejson

# Define search term
searchTerm = ""parrot""

# Replace spaces ' ' in search term for '%20' in order to comply with request
searchTerm = searchTerm.replace(' ','%20')


# Start FancyURLopener with defined version 
class MyOpener(FancyURLopener): 
    version = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127     Firefox/2.0.0.11'
    myopener = MyOpener()

    # Set count to 0
    count= 0

    for i in range(0,10):
    # Notice that the start changes for each iteration in order to request a new set of   images for each loop
    url = ('https://ajax.googleapis.com/ajax/services/search/images?' + 'v=1.0& q='+searchTerm+'&start='+str(i*10)+'&userip=MyIP')
    print url
    request = urllib2.Request(url, None, {'Referer': 'testing'})
    response = urllib2.urlopen(request)

# Get results using JSON
    results = simplejson.load(response)
    data = results['responseData']
    dataInfo = data['results']

# Iterate for each result and get unescaped url
    for myUrl in dataInfo:
        count = count + 1
        my_url = myUrl['unescapedUrl']
        myopener.retrieve(myUrl['unescapedUrl'],str(count)+'.jpg')        

After downloading few pages I am getting an error as follows:
Traceback (most recent call last):
  File ""C:\Python27\img_google3.py"", line 37, in <module>
    dataInfo = data['results']
TypeError: 'NoneType' object has no attribute '__getitem__'

What to do ??????   
",https://stackoverflow.com/questions/20716842/python-download-images-from-google-image-search,AI
437,How can I get the CSS Selector in Chrome?,"
I want to be able to select/highlight an element on the page and find its selector like this:

div.firstRow
  div.priceAvail>div>div.PriceCompare>div.BodyS

I know you can see the selection on the bottom after doing an inspect element, but how can I copy this path to the clipboard? In Firebug I think you can do this, but don't see a way to do this using the Chrome Developer Tools and search for an extension did not turn-up anything.
This is what I am trying to do for more reference:
http://asciicasts.com/episodes/173-screen-scraping-with-scrapi
",https://stackoverflow.com/questions/4500572/how-can-i-get-the-css-selector-in-chrome,AI
438,Web scraping in PHP,"
I'm looking for a way to make a small preview of another page from a URL given by the user in PHP.
I'd like to retrieve only the title of the page, an image (like the logo of the website) and a bit of text or a description if it's available. Is there any simple way to do this without any external libraries/classes? Thanks
So far I've tried using the DOCDocument class, loading the HTML and displaying it on the screen, but I don't think that's the proper way to do it
",https://stackoverflow.com/questions/9813273/web-scraping-in-php,AI
440,Android - Parse JS generated urls with JSOUP,"
im trying to parse url generated by Bootstrap`s Bootpage.js which looks like
https://example.com/#page-2
but JSOUP cant parse it and showing main url.
how to get normal link from Bootpage or how to make JSOUP to parse it.
Parsing code:
Jsoup.connect(""https://example.com/#page-2"").followRedirects(true).get();

",https://stackoverflow.com/questions/39140121/android-parse-js-generated-urls-with-jsoup,AI
441,UnicodeEncodeError: 'ascii' codec can't encode character '\xe9' - -when using urlib.request python3,"
I'm writing a script that goes to a list of links and parses the information.
It works for most sites but It's choking on some with 
""UnicodeEncodeError: 'ascii' codec can't encode character '\xe9' in position 13: ordinal not in range(128)""
It stops on client.py which is part of urlib on python3
the exact link is:
http://finance.yahoo.com/news/caf茅s-growing-faster-than-fast-food-peers-144512056.html
There are quite a few similar postings here but none of the answers seems to work for me.
my code is:
from urllib import request

def __request(link,debug=0):      

try:
    html = request.urlopen(link, timeout=35).read() #made this long as I was getting lots of timeouts
    unicode_html = html.decode('utf-8','ignore')

# NOTE the except HTTPError must come first, otherwise except URLError will also catch an HTTPError.
except HTTPError as e:
    if debug:
        print('The server couldn\'t fulfill the request for ' + link)
        print('Error code: ', e.code)
    return ''
except URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('timeout')
        return ''    
else:
    return unicode_html

this calls the request function
link = 'http://finance.yahoo.com/news/caf茅s-growing-faster-than-fast-food-peers-144512056.html'
page = __request(link)
And the traceback is:
Traceback (most recent call last):
  File ""<string>"", line 250, in run_nodebug
  File ""C:\reader\get_news.py"", line 276, in <module>
    main()
  File ""C:\reader\get_news.py"", line 255, in main
    body = get_article_body(item['link'],debug=0)
  File ""C:\reader\get_news.py"", line 155, in get_article_body
    page = __request('na',url)
  File ""C:\reader\get_news.py"", line 50, in __request
    html = request.urlopen(link, timeout=35).read()
  File ""C:\Python33\Lib\urllib\request.py"", line 156, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Python33\Lib\urllib\request.py"", line 469, in open
    response = self._open(req, data)
  File ""C:\Python33\Lib\urllib\request.py"", line 487, in _open
    '_open', req)
  File ""C:\Python33\Lib\urllib\request.py"", line 447, in _call_chain
    result = func(*args)
  File ""C:\Python33\Lib\urllib\request.py"", line 1268, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File ""C:\Python33\Lib\urllib\request.py"", line 1248, in do_open
    h.request(req.get_method(), req.selector, req.data, headers)
  File ""C:\Python33\Lib\http\client.py"", line 1061, in request
    self._send_request(method, url, body, headers)
  File ""C:\Python33\Lib\http\client.py"", line 1089, in _send_request
    self.putrequest(method, url, **skips)
  File ""C:\Python33\Lib\http\client.py"", line 953, in putrequest
    self._output(request.encode('ascii'))
UnicodeEncodeError: 'ascii' codec can't encode character '\xe9' in position 13: ordinal not in range(128)

Any help appreciated It's driving me crazy , I think I've tried all combinations of x.decode    and similar 
(I could ignore the offending characters if that is possible.)
",https://stackoverflow.com/questions/22734464/unicodeencodeerror-ascii-codec-cant-encode-character-xe9-when-using-ur,AI
443,pandas read_html ValueError: No tables found,"
I am trying to scrap the historical weather data from the ""https://www.wunderground.com/personal-weather-station/dashboard?ID=KMAHADLE7#history/tdata/s20170201/e20170201/mcustom.html"" weather underground page. I have the following code: 
import pandas as pd 

page_link = 'https://www.wunderground.com/personal-weather-station/dashboard?ID=KMAHADLE7#history/tdata/s20170201/e20170201/mcustom.html'
df = pd.read_html(page_link)
print(df)

I have the following response: 
Traceback (most recent call last):
 File ""weather_station_scrapping.py"", line 11, in <module>
  result = pd.read_html(page_link)
 File ""/anaconda3/lib/python3.6/site-packages/pandas/io/html.py"", line 987, in read_html
  displayed_only=displayed_only)
 File ""/anaconda3/lib/python3.6/site-packages/pandas/io/html.py"", line 815, in _parse raise_with_traceback(retained)
 File ""/anaconda3/lib/python3.6/site-packages/pandas/compat/__init__.py"", line 403, in raise_with_traceback
  raise exc.with_traceback(traceback)
ValueError: No tables found

Although, this page clearly has a table but it is not being picked by the read_html. I have tried using Selenium so that the page can be loaded before I read it. 
from selenium import webdriver
from selenium.webdriver.common.keys import Keys

driver = webdriver.Firefox()
driver.get(""https://www.wunderground.com/personal-weather-station/dashboard?ID=KMAHADLE7#history/tdata/s20170201/e20170201/mcustom.html"")
elem = driver.find_element_by_id(""history_table"")

head = elem.find_element_by_tag_name('thead')
body = elem.find_element_by_tag_name('tbody')

list_rows = []

for items in body.find_element_by_tag_name('tr'):
    list_cells = []
    for item in items.find_elements_by_tag_name('td'):
        list_cells.append(item.text)
    list_rows.append(list_cells)
driver.close()

Now, the problem is that it cannot find ""tr"". I would appreciate any suggestions. 
",https://stackoverflow.com/questions/53398785/pandas-read-html-valueerror-no-tables-found,AI
445,Scraping dynamic data selenium - Unable to locate element,"
I am very new to scraping and have a question. I am scraping worldometers covid data. As it is dynamic - I am doing it with selenium.
The code is the following:
from selenium import webdriver
import time

URL = ""https://www.worldometers.info/coronavirus/""

# Start the Driver
driver = webdriver.Chrome(executable_path = r""C:\Webdriver\chromedriver.exe"")
# Hit the url and wait for 10 seconds.
driver.get(URL)
time.sleep(10)
#find class element
data= driver.find_elements_by_class_name(""odd"" and ""even"")
#for loop
for d in data:
    country=d.find_element_by_xpath("".//*[@id='main_table_countries_today']"").text
    print(country)

current output:
NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""xpath"",""selector"":"".//*[@id='main_table_countries_today']""}
  (Session info: chrome=96.0.4664.45)

",https://stackoverflow.com/questions/70206678/scraping-dynamic-data-selenium-unable-to-locate-element,AI
446,Failproof Wait for IE to load,"
Is there a foolproof way for the script to wait till the Internet explorer is completely loaded?
Both oIE.Busy and / or oIE.ReadyState are not working the way they should: 
Set oIE = CreateObject(""InternetExplorer.application"")

    oIE.Visible = True
    oIE.navigate (""http://technopedia.com"")

    Do While oIE.Busy Or oIE.ReadyState <> 4: WScript.Sleep 100: Loop  

    ' <<<<< OR >>>>>>

    Do While oIE.ReadyState <> 4: WScript.Sleep 100: Loop

Any other suggestions?
",https://stackoverflow.com/questions/23299134/failproof-wait-for-ie-to-load,AI
448,"Selenium-Debugging: Element is not clickable at point (X,Y)","
I try to scrape this site by Selenium.
I want to click in ""Next Page"" buttom, for this I do:
 driver.find_element_by_class_name('pagination-r').click()

it works for many pages but not for all, I got this error
WebDriverException: Message: Element is not clickable at point (918, 13). Other element would receive the click: <div class=""linkAuchan""></div>

always for this page 
I read this question 
and I tried this 
driver.implicitly_wait(10)
el = driver.find_element_by_class_name('pagination-r')
action = webdriver.common.action_chains.ActionChains(driver)
action.move_to_element_with_offset(el, 918, 13)
action.click()
action.perform()

but I got the same error
",https://stackoverflow.com/questions/37879010/selenium-debugging-element-is-not-clickable-at-point-x-y,AI
449,Click a Button in Scrapy,"
I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking).
I found out that Scrapy can handle forms (like logins) as shown here. But the problem is that there is no form to fill out, so it's not exactly what I need.
How can I simply click a button, which then shows the information I need?
Do I have to use an external library like mechanize or lxml?
",https://stackoverflow.com/questions/6682503/click-a-button-in-scrapy,AI
450,Crawling multiple URLs in a loop using Puppeteer,"
I have an array of URLs to scrape data from:
urls = ['url','url','url'...]

This is what I'm doing:
urls.map(async (url)=>{
  await page.goto(url);
  await page.waitForNavigation({ waitUntil: 'networkidle' });
})

This seems to not wait for page load and visits all the URLs quite rapidly (I even tried using page.waitFor).
I wanted to know if am I doing something fundamentally wrong or this type of functionality is not advised/supported.
",https://stackoverflow.com/questions/46293216/crawling-multiple-urls-in-a-loop-using-puppeteer,AI
452,How to use Beautiful Soup to extract string in <script> tag?,"
In a given .html page, I have a script tag like so:
     <script>jQuery(window).load(function () {
  setTimeout(function(){
    jQuery(""input[name=Email]"").val(""name@email.com"");
  }, 1000);
});</script>

How can I use Beautiful Soup to extract the email address?
",https://stackoverflow.com/questions/38547569/how-to-use-beautiful-soup-to-extract-string-in-script-tag,AI
454,Save and render a webpage with PhantomJS and node.js,"
I'm looking for an example of requesting a webpage, waiting for the JavaScript to render (JavaScript modifies the DOM), and then grabbing the HTML of the page.
This should be a simple example with an obvious use-case for PhantomJS. I can't find a decent example, the documentation seems to be all about command line use.
",https://stackoverflow.com/questions/9966826/save-and-render-a-webpage-with-phantomjs-and-node-js,AI
455,Puppeteer - Protocol error (Page.navigate): Target closed,"
As you can see with the sample code below, I'm using Puppeteer with a cluster of workers in Node to run multiple requests of websites screenshots by a given URL:
const cluster = require('cluster');
const express = require('express');
const bodyParser = require('body-parser');
const puppeteer = require('puppeteer');

async function getScreenshot(domain) {
    let screenshot;
    const browser = await puppeteer.launch({ args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage'] });
    const page = await browser.newPage();

    try {
        await page.goto('http://' + domain + '/', { timeout: 60000, waitUntil: 'networkidle2' });
    } catch (error) {
        try {
            await page.goto('http://' + domain + '/', { timeout: 120000, waitUntil: 'networkidle2' });
            screenshot = await page.screenshot({ type: 'png', encoding: 'base64' });
        } catch (error) {
            console.error('Connecting to: ' + domain + ' failed due to: ' + error);
        }

    await page.close();
    await browser.close();

    return screenshot;
}

if (cluster.isMaster) {
    const numOfWorkers = require('os').cpus().length;
    for (let worker = 0; worker < numOfWorkers; worker++) {
        cluster.fork();
    }

    cluster.on('exit', function (worker, code, signal) {
        console.debug('Worker ' + worker.process.pid + ' died with code: ' + code + ', and signal: ' + signal);
        Cluster.fork();
    });

    cluster.on('message', function (handler, msg) {
        console.debug('Worker: ' + handler.process.pid + ' has finished working on ' + msg.domain + '. Exiting...');
        if (Cluster.workers[handler.id]) {
            Cluster.workers[handler.id].kill('SIGTERM');
        }
    });
} else {
    const app = express();
    app.use(bodyParser.json());
    app.listen(80, function() {
        console.debug('Worker ' + process.pid + ' is listening to incoming messages');
    });

    app.post('/screenshot', (req, res) => {
        const domain = req.body.domain;

        getScreenshot(domain)
            .then((screenshot) =>
                try {
                    process.send({ domain: domain });
                } catch (error) {
                    console.error('Error while exiting worker ' + process.pid + ' due to: ' + error);
                }

                res.status(200).json({ screenshot: screenshot });
            })
            .catch((error) => {
                try {
                    process.send({ domain: domain });
                } catch (error) {
                    console.error('Error while exiting worker ' + process.pid + ' due to: ' + error);
                }

                res.status(500).json({ error: error });
            });
    });
}

Some explanation:

Each time a request arrives a worker will process it and kill itself at the end
Each worker creates a new browser instance with a single page, and if a page took more than 60sec to load, it will retry reloading it (in the same page because maybe some resources has already been loaded) with timeout of 120sec
Once finished both the page and the browser will be closed

My problem is that some legitimate domains get errors that I can't explain:
Error: Protocol error (Page.navigate): Target closed.

Error: Protocol error (Runtime.callFunctionOn): Session closed. Most likely the page has been closed.

I read at some git issue (that I can't find now) that it can happen when the page redirects and adds 'www' at the start, but I'm hoping it's false...
Is there something I'm missing?
",https://stackoverflow.com/questions/51629151/puppeteer-protocol-error-page-navigate-target-closed,AI
456,How to get the scrapy failure URLs?,"
I'm a newbie of scrapy and it's amazing crawler framework i have known! 
In my project, I sent more than 90, 000 requests, but there are some of them failed. 
I set the log level to be INFO, and i just can see some statistics but no details. 
2012-12-05 21:03:04+0800 [pd_spider] INFO: Dumping spider stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.ConnectionDone': 1,
 'downloader/request_bytes': 46282582,
 'downloader/request_count': 92383,
 'downloader/request_method_count/GET': 92383,
 'downloader/response_bytes': 123766459,
 'downloader/response_count': 92382,
 'downloader/response_status_count/200': 92382,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2012, 12, 5, 13, 3, 4, 836000),
 'item_scraped_count': 46191,
 'request_depth_max': 1,
 'scheduler/memory_enqueued': 92383,
 'start_time': datetime.datetime(2012, 12, 5, 12, 23, 25, 427000)}

Is there any way to get more detail report? For example, show those failed URLs. Thanks!
",https://stackoverflow.com/questions/13724730/how-to-get-the-scrapy-failure-urls,AI
458,Best practice - Git + Build automation - Keeping configs separate,"
Searching for the best approach to keep my config files separate, yet not introduce extra steps for new developers setting up their environments.
I am guessing a submodule would suffice to do the job, but then how would I switch configs seamlessly depending on the task at hand, aka pull in DEV config regularly, pull PROD branch of config repo during build?
Needs to be:

Easy and painless for new devs.  
PROD config files should only be accessible to select users + build user.

Thank you in advance.
",https://stackoverflow.com/questions/20822073/best-practice-git-build-automation-keeping-configs-separate,AI
459,Selenium waitForElement,"
How do I write the function for Selenium to wait for a table with just a class identifier in Python? I'm having a devil of a time learning to use Selenium's Python webdriver functions.
",https://stackoverflow.com/questions/7781792/selenium-waitforelement,AI
461,How to copy a file to a remote server in Python using SCP or SSH?,"
I have a text file on my local machine that is generated by a daily Python script run in cron. 
I would like to add a bit of code to have that file sent securely to my server over SSH.
",https://stackoverflow.com/questions/68335/how-to-copy-a-file-to-a-remote-server-in-python-using-scp-or-ssh,AI
473,Check if element is clickable in Selenium Java,"
I'm new to Selenium and need to check if element is clickable in Selenium Java, since element.click() passes both on link and label.
I tried using the following code, but it is not working:
WebDriverWait wait = new WebDriverWait(Scenario1Test.driver, 10);

if(wait.until(ExpectedConditions.elementToBeClickable(By.xpath(""(//div[@id='brandSlider']/div[1]/div/div/div/img)[50]"")))==null)

",https://stackoverflow.com/questions/38327049/check-if-element-is-clickable-in-selenium-java,AI
475,JMeter : How to record HTTPS traffic?,"
I'm using Apache JMeter 2.3, which now supports ""attempt HTTPS spoofing"" under the Proxy Server element. 
I've tried this on several different servers, and have had no success. 
Has anyone been able to successfully record from an HTTPS source with this setting?
Or barring successfully recording, can anyone share a work-around? When available, I simply have HTTPS turned off at the server level, but this is not always feasible. Thoughts?
",https://stackoverflow.com/questions/299529/jmeter-how-to-record-https-traffic,AI
478,How to make the Java.awt.Robot type unicode characters? (Is it possible?),"
We have a user provided string that may contain unicode characters, and we want the robot to type that string.
How do you convert a string into keyCodes that the robot will use?
How do you do it so it is also java version independant (1.3 -> 1.6)?
What we have working for ""ascii"" chars is
//char c = nextChar();
//char c = 'a'; // this works, and so does 'A'
char c = '谩'; // this doesn't, and neither does '膫'
Robot robot = new Robot();
KeyStroke key = KeyStroke.getKeyStroke(""pressed "" + Character.toUpperCase(c) );
if( null != key ) {
  // should only have to worry about case with standard characters
  if (Character.isUpperCase(c))
  {
    robot.keyPress(KeyEvent.VK_SHIFT);
  }

  robot.keyPress(key.getKeyCode());
  robot.keyRelease(key.getKeyCode());

  if (Character.isUpperCase(c))
  {
    robot.keyRelease(KeyEvent.VK_SHIFT);
  }
}

",https://stackoverflow.com/questions/397113/how-to-make-the-java-awt-robot-type-unicode-characters-is-it-possible,AI
479,python-pptx - How to replace keyword across multiple runs?,"
I have two PPTs (File1.pptx and File2.pptx) in which I have the below 2 lines
XX NOV 2021, Time: xx:xx 鈥?xx:xx hrs (90mins)
FY21/22 / FY22/23

I wish to replace like below
a) NOV 2021 as NOV 2022.
b) FY21/22 / FY22/23 as FY21/22 or FY22/23.
But the problem is my replacement works in File1.pptx but it doesn't work in File2.pptx.
When I printed the run text, I was able to see that they are represented differently in two slides.
def replace_text(replacements:dict,shapes:list):
    for shape in shapes:
        for match, replacement in replacements.items():
            if shape.has_text_frame:
                if (shape.text.find(match)) != -1:
                    text_frame = shape.text_frame
                    for paragraph in text_frame.paragraphs:
                        for run in paragraph.runs:
                            cur_text = run.text
                            print(cur_text)
                            print(""---"")
                            new_text = cur_text.replace(str(match), str(replacement))
                            run.text = new_text

In File1.pptx, the cur_text looks like below (for 1st keyword). So, my replace works (as it contains the keyword that I am looking for)

But in File2.pptx, the cur_text looks like below (for 1st keyword). So, replace doesn't work (because the cur_text doesn't match with my search term)

The same issue happens for 2nd keyword as well which is FY21/22 / FY22/23.
The problem is the split keyword could be in previous or next run from current run (with no pattern). So, we should be able to compare a search term with previous run term (along with current term as well). Then a match can be found (like Nov 2021) and be replaced.
This issue happens for only 10% of the search terms (and not for all of my search terms) but scary to live with this issue because if the % increases, we may have to do a lot of manual work. How do we avoid this and code correctly?
How do we get/extract/find/identify the word that we are looking for across multiple runs (when they are indeed present) like CTRL+F and replace it with desired keyword?
Any help please?
UPDATE - Incorrect replacements based on matching
Before replacement

After replacement

My replacement keywords can be found below
replacements = { 'How are you?': ""I'm fine!"",
                'FY21/22':'FY22/23',
                'FY_2021':'FY21/22',
                'FY20/21':'FY21/22',
                'GB2021':'GB2022',
                'GB2020':'GB2022',
                'SEP-2022':'SEP-2023',
                'SEP-2021':'SEP-2022',
                'OCT-2021':'OCT-2022',
                'OCT-2020':'OCT-2021',
                'OCT 2021':'OCT 2022',
                'NOV 2021':'NOV 2022',
                'FY2122':'FY22/23',
                'FY2021':'FY21/22',
                'FY1920':'FY20/21',
                'FY_2122':'FY22/23',
                'FY21/22 / FY22/23':'FY21/22 or FY22/23',
                'F21Y22':'FY22/23',
                'your FY20 POS FCST':'your FY22/23 POS FCST',
                'your FY21/22 POS FCST':'your FY22/23 POS FCST',
                'Q2/FY22/23':'Q2-FY22/23',
                'JAN-22':'JAN-23',
                'solution for FY21/22':'solution for FY22/23',
                'achievement in FY20/21':'achievement in FY21/22',
                'FY19/20':'FY20/21'}

",https://stackoverflow.com/questions/73219378/python-pptx-how-to-replace-keyword-across-multiple-runs,AI
482,Puppeteer wait for all images to load then take screenshot,"
I am using Puppeteer to try to take a screenshot of a website after all images have loaded but can't get it to work.
Here is the code I've got so far, I am using https://www.digg.com as the example website:
const puppeteer = require('puppeteer');

(async () => {
    const browser = await puppeteer.launch();
    const page = await browser.newPage();
    await page.goto('https://www.digg.com/');

    await page.setViewport({width: 1640, height: 800});

    await page.evaluate(() => {
        return Promise.resolve(window.scrollTo(0,document.body.scrollHeight));
    });

    await page.waitFor(1000);

    await page.evaluate(() => {
        var images = document.querySelectorAll('img');

        function preLoad() {

            var promises = [];

            function loadImage(img) {
                return new Promise(function(resolve,reject) {
                    if (img.complete) {
                        resolve(img)
                    }
                    img.onload = function() {
                        resolve(img);
                    };
                    img.onerror = function(e) {
                        resolve(img);
                    };
                })
            }

            for (var i = 0; i < images.length; i++)
            {
                promises.push(loadImage(images[i]));
            }

            return Promise.all(promises);
        }

        return preLoad();
    });

    await page.screenshot({path: 'digg.png', fullPage: true});

    browser.close();
})();

",https://stackoverflow.com/questions/46160929/puppeteer-wait-for-all-images-to-load-then-take-screenshot,AI
483,Android 鈥?multiple custom versions of the same app,"
Whats the best way to deploy several customized versions of a Android application? 
Currently I have a script to exchange the resource folder for getting a customized version of my app. It works great, but all custom versions still have the same package name in the AndroidManifest.xml. Therefore it is not possible to install two customized versions of the app at the same time.
This is one solution for this problem, but that has to be done by hand
Can you think of a more easy solution, or how this could be built into a skript?
(btw: it is not for a porn/spam/whatever app, not even a paid one)
", with the library\'s resources and classes available and recognized in the Variant project.)\n\n',AI
484,How to programmatically fill input elements built with React?,"
I'm tasked with crawling website built with React. I'm trying to fill in input fields and submitting the form using javascript injects to the page (either selenium or webview in mobile). This works like a charm on every other site + technology but React seems to be a real pain.
so here is a sample code 
var email = document.getElementById( 'email' );
email.value = 'example@mail.com';

I the value changes on the DOM input element, but the React does not trigger the change event.
I've been trying plethora of different ways to get the React to update the state.
var event = new Event('change', { bubbles: true });
email.dispatchEvent( event );

no avail
var event = new Event('input', { bubbles: true });
email.dispatchEvent( event );

not working
email.onChange( event );

not working
I cannot believe interacting with React has been made so difficult. I would greatly appreciate any help. 
Thank you
",https://stackoverflow.com/questions/40894637/how-to-programmatically-fill-input-elements-built-with-react,AI
486,How can I use persisted cookies from a file using phantomjs,"
I have some authentication requried to hit a particular url. In browser I need to login only once, as for other related urls which can use the session id from the cookie need not required to go to the login page. 
Similarly, can I use the cookie generated in the cookie file using --cookies-file=cookies.txt in the commandline in phantomjs to open other page which requires the same cookie detail.
Please suggest.
",https://stackoverflow.com/questions/18739354/how-can-i-use-persisted-cookies-from-a-file-using-phantomjs,AI
502,PhantomJS failing to open HTTPS site,"
I'm using the following code based on loadspeed.js example to open up a https:// site which requires http server authentication as well.
var page = require('webpage').create(), system = require('system'), t, address;

page.settings.userName = 'myusername';
page.settings.password = 'mypassword';

if (system.args.length === 1) {
    console.log('Usage: scrape.js <some URL>');
    phantom.exit();
} else {
    t = Date.now();
    address = system.args[1];
    page.open(address, function (status) {
        if (status !== 'success') {
            console.log('FAIL to load the address');
        } else {
            t = Date.now() - t;
            console.log('Page title is ' + page.evaluate(function () {
                return document.title;
            }));
            console.log('Loading time ' + t + ' msec');
        }
        phantom.exit();
    });
}  

Its failing to load the page all the time. What could be wrong here? Are secured sites to be handled any differently? The site can be accessed successfully from browser though.
I'm just starting with Phantom right now and find it too good to stop playing around even though i'm not moving forward with this issue.
",https://stackoverflow.com/questions/12021578/phantomjs-failing-to-open-https-site,AI
504,scrape html generated by javascript with python,"
I need to scrape a site with python. I obtain the source html code with the urlib module, but I need to scrape also some html code that is generated by a javascript function (which is included in the html source). What this functions does ""in"" the site is that when you press a button it outputs some html code. How can I ""press"" this button with python code? Can scrapy help me? I captured the POST request with firebug but when I try to pass it on the url I get a 403 error. Any suggestions?
",https://stackoverflow.com/questions/2148493/scrape-html-generated-by-javascript-with-python,AI
507,Executing Javascript from Python,"
I have HTML webpages that I am crawling using xpath. The etree.tostring of a certain node gives me this string:
<script>
<!--
function escramble_758(){
  var a,b,c
  a='+1 '
  b='84-'
  a+='425-'
  b+='7450'
  c='9'
  document.write(a+c+b)
}
escramble_758()
//-->
</script>

I just need the output of escramble_758(). I can write a regex to figure out the whole thing, but I want my code to remain tidy. What is the best alternative?
I am zipping through the following libraries, but I didnt see an exact solution. Most of them are trying to emulate browser, making things snail slow.

http://code.google.com/p/python-spidermonkey/ (clearly says it's not yet possible to call a function defined in Javascript)
http://code.google.com/p/webscraping/ (don't see anything for Javascript, I may be wrong)
http://pypi.python.org/pypi/selenium (Emulating browser)

Edit: An example will be great.. (barebones will do)
",https://stackoverflow.com/questions/10136319/executing-javascript-from-python,AI
511,"Beautiful Soup cannot find a CSS class if the object has other classes, too","
if a page has <div class=""class1""> and <p class=""class1"">, then soup.findAll(True, 'class1') will find them both.
If it has <p class=""class1 class2"">, though, it will not be found.  How do I find all objects with a certain class, regardless of whether they have other classes, too?
",https://stackoverflow.com/questions/1242755/beautiful-soup-cannot-find-a-css-class-if-the-object-has-other-classes-too,AI
514,Protection from screen scraping [closed],"






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.


Closed 10 years ago.







                        Improve this question
                    



Following on from my question on the Legalities of screen scraping, even if it's illegal people will still try, so:
What technical mechanisms can be employed to prevent or at least disincentivise screen scraping?
Oh and just for grins and to make life difficult, it may well be nice to retain access for search engines. I may well be playing devil's advocate here but there is a serious underlying point.
",https://stackoverflow.com/questions/396817/protection-from-screen-scraping,AI
515,unable to call firefox from selenium in python on AWS machine,"
I am trying to use selenium from python to scrape some dynamics pages with javascript. However, I cannot call firefox after I followed the instruction of selenium on the pypi page(http://pypi.python.org/pypi/selenium). I installed firefox on AWS ubuntu 12.04. The error message I got is:
In [1]: from selenium import webdriver

In [2]: br = webdriver.Firefox()
---------------------------------------------------------------------------
WebDriverException                        Traceback (most recent call last)
/home/ubuntu/<ipython-input-2-d6a5d754ea44> in <module>()
----> 1 br = webdriver.Firefox()

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/webdriver.pyc in __init__(self, firefox_profile, firefox_binary, timeout)
     49         RemoteWebDriver.__init__(self,
     50             command_executor=ExtensionConnection(""127.0.0.1"", self.profile,
---> 51             self.binary, timeout),
     52             desired_capabilities=DesiredCapabilities.FIREFOX)
     53

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/extension_connection.pyc in __init__(self, host, firefox_profile, firefox_binary, timeout)
     45         self.profile.add_extension()
     46
---> 47         self.binary.launch_browser(self.profile)
     48         _URL = ""http://%s:%d/hub"" % (HOST, PORT)
     49         RemoteConnection.__init__(

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/firefox_binary.pyc in launch_browser(self, profile)
     42
     43         self._start_from_profile_path(self.profile.path)
---> 44         self._wait_until_connectable()
     45
     46     def kill(self):

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/firefox_binary.pyc in _wait_until_connectable(self)
     79                 raise WebDriverException(""The browser appears to have exited ""
     80                       ""before we could connect. The output was: %s"" %
---> 81                       self._get_firefox_output())
     82             if count == 30:
     83                 self.kill()

WebDriverException: Message: 'The browser appears to have exited before we could connect. The output was: Error: no display specified\n'

I did search on the web and found that this problem happened with other people (https://groups.google.com/forum/?fromgroups=#!topic/selenium-users/21sJrOJULZY). But I don't understand the solution, if it is. 
Can anyone help me please? Thanks!
",https://stackoverflow.com/questions/13039530/unable-to-call-firefox-from-selenium-in-python-on-aws-machine,AI
517,Perform screen-scape of Webbrowser control in thread,"
I am using the technique shown in 

WebBrowser Control in a new thread

Trying to get a screen-scrape of a webpage I have been able to get the following code to successfully work when the WebBrowser control is placed on a WinForm. However it fails by providing an arbitrary image of the desktop when run inside a thread.
Thread browserThread = new Thread(() =>
{
    WebBrowser br = new WebBrowser();
    br.DocumentCompleted += webBrowser1_DocumentCompleted;
    br.ProgressChanged += webBrowser1_ProgressChanged;
    br.ScriptErrorsSuppressed = true;
    br.Navigate(url);
    Application.Run();
});
browserThread.SetApartmentState(ApartmentState.STA);
browserThread.Start();

private Image TakeSnapShot(WebBrowser browser)
{
    int width;
    int height;

    width = browser.ClientRectangle.Width;
    height = browser.ClientRectangle.Height;

    Bitmap image = new Bitmap(width, height);

    using (Graphics graphics = Graphics.FromImage(image))
    {
        Point p = new Point(0, 0);
        Point upperLeftSource = browser.PointToScreen(p);
        Point upperLeftDestination = new Point(0, 0);

        Size blockRegionSize = browser.ClientRectangle.Size;
        blockRegionSize.Width = blockRegionSize.Width - 15;
        blockRegionSize.Height = blockRegionSize.Height - 15;
        graphics.CopyFromScreen(upperLeftSource, upperLeftDestination, blockRegionSize);
    }

    return image;
}

This obviously happens because of the method Graphics.CopyFromScreen() but I am unaware of any other approach. Is there a way to resolve this issue that anyone could suggest? or is my only option to create a form, add the control, make it visible and then screen-scrape? For obvious reasons I'm hoping to avoid such an approach.
",https://stackoverflow.com/questions/18675606/perform-screen-scape-of-webbrowser-control-in-thread,AI
518,Looping over urls to do the same thing,"
I am tring to scrape a few sites. Here is my code:
for (var i = 0; i < urls.length; i++) {
    url = urls[i];
    console.log(""Start scraping: "" + url);

    page.open(url, function () {
        waitFor(function() {
            return page.evaluate(function() {
                return document.getElementById(""progressWrapper"").childNodes.length == 1;
            });

        }, function() {
            var price = page.evaluate(function() {
                // do something
                return price;
            });

            console.log(price);
            result = url + "" ; "" + price;
            output = output + ""\r\n"" + result;
        });
    });

}
fs.write('test.txt', output);
phantom.exit();

I want to scrape all sites in the array urls, extract some information and then write this information to a text file.
But there seems to be a problem with the for loop. When scraping only one site without using a loop, all works as I want. But with the loop, first nothing happens, then the line 
console.log(""Start scraping: "" + url);

is shown, but one time too much.
If url = {a,b,c}, then phantomjs does:
Start scraping: a 
Start scraping: b 
Start scraping: c 
Start scraping:

It seems that page.open isn't called at all.
I am newbie to JS so I am sorry for this stupid question.
",https://stackoverflow.com/questions/26681464/looping-over-urls-to-do-the-same-thing,AI
519,Scraping contents of multi web pages of a website using BeautifulSoup and Selenium,"
The website I want to scrap is :
http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061
I want to get the last page number of the above the link for proceeding, which is 499 while taking the screenshot.

My code :
   from bs4 import BeautifulSoup 
   from urllib.request import urlopen as uReq
   from selenium import webdriver;import time
   from selenium.webdriver.common.by import By
   from selenium.webdriver.support.ui import WebDriverWait
   from selenium.webdriver.support import expected_conditions as EC
   from selenium.webdriver.common.desired_capabilities import         DesiredCapabilities

   firefox_capabilities = DesiredCapabilities.FIREFOX
   firefox_capabilities['marionette'] = True
   firefox_capabilities['binary'] = '/etc/firefox'

   driver = webdriver.Firefox(capabilities=firefox_capabilities)
   url = ""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061""

   driver.get(url)
   wait = WebDriverWait(driver, 10)
   soup=BeautifulSoup(driver.page_source,""lxml"")
   containers = soup.findAll(""ul"",{""class"":""pages table""})
   containers[0] = soup.findAll(""li"")
   li_len = len(containers[0])
   for item in soup.find(""ul"",{""class"":""pages table""}) : 
   li_text = item.select(""li"")[li_len].text
   print(""li_text : {}\n"".format(li_text))
   driver.quit()

I need help to figure out the error in my code for getting the last page number. Also, I would be grateful if someone give the alternate solution for the same and suggest ways to achieve my intention.
",https://stackoverflow.com/questions/47869382/scraping-contents-of-multi-web-pages-of-a-website-using-beautifulsoup-and-seleni,AI
521,How to download any(!) webpage with correct charset in python?,"
Problem
When screen-scraping a webpage using python one has to know the character encoding of the page. If you get the character encoding wrong than your output will be messed up.
People usually use some rudimentary technique to detect the encoding. They either use the charset from the header or the charset defined in the meta tag or they use an encoding detector (which does not care about meta tags or headers).
By using only one these techniques, sometimes you will not get the same result as you would in a browser.
Browsers do it this way:

Meta tags always takes precedence (or xml definition)
Encoding defined in the header is used when there is no charset defined in a meta tag
If the encoding is not defined at all, than it is time for encoding detection.

(Well... at least that is the way I believe most browsers do it. Documentation is really scarce.)
What I'm looking for is a library that can decide the character set of a page the way a browser would. I'm sure I'm not the first who needs a proper solution to this problem.
Solution (I have not tried it yet...)
According to Beautiful Soup's documentation.
Beautiful Soup tries the following encodings, in order of priority, to turn your document into Unicode:

An encoding you pass in as the
fromEncoding argument to the soup
constructor.
An encoding discovered  in the document itself: for instance,   in an XML declaration or (for HTML   documents) an http-equiv META tag. If   Beautiful Soup finds this kind of   encoding within the document, it   parses the document again from the   beginning and gives the new encoding   a try. The only exception is if you   explicitly specified an encoding, and   that encoding actually worked: then   it will ignore any encoding it finds   in the document.
An encoding sniffed   by looking at the first few bytes of   the file. If an encoding is detected
at this stage, it will be one of the
UTF-* encodings, EBCDIC, or ASCII.
An
encoding sniffed by the chardet
library, if you have it installed.
UTF-8
Windows-1252

",https://stackoverflow.com/questions/1495627/how-to-download-any-webpage-with-correct-charset-in-python,AI
523,View Generated Source (After AJAX/JavaScript) in C#,"
Is there a way to view the generated source of a web page (the code after all AJAX calls and JavaScript DOM manipulations have taken place) from a C# application without opening up a browser from the code?
Viewing the initial page using a WebRequest or WebClient object works ok, but if the page makes extensive use of JavaScript to alter the DOM on page load, then these don't provide an accurate picture of the page.
I have tried using Selenium and Watin UI testing frameworks and they work perfectly, supplying the generated source as it appears after all JavaScript manipulations are completed.  Unfortunately, they do this by opening up an actual web browser, which is very slow.  I've implemented a selenium server which offloads this work to another machine, but there is still a substantial delay.
Is there a .Net library that will load and parse a page (like a browser) and spit out the generated code?  Clearly, Google and Yahoo aren't opening up browsers for every page they want to spider (of course they may have more resources than me...).  
Is there such a library or am I out of luck unless I'm willing to dissect the source code of an open source browser?
SOLUTION
Well, thank you everyone for you're help.  I have a working solution that is about 10X faster then Selenium. Woo!
Thanks to this old article from beansoftware I was able to use the System.Windows.Forms.WebBrowser control to download the page and parse it, then give em the generated source.  Even though the control is in Windows.Forms, you can still run it from Asp.Net (which is what I'm doing), just remember to add System.Window.Forms to your project references.
There are two notable things about the code.  First, the WebBrowser control is called in a new thread.  This is because it must run on a single threaded apartment.
Second, the GeneratedSource variable is set in two places.  This is not due to an intelligent design decision :)  I'm still working on it and will update this answer when I'm done.  wb_DocumentCompleted() is called multiple times.  First when the initial HTML is downloaded, then again when the first round of JavaScript completes.  Unfortunately, the site I'm scraping has 3 different loading stages.  1) Load initial HTML 2) Do first round of JavaScript DOM manipulation 3) pause for half a second then do a second round of JS DOM manipulation.
For some reason, the second round isn't cause by the wb_DocumentCompleted() function, but it is always caught when wb.ReadyState == Complete.  So why not remove it from wb_DocumentCompleted()? I'm still not sure why it isn't caught there and that's where the beadsoftware article recommended putting it.  I'm going to keep looking into it.  I just wanted to publish this code so anyone who's interested can use it.  Enjoy!
using System.Threading;
using System.Windows.Forms;

public class WebProcessor
{
    private string GeneratedSource{ get; set; }
    private string URL { get; set; }

    public string GetGeneratedHTML(string url)
    {
        URL = url;

        Thread t = new Thread(new ThreadStart(WebBrowserThread));
        t.SetApartmentState(ApartmentState.STA);
        t.Start();
        t.Join();

        return GeneratedSource;
    }

    private void WebBrowserThread()
    {
        WebBrowser wb = new WebBrowser();
        wb.Navigate(URL);

        wb.DocumentCompleted += 
            new WebBrowserDocumentCompletedEventHandler(
                wb_DocumentCompleted);

        while (wb.ReadyState != WebBrowserReadyState.Complete)
            Application.DoEvents();

        //Added this line, because the final HTML takes a while to show up
        GeneratedSource= wb.Document.Body.InnerHtml;

        wb.Dispose();
    }

    private void wb_DocumentCompleted(object sender, 
        WebBrowserDocumentCompletedEventArgs e)
    {
        WebBrowser wb = (WebBrowser)sender;
        GeneratedSource= wb.Document.Body.InnerHtml;
    }
}

",https://stackoverflow.com/questions/1307800/view-generated-source-after-ajax-javascript-in-c-sharp,AI
529,Scrapy Python Set up User Agent,"
I tried to override the user-agent of my crawlspider by adding an extra line to the project configuration file. Here is the code:
[settings]
default = myproject.settings
USER_AGENT = ""Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36""


[deploy]
#url = http://localhost:6800/
project = myproject

But when I run the crawler against my own web, I notice the spider did not pick up my customized user agent but the default one ""Scrapy/0.18.2 (+http://scrapy.org)"". 
Can any one explain what I have done wrong. 
Note:
(1). It works when I tried to override the user agent globally: 
scrapy crawl myproject.com -o output.csv -t csv -s USER_AGENT=""Mozilla....""

(2). When I remove the line ""default = myproject.setting"" from the configuration file, and run scrapy crawl myproject.com, it says ""cannot find spider.."", so I feel like the default setting should not be removed in this case.
Thanks a lot for the help in advance.                            
",https://stackoverflow.com/questions/18920930/scrapy-python-set-up-user-agent,AI
534,Scraping javascript-generated data using Python,"
I want to scrape some data of following url using Python.
http://www.hankyung.com/stockplus/main.php?module=stock&mode=stock_analysis_infomation&itemcode=078340
It's about a summary of company information. 
What I want to scrape is not shown on the first page. 
By clicking tab named ""鞛鞝滍憸"", you can access financial statement. And clicking tab named ""順勱笀頋愲響?, you can access ""Cash Flow"". 
I want to scrape the ""Cash Flow"" data. 
However, Cash flow data is generated by javascript across the url.
The following link is that url which is hidden, http://stock.kisline.com/compinfo/financial/main.action?vhead=N&vfoot=N&vstay=&omit=&vwidth=
Cash flow data is generated by submitting some option value and cookie to this url.
As you perceived, itemcode=078340 in the first link means stock code and there are as many as 1680 stocks that I want gather cash flow data. I want make it a loop structure.
Is there good way to scrape cash flow data?
I tried scrapy but scrapy is difficult to cope with my another scraping code already I'm using.
",https://stackoverflow.com/questions/10052465/scraping-javascript-generated-data-using-python,AI
536,C# WebClient - View source question,"
I'm using  a C# WebClient to post login details to a page and read the all the results.
The page I am trying to load includes flash (which, in the browser, translates into HTML). I'm guessing it's flash to avoid being picked up by search engines???
The flash I am interested in is just text (not an image/video) etc and when I ""View Selection Source"" in firefox I do actually see the text, within HTML, that I want to see.
(Interestingly when I view the source for the whole page I do not see the text, within HTML, that I want to see. Could this be related?)
Currently after I have posted my login details, and loaded the HTML back, I see the page which does NOT show the flash HTML (as if I had viewed source for the whole page).
Thanks in advance,
Jim
PS: I should point out that the POST is actually working, my log in is successful.
",https://stackoverflow.com/questions/1471062/c-sharp-webclient-view-source-question,AI
537,How does a site like kayak.com aggregate content? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



Greetings,
I've been toying with an idea for a new project and was wondering if anyone has any idea on how a service like Kayak.com is able to aggregate data from so many sources so quickly and accurately. More specifically, do you think Kayak.com is interacting with APIs or are they crawling/scraping airline and hotel websites in order to fulfill user requests? I know there isn't one right answer for this sort of thing but I'm curious to know what others think would be a good way to go about this. If it helps, pretend you are going to create kayak.com tomorrow ... where is your data coming from?
",https://stackoverflow.com/questions/4607141/how-does-a-site-like-kayak-com-aggregate-content,AI
538,"Screen scraping: getting around ""HTTP Error 403: request disallowed by robots.txt""","
Is there a way to get around the following?
httperror_seek_wrapper: HTTP Error 403: request disallowed by robots.txt

Is the only way around this to contact the site-owner (barnesandnoble.com).. i'm building a site that would bring them more sales, not sure why they would deny access at a certain depth.
I'm using mechanize and BeautifulSoup on Python2.6.
hoping for a work-around
",https://stackoverflow.com/questions/2846105/screen-scraping-getting-around-http-error-403-request-disallowed-by-robots-tx,AI
540,Puppeteer waitForSelector on multiple selectors,"
I have Puppeteer controlling a website with a lookup form that can either return a result or a ""No records found"" message. How can I tell which was returned? 
waitForSelector seems to wait for only one at a time, while waitForNavigation doesn't seem to work because it is returned using Ajax.
I am using a try catch, but it is tricky to get right and slows everything way down.
try {
    await page.waitForSelector(SELECTOR1,{timeout:1000}); 
}
catch(err) { 
    await page.waitForSelector(SELECTOR2);
}

",https://stackoverflow.com/questions/49946728/puppeteer-waitforselector-on-multiple-selectors,AI
541,"Scrapy, scraping data inside a Javascript","
I am using scrapy to screen scrape data from a website. However, the data I wanted wasn't inside the html itself, instead, it is from a javascript. So, my question is:
How to get the values (text values) of such cases? 
This, is the site I'm trying to screen scrape:
https://www.mcdonalds.com.sg/locate-us/
Attributes I'm trying to get:
Address, Contact, Operating hours.
If you do a ""right click"", ""view source"" inside a chrome browser you will see that such values aren't available itself in the HTML.

Edit
Sry paul, i did what you told me to, found the admin-ajax.php and saw the body but, I'm really stuck now.
How do I retrieve the values from the json object and store it into a variable field of my own? It would be good, if you could share how to do just one attribute for the public and to those who just started scrapy as well.
Here's my code so far
Items.py 
class McDonaldsItem(Item):
name = Field()
address = Field()
postal = Field()
hours = Field()

McDonalds.py
from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector
import re

from fastfood.items import McDonaldsItem

class McDonaldSpider(BaseSpider):
name = ""mcdonalds""
allowed_domains = [""mcdonalds.com.sg""]
start_urls = [""https://www.mcdonalds.com.sg/locate-us/""]

def parse_json(self, response):

    js = json.loads(response.body)
    pprint.pprint(js)

Sry for long edit, so in short, how do i store the json value into my attribute? for eg
***item['address'] = * how to retrieve ****
P.S, not sure if this helps but, i run these scripts on the cmd line using
scrapy crawl mcdonalds -o McDonalds.json -t json ( to save all my data into a json file )
I cannot stress enough on how thankful i feel. I know it's kind of unreasonable to ask this of u, will totally be okay even if you dont have time for this.
",https://stackoverflow.com/questions/19021541/scrapy-scraping-data-inside-a-javascript,AI
542,Selenium Scroll inside of popup div,"
I am using selenium and trying to scroll inside the popup div on instagram.
I get to a page like 'https://www.instagram.com/kimkardashian/', click followers, and then I can't get the followers list to scroll down.
I tried using hover, click_and_hold, and a few other tricks to select the div but none of them worked.
What would the best way be to get this selected?
This is what I tried so far:
driver.find_elements_by_xpath(""//*[contains(text(), 'followers')]"")[0].click()
element_to_hover_over = driver.find_elements_by_xpath(""//*[contains(text(), 'Follow')]"")[12]
hover = ActionChains(webdriver).move_to_element(element_to_hover_over)
hover.click_and_hold()
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")

",https://stackoverflow.com/questions/38041974/selenium-scroll-inside-of-popup-div,AI
544,BeautifulSoup get_text does not strip all tags and JavaScript,"
I am trying to use BeautifulSoup to get text from web pages.
Below is a script I've written to do so. It takes two arguments, first is the input HTML or XML file, the second output file.
import sys
from bs4 import BeautifulSoup

def stripTags(s): return BeautifulSoup(s).get_text()

def stripTagsFromFile(inFile, outFile):
    open(outFile, 'w').write(stripTags(open(inFile).read()).encode(""utf-8""))

def main(argv):
    if len(sys.argv) <> 3:
        print 'Usage:\t\t', sys.argv[0], 'input.html output.txt'
        return 1
    stripTagsFromFile(sys.argv[1], sys.argv[2])
    return 0

if __name__ == ""__main__"":
    sys.exit(main(sys.argv))

Unfortunately, for many web pages, for example: http://www.greatjobsinteaching.co.uk/career/134112/Education-Manager-Location
I get something like this (I'm showing only few first lines):
html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd""
    Education Manager  Job In London With  Caleeda | Great Jobs In Teaching

var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-15255540-21']);
_gaq.push(['_trackPageview']);
_gaq.push(['_trackPageLoadTime']);

Is there anything wrong with my script? I was trying to pass 'xml' as the second argument to BeautifulSoup's constructor, as well as 'html5lib' and 'lxml', but it doesn't help.
Is there an alternative to BeautifulSoup which would work better for this task? All I want is to extract the text which would be rendered in a browser for this web page.
Any help will be much appreciated. 
",https://stackoverflow.com/questions/10524387/beautifulsoup-get-text-does-not-strip-all-tags-and-javascript,AI
545,Why does scrapy throw an error for me when trying to spider and parse a site?,"
The following code
class SiteSpider(BaseSpider):
    name = ""some_site.com""
    allowed_domains = [""some_site.com""]
    start_urls = [
        ""some_site.com/something/another/PRODUCT-CATEGORY1_10652_-1__85667"",
    ]
    rules = (
        Rule(SgmlLinkExtractor(allow=('some_site.com/something/another/PRODUCT-CATEGORY_(.*)', ))),

        # Extract links matching 'item.php' and parse them with the spider's method parse_item
        Rule(SgmlLinkExtractor(allow=('some_site.com/something/another/PRODUCT-DETAIL(.*)', )), callback=""parse_item""),
    )
    def parse_item(self, response):
.... parse stuff

Throws the following error
Traceback (most recent call last):
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/base.py"", line 1174, in mainLoop
    self.runUntilCurrent()
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/base.py"", line 796, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 318, in callback
    self._startRunCallbacks(result)
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 424, in _startRunCallbacks
    self._runCallbacks()
--- <exception caught here> ---
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 441, in _runCallbacks
    self.result = callback(self.result, *args, **kw)
  File ""/usr/lib/pymodules/python2.6/scrapy/spider.py"", line 62, in parse
    raise NotImplementedError
exceptions.NotImplementedError: 

When I change the callback to ""parse"" and the function to ""parse"" i don't get any errors, but nothing is scraped. I changed it to ""parse_items"" thinking I might be overriding the parse method by accident. Perhaps I'm setting up the link extractor wrong?
What I want to do is parse each ITEM link on the CATEGORY page. Am I doing this totally wrong?
",https://stackoverflow.com/questions/5264829/why-does-scrapy-throw-an-error-for-me-when-trying-to-spider-and-parse-a-site,AI
546,"Issue scraping page with ""Load more"" button with rvest","
I want to obtain the links to the atms listed on this page: https://coinatmradar.com/city/345/bitcoin-atm-birmingham-uk/
Would I need to do something about the 'load more' button at the bottom of the page?
I have been using the selector tool you can download for chrome to select the CSS path. 
I've written the below code block and it only seems to retrieve the first ten links. 
library(rvest)

base <- ""https://coinatmradar.com/city/345/bitcoin-atm-birmingham-uk/""
base_read <- read_html(base)
atm_urls <- html_nodes(base_read, "".place > a"")
all_urls_final <- html_attr(atm_urls, ""href"" )
print(all_urls_final)

I expected to be able to retrieve all links to the atms listed in the area but my R code has not done so.
Any help would be great. Sorry if this is a really simple question.
",https://stackoverflow.com/questions/56118999/issue-scraping-page-with-load-more-button-with-rvest,AI
547,Close a scrapy spider when a condition is met and return the output object,"
I have made a spider to get reviews from a page like this here using scrapy. I want product reviews only till a certain date(2nd July 2016 in this case). I want to close my spider as soon as the review date goes earlier than the given date and return the items list.
Spider is working well but my problem is that i am not able to close my spider if the condition is met..if i raise an exception, spider closes without returning anything.
Please suggest the best way to close the spider manually. Here is my code:
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from scrapy import Selector
from tars.items import FlipkartProductReviewsItem
import re as r
import unicodedata
from datetime import datetime 

class Freviewspider(CrawlSpider):
    name = ""frs""
    allowed_domains = [""flipkart.com""]
    def __init__(self, *args, **kwargs):
        super(Freviewspider, self).__init__(*args, **kwargs)
        self.start_urls = [kwargs.get('start_url')]


    rules = (
        Rule(LinkExtractor(allow=(), restrict_xpaths=('//a[@class=""nav_bar_next_prev""]')), callback=""parse_start_url"", follow= True),
)


    def parse_start_url(self, response):

        hxs = Selector(response)
        titles = hxs.xpath('//div[@class=""fclear fk-review fk-position-relative line ""]')

        items = []

        for i in titles:

            item = FlipkartProductReviewsItem()

            #x-paths:

            title_xpath = ""div[2]/div[1]/strong/text()""
            review_xpath = ""div[2]/p/span/text()""
            date_xpath = ""div[1]/div[3]/text()""



            #field-values-extraction:

            item[""date""] = (''.join(i.xpath(date_xpath).extract())).replace('\n ', '')
            item[""title""] = (''.join(i.xpath(title_xpath).extract())).replace('\n ', '')

            review_list = i.xpath(review_xpath).extract()
            temp_list = []
            for element in review_list:
                temp_list.append(element.replace('\n ', '').replace('\n', ''))

            item[""review""] = ' '.join(temp_list)

            xxx = datetime.strptime(item[""date""], '%d %b %Y ')
            comp_date = datetime.strptime('02 Jul 2016 ', '%d %b %Y ')
            if xxx>comp_date:
                items.append(item)
            else:
                break

        return(items)

",https://stackoverflow.com/questions/38331428/close-a-scrapy-spider-when-a-condition-is-met-and-return-the-output-object,AI
548,Screen Scraping of Image Links in PHP,"
I have a website that contains many different pages of products and each page has a certain amount of images in the same format across all pages. I want to be able to screen scrap each page's url so I can retrieve the url of each image from each page. The idea is to make a gallery for each page made up of hotlinked images.
I know this can be done in php, but I am not sure how to scrap the page for multiple links. Any ideas?
",https://stackoverflow.com/questions/3261820/screen-scraping-of-image-links-in-php,AI
549,What prevents me from using $.ajax to load another domain's html?,"
My domain:
<!DOCTYPE html>  
<html>
<head>
<title>scrape</title>
<script src=""http://code.jquery.com/jquery-1.7.1.min.js""></script>
</head>
<body>
    <script>
        $.ajax({url:'http://their-domain.com/index.html',
        dataType:'html',
            success:function(data){console.log(data);}
        });
    </script>
</body>
</html>

What prevents me from being able to scrape their-domain? Any work around?
Addendum: thank you all for the suggestions to use a server side script, but I am for the moment interested in solving this problem exclusively using the client.
If I format the request using ""jsonp"" I do at least get a response, but with the following error:""Uncaught SyntaxError: Unexpected token <"". So I am getting a response from their-domain but the parser expects it to be json. (As well it should.) I am hacking through this trying to see if their is a way to trick the client into accepting this response. Please understand that I know this is atypical.
<!DOCTYPE html>  
<html>
<head>
<title>scrape</title>
<script src=""http://code.jquery.com/jquery-1.7.1.min.js""></script>
</head>
<body>
    <script>
        $.ajax({url:'http://their-domain.com/index.html',
        dataType:'jsonp',
            success:function(data){console.log(data);}
        });
    </script>
</body>
</html>

",https://stackoverflow.com/questions/8944656/what-prevents-me-from-using-ajax-to-load-another-domains-html,AI
551,"Why is contains(text(), ""string"" ) not working in XPath?","
I have written this expression //*[contains(text(), ""Brand:"" )] for the below HTML code.


<div class=""info-product mt-3"">
  <h3>Informazioni prodotto</h3>


  Brand: <span class=""brand_title font-weight-bold text-uppercase""><a href=""https://mammapack.com/brand/ava"">Ava</a></span><br> SKU: 8002910009960<br> Peso Lordo: 0.471 kg <br> Dimensioni: 44.00 脳 145.00 脳 153.00 mm<br>

  <p class=""mt-2"">
    AVA BUCATO A MANO E2 GR.380</p>
</div>


The xpath that I have written is not working I want to select Node that contains text Brand:. Can someone tell me my mistake?
",https://stackoverflow.com/questions/71253563/why-is-containstext-string-not-working-in-xpath,AI
552,"Why is this HtmlAgilityPack operation invalid when there are, indeed, matching elements?","
I get ""InvalidOperationException > Message=Sequence contains no matching element"" with the following code:
private void buttonLoadHTML_Click(object sender, EventArgs e)
{
    GetParagraphsListFromHtml(@""C:\PlatypiRUs\fitt.html"");
}

// This code adapted from Kirk Woll's answer at 
   http://stackoverflow.com/questions/4752840/html-agility-pack-c-sharp-paragraph-
   parsing-problem
public List<string> GetParagraphsListFromHtml(string sourceHtml)
{
    var pars = new List<string>();
    HtmlAgilityPack.HtmlDocument doc = new HtmlAgilityPack.HtmlDocument();
    doc.LoadHtml(sourceHtml);
    foreach (var par in doc.DocumentNode
        .DescendantNodes()
        .Single(x => x.Id == ""body"")
        .DescendantNodes()
        .Where(x => x.Name == ""p""))
        //.Where(x => x.Name == ""h1"" || x.Name == ""h2"" || x.Name == ""h3"" || x.Name 
           == ""hp"" || )) <-- This is what I'd really like to do, but I don't know if   
           this is possible or, if it is, if the syntax is correct
    {
        pars.Add(par.InnerText);
    }
    // test
    foreach (string s in pars)
    {
        MessageBox.Show(s);
    }
    return pars;
}

Why is the code not finding the paragraphs?
I really want to find all the text (h1..3 or higher vals, too), but this is a start.
BTW: The html file I'm testing with does have some paragraph elements.
UPDATE
In response to Amy's implied request, and in the interest of full disclosure/ultimate illumination, here is the entire test html file:
<style>
body {
    background-color: orange;
    font-family: Verdana, sans-serif;
}

h1 {
    color: Blue;   
    font-family: 'Segoe UI', Verdana, sans-serif;
}

h2 {
    color: white;    
    font-family: 'Palatino Linotype', 'Palatino', sans-serif;
}

h3 {
    display: inline-block;
}
</style>

<h1>Found in the Translation</h1>
<h2>Bilingual Editions of Classic Literature</h2>
<div><label>Contact: </label><a href=""mailto:axx3andspace@gmail.com"">Found in the Translation</a></div>

<h2><cite>Around the World in 80 Days</cite> by Jules Verne (French &amp; English Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495308081"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51BCZUX2-dL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I0DOYRE"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51BCZUX2-dL._SL160_.jpg"" /></a>

<h2><cite>Gulliver's Travels</cite> by Jonathan Swift (English &amp; French Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495374688"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/517O76OyaWL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I5319ZO"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/517O76OyaWL._SL160_.jpg"" /></a>

<h2><cite>Journey to the Center of the Earth</cite> by Jules Verne (French &amp; English Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495409031"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/41hosXOIw8L._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I6LG25M"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/41qj8DfrihL._SL160_.jpg"" /></a>

<h2><cite>Treasure Island</cite> by Robert Louis Stevenson (English &amp; Finnish Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495418936"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51veMV3OiOL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00IA5V4KC"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51XNUWbA07L._SL160_.jpg"" /></a>

<h2><cite>Robinson Crusoe</cite> by Daniel Defoe (English &amp; French Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495448053"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51QQMRPrP9L._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I9IE8OY"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5128hqiw3DL._SL160_.jpg"" /></a>

<h2><cite>Don Quixote</cite> by Miguel de Cervantes Saavedra (Spanish &amp; English Side by Side)</h2>
<h3>Paperback</h3></br>
<h3>Volume I</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/149474967X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51HqjOPXLVL._SL160_.jpg"" /></a>
<h3>Volume II</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1494803445"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51NONygEMYL._SL160_.jpg"" /></a>
<h3>Volume III</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1494841983"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51G%2BW3ICHkL._SL160_.jpg"" /></a></br>
<h3>Kindle</h3></br>
<h3>Volume I</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HQMWPQ2"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51HqjOPXLVL._SL160_.jpg"" /></a>
<h3>Volume II</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HYN2QGM"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51NONygEMYL._SL160_.jpg"" /></a>
<h3>Volume III</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HLX519E"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51G%2BW3ICHkL._SL160_.jpg"" /></a></br>

<h2><cite>Alice's Adventures in Wonderland</cite> by Lewis Carroll (English &amp; German Side by Side)</h2>
<h3>Coming soon; for now, see:</h3></br/>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/193659420X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5143vIpQ2YL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00ESLTIYQ"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51%2BX0Dy7uNL._SL160_.jpg"" /></a>

<h2><cite>Alice's Adventures in Wonderland</cite> by Lewis Carroll (English &amp; Italian Side by Side)</h2>
<h3>Coming soon; for now, see:</h3></br/>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/193659420X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5143vIpQ2YL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00ESLTIYQ"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51%2BX0Dy7uNL._SL160_.jpg"" /></a>

<h2>Other Sites:</h2>
<p><a href=""http://usamaporama.azurewebsites.net/""  target=""_blank"">USA Map-O-Rama</a></p>
<p><a href=""http://www.awardwinnersonly.com/""  target=""_blank"">Award-winning Movies, Books, and Music</a></p>
<p><a href=""http://www.bigsurgarrapata.com/""  target=""_blank"">Garrapata State Park in Big Sur Throughout the Seasons</a></p>

UPDATE 2
This works (although it is with ""live"" web pages, and not html files saved to disk):
public List<string> GetParagraphsListFromHtml(string sourceHtml)
{
    var pars = new List<string>();
    HtmlAgilityPack.HtmlDocument doc = new HtmlAgilityPack.HtmlDocument();
    doc.LoadHtml(sourceHtml);

    var getHtmlWeb = new HtmlWeb();
    var document = getHtmlWeb.Load(""http://www.montereycountyweekly.com/opinion/letters/article_e333a222-942d-11e3-ba9c-001a4bcf6878.html""); 
    //http://www.bigsurgarrapata.com/ only returned one paragraph
    // http://usamaporama.azurewebsites.net/ <-- none
    // http://www.awardwinnersonly.com/ <- same as bigsurgarrapata
    var pTags = document.DocumentNode.SelectNodes(""//p"");
    int counter = 1;
    if (pTags != null)
    {
        foreach (var pTag in pTags)
        {
            pars.Add(pTag.InnerText);
            MessageBox.Show(pTag.InnerText);
            counter++;
        }
    }
    MessageBox.Show(""done!"");
    return pars;
}

",https://stackoverflow.com/questions/21788078/why-is-this-htmlagilitypack-operation-invalid-when-there-are-indeed-matching-e,AI
554,How do I make a simple crawler in PHP? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    


Closed 3 years ago.










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                        
                    





I have a web page with a bunch of links. I want to write a script which would dump all the data contained in those links in a local file.
Has anybody done that with PHP? General guidelines and gotchas would suffice as an answer.
",,AI
556,Scrapy - Reactor not Restartable [duplicate],"






This question already has answers here:
                        
                    



ReactorNotRestartable error in while loop with scrapy

                                (10 answers)
                            

Closed 3 years ago.



with:
from twisted.internet import reactor
from scrapy.crawler import CrawlerProcess

I've always ran this process sucessfully:
process = CrawlerProcess(get_project_settings())
process.crawl(*args)
# the script will block here until the crawling is finished
process.start() 

but since I've moved this code into a web_crawler(self) function, like so:
def web_crawler(self):
    # set up a crawler
    process = CrawlerProcess(get_project_settings())
    process.crawl(*args)
    # the script will block here until the crawling is finished
    process.start() 

    # (...)

    return (result1, result2) 

and started calling the method using class instantiation, like:
def __call__(self):
    results1 = test.web_crawler()[1]
    results2 = test.web_crawler()[0]

and running:
test()

I am getting the following error:
Traceback (most recent call last):
  File ""test.py"", line 573, in <module>
    print (test())
  File ""test.py"", line 530, in __call__
    artists = test.web_crawler()
  File ""test.py"", line 438, in web_crawler
    process.start() 
  File ""/Library/Python/2.7/site-packages/scrapy/crawler.py"", line 280, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 1194, in run
    self.startRunning(installSignalHandlers=installSignalHandlers)
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 1174, in startRunning
    ReactorBase.startRunning(self)
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 684, in startRunning
    raise error.ReactorNotRestartable()
twisted.internet.error.ReactorNotRestartable

what is wrong?
",https://stackoverflow.com/questions/41495052/scrapy-reactor-not-restartable,AI
562,How to give URL to scrapy for crawling?,"
I want to use scrapy for crawling web pages. Is there a way to pass the start URL from the terminal itself?
It is given in the documentation that either the name of the spider or the URL can be given, but when i given the url it throws an error:
//name of my spider is example, but i am giving url instead of my spider name(It works fine if i give spider name).

scrapy crawl example.com                 

ERROR:

File
  ""/usr/local/lib/python2.7/dist-packages/Scrapy-0.14.1-py2.7.egg/scrapy/spidermanager.py"",
  line 43, in create
      raise KeyError(""Spider not found: %s"" % spider_name) KeyError: 'Spider not found: example.com'

How can i make scrapy to use my spider on the url given in the terminal??
",https://stackoverflow.com/questions/9681114/how-to-give-url-to-scrapy-for-crawling,AI
564,Scrapy CrawlSpider doesn't crawl the first landing page,"
I am new to Scrapy and I am working on a scraping exercise and I am using the CrawlSpider.
Although the Scrapy framework works beautifully and it follows the relevant links, I can't seem to make the CrawlSpider to scrape the very first link (the home page / landing page). Instead it goes directly to scrape the links determined by the rule but doesn't scrape the landing page on which the links are. I don't know how to fix this since it is not recommended to overwrite the parse method for a CrawlSpider. Modifying follow=True/False also doesn't yield any good results. Here is the snippet of code:
class DownloadSpider(CrawlSpider):
    name = 'downloader'
    allowed_domains = ['bnt-chemicals.de']
    start_urls = [
        ""http://www.bnt-chemicals.de""        
        ]
    rules = (   
        Rule(SgmlLinkExtractor(aloow='prod'), callback='parse_item', follow=True),
        )
    fname = 1

    def parse_item(self, response):
        open(str(self.fname)+ '.txt', 'a').write(response.url)
        open(str(self.fname)+ '.txt', 'a').write(','+ str(response.meta['depth']))
        open(str(self.fname)+ '.txt', 'a').write('\n')
        open(str(self.fname)+ '.txt', 'a').write(response.body)
        open(str(self.fname)+ '.txt', 'a').write('\n')
        self.fname = self.fname + 1

",https://stackoverflow.com/questions/15836062/scrapy-crawlspider-doesnt-crawl-the-first-landing-page,AI
566,Nodejs: Async request with a list of URL,"
I am working on a crawler. I have a list of URL need to be requested. There are several hundreds of request at the same time if I don't set it to be async. I am afraid that it would explode my bandwidth or produce to much network access to the target website. What should I do?
Here is what I am doing: 
urlList.forEach((url, index) => {

    console.log('Fetching ' + url);
    request(url, function(error, response, body) {
        //do sth for body

    });
});

I want one request is called after one request is completed.
",https://stackoverflow.com/questions/47299174/nodejs-async-request-with-a-list-of-url,AI
567,How can I use different pipelines for different spiders in a single Scrapy project,"
I have a scrapy project which contains multiple spiders.
Is there any way I can define which pipelines to use for which spider? Not all the pipelines i have defined are applicable for every spider.
Thanks
",https://stackoverflow.com/questions/8372703/how-can-i-use-different-pipelines-for-different-spiders-in-a-single-scrapy-proje,AI
568,Python: maximum recursion depth exceeded while calling a Python object,"
I've built a crawler that had to run on about 5M pages (by increasing the url ID) and then parses the pages which contain the info' I need.
after using an algorithm which run on the urls (200K) and saved the good and bad results I found that the I'm wasting a lot of time. I could see that there are a a few returning subtrahends which I can use to check the next valid url.
you can see the subtrahends quite fast (a little ex' of the few first ""good IDs"") -
510000011 # +8
510000029 # +18
510000037 # +8
510000045 # +8
510000052 # +7
510000060 # +8
510000078 # +18
510000086 # +8
510000094 # +8
510000102 # +8
510000110 # etc'
510000128
510000136
510000144
510000151
510000169
510000177
510000185
510000193
510000201

after crawling about 200K urls which gave me only 14K good results I knew I was wasting my time and need to optimize it, so I run some statistics and built a function that will check the urls while increasing the id with 8\18\17\8 (top returning subtrahends ) etc'.
this is the function - 
def checkNextID(ID):
    global numOfRuns, curRes, lastResult
    while ID < lastResult:
        try:
            numOfRuns += 1
            if numOfRuns % 10 == 0:
                time.sleep(3) # sleep every 10 iterations
            if isValid(ID + 8):
                parseHTML(curRes)
                checkNextID(ID + 8)
                return 0
            if isValid(ID + 18):
                parseHTML(curRes)
                checkNextID(ID + 18)
                return 0
            if isValid(ID + 7):
                parseHTML(curRes)
                checkNextID(ID + 7)
                return 0
            if isValid(ID + 17):
                parseHTML(curRes)
                checkNextID(ID + 17)
                return 0
            if isValid(ID+6):
                parseHTML(curRes)
                checkNextID(ID + 6)
                return 0
            if isValid(ID + 16):
                parseHTML(curRes)
                checkNextID(ID + 16)
                return 0
            else:
                checkNextID(ID + 1)
                return 0
        except Exception, e:
            print ""somethin went wrong: "" + str(e)

what is basically does is -checkNextID(ID) is getting the first id I know that contain the data minus 8 so the first iteration will match the first ""if isValid"" clause (isValid(ID + 8) will return True).
lastResult is a variable which saves the last known url id, so we'll run until numOfRuns is
isValid() is a function that gets an ID + one of the subtrahends and returns True if the url contains what I need and saves a soup object of the url to a global varibale named - 'curRes', it returns False if the url doesn't contain the data I need.
parseHTML is a function that gets the soup object (curRes), parses the data I need and then saves the data to a csv, then returns True.
if isValid() returns True, we'll call parseHTML() and then try to check the next ID+the subtrahends (by calling checkNextID(ID + subtrahends), if none of them will return what I'm looking for I'll increase it with 1 and check again until I'll find the next valid url.
you can see the rest of the code here
after running the code I got about 950~ good results and suddenly an exception had raised -

""somethin went wrong: maximum recursion depth exceeded while calling a
  Python object""

I could see on WireShark that the scipt stuck on id - 510009541 (I started my script with 510000003), the script tried getting the url with that ID a few times before I noticed the error and stopped it.
I was really exciting to see that I got the same results but 25x-40x times faster then my old script, with fewer HTTP requests, it's very precise, I have missed only 1 result for 1000 good results, which is find by me, it's impossible to rum 5M times, I had my old script running for 30 hours and got 14-15K results when my new script gave me 960~ results in 5-10 minutes.
I read about stack limitations, but there must be a solution for the algorithm I'm trying to implement in Python (I can't go back to my old ""algorithm"", it will never end).
Thanks!
",https://stackoverflow.com/questions/6809402/python-maximum-recursion-depth-exceeded-while-calling-a-python-object,AI
569,Detecting honest web crawlers,"
I would like to detect (on the server side) which requests are from bots.  I don't care about malicious bots at this point, just the ones that are playing nice.  I've seen a few approaches that mostly involve matching the user agent string against keywords like 'bot'.  But that seems awkward, incomplete, and unmaintainable.  So does anyone have any more solid approaches?  If not, do you have any resources you use to keep up to date with all the friendly user agents?
If you're curious: I'm not trying to do anything against any search engine policy.  We have a section of the site where a user is randomly presented with one of several slightly different versions of a page.  However if a web crawler is detected, we'd always give them the same version so that the index is consistent.
Also I'm using Java, but I would imagine the approach would be similar for any server-side technology.
",https://stackoverflow.com/questions/544450/detecting-honest-web-crawlers,AI
575,Finding the layers and layer sizes for each Docker image,"
For research purposes I'm trying to crawl the public Docker registry ( https://registry.hub.docker.com/ ) and find out 1) how many layers an average image has and 2) the sizes of these layers to get an idea of the distribution.
However I studied the API and public libraries as well as the details on the github but I cant find any method to:

retrieve all the public repositories/images (even if those are thousands I still need a starting list to iterate through)
find all the layers of an image
find the size for a layer (so not an image but for the individual layer).

Can anyone help me find a way to retrieve this information?
Thank you!
EDIT: is anyone able to verify that searching for '*' in Docker registry is returning all the repositories and not just anything that mentions '*' anywhere? https://registry.hub.docker.com/search?q=*
",https://stackoverflow.com/questions/29696656/finding-the-layers-and-layer-sizes-for-each-docker-image,AI
576,Python: Disable images in Selenium Google ChromeDriver,"
I spend a lot of time searching about this.
At the end of the day I combined a number of answers and it works. I share my answer and I'll appreciate it if anyone edits it or provides us with an easier way to do this.
1- The answer in Disable images in Selenium Google ChromeDriver works in Java. So we should do the same thing in Python:
opt = webdriver.ChromeOptions()
opt.add_extension(""Block-image_v1.1.crx"")
browser = webdriver.Chrome(chrome_options=opt)

2- But downloading ""Block-image_v1.1.crx"" is a little bit tricky, because there is no direct way to do that. For this purpose, instead of going to: https://chrome.google.com/webstore/detail/block-image/pehaalcefcjfccdpbckoablngfkfgfgj
you can go to http://chrome-extension-downloader.com/
and paste the extension url there to be able to download the extension file.
3- Then you will be able to use the above mentioned code with the path to the extension file that you have downloaded.
",https://stackoverflow.com/questions/28070315/python-disable-images-in-selenium-google-chromedriver,AI
579,Writing items to a MySQL database in Scrapy,"
I am new to Scrapy, I had the spider code
class Example_spider(BaseSpider):
   name = ""example""
   allowed_domains = [""www.example.com""]

   def start_requests(self):
       yield self.make_requests_from_url(""http://www.example.com/bookstore/new"")

   def parse(self, response):
       hxs = HtmlXPathSelector(response)
       urls = hxs.select('//div[@class=""bookListingBookTitle""]/a/@href').extract()
       for i in urls:
           yield Request(urljoin(""http://www.example.com/"", i[1:]), callback=self.parse_url)

   def parse_url(self, response):
           hxs = HtmlXPathSelector(response)
           main =   hxs.select('//div[@id=""bookshelf-bg""]')
           items = []
           for i in main:
           item = Exampleitem()
           item['book_name'] = i.select('div[@class=""slickwrap full""]/div[@id=""bookstore_detail""]/div[@class=""book_listing clearfix""]/div[@class=""bookstore_right""]/div[@class=""title_and_byline""]/p[@class=""book_title""]/text()')[0].extract()
           item['price'] = i.select('div[@id=""book-sidebar-modules""]/div[@class=""add_to_cart_wrapper slickshadow""]/div[@class=""panes""]/div[@class=""pane clearfix""]/div[@class=""inner""]/div[@class=""add_to_cart 0""]/form/div[@class=""line-item""]/div[@class=""line-item-price""]/text()').extract()
           items.append(item)
       return items

And pipeline code is:
class examplePipeline(object):

    def __init__(self):               
        self.dbpool = adbapi.ConnectionPool('MySQLdb',
                db='blurb',
                user='root',
                passwd='redhat',
                cursorclass=MySQLdb.cursors.DictCursor,
                charset='utf8',
                use_unicode=True
            )
def process_item(self, spider, item):
    # run db query in thread pool
    assert isinstance(item, Exampleitem)
    query = self.dbpool.runInteraction(self._conditional_insert, item)
    query.addErrback(self.handle_error)
    return item
def _conditional_insert(self, tx, item):
    print ""db connected-=========>""
    # create record if doesn't exist. 
    tx.execute(""select * from example_book_store where book_name = %s"", (item['book_name']) )
    result = tx.fetchone()
    if result:
        log.msg(""Item already stored in db: %s"" % item, level=log.DEBUG)
    else:
        tx.execute(""""""INSERT INTO example_book_store (book_name,price)
                    VALUES (%s,%s)"""""",   
                            (item['book_name'],item['price'])
                    )
        log.msg(""Item stored in db: %s"" % item, level=log.DEBUG)            

def handle_error(self, e):
    log.err(e)          

After running this I am getting the following error 
exceptions.NameError: global name 'Exampleitem' is not defined

I got the above error when I added the below code in process_item method
assert isinstance(item, Exampleitem)

and without adding this line I am getting 
**exceptions.TypeError: 'Example_spider' object is not subscriptable

Can anyone make this code run and make sure that all the items saved into database?
",https://stackoverflow.com/questions/10845839/writing-items-to-a-mysql-database-in-scrapy,AI
581,How do I stop all spiders and the engine immediately after a condition in a pipeline is met?,"
We have a system written with scrapy to crawl a few websites. There are several spiders, and a few cascaded pipelines for all items passed by all crawlers.
One of the pipeline components queries the google servers for geocoding addresses.
Google imposes a limit of 2500 requests per day per IP address, and threatens to ban an IP address if it continues querying google even after google has responded with a warning message: 'OVER_QUERY_LIMIT'.
Hence I want to know about any mechanism which I can invoke from within the pipeline that will completely and immediately stop all further crawling/processing of all spiders and also the main engine.
I have checked other similar questions and their answers have not worked:

Force my scrapy spider to stop crawling


from scrapy.project import crawler
crawler._signal_shutdown(9,0) #Run this if the cnxn fails.


this does not work as it takes time for the spider to stop execution and hence many more requests are made to google (which could potentially ban my IP address)

import sys
sys.exit(""SHUT DOWN EVERYTHING!"")


this one doesn't work at all; items keep getting generated and passed to the pipeline, although the log vomits sys.exit() -> exceptions.SystemExit raised (to no effect)

How can I make scrapy crawl break and exit when encountering the first exception?


crawler.engine.close_spider(self, 'log message')


this one has the same problem as the first case mentioned above.
I tried:

scrapy.project.crawler.engine.stop()


To no avail
EDIT:
If I do in the pipeline:

from scrapy.contrib.closespider import CloseSpider

what should I pass as the 'crawler' argument to the CloseSpider's init() from the scope of my pipeline?
",https://stackoverflow.com/questions/9699049/how-do-i-stop-all-spiders-and-the-engine-immediately-after-a-condition-in-a-pipe,AI
583,Crawling the Google Play store,"
I want to crawl the Google Play store to download the web pages of all the android application (All the webpages with the following base url: https://play.google.com/store/apps/). I checked the robots.txt file of the play store and it disallows crawling these URLs. 
Also, when I browse the Google Play store I can only see top applications up to 3 pages for each of the categories. How can I get the other application pages?
If anyone has tried crawling the Google Play please let me know the following things:
a) Were you successful in crawling the play store. If yes, please let me know how you did that.
b) How to crawl the hidden application pages not visible in top apps for each of the categories?
c) Is there a techniques to download the applications also and not just the webpages?
I already searched around and found the following links:
a) https://code.google.com/p/android-market-api/ 
b) https://code.google.com/p/android-marketplace-crawler/source/checkout 
c) http://mohsin-junaid.blogspot.co.uk/2012/12/how-to-install-android-marketplace.html 
d) http://mohsin-junaid.blogspot.in/2012/12/how-to-download-multiple-android-apks.html

Thanks!
",https://stackoverflow.com/questions/17002181/crawling-the-google-play-store,AI
584,Scrapy Linkextractor duplicating(?),"
I have the crawler implemented as below.
It is working and it would go through sites regulated under the link extractor.
Basically what I am trying to do is to extract information from different places in the page:
- href and text() under the class 'news' ( if exists)
- image url under the class 'think block' ( if exists)
I have three problems for my scrapy:
1) duplicating linkextractor
It seems that it will duplicate processed page.  ( I check against the export file and found that the same ~.img appeared many times while it is hardly possible)
And the fact is , for every page in the website, there are hyperlinks at the bottom that facilitate users to direct to the topic they are interested in, while my objective is to extract information from the topic's page ( here listed several passages's title under the same topic ) and the images found within a passage's page( you can arrive to the passage's page by clicking on the passage's title found at topic page).
I suspect link extractor would loop the same page over again in this case.
( maybe solve with depth_limit?)
2) Improving parse_item
I think it is quite not efficient for parse_item. How could I improve it? I need to extract information from different places in the web ( for sure it only extracts if  it exists).Beside, it looks like that the parse_item could only progress HkejImage but not HkejItem (again I checked with the output file). How should I tackle this?
3) I need the spiders to be able to read Chinese.
I am crawling a site in HK and it would be essential to be capable to read Chinese.
The site:

http://www1.hkej.com/dailynews/headline/article/1105148/IMF%E5%82%B3%E4%BF%83%E4%B8%AD%E5%9C%8B%E9%80%80%E5%87%BA%E6%95%91%E5%B8%82

As long as it belongs to 'dailynews', that's the thing I want.
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.selector import Selector
from scrapy.http import Request, FormRequest
from scrapy.contrib.linkextractors import LinkExtractor
import items


class EconjournalSpider(CrawlSpider):
    name = ""econJournal""
    allowed_domains = [""hkej.com""]
    login_page = 'http://www.hkej.com/template/registration/jsp/login.jsp'
    start_urls =  'http://www.hkej.com/dailynews'

    rules=(Rule(LinkExtractor(allow=('dailynews', ),unique=True), callback='parse_item', follow =True),
           )


    def start_requests(self):
         yield Request(
         url=self.login_page,
         callback=self.login,
         dont_filter=True
         )
# name column
    def login(self, response):
        return FormRequest.from_response(response,
                    formdata={'name': 'users', 'password': 'my password'},
                    callback=self.check_login_response)

    def check_login_response(self, response):
        """"""Check the response returned by a login request to see if we are
        successfully logged in.
        """"""
        if ""username"" in response.body:       
            self.log(""\n\n\nSuccessfully logged in. Let's start crawling!\n\n\n"")
            return Request(url=self.start_urls)
        else:
            self.log(""\n\n\nYou are not logged in.\n\n\n"")
            # Something went wrong, we couldn't log in, so nothing happens

    def parse_item(self, response):
        hxs = Selector(response)
        news=hxs.xpath(""//div[@class='news']"")
        images=hxs.xpath('//p')

        for image in images:
            allimages=items.HKejImage()
            allimages['image'] = image.xpath('a/img[not(@data-original)]/@src').extract()
            yield allimages

        for new in news:
            allnews = items.HKejItem()
            allnews['news_title']=new.xpath('h2/@text()').extract()
            allnews['news_url'] = new.xpath('h2/@href').extract()
            yield allnews

Thank you very much and I would appreciate any help!
",https://stackoverflow.com/questions/31630771/scrapy-linkextractor-duplicating,AI
586,Hide Email Address from Bots - Keep mailto:,"
tl;dr
Hide email address from bots without using scripts and maintain mailto: functionality. Method must also support screen-readers.

Summary

Email obfuscation without using scripts or contact forms

Email address needs to be completely visible to human viewers and maintain mailto: functionality

Email Address must not be in image form.

Email address must be ""completely"" hidden from spam-crawlers and spam-bots and any other harvester type



Desired Effect:

No scripts, please. There are no scripts used in the project and I'd like to keep it that way.

Email address is either displayed on the page or can be easily displayed after some sort of user interaction, like opening a modal.

The user can click on on the email address which in turn would trigger the mailto: functionality.

Clicking the email will open the user's email application.
In other words, mailto: functionality must work.

The email address in not visible or not identified as an email address to bots (This includes the page source)

I don't have an inbox that's full of spam



What does NOT Work

Adding a contact form - or anything similar - instead of the email address

I hate contact forms. I rarely fill up a contact form. If there's no email address, I look for a phone number, and if that's not there, I start looking for an alternative service. I would only fill up a contact form if I absolutely have to.

Replacing the address with an image of the address

This creates a HUGE disadvantage to someone using a screenreader (please remember the visually impaired in your future projects)
It also removes the mailto: functionality unless you make the image clickable and then add the mailto: functionality as the href for the link, but that defeats the purpose and now the email is visible to bots.

What might work:

Clever usage of pseudo-elements in CSS

Solutions that make use of base64 encoding

Breaking up the email address and spreading the parts across the document then putting them back together in a modal when the user clicks a button (This will probably involve multiple CSS classes and the usage of anchor tags)

Alterting html attributes via CSS


@MortezaAsadi gracefully brought up the possibility in the comments below. This is the link to the full - The article is from 2012:
What if We Could Use CSS to Alter HTML Attributes?

Other creative solutions that are beyond my scope of knowledge.


Similar Questions / Fixes

JavaScript: Protect your email address by Joe Maller

(This a great fix suggested by Joe Maller, it works well but it's script based. Here's what it looks like;


<SCRIPT TYPE=""text/javascript"">

  emailE = 'example.com'

  emailE = ('yourname' + '@' + emailE)

  document.write('<A href=""mailto:' + emailE + '"">' + emailE + '</a>')

</script>

<NOSCRIPT>

  Email address protected by JavaScript

</NOSCRIPT>



Looking for a PHP only email address obfuscator function
(A Clever solution using both PHP and CSS to first reverse the email using PHP then reverse it back with CSS) A very promising solution that Works great! But it's too easy to solve.

Is it worth obfuscating email addresses on the web these days?


(JavaScript fix)

Best way to obfuscate an e-mail address on a website?

The selected answer works. It actually works really well. It involves encoding the email as html entities. Can it be improved?
Here's what it looks like;


<A HREF=""mailto:

&#121;&#111;&#117;&#114;&#110;&#097;&#109;&#101;&#064;&#100;&#111;&#109;&#097;&#105;&#110;&#046;&#099;&#111;&#109;"">

&#121;&#111;&#117;&#114;&#110;&#097;&#109;&#101;&#064;&#100;&#111;&#109;&#097;&#105;&#110;&#046;&#099;&#111;&#109;

</A>



Does e-mail address obfuscation actually work?

(The selected answer to this SuperUser question is great and it presents a study of the amount of spam received by using different obfuscation methods.
It seems that manipulating the email address with CSS to make it rtl does work. This is the same method used in the first question I linked to in this section.
I am uncertain what effects adding mailto: functionality to the fix would have on the results.

There are also many other questions on SO which all have similar answers. I have not found anything that fits my desired effect


The Question:
Would it be possible to increase the efficiency (ie as little spam as possible) of the email obfuscation methods above by combining two or more of the fixes (or even adding new fixes) while:
A- Maintaining mailto: functionality; and
B- Supporting screen-readers

Many of the answers and comments below pose a very good question while indicating the impossibility of doing this without some sort of js
The question that's asked/implied is:

Why not use js?

The answer is that I am allergic to js
Joking aside though,
The three main reasons I asked this question are:

Contact forms are becoming more and more accepted as a replacement
for providing an email address - which they should not.

If it can be done without scripting then it should be done without
scripting.

Curiosity: (as I am in fact using one of the js fixes currently) I wanted to see if discussing the matter would lead to a better way of doing it.


",https://stackoverflow.com/questions/41318987/hide-email-address-from-bots-keep-mailto,AI
588,Scrapy - how to identify already scraped urls,"
Im using scrapy to crawl a news website on a daily basis. How do i  restrict scrapy from scraping already scraped URLs. Also is there any clear documentation or examples on SgmlLinkExtractor.
",https://stackoverflow.com/questions/3871613/scrapy-how-to-identify-already-scraped-urls,AI
590,YouTube Data API to crawl all comments and replies,"
I have been desperately seeking a solution to crawl all comments and corresponding replies for my research. Am having a very hard time creating a data frame that includes comment data in correct and corresponding orders.
I am gonna share my code here so you professionals can take a look and give me some insights.
def get_video_comments(service, **kwargs):
    comments = []
    results = service.commentThreads().list(**kwargs).execute()

    while results:
        for item in results['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comment2 = item['snippet']['topLevelComment']['snippet']['publishedAt']
            comment3 = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
            comment4 = item['snippet']['topLevelComment']['snippet']['likeCount']
            if 'replies' in item.keys():
                for reply in item['replies']['comments']:
                    rauthor = reply['snippet']['authorDisplayName']
                    rtext = reply['snippet']['textDisplay']
                    rtime = reply['snippet']['publishedAt']
                    rlike = reply['snippet']['likeCount']
                    data = {'Reply ID': [rauthor], 'Reply Time': [rtime], 'Reply Comments': [rtext], 'Reply Likes': [rlike]}
                    print(rauthor)
                    print(rtext)
            data = {'Comment':[comment],'Date':[comment2],'ID':[comment3], 'Likes':[comment4]}
            result = pd.DataFrame(data)
            result.to_csv('youtube.csv', mode='a',header=False)
            print(comment)
            print(comment2)
            print(comment3)
            print(comment4)
            print('==============================')
            comments.append(comment)
                
        # Check if another page exists
        if 'nextPageToken' in results:
            kwargs['pageToken'] = results['nextPageToken']
            results = service.commentThreads().list(**kwargs).execute()
        else:
            break

    return comments

When I do this, my crawler collects comments but doesn't collect some of the replies that are under certain comments.
How can I make it collect comments and their corresponding replies and put them in a single data frame?
Update
So, somehow I managed to pull the information I wanted at the output section of Jupyter Notebook. All I have to do now is to append the result at the data frame.
Here is my updated code:
def get_video_comments(service, **kwargs):
    comments = []
    results = service.commentThreads().list(**kwargs).execute()

    while results:
        for item in results['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comment2 = item['snippet']['topLevelComment']['snippet']['publishedAt']
            comment3 = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
            comment4 = item['snippet']['topLevelComment']['snippet']['likeCount']
            if 'replies' in item.keys():
                for reply in item['replies']['comments']:
                    rauthor = reply['snippet']['authorDisplayName']
                    rtext = reply['snippet']['textDisplay']
                    rtime = reply['snippet']['publishedAt']
                    rlike = reply['snippet']['likeCount']
                    print(rtext)
                    print(rtime)
                    print(rauthor)
                    print('Likes: ', rlike)
                    
            print(comment)
            print(comment2)
            print(comment3)
            print(""Likes: "", comment4)

            print('==============================')
            comments.append(comment)
                
        # Check if another page exists
        if 'nextPageToken' in results:
            kwargs['pageToken'] = results['nextPageToken']
            results = service.commentThreads().list(**kwargs).execute()
        else:
            break

    return comments

The result is:

As you can see, the comments grouped under ======== lines are the comment and corresponding replies underneath.
What would be a good way to append the result into the data frame?
",https://stackoverflow.com/questions/64275331/youtube-data-api-to-crawl-all-comments-and-replies,AI
591,How to generate the start_urls dynamically in crawling?,"
I am crawling a site which may contain a lot of start_urls, like:
http://www.a.com/list_1_2_3.htm

I want to populate start_urls like [list_\d+_\d+_\d+\.htm],
and extract items from URLs like [node_\d+\.htm] during crawling. 
Can I use CrawlSpider to realize this function?
And how can I generate the start_urls dynamically in crawling?
",https://stackoverflow.com/questions/9322219/how-to-generate-the-start-urls-dynamically-in-crawling,AI
593,Submit data via web form and extract the results,"
My python level is Novice. I have never written a web scraper or crawler. I have written a python code to connect to an api and extract the data that I want. But for some the extracted data I want to get the gender of the author. I found this web site http://bookblog.net/gender/genie.php but downside is there isn't an api available. I was wondering how to write a python to submit data to the form in the page and extract the return data. It would be a great help if I could get some guidance on this.
This is the form dom: 
<form action=""analysis.php"" method=""POST"">
<textarea cols=""75"" rows=""13"" name=""text""></textarea>
<div class=""copyright"">(NOTE: The genie works best on texts of more than 500 words.)</div>
<p>
<b>Genre:</b>
<input type=""radio"" value=""fiction"" name=""genre"">
fiction&nbsp;&nbsp;
<input type=""radio"" value=""nonfiction"" name=""genre"">
nonfiction&nbsp;&nbsp;
<input type=""radio"" value=""blog"" name=""genre"">
blog entry
</p>
<p>
</form>

results page dom:
<p>
<b>The Gender Genie thinks the author of this passage is:</b>
male!
</p>

",https://stackoverflow.com/questions/8377055/submit-data-via-web-form-and-extract-the-results,AI
594,Using one Scrapy spider for several websites,"
I need to create a user configurable web spider/crawler, and I'm thinking about using Scrapy. But, I can't hard-code the domains and allowed URL regex:es -- this will instead be configurable in a GUI.
How do I (as simple as possible) create a spider or a set of spiders with Scrapy where the domains and allowed URL regex:es are dynamically configurable? E.g. I write the configuration to a file, and the spider reads it somehow.
",https://stackoverflow.com/questions/2396529/using-one-scrapy-spider-for-several-websites,AI
595,Removing all spaces in text file with Python 3.x,"
So I have this crazy long text file made by my crawler and it for some reason added some spaces inbetween the links, like this:
https://example.com/asdf.html                                (note the spaces)
https://example.com/johndoe.php                              (again)

I want to get rid of that, but keep the new line. Keep in mind that the text file is 4.000+ lines long. I tried to do it myself but figured that I have no idea how to loop through new lines in files.
",https://stackoverflow.com/questions/43447221/removing-all-spaces-in-text-file-with-python-3-x,AI
598,Send parallel requests but only one per host with HttpClient and Polly to gracefully handle 429 responses,"
Intro:
I am building a single-node web crawler to simply validate URLs are 200 OK in a .NET Core console application. I have a collection of URLs at different hosts to which I am sending requests with HttpClient. I am fairly new to using Polly and TPL Dataflow.
Requirements:

I want to support sending multiple HTTP requests in parallel with a
configurable MaxDegreeOfParallelism.
I want to limit the number of parallel requests to any given host to 1 (or configurable). This is in order to gracefully handle per-host 429 TooManyRequests responses with a Polly policy. Alternatively, I could maybe use a Circuit Breaker to cancel concurrent requests to the same host on receipt of one 429 response and then proceed one-at-a-time to that specific host?
I am perfectly fine with not using TPL Dataflow at all in favor of maybe using a Polly Bulkhead or some other mechanism for throttled parallel requests, but I am not sure what that configuration would look like in order to implement requirement #2.

Current Implementation:
My current implementation works, except that I often see that I'll have x parallel requests to the same host return 429 at about the same time... Then, they all pause for the retry policy... Then, they all slam the same host again at the same time often still receiving 429s. Even if I distribute multiple instances of the same host evenly throughout the queue, my URL collection is overweighted with a few specific hosts that still start generating 429s eventually.
After receiving a 429, I think I only want to send one concurrent request to that host going forward to respect the remote host and pursue 200s. 
Validator Method:
public async Task<int> GetValidCount(IEnumerable<Uri> urls, CancellationToken cancellationToken)
{
    var validator = new TransformBlock<Uri, bool>(
        async u => (await _httpClient.GetAsync(u, HttpCompletionOption.ResponseHeadersRead, cancellationToken)).IsSuccessStatusCode,
        new ExecutionDataflowBlockOptions {MaxDegreeOfParallelism = MaxDegreeOfParallelism}
    );
    foreach (var url in urls)
        await validator.SendAsync(url, cancellationToken);
    validator.Complete();
    var validUrlCount = 0;
    while (await validator.OutputAvailableAsync(cancellationToken))
    {
        if(await validator.ReceiveAsync(cancellationToken))
            validUrlCount++;
    }
    await validator.Completion;
    return validUrlCount;
}

The Polly policy applied to the HttpClient instance used in GetValidCount() above.
IAsyncPolicy<HttpResponseMessage> waitAndRetryTooManyRequests = Policy
    .HandleResult<HttpResponseMessage>(r => r.StatusCode == HttpStatusCode.TooManyRequests)
    .WaitAndRetryAsync(3,
        (retryCount, response, context) =>
            response.Result?.Headers.RetryAfter.Delta ?? TimeSpan.FromMilliseconds(120),
        async (response, timespan, retryCount, context) =>
        {
            // log stuff
        });

Question:
How can I modify or replace this solution to add satisfaction of requirement #2?
",https://stackoverflow.com/questions/57022754/send-parallel-requests-but-only-one-per-host-with-httpclient-and-polly-to-gracef,AI
600,Make a JavaScript-aware Crawler,"
I want to make a script that's crawling a website and it should return the locations of all the banners showed on that page.
The locations of banners are most of the time from known domains. But banners are not in the HTML as an easy image or swf-file. Most of the times a Javascript is used to show the banner.
So if a .swf-file or image-file is loaded from a banner-domain, it should return that url.
Is that possible to do? And how could I do that roughly?
Best would be if it can also returns the landing page of that ad. How to solve that?
",https://stackoverflow.com/questions/8326301/make-a-javascript-aware-crawler,AI
602,Detect Search Crawlers via JavaScript,"
I am wondering how would I go abouts in detecting search crawlers? The reason I ask is because I want to suppress certain JavaScript calls if the user agent is a bot.
I have found an example of how to to detect a certain browser, but am unable to find examples of how to detect a search crawler: 
/MSIE (\d+\.\d+);/.test(navigator.userAgent); //test for MSIE x.x
Example of search crawlers I want to block:
Google 
Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html) 
Googlebot/2.1 (+http://www.googlebot.com/bot.html) 
Googlebot/2.1 (+http://www.google.com/bot.html) 

Baidu 
Baiduspider+(+http://www.baidu.com/search/spider_jp.html) 
Baiduspider+(+http://www.baidu.com/search/spider.htm) 
BaiDuSpider 

",https://stackoverflow.com/questions/20084513/detect-search-crawlers-via-javascript,AI
604,How do I lock read/write to MySQL tables so that I can select and then insert without other programs reading/writing to the database?,"
I am running many instances of a webcrawler in parallel.
Each crawler selects a domain from a table, inserts that url and a start time into a log table, and then starts crawling the domain.
Other parallel crawlers check the log table to see what domains are already being crawled before selecting their own domain to crawl.
I need to prevent other crawlers from selecting a domain that has just been selected by another crawler but doesn't have a log entry yet.  My best guess at how to do this is to lock the database from all other read/writes while one crawler selects a domain and inserts a row in the log table (two queries).
How the heck does one do this?  I'm afraid this is terribly complex and relies on many other things.  Please help get me started.

This code seems like a good solution (see the error below, however):
INSERT INTO crawlLog (companyId, timeStartCrawling)
VALUES
(
    (
        SELECT companies.id FROM companies
        LEFT OUTER JOIN crawlLog
        ON companies.id = crawlLog.companyId
        WHERE crawlLog.companyId IS NULL
        LIMIT 1
    ),
    now()
)

but I keep getting the following mysql error:
You can't specify target table 'crawlLog' for update in FROM clause

Is there a way to accomplish the same thing without this problem?  I've tried a couple different ways.  Including this:
INSERT INTO crawlLog (companyId, timeStartCrawling)
VALUES
(
    (
        SELECT id
        FROM companies
        WHERE id NOT IN (SELECT companyId FROM crawlLog) LIMIT 1
    ),
    now()
)

",https://stackoverflow.com/questions/6621303/how-do-i-lock-read-write-to-mysql-tables-so-that-i-can-select-and-then-insert-wi,AI
605,Passing arguments to process.crawl in Scrapy python,"
I would like to get the same result as this command line :
scrapy crawl linkedin_anonymous -a first=James -a last=Bond -o output.json
My script is as follows :
import scrapy
from linkedin_anonymous_spider import LinkedInAnonymousSpider
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

spider = LinkedInAnonymousSpider(None, ""James"", ""Bond"")
process = CrawlerProcess(get_project_settings())
process.crawl(spider) ## <-------------- (1)
process.start()

I found out that process.crawl() in (1) is creating another LinkedInAnonymousSpider where first and last are None (printed in (2)), if so, then there is no point of creating the object spider and how is it possible to pass the arguments first and last to process.crawl()?
linkedin_anonymous :
from logging import INFO

import scrapy

class LinkedInAnonymousSpider(scrapy.Spider):
    name = ""linkedin_anonymous""
    allowed_domains = [""linkedin.com""]
    start_urls = []

    base_url = ""https://www.linkedin.com/pub/dir/?first=%s&last=%s&search=Search""

    def __init__(self, input = None, first= None, last=None):
        self.input = input  # source file name
        self.first = first
        self.last = last

    def start_requests(self):
        print self.first ## <------------- (2)
        if self.first and self.last: # taking input from command line parameters
                url = self.base_url % (self.first, self.last)
                yield self.make_requests_from_url(url)

    def parse(self, response): . . .

",https://stackoverflow.com/questions/34382356/passing-arguments-to-process-crawl-in-scrapy-python,AI
606,Best platform to host my webscraping backend?,"I am currently building a web scraping application and am in the process of deciding on a platform to host my backend. I am looking for a platform that can handle the resource-intensive nature of web scraping and can support the programming languages and frameworks that I am using (Selenium, BS, requests).

I have been researching various options such as Heroku and Vercel, but webscraping violates their terms of use.

I would like to know what others have found to be the best platform for hosting web scraping backends and why. Additionally, if you have any experience with a specific platform and its ability to handle web scraping, I would appreciate any insights you could provide.

Thank you in advance for your help!",https://www.reddit.com/r/webscraping/comments/10f2j1m/best_platform_to_host_my_webscraping_backend/,program
607,"Web scraping Etsy for best sellers of an Etsy search term, then their top items?","Is this plausible using JavaScript and how legal is doing so? I’ve been learning web dev for a year now and thought this could be a good idea for a portfolio piece but maybe employers would frown upon this?

I wanna make it for me personally as I do hand tool woodwork and thought it would be useful to have this data and then I thought surely I could program that?",https://www.reddit.com/r/webscraping/comments/10f3jx9/web_scraping_etsy_for_best_sellers_of_an_etsy/,program
608,Help with WebScraper.io?,"Before you start, no, I know nothing about programming or writing code. But is there anyone out there who can help write a sitemap for the WebScraper.io Google Chrome extension? I’m unable to download anything new on my laptop so I’m looking for someone who can write the code that I can copy and paste into web scraper io.",https://www.reddit.com/r/webscraping/comments/10dkyb4/help_with_webscraperio/,program
609,You need to verify if you're a person or a bot when using googles advanced parameters.,"I'm writing a movieprogram using webrequests and googles advanced parameters are very handy for this.

For example, you can't search imdb's lists but when you write i.e.:

inurl:https://www.imdb.com/list/ intitle:""heist""

You find plenty of heist lists.
But googles constant verification makes it impossible to use, is there a way to circumvent the verification procedure ?",https://www.reddit.com/r/webscraping/comments/10agca5/you_need_to_verify_if_youre_a_person_or_a_bot/,program
610,"Is there any webscraping bot, to extract booking.com price information?","There are free webscraper with them you can extract Hotel informations, rooms & prices on booking.com, if you pretending your travel date and the Hotel.

What I'm searching is something like a bot, extracting the Hotel price based on random travel periods and dates automatically. 

This data set could be analyzed to find pricing errors.

Is there any tool that can be used that way?
I have no programming skills. How difficult is it to sketch a tool doing this kind of work?",https://www.reddit.com/r/webscraping/comments/105pq5r/is_there_any_webscraping_bot_to_extract/,program
611,Webscraper for job postings hiring physics majors,"Hello, I’m looking for guidance on where to start with designing a program that goes through indeed and finds listings of jobs hiring a physics major to find all the potential jobs that would hire me with my physics major. I have a pretty limited knowledge of python. Thank you!",https://www.reddit.com/r/webscraping/comments/102sas0/webscraper_for_job_postings_hiring_physics_majors/,program
612,How to scrape data that is not visable on the site with Octoparse," Hi. I am new to programming and scripting. Learning now, but I would really need some help right now for a project if somebody would be nice to help me out. When using the Octaporase, it is pretty straight forward with extracting data that is visible on the page. Put when you want to extract data that is hidden, you need to know a bit more scripting I guess. 

I would like to extract the sku number from this site  [https://www.nespresso.com/se/se/order/capsules/vertuo](https://www.nespresso.com/se/se/order/capsules/vertuo) but I don´t know how.

The SKU number is not visible on the website but is in the background e.g( Nb-sku-coffee id=""109877"" where id is the product number) The Sku number is the ID of all product.

Could someone help me and explain how to extract this data that is not on the front page.  


Best regards",https://www.reddit.com/r/webscraping/comments/zz33l7/how_to_scrape_data_that_is_not_visable_on_the/,program
613,Scraping tumblr,"Hi everyone!  
I come to you in a time of need, since I have no idea how scraping or programming works, so I was hoping there's an easier way that I haven't found yet.  
Here's the deal: I need a way to find every post on tumblr that has a specific note. For example, if I were to reblog or like a post, my username would show up underneath and it'd say ""'username' liked this.""   
As you can imagine, there are millions of posts and going through all of their notes manually is impossible, so I figured that:  
a. Either I download the whole thing and somehow search locally on my pc, though it'd probably take a lot;  
b. Find a software that crawls the site and then apply a filter to that username to find the posts they've reacted to.  
Is this doable? Is there anyway someone could do this?  
I'm sorry if I don't make much sense, I have no experience with python and other stuff that is usually used in this sort of things. Thank you so much!",https://www.reddit.com/r/webscraping/comments/zxz7a7/scraping_tumblr/,program
614,Can you program a BeautifulSoup Web scrapper?,Looking for someone who knows how to program and write a web scrapper using the beautifulsoup software. Message me for more details about the web scrapper I'm looking to have created. Thank you!,https://www.reddit.com/r/webscraping/comments/zx324c/can_you_program_a_beautifulsoup_web_scrapper/,program
615,Need industry classification data from website,"Hi, I am looking to scrape industry classification data. Not an expert programmer, posting here as i was unable to find any element to search. Looking for a way forward. At this point i do not even know if this data can be scraped or not.

I have list of entries which will replace DCXINDIA in the URL, would like to scrape the information in the image below.

[https://www.nseindia.com/get-quotes/equity?symbol=DCXINDIA](https://www.nseindia.com/get-quotes/equity?symbol=DCXINDIA)

&#x200B;

https://preview.redd.it/5hhy5uzwqu7a1.png?width=598&format=png&auto=webp&v=enabled&s=368be7d8ae61e531d58d947c6e830386d573a46b

Thanks",https://www.reddit.com/r/webscraping/comments/zuaei6/need_industry_classification_data_from_website/,program
616,ok idk if this has bin done or not,"Selenium scraper that runs off a json file I'm codeing it in python but making it so ya just need to make a Jason string in the correct format and it is basically like an old school punch card therefore you could actually make the instructions using most not all program languages. Or even put together by hand allowing for simple tool that a novice someone who doesn't wanna learn . A whole new language and a whole new API could potentially use.or for larger projects may be able to submit a a json object and it could be ran with the benefit of rotating  proxy's and if needed some experience but without the trouble or cost of having to set it up from scratch .example maybe the customer is a web developer or even just a computer litterat person who can mess around with. Some developer tools on there browser  and figure out. What I think is a straight forward syntax of{'url':'starturl','varfl':'one per line variables in my test case parcel numbers','outfl':'outfile', 'actlst':[[{'id':'elementsid','act':'enter(v)'},{'class_name':'elementclasdname', 'act':'click'}],[this would be the second page of actions . As much as possible I will try to build in things like waiting for the page load and later add randomization selectable by option to void antibot detection that hopefully will have no more configuration then simply switching 'humametrics':from false to true at a very feature less level now. I don't really so far need much in features so I'm trying to get basic functions to be 100% instead of having random glitches but I know some of my future targets are not as easy 
I'm basically writing this cus I want to write apisbut find the gathering of data to supply them with to be a pain",https://www.reddit.com/r/webscraping/comments/ztmjzf/ok_idk_if_this_has_bin_done_or_not/,program
617,"As AI tools have quickly become a controversial topic in the game industry, modl.ai aims to make AI tools that support developers, not supplant them. Get more details here:",[https://www.gamedeveloper.com/programming/using-ai-bots-as-game-development-tools-not-replacements-with-modl-ai](https://www.gamedeveloper.com/programming/using-ai-bots-as-game-development-tools-not-replacements-with-modl-ai),https://www.reddit.com/r/Automate/comments/103wd0o/as_ai_tools_have_quickly_become_a_controversial/,program
618,Text to video,"What are programs that make it easy to find video or photo clips to pair with an audio file so that I can turn the audio file into a video?  I also want to add background music and sound effects. The audio files are 5 mintues stories are about different animals, trips, and experiences.",https://www.reddit.com/r/Automate/comments/100f0pm/text_to_video/,program
619,Automated batch tool for making preview images of 3D models?,"Is there a batch tool, program or script that could take many 3D model files and automatically create a few PNG/JPEG images of different angles for each one?",https://www.reddit.com/r/Automate/comments/zn5qms/automated_batch_tool_for_making_preview_images_of/,program
620,Customer Service task automation,"Forgive me if my question is stupid, but I want to know if it's possible to automate my tasks. I know there's MS PAD but I don't know if it'll work in my case as it is mixture of both desktop apps and web apps. 

So, my task is to pickup tickets on Edesk and respond to our eBay and Amazon customers. We copy the ticket # add it to our Excel tracker. Afterwards, we respond to the customer's message. There are message templates that we use but we still need to edit around it to match the customers concern. After responding, we copy both our response and our customer's message in a notepad and paste it to our internal API for documentation. As you can see it's already menial and boring. How do I automate this? Is there a macro like program that records what you do and simply runs it for you afterwards? Any suggestions are appreciated.",https://www.reddit.com/r/Automate/comments/zn69cn/customer_service_task_automation/,program
621,Best way to sort a dumps of thousands of photos?,"Anyone have a process they use to sort 1000s of unsorted photos? Cosplay rip downloaded from a site is like 4000 photos all in one folder. I'd like to get all the photos organized into the character cosplayed or the shoot. Other than manually going through and moving to folder anyone have a program, app, script or something they are aware of which can determine similar photos (using AI maybe) and sort them?

Thanks!",https://www.reddit.com/r/Automate/comments/zbp2hp/best_way_to_sort_a_dumps_of_thousands_of_photos/,program
622,I’m relatively new to robotics,"Not entirely sure if this is the correct place for this, but the previous one I was in didn’t accept people new to robotics. I’m trying to learn how to build and program any kind of Ai/robot? Does anybody have any kind of suggestion on where to start, the best places to learn the skills on programming, and overall just learn more about robotics in general?",https://www.reddit.com/r/Automate/comments/zbja9a/im_relatively_new_to_robotics/,program
623,Best open-source/self-hosted automation tools,"I played a lot with multiple automation tools in the last two weeks. I was focused on finding open-source and free solutions.

I narrowed down my research to these 5 open-source/free automation tools that you can use right away. Two of these are ready-to-use Zapier alternatives.

1- **Huginn** \- Huginn is a system for building agents that perform automated tasks for you. It’s like creating your own personal assistant, but without the need to learn a programming language.

Hosting: Self-hosted.

2- **automatisch** \- It is an automation tool that lets you easily create workflows in your web browser with no coding knowledge required. It's a user-friendly Zapier alternative.

Hosting: Self-hosted (for now).

3- **n8n** \- N8n is an open-source, no-code automation tool that lets you quickly create workflows with its drag-and-drop interface. It is based on nodes so you can connect anything to everything. The best Zapier alternative I've seen.

Hosting: Desktop, hosted, & self-hosted.

4- **Beehive** \- This is similar to how Huginn works. It's an event and agent system. Agents are triggered by events and perform their actions. There are multiple integrations (called Hives).

Hosting: Self-hosted.

5- **Power Automate** \- Microsoft's official no-code automation tool. It allows you to create flows for automating your tasks. It also has a desktop version (included in Windows 11). You can automate almost anything with it.

Hosting: Desktop and hosted (cloud flows - not free).

I have created some flows to automate Google Chrome using Power Automate, which works nicely. I am also using the n8n desktop version to connect multiple services I use.

\---

Links to these tools are in the comments.",https://www.reddit.com/r/Automate/comments/z6b5w9/best_opensourceselfhosted_automation_tools/,program
624,Automate youtube uploads? (AI/generated content on demand),"A perfect example (many of you will likely have seen this channel):

[https://www.youtube.com/@RoelVandePaar/videos](https://www.youtube.com/@RoelVandePaar/videos)

He uploads every minute, his software takes text from web forums (questions and answers) and compiles it into a video presentation, with a pre-recorded intro. Though this many seem a nuisance, his program has managed to produce great tutorials on fixing technical problems.

I would like to do something similar (though specific to a community), basically taking info from the web and putting into a unique video format, with relevant images. Can anyone help me with the basic idea? Would screenscrappers, API or a single script (such as with python) be able to do this? Any information would be an enormous help.",https://www.reddit.com/r/Automate/comments/z0d3wd/automate_youtube_uploads_aigenerated_content_on/,program
625,I built an AI() custom function for Google Sheets,"This function lets you do simple things like classification, and text/copy generation. 

But the really cool thing is it can be used as a general natural language programming tool. Don't know how to TitleCase a bunch of data in Sheets? Just do AI(""Titlecase this:""&CELLNUMBER).

Check it out and let know if you find it useful! https://www.abiraja.com/blog/natural-language-programming-in-google-sheets

Instructions for running this on your own sheets are in the blogpost above ^",https://www.reddit.com/r/Automate/comments/ywhnqw/i_built_an_ai_custom_function_for_google_sheets/,program
626,Record user input via a screen capture AI?,"Hi guys, I'm very new to automation and coding in general, I'm not sure if I asked my question right but I'm imagining of a program that initially records a user's mouse movement/keyboard inputs for a time and then save it as a sort of script so when I run it next time, it does the recorded movement by itself. Is it possible?

There isn't a task I'm handling currently that relates to this, so it's more simply of a random thought instead of a task I want to automate, it might bring some sort of relevance in the near future though

I've heard of keylogging but it only records a log of the user input but the mouse input is only collected through means unrelated to screen capture(at least how I' think it to be, didn't have firsthand experience with it)

So yeah. To summarize, I'm asking for the possibility of a program that can :

1 - record a user input through screen capture

2 - save it as a script

3 - run and execute recorded movement captured",https://www.reddit.com/r/Automate/comments/yul3ha/record_user_input_via_a_screen_capture_ai/,program
627,Automate screen events,"I have been using macros for a really long time, when i was a kid i developed vbasic macros to automate farming in my games, i'm a big fan of logitech / razer / any peripheal with the ability to set macros .. (\*\*\* insert you know i'm something of a developer myself joke \*\*\*).   


Anyways I allways enjoyed scripting but at the end of the day those inputs require human interaction to activate.... I know this is not strictly true because you always can use an api to retrieve recent information and activate those macros... but its painful to build an api for every game, chart software or website.  


My question is, its 2022, is there any nice machine learning screen recognition library or framework? do you guys know where should I start looking? Im open for any language  


What I want to build is a software with that library that allows me to:  
\- Input the screen pattern to activate  
\- Set up a script to use when the pattern is detected  
\- Listen to all screen events  
\- input a screen pattern to stop  
\- the program stops and continue listening  


Is that approach valid? Is it too bad for computer performance? is there any better way to automate scripts without reading screen events?  


Thanks all and excuse my english.",https://www.reddit.com/r/Automate/comments/ypkg3e/automate_screen_events/,program
628,Automate Mac app workflow,"Hey! Im a programmer that have a specific setup of windows and applications up when working on a specific project. 
How could I automate the setup of this?

What I want to achieve: run a script to open up vs code, run script in terminal, open up a terminal and go to a specific path and run scripts etc. place windows in a certain way.

I’m using a Mac.",https://www.reddit.com/r/Automate/comments/ymno4f/automate_mac_app_workflow/,program
629,Help with data graphing,"I'm trying to accomplish  the goal of automating the process creating a report that pulls:

The high and low points in activity  in the last 24 hours and 30 days

The average amount of activity lost the last 24hrs and 30 days

It's able to to generate that report at certain times of the day and a click of button

Ideally the report is generated in Excel

This is all being pulled from a site that has live data graphs.


Any help would be appreciated, it's safe to say I don't have alot of programming experience. I'm struggling with this and could use any help!",https://www.reddit.com/r/Automate/comments/ym0v4y/help_with_data_graphing/,program
630,Advice for learning to automate web interactions?,"I’m working on a project to download YouTube videos of songs and analyze their melodies. I would like to automate the process of:

1. Uploading a local MP3 file to https://basicpitch.spotify.com/ based on filename programmatically 
2. When processing is complete, pop open the parameters to tweak 
3. Adjust certain values on the slider to a pre-set point (ideally, but optional)
4. Download the resultant MIDI file from the web page

Can anyone describe how to accomplish this and/or point me to resources on learning this sort of automation? I have Windows and Mac machines at my disposal. Thank you!",https://www.reddit.com/r/Automate/comments/yc1fz2/advice_for_learning_to_automate_web_interactions/,program
631,Process mining as a non-data scientist," Hi all,

For the past years I have been working in finance (and operations) for a mid sized corporation. I was introduced to process mining a while ago (just in presentation form). It also popped up in a recent job I am interviewing for. Both times people referred to Celonis as the software of choice.

I am wondering whether picking up process mining as a non-data scientist (university finance degree and a love for IT) is doable? I mostly worked with excel (not a specialist, but proficient) for finance purposes and do not consider myself to be a data scientist. I have no experience in programming outside of some light VBA work. Would picking up software like Celonis be relatively okay or would this be a huge challenge? Is process mining more aimed towards true data scientists or is this also applicable to more general business analysts?

Thanks in advance for some insight!

Best,

Brandermant",https://www.reddit.com/r/Automate/comments/y8fm48/process_mining_as_a_nondata_scientist/,program
632,Why the heck are Rexroth controllers so popular?,"Many industrial robots in my area are constructed with Rexroth motion control products. Is there any particularly reason, beyond the programming flexibility?",https://www.reddit.com/r/Automate/comments/y33ayo/why_the_heck_are_rexroth_controllers_so_popular/,program
633,Windows automation: monitoring a logging software?,"Looking for a program that detects any change on selectable part of the screen and keeps a log

Some time ago I had found a software who had many monitoring abilities. One of them was allowing me to select a portion if the screen, record its value, and detecting any change occurring over time. The software also allowed to record all the events (in this case, when the screen portion changes) in a logfile, with time and date.

Any suggestion on a similar software?",https://www.reddit.com/r/Automate/comments/y39axs/windows_automation_monitoring_a_logging_software/,program
634,How to automate slicing of 3D models,"I am looking for a program that can automate slicing of 3D models or at least a program that can estimate print times and filament usage for use in a website. Right now I have to take the file, Manually slice then upload the file to my site for it to parse out this data.   


Is there a slicing program that either has an API I can use (I can host any of these programs locally on my server) or is there a way to autoslice using Prusa Slicer that I am missing?  


The only thing I really need is a program that can take a 3D model (Either STL or OBJ) and estimate a print time and Filament usage in grams. I can handle the rest. I'm just looking for a faster way to make quotes other than doing them all by hand",https://www.reddit.com/r/Automate/comments/y25bp9/how_to_automate_slicing_of_3d_models/,program
635,Industrial SW engineering UX stone age?,"At my workplace there are lots of different PLCs and other automation equipment which have one thing in common. Catastrophical user experience for engineers who develop and maintain the software (programming PLCs and robots etc.).

Yaskawa MPE720 came out of the stone age, it can not even assign the memory to the variables.  It has to be done manually. Epson RC+ looks like it has been made around '95. Staübli SRS is somehow OK. The best so far I have used is Beckhoff PC PLC - programming environment is MS Visual Studio XAE with all of it's bells and whistles. 

So, which PLC system would be the most contemporary considering SW engineer's user experience? I dream of something that could be programmed in VS Code, with GIT version control and with worker user interfaces built in HTML5 or with some other standard framework?",https://www.reddit.com/r/Automate/comments/xj14f7/industrial_sw_engineering_ux_stone_age/,program
636,Greenhouse automation project,"I have Automated greenhouse as my uni project and need help with heating/cooling mode transition. I know that program can't just go from heating to cooling and backwards, as it could result in great energy losses, but there is a great chance that on a sunny winter day you would need cooling mode during the midday and heating when sun goes down. 

So my question is: What conditions should I set for heating and cooling mode, so I can get smooth transitions during the day, and should I add something like standby mode when temperature is in it's setpoint?",https://www.reddit.com/r/Automate/comments/xhmen3/greenhouse_automation_project/,program
637,Help with automatic extraction of data from PDFs,"I have 4 PDFs with menus for the different messes in my college. I want to extract and add them to a database. I tried using the pdf2txt program in Linux but the extracted text was not arranged in a manner that I could easily separate menus for various meals of a week. As a temporary measure, I took screenshots of the various menus and passed them into OCR software but it is very time-consuming. Hence, I would like to automate the process. Is there any way programmatically or otherwise that I can easily extract the data? The PDFs in question can be found [here](https://drive.google.com/drive/folders/1QL7kaqUy7nTFG5BWYOsfqLk-sAYFA90W?usp=sharing) (Google Drive link)",https://www.reddit.com/r/automation/comments/10dhn2g/help_with_automatic_extraction_of_data_from_pdfs/,program
638,Does a Bachelors degree open more doors?,"I work in medical manufacturing and decided to go to school for Automation. I’m currently in an Associates program for Automation and Robotics Engineering. Most postings for Automation jobs only require a two year degree, but I was wondering if anyone who’s currently an Automation Engineer has any insight into whether turning it into a bachelors degree would be worth it?",https://www.reddit.com/r/automation/comments/107ppe8/does_a_bachelors_degree_open_more_doors/,program
639,"Just getting into browser automation, need advice",Currently using axiom to do some lead generation and simple task. I defiantly see the possibilities but am finding there's not a ton of info online about which programs are the best. If I just used Zapier and Axiom would that be enough to accomplish anything I want to do in terms of marketing and sales data automation or is there other programs I should be looking at?,https://www.reddit.com/r/automation/comments/106zj7u/just_getting_into_browser_automation_need_advice/,program
640,Help automating file saving,"Hi all - 

I am looking to get help automating saving a bunch of files, individually. I have an excel sheet that I need to print line by line 100+ times to PDF on a regular basis. It is cumbersome to do this all the time. I have a working macro in excel that will highlight and print each line, but I have to manually add the name and save it. Is there a program or something that I should download that will allow me to write a macro for everything, not just excel, ideally integrated with excel? Or is there a way to do it without having to download 3rd party software?

TIA",https://www.reddit.com/r/automation/comments/1056jkd/help_automating_file_saving/,program
641,Development Environments,"Hey all, we have an automation company customer specializing in manufacturing systems. They work with all the industry standard tools and controllers. 

Their in-house development is using windows 10 VMs on a VMware host to create POC including programming their controllers and PLC code in house. 

We are looking for a solution to streamline the development process. The hardware on prem is old and we are putting together some paths forward. 

What are others using to develop for customer solutions? We are an IT consulting firm performing an assessment and we wanted to see what others in our industry are doing. Any insight is appreciated. 

One constraint is that they have to bring laptops onsite for the POC and sometimes there is no internet. 

Thanks!",https://www.reddit.com/r/automation/comments/zz6qdh/development_environments/,program
642,How Does ChatGPT Help When Writing Automation Code With the Playwright Tool?,"In its first week of use, [ChatGPT](https://openai.com/blog/chatgpt/) smashed numerous Internet records. As someone who works in QA automation, my first idea was how I could use this wonderful platform to make it simpler for testers to work on Web automation.

As ChatGPT may be used to write code in a variety of programming languages and technologies. After more investigation, I made the decision to create some scenarios using it. *I used ChatGPT to develop the Cucumber feature file and various use cases based on UI situations.*

**Please follow the** [**link**](https://qaautomationlabs.com/chatgpt-for-automated-testing-for-playwright/) **for more detail**

*We can use ChatGPT to generate the Code but we can’t say that the generated code is perfectly fine. The generated code needs to be modified in some situations.*",https://www.reddit.com/r/automation/comments/zz0plw/how_does_chatgpt_help_when_writing_automation/,program
643,Has anyone ever worked with software from Survalent for SCADA systems?,"I am currently working on a SCADA for a shrimp farm, the software we are using is from Survalent. The map editor is SmartVU, and the program to link the variables is STC Explore. 

There is no information anywhere on the internet on how to use this software and I’m having a hard tome figuring it out.",https://www.reddit.com/r/automation/comments/zxcwg7/has_anyone_ever_worked_with_software_from/,program
644,Electrical/Control & Instrumentation Engineer to Automation Engineer,"I'm an electrical engineer with a couple of years experience behind me. Previously I was an electrician before pursuing my degrees to become an engineer. Based in Ireland. 

I've always loved automation, I excelled at it in college, plc programming, logic, robotics, but I ended up going down the path of an electrical design engineer mostly since leaving college in 2018, most recently I'm also a control & Instrumentation Engineer. I've been working in proximity to automation engineers throughout some projects and really like the idea of doing the role everyday.

What I'm struggling with is how or can I break into an automation engineer role? I've looked at 100s of roles for automation and I don't see any point in applying as I don't meet the criteria but I know I'd love doing all of the responsibilities! What can I do? I can't think of anything other than completing some PLC dojo courses etc but that still doesn't give me the experience required.

Edit: going back to college full time is out of the question, I've already done this to acquire two degrees. I would be willing to do a night time course but there are none I can see, there are some in mechatronics but I'm not sure I'm willing to offload €10k to get a 2nd level 8.",https://www.reddit.com/r/automation/comments/zl4w3x/electricalcontrol_instrumentation_engineer_to/,program
645,Starting School,"My company is putting me through a technical program at the local community college. For the entire duration of the program I will be using SQL and statistics in online college classes. February through March I'll be going through in-person classes on FANUC robotics and PLCs, as well as some GD&T/Schematic reading stuff at the applied technologies center.
  Not sure if anyone here is in manufacturing automation but I'm a little intimidated since my only technical work so far has been using SQL and Excel for some basic data work. 
  Any tips on the learning process? Or does this program even sound like it's covering enough information?",https://www.reddit.com/r/automation/comments/zkarwx/starting_school/,program
646,EIT automation online school opinions.,"Hello!

I'd like to further my education. I'm a mechatronic engineer, but worked mostly as a maintenance technician, in the mechanical side. I'd want to pursue a career in automation, but the only chances I have are online schools. I live and work in a little city in Bolivia, in latin america. 

Here is a link to some of EIT's programs.

[https://www.eit.edu.au/?post\_type=courses&s=&course-types%5B%5D=professional-certificate](https://www.eit.edu.au/?post_type=courses&s=&course-types%5B%5D=professional-certificate)

If anyone know about this EIT, and their programs, would you please give me your opinon. If anyone knows about another online program which grants some type of certificate in conjunctions with the learning, please I'd like to know. I know there is a huge gap between using a simulator than a real PLC, but I have a PLC and some stuff that would let me install a tiny system. Automation is huge, but Im more intereted in system dynamics and control (PID controllers, PLC/DCS, plant design and mostly industrial automation).

Thanks and cheers.",https://www.reddit.com/r/automation/comments/zcg63l/eit_automation_online_school_opinions/,program
647,Browser Automation Scheduling for Average Joe?,"I am looking for a free or cheap software for Mac (chrome or safari) that would allow me to set up automated waiver wire pickups for fantasy football, right after waivers are run (like 3:00am lol). I know programs like Selenium exist, but I don’t know how to code, so I can’t create my own program.",https://www.reddit.com/r/automation/comments/z091lj/browser_automation_scheduling_for_average_joe/,program
648,Help with Report automation,"I'm trying to accomplish  the goal of automating the process creating a report that pulls:

The high and low points in activity  in the last 24 hours and 30 days

The average amount of activity lost the last 24hrs and 30 days

It's able to to generate that report at certain times of the day and a click of button

Ideally the report is generated in Excel

This is all being pulled from a site that has live data graphs.


Any help would be appreciated, it's safe to say I don't have alot of programming experience. I'm struggling with this and could use any help!",https://www.reddit.com/r/automation/comments/ym0wn7/help_with_report_automation/,program
649,Duplicate Tickets," 

Hi, i work as app support.

I would like to delete duplicate tickets automatically.

Can someone guide me on this.

I can do some programing (Front end , and a little bit of python). But i dont seem to have any idea on how to do it.

The rough idea now i have is to use DOM and get the duplicate tickets close. If there is a faster way,it would be better or maybe a structure i can follow. Can some1 help me on this . Thanks",https://www.reddit.com/r/automation/comments/yeecop/duplicate_tickets/,program
650,Rockwell Learning+ material,"Would anyone happen to have copies or ability to send some educational material or guides that would possibly work with the learning+ classes? ie. 500, 5000 programming and troubleshooting?",https://www.reddit.com/r/automation/comments/y1y0mt/rockwell_learning_material/,program
651,Automation of windows program,"Hey Guys, am trying to automate a windows program. It has a input field and gives the elevation value after entering certain coordinates in it. Currently I am using python for it and its working great. But am using pyautogui.locateonscreen and it does not work well on other monitors. So is there any windows automation language or anything that I can use for my purposes. 
Thanks!",https://www.reddit.com/r/automation/comments/xwerjf/automation_of_windows_program/,program
652,Anyway to automate or macro this game modding process?,"So I'm part of a modding community for older NHL  games. One of the things we do is remove retired or young player portraits from the game and update with new images.  This is a time consuming process that takes 5-10 minutes per player. I appreciate we could probably get an app build but wanted some ideas about how we could automate speed up the process.  Its time consuming because we need to decrypted extract files make changes in Photoshop then rebuild. I have copied the instructions below but note its missing images as its a PDF. 

&#x200B;

Step 1: Find portrait ID  
Ex: McDavid, Portrait ID 9857  
Step 2: BIG File Extractor  
Open BigF  
Step 3: Find portrait  
Note, there are two locations. USRDIR\\fe\\ion\\artassets\\playerheads and  
USRDIR\\fe\\ion\\artassets\\playerheadssmall  
Both of these will need to be updated. One is for menu, one is for in-game.  
There are 3 folders within to split up the portraits, and one for blank silhouettes  
In this case, McDavid is in p8001\_12000, as we are looking for p9857.  
After selecting it, this should be your screen:  
These files all have their own use, and the names are all different depending on the portrait.  
Extract all to a new folder, I name it the portrait ID.  
We are looking for the Refpack compression that is not file 0. It is always the file with the biggest file  
size. In this case, it is file 3512.  
Step 4: QuickBMS decompression  
Open up QuickBMS and use the script refpack.bms  
Select the file name we found  
Save it in the same folder.  
Step 5: Photoshop  
Now there is a .dds file to edit. Open it up in photoshop.  
This is what we get  
Looks just like what is in the game with one difference – the black background. This is solved by  
using an alpha layer so we get what appears in the game:  
We’ll keep this in mind for later. For now, lets replace it with an updated portrait. Make sure the  
bottom of the picture cuts off where it did in the original.  
Now, lets get back to the alpha layer.  
In the bottom right corner in Photoshop, where you can add/change layers, there is a “channels”  
menu. Select it  
There are 5 layers. RGB, Red, Green, Blue, and Alpha. Select the alpha layer.  
Lets make it completley black. I use Alt+Backspace as a shortcut to fill the image black.  
Now, go back to layers, then hold Ctrl+Left click on the thumbnail for the portrait to select it  
Go back to Channels and fill that selected part in with white. I use Ctrl+Backspace to fill it in.  
Notice now if you turn on the visibility for RGB (and keep Alpha visible) there is a red tint. This will  
not show up in the game, but the rest of the image will.  
Save the .dds, you can overwrite the old one if you want, or save it as a new file. But make sure you  
use these settings:  
Step 6: QuickBMS Compression  
Open QuickBMS with the refpack\_compress.bms script. Select the .dds file you just saved  
Save that in the same folder.  
Now, you notice the folder contains a new file.  
Delete the original file, in this case 3512. Then, rename the compressed dds file to be that name,  
with no extension. You can also delete the unpacked dds file. Your folder should look like this:  
Step 7: Rebuild .big  
Back in the BIG File Extractor, select Rebuild BIG. Select the folder that you were working in.  
Save the file in the folder that held the original .big file. Sometimes this program crashed when  
rebuilding, I have found that to reduce that crash you rename the file so it saves as a new file rather  
than overwrite the original one. There is still a chance the rebuild can crash. If it does, simply try it  
again with a new name.  
Once it is done, you should see this  
If your new file has a file size of 0, there was an error in rebuilding the .big file. Try to do it again.  
If it has worked, simply delete the original file and rename your file to be the same as the original.  
Done! Check it out in-game!  
Remember that this was the first of two files to replace. The other one is in playerheadssmall. It is  
what is used in the actual game, not in the menus. You have to go through these same steps to  
replace that one, as the file size is different.",https://www.reddit.com/r/automation/comments/xsoo9l/anyway_to_automate_or_macro_this_game_modding/,program
653,What makes my control-system real-time?,"I'm currently working on my thesis for uni. In my project I control a virtual vision-based robot cell (using RoboDK) and control it using TwinCAT. These are communicating via OPC UA (using Python API).

An algorithm written in Python as well calculates and plans the path for the robot based on the information from the simulated 2D camera.

The system runs on windows.

Does all this mean that my robot control is real-time? (Because my robot program is not pre-compiled and uploaded but the trajectory is calculated while running.) Afterall all my system-components exist on a PC virtually and there is no hardware in the loop.

If not, what term would you use to describe this type of control? What makes it non-real-time? (Using windows makes it impossible to control in real-time?) Does the TwinCAT runtime (or PLC cyclic control in general) make it real-time?",https://www.reddit.com/r/automation/comments/xmu7vm/what_makes_my_controlsystem_realtime/,program
654,Getting into manufacturing automation at a small business-looking for resources,"I run a small business with my partner and we are getting a small manufacturing line going for our product. We have a system for getting boxes/cartons printed, creases, and cut (either in advance or on-demand).

I've been looking around at YouTube videos and various other automated equipment websites trying to reverse engineer some of this stuff. Are there other resources I should look into?

For reference: im an electrical and mechanical engineer with heavy programming experience, so I can build a lot of systems, im just looking for reference on how to build reliable systems of this nature. Thanks in advance for the help!",https://www.reddit.com/r/automation/comments/xjoovm/getting_into_manufacturing_automation_at_a_small/,program
655,Where should I start?,"Hey guys, in short, I want to create an automation that can download course documents/slides from our school’s website. I have only little knowledge about programming. What language should I learn, where should I start, and how long would it take to learn and build an automation that can do that.",https://www.reddit.com/r/automation/comments/xe9y5x/where_should_i_start/,program
656,Automate Progress Reports for Teacher?,"Greetings All. My wife is a teacher, and her school requires teachers to do weekly progress reports for their students. Last night I was helping her set up this year's forms (63 total), which involved inputting the student's name, the appropriate teacher's name, and the date. Unfortunately, this is a largely manual process, requiring her to manually tally the number of missing assignments and transpose the details for each into its appropriate box on the form on a weekly basis (this eats up between 2 and 3 hours, since she has to switch back and forth between pages). She commented that she wished this sort of project could be automated, and it got me thinking about options. Unfortunately, her school is cheap as hell and will not invest in any gradebook applications (they rely solely on G-Suite, using a shared Google Sheets workbook for grade tracking (I know, lol)). 

What I would like to do is come up with a way to automatically fill in the student's details to the form (right now it is a Google Doc with crazy formatting, would love to move it to Word or PDF it but I need to maintain compatibility with G-Suite), date the form, and then tally the number of assignments missing for each class and load it into the  appropriate box on the form. Then I would need to automatically save the output so it is ready to print, while maintaining an archive of the file.

I used to work for a company that used a program that ran VBS scripts to generate the day's reports in a variety of formats (some Word, some Excel, a couple of PDFs). Ideally I would love to do something similar. However, I am learning as I go. So, I am curious what everyone would recommend for achieving this goal (e.g. do I need a program, do I just need scripts, what language, etc.). My hope is to give her an option that scrapes the master gradebook on command and carries the data to another spreadsheet (in case any students are added later we won't have to tweak the program), then carries over the student's data to the appropriate fields, then looks at the line of grades and reports on number of missing assignments (they score 0-4, with 0 being missing and anything 1 or above being turned in, so this should be a simple item to report on). The output would then export as either an editable document, a PDF, or both. If anyone has any external sites that they would recommend I research, please post the links! I'm not looking for everyone to do this for me, I just need an idea of where to start. Thanks!",https://www.reddit.com/r/automation/comments/x9yzbt/automate_progress_reports_for_teacher/,program
657,AI Art in Relations to the Commercial Art and Design Industry,"I have seen some AI art. People in creative industries say it's a tool, others say it's nothing to worry about but I haven't heard people talk about how it's going to reduce jobs. I want to hear from people who better understand automation, what is AI art and/or similar programs going to do to art and design fields?",https://www.reddit.com/r/automation/comments/wuyjab/ai_art_in_relations_to_the_commercial_art_and/,program
658,Testing tools for Automation,"Automation testing can be a time-consuming and costly process. It is essential to choose the right automation testing tool that fits your requirements. There are various automated testing tools available on the market, but it is important to choose one which will assist your developers in writing reliable code. 

The Test Automation Solutions program you need can be found in our extensive list of options and frameworks. Many of the tools and frameworks are self-paced, so you can take things at your own pace. You can also pick from a variety of topics including test automation frameworks and tools to help write effective test cases.

You can select a test automation framework and a set of tools that best fit your needs. Check out these tools: 

1. **Cypress** 
2. **LambdaTest** 
3. **Selenium** 
4. **Puppeteer** 
5. **Playwright**",https://www.reddit.com/r/automation/comments/wpr25p/testing_tools_for_automation/,program
659,Get answers from answer sheet with OCR,"I need to get the answers from an answer sheet that looks like [this](https://remarksoftware.com/wp-content/uploads/2015/07/120-Question-Answer-Sheet_Page_1.jpg). I assume I could use some form of OCR that would interpret the position of the dots as a, b, c, etc but I don't really know how to search for that, and if it even exists. I'm not interested in programming it from scratch, because although it could be fun, I don't have enough time for it.

Edit: I found a better search term: ""OMR"" or ""Optical Mark Recognition"" and found some programs that do it. Will give FormRead a try but I accept suggestions.",https://www.reddit.com/r/automation/comments/wkfizk/get_answers_from_answer_sheet_with_ocr/,program
660,Automating a way to fill out a form in a desktop app,"The majority of my job is reading through technical reports of product failures, and filling out an assessment form (open text, dropdowns, radio buttons, etc) based on the information. Essentially I am making an assessment of the report. The open text portions of the forms are mostly things I can copy and paste from previous templates, with some slight modifications needed here and there. However the radio buttons and drop downs change depending on the type of report I’m reviewing. 

An example would be:
I have a report for test results which specify that the test failed due to user error. I would then use that report to complete the assessment form and check all the right boxes and enter the correct text needed to summary the issue.  

All of these reports and forms are housed in a proprietary desktop app on MacOS. 

I would like to automate this process to increase efficiency because my team process hundreds of these every week. 

Big disclaimer: I am not a programmer, but I am an engineer. 

Can anyone recommend an automated solution where I can basically just select the type of report I received and it somehow populates the entire form for me based on my selection? 
I assume it would need some type of script where I can pre-populate the form entries, but I have no clue how to do this. 

Using my previous example, perhaps the script could ask me what type of report I received, then I could select “testing-user error” and it would populate everything I need into the assessment form. 

Any help would be greatly appreciated!",https://www.reddit.com/r/automation/comments/w0lgp0/automating_a_way_to_fill_out_a_form_in_a_desktop/,program
661,pay rate,"I work in a sugar mill doing automation work long hours during harvest which is 3-4 months.. off season 40-60hrs... I install all the  gear,plc and do a good bit of programing and the instrumentation electrical and pneumatics. I'm in it for 4 years have computer and technician skills coming into the job but at the point boss man doesn't have to get out his office much if any...it's a pretty big mill produces 4.5 million pounds a sugar a day so one could imagine it's alot of converyours,turbines,electric motors, boilers,batch and continuous centrifugal machines etc... I work on everything.. I'm not as slick as the boss man but I have 9 other co-workers and and I'm ahead by a good bit technician work is what ibreally enjoy and do my best im pretty good at figuring out stuff im not familiar with or havent seen especially if im explained how the system works. also install and maintain they're air conditioners..don't wanna come off with a swollen head im just wondering what somebody like me should be making in this field",https://www.reddit.com/r/automation/comments/vwz23e/pay_rate/,program
662,Any way to automate iTunes downloads on Windows?,"I have a very large list of apps I would like backups of. It's somewhat deprecated, but Apple still lets you download .ipa files of apps in iTunes [12.6.5.3](https://12.6.5.3) on Windows 10 for people who want more control over app installation. I've created links that look like, for example:

itms://apps.apple.com/us/app/mobilefuge/id503610940?uo=4

I put the links into an html file I can open in a browser, and when I click on a link, the app's page opens in iTunes. Then I click a button in iTunes that says ""Get"", and it downloads to my PC. Rinse and repeat. Occasionally iTunes prompts me to re-enter my password, which I paste in and click OK.

This works, but it's incredibly tedious. Does anyone have suggestions for how I could automate this process? If it was all in a browser I could probably figure it out, but I've never done anything with automating a random program.

I actually found a program that says it does exactly what I want on Mac, but iTunes 12.6.5.3 won't run in recent mac OS's. It uses the JXA framework for OSX.

[https://github.com/attheodo/cherrypick](https://github.com/attheodo/cherrypick)",https://www.reddit.com/r/automation/comments/vrr5ed/any_way_to_automate_itunes_downloads_on_windows/,program
663,How to automate this boring task?,"It goes like this:
1- I have 800 names in a txtfile, each name is separated by a new line.
2- I have one picture 

I want to print each name in this picture, so I end up with 800 unique pictures. 
If you don't have an idea about how to automate this task, at least tell me what program to use to print these name, adobe photoshop, painter, or what?

Or there is any program that record screen clicks then repeats itself",https://www.reddit.com/r/automation/comments/vh2him/how_to_automate_this_boring_task/,program
664,Looking to automatically log me out of all my online accounts from all devices at the press of a button and when I leave work.,"I keep finding myself still logged in to google, reddit, amazon, etc when I come into work the next day. I'm always pretty pressed for time, and extremely forgetful to boot.

If there is already some pre-existing software that could run on my phone to log me out when I leave work or manually select to log out of all accounts, that would be great. If not, maybe there is something that would allow me to easily make what I want, I dont know if IFTT or something similar would work, I do know I suck at programming though. So if that's the only option, I'd just wind up having to pay someone I could trust not to steal my account info to write it for me.",https://www.reddit.com/r/automation/comments/un7065/looking_to_automatically_log_me_out_of_all_my/,program
665,Automation and political systems discussion,"In non complicated automated systems there’s two types and one could be seen as old and one as new or one as proprietary and one versatile but the reality is one type is based on things that are concrete in nature and where any change is minor and always an improvement where the other is based on a world where any business could go under at any moment and keeping with trends is more important than keeping up with needs, a world where recovering investment is more important than efficiency of operation or making things that last both in durability and requirement/need/desire to own. So my discussion prompt is what do you think about the move to using robotic arms and programming for everything instead of more mechanical versions that hardly require electricity and have all safeguards physical, where one motor can often do everything versus needing a software programmer? And my second part of my prompt is what are your options on politics and economics discouraging or encouraging automation use, should there be an incentive or even requirement? Should there be a classification system to identify automation that suits a need versus ones that serve a profit instead? Should a tighter standardization of motors and wear components be implemented as to allow automation of repairs also? Why or why not?",https://www.reddit.com/r/automation/comments/uj6u5v/automation_and_political_systems_discussion/,program
666,Determine who shared a Facebook post,"A friend of mine asked me if there was any way of programmatically getting a list of people who shared a post.  I can figure out how to get the number of shares, but haven't figured out how to get the actual list of people.  Anyone know how I can do this or where I can find some code snippets that do it?  

Thanks in advance",https://www.reddit.com/r/selenium/comments/10e2fu9/determine_who_shared_a_facebook_post/,program
667,selenium/Java interview prep,"Hello, I am planning to apply for some selenium based automation jobs. Anyone can give some tips on which topics in java programming and selenium should I focus on. Thanks in advance",https://www.reddit.com/r/selenium/comments/10cejuc/seleniumjava_interview_prep/,program
668,Selenium for Java or Python - advice sought,"Hi, I am a fairly beginner programmer with a strangely specific set of skills as a QA engineer. I have maintained test suites in a previous jobs which included adding and updating test code in Laravel and a different one using java. 

I have never set one up from scratch though and am a bit more comfortable building from the ground up with python but I wanted to get some input on which framework is better for a media focused site (think something similar to like Spotify or something).

Thanks in advance for your thoughts.",https://www.reddit.com/r/selenium/comments/1044cgj/selenium_for_java_or_python_advice_sought/,program
669,java error when using selenium,"i have been trying to fix a problem for a while. i am using eclipse luna which is an out-of-date version, I'm doing this so I can use larva but basically, I'm having an issue with setting up selenium. can anyone help me out?

code:

	 WebDriver driver = new ChromeDriver();

System.setProperty(""webdriver.chrome.driver"", ""C://Program Files//chromedriver//chromedriver.exe"");

	 driver.get(""[www.google.com](https://www.google.com)""); 

&#x200B;

error:

Exception in thread ""main"" java.lang.IllegalStateException: The path to the chromedriver executable must be set by the webdriver.chrome.driver system property; for more information, see [http://code.google.com/p/selenium/wiki/ChromeDriver](http://code.google.com/p/selenium/wiki/ChromeDriver). The latest version can be downloaded from [http://code.google.com/p/chromedriver/downloads/list](http://code.google.com/p/chromedriver/downloads/list)",https://www.reddit.com/r/selenium/comments/101q3oe/java_error_when_using_selenium/,program
670,Process unexpectedly closed with status 11,"Hello, I'm trying to run selenium but i get this error.

This is the program in python:

    ""First selenium script""
    from selenium import webdriver
    from selenium.webdriver.firefox.service import Service as FirefoxService
    from webdriver_manager.firefox import GeckoDriverManager
    
    
    driver = webdriver.Firefox(
        service=FirefoxService(executable_path=GeckoDriverManager().install()))
    
    driver.get(""https://www.google.com"")

This is the console output:

    alex@nobara ~/selenium$ python main.py
    Traceback (most recent call last):
      File ""/home/alex/selenium/main.py"", line 7, in <module>
        driver = webdriver.Firefox(
      File ""/home/alex/.local/lib/python3.10/site-packages/selenium/webdriver/firefox/webdriver.py"", line 177, in __init__
        super().__init__(
      File ""/home/alex/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py"", line 272, in __init__
        self.start_session(capabilities, browser_profile)
      File ""/home/alex/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py"", line 364, in start_session
        response = self.execute(Command.NEW_SESSION, parameters)
      File ""/home/alex/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py"", line 429, in execute
        self.error_handler.check_response(response)
      File ""/home/alex/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py"", line 243, in check_response
        raise exception_class(message, screen, stacktrace)
    selenium.common.exceptions.WebDriverException: Message: Process unexpectedly closed with status 11
    

This is the content of geckodriver.log:

    1666974769008	geckodriver	INFO	Listening on 127.0.0.1:58881
    1666974769634	mozrunner::runner	INFO	Running command: ""/usr/bin/firefox"" ""--marionette"" ""--remote-debugging-port"" ""56809"" ""--remote-allow-hosts"" ""localhost"" ""-no-remote"" ""-profile"" ""/tmp/rust_mozprofile6UjIeC""
    ExceptionHandler::GenerateDump cloned child 6111
    ExceptionHandler::WaitForContinueSignal waiting for continue signal...
    ExceptionHandler::SendContinueSignalToChild sent continue signal to child

I don't know what to do.

I'm on Linux fedora 36.",https://www.reddit.com/r/selenium/comments/yftdr9/process_unexpectedly_closed_with_status_11/,program
671,Noobie needs help,"Hello there,

&#x200B;

at the moment i try to programm a small bot for myself, but know i run into following error and im not able to solve it by myself.

 **AttributeError**: 'WebDriver' object has no attribute 'execute\_scipt' 

&#x200B;

The Script is pretty simple:

    from selenium import webdriver
    from selenium.webdriver.common.keys import Keys
    from webdriver_manager.chrome import ChromeDriverManager
    
    driver = webdriver.Chrome(ChromeDriverManager().install())
    driver.implicitly_wait(10)
    
    driver.get(""fantastic-website"")
    driver.execute_scipt('document.getElementsByName(""btn-add-to-cart"")[0].click()')

I alredy tested the JavaScript in the DevTools Console and the Script is working.

Can someone give me a hint?

&#x200B;

Kind regards",https://www.reddit.com/r/selenium/comments/ye0sfm/noobie_needs_help/,program
672,I'm making a big project to automate all the boring stuff at my company,"Hi everyone, I get started a project to check a lot of websites and scrape data from them, even though I'm wondering what *pattern design* I could use for this.

The idea basically is be able to extract some information from a file.csv and use it for scraping and filtering on these portals, furthermore, I would like to implement a GUI for being easier to use by my coworkers who don't know anything about programming or surfing on the terminal",https://www.reddit.com/r/selenium/comments/yb3hlg/im_making_a_big_project_to_automate_all_the/,program
673,Selenium is crashing without any errors,"Hello,

my Python Selenium Script is permanently crashing and I don't know why. I even redownloaded the Chrome Driver but it  still keeps crashing without any errors. My script is pretty fast so I don't know if it's so fast it crashes, because if I debug the program it runs completly normal and doesn't crash. This is the code I've newly written (and I feel like the error is coming from):

&#x200B;

        def find_element_selenium(self, by, name):
            return WebDriverWait(self.driver, TIMEOUT).until(EC.presence_of_element_located((by, name)))
    
        def find_elements_selenium(self, by, name):
            return WebDriverWait(self.driver, TIMEOUT).until(EC.presence_of_all_elements_located((by, name)))
    
        def load_new_page(self, url):
            self.driver.get(url)
            WebDriverWait(self.driver, TIMEOUT).until(EC.presence_of_element_located((By.CLASS_NAME, ""navigiumlogo"")))
    
        def login(self):
            print(""Bitte einloggen..."")
            self.driver.get(""https://www.navigium.de/schule/login/mainmenu.html"")
            WebDriverWait(self.driver, TIMEOUT).until(EC.presence_of_element_located((By.CLASS_NAME, ""navigiumlogo"")))

And this is the whole script for anyone that wants to see it: [https://pastebin.com/nQQCiHRN](https://pastebin.com/nQQCiHRN)

&#x200B;

Edit: I kinda found the error. I have a while loop that looks if a element is present with my find\_element\_selenium function, which works every where btw, but not with my while loop, so my guess is, that the while loop checks to often or something for the condition that it early-exits and doesn't even get in the while loop",https://www.reddit.com/r/selenium/comments/yb0n3p/selenium_is_crashing_without_any_errors/,program
674,Why is the third click action here timing out?,"I'm trying to navigate through this webpage so I can scrape data from the ""Matchups"" table. What I've written so far is able to click through the ""basic"" button and the ""matchups"" button, but the request to click the ""show numbers"" button always times out unless I give the full XPATH as the locator. I think that it could be that the condition ""element\_to\_be\_clickable"" is not met because an ad covers the button so it isn't visible, however I don't think that explains why it works when I supply the full XPATH.

    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    import time
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.common.action_chains import ActionChains
    
    s = Service(""C:\Program Files (x86)\chromedriver.exe"")
    driver = webdriver.Chrome(service=s)
    
    driver.get(""https://www.vicioussyndicate.com/data-reaper-live-beta/"")
    driver.maximize_window()
    
    wait = WebDriverWait(driver, 20)
    
    basicBtn = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=""basicBtn""]')))
    driver.execute_script(""arguments[0].click();"", basicBtn)
    
    table = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=""table""]')))
    driver.execute_script(""arguments[0].click();"", table)
    
    showNum = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=""showNumbers""]')))
    driver.execute_script(""arguments[0].click();"", showNum)
    
    data = list(map(lambda x: x.text, driver.find_elements(By.CLASS_NAME, ""textpoint"")))
    print(data)
    print(len(data))
                            
    
    time.sleep(5)
    driver.quit()

I could, of course, just supply the full XPATH, although I've heard that this is not good practice and is less resilient to changes to the website.

Note: This code also times out with showNum located [By.ID](https://By.ID)(""showNumbers"")

EDIT:

I think I've figured out the issue. There are two elements on the site with ID ""showNumbers"" which is curious site design and I'm still not sure the best way to work around this. Should I find all elements with this ID and access the second one, or just supply the full XPATH?",https://www.reddit.com/r/selenium/comments/y8bht8/why_is_the_third_click_action_here_timing_out/,program
675,Re-Connecting to existing browser session - python,"  
Hey guys, trying to use python and selenium to keep current browser session open and not require the need to relogin to website x every time i run the program. 

I had it working with this code yesterday, and i updated windows now it isnt working. I am receiving error: 'cannot connect to host, chrome unreachable.'. Ive tried several different ports and none seem to work, although when i remove the 'options' argument from the driver declaration and use only service,  i am able to successfully open a new browser. Leading me to believe my issue lies within line 5 of my code.  

&#x200B;

Thanks for taking a peek! 

&#x200B;

`path = r""C:\Users\xxxxxxxxxx\chromedriver_win32\chromedriver.exe""`  
`service = Service(executable_path=path)`  
`web = 'https://xxxxxx/com'`  


`options = Options()`  
`options.add_experimental_option(""debuggerAddress"", ""localhost:9222"")`  
`driver = webdriver.Chrome(service=service, options=options)`  
`driver.get(web)`",https://www.reddit.com/r/selenium/comments/y4ozzc/reconnecting_to_existing_browser_session_python/,program
676,How to stop while loop after scrolling,"Hello, I have a problem to stop while loop after scrolling. Maybe this is not the appropriate method to scrap this site, I am not sure.  I want to scrap all ads and after that the program to stop. In my way I need to stop the program manually. 

Can somebody help me to stop the program when there aren't any ads?

The site is this - [https://www.jobs.bg/en/front\_job\_search.php](https://www.jobs.bg/en/front_job_search.php)

This is the code - [https://pastebin.com/udL5VwjM](https://pastebin.com/udL5VwjM)

Thanks in advance!",https://www.reddit.com/r/selenium/comments/y1epsy/how_to_stop_while_loop_after_scrolling/,program
677,Webpage immediately closing.,"When I try to open a webpage, it immediately closes.

Here is my code.

from selenium import webdriver  
import time  


driver = webdriver.Chrome(executable\_path=""C:\\Drivers\\chromedriver.exe"")  
driver.get(""https://chromedriver.storage.googleapis.com/index.html?path=106.0.5249.61/"")  


time.sleep(99999)

&#x200B;

I tried other things to fix it thinking it was just because the code ended, but adding the sleep at the end didn't fix it. Here is what was relayed back to me from Pycharm

""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\Scripts\\python.exe"" ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\[main.py](https://main.py)"" 

C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\[main.py:4](https://main.py:4): DeprecationWarning: executable\_path has been deprecated, please pass in a Service object

  driver = [webdriver.Chrome](https://webdriver.Chrome)(executable\_path=""C:\\Drivers\\chromedriver.exe"")

Traceback (most recent call last):

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\[main.py](https://main.py)"", line 4, in <module>

driver = [webdriver.Chrome](https://webdriver.Chrome)(executable\_path=""C:\\Drivers\\chromedriver.exe"")

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\chrome\\[webdriver.py](https://webdriver.py)"", line 69, in \_\_init\_\_

super().\_\_init\_\_([DesiredCapabilities.CHROME](https://DesiredCapabilities.CHROME)\['browserName'\], ""goog"",

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\chromium\\[webdriver.py](https://webdriver.py)"", line 92, in \_\_init\_\_

super().\_\_init\_\_(

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\[webdriver.py](https://webdriver.py)"", line 272, in \_\_init\_\_

self.start\_session(capabilities, browser\_profile)

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\[webdriver.py](https://webdriver.py)"", line 364, in start\_session

response = self.execute(Command.NEW\_SESSION, parameters)

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\[webdriver.py](https://webdriver.py)"", line 429, in execute

self.error\_handler.check\_response(response)

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\[errorhandler.py](https://errorhandler.py)"", line 243, in check\_response

raise exception\_class(message, screen, stacktrace)

selenium.common.exceptions.SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version 106

Current browser version is 105.0.5195.128 with binary path C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe

Stacktrace:

Backtrace:

	Ordinal0 \[0x00A11ED3+2236115\]

	Ordinal0 \[0x009A92F1+1807089\]

	Ordinal0 \[0x008B66FD+812797\]

	Ordinal0 \[0x008D8C6D+953453\]

	Ordinal0 \[0x008D4200+934400\]

	Ordinal0 \[0x008D19C9+924105\]

	Ordinal0 \[0x0090806C+1146988\]

	Ordinal0 \[0x00907A6A+1145450\]

	Ordinal0 \[0x009018A6+1120422\]

	Ordinal0 \[0x008DA73D+960317\]

	Ordinal0 \[0x008DB71F+964383\]

	GetHandleVerifier \[0x00CBE7E2+2743074\]

	GetHandleVerifier \[0x00CB08D4+2685972\]

	GetHandleVerifier \[0x00AA2BAA+532202\]

	GetHandleVerifier \[0x00AA1990+527568\]

	Ordinal0 \[0x009B080C+1837068\]

	Ordinal0 \[0x009B4CD8+1854680\]

	Ordinal0 \[0x009B4DC5+1854917\]

	Ordinal0 \[0x009BED64+1895780\]

	BaseThreadInitThunk \[0x76906739+25\]

	RtlGetFullPathName\_UEx \[0x77908FD2+1218\]

	RtlGetFullPathName\_UEx \[0x77908F9D+1165\]

&#x200B;

&#x200B;

Process finished with exit code 1

&#x200B;

&#x200B;

&#x200B;

Thanks for any help you can provide.",https://www.reddit.com/r/selenium/comments/y08dhy/webpage_immediately_closing/,program
678,How do I extract data from a dynamic table embedded in a webpage?,"I'm trying to extract extract all the salary information from the table on the following URL: [https://www.fedsdatacenter.com/federal-pay-rates/](https://www.fedsdatacenter.com/federal-pay-rates/).

I'm not too familiar with Selenium or programming, so I apologize if I am using incorrect terminology. But I couldn't find any guidance on how to do this. If you could help me out, I would greatly appreciate it.",https://www.reddit.com/r/selenium/comments/xz3sfh/how_do_i_extract_data_from_a_dynamic_table/,program
679,Selenium doesn't continue in for loop?,"Hello,

My code is working fine only with the first category, but doesn't loop through the others categories. I can't figure out what happеning and why Selenium stops in the loop. The separate code is working fine, but in this for loop ( for i in range(16, 50): ... line 49) the program stops the execution after the first category.  Please, help me to solve this!

The code - [https://pastebin.com/WN4NwRRx](https://pastebin.com/WN4NwRRx)  with Chrome driver installation.

Thanks in advance!",https://www.reddit.com/r/selenium/comments/xxxx7j/selenium_doesnt_continue_in_for_loop/,program
680,Change default download directory in Python?,"Hi guys, I am an amateur programmer using Python, Selenium, and ChromeDriver. I am coding in PyCharm. My issue is that I can't seem to successfully change my default download directory. Please see the following code below, which hasn't worked for me to change the directory:

    from selenium import webdriver
    
    chromeOptions = webdriver.ChromeOptions()
    prefs = {""download.default_directory"" : ""C:/Users/popularweb6231/python_work/""}
    chromeOptions.add_experimental_option(""prefs"", prefs)
    chromedriver = ""C:/Users/popularweb6231/chromedriver.exe""
    driver = webdriver.Chrome(executable_path=chromedriver, options=chromeOptions)

Instead, it's just using the Chrome default ('\_user'/downloads) folder as the default folder. Am I doing something wrong? Please help :(",https://www.reddit.com/r/selenium/comments/xxj8x8/change_default_download_directory_in_python/,program
681,"What is Selenium used for, in simple words"," 

Selenium is a free (open source) automated testing tool for validating web applications across a variety of browsers and platforms. You can use multiple programming languages like Java, C#, Python, etc to create Selenium Test Scripts.

The Selenium test suite comprises four tools:

1. Selenium Integrated Development Environment (IDE)
2. Selenium Remote Control (RC)
3. Selenium WebDriver
4. Selenium Grid

Selenium Tools is a suite of software, each piece catering to a different organization's Selenium QA testing needs.",https://www.reddit.com/r/selenium/comments/xumus2/what_is_selenium_used_for_in_simple_words/,program
682,Can’t use camera headless chrome,Headless chrome doesn’t detect the camera.  I am running this Python program on a Linux mint laptop.  Is there any solution?,https://www.reddit.com/r/selenium/comments/xn7cf4/cant_use_camera_headless_chrome/,program
683,Need help with multiple elements and fixing code,"The script is coming along, and I want to thank everyone who have been of great assistance so far. 

    from selenium import webdriver
    from selenium.webdriver.common.keys import Keys
    from selenium.webdriver.common.by import By
    import login as login
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    import datetime
    import time
    
    x = datetime.datetime.now()
    x = x.strftime(""%b %d"")
    
    driver = browser = webdriver.Firefox()
    driver.set_window_size(1512, 799)
    driver.get(""https://connect.garmin.com/modern/activities"")
    
    driver.implicitly_wait(1)
    
    iframe = driver.find_element(By.ID, ""gauth-widget-frame-gauth-widget"")
    driver.switch_to.frame(iframe)
    
    driver.find_element(""name"", ""username"").send_keys(login.username)
    
    driver.find_element(""name"", ""password"").send_keys(login.password)
    driver.find_element(""name"", ""password"").send_keys(Keys.RETURN)
    
    driver.switch_to.default_content()
    
    time.sleep(10)
    
    driver.find_element(""name"", ""search"").send_keys(""Reading"")
    driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)
    
    time.sleep(2)
    
    time_read = 0
    time_meditated = 0
    time_programming = 0
    
    def get_modified_xpath(value):
    	return ""//span[text() = '{}']//ancestor::div[@class='list-item-container']//div[5]//div[2]//span//span[1]"".format(value)
    
    
    date_str = get_modified_xpath(x)
    
    current_time = driver.find_elements(By.XPATH, date_str)
    for times in current_time:
    	if len(times.text) >= 7:
    		result = time.strptime(times.text, ""%H:%M:%S"")
    		time_read += result.tm_hour * 60
    		time_read += result.tm_min
    		print(time_read)
    	else:
    		result = time.strptime(times.text, ""%M:%S"")
    		time_read += result.tm_min
    		print(time_read)
    
    time.sleep(1)
    
    driver.find_element(""name"", ""search"").clear()
    driver.find_element(""name"", ""search"").send_keys(""Meditation"")
    driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)
    
    time.sleep(3)
    
    current_time = driver.find_elements(By.XPATH, date_str)
    
    for times in current_time:
    	if len(times.text) >= 7:
    		result = time.strptime(times.text, ""%H:%M:%S"")
    		time_meditated += result.tm_hour * 60
    		time_meditated += result.tm_min
    		print(time_meditated)
    	else:
    		result = time.strptime(times.text, ""%M:%S"")
    		time_meditated += result.tm_min
    		print(time_meditated)
    
    time.sleep(1)
    
    driver.find_element(""name"", ""search"").clear()
    driver.find_element(""name"", ""search"").send_keys(""Programming"")
    driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)
    
    time.sleep(3)
    
    current_time = driver.find_elements(By.XPATH, date_str)
    
    time.sleep(1)
    
    for times in current_time:
    	if len(times.text) >= 7:
    		result = time.strptime(times.text, ""%H:%M:%S"")
    		time_programming += result.tm_hour * 60
    		time_programming += result.tm_min
    		print(time_programming)
    	else:
    		result = time.strptime(times.text, ""%M:%S"")
    		time_programming += result.tm_min
    		print(time_programming)
    
    print(f""You spent {time_read} minutes on Reading today"")
    print(f""You spent {time_meditated} minutes on Meditation today"")
    print(f""You spent {time_programming} minutes on Programming today"")
    
    # def get_time_from_page(activity, activity_spent):
    #
    # 	time.sleep(2)
    #
    # 	current_time = driver.find_elements(By.XPATH, date_str)
    #
    # 	driver.find_element(""name"", ""search"").clear()
    # 	driver.find_element(""name"", ""search"").send_keys(activity)
    # 	driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)
    #
    # 	for times in current_time:
    # 		if len(times.text) >= 7:
    # 			result = time.strptime(times.text, ""%H:%M:%S"")
    # 			activity_spent += result.tm_hour * 60
    # 			activity_spent += result.tm_min
    # 			print(activity_spent)
    # 		else:
    # 			result = time.strptime(times.text, ""%M:%S"")
    # 			activity_spent += result.tm_min
    # 			print(activity_spent)
    #
    # 	time.sleep(3)        

It isn't looking great doing the same thing three times, which is why I tried to make a function, but I have encountered issues.

First issue is that I am unsure how to give the function a variable that it should then add the minutes to. activity_spent for example, it doesn't seem to add the time when I call the function giving it the variable time_read or time_programmed, even though these variables exist already, or even if they don't.

Second issue is that I now need multiple different elements from the same one for two or three activities, walking, running and hiking. Here I want more than simply time, now I need the distance and maybe heart rate as well. 

Third issue, and last one, is that the next step would be to summarize the time spent that day in some creative format, maybe there is a library that can summarize it into a banner, that I then can use for the twitter bot? I will have to look into it.

Then fixing all the explicit waits to something better of course.

[Picture of website, layout and some HTML](https://imgur.com/btkXlhu)",https://www.reddit.com/r/selenium/comments/xl61o2/need_help_with_multiple_elements_and_fixing_code/,program
684,What is selenium automation testing?,"Automation testing is the process of executing a set of predefined tests over and over again in a rapid and repeatable way, ensuring that they are always up-to-date. Automation testing dramatically reduces the need for manual testing, allowing you to focus on more important work. It also makes executing tests faster and allowed for more frequent testing.

Selenium is an open source test automation framework written in Java. It supports various browsers and provides support for various testing frameworks such as JUnit, TestNG. Selenium uses different software called drivers to control the browser.

Some of the reasons behind its popularity are as follows:

1) Tests can be written in various programming languages.

2) Tests can be run in different operating systems.

3) It supports any browser that is available.

4) It can integrate with other software frameworks like TestNG and JUnit for project management and reporting purposes.",https://www.reddit.com/r/selenium/comments/xj51f3/what_is_selenium_automation_testing/,program
685,Dropdown problem on redesigned site,"Hello,

I'm a big seller on a site that doesn't have an API.  
I have a Selenium program to post and edit ads.

They redesigned site and my program doesn't work.

Firstly I want to click on [dropdown](https://ibb.co/2cscx3q), then to select [category](https://ibb.co/7zHhcJN).  
Category is hidden in inspect, and it appears when I click on a category.

After that, I don't need to click on a subcategory, it opens itself when I select a category. Now I need to [select a subcategory.](https://ibb.co/XFSFXs6)

I managed to click on category menu but i can't manage to select category and subcategory.

Can anyone help me please?",https://www.reddit.com/r/selenium/comments/xfktw9/dropdown_problem_on_redesigned_site/,program
686,Selenium Automation,"Selenium is the world’s most popular automating tool that makes it easy to build and maintain automated tests. Selenium introduced the concept of WebDriver, a browser automation framework that supports multiple languages and platforms such as Java, C#, Objective-C, Groovy, Perl, Python and Ruby.

 Selenium is free and an open source tool that provides an API for web developers to provide a language-independent method for writing tests. The underlying framework allows users to write test scripts in various programming languages like Java, Ruby or any other language. You can even use JSON, XML or any other data formats with Selenium without having to learn complex programming languages.

It tests your web applications by performing actions such as clicking buttons and verifying text displays in order to generate an automated functional test. Selenium can also run across different browsers to ensure your application display looks the same across them all.",https://www.reddit.com/r/selenium/comments/xc8x3e/selenium_automation/,program
687,safaridriver --enable hangs forever,"I'm trying to enable safaridriver, but it hangs and never return the prompt.

    $ /usr/bin/safaridriver --enable
    Password:
    <hangs forever>

Any other \`sudo program\` works fine. 

I'm just porting some tests to mac. chrome/chromedriver works perfect",https://www.reddit.com/r/selenium/comments/x6cb78/safaridriver_enable_hangs_forever/,program
688,Selenium makes my computer heat up and lose battery,"Hello everyone, I created a python script that does automation with selenium. However on my mac pro intel during use there is a sharp drop in the battery as well as overheating. Looking at the monitor I notice that the program is using 45% of the processor. Do you have an idea how to reduce this percentage so that this program runs ""normally""?",https://www.reddit.com/r/selenium/comments/x1jyfi/selenium_makes_my_computer_heat_up_and_lose/,program
689,Is VS Code a good IDE/text editor to create a selenium framework in?,"To give you all some context of why i am asking this:

I'm currently trying to create the same POM-based selenium webdriver framework for multiple programming languages (Java, C#, Python, and JavaScript). I am doing this so that when it comes to interviews based around Selenium, i will well-prepped and language agnostic since i've already got a Selenium template good to go for whatever language that company uses. I figured that JS, Java, C#, & Python were the most commonly-used languages when it comes to Selenium.

Anyhow... I've noticed so far that VS code is a pain in the ass with Selenium in Java. I installed the Extension Pack for Java, and that seemed to have given me everything i need to run Java Code. I then was able to get selenium to run with the main method. And then soon after a bit of tinkering with TestNG, i was able to get my example test to run from my test method within my testNG test class.

&#x200B;

Heres the problem. When i run my TestNG tests from the test section of VS code (with the java extenion pack installed), the only meaningful failure output i see is that the test method failed. But it doesn't show me which specific assertion in my test method failed. This leads me to think that maybe VS Code isn't the best IDE/editor to run selenium code for Java? What do you all think? 

&#x200B;

I would PREFER to use VS code because that's the same IDE/text Editor that I use with [Cypress.io](https://Cypress.io) automation. But it seems like TestNG is either NOT meant to be run with VS Code at all or i need to use a completely different assertion library or something with VS Code to have an easier ""automation experience"". 

Does anyone have any suggestions on 1) Which IDE/text editor i should be using with Java Selenium. and 2) Any unit test libraries or extensions i could use that i'm not using here?

&#x200B;

NOTE: I'm fully aware of Eclipse and jUnit, but i feel like that IDE is super old and crappy and JUnit doesn't have the same capabilities as TestNG.  Anyhow thanks for the feedback in advance!",https://www.reddit.com/r/selenium/comments/wxxehn/is_vs_code_a_good_idetext_editor_to_create_a/,program
690,How to allocate selenium particular amount of RAM? It uses all of the RAM any PC it is running on...,"If I have 8 GB RAM PC, it takes all of it in time and performs just fine. But, if I have 16 GB RAM, it uses all of it too and still performs fine. So it means it only needs 8 GB RAM. So I have a program that I just can't driver.quit() or something else, because I resize the windows to my liking, so I just can't driver quit then resize them back again too much effort. How to allocate a particular amount of RAM to the selenium web driver? I don't want to fill my RAM.",https://www.reddit.com/r/selenium/comments/wxs90a/how_to_allocate_selenium_particular_amount_of_ram/,program
691,How To Scrap Network Type 'XHR' / 'Fetch' Data In Selenium 4?,"## My goal ##

Im trying to scrap raw video stream data (.ts files) from [twitch.tv][1] using Selenium 4.
All live streams are fed in chunks of video,
I can access them manually by:

1. opening a chrome tab with a running [twitch.tv][1] livestream
2. open DevTools (F12)
3. go to Network tab > XHR
4. The stream of .ts (transport stream) files being fetched are my desired files.
5. I can just doubleclick on them and chrome downloads this small video chunk file.


I want to reproduce this using Selenium 4 but I have no experience with Web Programming (POST, Flow etc). My current programm 
is able to scrap image files. But once the response received is of .ts file (XHR/Fetch) it returns.

>DevToolsException: {""id"":11,""error"":{""code"":-32000,""message"":""No data found for resource with given identifier""},""sessionId"":""79BA2C212FABA878DB3524D7D0F49BDC""}


## I have tried ##
Calling [Network.getResponseBody][2] when the [Network.loadingFinished][3] event has fired but this also doesn't work. There is never the same requestID on either event.

Remarks: Im aware there is a Twitch API.

    public static void main(String[] args) {
        
        InitializeSeleniumDrivers();
        driver.get(""https://www.twitch.tv/thebausffs"");
        
        
        DevTools devTools = ((ChromeDriver) driver).getDevTools();
        devTools.createSession();
        devTools.send(Network.clearBrowserCache());
        devTools.send(Network.setCacheDisabled(true));
        devTools.send(Network.enable(Optional.empty(), Optional.empty(), Optional.of(100000000)));


        
        devTools.addListener(Network.responseReceived(), responseReceived -> {

            RequestId requestId = responseReceived.getRequestId();
            
            try {
                Command<Network.GetResponseBodyResponse> getBody = Network.getResponseBody(requestId);
                Network.GetResponseBodyResponse response = devTools.send(getBody);
            } catch (DevToolsException e) {
                e.printStackTrace();
            }

        });
    }



## Headers Example ##

**GENERAL**
```
Request URL: https://video-edge-c55dd0.ams02.abs.hls.ttvnw.net/v1/segment/CrEFZRTkEBMVDg5w4Ygn2pwqXKLGK5NAUAQ7ZWHeCORCjjFxfh9McgTBm_DTCvfP1MrZIg1jb2-oo2769tLAjFKjUd4AQaKtV3LeTEpPJyB_7ZAgolK-dSlLAqnC1xaI7z6iJCC4W1fb5RkkJmLk2D5nYEpyA17gSqe1eoB5zYsrDnal6Sm__B5LhxzOwTPOKI66jxXeIThm8tpaFGabccyd8AcT7RIfqCRv9Jas-IMQCqnBLLpIjk5rC-n4USQzLI6R4xGeTyTwMgX3BQ7EcxB-X62kUvsJm2O7Q2iJEI-ongDyyFRCapzo8iBtGgN2ruxvp8SeCKHO8j9NbS4jymG276ZigtnDXEQbxa6f5i9dHEcf9g1ump4RZtd48eOv6bPsGCDhFfULRd8adcM369ew90NrzyYbImQZnhFcnyqvfYIlCg-FFyjqJHVz37MZGc7TLbSh1YqmrkAClamXb8fFPGCXpsIrY-IDmKgTxh8tEmjbdacBWsKxxwJAOv-H6MUZB67MP1KMeT94YMjGXBcIjJo4JKeFCKoITCLJI4jjzqNmFa_efdlaJ89mUodxQRHJARV3qwdp04TSvZALBbOua6m-0T-01lOEYlr6w408mr5araj7c7gjpvrj_83jb0wqJG7ala1DBUg0U0Vx2rQxzumokyz66MxfMJy3ZSY92L-JdS47RjcOpilnpTI9bI8RPRyY4grds2SHDudWxgp-jJWgHdtbbFpuDCZENwOuU_-Agsf0lA_g59KnXnAuz59yovCO2C_O8ptkyoImgZ47qBPBIn-DDD-rzJloGD-GTQn4zGlmAFcg6GunjeW3PbHjKjMz8vA_K8NOF7ofO94YOtj_1khbCFGfH2_dF8zDwMSieR5Mvg7upQdzwgl_GAmf7OIAbHXwA1DqamnbAeWundcaDEM8dWDJF-pfTicm0CABKglldS13ZXN0LTIwtwQ.ts
Request Method: GET
Status Code: 200 OK
Remote Address: 185.42.204.31:443
Referrer Policy: strict-origin-when-cross-origin
```

**RESPONSE HEADER**
```
Accept-Ranges: bytes
Access-Control-Allow-Origin: *
Cache-Control: no-cache, no-store, private
Content-Length: 1589164
Content-Type: application/octet-stream
Date: Sun, 14 Aug 2022 16:56:31 GMT
```



**REQUEST HEADER**
```
Provisional headers are shown
Learn more
Referer
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36
```


  [1]: https://www.twitch.tv/
  [2]: https://chromedevtools.github.io/devtools-protocol/tot/Network/#event-responseReceived
  [3]: https://chromedevtools.github.io/devtools-protocol/tot/Network/#event-loadingFinished",https://www.reddit.com/r/selenium/comments/wocp4z/how_to_scrap_network_type_xhr_fetch_data_in/,program
692,Is Selenium or Puppeteer good for Browser Automation?,"Hello!

I'm new into programming and I had a question regarding the use of Selenium. As part of something fun to do, I was thinking of automating a Google search based on user input and then listing the results.

Basically, if a user searches typical book, it searches Google for it (automation linked maybe) and returns the result. I know it sounds they can do it themselves but I'll start off from here and add multiple other functions to it.


Is Selenium good for this or Puppeteer? I've heard both are automation tools but I don't know which one would be better in this case, as after searching much, I've seen many places say Selenium is good for 'testing'.

I don't have any issue with multiple browser or just a single browser atm since Puppeteer only works on Chrome.

Any help would be appreciated.

Thanks a ton! :)",https://www.reddit.com/r/selenium/comments/wkv6vp/is_selenium_or_puppeteer_good_for_browser/,program
693,I’m willing to pay to a selenium tutor please help,"I want a code that finds the best Aliexpress product in a specific niche(for example home sport training), based on different variables, like likes count.  the reason is i want to send links of those products to my audience(affiliate marketing), and it takes me too much time to do it myself.  can someone teach me to make a program that would actually save my time?",https://www.reddit.com/r/selenium/comments/wkvdoq/im_willing_to_pay_to_a_selenium_tutor_please_help/,program
694,Selenium can't find element with ID/Name,"Im trying to challenge myself by making selenium redeem 1 gamepass code on microsoft issue is I Found the ID but it doesn't work as in Selenium can't find it, [This](https://account.microsoft.com/billing/redeem) is the website I need selenium to recognize and type in it 

&#x200B;

this is the error

Traceback (most recent call last):

  File ""c:\\Users\\jeans\\Downloads\\New folder\\Microsoft\\[redeem.py](https://redeem.py)"", line 30, in <module>

gamepass = driver.find\_element([By.ID](https://By.ID), value=""tokenString"")

  File ""C:\\Users\\jeans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\[webdriver.py](https://webdriver.py)"", line 857, in find\_element

return self.execute(Command.FIND\_ELEMENT, {

  File ""C:\\Users\\jeans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\[webdriver.py](https://webdriver.py)"", line 435, in execute     

self.error\_handler.check\_response(response)

  File ""C:\\Users\\jeans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\[errorhandler.py](https://errorhandler.py)"", line 247, in check\_response

raise exception\_class(message, screen, stacktrace)

selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""css selector"",""selector"":""\[id=""tokenString""\]""}

  (Session info: chrome=104.0.5112.81)

Stacktrace:

Backtrace:

Ordinal0 \[0x00FA78B3+2193587\]

Ordinal0 \[0x00F40681+1771137\]

Ordinal0 \[0x00E541A8+803240\]

Ordinal0 \[0x00E824A0+992416\]

Ordinal0 \[0x00E8273B+993083\]

Ordinal0 \[0x00EAF7C2+1177538\]

Ordinal0 \[0x00E9D7F4+1103860\]

Ordinal0 \[0x00EADAE2+1170146\]

Ordinal0 \[0x00E9D5C6+1103302\]

Ordinal0 \[0x00E777E0+948192\]

Ordinal0 \[0x00E786E6+952038\]

GetHandleVerifier \[0x01250CB2+2738370\]

GetHandleVerifier \[0x012421B8+2678216\]

GetHandleVerifier \[0x010317AA+512954\]

GetHandleVerifier \[0x01030856+509030\]

Ordinal0 \[0x00F4743B+1799227\]

Ordinal0 \[0x00F4BB68+1817448\]

Ordinal0 \[0x00F4BC55+1817685\]

Ordinal0 \[0x00F55230+1856048\]

BaseThreadInitThunk \[0x76CEFA29+25\]

RtlGetAppContainerNamedObjectPath \[0x779B7A9E+286\]

RtlGetAppContainerNamedObjectPath \[0x779B7A6E+238\]",https://www.reddit.com/r/selenium/comments/wjv59o/selenium_cant_find_element_with_idname/,program
695,click() doesnt work for me,"this is my code:

`from selenium import webdriver`  
`from selenium.webdriver.common.by import By`  
`from selenium.webdriver.chrome.service import Service`  
`import time`  


`#set location the location of the webdriver`  
`s = Service('C:/browserdrivers/chromedriver')`  


`driver = webdriver.Chrome(service=s)`  


`driver.get(""https://he.aliexpress.com/"")`  


`driver.refresh()`  
`search_bar = driver.find_element(by=By.CLASS_NAME, value= ""search-key"")`  
`search_bar.send_keys(""hi"")`  
`time.sleep(5)`  


`enter = driver.find_element(by=By.CLASS_NAME, value=""search-button"")`  
`enter.click()`  


now instead of clicking the element it writes this:

^(""C:\\Program Files\\Python310\\python.exe"" ""C:/לימוד סלניום/s.py"")

^(Traceback (most recent call last):)

  ^(File ""C:\\לימוד סלניום\\)[^(s.py)](https://s.py)^("", line 21, in <module>)

[^(enter.click)](https://enter.click)^(())

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(webelement.py)](https://webelement.py)^("", line 88, in click)

^(self.\_execute(Command.CLICK\_ELEMENT))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(webelement.py)](https://webelement.py)^("", line 396, in \_execute)

^(return self.\_parent.execute(command, params))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(webdriver.py)](https://webdriver.py)^("", line 435, in execute)

^(self.error\_handler.check\_response(response))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(errorhandler.py)](https://errorhandler.py)^("", line 247, in check\_response)

^(raise exception\_class(message, screen, stacktrace))

^(selenium.common.exceptions.ElementClickInterceptedException: Message: element click intercepted: Element <input type=""submit"" class=""search-button"" value=""""> is not clickable at point (307, 161). Other element would receive the click: <div class=""\_3KrBP  \_3XMV3"">...</div>)

  ^((Session info: chrome=103.0.5060.134))

^(Stacktrace:)

^(Backtrace:)

	^(Ordinal0 \[0x00CB6463+2188387\])

	^(Ordinal0 \[0x00C4E461+1762401\])

	^(Ordinal0 \[0x00B63D78+802168\])

	^(Ordinal0 \[0x00B97F9B+1015707\])

	^(Ordinal0 \[0x00B95F68+1007464\])

	^(Ordinal0 \[0x00B93C6B+998507\])

	^(Ordinal0 \[0x00B929D9+993753\])

	^(Ordinal0 \[0x00B88613+951827\])

	^(Ordinal0 \[0x00BAC7DC+1099740\])

	^(Ordinal0 \[0x00B87FF4+950260\])

	^(Ordinal0 \[0x00BAC9F4+1100276\])

	^(Ordinal0 \[0x00BBCC22+1166370\])

	^(Ordinal0 \[0x00BAC5F6+1099254\])

	^(Ordinal0 \[0x00B86BE0+945120\])

	^(Ordinal0 \[0x00B87AD6+948950\])

	^(GetHandleVerifier \[0x00F571F2+2712546\])

	^(GetHandleVerifier \[0x00F4886D+2652765\])

	^(GetHandleVerifier \[0x00D4002A+520730\])

	^(GetHandleVerifier \[0x00D3EE06+516086\])

	^(Ordinal0 \[0x00C5468B+1787531\])

	^(Ordinal0 \[0x00C58E88+1805960\])

	^(Ordinal0 \[0x00C58F75+1806197\])

	^(Ordinal0 \[0x00C61DF1+1842673\])

	^(BaseThreadInitThunk \[0x753DFA29+25\])

	^(RtlGetAppContainerNamedObjectPath \[0x77127A9E+286\])

	^(RtlGetAppContainerNamedObjectPath \[0x77127A6E+238\])

&#x200B;

&#x200B;

^(Process finished with exit code 1)

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

also, sometimes the code apparently ends after this line: `driver.get(""`[`https://he.aliexpress.com/`](https://he.aliexpress.com/)`"")`, writing this:

^(""C:\\Program Files\\Python310\\python.exe"" ""C:/לימוד סלניום/s.py"")

^(Traceback (most recent call last):)

  ^(File ""C:\\לימוד סלניום\\)[^(s.py)](https://s.py)^("", line 12, in <module>)

^(driver.get("")[^(https://he.aliexpress.com/)](https://he.aliexpress.com/)^(""))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(webdriver.py)](https://webdriver.py)^("", line 447, in get)

^(self.execute(Command.GET, {'url': url}))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(webdriver.py)](https://webdriver.py)^("", line 435, in execute)

^(self.error\_handler.check\_response(response))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(errorhandler.py)](https://errorhandler.py)^("", line 247, in check\_response)

^(raise exception\_class(message, screen, stacktrace))

^(selenium.common.exceptions.WebDriverException: Message: unknown error: cannot determine loading status)

^(from unknown error: unexpected command response)

  ^((Session info: chrome=103.0.5060.134))

^(Stacktrace:)

^(Backtrace:)

	^(Ordinal0 \[0x00CB6463+2188387\])

	^(Ordinal0 \[0x00C4E461+1762401\])

	^(Ordinal0 \[0x00B63D78+802168\])

	^(Ordinal0 \[0x00B57210+750096\])

	^(Ordinal0 \[0x00B5675A+747354\])

	^(Ordinal0 \[0x00B55D3F+744767\])

	^(Ordinal0 \[0x00B54C28+740392\])

	^(Ordinal0 \[0x00B55228+741928\])

	^(Ordinal0 \[0x00B5EF2F+782127\])

	^(Ordinal0 \[0x00B69FBB+827323\])

	^(Ordinal0 \[0x00B6D310+840464\])

	^(Ordinal0 \[0x00B554F6+742646\])

	^(Ordinal0 \[0x00B69BF3+826355\])

	^(Ordinal0 \[0x00BBCF6D+1167213\])

	^(Ordinal0 \[0x00BAC5F6+1099254\])

	^(Ordinal0 \[0x00B86BE0+945120\])

	^(Ordinal0 \[0x00B87AD6+948950\])

	^(GetHandleVerifier \[0x00F571F2+2712546\])

	^(GetHandleVerifier \[0x00F4886D+2652765\])

	^(GetHandleVerifier \[0x00D4002A+520730\])

	^(GetHandleVerifier \[0x00D3EE06+516086\])

	^(Ordinal0 \[0x00C5468B+1787531\])

	^(Ordinal0 \[0x00C58E88+1805960\])

	^(Ordinal0 \[0x00C58F75+1806197\])

	^(Ordinal0 \[0x00C61DF1+1842673\])

	^(BaseThreadInitThunk \[0x753DFA29+25\])

	^(RtlGetAppContainerNamedObjectPath \[0x77127A9E+286\])

	^(RtlGetAppContainerNamedObjectPath \[0x77127A6E+238\])

&#x200B;

&#x200B;

^(Process finished with exit code 1)",https://www.reddit.com/r/selenium/comments/wicfqg/click_doesnt_work_for_me/,program
696,Proxy Rotating with Amazon API Gateway & LAMBDA,"I want to scrape google with selenium.   
I successfully run the program with lambda but lambda the giving only 1 IP   
so I want to integrate  Amazon API Gateway rotating proxy with my python script  


Any guide would be appreciated. :) 

Thank you",https://www.reddit.com/r/selenium/comments/wib6f7/proxy_rotating_with_amazon_api_gateway_lambda/,program
697,Not able to scrape all the reviews,"I am having trouble scraping the all the reviews from this website [https://www.petsmart.com/dog/food/fresh-food/freshpet-vitalandtradegrain-free-beef-and-bison-adult-dog-food-1095.html?cgid=100248&fmethod=Browse](https://www.petsmart.com/dog/food/fresh-food/freshpet-vitalandtradegrain-free-beef-and-bison-adult-dog-food-1095.html?cgid=100248&fmethod=Browse) . I am only getting old one not getting the recent one even though I have a sort filter parameter `Most Recent` in place. Here is my code:

    import requests
    from bs4 import BeautifulSoup
    from datetime import datetime
    from urllib.parse import unquote
    import math
    import pandas as pd
    
    
    class PetSmartReviewsDetail:
    
        results = []
    
        headers = {
            ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64; rv:108.0) Gecko/20100101 Firefox/108.0"",
            ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"",
            ""Accept-Language"": ""en-US,en;q=0.5"",
            ""Connection"": ""keep-alive"",
            ""Upgrade-Insecure-Requests"": ""1"",
            ""Sec-Fetch-Dest"": ""document"",
            ""Sec-Fetch-Mode"": ""navigate"",
            ""Sec-Fetch-Site"": ""none"",
            ""Sec-Fetch-User"": ""?1"",
            ""Sec-GPC"": ""1"",
        }
    
        params = {
            ""passkey"": ""208e3foy6upqbk7glk4e3edpv"",
            ""apiversion"": ""5.5"",
            ""displaycode"": ""4830-en_us"",
            ""resource.q0"": ""reviews"",
            ""filter.q0"": [
                ""isratingsonly:eq:false"",
                ""productid:eq:69272"",
                ""contentlocale:eq:en,en_US"",
            ],
            ""sort.q0"": ""submissiontime:desc"",
            ""stats.q0"": ""reviews"",
            ""filteredstats.q0"": ""reviews"",
            ""include.q0"": ""authors,products,comments"",
            ""filter_reviews.q0"": ""contentlocale:eq:en,en_US"",
            ""filter_reviewcomments.q0"": ""contentlocale:eq:en,en_US"",
            ""filter_comments.q0"": ""contentlocale:eq:en,en_US"",
            ""limit.q0"": ""30"",
            ""offset.q0"": ""0"",
            ""limit_comments.q0"": ""3"",
        }
    
        def fetch_prod_ids(self, url: str):
            print(f""Fetching product ID's from: {url}"", end="""")
            response = requests.get(url, headers=self.headers)
            print(f"" | Status code: {response.status_code}"")
            soup = BeautifulSoup(response.text, ""lxml"")
            product_ids = [
                link.get(""href"").split(""."")[0].split(""-"")[-1]
                for link in soup.find_all(""a"", {""class"": ""name-link""})
            ]
    
            return product_ids
    
        def parse_review_detail(self, product_id: str, API_URL: str):
            item = {}
            self.params[""filter.q0""][1] = f""productid:eq:{product_id}""
            response = requests.get(API_URL, params=self.params, headers=self.headers)
            print(f""Fetching product review api from: {unquote(response.url)}"", end="""")
            print(f"" | Status code: {response.status_code}"")
            json_blob = response.json()
            total_reviews = round(
                math.ceil(json_blob[""BatchedResults""][""q0""][""TotalResults""]) / 30
            )
    
            for i in range(0, total_reviews):
                review_page = i * 30 + 8
                self.params[""offset.q0""] = review_page
                self.params[""filter.q0""][1] = f""productid:eq:{product_id}""
                response = requests.get(API_URL, params=self.params, headers=self.headers)
                print(
                    f""Fetching paginated product review api from: {unquote(response.url)}"",
                    end="""",
                )
                print(f"" | Status code: {response.status_code}"")
                json_blob = response.json()
                product = json_blob[""BatchedResults""][""q0""][""Includes""][""Products""][
                    f""{product_id}""
                ]
                item[""Product_Name""] = product[""Name""]
                item[""Total_Rating""] = product[""ReviewStatistics""][""AverageOverallRating""]
                item[""User""] = [
                    r[""UserNickname""] for r in json_blob[""BatchedResults""][""q0""][""Results""]
                ]
                item[""Time""] = [
                    datetime.fromisoformat(r[""SubmissionTime""]).date().strftime(""%m-%d-%Y"")
                    for r in json_blob[""BatchedResults""][""q0""][""Results""]
                ]
                item[""Heading""] = [
                    r[""Title""] for r in json_blob[""BatchedResults""][""q0""][""Results""]
                ]
                item[""Detail""] = [
                    r[""ReviewText""] for r in json_blob[""BatchedResults""][""q0""][""Results""]
                ]
                item[""Recommend""] = []
                for recommend in json_blob[""BatchedResults""][""q0""][""Results""]:
                    if recommend[""IsRecommended""] == True:
                        item[""Recommend""].append(""Y"")
                    else:
                        item[""Recommend""].append(""N"")
    
                item[""No_helpful_vote""] = product[""ReviewStatistics""][""NotHelpfulVoteCount""]
                item[""Yes_helpful_vote""] = product[""ReviewStatistics""][""HelpfulVoteCount""]
    
                self.results.append(item)
    
        def to_csv(self):
            df = (
                pd.DataFrame(self.results)
                .fillna("""")
                .explode([""User"", ""Time"", ""Heading"", ""Detail"", ""Recommend""])
            )
    
            df.to_csv(f""petsmart_reviews_details.csv"", index=False)
    
            print('Stored results to ""petsmart_reviews_details.csv""')
    
        def run(self):
            api_url = ""https://api.bazaarvoice.com/data/batch.json""
            base_url = ""https://www.petsmart.com/dog/food/fresh-food/freshpet/?pmin=0.01&srule=best-sellers&format=ajax""
            product_ids = self.fetch_prod_ids(base_url)
            for _id in product_ids:
                self.parse_review_detail(_id, api_url)
            self.to_csv()
    
    
    if __name__ == ""__main__"":
        scraper = PetSmartReviewsDetail()
        scraper.run()

Here is the API url [https://api.bazaarvoice.com/data/batch.json?passkey=208e3foy6upqbk7glk4e3edpv&apiversion=5.5&displaycode=4830-en\_us&resource.q0=reviews&filter.q0=isratingsonly:eq:false&filter.q0=productid:eq:1095&filter.q0=contentlocale:eq:en,en\_US&sort.q0=submissiontime:desc&stats.q0=reviews&filteredstats.q0=reviews&include.q0=authors,products,comments&filter\_reviews.q0=contentlocale:eq:en,en\_US&filter\_reviewcomments.q0=contentlocale:eq:en,en\_US&filter\_comments.q0=contentlocale:eq:en,en\_US&limit.q0=8&offset.q0=0&limit\_comments.q0=3](https://api.bazaarvoice.com/data/batch.json?passkey=208e3foy6upqbk7glk4e3edpv&apiversion=5.5&displaycode=4830-en_us&resource.q0=reviews&filter.q0=isratingsonly:eq:false&filter.q0=productid:eq:1095&filter.q0=contentlocale:eq:en,en_US&sort.q0=submissiontime:desc&stats.q0=reviews&filteredstats.q0=reviews&include.q0=authors,products,comments&filter_reviews.q0=contentlocale:eq:en,en_US&filter_reviewcomments.q0=contentlocale:eq:en,en_US&filter_comments.q0=contentlocale:eq:en,en_US&limit.q0=8&offset.q0=0&limit_comments.q0=3) reviews are coming from. Can anyone please help me out here scrape all the reviews?",https://www.reddit.com/r/webscraping/comments/10fnico/not_able_to_scrape_all_the_reviews/,content
698,data completeness checks for dynamically loading page,"Hi, using Selenium to crawl and complete web scraping. The url loads dynamically and hence playing around with waiting time to fully load. I am not able to get the content-length meta data form the source to check teh data completeness. Is there any other way to check whether downloaded data is complete. As the page is loading dynamically, some long pages are incompletely downloaded. How do we get around this issue? Thanks.",https://www.reddit.com/r/webscraping/comments/10f4g5k/data_completeness_checks_for_dynamically_loading/,content
699,Does anyone knows how to surpass the download restrictions on Telegram?,"So, as title says, Is there anyway to surpass the media content (images/videos) restrictions on some Telegram Channels/Groups? I mean, Telegram some months ago added a function to disable  to download content if the channel/group owner wanted to. And I want to know if it's possible to avoid that blockade and download content anyways.

Thanks in advance.",https://www.reddit.com/r/webscraping/comments/1055xb6/does_anyone_knows_how_to_surpass_the_download/,content
700,thread-safe: a simple tool for saving local copies of your favorite Twitter threads,"Hey Everyone,

Happy new year! I wanted to share a CLI tool I built for saving a local copy of any Twitter thread. It's called thread-safe and it's designed to do just that - keep your favorite threads safe on your local machine. This way you never have to worry that they might one day just disappear...

More seriously, I wanted a simple way to save threads of interest \_without\_ having to use a third-party app that needs access to my Twitter account (or vice-versa) and that forces me to reply to the thread to ""unroll"" or otherwise save a copy.

thread-safe generates an HTML file containing all of a thread's contents including each tweet's text, links, and media attachments (images and videos). This file, all attachments, and a JSON data file are saved to the local filesystem and the HTML can be used to display the thread locally in a browser at any time.

By using a dedicated directory for all generated files, thread-safe can be used to maintain a local library of saved threads that can easily be searched using standard commandline tooling like (rip)grep, fzf, fd, and any other awesome tool of your choice.

I built thread-safe because I often find myself saving links to informative threads and am interested in preserving the author's content. This is exactly the use case that thread-safe addresses: generating local copies of these single-authored, consecutive series of tweets. Notably, thread-safe intentionally does not save the comments or discussion from other users that follow.

[https://github.com/dkaslovsky/thread-safe](https://github.com/dkaslovsky/thread-safe)

I hope you might find it useful and I'd love to take any feedback or suggestions that might come to mind!",https://www.reddit.com/r/webscraping/comments/101f7tx/threadsafe_a_simple_tool_for_saving_local_copies/,content
701,scraping JavaScript based website,"Hi guys, this is my first time taking a task on web scraping, I'm trying to scrape a JavaScript website, the problem is that I'm already using selenium by certain tags don't seem to be scraped even tho they appear in inspect tool and after disabling JavaScript

If anyone can help i will be sooo grateful, and thank you in advance

This is the link of the website: https://www.ouedkniss.com/services-evenements-divertissement/1

I'm trying to get the emails if those announcements: 

from bs4 import BeautifulSoup
from selenium.webdriver import Chrome
from selenium.webdriver.chrome.service import Service
from selenium.webdriver import ChromeOptions
import pandas as pd

Options = ChromeOptions()
Options.headless = True
driver_service = Service(executable_path=r""C:\Users\Lilia\Desktop\WebScraping\chromedriver.exe"")
driver = Chrome(service=driver_service)
driver.get('https://www.ouedkniss.com/services-evenements-divertissement/1')
soup = BeautifulSoup(driver.page_source, 'lxml')


def Extract_Emails():
    elements = soup.findAll('div', class_=""col-sm-6 col-md-4 col-lg-3 col-12"")
    script = soup.find('script')
    print(script)
    
    for element in elements:
      link = element.find('a', class_='d-flex flex-column flex-grow-1 v-card v-card--link v-sheet o-announ-card-content theme--dark')
      driver.get('https://www.ouedkniss.com'+link['href'])
      soup1 = BeautifulSoup(driver.page_source, 'lxml')
      email = soup1.find('span', class_='v-chip__content')
      print(email)

Sorry, i couldn't attach the pic of the output of the inspect tool",https://www.reddit.com/r/webscraping/comments/zydznc/scraping_javascript_based_website/,content
702,Scrape Fb post from group and push to web?,"Just came across this website that seems to be pulling content from their group automatically. They are then segmenting the data and pushing it to a site where they are allowing someone to play with filtering options like a live excel. Really curious if this is custom or this a platform providing this service?

The website is larvato.com 
(Not my site obviously)",https://www.reddit.com/r/webscraping/comments/zxyu63/scrape_fb_post_from_group_and_push_to_web/,content
703,Unable to get text from simple <div>,"Can someone help me this. I was able to get to this div. But when I tried to get the text 'time is now', it is not grabbing that. Here is the html sniplet:

`<div class=""col-md-9"">`

`<span id=""clock""></span><i class=""far fa-clock small mr-1""></i>time is now`

`</div>`

However, when I tried to read time using Python

`timetext = response.xpath(""//div[@id='ContentPlaceHolder1_maintable']/div[4]/div/div[2]"")[0].text`

I received no value. If I tried to 

`timetext = response.xpath(""//div[@id='ContentPlaceHolder1_maintable']/div[4]/div/div[2]"").text`

Then I am getting an error *'list' object has no attribute 'text'*

I know that I am able to read the correct div as I was able to read everything else. Thanks for your help.

TP",https://www.reddit.com/r/webscraping/comments/zuhjhf/unable_to_get_text_from_simple_div/,content
704,Why does the same beautifulSoup code not work for certain real estate sites?,"Just for practice, I’m trying to scrape real estate websites using the same basic code structure each time. For some reason it doesn’t work with this website. Here is my code:

import requests
from bs4 import BeautifulSoup
url = 'https://www.rentalia.com/holiday-rentals-italy/'
page = requests.get(url)
soup = BeautifulSoup(page.content, 'html.parser')

lists = soup.find_all(""div"", class_=""itemList itemRow col ng-scope s12 m6 l4"")
print(lists)

This code returns an empty list",https://www.reddit.com/r/webscraping/comments/zti5v1/why_does_the_same_beautifulsoup_code_not_work_for/,content
705,Why can't beautiful soup scrape this table?,"I am trying to scrape this [site](https://www.basketball-reference.com/boxscores/202110220LAL.html).

There are two tables on this site that I can't retrieve with beautiful soup. Their ids are ""line\_score"" and ""four\_factors"".

I am able to access all the other tables by their IDs, except this one. Any ideas?

This is my Python code:

`import requests`  
`a = requests.get(""https://www.basketball-reference.com/boxscores/202110220LAL.html"")`  
`from bs4 import BeautifulSoup`  
`soup = BeautifulSoup(a.content)`  
`soup.select(""#line_score"")`

This is the HTML of the table: 

`<table class=""suppress_all stats_table"" id=""line_score"" data-cols-to-freeze=""0"">`

`<caption>Line Score Table</caption>`



&#x200B;

   `<colgroup><col><col><col><col><col><col></colgroup>`

   `<thead>`

&#x200B;



`<tr class=""over_header"">`

`<th aria-label="""" data-stat=""header_tmp"" colspan=""6"" class="" over_header center"">Scoring</th>`

`</tr>`



`<tr>`

`<th aria-label=""&nbsp;"" data-stat=""team"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">&nbsp;</th>`

`<th aria-label=""1"" data-stat=""1"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">1</th>`

`<th aria-label=""2"" data-stat=""2"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">2</th>`

`<th aria-label=""3"" data-stat=""3"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">3</th>`

`<th aria-label=""4"" data-stat=""4"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">4</th>`

`<th aria-label=""T"" data-stat=""T"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">T</th>`

`</tr>`

`</thead>`

`<tbody><tr><th scope=""row"" class=""center "" data-stat=""team""><a href=""/teams/PHO/2022.html"">PHO</a></th><td class=""center "" data-stat=""1"">23</td><td class=""center "" data-stat=""2"">34</td><td class=""center "" data-stat=""3"">37</td><td class=""center "" data-stat=""4"">21</td><td class=""center "" data-stat=""T""><strong>115</strong></td></tr>`

`<tr><th scope=""row"" class=""center "" data-stat=""team""><a href=""/teams/LAL/2022.html"">LAL</a></th><td class=""center "" data-stat=""1"">26</td><td class=""center "" data-stat=""2"">18</td><td class=""center "" data-stat=""3"">23</td><td class=""center "" data-stat=""4"">38</td><td class=""center "" data-stat=""T""><strong>105</strong></td></tr>`

&#x200B;

`</tbody></table>`",https://www.reddit.com/r/webscraping/comments/zmuu0a/why_cant_beautiful_soup_scrape_this_table/,content
706,Next level web scrapers,"So a quick background, I am a fairly intermediate web scraping developer created 100s of projects with different combinations of selenium, playwright, scrapy, puppetteer or plain curl requests and bs4 rendering in Python or nodejs or even Java. 

I know threading techniques as well as proxies, anti detection, bypassing HCaptcha and fun captcha, bezier mouse movements for test score, erc.

I have always wondered how some companies out there like phantomBuster create very fast scrapers for things like Linkedin for example?

Now I started my Own personal scraping project to test this out and I have used possibly every combination of tools and every thing I could think of to make my scraping project go faster but It will never be as fast or even 50% as fast as those online website.

For example, there was a public post with 8k comments that I wanted to get the people name to test rhe scraper. And I had to wait until all comments are loaded (which I have to trigger the 'load more' button) to get the names.

After good 13 mins, I got the list. But using phantom buster, I got it after 2 mins only!

How do they even get it that fast with JS involved?? 

I remember Instagram had a secret special character that you put at the end of URL to make it load data without Js or something but I don't know about Linkedin.

I am not talking here about bot detection and blocking or bypassing log in or anything. Just the speed of scraping js-rely website.

EDIT 1: 

After karllorey, comment about the mobile version, let me say I did a similar experiment with Facebook. 

So there was a project that we wanted to scrap the posts of a selling group we owned on Facebook that had more than 1500 daily posts in India. The project involved getting data that could reach to a point where the V8 JS engine of Chrome/Firefox couldn't handle it anymore. 

So, I found an older version of the Facebook site targeted at old mobiles that only load 20 or 40 posts at a moment, and when you press load more, it destroys the previous DOM. 

It was overall faster and the memory was doing great. 

Do you think LinkedIn could have a similar platform or something? 

I don't think PhantomBuster save the posts in thread though or something similar because I tried on different post types and they were all similar results average of 2-2:30 mins.

Bonus Content about Benchmarking and testing: 

My experiment involved benchmarking the time from the moment the comments section were loaded, didn't take into consideration the startup time or anything else (because they can be altered by reusing the same browser and different optimizations) 

I also tested different tools all on the same post including selenium, playwright and other libraries that could parse JS. 

I believe as @VestaM mentioned in his comments it is about API exploiting but maybe it takes too much time to research it.",https://www.reddit.com/r/webscraping/comments/zlxotc/next_level_web_scrapers/,content
707,is zillow still scrapable with python bs4 2022?,"I tried to scrape it and I couldnt get any data back. I outputted the driver.page\_source to the terminal since that is that i pass that into the soup object but the terminal output displays html text that indicate the site was not found.

    <html xmlns=""http://www.w3.org/1999/xhtml"" data-l10n-sync=""true"" dir=""ltr"" lang=""en-US"">
      <head>
        <meta http-equiv=""Content-Security-Policy"" content=""default-src chrome:; object-src 'none'"" />
        <meta name=""color-scheme"" content=""light dark"" />
        <title data-l10n-id=""neterror-dns-not-found-title"">Server Not Found</title>
        <link rel=""stylesheet"" href=""chrome://global/skin/aboutNetError.css"" type=""text/css"" media=""all"" />
        <link rel=""icon"" id=""favicon"" href=""chrome://global/skin/icons/info.svg"" />
        <link rel=""localization"" href=""branding/brand.ftl"" />
        <link rel=""localization"" href=""toolkit/neterror/certError.ftl"" />
        <link rel=""localization"" href=""toolkit/neterror/netError.ftl"" />
      </head>
    
      <body class=""neterror"">
        <!-- PAGE CONTAINER (for styling purposes only) -->
        <div class=""container"">
          <div id=""text-container"">
            <!-- Error Title -->
            <div class=""title"">
              <h1 class=""title-text"" data-l10n-id=""dnsNotFound-title"">Hmm. We’re having trouble finding that site.</h1>
            </div>
    
            <!-- Short Description -->
            <p id=""errorShortDesc"">We can’t connect to the server at automationcontrolled. <span data-l10n-id=""neterror-dns-not-found-with-suggestion"" data-l10n-args=""{&quot;hostAndPath&quot;:&quot;www.automationcontrolled.com&quot;}"">Did you mean to go to <a href=""https://www.automationcontrolled.com/"" data-l10n-name=""website"">www.automationcontrolled.com</a>?</span></p>
            <p id=""errorShortDesc2""></p>
    
            <div id=""errorWhatToDo"" hidden="""">
              <p id=""errorWhatToDoTitle"" data-l10n-id=""certerror-what-can-you-do-about-it-title"">What can you do about it?</p>
              <p id=""badStsCertExplanation"" hidden=""""></p>
              <p id=""errorWhatToDoText""></p>
            </div>
    
            <!-- Long Description -->
            <div id=""errorLongDesc""><span data-l10n-id=""neterror-dns-not-found-hint-header""><strong>If you entered the right address, you can:</strong></span><ul><li data-l10n-id=""neterror-dns-not-found-hint-try-again"">Try again later</li><li data-l10n-id=""neterror-dns-not-found-hint-check-network"">Check your network connection</li><li data-l10n-id=""neterror-dns-not-found-hint-firewall"">Check that Firefox has permission to access the web (you might be connected but behind a firewall)</li></ul></div>
    
            <p id=""tlsVersionNotice"" hidden=""""></p>
    
            <p id=""learnMoreContainer"" hidden="""">
              <a id=""learnMoreLink"" target=""_blank"" rel=""noopener noreferrer"" data-telemetry-id=""learn_more_link"" data-l10n-id=""neterror-learn-more-link"" href=""https://support.mozilla.org/1/firefox/107.0.1/Linux/en-US/connection-not-secure"">Learn more…</a>
            </p>
    
            <div id=""openInNewWindowContainer"" class=""button-container"" hidden="""">
              <p><a id=""openInNewWindowButton"" target=""_blank"" rel=""noopener noreferrer"">
              <button class=""primary"" data-l10n-id=""open-in-new-window-for-csp-or-xfo-error"">Open Site in New Window</button></a></p>
            </div>
    
            <!-- UI for option to report certificate errors to Mozilla. Removed on
                 init for other error types .-->
            <div id=""prefChangeContainer"" class=""button-container"" hidden="""">
              <p data-l10n-id=""neterror-pref-reset"">It looks like your network security settings might be causing this. Do you want the default settings to be restored?</p>
              <button id=""prefResetButton"" class=""primary"" data-l10n-id=""neterror-pref-reset-button"">Restore default settings</button>
            </div>
    
            <div id=""certErrorAndCaptivePortalButtonContainer"" class=""button-container"" hidden="""">
              <button id=""returnButton"" class=""primary"" data-telemetry-id=""return_button_top"" data-l10n-id=""neterror-return-to-previous-page-recommended-button"">Go Back (Recommended)</button>
              <button id=""openPortalLoginPageButton"" class=""primary"" data-l10n-id=""neterror-open-portal-login-page-button"" hidden="""">Open Network Login Page</button>
              <button id=""certErrorTryAgainButton"" class=""primary try-again"" data-l10n-id=""neterror-try-again-button"" hidden="""">Try Again</button>
              <button id=""advancedButton"" data-telemetry-id=""advanced_button"" data-l10n-id=""neterror-advanced-button"">Advanced…</button>
            </div>
          </div>
    
          <div id=""netErrorButtonContainer"" class=""button-container""><button class=""primary try-again"" data-l10n-id=""neterror-try-again-button"">Try Again</button>
            
          </div>
    
          <div class=""advanced-panel-container"">
            <div id=""badCertAdvancedPanel"" class=""advanced-panel"" hidden="""">
              <p id=""badCertTechnicalInfo""></p>
              <a id=""viewCertificate"" href=""javascript:void(0)"" data-l10n-id=""neterror-view-certificate-link"">View Certificate</a>
              <div id=""advancedPanelButtonContainer"" class=""button-container"">
                <button id=""advancedPanelReturnButton"" class=""primary"" data-telemetry-id=""return_button_adv"" data-l10n-id=""neterror-return-to-previous-page-recommended-button"">Go Back (Recommended)</button>
                <button id=""advancedPanelTryAgainButton"" class=""primary try-again"" data-l10n-id=""neterror-try-again-button"" hidden="""">Try Again</button>
                <button id=""exceptionDialogButton"" data-telemetry-id=""exception_button"" data-l10n-id=""neterror-override-exception-button"">Accept the Risk and Continue</button>
              </div>
            </div>
    
            <div id=""blockingErrorReporting"" class=""advanced-panel"" hidden="""">
              <p class=""toggle-container-with-text"">
                <input type=""checkbox"" id=""automaticallyReportBlockingInFuture"" role=""checkbox"" />
                <label for=""automaticallyReportBlockingInFuture"" data-l10n-id=""neterror-error-reporting-automatic"">Report errors like this to help Mozilla identify and block malicious sites</label>
              </p>
            </div>
    
            <div id=""certificateErrorDebugInformation"" class=""advanced-panel"" hidden="""">
              <button id=""copyToClipboardTop"" data-telemetry-id=""clipboard_button_top"" data-l10n-id=""neterror-copy-to-clipboard-button"">Copy text to clipboard</button>
              <div id=""certificateErrorText""></div>
              <button id=""copyToClipboardBottom"" data-telemetry-id=""clipboard_button_bot"" data-l10n-id=""neterror-copy-to-clipboard-button"">Copy text to clipboard</button>
            </div>
          </div>
        </div>
      </body>
      <script src=""chrome://global/content/neterror/aboutNetErrorCodes.js""></script>
      <script type=""module"" src=""chrome://global/content/aboutNetError.mjs""></script>
    </html>",https://www.reddit.com/r/webscraping/comments/zjho5f/is_zillow_still_scrapable_with_python_bs4_2022/,content
708,Help with a project,"First project! I’m having troubles getting this code to scrape all the team names , expects points etc from this website: https://rugby4cast.com/predictions/
It is only showing the titles in the HTML 


This is the code I’m using:

import requests
from bs4 import BeautifulSoup
import pandas as pd

# Create an empty list to store the data
data = []

# Fetch the HTML content of the page using the requests library
url = 'https://rugby4cast.com/predictions/'
response = requests.get(url)

# Parse the HTML content using Beautiful Soup
soup = BeautifulSoup(response.content, 'html.parser')

# Use Beautiful Soup to find the HTML elements that contain the data you want to extract
date_elements = soup.find_all('span', attrs={'class': 'champ_date'})
home_team_name_elements = soup.find_all('div', attrs={'class': 'team_name'})
away_team_name_elements = soup.find_all('div', attrs={'class': 'team_name'})

# Loop through the date elements and extract the text from each element
for date_element in date_elements:
    date_text = date_element.text.strip()
    print(f'Date: {date_text}')

# Loop through the home team name elements and extract the text from each element
for home_team_name_element in home_team_name_elements:
    home_team_name_text = home_team_name_element.text.strip()
    print(f'Home team: {home_team_name_text}')

# Loop through the away team name elements and extract the text from each element
for away_team_name_element in away_team_name_elements:
    away_team_name_text = away_team_name_element.text.strip()
    print(f'Away team: {away_team_name_text}')

    # Add the data for the teams to the `data` list
    data.append([date_text, home_team_name_text, away_team_name_text])


# Create a Pandas DataFrame with the extracted data
df = pd.DataFrame(data, columns=['Date', 'Home', 'Home xP', 'Away', 'Away xP'])

# Write the data from the Pandas DataFrame to an HTML file
df.to_html('rugby_predictions_bs.html')",https://www.reddit.com/r/webscraping/comments/zhughh/help_with_a_project/,content
709,Help required for html parsing,"For one of our NLP project, we have scraped data (downloaded raw data in html format) form a web portal. We are trying to parse the html and finding difficulties as the template they have used in the portal varies over the period of time and quite difficult as sometimes they don't even use proper styling for headers or etc.. We have to identify header, divions, sub division and contents. This hierarchy style is not consistent as they have used several templates to build the portal.   
Any ideas on how to tackle the situation?",https://www.reddit.com/r/webscraping/comments/z9legw/help_required_for_html_parsing/,content
710,Website blocking me from one computer but not another one. I can't figure the difference.,"**TL;DR: Why does a site block one machine but not the other? They are both on the same LAN (same IP address) and are sending identical headers.**

I have a Python script that runs once per day to grab price data from a website. The script visits 5 different pages on the website with a 10 second gap between visits. It doesn't do anything crazy like sending it a million requests per second. Example page that triggers a 403 error: [https://www.apmex.com/product/1/1-oz-american-gold-eagle-coin-bu-random-year](https://www.apmex.com/product/1/1-oz-american-gold-eagle-coin-bu-random-year)

In the past couple weeks, I have been getting 403 Connection Refused errors. When I went to troubleshoot it, I found that the script still worked fine when I ran it from a different computer. We are all on the same LAN, so the target website would see the same IP address from both computers.

I went to [this site](https://www.whatismybrowser.com/detect/what-http-headers-is-my-browser-sending) with a python script to check the get request headers. Here are the headers it reported:

&#x200B;

||Working Computer|Not Working computer|
|:-|:-|:-|
|ACCEPT|`*/*`|same|
|**ACCEPT-ENCODING**|gzip, deflate, br|gzip, deflate (no 'br')|
|**CONNECTION**|keep-alive|same|
|**CONTENT-LENGTH**|||
|**CONTENT-TYPE**|||
|**USER-AGENT**|python-requests/2.28.1|python-requests/2.25.1|

So I updated the python requests module and modified the headers but I am still getting blocked. Any ideas how to fix?

## Here is the code:

    >>> import requests
    >>> from requests.adapters import HTTPAdapter, Retry
    >>> url = 'https://www.apmex.com/product/1/1-oz-american-gold-eagle-coin-bu-random-year'
    >>> s = requests.Session()
    >>> s.headers.update({'Accept-Encoding': 'gzip, deflate, br'})
    >>> retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504, 429])
    >>> s.mount('http://', HTTPAdapter(max_retries=(retries)))
    >>> # Here are the headers that I will send:
    >>> s.headers
    {'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'}
    >>> res = s.get(url)
    >>> res.status_code
    403",https://www.reddit.com/r/webscraping/comments/z7d2u1/website_blocking_me_from_one_computer_but_not/,content
711,How do you scrape lazy load sites,"So I'm trying to scrape this site [https://www.bergstromlexus.com/](https://www.bergstromlexus.com/) and it's rendered in javascript and a lazy load site. Usually when I come across these I do one of the following:

\-Send API requests (I'm trying to get product data from different categories so I can't do this)

\-Use requests-html (It isn't rendering the contents of the page where the cars are listed in the categories)

\-Use playwright (It isn't returning all the data and usually for these types of sites I have encountered there's something I can click on to make the script scroll down the page and load the data I need but there's none on this site)

Is there another method I could use or something I could change in the methods I tried already that'll help me scrape the data. Thanks",https://www.reddit.com/r/webscraping/comments/z4do13/how_do_you_scrape_lazy_load_sites/,content
712,Buffer full error from calling an API too many times,"    Traceback (most recent call last):
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connection.py"", line 174, in _new_conn
        conn = connection.create_connection(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\util\connection.py"", line 95, in create_connection
        raise err
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\util\connection.py"", line 85, in create_connection
        sock.connect(sa)
    OSError: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 703, in urlopen
        httplib_response = self._make_request(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 386, in _make_request
        self._validate_conn(conn)
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 1042, in _validate_conn
        conn.connect()
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connection.py"", line 358, in connect
        self.sock = conn = self._new_conn()
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connection.py"", line 186, in _new_conn
        raise NewConnectionError(
    urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000259E25E06A0>: Failed to establish a new connection: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\requests\adapters.py"", line 489, in send
        resp = conn.urlopen(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 815, in urlopen
        return self.urlopen(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 815, in urlopen
        return self.urlopen(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 815, in urlopen
        return self.urlopen(
      [Previous line repeated 7 more times]
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 787, in urlopen
        retries = retries.increment(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\util\retry.py"", line 592, in increment
        raise MaxRetryError(_pool, url, error or ResponseError(cause))
    urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.macmap.org', port=443): Max retries exceeded with url: /api/results/custom-duties-by-year?reporter=682&partner=784&product=940520&year=2021 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000259E25E06A0>: Failed to establish a new connection: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full'))
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""c:\Users\system_int\Documents\DataportalCrawlers\MacMap Data\macmap3.py"", line 121, in <module>
        final_data = My_function(
      File ""c:\Users\system_int\Documents\DataportalCrawlers\MacMap Data\macmap3.py"", line 46, in My_function
        response = s.get(url, headers=user_agent, verify=False)
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\requests\sessions.py"", line 600, in get
        return self.request(""GET"", url, **kwargs)
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\requests\sessions.py"", line 587, in request
        resp = self.send(prep, **send_kwargs)
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\requests\sessions.py"", line 701, in send
        r = adapter.send(request, **kwargs)
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\requests\adapters.py"", line 565, in send
        raise ConnectionError(e, request=request)
    requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.macmap.org', port=443): Max retries exceeded with url: /api/results/custom-duties-by-year?reporter=682&partner=784&product=940520&year=2021 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000259E25E06A0>: Failed to establish a new connection: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full'))

im getting this error when im trying to get data from an api and appending that data inside a dictionary.

then at the end im turning that dict into a dataframe using pandas to convert it to\_excel()

im getting the above error after 16334 call.

how can i fix that? ive tried the following:

    url = f'https://www.macmap.org/api/results/custom-duties-by-year?reporter=[dest]&partner=[expo]&product=[hs]&year=[yr]'
    # retry connection in case of error. you can also check all codes using re.status_codes._codes
    retries = Retry(total=10, backoff_factor=1200, status_forcelist=[
    				500, 502, 503, 504, 509])
    # mount the requests of our url with retries
    s.mount(url, HTTPAdapter(max_retries=retries))
    # try response from url
    try:
    	response = s.get(url, headers=user_agent, verify=False)
    # if response is ok get the data
    	if response.status_code == 200:
    		all_data = response.json()
    except ConnectionError as e:
    	print('Error : ' + e)
    	print('status code: ' + response.status_code)
    	print('content: ' + str(response.content))
    	time.sleep(800)
    	response = s.get(url, headers=user_agent, verify=False)
    	if response.status_code == 200:
    		all_data = response.json()

my exception is not catching the error and i tried to set backoff\_factor=1200 so to wait 20 mins incase a call fails but im still getting this same error.

what can i do?",https://www.reddit.com/r/webscraping/comments/yxpisa/buffer_full_error_from_calling_an_api_too_many/,content
713,Custom shortcuts to automate access AI on your Mac," 

Hello folks,

I'm always amazed by the power of GPT-3 and Open AI.

This post is a combination of both information and promotion, so please bear with me.

I've always wanted to use AI directly on my phone and computer, without having to go to OpenAI's playground or ChatGPT in the browser.

For that, I created a tiny Mac app called Elephas. I have shared the app in this group [in the past](https://www.reddit.com/r/Automate/comments/1027x84/using_gpt3_to_automate_content_repurposing_for/) as well and got some amazing feedback from the members :)

Since then many users have asked for the ability to add custom commands in the app, that they can use to invoke AI on their computer.

So I've just shipped a feature called ""Snippets"".

With this, **now you can assign custom shortcuts to OpenAI prompts and use them in your day-to-day workflow wherever necessary.**

This is how it works -

https://reddit.com/link/10eau6v/video/401eus15ilca1/player

 There are many more such utility features that can help you get the power of AI in your daily work.

You can get the app here - [Elephas](https://elephas.app/?ref=rAutomate-snippets)

You can try it out for FREE for 7 days.

Appreciate your feedback.

Do let me know any new features that you would like to see in the app.

Thanks",https://www.reddit.com/r/Automate/comments/10eau6v/custom_shortcuts_to_automate_access_ai_on_your_mac/,content
714,Using GPT-3 to automate content repurposing for social media,"Hello folks,

It's crazy how versatile and powerful GPT-3 and Open AI are.

This post is a combination of both information and promotion, so please bear with me.

I got some great feedback and support from the members of this subreddit on [my last post.](https://www.reddit.com/r/Automate/comments/z7sofx/using_gpt3_to_reply_to_automate_email_replies/) So sharing a post here again.

Many of our users had been asking for the ability to repurpose their existing blog and newsletter content into social media posts.

They are mostly busy content writers so this can be really useful to them in their day-to-day work.

So I tried a simple prompt - ""Summarize this for a tweet""

I took the content from an [OpenAI Blog](https://openai.com/blog/our-approach-to-alignment-research/) and summarized it into a tweet.

&#x200B;

https://preview.redd.it/lro33pbout9a1.png?width=800&format=png&auto=webp&v=enabled&s=641e3cbac828c2e0e4b5c4ff06a2568b3ec424e5

 

Next, I tried another prompt - ""Summarize this into a LinkedIn post""

And that worked alright as well.

&#x200B;

https://preview.redd.it/6rf7k1pput9a1.png?width=800&format=png&auto=webp&v=enabled&s=a8f0d633897284bff66e55a0992afe8be800c44e

 

Finally, I tried this prompt - ""Summarize this into a Facebook post.""

&#x200B;

https://preview.redd.it/3bpnuh4sut9a1.png?width=800&format=png&auto=webp&v=enabled&s=48d77576627b8acc3c78dea5b93232ff6b891f83

 

These prompts worked well so I decided to integrate them into our Mac app, and the users loved it.

Here is the final demo of how it works inside my app -

&#x200B;

https://reddit.com/link/1027x84/video/m8qtr6rtut9a1/player

 

It can be difficult to copy and paste the content into the playground.

If you have a Mac and want to do this more straightforward way then please try out my app [Elephas](https://elephas.app?ref=rAutomate-socialrepurpose)

I have built many such utilities into the app to help you use AI on a daily basis.

You can try it for free for 7 days.

Do share your feedback.

Hope you find it useful

Thanks",https://www.reddit.com/r/Automate/comments/1027x84/using_gpt3_to_automate_content_repurposing_for/,content
715,Automate scanned files to text-searchable PDF OCR conversion using OCRvision," 

Just drop your scanned files into a folder.

OCRvision software will OCR them and add an invisible text layer to the document.

After OCR, your scanned file content will appear in the text search results.

[https://www.ocrvision.com](https://www.ocrvision.com/?source=Reddit)",https://www.reddit.com/r/Automate/comments/zm4oi9/automate_scanned_files_to_textsearchable_pdf_ocr/,content
716,Chatbot requirements: technical and non-technical things to consider when everyone talks about ChatGPT,"Hi there! Just want to share some tips on how to craft the right chatbot when everyone talks about ChatGPT. First of all, a custom chatbot company or any chatbot platform that does custom integration can integrate your chatbot with ChatGPT instead of Dialogflow. So yeah, you can have an outstanding customer service chatbot that can handle other topics. However, the right question is should you? 

If you want a chatbot that does solve issues, not creates more, you must start with **the proper requirements.** Well-structured chatbot requirements lay the right foundation for your future chatbot development.  ChatGPT is just one of the options of how you can use AI and automation and may be not the best depending on your budget and goals. 

**Your chatbot requirements should include these steps:** 

\- defining the main problem you want to solve with the chatbot, 

\- measuring the impact of the problem,

\- determining the main chatbot goal/objective, 

\- understanding the market and target audience 

\- paying attention to the ""internal audience"" of the chatbot (the people or the team in your company who will be working with the chatbot).

Imagine you have found a problem when analyzing customer feedback. Most customers are saying the customer service response time is very long, and that's why they are giving you a low rating.

Your objective for the chatbot could sound like this: *""Decrease waiting time to 1 minute by the end of Q3 2023""* or *""Improve customer service response time from 18 minutes to 1 minute in the next Q""*

Having done this part, you can move to the next step, drafting the technical chatbot requirements. 

When working on the tech requirements, think about the following things:

* **Channels.** Which channels do you want your chatbot to be on? [Website](https://botscrew.com/blog/how-to-build-a-website-bot/), [WhatsApp](https://botscrew.com/blog/a-step-by-step-guide-to-create-chatbot-for-whatsapp-for-business/), Facebook, [SMS](https://botscrew.com/blog/sms-chatbot-a-complete-guide-for-business-use-cases/), [Instagram](https://botscrew.com/blog/instagram-chatbot/), email, etc.
* **Languages.** Which languages do you want your chatbot to “speak”? English, French, German, Arabian, etc? Should it speak one language or multiple?
* **Integrations.** Which tools do you need the chatbot to be integrated with? CRM, payment system, calendars, maps, custom internal tool, etc.
* **Chatbot's look and tone of voice.** If you have a specific vision of the chatbot, be sure to include this in the requirements. Also, if you have a very prominent brand personality and tone of voice, include that in your requirements as well.
* **KPIs and metrics.** Be sure to specify if you have any specific [metrics and KPIs](https://botscrew.com/blog/chatbot-metrics/) you have that you want the chatbot to meet.
* **Analytics and Dashboards.** Do you want the analytics to be in real-time? Are there any specific data you want to have on your dashboard like the number of users, automation rate, etc?
* **Technologies.** Do you have any specific technologies you want the chatbot to be built with? Is ChatGPT the right one for you? What are limitations of ChatGPT? 
* **NLP and AI.** Do you want the chatbot to have decision tree logic, Machine Learning (ML), Natural Language Processing (NLP), or Artificial intelligence (AI)?
* **Accessibility.** Do you need to meet some specific accessibility requirements like WCAG or ADA?
* **Users.** How many people from your team are going to use the chatbot? How many of your customers or conversations do you expect to use the chatbot?
* **Rich media**. Should the chatbot’s responses include text, hyperlinks, images, gifs, video, and PDF attachments?
* **Security.** Do you have any specific security measures and requirements you want the vendor or the chatbot to meet?
* **Hosting.** Where the chatbot and the user data will be hosted: on your own servers or on the cloud? If on the cloud, what will be the cloud service provider and server's location?

You can consider chatbot development and decide on chatbot vendors when you have a chatbot requirements outline. Here you can find what criteria to have [when deciding between chatbot vendors](https://botscrew.com/blog/essential-chatbot-requirements/?utm_source=RedditDecember&utm_medium=&utm_campaign=&utm_term=&utm_content=).",https://www.reddit.com/r/Automate/comments/zjzcsq/chatbot_requirements_technical_and_nontechnical/,content
717,AI + Customized templates to automate writing,"Hello folks, 

I'm a big fan of GPT-3 and Open AI.

This post is a combination of information and a bit of promotion, so please bear with me. 

You can use OpenAI to write most types of content like Blog posts, emails, ad copy etc.

But writing the prompts can be tedious, especially if you have to write the same type of content over and ove again.

You can solve this problem by using your own presets (common structure for every type of content)

 For example - there are 2 steps you need to follow to generate blog posts:

1. Step 1 - Define a common structure for all blog posts. A starting point you always use.
2. Step 2 - Pick a topic, and ask AI to generate a blog post on the topic that fits that structure.

You can do this for free in OpenAI's ""Playground""

I also included this feature in my Mac app. Here's what it looks like - 

https://reddit.com/link/zgzdx8/video/gmbhh5nhxv4a1/player

This is a massive time saver for creating any type of content. 

And the part that my users are loving the most is that they can define their own presets and truly make the AI work according to their wishes

Customized to their unique requirements  🤩

If you find this interesting, you might like my app.

There are many such tiny utilities I've built on top of OpenAI. (Including an AI keyboard for the iPhone)

You can try it for free for 7 days from the following link - [Elephas](https://elephas.app/?utm_campaign=rautomate-custompresets)

Do share your feedback.

Thanks",https://www.reddit.com/r/Automate/comments/zgzdx8/ai_customized_templates_to_automate_writing/,content
718,Join us in our E-commerce Media Automation Hackathon,"Inviting automation enthusiasts to build apps to automate the future of E-commerce 🤖

**When?** 11-17 November 2022

**Location:** Virtual 🌐

**Prize:** USD $200 cash prize + media rendering API credits to help you launch your product

Details below ⬇️

Over the past decade, we’ve witnessed a rapid digital transformation in online commerce. Content like user-generated content, testimonials, product images, data-driven creatives, etc in the form of digital media has been a crucial part of marketing for sellers. The demand for product content is only going to grow as more users adopt online shopping.

To keep up with this demand, we need innovative and efficient ways to tackle these problems. That is exactly what this hackathon aims to solve.

What can you build for this hackathon?

You can build anything that uses the Shotstack API to generate media and helps the E-Commerce sellers. Some project ideas could be:

* An automated product video generator plugin for an E-Commerce platform like Shopify, Amazon, Wix, WooCommerce, Ebay, BigCommerce, Magento, Etsy, etc.
* Web apps that automatically generate product content tailored for e-commerce.
* E-commerce tailored design marketplace.
* A personalized video bot for e-commerce platforms that automatically generates personalized videos when customers buy.
* Data-driven automated media generator for re-marketing based on customer behavior.
* Testimonial banner generator that automatically generates content from product reviews for social media marketing

and much more. To see it in action, try our [automated product promo video generator](https://shotstack.io/use-cases/scenarios/api/generate-promo-videos/) from code.

How to participate?

* Submit the [hackathon entry form](https://forms.gle/Fik7gpk2Qi81mrqe7)
* Sign up for a free [Shotstack account](https://dashboard.shotstack.io/register) to get your API key
* Use Shotstack API to build your submission
* Submit [your project here](https://forms.gle/dwAozLyXFT2B7dz96) before 17 November 2022, 11:59 p.m.(PT)

[**Visit our website**](https://shotstack.io/learn/shotstack-hackathon/) to learn more about this Hackathon.

If you have any questions, then use our [community forum thread.](https://community.shotstack.io/t/media-automation-for-e-commerce-hackathon/376)

**We can’t wait to see what you build!**",https://www.reddit.com/r/Automate/comments/yrwhnn/join_us_in_our_ecommerce_media_automation/,content
719,Chatbots in Marketing: 6 real examples,"Hi! There're plenty of articles about chatbot benefits for marketing, but not so many about real examples of how companies use chatbots for marketing. 🙃 So I gathered 6 chatbot examples from companies like Virgin Holidays, Coca-Cola, Choose Chicago, Honda, etc., with campaign descriptions, results, and stories behind the development of these chatbots.

Info includes anything I could get from media like Adweek, Drift, Campaign Bried to Forbes, Google Business, and my company's experience. 

[Read it here](https://botscrew.com/blog/chatbots-in-marketing-6-examples-on-how-to-wow-your-customers/?utm_source=Reddit_November&utm_medium=&utm_campaign=&utm_term=&utm_content=)

Let me know if it is helpful!",https://www.reddit.com/r/Automate/comments/yj6vt5/chatbots_in_marketing_6_real_examples/,content
720,Research on customers&chatbots. You'll find insights on what to improve in your customer service automation,"Hi, there! My team prepared the report on what customers love and hate about customer service chatbots. Together with our project managers, and marketers we analyzed data of thousands of conversations with real users, and made thi reserach with insigts, stats and recommendations on how to overcome struggels. 

This research would be helpful for: 

* Companies that have a chatbot that isn't performing well and they don't know why 
* Companies who want to implement a chatbot but are unsure about the best ways of creating a chatbot. 

**Important things!** 

*Here're more details about our research:*

&#x200B;

Use case: Customer support

Platform: Website

Language: English

Locations: Europe, USA

Industries: E-commerce, Retail, Retail health

&#x200B;

Here's the link to [the general article on the best chatbot practices and there you'll find the research](https://botscrew.com/blog/chatbot-best-practices/?utm_source=RedditResearchpromotion&utm_medium=&utm_campaign=&utm_term=&utm_content=).",https://www.reddit.com/r/Automate/comments/y761co/research_on_customerschatbots_youll_find_insights/,content
721,Individual answers to Google-Form submissions - automatically,"This is crazy-amazing from where I sit: I can automate anything and get a UI for free: can use the G-Form not only to trigger, but to give immediate feedback to users. A different one for each submission. All it takes is plain G-Forms and a small Apps Script.

Here's how (stripped-down example):

1. Create a new form
2. turn on ""Make this a quiz"", set ""Release grades"" to ""Immediately after each submission""
3. can have ""Collect e-mails"" on or off, does not matter
4. Define the question, do *not* give an answer scheme. In my case, I have one called ""Variation"". Details on why the form needs to be a quiz: see below
5. Add yet another Question (non-mandatory!) called ""Feedback""
6. Edit the confirmation message to sth like ""Click 'View Score' to see the result""
7. In the 3-dots menu (upper right), open the script editor
8. Paste a variation of this script (you can determine feedback in the script, call a webhook like I do, look sth up in Sheets, whatever)

&#x200B;

    function myFunction(e) {
      var form = e.source;
      var response = e.response;
      var variationItem = form.getItems().find(item => item.getTitle() === 'Variation');
      var feedbackItem = form.getItems().find(item => item.getTitle() === 'Feedback');
      var variationResponseText = response.getGradableResponseForItem(variationItem).getResponse();
      var feedbackResponse = response.getGradableResponseForItem(feedbackItem);
      var didAddFeedback = false;
      if (!feedbackResponse.getFeedback()) {
        console.log('Adding feedback - chosen option: ' + variationResponseText);
        var webhookUrl = 'https://<whateveryourapi>?variation=' + encodeURIComponent(variationResponseText);
        var webhookResponse = UrlFetchApp.fetch(webhookUrl, {headers: {'X-Test': 'whatever'}}); // can also have API key headers, etc.
        var webhookBody = JSON.parse(webhookResponse.getContentText());
        feedbackResponse.setFeedback(FormApp.createFeedback().setText((webhookBody[variationResponseText].ordered ? ('We ordered ' + variationResponseText) : ('Sorry, could not order ' + variationResponseText + ': ' + webhookBody[variationResponseText].reason)) + '\nalso check out https://<yourwebsite>/<path>/' + encodeURIComponent(variationResponseText)).build());
        response.withItemGrade(feedbackResponse);
        didAddFeedback = true;
        console.log('Added feedback');
      } else {
        console.log('Feedback already there');
      }
      if (didAddFeedback) {
        console.log('New feedback - submitting');
        form.submitGrades([response]);
        console.log('New feedback - submitted');
      }
    }
    
    function installMe() { // (call manually so far; for addon: onInstall)
      var form = FormApp.openById('<your form ID>');
      ScriptApp.newTrigger('myFunction')
          .forForm(form)
          .onFormSubmit()
          .create();
    }

of course, you can customize / change this - it's just a (working) example to give you an idea

what it does:

* react on form submissions
* add a ""Quiz feedback"" to the one submission (*that* is why we needed it to be a quiz)
* make sure we only add feedback once (you'd have an endless loop otherwise)
* the feedback can contain links that are also rendered as links (can also contain pre-filled links to other G-Forms, even; same goes the confirm message, btw - just that the confirmation message is always the same for all users)

then,

1. run installMe() from the script console. This adds the trigger - it's also visible in the script console then. You'll be asked to authorize an insecure app. As the app is your own: do that
2. Try the form

Hope you find this useful - sure let me know what you think!

**Screens step by step:**

Here is my example form:

&#x200B;

https://preview.redd.it/62wjursidrs91.png?width=1360&format=png&auto=webp&v=enabled&s=330976f3a8a8ba79a0158bb41a96028690db9edb

after submit:

&#x200B;

https://preview.redd.it/a6zyz4l0drs91.png?width=1114&format=png&auto=webp&v=enabled&s=26c739a013eeebf86f6b2eb0c6951352a120d8b4

Viewing the ""score"" - i.e. the *individual* feedback:

&#x200B;

https://preview.redd.it/7w910vc3drs91.png?width=1298&format=png&auto=webp&v=enabled&s=958a18944d68e78474ed503a8e23cc408d6f1525",https://www.reddit.com/r/Automate/comments/xzifni/individual_answers_to_googleform_submissions/,content
722,Bot Name Generator – here's a free tool to generate names for your chatbot,"Hi there! My team recently created one piece of content, and I'm so excited to share it with you! Bot Name Generator! It is for everyone who wants their chatbot to engage from the moment it introduces itself. You can generate names based on your industry and personality traits. For example, 

Creative + Travel Industry = Bethany, Mallory, Beep Boop, Anne Droid and more :)

Besides, on this page, you will see instructions, tips, recommendations, and helpful guidance on how to name your chatbot based on your industry. 

People have different expectations when talking to an e-commerce bot and a healthcare virtual assistant. So, if you need some professional opinion, [check out this page](https://botscrew.com/chatbot-name-generator#/?utm_source=Reddit&utm_medium=&utm_campaign=&utm_term=&utm_content=) with ideas and real-world examples!",https://www.reddit.com/r/Automate/comments/xrullw/bot_name_generator_heres_a_free_tool_to_generate/,content
723,Need help with automating the filling up of a word document,"I work for a company that does something called alarm activations. They have a document template that needs to be filled up every time there has been an alarm activated. We then receive a whatsapp message from the guard and take the content and populate a word document template we have and email it to the higher ups. I want to know how I can automate this whole process so that as soon as the message is received, the relevant content on the template is filled and sent to email directly. 

&#x200B;

How can i do this?",https://www.reddit.com/r/automation/comments/zveu7j/need_help_with_automating_the_filling_up_of_a/,content
724,Automate content distribution to Reddit using RPA,"Hi all, I've built a flow to automate content distribution to Reddit using RPA (RoboMotion)  


Here's the vid: [https://youtu.be/CKL\_f7f-q6o](https://youtu.be/CKL_f7f-q6o)  


By the way - I have nothing to sell. Just sharing my knowledge. 

&#x200B;

https://preview.redd.it/vzq3ipx9t30a1.jpg?width=890&format=pjpg&auto=webp&v=enabled&s=78ed284c5ab666e09a0ba6e9064e71736c7cd133",https://www.reddit.com/r/automation/comments/yvv208/automate_content_distribution_to_reddit_using_rpa/,content
725,Need help finding a tool that integrates Google Drive with Confluence Server.,"Hello!

I need to migrate tens of thousands of files of varying types from Google Drive into individual Confluence Server pages.

Can anyone recommend some good automation tools to check out?

Thanks in advance.

## Requirements

#### Must have

* Copy content from Drive to Confluence Server.
* Retain all content for each file.
* Retain most (if not all) formatting for Google Docs.
* Output a list/inventory to Google Sheets with specific details of what occurred.
* Meet corporate security requirements, including access limitations and data retention.
* Can be tested thoroughly before implementation.

#### Nice to have

* Can determine source file location on Drive side.
* Can recreate source file folder location on the Confluence side. e.g., If the source file was in /documents/taxes/ then it will create a documents page in confluence, a taxes page inside that, and put the file into that target.
* Can read Confluence labels.
* Content reformat or cleanup option. e.g., strip tags, remove markup, things like that.
* Integration with existing Confluence macros.",https://www.reddit.com/r/automation/comments/yldkrv/need_help_finding_a_tool_that_integrates_google/,content
726,Is there a tool to automate some click and search process?,"I am on a website which has many audio files, it doesn't have a download button but you can download the files if you take the link that is provided to the <source src=""http:..linkhere""/>. It's painful to go through all that manually so I am looking for a tool that will click on one link open inspect element (or even without opening) but just grabbing that link provided to source and open it in a new tab then right click and save it. I tried to write a script in nodejs that would do that without needing clicks and stuff but the problem is you need to be logged in to be able to read the content of page so I am not very good at that.",https://www.reddit.com/r/automation/comments/x7zxzj/is_there_a_tool_to_automate_some_click_and_search/,content
727,Scraping member list of a slack community's channel and save to a CSV file via Clicknium,"# Requirement Statements
Get member list of a slack community's channel and save to a CSV file.
We can start this simple beginner process quickly with [Clicknium](https://www.clicknium.com/).

# Environment Preparations
- Windows 10
- Visual Studio Code 1.69.2
- Clicknium 0.1.2
- Python 3.10.5
- Chrome 103.0.5060.134
> **Remarks:**  
>- Need run this sample in English region. 

# Run this sample
- Follow [clicknium getting started](https://www.clicknium.com/documents) to set up develop environment.
- Clone [sample repo](https://github.com/automation9417/automation-samples.git).
  ```
  git clone https://github.com/automation9417/automation-samples.git
  ```
- Open the folder 'WebSlackScrapingChannelMembersInfor' in Visual Studio code
- Open `sample.py` in visual studio code.
- Fill the sign config in  `sample.py`
  ```python
  sign_method_name="""" #google for Google account, slack_email for slack email.
  sign_in_email_or_phone="""" #google email or slack email
  sign_in_password="""" #account passwword
  ```
- Fill the slack config in `sample.py`
  ```python
  slack_community_url="""" #The URL of the slack community you want to send essage. e.g.""https://example.slack.com""
  slack_channel_name="""" #The name of the channel you want to send message.
  slack_message="""" #The message content
  ```
- Press `F5` to debug the sample or press `CTRL+F5` to run sample.

# Steps

### Assume Slack is not open in chrome, so we need open chrome with the community address firstly.  
   ```python
   #Use following code to open chrome with target url
   browser_tab=clicknium.chrome.open(""https://example.slack.com"") # update the address to your slack community.
   ```
### Assume Slack is not signed in, so we need to sign in slack with Google account or Slack account.  
![](imgs/sign_in_slack.png)
  - Google account sign in
    ```python
    from msilib.schema import Error
    from clicknium import clicknium, locator
    def google_sign_in(email,password):
        clicknium.find_element(locator.websites.slack.google_sign_in_btn).click()
        choose_account_lebel=clicknium.wait_appear(locator.websites.google_account.choose_account_label,wait_timeout=5)
        if choose_account_lebel:
            clicknium.find_element(locator.websites.google_account.use_another_account_btn).click()
        email_or_phone_input=clicknium.wait_appear(locator.websites.google_account.email_or_phone_input,wait_timeout=5)
        if email_or_phone_input:
            email_or_phone_input.set_text(email)
        else:
            error_msg=""email_or_phone_input not found.""
            raise Error(error_msg)
        clicknium.find_element(locator.websites.google_account.email_or_phone_next_btn).click()
        password_input=clicknium.wait_appear(locator.websites.google_account.password_input,wait_timeout=5)
        if password_input:
            password_input.set_text(password)
        else:
            error_msg=""password_input not found.""
            raise Error(error_msg)
        clicknium.find_element(locator.websites.google_account.password_next_btn).click()
    ```
  - Slack account sign in
    ```python
    from clicknium import clicknium, locator

    def slack_email_sign_in(email,password):
        clicknium.find_element(locator.websites.slack.slack_email_input).set_text(email)
        clicknium.find_element(locator.websites.slack.slack_password_input).set_text(password)
        clicknium.find_element(locator.websites.slack.slack_signin_btn).click()
    ```
### Cancel open slack desktop app  
![](imgs/cancle_open_slack_desktop.png)
  - Click `Cancel` button
      ```python
      from clicknium import clicknium, locator
      from clicknium.common.enums import *
      def close_open_desk():
          open_slack_cancel_btn= clicknium.wait_appear(locator.desktops.chrome.open_slack_win_cancel_btn,wait_timeout=10)  
          if open_slack_cancel_btn:
              open_slack_cancel_btn.click(by=MouseActionBy.MouseEmulation) 
      ```
### Choose use slack in browser  
![](imgs/choose_use_slack_in_browser.png)  
  - Click `use Slack in your browser`
      ```python
      from clicknium import clicknium, locator
      def use_slack_in_browser():
          use_slack_in_browser_button=clicknium.wait_appear(locator.websites.slack.use_slack_in_browser_button,wait_timeout=5)      
          if use_slack_in_browser_button:
              use_slack_in_browser_button.click()
      ```    
### Open search channel page.  
![](imgs/all_channels.png)  
  - Send hot key `Ctrl+Shift+L` to open search change page
    ```python
    def browse_channels():
        channels_menu_inner_span=clicknium.wait_appear(locator.websites.app_slack.channels_menu_inner_span,wait_timeout=5) 
        if channels_menu_inner_span:
            clicknium.send_hotkey(""{CTRL}{SHIFT}L"")
            sleep(1)
        else:
            msg=""channels menu not found.""
            raise Error(msg)
    ``` 
### Search and select the target channel.  
![](imgs/slack_search_select_channel.png)  
  - Enter the target channel name  
  - Click the `Search` icon  
  - Choose sort `A to Z`  
  ![](imgs/choose_sort_way.png)  

  - Select the target channel  
    ```python
    from msilib.schema import Error
    from clicknium import clicknium, locator
    def search_and_select_channel(channel_name):
        clicknium.find_element(locator.websites.app_slack.search_channel_tbx).clear_text()
        clicknium.find_element(locator.websites.app_slack.search_channel_tbx).set_text(channel_name)
        clicknium.find_element(locator.websites.app_slack.search_channel_btn).click()
        clicknium.find_element(locator.websites.app_slack.channel_sort_btn).click()
        clicknium.find_element(locator.websites.app_slack.sort_atoz_btn).click()
        matched_result_span=clicknium.wait_appear(locator.websites.app_slack.matched_result_span,{""channel_name"": channel_name})
        if matched_result_span:
            matched_result_span.click()
        else:
            msg=""No matched channel for ""+channel_name
            raise Error(msg)
    ```
### Get channel member count.  
![](imgs/channel_member_count.png)  
  - Use [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text) to get the member count  
    ```python
    from clicknium import clicknium, locator
    def get_member_count()->int:
        member_count=clicknium.find_element(locator.websites.app_slack.channel_member_count_span).get_text()
        member_count_int=0
        try:
            member_count_int=int(member_count.strip(' ').replace(',',''))
        except:
            member_count=clicknium.find_element(locator.websites.app_slack.channel_member_count_span).get_text()
            member_count_int=int(member_count.strip(' ').replace(',',''))
        return member_count_int
    ```
### Open members list window
![](imgs/open_member_list_win.png)  
  - Click `View members` to open the window
  - Select the `Members` tab
    ```python
    from msilib.schema import Error
    from clicknium import clicknium, locator

    def open_member_list_win():
        clicknium.find_element(locator.websites.app_slack.channel_member_count_span).click()
        member_list_win_members_tab=clicknium.wait_appear(locator.websites.app_slack.member_list_win_members_tab)
        if member_list_win_members_tab:
            member_list_win_members_tab.click()
        else:
            msg=""members tab not found.""
            raise Error(msg)
    ```
### Get members name one by one
![](imgs/get_member_name_one_by_one.png)  
  - Focus the search text box
  - Send `Down` hot key to select `Add People`
  - Loop to get member name
    - Send `Down` hot key to select member
    - Use [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text) to get the member name
    - Save name to CSV file  
  - Close member list window
    ```python
    import csv
    from time import sleep
    from clicknium import clicknium, locator

    def get_member_list(channel_name) -> str:
        browse_channels()
        search_and_select_channel(channel_name)
        member_count=get_member_count()
        open_member_list_win()

        clicknium.find_element(locator.websites.app_slack.channel_member_search_box).set_focus()
        add_peeple_btn=clicknium.wait_appear(locator.websites.app_slack.add_peeple_btn)
        if add_peeple_btn:
            clicknium.send_hotkey('{DOWN}')
            sleep(1)
        else:
            msg=""Add people button not found.""
            raise(msg)

        member_name_label_first=clicknium.wait_appear(locator.websites.app_slack.member_name_label_first)
        if not member_name_label_first:
            msg=""Member name label first not found.""
            raise(msg)

        csv_file_name=channel_name+'_names.csv'    
        with open(csv_file_name, 'w',encoding='utf-8', newline='') as csvfile:
            fieldnames = ['name']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            for num in range(1,member_count+1):
                clicknium.send_hotkey('{DOWN}')
                sleep(1)
                name=clicknium.find_element(locator.websites.app_slack.member_name_label_focus).get_text()
                writer.writerow({'name': name})         

        clicknium.find_element(locator.websites.app_slack.member_list_win_close_btn).click()
        return csv_file_name
    ```
### Get member email one by one
  - Loop  
    - Open members list window 
    ![](imgs/get_member_name_one_by_one.png)   
      ```python  
      open_member_list_win()
      ```  
    - Enter the member name to find member   
    - Click the target member to open the member profile  
    ![](imgs/open_member_profile.png)     
    - Use [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text) to get the member email in profile section  
    - Close the member profile section  
    ![](imgs/get_email_from_profile.png)  
      ```python
      import csv
      from msilib.schema import Error
      from threading import local
      from clicknium import clicknium, locator 
      from channel_operations import browse_channels, open_member_list_win, search_and_select_channel

      def get_member_email(name) -> str:
          open_member_list_win()
          clicknium.find_element(locator.websites.app_slack.channel_member_search_box).set_text(name)
          member_name_label=clicknium.wait_appear(locator.websites.app_slack.member_name_label_first,{""member_name"":name},wait_timeout=5)
          email=None
          if member_name_label:
              member_name_label.click()
              member_email_link=clicknium.wait_appear(locator.websites.app_slack.member_email_link,wait_timeout=5)
              if member_email_link:
                  email=member_email_link.get_text()
              else:
                  email=""email is not visible.""
              clicknium.find_element(locator.websites.app_slack.profile_close_btn).click()
          else:
              email=""user not found.""
              clicknium.find_element(locator.websites.app_slack.member_list_win_close_btn).click()
          return email

      def get_member_list_email(channel_name,name_list_csv_file) -> str:
          browse_channels()
          search_and_select_channel(channel_name)
          
          ret_csv_file_name=channel_name+'_member_emails.csv'
          with open(ret_csv_file_name, 'w',encoding='utf-8', newline='') as csvfile:
              fieldnames = ['name',""email""]
              writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
              writer.writeheader()

              with open(name_list_csv_file,encoding='utf-8', mode ='r')as file:
                  csvFile = csv.reader(file)
                  index=-1
                  for line in csvFile:
                      index=index+1
                      if index==0:
                          continue
                      name=line[0]
                      email=get_member_email(name)
                      if email:
                          writer.writerow({'name': name,""email"":email}) 
          return ret_csv_file_name
      ```
### Sign out.  
![](imgs/slack_sign_out.png)  
  - Click user avatar
  - Click `Sign out` 
    ```python
    from clicknium import clicknium, locator

    def sign_out():
        user_avatar_btn=clicknium.wait_appear(locator.websites.app_slack.user_avatar_btn,wait_timeout=5)
        if user_avatar_btn:
            user_avatar_btn.click()
            clicknium.find_element(locator.websites.app_slack.sign_out_btn).click()
    ```
### Close opened browser tab.  
   ```python  
   browser_tab.close()# close the opened browser tab.
   ``` 
# Tips 
- Pass variable to the locator  
In this sample channel name is passed to the `matched_result_span` locator as following
  - Define variable in locator  
   ![](imgs/pass_variable.png)  
  -  Pass variable in code
      ```python
      matched_result_span=clicknium.wait_appear(locator.websites.app_slack.matched_result_span,{""channel_name"": channel_name})
      ```
- Use wildcard in locator  
In this sample `channel_sort_btn` locator's sInfo is updated end with * as following
![](imgs/sort_btn_wildcard.png)  
# Concepts  
[Clicknium](https://www.clicknium.com/) provides excellent ways of the recorder and the concept of the Locator, which helps you finish developing efficiently without lots of details. Hence it is worth getting to know the concepts below.
1. [Locator](https://www.clicknium.com/documents/concepts/locator)
2. [Recorder](https://www.clicknium.com/documents/tutorial/recorder/)  
> **Functions involved**
>- [click](https://www.clicknium.com/documents/references/python/uielement/click)
>- [set_text](https://www.clicknium.com/documents/references/python/uielement/set_text)
>- [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text)
>- [open browser](https://www.clicknium.com/documents/references/python/webdriver/open)
>- [wait_appear](https://www.clicknium.com/documents/references/python/globalfunctions/wait_appear)
>- [activate browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/activate)
>- [close browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/close)
>- [find_element](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/find_element)
>- [set_focus](https://www.clicknium.com/documents/references/python/uielement/set_focus)
>- [get_property](https://www.clicknium.com/documents/references/python/uielement/get_property)
>- [send_hotkey](https://www.clicknium.com/documents/references/python/uielement/send_hotkey)   
# Get Started
1. Create a new folder. Open Visual Studio Code and press the keyboard shortcut `Ctrl+Shift+P` to select [Clicknium: Sample](https://www.clicknium.com/documents/tutorial/vscode/project_management) and select the newly created folder.
2. pip install clicknium
3. pip install pyperclip
4. Copy the '.locator' folder under 'WebSlackSendMessage' to your new created folder
5. Open `sample.py` and follow the steps above",https://www.reddit.com/r/automation/comments/wktjqf/scraping_member_list_of_a_slack_communitys/,content
729,Scraping someone's recent tweets via python UI automation module Clicknium,"# Requirement Statements
Get someone's recent tweets and save to a CSV file.
We can start this simple beginner process quickly with [Clicknium](https://www.clicknium.com/).

# Environment Preparations
- Windows 10
- Visual Studio Code 1.69.2
- Clicknium 0.1.3
- Python 3.10.5
- Chrome 103.0.5060.134
> **Remarks:**  
>- Need run this sample in English region. 

# Run this sample
- Follow [clicknium getting started](https://www.clicknium.com/documents) to set up develop environment.
- Clone [sample repo](https://github.com/automation9417/automation-samples.git).
  ```
  git clone https://github.com/automation9417/automation-samples.git
  ```
- Open the folder 'ScrapingPeopleRecentTweets' in Visual Studio code
- Open `sample.py` in visual studio code.
- Fill the sign config in  `sample.py`
  ```python
    sign_in_method_name="""" #google for Google account, twiiter_account for tweet account.
    account="""" #google email/phone, twitter username/email/phone
    verify_account=""""#Sign in with tweet account, may need secondary verification. e.g. sign in with twitter email use username/phone to verify.
    password="""" # password
  ```
- Fill a Twitter user name you want to scrape Tweets from  in `sample.py`
  ```python
  scrape_user_name="""" # The Twitter name you want to scrape Tweets from, must start with @. e.g. @exemple
  ```
- Press `F5` to debug the sample or press `CTRL+F5` to run sample.

# Steps

1. Assume Twitter is not open in chrome, so we need open chrome with the explore address firstly.  
   ```python
   #Use following code to open chrome with target url
   browser_tab=clicknium.chrome.open(""https://twitter.com/explore"") 
   ```
2. Assume Twitter is not signed in, so we need to sign in twitter with Google account or twitter account.  
![](https://github.com/automation9417/automation-samples/raw/main/ScrapingPeopleRecentTweets/imgs/sign_in_twitter.png)
  - Google account sign in
    ```python
    from time import sleep
    from clicknium import clicknium, locator
    from clicknium.common.enums import *
    def google_sign_in(email_or_phone,password):
        clicknium.find_element(locator.websites.twitter.login_btn).click()
        sleep(2)
        clicknium.send_hotkey(""{ESC}"")
        continue_with_google_btn=clicknium.wait_appear(locator.websites.twitter.continue_with_google_btn,wait_timeout=5)
        if continue_with_google_btn:
            continue_with_google_btn.click(by= MouseActionBy.MouseEmulation)
        else:
            clicknium.find_element(locator.websites.twitter.continue_as_x_btn).click(by= MouseActionBy.MouseEmulation)
            clicknium.find_element(locator.websites.google_accounts.use_other_account_btn).click()
        clicknium.find_element(locator.websites.google_accounts.email_or_phone_input).set_text(email_or_phone)
        clicknium.find_element(locator.websites.google_accounts.email_or_phone_next_btn).click()
        clicknium.find_element(locator.websites.google_accounts.password_input).set_text(password)
        clicknium.find_element(locator.websites.google_accounts.password_next_btn).click()
    ```
  - Twitter account sign in
    ```python
    from time import sleep
    from clicknium import clicknium, locator
    from clicknium.common.enums import *

    def sign_in_with_twitter_account(email_or_phone_or_username,verify_account,password):
        clicknium.find_element(locator.websites.twitter.login_btn).click()
        sleep(2)
        clicknium.send_hotkey(""{ESC}"")
        clicknium.find_element(locator.websites.twitter.twitter_account_input).set_text(email_or_phone_or_username)
        clicknium.find_element(locator.websites.twitter.login_next_btn).click()
        twitter_verify_input=clicknium.wait_appear(locator.websites.twitter.twitter_verify_input,wait_timeout=5)
        if twitter_verify_input:
            twitter_verify_input.set_text(verify_account)
            clicknium.find_element(locator.websites.twitter.login_next_btn).click()
        clicknium.find_element(locator.websites.twitter.twitter_password_input).set_text(password)
        clicknium.find_element(locator.websites.twitter.login_form_login_btn).click()
    ```    
3. Search and select a twitter user by username.  
![](https://github.com/automation9417/automation-samples/raw/main/ScrapingPeopleRecentTweets/imgs/search_target_user.png)   

    ```python
    from time import sleep
    from msilib.schema import Error
    from clicknium import clicknium, locator,ui
    from clicknium.common.enums import *
    import csv

    def search_and_select_user(username):
        clicknium.find_element(locator.websites.twitter.explore_menu).click()
        clicknium.find_element(locator.websites.twitter.search_text_box_input).click(by= MouseActionBy.MouseEmulation)
        clicknium.find_element(locator.websites.twitter.search_text_box_input).set_text(username)
        sleep(1)
        clicknium.send_hotkey('{ENTER}')
        
        search_people_tab=clicknium.wait_appear(locator.websites.twitter.search_people_tab,wait_timeout=10)
        if search_people_tab:
            search_people_tab.click()
        else:
            msg=""Search people tab not found.""
            raise Error(msg)
        
        target_search_people=clicknium.wait_appear(locator.websites.twitter.target_search_people,{""user_name"":username}, wait_timeout=10)
        if target_search_people:
            target_search_people.click()
        else:
            msg=""People:""+username+"" not found.""
            raise Error(msg)
    ```
4. Get user recent tweets.  
![](https://github.com/automation9417/automation-samples/raw/main/ScrapingPeopleRecentTweets/imgs/get_tweets.png)  
  - Use [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text) to get the tweets publish date, content and link, the result will be saved to a CSV file.  
    ```python
    from time import sleep
    from msilib.schema import Error
    from clicknium import clicknium, locator,ui
    from clicknium.common.enums import *
    import csv


    def get_user_recent_tweets(username)->str:
        search_and_select_user(username)

        clicknium.find_element(locator.websites.twitter.user_tweets_tab).click()
        tweet_article=clicknium.wait_appear(locator.websites.twitter.tweet_article, wait_timeout=10)
        if not tweet_article:
            msg=""Tweet not found.""
            raise Error(msg) 
        
        ret_csv_file_name=username+'_recent_tweets.csv'
        with open(ret_csv_file_name, 'w', newline='',encoding='utf-8') as csvfile:
            fieldnames = ['publish_date',""content"",""link""]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            tweet_articles=clicknium.find_elements(locator.websites.twitter.tweet_article)
            #Start for loop
            for index in range(1,tweet_articles.__len__()+1):
                selected_tweet_article=clicknium.wait_appear(locator.websites.twitter.selected_tweet_article,{""index"":index},wait_timeout=5) 
                if not selected_tweet_article:      
                    continue
                tweet_text=clicknium.wait_appear(locator.websites.twitter.tweet_text,{""index"":index},wait_timeout=2) 
                content=""""
                if tweet_text:
                    content=tweet_text.get_text()

                tweet_card=clicknium.wait_appear(locator.websites.twitter.tweet_card,{""index"":index},wait_timeout=2) 
                link=""""
                if tweet_card:
                    link=tweet_card.get_property(""href"")
                
                tweet_publish_date=clicknium.wait_appear(locator.websites.twitter.tweet_publish_date,{""index"":index},wait_timeout=2) 
                publish_time=""""
                if tweet_publish_date:
                    publish_time=tweet_publish_date.get_property(""datetime"")
                
                writer.writerow({'publish_date':publish_time,""content"":content,""link"":link}) 
            #End for loop
    ``` 
5. Sign out.  
![](https://github.com/automation9417/automation-samples/raw/main/ScrapingPeopleRecentTweets/imgs/sign_out.png)  
    ```python
    from clicknium import clicknium, locator

    def sign_out():
        user_avatar_btn=clicknium.wait_appear(locator.websites.app_slack.user_avatar_btn,wait_timeout=5)
        if user_avatar_btn:
            user_avatar_btn.click()
            clicknium.find_element(locator.websites.app_slack.sign_out_btn).click()
    ```
6. Close opened browser tab.  
   ```python  
   browser_tab.close()# close the opened browser tab.
   ``` 
# Tips 
- Pass variable to the locator  
In this sample user name is passed to the `target_search_people` locator as following
  - Define variable in locator  
   ![](https://github.com/automation9417/automation-samples/raw/main/ScrapingPeopleRecentTweets/imgs/pass_variable.png)  
  -  Pass variable in code
      ```python
      target_search_people=clicknium.wait_appear(locator.websites.twitter.target_search_people,{""user_name"":username}, wait_timeout=10)
      ``` 
# Concepts  
[Clicknium](https://www.clicknium.com/) provides excellent ways of the recorder and the concept of the Locator, which helps you finish developing efficiently without lots of details. Hence it is worth getting to know the concepts below.
1. [Locator](https://www.clicknium.com/documents/concepts/locator)
2. [Recorder](https://www.clicknium.com/documents/tutorial/recorder/)  
> **Functions involved**
>- [click](https://www.clicknium.com/documents/references/python/uielement/click)
>- [set_text](https://www.clicknium.com/documents/references/python/uielement/set_text)
>- [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text)
>- [open browser](https://www.clicknium.com/documents/references/python/webdriver/open)
>- [wait_appear](https://www.clicknium.com/documents/references/python/globalfunctions/wait_appear)
>- [activate browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/activate)
>- [close browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/close)
>- [find_element](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/find_element)
>- [set_focus](https://www.clicknium.com/documents/references/python/uielement/set_focus)
>- [get_property](https://www.clicknium.com/documents/references/python/uielement/get_property)
>- [send_hotkey](https://www.clicknium.com/documents/references/python/uielement/send_hotkey)   
# Get Started
1. Create a new folder. Open Visual Studio Code and press the keyboard shortcut `Ctrl+Shift+P` to select [Clicknium: Sample](https://www.clicknium.com/documents/tutorial/vscode/project_management) and select the newly created folder.
2. pip install clicknium
3. Copy the '.locator' folder under 'ScrapingPeopleRecentTweets' to your new created folder
4. Open `sample.py` and follow the steps above",https://www.reddit.com/r/automation/comments/wixouh/scraping_someones_recent_tweets_via_python_ui/,content
730,Clicknium Automation Sample Solution - Data Migration,"Many enterprises need data migration solution. For example, if IT system is upgraded,there is need to migrate data from legacy system to upgraded system. This is a sample for employee data migration solution with[Clicknium](https://www.clicknium.com/) desktop&web automation. Here is the details: migrate employee data from a legacy thick client application into new HR system. The manual steps are as the follows:

* query data from legacy client app based on employee id.
* query more meta data of employee through internal REST API.
* fill the data of employee to the new HR system(web portal).

# Run this sample

* follow [clicknium getting started](https://www.clicknium.com/documents/quickstart) to set up develop environment.
* clone this sample repo
* download and unzip [legacy thick client app](https://github.com/AutomationAnywhere/Employee-Data-Migration/raw/master/EmployeeList.zip) to local repo folder.
* clone [sample repo](https://github.com/clicknium/clicknium-samples).

&#8203;

    git clone https://github.com/clicknium/clicknium-samples.git 

* open the folder 'EmployeeDataMigration' in Visual Studio Code
* through pipinstall the dependent packages

requestsis used to query data through internal REST api.

    pip install requests 

* open app.pyin Visual Studio Code .
* press F5to debug the sample or CTRL+F5to run the sample. You will see the result as below:

https://preview.redd.it/4kkbpsyo1of91.png?width=522&format=png&auto=webp&v=enabled&s=6402101b350919763c67b1d00f041b6e135705a9

# The Purpose of the Sample

* open the legacy client application with subprocess module, and it will be used to query data later.

&#8203;

    current_dir = os.path.dirname(os.path.abspath(__file__)) employeeList_exe = os.path.join(current_dir, ""EmployeeList.exe"") process = subprocess.Popen(employeeList_exe) 

* open the browser with Clicknium python module and open new HR system. In this sample microsoft edge browser will be used.

&#8203;

    tab = cc.edge.open(""https://developer.automationanywhere.com/challenges/automationanywherelabs-employeedatamigration.html"") 

When the browser is opened, it will return to the edge tab/page.

* get the employee id in new HR system with Clicknium web automaton

&#8203;

    employee_id = tab.find_element(locator.employeedatamigration.developer.text_employeeid).get_text() 

* based on the captured employee\_id above, find the employee information on legacy client application with Clicknium desktop automation.

&#8203;

    cc.ui(locator.employeedatamigration.employee.edit_txtempid).set_text(employee_id, by='set-text')
    cc.ui(locator.employeedatamigration.employee.button_btnsearch).click(by='control-invocation')
    item[""first_name""] = cc.ui(locator.employeedatamigration.employee.edit_txtfirstname).get_text()
    item[""last_name""] = cc.ui(locator.employeedatamigration.employee.edit_txtlastname).get_text()
    item[""email_id""] = cc.ui(locator.employeedatamigration.employee.edit_txtemailid).get_text()
    item[""city""] = cc.ui(locator.employeedatamigration.employee.edit_txtcity).get_text()
    ... ...

automatically set text for employee\_id and click search button to capture all information such as first\_name, last\_name etc of this employee In desktop application operation,Clicknium uses mouse&keyboard simulation by default . In this legacy application, the input and button control support control invocation, so we can use it by pass parameter byin click and set\_text api.

* through requestsmodule, send http request to get the extra information(phone number, start date ) of the employee.

&#8203;

    response = requests.get(api_url + employee_id)
    print(datetime.datetime.now().strftime(""%H:%M:%S"") + "" get response"")
    obj = json.loads(response.content.decode('UTF-8'))
    item[""phoneNumber""] = obj[""phoneNumber""]
    item[""startDate""] = obj[""startDate""]

* fill the data into new HR system through clicknium web automation.

&#8203;

    tab.find_element(locator.employeedatamigration.developer.text_firstname).set_text(item[""first_name""])
    tab.find_element(locator.employeedatamigration.developer.text_lastname).set_text(item[""last_name""])
    tab.find_element(locator.employeedatamigration.developer.text_phone).set_text(item[""phoneNumber""])
    tab.find_element(locator.employeedatamigration.developer.text_email).set_text(item[""email_id""])
    ... ...
    tab.find_element(locator.employeedatamigration.developer.button_submitbutton).click()

In code above, you can see:

* The locator is separated from code , so the locator store can be managed independently. If the new HR system is upgradeD, the locator will changed and the locator store will be updated as well.
* As Clicknium provides unified API for both desktop and web automation, you can write automation code in the same way for browser and windows application.

# Locator

The [Locator](https://www.clicknium.com/documents/concepts/locator) is the identifier of UI element , which can be recorded or edited with [clicknium vs code extension ](https://marketplace.visualstudio.com/items?itemName=ClickCorp.clicknium).

In this sample, you can open the locator in Visual Studio Code , for example:

https://preview.redd.it/jhpfwk7v1of91.png?width=1211&format=png&auto=webp&v=enabled&s=8a4675d5acb9b5b35838548a8be0ccb8f62476e1

# Compare with Selenium & Playwright

* The web driver needs to be downloaded in Selenium matching exactly the browser . In this example, the Edge browser version is 103.0.1264.62, so there is a need to download the same version MS Edge web driver first.
* Selenium and playwright can only support web automation. So when it comes to employee data migration solution, you can operate on the legacy client app by using another library such as pywinauto.

# More samples

You can refer to more automation samples or solutions in [clicknium github samples](https://github.com/clicknium/clicknium-samples). Send [email](mailto:support@clicknium.com) to us or [Join Slack](https://join.slack.com/t/clicknium/shared_invite/zt-1cfxsstw7-s0CeJdhyg5wQ1h7_KKc6QQ).",https://www.reddit.com/r/automation/comments/wfwvul/clicknium_automation_sample_solution_data/,content
731,Python send a message to a slack community's channel.,"# Requirement Statements
Send a message to a slack community's channel.
We can start this simple beginner process quickly with [Clicknium](https://www.clicknium.com/).

# Environment Preparations
- Windows 10
- Visual Studio Code 1.69.2
- Clicknium 0.1.3
- Python 3.10.5
- Chrome 103.0.5060.134
- Python package pyperclip
> **Remarks:**  
>- Need run this sample in English region. 

# Run this sample
- Follow [clicknium getting started](https://www.clicknium.com/documents/quickstart) to set up develop environment.
- Clone [sample repo](https://github.com/automation9417/automation-samples.git).
  ```
  git clone https://github.com/automation9417/automation-samples.git
  ```
- Open the folder 'WebSlackSendMessage' in Visual Studio code
- Open `sample.py` in visual studio code.
- Fill the sign config in  `sample.py`
  ```python
  sign_method_name="""" #google for Google account, slack_email for slack email.
  sign_in_email_or_phone="""" #google email or slack email
  sign_in_password="""" #account passwword
  ```
- Fill the slack config in `sample.py`
  ```python
  slack_community_url="""" #The URL of the slack community you want to send essage. e.g.""https://example.slack.com""
  slack_channel_name="""" #The name of the channel you want to send message.
  slack_message="""" #The message content
  ```
- Press `F5` to debug the sample or press `CTRL+F5` to run sample.

# Steps

1. Assume Slack is not open in chrome, so we need open chrome with the community address firstly.  
   ```python
   #Use following code to open chrome with target url
   browser_tab=clicknium.chrome.open(""https://example.slack.com"") # update the address to your slack community.
   ```
2. Assume Slack is not signed in, so we need to sign in slack with Google account or Slack account.  
  - Google account sign in
    ```python
    from msilib.schema import Error
    from clicknium import clicknium, locator
    def google_sign_in(email,password):
        clicknium.find_element(locator.websites.slack.google_sign_in_btn).click()
        choose_account_lebel=clicknium.wait_appear(locator.websites.google_account.choose_account_label,wait_timeout=5)
        if choose_account_lebel:
            clicknium.find_element(locator.websites.google_account.use_another_account_btn).click()
        email_or_phone_input=clicknium.wait_appear(locator.websites.google_account.email_or_phone_input,wait_timeout=5)
        if email_or_phone_input:
            email_or_phone_input.set_text(email)
        else:
            error_msg=""email_or_phone_input not found.""
            raise Error(error_msg)
        clicknium.find_element(locator.websites.google_account.email_or_phone_next_btn).click()
        password_input=clicknium.wait_appear(locator.websites.google_account.password_input,wait_timeout=5)
        if password_input:
            password_input.set_text(password)
        else:
            error_msg=""password_input not found.""
            raise Error(error_msg)
        clicknium.find_element(locator.websites.google_account.password_next_btn).click()
    ```
  - Slack account sign in
    ```python
    from clicknium import clicknium, locator

    def slack_email_sign_in(email,password):
        clicknium.find_element(locator.websites.slack.slack_email_input).set_text(email)
        clicknium.find_element(locator.websites.slack.slack_password_input).set_text(password)
        clicknium.find_element(locator.websites.slack.slack_signin_btn).click()
    ```
3. Cancel open slack desktop app  
  - Click `Cancel` button
      ```python
      from clicknium import clicknium, locator
      from clicknium.common.enums import *
      def close_open_desk():
          open_slack_cancel_btn= clicknium.wait_appear(locator.desktops.chrome.open_slack_win_cancel_btn,wait_timeout=10)  
          if open_slack_cancel_btn:
              open_slack_cancel_btn.click(by=MouseActionBy.MouseEmulation) 
      ```
4. Choose use slack in browser  
![](imgs/choose_use_slack_in_browser.png)  
  - Click `use Slack in your browser`
      ```python
      from clicknium import clicknium, locator
      def use_slack_in_browser():
          use_slack_in_browser_button=clicknium.wait_appear(locator.websites.slack.use_slack_in_browser_button,wait_timeout=5)      
          if use_slack_in_browser_button:
              use_slack_in_browser_button.click()
      ```    
5. Open search channel page.  
  - Send hot key `Ctrl+Shift+L` to open search change page
    ```python
    def browse_channels():
        channels_menu_inner_span=clicknium.wait_appear(locator.websites.app_slack.channels_menu_inner_span,wait_timeout=5) 
        if channels_menu_inner_span:
            clicknium.send_hotkey(""^+l"")
            sleep(1)
        else:
            msg=""channels menu not found.""
            raise Error(msg)
    ``` 
6. Search and select the target channel.  
  - Enter the target channel name  
  - Click the `Search` icon  
  - Choose sort `A to Z`  
  ![](imgs/choose_sort_way.png)  

  - Select the target channel  
    ```python
    from msilib.schema import Error
    from clicknium import clicknium, locator
    def search_and_select_channel(channel_name):
        clicknium.find_element(locator.websites.app_slack.search_channel_tbx).set_text(channel_name)
        clicknium.find_element(locator.websites.app_slack.search_channel_btn).click()
        clicknium.find_element(locator.websites.app_slack.channel_sort_btn).click()
        clicknium.find_element(locator.websites.app_slack.sort_atoz_btn).click()
        matched_result_span=clicknium.wait_appear(locator.websites.app_slack.matched_result_span,{""channel_name"": channel_name})
        if matched_result_span:
            matched_result_span.click()
        else:
            msg=""No matched channel for ""+channel_name
            raise Error(msg)
    ```
7. Enter the message and send.  
  - Enter message
  - Click the `Send` icon
    ```python
    from clicknium import clicknium, locator
    import pyperclip

    def send_message(channel_name, message):
        navigate_to_browser_channel_page()
        search_and_select_channel(channel_name)
        clicknium.find_element(locator.websites.app_slack.channel_message_input).set_focus()
        clicknium.send_hotkey('^a')
        clicknium.send_hotkey('^x')
        pyperclip.copy(message)
        clicknium.send_hotkey('^v')
        clicknium.find_element(locator.websites.app_slack.send_message_btn).click()
    ```
8. Sign out.  
  - Click user avatar
  - Click `Sign out` 
    ```python
    from clicknium import clicknium, locator

    def sign_out():
        user_avatar_btn=clicknium.wait_appear(locator.websites.app_slack.user_avatar_btn,wait_timeout=5)
        if user_avatar_btn:
            user_avatar_btn.click()
            clicknium.find_element(locator.websites.app_slack.sign_out_btn).click()
    ```
9. Close opened browser tab.  
   ```python  
   browser_tab.close()# close the opened browser tab.
   ``` 
# Tips 
- Pass variable to the locator  
In this sample channel name is passed to the `matched_result_span` locator as following
  - Define variable in locator  
   ![](imgs/pass_variable.png)  
  -  Pass variable in code
      ```python
      matched_result_span=clicknium.wait_appear(locator.websites.app_slack.matched_result_span,{""channel_name"": channel_name})
      ```
- Use wildcard in locator  
In this sample `channel_sort_btn` locator's sInfo is updated end with * 
# Concepts  
[Clicknium](https://www.clicknium.com/) provides excellent ways of the recorder and the concept of the Locator, which helps you finish developing efficiently without lots of details. Hence it is worth getting to know the concepts below.
1. [Locator](https://www.clicknium.com/documents/concepts/locator)
2. [Recorder](https://www.clicknium.com/documents/tutorial/recorder/)  
> **Functions involved**
>- [click](https://www.clicknium.com/documents/references/python/uielement/click)
>- [set_text](https://www.clicknium.com/documents/references/python/uielement/set_text)
>- [open browser](https://www.clicknium.com/documents/references/python/webdriver/open)
>- [wait_appear](https://www.clicknium.com/documents/references/python/globalfunctions/wait_appear)
>- [activate browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/activate)
>- [close browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/close)
>- [find_element](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/find_element)
>- [set_focus](https://www.clicknium.com/documents/references/python/uielement/set_focus)
>- [get_property](https://www.clicknium.com/documents/references/python/uielement/get_property)
>- [send_hotkey](https://www.clicknium.com/documents/references/python/uielement/send_hotkey)  
# Get Started
1. Create a new folder. Open Visual Studio Code and press the keyboard shortcut `Ctrl+Shift+P` to select [Clicknium: Sample](https://www.clicknium.com/documents/tutorial/vscode/project_management) and select the newly created folder.
2. pip install clicknium
3. pip install pyperclip
4. Copy the '.locator' folder under 'WebSlackSendMessage' to your new created folder
5. Open `sample.py` and follow the steps above",https://www.reddit.com/r/automation/comments/wezzbp/python_send_a_message_to_a_slack_communitys/,content
732,Sample to demostrate clicknium web automation solution - customer onboarding," 

This is a sample of customer onboarding solution through [clicknium](https://www.clicknium.com/) web automation .

For one enterprise, customer onboarding has a significant impact on whether a customer keeps using your product or not, you may define the customer onboarding process in your comany internally, for example, need add new customer information into CRM(customer relationship management) system. If you can automatically to do customer onboarding process, it should improve the efficiency significantly. Here we demo one custoemr onboarding automation solution:

* load the missing customer's information from CSV file .
* open CRM system.
* iterate the records in CVS file , fill into CRM form and register each customer.

# Run this sample

* follow [clicknium getting started](https://www.clicknium.com/) to set up develop environment.
* clone [sample repo](https://github.com/clicknium/clicknium-samples).

&#8203;

    git clone https://github.com/clicknium/clicknium-samples 

* open the folder 'CustomerOnboarding' in Visual Studio code
* through pip  
 install the dependenct packages

requests  
 is used to download the CSV file and pandas  
 is used to read CSV file .

    pip install requests pip install pandas 

* open app.py  
 in Visual Studio Code .
* press F5  
 to debug the sample or press CTRL+F5  
 to run sample.

You will see the result:

https://preview.redd.it/wqeiug19g0f91.jpg?width=509&format=pjpg&auto=webp&v=enabled&s=340de9b8ee0349a117d3ae47195469daaaf9a8f9

# What the sample do

* open the testing CRM web portal.
* get the url of CSV to be download.
* download the CSV file .

&#8203;

    tab = cc.edge.open(""https://developer.automationanywhere.com/challenges/automationanywherelabs-customeronboarding.html"")
    url = tab.find_element(locator.customeronboarding.developer.a_downloadcsv).get_property(""href"")
    excelFile = requests.get(url)
    temp_file = os.path.join(os.getcwd(), 'missing.csv')
    open(temp_file, 'wb').write(excelFile.content)
    data = pd.read_csv(temp_file)

* iterate the records and fill the data into CRM system and register the customer.

&#8203;

    for idx, item in data.iterrows():
    tab.find_element(locator.customeronboarding.developer.text_customername).set_text(item[0])
    tab.find_element(locator.customeronboarding.developer.text_customerid).set_text(item[1])
    tab.find_element(locator.customeronboarding.developer.text_primarycontact).set_text(item[2])
    tab.find_element(locator.customeronboarding.developer.text_street).set_text(item[3])
    tab.find_element(locator.customeronboarding.developer.text_city).set_text(item[4])
    tab.find_element(locator.customeronboarding.developer.select_state).select_item(item[5])
    tab.find_element(locator.customeronboarding.developer.text_zip).set_text(""%05d"" % item[6])
    tab.find_element(locator.customeronboarding.developer.email_email).set_text(item[7])
    if item[8] == ""YES"":
        tab.find_element(locator.customeronboarding.developer.radio_activediscountyes).set_checkbox()
    else:
        tab.find_element(locator.customeronboarding.developer.radio_activediscountno).set_checkbox()
            
    if item[9] == ""YES"":
        nda = 'check'
    else:
        nda = 'uncheck'
    tab.find_element(locator.customeronboarding.developer.checkbox_nda).set_checkbox(check_type=nda)
    tab.find_element(locator.customeronboarding.developer.button_submit_button).click()

From above code , you can see:

* Locator is separate from code , so locator store can be managed independently, if the CRM system is upgrade, locator is changed, just need update the locator store.
* Easy to select option from dropdown list: tab.find\_element(<locator>).select\_item(<option>)
* Easy to check radio button/checkbox: tab.find\_element(<locator>).set\_checkbox()

# Locator

[Locator](https://www.clickcorp.com/documents#automation/locator) is the identifier of UI element, through [clicknium vs code extension](https://marketplace.visualstudio.com/items?itemName=ClickCorp.clicknium) can record/edit the locator.

# Compare with Selenium

* Selenium need download the webdriver which version should exactly match the browser , in this example, my Edge browser version is 103.0.1264.62  
, so I need download the same version msedge web driver first.
* Selenium does not support check operation for radion button, need use click instead.

&#8203;

    driver.find_element('id', 'activeDiscountYes').click() 

* To select option from dropdown list, need import additional class to wrapper.

&#8203;

    from selenium.webdriver.support.select import Select
    Select(driver.find_element('id', 'state')).select_by_value(item[5])

* Compare the running time In this sample, need fill 7 records, each record need submit 10 fields. From the log, we can see clicknium is more faster than selenium.

&#8203;

    [clicknium] Start to fill data:2022-07-21 16:10:15.938903
    [clicknium] End to fill data:2022-07-21 16:10:18.460162
    
    [selenium] Start to fill data:2022-07-21 15:08:30.528693
    [selenium] End to fill data:2022-07-21 15:08:37.517574

# More samples

You can find more automatin sample/solution from [clicknium github samples](https://github.com/clicknium/clicknium-samples)",https://www.reddit.com/r/automation/comments/wd5kt4/sample_to_demostrate_clicknium_web_automation/,content
733,automatically detecting elements or text within the web page using Selenium?,Im trying to create a script that could automatically detect the content of a web page and based on that content perform some operations .. Is it possible with selenium automation?,https://www.reddit.com/r/selenium/comments/10axdse/automatically_detecting_elements_or_text_within/,content
734,How do I wait for css selector to click?,"Hi, I have a monitoring script in Python + Selenium. Right now I use a lot of xpath and id to click and it is annoying because the frameworks used sometimes changes the id and xpath, so I would like to look for something that will stay the same for a longer time.

I figured CSS selector content could be nice, they seem to be named after what they really do in the application I monitor.

I tried to google it but I found no examples I understood how I could convert.

Right now my function looks like this, how can I modify that to use css selector?:

    def wait_for_xpath_click(params, element_xpath):
    	temp_element = WebDriverWait(browser, 60).until(
    		expected_conditions.presence_of_element_located((By.XPATH, element_xpath))
    		)
    	time.sleep(5)
    	temp_element.click()

Edit: I got it now:
It needs to look like this:

    expected_conditions.element_to_be_clickable((By.CSS_SELECTOR, selector)))",https://www.reddit.com/r/selenium/comments/zlorlo/how_do_i_wait_for_css_selector_to_click/,content
735,Java selenium Textarea/iFrame help,"I have problem of locating and entering any text into ""Content"" field on some blog using java selenium webdriver. It seems like textarea but when inspected, textarea is hidden and iFrame document is what I need to somehow locate and sendKeys there. So basicaly I need somehow to click on <p> under <body> of that document under iFrame which I dont know how. Everything I tried bring me Exceptions NoSuchElement or NotClickable. I would appreciate any suggestion based on exeperience, thanks.",https://www.reddit.com/r/selenium/comments/ze4n7p/java_selenium_textareaiframe_help/,content
736,#shadow-root (open) selenium python trying to access button inside shadow-root (open),"im trying to access a download button inside 4 shadow-root (open) what do i have to do?

the inspect element looks like this:

    <div class=""dataset-download-card"" data-test=""dataset-download-card"">
    	<hub-download-card dataset-id=""04c64cb5553843b8a644af6429b6633c_0"" spatial-ref-id=""4326"" data-element=""download-card"" hydrated="""">
    	 #shadow-root (open)
    	  <calcite-card dir=""ltr"" hydrated="""">
    	   #shadow-root (open)
    			<h3 slot=""title"">CSV</h3>
    			<dl slot=""subtitle"">
    				<dt class=""ltr"">File created</dt>
    				<dd>May 24, 2022, 01:58</dd>
    				<dt class=""ltr"">File size</dt>
    				<dd>28.9 KB</dd>
    			</dl>
    			<div slot=""footer-leading"">
    				<hub-download-notice file-status=""ready"" hydrated=""""/>
    				<calcite-button icon-position=""start"" alignment=""center"" appearance=""outline"" color=""blue"" scale=""m"" width=""full"" hydrated="""">
    				 #shadow-root (open)
    				  <button aria-label="""" class=""content--slotted icon-start-empty icon-end-empty"" type=""button"">
    						<span class=""content"">
    							<slot>
    								<#text>
    							</slot>
    						</span>
    					</button>
    					Download
    				</calcite-button>
    			</div>
    		</calcite-card>
    	</hub-download-card>
    </div>

so this is a chunk of the html code from inspect element. i am trying to access the Download button and click it.

how do i do that?

i tried using 

    driver.execute_script(""return document.querySelector('hub-download-card').shadowRoot.querySelector('calcite-card').shadowRoot.querySelector('calcite-button').shadowRoot.querySelector('button.content--slotted icon-start-empty icon-end-empty')"").click()

im getting error:

    JavascriptException: Message: TypeError: document.querySelector(...).shadowRoot.querySelector(...).shadowRoot.querySelector(...) is null",https://www.reddit.com/r/selenium/comments/z4bwjc/shadowroot_open_selenium_python_trying_to_access/,content
737,What else stops finding elements besides iframes,"I have a web page I'm trying to automate and it works perfectly until I get to a certain point, but then python stops finding anything on the last page.

I was using find element by link and by partial link but I also tried some different things with xpath, id, and css selector but still no dice.

After some googling, I also tried switching to the 2 iframes in the page (I did so by index) and back to the main content, but still not a die to be found. 

I noted that the links in question come in the same wrapper as a Javascript noop. Could that have something to do with it? What should I google/try next?

I'm not sure what to paste in here to ask for help. I've tried so many things that didn't work. Thanks for your time, those who read this far; whether you can help me or not, I appreciate you.",https://www.reddit.com/r/selenium/comments/z2psxn/what_else_stops_finding_elements_besides_iframes/,content
738,How to generate Extent Reports in Selenium?," 

1. Import the JAR file: degreereports-java-2.1.2.jar. After downloading the ZIP file, extract its contents to a folder.
2. Add the JAR file to the build path of the project using the Build Path -> Set Build Path option.
3. Create a new JAVA class for Scope Report with the following code.

&#8203;

    package com.browserstack.demo;
    import org.junit.AfterClass;
    import org.junit.BeforeClass;
    import org.junit.Test;
    import org.openqa.selenium.WebDriver;
    import org.openqa.selenium.chrome.ChromeDriver;
    import com.relevantcodes.extentreports.ExtentReports;
    import com.relevantcodes.extentreports.ExtentTest;
    import com.relevantcodes.extentreports.LogStatus;
    public class ExtentDemo {
    static ExtentTest test;
    static ExtentReports report;
    @BeforeClass
    public static void startTest()
    {
    report = new ExtentReports(System.getProperty(""user.dir"")+""ExtentReportResults.html"");
    test = report.startTest(""ExtentDemo"");
    }
    @Test
    public void extentReportsDemo()
    {
    System.setProperty(""webdriver.chrome.driver"", ""D:SubmittalExchange_TFSQAAutomation3rdpartychromechromedriver.exe"");
    WebDriver driver = new ChromeDriver();
    driver.get(""https://www.google.co.in"");
    if(driver.getTitle().equals(""Google""))
    {
    test.log(LogStatus.PASS, ""Navigated to the specified URL"");
    }
    else
    {
    test.log(LogStatus.FAIL, ""Test Failed"");
    }
    }
    @AfterClass
    public static void endTest()
    {
    report.endTest(test);
    report.flush();
    }
    }

### How to generate Extent Reports in Selenium using NUnit?

    [SetUpFixture]
    public abstract class Base
    {
    protected ExtentReports _extent;
    protected ExtentTest _test;
    
    [OneTimeSetUp]
    protected void Setup()
    {
    var dir = TestContext.CurrentContext.TestDirectory + ""\\"";
    var fileName = this.GetType().ToString() + "".html"";
    var htmlReporter = new ExtentHtmlReporter(dir + fileName);
    
    _extent = new ExtentReports();
    _extent.AttachReporter(htmlReporter);
    }
    
    [OneTimeTearDown]
    protected void TearDown()
    {
    _extent.Flush();
    }
    
    [TestFixture]
    public class TestInitializeWithNullValues : Base
    {
    [Test]
    public void TestNameNull()
    {
    Assert.Throws(() => testNameNull());
    }
    }
    
    [SetUp]
    public void BeforeTest()
    {
    _test = _extent.CreateTest(TestContext.CurrentContext.Test.Name);
    }
    
    [TearDown]
    public void AfterTest()
    {
    var status = TestContext.CurrentContext.Result.Outcome.Status;
    var stacktrace = string.IsNullOrEmpty(TestContext.CurrentContext.Result.StackTrace)
    ? """"
    : string.Format(""{0}"", TestContext.CurrentContext.Result.StackTrace);
    Status logstatus;
    
    switch (status)
    {
    case TestStatus.Failed:
    logstatus = Status.Fail;
    break;
    case TestStatus.Inconclusive:
    logstatus = Status.Warning;
    break;
    case TestStatus.Skipped:
    logstatus = Status.Skip;
    break;
    default:
    logstatus = Status.Pass;
    break;
    }
    
    _test.Log(logstatus, ""Test ended with "" + logstatus + stacktrace);
    _extent.Flush();
    }
    }

**Source:** [Guide to generate Extent reports in selenium](https://qacraft.com/guide-to-generate-extent-reports-in-selenium-webdriver/)",https://www.reddit.com/r/selenium/comments/ylromm/how_to_generate_extent_reports_in_selenium/,content
739,Trying to get if site is safe,"Hi, i was wondering if you can get via selenium information if site is safe eg: i have 2 sites one was flagged by google becouse it has phising content, and other site no. Yet selenium sees that both of the sites are safe. Any ideas?",https://www.reddit.com/r/selenium/comments/yjxyoy/trying_to_get_if_site_is_safe/,content
741,How do I click on Youtube cookies 'Accept all' correctly?,"[https://i.ibb.co/cy796c7/index.png](https://i.ibb.co/cy796c7/index.png)

Here's the code I currently use, but it doesn't work all the time. Sometimes it just errors out. Any ideas?

    el_xpath = '//*[@id=""content""]/div[2]/div[6]/div[1]/ytd-button-renderer[2]/a'
    WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.XPATH, el_xpath)))
    self.driver.find_element(""xpath"", el_xpath).click()

Error:

      File ""/home/admin/DEV/Python/bbot/scrap/__init__.py"", line 51, in click_accept_all
        WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.XPATH, el_xpath)))
      File ""/home/admin/DEV/Python/bbot/venv/lib/python3.10/site-packages/selenium/webdriver/support/wait.py"", line 90, in until
        raise TimeoutException(message, screen, stacktrace)
    selenium.common.exceptions.TimeoutException: Message: 
    Stacktrace:",https://www.reddit.com/r/selenium/comments/ycfwjn/how_do_i_click_on_youtube_cookies_accept_all/,content
742,--headless always enables javascript. Why?,"Hello experts,

I need to test a website with javascript enabled *and* disabled. Everything works fine in headful mode with Chrome, however, once I switch to headless Javascript seemingly cannot be disabled. Here's a self-contained python MWE:

```
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_experimental_option(""prefs"", {""profile.managed_default_content_settings.javascript"": 2})
# chrome_options.add_argument(""--headless"")
driver = webdriver.Chrome(options=chrome_options)
driver.get(""https://www.whatismybrowser.com/detect/is-javascript-enabled"")
driver.save_screenshot(""screenshot.png"")
driver.quit()
```

If run as is, the screenshot will tell that Javascript is disabled, however, if headless mode is switched on (uncomment the commented line) Javascript will suddenly be enabled, contrary to the configured settings.

Any idea why that is and how to fix it? I'm using Chrome/Chromedriver v106.0.5249.61.

Thank you!",https://www.reddit.com/r/selenium/comments/xwgv4n/headless_always_enables_javascript_why/,content
743,Selenium IDE send keys command,"Quick preface, this is in IDE. I do not plan on scripting but if someone can help me figure this out by using native IDE commands that would be ideal.

&#x200B;

I'm stuck trying to find a way to select the contents of a text field and delete said content in an automated fashion. I tried having the script simply type nothing into the text field but clicking update doesn't actually retain the empty text field so I need to have the script erase the contents.

My goal is to have a ***send keys*** command that will send CTRL+A which will select the contents of the text field and then send backspace after to clear the text. Unfortunately I don't have much experience with coding in general and even less with java so I have no idea how I would word it in the value field.

For example, I've tried  ${KEY\_CONTROL}+${KEY\_""A""},  ${KEY\_CONTROL+""A""},  ${KEY\_CONTROL}+""A"", but all of these either don't do anything, pastes the entire command value / partially, or they add an A to the text.

&#x200B;

Any help is welcome.",https://www.reddit.com/r/selenium/comments/xqi6oh/selenium_ide_send_keys_command/,content
745,Help with clicking href in python,"Hello everyone,

I’m new to Selenium and need some guidance on how to click on a link assigned to href.

Below are my elements and I need Selenium to auto click and access the link: clickme.com

<div data-se=""app-card-container"" class=""chiclet--container"" draggable=""true"">
    <a aria-label=""launch app"" class=""chiclet a--no-decoration"" data-se=""app-card"" href=""https://clickme.com"" rel=""noreferrer"">
        <article class=""chiclet--article"">
            <section class=""chiclet--main"" data-se=""app-card-main""><img class=""app-logo--image"" src=""https://ok.com"" alt=""my app"" /></section>
            <footer class=""chiclet--footer"" data-se=""app-card-footer"">
                <o-tooltip content=""AtL"" position=""bottom"" class=""hydrated"">
                    <div slot=""content""></div>
                    <div aria-describedby=""o-tooltip-73""><span class=""chiclet--app-title"" data-se=""app-card-title"">my app</span></div>
                </o-tooltip>
            </footer>
        </article>
    </a>
    <button class=""chiclet--action"" tabindex=""0"" aria-label=""Settings for app"" data-se=""app-card-settings-button"">
        <svg class=""chiclet--action-kebab"" width=""20"" height=""4"" viewBox=""0 0 20 4"" fill=""#B7BCC0"" xmlns=""http://www.w3.org/2000/svg"">
            <circle cx=""2"" cy=""2"" r=""2""></circle>
            <circle cx=""10"" cy=""2"" r=""2""></circle>
            <circle cx=""18"" cy=""2"" r=""2""></circle>
        </svg>
    </button>
</div>",https://www.reddit.com/r/selenium/comments/xiq7tf/help_with_clicking_href_in_python/,content
746,Pulling multiple elements from the same page,"So I am making a Garmin crawling script and I want it to pull multiple elements if they are from the same day and add the time together for some activities, time, distance and heart rate for another for example. 

[![Layout of website][1]][1]


  [1]: https://i.stack.imgur.com/brwIa.png

``` 
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import login as login
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import datetime
import time

x = datetime.datetime.now()
x = x.strftime(""%b %d"")

driver = browser = webdriver.Firefox()
driver.get(""https://connect.garmin.com/modern/activities"")

driver.implicitly_wait(1)

iframe = driver.find_element(By.ID, ""gauth-widget-frame-gauth-widget"")
driver.switch_to.frame(iframe)

driver.find_element(""name"", ""username"").send_keys(login.username)

driver.find_element(""name"", ""password"").send_keys(login.password)
driver.find_element(""name"", ""password"").send_keys(Keys.RETURN)

driver.switch_to.default_content()

time.sleep(10)

driver.find_element(""name"", ""search"").send_keys(""Reading"")
driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)

time.sleep(2)

element = driver.find_element(By.CSS_SELECTOR, '.activity-date > span:nth-child(1)').text

time.sleep(2)
print(element)

time_read = 0

if element == x:
	spent = driver.find_element(By.CSS_SELECTOR, 'li.list-item:nth-child(1) > div:nth-child(2) > div:nth-child(5) > div:nth-child(2) > span:nth-child(1) > span:nth-child(1)').text

        result = time.strptime(spent, ""%H:%M:%S"")

	time_read += result.tm_hour * 60

	time_read += result.tm_min

	print(time_read)

```

So this is my current code. It finds the date, checks if it is today and adds the minutes to the variable time_read. 

Now I need some help in how I go about adding multiple elements, and if this can be done with some kind of for loop, where it loops between the dates and can then extract the time from the element?

Do I need to set them up one by one, since I need to provide which element a specific iteration needs to pull from? So maybe I should have 5 or 6 checks for example, instead of some kind of loop that goes through and does it? Then it will be a lot of manual work, which makes me question if there isn't a better way to deal with it.

I do not want to use CSV.


Some relevant HTML
```
<div class=""pull-left activity-date date-col"">
        <span class=""unit"">Sep 14</span>
        <span class=""label"">2022</span>
    </div>

<span class=""unit"" title=""3:32:00""><span class="""" data-placement=""top"" title=""3:32:00"">3:32:00</span></span>

<span class=""unit"" title=""1:00:00""><span class="""" data-placement=""top"" title=""1:00:00"">1:00:00</span></span>
<span class="""" data-placement=""top"" title=""1:00:00"">1:00:00</span>
```

Also a bit unsure what the best way is to locate elements? Is CSS.SELECTOR good or should I use XPATH preferably?

Thanks",https://www.reddit.com/r/selenium/comments/xhhosb/pulling_multiple_elements_from_the_same_page/,content
747,Stuck on not being able to hit a button with Python + Selenium,"Hello,

I'm trying to automate a process since I can't with Vanguard ETFs, basically just buy 1 stock. I've dabbled in coding for the web with Python so I get the jist of what's happening.

I am all the way up to Preview Order but I can't seem to grasp clicking it.

The button itself has no id, and I've tried XPATH and it doesn't work.

Button code on inspecting it

`<button _ngcontent-trade-web-angular-c92="""" type=""button"" tdsbutton="""" tdsbuttonstyle=""primary"" data-testid=""btn-trade-preview-order"" tdsbuttonsize=""compact-below-xl"" class=""twe-flex-button-wrap__button tds-button tds-button--compact-below-xl""> Preview Order </button>`

XPath method

`driver.find_elements(By.XPATH, ""/html/body/twe-root/main/twe-trade/form/div/div[3]/div[2]/twe-trade-detail/tds-card/div/tds-card-body/div[3]/button[2]"").click()`

I've tried Searching for Preview Order as well.

`driver.find_element(By.XPATH, ""//button[text()=' Preview Order ']"").click()`

At this point I'm not sure what other options I have? The error is always ""Unable to locate element:""

&#x200B;

&#x200B;

EDIT:

I am able to get farther but I get this now.

""selenium.common.exceptions.ElementNotInteractableException: Message: element not interactable""

I am doing the following command now, gonna keep trying..

`driver.find_element(By.CSS_SELECTOR, ""[data-testid='btn-trade-preview-order']"").click()`",https://www.reddit.com/r/selenium/comments/xd7ww1/stuck_on_not_being_able_to_hit_a_button_with/,content
748,Selenium pulls wrong value from an element?,"from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import login as login
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import datetime

x = datetime.datetime.now()
x = x.strftime(""%d"")

driver = browser = webdriver.Firefox()
driver.get(""https://connect.garmin.com/modern/activities"")

driver.implicitly_wait(2)

iframe = driver.find_element(By.ID, ""gauth-widget-frame-gauth-widget"")
driver.switch_to.frame(iframe)

driver.find_element(""name"", ""username"").send_keys(login.username)

driver.find_element(""name"", ""password"").send_keys(login.password)
driver.find_element(""name"", ""password"").send_keys(Keys.RETURN)

driver.switch_to.default_content()

driver.implicitly_wait(10)

driver.find_element(""name"", ""search"").send_keys(""Reading"")
driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)

#element = driver.find_element(By.CLASS_NAME, ""unit"")
element = driver.find_element(By.XPATH, ""//html/body/div[1]/div[3]/div[2]/div[3]/div/div/div[2]/ul/li/div/div[2]/span[1]"")

print(element.text)

This is the code, the element ""unit"" should return ""Aug 25"", which I then want to use with ""x"" to make sure that I pull the correct data from a specific page. Problem is, it always returns today's date, even though the HTML says the correct one.

https://imgur.com/a/2d4YuQi

That is the page, any help is appreciated",https://www.reddit.com/r/selenium/comments/xccevf/selenium_pulls_wrong_value_from_an_element/,content
749,How do I correctly add chrome to my conda BASH path,"I'm trying to run a remote debugging port on Chrome with Selenium.

From what I understand, I need to add chrome to my bash path but am having trouble getting this to work.

I've been following this tutorial [https://apple.stackexchange.com/questions/228512/how-do-i-add-chrome-to-my-path](https://apple.stackexchange.com/questions/228512/how-do-i-add-chrome-to-my-path)

My bash screen currently looks like this:

    if [ $? -eq 0 ]; then eval ""$__conda_setup""""
     else if 
    [ -f
    ""/Users/user/opt/anaconda3/etc/profile.d/conda.sh"" ]; then         . ""/Users/user/opt/anaconda3/etc/profile.d/conda.sh""  
       else      
       export PATH=""/Users/user/opt/anaconda3/bin:$PATH:/Users/user/Applications/Google Chrome.app/Contents/MacOS:$PATH"""" 

When I run try to run Chrome from the terminal it doesn't recognise it.

&#x200B;

If i type echo $PATH i get the following:

/Users/user/opt/anaconda3/bin:/Users/user/opt/anaconda3/condabin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin

What am I doing wrong?",https://www.reddit.com/r/selenium/comments/x793kd/how_do_i_correctly_add_chrome_to_my_conda_bash/,content
750,How do I find an element that is in a drop-down menu that appears after text is inputted.,"When you go to [instagram.com](https://instagram.com) and type something into the search bar, a menu pops up right below the text box. I want to build a bot that displays some of the contents of that menu.

How do I do this?",https://www.reddit.com/r/selenium/comments/wv3euj/how_do_i_find_an_element_that_is_in_a_dropdown/,content
751,Taking screenshot of only relevant content of webpage | Selenium | Python,"How can I take screenshot of only relevant content of any webpage using Selenium and Python?

[I want to take the screenshot of the marked content (specifications) in this photo instead of whole page](https://i.stack.imgur.com/vlH7p.png)

[Example webpage link](https://www.startech.com.bd/benq-gw2480-fhd-monitor)

Currently I'm taking screenshot of the whole page. Also I want to avoid referencing any class or id while taking the screenshot. Please let me know if I can achieve this (if yes, HOW?) or have to change my requirements. If there is any workaround such as cropping the relevant content, please do share too. Thanks.",https://www.reddit.com/r/selenium/comments/wqssa7/taking_screenshot_of_only_relevant_content_of/,content
753,"No such element exception, yet clearly visible in HTML","I am trying to scrape a table, so my first step is to create a list of elements for each entry:

    entries = driver.find_elements(By.CLASS_NAME, ""gq-element"")

This works fine, and I get a list of WebElements. However, when I try and loop through this and extract content, I get an exception: 

    for entry in entries: 
    title = entry.find_element(By.CLASS_NAME, ""col-md-8 filter-content"")

`NoSuchElementException: no such element: Unable to locate element: {""method"":""css selector"",""selector"":"".col-md-8 filter-content""}`

Here is an example of what the HTML looks like (end goal is to extract blue text):

[https://imgur.com/h1oDuCb](https://imgur.com/h1oDuCb)

Any help would be greatly appreciated! Thanks.",https://www.reddit.com/r/selenium/comments/whto1l/no_such_element_exception_yet_clearly_visible_in/,content
754,need software to scrap some select data from a website and remplace them in my text file,"Hey guys, i hope you are doing well, like in title, need software to scrap some select data from a website and remplace them in my text file. is their software which can do this job, for exemple my text file has:

Description:

TITLE:

TYPE:

PLACE:

i need scrapper data which will be based only in that website and will use my custom text file as a template, then it will remplace auto data scrapped from that website and remplace each data in exact place of my text file.

So anytime i give it that URL, it will generate those data. sorry for my bad english",https://www.reddit.com/r/webscraping/comments/10ec8cc/need_software_to_scrap_some_select_data_from_a/,auto
755,Yellowpage scraper.,"Hey everyone,

I've been working on a web scraping project using Python to extract data from [YellowPage](https://github.com/sushil-rgb/YellowPage-scraper) and I wanted to share my experience and some tips for anyone else looking to do the same.

First, I used the popular library BeautifulSoup and Playwright to navigate and automate the website. The script asks user to enter a business name, location and number of pages to scrape and save it into an excel database accordingly. It extracts all the necessary data including emails as well. I feel I used lots of try and except clause, if someone has better approach them please free to share.

Another thing to watch out for is that the website structure can change frequently, so it's important to regularly check and update your code accordingly.

Overall, it was a fun and challenging project that taught me a lot about web scraping and working with dynamic websites.

Let me know if you have any questions or tips of your own to share!",https://www.reddit.com/r/webscraping/comments/10cbygu/yellowpage_scraper/,auto
756,Scraping embedded Google Slides?,"There's a website with over 40 embedded Google Slides (download, print disabled). Are there any tools to scrape or automate taking screenshots of the Google Slides?",https://www.reddit.com/r/webscraping/comments/1095z54/scraping_embedded_google_slides/,auto
757,what do you do when you get a access denied for trying to get an url? (Python Selenium),"&#x200B;

https://preview.redd.it/06zjmlfuo0ba1.png?width=1239&format=png&auto=webp&v=enabled&s=5c92ff00c29ad5fb0437a01debd9ddcb374fce82

im trying to call an url many times for webscraping.

    for i in international_data_indicators:
        for y in countries:
            url = f'https://www.migrationdataportal.org/international-data?i={i}&cm49={y}'
            driver.set_page_load_timeout(10)
            driver.get(url)
            time.sleep(1)
            driver.delete_all_cookies()```

i get the following error attached as picture and basically im blocked, i tried clearing cookies and cache but it didnt work as you can see.

what can i do?

im using python3 and selenium with firefox",https://www.reddit.com/r/webscraping/comments/107e5cw/what_do_you_do_when_you_get_a_access_denied_for/,auto
758,How many LI SN URLs can I convert to a LI Public URL in a day without getting sanctioned?,"[I got this error on LinkedIn today after scraping about 1,200 LI URLs to convert them into a public LI URL.](https://imgur.com/a/6q9QdiO) I was using the [Sales Navigator URL Converter](https://phantombuster.com/automations/sales-navigator/9068/sales-navigator-url-converter), and **I'm very nervous to make sure that this doesn't happen again.** 

I have another LI account that I plan on upgrading into a LI SN account. I've had this account for 2.5 years, and I have 100s of connections, and it's got a ""low spam score,"" for a lack of a better term. 

Anyways, I have already ~2,400 LinnkedIn Sales Navigator URLs that I've converted to Public LinkedIn URLs, and I'll need to scrape about 5,000 more. 

How many should I do in one day so that I don't have my account terminated?

I've scraped from a LI SN Lead List using dataminer.io, and I can do thousands of those daily, so I was a bit surprised that I can only scrape ~1,200 LI SN accounts today.",https://www.reddit.com/r/webscraping/comments/1071d9r/how_many_li_sn_urls_can_i_convert_to_a_li_public/,auto
759,Price Scraping and Monitoring Bot,"Hi 

I'm a beginner here 

Is there any readymade script out there with which I can scrap prices from websites to compare the prices from cheapest to expensive?

For ex: I want to scrap price for the best crms out there and wants to list crms from cheapest to expensive and same scenarios for like web hosting or any other niche

How can I do this? Any tool available?  Or do I need a developer to make a bot for each crm provider to get pricing and automate that",https://www.reddit.com/r/webscraping/comments/106tqgo/price_scraping_and_monitoring_bot/,auto
760,Gather public facebook events,"Hello!

I want to fetch Facebook events in my city. Unluckily, it seems like the API only provides this data to ""Facebook Marketing Partners"" (see [https://developers.facebook.com/docs/graph-api/reference/event/](https://developers.facebook.com/docs/graph-api/reference/event/))

&#x200B;

So, naturally webscraping is the only possibility left. However, I am concerned that facebook will detect that I automate stuff and just block my account if I scrape the website every few hours.

Is there another way to solve this problem? I guess keeping the session cookie and not always relogging is a must, otherwise I will run into captchas quickly. Has anyone ever tried this and can recommend something?",https://www.reddit.com/r/webscraping/comments/106n33r/gather_public_facebook_events/,auto
762,need help with puppeteer radio button clicking,"im using puppeteer to automate a survey, but the selector id of the buttons change every time so how can i always look for the right button",https://www.reddit.com/r/webscraping/comments/104yosd/need_help_with_puppeteer_radio_button_clicking/,auto
763,Scraping a Dynamic Webpage with rSelenium,"Hi! 

&#x200B;

I have been attempting to find a workaround for a web crawler that I am building to download a datafile file on a schedule. I have encountered a problem when trying to get the path for a dropdown menu, my problem specifically is that I just can't figure out the xpath or selector for it. 

[Here](https://opendata.dc.gov/datasets/integrated-tax-system-public-extract/explore) is the website. After navigating to the page of interest by clicking the download button, I then need to click the ""Download Options"" button to bring up a drop down where I can then click the dropdown element which initiates the file download. I've attached pics for reference of the download option button I am referencing. Additionally, I've provided my script thus far at the end. 

&#x200B;

Thank you in advance for your help! 

    # Load Packages
    ## Data Manipulation
    library(tidyverse)
    ## Webscaping
    library(RSelenium)
    ## Selenium Remote Server
    library(netstat)
    rs_driver_object <- rsDriver(
      browser = ""chrome"", 
      chromever = ""108.0.5359.22"", 
      verbose = F,
      port = free_port()
    )
    rem_dr <- rs_driver_object$client
    # Open a chrome page
    rem_dr$open()
    # Navigate to webpage
    rem_dr$navigate(""https://opendata.dc.gov/datasets/integrated-tax-system-public-extract/explore"")
    # Fit window to full screen
    rem_dr$maxWindowSize()
    # Navigate to Download Button and Click 
    rem_dr$findElement(using = ""xpath"", ""//*[contains(concat( ' ', @class, ' ' ), concat( ' ', 'btn-default', ' ' ))]"")$clickElement()
    # Navigate to Download Options and Click
    rem_dr$findElement(using = ""xpath"", ""//div//calcite-dropdown"")$clickElement()

&#x200B;

[First step in accessing the download options dropdown](https://preview.redd.it/ra5diarnp1aa1.jpg?width=1667&format=pjpg&auto=webp&v=enabled&s=a2e747be8c6e42115e3f0d00ddd33d01e270f462)

&#x200B;

&#x200B;

[Final step to initiate the download for the file](https://preview.redd.it/mzmnj2nsp1aa1.jpg?width=1667&format=pjpg&auto=webp&v=enabled&s=8fce7b7a584f8c02cc995888fe8a10843c6f0a51)",https://www.reddit.com/r/webscraping/comments/1036j0g/scraping_a_dynamic_webpage_with_rselenium/,auto
764,Gurufocus / Implemented new cloudflare protection?,"Hello - i was scraping data from gurufocus for a long time - eg. from this site

[https://www.gurufocus.com/term/pb/AAPL/PB-Ratio](https://www.gurufocus.com/term/pb/AAPL/PB-Ratio)  


But resently i saw a new protection from gurufocus and get this page when try to scrape data:

https://preview.redd.it/6sgt2395zz9a1.png?width=1320&format=png&auto=webp&v=enabled&s=6ab9538f367de0c6880f02afcee7308d38248ff0

I can find a workaround using residental proxies eg. from smartproxy.

Is there any other (maybe cheaper) solution to still get the data form gurufocus?",https://www.reddit.com/r/webscraping/comments/102zfub/gurufocus_implemented_new_cloudflare_protection/,auto
765,Scraper to collect APY data from Web3 frontends,"I'm working on a project which offers auto-compounding vaults for yield-bearing opportunities across a variety of Web3 protocols/dapps. Due to the nascent nature of this industry, there aren't many reliable sources of data and it looks like I'll have to scrape a few front-ends to grab the listed APYs in some cases. For example, I need to scrape the list of farms and their associated APYs from the Sushi.com front-end.

Any recommendations for the best route to go about doing this? I personally have no experience with scraping, but have a decent grasp on Typescript, JavaScript & Python. Appreciate any guidance you can share in this regard. Thanks!",https://www.reddit.com/r/webscraping/comments/102vcbo/scraper_to_collect_apy_data_from_web3_frontends/,auto
766,Scraping Aliexpress search page does not return all products,"

I have the below code, which I expect to return 60 products, but instead only returns 16:

    driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()))
    
    url = 'https://www.aliexpress.com/w/wholesale-silicone-night-light.html?SearchText=silicone+night+light""&""catId=0""&""initiative_id=SB_20230101130255""&""spm=a2g0o.productlist.1000002.0""&""trafficChannel=main""&""shipFromCountry=US""&""g=y'
    
    driver.get(url)
    
    driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
    
    html = driver.page_source
    soup = BeautifulSoup(html, 'lxml')
    
    product_links = []
    
    
    def get_element_title(element):
        return element.select('h1[class*=""manhattan--titleText--""]')[0].text
    
    
    def get_product_links(soup):
        for element in soup.select('a[class*=""manhattan--container--""]'):
            link = f""http:{element['href']}""
            product_links.append(link)
            print(get_element_title(element))
    
    
    get_product_links(soup)

I manually checked the class name for all the products, since I  thought maybe some of them have different class names in an effort to  stop scraping, but they all have the same class name.

Screenshot since I think the class names are randomly generated for different people

&#x200B;

https://preview.redd.it/ri63jykmdo9a1.png?width=490&format=png&auto=webp&v=enabled&s=93588d01bcab0e6b7c857e4548002d166ccc5aa4",https://www.reddit.com/r/webscraping/comments/101k1rd/scraping_aliexpress_search_page_does_not_return/,auto
767,Looking for python developers specialized in web scraping," I've been developing this automation software for almost one year. The software is PhantomBuster alternative, and it's released now. I plan to expand features that will make it more unique, and I'm looking for a python developer to join me in building software that will help people in the marketing area. More about the software - [https://myfolder.notion.site/myfolder/Phantom-Connect-0aedc60ce03043cc83f8fd55aa558d9c](https://myfolder.notion.site/myfolder/Phantom-Connect-0aedc60ce03043cc83f8fd55aa558d9c)",https://www.reddit.com/r/webscraping/comments/zzyspg/looking_for_python_developers_specialized_in_web/,auto
768,Python script that scrapes the People also ask section from Google in the niche you want and publishes it to a WP website.," 

I found a python script that scrapes the people also ask section in your niche and publishes automatically to your WordPress website.

[https://www.youtube.com/watch?v=JJMHZ2qbjBg&t=139s](https://www.youtube.com/watch?v=JJMHZ2qbjBg&t=139s)

Based on the article it scrapes 25 questions and answers in your niche and publishes them to your WP website as a post. It also scrapes an image and YT video based on the title.

More info [https://sarc-wv.com/people-also-ask-script-scrape-and-publish-full-step-by-step-installation-guide/](https://sarc-wv.com/people-also-ask-script-scrape-and-publish-full-step-by-step-installation-guide/)

I am wondering, how to create something like this, without paying for it.

Thanks",https://www.reddit.com/r/webscraping/comments/zyve7l/python_script_that_scrapes_the_people_also_ask/,auto
770,Having trouble scraping from wiktionary,"I don't really know if anyone has experience with this, but I'm trying to web scrape this table of most common Mandarin worsd from wiktionary using BeautifulSoup. 

[https://en.wiktionary.org/wiki/Appendix:Mandarin\_Frequency\_lists/1-1000](https://en.wiktionary.org/wiki/Appendix:Mandarin_Frequency_lists/1-1000)

Three problems: 1) it prints the table really weirdly, where I want it to be like on line 62  (as an example) and orderly (simplified, traditional, pinyin (latin pronounciation), and meaning). For some reason it just outputs the pinyin on the last line on one and starts again on a new line, with NaN and file after it

2) The meaning doesn't show up for any of them

https://preview.redd.it/ldgz5xieeq8a1.png?width=571&format=png&auto=webp&v=enabled&s=3c417e4e38c55ab3a6502dbcab15f38e137d331d

&#x200B;

3) It stops abruptly at line 312 with a bad character symbol when there's way more for it to go

&#x200B;

https://preview.redd.it/28u8jad1fq8a1.png?width=469&format=png&auto=webp&v=enabled&s=f4ca6872fe79e357582243cd2596816073e1e4f1

Any help would be greatly appreciated",https://www.reddit.com/r/webscraping/comments/zxq7og/having_trouble_scraping_from_wiktionary/,auto
771,is manual scrapping still alive?,"I am trying to understand if the current automated tools cover all spectrum of data scrapping projects? I feel like the automated tools, though beneficial in many scenarios, cannot cover many projects that might be small,m or very complex for an automated tool, or maybe because the tools are complex to use.",https://www.reddit.com/r/webscraping/comments/zv7t91/is_manual_scrapping_still_alive/,auto
773,Web Scraping,Is it legal to scrap data from a website like [rockauto.com](https://rockauto.com) ?,https://www.reddit.com/r/webscraping/comments/ztq5nu/web_scraping/,auto
774,"For Playwright, what language should I choose? Why is it not explained in docs?","I want to use Playwright for automation mainly for personal projects.

I am more proficient in Javascript but I wonder if it is a good idea to use Playwright to pick up Python. Because Python is a useful language. Any idea? I have find some discussion on Reddit about this, and I wonder why this very important question is not mentioned in the docs of Playwright? When I click ""Get start"" in the homepage of Playwright it just bring me to NodeJS Playwright document..",https://www.reddit.com/r/webscraping/comments/zsoszl/for_playwright_what_language_should_i_choose_why/,auto
775,webscrape remove.bg,i want to upload image and download from remove bg automatically without paying for their api using python. can i has full project that does that? thanks,https://www.reddit.com/r/webscraping/comments/zs3apj/webscrape_removebg/,auto
776,[Scrapy]Where can I find this graphed data???(please help or give suggestions)," 

I am trying to scrape **price history** of below image from this link: [https://www.akakce.com/laptop-notebook/en-ucuz-macbook-air-mgn63tu-a-apple-m1-8-gb-256-gb-ssd-13-3-notebook-fiyati,882468581.html](https://www.akakce.com/laptop-notebook/en-ucuz-macbook-air-mgn63tu-a-apple-m1-8-gb-256-gb-ssd-13-3-notebook-fiyati,882468581.html)

&#x200B;

https://preview.redd.it/8itr0p16gy6a1.png?width=1920&format=png&auto=webp&v=enabled&s=6330ecb47816d7a776c230e5b4662aee2c5ec5a9

But the site does not keep the price history data in the **inspect element** section. It only **dynamically** shows the the price of the date **where you last hovered your mouse on**(Its css selector is :""**#tooltip > span**"").

&#x200B;

https://preview.redd.it/v3zt5d47gy6a1.png?width=1920&format=png&auto=webp&v=enabled&s=c52ece5af5e7b43ec2af96c9734069668c1b41a0

Also The table does not get its data from **Network** requests(I checked **Fetch/XHR I am not sure if it is possible to be taken from other sections**).As you can see there is only **one request** in Fetch/XHR tab and it only returns a -1 and nothing else.

**Where can this data be. How can I find it so that I can scrape it?**",https://www.reddit.com/r/webscraping/comments/zqa0xo/scrapywhere_can_i_find_this_graphed_dataplease/,auto
777,Help with first scraper,"Hey guys I'm hoping someone might be able to help me with one of my first python projects.

&#x200B;

 I'm trying to make a simple scraper to make a list of apartments on craigslist. But had a hard time so just trying to get it to print the number of search results first for now.

I'm not sure why this code isn't working. 

    import requests
    from bs4 import BeautifulSoup
    
    session = requests.Session()
    # Make a GET request to the Craigslist page
    url = ""https://newyork.craigslist.org/search/mnh/apa#search=1~gallery~0~0""
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}
    page = requests.get(url, headers=headers)
    
    # Make sure the request was successful
    if page.status_code != 200:
        print(f'Error: request returned status code {page.status_code}')
    else:
        # Parse the HTML of the page
        soup = BeautifulSoup(page.text, 'html.parser')
    
        # Find the element with the class ""result-count""
        result_count_element = soup.find('div', {'class': 'result-count'})
    
        # Check if the element was found
        if result_count_element is None:
            print('Error: element with class ""result-count"" not found')
        else:
            # Extract the text of the element and split it by space
            result_count_text = result_count_element.text.split(' ')
    
            # The first element in the split text should be the number of search results
            result_count = result_count_text[0]
            print(f'Number of search results: {result_count}')
    

It prints: Error: element with class ""result-count"" not found

as far as i can tell it should be finding this result-count and printing 1,773

https://preview.redd.it/66tgpknghe6a1.png?width=789&format=png&auto=webp&v=enabled&s=2c3a82dc3559663b1a2c162f666f5f42f0fa0929

&#x200B;

Im a complete beginner any help is very much appreciated thanks.",https://www.reddit.com/r/webscraping/comments/znzud2/help_with_first_scraper/,auto
778,What is the best way to bypass the Octoparse human verification requirement?,"It appears that this screen appears when I enter the Zillow website address in octoparse. Could you please let me know what to do about it?

I am using residential proxy and user agent rotation.

https://preview.redd.it/07q71xxrw96a1.png?width=779&format=png&auto=webp&v=enabled&s=d301ddca5856604ff75a1562f01c3ec9205504cd",https://www.reddit.com/r/webscraping/comments/znghei/what_is_the_best_way_to_bypass_the_octoparse/,auto
779,Different views for google results?,"Hello - i tried to call a google-search using selenium -

For examples this link:[https://www.google.com/search?q=http%3A%2F%2Fwww.turci.it](https://www.google.com/search?q=http%3A%2F%2Fwww.turci.it)

When i do this with selenium i get this view:

https://preview.redd.it/un4mhw84296a1.png?width=1911&format=png&auto=webp&v=enabled&s=968744988af479899ff40126987e0acc1dc9a105

And when i open this in the incognito browser manually i get this view:

&#x200B;

https://preview.redd.it/19d9m0nk296a1.png?width=1879&format=png&auto=webp&v=enabled&s=96a07d1f0a05929eb3aee3cc7ba9494d6e3464d7

Why is this different?Also the html-structure seems to completely different?

Cookie i found in the manual opend browser window:

&#x200B;

https://preview.redd.it/pi70gms2i96a1.png?width=1119&format=png&auto=webp&v=enabled&s=968113dae59f74c0191c2a7ac3e186c2a41f016d",https://www.reddit.com/r/webscraping/comments/znd002/different_views_for_google_results/,auto
780,Best Configuration/Infrastructure for...,"I want to scrape a website for sales data. There are 24,000 pages to look at daily.

Initially, each page scrape will take \~ 60 seconds. After the initial data scrape/save, each page scrape should take \~ 15 seconds.

I am looking at using Python & Docker deployed with AWS Lambda but am running into issues, namely an AWS Lambda can only be running for max 15 minutes. 

I started researching the Lambda concurrency options, and it still seems like the process/code/config will be long & complicated.

Conversely, I ran a multi-threaded Python script from my local mac, and it successfully made it through the \~ 24K pages in \~ 16 hours (with max 15 seconds a page scrape). I just don't want to have to run locally everyday - I want it to run offsite/cloud automatically.

Thoughts/suggestions?",https://www.reddit.com/r/webscraping/comments/zmpakp/best_configurationinfrastructure_for/,auto
781,Get email from specific site?,"Hello - i try to collect the data from this website:

[https://www.11880-heizung.com/heizung/grosshabersdorf/fischer-sanitaer-u-heizungstechnik-gmbh-cokg-27726455.html](https://www.11880-heizung.com/heizung/grosshabersdorf/fischer-sanitaer-u-heizungstechnik-gmbh-cokg-27726455.html)

On the right side you can see there eg. the email-adress on the site.When i inspect the site i also see the entry in this form:

https://preview.redd.it/1h4yuaxrc26a1.png?width=562&format=png&auto=webp&v=enabled&s=e084e05d794e34b33b99c83f8216d74553a620b9

&#x200B;

I now get the information from this site using selenium and bs4 and the selected span looks like this in the end:

    <span class=""flex-1 truncate cursor-pointer"" onclick=""vueApp.enableContactForm({
                    'entryId' :&quot;27726455&quot;,
                    'entryName' :&quot;Fischer Sanit\u00e4r- u. Heizungstechnik GmbH &amp; Co.KG&quot;,
                    'trackingInvokedBy' : 'contact-on-detail',
                    'handleTracking': true
                });"">
     <a class=""__cf_email__"" data-cfemail=""6112041317080204210c18070812020904134f0504"" href=""/cdn-cgi/l/email-protection"">        
      [email protected]
     </a>
    </span>

As you can see the email is not visible anymore?

Any idea how i can get the email which is visible when inspecting the site?",https://www.reddit.com/r/webscraping/comments/zml1jr/get_email_from_specific_site/,auto
783,Need help selecting this div,"I'm scraping a very old site that uses ""#document"" tag in its html (????), and i need to find a child div inside this element tag.  
I'm using Selenium (Python). No xpath extensions worked.  
Does someone knows how could I select it?

I tried the /html/frameset/frame/#document/html/body/div\[2\]/div xpath but it seems that its not valid  


https://preview.redd.it/cuqp3d85iv4a1.png?width=484&format=png&auto=webp&v=enabled&s=10ab8054172f4ab17ed37e9ff36687d3f6091aa0",https://www.reddit.com/r/webscraping/comments/zgx7kj/need_help_selecting_this_div/,auto
784,"Ebay Scraping, filter out international results, select parents that do not have specific descendants","Hello all! I have some code that I put together to scrape ebay sold prices using BeautifulSoup and so far it is working pretty good. The only issue I currently have is that it also pulls prices for 2 categories that ebay adds to the search results page (International Sellers, and results matching fewer words)

I am struggling to filter these out. Its like I need to identify if the listing (the parent) contains a specific descendant and then filter out that parent. I hope that is clear, here is a sample:

[https://www.ebay.com/sch/57988/i.html?\_from=R40&\_nkw=Fjallraven%20nuuk%20parka&LH\_Complete=1&LH\_Sold=1&\_udlo=50&\_udhi=600&LH\_PrefLoc=1](https://www.ebay.com/sch/57988/i.html?_from=R40&_nkw=Fjallraven%20nuuk%20parka&LH_Complete=1&LH_Sold=1&_udlo=50&_udhi=600&LH_PrefLoc=1)

The picture below is an example of an item that I would like to filter out. It has a Span Class for location. This class only exists if the item is from an international seller.

https://preview.redd.it/mrf9dfbudr3a1.png?width=895&format=png&auto=webp&v=enabled&s=ca0f094757cdb0787e9d5abf6632b9095e8f1cef

    import xlwings as xw
    import bs4 as bs
    import requests
    import statistics
    
    @xw.func
    def get_prices(url,args =[]):
        url_base = requests.get(url).text
        soup = bs.BeautifulSoup(url_base,'lxml')
    
        products = []
        results = soup.find('div', {'class': 'srp-river-results clearfix'}).find_all('div', {'class': ['s-item__details clearfix'] })
        for item in results:
            price = item.find('span', class_='s-item__price').text.replace('$', '').replace(',', '')
    
            if 'to' not in price:
                price = float(price)
                products.append(price)
    
    #def calculate_averages(products):
        mean = round(statistics.mean(products), 2)
        median = round(statistics.median(products), 2)
        mode = round(statistics.mode(products), 2)
    
        return mean, median, mode",https://www.reddit.com/r/webscraping/comments/zbshbg/ebay_scraping_filter_out_international_results/,auto
785,"Spidergram is a collection of tools my company Autogram has built or enabled over the past several years to support our work to automate content inventories for large websites: it's part web crawler, part domain model, and part mad science. We released the first public beta today.","Spidergram is a toolbox for exploring, auditing, and analyzing complicated web sites — particularly ones that use multiple CMSs, span more than one domain, and are maintained by multiple teams inside an organization. 

https://github.com/autogram-is/spidergram

We built Spidergram to overcome some of the the roadblocks we hit when using existing crawling and inventory tools on our clients' complex, multi-site web properties. Our goal is to make it easier to answer complicated questions about big, sprawling web properties.",https://www.reddit.com/r/webscraping/comments/zb0u91/spidergram_is_a_collection_of_tools_my_company/,auto
786,Select correct html attribute in R with rvest.,"I have problem with rvest.Im want to add the Author but the selector dont run well. Maybe is because this is next to `href`.

    library(tidyverse)  
    library(rvest)   
    startTime <- Sys.time()  
    get_cg <- function(pages) {     
    cat(""Scraping page"", pages, ""\n"")    
     page <-str_c(""https://cgspace.cgiar.org/discover?     scope=10568%2F106146&query=cassava&submit=&rpp=10&page="", pages) %>%    read_html()     
    
    tibble(   
    title = page %>% html_elements("".ds-artifact-item"") %>% html_element("".description-info"") %>% html_text2(), # run well    
    
    fecha = page %>% html_elements("".ds-artifact-item"") %>%   
    html_element("".date"") %>%   
    html_text2(), # run well    
    
    Type = page %>% html_elements("".ds-artifact-item"") %>%  
     html_element("".artifact-type"") %>%  
     html_text2(), # run well    
    
    Autor= page %>%    html_elements("".ds-artifact-item"") %>%    html_element("".description-info"") %>%  
     html_attr(""href""), # not download the Authors 
    
    link = page %>%   html_elements("".ds-artifact-item"") %>%   html_element("".description-info"") %>%   
    html_attr(""href"") %>% # run well   
    str_c(""https://cgspace.cgiar.org"", .))}    
    
    df <- map_dfr(1, get_cg)    
    endTime <- Sys.time()  
    print(endTime - startTim)",https://www.reddit.com/r/webscraping/comments/zawrxi/select_correct_html_attribute_in_r_with_rvest/,auto
787,how to know if a website is scrapable or not?,"I was trying for hours to scrape a website 

this one : [https://www.ouedkniss.com/automobiles-citadine/1](https://www.ouedkniss.com/automobiles-citadine/1)

I'm using scrapy but i couldn't scrape anything, is it possible to know if a website is scrapable or not and what's up with this site?",https://www.reddit.com/r/webscraping/comments/z7xk0a/how_to_know_if_a_website_is_scrapable_or_not/,auto
788,What is the best coding language to do the following:,"Hello, I want to build an app, or a chrome extension, or a script that will help me automate browser tasks on a classifieds website, the tasks I want to automate are:  
\- Login to my account  
\- fetch a list of specific ads and store them on a database  
\- watch for newly added ads and notify me when a specific ad is published  
\- ability to submit a contact request to specific ad owners  
What technology is best to perform this kind of operations knowing that I will do a lot of DOM manipulation and XHR requests. I usually work with Node to build scrappers. I wonder if I should switch to Python? or if there's any other coding language I should consider like Python. You can suggest a node framework. I heard about phantomjs but I never tried before.Looking forward to your suggestions",https://www.reddit.com/r/webscraping/comments/z7qjcb/what_is_the_best_coding_language_to_do_the/,auto
789,How to scrape whole webpage from a website that unloads data once no on-screen with Octoparse?,"I'm having particular trouble with [this](https://voila.ca/products?sortBy=favorite&sublocationId=43a936d1-df1d-4bf1-a09c-b23c6a8edf63) website. I'm semi-used to Octoparse and usually scrolling websites aren't an issue but what I've found with this particular website is that the data un-loads once not on the display screen. As a result, if I tell Octoparse to scroll down the entire page and then scrape the data it only scrapes the last 8 or so on the bottom of the page (the Xpath points to the last 8 or so entries and can't figure out how to change this). I tried another approach where I scroll, then extract, then scroll, then extract over and over again and manually did an extract path (vs auto-detect), however, it pulls the same data for every extract. It does not switch to a new entry. I've tried various loops inside of scrolls and scrolls inside of loops but can't figure it out. Any help is appreciated!",https://www.reddit.com/r/webscraping/comments/z5gczk/how_to_scrape_whole_webpage_from_a_website_that/,auto
790,I created a the best local web scraping extension in 2 weeks,"Hi guys! I created a the best local web scraping extension in 2 weeks. [https://madscraper.com/extension](https://madscraper.com/extension)

What makes this different when many extensions exist?

1. Some websites are really stubborn to scrape from. We know because we tried 10 of the most popular scraping extensions for our own business. We rebuilt selectors from scratch to cover 99% of all use cases, no kidding.
2. Selects the data you really want. We give you variations of selections to choose from so you get exactly the rows you want.
3. Keeps your data on the cloud. Most web scrapers require you to download your data right away or pay to save your data online. Whatever you scrape is automatically saved so you can access it any day, anytime. Download formats in JSON & CSV.
4. Pagination Hell. Paginating simple table structures with next hyperlinks is easy. What is difficult for others is handling javascript buttons, single page apps and tricky actions. Some extensions only allow pagination on the cloud and charge for it.
5. Primary Keys. To prevent duplicate data, you can set a column as a primary key so that you only get fresh and updated data every time
6. Built for your team. You and others can scrape many websites, from different browsers & devices with a single source of truth.
7. Select once, run anytime. After you create a selection once, you or your team will never re-create it. Anytime you visit the same page, Mad Scraper checks for new data and updates.
8. It's SUPER simple to use. We tried 10 popular web scrapers and made this so much easier to use than all of them. A 1 minute tutorial is all you'll ever need. [https://madscraper.com/tutorial](https://madscraper.com/tutorial)
9. Built for developers. Web-hooks for fresh data and a JSON API link for your applications.
10. We'll listen to you. If you have any suggestion and it's a killer one, we'll work on it in a week or less!

IN OUR DECEMBER TIMELINE

* Google Sheets Integration
* AirTable Integration
* Deep Scraping
* and if we have the time, a landing page 

Give it a try and you'll love it. Get it at [https://madscraper.com/extension](https://madscraper.com/extension). In case you're wondering, IT""S FREE. All feedback & suggestions are welcome.",https://www.reddit.com/r/webscraping/comments/z532ii/i_created_a_the_best_local_web_scraping_extension/,auto
791,Scraping Specialcards Prices form Futbin,"Hello, I play Fifa Ultimate Team and I would like to create a spreadsheet with which the prices of the players are automatically updated via [Futbin.com](https://Futbin.com). You can get the prices for normal gold cards via e.g. [https://www.futbin.com/23/playerPrices?player=231747](https://www.futbin.com/23/playerPrices?player=231747)

These are the prices for Mbappe. But I would also like to have prizes from special cards. Unfortunately, they have the same id (231747) as the normal cards.

Anyone know how to get the prices?",https://www.reddit.com/r/webscraping/comments/z49ems/scraping_specialcards_prices_form_futbin/,auto
792,Has anyone made money building a product / service based on web-scrapping here ?,"Hey all,

I am looking for some inspiration on projects that you guys did and managed to get clients for. I am not looking for the freelancing stories but rather product or service stories.  
For instance you found a website that has some useful data, you scrapped it, repackaged it and managed to sell it to recurring paying customers.  
Also automation product / services ideas are welcome.  


Cheers",https://www.reddit.com/r/webscraping/comments/z28kqs/has_anyone_made_money_building_a_product_service/,auto
793,How do I get this bullet list (HTML),"I'm sucks in coding. I'm having period cramps and all. 

I tried

for li in soup.find\_all('li',{'style':'text-align: justify;'}):  
for item in li.find\_all('span'):  
job.setRawDescription(str(li.text))

[I want to get all of the bullet lists](https://preview.redd.it/vors74hg2m1a1.png?width=643&format=png&auto=webp&v=enabled&s=9729ba8eb232d0074b33f114b3fe2805a6f35c58)

https://preview.redd.it/r9jypaup2m1a1.png?width=737&format=png&auto=webp&v=enabled&s=f793354e25d37a4f7fde7efaed13d8fcfc2084e4

[My code](https://preview.redd.it/d8n3mro73m1a1.png?width=524&format=png&auto=webp&v=enabled&s=ee20917357de88650fb54564b14590887d3306de)

[it captures the lasttt one and not in \\""Your role & responsibilities:\\""](https://preview.redd.it/xrel0c4a3m1a1.png?width=480&format=png&auto=webp&v=enabled&s=7fff01026805cf3d3a12d9398815b6925d6b3d8f)",https://www.reddit.com/r/webscraping/comments/z2ctpy/how_do_i_get_this_bullet_list_html/,auto
794,Can't get HTML attribute,"Hi. I'm new to the web scraping world. I want to scrape the data-counter attribute (data-counter=""2332"") but I can't. I used the same way to get href attribute but this time it won't work. Can anyone help me? 

https://preview.redd.it/3egap5lfji1a1.png?width=1115&format=png&auto=webp&v=enabled&s=a0c2f815e969e5291d3149e1d997ef30dd2b18e5

[My usual way](https://preview.redd.it/47o9xdnwji1a1.png?width=434&format=png&auto=webp&v=enabled&s=a600963aa203e96b2c63b40a810209a71e504b49)

&#x200B;

[The error](https://preview.redd.it/jucaea63ki1a1.png?width=463&format=png&auto=webp&v=enabled&s=d70f6caf1d78f15bff3180c7c19ac0154ae5643c)

&#x200B;

[I tried int](https://preview.redd.it/881tfe97ki1a1.png?width=512&format=png&auto=webp&v=enabled&s=b133bedafe9ef21bf15d1f6b8c51a53d0f9202ea)

&#x200B;

[But error too](https://preview.redd.it/fn0yoprbki1a1.png?width=462&format=png&auto=webp&v=enabled&s=6b96fd7c2f069b743e0b1162a8a91bcd68374899)",https://www.reddit.com/r/webscraping/comments/z1v5x8/cant_get_html_attribute/,auto
795,Looking for software to scrape documents from EU institutions websites,"Hello!

My  task is to download documents from the main EU bodies. These websites  do not require any kind of log in, the documents and their material are all available freely. I bought ScrapeBox to help me with that but it’s not working as intended. 

Is there anyone who can help me automate this process (whether entirely or partially)? 

&#x200B;

Below is the link to my previous post. It has more information on what I am looking for.

[https://www.reddit.com/r/webscraping/comments/x6gb3e/i\_would\_like\_some\_help\_with\_troubleshooting/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/webscraping/comments/x6gb3e/i_would_like_some_help_with_troubleshooting/?utm_source=share&utm_medium=web2x&context=3)

&#x200B;

I have since given up on Scrapebox since 1) it was useless and 2) these crooks refuse to refund even though it was evident in our communications that their software cannot do what I need it to.

&#x200B;

I am now looking to for software that does this. Either to outright buy it or with a subscription fee.",https://www.reddit.com/r/webscraping/comments/z1t2wr/looking_for_software_to_scrape_documents_from_eu/,auto
796,Scraping a JS file and all its associated JS files,"Is it possible to download a JS file and all the associated JS files automatically?
This is the package from a CDN that I want to build locally:

https://jspm.dev/@spectrum-web-components/bundle/elements.js

This build of it is the only one that works for my use case.  Is it possible to download this JS file and all associate JS files?",https://www.reddit.com/r/webscraping/comments/z17o1i/scraping_a_js_file_and_all_its_associated_js_files/,auto
797,Browser automation Framework,Has anybody used the Browser automation framework what’s your experience?,https://www.reddit.com/r/webscraping/comments/z0pdch/browser_automation_framework/,auto
798,Can't get JSON content from HTML,"I'm scraping a website and it has JSON but my code doesn't seem to catch them. Everything else is fine but the last 4 rows give an error. Here's the snippet of the code:

&#x200B;

https://preview.redd.it/5uvjgm43a61a1.png?width=554&format=png&auto=webp&v=enabled&s=3a521ed768623922a0394551517ff52be0683741",https://www.reddit.com/r/webscraping/comments/z0fm0i/cant_get_json_content_from_html/,auto
799,Error on IMPORTHTML from Google Search,"Anyone know why I might be getting this error?

&#x200B;

https://preview.redd.it/p9meymbb9u0a1.png?width=1924&format=png&auto=webp&v=enabled&s=29029423b6294b1728bbd7b55d3b77fe0134d8f3

https://preview.redd.it/mz62p0ma9u0a1.png?width=416&format=png&auto=webp&v=enabled&s=a72a9b80813a88ab68c12560312193882513dcd1",https://www.reddit.com/r/webscraping/comments/yz3v9b/error_on_importhtml_from_google_search/,auto
800,Need some help to finish an online marketplace notification project,"Hi guys, I have set up a Gumtree (online marketplace) scraper, to scrape data from a specific item category every 2 mins and then export the data in JSON, HTML, etc.

I want to know how to automatically analyse the data and send an email/SMS when a scrape yields data that meet specific criteria. For example:

Price: less than £100
Keywords: Trek mountain bike 

So every time a new listing is created for a Trek mountain bike that is under £100, I would be notified instantly, within a minute or so of the listing being uploaded to Gumtree.

I already have the extracted data but I don’t know how to analyse it.

I am currently using Apify platform",https://www.reddit.com/r/webscraping/comments/yyn0ls/need_some_help_to_finish_an_online_marketplace/,auto
801,Scraping Facebook group posts for email addresses made in the comments,"I have a need to scan all of the comments on posts made in a given Facebook group (including clicking ""All Comments"" and ""show previous X comments"" to get all of the comments) to find email addresses posted in the comments. Users in these groups tend to post their email address as a comment and I want to capture all of them. Does anyone know an automated python script or something else that works?

I looked through GitHub and most of the scripts are outdated and I tried tweaking them but ran into other errors. I could code something custom but really just looking for something quick and free if anyone has a recent piece of code they can point me to that has worked for them?",https://www.reddit.com/r/webscraping/comments/yyatvv/scraping_facebook_group_posts_for_email_addresses/,auto
802,Challenging task. NEED HELP,"I am looking for a service or way to have people text a SMS number in whatsapp and have an automated response/reply that ask for a sequence of information in response to user input. I also want it to record the user responses into some kind of file or excel sheet.

&#x200B;

What is this called? is there a way to create this?

&#x200B;

If I didn't explain it well, what I am imagining is like a customer support automated text feature where you text a number enter in your name then the automated machine ask for your email, then you send it, then it ask for your order number etc. I need something like that over whatsapp and for the responses to be recorded.",https://www.reddit.com/r/Automate/comments/10fcdna/challenging_task_need_help/,auto
803,automatically changing TV brightness.,How can I automate my TV to change to yellow mode or less bright? Like through a iPhone routine/ smart home automation,https://www.reddit.com/r/Automate/comments/10ecfb9/automatically_changing_tv_brightness/,auto
805,Recommendation for automate,"I’m currently a legal Intern and my boss has graciously given the task  to summarize 300+ legal cases by June, I’d like to know if I can automate this task, by either using a website or by purchasing a software. For any how do I go about doing it. Thanks a lot in advance!!!!!",https://www.reddit.com/r/Automate/comments/10e1uc9/recommendation_for_automate/,auto
806,"Hot take: AI and ChapGPT are not ready yet, text expanders > AI","AI has a lot of promise, but I feel like they should only be used for idea generation right now. They're just too new to actually use for real work and I've seen many cases where they just aren't as accurate as they could be. They definitely could get there one day, but they aren't ready yet. 

Personally, I prefer to use a text expander (automates typing using pretyped phrases) to automate my writing. Text expander, Text Blaze, and aText are solid options. They aren't AI like ChatGPT, but they can help you automate without the risk that AI has right now. 

Just my thoughts, as I see people post ""Is ChatGPT the future"" every day in every single thread. Looking forward to everyone's thoughts!",https://www.reddit.com/r/Automate/comments/10dfb32/hot_take_ai_and_chapgpt_are_not_ready_yet_text/,auto
807,New Research From Google Shines Light On The Future Of Language Models ⭕,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs might allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* reduce hallucination and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example is words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (a proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/n7svd95hv0ca1.png?width=800&format=png&auto=webp&v=enabled&s=282b88e4c256236b65b23de9b8ac392abbb4e656)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)",https://www.reddit.com/r/Automate/comments/10bq9gp/new_research_from_google_shines_light_on_the/,auto
808,Performance of Automation Anywhere v.26,"Hi, Have you used the latest version 26 of Automation anywhere ? How's it faring compared to the previous AA versions, what better services are being offered ?",https://www.reddit.com/r/Automate/comments/10avwt7/performance_of_automation_anywhere_v26/,auto
809,New to UI Automation: Seeking Recommendations for Learning UI.Vision,"Hey everyone, I'm new to the world of UI automation and I'm interested in learning more about UI.Vision. Can anyone recommend some good resources or courses for a beginner to get started with UI.Vision? Are there any particular websites or places that you've found to be particularly helpful in learning this technology? Thanks in advance for your suggestions!",https://www.reddit.com/r/Automate/comments/10anuy5/new_to_ui_automation_seeking_recommendations_for/,auto
810,Trying to get saved Instagram posts into my Notion databases. Anyway to IFTTT++ it anywhere?,"Whenever I'm on Instagram, I hit the save/bookmark to denote that I'm interested in referencing something later.

I would love to find a way that when I hit save, the link to that Instagram post gets sent somewhere, anywhere. I'm particularly looking to put it in my notes database, but even if I can get it to a Google sheet, a list app, a doc, whatever, I could automate from there.

Unsure what to do other than manually copy pasting the link and describing it myself.

Note:
Instagram does have a saved items folder system, but the folder process for saved posts can only be utilized when you are saving a post from your feed, and not from inside a reels, or from the Saved section, but then it doesn't tell you which of your posts are not in folders. (imagine having your Gmail All Mail section, and your folders, but no inbox).",https://www.reddit.com/r/Automate/comments/10ai3jy/trying_to_get_saved_instagram_posts_into_my/,auto
811,Is there an AI that I can train to illustrate my photos in a specific style?,"We own a business where people send us their photos to be illustrated in a specific style. Is there an AI tool I can train to start automating this instead? It could help us save a lot of time and money. 

Here are the final [illustrations](https://imgur.io/a/XRmq2AC) - I don’t want to share the original photos but I’m sure you could imagine them, too.",https://www.reddit.com/r/Automate/comments/10aioqh/is_there_an_ai_that_i_can_train_to_illustrate_my/,auto
812,Intelligent Document Processing,"Hi!

I am trying to learn about Intelligent Document Processing

Need to build a automation tool to find some words in documents in pdf/word format

Make a check list about what was found 

These documents are digitalizations from a scanner 

Some documents have 900 pages or more, and some have bad quality digitalizations from decades ago (probably need to setup a database for each word)

I know there's several which can do that job, but I am looking for something more accessible, these available are too expensive targeting enterprises 

Any guidance would be very helpful!",https://www.reddit.com/r/Automate/comments/107n2wu/intelligent_document_processing/,auto
813,Lane following autonomous model car,"We have to develop a model car that should drive autonomously with the help of floor markings. The 2 floor marking lines are about 30cm apart and can have different colors, i.e. not just white or yellow. Do any of you know an open source project that already implements this or any tutorials I can start with. I don't have the training data to train an AI myself.

As hardware we have a Jetson Nano 4GB Ram, an Intel RealSense D435 Depth Camera and 4 mecanum wheels",https://www.reddit.com/r/Automate/comments/107idgh/lane_following_autonomous_model_car/,auto
814,Should I learn powershell or automation tools (Automation 360 & UiPath)?,"I already know python, I've done some web scraping using it and automated pulling sales data from multiple websites.

I also know SQL, and recently tried using some automation tools such as Automation Anywhere (didn't like it though).

And I'm wondering whether learning powershell would be good for a career in automation, or is it that RPA tools are mostly used in automation?

Thanks in advance.",https://www.reddit.com/r/Automate/comments/105x8ay/should_i_learn_powershell_or_automation_tools/,auto
816,Ideas for projects to automate?,"My friends and I are working on automation projects for learning purposes.

We tried pulling financial statements for every stock from a website and do some analysis on them (calculate current ratio and P/E).

We used Automation 360, and it sucked, but we managed to get it done.

We know python and wouldn't mind using it for automation.

I'm looking for projects ideas to work on that we can put on our resumes.

Thanks in advance.",https://www.reddit.com/r/Automate/comments/101fk0l/ideas_for_projects_to_automate/,auto
817,2023 US Automation Events and Conventions,"I work at an automation integration startup, but we currently don’t attend any machine or automation conventions. 

What are your favorite industry and indie events geared towards robotic and system automation?

This is the list I have so far

DesignCon 2023
Jan 31, 2023~Feb 2, 2023
Santa Clara, CA 

Smart Manufacturing - Take Automation to the Next Level with Sensors, Autonomous Hardware, AI and Software Robots
Feb 02,2023
Online

ATX West
FEBRUARY 7-9, 2023
Anaheim, CA

Promat 2023
March 20-23, 2023, Chicago IL.

Smart Manufacturing Automation Summit
29 Mar 2023 
Rosemont, United States

Automate
May 22, 2023~May 25, 2023
Detroit, MI

Embedded Vision Summit 2023
May 22, 2023~May 25, 2023
Santa Clara, California 

Food Northwest Process &amp; Packaging Expo
05 - 06 Apr 2023
Portland, Oregon 

Automate Show 2023
May 22 - May 25, 2023 
Detroit, MI 

Design Automation Conference 2023
Sunday, July 9, 2023, 
San Francisco, CA

World Congress on Industrial Automation
Mon, 20 - Wed, 22 Jul 2015
Burlingame, USA

Advanced Manufacturing Expo
09 - 10 Aug 2023
Grand Rapids, United States

Pack Expo
September 11—13, 2023
Las Vegas, NV USA

Fabtech 2023
September 11-14, 2023
Chicago IL

Industrial Automation North America
Sept 12 - 17
Chicago, USA

FA&amp;amp;amp;M Food Automation and Manufacturing
Oct 11~13 
Bonita Springs, FL",https://www.reddit.com/r/Automate/comments/100vcip/2023_us_automation_events_and_conventions/,auto
818,Are there any groups that are trying to help us prepare for the realities of automation?,"I've been playing around with ChatGPT for the past couple of weeks, and the potential is incredible. If you haven't tried it, I would highly recommend checking it out.

AI is improving rapidly, and there are very few jobs that I think are safe from being automated in the future. CGP Grey actually has an excellent video on the subject [Humans Need Not Apply](https://youtu.be/7Pq-S557XQU).

I don't think that AI will be ready to replace humans tomorrow, but I firmly believe that it will be possible within my lifetime, if not within the next 10 years.

Having said that, I don't think that our society is ready for millions of people to lose their jobs. We don't have a way to take care of or support these people.

Sadly, I think the ""human"" solution to AI will be to ban it, and guarantee that people will get to keep their jobs.

Personally, I'd like to see us move in the opposite direction. Rather than guaranteeing jobs, I'd rather see us guarantee housing, healthcare, education and maybe a Universal Basic Income?

People would have the freedom to work if they wanted to, but they wouldn't be required to. They could take care of their families, pursue higher education or pursue their passions.

I can't pretend to be an expert or to know what the best way to structure that type of society would be.

But if I can, I want to use my energy to work towards it. What is the best way that I can advocate for this? Are there any groups working on this right now? How can I join them?",https://www.reddit.com/r/Automate/comments/zwx1dq/are_there_any_groups_that_are_trying_to_help_us/,auto
819,HIPAA Compliant Automation?,"ISO of a HIPAA compliant automation software they would recommend. I'm an entrepreneur in healthcare and could benefit from just about anything, ranging from service to document generation.",https://www.reddit.com/r/Automate/comments/zvpdws/hipaa_compliant_automation/,auto
820,What do you automate and how?,"I can't think of anything I can automate. Looking for inspiration and ideas.

Edit: thanks for the input people",https://www.reddit.com/r/Automate/comments/zux1m4/what_do_you_automate_and_how/,auto
821,How to automate and anonymise process of collecting feedback?,"Tl;dr is there a way to collect survey data from multiple people using MS Access?  

I’m looking to improve an internal process which currently consists of internal feedback being sent to someone’s email, who then has to manually enter this data into a spreadsheet. This seems like a pretty wasteful process and I also think the lack of anonymity deters colleagues from giving honest feedback. I want to instead collect this feedback from colleagues, ideally as an anonymous form which is easily pulled into a spreadsheet/database.

In my previous role we would’ve used MS Forms, which was great as it would automatically populate a spreadsheet. However we have really strict information governance rules which bans access to MS Forms due to overseas data storage. I have MS Access installed but have never used it before. Is it possible to collect data using MS access? 

I know that there are a lot of paid tools (e.g. surveymonkey) etc that could easily do this. However, I work in the public sector - budgets are very tight and it’s unlikely they would agree to fund this. Even if they did agree, it would have to go through months of approvals, by which point I will be working on a different project anyway. However if anyone has come across any other Microsoft based solutions or free/open source solutions then that would be incredibly welcome :)",https://www.reddit.com/r/Automate/comments/zub64z/how_to_automate_and_anonymise_process_of/,auto
822,Task Automation - Help,"Looking to find a task automation solution that will take a link from a spreadsheet, paste it into a website (webpage speed calculator/analytics), extract the result (1 number) and paste the number back into the spreadsheet in the cell next to the cell with the website link. Ideas? New to the whole TAS world. Microsoft power automate or something similar?",https://www.reddit.com/r/Automate/comments/zu38ko/task_automation_help/,auto
823,Q: campaign design automation,"Question: im a freelance graphic designer looking to automate my workflow. A lot of the work I get from my clients is pretty simple, i create or I am given a hero image and I basically version it out at different sizes and add different logos and slogans to it. 

I can imagine this’ll get automated within the next 5 years but I do wonder.. Is there a way to automate this already? Like, put in the image, the campaign brief with the necessary versions needed and - bam- campaign assets are created? 

If not, do you suggest even trying to automate this? Or is that wayyy too big of a project for a beginner? Or maybe it’s just not the answer we need in the world lol

I’m still in the early stages of even understanding ai, automation and software development so any input is helpful. At the end of the day, i just want to get a conversation going about the topic relating to creative careers like graphic design. Feel free to discuss below. Thanks!",https://www.reddit.com/r/Automate/comments/zohof5/q_campaign_design_automation/,auto
824,What are the best platform/CRM for workflow automations?,"I am working with a real estate investment company and planning to transfer their current CRM over to a new platform. We are looking for a CRM platform that is highly customizable and has strong workflow automation. Ideally, I'd like to build a system that has ""decision tree"" functionality. An example of this would be ""If a new lead is entered into the CRM, then assign the lead to a team member and give them the task of 'call lead'."" From there, I would like almost a prompt to ask the user the following ""did the lead answer the phone?"" If the answer is yes, it would ask the user to write notes and then automatically schedule a follow-up call into their future to-do list. But, for example, if the lead did not answer, I would want the follow-up call set for an earlier date/time so that we make sure to contact the lead before it is too late. Basically, I would like to create a decision tree automation for the entire process of the business so that the system will be followed to the T every time. Does anyone have any suggestions for which CRM would be best for this? Thanks",https://www.reddit.com/r/Automate/comments/znhrde/what_are_the_best_platformcrm_for_workflow/,auto
827,I make a video on how to use the AI to your benefit. Give it a watch.,"&#x200B;

https://preview.redd.it/d6d2qqhzc56a1.png?width=234&format=png&auto=webp&v=enabled&s=bd1384496385aa16f98f04ae80f15f053fb40eb4",https://www.reddit.com/r/Automate/comments/zmzz5d/i_make_a_video_on_how_to_use_the_ai_to_your/,auto
828,Helllppp pleeeeaasssee,"Hi!! I’ve been trying to create an automated feature to login to my Amazon associates account to grab data and post to a Facebook page, but I can’t find anything that will do it all. Everything gets stuck on login. Can anyone point me in the right direction or tell me if it is not allowed (and/or possible) with Amazon associates? This is my last resort as I am not super techy but know my resources. Thank you in advance!",https://www.reddit.com/r/Automate/comments/zm3c7x/helllppp_pleeeeaasssee/,auto
829,How can you build Bots VIA API's with ElectroNeek,"Building your first bot on the ElectroNeek platform is simple. There are multiple ways to build a bot on our platform. Learn how you can build bots via APIs using ElectroNeek [Here](https://forum.electroneek.com/t/how-to-build-bots-via-api/902):

You can also join our community at [https://forum.electroneek.com](https://forum.electroneek.com/) for exclusive updates of our platform.

Happy Automation!!",https://www.reddit.com/r/Automate/comments/zl5r4o/how_can_you_build_bots_via_apis_with_electroneek/,auto
831,Accidentally deleted a flow in PowerAutomate,Idk if this is the group even but I accidentally deleted an important flow on PowerAutomate How do I go about re-covering it? 🤦🏽🤦🏽,https://www.reddit.com/r/Automate/comments/zkb1cd/accidentally_deleted_a_flow_in_powerautomate/,auto
832,The real opportunity in AI for most people will not be in AI but in building a front end around it.,"The real opportunity in AI for most people will not be in AI but in building a front end around it.

[https://news.ycombinator.com/item?id=33933117](https://news.ycombinator.com/item?id=33933117)

https://preview.redd.it/s2tuyqyna75a1.jpg?width=1280&format=pjpg&auto=webp&v=enabled&s=26e88d70e1b3c7f88d6833290defdd4e2b2224e6",https://www.reddit.com/r/Automate/comments/zifors/the_real_opportunity_in_ai_for_most_people_will/,auto
833,Give your opinions on robots in city life! (5 mins),"Form link here ->  [https://forms.gle/H2xWAknjyewLwCCp7](https://forms.gle/H2xWAknjyewLwCCp7)

We at the Spot team of YES!Delft Impact lab as part of Delft University of Technology in Europe are researching on and developing positive applications for robots and autonomous systems to be a part of our city life. Currently, we have a focus on exploring how drones and robots (Specifically SPOT the robot dog from Boston Dynamics) can support and help people in various day to day activities in public spaces within the city. Some of the uses we are exploring are fire evacuation, security while walking alone, urban mobility data analytics and many more. 

We would love to receive your opinions and feedback! Thank you very much in advance! ❤️

&#x200B;

https://preview.redd.it/tme8hcw1q25a1.jpg?width=1536&format=pjpg&auto=webp&v=enabled&s=d1fb367da76c138682de9ee3276a0a5e3fda78ef",https://www.reddit.com/r/Automate/comments/zhsruw/give_your_opinions_on_robots_in_city_life_5_mins/,auto
835,Auto Screenshot URL's,"Hi all!

I am looking for a solution to screenshot multiple URL's automatically, but something advanced enough that it can click elements and/or wait period of times in between each screenshot. I've had a look at the following but none have worked out so far:

* [http://stillio.com](http://stillio.com/)
* [http://blitapp.com](http://blitapp.com/)

If you have any suggestions, I'd love to hear them. Thank you all!",https://www.reddit.com/r/Automate/comments/zd8n89/auto_screenshot_urls/,auto
836,Looking for a way to automate renaming drawing files in SharePoint,"Hi people!

I've a long mundane task of renaming files in SharePoint (for 500 files). These files are PDFs and are drawing files issued for construction. The files needs to be prefixed with drawing number and revision number which are available inside the PDF. So, I have to open each file, copy the drawing number and the revision number from the PDF and prefix the file with these two inputs. I'm familiar with the concept of Power BI but I do not know how to use it in this case (I'm guessing Power BI is used for things like this usually?).  


Please suggest a way to automate this. I'd like to learn. Thank you",https://www.reddit.com/r/Automate/comments/zakxee/looking_for_a_way_to_automate_renaming_drawing/,auto
837,Help - Looking for recommendations for automating Outlook appointments into the billing system,"Hello All.

I am looking to automate the billing entry process I am using.

&#x200B;

Currently, I just create an outlook appointment from an email message, correct the start time and add the vendor category.

&#x200B;

End of the month, I manually copy the message body into the billing system, then alter the date, hours, distance travelled, type of work before submitting the entry.

This process is taking forever to complete, and it makes my hands hurt repeating the same movements.

&#x200B;

I have been thinking of ways to automate this process, and looked into airtable, exporting the appointments to csv and trying macros. None of these really worked. 

&#x200B;

I know the billing system is written in PHP, however I do not manage or admin the billing environment and have no way to modify code.

&#x200B;

Anyone know of a good place to start with this type of automation?

&#x200B;

Thank you for any assistance with this.",https://www.reddit.com/r/Automate/comments/z9vv4r/help_looking_for_recommendations_for_automating/,auto
838,Repository of scripts,"Are there any repositories of scripts or bots that people can just plug & play? 

For example, there's a great repository of PowerShell scripts for investigating parts of Windows that are commonly compromised during cybersecurity incidents here: [https://github.com/WiredPulse/PoSh-R2](https://github.com/WiredPulse/PoSh-R2)

Is there something similar for, say, boosting productivity in Excel? Or perhaps working with popular APIs? The possibilities are quite broad but it's surprising there aren't common automation scripts around.",https://www.reddit.com/r/Automate/comments/z97pmh/repository_of_scripts/,auto
839,Using GPT-3 to reply to automate email replies,"Hello folks,

It blows my mind that there are so many applications of GPT-3. This post is a combination of information and a bit of promotion, so please bear with me.

You can use OpenAI APIs to reply to emails. Just give it the right prompt and it will generate a decent piece of text that you can quickly send out as an email.

You can get a free OpenAI account and use their ""playground"" to test this.

Prompt:

>Reply to the following email in a professional tone.  
\[Actual email\]

You can replace the ""professional"" with different modes.

This worked well. Obviously, I had to make some adjustments specific to my app. 

Here is the final demo of the feature inside my app.

https://reddit.com/link/z7sofx/video/b4owfjl6vv2a1/player

If you find this interesting, I've built many more such tiny automations in my app. You can try it at the following link -  [Elephas](https://elephas.app/?utm_campaign=r-openai-email-reply-rautomate) 

Do share your feedback.

Thanks",https://www.reddit.com/r/Automate/comments/z7sofx/using_gpt3_to_reply_to_automate_email_replies/,auto
840,Using AI-powered automation to automatically classify interested prospects in your inbox,"Hi there!

At [Levity](https://levity.ai/) we help people and businesses save time usually spent on repetitive tasks and invest it where it's needed most.

We cover a lot of use cases, all of them working around unstructured data - take a look[here](https://levity.ai/use-cases) at all the processes you can automate!

One of our main use cases is email automation - we help marketing teams identify spam or interested prospects and label them in their inbox.

One of the companies we help is [Incendium Strategies](https://www.incendiumstrategies.com/)\- in their particular case, our platform automatically tags all incoming emails and classifies them according to whether it is coming from an interested prospect or not. Take a look [here](https://levity.ai/success-stories/outbound-email-automation) to see exactly how they made this happen by implementing Levity into their processes!

I'm happy to have a chat or answer any questions you may have. You can also book a one-on-one demo on our homepage if you'd like to see how Levity could work for your particular use case!",https://www.reddit.com/r/Automate/comments/z7zpwi/using_aipowered_automation_to_automatically/,auto
841,How can I automate a task on a website that lacks an API?,"My mother owns a clinic and uses [Fusion Web Clinic](https://fusionwebclinic.com/insights/), and I'd like to automate the generation of reports on the number of cancellations. But there is no API, how can I around this?",https://www.reddit.com/r/Automate/comments/z7btbv/how_can_i_automate_a_task_on_a_website_that_lacks/,auto
842,Zapier vs. AT Automations vs. Make (take the quiz),"I made a free quiz to help you decide between the three platforms: [https://www.fillout.com/blog/zapier-vs-make-vs-airtable-automations](https://www.fillout.com/blog/zapier-vs-make-vs-airtable-automations)

Are there any other platforms I should be considering?",https://www.reddit.com/r/Automate/comments/z705tc/zapier_vs_at_automations_vs_make_take_the_quiz/,auto
844,"Anyone have any tips for automating WordPress (development, design, connecting api databases, UX, updating content, etc.)","Any wordpress developers here have any suggestions for automating the development process of wordpress? Specifically for developing headless sites and connecting to external api databases? 

There are a bounch of new wp plugins to automate these processes with little coding, but there's just too many options to choose from. Anyone have any experience with any of these plugins? 

Thought I would ask if there are any experienced users here I before spend a bunch of time learning and testing a bunch of different plugins one-by-one.

Any suggestions are appecreciated, thanks in advance.",https://www.reddit.com/r/Automate/comments/z4mki0/anyone_have_any_tips_for_automating_wordpress/,auto
845,"How to utilize Slack, Trello, Figma & Zapier for my company?","I am hired as a automation expert to a company of to streamline their communication. The company has 50 employees and 6 teams.

We have got Slack, Trello, Figma & Zapier for all the works.

If you have worked with these softwares, tell me how to smoothen the communication between team.

You may say: Make separate trello boards for teams.
Ans: We did, but, still we have communication problems.

Question for you, how do I automate figma and trello?

Please Dump all the toughts and all the resources you have.",https://www.reddit.com/r/Automate/comments/z3tnl5/how_to_utilize_slack_trello_figma_zapier_for_my/,auto
846,Best Live Dashboard Solution for KPIs???,"Hello Everyone, 

I'm running a B2B agency and we're currently going through rounds of automation. One of the things that I'd like to implement is a dashboard that contains the following: 

\+ Financial data (MRR, growth, etc.) - coming from Quickbooks

\+ Customer success data (# of new customers, total number of clients, retention rate, etc.) - coming from Hubspot and Google Sheet.

I was wondering if there's a service that allows to integrate all these data sources and beautifully visualize them as live dashboard that can be accessed through the phone, laptop, and maybe also displayed onto a TV. And it'd allow for user management (certain people within the company can see certain information). 

Thanks in advance",https://www.reddit.com/r/Automate/comments/z301yf/best_live_dashboard_solution_for_kpis/,auto
847,Email Automation,"Emails are the primary form of communication used by businesses worldwide and are a crucial part of any business organization. Using email automation, you can automate your repetitive email tasks, including opening emails, reading and processing them, sorting them, extracting data from them, working on them, and responding to them accordingly. ElectroNeek offers integrated email activities that are ready to be used for automation across various platforms, including Microsoft Outlook, Microsoft 365, Google, Yahoo, iCloud, Yandex, and others. Deep dive into how you can automate your email activities here: [https://forum.electroneek.com/t/email-automation/1102](https://forum.electroneek.com/t/email-automation/1102)

Be part of our community, register here: [https://forum.electroneek.com](https://forum.electroneek.com/)

&#x200B;

https://preview.redd.it/joo6mgqwfj1a1.png?width=1620&format=png&auto=webp&v=enabled&s=f86de5a37192f999f938c94b9f02f89373e3b2f9",https://www.reddit.com/r/Automate/comments/z1zjpc/email_automation/,auto
848,Scan Vinyl barcode → Play on Spotify ?,"Hello automation friends.

I'm searching for a way to simplify the use of my vinyl collection for my significant other.  
A way could be to scan the barcode of a vinyl, and get redirected to the spotify page of the album.  
Is there any of you that use a third party apps, script or iOS shortcut that could do this trick?  
We mainly use Apple environnement and Sonos in the house.

Thanks a lot :)",https://www.reddit.com/r/Automate/comments/z199lj/scan_vinyl_barcode_play_on_spotify/,auto
849,GUI automation tool,"Hey all!

I am looking for a framework or tool, which can automate GUI software on Windows. A client of mine uses an application which poorly does not have any API. The only way of automating would be inserting directly to the DB. The vendor of the software locks th DB however and acces to it is just possible after a paid consultancy by the vendor.

So I am looking for a framework for automating GUI actions. It is just about inserting data into a basic form. Which tools should I look for?",https://www.reddit.com/r/Automate/comments/z0l2mf/gui_automation_tool/,auto
850,Ideas for intake process,"Hi everyone, I am setting up an automation department for my company, and wanted to get your thoughts on how everyone here has used intake process. what worked and why did you like it or didn't work, 

I've personally had experience with an intake from on MS Access, Power Apps, MS Excel and Jira as well.   


i want somehting super duper simple for the business, so simple that even a pre schooler can do something. 

&#x200B;

also for the intake, what data is important to have in your opinion?

for me i always want hourly savings and the SME as well as data on region and team etc

&#x200B;

thank you guys",https://www.reddit.com/r/Automate/comments/yxfbv8/ideas_for_intake_process/,auto
851,Modl.ai raises $8.4M to develop AI-driven play testing and QA bots,"We are thrilled to announce that we have raised $8.4 million to redefine the game development process. Our goal is to make game development more efficient and enjoyable by automating processes like exploratory testing, quality assurance testing—and many others.

And we can’t do this alone! Thank you for being so supportive. We’re eager to get the product into your hands, and this funding will help us get there faster.

[https://venturebeat.com/games/modl-ai-seriesa-ai-bot-qa-testing-griffin-gaming-microsoft-m12/](https://venturebeat.com/games/modl-ai-seriesa-ai-bot-qa-testing-griffin-gaming-microsoft-m12/)",https://www.reddit.com/r/Automate/comments/ywvfqm/modlai_raises_84m_to_develop_aidriven_play/,auto
852,From reddit to insta stories,"Thank you for reading this post. 
Is there a tool that allows me to take a certain type of post from a certain subreddit and automatically create a story on Instagram, posting it automatically?",https://www.reddit.com/r/Automate/comments/ywk2rn/from_reddit_to_insta_stories/,auto
853,Is RPA the right solution?,"Hello Automate experts,

I am interested in automating lead creation using LinkedIn Sales Navigator. There are different pieces to the process but the specific part to this that I am trying to figure out is entering a company name and job title into sales navigator and then taking those results and entering those names into a spreadsheet. So the formula I envision is a spreadsheet with company names and then the same job titles for all companies. RPA takes that data and one by one conducts searches in LinkedIn and the takes those results and copies and pastes them in the spreadsheet. Is there a better way to do this? Is this possible?",https://www.reddit.com/r/Automate/comments/yvht8a/is_rpa_the_right_solution/,auto
855,PDF Form Data to MS Access,"Hello all!

I have a tedious task that I am hoping that the fine people of Reddit can help me automate or, at the very least, help me implement a more efficient process.

The existing process is as follows:

1. Employees from Company A fill out a PDF form after every client interaction
2. Company A employees save the form in a specific folder for Company A Data Entry Person
3. Company A Data Entry Person prints all forms from folder
4. Company A Data Entry Person enters all fields into an MS Access form controlled by Company B
5. Company A Data Entry Person writes the Unique ID assigned to the MS Access entry in the upper corner of the PDF form paper copy.
6. Company A Data Entry Person files a paper copy of the PDF form in a specific filing cabinet.
7. Company B provides a report to Company A that summarizes Company A's monthly entries into the MS Access DB.
8. Company A Data Entry Person compares Company B report with Company A numbers (gathered by hand counting specific fields on the paper copy of the PDF forms).
9. Company A Data Entry Person corrects or disputes any discrepancies and informs Company B Data Entry Person that they can rerun the number and provide a new report.

Some variables to consider are as follows:

* Company A does not have access to anything in the MS Access DB except the MS Access forms. This is a contracted job that pays based on what is input into MS Access DB.
* PDF forms get assigned a unique ID when submitted into MS Access. The ID gets written on a paper copy of the PDF form to associate what is input in MS Access. It is how Company A disputes discrepancies between Company A's and Company B's data. Ideally, the data matches between the two. However, they never do.

I will add that the PDF form originated as a Word document supplied by Company B. I am open to converting back to the Word version if it helps. I couldn't see that being the appropriate file type for this application.

I believe that is the most relevant information that I can provide but will provide more if needed. This process involves thousands of PDF forms, and entering them one by one into the MS Access DB is killing Company A's productivity. There has to be a better way. Please help r/Automate!!!",https://www.reddit.com/r/Automate/comments/ysjd70/pdf_form_data_to_ms_access/,auto
857,Indoor positioning system basics for managers and non-technical people,"[https://www.youtube.com/watch?v=ELw2fd3nsc4](https://www.youtube.com/watch?v=ELw2fd3nsc4) \- video explanations

[https://marvelmind.com/pics/indoor\_positioning\_system\_basics.pdf](https://marvelmind.com/pics/indoor_positioning_system_basics.pdf) \- presentation

https://preview.redd.it/a31kmjqafwy91.png?width=960&format=png&auto=webp&v=enabled&s=4667072a116b462a4b9395a82cecfb37526b0387",https://www.reddit.com/r/Automate/comments/yqebew/indoor_positioning_system_basics_for_managers_and/,auto
858,Power Automate vs. Python,"Hi Everyone!

I'm working on my MS in Data Science and would consider myself proficient in Python. I just started a new job and I'm sitting through some very long training sessions on Power Automate, which I've never used or heard of before. 

From what I've seen thus far, it seems like Power Automate is just a bad way of writing code and automating things. Seems like it would be easier to just write a script and use Task Scheduler to run it to check for triggers. 

Am I missing something here?

Thanks!",https://www.reddit.com/r/Automate/comments/ypvyh0/power_automate_vs_python/,auto
860,Automate form filling,"How can I fill a website form with information of last year same form downloaded in pdf 

Let me explain. I want to automate to pass same information from last year from the exact text  to the new form in a website. Both form are the same just that 1 is downloaded and the other is online",https://www.reddit.com/r/Automate/comments/yo1502/automate_form_filling/,auto
863,PowerBI desktop automation,"Hello, I'm looking to automate the whole PowerBI process, from updating the data to publishing the new data on the PowerBI web service.

My PowerBI report is fed via a MariaDB database. The connection is established correctly. 

When the schema or the structure of the database changes, I would like the PowerBI report to update automatically without any manual action on my part.

I already have a working solution based on pywinauto. The ""problem"" is that PowerBI is forced to run. At that moment, the user has to stop what he was doing.

Of course, I can automate this task at night when there is no activity on the server, but I wanted to get your opinion about a possible alternative solution to automate this process without relying on GUI",https://www.reddit.com/r/Automate/comments/ylwdrh/powerbi_desktop_automation/,auto
864,The Role of RPA in Digital Transformation,"RPA can help development teams and business users cope with changing digital transformation and internal process modifications. It's feasible that RPA will impact practically every business and function, from data security to low-code application development and deployment. 

There are many RPA Tools are available, but top 5 tools are below:

Automation Anywhere  

IBM Robotic Process Automation 

UiPath  

Blue Prism  

Rocketbot 

What do you think- What is the [Role of RPA in Digital Transformation](https://www.zenesys.com/blog/the-role-of-rpa-in-digital-transformation)??",https://www.reddit.com/r/Automate/comments/ylu52q/the_role_of_rpa_in_digital_transformation/,auto
865,Jotform to Contract to Docusign help,"So we have a jotform we fill out internally with info as a client signs up with us, when submitted certain fields from that form (name, rate, start date) immediately populate a jotform PDF contract filling in the necessary fields, then that PDF is emailed to the client for signature.

The issue is there's no digital signature box available on that PDF and Jotform's new ""jotform sign"" feature doesn't integrate with that, we would have to fill out a separate contract and send it.

Ideally we want some of the fields from the form that is filled out to auto populate within a text contract that has a docusign type digital signing box that is auto sent to the client, any idea how to achieve this?",https://www.reddit.com/r/Automate/comments/yieq4q/jotform_to_contract_to_docusign_help/,auto
866,"Precise indoor geofencing solution for industrial applications (people, forklifts, robots, AGVs, cranes, drones)","[https://marvelmind.com/download/geofencing/](https://marvelmind.com/download/geofencing/)

* ±2cm accuracy
* Static and mobile geofencing zones
* 2D and 3D geofencing
* Different connectivity options
* Time trigger and distance trigger for geofencing violation

https://preview.redd.it/5ye6lkdg53x91.jpg?width=1280&format=pjpg&auto=webp&v=enabled&s=af726eb62ae87e3be0cc6e17e65ffe1cd7afb2e5",https://www.reddit.com/r/Automate/comments/yi2y1j/precise_indoor_geofencing_solution_for_industrial/,auto
867,Easy Invoice Manager - A Software to Automate Invoice Processing,"Manual invoice processing has many drawbacks which is why a lot of business are opting to upgrade towards automated invoice processing. [Easy Invoice Manager](https://easyinvoicemanager.com/) is a Software which enables businesses to fully automate their invoice processing without the need to having professional accountants. Following are some of the advantages you will get by using this amazing software: 

&#x200B;

* Data capture during Invoice upload, and no typing necessary
* Cloud storage of invoices
* Coding invoices for accounting
* Tracking current and future pay dates
* Manage cash flow with real time bank balance
* Manage vendor balances
* Get approvals on the invoice payments
* Get Accounts Payable aging reports
* Get paid invoice reports
* No accounting education required",https://www.reddit.com/r/Automate/comments/yfzh6v/easy_invoice_manager_a_software_to_automate/,auto
868,Does anyone have experience with ambi ROBOTICS?,"I imagine some of you work with, in some capacity, a robotics integration company. Have you worked with or started any conversations with ambirobotics? I am very interested in your experiences with them. 

In addition, if anyone would like to discuss other robotics integration companies, I'm eager to have a conversation about your experiences and the types of applications you might be working on. Specifically, I'm interested in end-of-line solutions for material handling and P&P systems for product distribution.

Automate! So future generations can be lazy! :)",https://www.reddit.com/r/Automate/comments/yfz795/does_anyone_have_experience_with_ambi_robotics/,auto
869,Text to JPG," 

# Text to Formatted Jpg

I have hundreds of text files that I need to use to make videos overlaid by audio. Essentially what I need to do is take a text file and convert it to an image like a jpg, and then repeat. Between the text file and jpg file, I need some formatting done, so it no longer looks like a text file.

so basically I need to repeat the following process a couple thousand times.

1. Text file
2. Format text file
3. convert to jpg

any help or suggestions much appreciated

example -

This is my text file currently -

📷

[text file](https://preview.redd.it/n89z4qdvx8w91.png?width=1867&format=png&auto=webp&v=enabled&s=b17cc5d66af19264db6b257fcc3ef9c7c15616d2)

current jpg -

&#x200B;

[current JPG](https://preview.redd.it/40ceigpwx8w91.png?width=567&format=png&auto=webp&v=enabled&s=d33a0e5ab4fdd9905c534c1049439d7fbaa633bb)

&#x200B;

What I would like as a jpg

&#x200B;

[something like this](https://preview.redd.it/gw7jilgyx8w91.png?width=190&format=png&auto=webp&v=enabled&s=b16db9f2217de3b8bc765461423c1b260cadba67)",https://www.reddit.com/r/Automate/comments/yee4v0/text_to_jpg/,auto
870,Text to Formatted Jpg,"I have hundreds of text files that I need to use to make videos overlaid by audio. Essentially what I need to do is take a text file and convert it to an image like a jpg, and then repeat. Between the text file and jpg file, I need some formatting done, so it no longer looks like a text file.

so basically I need to repeat the following process a couple thousand times.

&#x200B;

1. Text file
2. Format text file
3. convert to jpg

any help or suggestions much appreciated

example - 

&#x200B;

This is my text file currently - 

&#x200B;

&#x200B;

[Text file](https://preview.redd.it/at3bczgwf6w91.png?width=1867&format=png&auto=webp&v=enabled&s=7db628607954cb217811a5012a1227552350190b)

current jpg - 

&#x200B;

[current jpg](https://preview.redd.it/ueqy4gcyf6w91.png?width=567&format=png&auto=webp&v=enabled&s=b6720c8c991ba3657b099bce53c81831fe6368dc)

What I would like as a jpg 

&#x200B;

[something like this](https://preview.redd.it/56k914d1g6w91.png?width=190&format=png&auto=webp&v=enabled&s=fa388560e6b3d5a5cf585b0441aa7e92e0661269)

&#x200B;

edit: showed examples a fixed a couple of times to a couple thousand times",https://www.reddit.com/r/Automate/comments/ydh06l/text_to_formatted_jpg/,auto
871,Automating multiple emails on the basis of google form response?,"The club is looking for a simplified Library system and they just want to put in a QR code that directs us to a google form where people borrowing the book can fill their details(email/student ID/etc). But we have an automation requirement, which is to send an email to the individual once the return date is close. Any suggestions on this? If not related to this, any idea how these folks can manage their library? They do not have a lot of tech experience but clearly honor system isn't effective enough lol",https://www.reddit.com/r/Automate/comments/yckij8/automating_multiple_emails_on_the_basis_of_google/,auto
873,No Code Workflow Solution That Supports Snowflake/Jira/Email Sending?,"Hey everyone, 

Not sure if this is the right place to ask this. I'm looking for a No Code/Low Code solutions similar to Power Automate so that all employees at my company can quickly and easily build workflows that span the multiple products that we use. Ideally the solution should have built in integrations/connectors for:

* Snowflake
* Jira Software
* Gmail (Google Workspace)

&#x200B;

After the first initial setup time cost, each additional employee should be able to setup their automation without code changes and minimal integration effort. Any suggestions for solutions that solve these requirements?",https://www.reddit.com/r/Automate/comments/y9druu/no_code_workflow_solution_that_supports/,auto
874,One Click Automation(OCA). Enter your automation scenario to immediately receive an automation script.,Is it possible to develop a service where you submit your automation concept and receive a python automation script when you want to apply an automation scenario?,https://www.reddit.com/r/Automate/comments/y8wls0/one_click_automationoca_enter_your_automation/,auto
875,Automating searches on LinkedIn Sales Navigator,"Hello Automate community,

Is there a way to automate detailed searches on the Sales Navigator platform? From a spreadsheet I can feed into whatever system would be used, company names and/or domain names and then job titles. What I would need the system in question to do is find people with those specific job titles at the companies outlined on the sheet and then feed the results back to a spreadsheet. I think this can be done with one or more systems but I haven't identified all of the pieces yet so I was hoping to find some help. Thank you!",https://www.reddit.com/r/Automate/comments/y2k0sv/automating_searches_on_linkedin_sales_navigator/,auto
877,Algorithm Trading,Anybody knows how to automate taking positions and exiting in trading?,https://www.reddit.com/r/Automate/comments/y29ycj/algorithm_trading/,auto
879,Python Script for automated web page monitoring,"Hello automation community,

I am interested in finding a solution that could monitor thousands of websites and let me know on weekly basis when various technologies, plugins etc have been added or removed and send a report back. Basically looking for specific changes in the HTML. Is this possible at a large scale? (\~5k-10k sites)",https://www.reddit.com/r/Automate/comments/xzdv9q/python_script_for_automated_web_page_monitoring/,auto
880,Introducing the Basic Post-scarcity Map,"A society in which all basic human needs are provided at zero or very low cost without significant human work can be defined as basic post-scarcity.

Our civilization is getting close to the point where this is technically feasible.

To reach this milestone as soon as possible, we introduce here the Basic Post-scarcity Map project, an effort to map the current technological state of the art and to understand how far we are from a basic post-scarcity society. We are currently in alpha stage, and we are releasing early to gather feedback and collaborators.

This project is an attempt to provide unbiased answers to the following questions:

* What technological advancements are needed to reach basic post-scarcity?
* What is the state of the art, what resources are available to learn about it and who is currently working on improving it?
* How far are we from achieving basic post-scarcity and what are the bottlenecks?

To accomplish this, the basic idea is to build two maps: the first where we will deconstruct the basic needs needed to reach basic post-scarcity and the technical milestones needed to satisfy them with minimal human work at the minimum cost. The second in which we input and update the state of the art for each of the technical milestones.

References to the current state of the art and the resources needed to learn more about it are collected in separate pages forming a shared library.

This is an open source and collaborative project. All contributions are fact-based, with no projections, opinions, marketing or propaganda.

We believe that having a searchable and living assessment of the state of the art will enable people who want to work towards this goal to know what is needed, what is currently feasible and who is currently working on what.

We are aware of the many limitations of this approach and in particular we know that technical bottlenecks are not the only roadblocks to a basic post-scarcity civilization. However, we also think that it is not unreasonable to assume that reducing the cost and the manpower associated with fulfilling basic needs will make it easier for public and / or private actors to provide them as widely as possible.

Our goal is to make this project simple to contribute and update. At this stage we need help creating technical milestones. Domain experts are particularly welcome to shine light on the state of the art for the relevant milestones in their respective fields.Additionally, anyone with reference materials and / or knowledge of people currently involved in solving these problems can contribute by sharing their resources in the library.

The project is currently live at: [https://postscarcitymap.org/](https://postscarcitymap.org/)

You can make contributions directly to the library by editing the pages on our Gitlab repo: [https://gitlab.com/postscarcity/library/](https://gitlab.com/postscarcity/library/). At the moment the process of updating the tree is still pretty manual, but we plan to automate it more.

If you want to join us as part of the team and contribute regularly, we also have a Discord server: [https://discord.com/invite/vhc8EZkmEv](https://discord.com/invite/vhc8EZkmEv)

We welcome feedback and contributions of any size.",https://www.reddit.com/r/Automate/comments/xywvt4/introducing_the_basic_postscarcity_map/,auto
881,Automation Ideas in IT,"Hey everyone, i just wanted to ask for some ideas on what to automate in ur daily job as IT as HD,SD,Sysadmin ect.. What are some things that you have automated?",https://www.reddit.com/r/Automate/comments/xz1xm5/automation_ideas_in_it/,auto
882,Any tools to automatically follow confirmation links in your email?,"Hey 'maters! I was wondering if anyone already had a script/tool/app to automatically check your email(s) and follow the confirmation links in them sent from various sites or something similar? Even perhaps a guide to build my own? The email service is Gmail but it's used as a catch-all for all emails. If Gmail is an issue then I also have Office365 email hosting although I don't think that really matters since they both support SMTP/IMAP connections.  Any help is appreciated. Thank you,!",https://www.reddit.com/r/Automate/comments/xyxydu/any_tools_to_automatically_follow_confirmation/,auto
883,SMS auto-reply to a different number?,"Okay, I don't know if I am missing something or if it just doesn't exist. I have an ""unlimited"" data plan with my provider. When I use up 5GB in a day, I receive an SMS message stating that if I want to keep browsing at high speeds, I have to send a specific SMS to another number. Every 2GB afterwards the process repeats.

I did find some auto-reply apps on Google Play, but I haven't found one that can send a message to one number when receiving from another.

Is there a way to automate sending that message to keep using high speed internet or do I have to actively resend it every time I want to download something bigger during the span of a couple hours?",https://www.reddit.com/r/Automate/comments/xwj5yo/sms_autoreply_to_a_different_number/,auto
884,Bumblebee movement detection using MOC70T4 sensors,"Dear all,

&#x200B;

I am working on a research experiment where we teach bumblebees to pass trough a sensor to activate a sugar-water f feeder. The first prototype (picture attached) uses a sensor (left) that activates the feeder. The other sensor (right) records time feeding. We are using MOC70T4 sensors to detect the bumblebees movement responses, however, the first pilot shows that its not working as well we hoped. The bumblebees have to emit a very stereotyped response to activate the sensor, which often leads to repeated and not recorded responses.

&#x200B;

I would really appreciate suggestions of sensors that could be more appropriated for the task, or ways of setting up the sensors that facilitate the detection.

https://preview.redd.it/raohtt92nsq91.jpg?width=1079&format=pjpg&auto=webp&v=enabled&s=2e86c0f78b3f01bdfde41917a47f6d2a62dab875",https://www.reddit.com/r/Automate/comments/xr6vxo/bumblebee_movement_detection_using_moc70t4_sensors/,auto
885,Automate creating words file,"My job require creating new documents most of the time it’s same 
For each client we use around 5 documents with only 5-6 line different 
I now automate everything by using microcode but even with that it take around 5-7 minute 
however I want lvl up my automations and let it create all document 
For example entering the client information and it will create all 5 documents 
I used python (the paid library works but too expensive and free one usually change the format - size .. etc) 

So any idea how could I do this 
I can do  c,python,bash",https://www.reddit.com/r/Automate/comments/xr2yeb/automate_creating_words_file/,auto
886,Anyone able to help me identify the type of automation I need?,"I work for a tech consulting firm where automating internal tasks is highly valued. I really want to automate one of my tasks in work. 

We have a Master Resource List in an excel sheet for everyone in our team of 100 people. There are a number of fields to populate which make this a boring task. The fields include Name, employee number, manager etc. 

How can I automate this to only have to type in a name and then the script pulls the data from relevant sources? Would be very interested to use this as an opportunity to learn about automation. Any help would be greatly appreciate.",https://www.reddit.com/r/Automate/comments/xkz4eo/anyone_able_to_help_me_identify_the_type_of/,auto
887,Help: Automate count of items listed on a website.,"I was trying to figure out if there is a way to automate the count of a particular item listed on a website. For example: www.myeyedr.com/about/our-eye-doctors. This site provides a list of all doctors for a particular location. Currently, I am manually going through each location and counting the number of doctors provided. Is there an easier to way to automate this task?

I can cut down on some time using a count on inspect elements but that still requires me to load the entire list and doesn't work when the site redirects to another page.

Thanks for any inputs!",https://www.reddit.com/r/Automate/comments/xjw1ba/help_automate_count_of_items_listed_on_a_website/,auto
890,Looking to integrate data source with social media post,"I am already familiar with Zapier (and formerly automate.io). 

I already use the former for another project which monitors entries in a Google Sheets document, populated by Google Forms.

In this new case, I want to monitor output from an outside managed data source. 

##More specifically, my desired workflow is:
1. Retrieve recent EV Charging Stations added to AFDC Database
2. Parse data and filter by a single state, or a selection of ZIP Codes.
3. Post selected column fields to Social Media.

https://afdc.energy.gov/fuels/electricity_locations.html#/analyze

The data stored here can be retrieved via: CSV Download, HTML Embed, and Developer APIs.

The trick is, I don't know how to pull that data into a Zapier automation. 

Zapier does have a Webhook option, but it is a paid feature.

Is there a way of getting around this? My only other idea was to download the CSV on a schedule, and copypaste into a sheet on a regular basis. But that will require significantly more work on my end.

Any suggestions for how to accomplish this?",https://www.reddit.com/r/Automate/comments/xfzaoq/looking_to_integrate_data_source_with_social/,auto
891,Jira Automation - Release notes,"Hi, does anybody know how to Automate the creation of JIRA release notes without the plug in?",https://www.reddit.com/r/automation/comments/10f3m0e/jira_automation_release_notes/,auto
893,"Would anyone be willing to share details on how the UIPath pricing works? We are currently on Kofax RPA, I'd love to switch but I fear it will be much more expensive","Hey guys!

I'm the head of our department for Process Automation and one of the tools at our disposal is Kofax RPA. The business bought this in themselves (without telling IT) because a salesman sold it as fine for end users. Out of the 20 that trained, only 1 kept going and managed to DDOS the main system and so it got brought to IT's attention and came under their wing. I was then hired to work with it. I now work with this and several other systems to automate processes.

I love the ease of the software and you can spin up robots so easily. It's not cloud based, but we don't need to have any VMs because Kofax has a sort of emulated desktop where I can run excel, websites etc so we have 2 servers to run the bots as services and nothing else required. It's quick, it's easy.... but I HATE Kofax. 

There is no community, we've been asking for changes for 5 years and nothing happens. They don't offer cloud and our infrastructure if off-shored to TCS and it is always breaking down. The support is non existent. There is no training we had to learn by doing. When you ask their support or training teams, they come back going ""I don't know, we were not trained on this"". Upgrading the on prem software is week's of hard work and planning. The thing is great when it works but so flaky and not getting much investment.

I love the look of UI path but I fear it may end up waaaay more expensive. From what I've seen, you pay per month for the software but you also have to pay per month for each robot? We have 100+ processes running under Kofax and we are on their old licence - so we pay for the amoutn of ""resources"" we use at any given time. So we can run 100 robots just staggered across time so they're not all running at once. And some of the robots are big processes that can take up to a day but only go once a month, and other processes that take days but runs a couple of times a year. I can't pay per robot for a thing that rarely runs (but when it does, it enters 100s of thousands of rates into our system).

I'd love to hear from anybody willing to share a ballpark figure or even just confirm how it works for paying for robots and if I would be shelling out 1000s of dollars a year per process I automate. I will never get that past a business case!

Thanks!",https://www.reddit.com/r/automation/comments/10dd3sy/would_anyone_be_willing_to_share_details_on_how/,auto
894,Automating a farm: PLC Opta? or RPi and arduinos with Home Assistant?,"I'm trying to automating my farm and I have around 20 sensors that would control around 10 devices that should stabilise my environment. It seems like I'm biting off more than I can bite but I'm willing to put in the ground work.

Does anyone have experience automating a farm or something like that? Should I wait for the PLC Opta or use RPi and arduinos with Home Assistant?",https://www.reddit.com/r/automation/comments/10bkhns/automating_a_farm_plc_opta_or_rpi_and_arduinos/,auto
895,Looking for Python Automation Project Ideas,"Hello  folk, I've recently started looking into automating random side  projects I had lying around and ideas with Python. I'm having a blast,  but sadly my creativity is somewhat limited, hence this post. My  questions are:

\- what kind of project you have done which you are proud of?

\-  any Python libraries which are absolutely hidden gems? For me it has to  be Playwright, never heard of it before a friend mentioned it and I fell in  love with it

Thank you for your time and answers in advance, any input is highly appreciated. :)",https://www.reddit.com/r/automation/comments/10bjn1l/looking_for_python_automation_project_ideas/,auto
896,Requesting direction,"I have a process where I need to open \~4,800 individual links - each link will download an Excel file. I have developed processes that help me identify the files that I need to download, and my post-download processes are also confirmed. All of this ends up in a large analysis database that I use to help my company understand their hiccups in our billing process. None of these 4,800 or so files are all that large - about 15kb is the average size. Making it more complicated is that the files are behind a username/password combination. I have looked at PowerShell and for the moment, I have this batch download utility in Chrome where I can do about 500 at a time. But needless to say, this is a day-long job if I want to do this since I have to click the button to save 4800 times. I am not well versed in development languages (I know T-SQL far better) although I'm not afraid to Google anything and see if it works. The few web-based ""macro"" systems that I've tried have made it to about the 10th file and then the speed issues take over. So I am asking the community that if you had a static link with a particular file ID and you had 4800 of those links, what is a good method for automating the opening of the website, and downloading the file?",https://www.reddit.com/r/automation/comments/10azvcn/requesting_direction/,auto
897,How to automate a Google login flow using Selenium scripts?,"We are trying to automate a Google login flow in our Selenium script.  

When the test runs, Google treats this as suspicious/spam-bot activity, and asks for additional verification. So we get either a Captcha screen or a OTP code login screen. 

Has anyone solved this problem before?",https://www.reddit.com/r/automation/comments/10ak1sw/how_to_automate_a_google_login_flow_using/,auto
898,Automation consultant? Python ninja?,"Hi there. I run a small events website based in Las Vegas. I use some automation tools like Visualping and Magical to help me track and input events, but I'm hoping to add some more tools to my arsenal. 

I've toyed with learning some basic Python to help me scrape data, and messed with ChatGPT a little, but it's a bit of a learning curve that I don't have time for. I'll cop to some intellectual flabbiness, too, sure.

Ideally, what I'd like is to describe to an expert what I would like to automate and how I'd like to automate it, walk them through my process, and either have them point me to the right tools or even create the tools themselves. This is something I'm happy to pay for.

Does such a thing exist -- an automation consultant or Python or ChatGPT ninja who can build me a bespoke solution for my specific needs? Thanks.",https://www.reddit.com/r/automation/comments/109j9xn/automation_consultant_python_ninja/,auto
899,Is ChatGPT a trap for businesses?,"I've seen a lot of excitement from various businesses about using ChatGPT to let them reduce their staffing costs in fields like customer service. If you look at the numbers right now, it makes a lot of sense because the costs of ChatGPT are current really, really low. But the terms and conditions of ChatGPT make it very clear that those costs can change, and I wonder if businesses might be walking into something of a trap.   


There's a lot of money to be made in automation, I know, I am an industrial automation engineer. But I expect that OpenAI (the developers of ChatGPT) knew exactly how much money their software could save companies when they developed that software, which is why they did it, and they don't intend to let the companies pocket the lion's share of those savings that they did they work to realize.   


So right now, companies can be early adopters and help ChatGPT build market share, train their AI, integrate it with other software packages and demonstrate it's ability to displace workers in the real world. But companies that go down this route and going to make ChatGPT essential to their business operation. So when ChatGPT decides to 10x, 100x, 1000x their costs, those companies are going to have to pay it, and at that point they may have done a lot of work to make ChatGPT a viable alternative to a human employee only to pay OpenAI some non-trivial percentage of the pay of the human worker their displaced.",https://www.reddit.com/r/automation/comments/109c368/is_chatgpt_a_trap_for_businesses/,auto
900,Need help to automate a WhatsApp group chat data to word,"Hello, so at my work the sales team uses a WhatsApp group chat to communicate. The format is something like: order number, client name, picture of design selected and info about design (quantity for example), then the other product, followed by a dash line to indicate the order is over and then repeat same format for new order.   Now we have one guy who’s only task is to copy paste that data into word and print is as purchase order. I want to automate this process by directly somehow having people input the data into WhatsApp and it getting converted and pasted to word, so then end of the day all I have to do is to print all.   Any help is much appreciated.",https://www.reddit.com/r/automation/comments/109hw07/need_help_to_automate_a_whatsapp_group_chat_data/,auto
901,No-code Google Sheet automation,"Hi all, I just published an article on the no-code Google Sheet automation. [https://www.bardeen.ai/posts/google-sheet-automations](https://www.bardeen.ai/posts/google-sheet-automations)

It shares detail on how to automatically accomplish the following:

1. Copy LinkedIn company data to Google Sheets
2. Copy a list of meetings during a timeframe to a Google Sheet
3. Enrich TikTok links and save them to Google Sheets
4. Copy ClickUp tasks to Google Sheets
5. Get Google Search Results for a keyword and save them to Google Sheets",https://www.reddit.com/r/automation/comments/108s379/nocode_google_sheet_automation/,auto
902,Logical flow of moes brand zigbee push button?,"So I got my first zigbee hub and switches.

Could anyone help a dummy out here?

Here's the situ:

*   3 smart bulbs exist in OFFICE.

*  In Tuya Smart, I've setup the following:  
     IF office(123) OFF and PIR Motion Detected  
     THEN execute automation officeON

I created the inverse of the above so the bulbs turn off when detected as on.

I've also tapped to set ""When ALL conditions are met""

I understand why it does not work:  I've created a loop.  When I press the button, it's triggering both routines.  

The issue is I don't know how to set it up as Tuya wants me to do it.  This would be 1000x easier if the execution simply toggled the power state rather than having to make routines to check the power state. 

Another oddity is in another office, it works exactly as it should but that office only has 1 bulb.  The weird thing is I intentionally created a loop by setting PIR motion sensor to both ON and OFF state. 

Could anyone elaborate how ya'll got yours to work cause mines driving me insane?",https://www.reddit.com/r/automation/comments/108fdug/logical_flow_of_moes_brand_zigbee_push_button/,auto
904,How would I go about automating my MURAL tasks in my user story map from JIRA? (Project Management),"Hey guys, I find I'm manually spending quite some time manually updating my user story map every sprint. I'm thinking there must be a way to automate this, in the sense that when someone changes the status of their assigned task in JIRA, it would automatically update in MURAL?

I'm new to automation so any advice would be helpful and a fantastic learning experience. :)",https://www.reddit.com/r/automation/comments/107sgqi/how_would_i_go_about_automating_my_mural_tasks_in/,auto
907,Automate Word/Pages form on Mac,"Hi. I have a fairly straight forward form I have to fill in to kennel my dog on occasion. Is there anyway I can take data from a Numbers spreadsheet to automatically create the filled in form with dates etc included? I only have Mac but I do have Office Excel and Word if that's the only way, ideally looking for a Mac method using Shortcuts or Automator or something similar. Thank you",https://www.reddit.com/r/automation/comments/104uxmf/automate_wordpages_form_on_mac/,auto
908,Research to template automation help,"Hey everyone,

I'm doing a project that involves email outreach using templates that I fill in with information that's specific to the person I'm targeting. I'm wondering if there's a way to develop an app or if there's something out there that would allow me to feed it the information on a single person (if I could do multiple that would be even better) then it would provide me with the finished template with everything filled in. Currently, I'm doing everything with find and replace on Word however there has to be a faster way out there to do this. I tried my best to explain what I'm doing but I'll provide a photo of what exactly I mean with the custom email outreach below. Any thoughts or suggestions are greatly appreciated!

&#x200B;

https://preview.redd.it/vurdoataix9a1.png?width=1302&format=png&auto=webp&v=enabled&s=cfafb6558bd628bedb19e086a5f908d813289576",https://www.reddit.com/r/automation/comments/102pwz2/research_to_template_automation_help/,auto
909,Looking for feedback - Users subscribing but not understanding capabilities of product,"I built an application where users can click “record” then perform whatever actions in a browser like add data to CRM, send email, send message etc and then replace whatever value they typed in the recording with a dynamic value from google sheets. For example let’s say you want to send a bunch of users a message on Instagram and you have a list of usernames in a google sheet. You click record, go to Instagram, type in the username, type the message then click send. Then you edit this to use the value from google sheets instead of what you had actually typed.

Most of our users signed up to send automated instagram messages and have understood how that works pretty well but when wanting to automate actions on another site / in a different way it doesn't go too well.

Whatever you record can then be triggered by webhook, new row in google sheet, scheduled etc.

This is working great and we have a number of customers using it to send Instagram messages but they’re not seeing that the same idea can apply to LinkedIn, Facebook, openphone etc. without spending 10-30 minutes on a call they’re not understanding whatever action they record can be performed from a cloud service later with dynamic data so they don’t have to.

I’m a bit overly technical with explanations which is the fault for the majority of this but how would you better train users on the product without getting on 30 minute calls or building resource pages with videos that are just going unwatched

I understand a lot of these things can be done via api, with zapier etc but most users we have cannot do that and for some sites there is no api so there is no real alternative besides hiring a VA/being a developer to write a custom script/giving up your time

Thoughts?",https://www.reddit.com/r/automation/comments/zzfs0a/looking_for_feedback_users_subscribing_but_not/,auto
911,Email & Punch Card Automation ideas?,"I do a lot of interviews and emails for work, and recently HR said they want us to punch in, punch out when responding to an email. Is it a hassle? Yeah. Is it good they want us to get paid to work? Yeah. But I get the feeling it could be quasi-automated.

So the gist of it would be a tool that I can open, makes a logging stopwatch, scores it into a google sheet, which then tallies it into a total clocked hours and can be logged as a single item at the end of the month. It's super rare I'd work overtime in any given week, so not really worried about being stiffed a weekly bump.

This might exist in some form, but if not perhaps the parts of it already do.",https://www.reddit.com/r/automation/comments/zz5ymx/email_punch_card_automation_ideas/,auto
913,How do you use browser automation at work or in personal life?,"Hello all! I'm curious to know if anyone uses browser automation tools in their work or personal life. If so, could you share how you use these tools?  
I'm aware that they are commonly used for testing, data gathering, and marketing, but are there any other applications for these tools?",https://www.reddit.com/r/automation/comments/zygxp0/how_do_you_use_browser_automation_at_work_or_in/,auto
914,Low-Code Developer,"You love tech, building systems, and automating, to make it play together tighter than The London Philharmonic Orchestra.  
But there are two major problems.   
1. You are saying the same thing over and over again to customer. You just want to build cool stuff not be a customer relations manager.  
2. The lack of security. One week you have five projects the next week you have one. The feast is great but the famine feels like you have been placed in an Oliver Twist novel.

If you are looking for a new technical challenge and to be part of a fast-growing Marketing Agency (growing at a rate of over 300% per year) this could be the perfect fit for you.  


What we provide:  


1. Guaranteed 40 hours every week
2. Fully remote working (work from anywhere)
3. Competitive salary
4. 28 days of paid time off per year 
5. Regular working hours 
6. Supportive personal growth environment
7. We handle customer communication so you can do what you do best
8. Smart QA processes to make sure you have all the information you need and reduce unnecessary noise.
9. One clear point of communication so you don’t have to worry about dealing with multiple stakeholders
10. A team that lives by the values of effectiveness, data and accountability
11. Clear and documented SOPs that are constantly evolving to support the team in being as effective as possible.
12. Support and training on new tools and software to give you.
13. Weekly review to discuss anything you need help and support with to be the best version of yourself in your role.
14. Career development opportunities as the company grows. We believe in growing talent from within.
15. A clear vision that you can be part of that supports both financial goals and philanthropic goals to impact more lives in the world
16. $250 a year personal development scholarship that can be used for any kind of training you wish. Whether that be personal growth, learning a new skill, going to a mastermind or an industry event. It’s totally your choice. We support you.
17. A clear project management software system so you always know what you should be working on and what is a priority each. No more guessing games.
18. A fast-paced environment that is exciting to be part of.
19. To push the boundaries of what is possible with new technologies  
**What you will be doing**

As part of Amplify, you will be helping us complete strategic goals. To add clarity, you will either lead an activity, help manage an activity, and/or be accountable for the outcome of the activity.

**Direct Supervisor:** CEO

**Functional Supervisor:** Project Manager

## LMA Tasks

**Key:**  
L - Lead

M - Manage

A - Accountable  
**Tasks:**

1. LMA - Determining project requirements and developing work schedules for the team
2. MA - Delegating tasks and achieving daily, weekly, and monthly goals
3. M - Liaising with team members, management, and clients to ensure projects are completed to standard
4. A - Identifying risks and forming contingency plans
5. MA - Analyzing existing operations and scheduling training sessions and meetings to discuss improvements
6. MA - Keeping up-to-date with industry trends and developments
7. A - Updating work schedules and performing troubleshooting
8. M - Being transparent with the team about challenges, failures, and successes
9. LM - Writing progress reports and delivering presentations to the relevant stakeholders
10. LM - Create Go High Level automations and templates
11. LM - Create Active Campaign automations and templates

## Salary

1. $8000 per month based on 40 hours per week   
comment on this post to get the assessment link",https://www.reddit.com/r/automation/comments/zybroz/lowcode_developer/,auto
915,Help Automating a Word Document,"Hello! So at my current workplace, every time we get a new patient, we manually fill out multiple word documents. I have managed to combine all these word documents into one word document, however, there are too many fields for us to manually fill out and most of the fields are the same thing repeated. This document includes things like the patient's name, DOB, and etc. Is there anyway where I can make it so that whenever we get a new patient, I can just type the patient's name and DOB and it automatically populates the fields in the document? I know I can do this with Microsoft forms but the document is setup weird and has a table in the footer and whenever I submit the form, it wipes out everything in the footer and only leaves the filled in text. Does anyone have any ideas on how I could possibly automate this? Thanks!",https://www.reddit.com/r/automation/comments/zwvdul/help_automating_a_word_document/,auto
916,Auto play a YouTube video/livestream on a Google TV,"I have one of the new Chromecasts with Google TV and was wondering if I could make it always stream a specific YouTube stream as a cable TV replacement for my grandma (she always only watches one specific channel). Does anyone have suggestions on how to either force an Android TV (Google TV is based on android) to play 24/7 while still allowing the TV to turn off, or make it auto-load the video when the TV turns on? Any help would be much appreciated.",https://www.reddit.com/r/automation/comments/zx1951/auto_play_a_youtube_videolivestream_on_a_google_tv/,auto
918,How to automate creating technical evaluation sheets from pdf quotations,"Hi, I was wondering how to accomplish automating to create a technical evaluation sheet by extracting data from quotations in pdf files that are of different formats from different suppliers. Would it be possible ? If not should I require suppliers to fill a uniform pdf format?",https://www.reddit.com/r/automation/comments/ztb6u6/how_to_automate_creating_technical_evaluation/,auto
919,Automation Testing Market Witness the Growth of $52.7 billion by 2027,"Report determine and forecast the automation testing market based on component, testing type, dynamic testing, non-functional testing, endpoint testing, organization size, vertical, service and region from 2016 to 2027, and analyze various macro and microeconomic factors that affect the market growth.",https://www.reddit.com/r/automation/comments/zt9ooj/automation_testing_market_witness_the_growth_of/,auto
920,[BLOG] Use PowerApps for an automation Front-End,"To celebrate the [festive tech calendar](https://festivetechcalendar.com/) I've written a blog about how to use PowerApps to create a simple front end to execute Automation scripts hosted in Azure. I thought some people might be interested in it so here you can find it:

[https://autosysops.com/blog/use-powerapps-to-create-a-simple-front-end-for-automation](https://autosysops.com/blog/use-powerapps-to-create-a-simple-front-end-for-automation)",https://www.reddit.com/r/automation/comments/zspzt0/blog_use_powerapps_for_an_automation_frontend/,auto
921,Text expansion app for Android - allows SQL lite files,"Hi everyone!

I wasn't quite sure what subreddit to ask about this in so please let me know if I'm not in the right place. :)

I'm currently using a piece of freeware for text expansion and simpler macros on my PC that I'm really happy with. However, I'm unable to find an app for my Android phone that's compatible with it. The software doesn't have an official mobile app so I'm looking for any app for text automation that I can use to expand text (duh) while typing on my phone.

However, I'd like to be able to export the SQL lite database from my PC and import it into the Android app so that I have all the snippets ready to go. (I'm not expecting the macros to work or any of the shortcuts that include keyboard-specific keys like Ctrl or Alt).

I realize this won't be an automatic synch, much less a two-way synch, but it's good enough for now. :)

Looking forward to your advice Reddit community! :D I'm sure you'll provide words of wisdom as always!",https://www.reddit.com/r/automation/comments/zrdx60/text_expansion_app_for_android_allows_sql_lite/,auto
922,automated DJ software,"I made software using JavaScript that has about 300 songs and the software mixes the tempo matched instrumentals in key, each refresh creates a new endless mix and its powered by the super accurate Web Audio API hardware clock, even works on iOS.

The dopest part is that it is super flawless accurate as far as key matching and tempo matching goes and its all written in vanilla JavaScript. It even does some things DJs cant currently do, like considering the accurate tempo/pitch shifted key (like when a DJ speeds up a song) instead of the one the song is recorded at. As much fun as DJing has been to me for over 20 years, this is even more fun because it comes up with combinations I love and have never used before but they sound great together.

EDIT: [https://cappinkirk.com](https://cappinkirk.com) to listen live and [https://github.com/dmvjs/kwyjibo](https://github.com/dmvjs/kwyjibo) for the source code (doesnt include audio files yet for obvious reasons). This project is originally a hardware idea for selling to clubs but it amazingly it also works well on the web. I make mixtapes with it and export to sounddcloud also

EDIT #2: Happy Cake Day r/DJs!

EDIT #3: i posted a youtube video unedited with details [https://www.youtube.com/watch?v=61mAf\_8swEE](https://www.youtube.com/watch?v=61mAf_8swEE)

EDIT #4:

&#x200B;

[Songs are grouped by tempo and key](https://preview.redd.it/sqyvfp7tjt2a1.png?width=1367&format=png&auto=webp&v=enabled&s=ead23a47dedea9b41b464ce0ab67b4de7564f70f)

&#x200B;

[Adjacent keys are eligible for selection](https://preview.redd.it/uy79eudxjt2a1.png?width=1367&format=png&auto=webp&v=enabled&s=1011dc28b07ec99a7c1c7b089c41de1e4db3d34d)

&#x200B;

[lead files are 16 beats, body files are 64 beats](https://preview.redd.it/wl1qhv31kt2a1.png?width=1366&format=png&auto=webp&v=enabled&s=64afedab344cfe212d4a7384b804a4a86201fbb4)

EDIT 5: kwyjibo got featured in [https://bytes.dev](https://bytes.dev) JavaScript email thanks! [https://bytes.dev/archives/145](https://bytes.dev/archives/145)",https://www.reddit.com/r/automation/comments/zq7b83/automated_dj_software/,auto
923,Find Retailer Map on Website,"Is there a way to automate adding in new retailers onto a retailer map on a website? We get form submissions from new retailer accounts, (we have many retailers), and manually entering them into the website takes 2-5 mins each, and we have 267 to input. Could Zapier or something similar help with this? Thank you",https://www.reddit.com/r/automation/comments/zq06ou/find_retailer_map_on_website/,auto
924,Trying to figure out automatic switching for 220v machines.,So I've bought myself a new wood lathe which is 220v. My air compressor is also 220v. The problem is that I have only one outlet. I've been trying to figure out how I can automatically prevent the compressor from starting while the lathe is running. Any ideas on how I could accomplish this?,https://www.reddit.com/r/automation/comments/zo8x9o/trying_to_figure_out_automatic_switching_for_220v/,auto
926,How do i get an automation to go down to the next customer in my invoice register,Hi.  I was tasked to automate some processes at work and was looking for some help.  I have been asked to automate our billing invoices to automatically be assigned in our billing invoice master file.  I have been able to get the automation to work for one entire customer but cant get it to jump to the next invoice.   I was told i need to use some kind of loop but don't really know what kind to use.  Thanks,https://www.reddit.com/r/automation/comments/zkcmfh/how_do_i_get_an_automation_to_go_down_to_the_next/,auto
928,Requesting advice to begin automation learning,"Looking for a learning path and resources where I can learn automation.
A suggestion on route map of this learning path by self

Thanks and appreciation in advance for your guidance.",https://www.reddit.com/r/automation/comments/zjqwhw/requesting_advice_to_begin_automation_learning/,auto
929,Forward emails and more with a click of the mouse.,"At work a few people have to do this repetitive function:

* Some emails, but not all
* Need to be forward to a certain email address, always the same email address
* And also BCC to another  email address, always the same email address. BCC is important
* Add a certain message in the body of the email, first line is fine, always the same message
* we use Google Workspace


I am new to automation, but this sounds like a great first project.

What would be best to use? Google Apps Script, Zapier or some other tool?

Thanks in Advance.",https://www.reddit.com/r/automation/comments/zh6p21/forward_emails_and_more_with_a_click_of_the_mouse/,auto
930,Mitsubishi FX5U connection with Monitouch TS1070s,"Hi everyone, I am having a hard time trying to get a connection between this two devices. I am already lost on what went wrong. Please help me\~\~\~I've attached both my devices setting pics. My HMI keeps prompting PLC1 Communication Error Time-Out.

[V8](https://preview.redd.it/jxddi3sym14a1.jpg?width=544&format=pjpg&auto=webp&v=enabled&s=c815d6e53fcb957507876920e636331c0bcb4c12)

[GXworks3](https://preview.redd.it/xsrpvvtxm14a1.jpg?width=1517&format=pjpg&auto=webp&v=enabled&s=639122250ebfb1be722a5380ee09dbd19b96f940)",https://www.reddit.com/r/automation/comments/zd1olw/mitsubishi_fx5u_connection_with_monitouch_ts1070s/,auto
932,How to automate a specific song to wake me up the 1st of every month?,"Siri or Alexa, what automation would allow me to wake up to a specific song at 8:00am on the first of every month?",https://www.reddit.com/r/automation/comments/zaj50h/how_to_automate_a_specific_song_to_wake_me_up_the/,auto
933,How to Automate WhatsApp Message for free using Power Automate,[https://medium.com/@vanshkharidia7/how-to-automate-bulk-whatsapp-messaging-without-coding-30fe97f9832c](https://medium.com/@vanshkharidia7/how-to-automate-bulk-whatsapp-messaging-without-coding-30fe97f9832c),https://www.reddit.com/r/automation/comments/zad6pd/how_to_automate_whatsapp_message_for_free_using/,auto
934,Need tips,Hey guys! What automation tips would you give to a newbie?,https://www.reddit.com/r/automation/comments/z7wbk9/need_tips/,auto
935,Hey guys i'm very new to this topic!,Hey guys i'm very new to this topic can i ask someone a few questions about automation? Thanks,https://www.reddit.com/r/automation/comments/z7c03t/hey_guys_im_very_new_to_this_topic/,auto
936,VFD AutoRestart,"Good day all!
 
I am wondering if it is common practice in industry to use the auto restart parameter on VFDs?

TIA",https://www.reddit.com/r/automation/comments/z4btk9/vfd_autorestart/,auto
937,With what kind of probelms can neural net help in signal processing automation?,"Hi,  
This might be a silly question so I apologize in advance.

I was trying to search on google for what kind of problems ML or DL can be useful regarding signal processing or automation but I found nothing.  
Also the problems in these topics which I have stumbled onto during classes on my uni seem not to complicate, or it seems that using such algorithm on these problems would be like reinventing the wheel.  


Do you guys have any ideas in what fields in Automation/Signal processing such algorithms might be useful?  
(I don't have a million bucks robot to teach it to walk so such farfetched ideas that are impossible for a student to conduct a study on are out)  


Thank you for your help in advance, because I couldnt thought of any topic on which I can conduct a study on.  
If you will, just give me a hint where to look, or a loose idea where such things might be useful and somewhat doable.",https://www.reddit.com/r/automation/comments/z3bg1e/with_what_kind_of_probelms_can_neural_net_help_in/,auto
938,Best Dashboard Solutions for KPIs???,"Hello Everyone, 

I'm running a B2B agency and we're currently going through rounds of automation. One of the things that I'd like to implement is a dashboard that contains the following: 

\+ Financial data (MRR, growth, etc.) - coming from Quickbooks

\+ Customer success data (# of new customers, total number of clients, retention rate, etc.) - coming from Hubspot and Google Sheets. 

I was wondering if there's a service that allows to integrate all these data sources and beautifully visualize them as live dashboard that can be accessed through the phone, laptop, and maybe also displayed onto a TV. And it'd allow for user management (certain people within the company can see certain information). 

Thanks in advance",https://www.reddit.com/r/automation/comments/z3034c/best_dashboard_solutions_for_kpis/,auto
939,windows GUI automation tool/framework,"Hey all!

I am looking for a framework or tool, which can automate GUI software on Windows. A client of mine uses an application which poorly does not have any API. The only way of automating would be inserting directly to the DB. The vendor of the software locks th DB however and acces to it is just possible after a paid consultancy by the vendor.

So I am looking for a framework for automating GUI actions. It is just about inserting data into a basic form. Which tools should I look for?",https://www.reddit.com/r/automation/comments/z0l328/windows_gui_automation_toolframework/,auto
940,Automation for fast-growing B2B Service Agency,"Hello Everyone, 

I'm the CEO of a fast-growing B2B service agency focused on cold email lead generation. Our internal core products include Hubspot CRM, Front, Notion, Slack, Google Drive, and Gsuite. I was wondering if anyone has ideas of how to best use automations (Zapier, App Sheets, etc.) to streamline the data exchange between these different products. We're open to coding our own integrations as well. 

Thanks and I'm looking forward to your answers.",https://www.reddit.com/r/automation/comments/z0etl2/automation_for_fastgrowing_b2b_service_agency/,auto
942,Discover 2 years' history of receipts&invoices in your Gmail account(s) with one click,"We developed a great automation service that finds all invoices and receipts in your Gmail and Outlook accounts up to 2 years back. The service is now offered at AppSumo for a $69-lifetime deal. Take a look + tell us what you think.

[https://appsumo.com/products/wellybox](https://appsumo.com/products/wellybox?utm_source=partner-link&utm_medium=referral&utm_campaign=partner-166422)",https://www.reddit.com/r/automation/comments/z04zcv/discover_2_years_history_of_receiptsinvoices_in/,auto
943,Automating Weq4u.. via Zapier or scripts or something similar?," 

We are based in the UK and use a horizon phone system.

One of our departments have to often queue for hours to talk to someone in another call centre they need, it wastes a lot of time, and they are tied up when clients need them, they dont want to put the queued call on hold to talk o a client in fear of missing when its answered.

Weq4u is an excellent free service which takes the sting out of this but with a couple of caveats.

It routes your call queue request based on caller Id so you can't have more than 1. Plus all of our agents have the outbound CID set to our main office number, so that's 1 for the whole company and the return path for that number is our reception. So it can't work from this CID

When you are at the front of the queue it gives you a quick missed call for you to redial from the relevant CID to connect, it doesn't just join in you when you answer the call.

My thoughts were that we could have separate VoIP lines (with any UK provider) for the agents just for managing weq4u, so our agent makes the call out with their unique CID, they join the outbound queue and presses 9\* to end the call, so weq4u does its magic and waits for an agent.  
Then weq4u rings back to that VoIP line (2 rings missed call) to tell it that the agent is ready on the outbound call. We have an automation somehow which immediately rejects the weq4u call based on its CID, then redials it (which you have to do), dials a hunt group (external inbound) number on our horizon system and transfers the call onto that hunt group (or conferences it if that's easier).

I could name the horizon hunt groups based on which agents queue will be calling so that everyone in that department can see that Agent X's outbound call queue is ready to talk, if they are available they will grab it, if they are away or on the phone someone else will grab it and do what's needed so it's not missed.

It's only a small department, currently only 4 but looking to expand to 8 agents, they all sit together and can easily see whos busy and whos not.

It should work in theory, but I've been trawling VoIP systems and automation systems and I can't find an obvious way to make this happen.

The system would need to have unlimited minutes, include the 0333 number in the minutes, and be able to present UK CID's

I would appreciate any pointers or ideas I'm a sys admin (and I do a bit of easy dev), I'm not up on all the VoIP solutions out there.",https://www.reddit.com/r/automation/comments/ywvzbc/automating_weq4u_via_zapier_or_scripts_or/,auto
945,I need to click that link first!,"I am someone desperately looking for a house. This realtor sends you emails, from the same sender, that host a link that you click on for a viewing of the apartment. The first \~25 people get in. I want to set it up so that when I receive an email from that sender, it automatically clicks the 2nd link or that specified link with the same button text. How would someone with no, or minimal, coding experience do this?",https://www.reddit.com/r/automation/comments/yuvelx/i_need_to_click_that_link_first/,auto
946,Automated testing basics,"Running a large number of tests automatically on the developer's computer, server, or in the cloud to make sure everything functions as intended.

The testing phase of the software development lifecycle is crucial. By handling tedious and repetitive duties like regression tests, automation testing frees up testers to focus on other high-quality activities like conducting exploratory tests and analyzing test findings, among other things. As a result, automation testing makes it possible for you to complete more tests faster.

Manual testing may become challenging as there are more features to test or more software faults to find. Automated testing can help with this.

To keep the software development process productive and effective, automation testing investment is crucial. You may run tests in any environment you require, as frequently as you like, and with automation. As a result, security and stability are improved, time and effort are saved, and maintenance costs are decreased.",https://www.reddit.com/r/automation/comments/yqjg89/automated_testing_basics/,auto
947,Complete Reporting Automation,"How do you have your reports automated completely? Share the steps/tools and tech stack used. I am looking for a solution to automatically send report to a list of people or users who are interested in receiving it. 

How did you build the report (etl, transformatin). How are you generating the report and sending to users, in which frequency and how?

Any help, ideas, processes tools and tech will be appreciated.

The idea is to design the report once and make it work every week/days etc and send automated everything. Zero human in tervention after that.",https://www.reddit.com/r/automation/comments/yq1aom/complete_reporting_automation/,auto
948,Which is better? Cypress VS Selenium,"Hello guys!
I am pretty new (almost one year) into automation testing for websites. My question and debate topic is which one is better and gives more opportunities in order to test a website? Cypress or Selenium grid?",https://www.reddit.com/r/automation/comments/ym4sf2/which_is_better_cypress_vs_selenium/,auto
950,Reusability in Power Automate,I want to ask if someone has solved the reusability problem of Power Automate Desktop. How can I reuse the subflows in different Desktop flow?,https://www.reddit.com/r/automation/comments/ylj4p6/reusability_in_power_automate/,auto
952,Email Automation Challenge,"Hi there,  
Newbie to this group. I'm having trouble with an email automation challenge.  


Most mail tools allow you to ingest responses from a survey, create a contact and then send an email or sequence to a user and include merge fields based on their responses. But what if you are creating an automation where you have someone answering a form multiple times (like say a daily performance report) and you want to send them through an email sequence for each submission with the fields for each submission included with its unique merge fields? The problem I'm having in Mailchimp and other mailers is that their merge field contacts are overwritten so I can't send them a tailored sequence for EACH submission.   


Got any ideas for platforms that would help me out with this or an ideal way to set this up?",https://www.reddit.com/r/automation/comments/yl8n8y/email_automation_challenge/,auto
953,Benefits Of Automation Testing,"Automation testing is one of the most popular methods for software testing. It’s an extremely efficient, cost-effective solution that can be adopted at any stage of your software development lifecycle – and it eradicates ambiguity on whether to use automation testing or stick to manual testing.

Automated testing has many benefits, such as: 

**Shift-left testing**\- It is a part of continuous testing that conveys that the testing phase should be incorporated into the SDLC (Software Development Life Cycle), right from the requirement gathering phase to find bugs at an early stage. This can save money and time, by ensuring that bugs are found early by a tester.

**Easy regression testing-** Developers may need to perform a set of similar test cases over and over again, just to ensure that the bug has been removed. To make sure all testers have access to the latest changes, they have to spend time setting up the environment on their local machines and then run tests in parallel by creating new test plans and test cases every time. This leads to unnecessary wastage of time, money and efforts.

**Reduced business costs-** Automation testing offers significant benefits. It is advantageous for organizations because it reduces business expenses and enhances the quality of work. With automation testing, organizations are able to reduce additional expenses and maximize resource utilization.

**Improve the quality of manual tests-** With the help of test automation tools, you can perform manual testing very effectively. Test automation helps testers validate testing coverage in different environments and produces the required results at a much faster pace. These benefits of test automation have won customer favors in recent years.

**Better smoke testing-** Smoke tests are considered to be a best practice as they help avoid outages. These tests are used in the early phases of a project, and can catch major issues before tests begin running regularly. Automated smoke tests are an efficient way for teams to do this.",https://www.reddit.com/r/automation/comments/yid2dc/benefits_of_automation_testing/,auto
954,10 Key Benefits of Business Process Automation,"**What is business process automation?**

[Business process automation (BPA)](https://www.sattrixsoftware.com/what-is-business-process-automation.php) is the use of technology to automate repetitive, manual tasks in the workplace. By automating these tasks, businesses can improve efficiency, save money, and improve employee morale.

BPA can be used to automate a variety of tasks, including customer service, data entry, accounting, and human resources.

Examples of Business Process Automation

⦁	 Document routing

⦁	 Invoice processing

⦁	 Employee onboarding

⦁	 Data entry

⦁	 Screening against PEPs and sanctions lists

⦁	 Data deletion

⦁	 Transaction monitoring

**The top 10 benefits of business process automation are listed below.**

**1. Increased efficiency and productivity:** Business process automation can help you automate repetitive and time-consuming tasks, freeing up your employees to focus on more important tasks.

**2. Improved accuracy and quality:** Automating tasks can help to reduce errors and improve the overall quality of your outputs.

**3. Increased customer satisfaction:** When your processes are more efficient and accurate, your customers will be happier with the results.

**4. Allows employees to focus on higher-value tasks:** Automating low-value tasks frees up employees' time so they can focus on tasks that are more important to the business.

**5. Increased sales and revenue:** Automating your processes can help you to increase sales and revenue by making it easier and faster for your customers to purchase your products or services.

**6. Reduces errors and improves accuracy:** Automated processes are more accurate than manual ones, which can help to reduce errors and improve the quality of your products or services.

**7. Increased market share:** By automating your processes, you can improve your competitiveness and grab a larger share of the market.

**8. Increased profitability:** Automating your processes can help you to increase your profits by reducing costs and increasing

**9. Save time and money:** By automating tasks that are time-consuming and expensive to complete manually, businesses can save a significant amount of time and money.

**10. Higher productivity:** Organizations that use technology to automate processes see an increase in productivity as well. The main reason for this is that machines can handle multiple tasks at once, which speeds up processes.",https://www.reddit.com/r/automation/comments/yeobst/10_key_benefits_of_business_process_automation/,auto
955,extraction automation in excel,"Hi Everyone, wanted to ask if we can build a macro which can extract a Excel file from a password protected pdf (which we generally open from Adobe acrobat from left attachments section) is there an alternative way or std software for this automatation?",https://www.reddit.com/r/automation/comments/yepb85/extraction_automation_in_excel/,auto
957,Importing Youtube History via Clicknium and Python,"Hello Everyone, I created a script to import history from one YouTube account to another. There is likely a better way than this, but since nothing existed I just wrote something quick and thought I'd share.

# The Method and Why

This will automatically iterate over your google takeout export and load each video in your previous watch history so that it can be registered in a new account. I use a 10 second wait between each load to ensure the browser has fully loaded the page and YouTube puts it into your watch history.

I created this because I was trying to move my YouTube history from one account to another, and could not find an option. If you want to fully transfer an account, there is a method for this, but I could not find anything for ONLY transferring watch history.

# Prerequisites

Visual Studio Code (you can use any tool to run the python script, this was my go to)

Vivaldi browser (you can probably modify the code below easy enough to work with others, but this is what I used)

Your watch history exported from google takeout

# Exporting History in Google Takeout

Go to [https://takeout.google.com](https://takeout.google.com) on the account you want to transfer history from

Click ""deselect all""

Scroll down to YouTube and YouTube Music section and check it

Click multiple formats and swap watch history from HTML to JSON then click ok

Click ""All YouTube data included""

Click deselect all

Check the history checkbox then hit ok

Click next step

Send via email to yourself. Depending on your length of watch history this will take a varying amount of time.

# Next Steps

If you do not have visual studio code, install it. Load a folder in it and create a python script (I called it [ImportHistory.py](https://ImportHistory.py))

Login to YouTube in the browser you use with the script below (currently written for vivaldi) with the account you want to import to.

Copy and paste the following into your created script

    from clicknium import clicknium as cc
    import json
    import time
    
    # install chrome extension and open a tab to automate
    cc.chromium('vivaldi').extension.install_or_update() 
    tab = cc.chromium('vivaldi').open(""www.google.com"") #Can be any url, just need to open a tab
    
    # Open the google takeout json for watch history
    with open('watch-history.json', encoding=""utf8"") as f:
        data = json.load(f)
      
      
        #Loop through each item in our history.
        for i in data['history']:
            #If there is a title url we can load it
            if 'titleUrl' in i:
                #Strip the https:// from the url in order to make goto work properly.
                url = str(i['titleUrl']).replace(""https://"", """")
    
                #Open the url and then wait for it to fully register youtube history
                tab.goto(url)
                time.sleep(10) #Arbitrary number, feel free to make shorter if it works
                                
      
    # Closing file and tab
        tab.close()
        f.close()

# After your Takeout is downloaded

Extract the zip and navigate to \\Takeout\\YouTube and YouTube Music\\history\\ where a watch-history.json file should live.

Copy the json file to the same folder as your python script you made.

Run the python file which will open a new browser window and tab. This tab will load all videos in sequential order. It is useful to do this in a single tab as you can right click the tab and mute to let this run in the background while you continue doing whatever else.

# How Long Will it Take?

This depends on how long your watch history is, but roughly 10 seconds per video in your history. This might take a really long time. You may try shortening the time.sleep(10) to a smaller value. I was able to successfully run as low as 5 seconds, but this started to cause some issues with not all results being put in my youtube history (likely due to load times).

# Modifying To Use Another Browser

The only lines you should need to change to make this work with another browser is the first two that install the chrome extension and open a tab, specifically

    cc.chromium('vivaldi').extension.install_or_update() 
    tab = cc.chromium('vivaldi').open(""www.google.com"") #Can be any url, just need to open a tab

Clicknium supports other browsers, and if you change this to open a tab on a different browser it should work fine, but you may need to modify it slightly as I have not tested in any other browser.

&#x200B;

Hopefully this helps someone, but if you have the ability to just transfer an account I would recommend that as it is faster.",https://www.reddit.com/r/automation/comments/ydzgzn/importing_youtube_history_via_clicknium_and_python/,auto
958,Looking for An App or Alternatives,"Currently, I'm using the automation application Join - by joaoapps. It has a function to send whatever URL between the connected platforms, such as my PC and Android. As I was watching a YouTube video and sending it to my computer, I thought it would be convenient if I could swipe up with two fingers to trigger the app's action. I remember seeing an app years ago that created icons for particular parts or actions of an app, but I am not certain if it still exists, so I am here to ask if anyone knows this app, similar ones, or a workaround.

[https://play.google.com/store/apps/details?id=com.joaomgcd.join&hl=pt\_BR&gl=US](https://play.google.com/store/apps/details?id=com.joaomgcd.join&hl=pt_BR&gl=US)",https://www.reddit.com/r/automation/comments/y33z1v/looking_for_an_app_or_alternatives/,auto
959,Zapier-like for home computer,"Hi, fellow automators!

I was curious if a software, Linux-based, allows me to automate tasks on my computer (Ubuntu) that are repetitive and happen multiple times per day.

I am looking at something similar to Zapier, but for my desktop computer, not an online service. 

Something like: when folder ABC is updated with a file, make a copy of that new file and move it to Folder DEF

Is this possible?",https://www.reddit.com/r/automation/comments/y1jqyx/zapierlike_for_home_computer/,auto
962,How will I implement this automation,"I had this project implementation idea and found out that power automate was the best tool to do it. But I later found that I need a premium version to implement the features I want to.

I there any alternate way to implement what power automate premium version can provided its free",https://www.reddit.com/r/automation/comments/xv5bqu/how_will_i_implement_this_automation/,auto
964,how to automate switching between data transferring input devices,"Hello everyone, I have several devices that have an output D-sub socket for reach of them, and an output device that that has one input socket, this device allows me to read the data that the input device that's connected to it transfers, and I would like to automate the switching between the input devices instead of having to always unplug and plug each manually, how would I go about doing this? 

The issue that I see with using relays for example is simply the amount of switching that is needed, because I need to switch to all the pins of each D-sub socket that I want to use, are there boards that allow switching between such sockets?  Or should I really use an abysmal amount of relays instead? 

Thank you!",https://www.reddit.com/r/automation/comments/xq7jt5/how_to_automate_switching_between_data/,auto
968,how would you auto-save facebook group posts?,"It’d be nice if I can export posts from a facebook group automation. e.g. send an email copy to address. I have tried with IFTTT, make.com with no success. Also importantly it’s a closed group",https://www.reddit.com/r/automation/comments/x9sa6a/how_would_you_autosave_facebook_group_posts/,auto
969,how to automate data entry,"I was just given a job to pull data, names, phone numbers, email addresses and other ""job application"" data from a website to an excel spreadsheet.  I feel like this could easily be an automatic process but don't know how to go about automating it. Any ideas?",https://www.reddit.com/r/automation/comments/x56o8x/how_to_automate_data_entry/,auto
970,What states can an assembly line station be in?,"Hoping you all can sanity check me on this:

I'm using three conditions to dictate the state: part presence, cycle status, and line run-out status (When there is no more work in the queue)

|Part Present|In Cycle|Cell Running Out|State|Idle Reason|
|:-|:-|:-|:-|:-|
|1|1|0|Working|\-|
|1|0|0|Idle|Push|
|0|0|0|Idle|Pull|
|1|1|1|Working|\-|
|1|0|1|Idle|Push|
|0|0|1|Idle|Run out|

The point of this being to automatically detect and track how long each station spends working and idle, and why it is idle.

I omitted faulty states like in-cycle and no part present, but are there any other states that I'm missing?",https://www.reddit.com/r/automation/comments/wwk9gr/what_states_can_an_assembly_line_station_be_in/,auto
971,Need opinion for Github Integration with Automation Application.," **When integrating Github with your automation software, which is better? From a Security point of view? From a UX point of view? : -**

&#x200B;

1. Ask the user for the Personal Access Token every time they want to make changes to the connection(Add repo, org, etc. or Edit existing)
2. Ask for the token once, and let the user do unlimited changes till the token expires.",https://www.reddit.com/r/automation/comments/wvs8lj/need_opinion_for_github_integration_with/,auto
972,How to prevent from my email mark as spam?,"I am currently making a web app for my company. One of the features are automatically sending email notification to our customer to notify that we have started the job that they requested.

The problem is, when we open tested this for 3 months, our company email got marked as spam.

Is there any way to prevent this? I have seen so many people sending newsletter and a similar notification email without being marked as spam so I know it is possible.

I use Phyton, Django Framework, JavaScript and Next.js

Thank you in advance!",https://www.reddit.com/r/automation/comments/wvdq7i/how_to_prevent_from_my_email_mark_as_spam/,auto
974,What job title would this be?,"Currently I am a data analyst. My responsibilities in this job have mostly been using BI tools to make presentations and reports for our CSM's to show clients. However since the team started with me and two coworkers and we have a non-technical boss, we've had a lot of flexibility to do pretty much whatever we want.

A lot of those unrelated projects have tended to be with automation. I've created VBA macros for our finance people, used jupyter to make a script out of something that had been done manually, and designed workflows on [Tray.io](https://tray.io/). I know python and javascript at an intermediate level, but don't know much outside of actual coding (i.e. I can solve coding problems but don't know much about like how frontend and backend interact or about networking in general).

Is straight up automating things like a specific job? If I like those kinds of projects is there some sort of career path that I should be looking into?",https://www.reddit.com/r/automation/comments/wqyk9f/what_job_title_would_this_be/,auto
975,Pulover's Macro Not Stopping,"i've made a macro with pulover's macro creator to test out one of my bots, and I can't seem to stop it. I tried the default playback and play macro commands, as well as the stop, and they never seem to work.

[This is the macro I have](https://preview.redd.it/rzn4uf2fqbi91.png?width=1310&format=png&auto=webp&v=enabled&s=9de89b28bb978faf86eb8de2cb5d467d4634f6f6)",https://www.reddit.com/r/automation/comments/wqxjsk/pulovers_macro_not_stopping/,auto
976,My PowerShell Devops Automation Framework: Kasini3000(similar to ansible),"win,linux devops automation batch script framework.(It is similar to Puppet，Ansible，pipeline) 

Open source, free, cross-platform

&#x200B;

scenes to be used: 

Distributed tasks, file replication, master-slave server management, etc.

&#x200B;

i post it at powershell area ,to see:

[kasini3000](https://www.reddit.com/r/PowerShell/comments/wpk9nm/powershell_devops_automation_framework/)",https://www.reddit.com/r/automation/comments/wqj3x4/my_powershell_devops_automation_framework/,auto
978,Help with download csv automation,"Hi,

(sorry for my bad english, not my mother language).

I'm looking for a way to automate the following process to be done two times per day every monday to friday:

1. Open specific url in chrome
2. Log in with user and password
3. Navigate with several clicks to a ""export"" button that opens the window ""select where to download this file"" (this is not a link to the file, is a button).
4. Select a specific Google Drive folder to download the file.

I only know how a bit of coding in Google Apps Script,  I have tried Axiom, Chromium and Zapier, but I can not find a way to make it select the destination folder and download the file.

I am using a PC, windows, chrome, google drive.

Is there a tool with low code that I can set to do this? I am a one man company and I would prefer not having to pay an expensive license like UiPath or Automation Anywhere.

Also, is there a way to do this fully online so the task gets done even if I don't have my PC turned on?

Thanks for your help",https://www.reddit.com/r/automation/comments/wloput/help_with_download_csv_automation/,auto
983,Some popular UI automation tools/ softwares,"I would like to list some UI automation tools as following.

1. UIPath
2. Selenium
3. TestProject
4. UFT (United Functional Testing)/QTP (Quick Test Professional)
5. Clicknium
6. Katalon Studio
7. TestComplete
8. TestIM
9. Ranorex Studio
10. Squish
11. Rapise
12. AutonomIQ
13. Robocorp
14. Playwright",https://www.reddit.com/r/automation/comments/weunlp/some_popular_ui_automation_tools_softwares/,auto
985,Clicknium Automation Sample Solution - Finance Quarter Close,"For many enterprises, at the end of each fiscal quarter, the finance team spends much time making all of the financial obligations fulfilled. This is a sample of financial quarter close solution with [clicknium](https://www.clicknium.com/) automation. It can review transactions automatically. Here is the details: go through transactions in local financial system to find the matching transaction in the Bank system. If the match is found, change the `Transaction Status` to `Verified` in local financial system.

The manual steps are as follows:

* login to local financial system.
* Query the transactions in this quarter.
* login to the bank system.
* For each transaction:
   * based on `Payment Account`, navigate to corresponding page.
   * search 'Payment Amount'.
   * If the matched one is found, go back to local financial system, set the `Transaction Status` to Verified.
* After all transactions are reviewed, click `Submit` button in local financial system.

# Run this sample

* follow [clicknium getting started](https://www.clicknium.com/documents/quickstart) to set up develop environment.
* clone [sample repo](https://github.com/clicknium/clicknium-samples).

&#x200B;

    git clone https://github.com/clicknium/clicknium-samples.git

* open the folder 'QuarterCloseChallenge' in Visual Studio Code
* open `app.py` in Visual Studio Code.
* press `F5` to debug the sample or press `CTRL+F5` to run sample. You will see the result as below:

https://preview.redd.it/i18q40trz9e91.png?width=600&format=png&auto=webp&v=enabled&s=3315e7f70fd65c47dd2945f1fb1e399c60138187

# The Purpose of The Sample

* open local financial system to get the transaction count and scrape `Amount` and `Account` information for each transaction.

&#x200B;

    def get_transaction_count():
        transaction = []
        tab = cc.edge.open(""https://developer.automationanywhere.com/challenges/automationanywherelabs-quarterclose.html"", is_wait_complete=True, timeout=60)
        if tab.is_existing(locator.quaterclose.developer.button_onetrust_accept_btn_handler):
            tab.find_element(locator.quaterclose.developer.button_onetrust_accept_btn_handler).click()
        elems1 = tab.find_elements(locator.quaterclose.developer.text_paymentaccount)
        elems2 = tab.find_elements(locator.quaterclose.developer.text_paymentamount)
        count = len(elems1)
        for i in range(count):
            account = elems1[i].get_text()
            amount = elems2[i].get_text()
            transaction.append({""Amount"":amount, ""Account"":account, ""Status"":""Unverified""})
        return tab,transaction

Here we leverage Clicknium `find_elements` api to find all similar elements. For example, for element's locator of PO number:

https://preview.redd.it/iwtuqyo20ae91.png?width=1184&format=png&auto=webp&v=enabled&s=4d422b71810d7ee5fe8e1f81577365564df8297a

To record similar elements, you can click `Similar elements` in Clicknium Recorder:

https://preview.redd.it/02rzux140ae91.png?width=337&format=png&auto=webp&v=enabled&s=e92a68cb8f7e1df879d923541da15a1c94c392f0

The wizard will be shown as below:

https://preview.redd.it/28vqu2850ae91.png?width=509&format=png&auto=webp&v=enabled&s=9e8261f9b543460d1fe8ade55d9bfe076922ed14

You can record (`Ctrl`\+click) two or more elements, for example:

https://preview.redd.it/yt4gaoq70ae91.png?width=1153&format=png&auto=webp&v=enabled&s=80057d8e07bc4ef0e8be74140716299e012593e4

https://preview.redd.it/8yamkbs90ae91.png?width=508&format=png&auto=webp&v=enabled&s=5bde5f0eebcdeb7c7777481e07400cff837780af

It will show the counts of matched elements:

* open bank system and login
* iterate the transactions and go to the corresponding account page based on each transaction's account
* search the transaction's amount, and if the matched one is found, mark the transaction state as Verified.

&#x200B;

    def validate_transaction(transaction):
        bank_tab = cc.edge.open(""https://developer.automationanywhere.com/challenges/automationanywherelabs-arcadiabanklogin.html"", is_wait_complete=True, timeout=60)
        bank_tab.find_element(locator.quaterclose.developer.email_inputemail).set_text(""tammy.peters@petersmfg.com"")
        bank_tab.find_element(locator.quaterclose.developer.password_inputpassword).set_text(""arcadiabank!"")
        bank_tab.find_element(locator.quaterclose.developer.a_login).click()
        for item in transaction:
            bank_tab.find_element(locator.quaterclose.developer.a_action, {""account"":item[""Account""]}).click()
            bank_tab.wait_appear(locator.quaterclose.developer.table1)
            bank_tab.find_element(locator.quaterclose.developer.text).set_text(item[""Amount""])
            if bank_tab.is_existing(locator.quaterclose.developer.td_amount, {""amount"":item[""Amount""]}):
                item[""Status""] = ""Verified""
        bank_tab.close()

* go back to local financial system and  batch update transactions state.

&#x200B;

    def update_transaction_status(tab: BrowserTab, transaction):
        elems = tab.find_elements(locator.quaterclose.developer.select_status)
        count = len(elems)
        for i in range(count):
            elems[i].select_item(transaction[i][""Status""])
    
        tab.find_element(locator.quaterclose.developer.button_submitbutton).click()

# Locator

The [Locator](https://www.clicknium.com/documents/tutorial/locator) is the identifier of UI element, which can be recorded and edited with [clicknium vs code extension](https://marketplace.visualstudio.com/items?itemName=ClickCorp.clicknium).

# Compare with Playwright

* You need to write xpath to get similar elements.

&#x200B;

    elems1 = page.query_selector_all(""//*[contains(@id,'PaymentAccount')]"")
    elems2 = page.query_selector_all(""//*[contains(@id,'PaymentAmount')]"")

* You need to fill the text to search the transaction item by pressing Enter as well.

&#x200B;

    bank_page.locator(""[placeholder=\""Search\\.\\.\\.\""]"").fill(item[""Amount""])
    bank_page.press(""[placeholder=\""Search\\.\\.\\.\""]"",'Enter')

# More samples

You can refer to more automation samples and solutions in [clicknium github samples](https://github.com/clicknium/clicknium-samples). Send [email](mailto:support@clicknium.com) to us or [Join Slack](https://join.slack.com/t/clicknium/shared_invite/zt-1cfxsstw7-s0CeJdhyg5wQ1h7_KKc6QQ).",https://www.reddit.com/r/automation/comments/wa4x9r/clicknium_automation_sample_solution_finance/,auto
986,looking for a useful workflow tool built for personal use.,"I think I've wasted a year of my life just looking for apps. Let me know if anyone is spinning their wheels thinking this:

""One app does this thing, but it doesn't do this other thing, so I'll try to integrate them both. But I can't do that without this OTHER app.""

Yes, we'll if thats you that's also me too. Lol. Right now I'm looking for an app I can REALLY run my personal life from. I'm looking for notes, task management, project management, and automated workflows, and integrations. For example, if it's someone's birthday, I want to push a button and have a whole set of tasks appear automatically so I can remember the birthday in general.

I'm using notion, Evernote, Gmail, and google calender right now...


Anyone have any suggestions?",https://www.reddit.com/r/automation/comments/w9rmmq/looking_for_a_useful_workflow_tool_built_for/,auto
987,Configuration and State Management Tools Preference,"Which automation/configuration management tool do you prefer in terms of capability, learning curve, budget, self healing and so on, and why?. Just for a project I am working on. I promise I wont try to sell you anything. 

&#x200B;

Thanks community. 

[View Poll](https://www.reddit.com/poll/w4n8pk)",https://www.reddit.com/r/automation/comments/w4n8pk/configuration_and_state_management_tools/,auto
989,Microsoft Teams and Alexa Amazon,"I’m looking for a way to automate some home stuff. I work from home, as does my significant other. We just got smart light bulbs to show when we are on a call - as to not be interrupted. However, I’d like to set up automation that teams does this automatically when I’m on a call. 

So far I haven’t found very many solutions in my Google hunt for this. I do know how to code, a little. However, I’m not a developer or engineer by any means. 

If anyone has some advice/direction/ideas - I would love to hear them.",https://www.reddit.com/r/automation/comments/vzal2k/microsoft_teams_and_alexa_amazon/,auto
991,short survey,"Hello, I am computer science student working on a automation project, one of my task was create a survey, now I need to collect some responses. I would appreciate if anyone could take this survey it just takes like 2 minutes. I will live the link here [https://www.surveymonkey.com/r/LVVVTZH](https://www.surveymonkey.com/r/LVVVTZH)",https://www.reddit.com/r/automation/comments/vtiiko/short_survey/,auto
993,Help me to Automate Test Case Writing,"It's a very lengthy process to Create a Test Case document by own for every project.
I want to Automate this process.
Is there any tool or any other way to Automate Writing test cases by just providing it some info with similar modules such as login, register, ecomm user flow etc.",https://www.reddit.com/r/automation/comments/vqodhh/help_me_to_automate_test_case_writing/,auto
994,Why has the Helpmate robot failed?,"The Helpmate hospital robot was a project from the early 1990s with the attempt to increase the productivity in a hospital. The robot was able to deliver food to the patient's rooms and was doing so much cheaper than a human nurse.

Before we can answer the question why the Helpmate project has failed we have to answer, if this was the case. It seems, that there a different opinions available if a hospital robot makes sense or not. The idea from the manufacturing company was, that a robot can do repetitive tasks much faster and at lower costs and this makes a robot a good choice to automate something. The counter argument is, that robots won't improve the productivity because they are too complicated to use. What do you think, was the robot “Helpmate” a success?",https://www.reddit.com/r/automation/comments/vp3y7p/why_has_the_helpmate_robot_failed/,auto
995,I want to learn machine control,"Hey there! I'm a mechanical engineer who specializes in designing food processing equipment and production lines. I want to learn how to automate my designs so any referrals or learning sources would be great!

I have virtually zero idea on how to design and select the electric components for my applications.

The machines usually have stepper or servo motors, induction motors, pneumatics and sensors.",https://www.reddit.com/r/automation/comments/vlq7si/i_want_to_learn_machine_control/,auto
996,Checking University Website,"Hi all, 

i want to automate to check if there is any change on website of my university.

I have tries many many MANY ways but nothing works so i am writing this here after many failed attempts.

I am trying to automate checking if there are any new notifications on my university website. I need to go to It's page ([https://university.com/Index.jsp?)](https://university.com/Index.jsp?)) and to login with my credentials and then i am greeted with my profile on their webisode ([https://university.com/StudentIndex.jsp?)](https://university.com/StudentIndex.jsp?)) where i can see if there are any new notifications. I have made selenium script that basically goes to that website, takes screenshot and compare it to an image i have took when there are no new notifications. If they are not the same, there are notifications and via versa. Easy! But when i want to put that script on my Raspberry Pi it doesn't work. I don't have X on there so selenium doesn't like it. I have tried with all sorts of modules (PyVirtualDisplay,...) but everything just make my RPI(3B) freeze and nothing. Ok so i have gone in another direction. I looked at wget and curl. But problem is UNI website has bad ssl. Ok --no-check-certification does the work, but still it wouldn't get pass login page. Ok then i have tried with cookies which i get when i log in, but wget says [https://university.com/Index.jsp](https://university.com/Index.jsp)? does not exist but [https://university.com](https://university.com) does, but if you go there there is no login. WTF. Ok then i have tried HTTrack but i am confused on how to add cookies to login ( i am on Linux Arch so i can use only cli version).

Ok that doesn't work as well, ok go look into another direction. I have tried to look for some website that does that, and there are some.Nice! But they are all paid, and if not paid they don't work. I don't know why, probably cause it is dynamic website and not static. 

Ok what about Android app? There are some but they too have a problem cause it is dynamic. 

And after many days and attempts i am writing this here. Does anyone know what should i do? How can i check If there are new notifications on uni website ?",https://www.reddit.com/r/automation/comments/vl85cq/checking_university_website/,auto
997,how do i automate going to sites and log in?,"so i want to be able to go to like my mail, github, job searching site and many more then login to those sites. sometimes the site has like a code authentication tool.  
was wondering whats the best way to do this? selenium? pyautogui or something else?",https://www.reddit.com/r/automation/comments/vjfp59/how_do_i_automate_going_to_sites_and_log_in/,auto
998,How to automate downloading and emailing a bill?,"I'm an IT tech but never used any automation for my own tasks.

I need to grab a PDF copy of a bill each month, then email it on.

I have MS flow, but not much else.  What is the best way to handle this? Not sure how it can handle login vs logging in with stored cookie and all that jazz.

My ability to do this in AutoHotKey is lacking and would like something well supported and straight forward.  

Thank you!",https://www.reddit.com/r/automation/comments/vhi0qk/how_to_automate_downloading_and_emailing_a_bill/,auto
1000,What are some tools a someone in an office job without a development background can use to automate their tasks?,"I have an information based job within a large organisation.
Hoping to find some great tools to automate any tasks possible.
I've been using copy.ai, looking into Zapier and IFTTT.",https://www.reddit.com/r/automation/comments/va7y8f/what_are_some_tools_a_someone_in_an_office_job/,auto
1001,How to grab/open hyperlink from gmail group chat? (extract data after opening a link in gmail group chat),"Would like to automate weekly task and need some help...

&#x200B;

Weekly in a group chat (Gmail chat), my manager posts a link which I would open this link and extract data for company expense, employee attendance, etc...

&#x200B;

How would I go about this?  Will be using selenium to scrape data once I can get this link to open....  FYI, the group chat is active through out the day but only need a link  matching string **'report'** from gmail group chat to open

&#x200B;

Any advice would be really helpful since I am stuck..",https://www.reddit.com/r/automation/comments/va9y7v/how_to_grabopen_hyperlink_from_gmail_group_chat/,auto
1002,Why use TypeScript over JavaScript in Cypress?,"I use Cypress as my UI E2E automation framework. I see a lot of people suggesting to use TypeScript over JavaScript, but unsure on what the actual benefits are?",https://www.reddit.com/r/automation/comments/v73vsc/why_use_typescript_over_javascript_in_cypress/,auto
1003,Scan 2 or 3 barcodes to fill out spreadsheet Info in right place,"Hello,

im currently working on a project where i would like to automate data entry into a spreadsheet.

I have 256 Boxes that can be at 256 Spaces with Stuff in them and would like to use a barcode scanner to scan a barcode at the place and then at the box and have it fill out an cell thats coresponding to the place with the number of the box.

Does anyone have a somewhat easy solution for this?

All i can find are expensive subscription apps that would do that..",https://www.reddit.com/r/automation/comments/uy1aai/scan_2_or_3_barcodes_to_fill_out_spreadsheet_info/,auto
1004,Find out Best Automation Company in UK,We researched and compiled a list of the top automation companies in the United Kingdom in 2022. Read our blog to learn more about industrial automation providers and to find the best company for your needs. Visit [https://www.watbro.com/best-industrial-automation-company-in-the-uk-to-watch-in-2022/](https://www.watbro.com/best-industrial-automation-company-in-the-uk-to-watch-in-2022/),https://www.reddit.com/r/automation/comments/uxctlx/find_out_best_automation_company_in_uk/,auto
1005,arlo cameras,"I am trying to figure out a way to automatically turn on two indoor arlo cameras when everyone leaves the house and to automatically turn them off when one person arrives home. Anyone know a way to do this? I use apple HomeKit, alexa, and ifttt. I am willing to download something else tho if it will get it to work.",https://www.reddit.com/r/automation/comments/uwc5nt/arlo_cameras/,auto
1006,Arlo Cameras,"I am trying to figure out a way to automatically turn on two indoor arlo cameras when everyone leaves the house and to automatically turn them off when one person arrives home. Anyone know a way to do this? I use apple HomeKit, alexa, and ifttt. I am willing to download something else tho if it will get it to work.",https://www.reddit.com/r/automation/comments/uwc3pk/arlo_cameras/,auto
1007,DCS Crash Course,Folks I’ve worked with Siemens Insight building automation for few yrs and have good automation background. I’ve since move from that role and new opportunities show up with DCS process controls. Would greatly appreciate info about good learning materials/course for DCS👍,https://www.reddit.com/r/automation/comments/uqeav7/dcs_crash_course/,auto
1008,Inkbird STC-1000 but for humidity?,"Hi, I want to set up an automatic hygrometer ""thermostat"" that automatically shuts off a device whenever humidity enters chosen temperature. I'm familiar with the 65$ Inkbird humidity controller but I've seen some 20$ STC-1000 temperature controller and was wondering if there was a similar product for humidity.

Thanks in advance.",https://www.reddit.com/r/automation/comments/uo2qdk/inkbird_stc1000_but_for_humidity/,auto
1009,Need help automating a remote lookup,"My department accesses data from another department to see if a list of people is in their system. Currently, we do this manually everyday but want to automate the process. We were thinking of using something like auto hot key, etc but the problem is we access the server via a remote connection. The process is clicking a shortcut on my desktop, which remotely connects to the server; I click an app on their desktop and enter the employee ID into the system; if they are, we take a screenshot and update the status on a Google Sheet.",https://www.reddit.com/r/automation/comments/unm7n5/need_help_automating_a_remote_lookup/,auto
1010,Sending an Instagram message for every google form submission,"Hi everyone! 

Is there anyway to send an automatic instagram message when a google form is filled out and submitted? The instagram message will be sent to the username the person filling out the google form provides. 

I tried using automation tools like Zapier but they don't seem to have the feature of sending Instagram messages to users that fill out a google form. 

Any help would be much appreciated, 

Cheers!",https://www.reddit.com/r/automation/comments/ujgq7o/sending_an_instagram_message_for_every_google/,auto
1012,Automation that automatically applies for new appartments,"Hi all,

I live in a country where the housing market is pretty rough, my wife and I have been looking for anything bigger than our closet studio for over a year without any succes. It's honestly mood ruining to have to apply multiple times a week for an apartment and not get invited so I thought why not look for an automated solution. A while back I started learning Python for work but I am no where close to create an automation that refreshes the websites that upload the appartments and automatically applies for us. Would you guys know of any software that would do this? Preferably no-code? I would highly appreciate it!",https://www.reddit.com/r/automation/comments/ufaeem/automation_that_automatically_applies_for_new/,auto
1013,Are there any aps or software for this?,"Hey, guys. I need a help. I don't know anything about softwares, or about automation but I'd like to send one group's message to another group at the same time the message is received on the first group. Are there any automated softwares available for this? Do you guys know any? Any help is appreciated. Thanks!",https://www.reddit.com/r/automation/comments/ufcl56/are_there_any_aps_or_software_for_this/,auto
1014,Presearch auto search,"Hey everyone! I made a bot with Python that automatically search on Presearch which gives you money for it:

https://github.com/TheDriedWater/presearch-auto-search",https://www.reddit.com/r/automation/comments/udzmv0/presearch_auto_search/,auto
1015,Facebook Leads + Whatsapp auto,"Hi, I need to automate a message on Whatsapp for when a Lead fills my Facebook ad Lead Form.

Right now with zapier I send an automated email and sms to the leads. But I need to send a WhatsApp. Is it possible?

&#x200B;

Thanks",https://www.reddit.com/r/automation/comments/udu06a/facebook_leads_whatsapp_auto/,auto
1016,Automate tweet of Google Slides,"Hi All,

I have a Google slides document (1 slide) that is being updated daily, and I'd like to automate a scenario where I tweet that Google Slides doc (1 slide only) as an Image to Twitter daily at a any particular time of day.

I've been playing around with Zapier without any luck.

Any suggestions or someone has done something similar already?

Thanks",https://www.reddit.com/r/automation/comments/udt7ro/automate_tweet_of_google_slides/,auto
1017,Where to buy online in Germany?,"Hello guys, greetings from Brazil.

My wife’s cousin is coming from Germany and this maybe a chance to buy something, as she can bring small stuff.
I can only remember of Amazon and Saturn, are there any other online shop to look for automation and IT stuff like sff pc and cpu?

Thanks in advance!
Vielen Dank!",https://www.reddit.com/r/automation/comments/ud8pjk/where_to_buy_online_in_germany/,auto
1018,Automate sending custom messages to each link,"I have a spreadsheet where first column have the Linkedin profile links I want to connect, the second column B have the custom message I wrote for each contact. Now my workflow is to to open first link in new tab, click on Connect (or click on More then click on Connect button) A pop up will show, Click on message box, copy paste the message for that contact from spreadsheet in the message box and click Send button. This is a task I want to accomplish using automation so was wondering if there is a free software or something that can do this task for me automatically for each profile? I tried Pulover's Macro Creator but somehow its not working as intended. Any help would be appreciated.",https://www.reddit.com/r/automation/comments/ucz48a/automate_sending_custom_messages_to_each_link/,auto
1019,automation technology for business idea,"For the last few years there have been multiple times where I encountered repetitive tasks online that I wanted to automate. Typically i'll just hire a developer on Fiverr to create a bot to automate the process. 

&#x200B;

I always thought there had to be an easier way to automate simple mainstream tasks that most people encounter. 

&#x200B;

I created BotWarrior.io to solve this problem. Buy and sell bots to automate and simplify your workflow. 

&#x200B;

Out of curiosity, how do you guys automate easy repetitive tasks? Do you hire someone? Any insights would be super helpful. 

&#x200B;

Ryan",https://www.reddit.com/r/automation/comments/uaxtum/automation_technology_for_business_idea/,auto
1020,Automation in the workspace,"Wanted to quickly share a quick way to automate your workspaces. FaultFixers is a software that lets you can automate recurring maintenance tasks meaning you'll never have a broken toilet again!

https://www.faultfixers.com/blog/office-maintenance-software-must-have-features",https://www.reddit.com/r/automation/comments/u9ahr2/automation_in_the_workspace/,auto
1021,Running unlimited number of tests in less than 60 seconds,"I developed an interesting tool that uses AWS lambda to basically parallelize the hell out of running test. It is a test framework specific solution and the current version works really well for pytest selenium/playwright setups. I ran a 10,000 test framework setup in under 60 seconds. Each test was designed to take 8-10 seconds to mimic typical runtimes. 

Would anyone be interested in taking this tool for a trial run in your company?

Some cool features are

1. Use your own AWS account. Support for Azure, GCP coming soon

2. Support for pytest with selenium, appium, playwright tool

3. Flaky test detection logic

4. Number of tests does not impact the overall runtimes too much because cloud scales for you

5. Way way cheap due to low cost of lambda function 

6. Integration with test case management system like TestRail and Zephyr and MicroFocus ALM. Test results are automatically uploaded to your TCM

7. No code changes needed.",https://www.reddit.com/r/selenium/comments/10e0jd6/running_unlimited_number_of_tests_in_less_than_60/,auto
1022,Bot detection on google?,"So my code was working fine until a couple of days when I keep getting this error message from google: **Couldn't sign you in.** **This browser or app may not be secure.**

Here is my code:

>from selenium.webdriver import Chromefrom selenium.webdriver import ChromeOptionschrome\_options = ChromeOptions()chrome\_options.add\_argument(""--lang=en-US"")chrome\_options.add\_argument('--disable-blink-features=AutomationControlled')chrome\_options.add\_experimental\_option('prefs', {'intl.accept\_languages': 'en,en\_US'})chrome\_options.add\_experimental\_option('excludeSwitches', \['enable-logging', 'enable-automation'\])chrome\_options.add\_experimental\_option(""useAutomationExtension"", False)chrome\_options.set\_capability('dom.webdriver.enabled', False)chrome\_options.add\_argument(f'--user-data-dir={PATHS.CHROME\_SYS\_PATH}')chrome\_options.add\_argument(f'--profile-directory={sub\_dir}')chrome\_dirver = Chrome(options=chrome\_options, executable\_path=PATHS.CHROME\_DRIVER)chrome\_driver.get(r""[https://www.google.com/](https://www.google.com/)"")

Google keeps throwing the same error over and over!  Is there any way to avoid the detection on chrome?",https://www.reddit.com/r/selenium/comments/10cp1vh/bot_detection_on_google/,auto
1026,Add code once automation has started,"Hello,

&#x200B;

This may be a more general python question rather than specific to selenium. I am fairly new to python and selenium, but I'm typically pretty good at Google, but I can't find this answer.

&#x200B;

I use selenium to automate several admin tasks (user opens a ticket, I have selenium take that info and put it in the vendor system is one use case). What I am looking for is a way to run selenium to sign in to the sites in the morning, and I can insert and run a block of code as needed. (Same block, the only thing that changes is the ticket number)

&#x200B;

Right now I am using VScode to write in and run when I have several that are ""ready"", but that kinda defeats what I am looking for. Is there an editor that I can run that will keep Chrome open and that I can add text to as I go?

&#x200B;

Thank you!",https://www.reddit.com/r/selenium/comments/10a0b36/add_code_once_automation_has_started/,auto
1027,Run Python-selenium bot on Gitlab,"Hi everyone

Is it possible to run a python-selenium task automator on Gitlab 

Pardon me if this is a silly question, I'm pretty new here, dunno much about gitlab CI pipeline and stuff

Thanks in advance",https://www.reddit.com/r/selenium/comments/103w64j/run_pythonselenium_bot_on_gitlab/,auto
1028,Help Delaying Selenium Script,"Hey guys, I wanted to know if anyone knew how to delay a selenium script so I can manually type something into a website and then, when I'm ready, have my selenium automation run. I use selenium in python(I've seen java versions and stuff). I already tried making an if statement with a user input as the condition but that didn't work very well.",https://www.reddit.com/r/selenium/comments/zn591n/help_delaying_selenium_script/,auto
1029,Different ways to get xpath elements and CSS selectors,"I would like to expand my knowledge on this because sometimes I'm struggling with myself due to the website I'm trying to automate, always changes every time I refresh it.

And in your opinion what's the best option to use, CSS selector or XPATH",https://www.reddit.com/r/selenium/comments/zmdhwr/different_ways_to_get_xpath_elements_and_css/,auto
1030,Selenium Modules,"Hi fellow automation geeks.

If im making any sense, can you guys point me to a website or any reference that i can check for:

All the Selenium's modules, for example we all know webdriver module.

Like for example when we want to user the webdriver module we write

 

    from selenium import webdriver
    
    driver = webdriver.Chrome(""your path"")

and when you want to find elements you will use 

    from selenium.webdriver.common.by import By
    
    driver.find_element(By.XPATH, '//button[text()=""Some text""]')

i want to know where can i read about all the modules. like webdriver modules, and BY modules(if its a module) and common modules(if its a module).

Thnks again",https://www.reddit.com/r/selenium/comments/zfjdxw/selenium_modules/,auto
1031,Beginner's guide to web automation,"Hi everyone, this is my first time in reddit, as well in this subreddit.

Basically I'm confused, I don't know where to start with web automation. I've been searching the web but I still have a hard time understanding where does ""Selenium"" fit in the whole picture. I don't have a whole picture btw. I have experience on scripting in Linux, and with networking. But the web is still an unexplored territory and I need to be able to write basic scripts that can access links, fill login details and retrieve data.  

Any definition, any book, any resources are useful for me right now, either to understand what selenium is, or to get an idea about the whole concept of web automation.",https://www.reddit.com/r/selenium/comments/zd7zgd/beginners_guide_to_web_automation/,auto
1032,"some problems with find_element(By.NAME,""value"")","I'm working on a script using selenium that can login automatically login my college's imformation system.

Here's the code:
```python
from selenium import webdriver
from selenium.webdriver.common.by import By

from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager

# from selenium.webdriver.edge.service import Service as EdgeService
# from webdriver_manager.microsoft import EdgeChromiumDriverManager

driver = webdriver.Chrome(service = ChromeService(executable_path = ChromeDriverManager().install()))

# driver = webdriver.Edge(EdgeChromiumDriverManager().install())

driver.get(""http://stucis.ttu.edu.tw/stucis.htm"")

ID = ""studentid""
PASS = ""password""

ID_input = driver.find_element(By.NAME,""ID"")
PWD_input = driver.find_element(By.NAME,""PWD"")

ID_input.send_keys(ID)
PWD_input.send_keys(PWD)

driver.close()
```
and it comes with erros
```
Traceback (most recent call last):
  File ""C:\Users\USER\Desktop\Crawler\login_stucis.py"", line 19, in <module>
    ID_input = driver.find_element(By.NAME,""ID"")
  File ""C:\Users\USER\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\selenium\webdriver\remote\webdriver.py"", line 861, in find_element
    return self.execute(Command.FIND_ELEMENT, {""using"": by, ""value"": value})[""value""]
  File ""C:\Users\USER\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\selenium\webdriver\remote\webdriver.py"", line 444, in execute
    self.error_handler.check_response(response)
  File ""C:\Users\USER\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\selenium\webdriver\remote\errorhandler.py"", line 249, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""css selector"",""selector"":""[name=""ID""]""}
  (Session info: chrome=108.0.5359.95)
Stacktrace:
Backtrace:
        (No symbol) [0x00CBF243]
        (No symbol) [0x00C47FD1]
        (No symbol) [0x00B3D04D]
        (No symbol) [0x00B6C0B0]
        (No symbol) [0x00B6C22B]
        (No symbol) [0x00B9E612]
        (No symbol) [0x00B885D4]
        (No symbol) [0x00B9C9EB]
        (No symbol) [0x00B88386]
        (No symbol) [0x00B6163C]
        (No symbol) [0x00B6269D]
        GetHandleVerifier [0x00F59A22+2655074]
        GetHandleVerifier [0x00F4CA24+2601828]
        GetHandleVerifier [0x00D68C0A+619850]
        GetHandleVerifier [0x00D67830+614768]
        (No symbol) [0x00C505FC]
        (No symbol) [0x00C55968]
        (No symbol) [0x00C55A55]
        (No symbol) [0x00C6051B]
        BaseThreadInitThunk [0x761A6939+25]
        RtlGetFullPathName_UEx [0x77B08FD2+1218]
        RtlGetFullPathName_UEx [0x77B08F9D+1165]
```
Some fourms says it is becaust the driver's version is different.
I also tried:
```python
ID_input = driver.find_element(""name"",""ID"")
PWD_input = driver.find_element(""name"",""PWD"")
```
but can't works.",https://www.reddit.com/r/selenium/comments/zcduid/some_problems_with_find_elementbynamevalue/,auto
1033,ticket-booking algorithm,"I implement automatic ticket-booking using selenium and python, including auto-login, auto-redirection, and auto-booking features. If you are interested in learning selenium and want to start with a project. Or if you have any comments or issues related with the task, feel free to check it online. the link towards to github repo is: [https://github.com/JJerryJi/ticket-booking](https://github.com/JJerryJi/ticket-booking)",https://www.reddit.com/r/selenium/comments/z72cfy/ticketbooking_algorithm/,auto
1034,Error Selenium Python,"Hola a Todos 

Estoy automatizando con selenium y python con un webdriver de chrome, y ejecutando mis pruebas derrepente me aparecio este problema

23420:23136:1125/003102.494:ERROR:device\_event\_log\_impl.cc(215)\] \[00:31:02.494\] USB: usb\_device\_handle\_win.cc:1048 Failed to read descriptor from node connection: Uno de los dispositivos conectados al sistema no funciona. (0x1F)

estoy intentando de todo pero no me funciona, si alguien sabe pliiiz!",https://www.reddit.com/r/selenium/comments/z42y36/error_selenium_python/,auto
1036,Right click save link as in python,"hello guys i want to right click save link as then save on the save pop up that windows shows.

this is an example:

[https://www.who.int/data/gho/data/indicators/indicator-details/GHO/proportion-of-population-below-the-international-poverty-line-of-us$1-90-per-day-(-)](https://www.who.int/data/gho/data/indicators/indicator-details/GHO/proportion-of-population-below-the-international-poverty-line-of-us$1-90-per-day-(-))

go on this page in the data tab u can see EXPORT DATA in CSV format:Right-click here & Save link

so if u right click and save link as it will let u save the data as csv.

i want to automate that can it be done using selenium if so how?",https://www.reddit.com/r/selenium/comments/yvqpwu/right_click_save_link_as_in_python/,auto
1037,Selenium Side Runner does not create Result Files,"Hey Folks, I plan to test a website automatically on a headless server and I'm considering using Selenium for that since I know it from previous web scraping projects. I used Selenium IDE to record a little demo _.side_ file (opens a website and clicks a button) to test my setup. Then, I executed it using the Selenium side runner tool but it did not output anything although the output directory was set. Although my case is as simple as it can be I'm struggling so much already, partly because of poor documentation (ended up digging options/flags from source code) and the tool not doing what one is expecting, i.e. outputting results in a machine readable format. 

Here's what I've done:

**sides.yaml**

```yaml
capabilities:
  browserName: ""firefox""
timeout: 25000
```

**command**

```
selenium-side-runner --config-file=""config/side.yaml""  --output-directory=""results"" --debug tests/demo.side
```

**output**

```
Configuration: {
  baseUrl: '',
  capabilities: { browserName: 'firefox' },
  debug: true,
  filter: '.*',
  force: undefined,
  maxWorkers: 16,
  params: {},
  projects: [ '/home/user/workspace/test-website/tests/demo.side' ],
  proxyOptions: {},
  runId: '5d7ee74cc411318e92cb196738a08653',
  path: '/usr/local/lib/node_modules/',
  server: '',
  timeout: 25000
}
info: Running test demo
debug: Playing state changed prep
info: Building driver for firefox
info: Driver has been built for firefox
debug: Playing state changed playing
debug: executing open|/
debug: passed open|/
debug: executing click|linkText=Antworten!
debug: passed click|linkText=Antworten!
debug: executing click|name=q
debug: passed click|name=q
debug: executing type|name=q|blockchain
debug: passed type|name=q|blockchain
debug: executing click|css=input:nth-child(2)
debug: passed click|css=input:nth-child(2)
debug: Playing state changed finished
info: Finished test demo Success
 PASS  ../../../../../usr/local/lib/node_modules/selenium-side-runner/dist/main.test.js (6.686 s)
  Running project demo
    Running suite Default Suite
      ✓ Running test demo (6260 ms)

Test Suites: 1 passed, 1 total
Tests:       1 passed, 1 total
Snapshots:   0 total
Time:        6.728 s
Ran all test suites within paths ""/usr/local/lib/node_modules/selenium-side-runner/dist/main.test.js"".
Jest did not exit one second after the test run has completed.

This usually means that there are asynchronous operations that weren't stopped in your tests. Consider running Jest with `--detectOpenHandles` to troubleshoot this issue.
```

Also, when passing the `--output-format` flag, I get the following error:

```
error: unknown option '--output-format=jest'
```

I followed the instructions at https://www.seleniumhq.org/selenium-ide/docs/en/introduction/command-line-runner/ with command line runner version 4.0.0-alpha.16.

**EDIT:** I just noticed that I'm on a alpha version of a presumably new major release and thus there may be breaking changes and not yet complete documentation. All fine but why the hell is this shipped to users that don't request it explicitly? Shouldn't there be separate release channels for unstable versions?

**EDIT:** Downgraded to the last _3.x_ release from 3 years ago(!) and now it outputs results properly. However, it has >20 security vulnerabilities listed in its dependencies which is a red flag for me. Also, the `--output-format` flag is not recognized either which is okay for me but still does not match the docs.",https://www.reddit.com/r/selenium/comments/yu2n4x/selenium_side_runner_does_not_create_result_files/,auto
1038,VBA Firefox browser Selenium,"Hi guys, I just downloaded and learnt Selenium and Selenium Wrapper today for a VBA project. I just want to auto fill a form on web since it’s repetitive and time consuming but don’t want to keep opening and quitting Firefox browser all the time. Do you have any suggestions on how to write a function that only auto fill the form with values in specified cells? 
Thank you!!",https://www.reddit.com/r/selenium/comments/yt0gox/vba_firefox_browser_selenium/,auto
1039,ScreenPy Playwright v0.0.1 is released (and a humble request for help)!,"Hey friends, we released the [ScreenPy](https://screenpy-docs.readthedocs.io/en/latest/) extension [ScreenPy Playwright](https://screenpy-playwright-docs.readthedocs.io/en/latest/), and we need some help. With Selenium, i personally had a large, professional project to develop the extension with, so i feel that the [ScreenPy Selenium](https://screenpy-selenium-docs.readthedocs.io/en/latest/) extension is getting mature. But i don't have a similar project for Playwright.

If any of you have some time and interest, can you give some suggestions for Actions you would want to see in ScreenPy Playwright for it to cover your use cases? So far, there are only enough to be able to automate [this example test for SwagLabs.](https://github.com/ScreenPyHQ/screenpy_examples/blob/trunk/screenpy_playwright/swaglabs/features/test_cart.py#L15) 

We'd love to get your input! Also, is there a Playwright-specific subreddit? The only one i can find is for script-writing, you know, for theaters.",https://www.reddit.com/r/selenium/comments/ysivip/screenpy_playwright_v001_is_released_and_a_humble/,auto
1040,How can I automate tests for a whiteboard? The Chrome extension IDE recorder seems to record coordinates.,"I will soon need to automate some tests for a WebGL whiteboard (to draw and move objects), and I've been trying to practice on some sites that have examples of this (where you can move shapes around), and I noticed that the Selenium IDE Chrome extension recorder appears to track the coordinates. However, when I replay the recording (even after tinkering with what appear to be the coordinates), it fails to work.


Does anyone have experience with this who can share some advice on how to proceed?",https://www.reddit.com/r/selenium/comments/yrkbl2/how_can_i_automate_tests_for_a_whiteboard_the/,auto
1041,"Hey, scraping developers, I need your help!","Hey all, 

Are there any experienced scraping API’s tech-users (the tools like ScraperAPI, ScrapingBee, ScrapingBot, Zenrows, etc.)? Or just web scraping enthusiasts? I really need your help! 

My name is Alex, I am a scraping developer with a mission to build the best Proxy API tool out there (humble is not my way.)  So here is my project - [ScrapeIN’](https://scrapein.app/)  where I am trying to combine and automate the best practices for bypassing site protection and create all-in-one scraping infrastructure for any data engineer. 

I released the first MVP version of my Proxy API and want to make sure that it works as planned, so it would be awesome if you could help me out and test it for any issues and bugs. 

So to test my ScrapeIn you need to

1. Go [here](https://dashboard.scrapein.app/)
2. Register - it will allow you to use scraper for 14 days with 1000 credits. I can extend access on request if needed, just ping me here or in dms or by email. I don’t request credit card upon registration or anything, so don’t worry about the payment that supposedly should follow the trial😅
3. Look through our [API docs](https://dashboard.scrapein.app/docs) 
4. [Use ](https://dashboard.scrapein.app/)the API key given to you for scraping any public data from the web.  
5. [Use](https://dashboard.scrapein.app/query-builder) visual CSS selectors mode in order to extract the necessary data from a site accurately. 
6. Take and submit a short questionnaire Google [form](https://forms.gle/vbEaerevcoDjFNNc6).  
7. Enjoy increased ScrapeIN’ account balance by 1000 free credits! 

I really appreciate any of your feedback and thoughts about ScrapeIN’. Don’t hesitate to share with me any of your feedback in DMs or at support@scrapein.app.",https://www.reddit.com/r/selenium/comments/yrg70d/hey_scraping_developers_i_need_your_help/,auto
1042,Selenium IDE,"Selenium IDE is a free, easy-to-use browser automation tool that makes web application testing simple. It is an open source test automation tool that allows you to capture and replay online activity, which then translates into tests that can be rerun at any time. 

In order to construct Selenium test cases as a component of the Selenium suite, the Selenium IDE record & replay tool was released in 2006.

Install the extension (or add-on) for the relevant browser before beginning Selenium automation testing using Selenium IDE. Additionally, the IDE offers a GUI for documenting website interactions. 

Selenium IDE may now be used to test on Chrome browsers in addition to Firefox, which it was previously only accessible to test on. Cross-browser support and Selenium parallel testing are now supported by the IDE.",https://www.reddit.com/r/selenium/comments/yoj5gl/selenium_ide/,auto
1044,Is it possible to perform google authentication on a website without having to enter the password? Selenium always starts with a fresh session with not log ins or password saves,"I am writing an automation script for slack and currently I am using my email address and password to login. But for deploying this script (on a docker container), this is obviously not safe. The problem is that selenium starts with a completely fresh session every time the script is run. So, if I make the script click the google login button, I have to enter the google email address and password in order to log in. Is there some way to do this without password? Some google api or sdk probably?

&#x200B;

Pardon for the vague question. But I just haven't used anything like this before and am stuck on google authentication.

&#x200B;

Thank you in advance :)",https://www.reddit.com/r/selenium/comments/yidyvd/is_it_possible_to_perform_google_authentication/,auto
1045,"Chromium how to disable ""Save and autofill address""","Hello, recently I started having issues with my selenium autotests due to a chrome popup that appears when address form is being filled. From the options it can be manually disabled via Settings > Autofill > Addresses and more. My question is what is the chrome option name that can be used to disable this in my automation runs ?",https://www.reddit.com/r/selenium/comments/yicwf0/chromium_how_to_disable_save_and_autofill_address/,auto
1046,Packaging msedgedriver.exe with my app,"I have a small .Net app for automating a repetitive task I am required to do every day. After building and publishing the app in Visual Studio 2022, it leaves me with 2 files.

MyAutomation.exe
msedgedriver.exe

The msedgedriver.exe file has to be in the same directory as MyAutomation.exe, otherwise the Edge browser will not open. 

I'm using Edge since I know it will always be installed on a machine I'm using, not sure if this issue would be any different using Chrome.

Is there a reason this is not being packaged with MyAutomation into one executable file?",https://www.reddit.com/r/selenium/comments/yie13l/packaging_msedgedriverexe_with_my_app/,auto
1047,"Need help with making a generic Xpath within the Action (Helper location), that I can reuse on the web-application form page","Hi All,

&#x200B;

I’m new to automated testing with Selenium and have some questions regarding the following that I want to accomplish.

\-	I want to define a generic Xpath within the Action (Helper location), that I can reuse for several fields on the web-application form page.

\-	Basically I want to find the class element that I have written down in Step 2 and fill the k-textbox with a value by using the generic Xpath from Step 1.

&#x200B;

The below example is not working for me and I guess I have messed it up, would be nice If someone could provide me with an example. Thanks!

&#x200B;

Step1:

My current generic Xpath within the Action (Helper location), that I want use for several fields on the webpage form.

&#x200B;

Example what I got.

&#x200B;

public void SelectAndTypeEditFieldByLabelGeneric(string label, string searchtext)

{

TypeByXpath(""//\*\[contains(@class,'form-col') or contains(@class, 'form-input')\]//\*\[text()='"" + label + ""'\]/parent::div//input"", searchtext);

}

&#x200B;

Step 2:

From my steps location I fil in the input values with the reference toward the generic Xpath from step 1.

&#x200B;

Example what I got.

&#x200B;

\[When(@""I Fill In The Account Details"")\]

public void WhenIFillInTheAccountDetails(string label, string searchtext)

{

Action.SelectAndTypeEditFieldByLabelGeneric(""Bedrijfsnaam"", ""Test Company"");

Action.SelectAndTypeEditFieldByLabelGeneric(""Straat"", ""Test Street"");

Action.SelectAndTypeEditFieldByLabelGeneric(""Huisnummer"", ""999"");

    }  

Furthermore the When condition here has a relation with the SpecFlowFeature for the scenario.

&#x200B;

Scenario: Add A New Account

    When I Fill In The Account Detail

EDIT: I found the issue and solved the problem.

I changed: 

public void SelectAndTypeEditFieldByLabelGeneric(string label, string searchtext) With -> 'public void WhenIFillInTheAccountDetailsTest() .  

And within Action I changed the string searchtext with string value

public void SelectAndTypeEditFieldByLabelGeneric(string label, string searchtext)

{

TypeByXpath(""//\*\[contains(@class,'form-col') or contains(@class, 'form-input')\]//\*\[text()='"" + label + ""'\]/parent::div//input"", searchtext);

} ",https://www.reddit.com/r/selenium/comments/yfjv88/need_help_with_making_a_generic_xpath_within_the/,auto
1048,Website Blocking Selenium Input,"Some background: I have been working on a project for a while now that scrapes fares off Amtrak's site so a calendar view of fares can be seen at once. Initially, Amtrak would throw an error anytime I tried to make a search on the site, but adding the code below as an argument to options fixed that.

    ""--disable-blink-features=AutomationControlled""

Now, I am struggling with a much more challenging kind of error. Using the above code, I can access the site and perform searches. However, after making many consecutive searches (the number varies but around 5+), the site stops loading searches again for 10-20 minutes. What is particularly strange about this error is that Amtrak is not blocking my browser, if I manually enter the same information Selenium does through the webdriver browser the site loads fine. I have tried using the undetected\_chromedriver extension and altered my input to appear more human-like by entering phrases character by character, adding random delays between every action, and hovering over elements before clicking. Somehow, Amtrak is able to differentiate my human input from Selenium, and I have no idea how. I'd really appreciate any ideas for how to change my code to make the form input undetectable.",https://www.reddit.com/r/selenium/comments/yf9790/website_blocking_selenium_input/,auto
1049,Is it possible to automate clicking a browser extension?,"Hi.

I'm using Firefox on both my mobile phone and my PC and I have a tendency to open a lot of Facebook, 9gag, etc. tabs that contain videos on my mobile phone and then send the list of tabs to my PC where I open each link manually, click the VideoDownloadHelper addon on upper right, choose quality of video to download then swap over to next tab.

Is it possible to automate this with Selenium? The whole process would look like this:

1) Click VideoDownloadHelper
2) Click ""HLS streaming"" option in drowndown menu from VideoDownloadHelper with highest resolution
3) Ctrl+Tab to next tab
4) repeat until out of tabs


Is something like this possible with Selenium?",https://www.reddit.com/r/selenium/comments/yda3k6/is_it_possible_to_automate_clicking_a_browser/,auto
1050,Dealing with div number changes,"I am currently working in a automation of a kind of web form, but the it seems that the dibs numbers of the elements changes if I tried to retest my code (which means open the browser logging in, query the form and so on...). Here is an example of the  full xpath change:

1. 

/html/body/div[38]/div[2]/div/div[2]/div/div/div[1]/div[1]/div/table/thead/tr[2]/th[1]/span/span/span/input

2. 

/html/body/div[29]/div[2]/div/div[2]/div/div/div[1]/div[1]/div/table/thead/tr[2]/th[1]/span/span/span/input

If I try to use shorter version of xpath a get a ""grid number"" Id. 

Already tried to use contains and Starts with, but I got Not interactible element error.

Any thoughts?",https://www.reddit.com/r/selenium/comments/ycoqtv/dealing_with_div_number_changes/,auto
1051,"Absolute beginner to Selenium Java, can't figure this out","Hi everyone.

I'm applying for a new job and have to do a case due by tomorrow so I'm trying to learn Selenium Java.

I was following some guide on youtube and everything was smooth until I hit this absolute brick wall. So when this lady recorded this video a few months ago, the design of her AUT was different and the UI elements had ID's assigned to them.

&#x200B;

But the updated version of the site no longer has any UI elements with ID's. So now I have to figure this thing out by myself. I thought it might be good exercise and I'd tackle it in 5 minutes but I've been trying to give input to two textboxes and click one button for over two hours with no success. I'm about to absolutely lose my mind because all my efforts have been fruitless so far, I don't even know how many pages of stackoverflow I read or how many different xpath/CssSelector/whatever variations I tried. I just can't seem to get it. This ""minor inconvenience"" is driving me to the point of madness.

The site in question is linked below. Since it's a dummy site intended for testing, I don't think it really counts as advertising...

[https://opensource-demo.orangehrmlive.com/web/index.php/auth/login](https://opensource-demo.orangehrmlive.com/web/index.php/auth/login)

The username element:

<input class=""oxd-input oxd-input--active"" name=""username"" placeholder=""Username"" autofocus="""" data-v-844e87dc="""">    


Some of the things I tried (I tried god knows how many variations, just a few off the top of my head):

//input\[name='Username'\]

//input\[text='Username'\]

.// variations

..// variations

//Input\[text='Username'\]

//input\[placeholder='Username'\]

.//div/input\[text='Username'\]

//inputwpleıfowepfeprıogerg

Can. Someone. Click. The. Damn. Username. And. Password. And. Login. Fields?

Could somebody tell me what I'm failing to see in all this? I'll be eternally grateful if you could help me realize my error.

&#x200B;

Sorry for the rant, and thank you.",https://www.reddit.com/r/selenium/comments/yb33jv/absolute_beginner_to_selenium_java_cant_figure/,auto
1052,Connect Selenium to existing Chrome session,"Backstory - with atlassian's last update, Jira can no longer expand all comments on a ticket automatically.  Atlassian did this to ""fix"" a bug (one that doesn't affect us).  Now we need to click a bar, and then shift click another bar in order to see all of the comments on a ticket.  It's a giant pita.

What I'd like to do -  
Write a script that will interact with the web session (which is already loaded) and click/shift-click on the necessary bars. 

Is this something that selenium is capable of?",https://www.reddit.com/r/selenium/comments/y9vt26/connect_selenium_to_existing_chrome_session/,auto
1053,Need Help Trying to select Youtube password input,"Hi So, im trying to automate my login into youtube using Java. However when I am at the step to enter the password I am unable to Find the element by Class name, Xpath, Element name, input name etc. Can somebody help me I have no idea why Selenium refuses to allow me to select the password and sendKey(password). I would paste the code here but im havinig trouble pasting static html code.Because Most of the stuff is changing .  


  
Here is the link to login  
 [YouTube (google.com)](https://accounts.google.com/v3/signin/identifier?checkedDomains&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den%26next%3Dhttps%253A%252F%252Fwww.youtube.com%252F%253FthemeRefresh%253D1&dsh=S-838216074%3A1666168380688619&flowEntry=ServiceLogin&flowName=GlifWebSignIn&hl=en&ifkv=AQDHYWomsb0FTjE3dV8tSu_TswsHl8S6roC4pr_rK4XYyxecLpX3Yx5b7dLF9guxlFoVnqjvrfZh_Q&pstMsg=0&service=youtube) ",https://www.reddit.com/r/selenium/comments/y7x9r7/need_help_trying_to_select_youtube_password_input/,auto
1054,Help Locating Input Element,"I'm trying to locate and send keys to an an input element which is embedded in a div embedded in a label.  I've tried by ID, CLASS and various XPATHs, but with no luck.  Here is the HTML block in question, any help appreciated.  Thanks!

<label data-testid=""InputLoginValue-wrapper"" class=""input-text input-text-InputLoginValue"" for=""InputLoginValue"">

  <span data-testid=""InputLoginValue-title"" class=""field-name"">Username or Email Address</span>

  <div data-testid=""InputLoginValue-container"" class=""input-wrapper"">

<input class=""input-InputLoginValue"" type=""email"" autocomplete=""email"" autocorrect=""off"" spellcheck=""false"" tabindex=""0"" id=""InputLoginValue"" data-testid=""InputLoginValue"" placeholder=""Username or Email Address"" aria-invalid=""false"" aria-label=""Username or Email Address"" aria-required=""true"" required="""" value="""">

  </div>

  <div class=""error-container input-error input-error-email input-error-InputLoginValue"" style=""height: 0px;"">

<p class=""input-error input-error-email input-error-InputLoginValue"" id=""InputLoginValue-error"" data-testid=""InputLoginValue-error"" role=""alert""></p>

  </div>

</label>",https://www.reddit.com/r/selenium/comments/y7l8sg/help_locating_input_element/,auto
1055,Logging-in into my Google account using Selenium,"Hello, I am automating a daily task of mine and I'm required to log into my Google account in order to accomplish it. I tried using Python's Selenium module, but Google detects chromedriver and doesn't let me log-in. I was wondering if anyone has ever encountered this problem and if so how did you bypass it? Thank you guys in advance.",https://www.reddit.com/r/selenium/comments/y5ibgl/loggingin_into_my_google_account_using_selenium/,auto
1056,Selenium Timeout - Expected Condition returning False,"I'm coding a bot in Python that plays tic-tac-toe. The game is a Web  app written in React.js and is equipped with an AI of its own that  utilizes minimax. The user (which the Python bot simulates) is always X,  the AI is always O, and the user always moves first. The Python bot  plays by randomly selecting unmarked squares (this is only to  demonstrate automated UI testing).

**I was getting stuck inside a recursive function.**

    for i in clickedSquares:             
         if not winner:                 
              self.checkForWinner(load_browser, winner)                       
         elif i == random_square:                 
              self.test_playTTT(load_browser)             
         else:                 
              clickedSquares.append(random_square)

To fix this issue I added the **if not winner** condition  where ""winner"" is a string. This does terminate the loop; however, I'm  getting an error as soon as the checkForWinner() function is called  because winnerOh is always false. 

    winnerOh = WebDriverWait(load_browser, 10).until(EC.presence_of_element_located((By.XPATH, Tags.resultOh)))         
    
    winnerEx = WebDriverWait(load_browser, 10).until(EC.presence_of_element_located((By.XPATH, Tags.resultEx)))         
    
    tiedGame = WebDriverWait(load_browser, 10).until(EC.presence_of_element_located((By.XPATH, Tags.resultTie)))

 I'm looking for an element on the UI that declares the winner: X or O or  tie, which will only appear if the game is over.  So  WebDriverWait().until() is timing out waiting for that element to  appear, but it hasn't yet, because it's only the second move in the  game. 

I'm not sure how to fix this issue. If I remove the call to  checkForWinner() the bot will get stuck in the recursive call to  test\_playTTT(). The browser will not close after the game is over, and  the test will not end successfully.

**Is there another way to check the UI for the element I'm looking for that won't immediately return a False condition?  Is there a  better way for me to write this for loop?**

Linked is my post on StackOverflow with a full version of my Python method:

[https://stackoverflow.com/questions/74075172/selenium-timeout-expected-condition-returning-false](https://stackoverflow.com/questions/74075172/selenium-timeout-expected-condition-returning-false)

I'd appreciate any help.",https://www.reddit.com/r/selenium/comments/y47p05/selenium_timeout_expected_condition_returning/,auto
1057,Testing Tool," Hi All!

I wanted to inform the community of a service that makes it easier for automation test cases that involve email and sms validation such as MFA (multi factor authentication) They offer email and sms API’s that make it a breeze. They have free for ever accounts that are limited to emails only. Check them out at [swiftpigeon.io](http://swiftpigeon.io/)",https://www.reddit.com/r/selenium/comments/y2kjwb/testing_tool/,auto
1058,OOM chrome error,"Hi,

We are getting chrome OOM error while running selenium automation script through Jenkins.

And we tried replacing VMs as well but same OOM error there as well, even we updated the chrome version in VM but no luck.

Can anyone please help me out.

Thanks",https://www.reddit.com/r/selenium/comments/y1763z/oom_chrome_error/,auto
1059,Has anyone gotten webdriver-auto-update package to work? It seems like no matter what I do it can't find my chromedriver.exe.,"[`https://pypi.org/project/webdriver-auto-update/`](https://pypi.org/project/webdriver-auto-update/)

Code in this package where it is messing up:

        try:
            # Executes cmd line entry to check for existing web-driver version locally
            os.chdir(driver_directory)
            cmd_run = subprocess.run(""chromedriver --version"",
                                     capture_output=True,
                                     text=True)     
        except FileNotFoundError:
            os.chdir("".."")
            # Handling case if chromedriver not found in path
            print(""No chromedriver executable found in specified path\n"")
            download_latest_version(online_driver_version, driver_directory)

Every time it goes into the except because it can't find the chromedriver.exe... 

&#x200B;

Any suggestions? Could someone show an example of this code working?",https://www.reddit.com/r/selenium/comments/y19c3z/has_anyone_gotten_webdriverautoupdate_package_to/,auto
1060,proxy alternative?,"I'm trying to run an automation using selenium the problem is that the offers on the website are geo restricted, I wanted to go the proxy route but most of the free ones has problems, I was thinking VPN but as far as I know chrome doesn't have that option, I'm running on a VPS and making the whole system use a vpn is another mess, any suggestions?",https://www.reddit.com/r/selenium/comments/xwn6uf/proxy_alternative/,auto
1063,Error message that just doesn’t have a solution,"Hello again everyone!

I have been running this software to automate basic tasks for a while now but it seems that it won't work as it is supposed to anymore. Whenever I run the script, I get these error messages and the script just stops running:


\[70808:73532:0927/133844.457:ERROR:cert\_issuer\_source\_aia.cc(34)\] Error parsing cert retrieved from AIA (as DER):

ERROR: Couldn't read tbsCertificate as SEQUENCE

ERROR: Failed parsing Certificate


I tried looking this up on a few different forums but no-one seemed to know exactly what is happening. It's worth nothing that the target website's UI has changed BUT I have run the script successfully with the new UI.

Any ideas on what could cause this?",https://www.reddit.com/r/selenium/comments/xpgx4o/error_message_that_just_doesnt_have_a_solution/,auto
1064,Stale Element Not Found error,"Hi everyone,

I'm trying to automate some stuff in Selenium on a Single Page Application type of website.
As per action chain: the robot will accept cookies and then click on a CTA that redirects to another page. Being a single page application, CTRL+click is not working, so it will open in the same tab.

After the action chain, the script return a Stale element not found error. I tried recalling the find_element command after the action chain with a new XPATH that can be found in the new page, but the error still persists. 

Can anyone help me with some ideas regarding this? I tried so many workarounds and nothing is working...
I pasted the code in the comments section.
Thank you!",https://www.reddit.com/r/selenium/comments/xpgeuh/stale_element_not_found_error/,auto
1065,What do you guys think about using selenium vs karate for ui automation?,What do you guys think about using selenium vs karate for ui automation? Looking for good and bad for both. TIA.,https://www.reddit.com/r/selenium/comments/xp4abp/what_do_you_guys_think_about_using_selenium_vs/,auto
1066,How do I exit a Price Check Loop once desired or lower price is met and commit to purchase?,"Please forgive my crappy code as this is my first attempt to code overall. I'm trying to to automate an amazon purchase when the right set price hits in the code compared to to current amazon price. It seems that it just goes into infinite loop rather than refresh the page and execute once the set price or lower hits. Is there an easier way to code it?

&#x200B;

    
    while (amazon_price) >= int(buy_price):
        print(amazon_price)
        print('do not buy')
        random_wait_time = random.randrange(8.0, 20.0)
        print(random_wait_time)
        time.sleep(random_wait_time)
        wd.refresh()
    else:
        add_to_cart_button = wd.find_element_by_xpath('//*[@id=""a-autoid-2-offer-1""]/span/input')
        add_to_cart_button.click()
    
    view_cart_button = wd.find_element_by_xpath('//*[@id=""aod-offer-view-cart-1""]/span/input')
    
    view_cart_button.click()",https://www.reddit.com/r/selenium/comments/xm6pde/how_do_i_exit_a_price_check_loop_once_desired_or/,auto
1067,Scarping a website which shows data after Logging in and has also 2FA in place,"I am very new to scraping (almost zero knowledge) and have a task at hand which will need automation. As given in the title I need to scrap a few thousand of records which are at a website where I have to login and go through 2FA, put in the search parameter to see this data, the search parameters are going to change through the dropdown list. All I know yet is that I have to use Selenium to automate the process.

Can some one guide me into this? I will be really grateful and put up the code for everyone's use once the job is done!",https://www.reddit.com/r/selenium/comments/xjwb8m/scarping_a_website_which_shows_data_after_logging/,auto
1068,Basic framework for WebDriver in C# (need feedback...),"Hi, Folks!

I'm trying to create a lightweight framework for using Selenium WebDriver in C#.  I am at a crossroads where I need some feedback.  I am not sure if I want to make it into a NuGet package or just leave it open on GitHub.

I would like to:
1) Get some advice on adding an automatic execution log utilizing a package like NLog.
2) Receive feedback whether this framework is even a descent idea or not.  I think it is but I'm biased!

The code can be found at: https://github.com/vasagle-gleblu/Element34

Please be gentle!",https://www.reddit.com/r/selenium/comments/xjd781/basic_framework_for_webdriver_in_c_need_feedback/,auto
1069,"Chrome, Linux, headless, using client certificates"," 

I am having a problem passing my test user's PKI certificates in the headless mode. I am using Java Selenium WebDriver 4.3.0. When I run my test suite in normal mode, my profile and certificates are picked up perfectly. Profile users are selected by the ChromeOptions class by identifying the --user-data-dir= . I have different profiles for each of my test users. Then the certificate is selected by the policy setting (i.e, AutoSelectCertificateForUrls). That also works perfectly. As I navigate to different URL locations my test certificates are presented and accepted correctly when I run in the normal mode.

When I change the mode to Headless=true (i.e., ChromeOptions.addArguents(""--headless""), it all falls apart and no certificate is presented when I open a Chrome browser and hit any webpage.

I found that Firefox was extremely simple to manage profiles and PKI test certificates!!! When a test runs in normal mode and works perfectly, all I have to do is set the FirefoxOptions.addCommandLineOptions(""--headless""); and it still works perfectly in the headless mode. Not so with Chrome!!!

Does anyone know the correct solution? I could use the information. I am really stuck here.... Is there a way to still make Chrome present PKI certificates in headless mode or does anyone know that this feature really does not work for Chrome/Chromium? Then I could stop wasting my time!

Thanks in advance for your help!",https://www.reddit.com/r/selenium/comments/xjgz2a/chrome_linux_headless_using_client_certificates/,auto
1072,SQL injection with selenium,"Hi all, how do we approach sql injection automation  testing with selenium? Are there any best practices that you followed in your project?",https://www.reddit.com/r/selenium/comments/xi9oku/sql_injection_with_selenium/,auto
1073,Selenium_Web_Driver :- Chrome_Ran_Out_Of Memory,"In python I am using selenium web driver.

In the code i am logging in into a certain websites and i am download some specific reports and now I have automated this process. 

Earlier the code was working fine but now I am getting error in chrome

&#x200B;

Aww Snap

Something went wrong while displaying this webpage

Error Code : Out of memory

&#x200B;

Can someone help me out please.",https://www.reddit.com/r/selenium/comments/xhcuii/selenium_web_driver_chrome_ran_out_of_memory/,auto
1074,How do I post a question?,"Hello my fellow Selenium WebDriver coders.  I have been trying to post a question for 2 days and they all get deleted by AutoModerator.  Please, how do I get my questions posted?",https://www.reddit.com/r/selenium/comments/xfxas4/how_do_i_post_a_question/,auto
1075,Skipping HCaptcha,"Hello everyone, I have a python script that was skipping a HCaptcha beautifully until a week ago and then all of sudden it stoped working. 

To skip that I was using:

chrome_options.add_argument('--disable-blink-features=AutomationControlled')

Does anyone know if anything has changed? Also does anyone know how to keep skipping that?",https://www.reddit.com/r/selenium/comments/xdhj52/skipping_hcaptcha/,auto
1078,How can I run my selenium code again and again ?,Recently I was automating a website and i wanted to run my code with different data input that was in an excel sheet which was quite long with 2k-3k values . As a beginner I tried using for loop but some error comes after running the code for 20-30 times . Is there a better way to do this task ?,https://www.reddit.com/r/selenium/comments/xbc0ls/how_can_i_run_my_selenium_code_again_and_again/,auto
1079,ELI5 How do I encrypt/decrypt my password in Selenium ruby,"Ok, so I need to submit an automation test case for review. I've built the test case in Selenium ruby using RSpec. The test case has a plain text password in it and I'd like to have it encrypted and then decrypted when it's passed into the password field.   How would I do that, I've been searching all night and the answers I'm coming across are not very clear and I could use some community help. Is there a gem or something that I need that I haven't found?",https://www.reddit.com/r/selenium/comments/x9ol2z/eli5_how_do_i_encryptdecrypt_my_password_in/,auto
1080,Trigger selenium scripts automatically,"I made a selenium script in chrome that logs in and downloads a file but I need it to trigger automatically at a specific time.  I wrote a little script in autohotkey that can do the timing part but i don't know how to trigger the automation.  I've tried exporting the selenium script but I'm not sure what to do with it at that point.  Is there some command line arguments i can use that i haven't found yet?

It would be really cool if there was an option to export as a standalone executable.",https://www.reddit.com/r/selenium/comments/x7qpr2/trigger_selenium_scripts_automatically/,auto
1081,Can Selenium be used to test UWP applications?,"I have a UWP Point of Sale application made with C# and Xamarin and I want to create automated testing as the application is getting more and more complex.  Is Selenium a possible solution, and if not if someone could point me in the right direction I would appreciate that.

Thank you!",https://www.reddit.com/r/selenium/comments/x7jsud/can_selenium_be_used_to_test_uwp_applications/,auto
1082,Error when address is not found,"I'm trying to get Lat and Long coordinates, but when the address is not found the script stops with the following error:

&#x200B;

>*NameError                                 Traceback (most recent call last) /var/folders/tf/lvpv9kg11k14drq\_pv1f6w5m0000gn/T/ipykernel\_34533/135469273.py in <cell line: 31>()      51         lat = latlong\[0\]*       
>  
>*52         long = latlong\[1\] --->*   
>  
>*53 sheet\[i\]\[6\].value = lat.replace(""."","","")*        
>  
>*54     sheet\[i\]\[7\].value = long.replace(""."","","")*        
>  
>*55 # Caso não localize o endereço, serã informado na célula do excel NameError: name 'lat' is not defined*

&#x200B;

script:

`import time  # gerencia o tempo do script`

`import os  # lida com o sistema operacional da máquina`

`from selenium import webdriver  # importa o Selenium para manipulação Web`

`from tkinter import Tk  # GUI para selecionar arquivo que eu desejo`

`from tkinter.filedialog import askopenfilename`

`from tkinter import messagebox as tkMessageBox`

`from openpyxl import load_workbook, workbook  # biblioteca que lida com o excel`

`import re`

&#x200B;

`# escolha o arquivo em excel que você deseja trabalhar`

`root = Tk()  # Inicia uma GUI externa`

`excel_file = askopenfilename()  # Abre uma busca do arquivo que você deseja importar`

`root.destroy()`

&#x200B;

`usuario_os = os.getlogin()`

&#x200B;

`chromedriver = ""/usr/local/bin/chromedriver""  # local onde está o seu arquivo chromedriver`

`capabilities = {'chromeOptions': {'useAutomationExtension': False,`

`'args': ['--disable-extensions']}}`

`driver =` [`webdriver.Chrome`](https://webdriver.Chrome)`(chromedriver, desired_capabilities=capabilities)`

`driver.implicitly_wait(30)`

`# iniciando a busca WEB`

`driver.maximize_window()  # maximiza a janela do chrome`

`driver.get(""`[`https://www.google.com/maps`](https://www.google.com/maps)`"")  # acessa o googlemaps`

`time.sleep(5)`

 

`# importando o arquivo excel a ser utilizado`

`book = load_workbook(excel_file)  # abre o arquivo excel que será utilizado para cadastro`

`sheet = book[""Coordenadas""]  # seleciona a sheet chamada ""Coordenadas""`

`i = 2  # aqui indica começará da segunda linha do excel, ou seja, pulará o cabeçalho`

`for r in sheet.rows:`

&#x200B;

`endereco = sheet[i][1]`

`munic_UF = sheet[i][3]`

&#x200B;

`if str(type(endereco.value)) == ""<class 'NoneType'>"":`

`break`

&#x200B;

`endereco_completo = endereco.value + "" "" + munic_UF.value`

&#x200B;

`#preenche com o endereço completo e aperta o botão buscar`

`driver.find_element(""id"",""searchboxinput"").send_keys(endereco_completo)`

`driver.find_element(""id"",""searchbox-searchbutton"").click()`

&#x200B;

`# Aguarda carregar a URL e coleta os dados das coordenadas geográficas`

`time.sleep(5)`

`url = driver.current_url`

`latlong = re.search('@(.+?)17z', url)`

`if latlong:`

`latlong =` [`latlong.group`](https://latlong.group)`(1).rsplit("","", 2)`

`lat = latlong[0]`

`long = latlong[1]`

`sheet[i][6].value = lat.replace(""."","","")`

`sheet[i][7].value = long.replace(""."","","")`

`# Caso não localize o endereço, serã informado na célula do excel`

`if lat == None:`

`sheet[i][6].value= ""Nao foi possivel identificar""`

&#x200B;

`# Limpa os campos para nova busca de coordenadas`

`driver.find_element(""id"",""searchboxinput"").clear()`

`lat=""""`

`long=""""`

&#x200B;

`i += 1`

&#x200B;

`# salva o excel na área de trabalho`

`caminho_arquivo = os.path.join(""Resultado_final.xlsx"")`

[`book.save`](https://book.save)`(caminho_arquivo)`

&#x200B;

`# Avisa sobre a finalização do robô e encerra o script`

`window = Tk()`

`window.wm_withdraw()`

`tkMessageBox.showinfo(title=""Aviso"", message=""Script finalizado! Arquivo salvo na pasta Documentos!"")`

`window.destroy()`

`driver.close()`

&#x200B;

Could someone help me to keep running and ignore the error?",https://www.reddit.com/r/selenium/comments/x7ackf/error_when_address_is_not_found/,auto
1083,Error messsage that just doesn't have any solutions,"I have a selenium script running daily to automate certain tasks but for a few days it has stopped midway though and returned this error message when trying to write down a message on a text field:

&#x200B;

\[45128:40772:0904/162815.839:ERROR:interface\_endpoint\_client.cc(665)\] Message 0 rejected by interface blink.mojom.WidgetHost

&#x200B;

What is this related to? I have tried looking through the internet for a solution but no-one seems to have an answer.",https://www.reddit.com/r/selenium/comments/x5n3gj/error_messsage_that_just_doesnt_have_any_solutions/,auto
1084,Facebook Automation - can it b e done?,"Hi everyone   


I am looking for a way to add friends (who have mutual friends) on Facebook and send them a message seeing if they are interested in our products/services. I am doing this process manually but it takes a long time, and I know an automated way would be a great way to free up time.  


Is this possible to be done with python/selenium?  


Thanks in advance",https://www.reddit.com/r/selenium/comments/x3qbjt/facebook_automation_can_it_b_e_done/,auto
1086,Recommendation for open source Selenium Java library for automating tests?,"Hi, test automators 😊

Could you please share with me - the best  open source Selenium Java library.

My background is with Ruby 💎 where is available an open-source Ruby library for automating web browsers - [Watir](http://watir.com/)

Many thanks in advance!",https://www.reddit.com/r/selenium/comments/x1cthu/recommendation_for_open_source_selenium_java/,auto
1088,How to click on a word open to a hidden item?,"Hi guys, I tried to do an automation for a webpage. Forgive me because I am really new to this. There is a word 'Spend' that click in open a hidden item call 'My Dashboard'. Here is the code below.

<li id=""menu1"">

<a class="""" name="""" title=""Spend"" style=""font-size: xx-small; color: white; font-family: Arial,Helvetica,sans-serif; font-size: 9pt; font-weight: bolder;"" id="""" target="""" disabledby="""" disabledvalue="""" href=""#null"" enabledhref=""#null"" disabledhref=""""><span datasrc="""" datafld="""" dataformatas=""HTML"">

Spend</span></a>

<ul style=""display: block;"">

<li>

<a class="""" name="""" title=""My Dashboard"" style=""color: black; font-family: Arial, Helvetica, sans-serif; font-size: 8pt; background: rgb(255, 255, 204);"" id="""" target=""mainFrame"" disabledby="""" disabledvalue="""" href=""../common/submenu\_tabs.cfm?parentid=12663\&amp;\_Key=6482AB90889D539090861088310A837FC6E35781FC48AB069D9BCE06F9CA94"" enabledhref=""../common/submenu\_tabs.cfm?parentid=12663\&amp;\_Key=6482AB90889D539090861088310A837FC6E35781FC48AB069D9BCE06F9CA94"" disabledhref="""" onclick=""SelectItem(this, '12663', false);""><span datasrc="""" datafld="""" dataformatas=""HTML"">

My Dashboard</span></a>

</li>

</ul>

</li>

here is my code which login in.

button = browser.find\_element([By.ID](https://By.ID),""UserName"").send\_keys(username)

button1 = browser.find\_element([By.ID](https://By.ID),""Password"").send\_keys(password)

time.sleep(1)

button2 = browser.find\_element([By.ID](https://By.ID),""ext-gen36"")

[button2.click](https://button2.click)()

time.sleep(5)

link = browser.find\_element(By.XPATH,""//form\[@id='main'\]"")

[link.click](https://link.click)()

I am really appreciate for the help.",https://www.reddit.com/r/selenium/comments/wwnxhs/how_to_click_on_a_word_open_to_a_hidden_item/,auto
1089,Need help,"Hey everyone,

Recently I have wanted to learn how to code and automate web applications through selenium. Would anyone be able to provide me with a tutorial that shows me how to install the proper python library and web driver for google chrome?",https://www.reddit.com/r/selenium/comments/wrx6ln/need_help/,auto
1090,timeout exception issue,"Ho guys I am trying to automate the download of edx course and I want to iterate all video clicking on next button and download each video but the click get me no such element so i use webdriverwait but throw me timeout exception how can i resolve this?

Edit. I manager ti solve the problem using execute_script webdriver metod and selcting the button with JavaScript code",https://www.reddit.com/r/selenium/comments/wrj0gh/timeout_exception_issue/,auto
1091,Automatic update of chromedriver,"Hi, I use selenium to download latest chrome and firefox and then I auto update them. Unfortunately, when the Chrome is updated to a new version, a new chromedriver is requied for selenium to work with chrome. How do you guys solve this? My current idea is to download the newest version and store it, then have a check for what version my chrome is so when the stored chromedriver and chrome have the same versions I replace my current chromedriver with the stored one. Anyone who handles this in a smoother way?

Edit: This solves this problem: https://pypi.org/project/webdriver-manager/",https://www.reddit.com/r/selenium/comments/woswjj/automatic_update_of_chromedriver/,auto
1092,why is selenium chrome webdriver running with high data usage ?,"It's for Gmail auto login after running for 2 days it consumed **130GB of my quota!**, any suggestions for adding a line or disabling something to reduce that enormous **data usage**?",https://www.reddit.com/r/selenium/comments/wnorw9/why_is_selenium_chrome_webdriver_running_with/,auto
1093,Can I use Selendroid to automate a test in the official Instagram app?,"Hi community. I´m starting to read about Selendroid and I don´t get if it can be used for automate a test in, for example, the official Instagram app. (As long as I understand it can only be used with apps you are developing... or am I wrong ?)

Thank you !",https://www.reddit.com/r/selenium/comments/wn9qtn/can_i_use_selendroid_to_automate_a_test_in_the/,auto
1095,Selenium opening blank tab with 429 error when only making on request,"When I open this URL with webdriver in selenium, I get a blank page with a 429 request. I haven't sent too many request as I only do one and it doesn't work. I've tried multiple solutions but can't manage to do it. Any input would be appreciated. Here is my code:

`from selenium import webdriver  options = webdriver.ChromeOptions()  options.add_argument(""start-maximized"") # to supress the error messages/logs options.add_experimental_option('excludeSwitches', ['enable-logging']) options.add_experimental_option(""excludeSwitches"", [""enable-automation""]) options.add_argument(""disable-blink-features=AutomationControlled"") options.add_experimental_option('useAutomationExtension', False)  driver = webdriver.Chrome(options=options, executable_path=r""C:\\Users\\pople\\OneDrive\\Desktop\\chromedriver.exe"") driver.get('https://www.bluenile.com/diamond-search')`",https://www.reddit.com/r/selenium/comments/wifw8h/selenium_opening_blank_tab_with_429_error_when/,auto
1096,selenium firefox extensions,"hello,

 i have recently started using selenium it's great for automation.

what are the recommended extensions for firefox for a better selenium experience ,

also for vscode

and mainly is there an extension to find the unique tags(id,name,...) of  a certain object because it's taking too much time and sometimes i find it hard to find  the uniqueness in an object.",https://www.reddit.com/r/selenium/comments/wgp2zd/selenium_firefox_extensions/,auto
1097,How will ChatGPT affect web scraping?,It’s a pretty open ended question because I just want to know the general opinions of web scrapers on ChatGPT,https://www.reddit.com/r/webscraping/comments/100scxg/how_will_chatgpt_affect_web_scraping/,GPT
1106,Using AI GPT J to extract company names from news headlines,"I'm gonna show you how I'm extracting company names from a headline using GPT J, Make and Google Sheets. This would be particularly useful if you're scraping some headlines from the internet, like news articles or blog articles, and you want to build a contact list based on the company name.  


[https://www.youtube.com/watch?v=2N9UqJ68clQ](https://www.youtube.com/watch?v=2N9UqJ68clQ)",https://www.reddit.com/r/Automate/comments/ysb45a/using_ai_gpt_j_to_extract_company_names_from_news/,GPT
1110,Help with code to web scrape tripadvisor,"Hello! I have had help building this chunk of code but from this point on, I have no clue how to add the pagination. My goal is to gather the email data of all the hoteils on this link :  
[https://www.tripadvisor.pt/Hotels-g189100-Portugal-Hotels.html](https://www.tripadvisor.pt/Hotels-g189100-Portugal-Hotels.html)  


So, for example, if I wanted to scrape the info of the 3 first pages, i would print the name of the hotels and their respective email. 

Any piece of advice? I wouldn´t mind paying if I could get some help :)",https://www.reddit.com/r/webscraping/comments/10fk2ov/help_with_code_to_web_scrape_tripadvisor/,AI
1112,Need help completing this project.,"I am a newbie with web scrapping and I need help in completing this project.

I am trying to get the number form the span class that has the last ""star fill"" This is my desired output  seatcomfort  = 3 cabinstaffservice = 5 inflight = 5 FoodBeverages = 5 GroundService = 4 Valueformoney = 4 wifi = 5 

    seatcomfort = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Seat Comfort"")) td.review-rating-stars.stars, span.star fill')    
    
    cabinstaffservice = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Cabin Staff Service"")) td.review-rating-stars.stars, span.star fill')
    
    inflight = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Inflight Entertainment"")) td.review-rating-stars.stars, span.star fill')
    
    FoodBeverages = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Food & Beverages"")) td.review-rating-stars.stars, span.star fill')
    
    GroundService = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Ground Service"")) td.review-rating-stars.stars, span.star fill')
    
    Valueformoney  = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Value For Money"")) td.review-rating-stars.stars, span.star fill')
    
    wifi  = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Wifi & Connectivity"")) td.review-rating-stars.stars, span.star fill')
    --------------------------------------------------------------------------------
    #output 
    
    seatcomfort= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star"">4</span><span class=""star"">5</span></td>
    
    Cabinservice= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star fill"">5</span></td>
    
    inflight= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star fill"">5</span></td>
    
    foodbeverages= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star fill"">5</span></td>
    
    Groundservice= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star"">5</span></td>
    
    Valueformoney= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star"">5</span></td>
    
    wifi= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star fill"">5</span></td>",https://www.reddit.com/r/webscraping/comments/10f8rjk/need_help_completing_this_project/,AI
1114,How to launch a scrapy spider from a script in a Thread(),"Hi evryone,

I am using scrapy and I would like to start my spider from a script without blocking the process while scraping. Basically I have a little GUI with a start button, I don't want the window to be frozen when I press the start button, because I also want a Stop button to be able to interrupt a scrap if needed without terminating the process manually with Ctrl-C .

I tried to thread like this:

    def launch_spider(self, key_word_list, number_of_page):
            spider = SpiderWallpaper()
            process = CrawlerProcess(get_project_settings())
            process.crawl('SpiderWallpaper', keywords = key_word_list, pages = number_of_page)
    // if i use process.start() directly the main process is frozen waiting for the
    // scraping to complete so :
            mythread = Thread(target = process.start)
            mythread.start()
    output:
        Traceback (most recent call last):
      File ""/usr/lib/python3.10/threading.py"", line 1016, in _bootstrap_inner
        self.run()
      File ""/usr/lib/python3.10/threading.py"", line 953, in run
        self._target(*self._args, **self._kwargs)
      File ""/home/***/.local/lib/python3.10/site-packages/scrapy/crawler.py"", line 356, in start
        install_shutdown_handlers(self._signal_shutdown)
      File ""/home/***/.local/lib/python3.10/site-packages/scrapy/utils/ossignal.py"", line 19, in install_shutdown_handlers
        reactor._handleSignals()
      File ""/usr/lib/python3.10/site-packages/twisted/internet/posixbase.py"", line 142, in _handleSignals
        _SignalReactorMixin._handleSignals(self)
      File ""/usr/lib/python3.10/site-packages/twisted/internet/base.py"", line 1282, in _handleSignals
        signal.signal(signal.SIGTERM, reactorBaseSelf.sigTerm)
      File ""/usr/lib/python3.10/signal.py"", line 56, in signal
        handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
    
    ValueError: signal only works in main thread of the main interpreter
    

I saw this [snippet](https://snipplr.com/snippet/67015/revisions) which seems interesting but the import doesn't work with the latest version of scrapy.

Any ideas on how to manage the signal to be used in a subThread ?",https://www.reddit.com/r/webscraping/comments/10enkwt/how_to_launch_a_scrapy_spider_from_a_script_in_a/,AI
1115,"Between Stable Diffusion and ChatGPT, its clear blocking bots only stop the small guy.","We always knew this was the truth, but it doesnt get more 'in your face' than seeing AI art and ideas scraped from websites with anti-scraping technology. 

Its a good thing they defeated the anti-bot, because if they didn't we wouldn't have 2 new and useful technologies.

But why does wealth decide who gets access to public facing data?",https://www.reddit.com/r/webscraping/comments/10egpxv/between_stable_diffusion_and_chatgpt_its_clear/,AI
1116,Hosting web scraped data on vercel?,"Hello, I created a web scraper using playwright that scrapes several sneaker sites and puts it into a json format using node for an api. But, I keep getting error 500 function invocation failed. It works fine in local.   


This is what some of my logs look like:

    [GET] /
    10:55:15:55
    Please run the following command to download new browsers:              ║"",""║                                                                         ║"",""║     npx playwright install                                              ║"",""║                                                                         ║"",""║ <3 Playwright Team                                                      ║"",""╚═════════════════════════════════════════════════════════════════════════╝"",""    at /var/task/index.js:24:34"",""    at Layer.handle [as handle_request]

Is it not normal to use playwright like this? I understand playwright is usually used for integration testing. Should I be sending the data scraped to a database first and then pull data from there for the api.  Totally new to building my own api , so this is all learning process if someone can point me in the right direction <3.",https://www.reddit.com/r/webscraping/comments/10do4wp/hosting_web_scraped_data_on_vercel/,AI
1117,Mayday! Mayday! Impossible site to webscrape!? REWARD for solution!,"I have no idea how to scrape [these](https://ibb.co/HFNbw3j) (IV) numbers from [this](https://marketchameleon.com/Overview/IP/IV/) website. It contains stock numbers and I have tried to scrape them off the website for eons. And although I have made advancements I am yet to actually scrape them. I am a noobie at web scraping and coding overall but maybe you guys know how to obtain them. I am coding in python >3 and I have figure that the site is ajax-based since it wouldn’t work with a me want me get requests. PS A loggin shouldn’t be required since the numbers still show up in inspect although not being logged in.

[This](https://textdoc.co/pnuygOileDQf6YV1) is the script I am using at the moment. But it wouldn’t give me the numbers.

I have googled and apparently using a chrome driver should fix the Ajax problem. Therefore I have tried using [this](https://textdoc.co/BoglTirm1SA3Mt5R) script. But it doesn't work for whatever reason. And I have no clue how to fix it.

I’ll reward you who finds a python code that can obtain these numbers for me and put them in a list or string. Have a great new year everybody!!!!",https://www.reddit.com/r/webscraping/comments/10d881i/mayday_mayday_impossible_site_to_webscrape_reward/,AI
1118,How hard is it to scrape that?,"Hello guys! I'm very new here, so excuse me if my question is stupid, but I'm trying to figure out how hard it is to scrape the following data and send it to something like google sheets.

I have a WooCommerce website, and I need to extract a specific page from the WooCommerce Analytics tab on an hourly/daily basis. [Here is a link](https://imgur.com/UHthsj3)

P.S. (Using the native WooCommerce API is NOT an option)

Thanks!",https://www.reddit.com/r/webscraping/comments/10cyco7/how_hard_is_it_to_scrape_that/,AI
1119,can someone guide me about how to setup scrapy project,"I am starting a project where I have to scrape around 50 to 60 news website for live and current news. I am currently making individual scripts for now as all the websites have different tags for their article information like headline, date and article body. How do I go about setting a single project where all the scripts will run simultaneously, and factors that I should keep in mind. 

Like - As all websits have different tags, I don't think item loaders will be of help as I have to consider all the scenario where I have to perform a check on all tags related to that information i am about to extract.

I am fairly new at scrapy so any idea about how I would  go about it would be appreciated.",https://www.reddit.com/r/webscraping/comments/10cm3ox/can_someone_guide_me_about_how_to_setup_scrapy/,AI
1121,Where can I find a mentor?,"I find myself hitting a ceiling a lot and I don't know where to go next in order to become more advanced. Currently, I'm maybe at the intermediate level with an elementary knowledge of things such as browser fingerprinting, bypassing bot protection, captcha management, networking, and general web development.

What I'm really struggling with is not knowing what I don't know and not knowing how to find out what I don't know. Instinctually, I wanted to learn networking and back-end development at a deeper level as I believe that would at least give me an idea of what sort of information I'm sending to a website and what a website can do with that information. Then be able to reverse engineer that information somehow to at least give myself a better chance. But then again, it goes back to not knowing what I don't know.

Either way, any pointers would be greatly appreciated. I have a lot of really good ideas for projects that keep being put on hold because of the web scraping hangups so now is the time for me to really step up and put my money where my mouth is.",https://www.reddit.com/r/webscraping/comments/10blltx/where_can_i_find_a_mentor/,AI
1122,"How to use ""for"" to ""loop"" multiple urls ?","The current code is perfect to scrape the information for only one Url, and i would like to be able to scrape from multiple urls at once ( maybe use For url in Urls ) , i would like to add this ( URL : [https://www.6pm.com/p/gbg-los-angeles-ayvie-slate/product/9479982/color/642?zlfid=192&ref=pd\_detail\_1\_sims\_cv](https://www.6pm.com/p/gbg-los-angeles-ayvie-slate/product/9479982/color/642?zlfid=192&ref=pd_detail_1_sims_cv)). Here is the current code for just one url below. Please any help or direction would be appreciated

 

`import datetime`  
 `from bs4 import BeautifulSoup`  
 `import requests`

`def get_url_data_from_url_request(url):`  
  `print("">> get_url_data_from_url_request: ""+str(url))`

   `url_data = None`  
  `headers = {""user-agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML,`   
 `like Gecko) Chrome/90.0.4430.93 Safari/537.36""}`  
  `s = requests.session()`  
  `s.keep_alive = False`  
  `request = s.get(url, proxies=None, headers=headers)`  
  `print(""request.status_code: "", request.status_code )`  
  `url_data = request.text`  
  `request.connection.close()`  
  `s.close()`  
  `return url_data`

`def main():`  
  `print(""bdr.sandbox"")`  
  `generated_on = datetime.datetime.now()`  
  `print(generated_on)`  
  `source_product_url = ""`[`https://www.6pm.com/p/easy-spirit-epic-gray/product/9450972/color/11`](https://www.6pm.com/p/easy-spirit-epic-gray/product/9450972/color/11)`""`  
  `url_data = get_url_data_from_url_request(url=source_product_url)`  
  `soup = BeautifulSoup(url_data, ""lxml"")`  
  `id_element = soup.find('span', {""itemprop"": ""sku""}).text`  
  `print(id_element)`

`if __name__ == '__main__':`  
 `main()`",https://www.reddit.com/r/webscraping/comments/10bz75l/how_to_use_for_to_loop_multiple_urls/,AI
1123,Is it possible to create a script like this?,"[Mountainproject.com](https://Mountainproject.com) has data for 279,958 climbing routes. When searching for routes, you can modify based on location, climbing type, difficulty rating (as a range), and quality rating. At the results page, it gives you the option to export the first 1,000 results into an excel sheet.  


I'm interested in getting all of the route data together, but in order to do that I would need to create at least 280 separate results files, each with no more than 1,000 rows. I want to create a ""results checker"" script that runs an array of searches and reports the results for each one. For example, there are 36 different options for climbing difficult grades. I could have the script run 36 times and spit out how many search results there are for each grade. From there, I could get a sense of which grades I would have to further subdivide. The lowest difficulty grade only has 277 routes, so that would work, but there are quite a few with over 1,000 results.  


Is it possible to have a script do something like that?",https://www.reddit.com/r/webscraping/comments/10aixb5/is_it_possible_to_create_a_script_like_this/,AI
1124,Simple Scraper worked for me before but now it seems to be driving me mad....,"I'm trying to scrap this page ([https://elgl.org/celebrate-90-of-elgls-top-100-local-government-influencers-for-2022/](https://elgl.org/celebrate-90-of-elgls-top-100-local-government-influencers-for-2022/)) to create a base in Airtable. Simple Scraper will let me select all ""Names"" and ""LinkedIn"" links however it won't select all photos, bios, or locations. Do anyone have a suggestion as to what I'm doing wrong or perhaps an alternative? Thank you in advance for your time.",https://www.reddit.com/r/webscraping/comments/10afnj7/simple_scraper_worked_for_me_before_but_now_it/,AI
1125,Looking to scrape all the svg files on the internet (or at least a lot of them). Anyone has any thoughts on how to do that?,"It’s somewhat straight forward to land on a page and identify all the places where a url points to an svg or an svg tag appears explicitly in the html. 

The piece I’m struggling with might be super trivial to more accomplished scrapers out here: how do I find a seed list of reasonably professional websites to scrape (assuming professional websites have higher quality svgs generally). 

And also probably a basic question: given the landing page of a domain, it’s there any simple way to determine all the pages on that domain so I can scrape it?

I’m pretty handy with selenium but the basics of how to do mass scraping escapes me.",https://www.reddit.com/r/webscraping/comments/109lqmo/looking_to_scrape_all_the_svg_files_on_the/,AI
1126,Scrape Instagram posts,"When you `add ?__a=1&__d=dis` to an Instagram post URL like this `https://www.instagram.com/p/CnEl_HyM0_H/?__a=1&__d=dis` you get a lot of JSON data returned with info on an Instagram post. This even works in incognito mode when logged out. 

However when I try to access this using node.js on a render.com server I get:

    {""message"":""Please wait a few minutes before trying again."",""require_login"":true,""status"":""fail""} 

I get the same response when accessing the page throug a TOR browser. Any idea how I could fix this?",https://www.reddit.com/r/webscraping/comments/1093gcr/scrape_instagram_posts/,AI
1127,Is this a fair price for web scraping?,"Hi all,

After messing around with Github and realizing I know absolutely nothing about Python, I'm thinking about commissioning someone to do a project using Python script to scrape the Jeopardy archive at [J-archive.com](https://J-archive.com) into CVS files using 6 fields (air date, round, category, value, answer and question). I wouldn't be looking to scrape the entire archive, but instead from the year 2020 to the current date. Here's a few examples of parsers that perform(ed) this job featured on Github repositories: [https://github.com/whymarrh/jeopardy-parser](https://github.com/whymarrh/jeopardy-parser)

[https://github.com/jbovee/j-archive-parser](https://github.com/jbovee/j-archive-parser)

Knowing nothing about Python, I have been unable to get those parsers to work for me, so I took to Fiverr in order to get a quote on how much it would cost to have someone do this. After chatting with the guy for a while he said he could do it and asked me what my budget was, which I found a little curious because I was assuming he was going to quote me at least some kind of rough price and then we'd go from there. When I responded that I didn't come in with a set idea for a budget and instead am looking for someone who can perform the project and give me an estimate on a price he came back with $130. When I told him to let me do some more research and think about it he immediately dropped the price to $90, which is the point where I started to get car salesman vibes.

I don't know what a fair price for a project like this is, and I'm certainly not disputing that this price may very well be fair, and obviously I want this deal to be good for both sides, but how we got to that figure made me think I better at least go to Reddit and ask.

Also, is Fiverr OK to be going to for a project like this? The last thing I'd want to do is drop $100+ on a project and not have it work as intended.",https://www.reddit.com/r/webscraping/comments/1097o1y/is_this_a_fair_price_for_web_scraping/,AI
1128,StaleElementReferenceException,"im using python3 selenium with firefox.

how can i avoid this particular exception? it seems like everytime i try to do a loop over elements i get this exception sooner or later.

&#x200B;

    data_and_resources_ul = driver.find_element(By.CLASS_NAME,'resource-list')
    csv_or_tsv_total = data_and_resources_ul.find_elements(By.CLASS_NAME,'format-label')
    for csv_or_tsv in csv_or_tsv_total:
            csv_or_tsv.click()
            time.sleep(1)
            navbar = driver.find_element(By.CLASS_NAME,'tabs--primary')
            all_buttons = navbar.find_elements(By.TAG_NAME,'a')
            back_to_dataset = [a for a in all_buttons if a.get_attribute('href').endswith('dataset')]
            back_to_dataset[0].click()
            time.sleep(1)

this is my code csv\_or\_tsv.click() takes me to a new page so new url then i will add more code there after that i press back\_to\_dataset\[0\].click() takes me to the previous page where csv\_or\_tsv\_total exists.

so my for loop should not fail because the same elements where gathered in the first place when i was on that page initially.

the for loop crashes on second iteration with StaleElementReferenceException

how can i fix this?",https://www.reddit.com/r/webscraping/comments/1090134/staleelementreferenceexception/,AI
1129,Online services to run selenium scripts with GUI,"Hi everyone!
I am working for a client and he asked me to scrape Craigslist. I made the script in selenium Python. Now he wants to run that script on some online server/cloud service. But I do not have any experience in that.
I tried digital ocean droplet with Ubuntu OS but it did not work and everytime I start the script it gives some error. 
Actually the script does not have headless web driver and the droplet is purely command line. I tried installing GUI libraries but many of them Did not work or they were very slow.

So is there any other online service with Windows OS or any other with GUI?",https://www.reddit.com/r/webscraping/comments/108z1gj/online_services_to_run_selenium_scripts_with_gui/,AI
1130,What would you like to see in a free scraping service?,"Hello Everyone! I am running a scraping startup.

I think most services are ridiculously expensive and I would really like to give back to the community by offering a free tier that would suit most or even all personal needs.

So my question to you guys is how many requests per month would you like to see in such a service for free? Please be honest and realistic. I would love to give everyone milions of requests per month for free, but there are significant costs associated with running such an infrastructure.

What kind of features would you like such a service to have?

Currently I offer 2 main functionalities:

* Scraping API. It returns data from a target URL using proxies either purely as HTML or scraping specific fields and returning them as JSON
* Crawling. In the user panel users can configure scraping profiles to be used for different web pages with an UI that allows you to interactively click on elements of the target page to be selected for scraping. Full website crawling is configurable by creating a crawling agent which consists of list of URL patterns which should either be followed and/or scraped using an extraction profile. All this can be run as a job resulting in a JSON or CSV file with rows populated from all scraped pages.

The bottom line is I want to understand what you would like to see in a free tier of such a service and I will try my best to meet it.

Thanks for your time!",https://www.reddit.com/r/webscraping/comments/108krfw/what_would_you_like_to_see_in_a_free_scraping/,AI
1131,Need a recommendation for good residential proxy services provider," 

Hi,

I'm working on a small project for personal use (nothing commercial) and I want to try residential proxies to avoid getting CAPTCHA challenges.  
I want to buy a few GB for a trusted/premium provider cause I need those proxies to work at the money time.  
I've contacted Oxylabs but they won't access my url (although my project is 100% legit).  
I also see that this kind of URLs are also banned in Smartproxy, both of the providers have an extensive list of urls that they won't serve.  
Few questions to the experienced folks here:

  
1. Can any of you recommend a legit/trusted provider which has a pay-as-you-go plan and will sell few GBs at a reasonable price (10-20 $ / GB) and doesn't have that strict compliance   
and won't let me down with low quality addresses ?  
2. What do you think of Packetstream.io ? I see a lot of complains about people that share their bandwidth getting paid for the traffic they share, but what do you think about the IPs they provide to their clients ? I'm sure they aren't on par with Oxylabs but for me as a client, will it be throwing away 50$ ?  
Thanks for your words of wisdom",https://www.reddit.com/r/webscraping/comments/108hys9/need_a_recommendation_for_good_residential_proxy/,AI
1132,"I was temporary kicked off of LinkedIn, and now my account has been reinstated. When should I start web scraping again?","[here’s a little background of my story.](https://www.reddit.com/r/webscraping/comments/1071d9r/how_many_li_sn_urls_can_i_convert_to_a_li_public/) so now that Lincoln is working for me again, when should I start back to using my PhantomBuster to scrape about 200 to 300 profiles day?",https://www.reddit.com/r/webscraping/comments/108m6cg/i_was_temporary_kicked_off_of_linkedin_and_now_my/,AI
1133,How to use bs4 to extract only the size available ?," 

Currently working on a project, my goal is to create a scraper to check only the size available of each item with bs4

Website of interest: [https://www.6pm.com/p/gbg-los-angeles-ayvie-slate/product/9479982/color/642?zlfid=192&ref=pd\_detail\_1\_sims\_cv](https://www.6pm.com/p/gbg-los-angeles-ayvie-slate/product/9479982/color/642?zlfid=192&ref=pd_detail_1_sims_cv)

I’m trying to extract only the available size without showing the size that are not available.

&#x200B;

What i have done : 

size = soup.find('div', {""class"": ""zna-z Ana-z""}).text

print(size)

Return : 5.56.577.588.5 

&#x200B;

and when i try this one

size=soup.find('div', {""class"": ""dqa-z""}).text

Return :5.5     

&#x200B;

My expected return is to get only available size like “ 6.57 “  ( size6.5 and size7) because there are the one available i appreciate any help my way!",https://www.reddit.com/r/webscraping/comments/1081feg/how_to_use_bs4_to_extract_only_the_size_available/,AI
1135,how to scrape hidden input value with bs4," 

Currently working on a project, my goal is to create a scraper to check the availability of item with their respective sizes (stockId) with bs4

Website of interest: [https://www.6pm.com/p/bogs-b-moc-mid-winter-painted-black-multi/product/9419937/color/80?zlfid=192&ref=pd\_detail\_1\_sims\_cv](https://www.6pm.com/p/bogs-b-moc-mid-winter-painted-black-multi/product/9419937/color/80?zlfid=192&ref=pd_detail_1_sims_cv)

I’m trying to extract the data from the “only # left in stock “ and the size inside the <input type:hidden class.

so normally, a website will have out-of-stock on an item by default (if it's out of stock). for this specific site, the user needs to select a shoe size before the target scrape ""only # left in stock” if inventories are less than 10 for that specific size autherwise nothing is scrape .

What i have done : 

value = soup.find('input', attrs={'name': 'stockId'}).get('value')

print(value)

Return error

My expected return is to get Each size and know “ only # left in stock “ respectively if applicable.

i appreciate any help my way!",https://www.reddit.com/r/webscraping/comments/1075ghr/how_to_scrape_hidden_input_value_with_bs4/,AI
1136,Scraping only prices from 50 websites,"Hi 

I want to scrap prices from websites to compare the prices from cheapest to expensive?

For ex: I want to scrap price for the best crms out there and wants to list crms from cheapest to expensive and same scenarios for like web hosting or any other niche

There are more then 50 CRMs out there and wants to list them under one roof 

And then wants to create a bot that will check and update the pricing every 3 hours.

Someone has done it for domains as they have their apis
https://www.domcomp.com/

But what about who doesn't have APIs?",https://www.reddit.com/r/webscraping/comments/1074ycw/scraping_only_prices_from_50_websites/,AI
1138,"Sorry if this is the wrong sub, but is it possible to extract an email from an old youtube account?","Basically I can't remember the email I used to create a youtube account from back in 2010. I've tried retrieving it using google's tool where you enter your phone number but I have several gmail accounts linked to the same phone number. And it showed me a dead account I no longer use instead of the three active accounts also linked to that number, so it wasn't very effective.",https://www.reddit.com/r/webscraping/comments/104fxv6/sorry_if_this_is_the_wrong_sub_but_is_it_possible/,AI
1139,Are reddit's classes names change periodically (those with random letters and numbers and what not),"I'm building webscraper for the final project I'm doing, and I wonder is it a wise path to hardcode their names, or to just think of something else?

I saw other advice of searching through elements until I find something recognizable, but I'd rather do this because this is my first time web scraping.

I want to grab links to posts from the subreddits main page, then use those links to request HTML of these individual posts.",https://www.reddit.com/r/webscraping/comments/103xy6z/are_reddits_classes_names_change_periodically/,AI
1141,"Your Advice: How to scrape a webpage with pagination and an AJAX ""wall "" to download pdf's after the wall has been crossed?","Let me know if you'd like to see the link to the page.

Edit:
I am struggling to scrape this site: https://www.resbank.co.za/en/home/publications/statements/mpc-statements

What i am specifically looking to do is to go from page to page and click the links, which will open to new windows that contain pdf documents which I would like to download and zip together. I am specifically interested in the ""statements of monetary policy"" document.",https://www.reddit.com/r/webscraping/comments/1024ead/your_advice_how_to_scrape_a_webpage_with/,AI
1144,Best web scraping api's at the moment?,"Hi, we are currently using 3 web scraping API's mostly for legacy reasons and to be able to have a variety of web scrapers for different sites.

I am finding 2 of these performing poorly and am looking for alternatives.

Currenly using:

ScraperAPI. Have a legacy cheaper plan so have kept it, but often it will return a 500 error when scapfly works every time.

ScrapingBee. Got in when the started and they were very good. Also noticing a lot of failures where scrapfly would work.

ScrapFly. Seems to work really well, but pissed they decided to double our price for our plan.

Would like to keep scrapfly and find another solution and get rid of scrapingbee and scraperapi.

ScrapingFish and Scrapeops Proxy look good.

Any recommendations?",https://www.reddit.com/r/webscraping/comments/1016j3l/best_web_scraping_apis_at_the_moment/,AI
1145,python package or regex for crawling emails?,"I have a list of ~2000 unique urls I scraped for different stores. I'd like to crawl them for any emails found on the landing page or on the contacts page if there is one.
I've tried a couple of open source packages on GitHub as well as implementing a couple of email regex finders but none of them are really doing the job right.
I know I'll never be able to capture all of them, but was wondering what your preferred methods are in this case?",https://www.reddit.com/r/webscraping/comments/zzuhmz/python_package_or_regex_for_crawling_emails/,AI
1148,Recapture data from deleted Reddit replies,"I posted in a Reddit sub asking if it was ok to post a product I’m developing once it’s finished, in order to get peoples feedback. No one actually answered my original question but turns out a lot of people were interested and put their hand up to beta test the app once it becomes available. 

Turns out I should have read the rules - no “advertising” is allowed in, but that was what my initial question to the sub was about. 
The post stayed up for about a week and then the mod came along and deleted every response I got to the post. 

I would like to get the usernames of the people who responded to me so I can DM then in the future (most of them asked me to). Is there any way to recapture this data? I don’t know if scraping is an option for this? Or
might an older version of the post be cashed on my device somewhere?",https://www.reddit.com/r/webscraping/comments/zy8caf/recapture_data_from_deleted_reddit_replies/,AI
1149,"Webscraping for Emails across Internet, Verifying Emails"," I am learning python, and would like to learn how to webscrape for emails across the internet AND verify emails.  

Where can I find more information to focus on this area?  Thanks!",https://www.reddit.com/r/webscraping/comments/zxxslb/webscraping_for_emails_across_internet_verifying/,AI
1153,How to verify a customer paid on a confirmed order,"Hi!  New Facebook business owner and I sent a customer an invoice through the messenger app via Meta.  The customer confirmed the order, but how can I see if she paid?  And how would I enter a tracking number to prove shipment?  TIA!",https://www.reddit.com/r/webscraping/comments/zud41j/how_to_verify_a_customer_paid_on_a_confirmed_order/,AI
1154,Looking for a website where I can scrape results of baseball by innings.,"Hi everyone, 

For a personal project, I would like to have a dataset who contains all results of baseball matches by innings.  Is there anyone who know a website where I can scrape these data ? 

Thank you for your time.",https://www.reddit.com/r/webscraping/comments/ztlr5g/looking_for_a_website_where_i_can_scrape_results/,AI
1156,Any available API to scrape data from journal articles?,"Hi! I plan to obtain some data from journal articles.

I'm hoping to create a database to suit the needs of my project and was thinking whether there are any APIs available to assist me. The data that I am looking for are molecular data, mainly their optical properties and ADME-T.",https://www.reddit.com/r/webscraping/comments/ztd155/any_available_api_to_scrape_data_from_journal/,AI
1157,Grabbing data from betfair,"Hi, I'm having a little problem with a project I'm doing, I want to grab information from the ruby union page of betfair: [https://www.betfair.com/sport/rugby-union](https://www.betfair.com/sport/rugby-union)

I want to grab only match odds,  however I'm grabbing all the odds match odds, handicap and total points. How can I grab only match odds and not the others? 

I'm using this line to grab:

betodds= driver.find\_elements(By.CSS\_SELECTOR, 'span.ui-runner-price')

if you could help me it would be awesome.",https://www.reddit.com/r/webscraping/comments/zt6g7u/grabbing_data_from_betfair/,AI
1158,AZlyrics,"Does anyone know how to get around the AZlyrics bot detection? I tried adding a 2 second delay but it still kicked me out. Also, anyone know how to regain access once you're already kicked out? :L",https://www.reddit.com/r/webscraping/comments/zt4j2q/azlyrics/,AI
1160,I built a web scraping system with Python and Celery to scrape millions of websites,"Since working on my SaaS product full time I’ve learned a ton about web scraping. 

Yesterday I developed a systems design to scrape company career pages at scale. I’m slowly scaling up from 300 req/s. 

I can reach about 12,000 pages per second to process 2,000,000 websites in 7 days.

At that rate, I’ll be able to scrape the public pages of large sites like LinkedIn in no time!  

The system with built with Python/Scrapy, Celery, pandas, and Kubernetes. 

Happy to explain more if anyone is interested.",https://www.reddit.com/r/webscraping/comments/zr2xmo/i_built_a_web_scraping_system_with_python_and/,AI
1161,Merge two dataframes on key ignoring capitlization of that key,"i am using python pandas.

lets say i have two different dataframes, the two dataframes contains a col called country\_id

but one df has it uppercase like this COUNTRY\_ID and the other has it lower case country\_id

i want to merge these two frames on country\_id but it is not working because pandas is taking the two cols as different names:

so if i write:

    final_df = pd.merge(first_df, second_df, how='left', on='country_id')

or if i write:

    final_df = pd.merge(first_df, second_df, how='left', on='COUNTRY_ID')

both wont work and i will get a KeyError exception. how can i fix that without manipulating my dataframes columns.",https://www.reddit.com/r/webscraping/comments/zrfe8y/merge_two_dataframes_on_key_ignoring/,AI
1162,"finding chromedriver, glibc version compatibility","I'm trying to set up a webscraper in an amazon-linux terminal and I'm having issues with chromedriver glibc compatibility.

&#x200B;

I'm currently using chrome and chromedriver version \~108. So I decided to try installing chrome and chromedriver 102. I still get the same error and the list of versions to try is too huge to trial and error this.

&#x200B;

My glibc version is 2.26-62.amzn2   ...   and it seems more awkward to change that than to use older chromes.

&#x200B;

&#x200B;

Below is the error message when chromedriver tries to open.

&#x200B;

        Traceback (most recent call last):
          File ""/home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/service.py"", line 97, in start
            path = SeleniumManager().driver_location(browser)
          File ""/home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/selenium_manager.py"", line 74, in driver_location
            result = self.run((binary, flag, browser))
          File ""/home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/selenium_manager.py"", line 93, in run
            raise SeleniumManagerException(f""Selenium manager failed for: {command}. {stderr}"")
        selenium.common.exceptions.SeleniumManagerException: Message: Selenium manager failed for: /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager --browser chrome. /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager: /lib64/libc.so.6: version `GLIBC_2.29' not found (required by /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager)
        /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager: /lib64/libc.so.6: version `GLIBC_2.28' not found (required by /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager)

&#x200B;

How can I find chromedriver glibc version compatibility requirements / How can I find which version of chromedriver I need?",https://www.reddit.com/r/webscraping/comments/zr0gse/finding_chromedriver_glibc_version_compatibility/,AI
1164,"403 response using links from CSV, 200 response using links from in-memory lists?","I'm new to web-scraping, so I'm sorry if this is something obvious.

I'm currently trying to scrape all data from a website. The site is structured well, so my approach was to start at the home page, and work through scraping all links from each 'level'. so start at the home page, get all it's categories, then go through the categories one-by-one to get their categories, and so on.

So far, it's worked decently well, but the code was becoming a mess, so I decided to do a big refactor. One of the main things that I did, was I created cache CSV files to store each link, as opposed to using in-memory lists that I was using prior. I figured that by caching the links, I could go directly into a deeper web-page without having to scrape over the entire thing every time.

The bug is that whenever I try to navigate to these links from the CSV file, I get a 403 error. But if I run the branch where everything is still using in-memory lists, I get 200's for each response and am able to walk through the website as normal. I have already checked the URLs on both branches and they're consistently a match. If I navigate to the pages manually on both branches, they work.

For context, I'm using Python 3.9 with the BeautifulSoup4 and Requests modules on a Windows 10 machine.",https://www.reddit.com/r/webscraping/comments/zpybse/403_response_using_links_from_csv_200_response/,AI
1165,Anyone here who’s knowledgeable in Scrapy ItemLoaders? I’m having issues with incorporating multiple ItemLoaders in different parse functions within the same spider.,I’d be most grateful to anyone willing to help me with my problem as it’s the only thing holding me back from a successful project. I will share the problem in detail for anyone interested to show a beginner the ropes of Scrapy ItemLoaders. 🙏,https://www.reddit.com/r/webscraping/comments/zpvqtw/anyone_here_whos_knowledgeable_in_scrapy/,AI
1166,IMDB - WebScraping - Infinite Download Images and Video of a Movie,"Check out the code for how we can download all the videos and Images of a movie.

[https://github.com/pavan412kalyan/imdb-movie-scraper/blob/main/snippets/download\_all\_videos\_by\_movie\_id.py](https://github.com/pavan412kalyan/imdb-movie-scraper/blob/main/snippets/download_all_videos_by_movie_id.py)

[https://github.com/pavan412kalyan/imdb-movie-scraper/blob/main/snippets/Image\_downloaders.py](https://github.com/pavan412kalyan/imdb-movie-scraper/blob/main/snippets/Image_downloaders.py)

[https://www.youtube.com/watch?v=tGavdm9djjw&t=6](https://www.youtube.com/watch?v=tGavdm9djjw&t=6)",https://www.reddit.com/r/webscraping/comments/zpmqvy/imdb_webscraping_infinite_download_images_and/,AI
1167,Scrape CSV data embedded in Href tag,"Edit solved:

I just did a whole bunch of find and replaces on the %20 etc.. characters

Hi all,

I'm trying to download approx 30,000 csv files of racing results from the below website and i stuck on dealing with the CSV data.

[https://www.racingandsports.com.au/horse-racing-results/australia/hobart/2022-12-18](https://www.racingandsports.com.au/horse-racing-results/australia/hobart/2022-12-18)

The data i need is just embedded in the page inside a href tag that you click to download the data see below...

    <a href=""data:text/csv;charset=utf-8,RNo,Name,Prize%20Money,Time,Winner,Class,Dist,SOT,Time,Temp,Diff,RSSF,L600,Wins,AWI,API%0D%0A%221%22,%22hobart-united-fc-bm70-hcp%22,%22AUD%20$25,000%22,%222:12.70%22,%22SO%20ASTOUNDING%22,%22BM70%22,%222100%22,%22G%22,%22132.7%22,%2212.64%22,%22+0.09%22,%2286.35%22,%2236.45%22,%228%22,%2210.7%22,%223.21%22%0D%0A%222%22,%22ayc-netball-club-mdncl1%22,%22AUD%20$25,000%22,%221:40.47%22,%22NOT%20A%20BRASS%20RAZOO%22,%22CL1%22,%221600%22,%22G%22,%22100.47%22,%2212.56%22,%22+0.33%22,%2277.25%22,%2236.65%22,%221%22,%221.6%22,%221.16%22%0D%0A%223%22,%22wellington-cricket-club-maiden%22,%22AUD%20$25,000%22,%221:05.35%22,%22FEAR%20THE%20STING%22,%223U%20MDN%22,%221100%22,%22G%22,%2265.35%22,%2211.88%22,%22-0.02%22,%2294.05%22,%2235.57%22,%220%22,%220%22,%220.96%22%0D%0A%224%22,%22wakeful-club-bm64-hcp-fm%22,%22AUD%20$25,000%22,%221:12.35%22,%22THELMA%22,%22FM%20BM64%22,%221200%22,%22G%22,%2272.35%22,%2212.06%22,%22+0.10%22,%2288.75%22,%2236.50%22,%2215%22,%2225%22,%225.52%22%0D%0A%225%22,%22lindisfarne-afl-masters-class-1-hcp%22,%22AUD%20$25,000%22,%221:12.09%22,%22SISTINE%22,%22CL1%22,%221200%22,%22G%22,%2272.09%22,%2212.02%22,%22+0.05%22,%2290.45%22,%2236.15%22,%2210%22,%2210.3%22,%223.88%22%0D%0A%226%22,%22winzenberg-handicap%22,%22AUD%20$50,000%22,%221:05.30%22,%22DUNBRODY%20POWER%22,%22Qlty%22,%221100%22,%22G%22,%2265.3%22,%2211.87%22,%22-0.03%22,%2294.35%22,%2235.52%22,%2219%22,%2221.8%22,%225.69%22%0D%0A%227%22,%22j-j-roofing-bm68-hcp%22,%22AUD%20$25,000%22,%221:26.62%22,%22JOHNNY%20CHUTZPAH%22,%22BM68%22,%221430%22,%22G%22,%2286.62%22,%2212.11%22,%22+0.25%22,%220%22,%2236.43%22,%229%22,%2213.2%22,%223.67%22%0D%0A%228%22,%22ladbroke-it-bm62-hcp%22,%22AUD%20$25,000%22,%221:38.02%22,%22ROYAL%20AND%20TOUGH%22,%22BM62%22,%221600%22,%22G%22,%2298.02%22,%2212.25%22,%22+0.02%22,%220%22,%2237.03%22,%2211%22,%2212.9%22,%223.61%22%0D%0A"" class=""btn btn-boxed-g pull-left btn-export"" style=""margin-left:5px"" id=""btnCSV"" download=""Hobart Results - 18 Dec 2022.csv""> <i class=""fa fa-file-excel-o""></i><span> CSV</span> </a>   

I have been able to extract the string I need and then try and use something like...
 
    pd.read_csv(io.stringIO(csv_data))
 
This however just gives a 144 column wide file instead of the 13 columns and 8 lines for data.

ive tried adding encoding, removing ""data:text/csv... at the start.

ill be pushing the extracted CSV file directly to a Database for use later on.

Any ideas how to solve this?

thanks in advance",https://www.reddit.com/r/webscraping/comments/zpisk8/scrape_csv_data_embedded_in_href_tag/,AI
1168,Backend server can detect real IP ?,"So i build a scraper using python ( requests , bs4) to get data from a certain webapp.  
I've inspected the site headers and set up everything with headers, proxies (\~500 different IPs, http proxies ) all that stuff.  
What i've noticed after \~4K requests i've recieved:

  
`<html>`

`<head>`

`<title>403 Forbidden</title>`

`</head>`

`<body>`

`<h1>Error 403 Forbidden</h1>`

`<p>Forbidden</p>`

`<h3>Error 54113</h3>`

`<p>Details: cache-hel1410033-HEL 1671405206 1319037323</p>`

`<hr/>`

`<p>Varnish cache server</p>`

`</body>`

`</html>`

  
But i got the response everytime i've tried to sent a request ( with different proxy IP ) which was weird.  
So i've tested the same code on different machine ( different IP ) and it works with the same setup using the same proxies.  
I'm assuming the web-server is somehow detecting my real IP ? Is this the issue & is there a workaround ?  


Thanks",https://www.reddit.com/r/webscraping/comments/zpbq0x/backend_server_can_detect_real_ip/,AI
1170,What kind of data/security bot is this?,"I have tried multiple sites for scraping data and all of them use some form of bot detection. They all have different name for it but seems the exact type. Here is an example:

iiXnANQ1pn-a: 

iiXnANQ1pn-b:hj90iw

iiXnANQ1pn-c:AADwqRuFAQAAvTZNIeZfh\_hARZ6Hz6QwY0SI1cSOArylWZ

iiXnANQ1pn-d:AAaihIjBDKGNgUGASZAQhISy1WIYbrZv\_VhYxf\_\_\_\_\_mR38WAPtKFEdo7\_vYdczycqYG294

iiXnANQ1pn-f:A5t0tBuFAQAAKXxx7kkj\_xFYZdXDMUW9r5gjdp8inNcIGads\_vyBaqsaO9pgAWKP9DKcuG

&#x200B;

From another site:

x-incssdtm-a 1u4L2F2O=dJdrhgpEcAZ1A2-O43sSfvK3fgqAl3m

x-incssdtm-b f6844s

x-incssdtm-c AOC2eoaEAQAAOwY7DAC

x-incssdtm-d ABaChIjBDKGNgUGAQZIQhISi0

x-incssdtm-f A2VNnoaEAQAAsg

&#x200B;

Is this Akami? I dont think it is. This is something else .. any hints?",https://www.reddit.com/r/webscraping/comments/znio8q/what_kind_of_datasecurity_bot_is_this/,AI
1173,"ElementNotInteractableException: Message: Element <span class=""selection""> could not be scrolled into view","    select_span = driver.find_element(By.CLASS_NAME,'selection')
    select_span.text
    select_span.click()

i found the element i want and select\_span.text give me the text inside `<span class=""select2-selection__rendered"" id=""select2-indicatorDropdown-container"" role=""textbox"" aria-readonly=""true"" title=""Experienced violence since COVID-19"">Experienced violence since COVID-19</span>`

which is 'Experienced violence since COVID-19' but if i do select\_span.click() to click on this span and open the selection i get :

    ElementNotInteractableException: Message: Element <span class=""selection""> could not be scrolled into view

i keep getting this error almost daily and i never understand why..

there is no shadowroot here or iframe or anything since i got the element anyway, but when i try to click it i get this error why?? it happens to almost everything i try to click.

PS: i use python selenium and firefox browser",https://www.reddit.com/r/webscraping/comments/zmk7ro/elementnotinteractableexception_message_element/,AI
1175,Help with scraping ESPN college football data,"So I'll start by saying that I'm sure there's a better way to do this, but I have already written a bunch of files that work with this method, so I would greatly prefer to keep it. Basically, I am pulling ESPN URL links to college football games, such as :

[https://www.espn.com/college-football/game/\_/gameId/401437033](https://www.espn.com/college-football/game/_/gameId/401437033)

Adding ""&xhr=1"" (which should covert to JSON), and then opening the data with python using:

    response = urllib.urlopen(link)
    data = json.loads(response.read())

This worked all of last season and all of this season until this week, and it seems that specifically it's the ""&xhr=1"" piece that's no longer working (as I'm unable to get a JSON file anymore). Again I know there are other and better ways, but I have many other large files written to parse through the JSON files that I used to get.

Does anyone know anything more about this? Or if there is an easy way to take a link like the one I've given above and covert all the HTML code into JSON?

Many thanks for any help!",https://www.reddit.com/r/webscraping/comments/zjpipj/help_with_scraping_espn_college_football_data/,AI
1176,Looking for help/ insight,"Let me start by saying that I have zero experience and I only heard the term webscraping a couple days ago. Sorry if this is the wrong place to post this, please remove if so.

Every year I help do a big online comp for my company. I’m currently in the middle of doing it all by hand, typing/ copy pasting info into excel…. I got really fed up the other day and figured there had to be a better way to do this. With some googling I found Webscraping which seems to be what I’m looking for. I’ve been reading about it all weekend but I’m still pretty lost on how to actually start. I have zero coding experience and like I said earlier, only just heard about webscraping a couple days ago. What would you all recommend as a starting point for me to learn? 

If it helps, my company sells to most of the major retailers in the US (Home Depot, target, Walmart, Costco, etc). So those would be the sites Im pulling from.",https://www.reddit.com/r/webscraping/comments/zjj0f2/looking_for_help_insight/,AI
1178,What are some ways web scraping can be used UNETHICALLY?,"Doing a presentation for my computer science class about web scraping.

In my presentation I mentioned web scraping can be used ethically and unethically. Due to our school and presentation requirements we HAVE to provide examples of stuff we mention, it’s a pain in the butt, but it does teach us research and critical thinking skills.

Anyway, can you guys please tell me some ways the web scraping can be used unethically? Thanks in advance!",https://www.reddit.com/r/webscraping/comments/zhsybt/what_are_some_ways_web_scraping_can_be_used/,AI
1180,What do you charge for webscraping on average?,"I am getting the impression that webscraping is considered fairly low skilled job by many clients and thus expect it to be extremely cheap. So, I would like to get a general picture of compensation for webscraping.

I would really appreciate if you would participate in the poll.

Thanks

[View Poll](https://www.reddit.com/poll/zh2xa9)",https://www.reddit.com/r/webscraping/comments/zh2xa9/what_do_you_charge_for_webscraping_on_average/,AI
1181,How did everyone here learn how to webscrape? and have any of you made any cool projects with it?,"Hi everyone, this is my first ever reddit post (feels kind of unbelievable given that im a 22 year old web addict). I've been teaching myself how to code for the past few years and have gone fairly deep into web scraping in particular. I was just wondering what stories other people have in why they decided to learn how to do webscraping. It would also be cool to hear about any projects that any of you worked on; So far i've built a few bots that run social media pages for me and I intend to use it more in conjunction with AI and economic theory in the future (currently writing up the proposal for that). hope to hear something cool from one of you soon!",https://www.reddit.com/r/webscraping/comments/zgdi1w/how_did_everyone_here_learn_how_to_webscrape_and/,AI
1182,"What is the best, free, web scraping tool?","Hey all, I’m looking for a free web scraping tool that can scrape from multiple sources and pair data sets. 

Any recommendations?",https://www.reddit.com/r/webscraping/comments/zg93ht/what_is_the_best_free_web_scraping_tool/,AI
1183,Need web scraping professional to help me decide how to proceed with my lead generation idea,"I would like a pretty specific set of data scraped from 1 or 2 websites.  

Trying to decide if paying someone on fiverr when I need more leads is smarter than paying for the code?  If that question doesn’t make sense then I apologize, I have about 2 hours of researching web scraping under my belt.

Anyone want to steer me in the right direction and possibly gain another client?",https://www.reddit.com/r/webscraping/comments/zgjapg/need_web_scraping_professional_to_help_me_decide/,AI
1184,"Can I get 100,000+ rows from database at my library?","I was sent on a mission to obtain an updated list of dental offices in the USA. I was able to access this beautiful list via my library card, however I'm only able to download 500 rows at a time. I'd really love to get this all in one table as quickly as possible. Is this doable with a scraper? If so, any recommendations would be greatly appreciated.",https://www.reddit.com/r/webscraping/comments/zgcdyl/can_i_get_100000_rows_from_database_at_my_library/,AI
1185,Scraping this page,"This is probably a simple one for a subreddit dedicated to web scraping. 

I like the books on this page. I would like to add them to my notion page with books to read. I don't want to type over all the books. Is there an easy way to scrape the titles and authors to a spreadsheet? 

  
[https://www.mostrecommendedbooks.com/books-billionaires-read](https://www.mostrecommendedbooks.com/books-billionaires-read?ref=producthunt)",https://www.reddit.com/r/webscraping/comments/zf29un/scraping_this_page/,AI
1186,Checking html integrity check after scrapping,Would like to know any best practices available after a Web page has been scraped using Scrapy or Selenium python packages. Thanks.,https://www.reddit.com/r/webscraping/comments/zf3nrv/checking_html_integrity_check_after_scrapping/,AI
1187,Excel Web Scrape Not Working Due to Error,"My scraping method of choice is Excel PowerQuery, but it seems I have met my foe in a recent redesign of a site I pull information from.

[https://webapps.washingtoncountyor.gov/custody/#/?searchby=people&alpha=b](https://webapps.washingtoncountyor.gov/custody/#/?searchby=people&alpha=b)

I get this error:

Details: ""The [Web.Page](https://Web.Page) function requires Active Scripting to be enabled in Internet Explorer options. See [https://go.microsoft.com/fwlink/?LinkId=506565](https://go.microsoft.com/fwlink/?LinkId=506565) for details on how to enable Active Scripting.""

I have tried everything there is to try with enabling scripting in IE, registry, I have troubleshot it to death with no luck. Can someone please help?",https://www.reddit.com/r/webscraping/comments/zdl20v/excel_web_scrape_not_working_due_to_error/,AI
1189,Pre-trained Webscraping Models,I am looking to acquire a pretrained machine learning model in python for webscraping. I've look around online with no success. Does this exists? if not it is difficult to build on?,https://www.reddit.com/r/webscraping/comments/zbjaos/pretrained_webscraping_models/,AI
1191,HELP - Completely lost on simple Webscraper.io request,"Hi All, I'm really blowing it right now putting together a simple webscraper for [Auction.com](https://Auction.com).   


Would anyone have availability for a quick call or zoom to help me out?  


Literally all I need from the scrape is each house with Street address, city, state, zip, and auction date all in separate columns. Pretty simple, but I'm big dumb.",https://www.reddit.com/r/webscraping/comments/zasvfv/help_completely_lost_on_simple_webscraperio/,AI
1192,If API available from Optimum,"Hi all, new to scraping but familiar with python. Looking to see if I can pull an API from a web flow that involves entering an address and then getting the internet plans and pricing available. I would like to avoid selenium if possible and it seems like there should be an api but this usecase is a bit more complex than the tutorials. Any hints would be greatly appreciated!

&#x200B;

[https://order.optimum.com/Buyflow/Storefront](https://order.optimum.com/Buyflow/Storefront)",https://www.reddit.com/r/webscraping/comments/za1vv2/if_api_available_from_optimum/,AI
1193,Data Scraping for Updating Costs,"Hi!!

I have spent a good bit of time on this, and I am pretty well stumped.

I am trying to get put together a bit of a catalog of construction materials (2x6, 2x4, drywall, insulation, etc). 

I would like to be able to put this data into a spreadsheet, and have the prices update from the website (like Lowe's, Home Depot, or others) whenever the file is open without opening each individual page. This way the estimator can open the file, and have current pricing available.

I tried the get data from web option in Excel, and that just times out. I've watched a few videos, but that hasn't worked either. (User error, I'm sure.) If someone could point me in the right direction, I would appreciate it!",https://www.reddit.com/r/webscraping/comments/z9qf20/data_scraping_for_updating_costs/,AI
1194,Advice for consolidating multiple data sources,"A couple things to bear in mind:
- Data and scraping is not my domain of expertise, please be kind.
- If there are better communities to ask, please point me to them.

Problem: I am trying to consolidate data from multiple data sources

- Google Analytics
- PowerBI
- Other ""generic"" web reports -- basic tables

Because the sources are diverse, I was thinking scraping would be a better ""general purpose"" approach rather than trying to integrate with various APIs. Also, APIs aren't even available for some report services.

General thought process:
Access url >> adjust date range >> scrape results >> drop results in Excel 

How would you tackle this problem? What tools would you recommend?

Thanks for any guidance.",https://www.reddit.com/r/webscraping/comments/z96ywc/advice_for_consolidating_multiple_data_sources/,AI
1195,Scrapping restaurant addresses in the US,"Hello,   
I am looking to collect the address of restaurant chains in the US.   
Some big names like Burger King and some smaller ones like California Pizza kitchen.   


When I visit the website of these companies, I am usually confronted with a map where I can only see the restaurants close to my location. [https://www.cpk.com/locations](https://www.cpk.com/locations)  


Is there a way to scrape the the list of all their locations?   
tnx",https://www.reddit.com/r/webscraping/comments/z7umyt/scrapping_restaurant_addresses_in_the_us/,AI
1196,"Rotating Proxy Service for Many, Many Requests","I'm building my first scraping project, and I'm almost finished, but I still need to decide how I'm going to rotate proxies. The main issue that I'm running in to is that I am going to be sending up to 6000 requests daily at intervals of 5 seconds (I'm logging basketball scores and their respective live odds), and most proxy rotating services don't offer an affordable plan that would allow for this. Does anyone know of a good solution?",https://www.reddit.com/r/webscraping/comments/z64hbv/rotating_proxy_service_for_many_many_requests/,AI
1197,Webscraping instagram page with apify not working,"Ive been using the apify instagram page/ post scraper to scrape info on the 30 most recent posts for public accounts and it’s always worked seamlessly. Unfortunately there is one account that just doesn’t work with the apify actor. I’ve tried inserting the link in different ways tried a different actor waited a day and it just can’t webscrape that specific page. Do you guys know if there is anything i can do to get the data or if some instagram pages are protected and if there is a way to bypass that? Any help would be greatly appreciated and you would be saving me a ton of time for my project!

Here is the link to the profile i am trying to scrape: https://instagram.com/ginengine?igshid=YmMyMTA2M2Y=",https://www.reddit.com/r/webscraping/comments/z5wvc1/webscraping_instagram_page_with_apify_not_working/,AI
1199,eBay account image scraping,"Hi! I recently found an eBay account that has over 37,000 listings for various pages from extremely rare magazines that haven't been archived online. Most of these are the only scans available online for these magazines, even though some pages are missing. What would be the most efficient way to download every listing image from the account?

[https://www.ebay.com/str/mantiquesandbaseball](https://www.ebay.com/str/mantiquesandbaseball)",https://www.reddit.com/r/webscraping/comments/z5glnq/ebay_account_image_scraping/,AI
1201,Does anyone know what's going on here?,"I'm trying to scrape a website by listening in on one of it's websocket connections.  I've been able to start collecting these ws messages using selenium, but have no idea how to parse the payloads.  They look like this:

&#x200B;

eJyt09sKAiEQBuB3mdvdmLU9ZL1KdGEy0IKgOBZF9O5ZsNCCS1heOjof8o/u72Ad7MCTM0oT1OBUOMUCHu2VtfWEceNGnrFBDiqMHEb9WsRjhgIxtu86o1jH7osyZ4rtlYRH/b/dTXbzYYuy9Ozaogzep/BVW8QeknZfxN4k7bxZipxMqqGIPWUif34m3+R5Il0RWyYTyZvkkr1d+pWHJ8rvT88=\\

&#x200B;

If this encoding format looks familiar to anyone, please let me know what it is and what I can do to parse it!

Thanks!",https://www.reddit.com/r/webscraping/comments/z4vzwx/does_anyone_know_whats_going_on_here/,AI
1202,ticket data scraping project,"Hello guys, 

My company needs to collect data about ticket pricing in the US. We want to collect data from the top 7 websites, including Ticketmaster.  Data collection frequency should be two times per week.  I need to learn how to manage this situation; we can either hire a developer or search for a platform or company that offers this service.  The main goal is regular high-quality data delivery on our AWS bucket. What would you suggest for the most cost-effective solution?",https://www.reddit.com/r/webscraping/comments/z4fr30/ticket_data_scraping_project/,AI
1203,Need help extracting emails....,"So:

I have a task to gather all the contact emails from multiple people from a website.

The website is about movies and there are about 30 movies on a page.

To find the contact div, i need to firstly click on the movie img and then click on the contact div where i find the data i want: The contact email of the producer.

Now, there are about 300 pages. So almost a total of almost 9000 movies.

Is there any way I could extract the emails using some kind of web scraping method or shall I accept my misery and spend two months manually writing them? Thank you in advance and sorry!",https://www.reddit.com/r/webscraping/comments/z4h8xp/need_help_extracting_emails/,AI
1204,Webscraping with Machine Learning,"Hi y’all, sorry if this is the wrong spot for this question, I’ve been coding for a while now, and I’m working on this project for a data science internship. practically it’s a universal webscraper, that grabs specific text that is flexible for every website. 

The idea is to just grab all the text from the website and tokenize the input, and convert it. To a vector via tf-idf, before feeding it into a binary classification model. Which has been pre-trained on labeled data beforehand. If the model outputs us a success we’ll add the data, else we’ll discard it. 

The reason why I used machine learning to differentiate  between the text that is important and the irrelevant ones, is because the text I need doesn’t follow a uniform structure, ie every website could have it laid out differently. So regex and basic heuristics is out of the question as far as I know.

The result are pretty disappointing, it’s true positive rate is high, but so is the false positive rate. Has anyone does anything similar, or have any guidance?

Thank you in advance.",https://www.reddit.com/r/webscraping/comments/z2ulzr/webscraping_with_machine_learning/,AI
1205,HOW TO EXTRACT EMAIL ADRESS?,"Hello! I have a task to gather email adresses from a website that has multiple pages, and I would normally do this task by hand but there are 250+ pages and it would take an infinite amount of time.

Soo... I was wondering if there is any way I can extract the email adresses using a script? Thank you in advance!",https://www.reddit.com/r/webscraping/comments/z2tem3/how_to_extract_email_adress/,AI
1207,Scrape Wowprogress,"Hey veryone, I'm not and expert in exel/google sheets and I have basic basic knowledge around coding in general.  


This is what I wanna do,   


I want to scrape [https://classic.warcraftlogs.com/character/eu/mograine/kooinish](https://classic.warcraftlogs.com/character/eu/mograine/kooinish) ""Best Perf. AVG"" and then there is a number there. I want to take that number and present it in a google sheets. I have installed xpath in my browser, I have added Importfromweb ""another addon for google sheets"". This is what I type in and I only get errors. I have also tried other variants.  


=IMPORTFROMWEB(""https://classic.warcraftlogs.com/character/eu/mograine/kooinish"";""//div\[@class='best-perf-avg'\]/b"")   


\#ALL\_SELECTORS\_RETURN\_NULL  


I hope there is someone out there who could help me fetch that number and present it in a google sheet.  


Thanks!",https://www.reddit.com/r/webscraping/comments/z1sm36/scrape_wowprogress/,AI
1208,How to identify duplicate crawl data?,"Sometimes data is duplicated across multiple web pages. At other times, multiple pages contain identical but distinct data. By ""distinct data,"" I mean that it was created independently of other data.

Does any know of existing approaches for distinguishing duplicate and identical blobs of data? I'm thinking there may be some means for finger printing it, but computing, say, a vanilla SHA512 hash isn't going to work here.",https://www.reddit.com/r/webscraping/comments/z1ekic/how_to_identify_duplicate_crawl_data/,AI
1209,Webscrapper IO - No data scraped yet error,"Hi, 
Can anyone help me resolve the issue of No data scraped yet using Webscrapper IO extension. I’m trying to scrape data from Amazon and have built a site map that goes from one page to page that is recommended and so on. While the site map is able to navigate from one page to another, no data is being scraped. Anyone facing similar issue or is aware of root cause here ? Please help and let me know if you need more details to understand the issue better",https://www.reddit.com/r/webscraping/comments/z12ir7/webscrapper_io_no_data_scraped_yet_error/,AI
1210,API for scraping residential addressees?,"Hello! 

I’m new to scraping and couldn’t find the answer after researching and test runs on Apify and other sources. 

I’m looking for an API that can scrap Google Maps for residential mailing addresses that meet my search criteria in a city. Is this possible?

Thanks!",https://www.reddit.com/r/webscraping/comments/z0roic/api_for_scraping_residential_addressees/,AI
1211,BeautifulSoup - Scrape product and product variants and export it to csv,"I am trying to scrape this [website](https://www.petbarn.com.au/dogs/dog-food/dry-dog-food)  products listing what I am trying to achieve here is grab all the info  per product for example: product\_name, price and their variants info as  well like 10kg, 20kg, 3kg and their prices accordingly. I have search  the html they don't provide all the info I am looking for but under  script tag they have a json residing which could be useful. Here is the  json script tag:

    </script><script type=""text/x-magento-init"">
            {
                ""[data-role=swatch-option-111105]"": {
                    ""Magento_Swatches/js/swatch-renderer"": {
                        ""selectorProduct"": "".product-item-details"",
                        ""onlySwatches"": true,
                        ""enableControlLabel"": false,
                        ""numberToShow"": 16,
                        ""jsonConfig"": {""attributes"":{""1299"":{""id"":""1299"",""code"":""size"",""label"":""Size"",""options"":[{""id"":""6651"",""label"":""10kg"",""products"":[""116724""]},{""id"":""6780"",""label"":""20kg"",""products"":[""108981""]},{""id"":""6234"",""label"":""3kg"",""products"":[""108987""]}],""position"":""0""}},""template"":""$<%- data.price %>"",""currencyFormat"":""$%s"",""optionPrices"":{""108987"":{""baseOldPrice"":{""amount"":49.990908090909},""oldPrice"":{""amount"":54.99},""basePrice"":{""amount"":42.718180818182},""finalPrice"":{""amount"":46.99},""tierPrices"":[],""msrpPrice"":{""amount"":0}},""108981"":{""baseOldPrice"":{""amount"":172.71818081818},""oldPrice"":{""amount"":189.99},""basePrice"":{""amount"":128.17272627273},""finalPrice"":{""amount"":140.99},""tierPrices"":[],""msrpPrice"":{""amount"":0}},""116724"":{""baseOldPrice"":{""amount"":120.899999},""oldPrice"":{""amount"":132.99},""basePrice"":{""amount"":102.71818081818},""finalPrice"":{""amount"":112.99},""tierPrices"":[],""msrpPrice"":{""amount"":0}}},""priceFormat"":{""pattern"":""$%s"",""precision"":2,""requiredPrecision"":2,""decimalSymbol"":""."",""groupSymbol"":"","",""groupLength"":3,""integerRequired"":false},""prices"":{""baseOldPrice"":{""amount"":49.990908090909},""oldPrice"":{""amount"":54.99},""basePrice"":{""amount"":172.71818081818},""finalPrice"":{""amount"":189.99}},""productId"":""111105"",""chooseText"":""Choose an Option..."",""images"":[],""index"":{""108987"":{""1299"":""6234""},""108981"":{""1299"":""6780""},""116724"":{""1299"":""6651""}},""preSelectedGallery"":[],""channel"":""website"",""salesChannelCode"":""base"",""sku"":{""108987"":""127956"",""108981"":""127960"",""116724"":""501600""},""labels"":{""108987"":{""sales_flag_label"":""Great low price""},""108981"":{""sales_flag_label"":""Great low price""},""116724"":{""sales_flag_label"":""Great low price""}},""hasEndDate"":{""108987"":false,""108981"":false,""116724"":false},""dynamic"":{""name"":{""108987"":{""value"":""Black Hawk Fish And Potato Adult Dog Food - 3kg""},""108981"":{""value"":""Black Hawk Fish And Potato Adult Dog Food - 20kg""},""116724"":{""value"":""Black Hawk Fish & Potato Dog Food 10kg""}},""sku"":{""108987"":{""value"":""127956""},""108981"":{""value"":""127960""},""116724"":{""value"":""501600""}},""gtin"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""marketing_offer_short"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""advice_care"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""product_category"":{""108987"":{""value"":""Dry Food""},""108981"":{""value"":""Dry Food""},""116724"":{""value"":""Dry Food""}},""benefits"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""feeding_guide"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""health_condition_dietary"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""brand_filter"":{""108987"":{""value"":""Black Hawk""},""108981"":{""value"":""Black Hawk""},""116724"":{""value"":""Black Hawk""}},""ingredients"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""activity_level"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""size"":{""108987"":{""value"":""3kg""},""108981"":{""value"":""20kg""},""116724"":{""value"":""10kg""}},""food_type"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""health_benefits"":{""108987"":{""value"":""Total Wellbeing""},""108981"":{""value"":""Total Wellbeing""},""116724"":{""value"":""Total Wellbeing""}},""life_stage"":{""108987"":{""value"":""Adult""},""108981"":{""value"":""Adult""},""116724"":{""value"":""Adult""}},""flavour"":{""108987"":{""value"":""Fish""},""108981"":{""value"":""Fish""},""116724"":{""value"":""Fish""}},""nutritional_info"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""breed"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""nutritional_info_table"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""australia_made"":{""108987"":{""value"":""No""},""108981"":{""value"":""No""},""116724"":{""value"":""No""}},""nutrition_grade"":{""108987"":{""value"":""Premium""},""108981"":{""value"":""Premium""},""116724"":{""value"":""Premium""}},""lifestyle"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""weight_control"":{""108987"":{""value"":""No""},""108981"":{""value"":""No""},""116724"":{""value"":""No""}},""frequent_feeder_price"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""size_swatches"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}}}},
                        ""jsonSwatchConfig"": {""1299"":{""6651"":{""type"":""0"",""value"":null,""label"":""10kg""},""6780"":{""type"":""0"",""value"":null,""label"":""20kg""},""6234"":{""type"":""0"",""value"":null,""label"":""3kg""},""additional_data"":""{\""update_product_preview_image\"":\""1\"",\""use_product_image_for_swatch\"":0,\""text_swatch_as_multiple_select\"":\""1\"",\""swatch_input_type\"":\""text\""}""}},
                        ""mediaCallback"": ""https\u003A\u002F\u002Fwww.petbarn.com.au\u002Fswatches\u002Fajax\u002Fmedia\u002F"",
                        ""jsonSwatchImageSizeConfig"": {""swatchImage"":{""width"":30,""height"":20},""swatchThumb"":{""height"":90,""width"":110}},
                        ""showTooltip"": 1                }
                }
            }
        </script>

I have manage to parse that script tag and turned that into python  dictionary via json.loads() but couldn't figure out the best way to  extract info and export it to csv. here is my code so far:

    import requests
    import pandas as pd
    from bs4 import BeautifulSoup
    import json
    
    from datetime import datetime
    from datetime import date
    
    now = datetime.now()
    today = date.today()
    
    
    class PetBarnProdScraper:
    
        all_info = []
    
        def fetch(self, url):
            print(f""HTTP GET request to URL: {url}"", end="""")
            res = requests.get(url)
            print(f"" | Status Code: {res.status_code}"")
    
            return res
    
        def parse(self, response):
            soup = BeautifulSoup(response.text, ""html.parser"")
            product_urls = [a.get(""href"") for a in soup.select(""a.product-item-link"")]
            product_ids = [
                pid.get(""id"").split(""-"")[-1] for pid in soup.select(""div.product-item-info"")
            ]
            titles = [
                a.text.replace(""\n"", """").strip() for a in soup.select(""a.product-item-link"")
            ]
            old_price = [
                p.select_one(""span.price"").text for p in soup.select(""span.old-price"")
            ]
            ratings = [r.get(""title"") for r in soup.select(""div.rating-result"")]
            no_of_reviews = [review.text for review in soup.select(""a.action.view"")]
            data = (
                soup.select('script[type=""text/x-magento-init""]')[3]
                .text.replace(""\n"", """")
                .strip()
            )
            data_json = json.loads(data)
            data_j = json.loads(
                data_json[""*""][""Overdose_AdobeAnalytics/js/view/datalayer""][""datalayer""][0]
            )
    
            for idx in range(len(titles)):
                try:
                    ratings_count = ratings[idx]
                    reviews_count = no_of_reviews[idx]
                    last_price = old_price[idx]
                except:
                    ratings_count = ""N/A""
                    reviews_count = ""N/A""
                    last_price = ""N/A""
                d = {
                    ""Scraped_Date"": now.strftime(""%m/%d/%Y, %H:%M:%S"").split("","")[0],
                    ""Scraped_Time"": now.strftime(""%m/%d/%Y, %H:%M:%S"").split("","")[1],
                    ""product_name"": titles[idx],
                    ""price"": data_j[""PLP""][""products""][idx][""productPrice""],
                    ""old_price"": last_price,
                    ""ratings"": ratings_count,
                    ""number_of_reviews"": reviews_count,
                    ""productSKU"": data_j[""PLP""][""products""][idx][""productSKU""],
                    ""productSize"": data_j[""PLP""][""products""][idx][""productSize""],
                    ""priceWithoutTax"": data_j[""PLP""][""products""][idx][
                        ""productPriceLessTax""
                    ],
                    ""lifeStage"": data_j[""PLP""][""products""][idx][""lifeStage""],
                }
    
                for prod_id in product_ids:
                    details = soup.select_one(
                        f""script:-soup-contains('[data-role=swatch-option-{prod_id}]')""
                    )
                    labels = []
                    if details:
                        json_details = json.loads(details.text.replace(""\n"", """").strip())
                        json_endpoint = json_details[f""[data-role=swatch-option-{prod_id}]""]
                        label_option_list = json_endpoint[
                            ""Magento_Swatches/js/swatch-renderer""
                        ][""jsonConfig""][""attributes""][""1299""][""options""]
                        for lab in label_option_list:
                            labels.append(lab[""label""])
    
                            d[""label_options""] = labels
                print(d)
                self.all_info.append(d)
    
        def to_csv(self):
            df = pd.DataFrame(self.all_info).fillna("""")
    
            df.to_csv(f""{today}_petbarn.csv"", index=False)
    
            print('Stored results to ""petbarn.csv""')
    
        def run(self):
            for i in range(1, 2):  # total_number of pages
                url = f""https://www.petbarn.com.au/dogs/dog-food/dry-dog-food?p={i}""
    
                response = self.fetch(url)
    
                self.parse(response)
    
            self.to_csv()
    
    
    if __name__ == ""__main__"":
        scraper = PetBarnProdScraper()
        scraper.run()

every time I run that code the label\_options column has always the same  values which is the last one I am guessing. here is the output I am  getting:

    Scraped_Date,Scraped_Time,product_name,price,old_price,ratings,number_of_reviews,productSKU,productSize,priceWithoutTax,lifeStage,label_options
    11/21/2022, 00:31:47,Black Hawk Fish And Potato Adult Dog Food,189.99,N/A,N/A,N/A,black-hawk-fish-&-potato-adult-dog-food,,172.72,Adult,""['10kg', '20kg', '3kg']""
    11/21/2022, 00:31:47,SavourLife Ancient Grains Lean Chicken Adult Dog Food,159.99,N/A,N/A,N/A,savourlife-ancient-grains-lean-chicken-adult-dog-food,,145.45,Adult,""['10kg', '20kg', '3kg']""
    

Expected output:

    Scraped_Date,Scraped_Time,product_name,price,old_price,ratings,number_of_reviews,productSKU,productSize,priceWithoutTax,lifeStage,label_options
    11/21/2022, 00:31:47,Black Hawk Fish And Potato Adult Dog Food,189.99,N/A,N/A,N/A,black-hawk-fish-&-potato-adult-dog-food,,172.72,Adult,10kg
    11/21/2022, 00:31:47,Black Hawk Fish And Potato Adult Dog Food,189.99,N/A,N/A,N/A,black-hawk-fish-&-potato-adult-dog-food,,172.72,Adult,20kg
    11/21/2022, 00:31:47,Black Hawk Fish And Potato Adult Dog Food,189.99,N/A,N/A,N/A,black-hawk-fish-&-potato-adult-dog-food,,172.72,Adult,3kg
    11/21/2022, 00:31:47,SavourLife Ancient Grains Lean Chicken Adult Dog Food,159.99,N/A,N/A,N/A,savourlife-ancient-grains-lean-chicken-adult-dog-food,,145.45,Adult,3kg
    11/21/2022, 00:31:47,SavourLife Ancient Grains Lean Chicken Adult Dog Food,159.99,N/A,N/A,N/A,savourlife-ancient-grains-lean-chicken-adult-dog-food,,145.45,Adult,20kg

Can anyone please help me figure out the best way to get the expected output? Thanks!",https://www.reddit.com/r/webscraping/comments/z0ef5i/beautifulsoup_scrape_product_and_product_variants/,AI
1212,Legality of webscraping stock sites,"Hi, I've been doing on and off scraping of stock analysis websites (such as [Zacks.com](https://Zacks.com)) for a while now.

I was wondering a few questions regarding the rules. I know for many sites, especially bigger sites that grade stocks like CNN, Benzinga, Zacks, ..., it is absolutely against their TOS to scrape data, but I also see some people, even within this subreddit, say it's legal because it's public data or point to a legal decision. I know there probably isn't a clear line, but are there legal issues/rules/improvements to what I'm doing here:

* Scraping a site around every 10s for a ticker's grading (not 24/7 but maybe for 12hrs every 2 weeks)
* EDIT: the data gathered from this scraping isn't republished, just processed through algorithms and shared between a few of my friends

Is there a frequency that is allowed? Is stock analysis technically copyrighted/property of the site? Could I be sued for doing this?

I just want to get an idea of the rules I should follow to stay out of trouble, thank you!",https://www.reddit.com/r/webscraping/comments/yzvuvt/legality_of_webscraping_stock_sites/,AI
1213,Find Out-Of-Date WordPress Sites,"Is it possible for a scraper to find wordpress sites that are running on an out of date version of WP and collect:
URL, WP version, and any contact info (name number, email, contact page)?",https://www.reddit.com/r/webscraping/comments/yzyl2b/find_outofdate_wordpress_sites/,AI
1214,Proxy-Coin is the cheapest residential rotating proxy provider out there right now!,"We sell at 0.60$ per GiB!

Now you might be thinking, why is this so cheap? It is due to us being a new company and not trying to make too much profit and keep it fair for the clients.

We think clients should get the best support and proxies they can get so we made Proxy-Coin!

We support the following protocols: SOCKS5 and HTTP

Discord server: [https://discord.gg/w3rgx6DNGZ](https://discord.gg/w3rgx6DNGZ)",https://www.reddit.com/r/webscraping/comments/z00zi6/proxycoin_is_the_cheapest_residential_rotating/,AI
1215,resources for webscraping beginner,"As the title said, i am getting started with web scraping and am looking for some good resources (blogs, youtubr, ..etc)

To learn more about it, thank you for the help",https://www.reddit.com/r/webscraping/comments/yykic1/resources_for_webscraping_beginner/,AI
1218,I need Projects ideas,"I'm building a bot, that scrapes that online and store it in a PostgreSQL database. Then Make the data available with a rest api with flask, i'm having trouble with coming up with some idea of the type of data/website to scrape, any suggestion would help.",https://www.reddit.com/r/webscraping/comments/yy4s80/i_need_projects_ideas/,AI
1219,1000$ if you can help scrap images,"I am willing to pay 1000$ to whoever can get me all product images for specific SKUs (mostly non available anymore in SheIn.com)


For example:
swvest44201202734  
swshorts04200527196  
swdress07200403365  
rg5786  
NC8301gold  
sk2203015018855010  
sf2110143763514492  
sn2112311287464378  
si2203040063060131


Notes:
1. I will need a script with the source 
2. I have one image link of any specific product what I need is the rest of links of that product images 
3. Images are available on that server in a specific directory that I know",https://www.reddit.com/r/webscraping/comments/yy2aq2/1000_if_you_can_help_scrap_images/,AI
1220,"New to webscraping, should I better learn code or use code-free tools?","Hi folks!

Total newbie in web scraping. My (extremely small) knowledge in code limits to basic HTML and CSS.

Context: My company asks me to update the data bases that lists our competitors. I need information such as product name, if they have certain labels (that one is tricky as no one writes the norms in the same way), available sizes, product composition, warranty year and other specific terms to my market and so on. These informations need to be classified in an Excel spreadsheet.

I have never done data scraping but I am pretty sure there is something to do out there. Settling probably takes time as every website has a different layout. Also to note that my target websites are not really e-commerce websites. I work in B2B so it not possible to buy from, these are more ""display website"".

So my question today is, do you think it is worth investigating web scraping for this purpose ? 

If so, considering my (inexistant) knowledge in coding, is it better that I learn coding or use pre-made tools? Do you have any recommandations? 

I have been browsing some free tools (like chrome extension Web Scrapper) even tho I'm still extremely lost. For now, I'd like to keep using free stuff if possible. And once I know how to get things work, maybe I can show some early results to my manager and convince him to put some budget for it. 

Thanks in advance for your help! Any answer or piece of advice will be really helpful!",https://www.reddit.com/r/webscraping/comments/yxlv4y/new_to_webscraping_should_i_better_learn_code_or/,AI
1223,How can I automate sending email to recruiters?,"I've a list of companies(around 1000),  I want to apply jobs for. I want to attach my resume to the email. And a letter telling I want internship at some positions.

What'll be same?

1) Cover letter


2) Attached resume

What'll be different?

1) Email address",https://www.reddit.com/r/Automate/comments/10ew1x4/how_can_i_automate_sending_email_to_recruiters/,AI
1224,Sending commands to a speaker using a programming language,"I want to create a custom alarm clock that is controlled over the internet running on a remote server (probably a lambda on a schedule). Ideally I need a speaker which I can send commands to over the internet, unless I’m missing something. Current options I’ve thought about but seem like a pain in the ass to implement:
- Hacking a google/apple/Amazon device and sending it a command somehow (probably a locally hosted web server, but this defeats the whole thing of running it in the cloud).
- speaker that has an api interface that I can access if I allow it and has some sort of encryption

Any ideas?",https://www.reddit.com/r/Automate/comments/10eb9fd/sending_commands_to_a_speaker_using_a_programming/,AI
1228,Review of ChatGPT/AI for LinkedIn for marketing,"Recently, I found a Chrome extension called Engage AI. I've been using the tool for the past week or two. I wanted to share my experience with it.

The Chrome extension essentially writes comments for you for LinkedIn posts. All you have to do is copy the link of a post and paste it into the extension. The app scrapes information from the post and comments, then it generates a related comment for you using AI.

The comments generated by the extension are accurate and have perfect spelling and grammar. They sound genuine, human, and high-effort, far better than generic comments such as ""Great post"" or ""Thanks for sharing.""

Unfortunately, the comments aren’t always accurate. Occasionally, the app will generate a comment that doesn’t make much sense with the post and I need to refresh it. It’s also a shame that it’s specific for LinkedIn and no other platforms, so you’re out of luck if you don’t use LI.

It's not for everyone, but for someone like me who uses LinkedIn to build relationships with leads for my employer, it's a great help. I feel like VAs could benefit a lot from this, and anyone else trying to stay in front of your target audience on LinkedIn.

Personally, I comment on prospects' posts on a weekly basis. It can take me hours to come up with, say, twenty insightful and well-thought-out comments. With this extension, those hours are reduced to just a few minutes. I've gotten positive feedback from the comments I've made with it so far.

Overall, the app is a great time-saver and saves me the effort of needing to write a comment that makes sense in a field I’m not always familiar with. I’m already on the paid plan, but there’s a free trial if you want to test it out.

If anyone else has tried it, I would love to hear your thoughts.",https://www.reddit.com/r/Automate/comments/10bf7sg/review_of_chatgptai_for_linkedin_for_marketing/,AI
1233,Does an AI email assistant exist?,"Hey everyone,

What I'd ideally like is a kind of AI inbox assistant.

I get \*so many emails\*. I want them filtered into 'Reply now', 'Important', 'Read later', 'Not needed'.

The ideal case is that they would be further categorised by the kind of action to be taken. Then with an AI language model, a draft is there to send - e.g. when someone asks for a call, there are two drafts there: no thanks, and a yes with a link to my calendar. 

Or when a customer asks the same question as has been asked 100s of times before, the draft answer is there to send with 1 click.

Does this exist?",https://www.reddit.com/r/Automate/comments/104vpbv/does_an_ai_email_assistant_exist/,AI
1239,Inserting date in URL,"Hello, I go to high school in Denmark. The online system we use for viewing our schedule, sending messages etc. is called Lectio. It is based on Javascript AFAIK. I want a script that converts the current date to year-month-date, and inserts it in the middle of a URL. 

I have this link for getting my schedule of the day. [https://www.lectio.dk/lectio/202/SkemaAvanceret.aspx?type=ShowListAll&starttime=2023-01-04T00:00:00&endtime=2023-01-04T23:30:00](https://www.lectio.dk/lectio/205/SkemaAvanceret.aspx?type=ShowListAll&starttime=2023-01-04T00:00:00&endtime=2023-01-04T23:30:00)

The dates has to be in the following formate: year-month-date

Example: 2023-01-04

The script is going to run on my Iphone Shortcuts. (Shortcut activated, date inserted in URL, opens the URL in Safari).

Looking forward to hear from you,

\- DIS",https://www.reddit.com/r/Automate/comments/zwrp37/inserting_date_in_url/,AI
1242,Automating twitter replies to latest tweet of a search term?,"Is is possible to monitor a certain phrase on twitter, for instance a mention of my company name and reply to it with pre-written replies.",https://www.reddit.com/r/Automate/comments/zml9aj/automating_twitter_replies_to_latest_tweet_of_a/,AI
1243,can i create an interactive slack bot using my own account?,"Im looking to create something that post in certain groups of slack under my personal profile (for example, clock in group, post everyday at 8am ""clock in"")",https://www.reddit.com/r/Automate/comments/zm1kr6/can_i_create_an_interactive_slack_bot_using_my/,AI
1247,"In the Future, Will companies build their own Machine learning models or not??"," As access to Artificial intelligence is getting easier. I was thinking if companies would really need their own models or not??

Because big tech companies like OpenAI, Facebook, Microsoft, etc have a large amount of dataset/data with them. And more the data, the more advanced the model is going to be. So mostly they will be dominating the whole machine learning model with AI2B (AI to Business).

And Businesses & Companies won't even hire machine learning engineers too.

I'm new to ML, I'm still learning. I was just curious and hopped upon this conclusion. Pls, let me know if I'm being incorrect.",https://www.reddit.com/r/Automate/comments/zfxwnn/in_the_future_will_companies_build_their_own/,AI
1248,Intruder Detection Project,"Let's build an AI in [Python](https://youtu.be/q8_3H97Svms) to prevent our car be stolen, smart car that communicates in real time with the police. 🤖👮🏻 Do you think it could work?",https://www.reddit.com/r/Automate/comments/zemjpk/intruder_detection_project/,AI
1249,Would there be a way to respond to typed phrases on Android phone.,"I'm really trying to quit gambling, as it has affected my life horrible for months/years now. I've tried quite a lot and the urge is still there causing me to bypass the measures I set in place. Would there be a way to set my android phone to monitor me inputting a certain phrase (link/password to gambling) and close app/shut down the phone? I understand it's not the most practical, but I'm open to any other ideas.

TLDR: Can I set my phone to monitor typing and respond to certain inputs",https://www.reddit.com/r/Automate/comments/zdkhwe/would_there_be_a_way_to_respond_to_typed_phrases/,AI
1259,Auto call and relay message,"I’m looking for something that will call a number once a day and record that message. Then send me a text/ email/ notification/etc. with the recording.

The place I work for uses a hotline for work expectations the following day and I don’t want to call and wait for the message to be played. I want it when I’m ready. Saving me like 10 seconds per day :)",https://www.reddit.com/r/Automate/comments/z240ac/auto_call_and_relay_message/,AI
1262,Jotform Reverse Engineer help,"Hi! So in short, I've pulled apart the website and grabbed all the assets I can, however I still notice that within for-formuser.js - the submitted data gets shot over to [submit.jotform.com](https://submit.jotform.com) \- anyone know what kind of data/file is being sent? I know this depends on several things like the integrations you want, but we're kinda stuck. What we want to do:  


\- Rely on our own webservers to handle the html/js/css - which we've set up and tested   
\-Have the form we made send out a report email via our own servers  
\-Have the form data integrate into a SQL server we can query to run reports.   


We're a non profit food bank and really needing some direction on what needs to be done at this point. Thanks!",https://www.reddit.com/r/Automate/comments/z1ak0o/jotform_reverse_engineer_help/,AI
1267,"Are we too hypocritical for a ""As Need"" grocery automated system to eliminate food waste?","Hi Reddit,

&#x200B;

Just as the name suggests, I listened to this podcast where they spoke about eliminate food wast by downloading an app that mostly tracks that for you. But the basic idea is that you only every have as much as you need for a very short amount of time. Such as a week. only buying food in portions instead of bulk besides initial setup, this app is supposed to figure out all the buying for you but for all the documentaries and the 'don't litter' campaigns I don't think this is going to catch on and I'd love a discussion on if we actually want to work on the earth, eliminate food waste or if it's really just a fun talking point to have. A good thing to chat about at dinner parties",https://www.reddit.com/r/Automate/comments/yrtvsn/are_we_too_hypocritical_for_a_as_need_grocery/,AI
1268,Centralized Solution for Tasks/Scripts with Redundancy,"We support thousands of computers and servers in different network layers from various ""script servers"" for different team members. I am hoping to centralize all scripts/tasks to one pane of glass and also have a way of reporting on if those scripts ran successfully or had issues. I have heard ""PowerShell Universal"" can do this, but was wondering what other companies were doing. We are also trying to have more control/tracking of our scripts with version control, etc. What are others doing as far as running scripts (shell/bat and powershell) against several different servers/computers and environments from a central location (while having redundancy so the server can be rebooted or if there is an outage, etc).",https://www.reddit.com/r/Automate/comments/yre5h0/centralized_solution_for_tasksscripts_with/,AI
1293,Automated art painting?,"Looking for a service where I can sub!it graphic art and have it painted. Not simply a print but a painting.

With the proliferation of excellent A.I. text to art generators the common person can now create artistic works of art but there should be an option available to transfer the image to a yexturd medium and not merely a print.

I'm looking for such a service if it exists. Yes commissioning a human is one option I am also pursuing but I'm interested in taking an idea to result purely with technology.",https://www.reddit.com/r/Automate/comments/xqu39k/automated_art_painting/,AI
1294,graticule,"Hi. I am trying to make a flow where an app starts after reboot and additionally starts the beacon. Im having problem with the beacon.  How can I force the app to start the beacon ?

The app is to be found on google playstore under this link

https://play.google.com/store/apps/details?id=com.emilburzo.graticule",https://www.reddit.com/r/Automate/comments/xoozvp/graticule/,AI
1297,Rules update suggestions,"Hi all,

It was brought to my attention a mod we thought had left the team was continuing to take actions on the sub. I have removed said mod and undone their actions.

On that note, I think it is worthwhile revisiting the rules and ask the community what they want from this sub.",https://www.reddit.com/r/automation/comments/m0vfh2/rules_update_suggestions/,AI
1309,ChatGPT is absolutely insane,"I've never worked with a learning model that's so flexible and robust in terms of generative output, with the exception stable diffusion and/or midjourney.

Where do you folks see this tech going? Do you think we'll see additional open access, large-scale lawsuits against generative AI, or something else entirely?",https://www.reddit.com/r/automation/comments/zygstz/chatgpt_is_absolutely_insane/,AI
1312,Runedu.co,"Would you like to run your online language course fully digitally?

Digitize your education company:

 1. Minimize expenses  
2. Additional Income with AI  
3. Make decisions with KPI's  
4. Digital presence to boost sales",https://www.reddit.com/r/automation/comments/zth57w/runeduco/,AI
1316,Custom Inventory Management system with automated low inventory alert,"I love building systems with no-code tools. Today, I built a custom Inventory Management system with just no-code tools. The system includes a low inventory alert. Here's a video demonstrating the same, [https://youtu.be/XoAo666kiHw](https://youtu.be/XoAo666kiHw). The tools used were Utilize.app, Make, Google Sheets, and Gmail.

⚒️ Utilize is used to build an app to add and withdraw stock. Once the stock is withdrawn, Utilize triggers Make with the SKU information.

⚒️ Make is used to trigger Google Sheets and Gmail.

⚒️ Gmail sends an alert to the admin.

&#x200B;

I think many small businesses could benefit from a simple inventory management app. Looking forward to hearing thoughts from the community.",https://www.reddit.com/r/automation/comments/zgxi2k/custom_inventory_management_system_with_automated/,AI
1319,Working on aquaculture plc. Is a Brainbox good?,"A small PLC system to control a few motors, air pumps, and lighting based on a few variables. Do you have any suggestions as to which PLC system I should employ?

I want to use Wi-Fi and have a nice interface for my computer.

Thank you in advance!",https://www.reddit.com/r/automation/comments/z3678q/working_on_aquaculture_plc_is_a_brainbox_good/,AI
1325,Salary Question,"I’m an electrical engineering graduate working as a controls engineer that does start up commissions and software development. I’ve been doing this for about 1.5 years now. I’ve done additional things for my company beyond my base responsibilities such as building a wiki that’s quickly becoming a go-to source for information sharing, and expanding the tools we have for our project development/management suite.

With annual reviews coming up I’m wondering what is a fair salary increase. I live in WA on the western side of the Cascades so cost of living is up there if that makes a difference. 

Anyways…  advice/comments/concerns?",https://www.reddit.com/r/automation/comments/ywxqij/salary_question/,AI
1327,"Automation COE Market Growth, Opportunities Business Scenario, Share, Growth Size, Scope, Key Segments and Forecast to 2027","To provide detailed information related to major factors (drivers, restraints, opportunities, and industry-specific challenges) influencing the market growth.",https://www.reddit.com/r/automation/comments/ywln5t/automation_coe_market_growth_opportunities/,AI
1333,Anyone familiar with the DC2000 drives?,Need help repairing some boards on a DC2000 dc drive. If anyone has any knowledge on how to repair the DCFB boards DM me.,https://www.reddit.com/r/automation/comments/yl6j8j/anyone_familiar_with_the_dc2000_drives/,AI
1335,Automation COE Market is predicted to reach USD 1.5 billion by 2027 with CAGR of 36.9%," To provide detailed information related to major factors (drivers, restraints, opportunities, and industry-specific challenges) influencing the market growth",https://www.reddit.com/r/automation/comments/yk0ljt/automation_coe_market_is_predicted_to_reach_usd/,AI
1336,Tools for automation and daily tasks,"Hello, I m courios to find out more tools to use in my daily task as a infrastructure engineer. So far I use the following tools:
- dnsx (dns query)
- httpx (http requests and testing)
- mapcidr (quick calculate subnets)
- tmux .. for multiple terminals
- barrier (for remote connection to my computer through laptop)
- ansible ... Off course
- puppet/foreman
- kubernetes with helm/kind
- jq (for json parsing)
- ddosify (testing multiple paths or status code)
- mailcow-docker
- janus (for api gateway endpoints)
- docker gen (generate nginx config)
- acme-companion (generate ssl certificates)

What interesting tools do you use 🤗??",http://awesome-tools,AI
1340,Call for Participants - Online Survey on Explicability in Process Mining,"Hey everyone,

I recently posted in r/processmining but I'd like to repost in this channel since there is a wider audience that might have been involved with business process consulting. 

I am an industrial PhD candidate and my research is centered on Explainable AI in Process Mining.

If anyone here had previous experience with explainability strategies in PM consultancy - both as an expert or client - I would value your input for an online research questionnaire on the subject.

It should take no more than 10-15 min and you will be granted access to further insights on explicability strategies through the forthcoming scientific research article, plus the chance to participate in online focus groups.

You can participate in the questionnaire from here: [Microsoft Sway Landing Page](https://sway.office.com/Ew3baJ5vSyTHmWNs?ref=Link)

Thanks for your attention!",https://www.reddit.com/r/automation/comments/y8zu85/call_for_participants_online_survey_on/,AI
1341,Do you manage a community or coworking space? Want to do it better?,"Hello everyone,

My team recently built [CoWello](http://www.cowello.com/)—a tool to help community managers streamline member bookings and payments. This platform is meant for coworking spaces that rent out desks, conference rooms, and/or event spaces to remote workers, startups, and freelancers.

**Here’s a quick demo:** [https://youtu.be/dLo6OISz5xw](https://youtu.be/dLo6OISz5xw)

We’d love to know your thoughts! Please drop us an email at [hello@cowello.com](mailto:hello@cowello.com) or leave a comment below.

Cheers!",https://www.reddit.com/r/automation/comments/y7k15q/do_you_manage_a_community_or_coworking_space_want/,AI
1343,Automation COE Market Extrapolated to Reach $1.5 billion by 2027,"Report provide detailed information related to major factors (drivers, restraints, opportunities, and industry-specific challenges) influencing the market growth.",https://www.reddit.com/r/automation/comments/y2r84q/automation_coe_market_extrapolated_to_reach_15/,AI
1349,Dimensionally Accurate Rechargeable Batteries,"I bought my son his first race car and it takes 6 AA batteries which it just drains.

It fits any of your normal store-bought double A's just fine but four different pairs of rechargeables I've purchased are a bit too big.

Has anyone had a similar issue and found an AA battery that has the same diameter?",https://www.reddit.com/r/automation/comments/x8mhlf/dimensionally_accurate_rechargeable_batteries/,AI
1356,"Noob here, completely lost on this workflow","I have a list of company names, website URL,s and Linkedin URLs.   


I want to find their mailing address but I'm completely lost on how to get started. Spent the whole day trying to figure out solutions and I'm still in the same place as the start.   


So far I've tried to source:   


OpenCorporation - Only free to ""public good"" use cases  
Google Maps: ""can't find search query variables that pull the info in the list to consistently get accurate address

linkedin, which does give accurate addresses, but not always, and worst of all can't scrape it

&#x200B;

\+ many more ideas that didn't work.   


any help would be really appreciated",https://www.reddit.com/r/automation/comments/wld6x4/noob_here_completely_lost_on_this_workflow/,AI
1364,Excel and Google Sheet automation - Goal is Data Studio Dashboard,"I am creating dashboard in Data studio which will show business metrics pulled from 2 main sources. Social platforms/Google products and from excel file. 

My approach was to connect excel file (which is going to be on sharepoint/server) to Google sheets where I can easily export data to google data studio. 

Second approach was to connect excel file directly to Data Studio but I am not finding any solution for this. 

Its important to update on daily basis 

Thank you in advance",https://www.reddit.com/r/automation/comments/w99k9m/excel_and_google_sheet_automation_goal_is_data/,AI
1365,How to automate random pixel filling?,"Hey folks,

I’m trying to put together a fun activity for my office. I’ll spare you the unnecessary details, so here’s what I need:

I want a way of randomly filling in a specified grid if pixels, and I’d like to be able to have this operation generate and save multiple files.

Think of it like a QR code, with a 6x6 grid of pixels — so six pixels in length, and six pixels in width (meaning: a total area of 36 pixels). I’m looking for a way to generate a random assortment of combinations, and save each result (be it png, jpeg, pdf, etc).

Is there an easy way to do this? Ideally, I’d like to use 6 different colors, but I can figure that part out myself if I can just get some guidance on randomly generating the pixels. Thanks!",https://www.reddit.com/r/automation/comments/w2kjbd/how_to_automate_random_pixel_filling/,AI
1368,Software Brings Construction to An Assembly Line,"Software developed by Agorus enables precision manufacturing of the components that go into residential structures, allowing on-site construction to be accomplished in days rather than months. The San Diego-based construction technology startup was founded four years ago by two former Navy SEALs and has just raised $6.5 million in a venture capital seed round.

[\>>>](https://www.gcoportal.com/software-brings-construction-to-an-assembly-line/)

r/global_construction",https://www.reddit.com/r/automation/comments/vthpz8/software_brings_construction_to_an_assembly_line/,AI
1369,How Built Spaces can Henefit From Digital Engineering,"Replacing manual with digital processes is one way that engineers can ensure that the world's built spaces use energy as efficiently as possible, writes Carl Coken, vice president of Atrius Engineering, Acuity Brands. This can involve the placement of sensors throughout a project for optimal energy usage, paired with digital twins to provide a wide array of data for operators.

[\>>>](https://www.gcoportal.com/how-built-spaces-can-henefit-from-digital-engineering/)

r/global_construction",https://www.reddit.com/r/automation/comments/vspfjp/how_built_spaces_can_henefit_from_digital/,AI
1371,Robotic System Seals Road Cracks,"Only two workers are needed to operate the CrackPro Robotic Maintenance Vehicle that robotically seals road cracks. The truck-mounted system developed by SealMaster and Pioneer Industrial Systems uses artificial intelligence and high-resolution cameras and lasers to find and gauge cracks before applying sealant.

[\>>>](https://www.gcoportal.com/robotic-system-seals-road-cracks/)

r/global_construction",https://www.reddit.com/r/automation/comments/vmn1fi/robotic_system_seals_road_cracks/,AI
1374,Is there a website that lists all algorithms/AIs/robots/machines and categorizes them into what they can do to get an overview of what they can be used for?,If possible robots/AIs which need as little help as possible.,https://www.reddit.com/r/automation/comments/vi2v5b/is_there_a_website_that_lists_all/,AI
1377,How to automate financial tracking,"Is there a way, that doesn’t involve third party apps, to pull credit card and bank account transactions once a week and put them into excel? 
Currently I do this manually and it is painful. Excel’s web function doesn’t word, I assume due to passwords. 

Is there an easy way to scrub with python? Is this something a beginner can set up?",https://www.reddit.com/r/automation/comments/veu8rf/how_to_automate_financial_tracking/,AI
1378,Issue with Corretto - NoClassDefFoundError dev/failsafe/Policy,I've been pulling my hair out trying to figure out what's wrong. Why wouldn't it flag as not found during compilation. I'm using corretto Java 11 and Selenium 4. Newest versions of both.,https://www.reddit.com/r/automation/comments/veo7lh/issue_with_corretto_noclassdeffounderror/,AI
1379,Can anyone help me with a task in Python?,Can anyone help me with a task in Python that should go to Google and from there search for a site name to enter the site and there search for the price list go to the price page and output to the document of all prices below a certain amount,https://www.reddit.com/r/automation/comments/veen4r/can_anyone_help_me_with_a_task_in_python/,AI
1380,The Future of the Ready-Mixed Concrete Truck,"The ready-mixed concrete truck is a mobile manufacturing environment, not just a vehicle, writes Craig Yeack. 

He notes that as technology evolves, each provider seeks to add displays into the truck cab, but end-users want to choose their own communication platform, safety cameras, water meters and other sensor systems, which he argues should be developed “to plug and play” with various platforms.

[\>>>](https://www.gcoportal.com/the-future-of-the-ready-mixed-concrete-truck/)

r/global_construction",https://www.reddit.com/r/automation/comments/veebyt/the_future_of_the_readymixed_concrete_truck/,AI
1381,PODCAST: Thermal Intelligence Offers Jobsite Fuel Efficiency,"This Digging Deeper episode features an interview with Mark Malekoff, director of Thermal Intelligence - an equipment manufacturer out of Edmonton, Alberta, Canada. Mark and his co-founder used their first-hand experience in dealing with equipment that didn’t meet their standards for reliability and efficiency and brought a sustainable solution to construction jobsites.

[\>>>](https://www.gcoportal.com/thermal-intelligence-offers-jobsite-fuel-efficiency/)

r/global_costruction",https://www.reddit.com/r/automation/comments/vbbkdd/podcast_thermal_intelligence_offers_jobsite_fuel/,AI
1384,Python program to automatically fill out forms in a word doc?," We're always filling out the same information on various forms for customers (company name, W9 tax info, address, etc). Clients will send us their vendor form which is unique to them, and they use different wording (organization name, company name, legal name, etc). It's the same information I'm copying and pasting all the time.

Any tools that can do this for me? Ideally it would try it's best to fill it out and then I'd just double check it to ensure it's accuracy. Maybe it highlights any cells it has problems with?

I imagine this isn't something that we'd have to build custom, it's not that powerful I think but I can't find any good tools that aren't overboard with AI.",https://www.reddit.com/r/automation/comments/v97gyk/python_program_to_automatically_fill_out_forms_in/,AI
1385,"I need a code that would take my ""Liked Videos"" from Youtube and feed it into an Excel sheet or Database."," I have 5k Liked Videos on youtube. I need a code that would take attributes of a Video :

Title, Link, Thumbnail (if possible), and Audio would be a dream.

and feed it into some form of storage like a MySQL database or an excel sheet.",https://www.reddit.com/r/automation/comments/v48y2c/i_need_a_code_that_would_take_my_liked_videos/,AI
1386,Automate laundry with QR codes,"So I just had this idea and wanted to know if it’s possible. So we print QR codes on insides of clothing item’s and when you scan them it gives you the option to select either “wearing”, “in the wash”, “in the dryer”
What this would do is simply tell you where your clothing is so you never have to go searching for it again, leaving a mess in the closet
I was thinking about making a app to go with it that visually shows what clothing you’ve been wearing, the ones you don’t use and probably should get rid of, but this is secondary to the QR code thing",https://www.reddit.com/r/automation/comments/un6rjt/automate_laundry_with_qr_codes/,AI
1387,"HELP! How can I update a list of 10K eCommerce data (Description, Logo, etc) in Airtable or Google Sheet or Excel?","Hey everyone, I have a list of 10K eCommerce names and URLs in Airtable I'd like to update with the company logo, description, industry, etc. What's the best way to go about it?

Don't want to spend a month doing this :(",https://www.reddit.com/r/automation/comments/ujwcfh/help_how_can_i_update_a_list_of_10k_ecommerce/,AI
1389,automate excel entry,"hello everyone. I have zero knowledge about coding so if this can't be done without prior coding experience please let me know. 

I work for a good sized company that outfits industrial products in Utah, Nevada, Wyoming and some parts of California. I am apart of our accounts payable department and right the work load is more than we can handle. We switched to Infor CSD in August of 2021 and we are still struggling to get out of the hole we dug. Part of our job is that we need to reconcile the credit card charges our salesmen enter into Wells Fargo. We download an excel worksheet from Excel and then manually enter all the credit charges looking for pricing issues, unit of measure errors or anything else that would hold the entry back from being processed and shown as closed. when doing an entry it goes like this:

&#x200B;

PO number, vendor ID, invoice number, the amount and that's that.

we then move to the next task where we look at the purchase order and look for anything that would make this invoice an issue. if all is clear we just validate, and then final update the entry and we never see it again unless we need access to it for future reference. I was wondering if there was a way to set up a macro or maybe some basic coding that could take the information from Excel, enter for me and then at the end of everything, I will look at any entries that are flagged for an error. 

we have about 4 months of credit cards to reconcile and each month has about 200 or so lines in excel.

if this isn't enough info to go off of please let me know and I can try to provide more. I don't want to post a super long question and risk leaking customer info or anything like that.",https://www.reddit.com/r/automation/comments/uht32a/automate_excel_entry/,AI
1391,Automated light switch with Python (preferably commercialy available),"Hello everyone, I need to get a light or switch to place in a optical research setup. The idea is that while the measurements are being done the light is off, this can be done manually but sometimes we forget and we have to repeat everything. I thought of using a Arduino but there has to be a commercially available device that does this. Any idea?",https://www.reddit.com/r/automation/comments/ue4ocd/automated_light_switch_with_python_preferably/,AI
1397,"How to click in this element using selenium which has no id and name,","<span class=""ui button"" style=""user-select: none;""><em class=""icons"">search</em><i></i></span>

&#x200B;

span = driver.find\_element(By.XPATH, ""//span\[@class='ui button'\]"")

attach=wait(driver,20).until(ec.presence\_of\_element\_located((By.XPATH,""//html//body//div//div\[4\]//span\[4\]//em\[@class='search'\]"")))

ive tried this and also tried it with class name but not working",https://www.reddit.com/r/selenium/comments/10e3wtv/how_to_click_in_this_element_using_selenium_which/,AI
1398,This button is physically visible but...,"if you could find a button that doesn't have xpath, cssSelector, id, name, class, className, containsText, or tagName and doesn't have an id name in the DOM - how would you go about finding this button?  It is physically visible and clickable and I have no clue what to ask OpenAI at this point.",https://www.reddit.com/r/selenium/comments/10e2wcr/this_button_is_physically_visible_but/,AI
1400,General advice on setting up tests,"Hello! I am pretty new to testing and Selenium and I feel as though I'm *not getting it*. 

I'm currently building an e-commerce portfolio app with Django; right now I have my accounts app set up to register new users, log them in/out, and delete their accounts. Presumably I'd like to test all those features in a script: load my app, create a new user, log that new user in, log them out, delete the account.

I've encountered a countless number of technical difficulties even performing one of these tasks. I've found the official documentation to be contradictory and confusing (maybe it's just me); every tutorial I've found so far has used outdated syntax and only delves into the most uselessly superficial tasks (loading a URL and that's it).

So I'd like some advice on where to go to figure out what the process is for testing what I'm aiming to test. **What's the general strategy for setting up these tests**? Are there any up-to-date resources available that focus on more useful testing processes?

For a specific example of a problem I'm encountering: how does one handle loading a different page during the test? I have been able to register a new user; on clicking ""submit,"" it takes them to a login page. How do I wait for the new login page to load before continuing? Implicitly waiting doesn't seem to do anything, but `time.sleep()` does.

Even if someone has a link to a repo that includes some tests in Python (especially if it's Django!) would be wonderful to see. I learn by example pretty well. Thanks for any advice.",https://www.reddit.com/r/selenium/comments/109gvcx/general_advice_on_setting_up_tests/,AI
1402,Dealing with StaleElementReferenceException error,"Hi,

I am new to Selenium and I am getting a StaleElementReferenceException error but not sure why. I have tried to debug to no avail. It would be great if someone could point me to the issue. I have posted below links to the code on Gist and the stack trace as well. I have posted the code that contains the page object, the test and the stack trace.

NB: I have tried to link to the code I have posted on  Github Gist for easier reading  but it seems that Reddit will not allow external links in posts, which is unfortunate.  

[EditCustomer.java](https://EditCustomer.java)This is the page object.  

    package com.internetBanking.pageObjects;
    
    import java.time.Duration;
    
    import org.openqa.selenium.WebDriver;
    import org.openqa.selenium.WebElement;
    import org.openqa.selenium.support.CacheLookup;
    import org.openqa.selenium.support.FindBy;
    import org.openqa.selenium.support.How;
    import org.openqa.selenium.support.PageFactory;
    
    public class EditCustomerPage {
    	WebDriver driver;
    	
    	public EditCustomerPage(WebDriver driver) {
    		this.driver = driver;
    		PageFactory.initElements(driver, this);
    	}
    	
    	@FindBy(how = How.XPATH, using = ""//a[contains(text(),'Edit Customer')]"")
    	@CacheLookup
    	WebElement lnkEditCustomer;
    	
    	public void clickEditCustomer() {
    		lnkEditCustomer.click();
    	}
    	
    	
    	@FindBy(how = How.NAME, using = ""cusid"")
    	@CacheLookup
    	WebElement txtCustomerID;
    	
    	public void setCustomerID(String customerId) {
    		txtCustomerID.sendKeys(customerId);
    	}
    	
    	
    	@FindBy(how = How.NAME, using = ""AccSubmit"")
    	@CacheLookup
    	WebElement btnSubmit;
    	
    	public void submit() {
    		btnSubmit.click();
    	}
    	
    	
    	@FindBy(how = How.NAME, using = ""city"")
    	@CacheLookup
    	WebElement txtCity;
    	
    	public void custCity(String city) {
    		txtCity.sendKeys(city);
    	}
    	
    	public String getCustCity() {
    		return txtCity.getText();
    	}
    	
    	
    	@FindBy(how = How.NAME, using = ""state"")
    	@CacheLookup
    	WebElement txtState;
    	
    	public void custState(String state) {
    		txtState.sendKeys(state);
    	}
    	
    	public String getCustState() {
    		return txtState.getText();
    	}
    	
    	
    	@FindBy(how = How.NAME, using = ""sub"")
    	@CacheLookup
    	WebElement btnSubmitForm;
    	
    	public void submitForm() {
    		btnSubmitForm.click();
    	}
    	
    	
    	
    
    }
    

TC\_EditCustomer.java  This is the test

    package com.internetBanking.testCases;
    
    import java.io.IOException;
    import java.time.Duration;
    
    import org.testng.Assert;
    import org.testng.annotations.Test;
    
    import com.internetBanking.pageObjects.EditCustomerPage;
    import com.internetBanking.pageObjects.LoginPage;
    import com.internetBanking.utilities.XLUtils;
    
    public class TC_EditCustomer_004 extends BaseClass {
    	EditCustomerPage ec;
    	LoginPage lp;
    
    	@Test
    	public void EditCustomer() throws IOException, InterruptedException {
    		driver.manage().timeouts().implicitlyWait(Duration.ofSeconds(30));
    		driver.get(baseURL);
    		ec = new EditCustomerPage(driver);
    		lp = new LoginPage(driver);
    		
    		if (lp.iframeIsExists()) {
    			if (lp.iframeIsVisible()) {
    				logger.info(""GDPR popup displayed"");
    				System.out.println(""GDPR popup displayed"");
    				lp.switchToFrame();
    				lp.clickAccept();
    				lp.switchToDefault();
    			}
    		}
    
    		lp.setUserName(username);
    		lp.setPassword(password);
    		lp.clickSubmit();
    
    		ec.clickEditCustomer();
    
    		// retrieve customer number
    		String path = System.getProperty(""user.dir"") + ""\\src\\test\\java\\com\\internetBanking\\testData\\login.xls"";
    		String customerNumber = XLUtils.getCellData(path, ""Sheet1"", 1, 2);
    
    		// fill cust id and submit
    		ec.setCustomerID(customerNumber);
    		ec.submit();
    
    		// edit customer
    		ec.custCity(""Sheffield"");
    		ec.custState(""Yorkshire"");
    		ec.submitForm();
    
    		// dismiss alert
    		driver.switchTo().alert().accept();
    
    		// fill cust id and submit
    		Thread.sleep(5000);
    		ec.clickEditCustomer();
    		System.out.println(""Clicked Edit Cistomer"");
    		ec.setCustomerID(customerNumber);
    		ec.submit();
    		
    		//Verify if successfully edited
    		if(ec.getCustCity().equalsIgnoreCase(""Sheffield"") && ec.getCustState().equalsIgnoreCase(""Yorkshire"")) {
    			Assert.assertTrue(true);
    		}
    		else {
    			Assert.assertTrue(false);
    		}
    
    	}
    
    }
    

Stack Trace

    org.openqa.selenium.StaleElementReferenceException: stale element reference: element is not attached to the page document
      (Session info: chrome=107.0.5304.107)
    For documentation on this error, please visit: https://selenium.dev/exceptions/#stale_element_reference
    Build info: version: '4.5.0', revision: 'fe167b119a'
    System info: os.name: 'Windows 10', os.arch: 'amd64', os.version: '10.0', java.version: '11.0.11'
    Driver info: org.openqa.selenium.chrome.ChromeDriver
    Command: [ec39b1f7efd2e4cc6d31633d4c66d44b, sendKeysToElement {id=3c29de5c-eb57-4512-b455-b6a4bd6d35d6, value=[Ljava.lang.CharSequence;@3313d477}]
    Capabilities {acceptInsecureCerts: false, browserName: chrome, browserVersion: 107.0.5304.107, chrome: {chromedriverVersion: 107.0.5304.62 (1eec40d3a576..., userDataDir: C:\Users\fsdam\AppData\Loca...}, goog:chromeOptions: {debuggerAddress: localhost:50466}, networkConnectionEnabled: false, pageLoadStrategy: normal, platformName: WINDOWS, proxy: Proxy(), se:cdp: ws://localhost:50466/devtoo..., se:cdpVersion: 107.0.5304.107, setWindowRect: true, strictFileInteractability: false, timeouts: {implicit: 0, pageLoad: 300000, script: 30000}, unhandledPromptBehavior: dismiss and notify, webauthn:extension:credBlob: true, webauthn:extension:largeBlob: true, webauthn:virtualAuthenticators: true}
    Element: [[ChromeDriver: chrome on WINDOWS (ec39b1f7efd2e4cc6d31633d4c66d44b)] -> name: cusid]
    Session ID: ec39b1f7efd2e4cc6d31633d4c66d44b
    	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    	at org.openqa.selenium.remote.codec.w3c.W3CHttpResponseCodec.createException(W3CHttpResponseCodec.java:200)
    	at org.openqa.selenium.remote.codec.w3c.W3CHttpResponseCodec.decode(W3CHttpResponseCodec.java:133)
    	at org.openqa.selenium.remote.codec.w3c.W3CHttpResponseCodec.decode(W3CHttpResponseCodec.java:53)
    	at org.openqa.selenium.remote.HttpCommandExecutor.execute(HttpCommandExecutor.java:184)
    	at org.openqa.selenium.remote.service.DriverCommandExecutor.invokeExecute(DriverCommandExecutor.java:167)
    	at org.openqa.selenium.remote.service.DriverCommandExecutor.execute(DriverCommandExecutor.java:142)
    	at org.openqa.selenium.remote.RemoteWebDriver.execute(RemoteWebDriver.java:547)
    	at org.openqa.selenium.remote.RemoteWebElement.execute(RemoteWebElement.java:257)
    	at org.openqa.selenium.remote.RemoteWebElement.sendKeys(RemoteWebElement.java:113)
    	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    	at org.openqa.selenium.support.pagefactory.internal.LocatingElementHandler.invoke(LocatingElementHandler.java:52)
    	at com.sun.proxy.$Proxy24.sendKeys(Unknown Source)
    	at com.internetBanking.pageObjects.EditCustomerPage.setCustomerID(EditCustomerPage.java:34)
    	at com.internetBanking.testCases.TC_EditCustomer_004.EditCustomer(TC_EditCustomer_004.java:57)
    	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    	at org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:139)
    	at org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:677)
    	at org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:221)
    	at org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)
    	at org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:962)
    	at org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:194)
    	at org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:148)
    	at org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)
    	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
    	at org.testng.TestRunner.privateRun(TestRunner.java:806)
    	at org.testng.TestRunner.run(TestRunner.java:601)
    	at org.testng.SuiteRunner.runTest(SuiteRunner.java:433)
    	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:427)
    	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:387)
    	at org.testng.SuiteRunner.run(SuiteRunner.java:330)
    	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)
    	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:95)
    	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1256)
    	at org.testng.TestNG.runSuitesLocally(TestNG.java:1176)
    	at org.testng.TestNG.runSuites(TestNG.java:1099)
    	at org.testng.TestNG.run(TestNG.java:1067)
    	at org.testng.remote.AbstractRemoteTestNG.run(AbstractRemoteTestNG.java:115)
    	at org.testng.remote.RemoteTestNG.initAndRun(RemoteTestNG.java:251)
    	at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:77)

&#x200B;",https://www.reddit.com/r/selenium/comments/1086dfo/dealing_with_staleelementreferenceexception_error/,AI
1404,Python & Selenium - help / ideas,"Hi All,

This probably isn't the cleanest code anyone has seen but, currently I am looking for some help or even ideas.   This code I've made is a project for fun, reason why I made this is I like to travel and yes I get there are other things like Hopper and FlightTracker but wanted to try some things on my own.

&#x200B;

**Here is what the code does:** It goes to the [AA.com](https://AA.com) site > Searches for the airport I depart from and want to arrive > Enters in the travel dates > Searches for them > AA (Tells me the dates are incorrect) I tell it to hit the submit button again  and it works > Then it takes a screen shot of the  depart flight of the first half of the page, saves it in my downloads then  clicks on the first box because it is the cheapest > Then Takes a screenshot of a return flight and saves it to my download.

(I haven't put this code on reddit but if anyone wants it I can easily give it to them.)  The next steps are I have another script run a couple minutes after > Picks up the files I saved to my downloads > Attaches it to an email and then the email sends it to me)

&#x200B;

**What i'm trying to get help with is i'm trying to get rid of the old way screenshots and putting this info into an excel document, or even put text into an email with Flight number, Price, Date, Time... ETC but i've ran into a road block and i'm not even sure if this is possible. Would love some help if anyone has experience.**

&#x200B;

`from turtle import clear`
`from selenium import webdriver`
`from selenium.webdriver.common.keys import Keys`
`from selenium.webdriver.common.by import By`
`from selenium.webdriver.support.ui import WebDriverWait`
`from selenium.webdriver.support import expected_conditions as EC`
`import timeimport os`

`if os.path.exists(""C:/Users/Test/Downloads/AA/(Filename).png""):os.remove(""C:/Users/Test/Downloads/AA/(Filename).png"")else:print(""The file does not exist"")`

`if os.path.exists(""C:/Users/Test/Downloads/AA/(Filename2).png""):os.remove(""C:/Users/Test/Downloads/AA/(Filename2).png"")else:print(""The file does not exist"")`

`chrome_options = webdriver.ChromeOptions()chrome_options.add_argument(""--incognito"")driver = webdriver.Chrome(executable_path=""C:/Users/Test/Downloads/chromedriver_win32/chromedriver.exe"",options=chrome_options)`

#Variables
`ID1 = ""slice0Flight1MainCabin""`
`NAME = ""segments[0].orgin""`
`NAME1 = ""segments[0].destination""`
`NAME2 = ""segments[0].travelDate""`
`NAME3 = ""segments[1].travelDate""`
`NAME4 = ""closeBannerButton""`
`XPATH = ""//*[@id='flightSearchSubmitBtn']""`
`XPATH2 = ""//*[@id='slice0Flight1MainCabin']""`
`LINK_TEXT = ""https://www.aa.com/booking/find-flights""`

`driver.get(LINK_TEXT)`

`print(driver.title)`

`time.sleep(10)`

`button = driver.find_element(By.NAME, NAME4)`[`button.click`](https://button.click)`()`

`search = driver.find_element(By.NAME, NAME)search.send_keys(""PHX"")`

`search = driver.find_element(By.NAME, NAME1)`

`search.send_keys(""LHR"")`

`search = driver.find_element(By.NAME, NAME2)`

`search.send_keys(""09/20/23"")`

`time.sleep(5)search = driver.find_element(By.NAME, NAME3)`

`search.send_keys(""09/27/23"")`

`time.sleep(5)button = driver.find_element(By.XPATH, XPATH)`

[`button.click`](https://button.click)`()`

`#Sleep timertime.sleep(45)`

`button = driver.find_element(By.XPATH, XPATH)`

[`button.click`](https://button.click)`()`

`#Sleep timertime.sleep(20)`

`driver.execute_script(""window.scrollTo(0,500)"")driver.get_screenshot_as_file('C:/Users/Test/Downloads/AA/(FileName).png')`

`#Sleep timer`

`time.sleep(20)`

`button = driver.find_element(By.ID, ID1)`

`driver.execute_script(""arguments[0].click();"", button)`

`time.sleep(8)`

`driver.execute_script(""window.scrollTo(0,700)"")`

`driver.get_screenshot_as_file('C:/Users/Test/Downloads/AA/(FileName2).png')`

`driver.quit()`

&#x200B;

&#x200B;

Edit1: Weird spacing in my post",https://www.reddit.com/r/selenium/comments/102sxzq/python_selenium_help_ideas/,AI
1406,"[C#] How to resolve ""Cannot access a disposed object"" error","Hey, folks. I've got an error that keeps coming up in a variety of tests, seemingly at random. I'm sure it's not random, but I can't identify the pattern (and subsequently the fix).

For context I have 29 tests running on windows VMs through Azure DevOps. I've got it set to 10 threads (browsers) but I can change that.

The error comes from somewhere I wouldn't expect it. Basically, I'm waiting for the invisibility of an element. Something like:

>return wait.Until(ExpectedConditions.InvisibilityOfElementLocated(By.XPath(locator)));

or

>element.Click();

This isn't a complicated line of code, and generally speaking if it fails I would expect to get an exception. But ""Cannot access a disposed object"" doesn't really tell me what the problem is or how to resolve it.

&#x200B;

It's important to note that these tests don't fail when I run them on my machine against a browser (i.e. not in a VM). I'm not sure if it has something to do with timing, with threading. Any clues are appreciated.",https://www.reddit.com/r/selenium/comments/zzf828/c_how_to_resolve_cannot_access_a_disposed_object/,AI
1407,Youtube Ad Detector," I’ve been trying to use python selenium to watch YouTube videos for me and collect data. Getting the data is fairly easy, however, I run into problems when an ad pops up in YouTube. 

 For some reason, I can't figure out how to detect whether or not I have an ad.

  My current function is: 

def check\_ad():  try:             

WebDriverWait(driver, 20).until(         EC.presence\_of\_element\_located(driver.find\_element\_by\_xpath('//\*\[@id=""simple-ad-badge:g""\]'))             )           

print(""Ad detected"")        

 except:            

 print(""No Ad"")

Does anyone know any other way I can do this?",https://www.reddit.com/r/selenium/comments/zwarjd/youtube_ad_detector/,AI
1408,"C# - Element.Click() returns error, after waiting for element to be clickable.","Hey, folks. I'm losing my mind on this one. I have this block of code:

>getWaitUtils.waitForClickabilityOfElement(GetElement(elementName));  
>  
>GetElement(elementName).Click();

The first line uses this:

>return wait.Until(ExpectedConditions.ElementToBeClickable(givenElement));

So I have an element (IWebElement, since I'm in C#). I wait for that element to be clickable. That line passes. The next line attempts to click the element (that selenium has confirmed is clickable). I get an error:

>OpenQA.Selenium.ElementClickInterceptedException: element click intercepted: Element is not clickable at point (1173, 1113)

I don't get it. What's the point of the wait if the element can't be clicked? What do?",https://www.reddit.com/r/selenium/comments/ztqe7u/c_elementclick_returns_error_after_waiting_for/,AI
1409,Flush all like buttons,"Hello, 

I need help with iterating some like buttons on my LinkedIn feed. I was able to use the contains ""like"" descriptor to find all the buttons and scroll the page, but my current function keeps clicking the 1st like button even though it is not visible any longer. I have attempted to flush the variable, but the driver retains the original button as its main. Snippet below: 

&#x200B;

`def rerun():`  
 `print('running like function ')`  
 `all_buttons = buttons = driver.find_elements('xpath', ""//button[contains(.,'Like')]"")`  
 `like_buttons = [btn for btn in all_buttons]`  
 `while len(like_buttons) >= 1:`  
`for btn in like_buttons:`  
`driver.execute_script(""arguments[0].click();"", btn)`  
`driver.execute_script(""window.scrollBy(0,3000)"","""")`  
`time.sleep(2)`  
`del like_buttons`  
`del all_buttons`  
`print('bot is liking')`  
`rerun()`",https://www.reddit.com/r/selenium/comments/zrmis1/flush_all_like_buttons/,AI
1411,Custom profiles of chrome not running in multithreading,"Hi Everyone,

I have an issue ongoing, I am trying to run custom chrome profiles with selenium,

The issue is that a single profile runs fine but when I use ThreadPoolExecutor, and open like three chrome profiles in parallel, one out of them works fine but the rest two do not do anything, they are just like halted. The code is concerned  is as follow:

`def browserthread(link):`  
 `i=links.index(link)`  
 `chrome_options = webdriver.ChromeOptions()`  
 `chrome_options.add_argument(""user-data-dir=C:\\Users\\LENOVO\\AppData\\Local\\Google\\Chrome\\User Data"")`  
 `chrome_options.add_argument(f""--profile-directory=Profile {str(i+1)}"")`  
 `driver = webdriver.Chrome(options=chrome_options)`  
 `drivers.append(driver)`

`with ThreadPoolExecutor(max_workers=threadnum) as pool:`  
 `response_list = list(pool.map(browserthread,links))`  
`drivers.clear()`

If multiple threads are run without profile specification, than all the chrome instances work fine, but when three profiles are opened in separate threads, only one instance works fine meanwhile other two remain halted.

Please help if you know a solution to this issue, thanks in advance.",https://www.reddit.com/r/selenium/comments/zqnu0i/custom_profiles_of_chrome_not_running_in/,AI
1412,Why I can't find an element with time sleep but with webdriverwait the element appears,"Why I can't find an element with time.sleep even with 100 seconds wait but with webdriverwait the element appears with even much less wait time, what's the mechanism behind it",https://www.reddit.com/r/selenium/comments/zox3xu/why_i_cant_find_an_element_with_time_sleep_but/,AI
1413,Custom Chrome Profile not opening in Selenium,"Hi everyone,

I am facing a problem for days with selenium in opening a custom-made profile, I am using the following line of code to open it but failing:

`chrome_options.add_argument(""user-data-dir=C:\\Users\\LENOVO\\AppData\\Local\\Google\\Chrome\\User Data\\Profile 1"")`

In order to open the default profile it just needs to remove the last part of the path, like this:

`chrome_options.add_argument(""user-data-dir=C:\\Users\\LENOVO\\AppData\\Local\\Google\\Chrome\\User Data"")`

It opens the default profile successfully but whenever I try to open a custom-made profile it opens the chrome with native selenium setting,

How can this issue be resolved?

Thanks in advance.",https://www.reddit.com/r/selenium/comments/zmpvy4/custom_chrome_profile_not_opening_in_selenium/,AI
1417,Getting a NoSuchElementException when trying to dismiss popup,"I am testing a demo website for practice and I am receiving an error when trying to dismiss a cookie permissions popup (not really a popup but an iframe). The popup only appears once in an open browser session and will only appear if you close the browser and reopen website. I am testing logins using test data from an xls, so when the  webpage is opened, it dismisses the cookie popup logs in and then logs out and then attempts the next login in the xls. It tries to look for the cookie popup which will not appear as we have not closed the browser. I have written an 'if' statement that checks for the popup, to dismiss popup if it appears or continue as normal if it doesn't. But it does not continue and then fails the test.  I would like some help on what is causing this.

Here is the error message:

**org.openqa.selenium.NoSuchElementException: no such element: Unable to locate element: {""method"":""css selector"",""selector"":""#gdpr\\-consent\\-notice""}**

Here is a link to the code for the test. The If statement is on line 23.

[https://gist.github.com/fdama/5a73c1f95319f09266120dd658b425cc](https://gist.github.com/fdama/5a73c1f95319f09266120dd658b425cc)

Thanks in advance.",https://www.reddit.com/r/selenium/comments/zeziub/getting_a_nosuchelementexception_when_trying_to/,AI
1420,Element not interactable,"Hi Reddit, I m working on a script that uses selenium to click on all of the jobs on indeed. I have found that without fail it always returns an ""element not interactable"" error on the 11th LI element. I have tried to implement an implicit wait to wait until the element was clickable and it just resulted in a timeout error. This is the code that I have so far

    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.wait import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver import ActionChains
    import time
    import pandas as pd
    
    intialLink = 'https://www.indeed.com/jobs?q=software+engineer&l=Connecticut&vjk=d2a438c96f6e9c7e&from=gnav-util-jobsearch--indeedmobile'
    driver = webdriver.Chrome(executable_path='C:<path ommited for privacy reasons>\\chromedriver.exe')
    driver.get(intialLink)
    jobPannels = driver.find_elements(By.CSS_SELECTOR,"".jobsearch-ResultsList > li"")
    
    #it starts at the 9th li element
    for i in range(9, len(jobPannels)):
        print(jobPannels[i].tag_name)
        ActionChains(driver).move_to_element(jobPannels[i]).perform()
        #wait = WebDriverWait(driver, 15)
        #wait.until(EC.element_to_be_clickable(jobPannels[i]))
        time.sleep(1)
        jobPannels[i].click()

I've tried to look this up and all I can find are people saying to use the wait for it to work and like I said before I didn't get that to work. I suspect that this is something to do with the underlying HTML of the site.

Solution: I found out that it was the 12 li that was giving me trouble, not the 11th. The reason for this was that the 12 li only contained an empty div.",https://www.reddit.com/r/selenium/comments/z9uyn2/element_not_interactable/,AI
1421,Can't find an element which is visible on the windk,"I want to scrape the website: https://www.theguardian.com/world/coronavirus-outbreak
for newslinks.
Once the page is opened it asks to if or not accept cookies. There is a button to accept it which is visible in the screen which I want to click.
I tried to find it using xpath, class etc but no element is found.
I tried using wait to find the elements still it doesn't work. 
Can anyone help me solve this issue?",https://www.reddit.com/r/selenium/comments/z8vlmg/cant_find_an_element_which_is_visible_on_the_windk/,AI
1422,Issue with Selenium tests on AWS,"Hi all...

I've written a simple browser test with Python/Selenium that runs fine locally from my Mac, as well as from AWS Linux and Ubuntu Docker containers on my Mac. However, if I run the containers on an EC2 instance (with Docker installed, obvs), the test fails, always on the same step (which is loading a login page). I've tried an Ubuntu AWS EC2 instance and just installed all the component manually to run it natively from there, but that also fails in the same place.

So it seems that the issue is something to do with AWS, but I cannot for the life of me figure out what it may be so wondered if any of you glorious people might?",https://www.reddit.com/r/selenium/comments/z7r7lx/issue_with_selenium_tests_on_aws/,AI
1423,Get 5GB of clean Residential Proxies for free,"Hey all!

I'm looking for a few developers who are doing web scraping to make interviews about the proxy setup experience. The interview usually does not take more than 30-40 minutes. As a reward, I can offer 5GB of Residential Proxies.

Thanks in advance and don’t hesitate to DM me or at [product@soax.com](mailto:product@soax.com) to schedule a call.",https://www.reddit.com/r/selenium/comments/z7tgeg/get_5gb_of_clean_residential_proxies_for_free/,AI
1424,How to disable geckodriver.log file i dont want it to be created anywhere,"straight forward question i dont want any logs from geckodriver,

i am using python selenium with firefox and everytime i run the driver a geckodriver.log file gets created. how can i disable that i dont want this file to be created.",https://www.reddit.com/r/selenium/comments/z6ygq8/how_to_disable_geckodriverlog_file_i_dont_want_it/,AI
1426,xpath breaking,"So, I have a python script that at some point needs to get information from a website.
Everything is fine when I try to get ellement a, but element b breaks. This element lies deeper in the html code. Nothing would work.
I did figure out that after passing the 4th div or so that was when the xpath broke. When playing around with the website it seems that is roughly where the html changes when you press certain buttons.
 I figure the website makes use of something akin to tabs, but nothing seems to reflect this in the html. (And the ""default"" tab is the one I need anyways)
I can't really share the html and in python I've tried practicaly any way to access it that might exist (with the exception of going through sibling elements, as any element that is somewhat close to it is also unreachable)
Does anyone have an idea how I could fix this?",https://www.reddit.com/r/selenium/comments/z3a4am/xpath_breaking/,AI
1428,Select only buttons with aria-pressed,"Hello, 

&#x200B;

Im working on improving my like bot. I can get all the like buttons, but the bot is very indiscriminate about what it likes as I'm using 

&#x200B;

`driver.find_elements('xpath', ""//button[contains(.,'Like')]"")`

I need to find a way to only click on items that are `aria-pressed=""false'` 

&#x200B;

I have tried adding the condition after the statement as follows:

`river.find_elements('xpath', ""//button[contains(.,'Like')]"" and aria-pressed=""false')`  

&#x200B;

&#x200B;

Any suggestions are highly appreciated.",https://www.reddit.com/r/selenium/comments/z1rgdn/select_only_buttons_with_ariapressed/,AI
1429,How do you deal with 'difficult' elements?,"Bit of a selenium noob so apologies upfront. 

I often come across elements that are very clear and seem easy to interact with, just by looking at the page. However, when I try to click on said element it comes up saying it's not clickable. Do you have a checkbox list of things you work through to click on difficult elements like these? In my mind I'm picturing a flowchart sort of like, if element not clickable, is it a popup- do X, is it an input box - do Y. If that makes sense",https://www.reddit.com/r/selenium/comments/yy4y8l/how_do_you_deal_with_difficult_elements/,AI
1430,Selenium Find button containing text,"Hello,

&#x200B;

Im working on building a small linkedin bot that clicks on likes for my company's posts. The issue at the moment is that all like buttons are dynamic and therefore,  I cannot select via the regular text options. I have been trying to see if I could get something like the following working, but I'm getting an error::

`like = driver.find_element('xpath', ""//button[contains(text(),'Like')]"")`

`print(like)`

&#x200B;

Any help is greatly appreciated.",https://www.reddit.com/r/selenium/comments/yxptrt/selenium_find_button_containing_text/,AI
1431,Help with uploading file on Python Selenium using remote driver,"I am using python 3.9 and selenium 4.6.0 on Chrome. I have a script that needs to upload a file to an input, this works fine on local but fails when run on RemoteDriver. The code I am using is

    driver.find_element(By.XPATH, ""//input[@accept]"").send_keys('path to file')

When run on RemoteDriver the error returned is

    selenium.common.exceptions.WebDriverException: Message: unknown command: unknown command: session/cddd71e067d7717481fb8a635103c643/se/file

I've think it is due to this line in the remote\_connection.py file in selenium

    Command.UPLOAD_FILE: ('POST', ""/session/$sessionId/se/file"")

From the research I've done the 'se' in this case is a 'vendor\_prefix' for selenium but I cannot figure out how to either configure the remote driver to use a vendor prefix or remove that from POST path that is being passed (short of pulling my own version of the code and maintaining that).

For other functional reasons I can't revert to selenium 3x (which is an option I've seen to correct this), nor can I set w3c to False. Does anyone know how to work around this particular issue; either by getting send\_keys to operate as expected in this situation or using another method to upload the file? Thanks.",https://www.reddit.com/r/selenium/comments/yx49ls/help_with_uploading_file_on_python_selenium_using/,AI
1432,Trying to soup a page to get to an element because selenium is not finding it,"[https://www.who.int/data/gho/data/indicators/indicator-details/GHO/proportion-of-population-below-the-international-poverty-line-of-us$1-90-per-day-(-)](https://www.who.int/data/gho/data/indicators/indicator-details/GHO/proportion-of-population-below-the-international-poverty-line-of-us$1-90-per-day-(-))

Open this url and go to Data tab, inside data tab we have the unclickable link on the right side with inner text: EXPORT DATA in CSV format: Right-click here & Save link

in order to get the data i need to right click and save link as then save in order to get the data i need.

im trying to reach this element by using beautifulsoup but no matter what i do i can't see to find this <a> tag nor its href attribute.

i tried using selenium to get driver.source\_page then use that source\_page in beautifulsoup it still didnt get me the element.

and i tried using selenium itself find\_element i also couldn't reach that <a> i need.

what can i do?",https://www.reddit.com/r/selenium/comments/yvuqq9/trying_to_soup_a_page_to_get_to_an_element/,AI
1437,"Trying to Scroll inside a div with selenium, scroller function only scrolls up to a certain amount and then just stops","I want to get a list of all the list items which are present inside a div with a scroller. They are not loaded at once upon loading the page, rather the items are loaded dynamically as the user scrolls down (until there are no elements left). So, this is the scroller script which I tried to implement:

    def scroller():
        userList = None
        prev = 0    
    
        while True:
            time.sleep(5)
            userList = WebDriverWait(browser, 50).until(
                EC.presence_of_all_elements_located(( By.CLASS_NAME, '<class of list item>' ))
            )
            n = len(userList)
            if n == prev:
                break
            prev = n
            #getting the last element in the list in the view
            userList[-1].location_once_scrolled_into_view

This function scrolls the list upto a certain length, but doesn't go to the full length of the elements (not even half). Can someone please suggest a better way to do this?

Thank you",https://www.reddit.com/r/selenium/comments/yr3zpj/trying_to_scroll_inside_a_div_with_selenium/,AI
1438,Pressing spacebar in selenium (python) to scroll down in a table element,"What I need to do is, I need a list of all the elements which are basically list-items, but the list doesn't load at once, instead it loads part by part, so the following code doesn't get the list of all the list elements:

    userList = WebDriverWait(browser, 5000).until(
     EC.presence_of_all_elements_located(( By.CLASS_NAME, 'c-virtual_list__item' ))
    )

So, in order to get the list of all the elements present in the list/table, I need to scroll all the way down in the table. I am trying to do that by trying to replicate the following process:

1. Select the element with a scroller by clicking on it
2. Press space to scroll down

I wrote the following piece of code to try and accomplish that:

    scroller = WebDriverWait(browser, 5000).until(
        #this is a div element which contains a scroller
        EC.presence_of_element_located(( By.CLASS_NAME, 'c-table_view_keyboard_navigable_container' ))
    )
    
    prev = 0
    userList = None
    
    #scrolling until I read the end of the list
    while True:
        scroller.send_keys(Keys.SPACE)
        time.sleep(2)
        userList = WebDriverWait(browser, 5000).until(
            EC.presence_of_all_elements_located(( By.CLASS_NAME, 'c-virtual_list__item' ))
        )
        cur = len(userList)
        if cur == prev: break

But this line:  `scroller.send_keys(`[`Keys.SPACE`](https://Keys.SPACE)`)` throws an error:

>selenium.common.exceptions.ElementNotInteractableException: Message: element not interactable

I have seen some code snippets on stackoverflow where people select the body element:

`find_element(By.TagName, ""body"")`

and scroll down the webpage in a similar manner to what I have tried:

`element.send_keys(`[`Keys.SPACE`](https://Keys.SPACE)`)`

However, it doesn't work for me and throws the given error.

Can someone please help me make this work!?

&#x200B;

Thank you for your time :)",https://www.reddit.com/r/selenium/comments/yovwce/pressing_spacebar_in_selenium_python_to_scroll/,AI
1439,Can i find_element or find_elements that contain a class but this class only?,"lets say i have an elements like this:

    <div class=""full-width flex-buttons-container push""></div>
    <div class=""full-width push""></div>
    <div class=""full-width""></div>
    <div class=""full-width""></div>
    <div class=""full-width""></div>

if i use the following:

     driver.find_elements(By.CLASS_NAME,'full-width')

i will get all 5 divs but what i want is the elements that have full-width class and full-width class only.

so i want the last two divs only can i achieve something like that?",https://www.reddit.com/r/selenium/comments/yoh99r/can_i_find_element_or_find_elements_that_contain/,AI
1442,Need help with locating a button,"Hello, I am attempting to click the coupon button on  [https://coupons.safeway.com/weeklyad](https://coupons.safeway.com/weeklyad). I have tried the following code  to locate various coupons but it doesn't seem to be able to find the element. The error I get is  ""InvalidSelectorError"".  Does anyone have any tips? 

&#x200B;

    WebElement coupon1 = driver.findElement(By.xpath(""/html/body/flipp-router/flipp-publication-page/div/flipp-sfml-component/sfml-storefront/div/sfml-linear-layout/sfml-flyer-image[1]/div/button[1]""));
    

&#x200B;

    WebElement coupon1 = driver.findElement(By.cssSelector(""//button[@aria-label='Large Fuji Apples or Navel Oranges, , $1.28 lb Member Price . Select for details.']""));",https://www.reddit.com/r/selenium/comments/yi03yi/need_help_with_locating_a_button/,AI
1443,Element not Interactable exception,"Hi, I am trying to login to a website - [https://myaccount.play-cricket.com/idp-signin?state=bDdCdExYWXNzNVlTSTBPRUxiMjhzWU9KZW02SGhINTM0NWEySnNncE01VWZmcUZibjNMU2dYdjBuSXlhanltcg&client\_id=qqaXhehov6cu0sd7AEfd&redirect\_uri=https%3A%2F%2Flogin.ecb.co.uk%2Foauth2%2Fv1%2Fauthorize%2Fcallback&response\_type=code&scope=email+openid+profile](https://myaccount.play-cricket.com/idp-signin?state=bDdCdExYWXNzNVlTSTBPRUxiMjhzWU9KZW02SGhINTM0NWEySnNncE01VWZmcUZibjNMU2dYdjBuSXlhanltcg&client_id=qqaXhehov6cu0sd7AEfd&redirect_uri=https%3A%2F%2Flogin.ecb.co.uk%2Foauth2%2Fv1%2Fauthorize%2Fcallback&response_type=code&scope=email+openid+profile) using selenium. When I attempt to enter something into the password field I get the element not interactable exception. This is my code:  driver.find\_element([By.NAME](https://By.NAME), 'password').send\_keys(password) .

Any help would be appreciated",https://www.reddit.com/r/selenium/comments/yglfwr/element_not_interactable_exception/,AI
1451,Is there a method to wait without a timeout?,"Hello,

I have a method in which I load a new page and the user has to login. Currently I am waiting like this until the user is logged in:  
`WebDriverWait(self.driver, TIMEOUT).until(EC.presence_of_element_located((By.CLASS_NAME, ""navigiumlogo"")))`

(The logo shows when logged in, thats the condition I am waiting for here) But the problem here is that the user has to be logged in in 40 seconds (TIMEOUT) or it throws an error. And now my question is: Should I catch that error and ""rewait"" or is there a better way to wait until the user has logged in? (preferably without a timeout?)",https://www.reddit.com/r/selenium/comments/yc7ene/is_there_a_method_to_wait_without_a_timeout/,AI
1454,"Retrieve class schedule changes from website using python and selenium No Such Element error, ID, XPATH, and more.","My school has a system that tells us if our schedule has any changes.
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.support.select import Select


url = ""https://www.alliancetlv.com/עדכוני-מערכת""
driver = webdriver.Chrome()
driver.get(url)
driver.implicitly_wait(5.0)

examButton = driver.find_element(By.ID, 'TimeTableView1_btnChanges')
``` 
I'm trying to find an element, and later click it using selenium. every time i try to find literally anything it returns No Such Element error. I tried by ID, class name, name, and even **XPATH**.

this is the website: https://www.alliancetlv.com/עדכוני-מערכת
and I'm trying to click one of the tabs called ""changes/שינויים""

my end goal is to click the dropdown to the side, select a class, click the changes tab, then get all the data inside of it, then maybe format it. for now i just wanna understand why this wont fing work",https://www.reddit.com/r/selenium/comments/ya0tml/retrieve_class_schedule_changes_from_website/,AI
1455,"How to wait for a text to disappear, and then retrieve values from elements? Python","So, I'm trying to scrap values from a table. There's a <tr> element and inside of that there are 12 <td> elements which I want the values of. The issue is, for a split second, when the page loads, only one <td> element appears inside of the parent <tr> and this <td> contains a text, ""Some Text"" and then later it disappears and the <tr> element is populated with 12 <td> elements which holds the values I want. Here is what I've been doing...

    data_list = []
    driver.get(url)
    wait = WebDriverWait(driver, 30)
    data = driver.find_elements(By.TAG_NAME, ""td"")
    wait.until_not(EC.text_to_be_present_in_element((By.TAG_NAME, ""td""), ""Some Text""))
    
    for item in data:
      value = item.get_attribute(""innerHTML"")
      date_list.append(value)
    
    print(data_list)

But, I get this error from `value = item.get_attribute(""innerHTML"")`

    Message: stale element reference: element is not attached to the page document",https://www.reddit.com/r/selenium/comments/y93tfp/how_to_wait_for_a_text_to_disappear_and_then/,AI
1456,Log4j is not logging into file or console,"I'm new to Selenium and need help with logging. Currently cannot log anything using Log4j. Could you help mw with why? Here are some relevant snippets of my code:

Create logger as part of setup:

    public class BaseClass {
    	public String baseURL = ""https://demo.guru99.com/v3/index.php"";
    	public String username = ""demo"";
    	public String password = """";
    	public static WebDriver driver;
    	public static Logger logger;
    	
    	@BeforeClass
    	public void setup() {
    		WebDriverManager.edgedriver().setup();
    		driver = new EdgeDriver();
    		driver.manage().window().maximize();
    		
    		logger = LogManager.getLogger(""ebanking"");
    		
    		
    	}
    

Log something using Logger:

    public void loginTest() {
    		driver.get(baseURL);
    		logger.info(""URL is opened"");
    		LoginPage lp = new LoginPage(driver);
    		
    		driver.manage().timeouts().implicitlyWait(Duration.ofSeconds(2));
    		
    		if(lp.iframeIsVisible()) {
    			logger.info(""GDPR popup displayed"");
    			lp.switchToFrame();
    			lp.clickAccept();
    			lp.switchToDefault();
    		}

[log4j2.properties](https://log4j2.properties):

    name=PropertiesConfig
    property.filename = logs
    appenders = console, file
    
    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = [%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c{1} - %msg%n
    
    appender.file.type = File
    appender.file.name = LOGFILE
    appender.file.fileName=${filename}/MyLogs.log
    appender.file.layout.type=PatternLayout
    appender.file.layout.pattern=[%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c{1} - %msg%n
    
    loggers=file
    logger.file.name=demo
    logger.file.level = debug
    logger.file.appenderRefs = file
    logger.file.appenderRef.file.ref = LOGFILE
    
    rootLogger.level = debug
    rootLogger.appenderRefs = stdout
    rootLogger.appenderRef.stdout.ref = STDOUT",https://www.reddit.com/r/selenium/comments/y915ld/log4j_is_not_logging_into_file_or_console/,AI
1457,Driver doesn't locate the element sometimes even when using webDriverWait.,"Hello everyone.    
I'm trying to scrape the main page of TikTok. specifically the grid videos on the main page of any profile. for instance, if you inspect his page [https://www.tiktok.com/@shopcider](https://www.tiktok.com/@shopcider)   
and paste this CSS selector ""div\[mode='compact'\]"" you will get the gird of videos. Once I grab the element I can loop through each video by getting the divs inside the parent div.   
All is working perfectly. However sometimes when I run the code it keeps saying that this element can't be located!! even tho the previous run was successful.  


Im using selenuim js.  
`const videosGrid = await driver.wait(until.elementLocated(By.css(""div[mode='compact']"")), 5000)`  
`const videos = await videosGrid.findElements(By.xpath('div'))`  


I do have 5 seconds waiting for this element. I don't get why sometimes it works so fine and sometimes it doesn't find it.   
I'm using a headless browser and running the script on Heroku server.",https://www.reddit.com/r/selenium/comments/y8gksz/driver_doesnt_locate_the_element_sometimes_even/,AI
1462,Crawling site within shadowroot,"Hello, I'm a new trying to crawling several sites with bs4 + python.

it worked well til I found a site containing #shadow-root (open)

after some search, I understood it is a self DOM which can't grap as usual.



site structure

 <div style=""display"">    
   shadow-root (open)    
      <div class=""1""></div>    
      <section></section>    
       <div class=""2"">
            <ul></ul>
            <ul></ul>
            <ul></ul>
            <ul></ul>
            <ul></ul>
            ...
            <ul></ul></div>

</div>   



I tried to use pypi 'pyshadow'

shadow.find\_elements(""div\[class='2'\]"")



but it extract only some ul tag, not the whole ul tag



So I tried other thing



def expand\_element(element)

shadowroot = driver.execute\_('''return argument\[0\].shadowRoot''', element)

return shadowroot



tag\_shad = driver.find\_elements\_by\_xpath('여기에 div(class='1') XPATH')



And


shadow\_root = expand\_element(tag\_shad)

ul = shadow\_root.find\_elements(""div\[class='2'\]"")


But it gave me no element.


Can I get some help?",https://www.reddit.com/r/selenium/comments/y708k1/crawling_site_within_shadowroot/,AI
1464,Find elements by class name returns empty list,"Hi All,

I am fairly new to selenium, however have spent a fair amount of time trying alternate methods posted on various python forums in attempt to return a list of elements containing the class ""sports-event-entry"".  I have  tried multiple search types but all return an empty list.

Essentially I want to get the web address, then run:

data = driver.find\_elements(By.CLASS\_NAME,""sports-event-entry"")

I was wondering if anyone has had similar issues (in more recently times than some forums), and would be willing to share some trouble shooting techniques.

Cheers!",https://www.reddit.com/r/selenium/comments/y5dqv1/find_elements_by_class_name_returns_empty_list/,AI
1465,Helium/Selenium Cant Open New tab with hotkey,"so i have tried Ctrl + T and also F6 Key (to take me to the address  bar), but it goes straight to the search bar on google page. Please Help  Thanks

**My Code:**

    from selenium.webdriver import FirefoxOptions
      
    from helium import*   
    
    import time  
    
    useragent =""Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:86.0) Gecko/20100101 Firefox/86.0"" 
    
    options = FirefoxOptions() 
    
    options.set_preference(""general.useragent.override"",useragent) 
    
    s = start_firefox(f""https://www.google.com"",headless = False, options=options) 
    
    press(CONTROL + ""T"")  
    time.sleep(5) 
    
    write(""https://www.reddit.com"")   
    time.sleep(5) 
     
    kill_browser()",https://www.reddit.com/r/selenium/comments/y4p5hp/heliumselenium_cant_open_new_tab_with_hotkey/,AI
1467,What BDD/Cucamber type of language can be offered to business to create stories for UI automation without making SDETs life more difficult?,"I know that many SDETs would rather deal just with Selenium for UI tests and not with Selenium+Cucamber because maintaining both is a pain in the butt. If the business still wants some kind of BDD language to create story requirements in, what can you offer them without making your life as an SDET more difficult?  If I want to keep it simple and not deal with Cucamber, would it be a good idea to have the business create requirements in Gherkin and just keep them in Jira for references without actually running any Cucamber on my side?",https://www.reddit.com/r/selenium/comments/y437oc/what_bddcucamber_type_of_language_can_be_offered/,AI
1468,IsDisplayed() method and driver not recognized by intellij java,"When i try to assert.fail(elementlocator.isDisplayed()) i don't get the IsDisplayed method, its red and is unrecognised. I have selenium , webdriver and testng dependencies in mu pom.xml",https://www.reddit.com/r/selenium/comments/y40aq7/isdisplayed_method_and_driver_not_recognized_by/,AI
1469,Can't scrape price from website,"I'm struggling with my python script to print me the current price of certain items on a website.
I've tried so many different solutions I could find on Google but none of them is working.

This is how it looks on the website:

<span class=""h4 m-product-price"" >399,00 DKK</span>

I want my script to print 399,00 DKK

Are any of you guys able to help?",https://www.reddit.com/r/selenium/comments/y31hkx/cant_scrape_price_from_website/,AI
1471,Suggest a Reporting tool,"I need a reporting tool that has the following feature:
- Record Parallel tests (using ThreadLocal)
- Emailable report*
- Run History**

* - like ExtentReport where I can send it to stakeholders in a zip file and they can open the html file in their browser.
** - Something that would show me the number of tests run, pass count, and fail count of each test run.",https://www.reddit.com/r/selenium/comments/y1uaw0/suggest_a_reporting_tool/,AI
1472,I want to deploy selenium java on Netlify,"hey guys, i'm a little bit lost here. I want to deploy my selenium bot in netlify, so first of all, there's no tutorial, the closer i got was a tutorial on how to deploy in heroku, that itself isn't the issue cuz i manage to find the equivalent options in netlify, the issue is that the code part is written in python in the 3 sources i found, my small dumb monkey brain can't understand python, only poo languages (quite literally, only c# and java) and it seems i'm missing something in the docs, so, how could i translate this code or what would be the equivalency to java/c#? or at least, where's the specific documentation?

    import os
    from selenium import webdriver
    
    op= webdriver.ChromeOptions()
    op.binary_location = os.environ.get(""GOOGLE_CHROME_BIN"")
    op.add_argument(""--headless"")
    op.add_argument(""--no-sandbox"")
    op.add_argument(""--disable-dev-sh-usage"")
    
    driver = webdriver.Chrome(executable_path= os.environ.get(""CHROMDRIVER_PATH""),chrome_options=op)
    

&#x200B;",https://www.reddit.com/r/selenium/comments/y0y53x/i_want_to_deploy_selenium_java_on_netlify/,AI
1473,xpath help,"I know this is possible but can't wrap my head around the how. I have a table on a page where each row of the table is coded as an individual table. I need to click on the OPEN button on a specific row. But, the text in the cell I am looking for, I need to click the button in the previous cell. Anyone available to help here? Is there a good cheat sheet out there with problems and examples like this?

So ABC123 is what I need to look for, then I need to click the button located in the cell before that.

Thanks

    <div class=""dojoxGridRow dojoxGridRowOdd dojoxGridRowSelected"" role=""row"" aria-selected=""true"" style="""">
    <table class=""dojoxGridRowTable"" border=""0"" cellspacing=""0"" cellpadding=""0"" role=""presentation"" style=""width: 1128px; height: 30px;""><tbody>
    <tr><td tabindex=""-1"" role=""gridcell"" class=""dojoxGridCell nosort GridButton"" idx=""0"" style=""text-align: left;width:9%;""><div class=""grid-text-over""><input type=""button"" value=""Open"" class=""base-btn small green""></div></td>
    <td tabindex=""-1"" role=""gridcell"" class=""dojoxGridCell"" idx=""1"" style=""text-align: left;width:13%;""><div class=""grid-text-over"">ABC123</div></td> 

 Tried various combinations of this, but still not quite getting it.

    //input[@value='Open']//preceding::td[contains(text(),'ABC123'] 

Essentially I want to scan the entire page and look for a button that is followed by a cell that contains the text ""ABC123"".   I'm trying to click on that button.

Thanks for any pointers.",https://www.reddit.com/r/selenium/comments/y0j3b7/xpath_help/,AI
1475,Chromedriver and Google Chrome dependency,"Hi all,  


I was wondering if you guys ever had issues with `chromedriver` and google chrome not using the same version after your google chrome updates. Suddently your .py script won't be able to scrap data anymore on your remote server. I was wondering how you would prevent your script from failing to run. Maybe docker can help, but I was never able to make it work.  


Thanks for reading!",https://www.reddit.com/r/selenium/comments/xyy2hh/chromedriver_and_google_chrome_dependency/,AI
1476,"Multiple svg's of same class, want to select specific one.","I have the xpath to an svg object I want to click on: 

    XPath = ""//*[name()='svg' and u/class='r-4qtqp9 r-yyyyoo r-1xvli5t r-dnmrzs r-bnwqim r-1plcrui r-lrvibr r-1hdv0qi']""

The website is very nested, I was only able to find this object using the class code. However, there are multiple instances of this object, as found using: 

    s = np.random.normal(mu, sigma, 8)
    driver.get(report_url)
    
    cards = WebDriverWait(driver, s[2]/50).until(EC.visibility_of_all_elements_located((By.XPATH, Report_Circles)))
    Report_Circles_click=wait.until(EC.element_to_be_clickable(cards[-1]))
    
    time.sleep(s[1]/100)
    ActionChains(driver).move_to_element(Report_Circles_click).click().perform()

However, clicking on the last instance of this svg object isn't actually correct, I want to click on the specific svg circle that precedes an a href: 

    <a href=""/badcatstuff/status/1577447899591544833""

wondering if I could find that specific href first, then click on the svg object preceding that? 

if so, how would the xpath look like?",https://www.reddit.com/r/selenium/comments/xychlg/multiple_svgs_of_same_class_want_to_select/,AI
1477,How can I find_elements using multiple By's in python?,"ByChained doesn't work. Googling doesn't helps. I'm frustrated. I can try to make my code work by using bad practises (like indexing), but I don't want to do it",https://www.reddit.com/r/selenium/comments/xxurpi/how_can_i_find_elements_using_multiple_bys_in/,AI
1479,Hello I am trying to locate this element with the condition.,"Hi guys, I am new to Selenium. In my browser, when I am trying to locate and click the ""Missing"" button with the condition is the MPC need to match.  Sometime, when I search the items, it return multiple ""Missing"" button. I just want to click the missing which have the corrected MPC. Sometimes, it is first option. Sometimes, it is second or third options. I tried normalize-space. However, I cannot find a way to make it only choose the missing that is coordinate with the right MPC. Is it any way I can do it? Below is the link to the code.

browser.find\_elements(By.XPATH,""//div\[@class='x-grid-group-body'\]\[contains(text(),'10266')\] and \[contains(text(),'Missing')\]"")

[https://imgur.com/a/qu1oDAk](https://imgur.com/a/qu1oDAk)",https://www.reddit.com/r/selenium/comments/xvupko/hello_i_am_trying_to_locate_this_element_with_the/,AI
1485,just can't click this button,"I have tried findElement, Actions, CDP (gets IndexOutOfBoundsException) and JS. I have scrolled to the bottom, changed the location and size, waiting a fixed amount and for clickability. Selenium finds the element but says ""element not interactable: \[object HTMLInputElement\] has no size and location"" Can anyone suggest a way to make click work? $[0.click](https://0.click)() in devtools works. The button is obviously there. The site is [here](https://elicense.ohio.gov/oh_verifylicense) Thanks

//div\[@class='search-submit'\]/input\[@value='search'\]",https://www.reddit.com/r/selenium/comments/xj8r7h/just_cant_click_this_button/,AI
1488,How To Access Password Field To Create Google/Gmail Account,I'm trying to use Selenium to create a Gmail and cannot access the password field. There is no id and using tab to access it won't work either. How can I tell Selenium to find the password field? Thanks!,https://www.reddit.com/r/selenium/comments/xhhpoo/how_to_access_password_field_to_create/,AI
1491,Error in Setting up selenium in Node. Pls help :(," 

I am following a tutorial on how to set up selenium in a node environment: [https://medium.com/dailyjs/how-to-setup-selenium-on-node-environment-ee33023da72d](https://medium.com/dailyjs/how-to-setup-selenium-on-node-environment-ee33023da72d)

After following some steps I get an error while testing. These are the steps that I followed: So first i initialize npm environment on windows: mkdir node\_testing cd node\_testing npm init -y npm install selenium-webdriver

then I installed chromedriver (105.0.5195.52) for chrome version 105.0.5195.102 (not the exact same version because there isn't one). I also added the chromedrivers path to system paths.

After that im asked to run the following code as a test but it results in an error:

`const webdriver = require('selenium-webdriver'),`  
 `By = webdriver.By,`  
 `until = webdriver.until;`  
`const driver = new webdriver.Builder()`  
    `.forBrowser('chrome')`  
    `.build();`  
`driver.get('http://www.google.com').then(function(){`  
`driver.findElement(webdriver.By.name('q')).sendKeys('webdriver\n').then(function(){`  
 `driver.getTitle().then(function(title) {`  
 `console.log(title)`  
 `if(title === 'webdriver - Google Search') {`  
 `console.log('Test passed');`  
      `} else {`  
 `console.log('Test failed');`  
      `}`  
 `driver.quit();`  
    `});`  
  `});`  
`});`

**Result:** Google Chrome opens and the following prints in the terminal:  

`DevTools listening on ws://127.0.0.1:57150/devtools/browser/d1fc93f0-4bff-4963-b074-0069229e0fa0`

`C:\Users\ibo\node_testing\node_modules\selenium-webdriver\lib\error.js:522`

`let err = new ctor(data.message)`

`^`

&#x200B;

`ElementNotInteractableError: element not interactable`

  `(Session info: chrome=105.0.5195.126)`

`at Object.throwDecodedError (C:\Users\ibo\node_testing\node_modules\selenium-webdriver\lib\error.js:522:15)`    

`at parseHttpResponse (C:\Users\ibo\node_testing\node_modules\selenium-webdriver\lib\http.js:589:13)`

`at Executor.execute (C:\Users\ibo\node_testing\node_modules\selenium-webdriver\lib\http.js:514:28)`

`at processTicksAndRejections (node:internal/process/task_queues:96:5)`

`at async thenableWebDriverProxy.execute (C:\Users\ibo\node_testing\node_modules\selenium-webdriver\lib\webdriver.js:740:17) {`

  `remoteStacktrace: 'Backtrace:\n' +`

`'\tOrdinal0 [0x004DDF13+2219795]\n' +`

`'\tOrdinal0 [0x00472841+1779777]\n' +`

`'\tOrdinal0 [0x00384100+803072]\n' +`

`'\tOrdinal0 [0x003AE523+976163]\n' +`

`'\tOrdinal0 [0x003ADB93+973715]\n' +`

`'\tOrdinal0 [0x003CE7FC+1107964]\n' +`

`'\tOrdinal0 [0x003A94B4+955572]\n' +`

`'\tOrdinal0 [0x003CEA14+1108500]\n' +`

`'\tOrdinal0 [0x003DF192+1175954]\n' +`

`'\tOrdinal0 [0x003CE616+1107478]\n' +`

`'\tOrdinal0 [0x003A7F89+950153]\n' +`

`'\tOrdinal0 [0x003A8F56+954198]\n' +`

`'\tGetHandleVerifier [0x007D2CB2+3040210]\n' +`

`'\tGetHandleVerifier [0x007C2BB4+2974420]\n' +`

`'\tGetHandleVerifier [0x00576A0A+565546]\n' +`

`'\tGetHandleVerifier [0x00575680+560544]\n' +`

`'\tOrdinal0 [0x00479A5C+1808988]\n' +`

`'\tOrdinal0 [0x0047E3A8+1827752]\n' +`

`'\tOrdinal0 [0x0047E495+1827989]\n' +`

`'\tOrdinal0 [0x004880A4+1867940]\n' +`

`'\tBaseThreadInitThunk [0x7613FA29+25]\n' +`

`'\tRtlGetAppContainerNamedObjectPath [0x77707A9E+286]\n' +`

`'\tRtlGetAppContainerNamedObjectPath [0x77707A6E+238]\n'`

`}`

`PS C:\Users\ibo\node_testing> [12044:11068:0914/213837.751:ERROR:device_event_log_impl.cc(214)] [21:38:37.751] USB: usb_service_win.cc:104 SetupDiGetDeviceProperty({{A45C254E-DF1C-4EFD-8020-67D146A850E0}, 6}) failed: Element not found. (0x490)`

`[12044:11068:0914/213837.861:ERROR:device_event_log_impl.cc(214)] [21:38:37.862] USB: usb_device_handle_win.cc:1048 Failed to read descriptor from node connection: A device attached to the system is not functioning. (0x1F)`        

`[12044:11068:0914/213837.896:ERROR:device_event_log_impl.cc(214)] [21:38:37.896] USB: usb_device_handle_win.cc:1048 Failed to read descriptor from node connection: A device attached to the system is not functioning. (0x1F)`        

`[12044:11068:0914/213837.903:ERROR:device_event_log_impl.cc(214)] [21:38:37.902] USB: usb_device_handle_win.cc:1048 Failed to read descriptor from node connection: A device attached to the system is not functioning. (0x1F)`

 Any suggestions are welcome. I've been stuck on this for days.  Pls help I'm stuck.",https://www.reddit.com/r/selenium/comments/xeb9cs/error_in_setting_up_selenium_in_node_pls_help/,AI
1492,How to save a xpath attribute list to a variable?,"Hi, just a quick question: How to save a xpath href attribute list to a variable?

The html code is:

<div class=""title\\\\\\\_container"">    


<a target=""\\\\\\\_blank"" href=""\\\[https://link.com\\\](https://link.com)"">    


Im using

prop\_url = driver.find\_elements(By.XPATH, '//div\[@class=""title\_container""\]\[1\]/a')

To find the url.

I can confirm this is correct because i can print this and i get the list:

for p in prop\_url:print(p.get\_attribute('href'))

But i dont need to print this list, i need to save it to a variable so i can create a dictionary afterwards.

I tried

prop\_url\_strings = prop\_url.\_\_getattribute\_\_('href')

print(prop\_url\_strings)

And im getting this error:

prop\_url\_strings = prop\_url.\_\_getattribute\_\_('href')

AttributeError: 'list' object has no attribute 'href'

&#x200B;

Can someone help me fix this please.

\*\*\*\*\*\*\*\*\*

EDIT: nm i got it:

prop\_url\_links=\[\]  
for pu in prop\_url:  
prop\_url\_links.append(pu.get\_attribute('href'))  
print(prop\_url\_links)

I guess i just needed to ""think out loud"". Sometimes you dont ""visualize"" things in your head good enough.",https://www.reddit.com/r/selenium/comments/xe3ifp/how_to_save_a_xpath_attribute_list_to_a_variable/,AI
1493,Commenting on an Instagram post throws StaleElementReferenceException,"Hello,

I am trying to create a bot that likes and comments on posts that are on a user's Instagram feed. However, when I try to write a comment on a post using the .send\_keys() function, it throws the following exception: 

"" selenium.common.exceptions.StaleElementReferenceException: Message: stale element reference: element is not attached to the page document ""

&#x200B;

I understand the context of this exception, and I've tried some solutions already, such as:

* Using  WebDriverWait(browser, 60).until(EC.presence\_of\_element\_located(textarea)).
* Using  WebDriverWait(browser, 60).until(EC.element\_to\_be\_clickable(textarea)).
* Using  WebDriverWait(browser, 60).until(EC. staleness\_of(textarea)).
* Solution N°3 + refreshing the page before clicking, as advised here: [https://stackoverflow.com/a/62170140/7138725](https://stackoverflow.com/a/62170140/7138725)

&#x200B;

I don't know how to solve this problem after trying so many solutions and ways. Help is much appreciated.

Here is my full code:

    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import NoSuchElementException
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium import webdriver
    import random
    import time
    
    def sleep_for_period_of_time():
        limit = random.randint(7,10)
        time.sleep(limit)
    
    user = input(""Enter your username: "")
    pwd = input(""Enter your password: "")
    
    def main():
        options = webdriver.ChromeOptions()
        #Adding options
        options.add_argument(""--lang=en"")
        options.add_argument('--disable-gpu')
        options.add_argument(""start-maximized"")
        options.add_experimental_option(""detach"", True) #<- to keep the browser open
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
    
        browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        browser.get(""https://www.instagram.com"")
        sleep_for_period_of_time()
    
        #Finding identification elements
        username_input = browser.find_element(by=By.CSS_SELECTOR, value=""input[name='username']"")
        password_input = browser.find_element(by=By.CSS_SELECTOR, value=""input[name='password']"")
        #Typing username and password
        username_input.send_keys(user)
        password_input.send_keys(pwd)
        sleep_for_period_of_time()
        #Locating and clicking on the login button
        login_button = browser.find_element(by=By.XPATH, value=""//button[@type='submit']"")
        login_button.click()
        sleep_for_period_of_time()
        #Disabling notifiations
        browser.get(""https://instagram.com/"")
        acpt = browser.find_element(by=By.XPATH, value='/html/body/div[1]/div/div/div/div[2]/div/div/div[1]/div/div[2]/div/div/div/div/div/div/div/div[3]/button[2]')
        acpt.click()
        sleep_for_period_of_time()
        #Liking and commenting on 4 pictures on my feed
      
        #locating and clicking on the like button
        like = browser.find_element(by=By.XPATH, value='/html/body/div[1]/div/div/div/div[1]/div/div/div/div[1]/div[1]/div[2]/section/main/div[1]/section/div/div[3]/div[1]/div/article[1]/div/div[3]/div/div/section[1]/span[1]/button')
        like.click()
        print(""Liked!"")
        time.sleep(3)
    
        comment = ""Cool!""
        #Locating the text area on the post and commenting
        text_area = browser.find_element(by=By.CSS_SELECTOR, value='textarea[class=""_ablz _aaoc""]')
        text_area.click()
        sleep_for_period_of_time()
        
        text_area.send_keys(comment)",https://www.reddit.com/r/selenium/comments/xdju2p/commenting_on_an_instagram_post_throws/,AI
1496,Specflow on Docker,"Hey Guys

Anyone out there Running Specflow on Linux Docker Container ?

Looking for an example  tutorial 

&#x200B;

Thanks",https://www.reddit.com/r/selenium/comments/xc80d0/specflow_on_docker/,AI
1498,Web Scraping API Idea,"Hey guys,

A while back I created a project called Scrapeium ([website here](https://scrapeium.netlify.app/)), a query language for declaratively and simply extracting data from websites. Right now, it only works in the browser but I was wondering would you guys be willing to use something like this if it was available as a public API?",https://www.reddit.com/r/selenium/comments/xb50hx/web_scraping_api_idea/,AI
1500,SOLUTION FOR element not interactable: element has zero size,"I had a problem clicking an object that had zero size. The solution I found was to simply click the element location instead of the element itself. I did this by importing ActionChains.

Ac=ActionChains(driver)
Ac.move_to_element(<element name>).move_by_offset(0,0).click().preform()

Like all good code, this was borrowed without permission from someone who had the same question on StackOverflow a few years ago.",https://www.reddit.com/r/selenium/comments/x7ca56/solution_for_element_not_interactable_element_has/,AI
1502,selenium can't run on linux graphical system,"have anyone ever used selenium to launch chromium by chromedriver on linux grapical system?
i failed.
it prints ""syntax error : unterminated qouted string""",https://www.reddit.com/r/selenium/comments/x6b9ik/selenium_cant_run_on_linux_graphical_system/,AI
1504,Webdriver + Tkinter Singleton-esque pattern,"Hi, so I’m still pretty new to webdriver and wondering if this design pattern might be a solution to my problem. Generally I try to avoid singleton but it seems to make sense here, unless I’m misunderstanding how the driver actually works.

I’ve been building out a python tkinter gui to use with webdriver for my company, which helps my coworkers avoid many of the various tedious tasks associated with an ancient front end of an internal web app. Naturally, the buttons and other widgets’ commands need to call on webdriver in some fashion. 

Rather than lump everything into a single script, I’ve been developing this gui with classes and separating components into relevant modules. My selenium code is fashioned in the same way and I’ve been trying to implement it in an OOP manner and observe the POM as best as possible. 

The only solution that comes to mind is to implement the driver as a singleton. Instead of passing it around all my page objects and components and trying to make it work with tkinter’s event loop at the same time, I just create the single driver instance, and each of my page objects just get this driver as an arg. Due to python’s annoying import behavior, I can’t just import the single driver instance into all of my page objects, so I have to settle for passing this driver into page objects in wrapper functions bound to tkinter command attributes.

From google, I’ve been able to come up with this cobbled together code:

```
class WebdriverInstance:
    __driver__ = None


    def __init__(self):

        if WebdriverInstance.__driver__ is None:

            WebdriverInstance.__driver__ = self.__load_driver()
        else:
            raise Exception(""Driver already loaded."")


    @staticmethod
    def driver():
        if not WebdriverInstance.__driver__:
            WebdriverInstance()
        return WebdriverInstance.__driver__


    def __load_driver(self):
        …
```

It works and I’m able to import the instance into each of the tkinter widgets that need it.

Anyways, I’m a noob and just wondering if this approach makes sense or if there’s a more obvious and logical way I’m missing? If there’s another way to further decouple ui from the driver logic, that would be great too. Thanks for any help!",https://www.reddit.com/r/selenium/comments/x1t1fq/webdriver_tkinter_singletonesque_pattern/,AI
1506,SeleniumBasic v2.0.9.0 – Excel Macro Doesn’t Trigger Onchange Event,"Hello, I’m using SeleniumBasic v2.0.9.0 in an Excel macro to upload data from a spreadsheet into a web form. One of the dropdown values is supposed to update with the value of a previous entry, but the onchange event isn’t registering. Is there a way I can force the event to occur with my macro?

I don’t know much about writing code. Honestly, I’m just throwing stuff at the wall to see what will stick. I have tried the following:

>Waiting for the script to activate - obj.Wait 60000

&#x200B;

>Hitting tab in the field with the script - obj.FindElementByName(""variable"").SendKeys (""{TAB}"")

&#x200B;

>Clicking on the field with the script - obj.FindElementByName(""variable"").Click

&#x200B;

>Running this chunk I found on a forum - Set Refresh = obj.FindElementByName(""variable2"")  
>  
>obj.ExecuteScript ""arguments\[0\].click();"", Refresh

Nothing is registering as a change or running the event. I can’t share the page it is on, but I can share the element for it.

><select id=""variable2"" name=""variable2IPackage"" onchange=""jsf.ajax.request('variable2',event,{execute:'@this ','javax.faces.behavior.event':'valueChange'})"" size=""1"" style=""width: 150px;"" title=""variable2""> <option selected=""selected"" value=""""></option></select>

Any advice would be appreciated, as I am literally clueless. My Google-Fu has left me empty-handed.",https://www.reddit.com/r/selenium/comments/x07ufs/seleniumbasic_v2090_excel_macro_doesnt_trigger/,AI
1507,I'm stuck. Is it possible to launch selenium in docker container?,"Hi, i was trying to launch selenium in docker using Java. I was able to launch it in headless mode, but i need to see the browser window to interact with it and run a few scripts. How can I accomplish this in a docker container?

I'm using the selenium-standalone image from docker hub and running a jar file of my spring boot application inside the container.

EDIT: I want to run everything in a docker container, My Java scripts, the selenium browser, a react UI",https://www.reddit.com/r/selenium/comments/wzbn3z/im_stuck_is_it_possible_to_launch_selenium_in/,AI
1508,Cannot see option to run JUnit tests,"Hi,

I'm very new to Selenium and trying to run some tests for the first time. I am following a book which as has asked me to clone a repo which contains the tests. I have it set up in Eclipse but when I right click on the test and and select 'Run as', I do not see the option 'JUnit test', but just the 'Run Configurations'. What is it that I need to run the JUnit tests? 

Here is a link to a screenshot of my setup in eclipse:

[Setup  in Eclipse](https://ibb.co/rH0H1W5)

Thanks you.",https://www.reddit.com/r/selenium/comments/wz0hgw/cannot_see_option_to_run_junit_tests/,AI
1510,Solution for unmaintaned ExpectedCondition class?,"Was wanting to use wait.until instead of using a Thread.sleep() when waiting for a page/element to be found. What is currently the solution without using the unmaintained package seen here: [https://www.nuget.org/packages/DotNetSeleniumExtras.WaitHelpers/](https://www.nuget.org/packages/DotNetSeleniumExtras.WaitHelpers/)

I'm just worried about a future update wiping out that solution\^

Any suggestions are welcomed - thanks.",https://www.reddit.com/r/selenium/comments/wxqphj/solution_for_unmaintaned_expectedcondition_class/,AI
1512,headless for certain tests ?>,"I have a TestNG XML that has 20 tests.   
I want the first 15 to run on headless more and the last 5 to run normally through the UI since it has some document upload, download, and signing testing that fails when running headless. Is this possible?",https://www.reddit.com/r/selenium/comments/wwzg4g/headless_for_certain_tests/,AI
1514,Issue with pop-up window,"Hi all...  


I've got an issue I'm struggling to solve using the Selenium Python library.  
I have one test setup and working fine, however my second test (which should be the same as the first) is failing due to an element, in fact any element, not being found on a pop-up window.

Here's what works in my working test (`main_page` is defined earlier in the script, and the `print(signin_window)` shows a different window handle to the main window):

`for handle in driver.window_handles:`  
 `if handle != main_page:`  
 `signin_window = handle`  
 `print(signin_window)`  
`try:`  
 `driver.switch_to.window(signin_window)`  
`except:`  
 `print(""Cannot switch to popup"")`  
`try:`

 `driver.find_element(By.ID, ""username"").send_keys(""myusername"")`  
 `driver.find_element(By.ID, ""password"").send_keys(""mypassword"")`  
 `driver.find_element(By.ID, ""login-submit"").click()`  
`except:`  
 `print(""Cannot Login"")`

The exception ""Cannot switch to popup"" does not print, so I guess that the new window is being selected, however the next stage fails.   
I've tried just adding `driver.find_element(By.ID, ""myMainDiv"")`instead, which is the popups main container, and it triggers the exception, so it seems it's not finding any elements on the popup at all.  
My first thought would be that it's not selected the popup as it can't find any of the elements, but as the popup exception doesn't trigger, I'm not sure thats the case  


Is there any other way of debugging this to make sure I'm on the correct window or am I missing something else?  


Thanks!",https://www.reddit.com/r/selenium/comments/wvlqiw/issue_with_popup_window/,AI
1516,Handling failures,"Hi all...  
I'll caveat this with the fact I only started with Selenium yesterday, and I'm not really all that great with Python either, so please forgive me if I don't make a whole lot of sense.

I've written a simple test with Python that is checking the things I need to check (opens Chrome, loads page, handles marketing popup, checks a login, signs out, closes Chrome) and it seems to work fine, but I'm wondering what I can do to handle failures.  
Long term, I want to run a browser test, take a screenshot if there is a failure, and also send an email if any part of the test fails.  


I'm trying to understand if something exists in the Selenium library that would do this, or if this needs to be part of the Python script, if that makes sense?

Do I need to run each section of the test as a function of some sort, that will only continue if it passes?  
Do I need to look specifically at Python exception handling?  
Have I just answered my own question?  


Literally any pointers would be useful, apologies for my n00bness.",https://www.reddit.com/r/selenium/comments/wrhjoq/handling_failures/,AI
1517,Can someone explain this find elements behaviour?,"My code will search through the html source for any instances of a particular reg ex pattern.

When it finds one, it will loop through all elements based on an xpath search for the matched reg ex pattern.

    What I'm finding weird, is one such pattern appears once on the page, yet the the xpath loop finds 9 instances of elements. Code below:
    
    for regexmatches in regexpattern.finditer(htmlsource):
    
    	expr = (""£"" + regexmatches.group())
                    
    
        	for i in driver.find_elements(""xpath"", '//*[contains(normalize-space(), ""' + expr + '"")]'):

If i put counters below both the for statements, the first counter may be 80 or so, the second in the high hundreds.

Why would this be?",https://www.reddit.com/r/selenium/comments/wrg9pc/can_someone_explain_this_find_elements_behaviour/,AI
1518,staying logged in on each new session,"Every time I try using salesforce or any site with authentication, I'm met with either a login page, or a login AND an email verification. What's the best way to stay logged in? I've been scrolling through Google and stackoverflow with no success. 

I'm using a macbook and my chrome profile( in case that matters)",https://www.reddit.com/r/selenium/comments/wq00aq/staying_logged_in_on_each_new_session/,AI
1519,Help!!! + Cannot find Password Input Field using Python Selenium Webdriver,"Background: I have about two years of fiddling around with selenium. However, do not have much experience with css or html (web design) outside selenium.

Issue: I want to generate an email account. However, when I get to the password page, I am unable to find the Password input box to select the password for the account.

Here is the html code I found with inspect element. I want to find the password box and send it my password.

<input class=""form-control email-input-max-width"" type=""password"" id=""PasswordInput"" name=""Password"" aria-describedby=""PasswordDesc PasswordError"" data-bind=""css:

{

'has-error': showError(password)

},



textInput: password,



hasFocus: password.focused() \&amp;\&amp; !showPassword(),

moveOffScreen: showPassword(),

event: { keyup: onPasswordKeyUp },

ariaLabel: strings.ariaLblPassword,

attr:

{

'placeholder': strings.ariaLblPassword

}"" tabindex=""0"" aria-label=""Create password"" placeholder=""Create password"">

&#x200B;

Stuck and dont know what todo. Thanks in advance, I appreciate the help.",https://www.reddit.com/r/selenium/comments/wogile/help_cannot_find_password_input_field_using/,AI
1521,Selenium stops detecting element on second iteration of for loop in python,"I wrote a python code that uses selenium to input values from a list into google search, click on the first result and extract information from the website. The first iteration of the loop runs perfectly and extracts exactly what I need but the second iteration runs into trouble. The code is unable to locate the first result (let alone click it) even though the XPATH does not change. I have tried to use both time.sleep and WebDriverWait but none of these work (I just get a timeout exception). Is there something obvious that I’m missing? As I mentioned there are no changes in the websites structure in terms of classes, xpaths or ID and I’m really clueless as to why it happens.",https://www.reddit.com/r/selenium/comments/wo7jid/selenium_stops_detecting_element_on_second/,AI
1525,Script that works in Firefox visible browser but fails when headless,"I'm using selenium in python, and the title summarizes my problem. What is likely to cause this, and is there some way to overcome it? My script must run in a docker container, so it must be headless.",https://www.reddit.com/r/selenium/comments/wj6biz/script_that_works_in_firefox_visible_browser_but/,AI
1527,How to navigate a dropdown triggered by javascript using Python Selenium,"[Stackoverflow link](https://stackoverflow.com/posts/73255453/edit)

I am trying to navigate a dropdown button that operates via javascript. However, no matter what I try, the HTML list items it should have never seem to show up in selenium.

[An image of the dropdown button](https://i.stack.imgur.com/aiaDx.png)

Inspector page source:
    
    <div class=""dropdown"" style=""border-bottom: 1px solid #ebebeb; padding-bottom: 2px;"">
         <a class=""btn btn-default btn-sm"" onclick=""$(this).parent().toggleClass('open')"" title=""Select an Event"" style=""width: 220px"">
             <span id=""event-selection-span-id"">No event selected.</span>
                  <span class=""fa fa-caret-down routing-toolbar-menu""></span>
         </a>
         <ul class=""dropdown-menu user-dropdown dropdown-menu-left"" id=""call-events-list-id"">
         <li title=""ADRC Archived Call"" event_definition_id=""2f617fc5-c0b0-492a-92e2-561c39c239fc"" form_code=""AACOG_ADRC_CCC_ARCH"" onclick=""CallCenter.SetEvent(this)"" class=""list-group-item event-group-list-item"">ADRC Archived Call</li>
         <li title=""ADRC Information  Call"" event_definition_id=""0a22deba-4788-4647-bee6-47305e182eca"" form_code=""AACOG_ADRC_CCC"" onclick=""CallCenter.SetEvent(this)"" class=""list-group-item event-group-list-item"">ADRC Information  Call</li>
         </ul>
    </div>


Selenium page source:
    
    <div class=""dropdown open"" style=""border-bottom: 1px solid #ebebeb; padding-bottom: 2px;"">
         <a class=""btn btn-default btn-sm"" onclick=""$(this).parent().toggleClass('open')"" title=""Select an Event"" style=""width: 220px"">
               <span id=""event-selection-span-id"">No event selected.</span>
                     <span class=""fa fa-caret-down routing-toolbar-menu""></span>
         </a>
         <ul class=""dropdown-menu user-dropdown dropdown-menu-left"" id=""call-events-list-id""></ul>
    </div>


Here is what I've tried so far, all unsuccessful:

* Originally just tried finding the elements and using the .click() method to click in them. (e.g. `driver.find_element_by_xpath('//*[@id=""step-3""]/div[2]/a').click()` then `driver.find_element_by_xpath('//*[@id=""call-events-list-id""]/li[2]').click()`. Selenium could not find the list element in the second line.

* Then I tried a method that's worked for me before when the previous one didn't: finding the elements then using `driver.execute_script(""arguments[0].click()"", btn)` for each one. Like before, it worked for the main dropdown button but not the list item that should show up afterwords.

* So I figured I could just execute the javascript manually with the elements' JS paths using `driver.execute_script(""$(document.querySelector('#step-3 > div.dropdown > a')).parent().toggleClass('open');"")` then `driver.execute_script(""CallCenter.SetEvent(document.querySelector('#call-events-list-id > li:nth-child(2)'));"")`. This still didn't work and the list elements still did not show up in selenium's page source.

The strangest thing is the list elements show up in the inspector before you even click the main dropdown button. Therefore I should just be able to execute the second JS line manually with no problems, and that works fine when I do it in the browser. I have also tried just waiting for the list elements to show up when the source is loaded but they never show up no matter how long I set the delay for. 

So I am at my wit's end and could really use some help with this one.",https://www.reddit.com/r/selenium/comments/whbih2/how_to_navigate_a_dropdown_triggered_by/,AI
1534,odin,"A programmable, observable and distributed job orchestration system.",https://github.com//theycallmemac/odin,program
1535,process-automation-enablement,This repository contains content for SAP Process Automation trainings.,https://github.com//SAP-samples/process-automation-enablement,content
1536,n8n,Free and source-available fair-code licensed workflow automation tool. Easily automate tasks across different services.,https://github.com//n8n-io/n8n,auto
1537,argo-events,Event-driven automation framework,https://github.com//argoproj/argo-events,auto
1538,doit,task management & automation tool,https://github.com//pydoit/doit,auto
1539,utask,碌Task is an automation engine that models and executes business processes declared in yaml. 鉁忥笍馃搵,https://github.com//ovh/utask,auto
1540,obsei,"Obsei is a low code AI powered automation tool. It can be used in various business flows like social listening, AI based alerting, brand image analysis, comparative study and more .",https://github.com//obsei/obsei,auto
1541,mergeable,馃 All the missing GitHub automation 馃檪 馃檶 ,https://github.com//mergeability/mergeable,auto
1544,werk24-python,Automated Processing of Technical Drawings,https://github.com//W24-Service-GmbH/werk24-python,auto
1545,HVAC-Building-and-Process-Automation,"The library product HVAC Building & Process Automation SL for CODESYS contains functional components and HTML5 system macros for the creation of e.g. heating and ventilation systems, but also for room automation and other industrial system technology.",https://github.com//HVAC-By-Pfaender/HVAC-Building-and-Process-Automation,auto
1546,automation-protocols-metadata,"JSON file that contains an update metadata of Automation Protocols (Industrial control system, process automation, building automation, automatic meter reading, and automobile)",https://github.com//qeeqbox/automation-protocols-metadata,auto
1547,opal-nodes,"Nodes that are used by the OPAL framework to automate user activity for different types of resources such UI, Excel, PDF etc.",https://github.com//telligro/opal-nodes,auto
1548,GoFarmTech,GoFarmTech Arduino based Framework for developing agriculture/industry processes automation devices.,https://github.com//bratello/GoFarmTech,auto
1549,ReviewBuddy,ReviewBuddy is a Bonita BPM App that automates Review Processes,https://github.com//Ilagouilly/ReviewBuddy,auto
1550,LinkedInProfileViewer,This tool automates linked-in profile views using a excel sheet with profile url's,https://github.com//appumistri/LinkedInProfileViewer,auto
1551,playwright,"Playwright is a framework for Web Testing and Automation. It allows testing Chromium, Firefox and WebKit with a single API. ",https://github.com//microsoft/playwright,auto
1552,fastlane,馃殌 The easiest way to automate building and releasing your iOS and Android apps,https://github.com//fastlane/fastlane,auto
1554,Tasmota,"Alternative firmware for ESP8266 with easy configuration using webUI, OTA updates, automation using timers or rules, expandability and entirely local control over MQTT, HTTP, Serial or KNX. Full documentation at",https://github.com//arendst/Tasmota,auto
1555,semantic-release,馃摝馃殌 Fully automated version management and package publishing,https://github.com//semantic-release/semantic-release,auto
1556,appium,Cross-platform automation framework for all kinds of your apps built on top of W3C WebDriver protocol,https://github.com//appium/appium,auto
1557,InstaPy,馃摲 Instagram Bot - Tool for automated Instagram interactions,https://github.com//InstaPy/InstaPy,auto
1558,watchtower,A process for automating Docker container base image updates. ,https://github.com//containrrr/watchtower,auto
1559,Auto.js,Automation&Workflow JavaScript IDE on Android(瀹夊崜骞冲彴涓婄殑鑷姩鍖栧伐浣滄祦JavaScript IDE),https://github.com//hyb1996/Auto.js,auto
1560,awx,"AWX provides a web-based user interface, REST API, and task engine built on top of Ansible. It is one of the upstream projects for Red Hat Ansible Automation Platform.",https://github.com//ansible/awx,auto
1561,Detox,Gray box end-to-end testing and automation framework for mobile apps,https://github.com//wix/Detox,auto
1562,hammerspoon,Staggeringly powerful macOS desktop automation with Lua,https://github.com//Hammerspoon/hammerspoon,auto
1563,crawlee,Crawlee鈥擜 web scraping and browser automation library for Node.js that helps you build reliable crawlers. Fast.,https://github.com//apify/crawlee,auto
1564,autoscraper,"A Smart, Automatic, Fast and Lightweight Web Scraper for Python",https://github.com//alirezamika/autoscraper,auto
1565,mechanize,Mechanize is a ruby library that makes automated web interaction easy.,https://github.com//sparklemotion/mechanize,auto
1566,Automatic-Udemy-Course-Enroller-GET-PAID-UDEMY-COURSES-for-FREE,"Do you want to LEARN NEW STUFF for FREE? Don't worry, with the power of web-scraping and automation, this script will find the necessary Udemy coupons & enroll you for PAID UDEMY COURSES, ABSOLUTELY FREE!",https://github.com//aapatre/Automatic-Udemy-Course-Enroller-GET-PAID-UDEMY-COURSES-for-FREE,auto
1568,rod,A Devtools driver for web automation and scraping,https://github.com//go-rod/rod,auto
1569,mlscraper,馃 Scrape data from HTML websites automatically by just providing examples,https://github.com//lorey/mlscraper,auto
1571,python-automation-scripts,Simple yet powerful automation stuffs.,https://github.com//avidLearnerInProgress/python-automation-scripts,auto
1573,cadence,"Cadence is a distributed, scalable, durable, and highly available orchestration engine to execute asynchronous long-running business logic in a scalable and resilient way.",https://github.com//uber/cadence,AI
1575,couler,"Unified Interface for Constructing and Managing Workflows on different workflow engines, such as Argo Workflows, Tekton Pipelines, and Apache Airflow.",https://github.com//couler-proj/couler,AI
1580,VBA_Misc_Macros,This repo will contain various small macros and use cases i've implemented for my current work in order to speed up the process,https://github.com//theOther-Paul/VBA_Misc_Macros,AI
1584,Lulu,[Unmaintained] A simple and clean video/music/image downloader 馃懢,https://github.com//iawia002/Lulu,AI
1585,Edu-Mail-Generator,Generate Free Edu Mail(s) within minutes,https://github.com//AmmeySaini/Edu-Mail-Generator,AI
1586,Email-extractor,The main functionality is to extract all the emails from one or several URLs - La funcionalidad principal es extraer todos los correos electr贸nicos de una o varias Url,https://github.com//DiegoCaraballo/Email-extractor,AI
1587,redux,Bacterial phenotype prediction from low pass sequencing using recommender systems and machine learning classifiers.,https://github.com//jw44lavo/redux,Machine Learning
1588,atinfo/awesome-test-automation,"A curated list of awesome test automation frameworks, tools, libraries, and software for different programming languages. Sponsored by http://sdclabs.com",https://github.com/atinfo/awesome-test-automation,program
1589,ianmiell/shutit,Automation framework for programmers,https://github.com/ianmiell/shutit,program
1590,napalm-automation/napalm,Network Automation and Programmability Abstraction Layer with Multivendor support,https://github.com/napalm-automation/napalm,program
1591,f5devcentral/f5-automation-labs,"F5 Super-NetOps Programmability, Automation and DevOps Training Classes",https://github.com/f5devcentral/f5-automation-labs,program
1592,shankybnl/MobileAutomationFramework,"Single code base framework to test android and iOS app using appium (v6.1.0), maven, testng,java. Option to start appium server programmatically.",https://github.com/shankybnl/MobileAutomationFramework,program
1593,hANSIc99/Pythonic,Graphical Python programming for trading and automation,https://github.com/hANSIc99/Pythonic,program
1594,cytoscape/cytoscape-automation,Collection of scripts that include programmatic io and control of Cytoscape,https://github.com/cytoscape/cytoscape-automation,program
1595,tzuhsial/InstagramCrawler,"A non API python program to crawl public photos, posts or followers ",https://github.com/tzuhsial/InstagramCrawler,program
1596,chris-greening/instascrape,"Powerful and flexible Instagram scraping library for Python, providing easy-to-use and expressive tools for accessing data programmatically",https://github.com/chris-greening/instascrape,program
1597,rajat4665/web-scraping-with-python,In this repository i will expalin how to scrap websites using python programming language with BeautifulSoup and requestsmodulues,https://github.com/rajat4665/web-scraping-with-python,program
1598,obheda12/GitDorker,A Python program to scrape secrets from GitHub through usage of a large repository of dorks.,https://github.com/obheda12/GitDorker,program
1599,imoisharma/Python-Web-Scraping-,"When performing data science tasks, it's common to want to use data found on the internet. You'll usually be able to access this data in csv format, or via an Application Programming Interface (API). However, there are times when the data you want can only be accessed as part of a web page. In cases like this, you'll want to use a technique called web scraping to get the data from the web page into a format you can work with in your analysis.",https://github.com/imoisharma/Python-Web-Scraping-,program
1601,QuantumLiu/PornHub_downloader,"A GUI program,crawling links of .mp4 video files from page URL",https://github.com/QuantumLiu/PornHub_downloader,program
1602,mikeal/spider,Programmable spidering of web sites with node.js and jQuery,https://github.com/mikeal/spider,program
1603,ComplianceAsCode/content,"Security automation content in SCAP, Bash, Ansible, and other formats",https://github.com/ComplianceAsCode/content,content
1604,ibrennan/automation,A collection of automated scripts for processing sites / content. Uses CasperJS.,https://github.com/ibrennan/automation,content
1605,jinfagang/weibo_terminater,"Final Weibo Crawler Scrap Anything From Weibo, comments, weibo contents, followers, anything. The Terminator ",https://github.com/jinfagang/weibo_terminater,content
1606,ruippeixotog/scala-scraper,A Scala library for scraping content from HTML pages,https://github.com/ruippeixotog/scala-scraper,content
1607,vishaalagartha/basketball_reference_scraper,A python module for scraping static and dynamic content from Basketball Reference.,https://github.com/vishaalagartha/basketball_reference_scraper,content
1608,AH72KING/Instagram-scraping,Scraping different content from instagram without api,https://github.com/AH72KING/Instagram-scraping,content
1609,joelbarmettlerUZH/Scrapeasy,Scraping in python made easy - receive the content you like in just one line of code,https://github.com/joelbarmettlerUZH/Scrapeasy,content
1610,commoncrawl/news-crawl,News crawling with Storm-crawler - stores content as WARC,https://github.com/commoncrawl/news-crawl,content
1611,Cyber-Duck/Silverstripe-SEO,"A SilverStripe module to optimise the Meta, crawling, indexing, and sharing of your website content",https://github.com/Cyber-Duck/Silverstripe-SEO,content
1612,GoSecure/csp-auditor,Burp and ZAP plugin to analyse Content-Security-Policy headers or generate template CSP configuration from crawling a Website,https://github.com/GoSecure/csp-auditor,content
1613,sajjadium/DeepCrawling,Crawlium (DeepCrawling): A crawling platform based on Chrome (Chromium) browser to get a deeper look into the ecosystem of content inclusion on the Web.,https://github.com/sajjadium/DeepCrawling,content
1614,TeamHG-Memex/scrapy-crawl-once,Scrapy middleware which allows to crawl only new content,https://github.com/TeamHG-Memex/scrapy-crawl-once,content
1615,opensemanticsearch/open-semantic-etl,"Python based Open Source ETL tools for file crawling, document processing (text extraction, OCR), content analysis (Entity Extraction & Named Entity Recognition) & data enrichment (annotation) pipelines & ingestor to Solr or Elastic search index & linked data graph database",https://github.com/opensemanticsearch/open-semantic-etl,content
1616,Keep-Current/web-miner,"Crawls sites, to find new content and scrap it",https://github.com/Keep-Current/web-miner,content
1617,DianaCody/Spider_SinaTweetCrawler_java,"Spider_SinaTweetCrawler, to crawl tweet content from sinaTweet. (java)",https://github.com/DianaCody/Spider_SinaTweetCrawler_java,content
1618,ziyan/spider,Web Content Extraction Through Machine Learning,https://github.com/ziyan/spider,content
1619,selfshore/spiders,"模拟百度登陆(百度指数)，去哪儿航班爬虫，极验滑块，船讯网数据解密，大众点评登录，知乎登录，同盾滑块，腾讯滑块，易盾滑块,企业公示系统(过加速乐)，微店登录，拼多多anticontent",https://github.com/selfshore/spiders,content
1620,LoseNine/Crack-JS-Spider,JS破解逆向，破解JS反爬虫加密参数，已破解极验滑块w（2022.2.19），QQ音乐sign（2022.2.13），拼多多anti_content，boss直聘zp_token，知乎x-zse-96，酷狗kg_mid/dfid，唯品会mars_cid，中国裁判文书网（2020-06-30更新），淘宝密码，天安保险登录，b站登录，房天下登录，WPS登录，微博登录，有道翻译，网易登录，微信公众号登录，空中网登录，今目标登录，学生信息管理系统登录，共赢金融登录，重庆科技资源共享平台登录，网易云音乐下载，一键解析视频链接，财联社登录。,https://github.com/LoseNine/Crack-JS-Spider,content
1621,jenkinsci/jenkins,Jenkins automation server,https://github.com/jenkinsci/jenkins,auto
1622,google/it-cert-automation-practice,Google IT Automation with Python Professional Certificate - Practice files,https://github.com/google/it-cert-automation-practice,auto
1623,ifrankandrade/automation,"Automation with Python (Excel, scripts, etc)",https://github.com/ifrankandrade/automation,auto
1624,gradle/gradle,"Adaptable, fast automation for all",https://github.com/gradle/gradle,auto
1625,SeleniumHQ/selenium,A browser automation framework and ecosystem.,https://github.com/SeleniumHQ/selenium,auto
1626,Anuken/Mindustry,The automation tower defense RTS,https://github.com/Anuken/Mindustry,auto
1627,karatelabs/karate,Test Automation Made Simple,https://github.com/karatelabs/karate,auto
1628,capistrano/capistrano,Remote multi-server automation tool,https://github.com/capistrano/capistrano,auto
1629,octalmage/robotjs,Node.js Desktop Automation. ,https://github.com/octalmage/robotjs,auto
1630,puppetlabs/puppet,Server automation framework and application,https://github.com/puppetlabs/puppet,auto
1632,runatlantis/atlantis,Terraform Pull Request Automation,https://github.com/runatlantis/atlantis,auto
1633,SUSE-Cloud/automation,"Automation scripts for development, testing, and CI",https://github.com/SUSE-Cloud/automation,auto
1634,domoticz/domoticz,Open source Home Automation System,https://github.com/domoticz/domoticz,auto
1635,segmentio/nightmare,A high-level browser automation library.,https://github.com/segmentio/nightmare,auto
1636,mautic/mautic,Mautic: Open Source Marketing Automation Software.,https://github.com/mautic/mautic,auto
1637,argoproj/argo-events,Event-driven automation framework,https://github.com/argoproj/argo-events,auto
1638,evertramos/nginx-proxy-automation,Automated docker nginx proxy integrated with letsencrypt.,https://github.com/evertramos/nginx-proxy-automation,auto
1639,deepsyx/home-automation,Raspberry Pi 3 based home automation with NodeJS and React Native.,https://github.com/deepsyx/home-automation,auto
1640,AirtestProject/Airtest,UI Automation Framework for Games and Apps,https://github.com/AirtestProject/Airtest,auto
1641,Tencent/GAutomator, Automation for mobile games,https://github.com/Tencent/GAutomator,auto
1642,python-geeks/Automation-scripts,Repo for creating awesome automation scripts to make my panda lazier,https://github.com/python-geeks/Automation-scripts,auto
1643,home-assistant/core,:house_with_garden: Open source home automation that puts local control and privacy first.,https://github.com/home-assistant/core,auto
1644,google/EarlGrey,:tea: iOS UI Automation Test Framework,https://github.com/google/EarlGrey,auto
1645,Hammerspoon/hammerspoon,Staggeringly powerful macOS desktop automation with Lua,https://github.com/Hammerspoon/hammerspoon,auto
1646,networktocode/awesome-network-automation,Curated Awesome list about Network Automation,https://github.com/networktocode/awesome-network-automation,auto
1647,triaquae/CrazyEye,OpenSource IT Automation Software,https://github.com/triaquae/CrazyEye,auto
1648,shipitjs/shipit,Universal automation and deployment tool ⛵️,https://github.com/shipitjs/shipit,auto
1649,ansible/workshops,Training Course for Ansible Automation Platform,https://github.com/ansible/workshops,auto
1650,robotframework/robotframework,Generic automation framework for acceptance testing and RPA,https://github.com/robotframework/robotframework,auto
1651,geb/geb,Very Groovy Browser Automation,https://github.com/geb/geb,auto
1652,itglue/automation,A place for IT Glue's user community to learn ways of automating their documentation.,https://github.com/itglue/automation,auto
1653,treasure-data/digdag,Workload Automation System,https://github.com/treasure-data/digdag,auto
1654,xoseperez/espurna,Home automation firmware for ESP8266-based devices,https://github.com/xoseperez/espurna,auto
1655,jeedom/core,Software for home automation,https://github.com/jeedom/core,auto
1656,nutanix/Automation,Centralized Repo for community driven Nutanix automation,https://github.com/nutanix/Automation,auto
1657,FlaUI/FlaUI,UI automation library for .Net,https://github.com/FlaUI/FlaUI,auto
1658,aws-solutions/aws-waf-security-automations,This solution automatically deploys a single web access control list (web ACL) with a set of AWS WAF rules designed to filter common web-based attacks.,https://github.com/aws-solutions/aws-waf-security-automations,auto
1659,pydoit/doit,task management & automation tool,https://github.com/pydoit/doit,auto
1660,developeranaz/cloudshell-novnc-automation,Google Cloudshell Free VPS with novnc and Ngrok - AUTOMATION,https://github.com/developeranaz/cloudshell-novnc-automation,auto
1662,blacklanternsecurity/bbot,OSINT automation for hackers.,https://github.com/blacklanternsecurity/bbot,auto
1663,automagica/automagica,AI-powered Smart Robotic Process Automation 🤖,https://github.com/automagica/automagica,auto
1664,elmoallistair/google-it-automation,google it automation with python professional certificate,https://github.com/elmoallistair/google-it-automation,auto
1665,S3cur3Th1sSh1t/WinPwn,Automation for internal Windows Penetrationtest / AD-Security,https://github.com/S3cur3Th1sSh1t/WinPwn,auto
1666,yaqwsx/KiKit,Automation tools for KiCAD,https://github.com/yaqwsx/KiKit,auto
1667,Juniper/py-junos-eznc,Python library for Junos automation,https://github.com/Juniper/py-junos-eznc,auto
1668,mobile-dev-inc/maestro,Painless Mobile UI Automation,https://github.com/mobile-dev-inc/maestro,auto
1669,Blazemeter/taurus,Automation-friendly framework for Continuous Testing by,https://github.com/Blazemeter/taurus,auto
1670,microsoft/PowerStig,STIG Automation ,https://github.com/microsoft/PowerStig,auto
1671,Pulover/PuloversMacroCreator,Automation Utility - Recorder & Script Generator,https://github.com/Pulover/PuloversMacroCreator,auto
1672,smrtnt/Open-Home-Automation,"Open Home Automation with Home Assistant, ESP8266/ESP32 and MQTT",https://github.com/smrtnt/Open-Home-Automation,auto
1673,getgauge/gauge,Light weight cross-platform test automation,https://github.com/getgauge/gauge,auto
1674,binary-com/binary-bot,Visual automation for binary.com,https://github.com/binary-com/binary-bot,auto
1675,go-vgo/robotgo,"RobotGo, Go Native cross-platform GUI automation  @vcaesar",https://github.com/go-vgo/robotgo,auto
1676,habitat-sh/habitat,Modern applications with built-in automation,https://github.com/habitat-sh/habitat,auto
1677,DevonCrawford/Video-Editing-Automation,Toolkit of algorithms to automate the video editing process,https://github.com/DevonCrawford/Video-Editing-Automation,auto
1678,CIDARLAB/cello,Genetic circuit design automation,https://github.com/CIDARLAB/cello,auto
1679,AutoHotkey/AutoHotkey,AutoHotkey - macro-creation and automation-oriented scripting utility for Windows.,https://github.com/AutoHotkey/AutoHotkey,auto
1680,RexOps/Rex,"Rex, the friendly automation framework",https://github.com/RexOps/Rex,auto
1681,microsoft/playwright-python,Python version of the Playwright testing and automation library.,https://github.com/microsoft/playwright-python,auto
1682,awslabs/aws-security-automation,Collection of scripts and resources for DevSecOps and Automated Incident Response Security,https://github.com/awslabs/aws-security-automation,auto
1683,tensult/terraform,Terraform automation for Cloud,https://github.com/tensult/terraform,auto
1684,sergejey/majordomo,Home automation platform,https://github.com/sergejey/majordomo,auto
1685,mlrun/mlrun,Machine Learning automation and tracking,https://github.com/mlrun/mlrun,auto
1686,libretime/libretime,LibreTime: Radio Broadcast & Automation Platform,https://github.com/libretime/libretime,auto
1687,n8n-io/n8n,Free and source-available fair-code licensed workflow automation tool. Easily automate tasks across different services.,https://github.com/n8n-io/n8n,auto
1688,psake/psake,A build automation tool written in PowerShell,https://github.com/psake/psake,auto
1689,inlife/nexrender,📹  Data-driven render automation for After Effects,https://github.com/inlife/nexrender,auto
1690,reclosedev/pyautocad,AutoCAD Automation for Python ⛺,https://github.com/reclosedev/pyautocad,auto
1691,pywinauto/pywinauto,Windows GUI Automation with Python (based on text properties),https://github.com/pywinauto/pywinauto,auto
1692,natethegreate/hent-AI,Automation of censor bar detection,https://github.com/natethegreate/hent-AI,auto
1693,nitin42/Python-Automation,💻  These are some projects which I worked upon to automate stuffs using python,https://github.com/nitin42/Python-Automation,auto
1694,achiurizo/consular,Terminal automation,https://github.com/achiurizo/consular,auto
1695,pybuilder/pybuilder,Software build automation tool for Python.,https://github.com/pybuilder/pybuilder,auto
1696,OWASP/glue,Application Security Automation,https://github.com/OWASP/glue,auto
1697,paypal/SeLion,Enabling Test Automation in Java,https://github.com/paypal/SeLion,auto
1698,puppetlabs/puppetserver,Server automation framework and application,https://github.com/puppetlabs/puppetserver,auto
1699,os-autoinst/os-autoinst,OS-level test automation,https://github.com/os-autoinst/os-autoinst,auto
1700,avidLearnerInProgress/python-automation-scripts,Simple yet powerful automation stuffs.,https://github.com/avidLearnerInProgress/python-automation-scripts,auto
1701,wntrblm/nox,Flexible test automation for Python,https://github.com/wntrblm/nox,auto
1702,AirtestProject/iOS-Tagent,iOS support agent for automation,https://github.com/AirtestProject/iOS-Tagent,auto
1703,SeleniumHQ/selenium-ide,Open Source record and playback test automation for the web.,https://github.com/SeleniumHQ/selenium-ide,auto
1704,praetorian-inc/purple-team-attack-automation,Praetorian's public release of our Metasploit automation of MITRE ATT&CK™ TTPs,https://github.com/praetorian-inc/purple-team-attack-automation,auto
1705,xuan32546/IOS13-SimulateTouch,iOS Automation Framework iOS Touch Simulation Library,https://github.com/xuan32546/IOS13-SimulateTouch,auto
1706,pongasoft/glu,Deployment Automation Platform,https://github.com/pongasoft/glu,auto
1707,SeldomQA/seldom,Seldom automation testing framework based on unittest,https://github.com/SeldomQA/seldom,auto
1708,aderusha/HASwitchPlate,LCD touchscreen for Home Automation,https://github.com/aderusha/HASwitchPlate,auto
1709,service-bot/servicebot,Open-source subscription management & billing automation system,https://github.com/service-bot/servicebot,auto
1710,tfeldmann/organize,The file management automation tool.,https://github.com/tfeldmann/organize,auto
1711,Z-Wave-Me/home-automation,Z-Way Home Automation engine,https://github.com/Z-Wave-Me/home-automation,auto
1712,breuerfelix/instapy-gui,gui for instapy automation,https://github.com/breuerfelix/instapy-gui,auto
1713,tox-dev/tox,Command line driven CI frontend and development task automation tool.,https://github.com/tox-dev/tox,auto
1715,hugs/tapsterbot,Mobile device automation robot,https://github.com/hugs/tapsterbot,auto
1716,stirno/FluentAutomation,Simple Fluent API for UI Automation,https://github.com/stirno/FluentAutomation,auto
1717,ovh/cds,Enterprise-Grade Continuous Delivery & DevOps Automation Open Source Platform,https://github.com/ovh/cds,auto
1718,Unity-Technologies/AssetGraph,Visual Workflow Automation Tool for Unity.,https://github.com/Unity-Technologies/AssetGraph,auto
1719,strongdm/comply,"Compliance automation framework, focused on SOC2",https://github.com/strongdm/comply,auto
1721,muesli/beehive,A flexible event/agent & automation system with lots of bees 🐝,https://github.com/muesli/beehive,auto
1722,miyakogi/pyppeteer,Headless chrome/chromium automation library (unofficial port of puppeteer),https://github.com/miyakogi/pyppeteer,auto
1723,linkfy/Tools-for-Instagram,Automation scripts for Instagram,https://github.com/linkfy/Tools-for-Instagram,auto
1724,cloudera/cloudera-playbook,Cloudera deployment automation with Ansible,https://github.com/cloudera/cloudera-playbook,auto
1725,nodejs/automation,Better automation for the Node.js project,https://github.com/nodejs/automation,auto
1726,ARM-software/workload-automation,A framework for automating workload execution and measurement collection on ARM devices.,https://github.com/ARM-software/workload-automation,auto
1727,kubernetes-retired/kube-deploy,[EOL] A place for cluster deployment automation,https://github.com/kubernetes-retired/kube-deploy,auto
1728,NationalSecurityAgency/lemongrenade,Data-driven automation platform,https://github.com/NationalSecurityAgency/lemongrenade,auto
1729,DanMcInerney/pymetasploit3,Automation library for Metasploit,https://github.com/DanMcInerney/pymetasploit3,auto
1730,abaranovskis-redsamurai/automation-repo,Machine learning and process automation,https://github.com/abaranovskis-redsamurai/automation-repo,auto
1731,OCA/e-commerce,Odoo E-Commerce server automation addons,https://github.com/OCA/e-commerce,auto
1732,jenkins-infra/jenkins.io,A static site for the Jenkins automation server,https://github.com/jenkins-infra/jenkins.io,auto
1733,webdriverio/webdriverio,Next-gen browser and mobile automation test framework for Node.js,https://github.com/webdriverio/webdriverio,auto
1734,opencredo/test-automation-quickstart,"Quickstart project for test automation, covering performance, ui acceptance and api acceptance testing",https://github.com/opencredo/test-automation-quickstart,auto
1735,mergeability/mergeable,🤖 All the missing GitHub automation 🙂 🙌 ,https://github.com/mergeability/mergeable,auto
1736,GSA/fedramp-automation,FedRAMP Automation,https://github.com/GSA/fedramp-automation,auto
1737,PacktPublishing/Python-Automation-Cookbook,"Python Automation Cookbook, published by Packt",https://github.com/PacktPublishing/Python-Automation-Cookbook,auto
1738,2gis/Winium,Automation framework for Windows platforms,https://github.com/2gis/Winium,auto
1739,wix/Detox,Gray box end-to-end testing and automation framework for mobile apps,https://github.com/wix/Detox,auto
1740,CVEProject/automation-working-group,CVE Automation Working Group,https://github.com/CVEProject/automation-working-group,auto
1742,nightroman/Invoke-Build,Build Automation in PowerShell,https://github.com/nightroman/Invoke-Build,auto
1743,NetEaseGame/ATX,"Smart phone automation tool. Support iOS, Android, WebApp and game.",https://github.com/NetEaseGame/ATX,auto
1744,AutomationWithScripting/UdemyBoto3Scripts,AWS Automation boto3 scripts,https://github.com/AutomationWithScripting/UdemyBoto3Scripts,auto
1745,Infosys/Selenium-Testing-Automation-Framework,Selenium Open Source Testing Automation Framework (OSTAF),https://github.com/Infosys/Selenium-Testing-Automation-Framework,auto
1746,Readarr/Readarr,Book Manager and Automation (Sonarr for Ebooks),https://github.com/Readarr/Readarr,auto
1747,nautobot/nautobot,Network Source of Truth & Network Automation Platform,https://github.com/nautobot/nautobot,auto
1748,azureautomation/runbooks,Sample Automation runbooks,https://github.com/azureautomation/runbooks,auto
1749,lukeed/taskr,"A fast, concurrency-focused task automation tool.",https://github.com/lukeed/taskr,auto
1750,google/turbinia,Automation and Scaling of Digital Forensics Tools,https://github.com/google/turbinia,auto
1751,pyppeteer/pyppeteer,Headless chrome/chromium automation library (unofficial port of puppeteer),https://github.com/pyppeteer/pyppeteer,auto
1752,go-rod/rod,A Devtools driver for web automation and scraping,https://github.com/go-rod/rod,auto
1753,weidylan/Office_Automation_by_Using_Python,"Office Automation by Using Pythonf (For Excel, Word, PPT and PDF .....)",https://github.com/weidylan/Office_Automation_by_Using_Python,auto
1754,jobeasy-team/python-selenium-automation,"Test Automation with Python and Selenium, https://www.jobeasy.ai/automation",https://github.com/jobeasy-team/python-selenium-automation,auto
1755,webfp/tor-browser-selenium,Tor Browser automation with Selenium.,https://github.com/webfp/tor-browser-selenium,auto
1756,knassar702/scant3r,ScanT3r - Module based Bug Bounty Automation Tool,https://github.com/knassar702/scant3r,auto
1757,trailheadapps/automation-components,"Automation Components are a collection of reusable and production-ready extensions that include invocable actions, flow screen components and local actions.",https://github.com/trailheadapps/automation-components,auto
1758,tensult/aws-automation,AWS automation scripts and lambda functions,https://github.com/tensult/aws-automation,auto
1759,KathanP19/JSFScan.sh,Automation for javascript recon in bug bounty. ,https://github.com/KathanP19/JSFScan.sh,auto
1760,maurice-daly/DriverAutomationTool,Home of the Driver Automation Tool,https://github.com/maurice-daly/DriverAutomationTool,auto
1761,qsecure-labs/overlord,Overlord - Red Teaming Infrastructure Automation,https://github.com/qsecure-labs/overlord,auto
1762,jeremymaya/google-it-automation-with-python,Repository to keep track of Google IT Automation with Python provided by Coursera,https://github.com/jeremymaya/google-it-automation-with-python,auto
1763,swcarpentry/make-novice,Automation and Make,https://github.com/swcarpentry/make-novice,auto
1764,featurist/coypu,"Intuitive, robust browser automation for .Net",https://github.com/featurist/coypu,auto
1765,alex-page/github-project-automation-plus,🤖 Automate GitHub Project cards with any webhook event,https://github.com/alex-page/github-project-automation-plus,auto
1766,appium/appium,Cross-platform automation framework for all kinds of your apps built on top of W3C WebDriver protocol,https://github.com/appium/appium,auto
1767,Jmgr/actiona,Cross-platform automation tool,https://github.com/Jmgr/actiona,auto
1768,endormi/automation,Collection of automated tasks,https://github.com/endormi/automation,auto
1769,golemhq/golem,A complete test automation tool,https://github.com/golemhq/golem,auto
1770,ni-c/heimcontrol.js,Home-Automation with node.js and Raspberry PI,https://github.com/ni-c/heimcontrol.js,auto
1771,cfpb/jenkins-automation,Helpers for automating Jenkins via Groovy code—primarily job builders and utilities.,https://github.com/cfpb/jenkins-automation,auto
1772,jakewright/home-automation,Distributed home automation system,https://github.com/jakewright/home-automation,auto
1773,autobrr/autobrr,Automation for downloads.,https://github.com/autobrr/autobrr,auto
1774,nccgroup/port-scan-automation,Automate NMAP Scans and Generate Custom Nessus Policies Automatically,https://github.com/nccgroup/port-scan-automation,auto
1775,AppDaemon/appdaemon,:page_facing_up: Python Apps for Home Automation,https://github.com/AppDaemon/appdaemon,auto
1776,tobecrazy/SeleniumDemo,Selenium automation test framework,https://github.com/tobecrazy/SeleniumDemo,auto
1777,sdesalas/trifleJS,Headless automation for Internet Explorer,https://github.com/sdesalas/trifleJS,auto
1778,AirtestProject/Poco,A cross-engine test automation framework based on UI inspection,https://github.com/AirtestProject/Poco,auto
1779,Azure/sap-automation,This is the repository supporting the SAP deployment automation framework on Azure,https://github.com/Azure/sap-automation,auto
1780,mnkgrover08-zz/whatsapp_automation,"Whatsapp Automation is a collection of APIs that interact with WhatsApp messenger running in an Android emulator, allowing developers to build projects that automate sending and receiving messages, adding new contacts and broadcasting messages multiple contacts.",https://github.com/mnkgrover08-zz/whatsapp_automation,auto
1781,in28minutes/automation-testing-with-java-and-selenium,Learn Automation Testing with Java and Selenium,https://github.com/in28minutes/automation-testing-with-java-and-selenium,auto
1782,synesthesiam/rhasspy,Rhasspy voice assistant for offline home automation,https://github.com/synesthesiam/rhasspy,auto
1783,opensourceautomation/Open-Source-Automation,Open Source Automation is a home and commercial automation engine,https://github.com/opensourceautomation/Open-Source-Automation,auto
1785,anhtester/AutomationFrameworkSelenium,Test Automation Framework Selenium Java with TestNG building by Anh Tester,https://github.com/anhtester/AutomationFrameworkSelenium,auto
1786,saucelabs-training/automation-best-practices,This is an Automation Best Practices workshop designed to teach testing through cross-functional automation,https://github.com/saucelabs-training/automation-best-practices,auto
1787,mjolnirapp/mjolnir,Lightweight automation and productivity app for OS X,https://github.com/mjolnirapp/mjolnir,auto
1788,warhorse/warhorse,Infrastructure Automation,https://github.com/warhorse/warhorse,auto
1789,jgamblin/AWSScripts,Various AWS Automation Scripts,https://github.com/jgamblin/AWSScripts,auto
1790,microsoft/playwright,"Playwright is a framework for Web Testing and Automation. It allows testing Chromium, Firefox and WebKit with a single API. ",https://github.com/microsoft/playwright,auto
1791,BotLibre/BotLibre,"An open platform for artificial intelligence, chat bots, virtual agents, social media automation, and live chat automation.",https://github.com/BotLibre/BotLibre,auto
1792,Zeeshanahmad4/Facebook-Automation,Facebook automation,https://github.com/Zeeshanahmad4/Facebook-Automation,auto
1793,txels/autojenkins,Jenkins automation scripts,https://github.com/txels/autojenkins,auto
1794,autokey/autokey,"AutoKey, a desktop automation utility for Linux and X11.",https://github.com/autokey/autokey,auto
1795,hyb1996/Auto.js,Automation&Workflow JavaScript IDE on Android(安卓平台上的自动化工作流JavaScript IDE),https://github.com/hyb1996/Auto.js,auto
1796,lennylxx/ipv6-hosts,"Fork of https://code.google.com/archive/p/ipv6-hosts/, focusing on automation",https://github.com/lennylxx/ipv6-hosts,auto
1797,repeats/Repeat,"Cross-platform mouse/keyboard record/replay and automation hotkeys/macros creation, and more advanced automation features.",https://github.com/repeats/Repeat,auto
1798,senzhk/ADBKeyBoard,Android Virtual Keyboard Input via ADB (Useful for Test Automation),https://github.com/senzhk/ADBKeyBoard,auto
1799,prisma-archive/chromeless,🖥  Chrome automation made simple. Runs locally or headless on AWS Lambda.,https://github.com/prisma-archive/chromeless,auto
1800,Mad-robot/Spartan,My Recon Automation,https://github.com/Mad-robot/Spartan,auto
1801,aws-samples/aws-media-services-vod-automation,Sample code and CloudFormation scripts for automating Video on Demand workflows on AWS,https://github.com/aws-samples/aws-media-services-vod-automation,auto
1802,microsoft/AzureAutomation-Account-Modules-Update,An Azure Automation runbook that updates Azure modules imported into an Azure Automation account with module versions available on the PowerShell Gallery.,https://github.com/microsoft/AzureAutomation-Account-Modules-Update,auto
1803,redloro/smartthings,"SmartThings home automation services, apps and devices",https://github.com/redloro/smartthings,auto
1804,angiejones/automation-framework,"code for hybrid automation framework that supports UI, web services, and BDD",https://github.com/angiejones/automation-framework,auto
1805,riclolsen/OSHMI,SCADA HMI for substations and automation applications.,https://github.com/riclolsen/OSHMI,auto
1806,N-able/ScriptsAndAutomationPolicies,Standalone scripts and N-Central automation policies,https://github.com/N-able/ScriptsAndAutomationPolicies,auto
1807,gushphp/gush,Project Maintenance & Contributing Automation,https://github.com/gushphp/gush,auto
1808,Sergiy3013/genshin_automation,Automatic Daily Login Rewards (Multiple Accounts); Export wish history,https://github.com/Sergiy3013/genshin_automation,auto
1809,renyuneyun/Easer,User-defined explicit automation for Android,https://github.com/renyuneyun/Easer,auto
1810,controlm/automation-api-quickstart,Control-M Automation API quickstart,https://github.com/controlm/automation-api-quickstart,auto
1811,koutto/jok3r,Jok3r v3 BETA 2 - Network and Web Pentest Automation Framework,https://github.com/koutto/jok3r,auto
1812,microsoft/playwright-dotnet,.NET version of the Playwright testing and automation library.,https://github.com/microsoft/playwright-dotnet,auto
1813,getpopper/popper,Container-native task automation engine.,https://github.com/getpopper/popper,auto
1814,angiejones/automation-bookstore,automatable local webpage for Selenium workshop,https://github.com/angiejones/automation-bookstore,auto
1815,jairovadillo/pychromeless,Python Lambda Chrome Automation (naming pending),https://github.com/jairovadillo/pychromeless,auto
1816,nassir-malik/IOT-Pi3-Alexa-Automation,Use Raspberry Pi 3 as home automation device with Alexa,https://github.com/nassir-malik/IOT-Pi3-Alexa-Automation,auto
1817,rand0m1ze/ezsploit,Linux bash script automation for metasploit,https://github.com/rand0m1ze/ezsploit,auto
1818,JXA-Cookbook/JXA-Cookbook,Cookbook for JavaScript for Automation in Mac OS X Yosemite,https://github.com/JXA-Cookbook/JXA-Cookbook,auto
1819,mmarquee/ui-automation,Java wrapper for ms-uiautomation,https://github.com/mmarquee/ui-automation,auto
1820,automation-tools-bootcamp/automation-tools-bootcamp,A series of discussions about the development tools that enable automation of software projects.,https://github.com/automation-tools-bootcamp/automation-tools-bootcamp,auto
1821,kylemanna/docker-aosp,🏗 Minimal Android AOSP build environment with handy automation wrapper scripts,https://github.com/kylemanna/docker-aosp,auto
1822,paiml/python_devops_book,[Book-2020] Python For DevOps: Learn Ruthlessly Effective Automation,https://github.com/paiml/python_devops_book,auto
1823,hpsa/hpe-application-automation-tools-plugin,Jenkins plugin to run HP Application Automation tools,https://github.com/hpsa/hpe-application-automation-tools-plugin,auto
1825,rust-lang/simpleinfra,Rust Infrastructure automation,https://github.com/rust-lang/simpleinfra,auto
1826,HashNuke/hound,Elixir library for writing integration tests and browser automation,https://github.com/HashNuke/hound,auto
1827,ktbyers/pyplus_course,Python Network Automation Course,https://github.com/ktbyers/pyplus_course,auto
1828,ReimuNotMoe/ydotool,Generic command-line automation tool (no X!),https://github.com/ReimuNotMoe/ydotool,auto
1829,glitchassassin/lackey,Lackey - Graphical desktop automation with Python,https://github.com/glitchassassin/lackey,auto
1830,seleniumbase/SeleniumBase,A Python browser automation framework for creating reliable end-to-end tests.,https://github.com/seleniumbase/SeleniumBase,auto
1831,jjaw/Google-IT-automation-with-Python,Google's coursera course on Python automation,https://github.com/jjaw/Google-IT-automation-with-Python,auto
1832,ACloudGuru-Resources/Course_Introduction_to_Ansible,"Learn how to setup, develop, and utilize ansible automation.",https://github.com/ACloudGuru-Resources/Course_Introduction_to_Ansible,auto
1833,eliasnogueira/selenium-java-lean-test-architecture,Ready to use Lean Test Automation Architecture using Java and Selenium WebDriver to speed up your test automation,https://github.com/eliasnogueira/selenium-java-lean-test-architecture,auto
1834,atata-framework/atata,C#/.NET test automation framework for web,https://github.com/atata-framework/atata,auto
1835,vysecurity/ANGRYPUPPY,Bloodhound Attack Path Automation in CobaltStrike,https://github.com/vysecurity/ANGRYPUPPY,auto
1836,PacktPublishing/Python-Automation-Cookbook-Second-Edition,"Python-Automation-Cookbook-Second-Edition, published by Packt",https://github.com/PacktPublishing/Python-Automation-Cookbook-Second-Edition,auto
1837,alapanme/Cypress-Automation,Test Scripts demonstrating different features of the Cypress Automation Tool with Detailed Blog Articles. (check readme),https://github.com/alapanme/Cypress-Automation,auto
1838,cake-build/cake,:cake: Cake (C# Make) is a cross platform build automation system.,https://github.com/cake-build/cake,auto
1839,RedHatOfficial/ocp4-vsphere-upi-automation,Automates most of the manual steps of deploying OCP4.x cluster on vSphere,https://github.com/RedHatOfficial/ocp4-vsphere-upi-automation,auto
1840,ibm-cloud-architecture/terraform-openshift4-aws,OpenShift 4 installation automation asset,https://github.com/ibm-cloud-architecture/terraform-openshift4-aws,auto
1841,zebrunner/carina,"Carina automation framework: Web, Mobile, API, DB etc testing...",https://github.com/zebrunner/carina,auto
1842,eth-educators/eth-docker,Docker automation for Ethereum staking full nodes,https://github.com/eth-educators/eth-docker,auto
1843,anders94/raspberry-pi-home-automation,A node.js based home automation system based around the Raspberry Pi.,https://github.com/anders94/raspberry-pi-home-automation,auto
1844,netbox-community/netbox,The premiere source of truth powering network automation. Open source under Apache 2. Public demo: https://demo.netbox.dev,https://github.com/netbox-community/netbox,auto
1845,dopplertask/dopplertask,A revolutionary open-source automation tool,https://github.com/dopplertask/dopplertask,auto
1846,cerberustesting/cerberus-core,The Open Source Test Automation Platform.,https://github.com/cerberustesting/cerberus-core,auto
1847,arviedelgado/roro,"roro is a free, open-source robotic process automation software",https://github.com/arviedelgado/roro,auto
1848,Autodesk-Forge/forge-tutorial-postman,Postman collection for Forge Design Automation tutorials,https://github.com/Autodesk-Forge/forge-tutorial-postman,auto
1849,clearlinux/autospec,RPM packaging automation tool,https://github.com/clearlinux/autospec,auto
1850,USSCltd/dorks,google hack database automation tool,https://github.com/USSCltd/dorks,auto
1851,vmware/terraform-provider-vra,Terraform VMware vRealize Automation provider,https://github.com/vmware/terraform-provider-vra,auto
1852,BretFisher/docker-cicd-automation,GitHub Actions automation examples with Docker,https://github.com/BretFisher/docker-cicd-automation,auto
1853,lepton-eda/lepton-eda,GPL Electronic Design Automation,https://github.com/lepton-eda/lepton-eda,auto
1854,armzilla/amazon-echo-ha-bridge,emulates philips hue api to other home automation gateways,https://github.com/armzilla/amazon-echo-ha-bridge,auto
1855,OpenKore/openkore,A free/open source client and automation tool for Ragnarok Online,https://github.com/OpenKore/openkore,auto
1856,jaskie/PlayoutAutomation,Television broadcast automation system,https://github.com/jaskie/PlayoutAutomation,auto
1857,vasani-arpit/Social-Media-Automation,Automate social media because you don't have to be active on all of them😉. Best way to be active on all social media without actually being active on them. 😃,https://github.com/vasani-arpit/Social-Media-Automation,auto
1858,gwaredd/unium,Automation for Unity games,https://github.com/gwaredd/unium,auto
1859,mtrubs/intellibot,IntelliJ/PyCharm plugin for Robot Automation Framework,https://github.com/mtrubs/intellibot,auto
1860,zachcurry/emacs-anywhere,Configurable automation + hooks called with application information,https://github.com/zachcurry/emacs-anywhere,auto
1861,uc-cdis/cloud-automation,Automation for standing up Gen3 commons,https://github.com/uc-cdis/cloud-automation,auto
1862,ProtoTest/ProtoTest.Golem,C# Test Automation Framework,https://github.com/ProtoTest/ProtoTest.Golem,auto
1863,containers/automation,Automation scripts and configurations common across the containers org. repositories,https://github.com/containers/automation,auto
1864,lamoda/gonkey,Gonkey - a testing automation tool,https://github.com/lamoda/gonkey,auto
1865,phillips321/adaudit,Powershell script to do domain auditing automation ,https://github.com/phillips321/adaudit,auto
1866,paypal/nemo-core,Selenium-webdriver based automation in node.js,https://github.com/paypal/nemo-core,auto
1867,pulumi/automation-api-examples,Examples for the Pulumi Automation API https://pkg.go.dev/github.com/pulumi/pulumi/sdk/v3/go/auto?tab=doc,https://github.com/pulumi/automation-api-examples,auto
1868,abouillot/HomeAutomation,Home Automation repository,https://github.com/abouillot/HomeAutomation,auto
1869,ansible/ansible-hub-ui,Ansible Automation Hub UI,https://github.com/ansible/ansible-hub-ui,auto
1870,eNMS-automation/eNMS,An enterprise-grade vendor-agnostic network automation platform.,https://github.com/eNMS-automation/eNMS,auto
1871,SFDO-Tooling/CumulusCI,Python framework for building portable automation for Salesforce projects,https://github.com/SFDO-Tooling/CumulusCI,auto
1872,robusta-dev/robusta,"Kubernetes observability and automation, with an awesome Prometheus integration",https://github.com/robusta-dev/robusta,auto
1873,dataplat/dbatools,"🚀 SQL Server automation and instance migrations have never been safer, faster or freer",https://github.com/dataplat/dbatools,auto
1874,prahladyeri/VisualAlchemist,Open source database diagramming and automation tool,https://github.com/prahladyeri/VisualAlchemist,auto
1875,pimoroni/automation-hat,"Python library and examples for the Pimoroni Automation HAT, pHAT and HAT Mini",https://github.com/pimoroni/automation-hat,auto
1876,agarwalsarthak121/web_crawlers,I was always intrigued whenever I see how someone can automate a task with a few lines of code. Here are some interesting Python scripts that lets you automate various daily tasks.,https://github.com/agarwalsarthak121/web_crawlers,auto
1877,Mechazawa/REDBetter-crawler,A fork of whatbetter that can run autonomously,https://github.com/Mechazawa/REDBetter-crawler,auto
1878,EgorLakomkin/KTSpeechCrawler,Automatically constructing corpus for automatic speech recognition from YouTube videos ,https://github.com/EgorLakomkin/KTSpeechCrawler,auto
1880,opsdisk/pagodo,pagodo (Passive Google Dork) - Automate Google Hacking Database scraping and searching,https://github.com/opsdisk/pagodo,auto
1881,apify/crawlee,Crawlee—A web scraping and browser automation library for Node.js that helps you build reliable crawlers. Fast.,https://github.com/apify/crawlee,auto
1882,sparklemotion/mechanize,Mechanize is a ruby library that makes automated web interaction easy.,https://github.com/sparklemotion/mechanize,auto
1883,Lemons1337/Discord-Spammer,Advanced Discord Spammer with multiple options and auto scraping proxies!,https://github.com/Lemons1337/Discord-Spammer,auto
1884,aapatre/Automatic-Udemy-Course-Enroller-GET-PAID-UDEMY-COURSES-for-FREE,"Do you want to LEARN NEW STUFF for FREE? Don't worry, with the power of web-scraping and automation, this script will find the necessary Udemy coupons & enroll you for PAID UDEMY COURSES, ABSOLUTELY FREE!",https://github.com/aapatre/Automatic-Udemy-Course-Enroller-GET-PAID-UDEMY-COURSES-for-FREE,auto
1885,jbms/finance-dl,Tools for automatically downloading/scraping personal financial data.,https://github.com/jbms/finance-dl,auto
1886,linvo-io/linvo-scraper,Linkedin Automation Bot with every possible scraping! Valid for 2022 used by Linvo.io,https://github.com/linvo-io/linvo-scraper,auto
1887,zackw/tbbscraper,Automated website scraping over Tor,https://github.com/zackw/tbbscraper,auto
1888,guptachetan1997/crawling-projects,Web scraping and automation using python,https://github.com/guptachetan1997/crawling-projects,auto
1889,sebastien/wwwclient,"Advanced web browsing, scraping and automation",https://github.com/sebastien/wwwclient,auto
1890,shirosaidev/sharesniffer,Network share sniffer and auto-mounter for crawling remote file systems,https://github.com/shirosaidev/sharesniffer,auto
1892,corywalker/selenium-crawler,Sometimes sites make crawling hard. Selenium-crawler uses selenium automation to fix that.,https://github.com/corywalker/selenium-crawler,auto
1893,minhhungit/github-action-rss-crawler,Rss 100% auto crawling using Github Action,https://github.com/minhhungit/github-action-rss-crawler,auto
1894,webrecorder/browsertrix-old,Browsertrix: Containerized High-Fidelity Browser-Based Automated Crawling + Behavior System,https://github.com/webrecorder/browsertrix-old,auto
1895,maguowei/app-crawler,crawling App by uiautomator2 & mitmproxy,https://github.com/maguowei/app-crawler,auto
1896,GregBrimble/tailwindui-crawler-action,Automate the crawling and cataloging of the Tailwind UI components,https://github.com/GregBrimble/tailwindui-crawler-action,auto
1897,smicallef/spiderfoot,SpiderFoot automates OSINT for threat intelligence and mapping your attack surface.,https://github.com/smicallef/spiderfoot,auto
1902,googleapis/repo-automation-bots,"A collection of bots, based on probot, for performing common maintenance tasks across the open-source repos managed by Google on GitHub.",https://github.com/googleapis/repo-automation-bots,AI
1903,BMW-InnovationLab/BMW-YOLOv4-Training-Automation,This repository allows you to get started with training a state-of-the-art Deep Learning model with little to no configuration needed!  You provide your labeled dataset or label your dataset using our BMW-LabelTool-Lite and you can start the training right away and monitor it in many different ways like TensorBoard or a custom REST API and GUI. NoCode training with YOLOv4 and YOLOV3 has never been so easy.,https://github.com/BMW-InnovationLab/BMW-YOLOv4-Training-Automation,AI
1910,reportportal/reportportal,Main Repository. Report Portal starts here - see readme below.,https://github.com/reportportal/reportportal,AI
1913,thp/urlwatch,"Watch (parts of) webpages and get notified when something changes via e-mail, on your phone or via other means. Highly configurable.",https://github.com/thp/urlwatch,AI
1915,niqdev/packtpub-crawler,Download your daily free Packt Publishing eBook https://www.packtpub.com/packt/offers/free-learning,https://github.com/niqdev/packtpub-crawler,AI
1916,cassieeric/python_crawler,This repository is mainly about Python web crawler,https://github.com/cassieeric/python_crawler,AI
1918,kiliman/tailwindui-crawler,tailwindui-crawler downloads the component HTML files locally,https://github.com/kiliman/tailwindui-crawler,AI
1919,eliangcs/pystock-crawler,"Crawl and parse financial reports (XBRL) from SEC EDGAR, and daily stock prices from Yahoo Finance",https://github.com/eliangcs/pystock-crawler,AI
1920,samwize/python-email-crawler,"Search on Google, and crawls for emails related to the result",https://github.com/samwize/python-email-crawler,AI
1921,MA3STR0/kimsufi-crawler,Crawler that will send you an email alert as soon as servers on OVH/Kimsufi become available for purchase,https://github.com/MA3STR0/kimsufi-crawler,AI
1922,oGsLP/kuaishou-crawler,"As you can see, a kuaishou crawler",https://github.com/oGsLP/kuaishou-crawler,AI
1923,hedii/php-crawler,A php crawler that finds emails on the internets,https://github.com/hedii/php-crawler,AI
1924,mazzzystar/BaiduCrawler,Sample of using proxies to crawl baidu search results.,https://github.com/mazzzystar/BaiduCrawler,AI
1925,hzzlzz/crawler,A naive web crawler written in C,https://github.com/hzzlzz/crawler,AI
1926,VIDA-NYU/ache,ACHE is a web crawler for domain-specific search.,https://github.com/VIDA-NYU/ache,AI
1927,mehmetozkaya/DotnetCrawler,"DotnetCrawler is a straightforward, lightweight web crawling/scrapying library for Entity Framework Core output based on dotnet core. This library designed like other strong crawler libraries like WebMagic and Scrapy but for enabling extandable your custom requirements. Medium link : https://medium.com/@mehmetozkaya/creating-custom-web-crawler-with-dotnet-core-using-entity-framework-core-ec8d23f0ca7c",https://github.com/mehmetozkaya/DotnetCrawler,AI
1928,CallMeJake/BlockCrawler,Portable BitCoin Block Chain Explorer,https://github.com/CallMeJake/BlockCrawler,AI
1929,webrecorder/browsertrix-crawler,Run a high-fidelity browser-based crawler in a single Docker container,https://github.com/webrecorder/browsertrix-crawler,AI
1930,DMinerJackie/JewelCrawler,"豆瓣电影爬虫——a crawler which is able to crawl movie detail and short comments, save them to database mysql, also include Sentiment analysis based on comments",https://github.com/DMinerJackie/JewelCrawler,AI
1931,7eu7d7/pixiv_AI_crawler,基于深度学习的p站高质量涩图AI爬虫，可以学会你的XP,https://github.com/7eu7d7/pixiv_AI_crawler,AI
1932,je-suis-tm/web-scraping,"Detailed web scraping tutorials for dummies with financial data crawlers on Reddit WallStreetBets, CME (both options and futures), US Treasury, CFTC, LME, MacroTrends, SHFE and alternative data crawlers on Tomtom, BBC, Wall Street Journal, Al Jazeera, Reuters, Financial Times, Bloomberg, CNN, Fortune, The Economist",https://github.com/je-suis-tm/web-scraping,AI
1933,reanalytics-databoutique/webscraping-open-project,The web scraping open project repository aims to share knowledge and experiences about web scraping with Python,https://github.com/reanalytics-databoutique/webscraping-open-project,AI
1934,mjhea0/node-express-ajax-craigslist,scraping craigslist,https://github.com/mjhea0/node-express-ajax-craigslist,AI
1935,common-voice/cv-sentence-extractor,Scraping Wikipedia for fair use sentences,https://github.com/common-voice/cv-sentence-extractor,AI
1937,MartinKBeck/TwitterScraper,Repository containing all files relevant to my basic and advanced tweet scraping articles.,https://github.com/MartinKBeck/TwitterScraper,AI
1938,federicohaag/LinkedInScraping,Scraping of LinkedIn Profiles: Creates an Excel file containing the personal data and the last job position of all the provided LinkedIn profiles.,https://github.com/federicohaag/LinkedInScraping,AI
1939,niespodd/browser-fingerprinting,Analysis of Bot Protection systems with available countermeasures 🚿. How to defeat anti-bot system 👻 and get around browser fingerprinting scripts 🕵️‍♂️ when scraping the web?,https://github.com/niespodd/browser-fingerprinting,AI
1940,GoTrained/Scrapy-Craigslist,Web Scraping Craigslist's Engineering Jobs in NY with Scrapy,https://github.com/GoTrained/Scrapy-Craigslist,AI
1941,dharmafly/noodle,A node server and module which allows for cross-domain page scraping on web documents with JSONP or POST.,https://github.com/dharmafly/noodle,AI
1942,vifreefly/kimuraframework,"Kimurai is a modern web scraping framework written in Ruby which works out of box with Headless Chromium/Firefox, PhantomJS, or simple HTTP requests and allows to scrape and interact with JavaScript rendered websites",https://github.com/vifreefly/kimuraframework,AI
1943,niklasb/dryscrape,"[not actively maintained] A lightweight Python library that uses Webkit to enable easy scraping of dynamic, Javascript-heavy web pages",https://github.com/niklasb/dryscrape,AI
1944,banool/recreation-gov-campsite-checker,Scrapes the recreation.gov website to check for campsite availabilities 🏕🏕,https://github.com/banool/recreation-gov-campsite-checker,AI
1945,mjhea0/Scrapy-Samples,Scrapy examples crawling Craigslist,https://github.com/mjhea0/Scrapy-Samples,AI
1946,fugary/calibre-douban,"Calibre new douban metadata source plugin. Douban no longer provides book APIs to the public, so it can only use web crawling to obtain data. This is a calibre Douban plugin based on web crawling.",https://github.com/fugary/calibre-douban,AI
1947,pH-7/Crawling-Emails,Very simple bash script to crawl email addresses from a specific website.,https://github.com/pH-7/Crawling-Emails,AI
1949,EastonLee/Taobao_Crawler,A demo project based on Scrapy (with Selenium) crawling air conditioner sales data from Taobao.,https://github.com/EastonLee/Taobao_Crawler,AI
1950,getamis/eth-indexer,An Ethereum project to crawl blockchain states into database,https://github.com/getamis/eth-indexer,AI
1952,crytic/fluxture,A crawling framework for blockchains and peer-to-peer systems,https://github.com/crytic/fluxture,AI
1955,nmalcolm/Inventus,Inventus is a spider designed to find subdomains of a specific domain by crawling it and any subdomains it discovers.,https://github.com/nmalcolm/Inventus,AI
1957,kapilkchaurasia/Data-mining-python-script,"It contain various script on web crawling/ data mining of social web(RSS,facebook,twitter,Linkedin)",https://github.com/kapilkchaurasia/Data-mining-python-script,AI
1958,scrapy-plugins/scrapy-deltafetch,Scrapy spider middleware to ignore requests to pages containing items seen in previous crawls,https://github.com/scrapy-plugins/scrapy-deltafetch,AI
1959,edoardottt/cariddi,"Take a list of domains, crawl urls and scan for endpoints, secrets, api keys, file extensions, tokens and more",https://github.com/edoardottt/cariddi,AI
1960,relevance/tarantula,"a big hairy fuzzy spider that crawls your site, wreaking havoc",https://github.com/relevance/tarantula,AI
1961,hkbus/hk-bus-crawling,"Daily update the bus route, fare, stop information across KMB, CTB and NWFB in HK. The data information is ready to work with the ETA APIs provided by data.gov.hk",https://github.com/hkbus/hk-bus-crawling,AI
1962,9b/frisbee,Collect email addresses by crawling search engine results.,https://github.com/9b/frisbee,AI
1963,g0v/twly_crawler,Crawl Taiwan Congress Data by Scrapy,https://github.com/g0v/twly_crawler,AI
1964,iawia002/Lulu,[Unmaintained] A simple and clean video/music/image downloader 👾,https://github.com/iawia002/Lulu,AI
1965,kevincobain2000/email_extractor,Yes it works! Email Extractor by Full Url Crawl. Extract emails and web urls from a website with full crawl or option depth of urls to crawl using terminal and python.,https://github.com/kevincobain2000/email_extractor,AI
1966,Jack-Cherish/python-spider,:rainbow:Python3网络爬虫实战：淘宝、京东、网易云、B站、12306、抖音、笔趣阁、漫画小说下载、音乐电影下载等,https://github.com/Jack-Cherish/python-spider,AI
1967,kangvcar/InfoSpider,INFO-SPIDER 是一个集众多数据源于一身的爬虫工具箱🧰，旨在安全快捷的帮助用户拿回自己的数据，工具代码开源，流程透明。支持数据源包括GitHub、QQ邮箱、网易邮箱、阿里邮箱、新浪邮箱、Hotmail邮箱、Outlook邮箱、京东、淘宝、支付宝、中国移动、中国联通、中国电信、知乎、哔哩哔哩、网易云音乐、QQ好友、QQ群、生成朋友圈相册、浏览器浏览历史、12306、博客园、CSDN博客、开源中国博客、简书。,https://github.com/kangvcar/InfoSpider,AI
1968,taoyds/spider,scripts and baselines for Spider: Yale complex and cross-domain semantic parsing and text-to-SQL challenge,https://github.com/taoyds/spider,AI
1969,BaiduSpider/BaiduSpider,BaiduSpider，一个爬取百度搜索结果的爬虫，目前支持百度网页搜索，百度图片搜索，百度知道搜索，百度视频搜索，百度资讯搜索，百度文库搜索，百度经验搜索和百度百科搜索。,https://github.com/BaiduSpider/BaiduSpider,AI
1970,TheRook/subbrute,"A DNS meta-query spider that enumerates DNS records, and subdomains.",https://github.com/TheRook/subbrute,AI
1971,RASSec/pentestER-Fully-automatic-scanner,DNS Subdomain● Brute force ● Web Spider ● Nmap Scan ● etc,https://github.com/RASSec/pentestER-Fully-automatic-scanner,AI
1972,changetjut/ProxySpider,爬取http://www.xicidaili.com/上代理IP，并验证代理可用性,https://github.com/changetjut/ProxySpider,AI
1973,Henryhaohao/Wenshu_Spider,:rainbow:Wenshu_Spider-Scrapy框架爬取中国裁判文书网案件数据(2019-1-9最新版),https://github.com/Henryhaohao/Wenshu_Spider,AI
1974,zhaoolee/bdwenku-spider,一只百度文库的爬虫 A spider of baiduwenku,https://github.com/zhaoolee/bdwenku-spider,AI
1975,postmodern/spidr,"A versatile Ruby web spidering library that can spider a site, multiple domains, certain links or infinitely. Spidr is designed to be fast and easy to use.",https://github.com/postmodern/spidr,AI
1978,clips/pattern,"Web mining module for Python, with tools for scraping, natural language processing, machine learning, network analysis and visualization.",https://github.com/clips/pattern,Machine Learning
1979,MiyainNYC/Financial-Modeling,"Option Pricing, Volatility Prediction, Machine Learning, Black Scholas, Web Crawling",https://github.com/MiyainNYC/Financial-Modeling,Machine Learning
1981,Trying to extract a specific number or character from a body of text rather than the whole line.,"The all important info - 
VERSION BUILD=10101485
OS=macOS Monterey 12.1
Browser=95.0.2 (64-bit)
Other macros functioning as expected=YES

I'd like to my post with an apology, the last time I asked for support I simply gave up but didn't post to say thank you. 

I have been furthering my knowledge of iMacro and it's potential functions playing an online text based games. I'm trying to automate the purchase of a product. I think I've tried a variety of different methods and solutions between extracting css selectors, using the clipboard function and caused myself to become more confused. 
Code: Select allSET !EXTRACT_TEST_POPUP NO
URL GOTO=https://awebsite.com/site.php?page=foodhall
' copy required info
TAG POS=2 TYPE=DIV ATTR=TXT:This<SP>foodhall<SP>is<SP>owned<SP>by<SP>Dave<SP>U* EXTRACT=TXT
SET !CLIPBOARD {{!EXTRACT}}
WAIT SECONDS=2
' paste required info
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:https://awebsite.com/site.php?page=foodhall ATTR=NAME:buyfood CONTENT={{!CLIPBOARD}}
WAIT SECONDS=20
' submit
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:https://awebsite.com/site.php?page=foodhall ATTR=NAME:give
WAIT SECONDS=10

The comment out is for my reference 

The extract function works but I would like some assistance in getting a specific piece of information out of it. 
It returns the following -
This foodhall is owned by Harry's Uncle!
And being held by Harry!
It currently has 220 hams.
The bullet price is $1,000 for each ham!

I would like to extract the 220 in this case (the number changes over time) and paste it into a box below it and submit the form. 

I can provide the html where this information is found if required.

Any assistance would be greatly appreciated. The forum, program and extensions are fantastic products, I will be purchasing a full license in the near future!",https://forum.imacros.net/viewtopic.php?f=7&t=31863&sid=ddad572d7e36bd7d9ce3abd083a6d173,program
1982,Extract variable from output table to select hyperlink,"roblem Description Information:
1. iMacros Version: iMacros Personal Edition for Firefox 8.9.7 (VERSION BUILD=10021450)
2. Windows 10 (64-bit) [English]
3. Firefox 83
4. Included demos work ok.
5. Included VBS sample scripts run ok.
6. Not applicable/no specific recording or replay fails on a specific website.
7. Not applicable/no test for encounter of the same problem with the: iMacros Browser, iMacros for Internet Explorer, iMacros for Firefox, and iMacros for Chrome.

Problem:
I am trying to write an iMacros script that loads a webpage, fills out a form with the required attributes, searches, and from the search output table select a hyperlinked identifier (that changes every day) to open the page that allows me to download the CSV I need.

Ive managed to write the script to populate the form and search to produce the output I want to select. In the output table (attached capture) I want to select the hyperlinked value underneath 'OutpudId'. This I can do if I explicitly tell the script what to select but it changes everyday and I want the script to by dynamic to find the value in that position to then be able to select it day in day out.

section of code:
Code: Select allVERSION BUILD=10021450
URL GOTO=https://website.com/#/

TAG POS=1 TYPE=SELECT ATTR=ID:select-program-name CONTENT=%string:Merchant
TAG POS=1 TYPE=BUTTON ATTR=ID:add-iogls-button
TAG POS=1 TYPE=SELECT ATTR=TXT:1188233-HUK-UGL* CONTENT=%string:HUK
TAG POS=1 TYPE=BUTTON ATTR=TXT:Select
TAG POS=1 TYPE=INPUT:DATE ATTR=ID:startDate  CONTENT= EVAL(""var d=new Date();d.setDate(d.getDate()-1);var year=d.getFullYear();var month=d.getMonth()+1;var day=d.getDate();year+'-'+month+'-'+day;"")   ---select yesterday
TAG POS=1 TYPE=BUTTON ATTR=ID:download_button
TAG POS=1 TYPE=A ATTR=TXT:a0451cb7 ---- this is the variable I want to select. Hardcoded selects it but it changes every time the form is submitted


Ive attempted to use Extract in different ways to no success. Please see below:
Code: Select allTAG POS=1 TYPE=TD ATTR=TXT:OutputId* EXTRACT=OutputId
TAG POS=1 TYPE=A ATTR=TXT:OutputId

or
Code: Select allTAG POS=1 TYPE=TH ATTR=TXT:OutputId EXTRACT=TXT

Any help would be greatly appreciated




",https://forum.imacros.net/viewtopic.php?f=7&t=31306&sid=117ef7144580822f4982137e21eddfc2,program
1983,Extract from specific cell/row from CSV,"Hi,
i am new and currently using the freeware plugin for Chrome but i am considering buying the Personal Edition license since i am really interested in extract and use data from Excel.

I am using VERSION BUILD=1005 RECORDER=CR
Bowser Chrome 84
iMacros for Chrome
Windows 10 Professional 64-bit Operating system

I checked the demo projects and also searched this forum however i am not very good with programming and alot of the contents are really not easy to understand for me.
What i want to achieve, is to fill out a webform based on data i have locally saved on my harddrive. The data is somewhere in the Excel file so i cannot use the loop model where it seemingly just flies through the document and prints everyhting into a webform. What i need is instructions how to define and extract content from a specific line / column combination and put it into a specific field of the webform.

Can anybody help me with the code for this?",https://forum.imacros.net/viewtopic.php?f=7&t=31161&sid=117ef7144580822f4982137e21eddfc2,program
1984,Extracting users in instagram (in the likes of the photo),"Hi everybody! i am new on imacros scripting and i am not a programmer. I know very little about programming and I only adapt the command lines and analyze how they work, so I am making the program work.

I managed to learn about imacros at this time but still I need to explain step by step all detailed how to run the following program.

I make a video explain the problem.
Please look at the cursor carefully and interpret how it works, it's simple

https://youtu.be/tMbF3CawzRU


My goal is as follows: Open the profile of a user, select a photo and enter the ""Like"", within the ""I like"" enter the profile of each person to extract the username ""URL"".

The problem is that my program only extracts 11 users (Those that appear as ""DIV"" in the instagram code).

I designed another program which appears at the beginning of the video to demonstrate how it extracts perfectly but this is ONLY for FOLLOWERS - FOLLOWED.
Analyze the instagram code and this code allows me to ""preload"" the number of people I need and this makes the program WORK. Unlike the previous one (within the ""LIKES"" IT DOES NOT ALLOW ME ""PRE LOAD"" USERS and this limits the program (LOOP = 12-15-20-30-etc. It stops working with error # EANF #).

First program ( VIDEO ) code: ""FOLLOWES AND FOLLOWED (ONLY) ""
Code: Select allSET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP NO
SET !TIMEOUT_STEP 1

TAG POS={{!LOOP}} TYPE=A ATTR=CLASS:FPmhX<SP>notranslate<SP>_0imsa EXTRACT=HREF


SET !VAR1 {{!EXTRACT}}
SET !EXTRACT NULL

'Check EXTRACT
TAB OPEN
TAB T=2
URL GOTO={{!VAR1}}
WAIT SECONDS=2


MY GOALS CODE ( SECOND and the one that causes the problem ) "" ON LIKES PHOTO ""
Code: Select allSET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP NO
SET !TIMEOUT_STEP 1

TAG POS={{!LOOP}} TYPE=DIV ATTR=CLASS:_7UhW9<SP>xLCgt<SP>qyrsm<SP>KV-D4<SP>fDxYl<SP>rWtOq EXTRACT=TXT

SET !VAR1 {{!EXTRACT}}
SET !EXTRACT NULL

'Check EXTRACT
TAB OPEN
TAB T=2
URL GOTO=https://www.instagram.com/{{!VAR1}}
WAIT SECONDS=2



Sorry for my bad English, it was translated with Google Translator
HELP ME PLEASE!

Windows 10 x64
Chrome  Versi贸n 76.0.3809.132 (Build oficial) (64 bits)
Chrome Imacros FREE 10.0.5 
Firefox 69.0 (64-bit)
Firefox Imacros FREE 10.0.2.1450 
( It works on both ) but i prefer firefox for Imacros.

I don't think they need anything more than this.
I am looking to extract the number of users that I need (20-30-50 users within the likes of the photo)
Apparently the DIV structure does not allow pre-loading users and I don't know how to solve this in order to extract the number of users that I want

THANKS YOU FOR READ ME!",https://forum.imacros.net/viewtopic.php?f=7&t=30534&sid=a8b6b8836ef4b006067cba7caee672f3,program
1985,Help with download of stock data,"I want to download data from two pages into two different files.  These are tables containing stock market data.  After I get the downloaded data I will further process it with Excel Macros and VB6 coding.
IMacro 9.0.3
Firefox 55.0.3
Windows 10

<code>: (Sorry but I don't see the item to include code)

VERSION BUILD=10.4.28.1074
TAB T=1
TAB CLOSEALLOTHERS
SET !EXTRACT_TEST_POPUP NO
TAG POS=1 TYPE=TABLE ATTR=TXT:* EXTRACT=TXT
URL GOTO=https://finance.yahoo.com/portfolio/p_57/view/view_8
WAIT SECONDS=6
SAVEAS TYPE=EXTRACT FOLDER=c:/stox17/{{!NOW:mm-dd-yyyy}}/ FILE=download_AJ.csv
TAG POS=1 TYPE=TABLE ATTR=TXT:* EXTRACT=TXT
URL GOTO=https://finance.yahoo.com/portfolio/p_93/view/view_8
WAIT SECONDS=6
SAVEAS TYPE=EXTRACT FOLDER=c:/stox17/{{!NOW:mm-dd-yyyy}}/ FILE=download_KZ.csv

</code>

I have two issues with the downloaded data.
1.  The data is going into the wrong file.  The data from the first page is going into the file with suffix KZ and the data from the second page is going into the file with the suffix AJ.  I could simply rename the files but I don't think I should have to.
2.  The first column has the data duplicated in the same column: ie ""ABC"" comes through as ""ABCABC"".  Again I could handle that in programming but I don't think I should have to.

Thanx for any help and guidance.",https://forum.imacros.net/viewtopic.php?f=7&t=30356&sid=a8b6b8836ef4b006067cba7caee672f3,program
1986,Extracting complicated text,"Imacros browser V8.3 trial
Windows 10
Hey I am trying to make simple script to solve internet test for me automatically.
I made script that works well with simple text but simetimes text is confusing. Text on both sites look the same.
There is my script 
Code: Select allVERSION BUILD=8032216
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=http://www.odpowiedzi.c0.pl/algorytmika_programowanie/jak_wnioskuja_maszyny.html
TAB OPEN
TAG POS=1 TYPE=H4 ATTR=* EXTRACT=TXT
SET VAR1 {{!extract}}
SET !EXTRACT NULL
TAG POS=188 TYPE=P ATTR=* EXTRACT=TXT
SET VAR2 {{!extract}}
TAB T=2
URL GOTO=https://it-szkola.edu.pl/kkurs,kurs,59,test
TAG POS=1 TYPE=TH ATTR=TXT:*{{VAR1}}*
TAG POS=R1 TYPE=LABEL FORM=ID:testForm ATTR=TXT:{{VAR2}}


It copies question and correct answer then open site with test search for question then choose correct answer. 

Site with answers code:
Code: Select all        <h4>Question?</h4>
        <p>answer</p>
        <br>


Site with test:
Code: Select all<div class=""testPyt  "">
                                <table>
                                    <tr>
                                        <th colspan=""2"">
                                            <span class=""testPytLiczFull""><span class=""testPytLicz"">3</span>. </span>
                                             
                                            Question?<br />                                     </th>
                                    </tr>
                                                                            <tr>
                                            <td class=""testCheck""><input type=""checkbox"" name=""idodp[830845]"" id=""idodp_828949_830845"" value=""1"" /></td>
                                            <td class=""testQuest""><label for=""idodp_828949_830845"">ANSWER1</label></td>
                                        </tr>
                                                                                <tr>
                                            <td class=""testCheck""><input type=""checkbox"" name=""idodp[830846]"" id=""idodp_828949_830846"" value=""1"" /></td>
                                            <td class=""testQuest""><label for=""idodp_828949_830846"">ANSWER2</label></td>
                                        </tr>
                                                                                <tr>
                                            <td class=""testCheck""><input type=""checkbox"" name=""idodp[830844]"" id=""idodp_828949_830844"" value=""1"" /></td>
                                            <td class=""testQuest""><label for=""idodp_828949_830844"">ANSWER3</label></td>
                                        </tr>
                                        
                                </table>

                                <br /><br />
                            </div>


When the question is one sentence it works perfectly, but sometimes it looks like this.

Site with answers:
Code: Select all<h4>Question about script below:<br>

<script type=""text/javascript""><br>
  z = 1;<br>
  function a()<br>
  {<br>
    var z = 2;<br>
  }<br>
  a();<br>
  alert(z)<br>
</script></h4>
        <p>Answer</p>


Test site code:
Code: Select all<th colspan=""2"">
                                            <span class=""testPytLiczFull""><span class=""testPytLicz"">10</span>. </span>
                                             
                                            Co b臋dzie wynikiem wykonania nast臋puj膮cego skryptu:<br /><br />
  <script type=""text/javascript""><br />
  &nbsp;&nbsp;z = 1;<br />
  &nbsp;&nbsp;function a()<br />
  &nbsp;&nbsp;{<br />
  &nbsp;&nbsp;&nbsp;&nbsp;var z = 2;<br />
  &nbsp;&nbsp;}<br />
  &nbsp;&nbsp;a();<br />
  &nbsp;&nbsp;alert(z)<br />
  </script>                                        </th>


When i run code i get error that it can't find matching element.
Code: Select allError -1300: Cannot find HTML element of type ""TH:"" with attribute(s) ""TXT:*Co b臋dzie wynikiem wykonania nast臋puj膮cego skryptu: <script type=""text/javascript"">z = Array();z.push(""ABC"");z.push(""D"");z.push(""E"");alert(z[1])</script>*"".. Line 13: TAG POS=1 TYPE=TH ATTR=TXT:*{{VAR1}}*



Text on both sites look the same. 
Is there any universal solution that work with complicated text without difficult javascript?",https://forum.imacros.net/viewtopic.php?f=7&t=29984&sid=a8b6b8836ef4b006067cba7caee672f3,program
1987,Extract Data and Paste to Field with existing text,"So I just purchased the program for the file data extraction option and I'm having some trouble figuring out how to make it work. 

I have a CSV file with two columns.  Column 1 has a number that needs to be searched, and column 2 has a code that I need inserted into a field with existing information in that numbers page.  

For example:
Column 1  Column 2
1234567           7548
7954321           8475


Can anyone help with the data extraction that changes to each row every loop?",https://forum.imacros.net/viewtopic.php?f=7&t=28485&sid=01bf0edf807fd3d4bb507a35b87155ad,program
1988,iMacros Script to Run Excel Macro?,"Problem Description Information:
1. iMacros Version: iMacros for Firefox 8.9.7 (Build=8970419)
2. Windows 10 (64-bit) [English]
3. Firefox 45.9.0
4. Included demos work ok.
5. Included VBS sample scripts run ok.
6. Not applicable/no specific recording or replay fails on a specific website.
7. Not applicable/no test for encounter of the same problem with the: iMacros Browser, iMacros for Internet Explorer, iMacros for Firefox, and iMacros for Chrome.

Problem:
I am currently working on an iMacros script, and I am asking if there is a code that will allow iMacros to run an Excel macro?

For example something like:
Code: Select allVERSION BUILD=8970419 RECORDER=FX
WAIT SECONDS=1
TAB T=1
URL GOTO=http://*URL*/

*iMACROS CODE*

WAIT SECONDS=1
URL GOTO=imacros://run/?m=#Macro2

But instead of ending with running ""Macro2"" it runs an Excel macro instead; how would I do this in iMacros?

I tried the below code, and this did not work.
Code: Select allURL GOTO=C:\Program Files\Microsoft Office\Office13\EXCEL.exe

EDIT: Progress Update
Courtesy of the wiki: http://wiki.imacros.net/URL; I was able to find the ""URL GOTO="" command for running local files in Windows.
Code: Select allURL GOTO=file:///C:\Directory\TEST.CSV

As a result, I was able initiate the running of an Excel (.CSV) file where the ""URL GOTO="" triggers the browser prompt window below. How would I code iMacros to open the local file, and not download it?
",https://forum.imacros.net/viewtopic.php?f=7&t=28441&sid=01bf0edf807fd3d4bb507a35b87155ad,program
1989,"HTML TYPE, Copy data to notepad","Hello everyone!

I am newbie here. 
I recorded a macro but it doesn't run, seems like the programming involve javascript functions.. I don't know if it's a problem.

Ok. The error I found is this:
Error -1300: Cannot find HTML element of type 
TAG POS=1 TYPE=INPUT:TEXT ATTR=CLASS:v-textfield<SP>v-widget<SP>v-textfield-focus CONTENT=378.191.359-72

The code ( I recorded it):
TAG POS=2 TYPE=IMG ATTR=CLASS:v-image<SP>v-widget<SP>v-has-width<SP>v-has-height
WAIT SECONDS=3
TAG POS=1 TYPE=INPUT:TEXT ATTR=CLASS:v-textfield<SP>v-widget<SP>v-textfield-focus CONTENT=378.191.359-72
WAIT SECONDS=3
TAG POS=3 TYPE=DIV ATTR=TXT:飥?WAIT SECONDS=3
TAG POS=1 TYPE=DIV ATTR=TXT:ANA<SP>DOS<SP>SANTOS<SP>MARIA

To avoid this problem I used DS CMD but I don't know how to copy content of an type. Better explanation below:

TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:gwt-uid-141 CONTENT=0,05
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:gwt-uid-151 CONTENT=0,00

How do I copy that content to a file?

Thanks in advance!",https://forum.imacros.net/viewtopic.php?f=7&t=28310&sid=01bf0edf807fd3d4bb507a35b87155ad,program
1990,Opening the Extract Wizard in the iMacros Browser,"I would like to try the Extract Wizard which is described in some articles and tutorials but from what I've read it does not exist in the Firefox version of IMacros (which is what I've been using).  I had no luck installing the Chrome version of IMacros, so that leaves the IMacros Browser.  I'd like to open the IMacros Browser anyway because a lot of tutorials are written from that point of view. 

From what I've read, the independent IMacros browser should launch if I click on iMacros.exe in the Program Files folder. ( It's version 12.0.501.2305. )

However that click results only in an informational sales page which bears no resemblance to the browser depicted in various articles.  In addition, that page ""freezes"" on the screen so that I can get rid of it only with the task manager and sometimes only by rebooting!

I may have been reading outdated instructions on launching the browser.  How do I do it in the current version 12.0.501.2305?

Thanks!",https://forum.imacros.net/viewtopic.php?f=7&t=28041&sid=01bf0edf807fd3d4bb507a35b87155ad,program
1991,LinkedIn tagging question,"Windows 10
iMacros for Firefox. VERSION BUILD=9030808
Firefox 54.0.1 (32-bit)

Hi all,

I have a LinkedIn profile scraper that extracts profile data into a CSV (code at bottom of post). It does the job very well, with one key caveat: it can't account for instances where a piece of information is missing from a section of the profile. As an example, see this profile: https://www.linkedin.com/in/darylpereira/

Look at the second position in Daryl's profile and note that he did not include a date range. My program is trained to find position 2 and print it, then find date range 2 and print it. But it can't tell when there is no date range 2. Instead, the date range associated with the 3rd position will be considered the second date range. The result is that the dates are off for him (I have him joining IBM in this capacity in Nov 2005, he really joined in Sep 2008). Many, many profiles have this issue. I need the date range data to be accurately attributed to the positional and educational data. Otherwise, it's not useful for the analysis.

I believe the answer is that I need to be using a python or other coding interface so that I can integrate if/else functionality. But I'm really hoping someone might have a workaround for iMacros, because I'm not any good at coding. Anyway, all ideas are appreciated. Here's the code I mentioned (feel free to use it - the reference is just the list of profile links):
Code: Select allVERSION BUILD=9030808 RECORDER=FX
TAB T=1
TAB CLOSEALLOTHERS
SET !ERRORIGNORE YES
SET !DATASOURCE LinkedInURLs.csv
SET !DATASOURCE_COLUMNS 1
SET !LOOP 1
SET !DATASOURCE_LINE {{!loop}}

URL GOTO={{!COL1}}

WAIT SECONDS=10

ADD !EXTRACT {{!COL1}}

TAG POS=1 TYPE=h1 ATTR=CLASS:* EXTRACT=TXT

TAG POS=1 TYPE=h3 ATTR=class:""Sans-**px-black-**%-semibold"" EXTRACT=TXT
TAG POS=1 TYPE=span ATTR=class:pv-entity__secondary-title* EXTRACT=TXT
TAG POS=1 TYPE=h4 ATTR=class:pv-entity__date-range* EXTRACT=TXT
TAG POS=1 TYPE=h4 ATTR=class:pv-entity__location* EXTRACT=TXT

TAG POS=2 TYPE=h3 ATTR=class:""Sans-**px-black-**%-semibold"" EXTRACT=TXT
TAG POS=2 TYPE=span ATTR=class:pv-entity__secondary-title* EXTRACT=TXT
TAG POS=2 TYPE=h4 ATTR=class:pv-entity__date-range* EXTRACT=TXT
TAG POS=2 TYPE=h4 ATTR=class:pv-entity__location* EXTRACT=TXT

TAG POS=3 TYPE=h3 ATTR=class:""Sans-**px-black-**%-semibold"" EXTRACT=TXT
TAG POS=3 TYPE=span ATTR=class:pv-entity__secondary-title* EXTRACT=TXT
TAG POS=3 TYPE=h4 ATTR=class:pv-entity__date-range* EXTRACT=TXT
TAG POS=3 TYPE=h4 ATTR=class:pv-entity__location* EXTRACT=TXT

TAG POS=4 TYPE=h3 ATTR=class:""Sans-**px-black-**%-semibold"" EXTRACT=TXT
TAG POS=4 TYPE=span ATTR=class:pv-entity__secondary-title* EXTRACT=TXT
TAG POS=4 TYPE=h4 ATTR=class:pv-entity__date-range* EXTRACT=TXT
TAG POS=4 TYPE=h4 ATTR=class:pv-entity__location* EXTRACT=TXT

TAG POS=1 TYPE=h3 ATTR=class:pv-entity__school-name* EXTRACT=TXT
TAG POS=1 TYPE=p ATTR=class:""pv-entity__secondary-title pv-entity__degree-name*"" EXTRACT=TXT
TAG POS=1 TYPE=p ATTR=class:""pv-entity__secondary-title pv-entity__fos*"" EXTRACT=TXT
TAG POS=1 TYPE=p ATTR=class:pv-entity__dates* EXTRACT=TXT

SAVEAS TYPE=EXTRACT FOLDER=* FILE=LinkedInResults.csv

WAIT SECONDS=5
",https://forum.imacros.net/viewtopic.php?f=7&t=27767&sid=8ea127909c0d83c2fac4f53ec6c3d19a,program
1992,Extract data from an empty cell,"Hi,

I am not a professional programmer, but I am doing a small code to extract customer information from a corporate intranet.  

For every client there are several orders, but only one will be avaible (DISPONIBLE).  When the code finds an avaible order, it goes to delivery note url to extract the lines of the items: date, description and dispatched, and normally there are 4 rows.  If the date is empty, dispatched too.

I need that when one row has no date, it skips to the next.  With my code, the second open TAB (delivery note) doesn't close when any date is empty and then the extraction stops.

How do I make it jump to the next row to extract data when the date is empty?

Thanks.
Code: Select all// 1.- Carga de website
var visord1 = ""CODE:"";
visord1 += ""TAB T=1\n"";
visord1 += ""SET !ERRORIGNORE YES\n"";
visord1 += ""SET !TIMEOUT_STEP 2\n"";
visord1 += ""URL GOTO=https://intranet/ma1/jsp/VisorD/visord.jsp\n"";

// 2.- Men煤 ""buscar""
var visord2 = ""CODE:"";
visord2 += ""FRAME NAME=\""menu\""\n"";
visord2 += ""TAG POS=1 TYPE=A ATTR=TXT:Buscar\n"";

// 3.- Men煤 ""formulario""
var visord3 = ""CODE:"";
visord3 += ""FRAME NAME=\""Formulario\""\n"";
visord3 += ""SET !DATASOURCE VISORD_telf_nif.csv\n"";
visord3 += ""SET !DATASOURCE_COLUMNS 3\n"";
visord3 += ""SET !DATASOURCE_LINE {{csvline}}\n"";
visord3 += ""ADD !EXTRACT {{!COL1}}\n"";								//Extrae oficina 	Extract 1
visord3 += ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:principal ATTR=ID:nif CONTENT={{!COL2}}\n"";
visord3 += ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:principal ATTR=ID:telefono CONTENT={{!COL3}}\n"";
visord3 += ""TAG POS=1 TYPE=INPUT:BUTTON FORM=NAME:principal ATTR=NAME:enviarForm\n"";

// 4.- Men煤 ""b煤squeda""
var visord4 = ""CODE:""
visord4 += ""FRAME NAME=\""Busqueda\""\n"";
visord4 += ""SET !TIMEOUT_STEP 2\n"";							
visord4 += ""TAG POS=1 TYPE=TD ATTR=TXT:{{contador}} EXTRACT=TXT\n""; //Extrae contador   Extract 2
visord3 += ""ADD !EXTRACT {{contador}}\n"";	
visord4 += ""TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n""; 			//Extrae a帽o    	Extract 3
visord4 += ""TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";			//Extrae central    Extract 4
visord4 += ""TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";			//Extrae orden    	Extract 5
visord4 += ""TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";			//Extrae fecha    	Extract 6
visord4 += ""TAG POS=R2 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";			//Extrae tel茅fono  	Extract 7
visord4 += ""TAG POS=R3 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";			//Extrae estado    	Extract 8
visord4 += ""TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";			//Extrae nif    	Extract 9
visord4 += ""TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";			//Extrae cliente   	Extract 10

// 5.- Peticion Materiales
var visord5 = ""CODE:"";
visord5 += ""TAB OPEN NEW\n"";
visord5 += ""TAB T=2\n"";
visord5 += ""URL GOTO=https://intranet/ma1/servlet/VisorDPeticionMaterialesNewLook?anno={{ano}}&central={{central}}&orden={{orden}}\n"";
	visord5 += ""TAG POS=2 TYPE=TD ATTR=TXT:201* EXTRACT=TXT\n""; 
	visord5 += ""TAG POS=R1 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT\n""; 
	visord5 += ""TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";
		visord5 += ""TAG POS=4 TYPE=TD ATTR=TXT:201* EXTRACT=TXT\n"";
		visord5 += ""TAG POS=R1 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT\n"";
		visord5 += ""TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";
			visord5 += ""TAG POS=6 TYPE=TD ATTR=TXT:201* EXTRACT=TXT\n"";
			visord5 += ""TAG POS=R1 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT\n"";
			visord5 += ""TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";
				visord5 += ""TAG POS=8 TYPE=TD ATTR=TXT:201* EXTRACT=TXT\n"";
				visord5 += ""TAG POS=R1 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT\n"";
				visord5 += ""TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";
visord5 += ""TAB T=1\n"";
visord5 += ""TAB CLOSEALLOTHERS\n"";

// 6.- Escritura en fichero.CSV
var visordPRINT  = ""CODE:"";
visordPRINT += ""ADD !EXTRACT {{oficina}}\n"";

visordPRINT += ""ADD !EXTRACT {{num_order}}\n"";
	visordPRINT += ""ADD !EXTRACT {{ano}}\n"";
	visordPRINT += ""ADD !EXTRACT {{central}}\n"";
	visordPRINT += ""ADD !EXTRACT {{orden}}\n"";
	visordPRINT += ""ADD !EXTRACT {{fecha}}\n"";
	visordPRINT += ""ADD !EXTRACT {{telefono}}\n"";
	visordPRINT += ""ADD !EXTRACT {{estado}}\n"";
	visordPRINT += ""ADD !EXTRACT {{nif}}\n"";
	visordPRINT += ""ADD !EXTRACT {{cliente}}\n"";
		visordPRINT += ""ADD !EXTRACT {{date1}}\n"";
		visordPRINT += ""ADD !EXTRACT {{descripcion1}}\n"";
		visordPRINT += ""ADD !EXTRACT {{estado1}}\n"";
		visordPRINT += ""ADD !EXTRACT {{date2}}\n"";
		visordPRINT += ""ADD !EXTRACT {{descripcion2}}\n"";
		visordPRINT += ""ADD !EXTRACT {{estado2}}\n"";
		visordPRINT += ""ADD !EXTRACT {{date3}}\n"";
		visordPRINT += ""ADD !EXTRACT {{descripcion3}}\n"";
		visordPRINT += ""ADD !EXTRACT {{estado3}}\n"";
		visordPRINT += ""ADD !EXTRACT {{date4}}\n"";
		visordPRINT += ""ADD !EXTRACT {{descripcion4}}\n"";
		visordPRINT += ""ADD !EXTRACT {{estado4}}\n"";
visordPRINT += ""SAVEAS TYPE=EXTRACT FOLDER=* FILE=VISORD_o.csv\n"";


//#####################################################################
//#####################################################################

// 1.- Carga de website
iimPlay(visord1); 

// 2.- Men煤 ""buscar""		
iimPlay(visord2); 		


var save_csv = new Array();

// 3.- Men煤 ""formulario""
for (var csvline=1;csvline<30;csvline++)
{	
	iimSet(""csvline"",csvline);
	iimPlay(visord3);
	var oficina= iimGetLastExtract(1);					//alert(""oficina: ""+oficina);
	save_csv[0] = oficina;

	
// 4.- Men煤 ""b煤squeda""
	for (var contador=1;contador<4;contador++)
	{
		iimSet(""contador"",contador);
		iimPlay(visord4);
		var num_order = iimGetLastExtract(1);			//alert(""N潞: ""+num_order);
		save_csv[1] = num_order;
		if (num_order == ""#EANF#""){ break;}
		var ano = iimGetLastExtract(2);					//alert(""ano: ""+ano);
		save_csv[2] = ano;
		var central = iimGetLastExtract(3);				//alert(""central: ""+central);
		save_csv[3] = central;
		var orden = iimGetLastExtract(4);				//alert(""orden: ""+orden);
		save_csv[4] = orden;
		var fecha = iimGetLastExtract(5);				//alert(""fecha: ""+fecha);
		save_csv[5] = fecha;
		var telefono = iimGetLastExtract(6);			//alert(""telefono: ""+telefono);
		save_csv[6] = telefono;
		var estado = iimGetLastExtract(7);				//alert(""estado: ""+estado);
		save_csv[7] = estado;
		var nif = iimGetLastExtract(8);					//alert(""nif: ""+nif);
		save_csv[8] = nif;		
		var cliente= iimGetLastExtract(9);				//alert(""cliente: ""+cliente);	
		save_csv[9] = cliente;
		
// 5.- Peticion Materiales		
		if (estado == ""DISPONIBLE"")
		{
			iimSet(""ano"",ano);
			iimSet(""central"",central);
			iimSet(""orden"",orden);
			iimPlay(visord5);
			
			var date1= iimGetLastExtract(1);			//alert(""F.Entrega: ""+date1);
			if (date1 == ""#EANF#"" || date1 == ""__undefined__""){ break;}
						
			save_csv[10] = date1; 
			var descripcion1= iimGetLastExtract(2);		//alert(""Descripcion: ""+descripcion1);
			save_csv[11] = descripcion1;
			var estado1= iimGetLastExtract(3);			//alert(""Estado: ""+estado1);
			save_csv[12] = estado1;
			
			var date2= iimGetLastExtract(4);			//alert(""F.Entrega: ""+date2);
			save_csv[13] = date2;
			var descripcion2= iimGetLastExtract(5);		//alert(""Descripcion: ""+descripcion2);
			save_csv[14] = descripcion2;
			var estado2= iimGetLastExtract(6);			//alert(""Estado: ""+estado2);
			save_csv[15] = estado2;
			
			var date3= iimGetLastExtract(7);			//alert(""F.Entrega: ""+date3);
			save_csv[16] = date3;
			var descripcion3= iimGetLastExtract(8);		//alert(""Descripcion: ""+descripcion3);
			save_csv[17] = descripcion3;
			var estado3= iimGetLastExtract(9);			//alert(""Estado: ""+estado3);
			save_csv[18] = estado3;			
			
			var date3= iimGetLastExtract(10);			//alert(""F.Entrega: ""+date4);
			save_csv[19] = date3;
			var descripcion3= iimGetLastExtract(11);	//alert(""Descripcion: ""+descripcion4);
			save_csv[20] = descripcion3;
			var estado3= iimGetLastExtract(12);			//alert(""Estado: ""+estado4);
			save_csv[21] = estado3;			


			iimSet(""oficina"",save_csv[0]);
				iimSet(""num_order"",save_csv[1]);
				iimSet(""ano"",save_csv[2]);
				iimSet(""central"",save_csv[3]);
				iimSet(""orden"",save_csv[4]);
				iimSet(""fecha"",save_csv[5]);
				iimSet(""telefono"",save_csv[6]);
				iimSet(""estado"",save_csv[7]);
				iimSet(""nif"",save_csv[8]);
				iimSet(""cliente"",save_csv[9]);
					iimSet(""date1"",save_csv[10]);
					iimSet(""descripcion1"",save_csv[11]);
					iimSet(""estado1"",save_csv[12]);	
					iimSet(""date2"",save_csv[13]);
					iimSet(""descripcion2"",save_csv[14]);
					iimSet(""estado2"",save_csv[15]);	
					iimSet(""date3"",save_csv[16]);
					iimSet(""descripcion3"",save_csv[17]);
					iimSet(""estado3"",save_csv[18]);
					iimSet(""date4"",save_csv[19]);
					iimSet(""descripcion4"",save_csv[20]);
					iimSet(""estado4"",save_csv[21]);
					
			iimPlay(visordPRINT);
			save_csv = [];
		}
	}
}
",https://forum.imacros.net/viewtopic.php?f=7&t=27668&sid=8ea127909c0d83c2fac4f53ec6c3d19a,program
1993,inputting keywords from CSV file,"I am using a CSV file as input with keywords to my imacros program. I am able to read only the column and generate the search output that I need. I want the program to read even the rows. The input has to be read like a nested for loop ware every row and column have to be inputted in the form (1,2),(1,3),(1,4)..........(1,n) (2,3),(2,4)................(2,n) and so on. Is it possible to read the input file with imacros in this way or is there any other alternative for this.

Here is my program that scrapes information using keywords and saves parsed information in another file. However, I want to use both rows and columns and input more than one keywords at once.
Code: Select all
'VERSION BUILD=9030808 RECORDER=FX
 TAB T=1
 set !extract_test_popup no
 set !replayspeed fast
 set !timeout_page 200
 SET !ERRORIGNORE YES
 SET !TIMEOUT_STEP 2
 SET !Datasource keyword.CSV
 Set !Loop 1
 Set !Datasource_Line {{!Loop}}

 URL GOTO=https://twitter.com/search-advanced

 wait seconds=1
 TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:NoFormName ATTR=NAME:ands CONTENT=
 {{!COl1}}
 TAG POS=1 TYPE=BUTTON FORM=NAME:NoFormName ATTR=TXT:Search
 wait seconds=1

 ADD !EXTRACT {{!URLCURRENT}}
 Set url {{!Extract}}
 Set !Extract Null



'Keyword Scrape
Tag pos=1 type=h1 attr=class:SearchNavigation-titleText* Extract=Txt
Set key {{!Extract}}
Set !Extract Null


'Main Heading Scrapping
Tag pos=1 type=a attr=class:AdaptiveNewsLargeImageHeadline-title* 
Extract=Txt
Set mainheading {{!Extract}}
Set !Extract Null

'Main heading URL
Tag pos=1 type=a attr=class:AdaptiveNewsLargeImageHeadline-title* 
Extract=href
Set mainheadingurl {{!Extract}}
Set !Extract Null


'Date of Post
 Tag pos=1 type=a attr=class:AdaptiveNewsHeadlineDetails-date<sp>js-nav* 
 Extract=txt
 Set date {{!Extract}}
 Set !Extract Null


 'Username whose post this article
  TAG XPATH=//*[@id=""page-
container""]/div[2]/div/div/div[2]/div/div[2]/div/div[2]/div[2]/div[1]/a/span 
 Extract=Txt
 Set username {{!Extract}}
 Set !Extract Null

'extract user name
TAG POS=1 TYPE=A ATTR=TXT:@* EXTRACT=TXT
Set username1 {{!Extract}}
Set !Extract Null


Add !Extract {{mainheading}}
Add !Extract {{date}}
Add !Extract {{mainheadingurl}}
ADD !EXTRACT {{url}}
Add !Extract {{username}}
Add !Extract {{key}}
Add !Extract {{username1}}


SAVEAS TYPE=EXTRACT FOLDER=* FILE=test1_output.csv
clear'",https://forum.imacros.net/viewtopic.php?f=7&t=27539&sid=8ea127909c0d83c2fac4f53ec6c3d19a,program
1994,Issue Extract with firefox plugin,"Browser: Firefox 53.0.2 (32 bit)
S.O. Windows 8.1 Pro
plug in: iMacros for Firefox 9.0.3 (updated April, 24. 2017)

-----------------------

Hello, 

First of all , sorry for my bad english. 

I'm having an issue extracting TXT when in the middle of the text to extract there are Bold or Italic characters. It happens with Firefox plugin.

iMacros program works properly

I've changed the demo page (http://demo.imacros.net/Automate/Extract2) HTML to test if the problem was caused for the web page where I want to extract The text.

I've added bold characters in the middle of the phrase like this:
""The second line is extracted too"" for ""The second line is extracted too"".

In HTML: 

<td style=""width: 52%; outline: 1px solid blue;"" class=""bdytxt"">
 This line is extracted.<br>
 The <strong>second line</strong>
 is extracted, too.
</td>

Running this macro:

VERSION BUILD=8031994
TAB T=1
SET !EXTRACT_TEST_POPUP NO
URL GOTO=http://demo.imacros.net/Automate/Extract2
TAG POS=1 TYPE=TD ATTR=CLASS:bdytxt&&TXT:* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=+_{{!NOW:yyyymmdd_hhnnss}}

the result is: 

This line is extracted.
TheThis line is extracted.
Thesecond lineis extracted, too.

instead of:

This line is extracted.
The second line is extracted, too.


It seems as the extracted text repeats himself when finds the bold tag (<strong>)

Thank You in advance",https://forum.imacros.net/viewtopic.php?f=7&t=27484&sid=8ea127909c0d83c2fac4f53ec6c3d19a,program
1995,Extract blank when tag fails,"Hello,

I'm using iMacro's build 9030808 for Firefox 53.0.2 (32-bit) on Windows 7. I've been using iMacros to scrape social media profiles for certain pieces of information. Because information on the platform is self-reported, certain pieces of info are often missing, resulting in the tag failing. Ultimately, this means a lot of work for me on the back end sorting the data so that each piece of information is in the correct column. Instead, I'd like to extract a blank each time the program fails to find a piece of information.

I've looked all over the forums, but have yet to find a solution for this yet. This one appears to come close, but was never resolved. http://forum.imacros.net/viewtopic.php?f=7&t=25503

I imagine the solution would involve the EVAL function, but I don't have any experience in javascript. Is there an obvious way to do this? I've pasted an example script below for context:
Code: Select allVERSION BUILD=9030808 RECORDER=FX
TAB T=1
TAB CLOSEALLOTHERS
SET !ERRORIGNORE YES
SET !DATASOURCE Leadership.csv
SET !DATASOURCE_COLUMNS 1
SET !LOOP 1
SET !DATASOURCE_LINE {{!loop}}
URL GOTO={{!COL1}}

WAIT SECONDS=5

TAG POS=1 TYPE=h1 ATTR=CLASS:* EXTRACT=TXT
TAG POS=1 TYPE=h3 ATTR=class:""Sans-17px-black-85%-semibold"" EXTRACT=TXT
TAG POS=1 TYPE=span ATTR=class:""pv-position-entity__secondary-title pv-entity__secondary-title Sans-15px-black-55%"" EXTRACT=TXT
TAG POS=1 TYPE=h4 ATTR=class:""pv-entity__date-range Sans-**px-black-**%"" EXTRACT=TXT
TAG POS=1 TYPE=h4 ATTR=class:""pv-entity__location Sans-**px-black-**% block"" EXTRACT=TXT

TAG POS=2 TYPE=h3 ATTR=class:""Sans-17px-black-85%-semibold"" EXTRACT=TXT
TAG POS=2 TYPE=span ATTR=class:""pv-position-entity__secondary-title pv-entity__secondary-title Sans-15px-black-55%"" EXTRACT=TXT
TAG POS=2 TYPE=h4 ATTR=class:""pv-entity__date-range Sans-**px-black-**%"" EXTRACT=TXT
TAG POS=2 TYPE=h4 ATTR=class:""pv-entity__location Sans-**px-black-**% block"" EXTRACT=TXT

TAG POS=1 TYPE=h3 ATTR=class:""pv-entity__school-name Sans-17px-black-85%-semibold"" EXTRACT=TXT
TAG POS=1 TYPE=p ATTR=class:""pv-education-entity__secondary-title pv-entity__degree-name pv-entity__secondary-title Sans-**px-black-**%"" EXTRACT=TXT
TAG POS=1 TYPE=p ATTR=class:""pv-education-entity__secondary-title pv-entity__fos pv-entity__secondary-title Sans-**px-black-**%"" EXTRACT=TXT
TAG POS=1 TYPE=p ATTR=class:""pv-education-entity__date pv-entity__dates Sans-**px-black-**%"" EXTRACT=TXT

SAVEAS TYPE=EXTRACT FOLDER=* FILE=Leadership.csv

WAIT SECONDS=5


Thanks!",https://forum.imacros.net/viewtopic.php?f=7&t=27485&sid=8ea127909c0d83c2fac4f53ec6c3d19a,program
1996,Detect New Tab Open in Mozilla & Close it,"Team,

I browse to a page  which has a bunch of links , Each link does a Postback to a Remote server & get's a URL in response & then that received URl is opened as a Tab in Mozilla Browser . The  time it takes   for the Postback to return varies from widely & hence i need assistance to automate this in Imacros . Basically 

1. Tab1 has all links , which can be clicked by a Tag command 
2. Once the Postback is Received , the URL is opened in Tab 2 

ASK: We need to wait  till the Tab 2 is active ( this can be anywhere between 20 seconds to 2 minutes post link click on Tab1 )
wait 2 seconds & close the Tab 2 .

Please help on any pointers on how to achieve this via Imacros . I am not a programming guy , so please be a little detailed in response 

Any assistance is appreciated",https://forum.imacros.net/viewtopic.php?f=7&t=27215&sid=41f2b858fcf7ca695bb39ee1420df70b,program
1997,extract some info from facebook page,"Hi, a pictures says  more then a thousand words.  Right click and save if you can't see the whole image.


Build 9030808
Mac OSX Sierra
Firefox 50.1

Never programmed in imacros before but now I'm in the situation that I need to solve this task.  Hope someone can help me out. It's probably not that complicated but to me it would take a lifetime to figure this out, have been trying the whole day on my own.",https://forum.imacros.net/viewtopic.php?f=7&t=27166&sid=41f2b858fcf7ca695bb39ee1420df70b,program
1998,Having issue with extracting information by following links,"Imacros version: 10
Win 10
The sample scripts work and I am using visual studio to code the scripts.

This is what I am trying to do and it is for personal use so that I can choose the best dentist for my needs. I want to go to google and extract their healthgrade, vitals.com ratings.  For the healthgrade ratings, sometimes the rating is right on the google search results page and I sometimes have to click on the results link to scrape the ratings from the actual healthgrades site. For vitals.com ratings I will always have to click on the link to get the ratings from vitals.com.  The issue I'm having is this: I can can follow the correct links to vitals.com and healthgrades.com and extract the info but for some reason when I issue the BACK command to navigate back to the previous google results page to continue, the extract variable it seems is completely cleared and I only get the last extraction info into the csv file.  Is imacros capable of extracting info by following a multi level link structure?  (e.g. is it possible to retain the extracted data when you navigate into and out of links in the same macro?

Thanks.

Macro to extract the dentist's name and other important data from the search results page of dentaquest.com:
Code: Select allVERSION BUILD=10022823
TAB OPEN 
TAB T=1

TAG POS={{I}} TYPE=TD ATTR=* EXTRACT=TXT
SET name EVAL(""var extract = \""{{!EXTRACT}}\""; if (extract == \""#EANF#\"") MacroError(\""No more listings on this page\""); else extract.replace(\""#EANF#\"", \""\"").replace(/^\\s*|,\\s*$/g, \""\"");"")
SET !EXTRACT NULL

SET mypos {{I}}
ADD mypos 2

TAG POS={{mypos}} TYPE=TD ATTR=* EXTRACT=TXT
SET denttype EVAL(""var extract = \""{{!EXTRACT}}\""; if (extract == \""#EANF#\"") MacroError(\""No more listings on this page\""); else extract.replace(\""#EANF#\"", \""\"").replace(/^\\s*|,\\s*$/g, \""\"");"")
SET !EXTRACT NULL

ADD mypos 2
TAG POS={{mypos}} TYPE=TD ATTR=* EXTRACT=TXT
SET address EVAL(""var extract = \""{{!EXTRACT}}\""; if (extract == \""#EANF#\"") MacroError(\""No more listings on this page\""); else extract.replace(\""#EANF#\"", \""\"").replace(/^\\s*|,\\s*$/g, \""\"");"")
SET !EXTRACT NULL

ADD mypos 1
TAG POS={{mypos}} TYPE=TD ATTR=* EXTRACT=TXT
SET phone EVAL(""var extract = \""{{!EXTRACT}}\""; if (extract == \""#EANF#\"") MacroError(\""No more listings on this page\""); else extract.replace(\""#EANF#\"", \""\"").replace(/^\\s*|,\\s*$/g, \""\"");"")
SET !EXTRACT NULL

ADD !EXTRACT {{name}}
ADD !EXTRACT {{denttype}}
ADD !EXTRACT {{address}}
ADD !EXTRACT {{phone}} 

Macro to extract info from google search results and by following relevant search results links:
Code: Select allVERSION BUILD=10022823
TAB T=2
SET !TIMEOUT_PAGE 3

URL GOTO = https://www.google.com/#q={{name}}

WAIT SECONDS=1

'set initial anchor
TAG POS=1 TYPE=A ATTR=HREF:*healthgrades* EXTRACT=TXT
SET !EXTRACT NULL

'set search boundry
TAG POS=R1 TYPE=SPAN ATTR=CLASS:st EXTRACT=TXT
SET !ENDOFPAGE {{!TAGSOURCEINDEX}}
SET !EXTRACT NULL

'reset anchor
TAG POS=1 TYPE=A ATTR=HREF:*healthgrades* EXTRACT=TXT
SET !EXTRACT NULL

'extract ratings for healthgrades
TAG POS=R1 TYPE=DIV ATTR=CLASS:f<SP>slp EXTRACT=TXT
SET ratings EVAL(""var extract = \""{{!EXTRACT}}\""; extract.replace(\""#EANF#\"", \""\"").replace(/^\\s*|,\\s*$/g, \""\"");"")
SET !EXTRACT NULL

'follow zocdoc link and extra
TAG POS=1 TYPE=A ATTR=TXT:*Vitals*

'extract number of rating stars
TAG POS=1 TYPE=SPAN ATTR=CLASS:score<SP>overview-number EXTRACT=TXT
SET vitalstars EVAL(""var extract = \""{{!EXTRACT}}\""; extract.replace(\""#EANF#\"", \""\"").replace(/^\\s*|,\\s*$/g, \""\"");"")
SET !EXTRACT NULL

'extract number of votes
TAG POS=1 TYPE=A ATTR=HREF:*reviews* EXTRACT=TXT
SET vitalnum EVAL(""var extract = \""{{!EXTRACT}}\""; extract.replace(\""#EANF#\"", \""\"").replace(/^\\s*|,\\s*$/g, \""\"");"")
SET !EXTRACT NULL

WAIT SECONDS=1

BACK

ADD !EXTRACT {{name}}
ADD !EXTRACT {{denttype}}
ADD !EXTRACT {{address}}
ADD !EXTRACT {{phone}}
ADD !EXTRACT {{vitralstars}}
ADD !EXTRACT {{vitralnum}}

SAVEAS TYPE=EXTRACT FOLDER=C:\\Users\Helix\Documents\iMacros\Macros FILE=test.csv

TAB T=1
C# code:
Code: Select allusing System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace dentamacro
{
    class Program
    {
        static void Main(string[] args)
        {
            var i = 5;

            int timeout = 1;
            // Add a reference to iMacros Scripting Interface COM in your project to be able to access control iMacros
            // If you target .NET 4.0, set ""Embed Interop Types"" to false, in the reference to the iMacros Interface COM
            iMacros.Status status;
            iMacros.App app = new iMacros.App();

            app.iimOpen(""-V7"", false, timeout);

            while (i < 87)
            {
                app.iimSet(""I"", i.ToString());
                app.iimPlay(""C:\\Users\\Helix\\Documents\\iMacros\\Macros\\extractname.iim"");
                
                var name =  app.iimGetLastExtract(1);
                var denttype = app.iimGetLastExtract(2);
                var address = app.iimGetLastExtract(3);
                var phone = app.iimGetLastExtract(4);

                if (name == """" || name == ""#EANF#"")
                {
                    break;
                }

                app.iimSet(""name"", name);
                app.iimSet(""denttype"", denttype);
                app.iimSet(""address"", address);
                app.iimSet(""phone"", phone);

                Console.WriteLine(denttype);

                app.iimPlay(""C:\\Users\\Helix\\Documents\\iMacros\\Macros\\searchname.iim"");

                i = i + 9;

            }


        }
    }
}
",https://forum.imacros.net/viewtopic.php?f=7&t=26893&sid=41f2b858fcf7ca695bb39ee1420df70b,program
1999,Question,"Hi all,

I hope i place this topic in the right section.
I have a problem.

I want to extract some txt to a txt file 
But the text i want to extract is 10.000 or 20.000 or 30.000

But i i dont want the . in it. 
So only 10000
Can someone tell me how to remove the . in it?

I谩m using imacro version 8970419 for firefox 
and version 11.1.495.5175 as program.

Sorry for the crapy englisch but its not my main language

URL GOTO=*****
TAG POS=2 TYPE=TD ATTR=CLASS:maintxt EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:NoFormName ATTR=NAME:CONTENT={{!EXTRACT}}
WAIT SECONDS=5
CLICK X=523 Y=324
SAVEAS TYPE=EXTRACT FOLDER=C:\*** FILE=***

this is what i have.
the extact=txt is the 10.000
and in the content it has to place i without the .",https://forum.imacros.net/viewtopic.php?f=7&t=26376&sid=91bf0dc741658a74aa70a7f66f53d097,program
2000,Scrape images and save them into specific location facebook,"Hi, I'm new to IMacros and i have studied only the surface of the program watching youtube videos ^^

What i am trying to do is simple?
Save facebook pictures and user timeline

I know how to make it automatically go to the pictures pages of a certain user but my problem is that i don't know how to make imacros click on each picture and save them as an image file. And when the images are over, it should stop?
Facebook has this next picture button when i click on the picture and what i am trying to do is to make imacros click on that next image button till the image runs out and save them into a folder i've designated. 

Also, when i try to take a screenshot of the user feed. THere are two problems.
1. I want imacros to scroll to the end of the page in facebook but since facebook doesnt load all the page in once and loads each page only when i reach the end of page, i'm not sure how i can capture them all
2. I want to also include all comments in the imacros page. So i will be taking automatic screenshots after imacros clicks for example, ""show 3 more comments"" i'm not sure how to make imacros click all of the ""show more comments option"" before it saves a screenshots.

LOL sorry if i asked too many questions.
I'll appreciate if anyone can help!
thanks in advance!",https://forum.imacros.net/viewtopic.php?f=7&t=26601&sid=91bf0dc741658a74aa70a7f66f53d097,program
2001,detect dynamic content,"hi guys first of all i am terribly sorry i posted the wrong title i am not really sure what to write as i am really new to programming and this is my 1st time to try imacro before buying. I only want to accomplish a simple task that is to detect a certain content from a page. Sorry i dont know what terms to use maybe ill just explain it a bit more.

Pls bear with me here is an image of what the website script does and what i want to accomplish



this is how the code on the website looks like
Code: Select all<form action=""#"" method=""get"" id=""form1"">
 Guess the 1st letter of my name: <input type=""text"" name=""gletter""><br>
</form>

<button type=""submit"" form=""form1"" value=""Submit"">Submit</button>

<div class=""data-outcome item"">
   <!-- ajax result -->
</div>


everytime you press submit a result will show inside <!-- ajax result --> and the code will be
Code: Select all <div class=""inner-item"">
   <div class=""icon-correct"">W<span></span></div>
</div>


if i guess it correctly and
Code: Select all<div class=""inner-item"">
   <div class=""icon-lose"">L<span></span></div>
</div>


if i guess it wrong

so the code looks like this after every guess
Code: Select all<div class=""data-outcome item"">
        <div class=""inner-item"">
            <div class=""icn-win"">W<span></span></div>
        </div>
</div>

i already have an imacro code that letting me guess automatically 
Code: Select allVERSION BUILD=11.1.495.5175
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2

SET !VAR1 EVAL(""var rDW35=Math.floor(Math.random()*3 + 1); rDW35;"")
SET !VAR2 EVAL(""var rDW310=Math.floor(Math.random()*5 + 1); rDW310;"")
SET !VAR3 EVAL(""var rDW57=Math.floor(Math.random()*5 + 3); rDW57;"")
SET !VAR4 EVAL(""var rDW515=Math.floor(Math.random()*6 + 5); rDW515;"")
SET !VAR5 EVAL(""var rDW515=Math.floor(Math.random()*120 + 1); rDW515;"")

WAIT SECONDS={{!VAR5}}

'loop 1
TAG POS=2 TYPE=INPUT:TEXT ATTR=* CONTENT=A
WAIT SECONDS=1
TAG POS=1 TYPE=BUTTON:SUBMIT
WAIT SECONDS={{!VAR1}}

'loop 2
TAG POS=2 TYPE=INPUT:TEXT ATTR=* CONTENT=B
WAIT SECONDS=0.3
TAG POS=1 TYPE=BUTTON:SUBMIT
WAIT SECONDS={{!VAR1}}

'loop 3
TAG POS=2 TYPE=INPUT:TEXT ATTR=* CONTENT=C
WAIT SECONDS=0.3
TAG POS=1 TYPE=BUTTON:SUBMIT
WAIT SECONDS={{!VAR1}}


the code above allows me to guess 3 times actually, it can be 4 or 5 doesnt matter. What i want to do is detect the code if its a win or lose after i click the submit button, if i won and guess it correctly script will stop and go back from start and i lose script will continue until done.

i hope i have explained it well, simple tasks yet very hard for me to even explain it haha so you be the judge

thanks a lot in advance

Best Regards,
Dot",https://forum.imacros.net/viewtopic.php?f=7&t=26363&sid=91bf0dc741658a74aa70a7f66f53d097,program
2002,Searching through a webpage using iMacros / Javascript,"Hey everyone,

I know many of you probably hate it when new people come here and expect to get their problems solved, but I've spent so many hours on this problem and something tells me that it can't be that difficult to solve.

What I was trying to do is something very simple: I wanted to create a macro with iMacros that is able to go through a webpage, search for a certain word and if the word is found, click on a button. Else, refresh the page and start again.

I realized that there is no easy way to use if/else on iMacros, so that I'll need to add this with Javascript. Still, my major problem is found in the macro. Basically, if I use the extract option on iMacros, it will only skim through the HTML code of the page, if I understand it correctly. This means that it will need a certain TYPE. The problem is that this webpage I am looking to skim through does update time after time but does not show these new ""results"" in the HTML code. I think it uses some kind of Javascript or php, not showing the text I want to find in the HTML code. How do I still program iMacros to simply search through the ""visible"" text on the page (in a CTRL+F kind of way)?

The macro I had implemented in my .js file that is responsible for extracting looks like this:

VERSION BUILD=8970419 RECORDER=FX
TAB T=1
SET !EXTRACT_TEST_POPUP NO
TAG POS=1 TYPE=A ATTR=TXT:*Text* EXTRACT=TXT

In this example I try and search for the word [...]Text[...] If I try and use the wildcard TYPE=* it simply does not work.

So: Is there a way to search on a page and not only the HTML code with iMacros? How would I do it?
If it doesn't work wit iMacros, is there a way to simply do it with Javascript?

Thanks in advance.",https://forum.imacros.net/viewtopic.php?f=7&t=26381&sid=91bf0dc741658a74aa70a7f66f53d097,program
2003,Edit data in CSV,"Hello! I am trying to read data from csv, then use it in webform, but after this I need to edit this data in the CSV to be sure which lines has been used. 
How can I read only certain lines from CSV file which begin only with certain symbol? And how can I edit first symbol of CSV line to tell program not to use this line again next time?

Something like that:
Code: Select all@Line1
Line2
Line3

Reading line1 => used, goto line2; 
Reading line2 => not used, use line2;
Line2 used => change line2 to @line2;
Maybe I can write this ""@ parameter"" in col1, and if col1 = ""@"" then goto next line? How can I write this in IMacros?",https://forum.imacros.net/viewtopic.php?f=7&t=26163&sid=91bf0dc741658a74aa70a7f66f53d097,program
2004,Selective Search and Export to CSV,"Hello Team,

Firstly, thanks a million for making iMacros. Its the best thing I found to manage my work. I will skip to the query.

Please help me with the problem I am facing. I have reviewed many (really lot of) articles on this forum but my lack of knowledge in programming makes it very tough to understand the technical jargons used, or direct info provided without any explanations on why something needs to be where.

I am sorry, I could not find/understand how to find and sort selective data from one page,, and extract into selected(dedicated) columns to manage my portfolio.
[I use firefox(45.0.1)+iMacros plugin(just installed plugin yesterday so it should be the most/latest available. I use Windows7. I am clueless on programming and hence request you to kindly help me with some comments for the iMacros script that will be shown.]

I have been trying to extract data from only 2 columns of this webpage for randomly selected stocks from this link:
https://www1.nseindia.com/live_market/d ... market.htm

Now the code starts like this:
URL GOTO=https://www1.nseindia.com/live_market/d ... market.htm
鈥?The second command below is to be sorting the table by the symbol so that I can review later easily.
TAG POS=1 TYPE=A ATTR=TXT:Symbol

{{Above, the link is first opened and then I sort the table by the Symbol (alphabetically)}}

The problem is when I make an iMarco by selecting the column in the table, it gets a POS<ID> today, and tomorrow it may change.

[For example, when I select 鈥楤HEL鈥?as shown below, I get 飪?TAG POS=124 TYPE=TD ATTR=* EXTRACT=TXT.   The problem is the POS124 keeps changing dynamically and there is no way to track them. It can happen monthly or weekly, that the POS124 becomes POS138 tomorrow for that stock.]

I am looking for a few things as listed below:
鈥?The iMacro should automatically search for the term 鈥楤HEL鈥?in the first column and then pull its value(from price column), irrespective of the POS<ID> it holds for this month/week.
鈥?If I decide to monitor 5 stocks(like shown below in red), the iMacros should automatically push the time-stamp in column1 and then populate the 5stocks鈥?values in col2/3/4/5/6.
鈥?The iMacros should run until the system time becomes 3PM. 

Please suggest what scripts need to be added.

Here is a screenshot of the 2 columns.






I would be very very grateful to have any help or suggestions on this one. 

Thank you!",https://forum.imacros.net/viewtopic.php?f=7&t=26027&sid=e8ccee866abcf29ebd57b704a290f61c,program
2005,"Tag with Direct Screen, Open in new tab","I try make a comande for open a x,y direct click on a new tab, am not realy good for program

So my question is how Tag a element with 
IMAGESEARCH POS=1 IMAGE=sample.png CONFIDENCE=85
DS CMD=MOVETO X={!IMAGEX} Y={!IMAGEY}
...

OR

How extract a HREF from a image validation for open in a new tab 

Ty for help",https://forum.imacros.net/viewtopic.php?f=7&t=25535&sid=9d96e1268e5b272969ed509f53432633,program
2006,redoubles doublequotes when saving extracted tag as html,"Hello!
I'm using iMacro 8.9.4 as Firefox-Addon for FF 38.0 on my Linux Mint 17.1-machine with german language. I am pretty new to iMacro, but I have some programming-knowledge.

I try to extract a part of a website and save the output into an html-file. My Code (the last two lines are important):
Code: Select allTAG POS=1 TYPE=TITLE ATTR=TXT:* EXTRACT=TXT
SET !VAR1 EVAL(""\""{{!EXTRACT}}\"".replace(/([0-9]+)/,\""$1.html\"");"")
SET !EXTRACT NULL
TAG POS=1 TYPE=div ATTR=class:main EXTRACT=HTM
SAVEAS TYPE=EXTRACT FOLDER=~/path/to/file/ FILE={{!VAR1}} 

When I open the output-File. it looks like this:
Code: Select all""<div style=""""style-info ..."""" class=""""main"""">text and tags ...</div>""
Every double-quote is double-quoted a secound time. Additionally, the whole html-code is double-quoted.

The SaveAs.iim-Demo works, but there is no Data-Extraction involved.

Do you have an idea, how to solve the problem?

Thanks in advance!
Thomas131",https://forum.imacros.net/viewtopic.php?f=7&t=25160&sid=7fd87bd5f8edabeec090c4a1f0d5b352,program
2007,take text ajax page,"Hello everyone, this program is very powerful.
I do not quite copy me the all email addresses from this elenco (a-z) http://tinyurl.com/praticante , click on ""praticante abilitato"" and as ""prov"" write M.
It should not be difficult, but I can not, if someone is kind enough to help me I would be grateful.
Thank you",https://forum.imacros.net/viewtopic.php?f=7&t=25064&sid=7fd87bd5f8edabeec090c4a1f0d5b352,program
2008,Scraping a hard page,"Hi everyone, this is my first post as I just stumbled upon this package and I am trying to learn how to use it to load and scrape pages.

When loading this page in a browser all the information is displayed after a while.
https://www.precisionreloading.com/cart.php#!c=103&p=1
I am trying to scrape the product information like name, price, etc.  When I load that link into the sample program that comes with iMacros I definitely do not get the same information that is displayed in a web browser.  Can someone guide me on how this is done or if its even possible to accomplish.

I am using IE 11 or Chrome 44
Win7
VERSION BUILD=10.4.28.1074

I used this script to test.

        private void button1_Click(object sender, EventArgs e)
        {
            StringBuilder macro = new StringBuilder();
            macro.AppendLine(@""TAB T=1"");
            macro.AppendLine(@""TAB CLOSEALLOTHERS"");
            macro.AppendLine(@""URL GOTO=https://www.precisionreloading.com/cart.php#!c=103&p=1"");
            macro.AppendLine(@""SAVEAS TYPE=TXT FOLDER=* FILE=fil.txt"");

            string macroCode = ""CODE:"" + macro.ToString();
            PlayMacro(macroCode);
        }
    }






Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=24911&sid=7fd87bd5f8edabeec090c4a1f0d5b352,program
2009,Simple image capture,"Hello all or whoever reads this.
 Let me start off by saying, I know absolutely NOTHING about programming. If this makes you laugh, I'm perfectly OK with that. If iMacros isn't even considered programming, that's fine too. I'm looking for help here, not someone to just quickly say what I need to do and then expect me to do it. I won't be able to do that on my own. So please accept my apologies in advance for not knowing what i probably should know and please except my gratitude for helping me out.  Here's what I'm looking for. I need a simple way to automate the downloads on this wallpapers page.

http://www.hdwallpapers.in/

I'd preferably like to download all of them and have it save each category into it's own folder, ie. all the fantasy wallpapers in a fantasy folder, all the gaming ones in a gaming folder, and such. The main ones I've after are the gaming wallpapers though, so if that's all that can be done, then that's cool. 

Thanks for just taking the time to read this. G'nite....  ",https://forum.imacros.net/viewtopic.php?f=7&t=24670&sid=7fd87bd5f8edabeec090c4a1f0d5b352,program
2010,How to go through a list in a changing website!,"Hi! I am so new in the world of programming, that I am frustrated because I don't even know where to look and learn, so any advice is helpful here. . I am working. I work on an ophthalmology doctor's office and i have to go to his website, click on each patient's visit, go on to his information, find the wether we have had sent a referral report before, if not, i have to find if we have a fax number for the primary care doctor. If we have it I have to click on ""send report"" then a new window pops up, I have to click a new link to send a fax to the primary care doctor and send a report to the primary doctor. If the number has ""-"", ""/"" or anything else I have to erase them, then click send. After that, I have to do the same for each patient.  I have to do this hundreds of times a day. Is there anything you guys can suggest? At least pointing me in the right direction. 

I took a python beginners course and I loved it, but still I am very new to programming in general. I discovered iMacros and i feel I can do so much with it. But a little help would be so appreciated.",https://forum.imacros.net/viewtopic.php?f=7&t=24577&sid=030d300656bab0b18f7fde81bb00e1ce,program
2011,iMacros stopped working message,"iMacros 10.3 / IE 11 / Windows 7 / Language: English

I have a macro that uses iMacros to scrape information off of an invoice.  This macro worked in iMacros 6.9 and 9.  When we went to version 10 and 10.3, I've started encountering an issue.  The macro will go through 2, 3, 4, 5, or more invoices and a message is displayed that says ""iMacros has stopped working.  A problem caused the program to stop working correctly.  Please close the program."" A button is displayed that says ""Close the program"".  There is no other information to determine what the problem is or how to fix it.

The macro loops through a series of invoices.  After it extracts the appropriate information, the clicks on an ""X"" to close the invoice.  It them checks to see if that the application is on the correct screen.  If so, it moves to the invoice search screen and starts over.  The problem seems to occur after clicking on the ""X"" top close the invoice.

This is the code that closes the invoice (clicks on the ""X""):

IM = """"
IM = ""CODE:""
IM = IM + ""TAB T=1"" + vbNewLine
IM = IM + ""TAB CLOSEALLOTHERS"" + vbNewLine
IM = IM + ""FRAME NAME=container"" + vbNewLine
IM = IM + ""SET !TIMEOUT_STEP 5"" + vbNewLine
IM = IM + ""TAG POS=1 TYPE=BUTTON FORM=NAME:pageForm ATTR=NAME:Close"" + vbNewLine
IM = IM + ""WAIT SECONDS=2"" + vbNewLine

iret = iim1.iimPlay(IM)

I've also used the following FRAME NAMES to see if that would make a difference and it doesn't.

IM = IM + ""FRAME F=3"" + vbNewLine
IM = IM + ""FRAME NAME=*"" + vbNewLine


This is the code that moves to the invoice search screen by click on a tab in the application and selecting the appropriate option:

IM = """"
IM = ""CODE:""
IM = IM + ""TAB T=1"" + vbNewLine
IM = IM + ""TAB CLOSEALLOTHERS"" + vbNewLine
IM = IM + ""FRAME NAME=menu"" + vbNewLine
IM = IM + ""SET !TIMEOUT_STEP 10"" + vbNewLine
IM = IM + ""SET !TIMEOUT_PAGE 10"" + vbNewLine
IM = IM + ""TAG POS=1 TYPE=DIV ATTR=ID:title1"" + vbNewLine
IM = IM + ""DS CMD=CLICK X={{!TAGX}} Y={{!TAGY}} "" + vbNewLine
IM = IM + ""DS CMD=KEY X=0 Y=0 CONTENT={ENTER}"" + vbNewLine
IM = IM + ""WAIT SECONDS=3"" + vbNewLine

iret = iim1.iimPlay(IM)

I've also used the following FRAME NAMES to see if that would make a difference and it doesn't.

IM = IM + ""FRAME F=2"" + vbNewLine
IM = IM + ""FRAME NAME=*"" + vbNewLine

Any suggestions would be greatly appreciated.  Thanks for your help........",https://forum.imacros.net/viewtopic.php?f=7&t=24270&sid=030d300656bab0b18f7fde81bb00e1ce,program
2012,Extracting Tooltip from Onmouseover,"Hello fellow iMacros(u)sers  

So far I've had lots of fun and great success with iMacros. However I seemed to have reached my limit of programming capability with this one particular case.   

I've been trying to extract a Tooltip triggered by a mouseover event. I found the entire passage in the source code, so it's definitely exctractable right away, if anything I found out that iMacros has a feature to now trigger mouse events, but I didn't even get that far. I always hover over the respective thing on the website myself, just to see if iMacros will pick it up, but I'm not getting there.

The Code I'm trying to break, I changed the values a little bit.
(Note that whatever text I'm trying to extract appears twice, they are both the same and extracting one of them is enough for me.)
Code: Select all<td class="" c73w""><span class=""storage""><img src=""../../img/storage7.jpg"" onmouseout=""UnTip()"" onmouseover=""Tip('Text I want to extract 1 <br/>Text I want to extract 1 )"" class=""storageBar""><div class=""storageLevelDiv""><span onmouseout=""UnTip()"" onmouseover=""Tip('Text I want to extract 1 <br/>Text I want to extract 2')"" class=""storageLevel"">2.966.578</span></div></span></td>

I am trying to read the Tips basically. I know how to crop them and replace whatever I don't need, I also know how to extract the 2.966.578 part, I just can't seem to get the Tooltip. 
I've had various tries, here are my last ones, including some thing I found on the forums here and tried out. 
Code: Select all//iimPlayCode(""TAG POS=2 TYPE=SPAN ATTR=CLASS:storageLevel&&ONMOUSEOVER:Tip EXTRACT=TXT"");
	//iimPlayCode(""TAG POS=1 TYPE=TD ATTR=CLASS:onmouseover* EXTRACT=TXT"");
	//iimPlayCode(""TAG POS=7 TYPE=TD ATTR=CLASS:c73w:storageLevel&&ONMOUSEOVER:Tip EXTRACT=TXT"");
	//iimPlayCode(""TAG POS=7 TYPE=TD ATTR=CLASS:c73w CONTENT=EVENT:MOUSEOVER EXTRACT=TXT"");
	//iimPlayCode(""TAG POS=7 TYPE=SPAN ATTR=CLASS:c73w:storageLevel&&ONMOUSEOVER:Tip EXTRACT=TXT"");


This I also found on the forums here, but building it into above TAGs always gives me an error claiming it was a wrong use of Command:Tag
Code: Select all	//CONTENT=EVENT:MOUSEOVER


I would really appreciate if anyone could help me out there, how do I extract the Tip?
Thank you for your time and help!

Best regards,
Yurio",https://forum.imacros.net/viewtopic.php?f=7&t=24216&sid=030d300656bab0b18f7fde81bb00e1ce,program
2013,IE 11 / iMacros 10.0.2.2823 - iMacros stopped working,"I have a macro using (Excel VBA) that goes through out invoice application. The macro opens an invoice and extracts information on a number of different screens. It then closes the invoice, looks for the word ""reservation"" on the next screen. Once it finds ""reservation, it should open the next ticket. 

With IE 8 / iMacros 9, the macro did not encounter any issue. However, recently, we were upgraded to IE 11 and iMacros 10. When users run the macro now, they can get a ""iMacros has stopped working. A problem caused the program to stop working correctly. Please close the program."" message.  Sometimes it will go through 10 tickets before it displays the message while other times it will go through 3 tickets.  It's not consistent.

Following is the loop where the macro is looking for the word ""reservation"". this appears to be the place where the error / problem occurs.
lCnt = 0
vpagehead = """"

Do Until InStr(1, vpagehead1, ""Reservation Number"") > 0 Or InStr(1, vpagehead1, ""servation:"") > 0 Or lCnt = 25

    IM = """"
    IM = ""CODE:""
    IM = IM + ""TAB T=1"" + vbNewLine
    IM = IM + ""TAB CLOSEALLOTHERS"" + vbNewLine
    IM = IM + ""FRAME NAME=ReservationDetailFrame"" + vbNewLine
    IM = IM + ""SET !TIMEOUT_STEP 10"" + vbNewLine
    IM = IM + ""TAG POS=11 TYPE=TD ATTR=* EXTRACT=TXT"" + vbNewLine
    IM = IM + ""WAIT SECONDS=1"" + vbNewLine

    iret = iim1.iimPlay(IM)

    vpagehead1 = iim1.iimGetLastExtract
    lCnt = lCnt + 1

Loop

Does anyone see any reason why this code would cause an issue that would cause iMacros to close?

Thanks for the help..........",https://forum.imacros.net/viewtopic.php?f=7&t=23938&sid=030d300656bab0b18f7fde81bb00e1ce,program
2014,Download as Excel file from drop down option,"Hi,

I am not very familiar with iMacros (or even HTML), and got to know about it as an easier way (compared to writing a JavaScript code) for web scraping. I know my coding skills are very minimal, but I am very close to finding a solution to my data scraping needs using iMacros. I would really appreciate if this forum could provide a workable solution to the issue I am facing (advance apologies for technical imprecision):

First of all, I am using iMacros version 881205 with Firefox v 34.0.5 on Windows 7. I need to download data from a website such that I have to select (click on) Level A, which leads to Level B, clicking on which leads to Level C, which contains the required data (table). iMacros works perfectly for going back and forth between different levels, but I am facing trouble with saving the table, for which there is an option to download it as Excel (or PDF) from a drop down.

The actual trouble is not so much for downloading the table as Excel, as iMacros records that step as well:

TAG POS=1 TYPE=A ATTR=ID:ReportViewer_ctl05_ctl04_ctl00_ButtonLink
TAG POS=1 TYPE=A ATTR=TXT:Excel
TAB T=2
TAB T=1

(i did find a similar post http://forum.imacros.net/viewtopic.php? ... 742#p61655, but unfortunately it didn't provide a solution to my problem)

The problem occurs when it takes time to load the page (at Level C), the code gets stuck and shows error that TAB T = 2 does not exist. I guess this happens because when one clicks at the option to save as Excel, the browser opens a new tab (finding a way around this issue might as well solve my problem), and then closes it and returns to the original tab. 

To get over this issue, I have included WAIT SECONDS = 5 (or even a higher number) before this step, but sometimes when it takes longer than the specified number of seconds for a page to load, the whole thing just stops. Also, this is not a very efficient solution as I am making the program wait every time even when the page has loaded, and when the page actually takes time to load, this does not provide any help.

I also tried WAIT SECONDS = (10 | #DOWNLOADCOMPLETES) but it does not work. I have increased !TIMEOUT_PAGE and have SET !ERRORIGNORE YES.

The way I see it, I believe a possible solution could be to somehow avoid the TAB = 2 TAB = 1 steps altogether (?), or to write an IF condition for the code to enter TAB = 2 only when the page has loaded completely (?).

As I mentioned above, my coding skills are very limited and maybe not up to the task. So, I hope this forum can provide some solution to my problem. Also, please do tell if there is no way to go around this issue using iMacros so that I may look for some other way to scrape the data (please suggest); or, if nothing works out, will have to get on with manually downloading the data (which would be a shame i suppose ).

Sorry for the longish post (better technical understanding may have reduced the length). I really appreciate your time and effort.",https://forum.imacros.net/viewtopic.php?f=7&t=23787&sid=b07178a8376ad918bdc99d345a2f0b97,program
2015,looping through two recordsources,"Hello, I'm relatively new to iMacros and javascript, but not to programming...

First off, I'm running iMacros version 8.8.2
                                Firefox version 24
                           On Windows Xp Pro Ver 5.1 with service pack 3

heres what i need to be able to do:
open my first recordsource, loop through the data(.csv file), each record is an url that i want to goto,
then, open my second recordset, loop through each record and update the first recordset, based on what i have in the second...


here is what i've got so far......

var numberOfUrls = 1001
while(true){
for(var i=1;i<numberOfUrls;i++){
var macro = ""CODE: ""
macro+= ""SET !ERRORIGNORE YES""+""\n""
macro+= ""SET !REPLAYSPEED fast""+""\n""
macro+= ""SET !TIMEOUT_STEP 1""+""\n""
macro+= ""SET !DATASOURCE Grp1000USA.csv""+""\n""
macro+= ""SET !LOOP ""+i+""\n""
macro+= ""SET !DATASOURCE_LINE {{!LOOP}}""+""\n""
macro+= ""URL GOTO={{!COL1}}""+""\n""

the above works fine I can go to the url in column 1 and even manipulate the data there. 
But, what I need to be able to do is now open a second recordset and loop though all the records(1 per line) and update the first url, with col2 from the second recourd source
Heres my first problem, I cant seem to pass the value of !COL2 from the macro to the script

macro+= ""!VAR1={{!COL2}}""+""\n""	

iimPlay(macro)

//mycode =!VAR1

mycode=iimGetLastExtract()

window.alert(mycode)


	Then here instead of adding a single piece of data then looping on through the original first datasource, I need to be able to loop through the second data source and update each record in first datasource with any info in the COL i need to use


//macro+= ""TAG POS=1 TYPE=A ATTR=TXT:mytext""+""\n""
//macro+= ""TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:group.php ATTR=NAME:myname CONTENT={{!VAR1}}""+""\n""
//macro+= ""TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:group.php ATTR=NAME:action&&VALUE:Invite""+""\n""
iimPlay(macro)

any help would be apreciated...  tHANKS
}
}",https://forum.imacros.net/viewtopic.php?f=7&t=23681&sid=b07178a8376ad918bdc99d345a2f0b97,program
2016,Problem with encoding when using SAVEAS command,"I am using iMacros 8 and Internet Explorer 9.

I have a macro that downloads trademark data from the Belarus trademark office website. Each fetched trademark is saved as a complete web page and all downloaded marks are post processed by an other program to extract all relevant trademark data.

Recently the site lay-out was changed: the previous version of the page contained the header <META content=""text/html; charset=utf-8"" http-equiv=Content-Type> which resulted automaticaly in a saved html file of type U8-DOS where all Cyrillic characters where maintained.
The new version now contains the header <META content=""text/html; charset=windows-1251"" http-equiv=Content-Type> which results in a saved file of type DOS where all Cyrillic characters are malformed.
When saving the page manually with Internet Explorer you can switch the encoding from Cyrillic (Windows) to Unicode (UTF-8) to resolve this problem.
Any ideas how to accomplish the same result with iMacros?

Macro to reproduce the problem:

VERSION BUILD=8001865
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=http://www.belgospatent.org.by/database ... =ru&page=1
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:form1 ATTR=ID:textfield2 CONTENT=36966
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:form1 ATTR=ID:button&&VALUE:袧邪褔邪褌褜<SP>锌芯懈褋泻
TAG POS=1 TYPE=TD ATTR=TXT:1
TAB T=2
SAVEAS TYPE=CPL FOLDER=* FILE=""c:\BELA.htm""",https://forum.imacros.net/viewtopic.php?f=7&t=23508&sid=b07178a8376ad918bdc99d345a2f0b97,program
2017,Overwhelmed with iMacros for many things.,"1. Version Number
VERSION BUILD=9002379
2. Operating System 
Windows 8.1
3. Browser
iMacros Browser 9.00.2379
4. Demos
I have not tested all but saving html is working. 
5. VBS Samples
I'm having trouble getting the extract-2-file.vbs to work.  I don't see how the VB code is connected in any way to the iMacros code below. 
http://wiki.imacros.net/Extract-2-File.vbs
6. Problem URL
This is in the question
7. Different Browser
Ignored for the moment due to my confusion. 

Hello iMacros community.  I am absolutely amped to start using this product but I am overwhelmed trying to get a few couple computing statements working.  I'm not a completely novice programmer but I use mostly R and the syntax of making these statements work is running against my expectations. 

I will show you the code I have so far (only using the direct control instead of calling elements) with comments on what I'm trying to do. I've been reading through the wiki on how to approach these statements more than a couple times and now I think its time to ask for a little guidance even though I'm starting from an early stage. 
Code: Select allVERSION BUILD=9002379
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=http://www.courtrecords.alaska.gov/eservices/home.page.9
SIZE X=970 Y=652
WAIT SECONDS=3.000
DS CMD=CLICK X=279 Y=557 CONTENT=
WAIT SECONDS=3.000
DS CMD=CLICK X=430 Y=204 CONTENT=
WAIT SECONDS=3.000
DS CMD=KEY CONTENT=3AN-14-0000{{!loop}}CR
WAIT SECONDS=3.000
DS CMD=CLICK X=203 Y=231 CONTENT=
WAIT SECONDS=3.000
DS CMD=CLICK X=53 Y=158 CONTENT=
WAIT SECONDS=3.000
SAVEAS TYPE=HTM FOLDER=* FILE=3AN-14-0000{{!loop}}CR

After running the code elsewhere I can see that it is sensitive to the font and sizing of the browser so playing with it is necessary (duh). What I am trying to show is that I am doing the following order of steps

1. Go to the pubic court document website
2. click to search (I don't start out step one at the search form because the site is very painfully tracking my session)
3. Enter in my case code starting from 0. and click search
4. Click the first result
5. save the whole html file
6. repeat with the case number at 1,2, ...

Here is the TL:DR questions of my dilemma: 

Q1. How can I click dynamic links like I do in step 2 without using direct screen? (I found something on this under X/Y at http://wiki.imacros.net/First_Steps but I need a little more guidance as someone who has only finished the first half of javascript at code academy)

Q2. How can I run a ""while"" loop to conditionally only return cases that have links in step 4 (and hence, are real). 

Q3. Under Play, I can choose to either run a script or run a loop where I set the limit outside the script.  How can I set a limit to my loop within the script. 

Q4.  If I want to run multiple loops in one script with different limits how can I do this so the loop is contained within its respective code. 

Thanks.  And forgive my sloppy language for where more specific terms could have been used.",https://forum.imacros.net/viewtopic.php?f=7&t=22762&sid=7e74df639d022ccbaa0e92d884edcf30,program
2018,"New to Macro programming, Quick initial question","I have a basic programming background, and am willing to learn the necessary steps of writing a proper macro on my own, contingent upon the answer to this question. I'm planning on using iMacros for Firefox on Windows 7 or Mac OSX 10.6.8 (I have both available), and I'm curious if what I'm about to describe is possible.

This is the website I want to scrape for names: http://www.faseb.org/The-Directory/Sear ... ctory.aspx. I want to collect information about employees at certain organizations by running a search and selecting the link for their name. The link to THAT webpage is what holds most of the information I want. However, it's easy enough to filter by an institution name, but there is no unique URL for a page with a specific list of results. This posed a problem for my normal scraping software (outwit hub), where I used that program to write a macro that would work, if not for the fact that at the start of the execution of the macro, it loads the original page URL first (which always results in the default search page, AKA a blank page with no results). Selecting the ""search"", ""clear"" and the "">>"" buttons on the page don't load unique URLS, so makes me think that the database is just an active query or something, which makes automating using the program I have (outwit) a problem. 

Could I write a macro using iMacros for Firefox on either a Windows 7 machine or a Mac running Windows 7 or Mac OSX 10.6.8 that would allow me to extract data from a search described above while cycling through the resulting links on each page of the search? I'm sorry if this sounds a bit technically illiterate or beginnerish. Normally I would just say, ""hey why not just jump in and if I discover it's not possible a couple days in it was worth the experience"", but this is more of a time issue since I would need to have these contacts available by next week. Thank you for any help and I appreciate your patience with me!",https://forum.imacros.net/viewtopic.php?f=7&t=22652&sid=764a56e3c04defae9b390be083f6af7b,program
2019,Looking for some opensource scraping software,"I'm looking for an opensource scraper which can do (or be modified to do) this:

Parameters:
1. An array of candidate matches, e.g. ['email', 'contact', 'about'] or ['ceo', 'owner', 'founder'] etc.
2. An initial webpage to start from
3. A regular expression to match against (e.g. one for emails, or addresses)

Function: 
1. Check the page to see if the regular expression matches, and if successful, add the match to some sort of 'matches' object. 
2. Next, open any link, button, input, etc. which contains in its innerHTML or value or href (etc.) anything inside of the array of candidate matches. 
3. Repeat step 1, but do not crawl through a webpage that has already been crawled, and do not crawl through webpages that are off the initial domain.

I do not have the system resources to crawl through all the pages on a given domain, nor need one actually. Rather, I need a crawler that will only check links that seem viable given the array of candidate matches.

I don't want to re-invent the wheel and build a crawler out of scratch, but I am willing to take an opensource crawler and re-program some of it to meet my needs. I'm hoping for at least a good jumping off point.

It can be written in either PHP or javascript.",https://forum.imacros.net/viewtopic.php?f=7&t=22596&sid=764a56e3c04defae9b390be083f6af7b,program
2020,*** Please read this before reporting problems ***,"Post last updated: May 25, 2020.

If you open a new topic (or post for the first time in some existing topic) to report an issue or ask any question, we can respond much faster if your problem description includes the following information (if applicable, not all questions fit to all problems):

1. What version of iMacros are you using?
You find the iMacros version and build information at the top of every macro that you record, for example ""VERSION BUILD=10100795"". Also specify whether you are using a freeware, trial, or licensed version, e.g. Personal Edition. Please test with the latest version.

2. What operating system are you using? (please also specify language)
Example: Windows 10, Windows 8.1, Windows 7, Windows Server 2012R2, English, German, Chinese, ...

3. Which browser(s) are you using? (include version numbers)
Example: Chrome 74.0.3729.13, Firefox 64.0.2, IE 11/10/9, ...

4. Do the included demo macros work ok? 
This test helps us to determine if the problem is a general iMacros issue or is somehow related to a specific website or configuration.

5. If reporting a problem with the iMacros scripting interface, please also test if the included VBS sample scripts run ok.
This test helps us to determine if the iMacros scripting interface installation is ok.

6. If recording or replay fails on a specific website: Can you please post the URL of the web page and/or the macro that creates the problem?
If you cannot post the macro in a public user forum or the website requires a login, can you provide a similar example of the problem on a publicly accessible website?

7. Do you encounter the same problem recording or replaying the macro in different browsers, such as the iMacros browser, Firefox, Chrome, or Internet Explorer? 
Note: If the problem is specific to Firefox or Chrome, please post in either the iMacros for Firefox or iMacros for Chrome sub-forum respectively.

8. Please include any additional information that is relevant to reproducing the issue.
This includes a detailed description of the steps you've tried as well as the specific error codes and messages, macro logs, screen shots, etc. Please upload screenshots and other relevant files directly to the forum and not to a third-party hosting service!

This information allows us to quickly re-create the issue on our test systems and solve it ASAP.

Additional tips when posting

* Please ensure that the content of your post is relevant to the topic and/or forum where you post it.
For example, if you are having a problem extracting data from a web page and are using Firefox, create your post in the Data Extraction and Web Screen Scraping forum rather than the iMacros for Firefox forum (see note #7 above). If you are unsure which forum to post to, use the General Support & Discussions forum.

* Use a topic title that succinctly describes the problem or issue.
Avoid phrases such as ""Please Help"", ""Urgent"", ""Script not working"", etc. and refrain from using all capital letters and exclamation points. Also, double-check that the spelling of the title and body of the post are correct, since this makes it more likely to be found in searches by other users. Most modern browsers automatically check spelling as you type and underline mistakes, so please correct them before submitting your post.

* Don't post the same question to more than one forum.
If you've miscategorized your original post, just click the Exclamation Icon on the right of the post's subject/title line to report it and let us know where you would like it moved or merged.

* Always provide your FCI (Full Config Info), c.f. items 1-3 above, in your original post even if you think it may not be relevant.
Some of our mods may not even respond unless this information is included. Please don't put this information in your signature, because this can change over time and makes it more confusing when reviewing older posts. Include the information directly in the body of the post instead.

* When sharing screenshots or other relevant files, please upload them directly to the forum rather than use a third-party hosting/sharing service.
Too often these services stop functioning or the files become unavailable for whatever reason. Uploading them here ensures that the full context of the post remains consistent over time.

* Take a minute to review your forum notification settings

Thank you for helping us to help you better!",https://forum.imacros.net/viewtopic.php?f=20&t=3331&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2021,Dynamic Frame Tracking,"Configuration: Surface Pro 2, Win 10, Firefox 53, iMacros Standard Edition (x86) Version 11.0.246.4051
Hi,
 I extract data from a web page and in the last few months the frame number (no frame names that refer to table) seems to vary from numbers 14-18 every few days/weeks.  My work around now is to manually change number.  I would like to able to have macro handle that, but I can't figure out how to do it. The original macro has 300 lines of code and I need to access this particular frame twice, line 47 and then again at line 234.  Ideally the macro would determine frame number, then store in variable for second access.  I have searched the web and I know there are scripting solutions out there but I am confused/unsure how to specifically code and then incorporate script into the existing macro: ie: Run firstpart.iim, run framenumber.js, then run finalpart.iim??
Thanks

TEST Macro:
VERSION BUILD=11.0.246.4051
SET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP YES
TAB OPEN
TAB T=2
URL GOTO=http://armls.flexmls.com/
FRAME NAME=top_frame
TAG POS=2 TYPE=A ATTR=TXT:Map<SP>search
FRAME NAME=view_frame
TAG POS=1 TYPE=SELECT ATTR=NAME:sdlist CONTENT=%x'20080728221329913527000000'
TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=TXT:Use
TAG POS=1 TYPE=SELECT ATTR=NAME:s_3 CONTENT=%A:%AWC_C:%AWC_Y:%P:%C
TAG POS=1 TYPE=SPAN ATTR=ID:3_see_all_link
TAG POS=1 TYPE=INPUT:CHECKBOX ATTR=NAME:c_3_4_date CONTENT=YES
TAG POS=1 TYPE=INPUT:TEL ATTR=NAME:relative_3_4 CONTENT=365
TAG POS=1 TYPE=INPUT:CHECKBOX ATTR=ID:enabled_4 CONTENT=YES
FRAME NAME=iframe_map
TAG POS=1 TYPE=IMG ATTR=SRC:http://armls.flexmls.com/images/mapping/pin_24.png
FRAME NAME=view_frame
TAG POS=1 TYPE=SELECT ATTR=NAME:s_4 CONTENT=%SF
SET !VAR1 ""4897 e hazeltine ct""
SET !VAR2 ""85249""
SET !VAR7 ""chandler""
'PROMPT ""Please enter subject street address:"" !VAR1
'PROMPT ""Enter subject zip code:"" !VAR2
'PROMPT ""Enter subject city"" !VAR7
FRAME NAME=iframe_map
TAG POS=1 TYPE=IMG ATTR=SRC:http://armls.flexmls.com/images/mapping/pin_24.png
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:NoFormName ATTR=NAME:address CONTENT={{!VAR1}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:NoFormName ATTR=NAME:zip CONTENT={{!VAR2}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:NoFormName ATTR=NAME:city CONTENT={{!VAR7}}
TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=TXT:Locate
TAG POS=1 TYPE=DIV ATTR=ID:point_0
WAIT SECONDS=5
TAG POS=1 TYPE=SPAN ATTR=TXT:Use<SP>this<SP>location
TAG POS=1 TYPE=A ATTR=TXT:Radius<SP>Search
TAG POS=1 TYPE=A ATTR=TXT:Create<SP>Radius
WAIT SECONDS=10
FRAME NAME=view_frame
'Extract 12 month Neighborhood house price stats
TAG POS=1 TYPE=LI ATTR=ID:tab_stats
TAG POS=1 TYPE=A ATTR=TXT:Comparison<SP>Statistics
WAIT SECONDS=15
FRAME F=16
TAG POS=1 TYPE=TD ATTR=ID:hiLP EXTRACT=TXT
TAG POS=1 TYPE=TD ATTR=ID:ctLP EXTRACT=TXT
TAG POS=1 TYPE=TD ATTR=ID:lowSP EXTRACT=TXT
TAG POS=1 TYPE=TD ATTR=ID:hiSP EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:ctSP EXTRACT=TXT
TAG POS=1 TYPE=TD ATTR=ID:avgDom EXTRACT=TXT
FILEDELETE NAME=C:\Users\Public\Documents\iMacros\datasources\BBformdata\Chasedata.csv
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Public\Documents\iMacros\datasources\BBformdata FILE=Chasedata.csv",https://forum.imacros.net/viewtopic.php?f=7&t=27529&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2022,Hidden not found for copy,"Hello friends!
SyntaxError: wrong format of TAG command at line 3

My code HTML
Code: Select all<form method=""post"" action=""./Default.aspx"" onsubmit=""javascript:return WebForm_OnSubmit();"" id=""form1"">
<div class=""aspNetHidden"">
<input type=""hidden"" name=""__LASTFOCUS"" id=""__LASTFOCUS"" value="""">
<input type=""hidden"" name=""__EVENTTARGET"" id=""__EVENTTARGET"" value="""">
<input type=""hidden"" name=""__EVENTARGUMENT"" id=""__EVENTARGUMENT"" value="""">
<input type=""hidden"" name=""__VIEWSTATE"" id=""__VIEWSTATE"" value=""cd"">
</div>

<script type=""text/javascript"">
//<![CDATA[
var theForm = document.forms['form1'];
if (!theForm) {
    theForm = document.form1;
}
function __doPostBack(eventTarget, eventArgument) {
    if (!theForm.onsubmit || (theForm.onsubmit() != false)) {
        theForm.__EVENTTARGET.value = eventTarget;
        theForm.__EVENTARGUMENT.value = eventArgument;
        theForm.submit();
    }
}
//]]>
</script>


<script src=""/TAD/WebResource.axd?d=LwKrpeey8Lg20cB6iUnkjLYNGim3IjN79XR3AmD1ku6Y8wro1rcozWa35DdJuul_XmzJSQjCdlzF2kFjqmy1ZGIeZQCZOADzZYni63FYiTI1&amp;t=636426768960000000"" type=""text/javascript""></script>


<script src=""/TAD/ScriptResource.axd?d=chy14o2ELjrshtWIPSbK4hkKWFVJhCcJkCbTlui014UQlthoA0IQIATLmU4bdZxnMpGEoyG5Q29P316k56bln1zrIx1Wg6lLs89qTsrSABmbiMWvTtTGPt-s9jLN9FJ-AE4tGRd5vI5oUorwVcY-kVH2Bb7QYZc2WYe_oEkz4M81&amp;t=548dd326"" type=""text/javascript""></script>
<script src=""/TAD/ScriptResource.axd?d=BFZaFj2zlM87VBZFnBTC9w-44OLgECziO24m1DgoQkEkmrpkNs31OUt4pMqOJfYOjBhloREcyzSxPR0zjpHFXQNcOU7nrRYzpz-77gxZGDVjB95O7l32jgHa-5vpjBMUYfmDT3kdCw6cXZRf_aXtRMKBzYDYOSdlRnS1TWMK8FqXbrd1DnmM2kfXjnUydE0Y0&amp;t=72fc8ae3"" type=""text/javascript""></script>
<script type=""text/javascript"">
//<![CDATA[
if (typeof(Sys) === 'undefined') throw new Error('ASP.NET Ajax client-side framework failed to load.');
//]]>
</script>

<script type=""text/javascript"">
//<![CDATA[
function WebForm_OnSubmit() {
if (typeof(ValidatorOnSubmit) == ""function"" && ValidatorOnSubmit() == false) return false;
return true;
}
//]]>
</script>

<div class=""aspNetHidden"">

	<input type=""hidden"" name=""__VIEWSTATEGENERATOR"" id=""__VIEWSTATEGENERATOR"" value=""9037A323"">
	<input type=""hidden"" name=""__EVENTVALIDATION"" id=""__EVENTVALIDATION"" value=""/cf"">
</div>
    <script type=""text/javascript"">
//<![CDATA[
Sys.WebForms.PageRequestManager._initialize('ctl00$ctl14', 'form1', ['tctl00$ContentPlaceHolder1$UpdatePanelPaisProvLoc','ContentPlaceHolder1_UpdatePanelPaisProvLoc'], [], [], 90, 'ctl00');
//]]>
</script>

      
      <!-- ---toolbar--- -->
        <header>
          <nav class=""navbar navbar-top navbar-default"" role=""navigation"">
            <div class=""container"">
              <div>
                <div class=""navbar-header"">
                  <button type=""button"" class=""navbar-toggle"" data-toggle=""collapse"" data-target=""#main-navbar-collapse"">
                    <span class=""sr-only"">Toggle navigation</span>
                    <span class=""icon-bar""></span>
                    <span class=""icon-bar""></span>
                    <span class=""icon-bar""></span>
                  </button>
                  <a href=""http://www.dnrec.jus.gov.ar/"" class=""navbar-brand""><img src=""img/logoRNR_header.png""></a>
                </div>
                <div class=""collapse navbar-collapse"" id=""main-navbar-collapse"">
                  <ul class=""nav navbar-nav navbar-right"">
                    <li><a title=""Inicio"" href=""Default.aspx"">Nueva solicitud</a></li>
                  </ul>
                </div>
              </div>
            </div>
          </nav>
        </header>
        <!-- ---FIN toolbar--- -->

        <!--contenido-->
        <section style=""margin-top:0;padding-top:5px;"">
            <div class=""container"">
                <div class=""row"">
                    <div class=""col-md-12"">
                        
    <h4>Tr谩mite en l铆nea v铆a Banelco / Pagomiscuentas</h4>
    <span id=""Titulos_lblEmail"">info@datarg.com.ar</span>

                        <hr>
                        

    <script type=""text/javascript"">
        //solo para RequiredFieldValidator
        function BtnClick() {
            var val = Page_ClientValidate();
            if (!val) {//si falla
                var i = 0;
                for (; i < Page_Validators.length; i++) {
                    if (!Page_Validators[i].isvalid) {
                        $(""#"" + Page_Validators[i].controltovalidate).addClass(""input-validation-error"");
                    }
                }
            } else {//si no falla muestro el loading gif
                setTimeout(function () {
                    var loading = $("".loading"");
                    loading.show();
                    var top = Math.max($(window).height() / 2 - loading[0].offsetHeight / 2, 0);
                    var left = Math.max($(window).width() / 2 - loading[0].offsetWidth / 2, 0);
                    loading.css({ top: top, left: left });
                }, 200);
            }
            return val;
        }
    </script>
    <!-- Sumario de validaci贸n -->
    <div id=""ContentPlaceHolder1_valRes"" class=""validation-summary-errors"" style=""display:none;"">

</div>
    <div id=""ContentPlaceHolder1_panCons"" onkeypress=""javascript:return WebForm_FireDefaultButton(event, 'ContentPlaceHolder1_btCons')"">
	
        <div id=""ContentPlaceHolder1_panDatos"">
		
            <p><strong>ACLARACI脫N: Este formulario no acepta acentos o tildes.</strong></p>
            <!-- Tipo y n煤mero de documento -->
            <div class=""row myRow"">
                <div class=""form-group"">
                    <div class=""col-lg-3 col-md-3"">
                        <label>DNI n煤mero:</label>
                        <input name=""ctl00$ContentPlaceHolder1$txtNroDoc"" type=""text"" value=""38371114"" id=""ContentPlaceHolder1_txtNroDoc"" class=""form-control input-sm"" style=""text-transform:uppercase"">
                        <span id=""ContentPlaceHolder1_NDocSolicValid"" class=""validator"" style=""color:Red;display:none;"">Debe completar el N煤mero de Documento</span>
                        <span id=""ContentPlaceHolder1_NDocSolicValidRange"" class=""validator"" style=""color:Red;display:none;"">N煤mero de documento incorrecto</span>
                    </div>
                    <div class=""col-lg-3 col-md-3"">
                        <label>
                            G茅nero:</label>
                        <select name=""ctl00$ContentPlaceHolder1$ddlSexo"" id=""ContentPlaceHolder1_ddlSexo"" class=""form-control input-sm"">
			<option value=""0"">Seleccione</option>
			<option selected=""selected"" value=""M"">Masculino</option>
			<option value=""F"">Femenino</option>
			<option value=""X"">No binario</option>

		</select>
                        <span id=""ContentPlaceHolder1_cmpValSexo"" class=""validator"" style=""color:Red;display:none;"">Debe completar su g茅nero</span>
                    </div>
                    <div class=""col-lg-3 col-md-3"">
                        <label>
                            Tr谩mite a realizar:</label>
                        <select name=""ctl00$ContentPlaceHolder1$ddlTram"" id=""ContentPlaceHolder1_ddlTram"" class=""form-control input-sm"">
			<option value=""0"">Seleccione</option>
			<option value=""2"">24 Horas        ($300.00)</option>
			<option selected=""selected"" value=""3"">6 Horas         ($560.00)</option>
			<option value=""7"">Express         ($980.00)</option>

		</select>
                        <span id=""ContentPlaceHolder1_cmpValTram"" class=""validator"" style=""color:Red;display:none;"">Debe elegir un tipo de tr谩mite</span>
                    </div>
                </div>
            </div>
            <!-- Fecha de nacimiento -->
            <div class=""row myRow"">
                <div class=""form-group"">
                    <div class=""col-lg-3 col-md-3"">
                        <label>
                            Fecha de Nacimiento - D铆a:</label>
                        <select name=""ctl00$ContentPlaceHolder1$ddlDia"" id=""ContentPlaceHolder1_ddlDia"" class=""form-control input-sm"">
			<option value=""0"">Seleccione</option>
			<option value=""1"">1</option>
			<option selected=""selected"" value=""22"">22</option>


		</select>
                        <span id=""ContentPlaceHolder1_cmpValDia"" class=""validator"" style=""color:Red;display:none;"">Debe elegir su d铆a de nacimiento</span>
                    </div>
                    <div class=""col-lg-3 col-md-3"">
                        <label>
                            Mes:</label>
                        <select name=""ctl00$ContentPlaceHolder1$ddlMes"" id=""ContentPlaceHolder1_ddlMes"" class=""form-control input-sm"">
			<option value=""0"">Seleccione</option>
			<option value=""1"">Enero</option>
			<option selected=""selected"" value=""12"">Diciembre</option>

		</select>
                        <span id=""ContentPlaceHolder1_cmpValMes"" class=""validator"" style=""color:Red;display:none;"">Debe elegir su mes de nacimiento</span>
                    </div>
                    <div class=""col-lg-3 col-md-3"">
                        <label>
                            A帽o:</label>
                        <select name=""ctl00$ContentPlaceHolder1$ddlAnio"" id=""ContentPlaceHolder1_ddlAnio"" class=""form-control input-sm"">
			<option value=""0"">Seleccione</option>
			<option value=""1900"">1900</option>
		</select>
                        <span id=""ContentPlaceHolder1_cmpValAnio"" class=""validator"" style=""color:Red;display:none;"">Debe elegir su a帽o de nacimiento</span>
                        <span id=""ContentPlaceHolder1_valFNac"" class=""validator"" style=""color:Red;visibility:hidden;"">!</span>
                    </div>
                </div>
            </div>
            <div class=""row myRow"">
                <div class=""form-group"" style=""margin-bottom: 0px"">
                    <div class=""col-lg-3 col-md-3"">
                        <label>Domicilio:</label>
                        <input name=""ctl00$ContentPlaceHolder1$txtDomicilio"" type=""text"" value=""santa fe 121"" id=""ContentPlaceHolder1_txtDomicilio"" class=""form-control input-sm"" style=""text-transform:uppercase"">
                        <span id=""ContentPlaceHolder1_DomValid"" class=""validator"" style=""color:Red;display:none;"">Debe indicar su Domicilio</span>
                        <span id=""ContentPlaceHolder1_DirecSolicValidText"" class=""validator"" style=""color:Red;display:none;"">Utilice comillas simples y n煤meros</span>
                    </div>
                    <div class=""col-lg-3 col-md-3"">
                        <label>
                            Tel茅fono:</label>
                        <input name=""ctl00$ContentPlaceHolder1$txtNroTel"" type=""text"" value=""1137172405"" id=""ContentPlaceHolder1_txtNroTel"" class=""form-control input-sm"" style=""text-transform:uppercase"">
                        <span id=""ContentPlaceHolder1_rqValNroTel"" class=""validator"" style=""color:Red;display:none;"">Ingrese su n煤mero de tel茅fono</span>
                        <span id=""ContentPlaceHolder1_cusValNroTel"" class=""validator"" style=""color:Red;visibility:hidden;"">N煤mero de tel茅fono incorrecto. Ingrese s贸lo n煤meros. (Ej.: 1143216789)</span>
                    </div>
                    <div class=""col-lg-3 col-md-3"">
                        <label>Banco donde posee la cuenta:</label>
                        <select name=""ctl00$ContentPlaceHolder1$ddlBanco"" id=""ContentPlaceHolder1_ddlBanco"" class=""form-control input-sm"">
			<option value=""0000""> Seleccione su Banco</option>
			<option value=""SOLB"">Banco del Sol                           </option>
			<option value=""TUCU"">Banco Tucuman                           </option>
			<option value=""FNCS"">BBVA</option>
			<option value=""CTBK"">Citibank                                </option>
			<option value=""QLMS"">Comafi                                  </option>
			<option value=""CLOG"">Credilogros                             </option>
			<option value=""GLCA"">Galicia                                 </option>
			<option value=""RBTS"">HSBC                                    </option>
			<option value=""BSTN"">ICBC                                    </option>
			<option value=""IBAY"">Itau                                    </option>
			<option value=""BMBS"">Macro                                   </option>
			<option value=""SDMR"">Patagonia                               </option>
			<option selected=""selected"" value=""BROU"">Rep. Oriental del Uruguay               </option>
			<option value=""RIOP"">Santander                               </option>
			<option value=""SPRE"">Supervielle                             </option>

		</select>
                        <span id=""ContentPlaceHolder1_compBanco"" class=""validator"" style=""color:Red;display:none;"">Debe elegir un banco</span>
                        
                    </div>
                </div>
            </div>

            
            <div id=""ContentPlaceHolder1_UpdateProgressPaisProvLoc"" style=""display:none;"" role=""status"" aria-hidden=""true"">
			Actualizando...
		</div>
            <div id=""ContentPlaceHolder1_UpdatePanelPaisProvLoc"">
			
              <div class=""row myRow"">
                 <div class=""form-group"" style=""margin-bottom: 0px"">
                    <div class=""col-lg-3 col-md-3"">
                        <label>
                            Pais:</label>
                        <select name=""ctl00$ContentPlaceHolder1$ddlPaisSolic"" onchange=""javascript:setTimeout('__doPostBack(\'ctl00$ContentPlaceHolder1$ddlPaisSolic\',\'\')', 0)"" id=""ContentPlaceHolder1_ddlPaisSolic"" class=""form-control input-sm"">
				<option value=""0"">Seleccione</option>
				<option selected=""selected"" value=""12"">Argentina</option>
				<option value=""24"">Bolivia</option>
				<option value=""26"">Brasil</option>
				<option value=""36"">Chile</option>
				<option value=""132"">Paraguay</option>
				<option value=""133"">Per煤</option>
				<option value=""170"">Uruguay</option>
				<option value=""172"">Venezuela</option>
				<option value=""3"">Alemania</option>
				<option value=""13"">Australia</option>
				<option value=""14"">Austria</option>
				<option value=""15"">Armenia</option>
				<option value=""308"">Albania</option>
				<option value=""385"">Angola</option>
				<option value=""388"">Argelia</option>
				<option value=""391"">Arabia Saudita</option>
				<option value=""392"">Azerbaidjan</option>
				<option value=""411"">Apatrida</option>
				<option value=""423"">Andorra</option>
				<option value=""444"">Antillas Holandesas</option>
				<option value=""446"">Afganistan</option>
				<option value=""462"">Aruba</option>
				<option value=""463"">Anguila</option>
				<option value=""464"">Aland</option>
				<option value=""466"">Antartida</option>
				<option value=""369"">Bangladesh</option>
				<option value=""428"">Barbados</option>
				<option value=""455"">Bahamas</option>
				<option value=""19"">B茅lgica</option>
				<option value=""313"">Bulgaria</option>
				<option value=""330"">Bosnia</option>
				<option value=""377"">Bielorusia</option>
				<option value=""378"">Bhutan</option>
				<option value=""401"">Benin</option>
				<option value=""379"">Burkina Faso</option>
				<option value=""468"">Bonaire, San Eustaquio y Saba</option>
				<option value=""469"">Bahrein</option>
				<option value=""470"">San Bartolome</option>
				<option value=""471"">Bermudas</option>
				<option value=""472"">Brunei</option>
				<option value=""32"">Canad谩</option>
				<option value=""37"">China</option>
				<option value=""319"">Camboya</option>
				<option value=""386"">Camer煤n</option>
				<option value=""402"">Chipre</option>
				<option value=""39"">Colombia</option>
				<option value=""338"">Congo</option>
				<option value=""41"">Corea</option>
				<option value=""430"">Corea del Sur</option>
				<option value=""431"">Corea del Norte</option>
				<option value=""43"">Costa Rica</option>
				<option value=""364"">Costa de Marfil</option>
				<option value=""44"">Cuba</option>
				<option value=""344"">Croacia</option>
				<option value=""476"">Comoras</option>
				<option value=""477"">Curazao</option>
				<option value=""531"">Chad</option>
				<option value=""536"">Ciudad del Vaticano</option>
				<option value=""45"">Dinamarca</option>
				<option value=""443"">Dominica</option>
				<option value=""47"">Ecuador</option>
				<option value=""49"">El Salvador</option>
				<option value=""51"">Espa帽a</option>
				<option value=""242"">Escocia</option>
				<option value=""318"">Eslovenia</option>
				<option value=""327"">Eslovaquia</option>
				<option value=""450"">Emiratos 脕rabes Unidos</option>
				<option value=""52"">Estados Unidos</option>
				<option value=""382"">Etiop铆a</option>
				<option value=""393"">Estonia</option>
				<option value=""480"">Eritrea</option>
				<option value=""527"">Eswatini</option>
				<option value=""58"">Francia</option>
				<option value=""332"">Egipto</option>
				<option value=""333"">Filipinas</option>
				<option value=""371"">Finlandia</option>
				<option value=""482"">Fiyi</option>
				<option value=""243"">Gales</option>
				<option value=""433"">Gambia</option>
				<option value=""452"">Gab贸n</option>
				<option value=""60"">Georgia</option>
				<option value=""62"">Grecia</option>
				<option value=""65"">Guatemala</option>
				<option value=""139"">Gran Breta帽a</option>
				<option value=""325"">Guyana</option>
				<option value=""363"">Ghana</option>
				<option value=""400"">Guinea Bissau</option>
				<option value=""403"">Guinea</option>
				<option value=""424"">Republica de Guinea Ecuatorial</option>
				<option value=""454"">Reino Unido</option>
				<option value=""486"">Guernsey</option>
				<option value=""487"">Gibraltar</option>
				<option value=""488"">Guadalupe</option>
				<option value=""489"">Granada</option>
				<option value=""490"">Groenlandia</option>
				<option value=""491"">Guayana Francesa</option>
				<option value=""492"">Guam</option>
				<option value=""71"">Hait铆</option>
				<option value=""72"">Pa铆ses Bajos</option>
				<option value=""73"">Honduras</option>
				<option value=""75"">Hungr铆a</option>
				<option value=""328"">Hong Kong</option>
				<option value=""80"">Irlanda</option>
				<option value=""310"">India</option>
				<option value=""337"">Inglaterra</option>
				<option value=""358"">Indonesia</option>
				<option value=""456"">Antigua y Barbuda</option>
				<option value=""88"">Israel</option>
				<option value=""336"">Ir谩n</option>
				<option value=""365"">Iraq</option>
				<option value=""372"">Islandia</option>
				<option value=""89"">Italia</option>
				<option value=""473"">Isla Bouvet</option>
				<option value=""474"">Islas Cocos</option>
				<option value=""475"">Islas Cook</option>
				<option value=""478"">Isla de Navidad</option>
				<option value=""479"">Islas Caiman</option>
				<option value=""483"">Islas Feroe</option>
				<option value=""493"">Isla Heard e Islas McDonald</option>
				<option value=""494"">Isla de Man</option>
				<option value=""503"">Islas Marshall</option>
				<option value=""504"">Islas Marianas del Norte</option>
				<option value=""523"">Islas Salomon</option>
				<option value=""530"">Islas Turcas y Caicos</option>
				<option value=""535"">Islas Ultramarinas de EEUU</option>
				<option value=""538"">Islas Virgenes Britanicas</option>
				<option value=""539"">Islas Virgenes de los Estados</option>
				<option value=""91"">Jap贸n</option>
				<option value=""339"">Jordania</option>
				<option value=""361"">Kazakstan</option>
				<option value=""389"">Kirgizstan</option>
				<option value=""405"">Kenya</option>
				<option value=""422"">Liechtenstein</option>
				<option value=""99"">L铆bano</option>
				<option value=""317"">Laos</option>
				<option value=""331"">Lituania</option>
				<option value=""335"">Liberia</option>
				<option value=""360"">Jamaica</option>
				<option value=""367"">Letonia</option>
				<option value=""380"">Libia</option>
				<option value=""409"">Luxemburgo</option>
				<option value=""420"">Kuwait</option>
				<option value=""496"">Jersey</option>
				<option value=""497"">Kiribati</option>
				<option value=""499"">Lesoto</option>
				<option value=""115"">M茅xico</option>
				<option value=""316"">Moldavia</option>
				<option value=""324"">Mali</option>
				<option value=""334"">Marruecos</option>
				<option value=""362"">Monaco</option>
				<option value=""368"">Malasia</option>
				<option value=""373"">Nepal</option>
				<option value=""374"">Mozambique</option>
				<option value=""394"">Macedonia</option>
				<option value=""398"">Mauritania</option>
				<option value=""406"">Mongolia</option>
				<option value=""410"">Malta</option>
				<option value=""434"">Namibia</option>
				<option value=""435"">Madagascar</option>
				<option value=""453"">Myanmar</option>
				<option value=""484"">Micronesia</option>
				<option value=""500"">Macao (Rep煤blica China)</option>
				<option value=""502"">Maldivas</option>
				<option value=""505"">Montserrat</option>
				<option value=""506"">Martinica</option>
				<option value=""507"">Mauricio</option>
				<option value=""508"">Malawi</option>
				<option value=""509"">Mayotte</option>
				<option value=""122"">Nicaragua</option>
				<option value=""366"">Nigeria</option>
				<option value=""125"">Noruega</option>
				<option value=""309"">Nueva Zelanda</option>
				<option value=""314"">Pakist谩n</option>
				<option value=""510"">Nueva Caledonia</option>
				<option value=""511"">Niger</option>
				<option value=""512"">Norfolk</option>
				<option value=""513"">Niue</option>
				<option value=""514"">Nauru</option>
				<option value=""515"">Oman</option>
				<option value=""130"">Panam谩</option>
				<option value=""329"">Palestina</option>
				<option value=""436"">Papua Nueva Guinea</option>
				<option value=""135"">Polonia</option>
				<option value=""136"">Portugal</option>
				<option value=""517"">Palaos</option>
				<option value=""518"">Polinesia Francesa</option>
				<option value=""343"">Rep. Dem. Congo</option>
				<option value=""519"">Reunion</option>
				<option value=""141"">Dominicana</option>
				<option value=""300"">Puerto Rico</option>
				<option value=""312"">Ruanda</option>
				<option value=""326"">Rep煤blica Checa</option>
				<option value=""143"">Rumania</option>
				<option value=""144"">Rusia</option>
				<option value=""307"">Sin especificar</option>
				<option value=""418"">Qatar</option>
				<option value=""459"">San Cristobal y Nieves</option>
				<option value=""376"">Senegal</option>
				<option value=""407"">Serbia</option>
				<option value=""152"">Siria</option>
				<option value=""370"">Sierra Leona</option>
				<option value=""375"">Singapur</option>
				<option value=""157"">Suecia</option>
				<option value=""158"">Suiza</option>
				<option value=""381"">Sud谩n</option>
				<option value=""383"">Somal铆a</option>
				<option value=""395"">Sri Lanka</option>
				<option value=""427"">Surinam</option>
				<option value=""465"">Samoa Americana</option>
				<option value=""481"">Sahara Occidental</option>
				<option value=""498"">Santa Luc铆a</option>
				<option value=""501"">San Mart铆n (parte francesa)</option>
				<option value=""516"">Slas Pitcairn</option>
				<option value=""521"">Santa Helena</option>
				<option value=""522"">Svalbard y Jan Mayen</option>
				<option value=""524"">San Marino</option>
				<option value=""525"">San Pedro y Miguelon</option>
				<option value=""526"">Sudan del Sur</option>
				<option value=""528"">Sint Maarten (parte holandesa)</option>
				<option value=""529"">Seychelles</option>
				<option value=""537"">San Vicente y Las Granadinas</option>
				<option value=""167"">Turqu铆a</option>
				<option value=""323"">Taiwan (Rep. China)</option>
				<option value=""387"">Tunez</option>
				<option value=""390"">Tailandia</option>
				<option value=""404"">Trinidad y Tobago</option>
				<option value=""408"">Togo</option>
				<option value=""413"">Tanzania</option>
				<option value=""421"">Turkmenist谩n</option>
				<option value=""429"">Tonga</option>
				<option value=""467"">Tierras Australes y Ant谩rticas</option>
				<option value=""495"">Territorio Britanico del Ocean</option>
				<option value=""532"">Tayikistan</option>
				<option value=""533"">Tokelau</option>
				<option value=""534"">Tuvalu</option>
				<option value=""359"">Ucrania</option>
				<option value=""340"">Vietnam</option>
				<option value=""304"">Rep煤blica Sudafricana</option>
				<option value=""311"">Yugoslavia</option>
				<option value=""315"">U.R.S.S.</option>
				<option value=""396"">Uzbekistan</option>
				<option value=""399"">Uganda</option>
				<option value=""412"">Zimbabwe</option>
				<option value=""415"">Zambia</option>
				<option value=""425"">Timor-Leste</option>
				<option value=""451"">Rep煤blica Centroafricana</option>
				<option value=""458"">Rep煤blica de Kosovo</option>
				<option value=""540"">Vanuatu</option>
				<option value=""301"">Otros pa铆ses de Am茅rica</option>
				<option value=""414"">Cabo Verde</option>
				<option value=""541"">Wallis y Futuna</option>
				<option value=""542"">Union Europea (Com. Economica)</option>
				<option value=""543"">Mercado Comun del Sur(Mercosur</option>
				<option value=""544"">Org. de la Aviacion Civil Inte</option>
				<option value=""545"">Org. de Estados Americanos</option>
				<option value=""546"">Org. de las Naciones Unidas</option>
				<option value=""302"">Otros pa铆ses de Europa</option>
				<option value=""303"">Otros pa铆ses de Asi谩</option>
				<option value=""432"">Botsuana</option>
				<option value=""305"">Otros pa铆ses de Africa</option>
				<option value=""438"">Samoa</option>
				<option value=""306"">Otros pa铆ses de Ocean铆a</option>
				<option value=""441"">Belize</option>
				<option value=""442"">Montenegro</option>
				<option value=""448"">Rep煤blica de Yemen</option>
				<option value=""449"">Rep.Dem.de Sto.Tom茅 y Pr铆ncipe</option>
				<option value=""460"">Yibuti</option>
				<option value=""461"">Reino de Suazilandia</option>
				<option value=""445"">Burundi</option>
				<option value=""999"">Otros Paises</option>

			</select>
                        <span id=""ContentPlaceHolder1_rgValPais"" class=""validator"" style=""color:Red;display:none;"">Indique el Pais donde vive</span>
                    </div>
                    <div id=""ContentPlaceHolder1_PanProvLocSolic"" style=""width:100%;"">
				
                        <div class=""col-lg-3 col-md-3"">
                            <label>
                                Provincia:</label>
                            <select name=""ctl00$ContentPlaceHolder1$ddlProvSolic"" onchange=""javascript:setTimeout('__doPostBack(\'ctl00$ContentPlaceHolder1$ddlProvSolic\',\'\')', 0)"" id=""ContentPlaceHolder1_ddlProvSolic"" class=""form-control input-sm"">
					<option value=""0"">Seleccione</option>
					<option value=""2"">Buenos Aires</option>
					<option selected=""selected"" value=""1"">C.A.B.A.</option>
					<option value=""3"">Catamarca</option>
					<option value=""16"">Chaco</option>
					<option value=""17"">Chubut</option>
					<option value=""4"">C贸rdoba</option>
					<option value=""5"">Corrientes</option>
					<option value=""6"">Entre R铆os</option>
					<option value=""18"">Formosa</option>
					<option value=""7"">Jujuy</option>
					<option value=""19"">La Pampa</option>
					<option value=""8"">La Rioja</option>
					<option value=""9"">Mendoza</option>
					<option value=""20"">Misiones</option>
					<option value=""21"">Neuqu茅n</option>
					<option value=""22"">R铆o Negro</option>
					<option value=""10"">Salta</option>
					<option value=""11"">San Juan</option>
					<option value=""12"">San Luis</option>
					<option value=""23"">Santa Cruz</option>
					<option value=""13"">Santa Fe</option>
					<option value=""14"">Santiago del Estero</option>
					<option value=""24"">Tierra del Fuego</option>
					<option value=""15"">Tucum谩n</option>

				</select>
                            <span id=""ContentPlaceHolder1_rgValProv"" class=""validator"" style=""color:Red;display:none;"">Indique la Provincia donde vive</span>
                        </div>
                        <div class=""col-lg-3 col-md-3"">
                            <label>
                                Localidad:</label>
                            <select name=""ctl00$ContentPlaceHolder1$ddlLocSolic"" id=""ContentPlaceHolder1_ddlLocSolic"" class=""form-control input-sm"">
					<option value=""-1"">Seleccione</option>
					<option selected=""selected"" value=""0"">C.A.B.A.</option>

				</select>
                            <span id=""ContentPlaceHolder1_rgValLoc"" class=""validator"" style=""color:Red;display:none;"">Indique la Localidad donde vive</span>
                        </div>
                    
			</div>
                 </div>
              </div>
             
		</div>
        
	</div>
        <br>
        <p>
            <input type=""submit"" name=""ctl00$ContentPlaceHolder1$btCons"" value=""consultar"" onclick=""BtnClick();WebForm_DoPostBackWithOptions(new WebForm_PostBackOptions(&quot;ctl00$ContentPlaceHolder1$btCons&quot;, &quot;&quot;, true, &quot;&quot;, &quot;&quot;, false, false))"" id=""ContentPlaceHolder1_btCons"" class=""btn btn-primary"">
        </p>
    
</div> 
    
    <p>
        <span id=""ContentPlaceHolder1_lblErrorResult"" style=""color:Red;font-size:X-Large;font-weight:bold;"">Ocurri贸 un error al adherir la Solicitud mediante el servicio de pago Banelco. Por favor, intente nuevamente.<br>Recuerde que debe poseer una cuenta habilitada para operar por PagoMisCuentas. (<a target=""_blank"" href=""https://www.pagomiscuentas.com/como-obtener-mi-clave-y-hacer-un-pago"">www.pagomiscuentas.com</a>)<br><br>Error en banelco - Usuario inexistente(62)<br></span></p>
    
    
    
    <input type=""hidden"" name=""ctl00$ContentPlaceHolder1$hidden_tped_ant"" id=""ContentPlaceHolder1_hidden_tped_ant"">
    <input type=""hidden"" name=""ctl00$ContentPlaceHolder1$hidden_ped_ant"" id=""ContentPlaceHolder1_hidden_ped_ant"">
    <input type=""hidden"" name=""ctl00$ContentPlaceHolder1$hidden_tped_nvo"" id=""ContentPlaceHolder1_hidden_tped_nvo"" value=""P"">
    <input type=""hidden"" name=""ctl00$ContentPlaceHolder1$hidden_ped_nvo"" id=""ContentPlaceHolder1_hidden_ped_nvo"" value=""18151465"">
    <input type=""hidden"" name=""ctl00$ContentPlaceHolder1$hidden_solicitud_nva"" id=""ContentPlaceHolder1_hidden_solicitud_nva"" value=""03106142773"">
    <input type=""hidden"" name=""ctl00$ContentPlaceHolder1$hidden_cod_segur"" id=""ContentPlaceHolder1_hidden_cod_segur"">
    <input type=""hidden"" name=""ctl00$ContentPlaceHolder1$hidden_idFactura_nva"" id=""ContentPlaceHolder1_hidden_idFactura_nva"">
    <input type=""hidden"" name=""ctl00$ContentPlaceHolder1$hidden_via_renaper"" id=""ContentPlaceHolder1_hidden_via_renaper"" value=""N"">
    <!--loading gif-->
    <div class=""loading"">
        <div style=""margin-top: 10%"">
            Procesando datos...
            <br>
            Por favor espere...
            <br>
            <img src=""img/loader.gif"" alt="""">
        </div>
    </div>


                   </div>
                </div>
            </div>
        </section>
        <!--Fin contenido-->

        <!-- ---footer--- -->
        <footer class=""main-footer"">
            <div class=""container"">
                <div class=""row"">
                    <div class=""col-sm-6"">
                        <section class=""block block-block clearfix"" style=""padding-top:0px;"">
                            <img class=""image-responsive"" src=""img/logoRNR_footer.png"" alt="""">
                            <img class=""image-responsive"" src=""img/logoMinisterioFoot.jpg"" alt="""">
                        </section>
                    </div>

                    <div class=""col-md-3 col-sm-6 footerBtns"">
                        <div class=""region region-footer2"">
                            <section id=""block-menu-menu-footer-2"" class=""block block-menu clearfix"">
                                <ul class=""menu nav"">
                                   <li><a title=""Inicio"" href=""Default.aspx"">Nueva solicitud</a></li>
                                </ul>
                            </section>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- ---FIN footer--- -->

    
<script type=""text/javascript"">
//<![CDATA[
var Page_ValidationSummaries =  new Array(document.getElementById(""ContentPlaceHolder1_valRes""));
var Page_Validators =  new Array(document.getElementById(""ContentPlaceHolder1_NDocSolicValid""), document.getElementById(""ContentPlaceHolder1_NDocSolicValidRange""), document.getElementById(""ContentPlaceHolder1_cmpValSexo""), document.getElementById(""ContentPlaceHolder1_cmpValTram""), document.getElementById(""ContentPlaceHolder1_cmpValDia""), document.getElementById(""ContentPlaceHolder1_cmpValMes""), document.getElementById(""ContentPlaceHolder1_cmpValAnio""), document.getElementById(""ContentPlaceHolder1_valFNac""), document.getElementById(""ContentPlaceHolder1_DomValid""), document.getElementById(""ContentPlaceHolder1_DirecSolicValidText""), document.getElementById(""ContentPlaceHolder1_rqValNroTel""), document.getElementById(""ContentPlaceHolder1_cusValNroTel""), document.getElementById(""ContentPlaceHolder1_compBanco""), document.getElementById(""ContentPlaceHolder1_rgValPais""), document.getElementById(""ContentPlaceHolder1_rgValProv""), document.getElementById(""ContentPlaceHolder1_rgValLoc""));
//]]>
</s",,content
2023,Extract a hidden input ?,"Hello friends, I hope I can explain well what I need to do to see if you can help me.

URL: https://demo.imacros.net/Automate/Extract2

Part of the code
Code: Select all<input type=""hidden"" name=""Hidden1"" value=""hiddenvalue1"">

in the html code there is a hidden input and I need the value. VALUE = hiddenvalue1

MY CODE:
Code: Select allVERSION BUILD=1011 RECORDER=CR
URL GOTO=https://demo.imacros.net/Automate/Extract2
TAG POS=1 TYPE=INPUT:HIDDEN FORM=NAME:form1 ATTR=NAME:Hidden1 CONTENT=* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}  


For some unknown reason it can't find it. what do you think is the solution, or where can I read about extraction on hidden input.
SyntaxError: wrong format of TAG command at line 3

Thank you very much, I am attentive to your comments. Greetings.",https://forum.imacros.net/viewtopic.php?f=7&t=32045&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2024,Several images download from URL,"IMACROS VERSION BUILD=12.5.2018.1105
OS : WINDOWS 10 PRO 64BIT
BROWSER : IE 11


Hello.
Always thanks too much for your kind help and support.

I'd like to download 5 images in below URL and I used below tag.
First image download is successful, but from 2nd image download is not working.
How can I download several images? 
Awaiting your answer.

Best regards
Jandi 

My TAG
------------------------------------------------------------------------
VERSION BUILD=12.5.2018.1105
TAB T=1
TAB CLOSEALLOTHERS

URL GOTO=https://www.ikea.com/kr/ko/catalog/products/20391470/
TAG POS=1 TYPE=IMG ATTR=HREF:https:*S5.JPG
ONDOWNLOAD FOLDER=D: FILE=+_image_{{!NOW:yyyymmdd_hhnnss}}  
TAG POS=1 TYPE=IMG ATTR=HREF:https:*S5.JPG CONTENT=EVENT:SAVEITEM

TAG POS=2 TYPE=IMG ATTR=HREF:https:*S5.JPG
ONDOWNLOAD FOLDER=D: FILE=+_image_{{!NOW:yyyymmdd_hhnnss}}  
TAG POS=2 TYPE=IMG ATTR=HREF:https:*S5.JPG CONTENT=EVENT:SAVEITEM

TAG POS=3 TYPE=IMG ATTR=HREF:https:*S5.JPG
ONDOWNLOAD FOLDER=D: FILE=+_image_{{!NOW:yyyymmdd_hhnnss}}  
TAG POS=3 TYPE=IMG ATTR=HREF:https:*S5.JPG CONTENT=EVENT:SAVEITEM

TAG POS=4 TYPE=IMG ATTR=HREF:https:*S5.JPG
ONDOWNLOAD FOLDER=D: FILE=+_image_{{!NOW:yyyymmdd_hhnnss}}  
TAG POS=4 TYPE=IMG ATTR=HREF:https:*S5.JPG CONTENT=EVENT:SAVEITEM

TAG POS=5 TYPE=IMG ATTR=HREF:https:*S5.JPG
ONDOWNLOAD FOLDER=D: FILE=+_image_{{!NOW:yyyymmdd_hhnnss}}  
TAG POS=5 TYPE=IMG ATTR=HREF:https:*S5.JPG CONTENT=EVENT:SAVEITEM
------------------------------------------------------------------------


URL and (part of) HTML
I want to download below 5 picutres. (*S5.JPG)
------------------------------------------------------------------------
https://www.ikea.com/kr/ko/catalog/products/20391470/
...
<img id=""""zoomPopupImg0"""" class=""""productImgSlideshow __web-inspector-hide-shortcut__"""" src=""""/kr/ko/images/products/lumsas-lumsoseu-danmoleogeu-geulei__0621685_PE690048_S5.JPG"""" title="""""""">
...
<img id=""""zoomPopupImg1"""" class=""""productImgSlideshow"""" src=""""/kr/ko/images/products/lumsas-lumsoseu-danmoleogeu-geulei__0621682_PE690047_S5.JPG"""" title="""""""">
...
<img id=""""zoomPopupImg2"""" class=""""productImgSlideshow"""" src=""""/kr/ko/images/products/lumsas-lumsoseu-danmoleogeu-geulei__0621684_PE690049_S5.JPG"""" title="""""""">
...
<img id=""""zoomPopupImg3"""" class=""""productImgSlideshow"""" src=""""/kr/ko/images/products/lumsas-lumsoseu-danmoleogeu-geulei__0621683_PE690050_S5.JPG"""" title="""""""">
...
<img id=""""zoomPopupImg4"""" class=""""productImgSlideshow"""" src=""""/kr/ko/images/products/lumsas-lumsoseu-danmoleogeu-geulei__0622008_PE690237_S5.JPG"""" title="""""""">
...
------------------------------------------------------------------------",https://forum.imacros.net/viewtopic.php?f=7&t=30658&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2025,Scraping text/links from specific web page <div> section to clipboard?,"[EDITED after feedback - sorry for the long post, trying to input all relevant information...]

I've been working on a project for a couple of years (it's a continual thing, so it's never-ending).
I've progressed to a point of using mouse/keyboard macros to scrape a list of text/links from a set of variable-length pages, to paste into Excel; then run an Excel macro to manipulate that data; then return to the webpage, close it & repeat on the next one (I have some error-checking in place in case of a failure).
I do this every 6 months or so.

I am scraping about *75,000 cemetery index pages on http://www.billiongraves.com, copying the names/dates/links of the people interred there, then sorting, filtering and eventually editing errors & merging duplicate records.
*FYI - there are about 600,000 cemetery pages, but I do some data preparation first, extracting only the 75,000 pages with data on them.

Recently, because of some minor site changes and Firefox add-in customizations, the macros that I painstakingly created over time (to pixel-perfect page coordinates, with JitBit Macro Recorder) need to be shifted & changed, which will take me a week or more to do.  It's painful...

I'm thinking of iMacros as an alternative (or as an additional part of the process), as I would LIKE TO do the following, but am not sure it's capable of this.
A typical page has a particular <div><id> section which shows the data I want - it would be ideal if I could select JUST that <div> section and copy all of its contents at once, to the clipboard, which I can then paste into Excel.
*right now, my macro is scrolling & selecting specifically-positioned lines depending on the length of the list...

So I'm looking for this very basic need first, as I can build up more functionality around it later as I learn more about iMacros.

EXAMPLE PAGE: https://billiongraves.com/site-map?ceme ... 295&page=0 - in the Page Source is the section:
Code: Select all	<div id=""content"">
        <h1 style=""margin: 10px 0 25px 10px;"">BillionGraves Site Map</h1>
        <div class=""card"">
            <h1 style=""float:left; margin: 10px 0 10px 10px;"">Burial records in <a href='/cemetery/Bethesda-Cemetery/100295' >Bethesda Cemetery</a></h1>
            <br class=""clearfloat"" />
            <div style=""border-bottom:#CCC thin solid; width:916px;""> </div>

            <div class=""center"">
*******HERE IS THE DIV ID SECTION 'MULTIPLE' WHICH CONTAINS THE DATA I WANT TO COPY*******                <div id=""multiple"">
                    <div class='backlinks'><a href='/site-map'>Sitemap</a> > <a href='/site-map?country=United+States'>United States</a> > <a href='/site-map?country=United+States&state=Tennessee'>Tennessee</a> > <a href='/cemetery/Bethesda-Cemetery/100295'>Bethesda Cemetery</a></div><div><div class='record'><a href='/grave/William-R-Brooks/31780628' alt='Brooks, William R. (1833 - 1864)' title='Brooks, William R. (1833 - 1864)'>Brooks, William R. (1833 - 1864)</a></div><div class='record'><a href='/grave/Nathan-Andrew-Jackson/31709567' alt='Jackson, Nathan Andrew (1838 - 1864)' title='Jackson, Nathan Andrew (1838 - 1864)'>Jackson, Nathan Andrew (1838 - 1864)</a></div><div class='record'><a href='/grave/Josiah-S-Price/31694361' alt='Price, Josiah S (1838 - 1862)' title='Price, Josiah S (1838 - 1862)'>Price, Josiah S (1838 - 1862)</a></div><div class='record'><a href='/grave/Charles-J-Shropshire/31780629' alt='Shropshire, Charles J. (1841 - 1863)' title='Shropshire, Charles J. (1841 - 1863)'>Shropshire, Charles J. (1841 - 1863)</a></div><div class='record'><a href='/grave/William-A-Wingard/31709460' alt='Wingard, William  A. (1839 - 1864)' title='Wingard, William  A. (1839 - 1864)'>Wingard, William  A. (1839 - 1864)</a></div></div><br/><br/>Pages: <span>1</span>&nbsp;                </div>
            </div>
        </div>


My hope is that in selecting 'the entire block' - i.e. the whole <div> section - I can copy all the contents in one shot, rather than having macros scroll to the bottom to capture all the lines, which is not yet working 100% perfectly.

Is iMacros able to select a specific page section (edit: I have found that it can) and copy the hyperlink contents to clipboard or a file?  I see that it can extract content to a CSV file, for example (edit: in the paid version, not the freeware one).  This could work for me (if it creates a 2-column file, with the TEXT and also the LINK - really NEED both!), as I could later combine the CSVs and import to Excel in bulk.
*If the functionality is there to quickly/easily copy a defined <div> section, I'm happy to pay the $99 for the basic version to allow me to SAVEAS a file...

Any help or direction appreciated.


I just installed the (free) Firefox add-in ""iMacros for Firefox"" - v. 10.1.0.1485, on Windows10 Pro-64 (v.19043.1706) with Firefox v100 (64bit).",https://forum.imacros.net/viewtopic.php?f=7&t=31914&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2026,Web Scraping and select Content value based on condition from CSV,"I am using iMacros 8.9.7, Firefox 48.0
 Windows 10 64-bit Operating system

I am using the below code 
Code: Select allVERSION BUILD=8970419 RECORDER=FX
TAB T=1
SET !DATASOURCE 111.csv
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}
TAG POS=3 TYPE=TD ATTR=TXT:* EXTRACT=TXT
wait seconds=1
TAG POS=1 TYPE=A ATTR=TXT:Process
 SET !CLIPBOARD {{!EXTRACT}}
TAG POS=1 TYPE=A ATTR=TXT:Process
wait seconds=0.5
TAG POS=1 TYPE=SELECT FORM=ID:FormProcess ATTR=ID:ddlBLOUpdate CONTENT={{!COL2}}
wait seconds=0.5
TAG POS=1 TYPE=BUTTON FORM=ID:FormProcess ATTR=ID:aERAUD
wait seconds=0.5
TAG POS=4 TYPE=BUTTON ATTR=TXT:脳
wait seconds=0.5


In the CSV file I have two columns, Column 1 is the data which I am extracting usig code 
Code: Select allTAG POS=3 TYPE=TD ATTR=TXT:* EXTRACT=TXT

and in Column 2 I have the data which I want to put in the below code 
Code: Select allTAG POS=1 TYPE=SELECT FORM=ID:FormProcess ATTR=ID:ddlBLOUpdate CONTENT={{!COL1}}

For example in the CSV file I have the data like this 

Column 1 	Column 2 

1		10000
2		50000	
3		70000	
4		80000	
5		35000	

I want that if below code extracts 1 as value from the Web page
Code: Select allTAG POS=3 TYPE=TD ATTR=TXT:* EXTRACT=TXT

then in the below code Content it should select 10000
Code: Select allTAG POS=1 TYPE=SELECT FORM=ID:FormProcess ATTR=ID:ddlBLOUpdate CONTENT={{!COL1}}

Please help me with the code.",https://forum.imacros.net/viewtopic.php?f=7&t=31906&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2027,EXTRACT=HTM successfully but SET VAR doesn't seem to work,"currently using
Code: Select alliMacros for Chrome Version 8.4.4 Free + BRAVE Version 1.16.68 Chromium: 86.0.4240.111 + Win10 64

my full imacro is
Code: Select allTAG POS=1 TYPE=IMG ATTR=SRC:https://attachments.labeling-data.net/61d6064a23f8b40017ec5ede%2Fjpeg%2Fbebd7ce4-3ad6-42b9-9357-75f45e66a4eb?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hdHRhY2htZW50cy5sYWJlbGluZy1kYXRhLm5ldC82MWQ2MDY0YTIzZjhiNDAwMTdlYzVlZGUlMkYqIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjQ2NDYwMDAwfX19XX0_&Signature=etguaFnlGVEe8BNqlQOjhvG1VsGqZn-jLACxMTK-dgno3MhMxMsIwNjdIONRKTOlT2FUIdikIgOEX2YnCeQ5qiAgEbstEciGu730FzheiUHPUBLFbW60ChPCsDk-RqGUi0jqWVrXNxzL2fnAXD1VTT0aVzdtSt-8xAw-C5zY~hJSoTjarQT5Uy1Er8eYKQPDOCkjqsOfQfRdD8Frp6sLaXURFLRUw0fD5u7YP~~LfimJlV1vUKPSMp6hUtwoEHZKP64g7kQO6V~O-mz248xcR4iaFj8xVKLts9RHeaGgND32uzVYuKPclmwPvE3mAOczaP4lyvjr9Jxguqmu2xXo1Q__&Key-Pair-Id=APKAIGOZDNNPITVQK2FQ EXTRACT=HTM
SET !VAR1 EVAL(""if (\""{{!EXTRACT}}\"" == \""https://attachments.labeling-data.net/61d6064a23f8b40017ec5ede%2Fjpeg%2Fbebd7ce4-3ad6-42b9-9357-75f45e66a4eb?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hdHRhY2htZW50cy5sYWJlbGluZy1kYXRhLm5ldC82MWQ2MDY0YTIzZjhiNDAwMTdlYzVlZGUlMkYqIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjQ2NDYwMDAwfX19XX0_&Signature=etguaFnlGVEe8BNqlQOjhvG1VsGqZn-jLACxMTK-dgno3MhMxMsIwNjdIONRKTOlT2FUIdikIgOEX2YnCeQ5qiAgEbstEciGu730FzheiUHPUBLFbW60ChPCsDk-RqGUi0jqWVrXNxzL2fnAXD1VTT0aVzdtSt-8xAw-C5zY~hJSoTjarQT5Uy1Er8eYKQPDOCkjqsOfQfRdD8Frp6sLaXURFLRUw0fD5u7YP~~LfimJlV1vUKPSMp6hUtwoEHZKP64g7kQO6V~O-mz248xcR4iaFj8xVKLts9RHeaGgND32uzVYuKPclmwPvE3mAOczaP4lyvjr9Jxguqmu2xXo1Q__&Key-Pair-Id=APKAIGOZDNNPITVQK2FQ\"") {var s = \""AQUA/WATER/EAU\"";} else {var s = \""\"";} s;"")
'PROMPT !EXTRACT:<SP>{{!EXTRACT}}<br>!VAR2:<SP>{{!VAR2}}
TAG SELECTOR=""HTML>BODY>DIV>DIV:nth-of-type(3)>DIV>DIV>DIV:nth-of-type(2)>DIV:nth-of-type(2)>DIV:nth-of-type(2)>DIV>FORM>DIV>TEXTAREA"" CONTENT={{!VAR1}}
SET !EXTRACT NULL


i successfully extracted my desired image url but my SET VAR doesnt seem to work.
PS. this imacro works for TXT extraction. but i dont know what im missing on HTM extraction and using it in SET VAR.
Code: Select allTAG POS=1 TYPE=A ATTR=TXT:www.bitgear.com.au EXTRACT=TXT
SET !VAR1 EVAL(""if (\""{{!EXTRACT}}\"" == \""www.bitgear.com.au\"") {var s = \""Cryptocurrency\"";} else {var s = \""\"";} s;"")
'PROMPT !EXTRACT:<SP>{{!EXTRACT}}<br>!VAR2:<SP>{{!VAR2}}
TAG POS=2 TYPE=SPAN ATTR=TXT:{{!VAR1}}
SET !EXTRACT NULL
",https://forum.imacros.net/viewtopic.php?f=7&t=31874&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2028,Problem when inserting the word i want to search into a tag,"Hello!
Im trying to automate a search, but when I try to enter the word I want to search into the search box, iMacros doesnt recognize it and doesnt write anything. 
This is the code im using:

TAG POS=1 TYPE=INPUT:TEXT ATTR=CLASS:react-tagsinput-input CONTENT=Hello

is there any way to fix this?",https://forum.imacros.net/viewtopic.php?f=7&t=31890&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2029,Condition based Related Position,"Firefox 52.9.0 (32-bit)
iMacros 8.9.7
Win-10 (64-bit)

Hi,
      I want to open link (ie. select row which is related position value with conditions(ie. Col1 & Col2 column match). I have tried, but i didn't open link.
Kindly guide me..
Code: Select allVERSION BUILD=8970419 RECORDER=FX
TAB T=1
SET !TIMEOUT_STEP 0
SET !ERRORIGNORE YES
SET !DATASOURCE Attend-Edit.csv
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}

'URL GOTO=https://nregade1.nic.in/netnrega/mustrollattend_edit.aspx

'TAG POS=1 TYPE=INPUT:TEXT FORM=ID:form1 ATTR=ID:txtsearch_work CONTENT=368410
'TAG POS=1 TYPE=SELECT FORM=ID:form1 ATTR=ID:ddlworkcode CONTENT=#2
'TAG POS=1 TYPE=SELECT FORM=ID:form1 ATTR=ID:ddlmustroll CONTENT=%29314

TAG POS=1 TYPE=SPAN ATTR=ID:grveditmustroll_ctl*_lblApp_name
SET App_Name EVAL(""'{{!COL1}}'.replace(/\\n/g, '').trim();"")

TAG POS=1 TYPE=SPAN ATTR=ID:grveditmustroll_ctl*_lblRegNo1
SET Reg_No EVAL(""'{{!COL2}}'.replace(/\\n/g, '').trim();"")

TAG POS=R1 TYPE=A ATTR=ID:grveditmustroll_ctl*_LinkButton1&&TXT:*{{App_Name}}*&&TXT:*{{Reg_No}}*

'TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ID:form1 ATTR=ID:All CONTENT=YES
TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ID:form1 ATTR=ID:c1 CONTENT={{!COL4}}
TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ID:form1 ATTR=ID:c2 CONTENT={{!COL5}}
TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ID:form1 ATTR=ID:c3 CONTENT={{!COL6}}
TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ID:form1 ATTR=ID:c4 CONTENT={{!COL7}}
TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ID:form1 ATTR=ID:c5 CONTENT={{!COL8}}
TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ID:form1 ATTR=ID:c6 CONTENT={{!COL9}}
TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ID:form1 ATTR=ID:c7 CONTENT={{!COL10}}

'TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:form1 ATTR=ID:btnSave





Thanks & Regards,
S.Tamilselvan.",https://forum.imacros.net/viewtopic.php?f=7&t=31871&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2031,Extracting from Salesforce webpage,"FCI: Win 7 x32 + FF 49.0.2 (always the latest) + iMacro for FF 9.0.3 (always the latest)

I'm attempting to extract portions of text from a salesforce website and I""m not having any luck figuring out how to accomplish this task.

Here is a snippet of the code from the page that I'm trying to extract data from. ""Member Banks need to journal"" is the text I'm trying to extract. The website is internal to my work domain so a link is not going to be helpful.
Code: Select all</div>
        </td>
        <td class=""efhpCenterContent "">
        <div class=""efhpCenterContentBody"">
            <div class=""efhpCenterTopRow"">
                <span class=""efhpIcon"" style=""background: transparent url(/img/icon/cases16.png) 0 0 no-repeat;""><img title="""" alt="""" src=""/s.gif""/></span>
                <span class=""efhpCenterLabel"">Case Number</span>
                <span class=""efhpCenterValue  ""><span  title=""00056071"">00056071</span></span>
                <span class=""efhpSeparator""> </span>
                <span class=""efhpCenterLabel"">Created Date</span>
                <span class=""efhpCenterValue  "">9/20/2016 12:30 PM</span>
            </div>
            <div class=""efhpTitle"" title=""Member Banks need to journal"">Member Banks need to journal</div>


Here is some sample code that I've been playing with but can't seem to work it out.
I've tried inputting all manner of values for CLASS: [with is not present in the code] and for TYPE.

My hope is that once I see how it is done for this value I'll be able to build a iMarco that will Extract all the values I need into a CSV file. I'll also want to extract efhpCenterValue, efhpCenterLabel, and efhpCenterValue, etc.  Those are within a TD and SPAN so I'm not sure if they are handled differently. XPATH may be the way to go, but I'm not sure how to get the XPATH values.
Code: Select allVERSION BUILD=9030808 RECORDER=FX
TAB T=1
'URL GOTO=https://na33.salesforce.com/5003900001gmeKcAAI?nooverride=1
TAG POS=1 TYPE=DIV ATTR=TXT: EXTRACT=TXT


Thanks in Advance
de Plane
[Boss] de Plane [has landed!]",https://forum.imacros.net/viewtopic.php?f=7&t=26847&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2032,img download,"Hello

Windows 10, Firefox 95.0 (latest)
iMacros evaluation version Personal edition
Code: Select allTAB T=1
SET !DATASOURCE ""db.csv""
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}
SET !ERRORIGNORE YES
 
' Begin Loop

SET !VAR1 ""01-""
URL GOTO={{!COL16}}
TAG POS=1 TYPE=IMG ATTR=CLASS:shrinkToFit
ADD !VAR1 {{!COL6}}
PROMPT {{!VAR1}}

'works, I get the expected string in VAR1

ONDOWNLOAD FOLDER=* FILE=+_{{!VAR1}}
TAG POS=1 TYPE=IMG ATTR=SRC:{{!COL16}} CONTENT=EVENT:SAVEITEM

'does not work, I get a prompt asking me if I want to save 
'the name of the file is the name of the image found at that link, 
'in this case just a bunch of chars without any meaning

WAIT SECONDS=2
 
' End Loop 

What I would like to do is to navigate directly to a image hosted on a server - this part works
Then save the image with a filename chosen by me - this part doesn't work
Without being prompted to click 'Yes' on any dialog box - this part doesn't work

I worked with imacros some years ago and I remember being able to do this, but plenty has changed. 
So, is this still possible? For either the Personal or the Free editions.

Thank you for any pointers,",https://forum.imacros.net/viewtopic.php?f=7&t=31836&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2033,How to add text without deleting previous text in the text box?,"I want to create a macro which will not delete the previous text in the text box. For example, I have created this
Code: Select allVERSION BUILD=1011 RECORDER=CR
TAG POS=1 TYPE=SELECT FORM=ID:frmVakaBilgi ATTR=ID:IstemTipi CONTENT=%1
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:frmVakaBilgi ATTR=ID:LabId CONTENT=陌STANBUL<SP>BEYL陌KD脺Z脺<SP>DEVLET<SP>H

I want to add similar text in the same text box. I have searched but could not find satisfactory result.

Please help.",https://forum.imacros.net/viewtopic.php?f=7&t=31831&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2034,Extract Filtered Table,"Firefox 52.9.0 (32-bit)
iMacros 8.9.7
Win-10 (64-bit)

Hi,
    I am extracting Table data after filtering. It does not get filtered data. its extract Table normally. 

    How to select item one by one in dropdown list... "">"" this symbol does not work...
   Code: Select allTAG POS=1 TYPE=SELECT ATTR=ID:panchayat CONTENT=%>
    Please guide me...
Code: Select allVERSION BUILD=9030808 RECORDER=FX
TAB T=1
SET !TIMEOUT_STEP 0
'URL GOTO=https://bhuvan-app2.nrsc.gov.in/mgnrega/nrega_dashboard_phase2/
TAG POS=1 TYPE=SELECT ATTR=ID:select_central CONTENT=%7
TAG POS=1 TYPE=SELECT ATTR=ID:stage CONTENT=%1
TAG POS=1 TYPE=SELECT ATTR=ID:panchayat CONTENT=%>
wait seconds=.5
TAG POS=1 TYPE=BUTTON ATTR=TXT:Load<SP>The<SP>Report
TAG POS=40 TYPE=SELECT ATTR=TXT:102550100 CONTENT=$100
SET Extract NULL
wait seconds=1

'Filtering Pending
TAG POS=239 TYPE=INPUT:TEXT ATTR=* CONTENT=Pending<SP>for<SP>Geotagging

SET !EXTRACT NULL
TAG POS=2 TYPE=TD ATTR=TXT:2906015* extract = txt

SET Work_Code {{!EXTRACT}}
SET !EXTRACT NULL
SET !ERRORIGNORE YES
TAG POS=R2 TYPE=TD ATTR=TXT:* extract = txt
SET Work_Name {{!EXTRACT}}

SET !EXTRACT {{Work_Code}}[EXTRACT]{{Work_Name}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=BhuvanPh2.csv




",https://forum.imacros.net/viewtopic.php?f=7&t=31787&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2035,Help to extract SRC url and related field.,"Hi,
I have below code
Code: Select allVERSION BUILD=10.4.28.1074
TAB T=1     
TAB CLOSEALLOTHERS  
SET !DATASOURCE Criteria1.csv	
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}
URL GOTO=http://uat.allianceinsurance.in/
TAG POS=1 TYPE=IMG ATTR=SRC:http://uat.allianceinsurance.in/images/product-1.png
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:NameOfCompIds CONTENT=""Alliance Insurance Brokers""
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:ContactPersonIds CONTENT=Saravanan
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:EmailIDs CONTENT=saravanan@all.in
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:MobileNoIds CONTENT=8000000000
TAG POS=1 TYPE=A ATTR=ID:q2-hide
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:VotpId CONTENT=1234
TAG POS=37 TYPE=A ATTR=HREF:http://uat.allianceinsurance.in/EB/EbProduct.aspx#
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:TotalEmpIds CONTENT={{!COL1}}
TAG POS=1 TYPE=SELECT ATTR=ID:ctl00_Main_FamilyCompsnIDs CONTENT=${{!COL5}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:EmpSpouseIds CONTENT={{!COL2}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:EmpChildIds CONTENT={{!COL3}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:EmpParentsIds CONTENT={{!COL4}}
TAG POS=25 TYPE=SPAN ATTR=*
TAG POS=1 TYPE=SELECT ATTR=ID:ctl00_Main_SumInsTypeIDs CONTENT=${{!COL11}}
TAG POS=1 TYPE=INPUT:RADIO ATTR=NAME:ChkBxInptSumIn&&VALUE:{{!COL10}}
TAG POS=40 TYPE=SPAN ATTR=*
WAIT SECONDS=5
TAG POS=1 TYPE=SELECT ATTR=ID:BaseProductAsTxt46 CONTENT=$*{{!COL6}}
WAIT SECONDS=5
TAG POS=1 TYPE=SELECT ATTR=ID:BaseProductAsTxt55 CONTENT=$*{{!COL7}}
WAIT SECONDS=5
TAG POS=1 TYPE=SELECT ATTR=ID:BaseProductAsTxt57 CONTENT=$*{{!COL9}}
WAIT SECONDS=5
TAG POS=1 TYPE=SELECT ATTR=ID:BaseProductAsTxt61 CONTENT=$*{{!COL8}}
WAIT SECONDS=5
'TAG POS=1 TYPE=IMG ATTR=CLASS:""each-insurer"" EXTRACT=HTM
'SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Admin\Documents\iMacros\DataSources FILE=Criteria1.csv
'BACK
''
'TAG POS=1 TYPE=DIV ATTR=ID:""InsurLstShwDiv"" EXTRACT=TXT
'SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Admin\Documents\iMacros\DataSources FILE=Criteria1.csv
'BACK
'Correction
'TAG POS=4 TYPE=IMG ATTR=ID:STYLE:""width:157px;height:45px;background:black;"" EXTRACT=TXT
'SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Admin\Documents\iMacros\DataSources FILE=Criteria1.csv
'BACK
''
'TAG POS=4 TYPE=DIV ATTR=CLASS:""each-insurer-inner"" EXTRACT=TXT
'SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Admin\Documents\iMacros\DataSources FILE=Criteria1.csv
'BACK

'TAG POS=7 TYPE=DIV ATTR=CLASS:each-insurer-heading EXTRACT=HTM
'TAG POS=7 TYPE=DIV ATTR=CLASS:ID:InsurLstShwDiv EXTRACT=HTM
'TAG POS=7 TYPE=DIV ATTR=CLASS:ID:InsurLstShwDiv EXTRACT=HTM
'TAG POS=7 TYPE=IMG ATTR=CLASS:""img-fluid"" EXTRACT=HTM
'TAG POS=7 TYPE=IMG ATTR=SRC:*/Admin/CompLogo/ EXTRACT=HTM
'SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Admin\Documents\iMacros\DataSources FILE=Criteria1.csv
TAG POS=7 TYPE=DIV ATTR=CLASS:""each-insurer-inner"" EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Admin\Documents\iMacros\DataSources FILE=Criteria1.csv
TAG POS=7 TYPE=DIV ATTR=CLASS:""each-insurer-inner"" EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Admin\Documents\iMacros\DataSources FILE=Criteria1.csv

I am new to IMacros. 
My code till line TAG POS=1 TYPE=SELECT ATTR=ID:BaseProductAsTxt61 CONTENT=$*{{!COL8}} works fine.
There are 3 boxes on right hand side with Image name Cigna, ICICI and Edelweiss. The position of these 3 boxes will keep changing every time up and down. Also the number will also change based on the selection.
After line TAG POS=1 TYPE=SELECT ATTR=ID:BaseProductAsTxt61 CONTENT=$*{{!COL8}}  I want to extract that name and amount in same CSV file as mentioned below.

{{!COL11}}  |  {{!COL12}}   |  {{!COL13}}  |  {{!COL14}}  |  {{!COL15}}  |  {{!COL16}} 
CIGNA        |   94162.5      |   ICICI          |  95735         |  Edelweiss   |  99133.2

In {{!COL11}} the name of the top should come and number related to that in {{!COL12}} 
In {{!COL13}} the name of the second should come and number related to that in {{!COL14}} 
In {{!COL15}} the name of the third should come and number related to that in {{!COL16}}

 Also help to loop this code.",https://forum.imacros.net/viewtopic.php?f=7&t=31784&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2036,Unable to download multiple images from website,"I'm having trouble downloading images from a website where I need to run a few thousand records through and download the images that come up.  I'm using:

iMacros Browser (x86) Version 12.5.503.8802
Released on 11/5/2018
Licensed Product: iMacros Professional Edition
Windows 10 home 64 bit

The website is: https://bcpa.net/RecID.asp

I'm entering various parcel numbers and trying to download each of the pictures that come up.  Here are the various parcel numbers I'm testing:  494129-05-4220, 514026-01-0403, 504113-03-1410.  I don't know how to change my script to adjust for the different references for each photo. I know very little about coding and I think this is a simple issue but it's beyond my ability.

Here is my script:

VERSION BUILD=12.5.503.8802
TAB T=1
TAB CLOSEALLOTHERS
'SET !PLAYBACKDELAY 0.43
SET !TIMEOUT_STEP 1
SET !ERRORIGNORE YES
'use a data source
SET !DATASOURCE Broward.csv
'start on the second record
SET !LOOP 2
'use the rest of the data in the data source?
SET !DATASOURCE_LINE {{!LOOP}}
URL GOTO=https://bcpa.net/RecID.asp
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:FOLIO_NUMBER CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:IMAGE ATTR=NAME:submit
TAG POS=1 TYPE=IMG ATTR=BORDER:0&&ALT:Photographs&&SRC:https://bcpa.net/Images/photographs.gif
ONDOWNLOAD FOLDER=C:\Users\trey_\OneDrive\Documents\iMacros\Downloads\Florida\Broward FILE={{!COL1}}
TAG POS=1 TYPE=IMG ATTR=HREF:""https://bcpa.net/Photographs/504113/03/ ... 163228.jpg"" CONTENT=EVENT:SAVEITEM
ONDOWNLOAD FOLDER=C:\Users\trey_\OneDrive\Documents\iMacros\Downloads\Florida\Broward FILE={{!COL1}}_1
TAG POS=1 TYPE=IMG ATTR=HREF:""https://bcpa.net/Photographs/504113/03/ ... 081550.jpg"" CONTENT=EVENT:SAVEITEM
ONDOWNLOAD FOLDER=C:\Users\trey_\OneDrive\Documents\iMacros\Downloads\Florida\Broward FILE={{!COL1}}_2
TAG POS=1 TYPE=IMG ATTR=HREF:""https://bcpa.net/Photographs/504113/03/ ... 083137.jpg"" CONTENT=EVENT:SAVEITEM

Any help would be greatly appreciated.",https://forum.imacros.net/viewtopic.php?f=7&t=31516&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2037,Trouble Extracting Data - no reference position with automatic extractor,"I鈥檓 not a coder and I use imacros as it helps a non-coder like me.  I鈥檓 trying to extract data from a website but I don鈥檛 see a reference position when I use the automatic extractor.  I鈥檓 entering a parcel number into the 鈥淢ap/Block/Parcel鈥?field to get the total amount due.  Specifically I鈥檓 trying to extract the 鈥淭otal Due鈥?amount in red from the table.  What am I missing?  Any help would be greatly appreciated.  

I鈥檓 using:
iMacros 2021.0
Progress Software Corporation
iMacros version 14.2.2.1
Released on 7/31/2021


Website: http://web.florenceco.org/cgi-bin/ta/tax-inq.cgi
Sample parcel numbers to use in the 鈥淢ap/Block/Parcel鈥?field:
80019 02 014
80018 06 009



My script:
VERSION BUILD=2021.0
TAB T=1
TAB CLOSEALLOTHERS
'SET !PLAYBACKDELAY 0.2
SET !TIMEOUT_STEP 1
SET !ERRORIGNORE YES
'use a data source
SET !DATASOURCE Florence21.csv
'start on the second record
SET !LOOP 2
'use the rest of the data in the data source?
SET !DATASOURCE_LINE {{!LOOP}}
URL GOTO=http://web.florenceco.org/cgi-bin/ta/tax-inq.cgi
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:map CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:block CONTENT={{!COL2}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:parcel CONTENT={{!COL3}}
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=*
TAG POS=1 TYPE=FONT ATTR=* EXTRACT=TXT
TAG POS=1 TYPE=FONT ATTR=* EXTRACT=TXT
TAG POS=1 TYPE=FONT ATTR=TXT:""Total Due"" EXTRACT=TXT
TAG POS=1 TYPE=FONT ATTR=* EXTRACT=TXT
'save the extract to the following location
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\trey_\OneDrive\Documents\iMacros\Downloads\SC\Florence FILE=Florence21Extract9-17.csv


This is what I get when I run the script:

Date: Sep 20, 2021	Date: Sep 20, 2021	Total Due	Date: Sep 20, 2021",https://forum.imacros.net/viewtopic.php?f=7&t=31762&sid=ddad572d7e36bd7d9ce3abd083a6d173,content
2039,"SOF: ""Save all HREF's inside LI's in a named UL to a '.txt' File""","""Interesting Thread on SOF, surprisingly ""'Good Quality""... First time I gave a '+1"" I think...! (Or one out of  5 max...!)
https://stackoverflow.com/q/68442789/3799241

=> Opening a Parallel Thread on our Forum, @OP from SOF hasn't reacted so far, and I never had a ""good"" Relationship with that Forum/Site, stupidly Buggy + Monopoly Game about Posting and full of big Ego's controlling the Content... OK, no Comment...!   )

I started posting an ""Answer"" on the Site, but I'm ""afraid"" it will ""disappear"" very soon, either from the User deleting their Qt once the get their Answer/Script working, or some over-zealous ""Cleaning-Bot"" or some over-zealous Mod with 10k+ Rep on the Site who won't like my ""Free Talking"", ah-ah...!
, 
How can I save all href values from list items to a text file with iMacros?

I am a newbie to imacros but have version 12.0.501.6698 installed on the PC.

I am trying to extract from the html of a page all the href values located in multiple list items. Then save those URLs into a text file.

The number of list items can be different so I cannot use a loop of a known number of iterations; I have to grab all the list items and extract the href attribute values.

Example of the format of the html code
Code: Select all<ul class=""bullet-list columns-2 columns--regular"">
<li><a href=""/search/agents/results.htm?location=ampthill"" >Estate Agents in Ampthill</a></li>
<li><a href=""/search/agents/results.htm?location=barton_le_clay"" >Estate Agents in Barton-Le-Clay</a></li>
<li><a href=""/search/agents/results.htm?location=bedford"" >Estate Agents in Bedford</a></li>
<li><a href=""/search/agents/results.htm?location=biggleswade"" >Estate Agents in Biggleswade</a></li>
<li><a href=""/search/agents/results.htm?location=bromham"" >Estate Agents in Bromham</a></li>
<li><a href=""/search/agents/results.htm?location=clapham_beds"" >Estate Agents in Clapham</a></li>
</ul>

I have looked at the code in similar articles such as - iMacros: Extract ID attribute from a ul li list

This is the code I have tried in imacros.
Code: Select allVERSION BUILD=12.0.501.6698
TAB T=1
SET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP NO
TAB CLOSEALLOTHERS
'SET !PLAYBACKDELAY 0.00
URL GOTO=https://www.home.co.uk/search/agents/?county=beds

TAG POS=1 TYPE=UL ATTR=ID:bullet-list EXTRACT=LI
TAG POS=R{{!LOOP}} TYPE=A ATTR=ID:* EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=* FILE=c:\Development\towns.txt

I get an error Box

enter image description here

I have also tried modifying all the permutations for the Values of TYPE and what to EXTRACT.

The text file that should store the URLs from the href attributes in this line:

<li><a href=""/search/agents/results.htm?location=clapham_beds"" >Estate Agents in Clapham</a></li>

just contains a line #EANF# not ""/search/agents/results.htm?location=clapham_beds""

>>>

https://i.stack.imgur.com/lb3OV.png",https://forum.imacros.net/viewtopic.php?f=7&t=31672&sid=117ef7144580822f4982137e21eddfc2,content
2040,How to extract info from tables using CSS Selectors?,"Hi guys, so I am new at this. A few Info:
VERSION BUILD=1010 RECORDER=CR
Chrome Version 90.0.4430.93 64 bits
Windows 10

Also, I am sorry if I say something wrong and I will try to give full information as possible.

1) I am trying  to scraping a table, though could not work with regular TAG POS and now I am using CSS Selectors, though with no success.

2) Attached is the example of the html page I am trying to scrape. Observation: After the successful captcha input.

3) The Captcha killing - this part is working very well. The problem starts selecting tags then extracting at line 27.

Here is the complete macro:
Code: Select allVERSION BUILD=1010 RECORDER=CR
TAB T=1
TAB CLOSEALLOTHERS
'SET !PLAYBACKDELAY 0.00
URL GOTO=https://servicos.receita.fazenda.gov.br/Servicos/cnpjreva/cnpjreva_solicitacao.asp

TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:cnpj CONTENT=00.360.305/0001-04

' Insert your Anti-Captcha API key here
SET antiCaptchaApiKey XXXXXX

' Fetch Anti-Captcha API key in TEXTAREA.g-recaptcha-response element
TAG POS=1 TYPE=TEXTAREA ATTR=CLASS:g-recaptcha-response CONTENT={{antiCaptchaApiKey}}
' Or you can place the API key in DIV#anticaptcha-imacros-account-key, it will also work
'URL GOTO=javascript:(function(){var<SP>d=document.getElementById(""anticaptcha-imacros-account-key"");d||(d=document.createElement(""div""),d.innerHTML=""{{antiCaptchaApiKey}}"",d.style.display=""none"",d.id=""anticaptcha-imacros-account-key"",document.body.appendChild(d))})();
'
' Include recaptcha.js file with all the functional
URL GOTO=javascript:(function(){var<SP>s=document.createElement(""script"");s.src=""https://cdn.antcpt.com/imacros_inclusion/recaptcha.js?""+Math.random();document.body.appendChild(s);})();

' Most important part: we wait 120 seconds until an AntiCatcha indicator 
' with class ""antigate_solver"" gets in additional ""solved"" class
SET !TIMEOUT_STEP 120
TAG POS=1 TYPE=DIV ATTR=CLASS:""*antigate_solver*solved*""

TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=TXT:Consultar

TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(2)>TBODY>TR>TD>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL1}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(2)>TBODY>TR>TD:nth-of-type(3)>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL2}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(3)>TBODY>TR>TD>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL3}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(4)>TBODY>TR>TD>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL4}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(4)>TBODY>TR>TD:nth-of-type(3)>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL5}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(5)>TBODY>TR>TD>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL6}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(6)>TBODY>TR>TD>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL7}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(7)>TBODY>TR>TD>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL8}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(8)>TBODY>TR>TD>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL9}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(8)>TBODY>TR>TD:nth-of-type(3)>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL10}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(8)>TBODY>TR>TD:nth-of-type(5)>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL11}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(9)>TBODY>TR>TD>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL12}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(9)>TBODY>TR>TD:nth-of-type(3)>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL13}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(9)>TBODY>TR>TD:nth-of-type(5)>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL14}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(9)>TBODY>TR>TD:nth-of-type(7)>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL15}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(10)>TBODY>TR>TD:nth-of-type(3)>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL16}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(11)>TBODY>TR>TD>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL17}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(12)>TBODY>TR>TD>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL18}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(12)>TBODY>TR>TD:nth-of-type(3)>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL19}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(13)>TBODY>TR>TD"" EXTRACT=TXT !EXTRACT {{!COL20}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(14)>TBODY>TR>TD>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL21}}
TAG SELECTOR=""HTML>BODY>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>TABLE>TBODY>TR>TD>TABLE:nth-of-type(14)>TBODY>TR>TD:nth-of-type(3)>FONT:nth-of-type(2)>B"" EXTRACT=TXT !EXTRACT {{!COL22}}

SAVEAS TYPE=EXTRACT FOLDER=* FILE=Extract_{{!NOW:ddmmyy_hhnnss}}.csv

So I dont know how to exactly select the correct tags without the ""normal mode"" because they return the same tags for every selection on the table, and I am completely lost using CSS selectors.

How you guys can help me and I am completely all ears to learn a solution.",https://forum.imacros.net/viewtopic.php?f=7&t=31523&sid=117ef7144580822f4982137e21eddfc2,content
2041,Problem with selecting a checkbox next to target text (text has no HTML tags),"Good evening everyone!

I can't solve a problem with iMacros and it is really bugging me, so I would really appreciate if someone could help me. I hope I've opened the topic on the correct board; otherwise, please forgive me, for this is my first post.
I spent hours and hours going through the wiki and this forum, but I cannot reach my goal of selecting checkboxes with the target text next to them on a specific website. So I'm starting to wonder if it's the problem of the HTML code, or if I'm doing something entirely wrong. I will describe the details below.

1. Config Info: 

iMacros for Firefox plugin 8.9.7, free (VERSION BUILD=8970419); browser: Pale Moon 28.9.3; OS: Windows 10 Pro 64-bit English

2. Target site code: 

The text content were replaced with general English terms because they were originally in Hungarian. (Also, ""etc etc"" was written by me in the STYLE section because it was a rather long style section with no real impact on my issue/macro itself.)  
Code: Select all<DIV id=""cuccLayer"" STYLE=""position:absolute; font-weight:bold; (etc etc...); "">
		 <center>Title1</center>
		 <hr>

<input type=""checkbox"" onClick=""cuccArray[0]=1-cuccArray[0];""> 410 pcs. ITEM1<br><input type=""checkbox"" onClick=""cuccArray[1]=1-cuccArray[1];""> 27 pcs. ITEM2<br><input type=""checkbox"" onClick=""cuccArray[2]=1-cuccArray[2];""> 9 pcs. ITEM3<br><input type=""checkbox"" onClick=""cuccArray[3]=1-cuccArray[3];""> 1 pc. ITEM4<br><input type=""checkbox"" onClick=""cuccArray[4]=1-cuccArray[4];""> 89 pcs. ITEM5<br><input type=""checkbox"" onClick=""cuccArray[5]=1-cuccArray[5];""> 23 pcs. ITEM6<br>
		 <hr>
		 <center>
		 <input type=""image"" src="".../SUBMIT.gif"" title=""SUBMIT"" onClick=""document.urlap.par1.value=cuccArrayToString(); document.urlap.Submit.value='svSelejtezes'; document.urlap.submit();"">
		 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		 <input type=""image"" src="".../sample2.gif"" width=""35"" height=""35"" title=""Title2"" onClick=""cuccHide(); return false;"">
		 </center>
</DIV>

3. Problem: 

I'm trying to write a macro that will look for specific text(s) on a site and then select the checkbox on the left side of it. For example, let's say I want to select the checkboxes to the left of the texts ""pcs.<SP>ITEM3"", ""pcs.<SP>ITEM4"" and ""pcs.<SP>ITEM5"". 

However, the checkboxes and the text are in a shared DIV container called ""cuccLayer"" and the texts don't have their own HTML tags and because of that, I fail to use relative positioning, even though I went over a lot of chivracq's posts dealing with the topic. I simply cannot come up with a solution, because all the macros I wrote (even after tweaking with POS=R1,R-1, etc) seem to handle all the text as a whole (verified by extraction in test mode) and I cannot position the checkboxes. My last idea before giving up and posting here was using relative positioning with the <br>tag, with no success (maybe TAG cannot be used with empty tags lacking an end tag, or simply <br> is not recognized?) 

The position/order of the checkboxes + texts are not static but PHP/JS-related, so the checkbox must be selected based on the current text following it (for more info, see: end of section 4).

4. Context, additional info about the target site and my macro: 

A sudden wave of nostalgia hit me, and I started to play again with a 19 year old Hungarian browser-based online RPG which I really enjoyed as a teen. Currently, I'm progressing with the quests. However, because only a few die-hard fans play the game anymore, making money is quite difficult because of a lack of ingame economy, so the only solution left is ""farming"" gold (meaning doing repetitive tasks to get items that I can sell in the game's own shop itself so I can generate gold without the need to trade with other players on the ingame marketplace).

I already succesfully wrote several ""farming"" macros that combine both JavaScript and iMacros (this is the main reason I'm using an older build with Pale Moon, due to the need for JS support). This means that the macro automates an ingame activity called mining, and my player character automatically mines ores with various value to sell. However, after a while, my inventory overflows with heavy and cheap ores, taking up much space from the ligher and more expensive ones, such as diamonds etc. When this occurs, a new interface comes in, telling me that I'm carrying too much stuff and I need to throw away something (this currently requires manual intervention on my part, meaning my macro is not fully automated). My target code provided in section 2 is this problematic interface which lists all the amount (""x pcs.""), and items (""ITEM3, ITEM4 and ITEM5"") in my bag and ask me to select with the checkbox what I want to throw away with a submit form. ""ITEM3, ITEM4 and ITEM5"" are the heavy and cheap stuff I want to automatically dispose of, so my macro can run in a loop and keep only the expensive items.

Also, because this is a PHP/JS-based online game, the array of items in my inventory are generated by the game and therefore dynamic, so I guess the onClick=""cuccArray[x]=1-cuccArray[x]; attribute of the checkbox I want to select cannot really be predicted/automated, because if the items in my inventory change, the array can be numbered completely differently.

5. Closing thoughts

I deliberately chose not to include any of my macros I experimented with, because I tried so many combinations that I wouldn't even know which one to write here. So if this seems as lazy to anyone, please forgive me. Thank you for everyone who went through my post, I would really appreciate some help. Even if my goal cannot be achieved because of the HTML structure, I would like to know it, because otherwise I will keep trying and failing. It might have started out as silly nostalgia but now the idea of me not being to solve the macro is bugging me even more than my love for that old browser RPG. I also really hope that the solution is not something really easy because I would feel very dumb (as I've mentioned, I've already succesfully did scripts and macros for other parts of this game, so I can use iMacros on at least a basic level).",https://forum.imacros.net/viewtopic.php?f=7&t=31442&sid=117ef7144580822f4982137e21eddfc2,content
2042,TAG POS=R1 gives the wrong result. Why?,"I USE IMACROS V12.5.503 BROWSER AND WIN 10 PRO X64
hi 
this code:
Code: Select all'next line worked
TAG POS=1 TYPE=H4 ATTR=TXT:""DOGE""&&CLASS:m-2 EXTRACT=TXT
'next line not working
TAG POS=R1 TYPE=H4 ATTR=TXT:""0.000000000000000""&&CLASS:cloud-percent CONTENT=EVENT:FAIL_IF_FOUND

I tried to write a code that if see this number""0.000000000000000""(in doge box), the macro give me -1310 error but According to the photo, at present the number inside the Dodge Box is not equal to the searched number(TXT:""0.000000000000000"").
but macro gives me an error code -1310. i use EXTRACT:TXT instead of CONTENT=EVENT:FAIL_IF_FOUND to find that element and I found that it returns the number 0.000000000000000, which is inside the next box. Why?
If I remove TXT:""0.000000000000000"" from the second line of code, the second line of code points to the number inside the Dodge box, and if I return it, it points to the number inside the next box!!!!!!
thanks",https://forum.imacros.net/viewtopic.php?f=7&t=31422&sid=117ef7144580822f4982137e21eddfc2,content
2043,Extract Captcha text  to Login,"Firefox 52.9.0 (32-bit)
iMacros 8.9.7
Win-10 (64-bit)

Hi,
     I am trying to Login page but struggled in Captcha / Security Code. I can only extracted Captcha / Security as image but not  text. I request anyone to help. 
Code: Select allURL GOTO=http://nregade4.nic.in/netnrega/MGNREGA_new/Nrega_Login.aspx?salogin=Y&state_code=25
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_ddl_FinYr CONTENT=%2020
SET !ENCRYPTION NO
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_txt_UserID CONTENT=29720579357
'Type Password
TAG POS=1 TYPE=INPUT:PASSWORD FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_txt_PW CONTENT=
PAUSE
TAG POS=1 TYPE=IMG ATTR=SRC:http://nregade4.nic.in/netnrega/CaptchaImage/JpegImage.aspx CONTENT=EVENT:SAVE_ELEMENT_SCREENSHOT
PAUSE
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_txt_Captcha CONTENT=
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_Button1





Thanks & Regards,
S. Tamilselvan.",https://forum.imacros.net/viewtopic.php?f=7&t=31277&sid=117ef7144580822f4982137e21eddfc2,content
2044,using css selector for imacros with conditions,"iMacros: v12
Browser: Internet Explorer 11
OS: Windows 10

Hi... want to know if conditional css selector works with iMacros? I want to run a web scraper. The below selector is working with Web Scrape chrome extension, but it doesn't work with iMacros.

Selector:
Code: Select alldiv.s-expand-height:has(span.a-price.a-text-price), .celwidget div.s-item-container:has(span.a-price.a-text-price), div.s-include-content-margin:has(span.a-price.a-text-price)


I tried this with iMacros in below formats, but not working

Format 1
Code: Select allTAG POS={{!LOOP}} TYPE=DIV ATTR=CLASS:""s-expand-height:has(span a-price.a-text-price), celwidget s-item-container:has(span.a-price.a-text-price), s-include-content-margin:has(span.a-price.a-text-price)"" EXTRACT=TXT

Format 2
Code: Select allTAG POS={{!LOOP}} TYPE=DIV ATTR=CLASS:""div.s-expand-height:has(span.a-price.a-text-price), .celwidget div.s-item-container:has(span.a-price.a-text-price), div.s-include-content-margin:has(span.a-price.a-text-price)"" EXTRACT=TXT

My complete iMacros script looks like this.
Code: Select allSET !DATASOURCE E:\imacros\urllist1.csv
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}

URL GOTO={{!COL1}}
WAIT SECONDS={{!COL2}}

TAG POS={{!LOOP}} TYPE=DIV ATTR=CLASS:""s-expand-height:has(span.a-price.a-text-price), .celwidget s-item-container:has(span.a-price.a-text-price), s-include-content-margin:has(span.a-price.a-text-price)"" EXTRACT=TXT
ADD !EXTRACT {{!URLCURRENT}}

'TAG POS={{!LOOP}} TYPE=DIV ATTR=CLASS:""s-expand-height s-include-content-margin s-border-bottom s-latency-cf-section"" EXTRACT=TXT

SAVEAS TYPE=EXTRACT FOLDER=E:\imacros FILE=data.csv",https://forum.imacros.net/viewtopic.php?f=7&t=31311&sid=117ef7144580822f4982137e21eddfc2,content
2046,Extract Table & Append,"Firefox 52.9.0 (32-bit)
iMacros 8.9.7
Win-10 (64-bit)
Hi,
      I have voucher number using this, I  extract the table from website and I need to append table in loop but i could not extract as table format. Where i have to change in my code, please guide me...
Code: Select allVERSION BUILD=8970419 RECORDER=FX
TAB T=1
SET !TIMEOUT_STEP 0
SET !DATASOURCE Vr_No.csv
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}
'URL GOTO=http://mnregaweb4.nic.in/netnrega/homesearch.htm
FRAME F=1
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:form1 ATTR=ID:txt_keyword2 CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:form1 ATTR=ID:btn_go
TAB T=2

'Extract text

TAG POS=2 TYPE=TABLE ATTR=TXT:* EXTRACT=TXT

'Remove quotes from extracted text
SET TAB EVAL(""var extr2=\""{{!EXTRACT}}\""; extr2.replace(/'/g,'');"")
'Save extracted data
SAVEAS TYPE=EXTRACT FOLDER=* FILE=VrTable.csv









",https://forum.imacros.net/viewtopic.php?f=7&t=31305&sid=117ef7144580822f4982137e21eddfc2,content
2047,How to extract nested informations using XPATH?,"iMacros ver: 10.0.2.1450 (FREE), Firefox, Windows 10

Hello,
The objective is to extract the value of HTML DOM Property such as id,href and data-download-file-url for each of the images displayed from this [website][https://www.freepik.com/search?dates=an ... rt=popular]. I believe XPATH will be suitable for this task as each of the image can be accessed by the following generalise XPATH Code: Select all/html/body/main/section[2]/div/div/figure[X]/div

with the capital X indicate the Image label that take the value from 1 to 50, for the aforementioned website.

I know that, to extract the properties of Figure 1, for example, can be achieved by
Code: Select allTAG XPATH=""/html/body/main/section[2]/div/div/figure[1]""  EXTRACT=TXT

However, the line above outputted all DOM Property including the one that I am not interested with.

According to the tutorial  below;

[OP1][https://forum.imacros.net/viewtopic.php?t=26155]
[OP2][https://stackoverflow.com/questions/385 ... cros-xpath]

Extracting specific DOM property can be achieved by something like the following
Code: Select allTAG XPATH=""/html/body/main/section[2]/div/div/figure[1]/div[@id='showcase__content'] ""  EXTRACT=TXT

However, the execution instead give an error.

I really appreciate if someone can shed some light about this problem.",https://forum.imacros.net/viewtopic.php?f=7&t=30959&sid=117ef7144580822f4982137e21eddfc2,content
2048,Exit script if account not found and move to next,"Greetings,

I have a VB script that runs an ""open account"" imacro that pulls up accounts in a web portal and then runs another imacro script to extract various fields. It works well enough but the problem is, if the account in question is not found, it continues to run the extract script. What I would like it to do is abort/exit if the account can not be pulled up and continue to the next account while entering ""not found"" beside the offending entry. Thank you!

My setup details are: 

Microsoft Windows 10 Home
64-bit Operating System
Installed UI Culture: English (Canada)
Internet Explorer version 9.11.18362.0
iMacros version 8.0.3.2216 (don't hate me)

The scripts I am using look like this:

Open Account imacro
set !TIMEOUT_STEP 1
SET !ERRORIGNORE YES

FRAME NAME=main

'INITIAL SCREEN 
TAG POS=1 TYPE=NOBR FORM=NAME:form1 ATTR=TXT:Lookup
 TAG POS=1 TYPE=NOBR FORM=NAME:frmPrimary ATTR=TXT:Lookup 
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Lookup ATTR=NAME:txtEstateNumber CONTENT={{Acct#}}
TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:Lookup ATTR=NAME:btnSearch
' click claim button/OPEN ACCOUNT
TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:Lookup ATTR=NAME:dgPostSetup$ctl02$ClaimCode

Get Account Details

set !TIMEOUT_STEP 1
SET !ERRORIGNORE YES
FRAME NAME=main
'set !replayspeed medium

'Bank's Name - THIS IS ALSO THE LINK TO DETAILS SCREEN
TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:frmPrimary ATTR=NAME:dgParty$ctl02$bthParty EXTRACT = TXT
'BANK TYPE
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmPrimary ATTR=NAME:txtBankruptcyType EXTRACT = TXT
'Trustee 
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmPrimary ATTR=NAME:txtTrusteeName EXTRACT = TXT
'CONTACT NAME DROPDOWN
TAG POS=1 TYPE=SELECT FORM=NAME:frmPrimary ATTR=NAME:ddlContact  EXTRACT = TXT
'REQUEST STATUS DROPDOWN
TAG POS=1 TYPE=SELECT FORM=NAME:frmPrimary ATTR=NAME:ddlRequest EXTRACT = TXT
'Status 
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmPrimary ATTR=NAME:txtStatus EXTRACT = TXT
'Status Date
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmPrimary ATTR=NAME:txtStatusDatebox EXTRACT = TXT
'Status Days
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmPrimary ATTR=NAME:txtStatDay EXTRACT = TXT
'Urgent note! 
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmPrimary ATTR=NAME:txtFlag1 EXTRACT = TXT
'Instruction Date
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmPrimary ATTR=NAME:txtInstructDatebox EXTRACT = TXT
'Instruction Days
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmPrimary ATTR=NAME:txtInsDay EXTRACT = TXT

'========GET ACCOUNT DETAILS MENU CONTROL 2============
'TAGS NOT WORKING, HAVE TO USE DIRECT SCREEN
'SIZE X=1071 Y=837
DS CMD=CLICK X=356 Y=21
WAIT SECONDS=1
DS CMD=CLICK X=355 Y=65
'=================

FRAME NAME=main
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmAccountInfo ATTR=NAME:ctl04$txtBankruptBalance EXTRACT = TXT
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmAccountInfo ATTR=NAME:ctl04$txtBankruptDate EXTRACT = TXT
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmAccountInfo ATTR=NAME:ctl04$txtOutstanding EXTRACT = TXT
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmAccountInfo ATTR=NAME:ctl04$txtLoanNum EXTRACT = TXT
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmAccountInfo ATTR=NAME:ctl04$txtOtherNum EXTRACT = TXT
TAG POS=1 TYPE=SELECT FORM=NAME:frmAccountInfo ATTR=NAME:ctl04$ddlProduct EXTRACT = TXT
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:frmAccountInfo ATTR=NAME:ctl04$txtOwing EXTRACT = TXT
TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:frmAccountInfo ATTR=NAME:btnExit

VB script

Code: Select allSub LOGIN_GET_FILE_DETAILS()
    Dim iim1, iret, row, totalrows, iim, FirstRow, c, xx
    Dim oSh As Worksheet
    Set oSh = ActiveSheet
    Set iim1 = CreateObject(""imacros"")
    iret = iim1.iimInit
    
    iim1.iimPlay (""D:\OneDrive\iMacros\Login.iim"")

    DoEvents
    
     FirstRow = ActiveSheet.Range(""p1"")
    totalrows = ActiveSheet.Range(""q1"")

   For row = FirstRow To totalrows
    c = 13 ' c = column start number
    x = 1 ' x = extract#
    xx = 1 ' loop counter for saving every 100 loops
    
     iret = iim1.iimSet(""Acct#"", Cells(row, 3).value)
     iret = iim1.iimPlay(""D:\OneDrive\iMacros\Teranet\TeranetOpenAccount.iim"")
     iret = iim1.iimPlay(""D:\OneDrive\iMacros\Teranet\Teranet_GET_ALL_DETAILS.iim"")
    
     'iret = iim1.iimSet(""-tray"", """") 'iim1.iimSet(""-silent"", """")
    
     Cells(row, c).value = Replace(iim1.iimGetLastExtract(x), ""[EXTRACT]"", """")
     c = c + 1
     x = x + 1
     Cells(row, c).value = Replace(iim1.iimGetLastExtract(x), ""[EXTRACT]"", """")
     c = c + 1
     x = x + 1
 

    Next row
    
 oSh.ListObjects(1).DataBodyRange.WrapText = False

Application.ScreenUpdating = True
Application.EnableEvents = True

  
End Sub",https://forum.imacros.net/viewtopic.php?f=7&t=31162&sid=117ef7144580822f4982137e21eddfc2,content
2049,Calculation of extracted values,"Hi i found this topic [url]viewtopic.php?f=7&t=29657[/url ]interesting and try to figure out a calucalation based macro myself.

I am using VERSION BUILD=10021450 in FF
Bowser FF 79.0 
iMacros for FF
Windows 10 Professional 64-bit Operating system
Using the free version currently 

Currently my code is following:
Code: Select allVERSION BUILD=10021450
TAB T=1
TAG POS=1 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
SET !VAR1 EVAL(""var s='{{!EXTRACT}}'; var z=s*1.05; z;"")
PROMPT {{!VAR1}}

TAB T=2  
'URL GOTO=http://demo.imacros.net/Automate/AutoDataEntry
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:demo ATTR=ID:fname CONTENT={{!VAR1}}

What i am trying to reach is get the price of an item in step one and add  +5% via the EVAL command to the price before i paste it into another field.
However something seems to be wrong with my code. It just extracts the price and does not modify nor paste it into the field. I made sure it only extracts numbers (Format: XX,XXX) and nothing else.
Any idea on this?

In another step i would also like to divide the price through a fixed number. Example would be to calculate how many items i can purchase for a fixed amount i have in my wallet. Would this worK?
Code: Select allVERSION BUILD=10021450
TAB T=1
TAG POS=1 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
SET !VAR1 EVAL(""var s='{{!EXTRACT}}'; var z=s/1000; z;"")
PROMPT {{!VAR1}}

TAB T=2  
'URL GOTO=http://demo.imacros.net/Automate/AutoDataEntry
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:demo ATTR=ID:fname CONTENT={{!VAR1}}",https://forum.imacros.net/viewtopic.php?f=7&t=31166&sid=117ef7144580822f4982137e21eddfc2,content
2050,!Extract,"So far i have made this 

SET !EXTRACT_TEST_POPUP NO
SET !ERRORIGNORE YES
TAG POS=1 TYPE=BUTTON ATTR=TXT:Cancel<SP>this<SP>order
SET !ERRORIGNORE NO
SET !TIMEOUT_STEP 1
SET !TIMEOUT_PAGE 400
TAG POS=11 TYPE=TD ATTR=TD: EXTRACT=TXT
TAG POS=1 TYPE=BUTTON ATTR=TXT:Place<SP>buy<SP>order
TAG POS=1 TYPE=INPUT:NUMBER ATTR=ID:Amount CONTENT=1
TAG POS=1 TYPE=INPUT:NUMBER ATTR=ID:Price CONTENT=""this is where i want the extracted value to be entered with +0.01""

so what i want to achieve is input the extracted txt from POS 11 and input it in the POS=1 ATTR=ID:Price but with addition of 0.01, so for example if the extracted txt is 70 then i want it to input 70.01
can this be done ? 
i'm very new to this sort of things, it took me 2 hrs to make what i posted above but i cant seem to figure out the final bit",https://forum.imacros.net/viewtopic.php?f=7&t=29657&sid=117ef7144580822f4982137e21eddfc2,content
2052,Trying to redo loop if EANF,"Hi guys, first post,  trying to self teach iim and its going pretty well but i need to redo some of my web scraping and im a bit stumped.   Heres the raw code,  I use it to enter 2 variables and then press go and i want to scrape the output  The issue is My internet is rotating Ip every so often and if it times up just right when it rotates then the output is EANF,  for some reason if i get too many of those then iim just locks up and it doesn't work anymore,  I like to set this up to run all night and set it to run slow so it never messes up.  I think what i need to do though is something along the lines of

If Extract = #EANF# then redo this loop using same variables

SAVEAS TYPE=EXTRACT FOLDER=C:\Users\hirea\Downloads FILE=Output.csv   

This is the line that extracts the output and occasionally cant find the expected Anchor,  I want some sort of test at this point in the loop that says if #EANF# then go back up to   ""TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:""first Variable"" CONTENT={{!COL1}}""

Heres my config
Windows 10 Pro Version 1909 English
imacros Browser V12.6.505.4525   have license but was using Internet explorer plugin and not sure how to migrate liscense
Demos seem to work
Not sure how to test VBS scripting interface
get the same issue in IE explorer and imacros browser  The issue is my cycline IP kills the internet for a few seconds and inevitably my iim is trying to scrap data at that exact moment

VERSION BUILD=12.5.2018.1105
'
URL GOTO=""the website im scraping""
SET !DATASOURCE C:\Users\hirea\Desktop\""the file with the 2 inputs""
SET !LOOP 1
'Increase the current position in the file with each loop 
SET !DATASOURCE_LINE {{!LOOP}}
SET !WAITPAGECOMPLETE no
TAB T=1
SET !ERRORIGNORE YES
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:""first Variable"" CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:""2nd Variable"" CONTENT={{!COL2}}
WAIT SECONDS=2
TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=TXT:""Give me the output""
ADD !EXTRACT {{!COL1}}
ADD !EXTRACT {{!COL2}}
WAIT SECONDS=2
TAG POS=1 TYPE=DIV ATTR=CLASS:""output"" EXTRACT=TXT
WAIT SECONDS=2
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\hirea\Downloads FILE=Output.csv
URL GOTO=""the website im scraping""
WAIT SECONDS=6",https://forum.imacros.net/viewtopic.php?f=7&t=31150&sid=117ef7144580822f4982137e21eddfc2,content
2053,extract and fill type select from form,"Imacros browser v12.0.501.6698 and
imacros chrome 83.0.4103.116  (64 bit)
Windows 10 (64-bit)
All demo macros work fine

Code: Select allTAB T=1
FRAME F=1
TAG POS=1 TYPE=SELECT ATTR=NAME:kat1 CONTENT=%169v
TAB T=2
FRAME F=0
TAG POS=1 TYPE=SELECT ATTR=ID:_006608869979543186 CONTENT=%550 

these are my imacros codes. i wanna extract  selected category from tab 1 and fill to tab2. for example,if i selected ""tarih"" from tab 1 i want to fill to tab2 select ""tarih"".but 2 tabs have different value. these are cods from 2 sites.


tab 1 codes
Code: Select all<select name=""kat1"" size=""20"" onchange=""subkat()"" class=""urun_kutu2"">
<option value=""43v"">Bilim ve Teknik &gt;&gt;</option>
<option value=""350v"">脟izgi Roman &gt;&gt;</option>
<option value=""59v"">脟ocuk Kitaplar谋 &gt;&gt;</option>
<option value=""19v"">Dergiler &gt;&gt;</option>
<option value=""193v"">Di臒er &amp; 脟e艧itli &gt;&gt;</option>
<option value=""65v"">Din &gt;&gt;</option>
<option value=""77v"">Edebiyat &gt;&gt;</option>
<option value=""109v"">Ekonomi &amp; 陌艧 D眉nyas谋 &gt;&gt;</option>
<option value=""117v"">Felsefe &amp; D眉艧眉nce &gt;&gt;</option>
<option value=""124v"">Hukuk &gt;&gt;</option>
<option value=""3v"">Osmanl谋ca &gt;&gt;</option>
<option value=""132v"">Referans &amp; Ba艧vuru &gt;&gt;</option>
<option value=""141v"">Sa臒l谋k &gt;&gt;</option>
<option value=""150v"">Sanat &gt;&gt;</option>
<option value=""102v"">S谋nav ve Ders Kitaplar谋 &gt;&gt;</option>
<option value=""163v"">Spor &gt;&gt;</option>
<option value=""169v"">Tarih &gt;&gt;</option>
<option value=""184v"">Toplum &amp; Siyaset &gt;&gt;</option><option value=""0"">-------------------------------</option>
</select>


tab 2 codes
Code: Select all<select class=""column add-product-category-listboxes"" id=""_05083001410408683"" size=""22"">
<option value=""271"">Edebiyat, Kurgu &gt;</option>
<option value=""550"">Tarih &gt;</option>
<option value=""502"">Sosyal Bilimler &gt;</option>
<option value=""599"">Osmanl谋ca Nadir Kitaplar &gt;</option>
<option value=""417"">S眉reli Yay谋nlar &gt;</option>
<option value=""190"">Bilim, Teknik, Ara艧t谋rma &gt;</option>
<option value=""493"">Hukuk &gt;</option>
<option value=""590"">陌艧 ve Ekonomi &gt;</option>
<option value=""182"">Ba艧vuru Kaynaklar谋 &gt;</option>
<option value=""237"">脟izgi Roman &gt;</option>
<option value=""230"">脟ocuk Kitaplar谋 &gt;</option>
<option value=""242"">Dil ve Filoloji &gt;</option><option value=""478"">Din ve Teoloji &gt;</option>
<option value=""335"">Harita ve Atlas &gt;</option>
<option value=""339"">Mahalli/Yerel Kitaplar &gt;</option>
<option value=""239"">Mizah, E臒lence ve Oyun &gt;</option>
<option value=""342"">脰zel 陌lgi Alanlar谋 &gt;</option>
<option value=""363"">Sanat &gt;</option>
<option value=""410"">Spor &gt;</option>
<option value=""1176"">Kitap Aksesuarlar谋 &gt;</option>
<option value=""600"">Di臒er Kitaplar &gt;</option>
<option value=""392"">S谋nav ve Ders Kitaplar谋 &gt;</option></select>",https://forum.imacros.net/viewtopic.php?f=7&t=31114&sid=117ef7144580822f4982137e21eddfc2,content
2054,Selecting a specific range of my pinterest followers to follow back,"Imacros Paid Personal Edition Version 10.0.2.1450 Last Updated June 20, 2020
Firefox 77.0.1 (64-bit)
Windows 10 (64-bit)
All demo macros work fine


I have tried relentlessly over the past week to adjust my script so it only scrapes followers that themselves have at least a certain amount of followers. One of my accounts get spammed persistent and I simply need to filter out the noise from real meaningful followers

The following script does in fact work quite well down only to one problem, the loop. It runs fine all the way to the extract tag position and than instead of grabbing the actual tag of the current loop it somehow takes the next one instead. I even utilized the great idea about the loop-switch posted in the forum to jump out if a condition is not met

What it currently does is to filter out all my followers that have themselves at least 400+ followers and writes it into the csv file. Perhaps CONTENT=MOUSEOVER is wrong but I am not sure.

PROBLEM 1: the first TAG POS is working correctly, however further down the TAG POS should still be in the same LOOP iteration but it is not, it is taking the next found TAG instead.

PROBLEM 2: SAVEAS EXTRACT does not write the extract each into a new line inside the csv file but simply all into one line.

I would greatly appreciate some help to hopefully get it solved as I am out of ideas on this one. Thank you


Code: Select allVERSION BUILD=10021450
SET !VAR0 400
TAB T=1
url goto=javascript:window.scrollBy(0,0)
SET !EXTRACT_TEST_POPUP NO
SET !ERRORIGNORE YES
SET !TIMEOUT_STEP 0
SET !TIMEOUT_MACRO 10

TAG POS={{!LOOP}} TYPE=DIV ATTR=class:""tBJ dyH iFc yTZ pBj tg7 IZT swG"" EXTRACT=TXT

SET !VAR1 EVAL(""var x=\""{{!EXTRACT}}\""; x=x.replace(/\\D+/g,\""|\"");"")

SET !VAR2 EVAL(""var s='{{!VAR1}}'; var x,y,z; x=s.split('|'); y=x[1]; z=y.trim(); z;"")

SET !EXTRACT NULL
SET LOOP_Switch EVAL(""var s=\""{{!VAR2}}\"", d = parseFloat(s); if(d < {{!VAR0}}){n=1;} else{n='0';}; n;"")
SET !TIMEOUT_STEP {{LOOP_Switch}}

TAG POS={{!LOOP}} TYPE=DIV ATTR=data-test-id:""user-rep"" CONTENT=MOUSEOVER
TAG POS=R1 TYPE=A ATTR=* EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\private\Documents\iMacros\DataSources\ FILE=i-am-following.csv
",https://forum.imacros.net/viewtopic.php?f=7&t=31092&sid=117ef7144580822f4982137e21eddfc2,content
2055,exporting data from powerBI - Server,"Hello team,
hope you are doing great.
i'm trying to download data automatically from powerBI server.
i run this script but usually stops at this step 
Code: Select allTAG POS=1 TYPE=BUTTON ATTR=ARIA-EXPANDED:false&&ARIA-HASPOPUP:true&&CLASS:vcMenuBtn&&TYPE:button&&ARIA-LABEL:More<SP>options&&PBI-FOCUS-TRACKER-IDX:1&&TXT:
all other parts of script are working well except this one.
the issue is that there is a dropdown menue button that contains export option. but the script stops at this step
here is the code after credentials
Code: Select allTAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:i0281 ATTR=ID:idSIButton9
wait seconds = 20
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:/kmsi ATTR=ID:idSIButton9
wait seconds = 20
TAG POS=3 TYPE=BUTTON ATTR=_NGCONTENT-WME-C23:&&CLASS:groupToggle<SP>forceButtonVisible<SP>pbi-focus-outline<SP>ng-star-inserted&&TITLE:Show<SP>the<SP>navigation<SP>pane&&ARIA-EXPANDED:false&&TXT:
wait seconds = 20
TAG POS=1 TYPE=SPAN ATTR=TXT:export_data
wait seconds = 20
TAG POS=1 TYPE=BUTTON ATTR=ARIA-EXPANDED:false&&ARIA-HASPOPUP:true&&CLASS:vcMenuBtn&&TYPE:button&&ARIA-LABEL:More<SP>options&&PBI-FOCUS-TRACKER-IDX:1&&TXT:
wait seconds = 20
TAG POS=1 TYPE=H6 ATTR=ID:contextmenu-label-text3
wait seconds = 20
ONDOWNLOAD FOLDER=* FILE=+_{{!NOW:yyyymmdd_hhnnss}} WAIT=YES
wait seconds = 20
TAG POS=2 TYPE=BUTTON ATTR=TXT:Export
wait seconds = 20
TAG POS=1 TYPE=BUTTON ATTR=ID:powerBIUserInfoBtn
wait seconds = 20
TAG POS=1 TYPE=A ATTR=TXT:Sign<SP>out
wait seconds = 20

i googled it a lot but couldn't reach a solution so i came here to Imacros experts to help me
thanks in advance",https://forum.imacros.net/viewtopic.php?f=7&t=31070&sid=117ef7144580822f4982137e21eddfc2,content
2056,Extract part of an URL and modify this,"Hi, i am newby yet using scripts...  i am trying to extract a part of an URL and modify like this example:
- generic website openned with macro function TAG, example: genericsite . com/task/www. genericsite2 .com/user/
- i want extract only user part: /user/
- after all this i want add: www. genericsite2 .com/ before user
final result: www. genericsite2 .com/user/

I am running this code on imacros build 10021450, firefox 76, windows 10 x64 pro ENG.

The code that i am using is:

Code: Select allCLEAR
SET !EXTRACT_TEST_POPUP NO
SET !ERRORIGNORE YES
SET !REPLAYSPEED FAST
SET !TIMEOUT_PAGE 60
SET !TIMEOUT_STEP 0
SET !LOOP -9999999
SET !ENCRYPTION NO


'----------ID DA CONTA----------------
SET !VAR1 35372878524
'----------A脟脙O A EXECUTAR------------
SET !VAR2 1
'-------------------------------------


TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=https://websitegeneric.com.br/painel/conectar
WAIT SECONDS=2
TAG POS=1 TYPE=SELECT ATTR=ID:conta_id CONTENT=%{{!VAR1}}
TAG POS=1 TYPE=SELECT ATTR=ID:acao_id CONTENT=%{{!VAR2}}
WAIT SECONDS=4
TAG POS=1 TYPE=P ATTR=TXT:Ver<SP>link
WAIT SECONDS=4

'HERE I NEED EXTRACT URL AND MODIFY URL

TAB T=2
WAIT SECONDS=3
TAG POS=1 TYPE=BUTTON ATTR=TXT:Seguir
TAG POS=1 TYPE=BUTTON ATTR=TXT:Follow
WAIT SECONDS=4
TAB T=1
TAB CLOSEALLOTHERS
WAIT SECONDS = 2
TAG POS=1 TYPE=BUTTON ATTR=TXT:Confirmar
WAIT SECONDS=180
",https://forum.imacros.net/viewtopic.php?f=7&t=31024&sid=117ef7144580822f4982137e21eddfc2,content
2057,Extraction of Table from a web page,"I am using Bowser Firefox 56.0
iMacros for Firefox 8.9.7
Windows 7 Professional 64-bit Operating system

I am using the below code to extract data from a website
Code: Select allVERSION BUILD=8970419 RECORDER=FX
'Uses a Windows script to submit several datasets to a website, e. g. for filling an online database
TAB T=1     
' Specify input file (if !COL variables are used, IIM automatically assume a CSV format of the input file
'CSV = Comma Separated Values in each line of the file
SET !DATASOURCE TEST.csv

'SET !DATASOURCE_COLUMNS 2
'Start at line 2 to skip the header in the file
SET !LOOP 2
'Increase the current position in the file with each loop 
SET !DATASOURCE_LINE {{!LOOP}}
SET !EXTRACT_TEST_POPUP NO
SET My_Data EVAL(""var s='{{!EXTRACT}}'; var x,y,z; z=s.split('[EXTRACT]').join('<BR>'); z;"")
TAG POS=1 TYPE=LEGEND FORM=ID:aspnetForm ATTR=TXT:Select<SP>Location<SP>for<SP>RoR
TAG POS=1 TYPE=TD ATTR=TXT:District
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_ddlDistrict CONTENT=%5
WAIT SECONDS=3
TAG POS=1 TYPE=TD ATTR=TXT:Tahasil
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_ddlTahsil CONTENT=%4
WAIT SECONDS=3
TAG POS=1 TYPE=TD ATTR=TXT:Village
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_ddlVillage CONTENT=%62
WAIT SECONDS=3
TAG POS=1 TYPE=SPAN ATTR=ID:ctl00_ContentPlaceHolder1_lblColumnName
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_ddlBindData CONTENT=%{{!COL1}}
WAIT SECONDS=1
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_btnRORFront
'TAG POS=1 TYPE=DIV ATTR=TXT:Schedule<SP>I<SP>Form<SP>No.39-A
'TAG POS=1 TYPE=TD ATTR=TXT:喱ム喱ㄠ<SP>喱ㄠ喹嵿喱?SP>:<SP>""149""
'Anchor:
TAG POS=1 TYPE=TD ATTR=TXT:喱溹喱苦喱距喱權瓖喱?SP>喱ㄠ喱?SP>喱?SP>喱栢瓏喹编喱?SP>喱<SP>喱栢喱苦瓱喱距喱?SP>喱曕瓖喱班喱苦瑫*
'TAG POS=1 TYPE=TD ATTR=TXT:1)<SP>喱栢喱苦瓱喱距喱?SP>喱曕瓖喱班喱苦瑫<SP>喱ㄠ喹嵿喱?SET !EXTRACT NULL
TAG POS=R3 TYPE=TD ATTR=TXT:* EXTRACT=TXT
SET My_Data {{!EXTRACT}}
'TAG POS=1 TYPE=TD ATTR=TXT:1
SET !EXTRACT NULL
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
ADD My_Data {{!EXTRACT}}
'TAG POS=1 TYPE=TD ATTR=TXT:2)<SP>喱瓖喱班瑴喱距<SP>喱ㄠ喱?<SP>喱喱む喱?SP>喱ㄠ喱?<SP>喱溹喱む<SP>喱?SP>喱喱膏喹嵿*
SET !EXTRACT NULL
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
ADD My_Data {{!EXTRACT}}
'TAG POS=1 TYPE=TD ATTR=TXT:3)<SP>喱膏瓖喱掂喹嵿
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
ADD My_Data {{!EXTRACT}}
'TAG POS=1 TYPE=TD ATTR=TXT:喱膏瓖喱ム喱む喱喱?SET !EXTRACT NULL
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
ADD My_Data {{!EXTRACT}}
'TAG POS=1 TYPE=TD ATTR=TXT:喱膏瓖喱ム喱む喱喱?SET !EXTRACT NULL
TAG POS=1 TYPE=SPAN ATTR=ID:gvfront_ctl02_lblSpecialCase EXTRACT=TXT
ADD My_Data {{!EXTRACT}}    
'PROMPT {{!EXTRACT}}
'PROMPT {{My_Data}}
SET !CLIPBOARD {{My_Data}}
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:form1 ATTR=ID:btnBackPg
TAG POS=2 TYPE=TABLE ATTR=* EXTRACT=TXT
SET TABLE EVAL(""'{{!EXTRACT}}'.trim()"")
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:form1 ATTR=ID:btnKhatiyan
TAG POS=1 TYPE=TD ATTR=TXT:District

Up to this portion I am using the code to copy the front web page data
Code: Select all     TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:form1 ATTR=ID:btnBackPg

and after this I am using the code to copy the table in the backside of the page
Code: Select all TAG POS=2 TYPE=TABLE ATTR=* EXTRACT=TXT
        SET TABLE EVAL(""'{{!EXTRACT}}'.trim()"")
        
       TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:form1 ATTR=ID:btnKhatiyan
        TAG POS=1 TYPE=TD ATTR=TXT:District

But I am able to extract only front page data, and the code after clicking the Back button doesn't work, and when I am using the only code 
Code: Select all TAG POS=2 TYPE=TABLE ATTR=* EXTRACT=TXT
        SET TABLE EVAL(""'{{!EXTRACT}}'.trim()"")

It extracts the data but not in table format.



 table 


I want to extract the marked data after the columns showing 7 8 9 10 11 12 till the end of page. And for some pages there are 100's of rows are there in a table to extract. What should be the change in the code ?",https://forum.imacros.net/viewtopic.php?f=7&t=30995&sid=117ef7144580822f4982137e21eddfc2,content
2059,Extract with relative Positioning,"This is my first post please if there is any Mistake please ignore and solve that problem and dont disapproved it dear moderator   


Hello i want where the *** come imacros just pick the next word which come between <sp><sp> and paste it to the next line 2. this is my 3 line query can any one help me please


the correct script


VERSION BUILD=1005 RECORDER=CR
line 1 TAG POS=1 TYPE=TD ATTR=TXT:***<SP>mak<SP>joined<SP>#Gupshupcorner
line 2 TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:https://www.gupshupcorner.com ATTR=* CONTENT=/privmsg<SP>mak<SP>hi
line 3 TAG POS=1 TYPE=DIV ATTR=TXT:Send

as i want

VERSION BUILD=1005 RECORDER=CR
line 1 TAG POS=1 TYPE=TD ATTR=TXT:***<SP>mak<SP>joined<SP>#Gupshupcorner
line 2 TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:https://www.gupshupcorner.com ATTR=* CONTENT=/privmsg<SP>name<SP>hi
line 3 TAG POS=1 TYPE=DIV ATTR=TXT:Send


as i want in the second line on the name of mak it must take what ever name is come on mak or you can say what ever word come after *** and betweeen space it just pick that and paste it to the /privmsg<sp>name<sp>


please help 
WINDOWS 10
GOOGLE CHROME Version 80.0.3987.149 (Official Build) (64-bit)
Imacros Version=10.0.5",https://forum.imacros.net/viewtopic.php?f=7&t=30934&sid=a8b6b8836ef4b006067cba7caee672f3,content
2060,Scrape data to different columns,"Hey! 
i need to scrape this and other Products from different sites but what i need is to scrape the name of the Product (TAG POS=4 TYPE=DIV ATTR=CLASS:cellContent) into first column first row and then in the second column i need the price(TAG POS=1 TYPE=SPAN ATTR=ID:planSelectionPrice-1s  ) to be. Then second Product second rown first coulumn and second price second row second column etcetc. please help i have been on this for a week now! it needs to be saved into csv-file.

Produkt1  Price1
Produkt2  Price2

Code: Select allVERSION BUILD=11.0.246.4051
SET !EXTRACT_TEST_POPUP NO
SET !ERRORIGNORE YES
TAB T=1
TAB CLOSEALLOTHERS
1
URL GOTO=https://www.netflix.com/signup?locale=sv-SE
TAG POS=1 TYPE=BUTTON ATTR=TXT:VISA<SP>ABONNEMANG
WAIT SECONDS=1

*first Product and price

TAG POS=4 TYPE=DIV ATTR=CLASS:cellContent 
TAG POS=1 TYPE=SPAN ATTR=ID:planSelectionPrice-1s  

*second Product and price :

TAG POS=5 TYPE=DIV ATTR=CLASS:cellContent  
TAG POS=1 TYPE=SPAN ATTR=ID:planSelectionPrice-2s",https://forum.imacros.net/viewtopic.php?f=7&t=30243&sid=a8b6b8836ef4b006067cba7caee672f3,content
2061,Problems with copying content of page,"Hello 

I have a problem with copying and pasting content of a webpage. 
The problem I am having is that it just copies the link and not the content on that page. Am I doing something wrong? 
Code: Select allMy script:
VERSION BUILD=1005 RECORDER=CR
URL GOTO=https://kultoical.dries007.net/
TAG POS=1 TYPE=INPUT:DATE FORM=ACTION:https://kultoical.dries007.net/ ATTR=ID:start CONTENT=2019-11-01
TAG POS=1 TYPE=INPUT:DATE FORM=ACTION:https://kultoical.dries007.net/ ATTR=ID:end CONTENT=2020-01-31
TAG POS=1 TYPE=BUTTON FORM=ACTION:https://kultoical.dries007.net/ ATTR=TXT:Next
TAG POS=1 TYPE=A ATTR=TXT:https://webwsp.aps.kuleuven.be/sap/opu/odata/sap/zc_ep_sched* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
TAB T=2
TAB T=1
TAG POS=1 TYPE=TEXTAREA FORM=ACTION:https://kultoical.dries007.net/ ATTR=ID:json CONTENT={{!VAR1}}

Thanks (Happy to provide more information)",https://forum.imacros.net/viewtopic.php?f=7&t=30701&sid=a8b6b8836ef4b006067cba7caee672f3,content
2062,Issues with EXTRACT (Using Xero software),"1. What version of iMacros are you using?
VERSION BUILD=1005

2. What operating system are you using? (please also specify language)
Windows 10

3. Which browser(s) are you using? (include version numbers)
Version 78.0.3904.97 (Official Build) (64-bit)

Hi all,

Hoping you can help. VERY new to this iMacros so forgive my ignorance. I am trying to run a loop of updating bills on the Xero accounting software and have hit an issue with copy/paste text using EXTRACT function. My code (or portion of it ) is below;
Code: Select allTAG POS=1 TYPE=A ATTR=ID:addNewLineItemButton
TAG POS=1 TYPE=DIV ATTR=ID:ext-gen60
TAG POS=1 TYPE=IMG ATTR=ID:ext-gen63
TAG POS=1 TYPE=DIV ATTR=TXT:011:<SP>Daily<SP>food<SP>sales

TAG POS=1 TYPE=DIV ATTR=TXT:* EXTRACT=TXT
SET !VAR2 {{!EXTRACT}}
PROMPT {{!VAR2}}
TAG POS=3 TYPE=DIV ATTR=TXT:1.00
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:frmMain ATTR=ID:ext-comp-1005 CONTENT={{!VAR2}}
SET !EXTRACT NULL#

I am having an issue specifially with the line
Code: Select allTAG POS=1 TYPE=DIV ATTR=TXT:* EXTRACT=TXT


When I prompt to see what !VAR2 is, the returned text string is almost like it has done a crtl-A of the whole webpage. Am I doing something obviously wrong? Does Xero have stange things imbedded into their text strings that is messing me around? 

In short, I am trying to copy one cell of data to another. It is always a number string. Never text.

Thanks for your help",https://forum.imacros.net/viewtopic.php?f=7&t=30684&sid=a8b6b8836ef4b006067cba7caee672f3,content
2063,Playback Not Working,"Hello,

I have an issue where the macro seems to have recording with no issues but when I play it back the page does seem to recognize that the information was entered. As a result there is nothing to extract and so it fails.

I am using iMacros 11.5.4498.2403

The browser is in iMacros. Not sure if it uses Chrome or IE or what.

All other macros I have created work with no issues.

VERSION BUILD=11.5.498.2403
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
URL GOTO=https://www.ups.com/track?loc=en_US&requester=ST/
TAG POS=1 TYPE=BUTTON ATTR=ID:stApp_btn_refTrack
TAG POS=1 TYPE=LABEL ATTR=TXT:Freight
TAG POS=2 TYPE=INPUT:RADIO ATTR=NAME:trkShipmentType
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:trkShipmentReference CONTENT=714234
TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=ID:stApp_trkRefTrkBtn
TAG POS=1 TYPE=SPAN ATTR=ID:stApp_lblTrackingNumber EXTRACT=TXT


That is the macro that I recorded with no issues but when I replay it the error on the screen says that the reference number I entered.. 714234 isn't there and therefor the following page doesn't open for the extract portion to run.

Please let me know if there is any more information I can provide.

Thanks...

Harry.",https://forum.imacros.net/viewtopic.php?f=7&t=30638&sid=a8b6b8836ef4b006067cba7caee672f3,content
2064,Unable to Automate Image Download Using Wizard and Other Methods,"I'm having an issue downloading pictures from some websites that have real estate information.  I'm using a little trick chivracq taught me by going directly to the webpage for particular properties I'm searching to extract data and download images.  When I try to download the pictures using the wizard I always get an error.  It looks like there is specific detail in the images that I am not able to code around.  I've tried everything I can think of (direct mode, event mode, screenshots, etc. but however these images are stored on the website is keeping me from being able to download them.

Here is the website:  https://qpublic.schneidercorp.com/Appli ... ageID=7209

I'm using:
iMacros for IE 30-day Enterprise trial version VERSION BUILD=12.5.2018.1105 Windows 10 home 64bit ie version 11.950.17134.0


Here is my script:

VERSION BUILD=12.5.2018.1105
TAB T=1
TAB CLOSEALLOTHERS
TAB T=1
TAB CLOSE
TAB T=1
'SET !PLAYBACKDELAY 1.01
'use a data source
SET !DATASOURCE Jessamine.csv
'start on the second record
SET !LOOP 2
'use the rest of the data in the data source?
SET !DATASOURCE_LINE {{!LOOP}}
SET !VAR1 {{!COL1}}
URL GOTO=https://qpublic.schneidercorp.com/Appli ... ue={{!COL1}}
SET !TIMEOUT_STEP 1
SET !ERRORIGNORE YES
TAG POS=1 TYPE=SPAN ATTR=ID:ctlBodyPane_ctl00_ctl01_lblLocationAddress EXTRACT=TXT
TAG POS=4 TYPE=TD ATTR=CLASS:value-column EXTRACT=TXT
ONDOWNLOAD FOLDER=C:\Users\trey_\OneDrive\Documents\iMacros\Downloads\Jessamine FILE={{!COL1}}
TAG POS=1 TYPE=IMG FORM=ACTION:./Application.aspx?AppID=864&LayerID=16311&PageTypeID=4&PageID=7211&Q=1260569653&KeyValue=006-20-00-010.00 ATTR=HREF:""https://venturi.blob.core.windows.net/f ... scd=inline"" CONTENT=EVENT:SAVEITEM
'save the extract to the following location
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\trey_\OneDrive\Documents\iMacros\Downloads\Jessamine FILE=JessamineExtract


Here are three sample parcel numbers with pictures I'm trying to download:

006-20-00-010.00
010-00-00-028.00
010-00-00-058.00

Any help on how to download these would be much appreciated.",https://forum.imacros.net/viewtopic.php?f=7&t=30567&sid=a8b6b8836ef4b006067cba7caee672f3,content
2065,Error 1100 - Wrong format of TAG,"Let me start by saying I'm not familiar with coding at all.  I used Imacros 15 years ago but have basically forgotten how to use it.  

I'm trying to input property parcel numbers into a county website and extract property information.  I have hundreds of parcel numbers I need to search and I have a CSV file with one column at inputs.  When I run my macro, i get the following error:  

Error -1100: Wrong format of TAG TYPE=INPUT:TEXT FORM=Listing ATTR=NAME:p.parcelid CONTENT={{!COL}}  command, at line: 12

Here is my script:

VERSION BUILD=12.5.2018.1105
TAB T=1
TAB CLOSEALLOTHERS
'SET !PLAYBACKDELAY 0.00
URL GOTO=http://sc-beaufort-county.governmax.com ... 1E9228FFCD
FRAME NAME=body
TAG POS=1 TYPE=A ATTR=TXT:""Real Property""
TAG POS=1 TYPE=A ATTR=TXT:""Property ID (PIN)""
SET !DATASOURCE Beaufort_Test.csv
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}
TAG TYPE=INPUT:TEXT FORM=Listing ATTR=NAME:p.parcelid CONTENT={{!COL1}} 
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:go
TAG POS=8 TYPE=FONT ATTR=COLOR:black EXTRACT=TXT
TAG POS=1 TYPE=A ATTR=TXT:""Property ID (PIN)""

Can anyone help me diagnose the problem?

Thanks!",https://forum.imacros.net/viewtopic.php?f=7&t=30518&sid=a8b6b8836ef4b006067cba7caee672f3,content
2066,Help Extract email,"Hello
I need to extract emails from a site
And save all emails in an Excel file.
The number of emails per page of this site is 15.
Then the imacros is redirected to the second page after 3 seconds to save the subsequent emails
Please write me the imacros code
Extract emails after <p class=""info"">.
Some of the codes are as follows:



Code: Select all<div id=""search-result"" class=""frame hover-actions "">
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/c745442fc9fc7839cd913b673353064d?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/3842506086"">user1@email.com</a>

    
  </h1>

  <p class=""info"">
    user1@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">





  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/367dce6a6d42f8702d449b98ff73f587?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/3842487986"">user2@email.com</a>

    
  </h1>

  <p class=""info"">
    user2@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">





  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/5f47d2c7c45ba7706635da8f5c7d50b9?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/5634494986"">user3@email.com</a>

    
  </h1>

  <p class=""info"">
    user3@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">





  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/7828680b2f5424039386d63fb384e9aa?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/3263723266"">user4@email.com</a>

    
  </h1>

  <p class=""info"">
    user4@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">





  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/dd352b5b5e33d7aee7fbac3c8ba41738?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""https://site.site.com/agent/#/users/3246135903"" target=""_blank"">user5@email.com</a>

    
  </h1>

  <p class=""info"">
    user5@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">





  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/0ba144e374a1543256b2aaa1e1c9286a?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/380045625671"">user6@email.com</a>

    
      at <a class=""light"" href=""https://site.site.com/agent/#/organizations/34944089"" target=""_blank"">TEST</a>
      
  </h1>

  <p class=""info"">
    user6@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">

      <span class=""user_action assume"">
        <a title=""Temporarily sign in as this user"" data-method=""post"" href=""/users/380045625671/assume"">assume</a>
      </span>


      <span class=""user_action"">
        <a class=""edit_this"" rel=""edit"" href=""/users/380045625671/edit?return_to=%2Fusers%3Fcommit%3DSearch%26page%3D1%26query%3site.com%26utf8%3D%25E2%259C%2593"">edit</a>
      </span>


  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/8f41e7677c1e7653bd77587e6b64ee36?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/377593813672"">user7@email.com</a>

    
      at <a class=""light"" href=""/organizations/34944089"">TEST</a>
      
  </h1>

  <p class=""info"">
    user7@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">

      <span class=""user_action assume"">
        <a title=""Temporarily sign in as this user"" data-method=""post"" href=""/users/377593813672/assume"">assume</a>
      </span>


      <span class=""user_action"">
        <a class=""edit_this"" rel=""edit"" href=""/users/377593813672/edit?return_to=%2Fusers%3Fcommit%3DSearch%26page%3D1%26query%3Dsite%26utf8%3D%25E2%259C%2593"">edit</a>
      </span>


  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/2c4aaa73c642f2e4244541d2850033a5?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/375872386811"">user8@email.com</a>

    
      at <a class=""light"" href=""/organizations/34944089"">TEST</a>
      
  </h1>

  <p class=""info"">
    user8@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">

      <span class=""user_action assume"">
        <a title=""Temporarily sign in as this user"" data-method=""post"" href=""/users/375872386811/assume"">assume</a>
      </span>


      <span class=""user_action"">
        <a class=""edit_this"" rel=""edit"" href=""/users/375872386811/edit?return_to=%2Fusers%3Fcommit%3DSearch%26page%3D1%26query%3Dsite.com%26utf8%3D%25E2%259C%2593"">edit</a>
      </span>


  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/8fcf6ea45d677b29944b387edcc6063a?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/374736531751"">user9@email.com</a>

    
      at <a class=""light"" href=""/organizations/34944089"">TEST</a>
      
  </h1>

  <p class=""info"">
    user9@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">

      <span class=""user_action assume"">
        <a title=""Temporarily sign in as this user"" data-method=""post"" href=""/users/374736531751/assume"">assume</a>
      </span>


      <span class=""user_action"">
        <a class=""edit_this"" rel=""edit"" href=""/users/374736531751/edit?return_to=%2Fusers%3Fcommit%3DSearch%26page%3D1%26query%3Dsite.com%26utf8%3D%25E2%259C%2593"">edit</a>
      </span>


  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/ee8ebb189d4c0c8cdd3bf189f679b97b?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/372962629991"">user10@email.com</a>

    
      at <a class=""light"" href=""/organizations/34944089"">TEST</a>
      
  </h1>

  <p class=""info"">
    user10@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">

      <span class=""user_action assume"">
        <a title=""Temporarily sign in as this user"" data-method=""post"" href=""/users/372962629991/assume"">assume</a>
      </span>


      <span class=""user_action"">
        <a class=""edit_this"" rel=""edit"" href=""/users/372962629991/edit?return_to=%2Fusers%3Fcommit%3DSearch%26page%3D1%26query%3Dsite.com%26utf8%3D%25E2%259C%2593"">edit</a>
      </span>


  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/cc87788e4abef60ee00eb22545df04f3?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/367857607852"">user11@email.com</a>

    
      at <a class=""light"" href=""/organizations/34944089"">TEST</a>
      
  </h1>

  <p class=""info"">
    user11@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">

      <span class=""user_action assume"">
        <a title=""Temporarily sign in as this user"" data-method=""post"" href=""/users/367857607852/assume"">assume</a>
      </span>


      <span class=""user_action"">
        <a class=""edit_this"" rel=""edit"" href=""/users/367857607852/edit?return_to=%2Fusers%3Fcommit%3DSearch%26page%3D1%26query%3Dsite.com%26utf8%3D%25E2%259C%2593"">edit</a>
      </span>


  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/3d9c5896554ecdf24e389c77976d0b6c?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/367771491352"">user12@email.com</a>

    
      at <a class=""light"" href=""/organizations/34944089"">TEST</a>
      
  </h1>

  <p class=""info"">
    user12@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">

      <span class=""user_action assume"">
        <a title=""Temporarily sign in as this user"" data-method=""post"" href=""/users/367771491352/assume"">assume</a>
      </span>


      <span class=""user_action"">
        <a class=""edit_this"" rel=""edit"" href=""/users/367771491352/edit?return_to=%2Fusers%3Fcommit%3DSearch%26page%3D1%26query%3site.com%26utf8%3D%25E2%259C%2593"">edit</a>
      </span>


  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/807ade03aa3487d63824692f239464aa?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/366101821851"">user13@email.com</a>

    
      at <a class=""light"" href=""/organizations/34944089"">TEST</a>
      
  </h1>

  <p class=""info"">
    user13@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">

      <span class=""user_action assume"">
        <a title=""Temporarily sign in as this user"" data-method=""post"" href=""/users/366101821851/assume"">assume</a>
      </span>


      <span class=""user_action"">
        <a class=""edit_this"" rel=""edit"" href=""/users/366101821851/edit?return_to=%2Fusers%3Fcommit%3DSearch%26page%3D1%26query%3Dsite.com%26utf8%3D%25E2%259C%2593"">edit</a>
      </span>


  </div>

      </div>
      <div class=""item search-result-user "">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/1279e228cbf8c5d35c16240c4f189ffb?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/361974883931"">user14@email.com</a>

    
      at <a class=""light"" href=""/organizations/34944089"">TEST</a>
      
  </h1>

  <p class=""info"">
    user14@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">

      <span class=""user_action assume"">
        <a title=""Temporarily sign in as this user"" data-method=""post"" href=""/users/361974883931/assume"">assume</a>
      </span>


      <span class=""user_action"">
        <a class=""edit_this"" rel=""edit"" href=""/users/361974883931/edit?return_to=%2Fusers%3Fcommit%3DSearch%26page%3D1%26query%3Dsite.com%26utf8%3D%25E2%259C%2593"">edit</a>
      </span>


  </div>

      </div>
      <div class=""item search-result-user nobottom"">
          <div class=""icon bdr"">
  <img src=""https://secure.site.com/avatar/236827602fadb0918104b3bd295d47dd?size=31&amp;default=https%3A%2F%2Fassets.site.com%2Fimages%2F2016%2Fdefault-avatar-80.png&amp;r=g"" alt=""User"" class=""user-thumb"">
  <div>
      
  </div>
</div>

<div class=""item-info"">
  <h1>
    <a href=""/users/360378749343"">user15@email.com</a>

    
      at <a class=""light"" href=""/organizations/34944089"">TEST</a>
      
  </h1>

  <p class=""info"">
    user15@email.com
    (unverified)
  </p>

</div>


  <div class=""user_actions"">

      <span class=""user_action assume"">
        <a title=""Temporarily sign in as this user"" data-method=""post"" href=""/users/360378749343/assume"">assume</a>
      </span>


      <span class=""user_action"">
        <a class=""edit_this"" rel=""edit"" href=""/users/360378749343/edit?return_to=%2Fusers%3Fcommit%3DSearch%26page%3D1%26query%3Dsite.com%26utf8%3D%25E2%259C%2593"">edit</a>
      </span>


  </div>

      </div>
  </div>
  
  <a class=""next_page"" rel=""next"" href=""/users?commit=Search&amp;page=2&amp;query=ingramcontent.com&amp;utf8=%E2%9C%93"">Next 禄</a>
  
  
  



Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=30511&sid=a8b6b8836ef4b006067cba7caee672f3,content
2067,Scrape with smart XPath,"Usually i scrape with a tool, which understands ""smart"" XPath - means, if i set scraper to scrape XPath Code: Select all//div[@class='example-class'], so i get content from all occurences of this class. I really need this, because the site i scrape has different amount of this class occurences - from zero to 10.

If i use this kind of expression with iMacros, i get only the first occurence of this class. How should i setup Xpath in iMacros to get all occurences of the class, independently of the amount?",https://forum.imacros.net/viewtopic.php?f=7&t=30361&sid=a8b6b8836ef4b006067cba7caee672f3,content
2068,"Meta name Description, Keywords...to extract","Hello,

Good application this IMacro!
I was so far able to get most of what I needed to automate but I am stuck for this one. 

I need to extract the Meta name tags in the headers web page: Description, keyword, location...ETC.
I was able to extract the title and the URL of the visited site and save to a CSV file. So far so good for a new cummer!

Here is an example of the tags in the header:
Code: Select all<meta name=""Description"" content=""Description I would like to extract"">
<meta name=""Keywords"" content=""Keywords I would like to extract"">


I have tried several ways, but none worked so far. I got wrong format for the tag or the ""#EANF#"" error code on any of the trials I did.

I did look on the Forum and found some related posts, but no solutions were given, even if possible.

Please help!
Many thanks",https://forum.imacros.net/viewtopic.php?f=7&t=7327&sid=a8b6b8836ef4b006067cba7caee672f3,content
2069,Add leading zero with Eval Command,"Good day all.

I'm new with iMacros and this is the first matter I have.
Can you please take a look at my code below. I've tried to use EVAL command but the demo/example is too complicated.
I only know code similar to VB, not JS, HTML ... so please kindly understand.
Code: Select allVERSION BUILD=12.0.501.6698
TAB T=1
TAB CLOSEALLOTHERS
'SET !PLAYBACKDELAY 0.00

SET !DATASOURCE C:\\Users\\User\\Desktop\\001.csv
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}} 

URL GOTO=file://MasterPC/Report.html
TAG POS=2 TYPE=INPUT:TEXT ATTR=CLASS:txtNormal_1 CONTENT={{!COL1}}


The case is as below.
The 001.csv contains some columns to fill to Report.html.
1. Is that possible in any case to keep the leading zero in CSV file so after the file is saved from Excel, iMacros still can read the correct one, as 001 but not 1 as now.
2. How can I put command to check/revise the COL1 so if it detects COL1 < 10, to put COL1 as 01 (then 001 to be similar).

Im using Windows 10 / iMacro Editor 12.0.0.151 / Excel 2017 / IE11.
Thanks for your reading.",https://forum.imacros.net/viewtopic.php?f=7&t=30217&sid=a8b6b8836ef4b006067cba7caee672f3,content
2070,Get translation DATA from iframe.,"I am trying to copy the data from the ifrane with the translated content. Copied to clip board or stored in a variable, both would be ok. BUt i think copied to clip board would be best. Later i need to paste the data into a form but thats an other story.  

I tried a lot of things, this is the latest.
Code: Select allVERSION BUILD=12.5.2018.1105
TAB T=1
TAB CLOSEALLOTHERS
'SET !PLAYBACKDELAY 0.00
URL GOTO=https://translate.google.de/translate?sl=auto&tl=de&u=https://en.wikipedia.org/wiki/Main_Page
FRAME NAME=c
TAG POS=1 TYPE=HTML ATTR=* EXTRACT=HTM 
PROMPT {{!EXTRACT}}

Thanks so much for your help, I downloaded the demo a few days ago so i am a real beginner.",https://forum.imacros.net/viewtopic.php?f=7&t=30211&sid=a8b6b8836ef4b006067cba7caee672f3,content
2071,TAG not triggering a download,"On Windows 10 Pro x64, iMacros Browser V12.5.503.8802
My knowledge of iMacros is mostly using the recorder and making very basic changes as needed.

My macro navigates a reporting page, generates a report, clicks a ""Download to Excel"" text hyperlink, but for some reason does not trigger the download. The TAG command appears to be functioning correctly because a box is placed over the hyperlink in the iMacros ui. If I manually click the hyperlink in the iMacros Browser, the download triggers immediately. I experimented with the SAVETARGETAS, but had the same issue. 

The error I receive is: Error -1410: No download detected.
Code: Select allSET !TIMEOUT_DOWNLOAD 10
WAIT SECONDS=2
FRAME F=1
ONDOWNLOAD FOLDER=* FILE=* WAIT=YES 
TAG POS=1 TYPE=SPAN ATTR=TXT:""DOWNLOAD TO EXCEL"" 
'TAG POS=1 TYPE=SPAN ATTR=TXT:""DOWNLOAD TO EXCEL"" CONTENT=EVENT:SAVETARGETAS
WAIT SECONDS=1

Unfortunately, the website is a company CRM so I am unable to share the URL, but I'm happy to share any other info regarding the page. Thank you!",https://forum.imacros.net/viewtopic.php?f=7&t=30203&sid=a8b6b8836ef4b006067cba7caee672f3,content
2072,How to extract only the required letters based on number requirement from the last word.,"I want to pass a test when I ain't there.

<div id=""content"">
<center>
<font color=""gold"" size =""3""> Hey! Are you here?</font>
<br>
<br>
<font size=""3""> Administrator/helper would like to know whether you're here...</font>
<br>
<font size=""3"" color=""lime"">
""Reply with the first 4 characters of the following text: Rainbow""

It isn't always 4 it tends to be different numbers. How would I pass this test?",https://forum.imacros.net/viewtopic.php?f=7&t=30185&sid=a8b6b8836ef4b006067cba7caee672f3,content
2074,Questions on data valadation and extraction to CSV,"This has been written again as I was corrected for my previous attempt 
In the small amount code below I would like to do the following:

1. verify that the searched value appears on the page that was retrieved
     If so then proceed with text extraction
     If not then write a simple text string to a field in the excel file were data will be extracted to. EXAMPLE -  search value (col1) / not found (col2)
2. In the code below POS 7 and POS 8 are associated.  I would like to write them on the same row in excel. EXAMPLE - search  value (col1 row 1) POS7 (col 3 row 1) POS* ( col4 row 1)
3. After all data has been written for that search value, write the next search value data on a new row in the excel file
4. Loop through this until all items have been searched.

I hope this is more clear.

Again thanks ina dvance

VERSION BUILD=12.5.503.8802
TAB T=1
TAB CLOSEALLOTHERS
'SET !EXTRACT_TEST_POPUP
SET !DATASOURCE C:\---\---\----\-----.csv
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}
WAIT SECONDS=0.5
URL GOTO = https://www.---.com
WAIT SECONDS=0.5
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:search--site CONTENT={{!COL1}} 
TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=TXT:Search
TAG POS=1 TYPE=A ATTR=TXT:*
TAG POS=1 TYPE=H2 ATTR=CLASS:item-class EXTRACT=TXT
TAG POS=1 TYPE=H3 ATTR=CLASS:item-number EXTRACT=TXT
TAG POS=7 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=8 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=9 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=10 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=11 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=12 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=13 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=14 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=15 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=16 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=19 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=20 TYPE=TD ATTR=* EXTRACT=TXT
BACK",https://forum.imacros.net/viewtopic.php?f=7&t=30029&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2075,Save extracted DIV to new page/file,"Hi!

http://www.vitisvitae.be/

From this page I would like to extract the div element content.
Code: Select allVERSION BUILD=7601105 RECORDER=FX
VERSION BUILD=6600525     
TAB T=1        
SET !EXTRACT_TEST_POPUP NO
TAG POS=1 TYPE=DIV ATTR=ID:content EXTRACT=HTM
SAVEAS TYPE=EXTRACT FOLDER=* FILE=+{{!URLCURRENT}}.htm

In this version I only have text in a csv file, I need the html of the extracted div.
Code: Select allVERSION BUILD=7601105 RECORDER=FX
TAB T=1
TAG POS=1 TYPE=DIV ATTR=ID:content
SAVEAS TYPE=HTM FOLDER=* FILE=+{{!URLCURRENT}}

In this version I can select the div element I want to extract but it saves the whole page.
I only want to save the div element.

It would be realy nice if someone could point me in the right direction.

Thanks!",https://forum.imacros.net/viewtopic.php?f=7&t=19753&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2076,Inconsistent #EANF#,"I need help making this script more reliable. I use this macro to help me create a contact record in my crm system if I have a facebook friend that I wanted added to my crm so I don't have to manually enter all the information.

Often times I get the #EANF# for any one of my three variables, but as soon as I enable the !EXTRACT_TEST_POPUP YES the script runs correctly on what wouldn't work previously.

I can run this macro and won't work, but then it will start working.
Code: Select allVERSION BUILD=1005 RECORDER=CR
SET !EXTRACT_TEST_POPUP NO

TAG XPATH=""//*[@id=""fb-timeline-cover-name""]/a"" EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
SET !EXTRACT NULL

TAG XPATH=""//*[@id=""fb-timeline-cover-name""]/a"" EXTRACT=HREF
SET !VAR2 {{!EXTRACT}}
SET !EXTRACT NULL

TAG XPATH=""//*[@id=""u_0_1i""]"" EXTRACT=HREF
SET !VAR3 {{!EXTRACT}}
SET !EXTRACT NULL

URL GOTO=https://scraper.pipedrive.com/pipeline

TAG POS=1 TYPE=SPAN ATTR=TXT:Add<SP>deal

TAG POS=1 TYPE=INPUT:TEXT ATTR=tabindex:1 CONTENT={{!VAR1}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=tabindex:2 CONTENT={{!VAR2}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=tabindex:3 CONTENT={{!VAR3}}

TAG POS=1 TYPE=A ATTR=TXT:Attach<SP>products
TAG POS=1 TYPE=INPUT:TEXT ATTR=name:product CONTENT=Business<SP>Coaching


TAG POS=2 TYPE=SPAN ATTR=TXT:Save

Thank you for your support!",https://forum.imacros.net/viewtopic.php?f=7&t=30009&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2077,Loop inputs through csv while scrapping,"Hey everyone. My code is below. What I am trying to do is loop the inputs the contents of cells in my csv file in --> ""client CONTENT=cell contents "" (4th line from the bottom). So, my csv has a bunch of client codes, and I am trying to get the macro to log in to my account, get the client code from the csv file, input it in the box, go to the page and extract a certain line. After that, i want it to go back to main page, go to the next line in the csv file (so second client) and input the second code. Keep doing this until the csv has no more lines. My current code stops after the first input from the csv file. 

I am using the iMacros Browser and there is no loop options. I attached a picture of what the display is. 

Does anyone know how I can do this? Thank you in advance! 



VERSION BUILD=12.5.503.8802
TAB T=1
TAB CLOSEALLOTHERS

SET !EXTRACT_TEST_POPUP
SET !DATASOURCE """"""""name of my file"""""""".csv
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}
WAIT SECONDS=0.5
URL GOTO = https://www. of the website (can't show the real one)""""""
WAIT SECONDS=0.5
TAG POS=1 TYPE=A ATTR=TXT:""login""
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:USERID CONTENT=""""""""my username""""""""
SET !ENCRYPTION NO
WAIT SECONDS=0.5
TAG POS=1 TYPE=INPUT:PASSWORD ATTR=NAME:PASSWORD CONTENT=""""""""my password""""""""
WAIT SECONDS=0.5
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:submit  
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:client CONTENT={{!COL2}}
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:button.access.bn
TAG POS=3 TYPE=H2 ATTR=* EXTRACT=TXT
BACK",https://forum.imacros.net/viewtopic.php?f=7&t=29998&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2078,extract DIV text with (maybe) relative positioning,"Hello

I have the following structure:
Code: Select all<div class=""left"">
  <h2>title</h2>
  <div class=""cls"">
    <a>link</a>
    some text
  </div>
  <div class=""cls"">
    <a>link</a>
    some text
  </div>
  <div class=""cls"">
    <a>link</a>
    some text
  </div>
  <div class=""cls"">
    <a>link</a>
    some text
  </div>
</div>
... about 30 times

Unfortunately <div class=""left""> can be positioned in various places in the page, so I cannot use XPath to get to it or to any of the inside divs
There's also no telling what comes next. Lastly the <div class=""cls""> can be found in various other places in the page.
So is there a way to extract the text of the divs inside? 
Ideally, is there a way to say extract the text from body > div.content > div.left > div:nth-child(2) or (3) or (30)?

The whole structure
would be something like:
Code: Select allbody
  content
    div.title
    div.intro text
    div.short abstract
    div.gallery
    div.left
    div.right
    div.links
    div.bio
    div.more text
  /content
/body

The only ones that will always appear on any page are the div.title and div.left",https://forum.imacros.net/viewtopic.php?f=7&t=29956&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2079,multiple row different extract areas,"Dear all, I know the rules, before checks on the forum to see if there is some similar FAQ .... but I can't find somenthing helpful.

I am trying to build an .iim to scarp some data from a web commerce in order to find a good spare parts for my very old car and because I am new with that wonderful ""iMacros personal V12"" tool I would see if it can help me in finding competitive prices by different e-commerce.
I did try to write the instruction but I can't make it clear when there are more suppliers for one item because for someone are missing info in the web (price or correspondenting code) and when the instruction do scrap the webpage, it download random info with those available only, not following any schema. However I am posting my question trying to make more clear what I don't understand about it if there is anybody whom can help me. 

Here is my (sketch) instruction: 

SET !extract_TEST_POPUP NO 
VERSION BUILD=12.0.501.6698
TAB T=1
SET !DATASOURCE C:\Users\Massimo\Documents\iMacros\DataSources\test.csv
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}
SET !EXTRACT NULL
SET !ERRORIGNORE YES
ADD !EXTRACT {{!COL1}}
ADD !EXTRACT {{!NOW:yyyy/mm/dd_hhnn}}

'SET !PLAYBACKDELAY 0.00
URL GOTO=https://https://www.autoparti.it/ricerc ... rd={{!COL1}}

TAG POS=1 TYPE=INPUT:SEARCH ATTR=NAME:pcode CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=CLASS:header-search__search-submit-btn
TAG POS=R1 TYPE=DIV ATTR=CLASS:art EXTRACT=TXT
TAG POS=R1 TYPE=SPAN ATTR=CLASS:price EXTRACT=TXT
TAG POS=R2 TYPE=DIV ATTR=CLASS:art EXTRACT=TXT
TAG POS=R2 TYPE=SPAN ATTR=CLASS:price EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=*


(I did use a .csv file with codes in order to keep trace of my research).

As you can see here above, I did use R to set POS, here R1, R2 but there are spare parts with more suppliers which need by the circumstance more Rn..., I do wonder if there is a way to set the instruction in order to use a general TAG, able to select any POSITION as many as are available. 


Thanks for any help.",https://forum.imacros.net/viewtopic.php?f=7&t=29873&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2080,Extracting Data results in ugly output,"I am trying to scrape data from the website: https://reports.myreca.ca/publicsearch.aspx

The table that is generated is an ASP.NET reportviewer table. 

I am providing data source in a CSV file, and I plan on setting this to loop for the amount of records we have. 

The problem is that the saved file format is less than diserable and near impossible to work with.

My script:

VERSION BUILD=12.0.501.6698
'Uses a Windows script to submit several datasets to a website, e. g. for filling an online database
TAB T=1     
TAB CLOSEALLOTHERS  
' Specify input file (if !COL variables are used, IIM automatically assume a CSV format of the input file
'CSV = Comma Separated Values in each line of the file
SET !DATASOURCE Address.csv
'Start at line 2 to skip the header in the file
SET !LOOP 2
'Increase the current position in the file with each loop 
SET !DATASOURCE_LINE {{!LOOP}}
' Fill web form   
URL GOTO=https://reports.myreca.ca/publicsearch.aspx
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:Button1
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:TextBox2 CONTENT={{!COL1}}
WAIT SECONDS=3
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:Button3

TAG POS=1 TYPE=TABLE ATTR=ID:ReportViewer1_fixedTable EXTRACT=TXT



SAVEAS TYPE=EXTRACT FOLDER=* FILE=report.csv

--
In excel the entire output is saved on one cell and is really hard to work with. Is there a way that I can set it so the export is in somewhat workable format? 



The output:

""













Legend for Sectors: Res. = Residential, Comm. = Commercial, PM. = Property Management, Rur. = Rural





Executed on: 8/14/2018 9:48:55 AM





Real Estate






Status

Licence History


First

Middle 


Last 


AKA


Brokerage


City

Class

Issue Date

Real Estate Sectors

Authorized

View

Greg

Alan

Steele

EXCELLENCE REAL ESTATE EDMONTON LTD. O/A RE/MAX EXCELLENCE

Edmonton

Associate

10/1/2017



Res. Comm. PM. Rur.

Authorized

View

Gregory

John

Steele

GREG

MOUNTAIN VIEW REAL ESTATE INC. O/A RE/MAX REAL ESTATE (MOUNTAIN VIEW)

Calgary

Associate

10/1/2017



Res. Comm. PM. Rur.""",https://forum.imacros.net/viewtopic.php?f=7&t=29691&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2081,HOW TO COPY DATA ON A WEB-PAGE AND PAST TO CSV FILE,"Hi My fellow Imacros Users,


I am running a imacros code that should enable me copy the balance of a cryptocurrency balance on https://tokenbalance.com/ and past it to csv file on my computer. I try using the extract function but  it not working, please i need your asistant. See detalis below:

Code: Select allVERSION BUILD=1003 RECORDER=CR
SET !EXTRACT_TEST_POPUP NO
URL GOTO=https://tokenbalance.com/
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:etheraddr CONTENT=0xB4D11A36d08695d13FCE93528412EEd7b24b8580
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:contractaddr CONTENT=0x177d39ac676ed1c67a2b268ad7f1e58826e5b0af
TAG POS=1 TYPE=A ATTR=ID:getbalancebtn
Wait Seconds=15
TAG POS=1 TYPE=SPAN ATTR=ID:coinbalance EXTRACT = TXT
",https://forum.imacros.net/viewtopic.php?f=7&t=29662&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2082,EXTRACT=CHECKED wont find check box,"Hey guys, need a little help here. 

Windows 10 Pro
iMacros Browser V12.0.501.6698
iMacros BUILD=12.0.501.6698

I have a check box and I am trying to select the check box if it is not selected.  

To start out I have seen this thread https://forum.imacros.net/viewtopic.php?t=27248
and I have the main concept down I just cannot get the EXTRACT to return a YES or NO.

Here is my code using regular record:
Code: Select allVERSION BUILD=12.0.501.6698
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=https://www.b50first.com/prod/login_app.aspx
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:tbCompany CONTENT=xx
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:tbUser CONTENT=xx
TAG POS=1 TYPE=INPUT:PASSWORD ATTR=NAME:tbPassword CONTENT=xx
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:bLogin
TAG POS=1 TYPE=SPAN ATTR=TXT:Reports
TAG POS=1 TYPE=SPAN ATTR=TXT:GL<SP>Account<SP>Balances
'New tab opened
TAB T=2
TAG POS=1 TYPE=SPAN ATTR=ID:ctl00_cph_ctl02_MainLocTree_tree_SelAll_D  'This line selects check box


here is the line when I use the expert recorder:
Code: Select allTAG POS=1 TYPE=SPAN FORM=NAME:aspnetForm ATTR=ID:ctl00_cph_ctl02_MainLocTree_tree_SelAll_D&&CLASS:dxWeb_edtCheckBoxUnchecked_1<SP>dxICheckBox_1 'Unchecked Box
TAG POS=1 TYPE=SPAN FORM=NAME:aspnetForm ATTR=ID:ctl00_cph_ctl02_MainLocTree_tree_SelAll_D&&CLASS:dxWeb_edtCheckBoxChecked_1<SP>dxICheckBox_1 'Checked Box

So I either get two returns, the one below returns that EXTRACT=CHECKED only works on check boxes:
Code: Select allTAG POS=1 TYPE=SPAN FORM=NAME:aspnetForm ATTR=ID:ctl00_cph_ctl02_MainLocTree_tree_SelAll_D EXTRACT=CHECKED

This line returned an error saying the extract value is #EANF#:
Code: Select allTAG POS=1 TYPE=HIDDEN FORM=NAME:aspnetForm ATTR=ID:ctl00_cph_ctl02_MainLocTree_tree_SelAll_D EXTRACT=CHECKED

Additionally here is the HTML from the website that selects the checkbox (I think):
Code: Select all<input type=""hidden"" name=""ctl00$cph$ctl02$MainLocTree$tree$STATE"" id=""ctl00_cph_ctl02_MainLocTree_tree_STATE"" value=""Y2dkZ2ACYQYQUChyYGdkYmT7/5+dgZ2V/Q2Q5AIKs3MzsRoZWhgbQygTCGUKpIxMLMwglDmQMjE0AMlZWpiZQShzCGUBoiwNTSGUGdBUNiY2v/yUVE8XdiaegMSi1LwSGJfNvaggPsCbmYnNJz8ZzOAASYWkVpSwg8X8korY2RkA"">

Any advise on why it isn't finding the checkbox?

Thanks!",https://forum.imacros.net/viewtopic.php?f=7&t=29638&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2083,Extract information from a web page with a map,"Hi! 

I trying extract information from the branches of the fedex website, however, only the first branch office is saved, how could I save all the results obtained?

This is the script that I have so far.

VERSION BUILD=12.0.501.6698
TAB T=1
TAB CLOSEALLOTHERS
SET !EXTRACT_TEST_POPUP NO
SET !LOOP 10
'SET !PLAYBACKDELAY 0.00
URL GOTO=https://www.fedex.com/locate/index.html?locale=es_MX
TAG POS=1 TYPE=INPUT:TEXT ATTR=ROLE:search CONTENT=Jalisco
EVENT TYPE=CLICK SELECTOR=""#geoAutoComplete>FIELDSET>INPUT:nth-of-type(2)"" BUTTON=0
TAG POS={{!LOOP}} TYPE=P ATTR=CLASS:fx-location-title EXTRACT=TXT
TAG POS={{!LOOP}} TYPE=P ATTR=CLASS:fx-location-address EXTRACT=TXT
TAG POS={{!LOOP}} TYPE=DIV ATTR=CLASS:fx-location-times<SP>fx-cf EXTRACT=TXT
TAG POS={{!LOOP}} TYPE=DIV ATTR=CLASS:fx-location-times<SP>fx-cf EXTRACT=TXT
WAIT SECONDS=1.5
TAG POS=1 TYPE=TABLE ATTR=* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=*FILE=*.csv

Regards,",https://forum.imacros.net/viewtopic.php?f=7&t=29419&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2084,Can't concate Extracted value with ATTR element,"I'm trying to EXTRACT a value from a TD and use that value as ATTR:ID element. But getting this error- RuntimeError: element SELECT specified by ID:bb_sbs_code__{{!EXTRACT}} was not found, line: 8
I'm using following script - 
Code: Select allVERSION BUILD=1001 RECORDER=CR
URL GOTO=some_url
TAG POS=18 TYPE=BUTTON FORM=ACTION:some ATTR=TXT:Update<SP>SBS
WAIT SECONDS=2
TAB T=2
TAG POS=4 TYPE=TD ATTR=TXT:* EXTRACT=TXT
PROMPT {{!EXTRACT}}
TAG POS=1 TYPE=SELECT ATTR=ID:bb_sbs_code__{{!EXTRACT}} CONTENT=%901009
TAG POS=1 TYPE=SELECT ATTR=ID:purpose_code__{{!EXTRACT}} CONTENT=%1101
TAG POS=1 TYPE=SELECT ATTR=ID:sbs_sme_code__{{!EXTRACT}} CONTENT=%99
TAG POS=1 TYPE=SELECT ATTR=ID:sbs_product_code__{{!EXTRACT} CONTENT=%22200
TAG POS=1 TYPE=SELECT ATTR=ID:sbs_security_code__{{!EXTRACT}} CONTENT=%76
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:interest_rate__{{!EXTRACT}} CONTENT=9
TAG POS=1 TYPE=DIV ATTR=ID:content
TAB T=1
REFRESH

Please help to solve the issue.",https://forum.imacros.net/viewtopic.php?f=7&t=29370&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2085,variable {{!colN}},"I am creating Imacros bot to submit site in the directory. So I want to give {{!colN}} custom value
I have 2 CSV file, info.csv and category.csv files. 
In file.csv i am writing a custom number for example 3, and want to fill a form with {{!col3}} from category.csv

What I tried

1 - {{!col{!col16}} This isn't working

2

set !var1 {{!col16}}
SET !var3 EVAL(""var r='{{!var1}}'; var x='{{!col' + r + '}}'; x;"")
PROMPT Var_N:<SP>_{{Var_N}}_{{!Var1}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:Address CONTENT={{!var3}}
This text writing {{!col3}} and not value from file

3) I used some javascript and my site to get back value

macro += ""tab open"" + ""\n"";
macro += ""tab t=2"" + ""\n"";
macro += ""URL GOTO=http://example.com/email/gela.php/{{!col16}}"" + ""\n"";
var url = window.location.pathname;
var id = url.substring(url.lastIndexOf('/') + 1);
macro += ""tab close"" + ""\n"";
macro += ""tab t=1"" + ""\n"";
But this code getting URL from the site where is was first. for example, home file is google.com and then redirect to example.com/email/gela.php/{{!col16}} this code take value from google.com

Do you have any idea I am working on it for 3 days
Pleaseeee........",https://forum.imacros.net/viewtopic.php?f=7&t=28871&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2086,EXTRACT plain text inside div but not any other tags,"HTML:

<div>
        <h1>title</h1>
        <div>address</div>
        <div>extrainfo</div>
        <div>location</div>
        <div>phone</div>
        <div>Website: <a href=""http:s//www.google.ca"">www.google.ca</a></div>
Random description and random products can be bought for $1.99 monday-friday
</div>

ISSUE:

The problem is the Random description line above. It's not within a tag like the other fields so I can't grab only it's data. Instead, it grabs all the content inside the master div combined. Is there a way to only grab plain text, or deny tags another level deep?

MACRO:

SET !ERRORIGNORE YES
SET !TIMEOUT_STEP 1
SET !EXTRACT_TEST_POPUP NO
TAG SELECTOR=""#content>DIV>DIV>DIV>DIV>DIV>DIV>DIV>H1"" EXTRACT=TXT
TAG SELECTOR=""#content>DIV>DIV>DIV>DIV>DIV>DIV>DIV>DIV"" EXTRACT=TXT
TAG SELECTOR=""#content>DIV>DIV>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)"" EXTRACT=TXT
TAG SELECTOR=""#content>DIV>DIV>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(3)"" EXTRACT=TXT
TAG SELECTOR=""#content>DIV>DIV>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(4)"" EXTRACT=TXT
TAG SELECTOR=""#content>DIV>DIV>DIV>DIV>DIV>DIV>DIV>DIV:nth-of-type(5)>a"" EXTRACT=HREF
TAG SELECTOR=""#content>DIV>DIV>DIV>DIV>DIV>DIV>DIV"" EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\savefilelocation FILE=*",https://forum.imacros.net/viewtopic.php?f=7&t=28827&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2087,Macro perfomance slow when extracting table from javascript,"VERSION BUILD=11.5.498.2403,  Windows 7 , IE11

The following macro extracts a HTML table to csv file.  There are approximately 14,000 rows in the table.  When I run the macro from the IMacros Browser in IE the macro extracts the file in less than one minute.  When I attempt to execute the macro from javascript  using iimPlay the macro runs for more than 30 minutes and never returns any results.  If I execute the same macro using iimPlay on a smaller table(12 rows) the macro returns the csv file.

Is there a setting or something I can check while executing the macro from javascript to get the same performance I get when executing the macro in IMacros browser?

Macro
Code: Select allVERSION BUILD=11.5.498.2403
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
SET !EXTRACT_TEST_POPUP NO
URL GOTO=https://www.exchange.com/Start/
TAG POS=1 TYPE=A ATTR=TXT:Pending<SP>Inspections
TAG POS=1 TYPE=SELECT ATTR=NAME:vendor_id CONTENT=%863885910
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:action
TAG POS=7 TYPE=TABLE ATTR=TXT:* EXTRACT=TXT 
SAVEAS TYPE=EXTRACT FOLDER=\xx\nas\Downloads FILE=cis_{{!NOW:yymmdd_hhnnss}}.csv",https://forum.imacros.net/viewtopic.php?f=7&t=28551&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2088,iMacros keeps crashing / not performing steps,"iMacros Version: v12.0.501.6698
Windows Version: 10
Browser: iMacros Browser & IE v11.334.16299.0
Website: LinkedIn.com - Specifically Company Pages I.E: https://www.linkedin.com/company/verizon
Script:
Code: Select allTAB T=1     
TAB CLOSEALLOTHERS  
' Specify input file (if !COL variables are used, IIM automatically assume a CSV format of the input file
'CSV = Comma Separated Values in each line of the file
SET !DATASOURCE D:\*********\input.csv
'Start at line 2 to skip the header in the file
SET !LOOP 78
'Increase the current position in the file with each loop 
SET !DATASOURCE_LINE {{!LOOP}}
SET !TIMEOUT_PAGE 15
SET !TIMEOUT_STEP 15
SET !ERRORIGNORE Yes
' Fill web form   
'SET !PLAYBACKDELAY 0.00 
URL GOTO=https://www.google.com.au/search?ei=j7-1WseaJsH18QWekaywCg&q=a&oq=a&gs_l=psy-ab.3..0i13k1l10.465.1069.0.1291.2.2.0.0.0.0.229.229.2-1.1.0....0...1.1.64.psy-ab..1.1.227....0.CUOUQ7bRfCI
WAIT SECONDS=5
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:q CONTENT={{!COL1}}<SP>Linkedin
TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=NAME:btnG
WAIT SECONDS=5
TAG POS=1 TYPE=CITE ATTR=TXT:* EXTRACT=TXT
URL GOTO={{!EXTRACT}}
SET !EXTRACT NULL
SET !TIMEOUT_STEP 15
TAG POS=1 TYPE=H2 ATTR=TXT:About<SP>us
SET !TIMEOUT_STEP 6
ADD !EXTRACT {{!COL1}}
TAG POS=1 TYPE=H1 ATTR=CLASS:org-top-card-module__name<SP>Sans-26px-black-85%-light EXTRACT=TXT
TAG POS=1 TYPE=P ATTR=CLASS:org-about-company-module__company-staff-count-range<SP>Sans-15px-black-70%<SP>mb3 EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=CLASS:company-industries<SP>org-top-card-module__dot-separated-list EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=CLASS:org-top-card-module__location<SP>org-top-card-module__dot-separated-list EXTRACT=TXT
ADD !EXTRACT {{!URLCURRENT}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=*

I have an input list of which I'm trying to match against LinkedIn details. The script it self works perfectly. However after a nominal amount of loops (some times 10 sometimes 100) one of the following will happen:

- iMacros will keep steping through commands however it doesn't seem to pass them onto the browser
- IE / Macros will crash

I'd obviously prefer to run this in Chrome / FF but I don't believe I can use the EXTRACT or SAVEAS command without purchasing the full license. Any ideas of work arounds I need to go through over 3000 inputs and it's killing me to have to continiously have to check on it?",https://forum.imacros.net/viewtopic.php?f=7&t=28502&sid=7ae4e60bc4f3ad89d3e546c9096547af,content
2090,Can't get Imacros to download a PDF,"IE = 11.09600.18893
IMACROS=12.0501.6698

Hi, I would appreciate if someone can help me figure this one out or lead me to the where i can read additional materials so i can figure out myself.

Currently after the last command, a pop up window will pop up and reveal the available documents to download. The problems are 1) the POS is different everytime. I know how to use relative position, but I can't figure out what is the anchor to use. 2)the biggest problem is, even if I know the TAG POS to use for the download link, when I enter that command to the Imacros, it wouldn't do anything. For example, in the example provided below, I used Imacros recording function to figure out the command for the download link is TAG POS=19 TYPE=TD ATTR=* , but when I manually enter the command in the script before, it didn't do anything.

Code: Select allSET !ERRORIGNORE YES
TAB T=1
URL GOTO=https://ccfs.sos.wa.gov/#/Home
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:UBINumber CONTENT=604140761 
TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=TXT:Search
TAG POS=1 TYPE=TD ATTR=CLASS:ng-binding
TAG POS=R-1 TYPE=A ATTR=TXT:*
SET !TIMEOUT_STEP 1
TAG POS=1 TYPE=INPUT:BUTTON ATTR=ID:btnFilingHistory
TAG POS=1 TYPE=TD ATTR=TXT:Initial*
TAG POS=R1 TYPE=A ATTR=TXT:View<SP>Documents
",https://forum.imacros.net/viewtopic.php?f=7&t=28475&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2091,Problem in Nested Looping,"I have been assigned to collect information from marketplace website. This responsibility is very important for me because it's relating to my promotion in my job. The link of the marketplace is : 
https://shopee.co.id/Handphone-Aksesori ... rtBy=sales

it's consist of a number of pages, and each page contain 50 products. I must be able click each the link product, each the next navigation button to move to next page, and collect data on each of it by using imacros. The imacros script that I have created so far :

VERSION BUILD=8970419 RECORDER=FX
TAB T=1
TAG POS={{!LOOP}} TYPE=DIV ATTR=CLASS:shopee-item-card__text-name
TAG POS=1 TYPE=H1 ATTR=CLASS:shopee-product-info__header__text&&ITEMPROP:name EXTRACT=TXT
TAG POS=1 TYPE=DIV ATTR=CLASS:shopee-product-info__header__real-price EXTRACT=TXT
TAG POS=1 TYPE=DIV ATTR=CLASS:shopee-product-info__header__sold-count EXTRACT=TXT
TAG POS=1 TYPE=DIV ATTR=CLASS:shopee-product-detail__parameters__content EXTRACT=TXT
TAG POS=2 TYPE=DIV ATTR=CLASS:shopee-product-detail__parameters__content EXTRACT=TXT
TAG POS=3 TYPE=DIV ATTR=CLASS:shopee-product-detail__parameters__content EXTRACT=TXT
TAG POS=4 TYPE=DIV ATTR=CLASS:shopee-product-detail__parameters__content EXTRACT=TXT
TAG POS=5 TYPE=DIV ATTR=CLASS:shopee-product-detail__parameters__content EXTRACT=TXT
TAG POS=6 TYPE=DIV ATTR=CLASS:shopee-product-detail__parameters__content EXTRACT=TXT
TAG POS=1 TYPE=DIV ATTR=CLASS:product-page-seller-info__shop-name EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=F:\KRIS\ FILE=CONTOH.CSV
BACK


The problem is, so far this script is only working in each  page, and it does not able to move to next page automatically. Could  anyone give me some advice ?",https://forum.imacros.net/viewtopic.php?f=7&t=28473&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2092,newbie: Extract A HREF or Checkbox variable,"How do I get the HREF from the Anchor tag or extract the ""recordIDs"" checkbox variable below and use URL GOTO to for clicking the link?  I am executing a search results from a CSV file loop and need to go to the Detail page of each Contact record.  I am having trouble referencing the EXTRACT=HREF as the script cannot find the Anchor so not sure how to do this yet.  the 
Code: Select allVERSION BUILD=12.0.501.6698
TAB T=1
' TAB CLOSEALLOTHERS
'SET !PLAYBACKDELAY 1.03

SET !DATASOURCE CrownPointContactRemoveList.csv
SET !LOOP 8
SET !DATASOURCE_LINE {{!LOOP}}

URL GOTO=https://siteadmin-staging.vin65.com/dashboard/index.cfm?method=welcome.frames

'click Contacts 
TAG POS=1 TYPE=A ATTR=TXT:Contacts

'enter Email from CSV and search 
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:email CONTENT={{!COL11}}
TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=TXT:Search

'get HREF from Anchor tag in html 
'ERROR: can't find anchor tag 
TAG POS=1 TYPE=A ATTR=TXT:Cust<SP>No:<SP>*{{!COL11}}*<SP>Email:<SP>*{{!COL11}}*<SP>Phone:<SP>*{{!COL12}}* EXTRACT=HREF

'hardcoded this works
TAG POS=1 TYPE=A ATTR=TXT:Cust<SP>No:<SP>1085<SP>Email:<SP>johndoe@aol.com<SP>Phone:<SP>(510)<SP>555-6178 EXTRACT=HREF

URL GOTO={{!EXTRACT}}

Code: Select all	<table class=""table table-hover table-striped"">
			<tbody>
			<tr>
				<th></th>
				<th>Name / Address</th>
				<th>Info</th>				
				<th>Stats</th>			
				<th></th>
			</tr>
			
				<tr>
					<td><input type=""checkbox"" name=""recordIDs"" value=""045DCA49-A11C-127B-4A96-622311917403""></td>
					<td><a href=""index.cfm?method=contactListings.view&contactID=045DCA49-A11C-127B-4A96-622311917403&urlstring=CUSTOMSEARCH%3D%26phone%3D%26ZIPCODE%3D%26firstName%3D%26DATEMODIFIEDFROM%3D%26customerNumber%3D%26email%3Djohndoe%2540aol%252Ecom%26company%3D%26Q%3D%26DATEMODIFIEDTO%3D%26city%3D%26stateCode%3D%26contactTypeID%3D%26lastName%3D%26&page=1"">
						
						<strong>John Doe </strong><br>
						1008 John Doe DR<br>
						John Doe City,
						CA,
						94530
					</a></td>
					<td><a href=""index.cfm?method=contactListings.view&contactID=045DCA49-A11C-127B-4A96-622311917403&urlstring=CUSTOMSEARCH%3D%26phone%3D%26ZIPCODE%3D%26firstName%3D%26DATEMODIFIEDFROM%3D%26customerNumber%3D%26email%3Djohndoe%2540aol%252Ecom%26company%3D%26Q%3D%26DATEMODIFIEDTO%3D%26city%3D%26stateCode%3D%26contactTypeID%3D%26lastName%3D%26&page=1"">
						Cust No: 1085<br>
						Email: johndoe@aol.com<br>
						Phone: (510) 555-6178 
					</a></td>
					<td><a href=""index.cfm?method=contactListings.view&contactID=045DCA49-A11C-127B-4A96-622311917403&urlstring=CUSTOMSEARCH%3D%26phone%3D%26ZIPCODE%3D%26firstName%3D%26DATEMODIFIEDFROM%3D%26customerNumber%3D%26email%3Djohndoe%2540aol%252Ecom%26company%3D%26Q%3D%26DATEMODIFIEDTO%3D%26city%3D%26stateCode%3D%26contactTypeID%3D%26lastName%3D%26&page=1"">
						# of Orders: 0<br>
						Last Order: <br>
						LTV: $0.00<br>
						Contact Status: Prospect
					</a></td>
					<td class=""text-right""><a href=""index.cfm?method=contactListings.view&contactID=045DCA49-A11C-127B-4A96-622311917403&urlstring=CUSTOMSEARCH%3D%26phone%3D%26ZIPCODE%3D%26firstName%3D%26DATEMODIFIEDFROM%3D%26customerNumber%3D%26email%3Djohndoe%2540aol%252Ecom%26company%3D%26Q%3D%26DATEMODIFIEDTO%3D%26city%3D%26stateCode%3D%26contactTypeID%3D%26lastName%3D%26&page=1""><i class=""icon-search""></i></a></td>
				</tr>
			
			</tbody>
		</table>

",https://forum.imacros.net/viewtopic.php?f=7&t=28472&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2093,Concatenate data when i read extract data,"Hello,

Sorry for my english, i'm french. I have the business version 7.0.

I try to extract three variables from input text in my website. When i test variable read by Imacros, i have no problem, the dialog box show me the good variables. But when i fill an other form with this extract variables, ""Jos茅"" in the first field (good), ""Jos茅 PEREZ[EXTRACT]contact@mail.com"" in the second field, ""Jos茅[EXTRACT]contact@mail.com[EXTRACT]Actualit茅s des blogs influents"" in the third field in the form.

The second and third variable concatenate in the filed on the form.
Code: Select allTAG POS=1 TYPE=A ATTR=TXT:Informenter
TAG POS=1 TYPE=SELECT FORM=CONTENTEDITABLE:inherit ATTR=NAME:site CONTENT=%48
TAG POS=1 TYPE=INPUT:SUBMIT FORM=CONTENTEDITABLE:inherit ATTR=VALUE:Valider
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:nom&&VALUE:Jos茅 EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
SET !EXTRACT NULL
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:email&&VALUE:contact@mail.com EXTRACT=TXT
SET !VAR2 {{!EXTRACT}}
SET !EXTRACT NULL
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:texte ATTR=NAME:titre1 EXTRACT=TXT 
SET !VAR3 {{!EXTRACT}}
SET !EXTRACT NULL

URL GOTO=http://www.bubastis.be/annuaire/submit_site.php3?id_cat=TXT
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:form2submit ATTR=NAME:nom_proprio CONTENT={{!VAR1}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:form2submit ATTR=NAME:email_proprio CONTENT={{!VAR2}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:form2submit ATTR=NAME:titre CONTENT={{!VAR3}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:form2submit ATTR=NAME:url CONTENT=http://www.monsite.com
TAG POS=1 TYPE=TEXTAREA FORM=NAME:form2submit ATTR=NAME:description CONTENT=Ma description
TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:form2submit ATTR=VALUE:Soumettre<SP>le<SP>site

Thanks for your help,

Jos茅",https://forum.imacros.net/viewtopic.php?f=7&t=10727&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2094,How to create and reference a text file,"I am using Firefox on a 64 bit windows 10 OS.


My name is Christian. I am a newbie and just recently I installed the iMacro plugin.

I am trying to create a macro that does the following:
1. Copy a line of text from a Single colum CSV file
2.Go to blogger.com>>Create New Post
3. Paste the text in the ""Title"" box and click on publish.
4.Repeat steps 1-3 for the subsequent lines in the text file until the last line.

My problem.
The imacro I recorded cannot reference a text file

Here is the code:
Code: Select allVERSION BUILD=9030808 RECORDER=FX
TAB T=1
URL GOTO=https://www.blogger.com/blogger.g?blogID=77852846747726687#editor/src=sidebar
URL GOTO=blogger.com/
TAG POS=1 TYPE=A ATTR=TXT:New<SP>post
wait seconds=10
TAG POS=2 TYPE=INPUT:TEXT FORM=NAME:postingForm ATTR=* CONTENT=Oga
TAG POS=1 TYPE=BUTTON FORM=NAME:postingForm ATTR=TXT:Publish

This is the path to the CSV file:C:\Users\la~tino\Documents\iMacros\Datasources\alumini.CSV

Thanks for your most anticipated solution",https://forum.imacros.net/viewtopic.php?f=7&t=28358&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2095,"Download pdf/audio/video from page, click next page do same","Dear friends, I hope I can find some help, is my first time with imacros (I know algorithms, but I've never used imacros).

I am a participant in a website with videos and study material in pdf, and I want to download them so I can watch and study offline.

So, I'm logged into the page, and I'm going to the course I want to download, link:
https: //www.****.com/course/cod/sc%6CTU1EaL7Z%9W/v/OQlrb2uhlnS%9W/c/KvcHnMerTdz%9W

I tried, with Chrome Imacros recording function of imacros to save  each action and repeating it to do the operation in next pages, I'll attach the script so you understand.

In the page, I download the video, I use the Adobe HDS / HLS Video Saver addon (withou imacros i can download any m3u8 with this addon), the link to open the addon is:
chrome-extension://pibndofbpkoaipoidbkephfhhnapkccn/download.html

Is possible record actions of a another addon? even if this addon is just a new tab with specifc functions?
In this link, the m3u8 file is brought by clicking on the link inside the addon, the addon joins the parts and the download of a unified file of the video.
Then I click clear and close the tab.

I go back to the first page, and I click to download the pdf booklet and then click to download the pdf slide and after downlaod the audiofile.

Then I click on the next page, where there same structure page with a new video and other materials and I have re-created the process on this new page.

However the script is static, I want to leave it dynamic, to go on going on each page by clicking the ""next page"" button and the materials being downloaded in each one.

Another detail, that in the static there are some problems, in this part:
GOTO URL = chrome-extension://pibndofbpkoaipoidbkephfhhnapkccn/download.html

imacros adds  ""http://"" and ignores after the ""extension"" the "":"" and try to open in the new tab like this, imacros try open this:
http://chrome-extension//pibndofbpkoaip ... nload.html

1 - How do I open it in the new tab exactly ""chrome-extension://pibndofbpkoaipoidbkephfhhnapkccn/download.html""    ?

2 - inside this addon page which is a new tab, it seems that imacros did not save the command to click on the link, which makes the addon download the m3u8 and merge all the videos into a single file.
So in this way, as the addon is not opened as per the link above, I can not download the m3u8 video. Its possible use this addon with imacros? if not, any another suggestions to download m3u8 using imacros?

3 - When imacros downloads the contents of this first page, it goes to the next page according to the next page button, but it does not downloads the contents of the second page, but download the contents of the same first page again.
How to make it go to the next page as the button and download the contents of the next page and not the first?

That is, I want to make this dynamic, I did the procedure 3 times to be easy to understand the static imacro to try to make it dynamic.
So I want do a dynamic imacro that will always click ""next page"" automatically regardless of how many pages it is and download the contents of each one of them.

In short, for the time being this static script can download the pdfs and the audio normally, is only with the problem that I mentioned in not downloading the contents of the next page and too the problem of not being able to download the m3u8 video with imacros using the HLS addon.
Script:
VERSION BUILD=1001 RECORDER=CR
URL GOTO=https://www.****.com.br/course/cod/sc%6CTU1EaL7Z%9W/v/OQlrb2uhlnS%9W/c/KvcHnMerTdz%9W
TAB OPEN
TAB T=2
URL GOTO=chrome://newtab/
URL GOTO=chrome-extension://pibndofbpkoaipoidbkephfhhnapkccn/download.html
TAB T=1
TAG POS=1 TYPE=SMALL ATTR=TXT:Download<SP>Booklet<SP>pdf
TAB T=2
TAB T=1
TAG POS=1 TYPE=SMALL ATTR=TXT:Download<SP>slide<SP>lesson
TAB T=2
TAB T=1
TAG POS=1 TYPE=SMALL ATTR=TXT:Download<SP>audiolesson
TAG POS=1 TYPE=A ATTR=TXT:Download<SP>lesson<SP>audio
TAB T=2
TAB T=1
TAG POS=1 TYPE=BUTTON ATTR=TXT:Close
TAG POS=1 TYPE=A ATTR=TXT:Next<SP>page<SP>lesson
TAB OPEN
TAB T=2
URL GOTO=chrome://newtab/
URL GOTO=chrome-extension://pibndofbpkoaipoidbkephfhhnapkccn/download.html
TAB T=1
TAG POS=1 TYPE=SMALL ATTR=TXT:Download<SP>Booklet<SP>pdf
TAB T=2
TAB T=1
TAG POS=1 TYPE=SMALL ATTR=TXT:Download<SP>slide<SP>lesson
TAB T=2
TAB T=1
TAG POS=1 TYPE=SMALL ATTR=TXT:Download<SP>audiolesson
TAG POS=1 TYPE=A ATTR=TXT:Download<SP>lesson<SP>audio
TAB T=2
TAB T=1
TAG POS=1 TYPE=BUTTON ATTR=TXT:Close
TAG POS=1 TYPE=A ATTR=TXT:Next<SP>page<SP>lesson
TAB OPEN
TAB T=2
URL GOTO=chrome://newtab/
URL GOTO=chrome-extension://pibndofbpkoaipoidbkephfhhnapkccn/download.html
TAB T=1
TAG POS=1 TYPE=B ATTR=TXT:Booklet
TAB T=2
TAB T=1
TAG POS=1 TYPE=SMALL ATTR=TXT:Download<SP>slide<SP>lesson
TAB T=2
TAB T=1
TAG POS=1 TYPE=A ATTR=TXT:Audiolesson<SP>Download<SP>audiolesson
TAG POS=1 TYPE=A ATTR=TXT:Download<SP>lesson<SP>audio
TAB T=2
TAB T=1
TAG POS=1 TYPE=BUTTON ATTR=TXT:Close",https://forum.imacros.net/viewtopic.php?f=7&t=28290&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2097,Copy Entire Column using DATASOURCE,"Firefox Version: 51.0 (32-bit)
iMacros Version: 9.0.3
OS: Windows 10 64-bit

Hello Everyone,

I need help in order to copy entire column using DATASOURCE or any other command.

Actually, I can't use CONCATENATE(excel function) while saving excel as .csv or .txt due to characters limit.

That's why I need a way around to copy complete column and paste it into webpage text area.

This is the code I am using right now:
Code: Select allVERSION BUILD=9030808 RECORDER=FX
TAB T=1
URL GOTO=http://....
WAIT SECONDS = 5
SET !DATASOURCE MyWorkbook.txt
SET !DATASOURCE_COLUMNS 1
WAIT SECONDS = 5
TAG POS=1 TYPE=TEXTAREA ATTR=CLASS:ace_text-input&&AUTOCORRECT:off&&AUTOCAPITALIZE:off&&SPELLCHECK:false&&WRAP:off&&TXT: CONTENT={{!COL1}}
WAIT SECONDS = 5

Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=28250&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2098,Site now uses infinite scrolling - what to do?,"I'm sorry, I'm very new to this. A friend wrote this a long time ago. I reckon I have to tell `extractFromLibrary` to use some scroll mechanism instead of parsing through pages? Thanks!
Code: Select all// Instead of just extracting all the books from one library, this script imports a list of library's from CSV
// and for each, saves all of their books to the output CSV

// To speed up the running of this script, the following iMacros preferences are recommended...
// Go to iMacros options, then the ""general"" tab
// Set ""Replay Speed"" to fast
// Under ""Visual Effects"", untick ""scroll to object when found"" as well as ""Highlight object when found""
// Under ""Javascript scripting settings"", untick ""Show Javascript during replay""

// File is read from the ""datasources"" path set in iMacros prefs, not ""downloads"" path pref
const inputFileName= ""Libraries-to-extract-from.csv"";
// The starting row number (to enable importing just part of a large CSV)
const startRowID = 1;
// Name of the file where the results are output (is saved into the ""Downloads"" folder set in iMacros prefs)
// NB: Every time this script is run, the results are just added to the end of this file.
// So delete/rename the output file if needed - to avoid duplicate entries.
const outputFileName= ""Books-url-list.csv"";

// ###################

// Global variable for status message, since using iimDisplay() clears previous messages
var statusMessage;

addStatusMessage(""Importing "" + inputFileName + "", starting at line "" + startRowID);

// For each library in the CSV file, import all of their books
var rowID = 1;
while (true) {
    // Not using addStatusMessage() directly, since want to throw away this last message afterwards
    iimDisplay(statusMessage + ""\n-Processing row "" + rowID);
    var currentRowContents = getCSVRow(rowID);
    if (!currentRowContents) {
        // Break if end of file reached, or if there was an error reading the file (eg file not found)
        addStatusMessage(""Exiting on row "" + rowID + "". Either an error has occurred, or the end of file was reached."");
        break;
    }
    extractFromLibrary(currentRowContents);
    rowID++;
}

function extractFromLibrary(targetLibrary) {
    // URL of the books page to process
    var targetBooksPage = targetLibrary + ""/books"";
    goToPage(targetBooksPage);

    var lastPageID = getLastPageID();
    addStatusMessage(""Saving pages 1->"" + lastPageID + "" for "" + targetBooksPage);

    for (var i = startFromPageID; i <= lastPageID; i++) {
        // Not using addStatusMessage() directly, since want to throw away this last message afterwards
        iimDisplay(statusMessage + ""\n-Processing page "" + i);
        // Start of script navigated to page 1 already, so only need to change if i is not 1
        if (i != 1) goToPage(targetBooksPage + ""?page="" + i);
        processCurrentPage();
    }
}


/* Helper Functions */

function runMacro(macro) {
    // Runs the specified macro with a reduced tag timeout of 3 seconds (default is 60)
    return iimPlay(""CODE:"" + ""SET !TIMEOUT_TAG 3\n"" + macro);
}
function addStatusMessage(newMessage) {
    // Using iimDisplay() clears previous messages, so global statusMessage variable used to save them
    if (!statusMessage) {
        statusMessage = ""Starting script..."";
    }
    statusMessage += ""\n-"" + newMessage;
    iimDisplay(statusMessage);
}
function getCSVRow(rowID) {
    var result = runMacro(""SET !DATASOURCE "" + inputFileName +
    ""\nSET !DATASOURCE_COLUMNS 1"" +
    ""\nSET !DATASOURCE_LINE "" + rowID +
    ""\nSET !EXTRACT {{!COL1}}"");
    if (result < 0) {
        // Fetching the row failed. Could be due to end of file or else file not found.
        return null;
    } else {
        return iimGetLastExtract(1);
    }
}
function goToPage(url) {
    // Navigates to the desired URL with images turned off, to decrease pageload time
    runMacro(""FILTER TYPE=IMAGES STATUS=ON"" +
            ""\nURL GOTO="" + url);
}
function getLastPageID() {
    // Extract the page ID of the last page of books, using relative positioning numbering
    // The site uses ""Page 1"", ""Page 2"", ""..."", ""Page N"", ""Next"" type site navigation
    // First finds the ""Next"" link, than extracts the link text immediately prior to it, to get last page ID
    runMacro(""TAG POS=1 TYPE=A ATTR=TXT:Next EXTRACT=TXT"" +
            ""\nTAG POS=R-1 TYPE=A ATTR=TXT:* EXTRACT=TXT"");
    if (iimGetLastExtract(2) == ""#EANF#"") {
        // Tags not found, or timeout reached
        addStatusMessage(""No next page button found, so there must only be one page total"" +
                "" (or else the page didn't finish loading in 60s)."");
        lastPageID = 1;
    } else {
        // Tags found, so use the link text value
        lastPageID = iimGetLastExtract(2);
    }
    return lastPageID;
}
function processCurrentPage() {
    var i = 0;
    while (true) {
        i++;
        // Attempt extraction of next library book link
        // Note: extraction and saving to CSV were not combined, since hard/impossible to know when to stop,
        // since logic not possible inside macros - and whenever SAVEAS TYPE=EXTRACT is used, the 
        // EXTRACT variable is cleared. So iimGetLastExtract(1) always returns null, regardless of success or 
        // failure. Even if the iimPlay return code was checked instead, #EANF# junk would still have been added
        // to the last row of the CSV, which isn't desired. 
        // To reduce the slowdown caused by splitting the steps, the EXTRACT variable is manually set before 
        // using SAVEAS, rather than wasting time using TAG again.
        runMacro(""TAG POS="" + i + "" TYPE=A ATTR=CLASS:library-link&&TITLE: EXTRACT=HREF"");
        var currentLibraryURL = iimGetLastExtract(1);
        // If that link was found, save to the next line of the CSV, otherwise break out of loop
        if (currentLibraryURL == ""#EANF#"") {
            break;
        } else {
            runMacro(""SET !EXTRACT "" + currentLibraryURL +
                    ""\nSAVEAS TYPE=EXTRACT FOLDER=* FILE="" + outputFileName);
        }
    }
}
",https://forum.imacros.net/viewtopic.php?f=7&t=27969&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2099,Extract In Multiple HTML Tag,"Hi, i would like to ask how to extract the BOLD TEXT ""The Photographer (2017)"" from this html tag below?

<h1><a href=""hxxt://sample.com"" title=""The Photographer (2017)"" target=""blank;""><img src=""http://www.senimovies.net/wp-content/th ... .png""/></a> The Photographer (2017) </h1>

I have use this code
TAG POS=1 TYPE=H1 ATTR=* EXTRACT=TXT

The result is like looping so many times like this
The Photographer (2017)The Photographer (2017)The Photographer (2017)The Photographer (2017)The Photographer (2017)The Photographer (2017)The Photographer (2017)The Photographer (2017)",https://forum.imacros.net/viewtopic.php?f=7&t=28262&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2100,Copy a portion of Web page Using iMacros,"I am using Firefox 48.0 iMacros for Firefox 9.0.3 & Windows7 Professional 64-bit Operating system.

Code: Select allVERSION BUILD=9030808 RECORDER=FX
TAB T=1
SET !EXTRACT_TEST_POPUP NO

URL GOTO=http://164.100.140.80/RoRView.aspx
TAG POS=1 TYPE=LEGEND FORM=ID:aspnetForm ATTR=TXT:Select<SP>Location<SP>for<SP>RoR
TAG POS=1 TYPE=TD ATTR=TXT:District
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_ddlDistrict CONTENT=%14
WAIT SECONDS=1
TAG POS=1 TYPE=TD ATTR=TXT:Tahasil
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_ddlTahsil CONTENT=%1
WAIT SECONDS=1
TAG POS=1 TYPE=TD ATTR=TXT:Village
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_ddlVillage CONTENT=%315
WAIT SECONDS=1
TAG POS=1 TYPE=SPAN ATTR=ID:ctl00_ContentPlaceHolder1_lblColumnName
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_ddlBindData CONTENT=%1
WAIT SECONDS=1

TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_btnRORFront

'TAG POS=1 TYPE=DIV ATTR=TXT:Schedule<SP>I<SP>Form<SP>No.39-A
'TAG POS=1 TYPE=TD ATTR=TXT:喱ム喱ㄠ<SP>喱ㄠ喹嵿喱?SP>:<SP>""149""
'Anchor:
TAG POS=1 TYPE=TD ATTR=TXT:喱溹喱苦喱距喱權瓖喱?SP>喱ㄠ喱?SP>喱?SP>喱栢瓏喹编喱?SP>喱<SP>喱栢喱苦瓱喱距喱?SP>喱曕瓖喱班喱苦瑫*

'TAG POS=1 TYPE=TD ATTR=TXT:1)<SP>喱栢喱苦瓱喱距喱?SP>喱曕瓖喱班喱苦瑫<SP>喱ㄠ喹嵿喱?SET !EXTRACT NULL
TAG POS=R3 TYPE=TD ATTR=TXT:* EXTRACT=TXT
SET My_Data {{!EXTRACT}}

'TAG POS=1 TYPE=TD ATTR=TXT:1
SET !EXTRACT NULL
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
ADD My_Data {{!EXTRACT}}

'TAG POS=1 TYPE=TD ATTR=TXT:2)<SP>喱瓖喱班瑴喱距<SP>喱ㄠ喱?<SP>喱喱む喱?SP>喱ㄠ喱?<SP>喱溹喱む<SP>喱?SP>喱喱膏喹嵿*
SET !EXTRACT NULL
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
ADD My_Data {{!EXTRACT}}

SET !EXTRACT NULL
TAG POS=1 TYPE=SPAN ATTR=ID:gvfront_ctl02_lblName EXTRACT=TXT
'TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
ADD My_Data {{!EXTRACT}}

'TAG POS=1 TYPE=TD ATTR=TXT:3)<SP>喱膏瓖喱掂喹嵿
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
ADD My_Data {{!EXTRACT}}

'TAG POS=1 TYPE=TD ATTR=TXT:喱膏瓖喱ム喱む喱喱?SET !EXTRACT NULL
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
ADD My_Data {{!EXTRACT}}

'PROMPT {{!EXTRACT}}
'PROMPT {{My_Data}}

SET !CLIPBOARD {{My_Data}}
PROMPT {{!CLIPBOARD}}

I am using the above code to copy the SL 1,2,3 of the webpage after clicking the ROR front page button.
Reference link http://forum.imacros.net/viewtopic.php?f=7&t=26484
Similarly I want to copy the webpage of ROR BACK PAGE of the marked area



The above portion is showing in iMacros like 
Code: Select allTAG POS=6 TYPE=DIV ATTR=TXT:喱栢喱苦瓱喱距喱?SP>喱曕瓖喱班喱苦瑫<SP>喱ㄠ瑐<SP>:<SP>1喱瓕喱溹<SP>:<SP>喱呧瑱喹嵿瑮喱距喱喹嵿喱溹喱侧瓖*
but I don't know what code I should write below to this to copy the above TAG POS 6.
Please help.",https://forum.imacros.net/viewtopic.php?f=7&t=28216&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2101,Error -1300 when extracting data,"Hello-

I am trying to extract data from a webpage but am getting this error when I try to actually select the table for download.

This is the error that is returned:

Error -1300: Cannot find HTML element using specified Selector expression: #highcharts-f2n5udx-4>svg>g:nth-of-type(7)>path. Line 9: TAG SELECTOR=""#highcharts-f2n5udx-4>svg>g:nth-of-type(7)>path""

Not sure what the issue is here.  Here is the full script, for reference.  I am getting the error on the ""TAG SELECTOR"" row.

VERSION BUILD=11.5.498.2403
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
URL GOTO=http://www.bonds.is/market-overview/?ty ... okid=75201
'
TAG POS=1 TYPE=SELECT ATTR=ID:type CONTENT=%Yield
TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=ID:btnDraw
TAG SELECTOR=""#highcharts-f2n5udx-4>svg>g:nth-of-type(7)>path""
ONDOWNLOAD FOLDER=d:\intdaily\iceland\bond FILE=Y0414.csv WAIT=YES
TAG POS=1 TYPE=DIV ATTR=TXT:Download<SP>CSV
TAG POS=1 TYPE=SELECT ATTR=ID:type CONTENT=%Price
TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=ID:btnDraw
TAG SELECTOR=""#highcharts-f2n5udx-8>svg>g:nth-of-type(7)>rect""
ONDOWNLOAD FOLDER=d:\intdaily\iceland\bond FILE=P0414.csv WAIT=YES
TAG POS=1 TYPE=DIV ATTR=TXT:Download<SP>CSV",https://forum.imacros.net/viewtopic.php?f=7&t=28202&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2102,Saving CAPTCHA image not working...sooo close,"Hello, 

I am trying to save a captcha image and while it is saving an image it is saving the wrong part of the screen.

I am using:

version:
10022823

windows: 7

Tested in: imacros browser and imacros for IE


Here is my code:
Code: Select allVERSION BUILD=10022823
CLEAR 
SET !DATASOURCE ""C:\\account_data.csv""
PROXY ADDRESS={{!COL1}}
TAB T=1
ONDOWNLOAD FOLDER=C:\captcha FILE=captcha.jpg WAIT=YES
TAB CLOSEALLOTHERS
URL GOTO=https://accounts.google.com/ServiceLogin?service=adwords&hl=en-US&ltmpl=signup&passive=false&ifr=false&alwf=true&continue=https://adwords.google.com/um/SignupToken&app=Signup&sacu=1&sourceid=awo&subid=ww-ns-g-awhp_nelsontest3_nel_e
TAG POS=2 TYPE=INPUT:RADIO ATTR=NAME:keyGoogleAccount CONTENT=YES
FRAME NAME=newAccountLoginBox
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:createaccount ATTR=NAME:Email CONTENT={{!COL2}}
SET !ENCRYPTION NO
TAG POS=1 TYPE=INPUT:PASSWORD FORM=NAME:createaccount ATTR=NAME:Passwd CONTENT={{!COL3}}
TAG POS=1 TYPE=INPUT:PASSWORD FORM=NAME:createaccount ATTR=NAME:PasswdAgain CONTENT={{!COL3}}
TAG POS=1 TYPE=IMG FORM=NAME:createaccount ATTR=SRC:*https://accounts.google.com/Captcha?ctoken* CONTENT=EVENT:SAVE_element_screenshot
WAIT SECONDS=5
TAB OPEN
TAB T=2
SET !EXTRACT_TEST_POPUP NO
URL GOTO=http://api.deathbycaptcha.com/decaptcher?function=picture2&print_format=html
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:http://api.deathbycaptcha.com/decaptcher ATTR=NAME:username CONTENT=<my_user_name>
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:http://api.deathbycaptcha.com/decaptcher ATTR=NAME:password CONTENT=<my_password>
TAG POS=1 TYPE=INPUT:FILE FORM=ACTION:http://api.deathbycaptcha.com/decaptcher ATTR=NAME:pict CONTENT=C:\captcha\image.jpg
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://api.deathbycaptcha.com/decaptcher ATTR=VALUE:Send
WAIT SECONDS=30
TAG POS=6 TYPE=TD ATTR=* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
TAB CLOSE
TAB T=1
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:createaccount ATTR=NAME:newaccountcaptcha CONTENT=!VAR1
TAG POS=1 TYPE=INPUT:CHECKBOX FORM=NAME:createaccount ATTR=NAME:TermsOfService CONTENT=YES

Everything runs fine but when it processes line 15 in the Imacros browser instead of the captcha image being saved it just saves a portion of the top left of the screen.  In the IE imacros browser it gives this error:
Error -1000: Cannot copy the item being saved to destination file: C:\captcha\captcha.jpg Unable to save screenshot of the current browser page.  Notice that only HTML documents are currently supported  (no PDF or office documents loaded via plugins). Line 15: TAG POS=1 TYPE=IMG FORM=NAME:createaccount ATTR=SRC:*https://accounts.google.com/Captcha?ctoken* CONTENT=EVENT:SAVEPICTUREAS

Which makes sense because there is no file type in the code of the page that specifies the type of file for the CAPTCHA image.  Instead there is a randomly generated ""key"" that is used to reference the image.

I feel very close to getting this to work but something is either out of place/sequence or the references are wrong somehow.

I've spent 3 hours on this and it's driving me nuts    Any ideas?",https://forum.imacros.net/viewtopic.php?f=7&t=23063&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2103,Download File in -silent mode,"iMacros V12.0.501.6698
Windows 10 Pro
Purchased Enterprise Edition
Ran in chrome Version 63.0.3239.52 (Official Build) beta (64-bit)



Hi, I have set up an imacros vb script to login to a site and download a file - it is working when browser is being ran normally, but when I run it in -silent mode, which is what i would like to do, it is not able to download the file. I am unable to share the website.

ive got the full script running in chrome. When I run the script in silent mode it does download a file but the file is named ""download"" -- which is the text of the download button, and contains this message:
Code: Select all  <div class=""container-fluid"">   
     <div class=""tab-content"">
     <br />
        <div class=""col-lg-12"">
           <div class=""row"">
              <div class=""well"">   
                 <center><b><font color=""red""> An error has occurred:</font></b></center>
                 <center><i> Request not supported this way. </i></center><br /> 
                 <p>If you believe your request is valid, please try to submit the request later. </p>
                 <p>If that does not work please contact [CLIENT at 1-800-CLIENT NUMBER]. You will be asked for the details on
                 your computing environment, error information, how to reproduce it, and your contact information.</p>
                 <br />
                 <button class=""btn btn-primary"" onclick=""history.back()"">Back to Previous Page</button>
           </div>
         </div>
       </div>    
     <br /> 

     </div>
 </div>  
     <script src=""/MailBoxWebService/resources/dist/js/jquery-1.11.0.min.js""></script> 
     <script src=""/MailBoxWebService/resources/dist/js/bootstrap.min.js""></script>   
</body>


The part of the script that is downloading the file is the td[6] in the below. I have tried this using the imacros browser, ie, and chrome --

Does anyone know how I can get this to download in silent mode? I am open to using any browser.
Code: Select all                Set M = Nothing
                Skip = False
                Browser = iim1.iimInit("""", True, 300)
                Browser = iim1.iimSet(""Count"", Count)
                WScript.Echo(""Beginning Loop for File Info and File Download. Current Loop = "" & Count & ""Of"" & Total)
                    M = ""CODE:""
                    M = ""VERSION BUILD=844 RECORDER=CR"" + N
                    M = M + ""SET !PLAYBACKDELAY 0.00"" + N
                    M = M + ""SET !var1 {{Count}}"" + N
                    M = M + ""TAB T=1"" + N
                    M = M + ""TAG XPATH=//*[@id=""""file_details""""]/tbody/tr[{{COUNT}}]/td[2] EXTRACT=TXT"" + N
                    M = M + ""TAG XPATH=//*[@id=""""file_details""""]/tbody/tr[{{COUNT}}]/td[3] EXTRACT=TXT"" + N
                    M = M + ""TAG XPATH=//*[@id=""""file_details""""]/tbody/tr[{{COUNT}}]/td[4] EXTRACT=TXT"" + N
                    M = M + ""TAG XPATH=//*[@id=""""file_details""""]/tbody/tr[{{COUNT}}]/td[5] EXTRACT=TXT"" + N
                    M = M + ""ONDOWNLOAD FOLDER=C:\Temp FILE=* WAIT=YES"" + N
                    M = M + ""WAIT SECONDS=2"" + N
                    M = M + ""TAG XPATH=//*[@id=""""file_details""""]/tbody/tr[{{COUNT}}]/td[6]/a"" + N
                    M = M + ""WAIT SECONDS=5"" + N
                    M = M + ""SET !TIMEOUT_STEP 60"" + N
                Browser = iim1.iimPlayCode(M)
                FileName = iim1.iimGetLastExtract(1)
                FileDate = iim1.iimGetLastExtract(2)
                FileSize = iim1.iimGetLastExtract(3)
                FileIteration= iim1.iimGetLastExtract(4)      

the html for the ""download"" element is 
Code: Select all<a href=""#"" class=""btn btn-primary"" onclick=""downloadFile('rebrkso/.history/REBRKSO.RECAIFPL.J2017333.N06019018951513.2017-11-29.0410','REBRKSO','REBRKSO.RECAIFPL.J2017333.N06019018951513');"">Download</a>

the full script is 

Code: Select allOption Explicit

Const iM_ElementNotFound        = -1300
Const iM_EndOfFile              = -1240
Const iM_EvalError              = -1340
Const iM_BrowzConnErr           = -1340
Const iM_BadParam               = -1200
Const iM_Success                = 1
Const ForAppending              = 8
Const OpenAsASCII               = 0
Const CreateIfNotExist          = True

Dim Count, Date, Extract, FileName, FileDate, FileSize, FileIteration, i, iMacros, M, objFile, Processed, Skip, FileInfo

Dim WshShell    : Set WshShell  = CreateObject(""WScript.Shell"")
Dim iim1        : Set iim1      = CreateObject(""iMacros"")
Dim Browser     : Browser       = iim1.iimOpen(""-cr"", true)
Dim objFso      : Set objFso    = CreateObject(""Scripting.FileSystemObject"")
Dim iDayNumber  : iDayNumber    = DateDiff(""d"", CDate(""1/1/"" & Year(Now)), Now) + 1
Dim varLMonth   : varLMonth     = Month(Now) - 1
Dim varDateFull : varDateFull   = Month(Now) & ""-"" & Day(Now) & ""-"" & Year(Now)
Dim varOldPW    : varOldPW      = ""Client"" & Year(Now) & varLMonth
Dim Output      : Output        = ""C:\temp\Output"" & ""_"" & varDateFull & "".csv""
Dim LogName     : LogName       = ""C:\temp\Client1 iMacros Error log"" & varDateFull & "".txt""
Dim DateSearch  : DateSearch    = ""J"" & Year(Now) & iDayNumber
Dim CountString : CountString   = ""Default""
Dim Total       : Total         = ""Default""
Dim FirstRun    : FirstRun      = ""True""
Dim NewMonth    : NewMonth      = ""False""
Dim N           : N             = vbNewLine

    If Len(Day(Now))    =   1   Then varDay         = ""0"" & varDay
    If Len(Month(Now))  =   1   Then varMonth       = ""0"" & varMonth
    If Day(Now)         =   1   Then FirstOfMonth   = ""True""

    If objFSO.FileExists(Output) Then
        Set Output = objFSO.OpenTextFile(Output, ForAppending, OpenAsASCII)
        FirstRun = False
    Else
        Set Output = objFSO.CreateTextFile(Output, ForAppending, OpenAsASCII)
        If Err <> 0 Then
        End If
    End If
        Browser = iim1.iimSet(""DateSearch"", DateSearch)
        Browser = iim1.iimSet(""varOldPW"", varOldPW)
        
            M = ""CODE:""
            M = ""VERSION BUILD=844 RECORDER=CR"" + N
            M = M + ""TAB T=1"" + N
            M = M + ""TAB CLOSEALLOTHERS"" + N
            M = M + ""URL GOTO=https://www.website.com/"" + N
            M = M + ""TAG SELECTOR=#httpd_username CONTENT=username"" + N
            M = M + ""TAG SELECTOR=#loginButton"" + N
            M = M + ""SET !ENCRYPTION NO"" + N
                If NewMonth = ""False"" Then  
                    M = M + ""TAG POS=1 TYPE=INPUT:PASSWORD ATTR=NAME:httpd_password CONTENT=Password{{!NOW:yyyymm}}"" + N
                    M = M + ""WAIT SECONDS=1"" + N
                    M = M + ""TAG SELECTOR=#loginButton"" + N
                Else
                    M = M + ""TAG SELECTOR=#httpd_password CONTENT=password{{varOldPW}}"" + N
                    M = M + ""ONDIALOG POS=1 BUTTON=OK CONTENT="" + N
                    M = M + ""TAG SELECTOR=#httpd_username CONTENT=username"" + N
                    M = M + ""TAG SELECTOR=#loginButton"" + N
                    M = M + ""TAG XPATH=""""//*[@id='home']/div[3]/div/div/input"" + N
                    M = M + ""TAG POS=1 TYPE=INPUT:PASSWORD ATTR=NAME:currentPassword CONTENT={{varOldPw}}"" + N
                    M = M + ""TAG POS=1 TYPE=INPUT:PASSWORD ATTR=NAME:newPassword CONTENT={{!NOW:yyyymm}}"" + N
                    M = M + ""TAG POS=1 TYPE=INPUT:PASSWORD ATTR=NAME:confirmPassword CONTENT={{!NOW:yyyymm}}"" + N
                    M = M + ""TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:changepassword"" + N
                    M = M + ""TAG SELECTOR=#serviceList>A>IMG"" + N
                    M = M + ""URL GOTO=https://website.com/MailBoxWebService/webmailbox/home"" + N
                End If
            M = M + ""WAIT SECONDS=2"" + N
            M = M + ""TAG SELECTOR=""""#serviceList>A>IMG"""""" + N
            M = M + ""TAG POS=1 TYPE=B ATTR=TXT:Mail<SP>Box"" + N
            M = M + ""TAG POS=1 TYPE=A ATTR=TXT:MyCompany<SP>and<SP>MyCompany<SP>Outbox<SP>(REBRKSO)"" + N
            M = M + ""SET !TIMEOUT_PAGE 180"" + N
            M = M + ""SET !TIMEOUT_STEP 100"" + N
            M = M + ""EVENT TYPE=CLICK SELECTOR=""""HTML>BODY>DIV>DIV:nth-of-type(2)>DIV:nth-of-type(2)>DIV>FORM>DIV>DIV:nth-of-type(2)>LABEL>INPUT"""" BUTTON=0"" + N
            M = M + ""EVENTS TYPE=KEYPRESS SELECTOR=""""HTML>BODY>DIV>DIV:nth-of-type(2)>DIV:nth-of-type(2)>DIV>FORM>DIV>DIV:nth-of-type(2)>LABEL>INPUT"""" CHARS=""""{{DateSearch}}"""""" + N
            'M = M + ""TAG POS=1 TYPE=INPUT:SEARCH FORM=NAME:downloadFiles ATTR=* CONTENT={{DateSearch}}"" + N
            M = M + ""TAG SELECTOR=#file_details_info EXTRACT=TXT"" + N
            M = M + ""SET !TIMEOUT_STEP 25"" + N
        Browser = iim1.iimPlayCode(M)
        CountString = iim1.iimGetLastExtract(1)
        Total = trim(split(CountString, "" "")(5))
            For Count = 1 to Total
                Set M = Nothing
                Skip = False
                Browser = iim1.iimInit("""", True, 300)
                Browser = iim1.iimSet(""Count"", Count)
                WScript.Echo(""Beginning Loop for File Info and File Download. Current Loop = "" & Count & ""Of"" & Total)
                    M = ""CODE:""
                    M = ""VERSION BUILD=844 RECORDER=CR"" + N
                    M = M + ""SET !PLAYBACKDELAY 0.00"" + N
                    M = M + ""SET !var1 {{Count}}"" + N
                    M = M + ""TAB T=1"" + N
                    M = M + ""TAG XPATH=//*[@id=""""file_details""""]/tbody/tr[{{COUNT}}]/td[2] EXTRACT=TXT"" + N
                    M = M + ""TAG XPATH=//*[@id=""""file_details""""]/tbody/tr[{{COUNT}}]/td[3] EXTRACT=TXT"" + N
                    M = M + ""TAG XPATH=//*[@id=""""file_details""""]/tbody/tr[{{COUNT}}]/td[4] EXTRACT=TXT"" + N
                    M = M + ""TAG XPATH=//*[@id=""""file_details""""]/tbody/tr[{{COUNT}}]/td[5] EXTRACT=TXT"" + N
                    M = M + ""ONDOWNLOAD FOLDER=C:\Temp FILE=* WAIT=YES"" + N
                    M = M + ""WAIT SECONDS=2"" + N
                    M = M + ""TAG XPATH=//*[@id=""""file_details""""]/tbody/tr[{{COUNT}}]/td[6]/a"" + N
                    M = M + ""WAIT SECONDS=5"" + N
                    M = M + ""SET !TIMEOUT_STEP 60"" + N
                Browser = iim1.iimPlayCode(M)
                FileName = iim1.iimGetLastExtract(1)
                FileDate = iim1.iimGetLastExtract(2)
                FileSize = iim1.iimGetLastExtract(3)
                FileIteration= iim1.iimGetLastExtract(4)                                
                Wscript.Echo(""File Name = "" & FileName)
                Wscript.Echo(""File Date = "" & FileDate)
                Wscript.Echo(""File Size = "" & FileSize)
                Wscript.Echo(""File Iteration = "" & FileIteration)
                If objFSO.FileExists(FileName) Then
                    Count = Count + 1
                Else
                    If Browser = iM_Success Then
                        FileInfo = FileName & "","" & FileDate & "","" & FileSize & "","" & FileIteration
                        Count = Count + 1
                        Output.Write(FileInfo) & vbCrLf
                    ElseIf Browser = iM_BadParam Then
                        Count = Count + 1
                        FileInfo = iim1.iimGetLastExtract(1)
                    Else
                    End If
                End If
            Next
        Set M  = Nothing
        Browser = iim1.iimInit(""-cr"", false, 300)
        Browser = iim1.iimSet(""Count"", Count)
            M = ""CODE:""
            M = ""VERSION BUILD=844 RECORDER=CR"" + N
            M = M + ""TAG POS=1 TYPE=INPUT:SEARCH ATTR=ARIA-CONTROLS:file_details"" + N
            M = M + ""TAG POS=1 TYPE=H4 ATTR=TXT:Files<SP>Available<SP>for<SP>Download<SP>under<SP>mailbox<SP>REBRKSO"" + N
            M = M + ""TAG POS=1 TYPE=A ATTR=TXT:Account"" + N
            M = M + ""TAG POS=1 TYPE=SPAN ATTR=CLASS:glyphicon<SP>glyphicon-log-out"" + N
        Browser = iim1.iimPlayCode(M)
    Output.Close                                                        'Close the CSV File
    Browser = iim1.iimExit()                                            'Close the Browser
    WScript.Echo(""Data Extracted : "" & FileInfo)
    If Count <> Total Then WScript.Echo(""Proceeding to next iteration ("" & Count & "")"")
    If Count = Total Then WScript.Echo(""All iterations completed ("" & Count & "")"" & N & ""Closing browser, input file & output file) "")
    If Browser < 0  then                                                'If there is an error, take a screenshot, send email, and stop processing
        Browser = iim1.iimTakeBrowserScreenshot (""./screenshot of Discover Error  ""  & Hour(Now)&Minute(Now)&Second(Now) & "".png"")
        Output.Close    
        Browser = iim1.iimExit()
        If iscr  < 0 then
                                                                        'Need to Send an Email      
        End If
    End If
    WScript.Echo(vbNewLine & ""------------ Processing Complete ------------"")
    WScript.Quit()
    


Thanks for any help!",https://forum.imacros.net/viewtopic.php?f=7&t=28184&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2104,"= Replace(iim1.iimGetLastExtract(), ""[EXTRACT]"", """")VBA prob","Hi guys  i have any problem could you some one help me please ...

i try some data extract....  if solve the problem i will  use  same  loop   in my code  therefore  i write only   abbreviated  sample from my code ... 

Code: Select allMyMacroCode = MyMacroCode + ""TAG POS=1 TYPE=INPUT:SUBM陌T FORM=NAME:sifreGirisForm ATTR=NAME:submitButton"" + vbNewLine
MyMacroCode = MyMacroCode + ""TAG POS=1 TYPE=A ATTR=TXT:Telefon<SP>No<SP>Sorgulama"" + vbNewLine
MyMacroCode = MyMacroCode + ""FRAME f = 2"" + vbNewLine
MyMacroCode = MyMacroCode + ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:form1 ATTR=NAME:ba CONTENT=xxxx"" + vbNewLine
MyMacroCode = MyMacroCode + ""TAG POS=1 TYPE=INPUT:BUTTON FORM=NAME:form1 ATTR=NAME:submit"" + vbNewLine
MyMacroCode = MyMacroCode + ""TAG POS=1 TYPE=TH ATTR=TXT:Birim<SP>Ad谋"" + vbNewLine
MyMacroCode = MyMacroCode + ""TAG POS=R2 TYPE=TD ATTR=* EXTRACT=TXT"" + vbNewLine
MyMacroCode = MyMacroCode + ""SET !EXTRACTDIALOG YES"" + vbNewLine
MyMacroCode = MyMacroCode + ""PROMPT {{!EXTRACT}}"" + vbNewLine

'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
'''''  ''""""""""""""""""""""""""""' yes i can  see  exctract  just here ... no problem..  when  PROMPT   dialog style """"""""""""""""""""""""
'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx


'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
iret = iim1.iimset(""TCNO"", ws3.Cells(2, 2).Value)
iret = iim1.iimset(""PASS"", ws3.Cells(2, 3).Value)
ExtractedValue = Replace(iim1.iimGetLastExtract(), ""[EXTRACT]"", """")
iret = iim1.iimPlayCode(MyMacroCode)

MsgBox (ExtractedValue)

'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
'''''  ''""""""""""""""""""""""""""' there is problem msgbox value  get empty """"""""""""""""""""""""  i trry  Replace(iim1.iimGetLastExtract(1)   msgbox value  get NODATA value ..
'''' what is the wrong ....
'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx


widows 8  64 bit
office  2010 
imacros  10  brovser   10.02.2823
VERSION BUILD=10022823
mozilla firefox 

thnx you so much",https://forum.imacros.net/viewtopic.php?f=7&t=28149&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2105,"Auto click ""Save As"" button (SOLVED)","iMacros (8.44) + Browser (Chrome 61.0.3163.100 (Official Build) (32-bit)) + OS (Windows 7 Ultimate)
Code: Select allVERSION BUILD=844 RECORDER=CR

TAB OPEN
TAB T=2
SET !DATASOURCE savepicture.csv
SET !ERRORIGNORE YES
SET !LOOP 1

set !replayspeed medium
URL GOTO={{!COL2}} 
ONDOWNLOAD FOLDER=C:\Users\men_tshirt FILE={{!COL1}}.jpg WAIT=yes
TAG POS=1 TYPE=IMG ATTR=HREF:https://*.jpg CONTENT=EVENT:SAVEITEM

TAB CLOSE

The macro stuck at the save as pop-up. Is there anyway to automate clicking the save as? I've tried event=click and even keypress but failed. Any advice? Thanks.",https://forum.imacros.net/viewtopic.php?f=7&t=28105&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2106,how to loop from certain line of script,"Hi;
running FF 55.0.3 on Win 7 32 bit, imacros ver. v8.9.7

I did use the script on this thread with the help of chivracq
http://forum.imacros.net/viewtopic.php? ... ail#p70484

but I want to do little more on this, and struggle to make it work,
I want the script skip one or few lines of script and continue.
but I could not figure it out how to do it with imacros, I am not sure if this can be done with imacros coding.
found some js loop script on this site, none I could use for this.

I don't think it is needed to paste the script from the link above, because I need only make the loop thing works.
this is what I want to do:

1- TAB 1- go to google and do a search
2- extract the content of the first page results, and click to the next page of the result
3- TAB 2 ,go to second url e.g. xyz.com
4- paste the data in there, extract new data
5- save the new data to a file
6- go back to line 2( which is On TAB 1, the second page of google result)


and continues to line 5, and then again from line 2.
so it is a loop, only the first time it starts at line 1, the second time and further starts from line 2 until the number of loops ends

I need it to set a number of loops as well, I don't want to do the loop unlimited.

appreciate any help on this.",https://forum.imacros.net/viewtopic.php?f=7&t=28014&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2107,Need help with simple google result page extract,"Hello;
I need help with my code, it does not work and gives me error:
allocation size overflow, line: 10 (Error code: -1001)

what I want to do is simple,
part 1:
search Google for a keyword, when the result appears, copy the whole page to a text or csv file
just like when you highlight the page using (Ctrl+A)and copy paste it to a text file,

part 2:
then I want the code continues on to the next page of the result, and do the same thing, add it to the same file and continues until 
the end of result pages, but I am stuck in the fist part, I have not tried the second part yet.

here is my code:
Code: Select allVERSION BUILD=9030808 RECORDER=FX
SET !EXTRACT_TEST_POPUP NO
TAB T=1
URL GOTO=https://www.google.com/
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:tsf ATTR=ID:lst-ib CONTENT=dog<sp>grooming
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:tsf ATTR=NAME:btnK
TAG POS=1 TYPE=HTML ATTR=* EXTRACT=TXT
WAIT SECONDS=2
SAVEAS TYPE=EXTRACT FOLDER=* FILE=+{{!NOW:ddmmyyyy}}.txt


appreciate your help to make this work.",https://forum.imacros.net/viewtopic.php?f=7&t=28000&sid=01bf0edf807fd3d4bb507a35b87155ad,content
2109,Simple YouTube macro isn't working,"This simple series of clicks is supposed to hit the SHARE button on YouTube, then COPY button for the URL that appears in the popup, then dismiss the popup. (Note that YT has changed the layout of the page very recently).

VERSION BUILD=9030808 RECORDER=FX
TAB T=1
TAG POS=6 TYPE=YT-FORMATTED-STRING ATTR=ID:text
TAG POS=8 TYPE=YT-FORMATTED-STRING ATTR=ID:text
TAG POS=1 TYPE=DIV ATTR=ID:owner-container
TAG POS=8 TYPE=YT-FORMATTED-STRING ATTR=ID:text
TAG POS=2 TYPE=DIV ATTR=ID:info

However, it's as if the string is not passed to the clipboard properly. Instead, the clipboard contains the string associated with the previous run of the macro. That is why there are two tries at TAG POS=8 - trying to get the current URL onto the clipboard.

The purpose is to extract the YouTube link for pasting into an online content management system. I can't figure out the hitch here.",https://forum.imacros.net/viewtopic.php?f=7&t=27929&sid=8ea127909c0d83c2fac4f53ec6c3d19a,content
2110,If equals info in excel cell then...,"Version 11.5
Windows 10 
IE 11

If the number I extract from a website is equal to the number in Column 1 of excel I would like to grab the name in Column 2 of excel and paste it back into a field on the website.  If it does not equal then check next row until there's a match.  Is it possible to perform this?  I imagine I must have to use EVAL to check for a match but I am unsure how to set up the EVAL statement below.
TAG POS=26 TYPE=TD ATTR=* EXTRACT=TXT
SET !LOOP 1
SET !DATASOURCE C:\Users\*\Desktop\*.csv
SET !VAR1 EVAL(""if({{!COL1}}=={{!EXTRACT}}){{{!COL2}}}else{do nothing};"") 
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:txtBorrower CONTENT={{!VAR1}}",https://forum.imacros.net/viewtopic.php?f=7&t=27115&sid=8ea127909c0d83c2fac4f53ec6c3d19a,content
2111,Data extraction from .CSV -> export/submission on webpage.,"System info:
1. VERSION BUILD=9.030808 RECORDER=FX
2. Windows 10 - English
3. Firefox 54.0.1
4. The macro works okay, it just doesn't have all the features I need.
5. No problems
6. No problems
7. Just need help with the macro.

I am attempting to submit a large amount of zip codes (for each state in the U.S.), and would like to export data from my .CSV file and loop the function to run through the single column of data.

----------------------------------------------------------------------------------------------------------------------------------------------

Here is my example:
SET !DATASOURCE zips.csv
SET !DATASOURCE_COLUMNS 1
SET !ERRORIGNORE YES
'
'
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:agentProfileForm ATTR=ID:serviceAreaZip CONTENT={{!COL1}}
WAIT SECONDS=1
TAG POS=1 TYPE=INPUT:BUTTON FORM=ID:agentProfileForm ATTR=ID:zips

----------------------------------------------------------------------------------------------------------------------------------------------
The function works partially (I am open to any, and all suggestions). Furthermore, after I type in the zip code, the website displays a ""X reps found in this area."" I was curious to see if it is possible to basically export a number from the answer ""X"" and import it into the 2nd column of my .CSV file.

To summarize it, I am looking to critique my macro to make sure I have set it up correctly, and to also seek assistance on how to export an answer from the submission on the website to the 2nd column in my .CSV file. I appreciate any assistance that I may receive.

(Pic of the result whenever it finds reps)
https://drive.google.com/file/d/0B-7PKC ... 0txMF92bmM

(Pic of the result whenever it doesn't find reps)
https://drive.google.com/open?id=0B-7PK ... WUtUzRESGM

EDIT/UPDATE:
After some consideration, I have gotten to the point where I am able to extract the text into the .CSV file but I am having difficulty trying to choose which column to output the data into. It seems to be that the input isn't the problem, just the export from the text extraction to the input of the extracted data into the .CSV.",https://forum.imacros.net/viewtopic.php?f=7&t=27807&sid=8ea127909c0d83c2fac4f53ec6c3d19a,content
2112,Bittrex,"Hi guys, 
my macro is for bittrex.com.
I wanna make a macro for make a sale order 15% bigger the ask price .

I have write a code but he dont work....
VERSION BUILD=9030808 RECORDER=FX
TAB T=1
TAG POS=1 TYPE=DIV ATTR=TXT* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
SET INC 1.15
SET VALUE EVAL(""{{!VAR1}}*{{INC}}"")
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:form_Sell ATTR=NAME:price_Sell CONTENT={{!VALUE}}

anyone can help me.
thanks (sorry for my english im french)",https://forum.imacros.net/viewtopic.php?f=7&t=27806&sid=8ea127909c0d83c2fac4f53ec6c3d19a,content
2113,Search Results,"Being new to iMacros, I'm attempting to write a macro to process through a search of names.
I've looked at the documentation and search the forum, but didn't find any examples of what I need to do.
Seems like it would be simple to do, but so far, nothing I've tried works. Could be my syntax of just not using the
command correctly.
Here is the issue: after a user logs in, they need to enter a persons last name to do a search on that.
I prompt for the name and store it in var1, that part works okay.
Once I use that in the search, the search sometimes returns maybe 25 or more last name, first name pairs that
might stretch over 7, 8, 9 pages.
The user then negotiates over those pages till they find a match.
Could be anywhere, say it's on page 4.
I then need to capture the results of that exact match and then use that data later one.
This is what I have 

PROMPT ""Please enter last name name:"" !VAR1 
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:/Access/NameSearch/Results ATTR=NAME:TextSearchQuery CONTENT={{!VAR1}}
TAG POS=1 TYPE=BUTTON:SUBMIT FORM=ACTION:/Access/NameSearch/Results ATTR=ID:SearchButton

/* User now is presented with a list, several pages of names that they 
   must select from

TAG POS=1 TYPE=A ATTR=TXT:2
TAG POS=1 TYPE=A ATTR=TXT:3
TAG POS=1 TYPE=A ATTR=TXT:4

/* After the user finds the correct match, they select it and continue on
   with the results of their selection. This is the results from their select
    action:

TAG POS=1 TYPE=TD ATTR=TXT:SMITH,<SP>JOHN<SP>A

I've tried a number of solutions, nothing has worked. 
Here is one example of what I tried with the select command:

TAG POS=1 TYPE=SELECT FORM=NAME:form1 ATTR=NAME:select1 CONTENT=${{!VAR1}}

I just need to know what the user selected, grab that data and continue on.
Any tips or suggestions would be appreciated.",https://forum.imacros.net/viewtopic.php?f=7&t=27803&sid=8ea127909c0d83c2fac4f53ec6c3d19a,content
2114,Extracting attributes values dynamically imacros,"This is the html text
Code: Select all  <select name=""ctl00$ContentPlaceHolder1$ddltype"" id=""ctl00_ContentPlaceHolder1_ddltype"" class=""page-heading"" style=""background-color: rgb(255, 244, 244); width: 125px; outline: 1px solid blue;"">
    			<option value=""0"">Select Bricks</option>
    			<option value=""brk_A_price"">A</option>
    			<option value=""brk_B_price"">B</option>
    			<option value=""brk_oth_price"">Others</option>
    
    		</select>
    .
    .
    .
    .
    <select name=""ctl00$ContentPlaceHolder1$ddlReportOn"" id=""ctl00_ContentPlaceHolder1_ddlReportOn"" class=""page-heading"" style=""background-color:#FFF4F4;width:135px;"">
    			<option value=""0"">Select Report For</option>
    			<option value=""1"">Comparison</option>
    			<option value=""2"">Variation</option>
    
    		</select>

So from above html I want to extract option Values i.e brk_A_price,brk_B_price,brk_oth_price from first select tag and 1, 2 from second select tag.
Code: Select all 'code to get brk_A_price,brk_B_price,brk_oth_price
    TAG POS=1 TYPE=SELECT ATTR=id:ctl00_ContentPlaceHolder1_ddltype&&TXT:* EXTRACT=HTM
    TAG POS R1 TYPE=option ATTR=value&&TXT
    
    'code to get 1, 2
    TAG POS=1 TYPE=SELECT ATTR=id:ctl00_ContentPlaceHolder1_ddlReportOn&&TXT:* EXTRACT=HTM
    TAG POS R1 TYPE=option ATTR=value&&TXT

However, for every dropdown I would want to change just the **id** attribute, but for the code I tried it is just able to extract the text and not values from `<option..>` tag.

> p.s: http://briks.gov.in/Report/Rept_Building_Brik.aspx is the site from where I'm extracting data

Any suggestion on what changes should be made.
Any help would be much appreciated.
Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=27758&sid=8ea127909c0d83c2fac4f53ec6c3d19a,content
2116,Checking for a word in .csv file,"CIM iMacros for FF v9.0.3 Firefox 54.0 Windows 10

I want to check some data from .csv file with random word from site. If that word exists - to do something and if the word doesn't exist - to do something else. But I didn't find anything similar. EVAL function should do that but I don't know how to check whole .csv 

Example:
.csv file example.csv

content from example.csv is

science
cubic
virtual
internet
form
future

If I check the word science then it does one operation and if the word is clinic then something else. Is this possible with iMacro and .csv file

If yes, just give me some directions, please. If not, sorry for taking your time, simple no is just fine.

Cheers !",https://forum.imacros.net/viewtopic.php?f=7&t=27644&sid=8ea127909c0d83c2fac4f53ec6c3d19a,content
2117,How to scrap an website when you don't know the URL for that,"I have this website that I want retrieve information from. Unfortunately I can't show you because it is password protected.

The site is a list of tables that are dynamically created. 
 they design the link to activate a script. The first link is:
javascript:__doPostBack('ctl00$ContentPlaceHolder1$GridView1$ctl02$LinkButton1','')

The second link is the same, but with the last variable changed ($ctl03')""). It goes from 0 to 30


so is there any possiblity to change url automatic each loop",https://forum.imacros.net/viewtopic.php?f=7&t=27578&sid=8ea127909c0d83c2fac4f53ec6c3d19a,content
2118,How to loop Css clicks and click subsequent CSS,"Config: Windows 10 64 bit, IMacro Enterprise v11.5.499_3066, Imacro Browser, 
http://forum.imacros.net/search.php 
Essentially I want to click [All Results] and then once that鈥檚 done.. I then want to click the nav tabs (that open once I鈥檝e clicked [ALL RESULTS].  Lets say once I click [All Results] it had 1400 different buttons I wanted to click, it doesn鈥檛 but lets say it did.  How would I go about looping this?  


The Css after the [ALL RESULTS] is鈥?#st > option:nth-child(1) 
#st > option:nth-child(3) 
#st > option:nth-child(1) 
I have thus far鈥?VERSION BUILD=11.5.499.3066
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
URL GOTO=http://forum.imacros.net/search.php
TAG SELECTOR=""#st"" CONTENT=%1 
TAG POS=((!LOOP)) TYPE=SELECT ATTR=CLASS* EXTRACT=TXT <-  Is there a way to do this for CSS like increasing in a variable or somehow not having to type the exact CSS name and all for each of them? As I want it to continually loop click TAG SELECTOR=""#st"" CONTENT=%1 and then click subsequent CSS.  Thanks in advance, I appreciate it.",https://forum.imacros.net/viewtopic.php?f=7&t=27536&sid=8ea127909c0d83c2fac4f53ec6c3d19a,content
2120,Use ONWEBPAGEDIALOG with the MACRO parameter,"I have a macro that populates a WebPage Dialog box.  It was working until just recently.  We have incorporated the information on how to ""Use ONWEBPAGEDIALOG with the MACRO parameter"".  There are 2 issues: 

(1)There are 2 text boxes and 2 drop downs with start and end time.  Following these instructions, I can record changing the text boxes but the drop downs are frozen.  Is this part of iMacros or some option in IE?

(2) When I record changing the text boxes, I compare it to the existing code and it is identical.  I even pasted over the existing code with what I recorded.

When I run the macro, I get an error code -1300.  It is erroring out on the following line:

TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:popupForm ATTR=NAME:eventDesc CONTENT=Memorial<SP>Day

This is the first line in the RFSDialgCHECK.iim.  I'm not sure what it can't find this element when it is what is recorded in iMacros.

Following is the code that runs the the RFSDialgCHECK.iim:

Dim ID As String

 IM = """"
 IM = ""CODE:""
 IM = IM + ""TAB T=1"" + vbNewLine
 IM = IM + ""ONWEBPAGEDIALOG MACRO=C:\Temp\RFSDialgCHECK.iim"" + vbNewLine
 IM = IM + ""TAG POS=5 TYPE=INPUT ATTR=CLASS:*"" + vbNewLine
 IM = IM + ""WAIT SECONDS=2"" + vbNewLine

 Dim eScript As Object
 Set ObjScr = New Scripting.FileSystemObject
 Set eScript = ObjScr.CreateTextFile(""C:\Temp\RFSDialgCHECK.iim"", True)

 ID = ID + ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:popupForm ATTR=NAME:eventDesc CONTENT="" & EDescr + vbNewLine

 If OTime = ""CLOSED"" Then

      ID = ID + ""TAG POS=1 TYPE=SELECT FORM=NAME:popupForm ATTR=NAME:eventOpenTime CONTENT=%"" & OTime + vbNewLine
      ID = ID + ""TAG POS=1 TYPE=SELECT FORM=NAME:popupForm ATTR=NAME:eventCloseTime CONTENT=%"" & ETime + vbNewLine

Else

     ID = ID + ""TAG POS=1 TYPE=SELECT FORM=NAME:popupForm ATTR=NAME:eventOpenTime CONTENT=%"" & NewOpenTime + vbNewLine
     ID = ID + ""TAG POS=1 TYPE=SELECT FORM=NAME:popupForm ATTR=NAME:eventCloseTime CONTENT=%"" & NewCloseTime + vbNewLine

End If

ID = ID + ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:popupForm ATTR=NAME:eventStartDate CONTENT="" & EDate + vbNewLine
ID = ID + ""TAG POS=1 TYPE=INPUT:BUTTON FORM=NAME:popupForm ATTR=NAME:add"" + vbNewLine

eScript.Write ID
eScript.Close

iret = iim1.iimPlay(IM)

Any help would be appreciated.  Thanks for your time.",https://forum.imacros.net/viewtopic.php?f=7&t=24275&sid=8ea127909c0d83c2fac4f53ec6c3d19a,content
2121,"aliexpress data extraction in ""product description""","hi,

i try to scraping aliexpress product page to get ""product description"" tab section.

https://www.aliexpress.com/item/Vfemage ... 72852.html

there is page i need to get ""product description""

i inspect this section as TAG POS=1 TYPE=DIV ATTR=class:description-content EXTRACT=HTM

however, an extract result was contain following only:

        <div class=""ui-box-title"">Product Description</div>
        <div class=""ui-box-body"">

            <div class=""description-content"" data-role=""description"" data-spm=""1000023"">
            <div class=""loading32""></div>
            </div>

        </div>

it seems they hide a HTM content or using any javascript to hide.

is there any method to reveal a content when scraping?

thank",https://forum.imacros.net/viewtopic.php?f=7&t=27455&sid=8ea127909c0d83c2fac4f53ec6c3d19a,content
2123,Change Drop Down Button Using CSV,"Version 9.0.3
Windows 10
Firefox 50.1.0

Any one help this code, please

VERSION BUILD=9030808 RECORDER=FX
TAB T=1
SET !DATASOURCE ""C:\\Users\\OFFICE\\Documents\\iMacros\\Datasources\\JobCard-No.csv""
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}

'Select Village name
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_DDL_Village EXTRACT=TXT

' Extract Length 15. : TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_DDL_Village CONTENT=%2906015005007<SP><SP><SP><SP><SP>

SET !VAR5 EVAL(""'{{!EXTRACT}}'.trim();"")

'After trim Extract Length is 13. Because i need to check both in .csv 4th column length is 13.

SET !VAR3 EVAL(""if({{!COL4}}!={{!VAR5}}){TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_DDL_Village CONTENT={{!COL4}}} else{do nothing and move to next row until a match is found};"") 

I have some error.

missing ; before statement, line: 22 (Error code: -1001)


.CSV File contain




",https://forum.imacros.net/viewtopic.php?f=7&t=27415&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2124,Extracting Attributes,"It is to my understanding that Imacros can extract parameters now as explained here: http://wiki.imacros.net/TAG#Extract_Custom_Attributes

However, it doesn't explain much about this. After testing on Firefox and not getting it to work, I realized it said it only works with 10.2 which isn't a Firefox version. Does it work with Internet Explorer Imacros?

I hear it can be done with scripts but I don't have a lot of knowlege on so if someone can point me in the direction, the below content in the ""value"" attribute is what needs to be extracted.
Code: Select all<input name=""bullet_point1"" value=""CONTENT TO BE EXTRACTED"">",https://forum.imacros.net/viewtopic.php?f=7&t=27224&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2125,Extract Point Changes - How do I Extract Text,"Hello,

I'm having an issue with a extract point for my macro. When I change the search parameter the site is returning the data in a different point and I am unable to extract the information.

This is the first macro where the search parameter is 50288578 and the extract point is the last line below.
Code: Select allVERSION BUILD=9002379
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=https://www.rrts.com/Pages/Home.aspx
TAG POS=2 TYPE=I FORM=ACTION:/Pages/Home.aspx ATTR=CLASS:icon-tasks
WAIT SECONDS=1
TAG POS=2 TYPE=INPUT:RADIO FORM=ACTION:/Tools/Tracking/Pages/default.aspx ATTR=NAME:ctl00$ctl51$g_58aef1dd_2091_421b_99ff_86b055c64d8b$Type CONTENT=YES
WAIT SECONDS=1
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:/Tools/Tracking/Pages/default.aspx ATTR=NAME:ctl00$ctl51$g_58aef1dd_2091_421b_99ff_86b055c64d8b$rptPRO$ctl00$rtbPRO CONTENT=50288578
WAIT SECONDS=1
TAG POS=1 TYPE=INPUT:BUTTON FORM=ACTION:/Tools/Tracking/Pages/default.aspx ATTR=NAME:ctl00$ctl51$g_58aef1dd_2091_421b_99ff_86b055c64d8b$cmdSubmit
WAIT SECONDS=10
TAG POS=1 TYPE=SPAN FORM=ACTION:/Tools/Tracking/Pages/MultipleResults.aspx?PUNS=NTAyODg1Nzg= ATTR=ID:ctl00_ctl51_g_a00bf657_07a9_444e_9b38_fb00487e0952_TraceResultID50288578_lblPro* EXTRACT=TXT


This is the second macro where the search parameter is 50288721 and the extract point is the last line below.
Code: Select allVERSION BUILD=9002379
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=https://www.rrts.com/Pages/Home.aspx
TAG POS=2 TYPE=I FORM=ACTION:/Pages/Home.aspx ATTR=CLASS:icon-tasks
WAIT SECONDS=1
TAG POS=2 TYPE=INPUT:RADIO FORM=ACTION:/Tools/Tracking/Pages/default.aspx ATTR=NAME:ctl00$ctl51$g_58aef1dd_2091_421b_99ff_86b055c64d8b$Type CONTENT=YES
WAIT SECONDS=1
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:/Tools/Tracking/Pages/default.aspx ATTR=NAME:ctl00$ctl51$g_58aef1dd_2091_421b_99ff_86b055c64d8b$rptPRO$ctl00$rtbPRO CONTENT=50288721
WAIT SECONDS=1
TAG POS=1 TYPE=INPUT:BUTTON FORM=ACTION:/Tools/Tracking/Pages/default.aspx ATTR=NAME:ctl00$ctl51$g_58aef1dd_2091_421b_99ff_86b055c64d8b$cmdSubmit
WAIT SECONDS=10
TAG POS=1 TYPE=SPAN FORM=ACTION:/Tools/Tracking/Pages/MultipleResults.aspx?PUNS=NTAyODg3MjE= ATTR=ID:ctl00_ctl51_g_a00bf657_07a9_444e_9b38_fb00487e0952_TraceResultID50288721_lblPro* EXTRACT=TXT

Notice that the extract line is in a different place.

Marco 1
Code: Select allTAG POS=1 TYPE=SPAN FORM=ACTION:/Tools/Tracking/Pages/MultipleResults.aspx?PUNS=NTAyODg1Nzg= ATTR=ID:ctl00_ctl51_g_a00bf657_07a9_444e_9b38_fb00487e0952_TraceResultID50288578_lblPro* EXTRACT=TXT

Macro 2
Code: Select allTAG POS=1 TYPE=SPAN FORM=ACTION:/Tools/Tracking/Pages/MultipleResults.aspx?PUNS=NTAyODg3MjE= ATTR=ID:ctl00_ctl51_g_a00bf657_07a9_444e_9b38_fb00487e0952_TraceResultID50288721_lblPro* EXTRACT=TXT

As a result I am unable to automate this from code if I make the search parameter a variable and try to repeat the macro by just changing the search parameter.

How do I adjust for this so that I am always able to extract the data regardless of the search parameter?

Thank you in advance for you help.

Harry.",https://forum.imacros.net/viewtopic.php?f=7&t=27319&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2126,Extract Gift Card Balance,"I'm working on an iMacros that 

* Checks the balance of a gift card
* Extracts that balance into a spreadsheet

I'm using a 3 column CSV (datasource):

* Card #
* Exp date
* PIN

A new tab is opened for each line of datasource.

I would like to extract the balance into the original datasource, so that the new CSV would have 4 columns:

* Card #
* Exp date
* PIN
* Balance

I know I could extract it into a separate CSV, then paste it back into my original datasource, but I'd prefer to skip this extra step and have everything in one file.

Here is the incomplete code so far; I haven't added a destination for the extraction yet, b/c I'm hoping to add it back to the datasource - if that's even possible.
Code: Select allVERSION BUILD=5010424 RECORDER=CR
' Check the balance of a Simon Gift Card. Prompts for the Captcha, submits and opens a new tab to check the next gift card balance.
' This method is useful if you have a # of cards to check: the balance will load in the background as another tab opens and checks the next card.
' Once it finishes the spreadsheet, visit each tab to see the balance on your card.
' Feel free to modify and adapt this script.
SET !DATASOURCE SimonGiftCards.csv 
SET !DATASOURCE_COLUMNS 3
SET !VAR1 2
SET !LOOP {{!VAR1}}
SET !DATASOURCE_LINE {{!LOOP}} 
SET !ERRORIGNORE YES
URL GOTO=https://www.simon.com/giftcard/card_balance.aspx
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:aspnetForm ATTR=ID:ctl00_ctl00_FullContent_MainContent_tbNumber CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:aspnetForm ATTR=ID:ctl00_ctl00_FullContent_MainContent_tbExpDate CONTENT={{!COL2}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:aspnetForm ATTR=ID:ctl00_ctl00_FullContent_MainContent_tbCid CONTENT={{!COL3}}
' The Captcha was covered by the dialog box, so added a new position tag to move the webpage back to the top.
TAG POS=1 TYPE=SECTION ATTR=TXT:The<SP>Simon<SP>Giftcard庐<SP>Gift<SP>Card<SP>Corporate<SP>Sa*
PROMPT ""Captcha:"" !VAR3
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:aspnetForm ATTR=ID:ctl00_ctl00_FullContent_MainContent_CaptchaCodeTextBox CONTENT={{!VAR3}}
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:aspnetForm ATTR=ID:ctl00_ctl00_FullContent_MainContent_checkBalanceSubmit
' Wait for the results page to open with the balance of the gift card.
WAIT SECONDS=20
' Extract the balance of the gift card 
TAG POS=1 TYPE=SPAN ATTR=ID:ctl00_ctl00_FullContent_MainContent_lblBalance EXTRACT=TXT
WAIT SECONDS=20
TAB OPEN
TAB T={{!LOOP}}
SET !VAR2 {{!LOOP}}
ADD !VAR2 -{{!VAR1}}
ADD !VAR2 2
TAB T={{!VAR2}}

FCIM:
VERSION BUILD=5010424 RECORDER=CR
Mac OS X 10.12.3 (16D32)
Chrome Version 56.0.2924.87 (64-bit) - also use Chrome Canary and Firefox
Included demos work",https://forum.imacros.net/viewtopic.php?f=7&t=27265&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2127,Extracting data and paginating on webpage,"Hello respected coders and moderators,

iMacros - latest version (i am not sure where to find that) , Firefox 51.0.1 (32-bit), Windows 7 SP1

I have a list of rows which I use as a datasource for a search form on a website. This website then gives results which could be anywhere from 0 to any number. All these results are then shown with pagination. My requirement is to find the results and paginate among the pages, extract and save as CSV file. I wrote a script which does most of it. However there are 2 problems:

1) I am never sure how many records will show up
2) I am not sure how to handle no records found scenario
3) I am not sure how to paginate if the pagination control exists and if it doesnt exist then obviously need to handle that scenario.

Code: Select allVERSION BUILD=6861208     
TAB T=1     
'TAB CLOSEALLOTHERS     

'SET !EXTRACT_TEST_POPUP YES
'SET !ERRORIGNORE YES

'Declaring all the variables which the macro will need

SET !DATASOURCE C:\pc\list.csv
SET !DATASOURCE_COLUMNS 1
SET !DATASOURCE_COLUMNS 2

SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}

URL GOTO=http://www.mywebsite.com/amember4/mymember/myday/mysearch.html

'Set the values in the search fields from my source csv file
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:formtwnonly ATTR=ID:q3237 CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:formtwnonly ATTR=ID:q3238 CONTENT={{!COL2}}
'Invoke search
TAG POS=1 TYPE=INPUT:IMAGE FORM=NAME:formtwnonly ATTR=ID:ajax_bt7

FRAME F=1

'Extract text
TAG POS=1 TYPE=TABLE ATTR=TXT:* EXTRACT=TXT
'TAG POS=2 TYPE=TABLE ATTR=TXT:* EXTRACT=TXT
'TAG POS=3 TYPE=TABLE ATTR=TXT:* EXTRACT=TXT

'Remove quotes from extracted text
SET !VAR2 EVAL(""var extr2=\""{{!EXTRACT}}\""; extr2.replace(/'/g,'');"")

'FRAME NAME=""iframe_a""
'Paginate
'However it may not be always shown
'TAG POS=1 TYPE=FONT ATTR=TXT:Next<SP>50<SP>>>

'Save extracted data
SAVEAS TYPE=EXTRACT FOLDER=C:\pc FILE=TestData.csv

I would appreciate help if any!",https://forum.imacros.net/viewtopic.php?f=7&t=27253&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2128,Extract Data Using csv. & String Fn. Open Multi Tab,"Version 9.0.3
Windows 10 
Firefox 50.1.0

Sir,
       I extract data from csv. file. contain few digits. 

csv.file :
/87-
/90-
/482-
/77-
/85-
/279-
/489-
/285-

Using It extracted from drop down list  and compare the  both csv.file & extracted. 
In drop down list different from extracted text (i.e. index value extracted )





So i could not use #EANF# method. 

Some time extracted text vary. (i.e. index value length)



EXTRACTED DATA.jpg (12.5 KiB) Viewed 3733 times


so depends upon csv file length. compare extracted text. In mean time I am using String function. I get  following error. 

""VAR4 is not defined, line: 26 (Error code: -1001)""

Kindly do the needful. 
Here is the code :

VERSION BUILD=9030808 RECORDER=FX
TAB T=1
SET !DATASOURCE ""C:\\Users\\OFFICE\\Documents\\iMacros\\Datasources\\JobCard-No.csv""
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}

'Select Beneficiary Job Card No.
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_ContentPlaceHolder1_DDL_Registration CONTENT=%*{{!COL1}}*
WAIT SECONDS=1

SET !EXTRACT NULL
'Extract your Element:
TAG POS=1 TYPE=SELECT ATTR=NAME:ctl00$ContentPlaceHolder1$DDL_Registration EXTRACT=TXT
SET !VAR1 EVAL(""var s=\""{{!COL1}}\""; s.length"")
PROMPT {{!EXTRACT}}
SET !VAR3 18
SET !VAR4 !VAR3 s.toString()
SET !VAR5 !VAR1.toString()
SET !Sub_Str1 EVAL(""var s=\""{{!EXTRACT}}\""; s.substr(VAR4,VAR5)"")
PROMPT {{!Sub_Str1}}
 'Spit out a ""1"" or ""2"" to reuse for ""TAB T=n"":
SET TAB_Nb EVAL(""var s='{{Sub_Str1}}'; var z; if(s=='{{!COL1}}'){z=1;} else{z=2;}; z;"")
'Stay on TAB_1 if Element found or switch to TAB_2 if not found:
TAB OPEN
TAB T={{TAB_Nb}}",https://forum.imacros.net/viewtopic.php?f=7&t=27171&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2129,Table data extraction-format not retained,"Hi,

Browser : Internet explorer 11.0
Os : windows 7 enterprise 
I MAcro :11.1.495.5175


Trying to extract a table in csv.

 I have successully extracted the table but not able to retain the format.
 Can't use firefox as the official site is accessible through internet explorer only.

 any help on this highly appreciated .

thank you

Macro code used :

VERSION BUILD=11.1.495.5175
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
URL GOTO=http://20.17.236.130:7220/nbALoginModule/jsp/Links.jsp
'change login id as per the requirement
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:uid CONTENT=nblife41
SET !ENCRYPTION NO
TAG POS=1 TYPE=INPUT:PASSWORD ATTR=NAME:password CONTENT=Life1234 
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:submitLogin
ONDIALOG POS=1 BUTTON=NO
'change the environment name as per the requirment
'TAG POS=1 TYPE=A ATTR=TXT:LnbA<SP>Dev<SP>-<SP>2
'TAG POS=1 TYPE=A ATTR=TXT:LnbA<SP>IST<SP>-<SP>2
TAG POS=1 TYPE=A ATTR=TXT:LnbA<SP>Admin-Cloud<SP>UAT
'New tab opened
'TAG POS=1 TYPE=A ATTR=TXT:LnbA<SP>QA7
TAB T=2
'New tab opened
TAB T=3
TAB T=2
TAB CLOSE
TAB T=2
FRAME NAME=mainContentFrame
TAG POS=1 TYPE=TD ATTR=ID:dm0m0i0tdText
TAG POS=1 TYPE=TD ATTR=ID:dm0m1i0tdText
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:menu:coordmenuForm:menuFormButton
TAG POS=1 TYPE=TD ATTR=ID:dm0m1i0tdText
wait seconds=5

FRAME NAME=file
SET !EXTRACT_TEST_POPUP NO

TAG POS=1 TYPE=DIV ATTR=ID:pollersTableData EXTRACT=TXT

SAVEAS TYPE=EXTRACT FOLDER=C:\Users\pjain74\Desktop\macro FILE=pollers1_data.csv



format of data extracted : file attached

The cloumns should appear like this  :

XML103 Document Input     (N2DOCIN)       Waiting   2       50",https://forum.imacros.net/viewtopic.php?f=7&t=27065&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2130,Determining frame ID's/fields for ReCaptcha,"FF43, Imacros 8.9.6 , Windows 7 Ultimate 32 bit 


Hi , 

I am going absolutely crazy with this as I am like 95% done but cant nail it ! I just cant work out the frame Id's/fields needed to define the input given from 2Captcha which is just box 1, and box 4 , for example. I'm sure you know what I mean  


Code: Select allVERSION BUILD=8961227 RECORDER=FX
TAB T=1
SET !REPLAYSPEED FAST
SET !EXTRACT_TEST_POPUP NO
SET !ERRORIGNORE YES
SET !DATASOURCE ""C:\Users\dan\Documents\Imacros\Datasources\kikname.txt""
SET !VAR1 EVAL(""var randomNumber=Math.floor(Math.random()*100000 + 1); randomNumber;"")
URL GOTO=https://kikfinder.com/submit
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:NoFormName ATTR=ID:username CONTENT={{!COL1}}
TAG POS=1 TYPE=SELECT FORM=NAME:NoFormName ATTR=ID:gender CONTENT=%female
TAG POS=1 TYPE=TEXTAREA FORM=NAME:NoFormName ATTR=ID:description CONTENT=hi
FRAME F=1
TAG POS=1 TYPE=DIV ATTR=ROLE:presentation&&CLASS:recaptcha-checkbox-checkmark&&TXT:
WAIT SECONDS=10
ONDOWNLOAD FOLDER=C:\Users\dan\Documents\iMacros\Kikfinder\ FILE=payload.jpg WAIT=YES
FRAME F=2
TAG POS=1 TYPE=IMG ATTR=SRC:https://www.google.com/recaptcha/api2/payload?c=* CONTENT=EVENT:SAVEPICTUREAS
WAIT SECONDS=5
TAB OPEN
TAB T=2
URL GOTO=https://2captcha.com/imacros.html
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:http://rucaptcha.com/in.php ATTR=NAME:key CONTENT=0e5c7dd0074b93e4b458f3b2ee2931d4
TAG POS=1 TYPE=INPUT:FILE FORM=ACTION:http://rucaptcha.com/in.php ATTR=NAME:file CONTENT=C:\Users\dan\Documents\iMacros\Kikfinder\payload.jpg
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=TYPE:submit&&VALUE:recognize
ONDIALOG POS=1 BUTTON=OK CONTENT=
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://rucaptcha.com/in.php ATTR=*
WAIT SECONDS=1
SET !EXTRACT NULL
TAG POS=1 TYPE=* ATTR=* EXTRACT=TXT
WAIT SECONDS=1
TAB CLOSE
TAB T=1
FRAME F=2
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:rc-imageselect-response-field CONTENT={{!EXTRACT}}
wait seconds=2
wait seconds=900



Like I say, I think the extraction is fine and everything else , but I cant for the life of me work out the frame to define or the response field..The rc-imageselect-response is the last attempt I made before smashing my face into the wall....

Would really appreciate your help as you have helped me before and worked wonders ",https://forum.imacros.net/viewtopic.php?f=7&t=25600&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2131,Need to hit particular keyword using datasouce in imacros,"IE :8.1
windows 7 enterprise
 Imacro: 11.1.495.5175

*******************************



 snapshot of webpage 
Hi,

Attached is the snapshot of site am working on.

what i want to do is run imacro picking up the keywords from datasource and hit the keyword  and extract data.

Like:

LnbA Admin-Cloud QA2
LnbA Admin - Dev1
LnbA Admin - Dev3

are the tags i want my imacro to click one by one (in each loop and extract data)

below is the code am using to hit a keyword once.

VERSION BUILD=11.1.495.5175
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
URL GOTO=http://20.17.236.130:7220/nbALoginModule/jsp/Links.jsp
'change login id as per the requirement
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:uid CONTENT=useridXX
SET !ENCRYPTION NO
TAG POS=1 TYPE=INPUT:PASSWORD ATTR=NAME:password CONTENT=passXX
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:submitLogin
ONDIALOG POS=1 BUTTON=NO
'change the environment name as per the requirment
'TAG POS=1 TYPE=A ATTR=TXT:LnbA<SP>Dev<SP>-<SP>2


how am trying to loop it though (which is not working):


VERSION BUILD=11.1.495.5175
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
URL GOTO=http://20.17.236.130:7220/nbALoginModule/jsp/Links.jsp
'change login id as per the requirement
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:uid CONTENT=userXX
SET !ENCRYPTION NO
TAG POS=1 TYPE=INPUT:PASSWORD ATTR=NAME:password CONTENT=passXX
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:submitLogin
ONDIALOG POS=1 BUTTON=NO
SET !DATASOURCE C:\Users\pjain74\Desktop\macro\pollers_input.csv
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}
TAG POS=1 TYPE=A ATTR=TXT:{{!COL1}}

********************
sharing the  piece of xml
**************


<tr>			
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbaldev.axa-equitable.com:10225 ... o&app=LIFE"" target=""_blank"")"">
			LnbA DInfomrmals</a></td>
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbaldev.axa-equitable.com/nba/sso&app=LIFE"" target=""_blank"")"">
			LnbA Dev</a></td>
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=http://nbaldev2.axa-equitable.com:9080/nba/sso&app=LIFE"" target=""_blank"")"">
			LnbA Dev-2</a></td>
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbaldev.axa-equitable.com:10234 ... o&app=LIFE"" target=""_blank"")"">
			LnbA Dev3</a></td>
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbaldev.axa-equitable.com:10242 ... o&app=LIFE"" target=""_blank"")"">
			LnbA Dev4</a></td>
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbalqa1.axa-equitable.com/nba/sso&app=LIFE"" target=""_blank"")"">
			LnbA QA1</a></td>
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbalqa2.axa-equitable.com/nba/sso&app=LIFE"" target=""_blank"")"">
			Legacy Migrated </a></td>
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbalqa3.axa-equitable.com/nba/sso&app=LIFE"" target=""_blank"")"">
			LnbA QA3</a></td>
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbalperf.axa-equitable.com/nba/sso&app=LIFE"" target=""_blank"")"">
			LnbA QA4</a></td>
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbalqa5.axa-equitable.com/nba/sso&app=LIFE"" target=""_blank"")"">
			LnbA QA5</a></td>
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbalqa6.axa-equitable.com/nba/sso&app=LIFE"" target=""_blank"")"">
			LnbA QA6</a></td>

			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbalist.axa-equitable.com/nba/sso&app=LIFE"" target=""_blank"")"">LnbA IST</a></td>

			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbalqa8.axa-equitable.com/nba/sso&app=LIFE"" target=""_blank"")"">LnbA QA8</a></td>
			<td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbalifeperfabs.axa-equitable.co ... o&app=LIFE"" target=""_blank"")"">LnbA ABS_PERFF</a></td>
			 <!-- <td><a href=""/nbALoginModule/jsp/nbAPost.jsp?urld=https://nbaluat2.axa-equitable.com/nba/sso&app=LIFE"" target=""_blank"")"">LnbA UAT</a></td>  -->


			</tr>


I guess the last line is creating the problem.

any help is highly appreciable

Thank you.",https://forum.imacros.net/viewtopic.php?f=7&t=27068&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2132,E-mail triggers Website - Assistance Appreciated,"Ok, so what i'm attempting to accomplish is to scan my gmail for a specific subject line and if available then go to a URL and login automatically. I also want it to check if the website is already logged in and if not than login (website times out after like 10-20 minutes or so). What I have written so far is the macro opens up my gmail, clicks on an e-mail with a subject verbiage that I have provided then I open a 2nd Tab, navigate to the URL and Login. All this works however the below is what im missing that i'm unable to figure out:

1 - Scan gmail until the e-mail with that specific subject line is available (I dont actually need to click on the e-mail, i just need to know its available as this will be my trigger to navigate to a website).
2 - If e-mail available, navigate to the website but if the site directs me back to the login page than login (I have the login macro all setup just need to know how to identify if website is logged in or not).
3 - I dont want the macro to keep looping through if an e-mail is found and the website is open. only loop (scan) if e-mail not available and website (tab-2) closed.

Below is what I have so far. If anyone could provide assistance that would be appreciated. BTW, my gmail is set to Basic HTML as I read that would be better for iMacro. I'm open to opinions on this as well.
Code: Select allVERSION BUILD=10022823
'Check e-mail/open e-mail
TAB T=1
URL GOTO=https://mail.google.com/mail/u/0/h/1rmx10yed8z4p/?&
TAG POS=1 TYPE=SPAN FORM=NAME:f ATTR=TXT:*<SP>email<SP>subject<SP>verbiage.<SP>*

'Open a new tab and navigate to website of my choice
TAB OPEN
TAB T=2
URL GOTO=https://www.WebsiteExample/AgentDashboard.html/dashboard

'Enter login information and enter site
SIZE X=1172 Y=583
WAIT SECONDS=2.72
DS CMD=CLICK X=524 Y=173 CONTENT=
DS CMD=KEY CONTENT=MyUserName
DS CMD=CLICK X=505 Y=230 CONTENT=
DS CMD=KEY CONTENT=MyPassword
DS CMD=CLICK X=596 Y=281 CONTENT=
WAIT SECONDS=5

System Info:
iMacro v.10.02.2823
iMacro Browser - Stealth Mode (identify as native Internet Explorer)
Windows 10 Pro",https://forum.imacros.net/viewtopic.php?f=7&t=27148&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2133,Scrape elements which are generated by php code,"Hi,
I want to scrape content from webpage which is not visible in Source code. It's probably generated by php script, so I cant find it looking at source code. Of course I can click ""inspect element"" and see the html code of elements I want to extract. What I also can do is clicking at them. 

Event clicks are very similar:
Code: Select allEVENT TYPE=CLICK SELECTOR=""HTML>BODY>DIV>DIV:nth-of-type(2)>DIV>DIV>DIV>DIV>DIV:nth-of-type(5)>DIV:nth-of-type(2)>DIV>DIV>DIV:nth-of-type(3)>DIV:nth-of-type(1)>DIV>TABLE>TBODY>TR>TD:nth-of-type(2)>DIV:nth-of-type(3)>DIV:nth-of-type(2)"" BUTTON=0
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>DIV>DIV:nth-of-type(2)>DIV>DIV>DIV>DIV>DIV:nth-of-type(5)>DIV:nth-of-type(2)>DIV>DIV>DIV:nth-of-type(3)>DIV:nth-of-type(2)>DIV>TABLE>TBODY>TR>TD:nth-of-type(2)>DIV:nth-of-type(3)>DIV:nth-of-type(2)"" BUTTON=0
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>DIV>DIV:nth-of-type(2)>DIV>DIV>DIV>DIV>DIV:nth-of-type(5)>DIV:nth-of-type(2)>DIV>DIV>DIV:nth-of-type(3)>DIV:nth-of-type(3)>DIV>TABLE>TBODY>TR>TD:nth-of-type(2)>DIV:nth-of-type(3)>DIV:nth-of-type(2)"" BUTTON=0
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>DIV>DIV:nth-of-type(2)>DIV>DIV>DIV>DIV>DIV:nth-of-type(5)>DIV:nth-of-type(2)>DIV>DIV>DIV:nth-of-type(3)>DIV:nth-of-type(4)>DIV>TABLE>TBODY>TR>TD:nth-of-type(2)>DIV:nth-of-type(3)>DIV:nth-of-type(2)"" BUTTON=0

As you can see only 1 number is changing. My first question: Is it possible to extract href from these selectors? 

When I click ""inspect element"" and go to the element I want to extract this is what I see:
Code: Select all<div class=""gs-per-result-labels"" url=""http://this-is-what-i-want-to-extract.html""></div>

I tried by scraping by XPATH but it seem not to work (I think because imacros cant find these elements in source code). Do you have any tips for me?


MY OS:
OS: Windows 7 N service pack 1 64 bit
Intel Core i7-4700MQ
16gb ram
gt 755m

Firefox 50.1.0
iMacros for Firefox 9.0.3
VERSION BUILD=9030808 RECORDER=FX",https://forum.imacros.net/viewtopic.php?f=7&t=27099&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2134,Extract Downloaded File location is failed,"Hello, i have created code to save pic/image and extract the file download location to csv file.

save image is work , but extract file downloaded location to csv file is failed , result is ""__undefined__"" in csv file.

this is my code
Code: Select allVERSION BUILD=8970419 RECORDER=FX
SET !EXTRACT_TEST_POPUP NO

TAB T=1
URL GOTO=https://www.tokopedia.com/sparepart-tv-69/lampu-led-3-volt-10-kancing-backlight-3-volt-10-kancing-cembung-2
ONDOWNLOAD FOLDER=C:\Users\Yenyen<SP>Nainggolan\Pictures FILE=+_{{!NOW:yyyymmdd_hhnnss}} WAIT=YES
TAG POS=1 TYPE=IMG ATTR=SRC:https://ecs7.tokopedia.net/img/cache/300/product-1/*.jpg CONTENT=EVENT:SAVEPICTUREAS

ADD !EXTRACT {{!DOWNLOADED_FILE_NAME}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=link<SP>gambar.csv


can some one help me ?  ",https://forum.imacros.net/viewtopic.php?f=7&t=27058&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2135,My macro copies too much,"Hello,

I just created my first macro and it seems doing a little bit too much. I need to automate the following task: copy a cell from a csv file, paste it to google.com/finances search area, get search result, scroll down, copy industry and sector and paste this info to the csv file. My issue is that my macro seems copying everything from the web page and I get industry and sector just on the line 498 of my csv file. 
And I have a couple more questions: how could I avoid it throws me an error if google doesn't find anything, or it runs into an empty cell on my csv file?
I will really appreciate your help.

My macros is bellow:
Code: Select allVERSION BUILD=11.5.498.2403
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
SET !DATASOURCE ""C:\\Users\\Agne\\Desktop\\bandymas\\kitas_failas.csv""
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}
URL GOTO=google.com/finance
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:q CONTENT={{!COL1}}
TAG POS=1 TYPE=BUTTON:SUBMIT ATTR=ID:gbqfb
TAG POS=10 TYPE=DIV ATTR=CLASS:g-unit<SP>g-first EXTRACT=TXT
SAVEAS TYPE=TXT FOLDER= C:\\Users\\Agne\\Documents\\imacros FILE=naujas.csv",https://forum.imacros.net/viewtopic.php?f=7&t=27051&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2136,Extract current selected dropdown into variable,"Hello iMacros pros!

I have a dropdown box with 75 items within it. 

That dropdown box is currently selected on an item. 

Here is the html:
Code: Select all<select id=""toolbar_uid"" name=""toolbar_uid"" onchange=""swapCharacter()"" style=""font-size:8pt;width:120px;"">
                                                            <option value=""267096"" >SomethingHere</option>
                                                                <option value=""267095"" >SomethingThere</option>
                                                                <option value=""267090"" >TheresSomething</option>
                                                                <option value=""74473"" selected>Here'sSomething</option>

How can I extract the CURRENTLY SELECTED option value=""77473"" and store into a variable so that later in the iMacro I can do something like this:
Code: Select allTAG POS=1 TYPE=SELECT FORM=NAME:NoFormName ATTR=NAME:awardto CONTENT=%{{myVariable}}",https://forum.imacros.net/viewtopic.php?f=7&t=26747&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2137,Extracting first of two lines,"Windows 10, IE11, VERSION BUILD=11.5.498.2403

When I go to Extract the first person's name below of John Doe it's also grabbing the second line Ann Doe, I'm trying to extract just John Doe.

It's shows like this on the website...
John Doe
Ann Doe

the htm looks like this...
'<td><span id=""ctl00_ContentPlaceHolder1_lblBorrowers"" style=""border: 1px solid green; 
'background-color: magenta;"">John Doe<br>Ann Doe</span></td>

I was attempting something along this line but I don't think I'm even close
Code: Select allTAG POS=1 TYPE=SPAN ATTR=ID:ctl00_ContentPlaceHolder1_lblBorrowers EXTRACT=TXT
SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s.split('<br>')[0];"")


I tried '</br>'  tried '<br/>' etc.  But now I'm thinking <br> has nothing to do with it.  

Any idea?  

Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=26867&sid=41f2b858fcf7ca695bb39ee1420df70b,content
2139,Tag Processing Delay,"I am trying to extract some data from a Web site. I am able to successfully navigate thru a number of pages to get to the one that has the data to extract. It works...but it seems to often hang for minutes before completing. Here's the line that causes the delay: 
TAG POS=1 TYPE=A ATTR=ID:ctl00_mainContent_lnkbutDates EXTRACT=TXT

The rest of the site is very responsive. Is there anything that I can do to speed up the process. I have 20-30 extractions per page (about 10 pages) so performance is important.
Some details: I/E 11, iMacros 11, Windows 7

Thanks for any help!",https://forum.imacros.net/viewtopic.php?f=7&t=26782&sid=91bf0dc741658a74aa70a7f66f53d097,content
2140,Problem with Dailymotion,"hello, i'm try to build imacros with my channel in dailymotion.
i want to add new thumbnail and new tag. but it doesnt work. 
anyone can help me? this is my code
Code: Select allVERSION BUILD=9030808 RECORDER=FX
SET !ERRORIGNORE YES
SET !DATASOURCE dmtest.csv
SET !DATASOURCE_COLUMNS 9
SET !DATASOURCE_LINE {{!LOOP}}
SET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP NO

TAG POS=3 TYPE=DIV ATTR=TXT:Choose<SP>image<SP>to<SP>upload
TAG POS=1 TYPE=INPUT:FILE FORM=ID:upload_preview_form ATTR=ID:upload_preview_form_video_preview CONTENT={{!COL1}}
WAIT SECONDS=5
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:video_edit_0 ATTR=ID:info_0_tags CONTENT={{!COL2}},{{!COL3}}
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:video_edit_0 ATTR=ID:save


NB: Col1 = location image
Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=26781&sid=91bf0dc741658a74aa70a7f66f53d097,content
2141,RE: Fill form field ONLY if blank,"I'm trying to work with this form, and sometimes the info is pre-filled by the website. Is there a way that if data is in the form field, to not overwrite it? Here is the start of my code below:

VERSION BUILD=8970419 RECORDER=FX
TAB T=1
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:PropertyInfo* ATTR=NAME:estYearPurchased CONTENT=12/2012

So if content was blank, then I can pre-fill with my data. However, if content has data, to NOT pre-fill with my info?

Thanks!",https://forum.imacros.net/viewtopic.php?f=7&t=26797&sid=91bf0dc741658a74aa70a7f66f53d097,content
2142,How to click textbox from javascript site,"1. What version of iMacros are you using?
     VERSION BUILD=8970419 RECORDER=FX or Imacros For Firefox 8.9.7
2. What operating system are you using? (please also specify language)
     Windows 10 64 bit (English)
3. Which browser(s) are you using? (include version numbers)
     Firefox 49.0.1
4. Do the included demo macros work ok? 
     Yes
6. If recording or replay fails on a specific website: Can you please post the URL of the web page and/or the imacro that creates the problem? (Need login)
      Code: Select allhttps://seller.shopee.co.id/portal/product/new 
7. Do you encounter the same problem with the iMacros Browser, iMacros for Internet Explorer and iMacros for Firefox?
    N/A

i'm trying to insert content from csv into a deskripsi textbox (see below)



my problem is i can't insert content to textbox because the code on this site is : 
Code: Select all'//--------------------------------Before Manual Click----------------------------------------------\\
<div class=""edit-input col-8""><div id=""ember1751"" class=""ember-view shopee-textarea shopee-textbox textarea""><div class=""shopee-validation-toast ""><!----></div><div class=""input""> <div id=""ember1752"" class=""ember-view liquid-container"" style=""""><div id=""ember1754"" class=""ember-view liquid-child"" style=""top: 0px; left: 0px;"">        <div class=""placeholder"">Deskripsi Produk dan <s5>#hashtag</s5></div>
</div></div></div><div id=""ember1755"" class=""lf-fade ember-view liquid-container""><!----></div><!----></div></div>

'//--------------------------------After Manual Click the code changed like this----------------------\\
<div class=""edit-input col-8""><div id=""ember1751"" class=""ember-view shopee-textarea shopee-textbox textarea focused""><div class=""shopee-validation-toast ""><!----></div><div class=""input""> <div id=""ember1752"" class=""ember-view liquid-container"" style=""""><div id=""ember1881"" class=""ember-view liquid-child"" style=""top: 0px; left: 0px;"">         <textarea id=""ember1886"" maxlength=""3000"" placeholder=""Deskripsi Produk dan #hashtag"" spellcheck=""false"" class=""ember-view ember-text-area""><!----></textarea>
</div></div></div><div id=""ember1755"" class=""lf-fade ember-view liquid-container""><!----></div><!----></div></div>

this is my code i'm trying many tag xpath but still can't get textarea 
Code: Select allTAG XPATH=""/html/body/div[1]/div/div[2]/form/div[1]/div[2]/div[2]/div/div[2]""
TAG XPATH=""/html/body/div[1]/div/div[2]/form/div[1]/div[2]/div[2]/div/div[2]/div""
TAG XPATH=""""/html/body/div[1]/div/div[2]/form/div[1]/div[2]/div[2]/div/div[2]/div/div""
TAG XPATH=""/html/body/div[1]/div/div[2]/form/div[1]/div[2]/div[2]/div/div[2]/div/div/div""
'This is the textbox xpath it work if i click manual the textbox
TAG XPATH=""/html/body/div[1]/div/div[2]/form/div[1]/div[2]/div[2]/div/div[2]/div/div/textarea"" CONTENT={{!COL3}}
WAIT SECONDS = 2
TAG XPATH=""/html/body/div[1]/div/div[2]/form/div[1]/div[5]/div/div[1]/div[2]/div/div[3]/input"" CONTENT={{!COL2}}
WAIT SECONDS = 2
TAG XPATH=""/html/body/div[1]/div/div[2]/form/div[1]/div[5]/div/div[2]/div[2]/div/div[2]/input"" CONTENT={{!COL4}}
WAIT SECONDS = 2
'TAG POS=1 TYPE=DIV FORM=ID:ember* ATTR=TXT:Pilih<SP>Kategori&&CLASS:scs-label
TAG POS=1 TYPE=SPAN ATTR=TXT:Pilih<SP>Kategori
TAG POS=2 TYPE=DIV ATTR=TXT:{{!COL6}}
TAG POS=2 TYPE=DIV ATTR=TXT:{{!COL7}}
TAG POS=4 TYPE=DIV ATTR=TXT:{{!COL8}}
WAIT SECONDS = 2
TAG XPATH=""/html/body/div[1]/div/div[2]/form/div[2]/div/div[1]""
WAIT SECONDS = 5


if i'm using imageclick through IE Browser it work perfectly but i need to play this through firefox
Code: Select allIMAGECLICK POS=1 IMAGE=C:\Users\biovo\Documents\iMacros\Datasources\shope.png CONFIDENCE=48
TAG XPATH=""/html/body/div[1]/div/div[2]/form/div[1]/div[2]/div[2]/div/div[2]/div/div/textarea"" CONTENT={{!COL3}}
WAIT SECONDS = 2

now my question is how to do a click on that textbox from firefox without manual click or ds click like that .

Thank You, 
Sorry for my bad english ",https://forum.imacros.net/viewtopic.php?f=7&t=26761&sid=91bf0dc741658a74aa70a7f66f53d097,content
2143,Stripping quotes and ; from html,"Hello all,
took me a couple days to figure this out so thought I would share:

text imacros extractedCode: Select all<span content=""sku:17508782"" itemprop=""productID"" style=""outline: 1px solid blue;""></span>

what I actually wanted
Code: Select all17508782

what I used to get it
Code: Select allSET !EXTRACT NULL  
'Reseting extract to null, then giving the item number in html
TAG POS=R1 TYPE=SPAN ATTR=content:sku* EXTRACT=HTM
SET myVarA EVAL(""var a=\""{{!EXTRACT}}\""; a.replace(/\""/g, \""\"");"") 
'PROMPT  Variable<SP>Var4:<SP><SP>{{myVarA}}
SET myVarB EVAL(""var b=\""{{myVarA}}\""; b.replace(/;/g, \""\"");"")  
SET myVarC EVAL(""var c=\""{{myVarB}}\""; c.replace(\""<span content=sku:\"");"")  
SET myVarD EVAL(""var d=\""{{myVarC}}\""; d.replace(\"" itemprop=productID style=outline: 1px solid blue></span>\"",\""\"");"")  
SET itemNumb EVAL(""var e=\""{{myVarD}}\""; e.replace(\""undefined\"",\""\"");"")     
'PROMPT  Variable<SP>itemNumb:<SP><SP>{{itemNumb}}

hope this is useful to someone else, prompts aren't necessary, I just used them when hacking out a solution",https://forum.imacros.net/viewtopic.php?f=7&t=26752&sid=91bf0dc741658a74aa70a7f66f53d097,content
2144,How to extract results from haveibeenpwned.com,"I am trying to extract the output of queries submitted to haveibeenpwned.com. (Great website BTW!) 

You enter an email address and it returns a result basically saying ""Good News"" or ""Oh No"". I just want to know if the email address is ""Good News"" answer. If it's an ""Oh No"" answer then I want iMacros to return something else like #EANF (I can sort emails later) 

This ought to be easy, but the issue is that the HTML  has both answers, so whether the page is showing ""Good News"" or ""Oh No"", iMacros can see both answers because both are present in the HTML but only one is displayed. 

How can I distinguish between the displayed answer and the hidden answers and just capture the displayed answer. 

Here's a simple example:
TAB T=1
'URL GOTO=https://haveibeenpwned.com/ - just open this page and leave it open
SET !ERRORIGNORE YES
SET !TIMEOUT_STEP 3
'This is my list of emails to enter on the page one by one in a loop
SET !DATASOURCE emails.csv

'The next bit enters the email address into the form on the page
TAG POS=1 TYPE=INPUT:EMAIL FORM=ACTION:/ ATTR=ID:Account CONTENT={{!col1}}
TAG POS=1 TYPE=BUTTON FORM=ACTION:/ ATTR=ID:searchPwnage
wait seconds=2

'The next bit just extracts the answer ""Good news"" - if this isn't present I would expect #EANF -  but not so because it's there on the page but hidden
TAG POS=1 TYPE=H2 ATTR=TXT:Good* extract=txt

ADD !EXTRACT {{!col1}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=extractPwned.csv
wait seconds=5",https://forum.imacros.net/viewtopic.php?f=7&t=26683&sid=91bf0dc741658a74aa70a7f66f53d097,content
2146,SAVE PDF - Download not finished,"Hey guys,

I am currently using IMacros V11.1.495.5175, Firefox 48, Windows 7 German.

I have a problem with a website where I am trying to download a pdf and the saved file is not completed. 
Instead of a pdf file I get a file which has the extension ""Datei"" and the size it's lower than what it should be (eg. 200kb downloaded out of 344kb).

My code is:

var xpath = ""//ul[@class='techRefContentList']/li["" + i + ""]/div/a""; //I am using xpath to find the <a> tag of the pdf 
var pdf_extract_macro = ""CODE: ONDOWNLOAD FOLDER=* FILE=myfile.pdf WAIT=YES"" + ""\n"";//I set the default download folder because I know there are some issues with it if I try to put a preferential folder (pls tell me otherwise..I really need a preferential folder)
pdf_extract_macro += ""TAG XPATH="" + '""' + xpath + '""' + "" CONTENT=EVENT:SAVEITEM"" + ""\n"";//saving the pdf when the download begins
iimPlay(pdf_extract_macro);

When I look in the download folder, I find a ""Datei"" file that I can't use so I'm guessing the download stops mid way.
This code is being run on two other websites and the pdf is downloaded fine on those.
On this particular site the download link is not in the href but in an onaction property. I don't know if that is the reason why the download is stopping. 

What could I do to make it wait until it's finished ? 

WAIT #ONDOWNLOADCOMPLETE# it's not supported
I was thinking of using !DOWNLOADED_SIZE to check if the download is finished but I don't know how to get the full size of the pdf to compare them.

I'm kind of stuck right now.

My solution was to set the browser to save the pdf in the download folder when the download begins and just TAG the <a>.
This works but I can't set the files name and it's a pain to go trough all the pdfs to find the one I'm looking for.

Any help is greatly appreciated guys.

Thank you !",https://forum.imacros.net/viewtopic.php?f=7&t=26630&sid=91bf0dc741658a74aa70a7f66f53d097,content
2147,how to extract value from the dropdown list? to csv ?,"Code: Select all <select name=""select1"">
 <option value="""">Select Fruits</option>
 <option value=""AP 1"">Apple</option>
 <option value=""BN"">Banana</option>
 <option value=""Special"">Kiwi</option>
 </select>


i only found imacros able to search the value but how to store the value ?
Code: Select allTAG POS=1 TYPE=SELECT FORM=NAME:form1 ATTR=NAME:select1 CONTENT=%BN


code in imacros was 
Code: Select allTAG POS=1 TYPE=SELECT:NAME FORM=ID:demo ATTR=TXT:*&&ID:select1 EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=value.csv
 

if selected Banana, how to i save the value BN and not the Banana ?",https://forum.imacros.net/viewtopic.php?f=7&t=23051&sid=91bf0dc741658a74aa70a7f66f53d097,content
2148,Accessing content window - Multiprocess Firefox,"Hey guys,

I am currently using IMacros V11.1.495.5175, Firefox 48, Windows 7 German.
I am writing the code in js and calling iMacros from there.
I just ran into an issue and I don't know if it's going to be a problem in the future.

I have this line: 
final_macro += ""ADD !EXTRACT "" + '""' + lager_col2[0].textContent + '""' + ""\n"";	

When I ran my js in the browser I got this error:
Error: Accessing content window is not supported in multiprocess Firefox, line 208 (Error code: -991)

Should I be worried ? Is the use of textContent forbidden ? 
The problem is that ""lager_col2[0]"" sometimes happen to be a link and I only want the text inside the link, not the tags and everything. So naturally I went with textContent. 
How could I get around this ?麓

Additionally, am I setting myself up for trouble when trying to access certain nodes with the use of window.document.getElementsByClassName ? 
I feel like everyone is going to advise me against it but it was easier than using the traditional TAG POS/XPATH/etc

Thank you !",https://forum.imacros.net/viewtopic.php?f=7&t=26524&sid=91bf0dc741658a74aa70a7f66f53d097,content
2150,Extract Mystery,"Somewhere in one of these 3 scripts 
Billing_First.iim
Code: Select allVERSION BUILD=9052613
SET !REPLAYSPEED FAST
SET MyWait 3
SET PID ""023967496""
SET POS ""99""
SET Proc ""S5102""
SET Mod ""UC""
WAIT SECONDS={{MyWait}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Datapanel:_ctl0:_ctl4:ClaimRecipient:ds_ClaimRecipient:mb_ds_ClaimRecipient CONTENT={{ConsumerID}}
WAIT SECONDS={{MyWait}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl2:PerformingProvider_ProviderID:mb_PerformingProvider_ProviderID CONTENT={{PID}}
WAIT SECONDS={{MyWait}}
TAG POS=1 TYPE=ACRONYM FORM=NAME:Form ATTR=TXT:Hard-Copy<SP>Attachments
WAIT SECONDS={{MyWait}}
TAG POS=1 TYPE=ACRONYM FORM=NAME:Form ATTR=TXT:Hard-Copy<SP>Attachments
IMAGESEARCH POS=1 IMAGE=MCD.png CONFIDENCE=80
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl3:FirstServiceDate:mb_FirstServiceDate CONTENT={{DateOfService}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl4:LastServiceDate:mb_LastServiceDate CONTENT={{LDateOfService}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl5:PlaceOfService:ds_PlaceOfService:mb_ds_PlaceOfService CONTENT={{POS}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl6:Procedure:ds_Procedure:mb_ds_Procedure CONTENT={{Proc}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl7:Modifier1:ds_Modifier1:mb_ds_Modifier1 CONTENT={{Mod}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl10:BilledQuantity:mb_BilledQuantity CONTENT={{Units}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl11:BilledAmount:mb_BilledAmount CONTENT={{Rate}}
TAG POS=1 TYPE=ACRONYM FORM=NAME:Form ATTR=TXT:Hard-Copy<SP>Attachments
TAG POS=3 TYPE=A FORM=NAME:Form ATTR=TXT:add
'TAG POS=3 TYPE=A FORM=NAME:Form ATTR=TXT:add<SP>N


Billing_First2.iim
Code: Select all' Macro without ConsumerID
VERSION BUILD=9052613
SET !REPLAYSPEED FAST
SET MyWait 1
SET PID ""023967496""
SET POS ""99""
SET Proc ""S5102""
SET Mod ""UC""
WAIT SECONDS={{MyWait}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl2:PerformingProvider_ProviderID:mb_PerformingProvider_ProviderID CONTENT={{PID}}
WAIT SECONDS={{MyWait}}
TAG POS=1 TYPE=ACRONYM FORM=NAME:Form ATTR=TXT:Hard-Copy<SP>Attachments
WAIT SECONDS={{MyWait}}
TAG POS=1 TYPE=ACRONYM FORM=NAME:Form ATTR=TXT:Hard-Copy<SP>Attachments
IMAGESEARCH POS=1 IMAGE=MCD.png CONFIDENCE=80
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl3:FirstServiceDate:mb_FirstServiceDate CONTENT={{DateOfService}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl4:LastServiceDate:mb_LastServiceDate CONTENT={{LDateOfService}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl5:PlaceOfService:ds_PlaceOfService:mb_ds_PlaceOfService CONTENT={{POS}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl6:Procedure:ds_Procedure:mb_ds_Procedure CONTENT={{Proc}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl7:Modifier1:ds_Modifier1:mb_ds_Modifier1 CONTENT={{Mod}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl10:BilledQuantity:mb_BilledQuantity CONTENT={{Units}}
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Detail:Datapanel:_ctl0:_ctl11:BilledAmount:mb_BilledAmount CONTENT={{Rate}}
TAG POS=1 TYPE=ACRONYM FORM=NAME:Form ATTR=TXT:Hard-Copy<SP>Attachments
TAG POS=3 TYPE=A FORM=NAME:Form ATTR=TXT:add
'TAG POS=3 TYPE=A FORM=NAME:Form ATTR=TXT:add<SP>N


PaidAmt.iim
Code: Select allVERSION BUILD=10002738
SET !EXTRACT_TEST_POPUP NO
ADD !EXTRACT {{ConsumerID}}
TAG POS=2 TYPE=A FORM=NAME:Form ATTR=TXT:Paid EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:Form ATTR=NAME:dnn:ctr366:ClaimPhysicianInformation:_ctl2:Status:Datapanel:_ctl1:_ctl4:PaidAmo* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=status.csv

The extract is sending data to a file named  BillingStaus.csv in folder BillingData
I can't seem to find where to change the path to the folder or change the filename
Or where the extract code is called, I thought I would see a call to PaidAmt.iim

Thank you in advance for any all suggestions
gchichester",https://forum.imacros.net/viewtopic.php?f=7&t=26549&sid=91bf0dc741658a74aa70a7f66f53d097,content
2151,Loop through results,"Hi everyone.

I need to extract data from a site. There is a simple SELECT option, an INPUT, and a SEARCH BUTTON. So far so good.

Now, the result pages, are not paginated in the traditional way. There is no link to Page 1/2/...., only next page button, which is an input, and it acts like a submit. This input has a NAME attribute, which, on the last page is missing. This is fine with me, because the macro will stop automatically once finished. 

I need to go through all the pages, and extract the data. Now bear with me, I've started to look into iMacros only yesterday, so I'm not really familiar with all the options..
This is the code I've came up so far, and it works for the first page:
Code: Select allVERSION BUILD=8970419 RECORDER=FX
SET !EXTRACT_TEST_POPUP NO
SET !ERRORIGNORE YES

URL GOTO=https://www.eofcom.admin.ch/eofcom/public/searchEofcom_InaFree.do
' select option 2 from the dropdown
TAG POS=1 TYPE=SELECT FORM=ID:chofcomelicensingpresentationnainaformEofcomInaSearchForm ATTR=NAME:nrt CONTENT=%2
' insert value into input box
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:chofcomelicensingpresentationnainaformEofcomInaSearchForm ATTR=NAME:pnp CONTENT=0002
' submit search
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:chofcomelicensingpresentationnainaformEofcomInaSearchForm ATTR=NAME:doSearchFreeByNumber

' *** this is the block I would need to loop through
' get out the text from the result table
TAG POS=1 TYPE=TBODY ATTR=TXT:* EXTRACT=TXT
' remove all the spaces
SET !EXTRACT EVAL(""'{{!EXTRACT}}'.replace(/\s/g, \""\"");"")
' show the result
PROMPT ""{{!EXTRACT}}""
' goto next page
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:chofcomelicensingpresentationnainaformEofcomInaSearchForm ATTR=NAME:getNextInaPage
' *** end block of loop

I've tried to Code: Select allSET !LOOP 25and start the macro in loop mode, but it doesn't work, plus I don't know beforehand how many result pages would be.
From my researches over the net couldn't figure out how to achieve that. Any help would be greatly appreciated.

I am using the free iMacros Firefox extiension (latest, downloaded yesterday), in Debian 8.5, but I also have an XP machine with the trial iMacros Version 11.1.

Note: before someone would complain about legal issues related to scraping data, I have to mention that my company contacted the owners of the site, and asked for an API, which is not available, and they suggested to use the search page instead.

Thanks.",https://forum.imacros.net/viewtopic.php?f=7&t=26511&sid=91bf0dc741658a74aa70a7f66f53d097,content
2152,How to remove string in a EXTRACT,"This is my code
Code: Select allSET !EXTRACT_TEST_POPUP NO
URL GOTO=https://www.youtube.com/results?search_query=Calvin+Harris+-+This+Is+What+You+Came+For+%28Official+Video%29+ft.+Rihanna
TAG POS=1 TYPE=A ATTR=CLASS:yt-uix-sessionlink<SP>yt-uix-tile-link<SP>yt-ui-ellipsis<SP>yt-ui-ellipsis-2<SP><SP><SP><SP><SP><SP><SP>spf-link<SP> EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=* FILE=*

I've extracted a csv file content is ""https://www.youtube.com/watch?v=kOkQ4T5WO9E""
but I just want to get content is ""kOkQ4T5WO9E""
what should I do?

I'm sorry, my english is bad.",https://forum.imacros.net/viewtopic.php?f=7&t=26508&sid=91bf0dc741658a74aa70a7f66f53d097,content
2153,Javascript elements - can not click to the right button,"Windows 2012 Server | Windows 8.1
Firefox 47.0.1
iMacros for Firefox 8.9.7

Website: vk.com

After I had created a post on my wall and decided to edit this post and add a poll, I've found that I can not make it.

If you want to help me, you may prefer to do it by yourself from scratch, or here is the code I'm using:
Code: Select allTAG POS=1 TYPE=DIV ATTR=CLASS:""post_edit_button fl_r""
TAG POS=1 TYPE=DIV ATTR=CLASS:""post_edit_button fl_r""
TAG POS=1 TYPE=NOBR ATTR=TXT:Attach CONTENT=EVENT:MOUSEOVER
WAIT SECONDS=1.5
TAG POS=1 TYPE=NOBR ATTR=TXT:Other.. CONTENT=EVENT:MOUSEOVER
WAIT SECONDS=1.5
TAG POS=1 TYPE=NOBR ATTR=TXT:Poll CONTENT=EVENT:MOUSEOVER
WAIT SECONDS=1.5
TAG POS=1 TYPE=NOBR ATTR=TXT:Poll

After clicking on last element (TAG POS=1 TYPE=NOBR ATTR=TXT:Poll) JavaScript adds Poll to the form of new post and not to an existing post that I'm editing.

Looks like it happens because of they are using same function with same class, so I've started to look around in order to find the way how to fix it, but I still can't find the way after hours of searching, reading and experimenting.

Clicking to an element by it's XPATH gives an error ""RuntimeError: Element DIV is not visible, line 3 (Error code: -921)"".
Code: Select allEVENT TYPE=CLICK XPATH=""id('add_media_menu_3')/div/div/div/div[2]"" BUTTON=0

I hope there should be an easy way how to manage is, so I ask iMacros community to help me.
I believe this case could be a good example for many situations.",https://forum.imacros.net/viewtopic.php?f=7&t=26500&sid=91bf0dc741658a74aa70a7f66f53d097,content
2154,Automate a filling form with a massive .csv in a table,"Hello!
- iMacros Version: 8.9.70.419
- Browser:Firefox: 47.0.1
- OS: Windows 10 Pro

I need your help regarding the performance of this script I have built in iMacros. It's doing sort of what I intend, sort of  
I want to load some basic information in a couple of text boxes everytime I click on the element (where the loop should be placed)
ctl00_PlaceHolderMain_BandejaEntidades_LstBandejaEntidades_ctrl{{!LOOP}}_LbtnVer.

The issues I'm having are the following:

1. The table where the data is located only displays 10 elements (although I can click on an index below the table that jumps through every 10 elements= This is called TAG POS=1 TYPE=A ATTR=TXT:354) . I want to loop over 3500 items. In addition to that, and to make things worse, the number of elements grow everyday but backwards: every new element starts at ""PlaceHolderMain"" in 0 so the first element that was loaded months ago will be the last on the list. But let's ignore this for a moment, so say I want to loop from 1 to 3500 elements, but since it only displays 10 elements, I get an error in the loop, because it cannot find the element 11, for example. How can I loop this to ALL the elements?

2. When I put the condition: SET !SINGLESTEP YES the code seems to work fine (assuming I just want to loop 10 elements, which I AM NOT!, because of the problem aforementioned). However, this is extremely tedious, I just want to run this and get over it. But the code get stuck when it encounters the line FRAME F=1. Not a runtime error, not running out of time, it just stops! Any idea why and how it can be solved?

3. The server response is extremely slow, every step when it makes a call to the server takes about 30-40 seconds. I've tried countless times to solve this with the IT people, but it seems that I cannot expect anything much from them  Ignoring this, how can I make the script run a little bit faster? Specially the part when it fills out the form and then returns to the main table?

4. When the first loop of filling out the form I got the headers of the .csv file, not the second line that I intended. But since SET !DATASOURCE_LINE {{!LOOP}} would loop every line starting from the element 1, this mean I would have to make another loop? So DATASOURCE starts at 2 but the rows on the table will start at 1 (actually, 0)? I'm really confused right now. Any idea how to solve this? It is my understanding that I would need to create a Javascript file? but I have no idea how to do this!

If someone can give me a hand orientating me on this issues, Im more than grateful!
Edit: I change the comments on the code....


Code: Select allVERSION BUILD=8970419 RECORDER=FX
TAB T=1
SET !LOOP 1
SET !SINGLESTEP YES
SET !DATASOURCE Prueba_Automata_SIPOV_v2.csv
SET !DATASOURCE_COLUMNS 5
SET !DATASOURCE_LINE {{!LOOP}}
'Login is pre-loaded, it should take me to the main table called ""FRMBANDEJAENTIDADES""
URL GOTO=https://www.minagricultura.gov.co/sites/sipov/_layouts/15/SHP-13-MinAgricultura-SIPV/SIPV/FrmBandejaEntidades.aspx
'Click en ""Ingresar""
'TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:aspnetForm ATTR=ID:ctl00_PlaceHolderMain_g_4efd4b82_233f_4f69_a43f_afbd14d8c69e_btnIngresar
'Va a la ultima hoja de la tabla (354)
'TAG POS=1 TYPE=A ATTR=TXT:354
'Click on the button to open another table( ""Ver""): ctrl{n} should be the Loop that allow me to access to every single row to fill the data
TAG POS=1 TYPE=A ATTR=ID:ctl00_PlaceHolderMain_BandejaEntidades_LstBandejaEntidades_ctrl{{!LOOP}}_LbtnVer
'To open the frame to fill in the data (WHEN I EXECUTE THIS WITHOUT SET !SINGLESTEP IT STOPS THE SCRIPT, NOT AN ERROR. IT JUST STOPS)
FRAME F=1
'Fill in the textbox with information retrieved from CSV
TAG POS=1 TYPE=TEXTAREA FORM=ID:aspnetForm ATTR=ID:ctl00_PlaceHolderMain_FrmFallo__supervisionEntidades_TxtAccionesDerivadasOrden CONTENT={{!COL2}}
'Fill in the textbox with information retrieved from CSV
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:aspnetForm ATTR=ID:ctl00_PlaceHolderMain_FrmFallo__supervisionEntidades_DtFechaGestion_DtFechaGestionDate CONTENT={{!COL3}}
'Fill in the textbox with information retrieved from CSVo
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:aspnetForm ATTR=ID:ctl00_PlaceHolderMain_FrmFallo__supervisionEntidades_TxtDiasTranscurridos CONTENT={{!COL4}}
'Fill in the textbox with information retrieved from CSV
TAG POS=1 TYPE=TEXTAREA FORM=ID:aspnetForm ATTR=ID:ctl00_PlaceHolderMain_FrmFallo__supervisionEntidades_TxtObservaciones CONTENT={{!COL5}}
'Clicks on the button to add the information placed before
TAG POS=1 TYPE=A ATTR=ID:ctl00_PlaceHolderMain_FrmFallo__supervisionEntidades_LbtnAgregar
'驴Regresa a la tabla inicial?
'FRAME F=1
'No s茅!
'WAIT SECONDS=45
'TAG POS=4 TYPE=SPAN ATTR=CLASS:s4-clust&&TXT:
",https://forum.imacros.net/viewtopic.php?f=7&t=26470&sid=91bf0dc741658a74aa70a7f66f53d097,content
2155,"table export to CSV after ""javascript:__doPostBack""","Hello!
First of all, I have to say I just started using imacros today so you can say Im really just a noob   nevertheless I find imacros amazingly powerful. I would really like to learn more about it!
The problem I want to solve is this: I want to extract all the elements on the table that FOLLOWS after I click a link that is referenced as ""javascript:_doPostBack"". Each link ""hosts"" a table, so I need to loop this. If possible, to merge all the resulting tables into one without having to go trough a macro in VBA.
Code: Select allVERSION BUILD=8970419 RECORDER=FX
SET !LOOP 1
SET !EXTRACT_TEST_POPUP NO
TAB T=1
URL GOTO=http://190.24.134.230/tierras/sentencias.aspx
TAG POS=1 TYPE=A ATTR=HREF:javascript:__doPostBack('ctl00$MainContent$GVSentencias','Select${{!LOOP}}')
TAG POS=1 TYPE=TABLE ATTR=* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=TEST.CSV

I'm unsure on how to get the table after the loop? I've tried with relative positioning (TAG POS=1) but it brings *EANF* which I assume is an error in the retrieval?
Any help would be highly appreciated. Im pretty sure this is a dumb question to ask...",https://forum.imacros.net/viewtopic.php?f=7&t=26464&sid=91bf0dc741658a74aa70a7f66f53d097,content
2156,Error when communicating with the native messaging host,"Hi guys!

I am newbie to iMACROS and I got some troubles with my script. Any help is appreciated!  

So I created a script to grab information from online and store it in my local csv file. I'm aiming to do it in a loop about size 1000 every time. The script works fine when I run it in smaller loops (ex. <50). 

This morning, I tried to run it in 800 loops, but it stopped at loop 49 and gives me this error. Error: Error when communicating with the native messaging host., line: 36. Line 36 is the last line of my script. And I found that every time I got this error, I just need to change the output file name(both in script and the actual output file's name) and continue running the loop from where it stopped last time. And the script will be good for another 40 or 50 loops until this error pops up again... This is pretty confusing... Please let me know if you have any suggestions  


Below is my CIM:

+ iMacros for Chrome 8.3.5
+ Windoes 8.1 Enterprise
+ Google Chrome Version 50.0.2661.102 m


Below is my script:

VERSION BUILD=8350307 RECORDER=CR
SET !DATASOURCE C:\Users\crispyizzy\Desktop\input.csv
SET !DATASOURCE_COLUMNS 6
SET !DATASOURCE_LINE {{!LOOP}}
SET !EXTRACT_TEST_POPUP NO


URL GOTO=https://myURL.com
FRAME F=3
TAG POS=1 TYPE=A ATTR=ID:C4_W16_V17_IC_INBOX
WAIT SECONDS=2
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:myFormId ATTR=ID:123_search_parameters[2].VALUE1 CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:myFormId ATTR=ID:123_search_parameters[3].VALUE1 CONTENT={{!COL2}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:myFormId ATTR=ID:123_search_parameters[6].VALUE1 CONTENT={{!COL4}}
SET !EXTRACT {{!COL1}}
ADD !EXTRACT {{!COL2}}
ADD !EXTRACT {{!COL3}}
ADD !EXTRACT {{!COL5}}
ADD !EXTRACT {{!COL4}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:myFormId ATTR=ID:123_search_MAX_HITS CONTENT=1
TAG POS=1 TYPE=B ATTR=TXT:Search
WAIT SECONDS=3
TAG POS=1 TYPE=A ATTR=ID:123_items_table[1].incident_id
TAG POS=1 TYPE=TABLE ATTR=CELLSPACING:2 EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:123_cssngprojh_struct.description EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:123_catselvn_cat_label EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:123_btadminh_priority EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:123_btorgset_struct.service_org_resp_short EXTRACT=TXT
TAG POS=1 TYPE=A ATTR=ID:123_btpartnerset_soldto_name EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:123_btcustomerh_convert_system_usage EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:123_btadminh_last_change EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:123_btadminh_zcreated_at EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:123_zinccreatdate_datetime EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:123_btadminh_zconfirmed_at EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\crispyizzy\Desktop FILE=output                                     // This is line 36!
",https://forum.imacros.net/viewtopic.php?f=7&t=26337&sid=91bf0dc741658a74aa70a7f66f53d097,content
2158,Getting Data source to Loop,"Trying to get my data source (Step2.csv) to loop to the next line of data...it keeps pulling from line 1 and not heading down the list...

for : var imacros_main32  : I bolded the area I need help with. Thanks
Code: Select all//
// This imacros script created by 9kw.eu
// Find more here: http://www.9kw.eu/
//
// The script fill the captcha of recaptcha v2 demo.
//
// Note for users with firefox with the message ""Firefox prevented this page from automatically reloading."":
// UNCHECK the option ""warn me when pages try to redirect"" in your browser
// You find the checkbox under Options > Advanced > General > Accessibility
//
// More informations with iMacros for Firefox and speed up javascript:  
// http://wiki.imacros.net/iMacros_for_Firefox#Javascript_Scripting_Interface
//
// You can use more instances with Firefox:
// http://kb.mozillazine.org/Opening_a_new_instance_of_Firefox_with_another_profile
// https://support.mozilla.org/en-US/kb/profile-manager-create-and-remove-firefox-profiles
// http://kb.mozillazine.org/Command_line_arguments
// Like -P ""My Profile"" -no-remote or -ProfileManager
//
// Path to the captcha image with timestamp
// Tempfolder like C:\ or C:\TEMP\ under windows or like /tmp/ under linux
// In Javascript and windows with escape like C:\\TEMP\\
var tempfolder = 'C:\\TEMP\\';//optional, like C:\\TEMP\\

// Tempslash (Path with slashs as (tempfolder) and with the htmlfile recaptcha_v2.html
var tempslash = ""C:/TEMP/"";//optional, like C:/TEMP/

// Step 1: Config for 9kw.eu for your apikey
var apikey = """";

//And priority (prio 1-20)
var prio = ""1"";

// Loops
var max_loop = 1;
var max_wait = """";//seconds, optional

// Url
var myurl = ""URL GOTO=https://www.textnow.com/signup"";
myurl += """";//extra lines

// Submit
var mysubmit = ""TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:NoFormName ATTR=*\n"";
mysubmit += """";//extra lines

// Replayspeed
var replayspeed = ""FAST"";//MEDIUM
var replayspeed_clicks = ""FAST"";

// Debug
var breakOnError = false;
var onBreakAlert = true;
var logFile = true;
var logFileName = 'iimLog.txt';
var max_loop_limit = 500;

//Performance for recaptchav2 like default 1 + 10 = Frame 11 as start for few functions
var higher_frame_number = 1;//Standard (like 6)
var other_frame_number = 0;//Startframe
var max_frame_number = 15;//Standardframe as max (like 15)
var total_frame_number = 20;//SearchFrame+SearchOK

//header
var imacros_header = ""SET !EXTRACT_TEST_POPUP NO\n"";
imacros_header += ""SET !ERRORIGNORE NO\n"";
imacros_header += ""SET !TIMEOUT_PAGE 999\n"";
imacros_header += ""SET !TIMEOUT_STEP 999\n""
imacros_header += ""SET !REPLAYSPEED ""+replayspeed+""\n"";

//Main
var logFilePath = getiMacrosFolder('Logs')+'\\'+logFileName;
var tempfile;

var httpRequestObserver =  {  
  observe: function(subject, topic, data)  {  
    if (topic == ""http-on-modify-request"") {  
      var httpChannel = subject.QueryInterface(Components.interfaces.nsIHttpChannel);  
      if (
      	/funcaptcha.com|solvemedia.com|keycaptcha.com|confidenttechnologies.com/.test(httpChannel.originalURI.host) ||
      	/google.com\/recaptcha|gstatic.com\/recaptcha/.test(httpChannel.URI.spec) 
      
      ) {  
	      httpChannel.setRequestHeader(""Accept-Language"", ""en,en-US;de;q=0.5"", false);  
      }
    }  
  },  
  
  get observerService() {  
    return Components.classes[""@mozilla.org/observer-service;1""]  
                     .getService(Components.interfaces.nsIObserverService);  
  },  
  
  register: function()  
  {  
    this.observerService.addObserver(this, ""http-on-modify-request"", false);  
  },  
  
  unregister: function()  
  {  
    this.observerService.removeObserver(this, ""http-on-modify-request"");  
  }  
}; 
var observerService = Components.classes[""@mozilla.org/observer-service;1""].getService(Components.interfaces.nsIObserverService);
observerService.addObserver(httpRequestObserver, ""http-on-modify-request"", false);
httpRequestObserver.register();

(function() {
	// Syntaxcheck: API Key, prio
	if(apikey.match(/^[a-zA-Z0-9]+$/) && apikey.length <= 50 && apikey.length >= 5){}else{alert_message(""API Key \""""+apikey+""\"" is wrong."");return false;}
	var d = parseFloat(prio); if(d >= 0 && d <= 20){}else{alert_message(""Prio \""""+prio+""\"" is not in the set range."");return false;}

	// only valid number
	if (/[^\d]/.test(max_loop)) {
		alert_message('Please enter a valid number for loop play');
		return false;
	}
	
	max_loop = parseInt(max_loop);
	if(max_loop > max_loop_limit){
		alert_message('Loop limit exceed (' + max_loop_limit + ') , edit the following variable to use your own limit.\n\nvar max_loop_limit = ' + max_loop_limit + ';');
		return false;
	}	
	
	for(var loop = 1; loop <= max_loop; loop++){
		var function_code = """";
		function_code = recaptchav2();
		if(function_code == false && breakOnError == true){
			return false;
		}
		if (/[\d]/.test(max_wait)) {
			wait(max_wait);
		}
	}
})();

httpRequestObserver.unregister();
observerService.removeObserver(httpRequestObserver, ""http-on-modify-request"");
//End

// functions
// recaptcha v2
function recaptchav2(){
	var imacros_first = ""CODE:""+imacros_header+""TAB CLOSEALLOTHERS\n""
	imacros_first += ""TAB T=1\n"";
	imacros_first += myurl;
	iimPlay(imacros_first)
	
	// start execution
	var start = new Date(), end; // for benchmarking
	
	// Step 2: Save the captcha picture to local disk
	// initial click macro
	var frame = SearchFrame(""EVENT TYPE=CLICK SELECTOR=\""#recaptcha-anchor>DIV:nth-of-type(5)\"" BUTTON=0"",0);
	iimSet(""frame"",frame)
	//Wait a random number of seconds
	wait(Math.floor(Math.random()*3 + 1));
	var checkOK;
	var more_correct_solutions;
	var captchaid;
	
	if(/[^\d]/.test(frame) || frame < 1){
		if(logFile){
			log_message(""No captcha found as frame number."");
		}
		return false;
	}

	for(var i3=1;i3<=5;i3++){
		captchaid = """";
		tempfile = ""captcha_""+tempfile_date()+"".jpg"";

		// macro for checking captcha checkbox
		var solved = SearchOK(""TAG POS=1 TYPE=SPAN ATTR=ID:recaptcha-anchor&&aria-checked:true CONTENT=EVENT:MOUSEOVER"",1+other_frame_number,max_frame_number);
		if(solved > 0){
			end =+ new Date();
			if(logFile){
				log_message('Captcha is solved\n\rTime spent: '+Math.floor((end-start)/1000));
			}
			//break;
		}else{
			if(i3 > 1){
				//more_correct_solutions = SearchOK(""TAG POS=2 TYPE=DIV ATTR=TXT:Multiple<SP>correct<SP>solutions<SP>required<SP>-<SP>please*&&STYLE: CONTENT=EVENT:MOUSEOVER"",0+other_frame_number);
				if(more_correct_solutions < 1){
					checkOK = SearchOK(""TAG POS=1 TYPE=SPAN ATTR=ID:recaptcha-anchor&&aria-checked:true CONTENT=EVENT:MOUSEOVER"",1+other_frame_number,max_frame_number);
					if(checkOK < 1){
						iimPlay(imacros_first)
						wait(3)
						SearchOK(""EVENT TYPE=CLICK SELECTOR=\""#recaptcha-anchor>DIV:nth-of-type(5)\"" BUTTON=0"",0+other_frame_number);
					}
				}
			}

			var save_image_header = ""SET !REPLAYSPEED ""+replayspeed+""\n"";
			if(tempfolder.length > 3){
				save_image_header = ""ONDOWNLOAD FOLDER=""+tempfolder+"" FILE=""+tempfile+"" WAIT=YES\n"";
			}else{
				save_image_header = ""ONDOWNLOAD FOLDER=""+getiMacrosFolder(""DataSources"")+"" FILE=""+tempfile+"" WAIT=YES\n"";
			}
			var save_image = save_image_header+""TAG POS=1 TYPE=DIV ATTR=ID:rc-imageselect CONTENT=EVENT:SAVE_ELEMENT_SCREENSHOT"";
			frame = SearchFrame(save_image,frame);
			if(frame > 0){
				iimSet(""frame"",frame)
			}
			//iimDisplay(""Frame: ""+frame)
			var clickframe = SearchFrame(""TAG POS=1 TYPE=IMG ATTR=SRC:https://www.google.com/recaptcha/api2/payload?c=* CONTENT=EVENT:MOUSEOVER"",frame,max_frame_number);
			// find the captcha with 10 images and higher
			var find10 = SearchOK(""TAG POS=12 TYPE=IMG ATTR=SRC:https://www.google.com/recaptcha/api2/payload?c=* CONTENT=EVENT:MOUSEOVER"",clickframe,clickframe);

			// check for the recaptcha with 10 images or find the captcha with 9 images
			var find9;
			if(find10 < 1){
				find9 = SearchOK(""TAG POS=9 TYPE=IMG ATTR=SRC:https://www.google.com/recaptcha/api2/payload?c=* CONTENT=EVENT:MOUSEOVER"",clickframe,clickframe);
			}
			
			// check for the recaptcha with 9 images or find the captcha with 8 images
			var find8;
			if(find9 < 1){
				find8 = SearchOK(""TAG POS=8 TYPE=IMG ATTR=SRC:https://www.google.com/recaptcha/api2/payload?c=* CONTENT=EVENT:MOUSEOVER"",clickframe,clickframe);
			}
			// check for the recaptcha with 8 images or find the captcha with enough images like 2 and more
			var find2;
			if(find8 < 1){
				find2 = SearchOK(""TAG POS=2 TYPE=IMG ATTR=SRC:https://www.google.com/recaptcha/api2/payload?c=* CONTENT=EVENT:MOUSEOVER"",clickframe,clickframe);
			}
			
			// new captcha image with numbers for the captcha service
			if(find10 > 0 || find9 > 0 || find8 > 0 || find2 > 0){
				var recaptcha_v2_error = false;
				
				for(var ihtml=1;ihtml<=2;ihtml++){
					var imacros_main1 = ""CODE:"";
					imacros_main1 += imacros_header+""TAB OPEN\n"";
					imacros_main1 += ""TAB T=2\n"";
					if(tempslash.length > 0){
						imacros_main1 += ""URL GOTO=file:///""+tempslash+""recaptcha_v2.html?"";
					}else{
						imacros_main1 += ""URL GOTO=file:///""+getiMacrosFolder('DataSources').replace(/\\/g, '/')+'/'+""recaptcha_v2.html?"";
					}
					if(more_correct_solutions > 0){
						if(find10 > 0){
							imacros_main1 += ""6"";
						}else if(find9 > 0){
							imacros_main1 += ""7"";
						}else{
							imacros_main1 += ""8"";
						}
					}else{
						if(find10 > 0){
							imacros_main1 += ""2"";
						}else if(find9 > 0){
							imacros_main1 += ""1"";
						}else{
							imacros_main1 += ""3"";
						}
					}
					if(tempslash.length > 0){
						imacros_main1 += ""=file:///""+tempslash+tempfile+""\n"";
					}else{
						imacros_main1 += ""=file:///""+getiMacrosFolder('DataSources').replace(/\\/g, '/')+'/'+tempfile+""\n"";
					}
					imacros_main1 += ""WAIT SECONDS=1\n"";
					iimPlay(imacros_main1)
					
					var html_ok = SearchOK(""TAG POS=1 TYPE=DIV ATTR=CLASS:c CONTENT=EVENT:MOUSEOVER"",0,0);
					if(html_ok > 0){
						var imacros_main1a;
						if(tempfolder.length > 3){
							imacros_main1a = ""CODE:""+imacros_header+save_image_header+""FILEDELETE NAME=""+tempfolder+tempfile+""\n"";				
						}else{
							imacros_main1a = ""CODE:""+imacros_header+save_image_header+""FILEDELETE NAME=""+getiMacrosFolder(""DataSources"")+'\\'+tempfile+""\n"";									
						}
						imacros_main1a += ""TAG POS=1 TYPE=DIV ATTR=* CONTENT=EVENT:SAVE_ELEMENT_SCREENSHOT\n"";
						imacros_main1a += ""WAIT SECONDS=3\n"";
						imacros_main1a += ""TAB CLOSE\n"";
						iimPlay(imacros_main1a)
						break;
					}else{
						if(ihtml == 1){
							iimPlay(""CODE:"");
							var imacros_main1b = ""CODE:URL GOTO=http://www.9kw.eu/grafik/api/recaptcha_v2.html\n"";
							imacros_main1b += ""WAIT SECONDS=1\n"";
							imacros_main1b += ""TAG POS=1 TYPE=HTML ATTR=* EXTRACT=HTM\nSAVEAS TYPE=HTM FOLDER=\""""+getiMacrosFolder('DataSources')+""\"" FILE=recaptcha_v2.html\n"";
							imacros_main1b += ""WAIT SECONDS=1\n"";
							imacros_main1b += ""TAB CLOSE\n"";
							iimPlay(imacros_main1b)
						}else{
							alert_message(""No recaptcha_v2.html found on your local browser. Please check it."");
							recaptcha_v2_error = true;
							break;
						}
					}
				}
				if(recaptcha_v2_error){
					if(breakOnError){
						return false;
					}else{
						break;
					}					
				}
			}
				
			//Step 3: Open a new tab, and go to 9kw.eu, and submit the captcha picture
			var imacros_main2 = ""CODE:""+imacros_header;
			imacros_main2 += ""TAB OPEN\n"";
			imacros_main2 += ""TAB T=2\n"";
			imacros_main2 += ""URL GOTO=http://www.9kw.eu/grafik/form.html\n"";
			//The apikey is used to identify each of our customers, which you can get from the our page. It is assigned to the CONTENT.
			imacros_main2 += ""TAG POS=1 TYPE=INPUT ATTR=NAME:apikey CONTENT=""+apikey+""\n"";
			//Priority in our system like min. 0 to max. 20 (cost +0-20)
			imacros_main2 += ""TAG POS=1 TYPE=INPUT ATTR=NAME:prio CONTENT=""+prio+""\n"";
			//Options for the form. See more under http://www.9kw.eu/grafik/form.html and http://www.9kw.eu/api.html
			imacros_main2 += ""TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ACTION:/index.cgi ATTR=NAME:selfsolve CONTENT=NO\n"";
			imacros_main2 += ""TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ACTION:/index.cgi ATTR=NAME:confirm CONTENT=NO\n"";
			imacros_main2 += ""TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ACTION:/index.cgi ATTR=NAME:case-sensitive CONTENT=NO\n"";
			imacros_main2 += ""TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ACTION:/index.cgi ATTR=NAME:nomd5 CONTENT=YES\n"";
			//We need only numbers for this captcha
			if(find10 > 0 || find9 > 0 || find8 > 0 || find2 > 0){
				imacros_main2 += ""TAG POS=1 TYPE=INPUT:CHECKBOX FORM=ACTION:/index.cgi ATTR=NAME:numeric CONTENT=YES\n"";
			}
			imacros_main2 += ""TAG POS=1 TYPE=INPUT ATTR=NAME:source CONTENT=imacros\n"";
			// recaptcha v2 for non text captcha or recaptcha for text captcha
			if(find10 > 0 || find9 > 0 || find8 > 0 || find2 > 0){
				imacros_main2 += ""TAG POS=1 TYPE=INPUT ATTR=NAME:oldsource CONTENT=recaptchav2\n"";
			}else{
				imacros_main2 += ""TAG POS=1 TYPE=INPUT ATTR=NAME:oldsource CONTENT=recaptcha\n"";
			}
			//The path of the captcha picture saved is assigned to the CONTENT
			if(tempfolder.length > 0){
				imacros_main2 += ""TAG POS=1 TYPE=INPUT ATTR=NAME:file-upload-01 CONTENT=""+tempfolder+tempfile+""\n"";
			}else{
				imacros_main2 += ""TAG POS=1 TYPE=INPUT ATTR=NAME:file-upload-01 CONTENT=""+getiMacrosFolder('DataSources')+'\\'+tempfile+""\n"";
			}
			//Submit the formdata to 9kw.eu
			imacros_main2 += ""TAG POS=1 TYPE=INPUT ATTR=TYPE:submit\n"";
			//Clean the !EXTRACT variable for the next task
			imacros_main2 += ""SET !EXTRACT NULL\n"";
			//Extract the characters that are recoginzed from the picture of captcha.
			imacros_main2 += ""TAG POS=1 TYPE=INPUT ATTR=NAME:result EXTRACT=TXT\n"";
			iimPlay(imacros_main2)
			var answer = iimGetLastExtract();
			//Step 4: Check the captcha answer (text or nothing like #EANF# = Extraction Anchor Not Found)
			if(answer == ""#EANF#"" || answer == ""ERROR NO USER""){
				answer = """";
			}
			
			//Extract the captchaid from your captcha submit
			var imacros_main3 = ""CODE:"";
			imacros_main3 += ""SET !EXTRACT NULL\n"";
			imacros_main3 += ""TAG POS=1 TYPE=INPUT ATTR=NAME:captchaid EXTRACT=TXT\n"";
			imacros_main3 += ""SET captchaid {{!EXTRACT}}\n"";
			imacros_main3 += ""TAB CLOSE\n"";
			imacros_main3 += ""FRAME F={{frame}}\n"";
			iimPlay(imacros_main3)
			captchaid = iimGetLastExtract();
			
			//Clean the !EXTRACT variable for the next task
			var imacros_main4 = ""SET !EXTRACT NULL\n"";
			iimPlay(imacros_main4)
			
			//Display extracted data (only for debug)
			//iimDisplay(""captchaid: ""+captchaid+""\nanswer: ""+answer)
			
			//Step 5: Fill the recognized characters to the verification box (Click the pictures 1..16)
			if(find10 > 0 || find9 > 0 || find8 > 0 || find2 > 0){
				var nothing = 0;
				var myarray = answer.split('');
				if(myarray.length < 2){
					alert_message(""No answer found. Check the history on 9kw.eu!"");
					if(breakOnError){
						return false;
					}else{
						break;
					}
				}else{
					for(var i = 0;i < myarray.length; i++){
						if(myarray[i].match(/^[0-9]+$/)){
							if(find10 > 0){
								var two_values;
								if(myarray[i+1].match(/^[0-9]+$/)){
									two_values = myarray[i]+myarray[i+1];
								}else{
									two_values = myarray[i];
								}
								i += 1;
								two_values = two_values.replace(""0"", """");
								nothing += SearchOK(""SET !REPLAYSPEED ""+replayspeed_clicks+""\nTAG POS=""+two_values+"" TYPE=IMG ATTR=SRC:https://www.google.com/recaptcha/api2/payload?c=*"",1+higher_frame_number,max_frame_number);
							}else{
								nothing += SearchOK(""SET !REPLAYSPEED ""+replayspeed_clicks+""\nTAG POS=""+myarray[i]+"" TYPE=IMG ATTR=SRC:https://www.google.com/recaptcha/api2/payload?c=*"",1+higher_frame_number,max_frame_number);
							}
						}
					}
					if(nothing < 2){
						alert_message(""Not enough clicks are possible. Check the min/max frame number."");
						if(breakOnError){
							return false;
						}else{
							break;
						}
					}
				}
			}else{//text captcha
				SearchFrame(""TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:default-response CONTENT=""+answer,0+other_frame_number,max_frame_number);				
			}
				
			//Click the Verify button
			if(find10 > 0 || find9 > 0 || find8 > 0 || find2 > 0){
				SearchFrame(""EVENT TYPE=CLICK SELECTOR=\""HTML>BODY>DIV>DIV>DIV:nth-of-type(3)>DIV:nth-of-type(2)>DIV>DIV:nth-of-type(2)>DIV\"" BUTTON=0"",0+other_frame_number,max_frame_number);
			}else{
				SearchFrame(""TAG POS=1 TYPE=DIV ATTR=ID:recaptcha-verify-button"",0+other_frame_number,max_frame_number);			
			}
		}
		wait(1)
		var solved_check = SearchOK(""TAG POS=1 TYPE=SPAN ATTR=ID:recaptcha-anchor&&aria-checked:true CONTENT=EVENT:MOUSEOVER"",1+other_frame_number,max_frame_number);
			
		// Submit
		if(solved_check > 0){
			SearchFrame(mysubmit,0+other_frame_number);
		}else{
			more_correct_solutions = SearchOK(""TAG POS=2 TYPE=DIV ATTR=TXT:Multiple<SP>correct<SP>solutions<SP>required<SP>-<SP>please*&&STYLE: CONTENT=EVENT:MOUSEOVER"",0+other_frame_number);
		}
		
		var imacros_main31 = ""CODE:""+imacros_header;{
			imacros_main31 += ""CLEAR\n"";
			imacros_main31 += ""SET !ERRORIGNORE YES\n"";
			imacros_main31 += ""SET !DATASOURCE C:\\Log\\Step2.csv\n"";
			imacros_main31 += ""SET !LOOP 1\n"";
			imacros_main31 += ""SET !DATASOURCE_LINE {{loop}}\n"";
			imacros_main31 += ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:userForm ATTR=TYPE:text&&NG-FOCUS-ASYNC:movePlaceholder($event);clearErrors(userForm.firstname);&&NG-MAXLENGTH:42&&ID:first&&REQUIRED:&&NG-MODEL:user.first_name* CONTENT={{!COL1}}\n"";
			imacros_main31 += ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:userForm ATTR=TYPE:text&&NG-FOCUS-ASYNC:movePlaceholder($event);clearErrors(userForm.lastname);&&NG-MAXLENGTH:42&&NG-REQUIRED:true&&NG-MODEL:user.last_name* CONTENT={{!COL2}}\n"";
			imacros_main31 += ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:userForm ATTR=TYPE:text&&NG-FOCUS-ASYNC:movePlaceholder($event);clearErrors(userForm.username);&&NG-PATTERN:/^(?![0-9]*$)[a-zA-Z0-9_][a-zA-Z0-9_.]*[a-zA-Z0-9_]$/&&ON-BLURS:username* CONTENT={{!COL3}}\n"";
			imacros_main31 += ""TAG POS=1 TYPE=INPUT:PASSWORD FORM=NAME:userForm ATTR=TYPE:password* CONTENT={{!COL4}}\n"";
			imacros_main31 += ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:userForm ATTR=TYPE:text&&NG-FOCUS-ASYNC:movePlaceholder($event);<SP>clearErrors(userForm.email)* CONTENT={{!COL5}}\n"";
			imacros_main31 += ""TAG POS=1 TYPE=BUTTON FORM=NAME:userForm ATTR=CLASS:btn<SP>btn-primary<SP>btnFullWidth<SP>standardBtn<SP>ng-binding&&TYPE:submit&&NG-HIDE:processing\n"";
			imacros_main31 += ""WAIT SECONDS=13\n"";
			iimPlay(imacros_main31)
			}
			
			var imacros_main32 = ""CODE:""+imacros_header;{
			imacros_main31 += ""!TIMEOUT_STEP 3"" + ""\n"";
			imacros_main32 += ""WAIT SECONDS=1\n"";
			imacros_main32 += ""TAG POS=1 TYPE=A ATTR=CLASS:btn-primary<SP>unlocked-button\n""
			imacros_main32 += ""WAIT SECONDS=2\n"";
			imacros_main32 += ""EVENT TYPE=CLICK SELECTOR=\""#areacode\"" BUTTON=0\n""
			imacros_main32 += ""EVENTS TYPE=KEYPRESS SELECTOR=\""#areacode\"" CHARS=\""815\""\n"";
			imacros_main32 += ""WAIT SECONDS=2\n"";
			imacros_main32 += ""EVENT TYPE=CLICK SELECTOR=\""#enterAreaCodeForm>DIV:nth-of-type(2)>INPUT\"" BUTTON=0\n"";
			imacros_main32 += ""WAIT SECONDS=3\n"";
			imacros_main32 += ""EVENT TYPE=CLICK SELECTOR=\""#fb-close-button\"" BUTTON=0\n"";
			imacros_main32 += ""WAIT SECONDS=1\n"";
			iimPlay(imacros_main32)
			}
			
		
		//Step 6: Check and send the captcha feedback back to the captcha service (OK:1, NotOK:2, EN: Right/False, DE: Richtig/Falsch)
		var htmlcode = """";
		if(captchaid.length > 0){
			if(solved_check > 0){
				var imacros_main4b = ""CODE:TAG POS=1 TYPE=DIV ATTR=CLASS:recaptcha-success EXTRACT=TXT\n"";
				iimPlay(imacros_main4b)
				htmlcode = iimGetLastExtract();
			}
		}
						
		var feedback;
		if(htmlcode.match(/Juhu/) || htmlcode.match(/Hooray/) || solved_check > 0 || more_correct_solutions < 1){
			feedback = 1;
		}else{
			feedback = 2;
		}
			
		if(captchaid.length > 0){
			var imacros_main5 = ""CODE:TAB OPEN\n"";
			imacros_main5 += ""TAB T=2\n"";
			imacros_main5 += ""URL GOTO=http://www.9kw.eu/index.cgi?source=imacros&action=usercaptchacorrectback&apikey=""+apikey+""&correct=""+feedback+""&id=""+captchaid+""\n"";
			imacros_main5 += ""TAB CLOSE\n"";
			iimPlay(imacros_main5)
			
			// Cleanup: Delete the old captcha picture
			if(tempfolder.length > 0){
				iimPlay(""CODE:""+imacros_header+""SET !ERRORIGNORE YES\nFILEDELETE NAME=""+tempfolder+tempfile+""\n"");
			}else{
				iimPlay(""CODE:""+imacros_header+""SET !ERRORIGNORE YES\nFILEDELETE NAME=""+getiMacrosFolder('Downloads')+tempfile+""\n"");
			}
		}
		if(feedback == ""1""){
			break;
		}
	}
	return;
}


// random between 1 and 9
function rand(){
	return Math.floor(Math.random()*9) + 1;
}

// wait function like wait(2) for two seconds
function wait(waittime){
	iimPlay(""CODE: WAIT SECONDS=""+waittime+""\n"");
	return;
}

// wait function like wait(2) for two seconds
function getextract(shortcode){
	iimPlay(""CODE: ""+shortcode);
	return iimGetLastExtract();
}

// date functione
function tempfile_date() {
	now = new Date();
	year = """" + now.getFullYear();
	month = """" + (now.getMonth() + 1); if (month.length == 1) { month = ""0"" + month; }
	day = """" + now.getDate(); if (day.length == 1) { day = ""0"" + day; }
	hour = """" + now.getHours(); if (hour.length == 1) { hour = ""0"" + hour; }
	minute = """" + now.getMinutes(); if (minute.length == 1) { minute = ""0"" + minute; }
	second = """" + now.getSeconds(); if (second.length == 1) { second = ""0"" + second; }
	return year + month + day + ""_"" + hour + minute + second;
}

// find frame with true or false as return
function SearchOK(checkcode,firstframenumber,lastframenumber){
	var check = ""CODE:""+""SET !REPLAYSPEED ""+replayspeed+""\n"";
	check += ""SET !ERRORIGNORE YES"" + ""\n"";
	check += ""SET !TIMEOUT_STEP 0"" + ""\n""; 
	check += ""FRAME F={{i}}"" + ""\n""; 
	check += ""SET !ERRORIGNORE NO"" + ""\n"";
			
	if (checkcode != """"){
		check += checkcode + ""\n""; 
	}else{
		return 0;
	}
		
	if(/^\d+$/.test(firstframenumber)) {
		frame = firstframenumber;
	}else{
		frame = 1;
	}
	
	if(/^\d+$/.test(lastframenumber)) {
		frame2 = lastframenumber;
	}else{
		frame2 = total_frame_number;
	}

	for(var i=frame;i<=frame2;i++){
		iimSet(""i"",i)

		if(iimPlay(check) == true){
			return 1;
		}
	}
	return 0;
}

// find frame with the framenumber as return
function SearchFrame(checkcode,firstframenumber){
	var check = ""CODE:""+""SET !REPLAYSPEED ""+replayspeed+""\n"";
	check += ""SET !ERRORIGNORE YES"" + ""\n"";
	check += ""SET !TIMEOUT_STEP 0"" + ""\n""; 
	check += ""FRAME F={{i}}"" + ""\n""; 
	check += ""SET !ERRORIGNORE NO"" + ""\n"";
			
	if (checkcode != """"){
		check += checkcode + ""\n""; 
	}else{
		return 0;
	}
		
	if(/^\d+$/.test(firstframenumber)) {
		frame = firstframenumber;
	}else{
		frame = 1;
	}

	for(var i=frame;i<=total_frame_number;i++){
		iimSet(""i"",i)

		//if the result of the macro is true save frame number and break
		if(iimPlay(check) == true){
			frame = i;
			break;
		}
	}
	//return frame number
	return frame;
}

// Get the imacros folder
function getiMacrosFolder(folderName){
   var pname;
   switch (folderName){
      case ""Macros"" :
         pname = ""defsavepath"";
         break;
      case ""DataSources"" :
         pname = ""defdatapath"";
         break;
      case ""Downloads"" :
         pname = ""defdownpath"";
         break;
      case ""Logs"" :
         pname = ""deflogpath"";
         break;
      default :
         throw folderName + "" is not a valid iMacros folder name"";
         break;
   }
   return imns.Pref.getFilePref(pname).path;
}

// Logfunction
function log_message(msg) {
	var time = new Date().toString().replace(/\s+GMT.*/, '');
	msg = time + ' - ' + msg + '\n';
	var file_o = imns.FIO.openNode(logFilePath);
	imns.FIO.appendTextFile(file_o, msg);
}

// Logfunction
function alert_message(msg){
	if(logFile){
		log_message(msg);
	}

	iimDisplay(msg);

	if(onBreakAlert){
		alert(msg);
	}
}


var imacros_main31 = ""CODE:""+imacros_header;{
         imacros_main31 += ""CLEAR\n"";
         imacros_main31 += ""SET !ERRORIGNORE YES\n"";
         imacros_main31 += ""SET !DATASOURCE C:\\Log\\Step2.csv\n"";
         imacros_main31 += ""SET !LOOP 1\n"";
         imacros_main31 += ""SET !DATASOURCE_LINE {{loop}}\n"";
         imacros_main31 += ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:userForm ATTR=TYPE:text&&NG-FOCUS-ASYNC:movePlaceholder($event);clearErrors(userForm.firstname);&&NG-MAXLENGTH:42&&ID:first&&REQUIRED:&&NG-MODEL:user.first_name* CONTENT={{!COL1}}\n"";
         imacros_main31 += ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:userForm ATTR=TYPE:text&&NG-FOCUS-ASYNC:movePlaceholder($event);clearErrors(userForm.lastname);&&NG-MAXLENGTH:42&&NG-REQUIRED:true&&NG-MODEL:user.last_name* CONTENT={{!COL2}}\n"";
         imacros_main31 += ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:userForm ATTR=TYPE:text&&NG-FOCUS-ASYNC:movePlaceholder($event);clearErrors(userForm.username);&&NG-PATTERN:/^(?![0-9]*$)[a-zA-Z0-9_][a-zA-Z0-9_.]*[a-zA-Z0-9_]$/&&ON-BLURS:username* CONTENT={{!COL3}}\n"";
         imacros_main31 += ""TAG POS=1 TYPE=INPUT:PASSWORD FORM=NAME:userForm ATTR=TYPE:password* CONTENT={{!COL4}}\n"";
         imacros_main31 += ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:userForm ATTR=TYPE:text&&NG-FOCUS-ASYNC:movePlaceholder($event);<SP>clearErrors(userForm.email)* CONTENT={{!COL5}}\n"";
         imacros_main31 += ""TAG POS=1 TYPE=BUTTON FORM=NAME:userForm ATTR=CLASS:btn<SP>btn-primary<SP>btnFullWidth<SP>standardBtn<SP>ng-binding&&TYPE:submit&&NG-HIDE:processing\n"";
         imacros_main31 += ""WAIT SECONDS=13\n"";
         iimPlay(imacros_main31)
         }",https://forum.imacros.net/viewtopic.php?f=7&t=26266&sid=e8ccee866abcf29ebd57b704a290f61c,content
2159,Remove last 4 characters from input text field,"Hi! I am fairly new to iMacros. I am using iMacros to extract some text from title of the page then paste that text in a text box. Currently when I put that extracted text, it gets .mp4 written in it (title contains .mp4). I want to remove '.mp4' and replace it with '.com'
Currently I am doing this:
Code: Select allTAG POS=1 TYPE=INPUT:TEXT FORM=NAME:NoFormName ATTR=ID:videoedit_title EXTRACT=TXT
SET !VAR2 {{!EXTRACT}}
SET !EXTRACT NULL
TAG POS=1 TYPE=TEXTAREA FORM=NAME:NoFormName ATTR=ID:videoedit_description CONTENT=""{{!VAR2}}.com""
wait seconds=2

This adds .com to the end but there is no code to remove .mp4

Any help would be appreciated. Thanks.",https://forum.imacros.net/viewtopic.php?f=7&t=26221&sid=e8ccee866abcf29ebd57b704a290f61c,content
2160,Blocking Ads or Selecting alternate xpath element,"Imacros Desktop V10 Windows 7 

Intermittent google ad serving is interrupting my script - when the ad is served the required Xpath div changes from 6 to 7. Is there a way of selecting the alternate xpath if the first is not found (i.e. when the ad is served) Or anyway of blocking it? Filtering images doesn't work...

Thanks
Code: Select allVERSION BUILD=10022823
TAB T=1
TAB CLOSEALLOTHERS
SET !DATASOURCE C:\File.csv
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}
SET !TIMEOUT_STEP 1
SET !ERRORIGNORE YES
'CLEAR
URL GOTO=https://www.endole.co.uk/search/?search=Asset
FILTER TYPE=IMAGES STATUS=ON
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:/search/ ATTR=NAME:search CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:/search/ ATTR=CLASS:search_button
WAIT SECONDS=1.206
TAG XPATH=/html/body/div[7]/div[1]/div[6]/div/div[1]/a <!-- Alt Xpath -/html/body/div[7]/div[1]/div[7]/div/div[1]/a  -->
WAIT SECONDS=0.06
TAG POS=1 TYPE=H1 ATTR=* EXTRACT=TXT
TAG POS=3 TYPE=H5 ATTR=* EXTRACT=TXT
TAG POS=5 TYPE=H5 ATTR=* EXTRACT=TXT
TAG POS=6 TYPE=H5 ATTR=* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\ FILE=C:\File.csv",https://forum.imacros.net/viewtopic.php?f=7&t=26216&sid=e8ccee866abcf29ebd57b704a290f61c,content
2161,click the first link based on search result from CSV,"I need to click a link that isn't based on the text value of that link, so what ever the link will be on each iteration. I have looked at xpath but obviously the value will change each time. Any pointers would be gratefully received.
Code: Select allVERSION BUILD=10022823
TAB T=1
TAB CLOSEALLOTHERS
SET !DATASOURCE C:\Users\Tom\Desktop\EANF.csv 
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}
URL GOTO=https://www.endole.co.uk/company/
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:/search/ ATTR=NAME:search CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:/search/ ATTR=CLASS:search_button

TAG POS=1 TYPE=A ATTR=TXT:Infosys<SP>Limited 
<!-- This needs to be defined as relating to the result from the CSV -->

TAG POS=10 TYPE=LI ATTR=CLASS:overview_bullet EXTRACT=TXT
TAG POS=11 TYPE=LI ATTR=CLASS:overview_bullet EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Tom\Desktop FILE=C:\Users\Tom\Desktop\EANFWrite.csv",https://forum.imacros.net/viewtopic.php?f=7&t=26142&sid=e8ccee866abcf29ebd57b704a290f61c,content
2162,Data that is extracted does not change,"Imacros VERSION BUILD=8350307 
Windows 10
Google chrome Version 50.0.2661.87 m (64-bit)

I have setup a rather long macro that extracts data from a website based on the output from a dropdown box and data input section. I require to input information that changes the output however the area that it extracts is the same.

My problem is that the information that is extracted does not change and only appears to extract the first set of inputs out of a total of five inputs.

If i have not explained anything well enough or you need more explanation please ask and i will attempt to add more information to help you answer the problem. I'm a rather new user of this forum with only my second post so any help with the explanations as well would be great, i have pasted the code below for troubleshooting.

Thanks for any help that you can provide.
Code: Select allVERSION BUILD=8340723 RECORDER=CR
'Setting up input data and looping
SET !DATASOURCE Shippinginput.csv 
SET !DATASOURCE_COLUMNS 11
'SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}
SET !EXTRACT_TEST_POPUP NO
SET !TIMEOUT_PAGE 10
SET !ERRORIGNORE YES
'Varible url based on Sku name
'First area code
URL GOTO=http://www.australiandirect.com.au/?rf=kw&kw={{!COL1}}
'Outputting Sku Name
' Reset webpage bypass values to defaults
SET !ERRORIGNORE NO
TAG POS=1 TYPE=P ATTR=TXT:{{!COL1}} Extract=txt
'searching for product using Sku data
'  bypass webpage loading time
SET !TIMEOUT_PAGE 10
TAG POS=1 TYPE=IMG ATTR=SRC:http://www.australiandirect.com.au/assets/thumb/{{!COL1}}*
'Inputing postcode and suburb for first time
WAIT SECONDS=8
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:zip CONTENT={{!COL2}}
WAIT SECONDS=8
TAG POS=1 TYPE=SELECT ATTR=ID:shloc_selector_cysel CONTENT=%{{!COL3}}*
TAG POS=1 TYPE=BUTTON FORM=NAME:NoFormName ATTR=TXT:Calculate
WAIT SECONDS=4
'Outputting Standard shipping
' Ignore error incase of extract error
SET !ERRORIGNORE YES
TAG POS=1 TYPE=DIV ATTR=TXT:Add<SP>To<SP>Cart<SP>Add<SP>To<SP>Wishlist<SP>Standard<SP>Sh* EXTRACT=TXT
SET Std_Shipping_Tmp EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.split('Standard Shipping'); y=x[1].split('- '); z=y[1].substr(0,7); z;"")
SET Std_Shipping EVAL(""var s='{{Std_Shipping_Tmp}}'; var z=s.trim(); z;"")
' Outputting Express shipping
TAG POS=1 TYPE=DIV ATTR=TXT:Add<SP>To<SP>Cart<SP>Add<SP>To<SP>Wishlist<SP>Standard<SP>Sh* EXTRACT=TXT
SET Express_Shipping_Tmp EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.split('Express Shipping'); y=x[1].split('- '); z=y[1].substr(0,7); z;"")
SET Express_Shipping EVAL(""var s='{{Express_Shipping_Tmp}}'; var z=s.trim(); z;"")
SET !ERRORIGNORE NO
'Inputing postcode and suburb second time
'WAIT SECONDS=8
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:zip CONTENT={{!COL4}}
WAIT SECONDS=8
TAG POS=1 TYPE=SELECT ATTR=ID:shloc_selector_cysel CONTENT=%{{!COL5}}*
TAG POS=1 TYPE=BUTTON FORM=NAME:NoFormName ATTR=TXT:Calculate
WAIT SECONDS=4
'Outputting Standard shipping
' Ignore error incase of extract error
SET !ERRORIGNORE YES
TAG POS=1 TYPE=DIV ATTR=TXT:Add<SP>To<SP>Cart<SP>Add<SP>To<SP>Wishlist<SP>Standard<SP>Sh* EXTRACT=TXT
SET Std_Shipping_Tmp EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.split('Standard Shipping'); y=x[1].split('- '); z=y[1].substr(0,7); z;"")
SET Std_Shipping EVAL(""var s='{{Std_Shipping_Tmp}}'; var z=s.trim(); z;"")
' Outputting Express shipping
TAG POS=1 TYPE=DIV ATTR=TXT:Add<SP>To<SP>Cart<SP>Add<SP>To<SP>Wishlist<SP>Standard<SP>Sh* EXTRACT=TXT
SET Express_Shipping_Tmp EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.split('Express Shipping'); y=x[1].split('- '); z=y[1].substr(0,7); z;"")
SET Express_Shipping EVAL(""var s='{{Express_Shipping_Tmp}}'; var z=s.trim(); z;"")
SET !ERRORIGNORE NO
'Inputing postcode and suburb for third time
'WAIT SECONDS=8
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:zip CONTENT={{!COL6}}
WAIT SECONDS=8
TAG POS=1 TYPE=SELECT ATTR=ID:shloc_selector_cysel CONTENT=%{{!COL7}}*
TAG POS=1 TYPE=BUTTON FORM=NAME:NoFormName ATTR=TXT:Calculate
WAIT SECONDS=4
'Outputting Standard shipping
' Ignore error incase of extract error
SET !ERRORIGNORE YES
TAG POS=1 TYPE=DIV ATTR=TXT:Add<SP>To<SP>Cart<SP>Add<SP>To<SP>Wishlist<SP>Standard<SP>Sh* EXTRACT=TXT
SET Std_Shipping_Tmp EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.split('Standard Shipping'); y=x[1].split('- '); z=y[1].substr(0,7); z;"")
SET Std_Shipping EVAL(""var s='{{Std_Shipping_Tmp}}'; var z=s.trim(); z;"")
' Outputting Express shipping
TAG POS=1 TYPE=DIV ATTR=TXT:Add<SP>To<SP>Cart<SP>Add<SP>To<SP>Wishlist<SP>Standard<SP>Sh* EXTRACT=TXT
SET Express_Shipping_Tmp EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.split('Express Shipping'); y=x[1].split('- '); z=y[1].substr(0,7); z;"")
SET Express_Shipping EVAL(""var s='{{Express_Shipping_Tmp}}'; var z=s.trim(); z;"")
SET !ERRORIGNORE NO
'Inputing postcode and suburb for forth time
'WAIT SECONDS=8
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:zip CONTENT={{!COL8}}
WAIT SECONDS=8
TAG POS=1 TYPE=SELECT ATTR=ID:shloc_selector_cysel CONTENT=%{{!COL9}}*
TAG POS=1 TYPE=BUTTON FORM=NAME:NoFormName ATTR=TXT:Calculate
WAIT SECONDS=4
'Outputting Standard shipping
' Ignore error incase of extract error
SET !ERRORIGNORE YES
TAG POS=1 TYPE=DIV ATTR=TXT:Add<SP>To<SP>Cart<SP>Add<SP>To<SP>Wishlist<SP>Standard<SP>Sh* EXTRACT=TXT
SET Std_Shipping_Tmp EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.split('Standard Shipping'); y=x[1].split('- '); z=y[1].substr(0,7); z;"")
SET Std_Shipping EVAL(""var s='{{Std_Shipping_Tmp}}'; var z=s.trim(); z;"")
' Outputting Express shipping
TAG POS=1 TYPE=DIV ATTR=TXT:Add<SP>To<SP>Cart<SP>Add<SP>To<SP>Wishlist<SP>Standard<SP>Sh* EXTRACT=TXT
SET Express_Shipping_Tmp EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.split('Express Shipping'); y=x[1].split('- '); z=y[1].substr(0,7); z;"")
SET Express_Shipping EVAL(""var s='{{Express_Shipping_Tmp}}'; var z=s.trim(); z;"")
SET !ERRORIGNORE NO
'Inputing postcode and suburb for last time
'WAIT SECONDS=8
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:zip CONTENT={{!COL10}}
WAIT SECONDS=8
TAG POS=1 TYPE=SELECT ATTR=ID:shloc_selector_cysel CONTENT=%{{!COL11}}*
TAG POS=1 TYPE=BUTTON FORM=NAME:NoFormName ATTR=TXT:Calculate
WAIT SECONDS=4
'Outputting Standard shipping
' Ignore error incase of extract error
SET !ERRORIGNORE YES
TAG POS=1 TYPE=DIV ATTR=TXT:Add<SP>To<SP>Cart<SP>Add<SP>To<SP>Wishlist<SP>Standard<SP>Sh* EXTRACT=TXT
SET Std_Shipping_Tmp EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.split('Standard Shipping'); y=x[1].split('- '); z=y[1].substr(0,7); z;"")
SET Std_Shipping EVAL(""var s='{{Std_Shipping_Tmp}}'; var z=s.trim(); z;"")
' Outputting Express shipping
TAG POS=1 TYPE=DIV ATTR=TXT:Add<SP>To<SP>Cart<SP>Add<SP>To<SP>Wishlist<SP>Standard<SP>Sh* EXTRACT=TXT
SET Express_Shipping_Tmp EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.split('Express Shipping'); y=x[1].split('- '); z=y[1].substr(0,7); z;"")
SET Express_Shipping EVAL(""var s='{{Express_Shipping_Tmp}}'; var z=s.trim(); z;"")
'Header added manually in the File...
' Reset webpage bypass values to defaults
SET !ERRORIGNORE NO
SET !EXTRACT {{!COL1}}
ADD !EXTRACT {{Std_Shipping}}
ADD !EXTRACT {{Std_Shipping}}
ADD !EXTRACT {{Std_Shipping}}
ADD !EXTRACT {{Std_Shipping}}
ADD !EXTRACT {{Std_Shipping}}
ADD !EXTRACT {{Express_Shipping}}
ADD !EXTRACT {{Express_Shipping}}
ADD !EXTRACT {{Express_Shipping}}
ADD !EXTRACT {{Express_Shipping}}
ADD !EXTRACT {{Express_Shipping}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=Extract_{{!NOW:ddmmyy}}.csv",https://forum.imacros.net/viewtopic.php?f=7&t=26136&sid=e8ccee866abcf29ebd57b704a290f61c,content
2163,Extracting from dropdown by xpath,"VERSION BUILD=8961227
WIndows XP
Firefox 40.0.1

Is it possible to extract from dropdown if I mark it by xpath ?
Code: Select allTAG XPATH=""/html/body/div[2]/div/div[4]/div/div[1]/div[6]/div[2]/div[2]/div/div[3]/div/table/tbody/tr[1]/td[3]/select"" CONTENT=%2 EXTRACT=TXT 

I receive:
wrong format of TAG command, line 4 (Error code: -910)

As it is mentioned here -> http://wiki.imacros.net/Data_Extraction ... T_Elements it works for select elements but if it works for select elements marked by xpath ?",https://forum.imacros.net/viewtopic.php?f=7&t=25987&sid=e8ccee866abcf29ebd57b704a290f61c,content
2164,insert  name of the image,"I need to insert, in the  a.txt  also the  name of the image:  immages_{{!NOW:mmdd_hhnnss}}.jpg


ONDOWNLOAD FOLDER=* FILE=immages_{{!NOW:mmdd_hhnnss}}.jpg
TAG POS=1 TYPE=IMG ATTR=HREF:* CONTENT=EVENT:SAVEITEM
SAVEAS TYPE=EXTRACT FOLDER=* FILE=a.txt


I speak little English, I try to explain, better

Image is renames: 0318_48195648.jpg
It is placed in a.txt 0318_48195648.jpg",https://forum.imacros.net/viewtopic.php?f=7&t=25984&sid=e8ccee866abcf29ebd57b704a290f61c,content
2165,How to extract a variable number,"Imacros VERSION BUILD=8350307 
Windows 10
Google chrome Version 49.0.2623.87 m (64-bit)

So far i have a simple macro set-up to extract the price of the standard shipping from the webpage, however when i use the record setting it highlights the whole column and displays alot of unnecessary information. After the information is removed and only the price of the shipping remains i want to be able to use the macro to extract the shipping regardless of the two inputs.

If i have not explained anything well enough or you need more explanation please ask and i will attempt to add more information to help you answer the problem. I'm a new user of this forum and of marco so any help with the explanations as well would be great, i have pasted a small portion of the code below for troubleshooting. 

Thanks for any help that you can provide.

VERSION BUILD=8350307 RECORDER=CR
URL GOTO=http://www.australiandirect.com.au/buy/ ... ry/KA12230
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:zip CONTENT=6215
WAIT SECONDS=5
TAG POS=1 TYPE=SELECT ATTR=ID:shloc_selector_cysel CONTENT=%PRESTON<SP>BEACH;WA
TAG POS=1 TYPE=BUTTON ATTR=ID:btn-calculator
TAG POS=1 TYPE=DIV ATTR=TXT:Add<SP>To<SP>Cart<SP>Add<SP>To<SP>Wishlist<SP>Standard<SP>Sh* Extract=txt",https://forum.imacros.net/viewtopic.php?f=7&t=25978&sid=e8ccee866abcf29ebd57b704a290f61c,content
2166,"Need data extraction to work in the ""loop""","I am hoping someone could advise me on the script below, I am new to this.  I am having 2 issues now: 1) It will do the data extraction part only if I do ""play"", without the loop.  But I want it on a loop.  I need it to pull the entry from a cell on my datasource, enter it on the website, search, and save the result if any, and then do it on a loop moving down each cell. 2) Also, I would like to either have the extraction from each loop saved into one file, or else not save it if there is no result.  I am using the iMacros Browser (trial) in Windows, because I needed the Image wizard for the slider bar and text extraction.   

VERSION BUILD=8881205 RECORDER=FX 
TAB T=1 
'CSV = Comma Separated Values in each line of the file 
SET !DATASOURCE vietnam.csv
'Number of columns in the CSV file. This must be accurate! 
SET !DATASOURCE_COLUMNS 1 
'Start at line 2 to skip the header in the file 
SET !LOOP 1 
'Increase the current position in the file with each loop 
SET !DATASOURCE_LINE {{!LOOP}}
URL GOTO=https://sanctionssearch.ofac.treas.gov/
TAG POS=1 TYPE=IMG ATTR=ID:Slider1_handleImage
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:ctl00$MainContent$Slider1 CONTENT=80
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:aspnetForm ATTR=ID:ctl00_MainContent_txtLastName CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:aspnetForm ATTR=ID:ctl00_MainContent_btnSearch
TAG POS=1 TYPE=DIV ATTR=ID:ctl00_MainContent_pnlResults EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=Extract_{{!NOW:ddmmyy_hhnnss}}.csv
WAIT SECONDS=3",https://forum.imacros.net/viewtopic.php?f=7&t=25934&sid=e8ccee866abcf29ebd57b704a290f61c,content
2167,Can iMacros extract metadata?,"Hello,

I have a list of URLs that I want to extract metadata from.  I am only concerned with extracting the date from each page:

<meta name=""dc.date.created"" content=""2004-07-23"" />

I want to know when each page was created - but there are hundreds of pages.

Please let me know if this is possible with the software asap.

Thank you.",https://forum.imacros.net/viewtopic.php?f=7&t=20830&sid=e8ccee866abcf29ebd57b704a290f61c,content
2168,Extracting text from one textbox into separated lines.,"VERSION BUILD=8.9.6
Windows 7
Mozilla Firefox 44.0.2
So I have a textbox, which contain this text (example):
facebook.com/12345678910111
facebook.com/23456789101112
facebook.com/34567891011121
After using extraction I always got this (text without line breaks):
""http://facebook.com/12345678910111http: ... 7891011121""
The problem is, that I need iMacros to save text from textbox with line breaks.
I need this 'coz this textbox contains list of FaceBook IDs. After saving them in txt file, I use it as sourcefile for !DATASOURCE, so I can input into searchbox on facebook 1 ID at each loop of iMacros.
Code: Select allTAG POS=1 TYPE=TEXTAREA ATTR=ID:text EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=FBID.txt
Code: Select allSET !DATASOURCE FBID.txt
SET !DATASOURCE_LINE {{!LOOP}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:gedit_users_search_inp CONTENT={{!COL1}}

If anyone could help me, I would appreciate.",https://forum.imacros.net/viewtopic.php?f=7&t=25822&sid=e8ccee866abcf29ebd57b704a290f61c,content
2170,Add the current time and date into field,"Hello Newbie here!! I am looking for a way to add the current time and date into a webpage. I recorded a macro and put in the current time and date but when I run the macro obviously it puts in the date it was when I created the macro and not the current date. Below is the macro code. Thanks

TAG POS=1 TYPE=SELECT FORM=NAME:frmDetail ATTR=NAME:Date_1 CONTENT=%2
TAG POS=1 TYPE=SELECT FORM=NAME:frmDetail ATTR=NAME:Date_2 CONTENT=%17
TAG POS=1 TYPE=SELECT FORM=NAME:frmDetail ATTR=NAME:Date_3 CONTENT=%2016
TAG POS=1 TYPE=SELECT FORM=NAME:frmDetail ATTR=NAME:Date_4 CONTENT=%1
TAG POS=1 TYPE=SELECT FORM=NAME:frmDetail ATTR=NAME:Date_5 CONTENT=%:00
TAG POS=1 TYPE=SELECT FORM=NAME:frmDetail ATTR=NAME:Date_6 CONTENT=%PM",https://forum.imacros.net/viewtopic.php?f=7&t=25801&sid=9d96e1268e5b272969ed509f53432633,content
2171,extracting all link in a web page,"i'm trying to extract links in a web page but i only can get one link. can anyone help me to fix my code for extacting all links in it?

VERSION BUILD=8961227 RECORDER=FX
TAB T=1
URL GOTO=https://www.blogger.com/profile/16168795899587474395
TAG POS=1 TYPE=DIV ATTR=CLASS:contents-after-sidebar*
TAG POS=1 TYPE=li ATTR=CLASS:sidebar-item*
TAG POS=R{{!LOOP}} TYPE=A ATTR=HREF:* EXTRACT=HREF

thanks ",https://forum.imacros.net/viewtopic.php?f=7&t=25767&sid=9d96e1268e5b272969ed509f53432633,content
2172,Data extracting from PDF,"Looking for help creating a script/workflow for extracting data from a PDF and separating the contents based on unique identifiers used as delimitation.

If anyone has experience with this sort of exercise please get in touch with me. Need help ASAP.",https://forum.imacros.net/viewtopic.php?f=7&t=25769&sid=9d96e1268e5b272969ed509f53432633,content
2173,i can't extract text to a variable in vba,"Code: Select allDim iim, status
Set iim = CreateObject(""imacros"")
status = iim.iimOpen(""-ie"", False)
Dim macro

macro = ""VERSION BUILD=10022823"" + vbNewLine
macro = macro + ""TAB T=1"" + vbNewLine
macro = macro + ""TAB CLOSEALLOTHERS"" + vbNewLine
macro = macro + ""URL GOTO=https://visa.mofa.gov.sa/Omra/addOmraVisa"" + vbNewLine
macro = macro + ""TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:/Omra/addOmraVisa ATTR=NAME:ApplicationNumber CONTENT=66359690"" + vbNewLine
macro = macro + ""TAG POS=1 TYPE=DIV FORM=ACTION:/Omra/addOmraVisa ATTR=TXT:Application<SP>Number<SP>Choose<SP>Saudi<SP>Mission<SP>selectSearch"" + vbNewLine
macro = macro + ""'text input activated"" + vbNewLine
macro = macro + ""TAG POS=1 TYPE=SPAN FORM=ACTION:/Omra/addOmraVisa ATTR=CLASS:t-input"" + vbNewLine
macro = macro + ""TAG POS=1 TYPE=LI ATTR=TXT:Islamabad"" + vbNewLine
macro = macro + ""TAG POS=1 TYPE=BUTTON:SUBMIT FORM=ACTION:/Omra/addOmraVisa ATTR=NAME:CommandName"" + vbNewLine
macro = macro + ""'text input activated"" + vbNewLine
macro = macro + ""TAG POS=3 TYPE=SPAN ATTR=CLASS:addlabel EXTRACT=TXT"" + vbNewLine
macro = macro + ""TAG POS=3 TYPE=SPAN ATTR=CLASS:addlabel EXTRACT=TXT"" + vbNewLine
macro = macro + ""TAG POS=3 TYPE=SPAN ATTR=CLASS:addlabel EXTRACT=TXT"" + vbNewLine
macro = macro + ""TAG POS=3 TYPE=SPAN ATTR=CLASS:addlabel EXTRACT=TXT"" + vbNewLine
Me.Text3.Value = iim.iimgetlastextract(1)

status = iim.iimPlayCode(macro)
i can't copy the EXTRACT=TXT to  me.text3.value thankx in advance",https://forum.imacros.net/viewtopic.php?f=7&t=25737&sid=9d96e1268e5b272969ed509f53432633,content
2174,imacros browser crash,"Good evening to everybody,

I have a problem with my iMacros script in iMacros Browser.

I'm using Windows 7

This is the message of the CrashReport.txt:

========================================
2016-01-18 11:54:07Z
iMacros version 10.3.27.5830, IE version 11.0.9600.16428, OS version Microsoft Windows NT 6.1.7601 Service Pack 1

Exception at System.Drawing.Graphics FromHdcInternal(IntPtr)
Memoria insufficiente.
   in System.Drawing.Graphics.FromHdcInternal(IntPtr hdc)
   in System.Drawing.BufferedGraphicsContext.CreateBuffer(IntPtr src, Int32 offsetX, Int32 offsetY, Int32 width, Int32 height)
   in System.Drawing.BufferedGraphicsContext.AllocBuffer(Graphics targetGraphics, IntPtr targetDC, Rectangle targetRectangle)
   in System.Drawing.BufferedGraphicsContext.Allocate(IntPtr targetDC, Rectangle targetRectangle)
   in System.Windows.Forms.Control.WmPaint(Message& m)
   in System.Windows.Forms.Control.WndProc(Message& m)
   in System.Windows.Forms.Label.WndProc(Message& m)
   in System.Windows.Forms.Control.ControlNativeWindow.OnMessage(Message& m)
   in System.Windows.Forms.Control.ControlNativeWindow.WndProc(Message& m)
   in System.Windows.Forms.NativeWindow.Callback(IntPtr hWnd, Int32 msg, IntPtr wparam, IntPtr lparam)

========================================
2016-01-18 11:54:32Z
iMacros version 10.3.27.5830, IE version 11.0.9600.16428, OS version Microsoft Windows NT 6.1.7601 Service Pack 1

Exception at IntPtr GetCOMIPFromRCW(System.Object, IntPtr, IntPtr ByRef, Boolean ByRef)
Impossibile utilizzare oggetti COM separati dai relativi RCW sottostanti.
   in System.StubHelpers.StubHelpers.GetCOMIPFromRCW(Object objSrc, IntPtr pCPCMD, IntPtr& ppTarget, Boolean& pfNeedsRelease)
   in SHDocVw.WebBrowserClass.PutProperty(String Property, Object vtValue)
   in iMacros.Core.Logic.DirectScreenApi.HKReplay_Stop(WebBrowserClass webBrowser)
   in iMacros.Core.Logic.MacroPlayer.UninitializeDSPlayback(WebBrowserClass webBrowser)
   in iMacros.Core.Logic.MacroPlayer.Play(Macro macro, Boolean blockCommands)
   in iMacros.WinUI.Common.SidebarControl.PlayStart(Func`1 getMacro, Int32 repeatCount)
   in iMacros.WinUI.Common.SidebarControl.PlayStart(String relativePath, Int32 repeatCount)
   in iMacros.WinUI.Common.SidebarControl.#=qw2TiTV0rLGKHkj17Rx2ftltL2UetAgk7i1eWYslwXKQ=(Object #=qBdLLW6tIgV9gex7GuA6ibw==, EventArgs #=qR9TIpSAhK0gAw1hHShpghw==)
   in System.Windows.Forms.Control.OnClick(EventArgs e)
   in System.Windows.Forms.Button.OnClick(EventArgs e)
   in System.Windows.Forms.Button.WndProc(Message& m)
   in System.Windows.Forms.Control.ControlNativeWindow.OnMessage(Message& m)
   in System.Windows.Forms.Control.ControlNativeWindow.WndProc(Message& m)
   in System.Windows.Forms.NativeWindow.Callback(IntPtr hWnd, Int32 msg, IntPtr wparam, IntPtr lparam)

This is the script Code:

VERSION BUILD=8881205 RECORDER=FX
TAB T=1
SET !ERRORIGNORE YES
SET !TIMEOUT_STEP 0
SET !EXTRACT_TEST_POPUP NO
SET !DATASOURCE prova.csv
SET !DATASOURCE_LINE {{!LOOP}}
'PROMPT {{!COL1}}


FRAME NAME=mainFrame

TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:fSearch ATTR=CLASS:f CONTENT={{!COL1}}
SIZE X=992 Y=735
DS CMD=MOVETO X=460 Y=267 CONTENT=
WAIT SECONDS = 0.1
DS CMD=LDOWN X=460 Y=267 CONTENT=
DS CMD=LUP X=460 Y=267 CONTENT=
DS CMD=MOVETO X=426 Y=294 CONTENT=
DS CMD=MOVETO X=262 Y=344 CONTENT=

WAIT SECONDS = 0.5


DS CMD=MOVETO X=243 Y=344 CONTENT=
DS CMD=MOVETO X=346 Y=352 CONTENT=
DS CMD=RDOWN X=346 Y=352 CONTENT=
DS CMD=RUP X=346 Y=352 CONTENT=
DS CMD=MOVETO X=362 Y=361 CONTENT=
DS CMD=MOVETO X=380 Y=361 CONTENT=
DS CMD=LDOWN X=380 Y=361 CONTENT=
DS CMD=LUP X=380 Y=361 CONTENT=
DS CMD=MOVETO X=215 Y=456 CONTENT=


WAIT SECONDS = 0.5

FRAME NAME=""mainFrame""
TAG POS=1 TYPE=INPUT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=2 TYPE=INPUT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=1 TYPE=SELECT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=3 TYPE=INPUT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=1 TYPE=INPUT ATTR=CLASS:""f fR fD""&&VALUE:* EXTRACT=TXT 
TAG POS=2 TYPE=SELECT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=4 TYPE=INPUT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT 
TAG POS=5 TYPE=INPUT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=6 TYPE=INPUT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=7 TYPE=INPUT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=8 TYPE=INPUT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=9 TYPE=INPUT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=3 TYPE=SELECT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=4 TYPE=SELECT ATTR=CLASS:""f fR""&&VALUE:* EXTRACT=TXT
TAG POS=1 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=2 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=3 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=4 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=5 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=6 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=7 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=8 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=9 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=10 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=11 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=12 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
TAG POS=13 TYPE=DIV ATTR=CLASS:""f fO""&&VALUE:* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=test.csv

URL GOTO=http://prova.it

WAIT SECONDS = 2

SIZE X=992 Y=735
'DS CMD=MOVETO X=176 Y=526 CONTENT=
'DS CMD=MOVETO X=252 Y=646 CONTENT=
'DS CMD=MOVETO X=202 Y=709 CONTENT=
'DS CMD=MOVETO X=137 Y=726 CONTENT=
'DS CMD=MOVETO X=199 Y=709 CONTENT=
'DS CMD=MOVETO X=140 Y=723 CONTENT=
'DS CMD=MOVETO X=134 Y=723 CONTENT=
'DS CMD=MOVETO X=80 Y=732 CONTENT=
'DS CMD=MOVETO X=48 Y=726 CONTENT=
'DS CMD=MOVETO X=295 Y=723 CONTENT=
'DS CMD=MOVETO X=359 Y=729 CONTENT=
'DS CMD=MOVETO X=167 Y=729 CONTENT=
'DS CMD=MOVETO X=48 Y=728 CONTENT=
DS CMD=MOVETO X=12 Y=725 CONTENT=
DS CMD=MOVETO X=20 Y=725 CONTENT=
DS CMD=LDOWN X=20 Y=725 CONTENT=
DS CMD=MOVETO X=7 Y=726 CONTENT=
DS CMD=LDOWN X=7 Y=726 CONTENT=
DS CMD=LDBLCLK X=7 Y=726 CONTENT=
DS CMD=LDOWN X=7 Y=726 CONTENT=
DS CMD=MOVETO X=58 Y=537 CONTENT=
DS CMD=MOVETO X=61 Y=521 CONTENT=
DS CMD=MOVETO X=51 Y=518 CONTENT=
DS CMD=MOUSEWHEEL X=51 Y=518 CONTENT=120
DS CMD=MOUSEWHEEL X=51 Y=518 CONTENT=120
DS CMD=MOUSEWHEEL X=51 Y=518 CONTENT=120
DS CMD=MOUSEWHEEL X=51 Y=518 CONTENT=120
DS CMD=MOUSEWHEEL X=51 Y=518 CONTENT=120
DS CMD=MOUSEWHEEL X=51 Y=518 CONTENT=120
DS CMD=MOVETO X=50 Y=515 CONTENT=
DS CMD=MOUSEWHEEL X=50 Y=515 CONTENT=120
DS CMD=MOUSEWHEEL X=50 Y=515 CONTENT=120
DS CMD=MOUSEWHEEL X=50 Y=515 CONTENT=120
DS CMD=MOUSEWHEEL X=48 Y=515 CONTENT=120
DS CMD=MOUSEWHEEL X=48 Y=515 CONTENT=120
DS CMD=MOUSEWHEEL X=48 Y=515 CONTENT=120
DS CMD=MOUSEWHEEL X=45 Y=514 CONTENT=120
DS CMD=MOUSEWHEEL X=45 Y=514 CONTENT=120
DS CMD=MOUSEWHEEL X=45 Y=514 CONTENT=120
DS CMD=MOVETO X=38 Y=508 CONTENT=
DS CMD=MOUSEWHEEL X=38 Y=508 CONTENT=120
DS CMD=MOUSEWHEEL X=38 Y=508 CONTENT=120
DS CMD=MOUSEWHEEL X=38 Y=508 CONTENT=120
DS CMD=MOUSEWHEEL X=39 Y=507 CONTENT=240
DS CMD=MOUSEWHEEL X=39 Y=507 CONTENT=240
DS CMD=MOUSEWHEEL X=39 Y=507 CONTENT=240
DS CMD=MOVETO X=44 Y=505 CONTENT=
DS CMD=MOVETO X=47 Y=510 CONTENT=
DS CMD=MOVETO X=70 Y=498 CONTENT=
DS CMD=MOVETO X=173 Y=272 CONTENT=
DS CMD=MOVETO X=189 Y=188 CONTENT=
DS CMD=MOVETO X=196 Y=162 CONTENT=
DS CMD=MOUSEWHEEL X=196 Y=162 CONTENT=120
DS CMD=MOUSEWHEEL X=196 Y=162 CONTENT=120
DS CMD=MOUSEWHEEL X=196 Y=162 CONTENT=120
DS CMD=MOUSEWHEEL X=196 Y=156 CONTENT=120
DS CMD=MOUSEWHEEL X=196 Y=156 CONTENT=120
DS CMD=MOUSEWHEEL X=196 Y=156 CONTENT=120
DS CMD=MOVETO X=198 Y=152 CONTENT=
DS CMD=MOUSEWHEEL X=198 Y=152 CONTENT=120
DS CMD=MOUSEWHEEL X=198 Y=152 CONTENT=120
DS CMD=MOUSEWHEEL X=198 Y=152 CONTENT=120
DS CMD=MOVETO X=199 Y=146 CONTENT=
DS CMD=MOVETO X=198 Y=158 CONTENT=
DS CMD=MOVETO X=135 Y=204 CONTENT=
DS CMD=MOVETO X=137 Y=224 CONTENT=
DS CMD=MOVETO X=84 Y=361 CONTENT=
DS CMD=MOVETO X=60 Y=364 CONTENT=
DS CMD=MOVETO X=42 Y=364 CONTENT=
DS CMD=MOVETO X=50 Y=361 CONTENT=
DS CMD=MOVETO X=54 Y=360 CONTENT=
DS CMD=MOVETO X=47 Y=358 CONTENT=
DS CMD=MOVETO X=54 Y=357 CONTENT=
DS CMD=LDOWN X=54 Y=357 CONTENT=
DS CMD=LUP X=54 Y=357 CONTENT=
DS CMD=MOVETO X=73 Y=399 CONTENT=

WAIT SECONDS = 1


Can you help me?

Thanks in advance for the answers.",https://forum.imacros.net/viewtopic.php?f=7&t=25651&sid=9d96e1268e5b272969ed509f53432633,content
2175,Trim/replace/split extracted htm Before and After.,"Hello,

I'm having trouble trimming/replacing or splitting extracted html, I have tried every code from the forum and nothing seems to be working. I need to get ""Limited stock available."" and replace the text if found with a 1 for csv saving. I was able to remove everything after ""available."" but have no idea how to remove all text before ""Limited"". what's the current way to do this?

TAG POS=1 TYPE=DIV ATTR=CLASS:notices<SP>clear<SP>clearfix EXTRACT=HTM
SET !VAR2 EVAL(""var s=\""{{!VAR1}}\""; s.split(\"" (\"", 1);"")
PROMPT {{!VAR2}}
<div class=""notices clear clearfix"" style=""border: 1px solid blue; border-image: none;"" oldBorder=""undefined"">                             <a class=""notice save"" aria-describedby=""qtip-0"" href=""#"" data-content='Buy <em class=""placeholder"">6</em> of this item for $<em class=""placeholder"">4.77</em> each' data-hasqtip=""0""><span class=""sprite sprite-save""></span></a>                            <a class=""notice av un"" aria-describedby=""qtip-1"" href=""#"" data-content='<div>Limited stock available. (0)<br/><br/> Inventory levels fluctuate throughout the day as products are being received and shipped.  Contact your Customer Service Specialist for further availablity.</div><div style=""margin: 2px 0; padding: 3px 8px; font-weight: bold; color: #FFF; background-color: #8A3C18;"">Availability outside of your region</div><div><span style=""display: inline-block; width: 225px; padding: 0 8px;"">Portland, OR:</span> <span style=""display:inline-block;width:10px;height:10px;background-color:#0F0""></span></div><div><span style=""display: inline-block; width: 225px; padding: 0 8px;"">Denver, CO:</span> <span style=""display:inline-block;width:10px;height:10px;background-color:#0F0""></span></div><div><span style=""display: inline-block; width: 225px; padding: 0 8px;"">Fairless Hills, PA:</span> <span style=""display:inline-block;width:10px;height:10px;background-color:#0F0""></span></div><div style=""margin: 2px 0; padding: 2px; height: 2px; background-color: #8A3C18;""></div><div style=""font-size: 10px; padding: 0 8px;"">Fulfillment from outside of your region may incur additional freight charges. Please contact your Customer Service Specialist for assistance.</div>' data-hasqtip=""1""><span class=""sprite sprite-oos""></span></a>                    </div>

iMacros Browser (x86) Version 10.4.28.1074
IE 11
Windows 8.1",https://forum.imacros.net/viewtopic.php?f=7&t=25627&sid=9d96e1268e5b272969ed509f53432633,content
2176,"If field doesn't exist, how to skip it ?","Im scraping fields Town or Street addresses from my urls.
Sometimes my url doesn't Town or Street fields and when this happens, 
imacros will do ""Tag waiting 6 seconds"" command automatically.

Is it possible to skip Town or Street code if it doesn't exist in url ?
How should I modify my code.  
Code: Select allVERSION BUILD=8601111 RECORDER=FX
SET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP NO
SET !DATASOURCE addresses.csv
SET !ENCRYPTION NO
PROXY ADDRESS={{!COL2}}
SET !ENCRYPTION NO
ONLOGIN USER={{!COL3}} PASSWORD={{!COL4}}

TAB T=1
WAIT SECONDS=2
URL GOTO={{!COL1}}

Tag Pos=1 Type=Div Attr=Class:large-4*&&Txt:Street<sp>address Content=Event:MouseOver
Tag Pos=R1 Type=Div Attr=* Extract=Txt
Set street {{!Extract}}
Set !Extract Null
Set FinalVar Eval(""var a = '{{street}}'; if (a != '#EANF#') { b = a; } else { b = 'Not found!'; } b;"")

Tag Pos=1 Type=Div Attr=Class:large-4*&&Txt:Town Content=Event:MouseOver
Tag Pos=R1 Type=Div Attr=* Extract=Txt
Set town {{!Extract}}
Set !Extract Null
Set FinalVar Eval(""var a = '{{town}}'; if (a != '#EANF#') { b = a; } else { b = 'Not found!'; } b;"")

add !extract {{street}}
add !extract {{town}}

SAVEAS TYPE=EXTRACT FOLDER=c:\imacros FILE=test1.csv


Help appreciated !",https://forum.imacros.net/viewtopic.php?f=7&t=25503&sid=9d96e1268e5b272969ed509f53432633,content
2177,Captcha Solutions,"Here one captcha solver support for iMacros from Captcha Solutions http://www.captchasolutions.com/ They solve captchas for as low as $0.0017 per captcha solved.


VERSION BUILD=8820413 RECORDER=FX
TAB T=1

ONDOWNLOAD FOLDER=c:\ FILE=captcha.jpg

TAG POS=1 TYPE=IMG ATTR=HREF:*captcha* CONTENT=EVENT:SAVE_ELEMENT_SCREENSHOT

SET !EXTRACT_TEST_POPUP NO
TAB OPEN
TAB T=2

URL GOTO=http://api.captchasolutions.com/solve
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:http://api.captchasolutions.com/solve ATTR=NAME:p CONTENT=imacros
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:http://api.captchasolutions.com/solve ATTR=NAME:key CONTENT=YOUR_API_KEY_HERE
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:http://api.captchasolutions.com/solve ATTR=NAME:secret CONTENT=YOUR_SECRET_KEY_HERE
TAG POS=1 TYPE=INPUT:FILE FORM=ACTION:http://api.captchasolutions.com/solve ATTR=NAME:captcha CONTENT=C:\captcha.jpg
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://api.captchasolutions.com/solve ATTR=VALUE:Send
wait seconds=5
TAG POS=6 TYPE=TD ATTR=* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}

TAB CLOSE
TAB T=1

TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:NoFormName ATTR=NAME:captcha CONTENT={{!var1}}",https://forum.imacros.net/viewtopic.php?f=7&t=25482&sid=9d96e1268e5b272969ed509f53432633,content
2178,XPATH wiki is wrong,"Browser = FF 42.0
iMacros = 8.9.4
OS = Windows 7 & 10, Mac 10.7.5 - 10.9

The information on this page is wrong. 
http://wiki.imacros.net/XPATH 
This is the solution. 
Code: Select allTAB T=1     
TAB CLOSEALLOTHERS  
URL GOTO=http://demo.imacros.net/Automate/TestForm1
TAG XPATH=""//input[@name='name']"" CONTENT=""Tom Tester""
TAG XPATH=""//select[@name='food']"" CONTENT=$Pizza
TAG XPATH=""//select[@name='drink']"" CONTENT=$Water
TAG XPATH=""//input[@id='small']"" CONTENT=YES
TAG XPATH=""//select[@name='dessert']"" CONTENT=$Apple<SP>Pie:$Fruits
TAG XPATH=""//input[@value='Yes']"" CONTENT=Yes
SET !ENCRYPTION NO
TAG XPATH=""//input[@name='Reg_code']"" CONTENT=""demo123""
TAG XPATH=""//textarea[@name='Remarks']"" CONTENT=""Hi Tom! Sorry, your script didn't work. Hacked by RethinkReality.""
TAG XPATH=""//button[@type='submit']""
WAIT SECONDS=5
URL GOTO=http://demo.imacros.net/Automate/OK
#EDIT: Had extra quotation marks when transferred it from JS to iim, it works now.",https://forum.imacros.net/viewtopic.php?f=7&t=25414&sid=9d96e1268e5b272969ed509f53432633,content
2179,Extracting Content into specific columns in Excel,"Hi Team,

I managed to pull the content from a web source onto a spreadsheet. But it is all getting displayed in 1 column sequentially. I want to direct the extracted content each into specific columns. Please help me out on this.

Thanks in Advance",https://forum.imacros.net/viewtopic.php?f=7&t=25396&sid=9d96e1268e5b272969ed509f53432633,content
2180,Extracting data for flights,"Hello,

I'm trying to use iMacros to extract some data from Google Flights, however I'm not able to extract all the airline names from the cheapest flight, however it only extracts the content from the tooltip even if I'm trying to extract all the text inside the span class ""tooltipUseHtml tooltipLeft"". On this website I would like it to extract the following text: ""JAL, Iberia 路 Iberia Express"" instead of ""IB 3740 operated by Iberia Express"" as it does at this moment.

Code: Select allTAG POS=1 TYPE=DIV ATTR=CLASS:tooltipContent<SP>HMTHUQB-d-k EXTRACT=TXT

That's the code I'm using but as I said it's not working as expected... Extracting both sentences ""IB 3740 operated by Iberia Express"" and ""JAL, Iberia 路 Iberia Express"" would also be fine.

Can anybody please help me to get this right?

Thanks for your help!",https://forum.imacros.net/viewtopic.php?f=7&t=25312&sid=9d96e1268e5b272969ed509f53432633,content
2181,how to extract the data more accurate and precise.,"Hello friends.
I'm learning extract with iMacros.
The following script that I created.
Code: Select allTAG POS=1 TYPE=IMG ATTR=CLASS:cboxPhoto&&HREF:https://*.*&&ALT:* CONTENT=EVENT:SAVEPICTUREAS
TAG POS=1 TYPE=IMG ATTR=CLASS:cboxPhoto&&HREF:https://*.*&&ALT:*&&TXT:* EXTRACT=HREF
TAG POS=1 TYPE=BUTTON ATTR=TYPE:button&&ID:cboxNext
ONDOWNLOAD FOLDER=D:\Hasil<SP>Scrape\image\ FILE=*
WAIT SECONDS=2
TAG POS=1 TYPE=IMG ATTR=CLASS:cboxPhoto&&HREF:https://*.*&&ALT:* CONTENT=EVENT:SAVEPICTUREAS
TAG POS=1 TYPE=IMG ATTR=CLASS:cboxPhoto&&HREF:https://*.*&&ALT:*&&TXT:* EXTRACT=HREF
TAG POS=1 TYPE=BUTTON ATTR=TYPE:button&&ID:cboxNext
ONDOWNLOAD FOLDER=D:\Hasil<SP>Scrape\image\ FILE=* 
WAIT SECONDS=2

and the result is like this:
Code: Select allhttp://sample.com/img/656757657h576567.jpg
1. csv file in the form of data like this
2. JPG files.

This script has been worked.

The question that makes me confused.
1. how can I generate a csv file which only the file name only, without the full URL. example 656757657h576567.jpg
2. how that can change the file name 656757657h576567.jpg automatically according to the title of the article?

Anyway I use iMacros for Firefox
I use Firefox 42 OS Windows 7, iMacros Version 8.9.4

thank you for willing to help. I hope the day you're getting better.",https://forum.imacros.net/viewtopic.php?f=7&t=25329&sid=9d96e1268e5b272969ed509f53432633,content
2182,Page not opening,"Hello

I have macros working successfully with two different websites but am having a problem with a third.
When recording the website responds as expected, but when I run the macro it doesn't carry out a page opening.
Obviously it then fails on the next line.
I have removed the remainder of the macro below the fail line, and when this is run it says it has completed successfully, but the last event hasn't happened.
I'm using IE11 and the recording is:

VERSION BUILD=11.0.246.4051
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
URL GOTO=https://www.XXXXXXXXXXlogin.aspx
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:ctl00$ContentPlaceHolder1$txtEmailAddress CONTENT=XXXXXXXXX
SET !ENCRYPTION NO
TAG POS=1 TYPE=INPUT:PASSWORD ATTR=NAME:ctl00$ContentPlaceHolder1$txtpassword CONTENT=XXXXXXXXX
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:ctl00$ContentPlaceHolder1$btnLogin
TAG POS=1 TYPE=SPAN ATTR=TXT:Go<SP>To<SP>Premium
'New tab opened
TAB T=2
wait seconds=35
TAG POS=3 TYPE=DIV ATTR=TXT:81132633598

(The wait 35 seconds is because this page is slow to open.)

The last line doesn't open the page containing the customer information. During recording when the element is clicked it is highlighted green and the page opens, but during play it is outlined but not highlighted and nothing happens.
Right clicking on the page doesn't offer an Inspect Element option, but using the F12 pointer the element is highlighted as:

<div title=""81132633598"" style=""width: 62px; height: 13px; vertical-align: middle;"" unselectable=""on"">81132633598</div>
Unfortunately I can't post the website details.

Hope someone may be able to help.
TIA",https://forum.imacros.net/viewtopic.php?f=7&t=25288&sid=9d96e1268e5b272969ed509f53432633,content
2183,Eval(),"OS : Windows 7 Ultimate 64 bits
FF 41.0.2
Imacros

Good afternoon , is there any way I can extract part of the html ID and insert into another ?

Ex :
Code: Select allVERSION BUILD=8940826 RECORDER=FX
TAB T=1
URL GOTO=http://192.168.0.254/heaven/menu/menu.php
TAG POS=1 TYPE=A ATTR=ID:item_210
FRAME NAME=""menu_iframe""
TAG POS=1 TYPE=IMG ATTR=ID:id_img_pesq_top
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:F1 ATTR=ID:SC_idcliente CONTENT=520
TAG POS=1 TYPE=IMG ATTR=ID:id_img_sc_b_pesq_bot
TAG POS=1 TYPE=IMG ATTR=SRC:http://192.168.0.254/heaven/_lib/img/sys__NM__sys__NM__nm_primeiro_tec_bmd_edit.gif
TAG POS=1 TYPE=A ATTR=TXT:Financeiro*
FRAME NAME=""mnuClientes_iframe""
TAG POS=1 TYPE=SPAN ATTR=ID:id_sc_field_datavencimento_*&&TXT:*/11/2015 EXTRACT=TXT
SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; if(s = */11/2015){MacroError(\""TRUE\"");} else {MacroError(\""FALSE\"");};"")
SET !EXTRACT NULL
PROMPT {{!VAR1}}
Code: Select allTAG POS=1 TYPE=SPAN ATTR=ID:id_sc_field_datavencimento_*&&TXT:*/11/2015 EXTRACT=TXT

In that part of the code takes any text containing / 11/2014 in which id_sc_field_datavencimento_*

That is, I want to extract what is in the * add and elsewhere

EX :
Code: Select allvar x = EXTRACT value of *
var y = id_test_

if x = 1 or 2 or 3 or 4 then
y = y + x


Another question

I can use the * in eval ?

SET !VAR1 EVAL(
""var s=\""{{!EXTRACT}}\"";
if(s = */11/2015){MacroError(\""TRUE\"");}
else 
{MacroError(\""FALSE\"");};"")",https://forum.imacros.net/viewtopic.php?f=7&t=25265&sid=9d96e1268e5b272969ed509f53432633,content
2184,Automatically downloading vCard .vcf's,"I have a webpage with up to 150 vCard files. I'd like to download them automatically into iMacros folder. 

I've chased this around for hours, FAQ coverage of ONDOWNLOAD command seems sparse. 

I am a self-admitted total noob at macros and scripting. I'm sure this is something simple I am missing from my inexperience. Any assistance would be appreciated. Thanks 

Here's the variations I have tried.
Code: Select allVERSION BUILD=11.0.246.4051
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
URL GOTO=URL OF SITE
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>TABLE:nth-of-type(4)>TBODY>TR>TD>TABLE:nth-of-type(2)>TBODY>TR>TD>TABLE>TBODY>TR>TD>TABLE>TBODY>TR>TD>H4>SELECT"" BUTTON=0
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>TABLE:nth-of-type(4)>TBODY>TR>TD>TABLE:nth-of-type(2)>TBODY>TR>TD>TABLE>TBODY>TR>TD>TABLE>TBODY>TR>TD>H4>SELECT>OPTION:nth-of-type(11)"" BUTTON=0
EVENT TYPE=CLICK SELECTOR=""#as_keyword"" BUTTON=0
EVENTS TYPE=KEYPRESS SELECTOR=""#as_keyword"" CHARS=""__""
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>TABLE:nth-of-type(4)>TBODY>TR>TD>TABLE:nth-of-type(2)>TBODY>TR>TD>TABLE>TBODY>TR>TD>TABLE>TBODY>TR>TD>H4>INPUT:nth-of-type(2)"" BUTTON=0
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>TABLE:nth-of-type(4)>TBODY>TR>TD>TABLE:nth-of-type(2)>TBODY>TR>TD>TABLE>TBODY>TR:nth-of-type(6)>TD>TABLE>TBODY>TR:nth-of-type(2)>TD:nth-of-type(8)>H5>A:nth-of-type(2)"" BUTTON=0
'New tab opened
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>TABLE:nth-of-type(4)>TBODY>TR>TD>TABLE:nth-of-type(2)>TBODY>TR>TD>TABLE>TBODY>TR:nth-of-type(6)>TD>TABLE>TBODY>TR:nth-of-type(3)>TD:nth-of-type(8)>H5>A:nth-of-type(2)"" BUTTON=0
'New tab opened
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>TABLE:nth-of-type(4)>TBODY>TR>TD>TABLE:nth-of-type(2)>TBODY>TR>TD>TABLE>TBODY>TR:nth-of-type(6)>TD>TABLE>TBODY>TR:nth-of-type(4)>TD:nth-of-type(8)>H5>A:nth-of-type(2)"" BUTTON=0
'New tab opened
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>TABLE:nth-of-type(4)>TBODY>TR>TD>TABLE:nth-of-type(2)>TBODY>TR>TD>TABLE>TBODY>TR:nth-of-type(6)>TD>TABLE>TBODY>TR:nth-of-type(5)>TD:nth-of-type(8)>H5>A:nth-of-type(2)"" BUTTON=0
'New tab opened 
Code: Select allVERSION BUILD=11.0.246.4051
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
URL GOTO=WEB URL
TAG POS=1 TYPE=SELECT ATTR=TXT:Company<SP>Name<SP>City<SP>State<SP>Zip<SP>Code<SP>Country<SP>Phone<SP>Toll<SP>Free<SP>Phone<SP>Mobile<SP>First<SP>Nam*&&CLASS:input_form&&STYLE:z-index:<SP>20;&&NAME:as_field CONTENT=%email
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:as_keyword&&CLASS:input_form&&SIZE:15&&NAME:as_keyword CONTENT=__
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=ONCLICK:return<SP>checkSearchKeyword(document.search_form.as_keyword.value);&&CLASS:button_submit_grey&&TYPE:submit&&VALUE:Search
TAG POS=1 TYPE=A ATTR=TXT:vCard
ONDOWNLOAD FOLDER=* FILE=* WAIT=NO
TAG POS=2 TYPE=A ATTR=TXT:vCard
ONDOWNLOAD FOLDER=* FILE=* WAIT=NO
TAG POS=3 TYPE=A ATTR=TXT:vCard
ONDOWNLOAD FOLDER=* FILE=* WAIT=NO
TAG POS=4 TYPE=A ATTR=TXT:vCard
ONDOWNLOAD FOLDER=* FILE=* WAIT=NO
TAG POS=5 TYPE=A ATTR=TXT:vCard
ONDOWNLOAD FOLDER=* FILE=* WAIT=NO
TAG POS=6 TYPE=A ATTR=TXT:vCard
ONDOWNLOAD FOLDER=* FILE=* WAIT=NO
TAG POS=7 TYPE=A ATTR=TXT:vCard
ONDOWNLOAD FOLDER=* FILE=* WAIT=NO
TAG POS=8 TYPE=A ATTR=TXT:vCard
TAG POS=9 TYPE=A ATTR=TXT:vCard
TAG POS=10 TYPE=A ATTR=TXT:vCard
TAG POS=11 TYPE=A ATTR=TXT:vCard
TAG POS=12 TYPE=A ATTR=TXT:vCard
TAG POS=13 TYPE=A ATTR=TXT:vCard
TAG POS=14 TYPE=A ATTR=TXT:vCard",https://forum.imacros.net/viewtopic.php?f=7&t=25241&sid=9d96e1268e5b272969ed509f53432633,content
2185,I want to copy something from addressbar,"Hi guys 

I am very new on iMacro.I want to extract a URL only copy something from URL and paste it on another tab.Such as

My url look like  http://indianvisa-bangladesh.nic.in/vis ... %205%20min.

I just copy 6 digit 2C4DD6 which is called OTP from url and paset it another TAB

Please help me.


I have this command it copy from body but I want copy from Address bar

TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:OnlineForm ATTR=ID:btn2
TAG POS=4 TYPE=TD ATTR=* EXTRACT=TXT
SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s.split(' ')[9];"")
SET !VAR2 EVAL(""var parts = \""{{!VAR1}}\"".split(\"".\""); parts[parts.length-2];"")
SET !VAR2 EVAL(""if (\""{{!VAR2}}\"" == '#EANF#') ''; else \""{{!VAR2}}\"";"")
TAB T=3
SET !ENCRYPTION NO
TAG POS=1 TYPE=INPUT:PASSWORD FORM=NAME:OnlineForm ATTR=ID:otp CONTENT={{!VAR2}}
SET !EXTRACT NULL
TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:OnlineForm ATTR=ID:btn",https://forum.imacros.net/viewtopic.php?f=7&t=25238&sid=9d96e1268e5b272969ed509f53432633,content
2186,Removing all characters before first dash (-) in a string,"We have been inserting a user ID before our users email address and now need to remove these users IDs. I need to remove the all characters before the first dash (including the first dash) in this field and then paste it back with just the email.

They could be formatted like this: 1370546521-james-bond123@yahoo.com
Need the result to be: james-bond123@yahoo.com

I have iMacros 8.9.4, Firefox 40.0.3 and Windows 7 and have been using the following function to remove some leading XXXX's before a credit card number and thought I could build on it:

TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:aspnetForm ATTR=ID:ctl00_MainContent_ucCustomerProfile_tbEmail EXTRACT=TXT
SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s.replace(/[XXXX]+/g,'')"")
SET !EXTRACT NULL

I have been messing with the EVAL function and havent been able to get something working. I would really appreciate any help, thanks ahead of time!",https://forum.imacros.net/viewtopic.php?f=7&t=25217&sid=9d96e1268e5b272969ed509f53432633,content
2188,data extraction php generated,"i am trying to extract from a textarea. the contents of the text area does not show in the page source. inspect element shows only 1 line of code:
<textarea class=""formCode""></textarea>
so i suppose the contents inside the textarea is dynamically generated by php.

my iMacro looks like this:
TAG POS={{!LOOP}} TYPE=TEXTAREA ATTR=CLASS:formCode&&TXT:* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\myfolder FILE={{!LOOP}}.txt

during the run, i do see the textarea (form) is selected/outlined.

the saved file contains just quotes. so nothing is extracted? how do i get the contents in the textarea?",https://forum.imacros.net/viewtopic.php?f=7&t=25180&sid=7fd87bd5f8edabeec090c4a1f0d5b352,content
2189,Get the coordinates (XY) of an element,"Hello! I am trying to learn the position of the element on the page using JS: Code: Select allURL GOTO=JavaScript:links=window.content.document.links;pos=links[0].getBoundingClientRect();x=Math.floor(pos.left);y=Math.floor(pos.top);


value is calculated, but how do I pass this value to the PHP-script? or is there some iMacros-command (without using js)?",https://forum.imacros.net/viewtopic.php?f=7&t=25138&sid=7fd87bd5f8edabeec090c4a1f0d5b352,content
2190,Replay Losing Content,"I recording the following script but when I replay it the content of the text box (e.g. 438-362-13-00) disappears as soon as the tag command that fills in the content executes.   So on replay the web site displays an error instead of searching for the requested parcel number.    Any idea what is happening and what I might be able to do to make it work?


VERSION BUILD=10.3.27.5830
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=https://arcc.sdcounty.ca.gov/Pages/Asse ... l-Tax.aspx
TAG POS=1 TYPE=SELECT FORM=NAME:aspnetForm ATTR=NAME:ctl00$m$g_d30f33ca_a5a7_4f69_bb21_cd4abc25ea12$ctl00$ddlSearch CONTENT=%1
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:aspnetForm ATTR=NAME:ctl00$m$g_d30f33ca_a5a7_4f69_bb21_cd4abc25ea12$ctl00$txtSearch CONTENT=438-362-13-00
TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:aspnetForm ATTR=NAME:ctl00$m$g_d30f33ca_a5a7_4f69_bb21_cd4abc25ea12$ctl00$btnSearch",https://forum.imacros.net/viewtopic.php?f=7&t=25060&sid=7fd87bd5f8edabeec090c4a1f0d5b352,content
2191,SOLVED: Decode URL entities on extraction,"Hi
My iMacro takes URLs from a csv file, loads them into browser one by one, searchs a HTML element with defined CSS class and, on finding, extracts the content of the HTML element as HTML source code. Then it saves the extracted code together with the URL, where the finding originates from, into created csv file.

The problem is, that if i open extraction csv file in Excel or code editor, all extracted URLs are with encoded entities, like http_s://www.google.de/search?q=[b]g%C3%83%C2%BC ... bernachten[/b] . But i need to have decoded entities in the extracted URLs. Beside of this, it seems, like the csv file, created by iMacros, wouldn't be in UTF-8: i guess so, because after i decoded extracted URLs in the csv file with a VBA script directly in Excel, some entities, like space, were correctly decoded, but entities like german characters 眉枚盲, are broken like http_s://www.google.de/search?q=g脙茠脗录nstig 脙茠脗录bernachten

I was thinking about something like
Code: Select allSET !VAR2 decodeURIComponent ({{!URLCURRENT}}) 
ADD !EXTRACT {{!VAR2}}
or
Code: Select allSET !VAR2 unescape ({{!URLCURRENT}}) 
ADD !EXTRACT {{!VAR2}}
but this with some certainty don't work.
Would somebody point me to (quick and dirty) solution? 
thanks!

SOLVED: 
Code: Select allSET !EXTRACT {{!URLCURRENT}}
SET !EXTRACT EVAL(""decodeURI('{{!EXTRACT}}');"")
SAVEAS TYPE=EXTRACT FOLDER=* FILE=+{{!NOW:ddmmyyyy}}.csv
did the trick:)

PS. the whole iMacros code with solved URL decoding is:
Code: Select allVERSION BUILD=8881205 RECORDER=FX
SET !TIMEOUT_STEP 0
SET !ERRORIGNORE YES
TAB T=1
SET !DATASOURCE urls.csv 
SET !DATASOURCE_COLUMNS 1
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}
SET !VAR1 EVAL(""var randomNumber=Math.floor(Math.random()*30 + 15); randomNumber;"")
URL GOTO={{!COL1}}
TAG POS=1 TYPE=DIV ATTR=CLASS:_gT&&TXT:* EXTRACT=HTM      
ADD !EXTRACT {{!URLCURRENT}}
SET !EXTRACT EVAL(""decodeURI('{{!EXTRACT}}');"")
SAVEAS TYPE=EXTRACT FOLDER=* FILE=+{{!NOW:ddmmyyyy}}.csv
WAIT SECONDS={{!VAR1}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=+{{!NOW:ddmmyyyy}}.csv[/code]",https://forum.imacros.net/viewtopic.php?f=7&t=25042&sid=7fd87bd5f8edabeec090c4a1f0d5b352,content
2192,How to extract & save all text with similar html tags,"How can I extract all text associated with similar html tags? Because currently it enters POS=1 and only one text is saved. If you check/run the below script you will understand what I mean.

I know that by adding POS=2, POS=3 and so on I can extract the complete data, but that will become very tedious since I have too many zip codes to input & every zip code have different number of results.

Thanks in advance for your help!
Code: Select allVERSION BUILD=10022823
SET !EXTRACT_TEST_POPUP NO
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=http://www.promotionalproductswork.org/promotional-consultant-locator.aspx
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:aspnetForm ATTR=NAME:ctl00$PlaceHolderMain$locatetext CONTENT=89123
TAG POS=1 TYPE=SELECT FORM=NAME:aspnetForm ATTR=NAME:ctl00$PlaceHolderMain$locatewithin CONTENT=%100
TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:aspnetForm ATTR=NAME:ctl00$PlaceHolderMain$locatesubmit
TAG POS=1 TYPE=SPAN ATTR=CLASS:street-address EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=CLASS:postal-code EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=CLASS:region EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=CLASS:locality EXTRACT=TXT
TAG POS=1 TYPE=A ATTR=CLASS:email EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=CLASS:tel EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=testing.CSV",https://forum.imacros.net/viewtopic.php?f=7&t=24900&sid=7fd87bd5f8edabeec090c4a1f0d5b352,content
2193,<p> content extract has #EANF#,"Hello,

I want to extract content of the one <p> tag. On the page are two P tags as follows:
Code: Select all<p class=""yellow"">Duration: 18:53</p>
<p class=""yellow"">Published: 08/01/2015</p>
I wanna extract text Published: 08/01/2015. My script: Code: Select allTAG POS=2 TYPE=p ATTR=class:yellow EXTRACT=txt
PROMPT {{!EXTRACT}} or Code: Select allTAG POS=1 TYPE=p ATTR=TXT:Published* EXTRACT=txt
PROMPT {{!EXTRACT}} either don't work - promt has #EANF#. But in processing it looks like the right <p> element has been chosen, because text is in some green border:



So do I do something wrong? It's really weird 
Thank you so much for replies!",https://forum.imacros.net/viewtopic.php?f=7&t=24907&sid=7fd87bd5f8edabeec090c4a1f0d5b352,content
2194,Populating error if element is visible on page,"Hi Everyone,

I'm not very tech savvy, but I'm trying to create a macro to run on a loop. I'm looking for a specific element, and if that element appears on the page, I'd like to generate a popup error.

Upon inspecting the targeted element on the page, this is what I found:

<div ng-switch-when=""true"" ng-switch=""item.computedStatus"" class=""ng-scope"">
<!-- ngSwitchWhen: LIVE -->
<!-- ngSwitchWhen: PAUSED -->
<!-- ngSwitchWhen: COMPLETED -->
<!-- ngSwitchWhen: UPCOMING -->
<i class=""fa fa-pause ng-scope"" ng-switch-when=""PAUSED"" bs-popover=""{content: ('shared.common.paused' | i18n) + ' ' }"" data-trigger=""hover"" data-placement=""top"" data-original-title="""" title="""">
</i></div>

I'd like to populate an error box anytime item.computedStatus = PAUSED. 

Item.computedStatus is an element that could show up in multiple locations on the page.

(Sorry for reposting, but I figured this would garner more traffic here)",https://forum.imacros.net/viewtopic.php?f=7&t=24680&sid=7fd87bd5f8edabeec090c4a1f0d5b352,content
2195,Download image on page from a csv cannot find Element,"Trying to use a data source to go to a page. Download the image. Save it and repeat the function.  I have put this together but each time I get the error Error -1300: Cannot find HTML element of type ""IMG:"" with attribute(s) ""HREF:http://*.jpg"".. Line 13: TAG POS=1 TYPE=IMG ATTR=HREF:http://*.jpg CONTENT=EVENT:SAVEITE
What am i missing?  This the script.
VERSION BUILD=10.4.28.1074
TAB T=1
TAB CLOSEALLOTHERS

SET !DATASOURCE DATASC1.csv
SET !DATASOURCE_COLUMNS 1

URL GOTO=http://www.pactiv.com/products/containe ... id/{{!COL1}}

ONDOWNLOAD FOLDER=* FILE=+_image_{{!NOW:yyyymmdd_hhnnss}}  

TAG POS=1 TYPE=IMG ATTR=HREF:http://*.jpg CONTENT=EVENT:SAVEITEM",https://forum.imacros.net/viewtopic.php?f=7&t=24668&sid=7fd87bd5f8edabeec090c4a1f0d5b352,content
2196,"[solved] Dropdown menu does not work in ""TYPE=LI"" element","Hello
I recorded a macro in firefox (also tried with imacro browser) and the code for dropdown menu looks like this
Code: Select allTAG POS=1 TYPE=LI ATTR=ID:mainCategorySelect_chzn_o_1
TAG POS=1 TYPE=LI ATTR=ID:mainCategorySelect_chzn_o_2
TAG POS=1 TYPE=LI ATTR=ID:mainCategorySelect_chzn_o_3


There is no CONTENT tag. As a result when I play the macro, it does not select anything from the dropdown menu.

Normally when selecting an option from dropdown menu, the recorded macro look somewhat like this
Code: Select allTAG POS=1 TYPE=SELECT FORM=NAME:prod ATTR=NAME:variantID_4 CONTENT=%234


I am using firefox 38.0.5, windows 7 32 bit and imacros addon 8.9.2.1",https://forum.imacros.net/viewtopic.php?f=7&t=24661&sid=7fd87bd5f8edabeec090c4a1f0d5b352,content
2198,Extracting from a Result Form,"I am running the code on Mozilla Firefox, Windows 8.1. 

This is what I have right now:
Code: Select allVERSION BUILD=8920312 RECORDER=FX
set !var1 100000
add !var1 {{!loop}}
TAB T=1
URL GOTO=https://www.us.mensa.org/AML/?LinkServID=E29B963B-116B-4345-8777B6A0DB84D8E6
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:frmRenew ATTR=NAME:txtMemNum CONTENT=100{{!var1}}
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:frmRenew ATTR=*
TAG POS=1 TYPE=P ATTR=CLASS:bdytxt&&TXT:* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=+_{{!NOW:yyyymmdd_hhnnss}}


When I run this, the extracted files just show #ENAF#. What am I doing wrong? Basically i want to extract the names, if any for the membership numbers. Also it would be great if the data can be extracted to one single file. 

Thanks!",https://forum.imacros.net/viewtopic.php?f=7&t=24600&sid=030d300656bab0b18f7fde81bb00e1ce,content
2199,Data Extraction and TAG value form a csv file,"Hello, in advance sorry for my bad english..

I'd like to extract particulat txt from different url registered in a csv file datasource (i verifie if an article is available or not available..)

My script is working fine : 
VERSION BUILD=8920312 RECORDER=FX
TAB T=1
SET !DATASOURCE source.csv
SET !LOOP 1
URL GOTO={{!col1}}
SET !TIMEOUT_STEP 1
TAG POS=1 TYPE=* ATTR=TXT:available EXTRACT=TXT
ADD !EXTRACT {{!URLCURRENT}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=*

The loop take url value from my datasource source.csv and record txt value (available) in output csv file.

I'd like to specifie the 'texttoextract' ('available' or 'notavailable' or 'size36'...form the source.csv in {{col2}} because this value can be diff茅rent in some urls.

Exemple of my input csv file
http://www.test.com/page1,available
http://www.test.com/page2,notavailable
http://www.test.com/page3,notavailable
http://www.test.com/page4,size36
http://www.test.com/page4,size38
http://www.test.com/page4,size40

I try this code 
TAG POS=1 TYPE=* ATTR=TXT CONTENT={{col2}} EXTRACT=TXT and other version but that's doesn't work  (i'm a debutant !)

Can you help me please

Regards

Oliver",https://forum.imacros.net/viewtopic.php?f=7&t=24594&sid=030d300656bab0b18f7fde81bb00e1ce,content
2200,Windows Server Download Image Does Not Work,"I am trying to download an image file.  The following code works on Windows 8.1 BUT it does not work on Windows Server 2008 R2

macro = macro + ""URL GOTO=https://www.XXXXXXXX.com"" + vbNewLine
macro = macro + ""TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:emailSyndicationVerificationForm ATTR=NAME:email CONTENT="" + Email + vbNewLine
macro = macro + ""TAG POS=1 TYPE=IMG FORM=NAME:emailSyndicationVerificationForm ATTR=SRC:https://www.XXXXX.com/a/captcha?ct=*"" + vbNewLine
macro = macro + ""ONDOWNLOAD FOLDER=* FILE="" + CaptchaImageName + "".jpg "" + vbNewLine
macro = macro + ""'Download the picture"" + vbNewLine
macro = macro + ""TAG POS=1 TYPE=IMG ATTR=HREF:https://www.XXXXX.com/a/captcha?ct=* CONTENT=EVENT:SAVEITEM"" + vbNewLine

What is wrong?",https://forum.imacros.net/viewtopic.php?f=7&t=24584&sid=030d300656bab0b18f7fde81bb00e1ce,content
2201,Captcha if help,"Hi
Please edit this IIM code to JS
Code: Select allVERSION BUILD=10.4.28.1074
TAB T=1
ONDOWNLOAD FOLDER=D:\captcha FILE=image.jpg
IF {TAG POS=1 TYPE=IMG ATTR=HREF:""http://site.com/captcha.aspx"" CONTENT=EVENT:SAVEITEM} exists
Then
TAB OPEN
TAB T=2
URL GOTO=http://api.dbcapi.me/decaptcher?function=picture2&print_format=html
TAG POS=1 TYPE=INPUT ATTR=NAME:username CONTENT=myuser
TAG POS=1 TYPE=INPUT ATTR=NAME:password CONTENT=mypass
TAG POS=1 TYPE=INPUT ATTR=NAME:pict CONTENT=D:\captcha\image.jpg
TAG POS=1 TYPE=INPUT ATTR=TYPE:submit
TAG POS=6 TYPE=TD ATTR=* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
TAB CLOSE
TAB T=1
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:newusersignup ATTR=ID:recaptcha_response_field CONTENT={{!VAR1}}
SAVEAS TYPE=CPL FOLDER=* FILE=*
else
SAVEAS TYPE=CPL FOLDER=* FILE=*
Thank you",https://forum.imacros.net/viewtopic.php?f=7&t=24423&sid=030d300656bab0b18f7fde81bb00e1ce,content
2202,Extracting Text from TXT File,"Before I discuss the issue I'm having, here is a description of my current set-up.

iMacros Version - VERSION BUILD=9002379
OS - Windows 8.1 (English)
Browser - Firefox 37.0.2 / iMacros Web Browser

The demo's work okay, I'm just not sure how to do the thing I'm trying to do. So what I would like to do is extract a line of text from a TXT File located in my documents folder, and paste the info into a box located on a webpage. (if you don't know what I mean by ""box"" I mean any area you enter text into, as in the title of a thread for example)

I did have a code that used to do it for me but removed that file from iMacros. Before, the iMacros code looked something like the following :
Code: Select allSET !DATASOURCE C:\Users\RandomText\RandomText\RandomText.txt
SET !DATASOURCE_COLUMNS 1
SET !DATASOURCE_LINE {{!LOOP}}
SET !EXTRACT_TEST_POPUP NO

Then to get it to paste the line of text I would use this :
Code: Select allCONTENT={{!COL1}}

So how would I specify a location to get the data from? I've searched over the Internet but cannot seem to find a resolution for this problem. Any help is appreciated. ",https://forum.imacros.net/viewtopic.php?f=7&t=24331&sid=030d300656bab0b18f7fde81bb00e1ce,content
2205,select the last item in a dropdown menu,"I have a macro for selecting a few drop down menus and then click on a list.

the data in dropdowns change. I want to select the last item in a list. how can I do that?

this is my macro: Code: Select allVERSION BUILD=8881205 RECORDER=FX
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=http://evento.ir/!/leantehran
CLEAR
TAG POS=1 TYPE=SELECT FORM=ACTION:/events/leantehran/tickets/registration ATTR=ID:ShowTimes_0__Varients_0__SelectedQuantity CONTENT=%9
TAG POS=1 TYPE=SELECT FORM=ACTION:/events/leantehran/tickets/registration ATTR=ID:ShowTimes_0__Varients_1__SelectedQuantity CONTENT=%20
TAG POS=1 TYPE=SELECT FORM=ACTION:/events/leantehran/tickets/registration ATTR=ID:ShowTimes_0__Varients_2__SelectedQuantity CONTENT=%20
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:/events/leantehran/tickets/registration ATTR=ID:buy
WAIT SECONDS=300


I'm using firefox 31 on OSX 10.10",https://forum.imacros.net/viewtopic.php?f=7&t=23841&sid=030d300656bab0b18f7fde81bb00e1ce,content
2206,Saving TYPE=EXTRACT as CSV...with Commas?,"Hello All,

I'm new to iMacros and the forum. Let me start off by saying that this is a GREAT product and I'm glad I found out about it. I am currently working on a large data compilation project that involves scraping about 1,175 individual search queries from a particular website. Needless to say, the software is saving me a lot of time.

I've read the documentation on loading data into webforms and extracting text from the output; very straightforward. I would like, however, to further explore how manipulating and saving webpage text via `EXTRACT` and `SAVEAS` could vary. I have one particular question in mind. Here is the macro I have conjured up so far:
Code: Select allVERSION BUILD=10.3.27.5830
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=http://www.adviserinfo.sec.gov/IAPD/Content/Search/iapd_Search.aspx
SET !DATASOURCE ""C:\\legalnames.csv""
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}
TAG POS=1 TYPE=INPUT:RADIO FORM=ACTION:iapd_Search.aspx ATTR=ONCLICK:javascript:setTimeout('__doPostBack(\'ctl00$cphMainContent$ucUnifiedSearch$rdoOrg\',\'\')',<SP>0)&&ID:ctl00_cphMainContent_ucUnifiedSearch_rdoOrg&&TYPE:radio&&NAME:ctl00$cphMainContent$ucUnifiedSearch$rdoSearchBy&&VALUE:rdoOrg CONTENT=YES
TAG POS=1 TYPE=SPAN FORM=ACTION:iapd_Search.aspx ATTR=TXT:Firm<SP>Name<SP>or<SP>CRD#<SP>or<SP>SEC#:&&ID:ctl00_cphMainContent_ucUnifiedSearch_lblFirm
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:iapd_Search.aspx ATTR=ARIA-HASPOPUP:true&&ROLE:textbox&&ONKEYPRESS:return<SP>AutoSubmitOnTextBox('ctl00_cphMainContent_ucUnifiedSearch_btnFreeFormSearch',event);&&ID:ctl00_cphMainContent_ucUnifiedSearch_txtFirm&&CLASS:SectionData<SP>ui-autocomplete-input&&ARIA-AUTOCOMPLETE:list&&STYLE:width:<SP>240px;&&NAME:ctl00$cphMainContent$ucUnifiedSearch$txtFirm&&AUTOCOMPLETE:off CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:iapd_Search.aspx ATTR=ONCLICK:javascript:WebForm_DoPostBackWithOptions(new<SP>WebForm_PostBackOptions(""ctl00$cphMainContent$ucUnifiedSearch$btnFreeFormSearch"",<SP>"""",<SP>true,<SP>"""",<SP>"""",<SP>false,<SP>false))&&ID:ctl00_cphMainContent_ucUnifiedSearch_btnFreeFormSearch&&CLASS:pagerCurrentPageButton&&TYPE:submit&&NAME:ctl00$cphMainContent$ucUnifiedSearch$btnFreeFormSearch&&VALUE:Start<SP>Search
TAG POS=2 TYPE=B ATTR=* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=*

Sorry if it's messy. Here's a link to the macro: https://www.dropbox.com/s/stsltfvph2zr4bn/IAPD.iim?dl=0 and the dataset I am plugging into the webform:https://www.dropbox.com/s/mmu6y13d3or9n ... s.csv?dl=0.

Basically, the extract .csv file is unable to preserve punctuation (commas) in an investment firm's name because the file is, of course, comma-separated. Here https://www.dropbox.com/s/h9ro8igxcxnfy ... t.csv?dl=0 is a link to the extract.

I need to merge this database with another and it is really really imperative that I keep the output identical to what comes out on the webpage, including punctuation, spaces, capitalization, acute accents...etc.

Has anyone been able to do this? Would setting EXTRACT to some other type of formatting potentially preserve punctuation?

Any and all advice would be greatly greatly appreciated. Thank you!",https://forum.imacros.net/viewtopic.php?f=7&t=23977&sid=030d300656bab0b18f7fde81bb00e1ce,content
2207,extral data been extract from table,"Here is the html page looks like

Here has a javascript defined in html page header

function displayicd(code,contname)
{
	if (code == ""0"")
	document.getElementById(contname).innerHTML=""ICD-10"";
	else if (code == ""9"")
	document.getElementById(contname).innerHTML=""ICD-9"";
	else
	document.getElementById(contname).innerHTML="""";
}

The table in html content looks like 

<table border=""0"" width=""90%"" class=""formbkgrnd"">
<tr align=""left"" >
  <td width=""7%"" valign=""bottom"" class=""content"">  ICD-CM Type</td>
  <td width=""5%"" valign=""bottom"" class=""content""> ICD Code</td>
  <td width=""28%"" valign=""bottom"" class=""content""> Diagnosis Description</td>
  <td width=""10%"" valign=""bottom"" class=""content""> Date of Onset</td>
  </tr>
<tr>

  <td width=""5%"" bgcolor=""white"" class=""subtitle""><p bgcolor=""white"" class=""subtitle"" id=""1""></p> <script>displayicd(""9"",""1"")</script></td>
  <td width=""5%"" bgcolor=""white"" class=""subtitle"">V30.0</td>
  <td width=""28%"" bgcolor=""white"" class=""subtitle"">SINGLE LIVEBORN-IN HOSP</td>
  <td width=""10%"" bgcolor=""white"" class=""subtitle""></td>
  </tr>
</table>

Here is my imacro script to get everything from table because the content may be changed in different condition. In this case, I got the javasctip function displayicd(""9"",""1"") and javasctipt function returned value ICD-9 in my extract result.   My question is how can get away the javascript function name in my return result? 

' POS
TAG POS=R1 TYPE=TABLE ATTR=* EXTRACT=TXT",https://forum.imacros.net/viewtopic.php?f=7&t=23960&sid=030d300656bab0b18f7fde81bb00e1ce,content
2208,Selecting a value in a text enable drop down value,"Dear iMacro Gurus,

I am new to this forum and sorry if I am posting my question in a wrong section. But I really need your help. I have a courier service form where I have to fill data of a shipper and receiver which could be a change value every time. I create a data csv and start filling up form via macro, every text field got right data but as soon as I got some drop down menu it goes stuck and shows error. its a text enable drop down menu, where you can write and select value on show. to get my task done I tried different available solution of imacro but didn't get success. Could any of you can help me with this code. 
Code: Select allSET !ERRORIGNORE YES
SET !ExTRACT_TEST_POPUP NO
SET !DATASOURCE leopost.csv
SET !DATASOURCE_COLUMNS 10
SET !DATASOURCE_LINE {{!LOOP}}
URL GOTO=http://websitedomainremoved.com/booked_packet/add
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:booked_packet_date CONTENT={{!COL1}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:booked_packet_no_piece CONTENT={{!COL2}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:booked_packet_weight CONTENT={{!COL3}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:booked_packet_collect_amount CONTENT={{!COL4}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:booked_packet_order_id CONTENT={{!COL5}}
TAG POS=9 TYPE=INPUT:TEXT FORM=ID:shipment_id ATTR=ID:* CONTENT=$#ADNAN
WAIT SECONDS=10
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:shipment_name_eng CONTENT={{!COL7}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:shipment_email CONTENT={{!COL8}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:shipment_phone CONTENT={{!COL9}}
TAG POS=1 TYPE=TEXTAREA FORM=ID:addForm ATTR=ID:shipment_address CONTENT={{!COL10}}
TAG POS=13 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=* CONTENT={{!COL11}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:consignment_name_eng CONTENT={{!COL12}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:consignment_email CONTENT={{!COL13}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:consignment_phone CONTENT={{!COL14}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:consignment_phone_two CONTENT={{!COL15}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=ID:consignment_phone_three CONTENT={{!COL16}}
TAG POS=1 TYPE=TEXTAREA FORM=ID:addForm ATTR=ID:consignment_address CONTENT={{!COL17}}
TAG POS=19 TYPE=INPUT:TEXT FORM=ID:addForm ATTR=* CONTENT={{!COL18}}
TAG POS=1 TYPE=TEXTAREA FORM=ID:addForm ATTR=ID:booked_packet_comments CONTENT={{!COL19}}
TAG POS=1 TYPE=INPUT:BUTTON FORM=ID:addForm ATTR=ID:bookPacketPrint
WAIT SECONDS=10
ADD !EXTRACT {{!URLCURRENT}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=bills.csv ",https://forum.imacros.net/viewtopic.php?f=7&t=23791&sid=030d300656bab0b18f7fde81bb00e1ce,content
2210,JS variable into iMacros code,"Hi all

I don't know if it's possible
I will give an example
Code: Select allvar macro;
var ret;
var retcode;
var a; 

ret = iimPlay(""iimLastExtract/extract_one"")
a = iimGetLastExtract(1)

macro = ""CODE:"";
macro += ""TAG POS=1 TYPE=TEXTAREA FORM=ID:xxx ATTR=ID:xxx CONTENT={{a}}"" +  ""\n"";
retcode = iimPlay(macro);



I can not put into ""CONTENT"" predefined variable ""a""
is it possible this way ?

Using Windows xp sp3, Firefox 30 and iMacros for Firefox 8.8.2",https://forum.imacros.net/viewtopic.php?f=7&t=23668&sid=b07178a8376ad918bdc99d345a2f0b97,content
2212,how to let csv columns to INPUT,"Code: Select allTAG POS=1 TYPE=INPUT:TEXT ATTR=ID:nameIpt CONTENT=a323256333445
TAG POS=1 TYPE=INPUT:PASSWORD ATTR=ID:mainPwdIpt CONTENT=a123456


my a file  name.csv, it has two columns,  name and password

how to write the code, let the date name to  CONTENT=a323256333445

and password to CONTENT=a123456",https://forum.imacros.net/viewtopic.php?f=7&t=23663&sid=b07178a8376ad918bdc99d345a2f0b97,content
2213,"just registered information, how to be deleted","Code: Select allCMDLINE !DATASOURCE liuuuu.csv
SET !DATASOURCE_COLUMNS 2
SET !LOOP 3
SET !DATASOURCE_LINE {{!LOOP}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:nameIpt CONTENT={{!COL1}}
SET !ENCRYPTION NO
TAG POS=1 TYPE=INPUT:PASSWORD ATTR=ID:mainPwdIpt CONTENT={{!COL2}}
TAG POS=1 TYPE=INPUT:PASSWORD ATTR=ID:mainCfmPwdIpt CONTENT={{!COL2}}

I want to register lots of account, the info came from csv file,

In order to ban repeating infos registration, I want to delete the used infos.

In other words, just registered information will be deleted.   how to modify the codes????",https://forum.imacros.net/viewtopic.php?f=7&t=23664&sid=b07178a8376ad918bdc99d345a2f0b97,content
2214,Is timestamp possible against the data element extracted?,"OS: Windows 8.1 Pro
imacros Version : iMacros Sidebar for Internet Explorer (x86) Version 10.2.26.4235

Is there a way of getting timestamp against each row of data that is getting extracted.

My current code:

VERSION BUILD=10.2.26.4235
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=http://www.nseindia.com/live_market/dyn ... .htm?cat=O
WAIT SECONDS=5
SET !EXTRACT_TEST_POPUP NO
TAG POS=81 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=51 TYPE=TD ATTR=CLASS:number EXTRACT=TXT
TAG POS=52 TYPE=TD ATTR=CLASS:number EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=D:\ FILE=MAS_Volume_{{!NOW:ddmmyy_hhnn}}.csv
TAG POS=89 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=56 TYPE=TD ATTR=CLASS:number EXTRACT=TXT
TAG POS=57 TYPE=TD ATTR=CLASS:number EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=D:\ FILE=MAS_Volume_{{!NOW:ddmmyy_hhnn}}.csv
TAG POS=97 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=61 TYPE=TD ATTR=CLASS:number EXTRACT=TXT
TAG POS=62 TYPE=TD ATTR=CLASS:number EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=D:\ FILE=MAS_Volume_{{!NOW:ddmmyy_hhnn}}.csv


This is giving me a csv file as
abc, 71.05, 3.00%
def, 132.50, 1.50%
ghi, 24.35, -0.96%

the file name is having the timestamp.

What I want to know is there a possibility of getting the csv file contents to be as follows:
271014_1052, abc, 71.05, 3.00%
271014_1052, def, 132.50, 1.50%
271014_1052, ghi, 24.35, -0.96%

any help appreciated.",https://forum.imacros.net/viewtopic.php?f=7&t=23541&sid=b07178a8376ad918bdc99d345a2f0b97,content
2215,Extracting from table not working correctly,"Hi,

I am using IE 8.0.6001.18702 on Windows XP SP 3, and iMacros for IE8 Version 10.2.26.4235

I am trying to extract the table data into a csv file as per below code

VERSION BUILD=10.2.26.4235
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=http://www.nseindia.com/live_market/dyn ... .htm?cat=O
TAG POS=1 TYPE=DIV ATTR=ID:tab8Content EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=D:\ FILE=mytable_{{!NOW:yymmdd_hhnnss}}.csv

Unfortunately, the output is not coming with the columns separated correctly --- especially since some of the column values also has a comma in it.

Have tried various combinations including 
TAG POS=1 TYPE=TABLE ATTR=TXT:* EXTRACT=TXT
which is giving correct column breakup but no values !!


Any help is appreciated. Thanks.",https://forum.imacros.net/viewtopic.php?f=7&t=23537&sid=b07178a8376ad918bdc99d345a2f0b97,content
2216,Loop through dynamically created pages (bttn click + scrape),"Dear all,

I have been recently started using iMacros for scraping webpages dynamically created.

The following code works for the first table I am interested in. What it does is to:
(1) click two buttons;
(2) extract the table
Code: Select all
VERSION BUILD=10022823
TAB T=1
URL GOTO=http://admitere.edu.ro/2010/staticRepI/j/
SIZE X=872 Y=527
WAIT SECONDS=0.5

'Click ""button 1""
DS CMD=CLICK X=223 Y=314 CONTENT=
WAIT SECONDS=0.5
'Click ""button 2""
DS CMD=CLICK X=421 Y=259 CONTENT=
WAIT SECONDS=0

'Scrape table 1
FRAME NAME=judete
TAG POS=1 TYPE=TABLE ATTR=CLASS:*mainTable* EXTRACT=TXT
WAIT SECONDS=0.5

'Save as csv
SAVEAS TYPE=EXTRACT FOLDER=* FILE=table_1.csv



So far so good. Now what I would like to do is to click a third button (""button 3"") that would - again dynamically - create the next table, and scrape it. 
To this aim, I tried to add the following lines of code to the previous ones:
Code: Select all'Click ""button 3""
DS CMD=CLICK X=312 Y=172 CONTENT=
WAIT SECONDS=3

'Scrape table 2
FRAME NAME=judete
TAG POS=1 TYPE=TABLE ATTR=CLASS:*mainTable* EXTRACT=TXT
WAIT SECONDS=0.5

But unfortunately the first table is not updated as button 3 is not clicked (i.e. I scrape, again, the first table).
The strange thing is that if I repeat all the above passages without extracting the first table, then ""button 3"" is correctly clicked and the second table is scraped as expected:
Code: Select allVERSION BUILD=10022823
TAB T=1
URL GOTO=http://admitere.edu.ro/2010/staticRepI/j/
SIZE X=872 Y=527
WAIT SECONDS=0.5

'Click ""button 1""
DS CMD=CLICK X=223 Y=314 CONTENT=
WAIT SECONDS=0.5
'Click ""button 2""
DS CMD=CLICK X=421 Y=259 CONTENT=
WAIT SECONDS=0
'Click ""button 3""
DS CMD=CLICK X=312 Y=172 CONTENT=
WAIT SECONDS=3

'Scrape table 2
FRAME NAME=judete
TAG POS=1 TYPE=TABLE ATTR=CLASS:*mainTable* EXTRACT=TXT
WAIT SECONDS=0.5

'Save as csv
SAVEAS TYPE=EXTRACT FOLDER=* FILE=table_1.csv


In other words, the previous code clicks the three buttons and scrapes the second table, but if I try to scrape also the first one (before clicking button 3) then button 3 is not clicked so that table 2 is not scraped.

Could anyone explain to me the reason of what's happening? I am clearly missing some basic crucial step here.. 
Maybe the reason is that I cannot extract more than element at time? (meaning that I have to embed the code in a, e.g., VB loop script)

My final goal is to scrape the first, say, 10 tables.
That is, ideally I would like to keep clicking button 3 and scraping tables by implementing a loop.
Thank you very much for any suggestions.",https://forum.imacros.net/viewtopic.php?f=7&t=23538&sid=b07178a8376ad918bdc99d345a2f0b97,content
2218,Extracting URLS from google search,"Hello,
I am hoping to extract urls based on a google searches of inputted data. My code is working, but too well. I'm getting a lot more data than just what's in the element. 
Could anyone help me extract /just/ the url from the google search? Here's my code:

VERSION BUILD=10.1.25.8883
TAB T=1
TAB CLOSEALLOTHERS
SET !DATASOURCE accounts.csv
SET !LOOP 2
SET !DATASOURCE_LINE {{!LOOP}}

TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:gbqf ATTR=NAME:q CONTENT={{!COL1}}
TAG POS=1 TYPE=BUTTON:SUBMIT FORM=NAME:gbqf ATTR=NAME:btnG
TAG POS=1 TYPE=CITE ATTR=CLASS:_Rm EXTRACT=HREF

SAVEAS TYPE=TXT FOLDER=C:\Users\Jessi FILE=webdata.csv


Thanks in advance!",https://forum.imacros.net/viewtopic.php?f=7&t=23472&sid=b07178a8376ad918bdc99d345a2f0b97,content
2219,Tag command doesn't run,"Tag command what I want to use include space such as: NEW BRUNSWICK

Help me. Code is below

VERSION BUILD=10002738
TAB T=1
TAB CLOSEALLOTHERS
SET !DATASOURCE LIST.csv
SET !DATASOURCE_LINE {{!LOOP}}
URL GOTO=http://pse5-esd5.ainc-inac.gc.ca/fnp/Ma ... x?lang=eng
TAG POS=1 TYPE=SELECT ATTR=NAME:ctl00$ddlRegion CONTENT=$AB

TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:ctl00$btnSearch&&VALUE:Search

TAG POS=1 TYPE=TABLE ATTR=ID:ctl00_dgFNlist&&TXT:* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE={{!COL3}}_{{!NOW:yymmdd}}.csv
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:ctl00$btnNext&&TITLE:Next
SET !ERRORIGNORE YES",https://forum.imacros.net/viewtopic.php?f=7&t=23463&sid=b07178a8376ad918bdc99d345a2f0b97,content
2220,line 2 wrong size of SET command (Error code:: SyntaxError -,"hello, 
I started on imacros for firefox, and I try a macro I just how.This macro does not work and returns the error suvant message: line 2 wrong size of SET command (Error code:: SyntaxError -910 ). 

I put below the text of the macro. 
VERSION BUILD = 8820413 RECORDER = FX 
SET! DATASOURCE Without titre.csv 
SET! DATASOURCE_COLUMNS 9 
SET! LOOP 1 
SET! DATASOURCE_LINE {{!}} LOOP 
WAIT SECONDS = 5 
TAB T = 1 
URL GOTO = http: //oxiforms.com/form.php f = 6FE99B4B-7527-4088-A6EB-DCD935346084? 
TAG POS = 1 TYPE = INPUT: TEXT FORM = ID: form ATTR = ID: 7FFE1F2B-8259-43B7-BBDF-E19B2A0CCD4A_JJ CONTENT = {{COL1}!} 
TAG POS = 1 TYPE = INPUT: TEXT FORM = ID: form ATTR = ID: 7FFE1F2B-8259-43B7-BBDF-E19B2A0CCD4A_MM CONTENT = {{COL 2}!} 
TAG POS = 1 TYPE = INPUT: TEXT FORM = ID: form ATTR = ID: 7FFE1F2B-8259-43B7-BBDF-E19B2A0CCD4A_AA CONTENT = {{} COL3!} 
TAG POS = 1 TYPE = SELECT FORM = ID: form ATTR = ID: 744E42F3-44F3-4251-B304-5E46674902F1 CONTENT =% {} {COL4!} 
TAG POS = 1 TYPE = SELECT FORM = ID: form ATTR = ID: FADEE1D4-A1CE-4542-AE7C-47FAD64CE914 CONTENT =% {} {COL5!} 
TAG POS = 1 TYPE = INPUT: TEXT FORM = ID: form ATTR = ID: E7052250-27FC-4215-9133-44401DC5D7AA CONTENT = {{} COL6!} 
TAG POS = 1 TYPE = INPUT: TEXT FORM = ID: form ATTR = ID: 966A26AD-5727-42A3-A5C4-2FFC230B6522 CONTENT = {{} COL7鈥嬧€?} 
TAG POS = 1 TYPE = INPUT: TEXT FORM = ID: form ATTR = ID: 31E7F960-A50E-4BFF-BEAC-F2CDFECE7D66 CONTENT = {{} COL8!} 
TAG POS = 1 TYPE = INPUT: TEXT FORM = ID: form ATTR = ID: 7084345C-5BCE-443B-8969-5A2E0D29A540 CONTENT = {{} COL9!} 
TAG POS = 1 TYPE = INPUT: SUBMIT FORM = ID: form ATTR = NAME: continue 

I would like to know what does not work and how to fix it.   
Please to excuse the mistakes of language, do not speak English properly, I use an online translator.  

Thank you in advance for your answers.",https://forum.imacros.net/viewtopic.php?f=7&t=23411&sid=b07178a8376ad918bdc99d345a2f0b97,content
2221,extract number and put it in other input,"Hello all, i want to extract number and put it in other input.


Link to website: http://obsifight.fr/voter

I want to make an ""AutoVoter"" for a minecraft serveur, i want to extract the ""OUT"" on the RPG-Paradize page for ""Obsifight server"" and put it on the input marked in red on the script( http://rpg-paradize.com/?page=topsite )

All other part of script is done, but i dont know how to make this ..

Script: 
VERSION BUILD=8820413 RECORDER=FX
TAB CLOSEALLOTHERS
URL GOTO=http://obsifight.fr/voter/
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:loadDatas.php ATTR=ID:inputString CONTENT=Name here
TAG POS=1 TYPE=BUTTON FORM=ACTION:loadDatas.php ATTR=TXT:Continuer
TAG POS=1 TYPE=IMG ATTR=SRC:http://obsifight.fr/images/rpg.jpg
TAB T=2
TAB T=1
WAIT SECONDS=30
TAG POS=2 TYPE=INPUT:TEXT ATTR=* CONTENT=EXTRACTED NUMBER HERE
TAG POS=1 TYPE=BUTTON ATTR=TXT:Confirmer
WAIT SECONDS=1
TAG POS=1 TYPE=A ATTR=TXT:Plus<SP>tard
TAB CLOSEALLOTHERS
WAIT SECONDS=10801

kind regards

-Burn",https://forum.imacros.net/viewtopic.php?f=7&t=23371&sid=b07178a8376ad918bdc99d345a2f0b97,content
2223,"REGEXP throws ""wrong format of SEARCH"" in FFox","This regex[1] throws an search error[2] in FireFox... anyone know why?

[1] SEARCH SOURCE=REGEXP:""r=\""1\""[\s\S]*?<a href=\""([^\""]*?)\""[^$]*?\$([^<]*)"" EXTRACT=""$1,$2""

[2] SyntaxError: wrong format of SEARCH command, line 18 (Error code: -910)

Full macro for ref:
-----------------------------------------------
VERSION BUILD=7500718 RECORDER=FX
TAB T=1
'CSV = Comma Separated Values in each line of the file
SET !DATASOURCE C:\Users\Michelle\Downloads\ebay-get.csv

'SET !DATASOURCE_COLUMNS 1
'Start at line 1, there is no header in the file
SET !LOOP 1
'Increase the current position in the file with each loop
SET !DATASOURCE_LINE {{!LOOP}}
' Fill web form
URL GOTO=http://www.ebay.com/
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:gh-f ATTR=NAME:_nkw CONTENT={{!COL1}}

TAG POS=1 TYPE=INPUT:submit FORM=ID:gh-f ATTR=TXT:Search
'We executed the search, now try to extract the data

SEARCH SOURCE=REGEXP:""r=\""1\""[\s\S]*?<a href=\""([^\""]*?)\""[^$]*?\$([^<]*)"" EXTRACT=""$1,$2""

SAVEAS TYPE=EXTRACT FOLDER=* FILE=ebay-urls-prices.csv

'set the delay lower limit
SET !VAR1 30
'set the remaining time interval that need randomized
SET !VAR2 90
'calculate the random number
SET !VAR3 EVAL(""var randomNumber=Math.floor(Math.random()*\""{{!VAR2}}\"" +\""{{!VAR1}}\""); randomNumber;"")
WAIT SECONDS={{!VAR3}}",https://forum.imacros.net/viewtopic.php?f=7&t=23279&sid=7e74df639d022ccbaa0e92d884edcf30,content
2224,How to extract entire page,"I need to get the contents of the entire web page and save it off as a file.  How can I extract the entire web page, not just an element?

Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=23275&sid=7e74df639d022ccbaa0e92d884edcf30,content
2225,Clipboard,"Code: Select allTAG POS=1 TYPE=SPAN FORM=ACTION:/pb/ww/wf28a.validate.action ATTR=ID:serverPoints EXTRACT=TXT
SET !CLIPBOARD {{!EXTRACT}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ACTION:/pb/ww/wf28a.validate.action ATTR=NAME:model.points CONTENT={{!CLIPBOARD}}it work but there is an huge problem now.
The should extract only the Number but the result look like this:
Code: Select all                            0so how can i remove all fidget exept number from 0-9?
Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=23100&sid=7e74df639d022ccbaa0e92d884edcf30,content
2226,Getting error While using CSV,"hey here i had a another CSV file containing value but i got error while i am using it i had using FF 30 beta in windows 7 32 bit
Code: Select allVERSION BUILD=8820413 RECORDER=FX
TAB T=1
TAB CLOSEALLOTHERS  
SET !DATASOURCE Data.csv
'SET !VAR1 EVAL(""var randomNumber=Math.floor(Math.random()*88 + 2); randomNumber;"")
SET !DATASOURCE_LINE {{!VAR1}}
URL GOTO=http://nagercoil.olx.in/posting.php?src=8&categ_id=822
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:posting-form-1 ATTR=ID:title CONTENT={{!COL1}}
TAG POS=1 TYPE=SELECT FORM=ID:posting-form-1 ATTR=ID:position_type CONTENT=%1
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:posting-form-1 ATTR=ID:salary_C CONTENT=3,000
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:posting-form-1 ATTR=ID:salary_toC CONTENT=6,000
TAG POS=1 TYPE=SELECT FORM=ID:posting-form-1 ATTR=ID:salary_range CONTENT=%2
TAG POS=1 TYPE=SELECT FORM=ID:posting-form-1 ATTR=ID:optionals[OptionalOne] CONTENT=**
WAIT SECONDS=3
TAG POS=1 TYPE=SELECT FORM=ID:posting-form-1 ATTR=ID:optionals[OptionalTwo] CONTENT=***
TAG POS=1 TYPE=TEXTAREA FORM=ID:posting-form-1 ATTR=ID:description CONTENT=***
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:posting-form-2 ATTR=ID:contact-name CONTENT=***
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:posting-form-2 ATTR=ID:email CONTENT=****
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:posting-form-2 ATTR=ID:phone CONTENT=****
'TAG POS=1 TYPE=INPUT:TEXT FORM=ID:posting-form-2 ATTR=ID:neighborhood_name CONTENT={{!VAR1}}
'TAG POS=1 TYPE=BUTTON FORM=ID:form-success ATTR=ID:btnPublish

some one say wats the mistake...",https://forum.imacros.net/viewtopic.php?f=7&t=23151&sid=7e74df639d022ccbaa0e92d884edcf30,content
2227,how can I cut the extracted data,"hello
I have a problem. I want to cut the extracted data. I import text from a specific location and there is: day, date and time. it looks like this:

""Thursday 2014-06-19 9:00 - 12:00 AM""

I want to remove ""Thursday"" and ""AM"" copy only ""2014-06-19 9:00 - 12:00"" in another place

I might add that these are the variables and - tomorrow the date will change. you have any ideas?

example:
Code: Select allVERSION BUILD=8820413 RECORDER=FX
TAG POS=1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
SET !EXTRACT NULL
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:form1 ATTR=ID:TF1 CONTENT={{!VAR1}}

Using Windows 7 sp1, Firefox 30 and iMacros for Firefox 8.8.2

I read that I can use ""EVAL"" function. But I don't know how ?",https://forum.imacros.net/viewtopic.php?f=7&t=23013&sid=7e74df639d022ccbaa0e92d884edcf30,content
2228,Array,"Hallo Leute leider hab ich bei Google nichts dazu gefunden.

Ich w眉rde gerne ein Macro schreiben das mehrere Handykartenregistriert Nummern und Freischaltcodes liegen vor und das Script l盲uft auch allerdings w眉rde ich gerne alles Automatisieren


TAG POS=1 TYPE=INPUT:TEL FORM=IDrepaidSignupForm ATTR=ID:signup_mobilfunknummer CONTENT=HANDYNUMMERARRAY
WAIT SECONDS=5
TAG POS=1 TYPE=INPUT:TEL FORM=IDrepaidSignupForm ATTR=ID:signup_iccb CONTENT=FREISCHALTCODEARRAY
WAIT SECONDS=5
TAG POS=1 TYPE=INPUT:CHECKBOX FORM=IDrepaidSignupForm ATTR=IDpt_smart CONTENT=NO
WAIT SECONDS=5
TAG POS=1 TYPE=A ATTR=ID:submit-button


Vll. kann mir ja jemand zeigen wie man mehrere Nummern und Freischaltcodes in ein Script bringt.

Danke",https://forum.imacros.net/viewtopic.php?f=7&t=22983&sid=7e74df639d022ccbaa0e92d884edcf30,content
2229,Extracting data with same POS?,"Hi I've always wondered how would I extract text if the text is a variable but has the same POS as a different text which is constant in every email. e.g.
VERSION BUILD=8820413 RECORDER=FX
TAB T=1
FRAME NAME=""mail.hsplit.detail.content""
TAG POS=3 TYPE=TD ATTR=TXT:UZHWCL7T
TAG POS=3 TYPE=TD ATTR=TXT:* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
TAG POS=3 TYPE=TD ATTR=TXT:To<SP>verify<SP>your<SP>email<SP>and<SP>receive<SP>your<SP>f*

As you can see the last line as the same POS as the 4th line and when i try extract the text using that code all i get is a bunch of lines with no text as you can see here: 















The text it is trying to extract (TAG POS=3 TYPE=TD ATTR=TXT:UZHWCL7T) always varies from email to email, it is always capitals and sometimes has random numbers, but it is always the same amount of characters.

Thank you so much in advance! I have tried looking for other threads before creating this one but i could not find one.",https://forum.imacros.net/viewtopic.php?f=7&t=22965&sid=7e74df639d022ccbaa0e92d884edcf30,content
2230,How to copy and paste?  HELP,"I've tried for a couple days to copy and paste, watching demos, editing macros, extracting, but Im not catching on.

These are the steps Im trying to achieve, very simple :l

1. open http://securepasswordgenerator.net
2. clicking ""generate""
3. copy the random password
4. Then open http://nourls.com
5. pasting that password into the custom url box
6. Type http://awesome.com in the upper URL box
7. click ""shorten""
8. copy the new shortened url text
9. open twitter.com
10. tweet that shortened url
11. Loop it

Man, I know this is easy, but it seems imacros isn't real user friendly.  The learning curve is bigger than Id suspect.

So What are some commands for copying and pasting in this situation?


----------------------------------------------


This is as far as I've gotten, but I havent figured out how to fill the extracted text into NoUrls.coms form --->

VERSION BUILD=10002738
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=http://securepasswordgenerator.net/
WAIT SECONDS=22.636
DS CMD=CLICK X=231 Y=340 CONTENT=
WAIT SECONDS=10.031
DS CMD=CLICK X=261 Y=393 CONTENT=
WAIT SECONDS=0.015
TAG POS=1 TYPE=INPUT:TEXT ATTR=CLASS:pgen-password-input EXTRACT=TXT
TAB OPEN
TAB T=2
URL GOTO=nourls.com

...
....
.....",https://forum.imacros.net/viewtopic.php?f=7&t=22852&sid=7e74df639d022ccbaa0e92d884edcf30,content
2231,split text  and paste in diferent fields,"Hello,

I need to fill a lot of Multiple Choice Questions with the answers into a form, and trying to work with iMacros, but I'm at the very beginning.

I just figured out how to get a loop that aks for copying the questions and answers and how to fill sth. in the different fields:
Code: Select allVERSION BUILD=8810214 RECORDER=FX
TAB T=1
TAG POS=1 TYPE=A ATTR=TXT:Neue<SP>Frage...<SP>
PROMPT ""Kopiere die neue Frage!""
SET !VAR1 {{!CLIPBOARD}}
PROMPT {{!VAR1}}
TAG POS={{!LOOP}} TYPE=TEXTAREA ATTR=ID:question CONTENT={{!VAR1}}
TAG POS={{!LOOP}} TYPE=INPUT:TEXT ATTR=ID:answer1 CONTENT=answer<SP>1
TAG POS={{!LOOP}} TYPE=INPUT:TEXT ATTR=ID:answer2 CONTENT=test
TAG POS={{!LOOP}} TYPE=INPUT:TEXT ATTR=ID:answer3 CONTENT=test	
TAG POS={{!LOOP}} TYPE=INPUT:TEXT ATTR=ID:answer4 CONTENT=dd
TAG POS={{!LOOP}} TYPE=INPUT:TEXT ATTR=ID:answer5 CONTENT=dd
TAG POS={{!LOOP}} TYPE=BUTTON ATTR=ID:addQuestionButton

the questions are like this:
Welche Aussagen zu Virusinfektionen in der Schwangerschaft treffen nicht zu?
a) F眉r die Absch盲tzung des Risikos einer Gef盲hrdung von Embryo oder Fetus sind die
lmpfanamnese鈥?das Schwangerschaftsalter und der Serostatus der Schwangeren von Bedeutung.
b) Voraussetzung f眉r eine embryonale oder fetale Infektion ist die Infektion der Schwangeren mit
Vir盲mie.
c) Eine ClVIV-Infektion stellt f眉r Fr眉hgeborene eine gr枚脽ere Bedrohung dar als f眉r Babys, die zum
Termin geboren wurden.
d) Das Risiko einer R枚telnembryopathie nimmt in den ersten 14 Gestationswochen stark ab.
e) Von Impfungen in der Schwangerschaft ist grunds盲tzlich abzuraten.

so there is the question with a diferent amount of lines and the answers, always starting with a), b), c).... or A), B), C)

what I need now, is to put everything in the corresponding field. Anyone has an idea?
Mine was to split the text from the clipboard to lines and put every line, that starts with ""a)"" or ""A)"" into answer 1 and so on, and leaving the rest for the question.
But as I said, I have no clue how to do it, so maybe someone can help me?

I hope I made clear what i need.

Thank you in advance
Eric",https://forum.imacros.net/viewtopic.php?f=7&t=22682&sid=7e74df639d022ccbaa0e92d884edcf30,content
2232,Export from page to web form,"Hi guys, I'm new here and just started looking into iMacros for firefox. Maybe what I am looking for is not available in this version, but if so, any help would be appreciated.

My question is, is it possible to take certain elements from one site's page (values, such as name, address, phone number, etc) and use that information to fill out a form on a different site, into their respective fields. Then go back to the first, pull up the next page with different info, but of the same type, and use it to automatically fill out the form on the second site in the same fashion.

If this is at all possible, please let me know, even if I can only do it for only one specific page to another, would be very helpful. If this functionality is not there by default but can be added, any pointers would be much appreciated. 

I appreciate your time and consideration.

If I have not able to communicate clearly, this example should show exactly what i am looking for:

page on site 123.com
(static content)

NAME:
Billy

PHONE #:
222-4444

ADDRESS:
554 W Main street

So, I want to be able to automatically export this information as follows:

page on differentsiteabc.org
(editable fields and blank by default)

NAME:
[___________] <- value 'Billy' should automatically enter here

PHONE #:
[___________] <- value '222-4444' should enter here

ADDRESS:
[___________] <- value '554 W Main street' should go here

 and so on and so forth




Again, thanks for any help provided, it really means a lot to me.",https://forum.imacros.net/viewtopic.php?f=7&t=22793&sid=7e74df639d022ccbaa0e92d884edcf30,content
2234,Problems Replicating Table in Excel,"Hello everyone,
I have been able to extract data from a website using iMacros. The problem has occurred in trying to get the data to extract in individual cells aligned correctly back in a table format. I have attached the 3 tables from the websites and the resulting Excel file. I have looked online and in the forum for help on this but am stuck. Your help  would be greatly appreciated! 

My iMacros version build=10002738, I am using Windows 7, 64 bit and IE 10. The demo macros work fine.
Here is the script:

VERSION BUILD=10002738
FRAME NAME=SearchFrame
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:FormOne ATTR=NAME:ClosedDateMin CONTENT=4/11/2013
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:FormOne ATTR=NAME:ClosedDateMax CONTENT=4/11/2014
TAG POS=1 TYPE=INPUT:BUTTON FORM=NAME:FormOne ATTR=NAME:button5
WAIT SECONDS=2
TAG POS=1 TYPE=FORM FORM=NAME:FormExport ATTR=NAME:FormExport EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Daron\Desktop FILE=ValueTrends.csv
TAG POS=1 TYPE=INPUT:BUTTON FORM=NAME:FormExport ATTR=NAME:B2
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:FormOne ATTR=NAME:ClosedDateMin CONTENT=4/11/2012
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:FormOne ATTR=NAME:ClosedDateMax CONTENT=4/11/2013
TAG POS=1 TYPE=INPUT:BUTTON FORM=NAME:FormOne ATTR=NAME:button5
WAIT SECONDS=2
TAG POS=1 TYPE=FORM FORM=NAME:FormExport ATTR=NAME:FormExport EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Daron\Desktop FILE=ValueTrends.csv
TAG POS=1 TYPE=INPUT:BUTTON FORM=NAME:FormExport ATTR=NAME:B2
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:FormOne ATTR=NAME:ClosedDateMin CONTENT=4/11/2011
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:FormOne ATTR=NAME:ClosedDateMax CONTENT=4/11/2012
TAG POS=1 TYPE=INPUT:BUTTON FORM=NAME:FormOne ATTR=NAME:button5
WAIT SECONDS=2
TAG POS=1 TYPE=FORM FORM=NAME:FormExport ATTR=NAME:FormExport EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Daron\Desktop FILE=ValueTrends.csv",https://forum.imacros.net/viewtopic.php?f=7&t=22723&sid=7e74df639d022ccbaa0e92d884edcf30,content
2236,EVAL command,"Hi guys,

I'm very new to iMacro, I have almost finished my first iMacro script  but am a little stumped and hoping someone can help.

I'm looking to extract a month from one page and paste into another, the month appears within a dropbox for example 02 - February, I only need the first two digits, not the "" - February"".

I beleive I can acheive this with EVAL? So far I am using the following to remove the - but I'm not sure how to remove the other characters and spaces.
Code: Select allTAB T=1
TAG POS=1 TYPE=SELECT ATTR=NAME:month[date_month] EXTRACT=TXT
SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s.replace(\""-\"",\""\""); "")
SET !EXTRACT NULL
SET !EXTRACT {{!VAR1}}
SET !CLIPBOARD {{!EXTRACT}}

TAB T=2
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:TranxForm ATTR=NAME:monthField CONTENT={{!CLIPBOARD}}
SET !EXTRACT NULL


Any help would be great!

Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=20219&sid=764a56e3c04defae9b390be083f6af7b,content
2237,"Help with NESTED LOOPS, please (Simple url changing proble","Hello I am very new to this and I was hoping that somebody might be able to help.
Essentially I need to save the content of several internet pages and I want to change the url each time after I have successfully saved a page.
Here is the code I have attempted to write

Code: Select all
    VERSION BUILD=8601111 RECORDER=FX
    TAB T=1
    SET !LOOP 2629
    'Maximum at 2647'
    URL GOTO=http://tds.hycom.org/thredds/dodsC/GLBa ... itude[1964][0:1:0],Longitude[0][{{!LOOP}}],u[0:1:104][2][1964][{{!LOOP}}],v[0:1:104][2][1964][{{!LOOP}}]
    SAVEAS TYPE=TXT FOLDER=C:\Users\itlemong\Desktop\Assessment_Data\Sep-Dec2008\1964 FILE=1964_{{!LOOP}}.xls



To explain myself better, say I want to go to the url: GOTO=http://tds.hycom.org/X.Y
where for example X would be between 100-259 and Y between 50-175
I want to open and save the page contents for X=100 and vary Y between all of its values (50-175)
then move on to X=101, do all the Y values
and so on and so forth.

Please help if you know a quick solution to my problem.",https://forum.imacros.net/viewtopic.php?f=7&t=22378&sid=764a56e3c04defae9b390be083f6af7b,content
2238,"How to remove ""US$""1.92 I need only the number.","hi friends. I'm a little tired looking information to fix this problem    I need to remove this ""US$ "" 

this is my code:
Code: Select allVERSION BUILD=8601111 RECORDER=FX
TAB T=1
TAB T=2
SET !EXTRACT_TEST_POPUP NO
TAG POS=7 TYPE=TD ATTR=TXT:* EXTRACT=TXT 
SET !CLIPBOARD {{!EXTRACT}}
TAB T=1
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:form ATTR=ID:elm_price_price CONTENT={{!CLIPBOARD}}
TAG POS=1 TYPE=A ATTR=TXT:Quantity<SP>discounts
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:form ATTR=NAME:product_data[prices][1][price] CONTENT={{!CLIPBOARD}}

I need only the number. like: 2.88 鉁?not: US$ 2.88 鉁?
thank you so much for your help.",https://forum.imacros.net/viewtopic.php?f=7&t=22329&sid=764a56e3c04defae9b390be083f6af7b,content
2241,Downloading pdf documents,I have to download 300 to 400 invoices from website on a monthly basis. SI there way using the iMacros or some of its script I can create to automate it ?,https://forum.imacros.net/viewtopic.php?f=7&t=31478&sid=ddad572d7e36bd7d9ce3abd083a6d173,auto
2243,Auto-Click button until a specific text appear,"I'm trying to make an auto-click to a button on a website and return; when a specific text appears on the page (not click it)

Here is my simple codes:
    Code: Select allHTML:
    <span class=""tr-country"" data-tr=""IQ"">Iraq</span>
    <i class=""flag message-flag flags-IQ""></i>

    iMacros Recorder:
    VERSION BUILD=1011 RECORDER=CR
    URL GOTO=https://ometv.chat/
    FRAME F=1
    TAG POS=2 TYPE=DIV ATTR=TXT:next
    TAG POS=2 TYPE=SPAN ATTR=TXT:South<SP>Africa (Some Random Country)
    TAG POS=1 TYPE=I ATTR=CLASS:flag<SP>message-flag<SP>flags-ZA
What I want is to check if the class ""tr-country"" has the data-tr of ""IQ"" then we don't click ""next"" button

How can I do that in a condition way?

Thanks in advance.

FCI:
iMacros for Chrome 10.1.1 Free
Windows 10 English
Version 96.0.4664.93 (Official Build) (64-bit)",https://forum.imacros.net/viewtopic.php?f=7&t=31838&sid=ddad572d7e36bd7d9ce3abd083a6d173,auto
2245,auto refresh of page when live stream freezes,"Hello, I searched a lot but couldn't find it. I will be glad if you help. https://ustvgo.tv/msnbc/ freezes from time to time while watching this live stream. I need the code to avoid this. I want the page to auto refresh with f5 when the live stream freezes. There is no warning that the live stream is frozen. buffering only. If code like this cannot be written, I found another alternative. Stream audio turns off when the live stream freezes. If the sound is muted, imacros needs to be refreshed with f5. That's what I thought, but I couldn't write code. I would be very happy if anyone can help.",https://forum.imacros.net/viewtopic.php?f=7&t=31774&sid=ddad572d7e36bd7d9ce3abd083a6d173,auto
2248,Scraping a real estate page,"I've wasted several days trying to get some information of this page:

https://www.pennymacusa.com/home-value-estimator

there is a text box that you type in an address.
then it displays the address in a dropdown list, which you click on
then you click the submint button.

my imacros script does all these things, but most of the time if freezes.  There is something, (i believe) in the webpage that trys to prevent automation.

am i asking this question at the right place?

can somebody tell me how to get around this?",https://forum.imacros.net/viewtopic.php?f=7&t=31482&sid=117ef7144580822f4982137e21eddfc2,auto
2250,How to automate links clicking from Sheets to browser?,"Hi guys,

First of all - nice to meet you all. I'm new here and i'm trying to get my first steps with this great tool.

I'm trying to automate a proccess of saving a lot of pages to linkedin lists
The process scenario is:
1. Click on a link from google sheets
2. Click on linkedin ""more"" (3 points) button
3. Save the page to a user's list
4. Select the relevant list

The problem is that I can't find a way to repeat these steps for Google Sheets 
I need, of course, to click on each link once and save it to the list, than go on to the next one

Browser: Chrome
OS: Windows 10 x64
iMacros free version

Script:
Code: Select allVERSION BUILD=1010 RECORDER=CR
URL GOTO=https://www.url.com
TAG POS=2 TYPE=A ATTR=HREF:http://www.linkedin.com/relevanttab
TAB T=2
TAG POS=23 TYPE=SVG ATTR=TXT:
TAG POS=1 TYPE=A ATTR=ID:ember415
TAB T=3
TAG POS=1 TYPE=BUTTON ATTR=ID:ember74
TAG POS=1 TYPE=DIV ATTR=TXT:Blacklist:<SP>it<SP>companies

Here's a screenshot for you to understand the structure (not so complicated, but still lol)



鈥忊€廏SEx.PNG (15.17 KiB) Viewed 7262 times



Thanks for your help!",https://forum.imacros.net/viewtopic.php?f=7&t=31268&sid=117ef7144580822f4982137e21eddfc2,auto
2253,Need Solution for google recaptcha - when captcha comes suddenly,"FCI: iMB v12.6 'Trial', Win10_x32

Hi
Please accept my honor and love in advance who is going to response this post.
Code: Select allVERSION BUILD=601105 RECORDER=FX
SET !TIMEOUT_STEP 0
SET !ERRORIGNORE YES
TAB T=1
SET !DATASOURCE data.csv
SET !DATASOURCE_COLUMNS 1
SET !DATASOURCE_LINE {{!LOOP}}

URL GOTO=https://www.google.com/search?&q={{!COL1}}

WAIT SECONDS=1   
TAG POS=1 TYPE=DIV ATTR=ID:result-stats EXTRACT=TXT
SET result EVAL(""var s=\""{{!extract}}\"";s=s.replace(/,/g,\""\"");s=s.split(\"" \"");s[1];"")
 
SET !EXTRACT NULL
ADD !EXTRACT {{result}}

SAVEAS TYPE=EXTRACT FOLDER=D:\mizanrobi\output-files FILE=result.csv
WAIT SECONDS=3



I am using above code to extract google search result as csv format. The code has no issue. Problem is after running the loops 7 times or generally 13 times or any random times captcha comes (I am not robot) and I need pause the loop to solve captcha manually. I am expecting to implement captcha solution in my code so each time captcha comes imacros solve it automatically and the the code runs without interrupt . As you can see I am using csv file as input to run google search which contain 2000+ rows so I can not sit and solve captcha manually.

I have search for solution in this forum and got info that may be I have to buy a captcha solution service but do not know how it will work with  my current iim file as the captcha comes suddenly rather than the site which contains a form with I am a robot captcha always there.

Thank You

FCI: iMB v12.6 'Trial', Win10_x32",https://forum.imacros.net/viewtopic.php?f=7&t=31017&sid=117ef7144580822f4982137e21eddfc2,auto
2256,How do I structure a code to auto copy any text in a pattern  XXXXX-XXXXX-XXXXX-XXXXX-XXXXX ?,"How do I structure a code to auto copy any text in a pattern  XXXXX-XXXXX-XXXXX-XXXXX-XXXXX ? This pattern will always have 5 letters then a (-) 5 times. Scattered with numbers and letters, for example EB3DA-JRDFF-EN733-E3YXD-P7Z6G. I have been able to make a code to auto copy text, but not unknown text. I'm using ""VERSION BUILD=1005 RECORDER=CR"" on Google Chrome Version 78.0.3904.108 (Official Build)
This is what I have tried...

VERSION BUILD=1005 RECORDER=CR
SET !EXTRACT_TEST_POPUP NO
TAG POS=1 TYPE=* ATTR=TXT:* EXTRACT=TXT
SET !CLIPBOARD {{!EXTRACT}}


Help would be much appreciated. Thanks in advance.",https://forum.imacros.net/viewtopic.php?f=7&t=30723&sid=a8b6b8836ef4b006067cba7caee672f3,auto
2257,Will this software scrap this site in the way that I want ?,"Hey all,
Will this software scrape this site ?
https://westernpower.com.au/faults-outa ... tages-map/
I would like to scrap it automaticly every 5 mins to compile a database of just how long certain areas are without power.
It needs a zoom to see all the data so that is why I am asking before buying anything.",https://forum.imacros.net/viewtopic.php?f=7&t=30277&sid=a8b6b8836ef4b006067cba7caee672f3,auto
2262,proper filling of csv columns,"iMacros=8.9.7, FF=52.xxx OS=WIN7
Code: Select allFILTER TYPE=IMAGES STATUS=ON
TAB T=1
SET !EXTRACT_TEST_POPUP NO
'SET !ERRORIGNORE YES
SET !TIMEOUT_STEP 0
TAG POS=1 TYPE=B ATTR=TXT:袙懈写械芯: EXTRACT=TXT
SET !EXTRACT NULL
TAG POS=R1 TYPE=SPAN ATTR=* EXTRACT=TXT
SET !VAR0 {{!EXTRACT}}
TAG POS=1 TYPE=B ATTR=TXT:袗褍写懈芯#1: EXTRACT=TXT
SET !EXTRACT NULL
TAG POS=R1 TYPE=SPAN ATTR=* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
TAG POS=1 TYPE=B ATTR=TXT:袗褍写懈芯#1: EXTRACT=TXT
SET !EXTRACT NULL
TAG POS=R2 TYPE=SPAN ATTR=* EXTRACT=TXT
SET !VAR2 {{!EXTRACT}}
TAG POS=1 TYPE=B ATTR=TXT:袗褍写懈芯#1: EXTRACT=TXT
SET !EXTRACT NULL
TAG POS=R4 TYPE=SPAN ATTR=* EXTRACT=TXT
SET !VAR3 {{!EXTRACT}}
TAG POS=1 TYPE=B ATTR=TXT:袗褍写懈芯#2: EXTRACT=TXT
SET !EXTRACT NULL
TAG POS=R1 TYPE=SPAN ATTR=* EXTRACT=TXT
SET !VAR4 {{!EXTRACT}}
TAG POS=1 TYPE=B ATTR=TXT:袗褍写懈芯#2: EXTRACT=TXT
SET !EXTRACT NULL
TAG POS=R2 TYPE=SPAN ATTR=* EXTRACT=TXT
SET !VAR5 {{!EXTRACT}}
TAG POS=1 TYPE=B ATTR=TXT:袗褍写懈芯#2: EXTRACT=TXT
SET !EXTRACT NULL
TAG POS=R4 TYPE=SPAN ATTR=* EXTRACT=TXT
SET !VAR6 {{!EXTRACT}}
TAG POS=1 TYPE=B ATTR=TXT:袗褍写懈芯#3: EXTRACT=TXT
SET !EXTRACT NULL
TAG POS=R1 TYPE=SPAN ATTR=* EXTRACT=TXT
SET !VAR7 {{!EXTRACT}}
TAG POS=1 TYPE=B ATTR=TXT:袗褍写懈芯#3: EXTRACT=TXT
SET !EXTRACT NULL
TAG POS=R2 TYPE=SPAN ATTR=* EXTRACT=TXT
SET !VAR8 {{!EXTRACT}}
TAG POS=1 TYPE=B ATTR=TXT:袗褍写懈芯#3: EXTRACT=TXT
SET !EXTRACT NULL
TAG POS=R4 TYPE=SPAN ATTR=* EXTRACT=TXT
SET !VAR9 {{!EXTRACT}}
SET !EXTRACT NULL
ADD !EXTRACT {{!var0}}
ADD !EXTRACT {{!var1}}
ADD !EXTRACT {{!var2}}
ADD !EXTRACT {{!var3}}
ADD !EXTRACT {{!var4}}
ADD !EXTRACT {{!var5}}
ADD !EXTRACT {{!var6}}
ADD !EXTRACT {{!var7}}
ADD !EXTRACT {{!var8}}
ADD !EXTRACT {{!var9}}
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\1\Documents\iMacros\Datasources FILE=EXTRACT_2.csv
URL GOTO=imacros://run/?m=EXTRACT_3.iim


issue is next: for example column 2 should contain extracted data !var2, column 3 should contain extracted data !var3 
BUT if  TAG POS=R2 TYPE=SPAN ATTR=* EXTRACT=TXT is missed (doesn't exist) it automatically replaced by the next !var
Let me be more clear: column1 = surname, column2 = name, column3 = city etc
but if name is missed, column2 will contain city.
How to solve it, how to mark non existing element with proper signature 
and get next column1 = surname, column2 = missed, column3 = city",https://forum.imacros.net/viewtopic.php?f=7&t=30050&sid=7ae4e60bc4f3ad89d3e546c9096547af,auto
2265,Variable reading and timeout loop,"Hi everyone! I'm just got iMacros cause I need it to create a bot for my economical simulator. In this game, you have to fight 10 times a day and work once to get paid.
If there are people who want to help, I would really appriciate it.
The theory it's simple, but I have no ideea how to write the code.

My plan is to write this bot that it can automatically fight 10 times, with a break of 10 minutes between each fight. I could do this by simply recording the macro, but there are some problems.
I can't beat someone who has more energy than me. So, I'm looking for a code that can read my victim's energy, and if the number is bigger than 5 (for example), than return and find another one who has less.
I've put here a printscreen that shows how this menu looks. The code should read vitim's energy from this menu, check if it is bigger than 5 and return to the list for searching another one with less energy. Or if it's possible to figure out from the begining the one that has less than 5% energy, why not.
I've thought that going by inspect element in chrome may help me. I located the line where energy is displayed, but I have no ideea how to write the code.

I've started building this code, but it can only do simple tasks cause I dont know how to improve it yet.
------------------------------------------------------------------------------------------------------------------------------
Code: Select allVERSION BUILD=1002 RECORDER=CR
URL GOTO=http://www.marketglory.com/account
TAG POS=2 TYPE=SPAN ATTR=TXT:Lupte<SP>Referali
TAG POS=1 TYPE=A ATTR=TXT:Ataca
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://www.marketglory.com/account/fight/view_fight/319762 ATTR=NAME:test
WAIT SECONDS=588
TAG POS=2 TYPE=SPAN ATTR=TXT:Lupte<SP>Referali
TAG POS=1 TYPE=A ATTR=TXT:Ataca
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://www.marketglory.com/account/fight/view_fight/319762 ATTR=NAME:test
WAIT SECONDS=587
TAG POS=2 TYPE=SPAN ATTR=TXT:Lupte<SP>Referali
TAG POS=1 TYPE=A ATTR=TXT:Ataca
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://www.marketglory.com/account/fight/view_fight/319762 ATTR=NAME:test
WAIT SECONDS=588
TAG POS=2 TYPE=SPAN ATTR=TXT:Lupte<SP>Referali
TAG POS=1 TYPE=A ATTR=TXT:Ataca
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://www.marketglory.com/account/fight/view_fight/319762 ATTR=NAME:test
WAIT SECONDS=589
TAG POS=2 TYPE=SPAN ATTR=TXT:Lupte<SP>Referali
TAG POS=1 TYPE=A ATTR=TXT:Ataca
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://www.marketglory.com/account/fight/view_fight/319762 ATTR=NAME:test
WAIT SECONDS=601
TAG POS=2 TYPE=SPAN ATTR=TXT:Lupte<SP>Referali
TAG POS=1 TYPE=A ATTR=TXT:Ataca
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://www.marketglory.com/account/fight/view_fight/319762 ATTR=NAME:test
WAIT SECONDS=603
TAG POS=2 TYPE=SPAN ATTR=TXT:Lupte<SP>Referali
TAG POS=1 TYPE=A ATTR=TXT:Ataca
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://www.marketglory.com/account/fight/view_fight/319762 ATTR=NAME:test
WAIT SECONDS=602
TAG POS=2 TYPE=SPAN ATTR=TXT:Lupte<SP>Referali
TAG POS=1 TYPE=A ATTR=TXT:Ataca
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://www.marketglory.com/account/fight/view_fight/319762 ATTR=NAME:test
WAIT SECONDS=603
TAG POS=2 TYPE=SPAN ATTR=TXT:Lupte<SP>Referali
TAG POS=1 TYPE=A ATTR=TXT:Ataca
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://www.marketglory.com/account/fight/view_fight/319762 ATTR=NAME:test
WAIT SECONDS=606
TAG POS=2 TYPE=SPAN ATTR=TXT:Lupte<SP>Referali
TAG POS=1 TYPE=A ATTR=TXT:Ataca
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://www.marketglory.com/account/fight/view_fight/319762 ATTR=NAME:test
WAIT SECONDS=605
-----------------------------------------------------------------------------------------------------------------------------------------------------------------

You can notice here that I copied the tasks because there are 10 fights to do and I want the macro to keep running. As I said, there is a timeout of 10min between fights, and I added that with a difference of couple seconds 
so I'm safe from getting banned, cause they could check that I'm doing my fights every day at the same interval. I will keep editing this timeout.
So, besides variable reading, I would also need a loop with an timeout of 10min between each task group.

I really want to learn some coding commands, as a begginer. If you guys can help me with that, I would really appriciate it.

As a summary, I need a code for reading variables who can chose the victim that has the energy under 5% and a loop one, with a timeout of 10mins between each fight, who would simplify that long code.
A tested fight, is defined by that:
Code: Select allURL GOTO=http://www.marketglory.com/account
TAG POS=2 TYPE=SPAN ATTR=TXT:Lupte<SP>Referali
TAG POS=1 TYPE=A ATTR=TXT:Ataca
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ACTION:http://www.marketglory.com/account/fight/view_fight/319762 ATTR=NAME:test

Thank you all. I'm hoping for a solution.",https://forum.imacros.net/viewtopic.php?f=7&t=29531&sid=7ae4e60bc4f3ad89d3e546c9096547af,auto
2267,Extract image path from table,"Specialty electrics draw eyes at July series CHICAGO -- Whether it was the free food or the products themselves, lots of the buyers in the NHMA Mid-Year Show here gravitated toward stalls with specialty kitchen electrics.

>> http://bitly.com/best-masticating-juicer
On display were an variety of appliances such as pastamakers, ice cream makers, breadmakers, cappuccino/espresso makers, juicers and waffle irons. While producers touted their goods, fresh bread, pasta, ice cream, cappuccino and espresso were prepared and served directly before the eyes -- and upset -- of attendees.
Judging from the numbers watching the food preparations take place, the interest level in the specialty market was large. And, many manufacturers agreed that the desire for specialty items is slowly increasing. However, according to some producers, only a few of the items now in the specialty niche are expected to profit from the increase in interest. In fact, most manufacturers of specialty products agreed that items such as pasta makers and ice cream manufacturers are leveling off either because of saturation in the market or because of what they predict overspecialization.

According to Lou Federico, divisional manager of kitchen products at Conair Corp., pastamakers and ice cream manufacturers are weakening. The reason, he explained, is these products need too much time and effort from consumers while delivering relatively few benefits and little convenience.
""We (Conair Corp.) have been assessing new niche categories. The market for pastamakers appears to be retrenching. Ice cream makers have also weakened. The biggest hangup in retail with ice cream manufacturers is the fact that it takes a lot of work to make something that's simpler to purchase already made."" Galileo Buzzi-Ferraris, president of Lello Appliances, distributor of Simac goods, concurred that interest in pastamakers is subsiding. He clarified that the availability of fresh pasta in many grocers' freezers has diminished the need for customers to exert the time and effort to make pasta at home.
Buzzi-Ferraris' view is that""the high-end ice cream business has been screwed-up when Donvier introduced its guide ice cream maker, which retails for just about $30."" Consequently, he said, prices for the whole marketplace were reduced. ""Now to sell huge numbers of (high end ) ice cream machines is quite hard,"" he added.

>> https://medium.com/@bestmasticatingjuic ... 01a47a8d59
In response to the Donvier unit, Simac is now offering the Gelato Pronto; a $79 manual ice cream maker. In accordance with Buzzi-Ferraris the reduced priced device is selling quite well. As for Donvier, federal sales manager Alice Shoemaker responded,""I don't believe we screwed it up,"" referring to reduced priced ice cream manufacturers. ""I think what we have done is altered the marketplace.""

Tom Fletcher, vice president of advertising at Gourmet Housewares, distributor of a line of Gaggia products, including a manual ice cream maker, also disagreed with Buzzi-Ferraris. He argued that lower priced ice cream manufacturers really extend the market. According to Fletcher,""There is a big difference between a $30 ice cream manufacturer and also a $600 unit"" He added that customers who purchase one would not be interested in the other.
One area where the two Gaggia's Fletcher and Simac's Buzzi-Ferraris did agree is the fact that cappuccino/espresso manufacturers are strong and should continue to rise tremendously. To fulfill the consumer demand for specialty coffee manufacturers, the two companies are marketing new blend cappuccino/espresso machines. ""Right now,"" said Fletcher,""cappuccino/espresso machines are in vogue, probably because customers are traveling more and therefore are exposed to various sorts of coffees. The American people as a whole is growing more sophisticated regarding the consumption of coffee and consequently they are looking for variety.""

Buzzi-Ferraris agreed. ""The cappuccino/espresso marketplace is big, and it is getting even larger. We are responding to this growing market by developing a fully automatic device that will make it even simpler to make espresso and cappuccino coffees. We'll introduce our newest cappuccino/espresso machine at the January Housewares Show (1990)."" According to Conair's Lou Federico, juicers and juice extractors are different examples of specialty items with growth potential.

>> https://www.facebook.com/Bestmasticatingjuicers.zones
""I visit two merchandise trends, first there is the juicer that squeezes out the juice of soft fruits and vegetables -- for example, oranges, grapefruits and berries. Then there's the juice extractor which pulls out juices from fruits and vegetables that don't seem juicy -- for instance, carrots, pears and apples. I think the products are being driven by the health trend, which has been around for a while and the diet craze, which frequently requires drinking considerable quantities of juices""
Presently, Conair Corp. is advertising a juicer and based on Federico, can be looking seriously at juice extractors. As with other manufacturers, Federico agreed that the secret to manufacturing specialty goods is twofold. ""Specialty electrics must provide benefits and conveniences,"" said Federico. ""If we (Conair) look at a specialty merchandise, we ask ourselves,'Can it be providing a benefit?' And'Can it be providing a benefit?' If the solution is no, we're not interested.""

He added,""We looked at breadmakers and decided it was too specialized. The final result is a small amount of bread which requires numerous hours to bake.""
On the flip side, Keizo Kawanishi, executive vice president of Zojirushi America Corp., pointed out that not all breadmakers are equally. By way of example, he said the Zojirushi breadmaker provides consumers versatility, he noted is an aspect many customers are searching for in specialty products.
""Other companies,"" explained Kawanishi,""have components that just bake bread. Our apparatus can bake cakes, bread and make jams. A great deal of people at the (Mid-Year) Show showed interest in our breadmakers. Consumers often get breadmakers for individual use, and later purchase them as presents.""",https://forum.imacros.net/viewtopic.php?f=7&t=29396&sid=7ae4e60bc4f3ad89d3e546c9096547af,auto
2268,Extracting prices from flight comparison sites - Possible?,"Would it be possible to extract pricing data from comparison sites like Google flights?

Searching for good deals on flight comparison websites is arduous to say the least.

Each month I want to be able to run automated searches for multiple destination across Europe for a combination of dates and extract all of that information into Excel to be processed to find the cheapest days and times to fly.

Can it be done?",https://forum.imacros.net/viewtopic.php?f=7&t=28552&sid=7ae4e60bc4f3ad89d3e546c9096547af,auto
2272,JScript variable EVAL error,"This used to work in Firefox before its Quantum release or until whenever the extension was automatically disabled as unsupported.

The error ...:

Error -1250: JScript statement in EVAL contains the following error: Variable 'repatt' has not been declared. Line 13: SET !VAR2 EVAL(""var s=\""{{!EXTRACT}}\""; var patt='[\\r\\n]'; repatt=new RegExp(patt, 'g'); s=s.replace(repatt,\""\""); "")


... is triggered by the following:

VERSION BUILD=12.0.501.6698
TAB T=1
SET !EXTRACT_TEST_POPUP NO
' after FF Quantum , installed the trial iMacros version and got 
'      ""Error -1200: Incorrect ATTR or FORM specifier: EXTRACT=TXT""
'   which resorting to the unassigned variable VAR1 may or may or may not be 
'   a suitable workaround for--the script now gets past it anyway.
' TAG POS=1 TYPE=title ATTR= EXTRACT=TXT
TAG POS=1 TYPE=title ATTR={{!VAR1}} EXTRACT=TXT
SET !VAR2 EVAL(""var s=\""{{!EXTRACT}}\""; var patt='[\\r\\n]'; repatt=new RegExp(patt, 'g'); s=s.replace(repatt,\""\""); "")
...


This is running on the iMacros browser installed earlier this week in the following set-up:

Microsoft Windows 10 Home
Microsoft Windows NT 6.3.9600.0
64-bit Operating System
Installed UI Culture: English (United States)
CLR version 4.0.30319.42000
Internet Explorer version 11.192.16299.0
Enhanced Protected Mode = False, 64-bit processes for EPM = False
BrowserEmulation Mode = IE11
iMacros Browser (x86) version 12.0.501.6698
------------------------------------------------------------
Auxiliary Libraries:
iMacros Direct Screen (iimds) 12.0.0.19
iMacros Image Recognition (iimIRm) 12.0.500.8234
iMacros Dialog Manager (imsys) 12.0.501.6698
------------------------------------------------------------
Scripting Interface Libraries:
iMacros IE Scripting Connector (iimConnector) 12.0.0.148
iMacros FX Connector (iimFirefoxConnector) 3.0.0.37
iMacros Scripting Interface (iimInterface) 12.0.0.148
iMacros FX&Cr Scripting (imtcp) 3.0.0.37
...


Some demo macros do encounter problems ...

Detect Javascript Errors.iim ...
    Error -1400: Script error detected but ONSCRIPTERROR CONTINUE=NO was specified.
Eval.iim ...
    Error -1340: Value is not in the set range. Line 25: SET !VAR2 EVAL(""var s=\""{{!EXTRACT}}\"", d = parseFloat(s); if(d > 99 && d < 101) d; else MacroError(\""Value is not in the set range\"");"")


At this stage in the script anyway, the goal is just to strip out quotes.  Later on similarly so for URL substrings.  

Thanks for any pointers regarding fixes or alternate ways to do this.",https://forum.imacros.net/viewtopic.php?f=7&t=28364&sid=01bf0edf807fd3d4bb507a35b87155ad,auto
2273,Trimming a word off the extract value using EVAL + .substr?,"iMacros for Firefox 9.0.3//Firefox 49.0.2//Windows 10 Pro [Eng]

So I'm going a little nuts here. I haven't used iMacros in a long time (last post on this account was 2011) and I seem to have forgotten a lot. But I had a problem automating some link clicking and I thought, hey that old iMacros extension might work! So here I am.

So my end goal for my script is to click on several hyperlinked downloads (.torrent files) to automate the downloading process for me.

First problem:
The amount of torrents on the page change, depending on what specific page you're looking at. Some pages on the site may have 7 torrents, others have 2.
Luckily the site lists the amount of torrents on a page in a text attribute (literal text is 'Torrents x').
So first, I need to extract the 'x'. Through quite a bit of searching, it seems the best way to do this is using EVAL + javascript to manipulate the extraction. I settled on using substr because I only need the last value for the extraction and the word Torrents is always the same amount of characters. This is what I have so far:
Code: Select allSET !EXTRACT_TEST_POPUP NO
TAG POS=1 TYPE=LI ATTR=TXT:Torrents:<SP>* EXTRACT=TXT
SET !VAR1 EVAL(""'{{!EXTRACT}}'.substr(10,1);"")
PROMPT {{!var1}}
Now, I know that I've got some wrong syntax in there, I'm just not fully understanding where it's at/what is wrong. I know it's something with the EVAL because if I just use SET !VAR1 {{!EXTRACT}} instead of the EVAL in the same code, I get the prompt: Torrents: x. So everything's working, all I want is the correct syntax for trimming Torrents:<SP> off and having VAR1=x.
--------------------------
My second problem (getting ahead of myself a bit here but better to ask now I guess):
This is the line of code to click on the download links. Basically there are multiple instances of this text on the page, all corresponding to links to different torrents. Each torrent link looks like this: 
[DownLoad]
So to find those links and click on them I use:
Code: Select allTAG POS=1 TYPE=A ATTR=TXT:DownLoad
TAG POS=2 TYPE=A ATTR=TXT:DownLoad
TAG POS=3 TYPE=A ATTR=TXT:DownLoad
etc...
What I need is to use the VAR1 from my first problem above and have it define the maximum amount of TAG POS to use. Basically if there's 23 torrents on the site, I need it to run:
Code: Select allTAG POS=1 TYPE=A ATTR=TXT:DownLoad
...
TAG POS=23 TYPE=A ATTR=TXT:DownLoad.
How would I go about doing that? I understand I can use TAG POS={{!var1}} to find the final DownLoad but I'm not sure how to use that to count up or how to design it so it stops the code after a certain amount.

Hopefully I explained myself clearly enough, if you need anymore information feel free to ask me to clarify. There's probably a much simpler way of doing this but I'm obviously clueless as to what it is.",https://forum.imacros.net/viewtopic.php?f=7&t=28323&sid=01bf0edf807fd3d4bb507a35b87155ad,auto
2276,Scraping dynamic list of links with download filename,"This might be a dumb question, but is iMacros capable of saving all links of a particular class on a page and use the filenames specified in the ""download"" attribute?  Basically I'm using an eBay image scraping tool to generate a list of links to each of a listing's images at maximum resolution, but because of CORS limitations, I must right-click each link to get the save dialog box to use the filename in the ""download"" attribute.  This is fine if there are only a few images, but it gets tedious if a listing as more than a dozen images, so I'm hoping to use a macro to automate this.  Is iMacros what I want?  How would I got about ensuring that the specified filenames are used?

The HTML code looks like this:

<a href=""https://i.ebayimg.com/images/g/l5kAAOSw ... -l1600.jpg"" class=""img_link"" id=""img_link_0"" download=""322871381840 s-l1600 0.jpg"">322871381840 s-l1600 0.jpg</a>
<br>
<a href=""https://i.ebayimg.com/images/g/KBgAAOSw ... -l1600.jpg"" class=""img_link"" id=""img_link_1"" download=""322871381840 s-l1600 1.jpg"">322871381840 s-l1600 1.jpg</a>
<br>
<a href=""https://i.ebayimg.com/images/g/CgwAAOSw ... -l1600.jpg"" class=""img_link"" id=""img_link_2"" download=""322871381840 s-l1600 2.jpg"">322871381840 s-l1600 2.jpg</a>
<br>
<a href=""https://i.ebayimg.com/images/g/gSQAAOSw ... -l1600.jpg"" class=""img_link"" id=""img_link_3"" download=""322871381840 s-l1600 3.jpg"">322871381840 s-l1600 3.jpg</a>
<br>
<a href=""https://i.ebayimg.com/images/g/xE8AAOSw ... -l1600.jpg"" class=""img_link"" id=""img_link_4"" download=""322871381840 s-l1600 4.jpg"">322871381840 s-l1600 4.jpg</a>
<br>
.
.
.",https://forum.imacros.net/viewtopic.php?f=7&t=28273&sid=01bf0edf807fd3d4bb507a35b87155ad,auto
2280,Data Search,"Hi,
I want to find data & save it on CSV automatically. Suppose ""Beyerdynamic T 90"" is a headphone model number, I want to find the Weight of this headphone from google.com automatically from the search result. Is it possible?",https://forum.imacros.net/viewtopic.php?f=7&t=27542&sid=8ea127909c0d83c2fac4f53ec6c3d19a,auto
2285,How do i extract this data,"<div style=""padding: 5px; margin: auto; text-align: center; color: #fefefe;"">
<div style=""margin: auto; width: 93%; padding-top: 16px; padding-bottom: 6px; text-align: left;"">
<span class=""onlineSep""> - </span><a href=""viewprofile.php?username=Max""><b style=""color: lime;"">Max</b></a><span class=""onlineSep""> - </span><a href=""viewprofile.php?username=Wolfsz""><b style=""color: lime;"">Wolfsz</b></a><span class=""onlineSep""> - </span><a href=""viewprofile.php?username=AsusMB""><b style=""color: #FFC753;""><u>AsusMB</u></b></a><span class=""onlineSep""> - </span><a href=""viewprofile.php?username=Slaviour""><b style=""color: #FFC753;""><u>Slaviour</u></b></a>

For example i will run the macro loop (4 times)
It should extract Max, Wolfsz, AsusMB, Slaviour
First loop would extract Max, Second would be Wolfsz etc 

However these usernames will change and i wouldn't know would this be possible(This is an example)",https://forum.imacros.net/viewtopic.php?f=7&t=27180&sid=41f2b858fcf7ca695bb39ee1420df70b,auto
2288,Need Expert Scraping,"I need someone who can provide long term work for me with the equivalent of the following for many countries;

Schedule scripts on server if it is required. Google scraping, Facebook scraping, yellow pages, linkedin, amazon, webshops, specialty sites and other sites with lists of any items. Scrape secured and protected sites, ability for crawlers to enter into login form, emulate ajax requests etc. If site block IP , use proxy or TOR.  Avoid captcha on site in automatic or manual mode. 

Export data into json, csv (excel), mysql, mongodb.",https://forum.imacros.net/viewtopic.php?f=7&t=25214&sid=41f2b858fcf7ca695bb39ee1420df70b,auto
2294,Extracting a web table into Excel columns,"Hi team,

1. What version of iMacros are you using? iMacros Browser v11.1.495.5175 [Trial Version day 1 of 30]  (very new )
2. What operating system are you using?  Windows 10 English 64Bit
3. Which browser(s) are you using? (include version numbers) IE 11.420...

I have been able to extract a table from a webpage and save it to a .csv file.

However when the saved files is opened in notepad, this is what is i get between the ................. lines
............................................................................................. 

Vehicle

Vehicle Type

Tier 4

Tier 3

Tier 2

Tier 1

Vehicle Time Zone

Last Attempted Connection

Last Completed Connection

Days Since Last Completed Connection

6118 (1000544)Hyster H18.00XMKawerauCWPGroupIMHYAP(GMT+12:00) Auckland, Wellington30-June-2016 10:51:1530-June-2016 10:51:381 days
5973 (1000739)Hyster H3.5FTTokoroaCWPGroupIMHYAP(GMT+12:00) Auckland, Wellington20-January-2016 23:46:4620-January-2016 23:47:09162 days
5978 (1000771)Hyster H4.0FTTokoroaCWPGroupIMHYAP(GMT+12:00) Auckland, Wellington27-June-2016 14:08:0527-June-2016 14:08:223 days
6132 (1000838)Hyster H12.00XMNelsonCWPGroupIMHYAP(GMT+12:00) Auckland, Wellington30-June-2016 09:20:4130-June-2016 09:21:311 days
5984 (1001004)Hyster H8.0FTHubCWPGroupIMHYAP(GMT+12:00) Auckland, Wellington30-June-2016 10:18:3630-June-2016 10:18:481 days
5983 (1001005)Hyster H8.00FTHubCWPGroupIMHYAP(GMT+12:00) Auckland, Wellington29-June-2016 10:39:5029-June-2016 10:40:292 days
6125 (1001107)Hyster H12.00XMNelsonCWPGroupIMHYAP(GMT+12:00) Auckland, Wellington29-June-2016 14:39:1329-June-2016 14:39:311 days
...................................................................................

The relevant imacro script lines are:

URL GOTO=http://203.xx.xxx.xxx/Fleet_Online/repo ... ivity.aspx
TAG POS=3 TYPE=TBODY ATTR=* EXTRACT=TXT

'The SAVEAS statement was added manually to write the extracted table to a file
'(The alternative way to get the extracted data is the Scripting Interface)
SAVEAS TYPE=EXTRACT FOLDER=C:\Downloads FILE=Connectivity_{{!NOW:yymmdd_hhnnss}}.csv

'WAIT SECONDS=2
URL GOTO=http://demo.imacros.net/Automate/OK 

.................................................................................................

The first 10 text rows are a single row of column headings.
The rest are the elements which go in the columns

I have searched for some time to find out why I cannot get comma and quotes delimited data being saved to the .csv file.

Can you please advise what I am doing wrong here.

Any assistance is much appreciated.

Cheers 
Bernard",https://forum.imacros.net/viewtopic.php?f=7&t=26410&sid=91bf0dc741658a74aa70a7f66f53d097,auto
2296,FAIL_IF_FOUND on Checkbox value,"Hi there..

I am VERY new to iMacros, and am so far very impressed.  
As I am only trialling the product at present I am using the free version. (v11)

I am using the product to create automated regression testing, and so checking results is a key element of my macros.

I am currently trying to run a macro which proves whether or not a check box on a page has been ticked.

If the checkbox is not ticked (or is ticked, whichever) I want the Macro to error (FAIL_IF_FOUND?)

I am not sure the best way to accomplish this.  I had it working using image search, but the free version does not include this feature.

How do I extract the value of a checkbox, and then fail the Macro if the value is either YES or NO?

Windows 8.1, IE11
iMacros Sidebar for Internet Explorer (x64) Version 11.1.495.5175
Released on 10/03/2016
Licensed Product: Free Edition

Many thanks in advance.",https://forum.imacros.net/viewtopic.php?f=7&t=26278&sid=e8ccee866abcf29ebd57b704a290f61c,auto
2298,Case for the Pros - Help with Looping & Excluding,"I am completely new to Imacros and I might need some help from you 

My plan is to automatically sent a message to specific users on an online marketplace.

I would like to have a script for that which allows me to execute whenever I need it.

First, go to this website: http://www.kapaza.be

Enter a search term: for example ""iphone 4""

Brings you here --> http://www.kapaza.be/nl/belgie?ca=0&l=0 ... =3&ps=&pe=

Then click on the Headline of every single result, scroll down the page and sent me message to the user by filling in: Name, E-Mail, Phone & Question. The Button to sent the message is  ""Verstuur je Bericht"" 

See here --> http://www.kapaza.be/nl/telefonie-acces ... 928109.htm

Now the tricky part comes...

1. How can I skip offers which have certain words in the Offer Headline? ( I only want to contact people who sell something and not those who are looking to buy an Iphone)
- For example words like: ""looking for"", ""Buying"" etc

2. How can I implement a function which checks if a member has already been contacted. I don麓t want to sent the same message twice to the same user. I was thinking to save every Offer Headline to an excel / csv file after the member has been contacted. If the script goes into the Offer it checks if Offer Headline is in the csv File. If yes --> skip. Maybe there麓s an easier option for this. Any help here is appreciated.

3. How can I run the script over several different pages. I don麓t want to stop if the script hits the last result on page 1.

Hope someone is able to help me with this!

Here麓s some additional info: 

Working on a Mac OS X (10.9.5)
Browser: Chrome (version 50.0.2661.102 (64-bit))

Thanks a lot ",https://forum.imacros.net/viewtopic.php?f=7&t=26237&sid=e8ccee866abcf29ebd57b704a290f61c,auto
2299,TAG selection and input for autogen custom textbox,"Hi Team,

I am using Imacros for one of the web application for screen scraping. 

Issue: Using a tag to select and input text into a hybrid custom text box as described in the screen shot. 

TAG POS=1 TYPE=INPUT:TEXT ATTR=ID:s2id_autogen1

The value for ATTR which in this case is autogen1 randomly changes to something like autogen22/autogen12/autogen18 etc.

Do let me know if you need any more details in this regard. Thank you.",https://forum.imacros.net/viewtopic.php?f=7&t=26085&sid=e8ccee866abcf29ebd57b704a290f61c,auto
2300,Extract Sliding Banners ?,"Hi All,

can anyone point me in the right direction to save screenshots each time a banner image changes ?

below website is what im trying to capture screenshots for..

http://www.autosessive.com/



imacros 11.1495.5175
Windows 8 pro x64
Firefox 44.02 +firefox imacro addon.",https://forum.imacros.net/viewtopic.php?f=7&t=26007&sid=e8ccee866abcf29ebd57b704a290f61c,auto
2301,Captcha img changes when downloaded,"At the moment I'm trying to display a captcha in a message box so I can complete it manually then let it continue automatically, but as I progress I intend to have captchas sent to a Captcha solving site. The problem I've got with one site is I haven't got a direct link to the image. So when I download the captcha I just get a different image to what is still on the webpage. 
It there a way round this?   

http://buxvertise.com/modules/captcha/c ... hp?r=login

I' started with iMacros 3 days ago so apologies if this is a stupid question. Ive searched through older posts but couldnt find anything
regarding it.   

I'm using iMacros version 6.90 with internet explorer 8 on windows 7",https://forum.imacros.net/viewtopic.php?f=7&t=25638&sid=e8ccee866abcf29ebd57b704a290f61c,auto
2304,Infinite scroll,"Hello,

I want to scrap a site where results are dynamically generated with an infinite scroll (in ajax).
When I arrive on the page, there's only 20 results.
So if I make a TAG POS=30 it doesn't work.

I need to scroll manually the page, then the next results appears and then the TAG POS=30 work.

How can I succeed in scrolling the page automatically with imacros for Firefox ?
I have tried with url goto=javascript:window.scrollBy(0,20000) which I have found on a forum, but it doesn't work too.

Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=25611&sid=9d96e1268e5b272969ed509f53432633,auto
2305,"Tag a link in a specific div on facebook, LinkshimAsyncLink","Sorry for my english ...
VERSION BUILD=10022823 iMacro browser
       I have some difficult to tag a HREF on facebook page, am able to tag the div to open the post but that open all in the story line, the link of post is hide in a dom process (down key, mouse over and mouse clear reval Href)

i have try on extract txt, with imageclick work but very slow and make some lag

facebook code is complex


(Auto Image Post on the Div)
TAG POS=1 TYPE=DIV ATTR=CLASS:_6l-<SP>__c_

It's it possible make a tag similar this ?  But working lol or with apps id maybe

(Exemple of Expert Recording Extract mode)
TAG POS=1 TYPE=A ATTR=ONCLICK:LinkshimAsyncLink.swap(this,<SP>""https:\/\/www.facebook.com\/l.php?u=https\u00253A\u00252F\u00252Fvs-fb-php-p1.......)"");&&ONMOUSEOVER:LinkshimAsyncLink.swap(this,<SP>""https:\/\/vs-fb-php-p1........"");&&CLASS:_52c6&&STYLE:background-color:<SP>magenta;&&HREF:https://vs-fb-php-p1..........&&TARGET:_blank&&OLDSTYLE:magenta

(But the tag not work when i record in expert)",https://forum.imacros.net/viewtopic.php?f=7&t=25576&sid=9d96e1268e5b272969ed509f53432633,auto
2311,Extracting a First name from a LinkedIn Profile,"Hi - bit of  newb at this.

So I'm currently workiong a lot with LinkedIn for my job, and a guy at work suggested imacro as an easy and quick way to automateextracting contact detailsfrom LinkedIn, and save it as a file so ican use it to mailshot through Microsoft office. So far it works great - but...

I'm not trying to fix what ain't broken, but i started playing around, and trying to polish my method, and tried to get imacro to extract the first name from a Linked Profile page and save it with the corresponding email address (allowing me to personalise every email). I can do the FULL name with ease, but I'm admitting defeat with the first name extraction.

Here is what i have so far: 

VERSION BUILD=10.4.28.1074
TAB T=1
TAG POS=1 TYPE=SPAN ATTR=CLASS:full-name EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s.split(' ')[0];"")
SET !EXTRACT NULL
SAVEAS TYPE=VAR1 FOLDER=C:\Users\Macros FILE=Email.csv

This isn't working -   


If any of you lovely people could assist with this, i would be eternally grateful.",https://forum.imacros.net/viewtopic.php?f=7&t=25116&sid=7fd87bd5f8edabeec090c4a1f0d5b352,auto
2312,captcha,how may i fill captcha automatically,https://forum.imacros.net/viewtopic.php?f=7&t=24379&sid=7fd87bd5f8edabeec090c4a1f0d5b352,auto
2313,Yellow Pages macro Difficulty,"Hi I have scripted this macro in order to extract data from Yellow pages.
The idea is to generate a CSV file with the data from each individual record in any given search we do.
So far this is the code I have made.
VERSION BUILD=6021121     
TAB T=1     
TAB CLOSEALLOTHERS     
URL
GOTO=http://www.yellowpages.com/miami-fl/web ... eb+hosting

SET !EXTRACT_TEST_POPUP NO  
VERSION BUILD=6021121     
TAB T=1     
TAB CLOSEALLOTHERS     
URL
GOTO=http://www.yellowpages.com/miami-fl/web ... eb+hosting

TAG POS=1 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=2 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=3 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=4 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=5 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=6 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=7 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=8 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=9 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=10 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=11 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=12 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=13 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=14 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=17 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=18 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=20 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=19 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=1 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=1 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=1 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=1 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=1 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=1 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT TAG POS=1 TYPE=DIV ATTR=CLASS:info&&TXT:* EXTRACT=TXT SAVEAS TYPE=EXTRACT FOLDER=* FILE=yellowpages{{!NOW:yymmdd}}.csv
TAG POS=1 TYPE=A ATTR=TXT:Next

Now here are the problems i have with this script. 
FIrst of all the CSV file contains no formatting. There is no separation between the records making them much harder to analyze.
And the second problem is that each page in Yellowpages contains 30 records and I can only TAG 20 POS. After I get to 20 POSi get the following error Automatic POS detection failed (POS > 20). please try to specify the element further.

my questions are the following.
How can I make the macro format the CSV file so that I have one value in each line.
And is there any way aroun the 20 POS limitation.
Best Regards.",https://forum.imacros.net/viewtopic.php?f=7&t=10390&sid=7fd87bd5f8edabeec090c4a1f0d5b352,auto
2314,how to save webpage every 1 second time,"I am start whit this
Code: Select allVERSION  BUILD=7500718 RECORDER=FX
'Simple but effective macro to saves the current page
'
!VAR1 FILE=+_NoName_Time_{{!NOW:yyyymmdd_hhnnss}}
'
'Save the page    
SAVEAS TYPE=CPL FOLDER=* FILE={{!VAR1}} 

but i this code I need to click ok I will a code automatic save my page in CPL every 1 second...
please help me",https://forum.imacros.net/viewtopic.php?f=7&t=24938&sid=7fd87bd5f8edabeec090c4a1f0d5b352,auto
2315,leech <titles> of all given IPs/urls,"hello
i want to get all the titles of the IP:PORT i have
like if i put googleIP:port and imacros it runs the scripts open the browser (or not) and auto save the <title> Google </title> in a text file
now instead of googleIP:PORT i want ot use about a 1000 IP:port that i have and want all the titles to be saved in a text file like if i run 50 random ip:ports in imicros it runs all those 50 ips and at the end i have a text file with such format

ip1:port <title> xxx </title>
ip2:port <title> yyy </title>
ip3:port <title> zzz </title>

or any format, format doesnt matter

so can anyone tell me how to do that? maybe make me a template script..

please help me out..

really need help

Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=24951&sid=7fd87bd5f8edabeec090c4a1f0d5b352,auto
2316,imacros loop forward in csv to dowload file,"brand new to coding and imacros.

I'm having trouble using LOOP to move forward in a CSV file (address.csv) in iMacros. I have the following simple CSV: Column A is URL and Column B is Filename.

I would like to:

go to URL specified in Column A, Row 2 (Row 1 is header)
Download the image specified in Column B, Row 2.
Go to the next row (Row 3) download the image and continue down the list (LOOP)
Using the following code, I'm able to get the first image in row 2 but can't move forward to row 3, 4, 5 to get the rest of the files. Any advise would be greatly appreciated!

Here's the code I tried to hack together using the demos and a macro i recorded:

VERSION BUILD=10.4.28.1074
'Uses a Windows script to submit several datasets to a website, e. g. for filling an online database
TAB T=1     
TAB CLOSEALLOTHERS  
' Specify input file (if !COL variables are used, IIM automatically assume a CSV format of the input file
'CSV = Comma Separated Values in each line of the file
SET !DATASOURCE Address.csv
'Start at line 2 to skip the header in the file
SET !DATASOURCE_COLUMNS 2
SET !LOOP 2
'Increase the current position in the file with each loop 
SET !DATASOURCE_LINE {{!LOOP}}
VERSION BUILD=8340723 RECORDER=CR
URL GOTO={{!COL1}}
ONDOWNLOAD FOLDER=* FILE=+_{{!NOW:yyyymmdd_hhnnss}} WAIT=YES
TAG POS=1 TYPE=BUTTON FORM=ACTION:/download-photo/{{!COL2}} ATTR=TXT:Download<SP>hi-res<SP>photo
TAG POS=1 TYPE=A ATTR=TXT:*Back*",https://forum.imacros.net/viewtopic.php?f=7&t=24921&sid=7fd87bd5f8edabeec090c4a1f0d5b352,auto
2318,graphically represent a dynamic csv table,"I created an iMacro  code that generate a csv file containing the evolution of a basketball game points along the time :
minutes,2,4,6,8,10, ...
points ,3,18,24,38,.. 
I want a **real-time** way to  graphically represent  this table (x:time,y:points).
So whenever iMacro add data to the csv file (a column to  the table ),the chart get updated ***automatically*** .",https://forum.imacros.net/viewtopic.php?f=7&t=24649&sid=7fd87bd5f8edabeec090c4a1f0d5b352,auto
2320,Extract files from URL,"Hello!

I'm new to iMacros, I need to know if it is possible to:

1. Give a list of URLs, each one containing an undetermined number of files
2. Automatically download all the files displayed (.pdf) to a dedicated folder for each URL (from two to 10 files per URL)
3. Loop through all the URLs (about 14500)

And how to do it, of course!

Warmest regards!   

Pegaso",https://forum.imacros.net/viewtopic.php?f=7&t=24249&sid=030d300656bab0b18f7fde81bb00e1ce,auto
2322,Save as with loop method,"I need to download bunch of files (xlsx, docx, pdf, ppt..) on specific url's.. 
How can I save as files automatically on some location with loop method? Please respond, I couldn't find any solution anywhere else.

example url's:
http://***.***.com/attachmentid=2000000001 to http://***.***.com/attachmentid=2000010000

TAB T=1
SET !TIMEOUT 1
SET !ERRORIGNORE YES
SET !DATASOURCE E:\2000000000-2000010000.csv
SET !DATASOURCE_COLUMNS 1
SET !DATASOURCE_LINE {{!LOOP}}
WAIT SECONDS=1
URL GOTO={{!col1}}",https://forum.imacros.net/viewtopic.php?f=7&t=24023&sid=030d300656bab0b18f7fde81bb00e1ce,auto
2323,How stract specific id,"hello, i need extract specific id from html

<input name=""57f0c832c7fda1c2fed10b93295a879d"" value="""" class=""textCtrl"" id=""ctrl_57f0c832c7fda1c2fed10b93295a879d"" autofocus=""true"" autocomplete=""off"" type=""text""></input>

i need extract only id=""ctrl_57f0c832c7fda1c2fed10b93295a879d""

help please",https://forum.imacros.net/viewtopic.php?f=7&t=23989&sid=030d300656bab0b18f7fde81bb00e1ce,auto
2324,printing webpages in pdfs from links in csv file,"Hello, please I need your help ... 
I have a ""big"" excel file with a lot of links in column B, and I need to automate the printing of these web pages in pdf format.
This links are a collection of web pages containing my favourite kitchen recipes,   and I would like to save them on my hard disk.
Every pdf should be a single pdf, with the filename equal to the value I wrote in column A in Excel(the name of the recipe).
 Is it possible with iMacros?   I read a lot   on this forum about start macros from a csv file, but I'm a newbie here   and I couldn't find how to do this job.
Please could you tell me how can I do?
Thank you to everybody ",https://forum.imacros.net/viewtopic.php?f=7&t=23986&sid=030d300656bab0b18f7fde81bb00e1ce,auto
2326,How to extract the data from javascript,"I am little bit confuse to create code for how to extract the data from javascript.
Example:
www.amazon.com/dp/B00B4II5LS

Code
<script type=""text/javascript"">
maintainHeight = function(){
    var mainHolder = document.getElementById(""main-image-container"");
    var imgTagWrapperId = document.getElementById(""imgTagWrapperId"");
    if(mainHolder && typeof mainHolder != 'undefined'){
        var width = mainHolder.offsetWidth;
        var ratio = 1.0;
        var shouldAutoPlay = false;
        var videoSizes = [[275, 275], [275, 275], [275, 275], [275, 275]];

        var containerHeight = width/ratio;
        containerHeight = Math.min(containerHeight, 575);
        mainHolder.style.height = containerHeight + ""px"";
        if(imgTagWrapperId && typeof imgTagWrapperId !== 'undefined' ){
            imgTagWrapperId.style.height = containerHeight + ""px"";
        }

        var landingImage = document.getElementById(""landingImage"");
        var imageHeight = Math.min(containerHeight, 275);
        var imageWidth = Math.min(width, 275);
        if(landingImage){
            landingImage.style.maxHeight = imageHeight + ""px"";
            landingImage.style.maxWidth  = imageWidth + ""px"";
        }

        if(shouldAutoPlay){
            if(landingImage){
                var autoPlayWidth=0, autoPlayHeight=0;
                for(var i=0; i < videoSizes.length; i++){
                    if( autoPlayWidth < videoSizes[0] ){
                        autoPlayWidth = videoSizes[0];
                        autoPlayHeight = videoSizes[1];
                    }
                }
                if( autoPlayWidth > 0 && autoPlayHeight > 0 ){
                    var videoRatio = autoPlayWidth/autoPlayHeight;
                    // increasing height of the slate image by 1px (and width with the ratio) so that black lines doesn't appear on ipad on right side.
                    var isIpad = navigator.userAgent.match(/ipad/i);
                    if( isIpad ){
                        containerHeight = containerHeight + 1;
                    }
                    landingImage.style.width = containerHeight * videoRatio + ""px"";
                    landingImage.style.height = containerHeight + ""px"";
                }

            }
        }
    }
};
maintainHeight();

window.onresize = function(){
    maintainHeight();
};
P.when('A').register(""ImageBlockATF"", function(A){   
    var data = {
                'colorImages': { 'initial': [{""hiRes"":null,""thumb"":""http://ecx.images-amazon.com/images/I/4 ... sIRVtL.jpg"":[275,275]},""variant"":""MAIN""}]}, 
                'colorToAsin': {'initial': {}},
                'heroImage': {'initial': null},
                'holderRatio': 1.0,
                'holderMaxHeight': 575,
                'useStretchyImageFix': true
                };
    A.trigger('P.AboveTheFold'); // trigger ATF event.    
    return data;    
});
</script>
==================================
I want this line to extract:

 var data = {
                'colorImages': { 'initial': [{""hiRes"":null,""thumb"":""http://ecx.images-amazon.com/images/I/4 ... sIRVtL.jpg"":[275,275]},""variant"":""MAIN""}]}, 
                'colorToAsin': {'initial': {}},

Please anyone help me
Regards,",https://forum.imacros.net/viewtopic.php?f=7&t=23753&sid=030d300656bab0b18f7fde81bb00e1ce,auto
2328,extract data from multiple hidden input fields,"hi i am using firefox  imacros addon to automate some of my testing ,

i have several hidden input fields which are generated dynamically after i upload files.

For example when i upload 5 image files , 5 hidden input fields appear on page similar to this code below 
Code: Select all<input type=""hidden"" value=""file1.jpg"" name=""file1.jpg"">file1.jpg 

<input type=""hidden"" value=""file2.jpg"" name=""file2.jpg"">file2.jpg 

<input type=""hidden"" value=""file3.jpg"" name=""file3.jpg"">file3.jpg 

<input type=""hidden"" value=""file4.jpg"" name=""file4.jpg"">file4.jpg 

<input type=""hidden"" value=""file5.jpg"" name=""file5.jpg"">file5.jpg 


i want to extract all values or txt of these tags .i tried this code
Code: Select allVERSION BUILD=8881205 RECORDER=FX
TAB T=1
TAG POS=1 TYPE=INPUT:HIDDEN  ATTR=* EXTRACT=TXTALL
' This prompt is for showing the extracted text
PROMPT  Extracted<SP>data:{{!EXTRACT}}


But it is only showing txt of first input tag that is file1.jpg.Any idea how to solve this ?",https://forum.imacros.net/viewtopic.php?f=7&t=23849&sid=b07178a8376ad918bdc99d345a2f0b97,auto
2329,Frame / iFrame Problem,"Good morning everyone.

I'm trying to automate a search of the website cablemover.com to check a list of addresses to see when service becomes available for them.  The problem is that when I fill in the address and zipcode and click the ""See offers"" button it pops up a frame with a seemingly random name that has a ""loading"" type of picture, then the results frame after that.

What I'd like to be able to do is pass {{address}} and {{zipcode}} to a simple scraping macro to tell me if any results show up and what they are if any.  The problem is that I can't seem to figure out how to anticipate where to look for the results because of the frames.

Any suggestions?

Thanks very much

**VERSION BUILD=10.2.26.4235 btw",https://forum.imacros.net/viewtopic.php?f=7&t=23582&sid=b07178a8376ad918bdc99d345a2f0b97,auto
2330,Cancel IE Post-Download box,"Hello all,

first I would like to thank chivracq for what I've leanrt so far.

I'm pretty sure this issue is damn easy, but it's been 2 days and I starting being quite frustrated 

First of all:
Windows 7
IE9.0.20 (8112.16421)
iMacro V10.1.25.8883

I am trying to download multiple files.
But after the first download, iMacro waits 60s until it stops. Files are rather small, and take no time to download.
I also tried the Demo-download Macro, and the issue is similar, I don't reach the last webpage as the macro waits, once the download is finished.
If I click any of the button on the box IE pops once download is over, suggesting me to Open the file, the folder, or just cancel, the macro will go on.

I am pretty sure it is easy, but I can't figure it out...
Thank you,
Emile.

The code:
http://wiki.imacros.net/Demo-Download
VERSION BUILD=10.1.25.8883
URL GOTO=http://demo.imacros.net/Automate/Downloads
ONDOWNLOAD FOLDER=* FILE=* WAIT=YES
TAG POS=2 TYPE=A ATTR=TXT:*Download*
WAIT SECONDS=6
URL GOTO=http://demo.imacros.net/Automate/OK

[Edit]: files are csv, if it helps.",https://forum.imacros.net/viewtopic.php?f=7&t=23476&sid=b07178a8376ad918bdc99d345a2f0b97,auto
2331,Problematic PDF Downloads,"CIM
Imacros version 101258883
Windows 8.1 English
Imacros browser

As with many of the websites I use I cannot reveal the login data or source.

I'm trying to download a PDF, but of course these is no link to the PDF, and it appears that there is no way to get the URL of the PDF because the link seems to be dynamically generated.

I would think this was a somewhat common issue.

I read the thread found here: http://forum.imacros.net/viewtopic.php?f=6&t=2355

It basically says you might automate the download by preventing the browser from opening the PDF. But after attempting to follow the instructions (they are somewhat outdated), and researching on my own how to disable a PDF from opening in a browser, the pesky PDF still loads in the browser.

Has anyone figured out how to deal with this problem or similar? Thanks for your time.

If I could get just automate key strokes I could simply Ctrl+Shift+S to save the pdf from the browser, but that is not supported is it?

Thanks for your time.",https://forum.imacros.net/viewtopic.php?f=7&t=23514&sid=b07178a8376ad918bdc99d345a2f0b97,auto
2332,[HELP ME] Auto Register and Account Verification with Email,"Hello everybody  
i'm newbie , i need a help to write macro auto register and email verification (with email temporary)  on : https://launch.stellar.org/#/register
I use email temporary http://temp-mail.org/
I want to register with username numerical order . EX: example1, example2, example3 , .....
For captcha I can fill it manually   
Please help me all !!!   
Sorry for my bad english  
*Image Attachment : 
1. Registration form

2. Copy recovery code

3. Paste recovery code


  I using Windows 8.1 x64, Firefox 34.0, iMacros for Firefox 8.8.2",https://forum.imacros.net/viewtopic.php?f=7&t=23518&sid=b07178a8376ad918bdc99d345a2f0b97,auto
2334,Mozilla Update trouble,"Hi, my mozilla automatically updated to version 33.0 (x86 en-US). In this interim, my imacro that uses .cvs files as source to extraction is presenting this kind of error: ""Component returned failure code: 0x8000ffff (NS_ERROR_UNEXPECTED) [nsIPrefBranch.getIntPref], line 5""

Follows a summarized version of my imacro (the .cvs files used are ""siteorigem"" and ""sitesaida""):

VERSION BUILD=8820413 RECORDER=FX
SET !EXTRACT_TEST_POPUP NO
TAB T=1
'SET !ERRORIGNORE YES
SET !DATASOURCE X:\Users\Tecnica\INFLACAO\Precos\PassagensAereas\siteorigem.csv 
URL GOTO={{!COL1}} 
WAIT SECONDS=3
TAG POS=1 TYPE=SPAN ATTR=TXT:Pre莽o<SP>por<SP>adulto
TAG POS=R5 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
TAG POS=2 TYPE=SPAN ATTR=TXT:Pre莽o<SP>por<SP>adulto
TAG POS=R5 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
TAG POS=3 TYPE=SPAN ATTR=TXT:Pre莽o<SP>por<SP>adulto
TAG POS=R5 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
TAG POS=4 TYPE=SPAN ATTR=TXT:Pre莽o<SP>por<SP>adulto
TAG POS=R5 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
TAG POS=5 TYPE=SPAN ATTR=TXT:Pre莽o<SP>por<SP>adulto
TAG POS=R5 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
TAG POS=6 TYPE=SPAN ATTR=TXT:Pre莽o<SP>por<SP>adulto
TAG POS=R5 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
TAG POS=7 TYPE=SPAN ATTR=TXT:Pre莽o<SP>por<SP>adulto
TAG POS=R5 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
TAG POS=8 TYPE=SPAN ATTR=TXT:Pre莽o<SP>por<SP>adulto
TAG POS=R5 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
TAG POS=9 TYPE=SPAN ATTR=TXT:Pre莽o<SP>por<SP>adulto
TAG POS=R5 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
TAG POS=10 TYPE=SPAN ATTR=TXT:Pre莽o<SP>por<SP>adulto
TAG POS=R5 TYPE=SPAN ATTR=TXT:* EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=TXT:Seguinte<SP>禄
WAIT SECONDS=3
SET !DATASOURCE X:\Users\Tecnica\INFLACAO\Precos\PassagensAereas\sitesaida.csv 
SAVEAS TYPE=EXTRACT FOLDER=X:\Users\Tecnica\INFLACAO\Precos\PassagensAereas FILE={{!COL1}}


One important caveat, it all worked before this update. I tried to downgrade my current version, but there are so many previous versions, and probably the best thing to do is adequate the new version to my imacro.

Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=23390&sid=b07178a8376ad918bdc99d345a2f0b97,auto
2336,Is there a way to dump also the #EANF# when tags not found i,"Hi,

I am facing this issue in version 7 imacro not in 6.

When I am trying extract few tags from a website, I see that #EANF# is not saved in the output file when tags are not found. This was not the case in earlier version.

This is breaking the column definition that is built into my automation. Number of values in a row keeps changing based on the tags found. But what I need is to have #EANF# for non found tags so that my columns in the output file are fixed and known.

Is there a way to dump also the #EANF# when tags not found in version 7? 
Your help is greatly appreciated.

Regards,
- Palani",https://forum.imacros.net/viewtopic.php?f=7&t=23122&sid=b07178a8376ad918bdc99d345a2f0b97,auto
2338,Trouble Exporting from a Website,"Hello all,

     I've been using a iMacros to scrape a web table of times entered for quite some time now. The company has redesigned their page and now I'm having a lot of trouble getting iMacros to pull the data. The site is password protected so I will try and provide as much relevant information as possible. 

The site is a .aspx site and I've included a screenshot of the page for visual reference. The source code is quite long and I've added the <div> and <table> that my data is located in to the bottom of this post. I've put the data I want to extract in bold

I have been working with iMacros for Firefox and when trying to manually use the Extract command and locate the table with the TAG POS command and TYPE=TABLE, I can't find the data. Or rather I've tried up to POS=10 and I haven't gone far enough. In order to avoid blindly picking positions, I downloaded Firefox for IE to use the Table Extraction Wizard. When I click on my table the preview window shows that it has found my data and creates the command:

TAG POS=1 TYPE=DIV FORM=ID:aspnetForm ATTR=ID:ct100_cphMain_gvDaily_GridData EXTRACT=TXT

I manually added:
SAVEAS TYPE=EXTRACT FOLDER=* FILE=test.csv

When I open my test.csv file I have #EANF# 


When I enter Excel and use the Get External Data from web function, I have no trouble. I can use Excel directly but I would prefer to use iMacros for this task if possible. Please help!

***Updated Config Info:
Computer 1
The iMacros for Firefox is 8.8.1 running as an add-on in Firefox v 29 and I'm running on Windows 7 64 bit
For Internet Explorer I am running IMacros V10 on IE V 10.0.9 same OS.

Computer 2
iMacros for Firefox v. 8.8.1 on Firefox v28
iMacros on IE is Version 8 on IE 11.0.96
Windows 7 64bit


Source Code Snippet:

<div id=""ctl00_cphMain_gvDaily_GridData"" class=""rgDataDiv"" style=""overflow-x:auto;overflow-y:auto;width:100%;height:300px;"">

				<table class=""rgMasterTable rgClipCells rgClipCells"" id=""ctl00_cphMain_gvDaily_ctl00"" style=""border-width:0px;width:100%;table-layout:fixed;overflow:hidden;empty-cells:show;"">
					<colgroup>
		<col style=""width:41px"" />
		<col style=""width:40px"" />
		<col style=""width:40px"" />
		<col style=""width:40px"" />
		<col style=""width:130px"" />
		<col style=""width:100px"" />
		<col style=""width:100px"" />
		<col style=""width:110px"" />
		<col style=""width:110px"" />
		<col style=""width:130px"" />
		<col style=""width:100px"" />
		<col style=""width:250px"" />
		<col style=""width:250px"" />
		<col style=""width:250px"" />
		<col style=""width:150px"" />
		<col style=""width:100px"" />
		<col style=""display:none;"" />
		<col style=""display:none;"" />
		<col style=""display:none;"" />
	</colgroup>
<thead style=""display:none;"">
						<tr>
							<th scope=""col""></th>
						</tr>
					</thead><tbody>
					<tr class=""rgRow"" id=""ctl00_cphMain_gvDaily_ctl00__0"">
						<td class=""rgResizeCol"">&nbsp;</td><td valign=""top""><input id=""ctl00_cphMain_gvDaily_ctl00_ctl04_columnSelectCheckBox"" type=""checkbox"" name=""ctl00$cphMain$gvDaily$ctl00$ctl04$columnSelectCheckBox"" /></td><td valign=""top"">
                                                            <div style=""padding-top: 6px;"">
                                                                <input type=""image"" name=""ctl00$cphMain$gvDaily$ctl00$ctl04$imgbtnDailySubmitItem"" id=""ctl00_cphMain_gvDaily_ctl00_ctl04_imgbtnDailySubmitItem"" title=""Submit"" src=""../Images/Submit_32.png"" />
                                                            </div>
                                                        </td><td valign=""top"">
                                                            <div style=""padding-top: 6px;"">
                                                                <input type=""image"" name=""ctl00$cphMain$gvDaily$ctl00$ctl04$imgbtnDailyDeleteItem"" id=""ctl00_cphMain_gvDaily_ctl00_ctl04_imgbtnDailyDeleteItem"" title=""Delete"" src=""../Images/Delete_32.png"" />
                                                            </div>
                                                        </td><td valign=""top"">Alexander Irizarry</td><td valign=""top"">4/28/2014</td><td valign=""top"">Regular</td><td valign=""top"">08:00 AM</td><td valign=""top"">05:00 PM</td><td valign=""top"">9.0</td><td valign=""top"">Job</td><td valign=""top"">(1171) 1171-FANTASYLAND-BE OUR GUEST</td><td valign=""top"">&nbsp;</td><td valign=""top"">(B) LABOR</td><td valign=""top"">TEST</td><td valign=""top"">MIA FIELD</td><td style=""display:none;"">36639</td><td style=""display:none;"">17670</td><td style=""display:none;"">1171</td>
					</tr><tr class=""rgAltRow"" id=""ctl00_cphMain_gvDaily_ctl00__1"">
						<td class=""rgResizeCol"">&nbsp;</td><td valign=""top""><input id=""ctl00_cphMain_gvDaily_ctl00_ctl06_columnSelectCheckBox"" type=""checkbox"" name=""ctl00$cphMain$gvDaily$ctl00$ctl06$columnSelectCheckBox"" /></td><td valign=""top"">
                                                            <div style=""padding-top: 6px;"">
                                                                <input type=""image"" name=""ctl00$cphMain$gvDaily$ctl00$ctl06$imgbtnDailySubmitItem"" id=""ctl00_cphMain_gvDaily_ctl00_ctl06_imgbtnDailySubmitItem"" title=""Submit"" src=""../Images/Submit_32.png"" />
                                                            </div>
                                                        </td><td valign=""top"">
                                                            <div style=""padding-top: 6px;"">
                                                                <input type=""image"" name=""ctl00$cphMain$gvDaily$ctl00$ctl06$imgbtnDailyDeleteItem"" id=""ctl00_cphMain_gvDaily_ctl00_ctl06_imgbtnDailyDeleteItem"" title=""Delete"" src=""../Images/Delete_32.png"" />
                                                            </div>
                                                        </td><td valign=""top"">Alexander Irizarry</td><td valign=""top"">4/28/2014</td><td valign=""top"">Regular</td><td valign=""top"">05:00 PM</td><td valign=""top"">06:00 PM</td><td valign=""top"">1.0</td><td valign=""top"">Job</td><td valign=""top"">(1171) 1171-FANTASYLAND-BE OUR GUEST</td><td valign=""top"">&nbsp;</td><td valign=""top"">(B) LABOR</td><td valign=""top"">TEST</td><td valign=""top"">MIA FIELD</td><td style=""display:none;"">36640</td><td style=""display:none;"">17671</td><td style=""display:none;"">1171</td>
					</tr><tr class=""rgRow"" id=""ctl00_cphMain_gvDaily_ctl00__2"">
						<td class=""rgResizeCol"">&nbsp;</td><td valign=""top""><input id=""ctl00_cphMain_gvDaily_ctl00_ctl08_columnSelectCheckBox"" type=""checkbox"" name=""ctl00$cphMain$gvDaily$ctl00$ctl08$columnSelectCheckBox"" /></td><td valign=""top"">
                                                            <div style=""padding-top: 6px;"">
                                                                <input type=""image"" name=""ctl00$cphMain$gvDaily$ctl00$ctl08$imgbtnDailySubmitItem"" id=""ctl00_cphMain_gvDaily_ctl00_ctl08_imgbtnDailySubmitItem"" title=""Submit"" src=""../Images/Submit_32.png"" />
                                                            </div>
                                                        </td><td valign=""top"">
                                                            <div style=""padding-top: 6px;"">
                                                                <input type=""image"" name=""ctl00$cphMain$gvDaily$ctl00$ctl08$imgbtnDailyDeleteItem"" id=""ctl00_cphMain_gvDaily_ctl00_ctl08_imgbtnDailyDeleteItem"" title=""Delete"" src=""../Images/Delete_32.png"" />
                                                            </div>
                                                        </td><td valign=""top"">Alexander Irizarry</td><td valign=""top"">5/2/2014</td><td valign=""top"">Regular</td><td valign=""top"">07:00 PM</td><td valign=""top"">08:00 PM</td><td valign=""top"">1.0</td><td valign=""top"">Job</td><td valign=""top"">(1180) 1180-LATITUDE 39 INDIANAPOLIS</td><td valign=""top"">&nbsp;</td><td valign=""top"">(B) LABOR</td><td valign=""top"">&nbsp;</td><td valign=""top"">MIA FIELD</td><td style=""display:none;"">36641</td><td style=""display:none;"">17672</td><td style=""display:none;"">1180</td>
					</tr>
					</tbody>

				</table>	</div>",https://forum.imacros.net/viewtopic.php?f=7&t=22808&sid=7e74df639d022ccbaa0e92d884edcf30,auto
2339,hOw to Place an if Conditions in Macros,"heloo everyone i need to Place an if Conditions in iMacros of my Code if the Text "" Welcome to ORANGE AD SOLUTIONS"" is present in the Website mean the iMacros should stop automatically..I am using imacros in Firefox 31.0 and my Code is 
Code: Select allVERSION BUILD=8820413 RECORDER=FX
TAB T=1
URL GOTO=http://orangeadsolutions.com/
TAG POS=1 TYPE=H1 ATTR=TXT:Welcome<SP>to<SP>ORANGE<SP>AD<SP>SOLUTIONS 






The Text is Present in the webpage so it should stop the Imacros if it is not present mean Click out the Home Button like
Code: Select allTAG POS=1 TYPE=A ATTR=TXT:Home

Some Help me to Correct my Code   ",https://forum.imacros.net/viewtopic.php?f=7&t=22990&sid=7e74df639d022ccbaa0e92d884edcf30,auto
2341,Why do i have all extracted text in one row?,"Hi.
First, sorry for my bad english, but i hope you will understand what i mean.
I seach since hours for an answer of my problem, but i can麓t find something what helps me.

I have write a small macro (short version)
Code: Select allVERSION BUILD=10002738
TAB T=1
TAB CLOSEALLOTHERS

Set !DATASOURCE list.csv
SET !DATASOURCE_COLUMNS 1
SET !DATASOURCE_LINE {{!LOOP}}
SET !LOOP 1

URL GOTO=URL{{!COL1}}
TAG POS=1 TYPE=SPAN ATTR=CLASS:bld<SP>lrg<SP>red EXTRACT=TXT 
TAG POS=1 TYPE=SPAN ATTR=CLASS:lrg<SP>bold EXTRACT=TXT
TAG POS=1 TYPE=SPAN FORM=ACTION:URL{{!COL1}}_New EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=* 

the result is, that i have all text in one field, like this:
 EUR 17,34,""Der Alte - Collector's Box Vol. 10 (Folgen 161-175) [5 DVDs]"",""EUR 3,73""

I want the data in one row, but seperated in three columns.
must the script not do this automatically?

I hope someone can help me.



~~~EDIT~~~
I found out something.
When i start the Imacros Browser - Tools - Options, and check the box: Use regional settings in CSV file, it works fine.
But when i play the script in firefox, i have the same problem like before. In firefox i cant find the option to change for regional settings.


~~~EDIT 2~~~
I found a solution, not good, but it works.
Iam from Germany.
I have to change the speek options at the control panel to English (USA).
Than i can open the csv file and have all values in seperatet columns.",https://forum.imacros.net/viewtopic.php?f=7&t=22700&sid=7e74df639d022ccbaa0e92d884edcf30,auto
2345,Image recognition within a defined scope,"Hello,
I want to automate keystrokes on a virtual keyboard.
The virtual keyboard is easily recognized with the following:
IMAGECLICK POS=1 IMAGE=virtualKB.bmp CONFIDENCE=95
But things become more difficult when I try to recognize a key belonging to the keyboard.
My question:
Is it possible to conduct the recognition of a particular key by limiting  the scope of the search to the keyboard (previously recognized) in order to reduce the number of false found ?
Keys on the keyboard are randomly scattered.

Regards,
Claude Animo",https://forum.imacros.net/viewtopic.php?f=7&t=22398&sid=764a56e3c04defae9b390be083f6af7b,auto
2348,Important Progress iMacros Product Update,"Dear iMacros Customers and Users, 

We are writing to inform you that Progress will be officially discontinuing the Progress iMacros product, with a planned End-Of-Life date of November 30, 2023. 

What does this mean? 
Moving forward, Progress will continue to support the iMacros product and will only provide critical fixes as needed until the EOL date. We will honor all service agreements through the existing term, except for those contracts that go beyond the EOL date. Customers with service agreements that go beyond the EOL date will be contacted separately to discuss their options and next steps. We will continue to offer service agreement renewals that do not exceed the EOL date. 

What are your next steps? 
Please remain on the lookout for more communications from Progress in the coming months, as more information will be made readily available to assist you as you prepare your transition plans. 

If you have additional questions, please email iMacrosInquiries@progress.com 

On behalf of Progress, thank you for your continued partnership and commitment. 

Sincerely,  
The Progress iMacros Team",https://forum.imacros.net/viewtopic.php?f=20&t=32084&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2350,trim() not working,"FCI: (Added by Mod from User's Sig.)
My Config: iMacros Browser 11.5 DESKTOP; Windows 7

>>>

I'm extracting data that contains spaces, and so I use the trim() function to removes the spaces.

But It doesn't work!

Here's a sample code :
Code: Select allVERSION BUILD=11.5.499.3066
TAB T=1
TAB CLOSEALLOTHERS

SET !VAR0 <SP><SP><SP><SP><SP><SP>Hi<SP>
SET someVar EVAL(""var some='{{!VAR0}}'.trim(); some;"")
PROMPT {{someVar}}

The code above should work normally but it returns this error :
Error -1250: JScript statement in EVAL contains the following error: Objects of type 'System.String' do not have such a member. Line 6: SET someVar EVAL(""var some='{{!VAR0}}'.trim(); some;"")

JavaScript does have trim() function and it works on IE and all browsers!

UPDATE : Apparently It works on FireFox Add-on but not on the iMacros Desktop Browser. Why doesn't it work on the iMacros Browser, which is supposed to be superior than the free add-ons ?!!",https://forum.imacros.net/viewtopic.php?f=7&t=27582&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2351,How to trim whitespace with EVAL,"Blakero wrote: 鈫慒ri Dec 02, 2022 5:50 am
Hello,
I'm extracting data that contains spaces, and so I use the trim() function to removes the spaces.
But It doesn't work!
Here's a sample code : 
Code: Select allVERSION BUILD=11.5.499.3066
TAB T=1
TAB CLOSEALLOTHERS

SET !VAR0 <SP><SP><SP><SP><SP><SP>Hi<SP>
SET someVar EVAL(""var some='{{!VAR0}}'.trim(); some;"")
PROMPT {{someVar}}

The code above should work normally but it returns this error :
Error -1250: JScript statement in EVAL contains the following error: Objects of type 'System.String' do not have such a member. Line 6: SET someVar EVAL(""var some='{{!VAR0}}'.trim(); some;"")
JavaScript does have trim() function and it works on IE and all browsers!


Hi Blakero,

Support for the trim() function is dependent on the version of the Javascript engine in use. It looks like you recorded your macro with an older version of the iMacros browser, and hence trim() is not available. However, you can use the replace() function with a regular expression to achieve the same result. An example is provided under the Manipulate extraction section on the EVAL command reference page.",https://forum.imacros.net/viewtopic.php?f=7&t=32106&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2353,Select a drowdonw value,"imacro 10.1.0.1485 / windows 10 / CHrome 107.0.5304.107


Hello,

i'm trying to select a value in a drowdown list but nothing change (sorry for my bad english 

I've read other post but it's beyond my strength

Here the code




I'm beginner and try several combinaison like TAG POS=1 TYPE=select-options ATTR=part:dropdown-option2&value:3 but 

If someone can give me a track to follow 

Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=32098&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2354,How to extract once every few loops?,"Hello all
My script extracts once every 60 loops, 1,60,120,180 .. etc TAG POS=1, so I use made this with EVAL.

But my problem is that {{Datastart}} is empty when no value to extract.

My question is how to make {{Datastart}} EVAL to work like {{1datastart}}? I mean to remain the first extract value from loop1 during the next 59 loops and to extract the value again at loop 61?
Code: Select allSET !ERRORIGNORE YES
SET 1datastart EVAL(""var n='{{!LOOP}}'; var z; if((n%60)==1){z=\""1\"";} else{z=0;}; z;"")
TAG POS={{1datastart}} TYPE=DIV ATTR=CLASS:""*cell-body tablecell-date sortable column-sorted*""&&TXT:*,* EXTRACT=TXT
SET datastart EVAL(""'{{!EXTRACT}}'.replace(/(Scheduled|Published)/g, '').trim();"")
Prompt {{datastart}}


I am using (FCI):
Code: Select alliMacros for CR v10.1.1 'PE', CR v105.0.5195.102 (_x64), Win10_x64. ('CR' = 'Chrome' / 'PE' = 'Personal Edition')
",https://forum.imacros.net/viewtopic.php?f=7&t=32073&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2355,Help with XPath,"I'm relatively new to iMacros and identifying XPaths is proving a problem. Some I am able to do either inspecting the element in Chrome or using the XPath Helper extension. However, this particular one is proving a problem and I would appreciate some help.

Page: https://www.achurchnearyou.com/church/3247/find-us/

I want to simply obtain the navlist__item-title from here:
Code: Select all<div class=""navlist__item-title"">St George's Church</div>

So how do I achieve this so I can include the XPath in a TAG command?
This is my failed attempt: 
Code: Select allTAG XPATH="".//div[@class='navlist__item-title']"" EXTRACT=TXT

Thanks.",https://forum.imacros.net/viewtopic.php?f=7&t=32060&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2356,Write YES or No in CVS if script runs properly,"VERSION BUILD=1011 TRIAL VERSION
Windows 10 64 bit
Chrome Version 106.0.5249.103

I've been using this platform for more than a week and have already created some fantastic scripts that make my job much easier. iMacros runs flawlessly, but the website we work on occasionally doesn't, thus I'm wondering if there's a method we can export if the loop was correctly completed or not and then export it to CSV or Directly to Excel so when loop is finished then we can check which Loop number we should do manual because it was not executed properly.

I'll use the same script I did in my most recent post. viewtopic.php?f=7&p=89250#p89250
Code: Select allTAG POS=2 TYPE=DIV ATTR=TXT:Visit
TAG POS=1 TYPE=A ATTR=TXT:Clickhere
TAB T=2
TAB T=1
TAG POS=1 TYPE=SPAN ATTR=TXT:Back
TAG POS=2 TYPE=DIV ATTR=TXT:Visit
TAG POS=1 TYPE=A ATTR=TXT:Clickhere
TAB T=2
TAB T=1
TAB CLOSEALLOTHERS

To help you understand what I'm trying to accomplish, I've attached the screenshot below. It indicates that if lines 2 and 7 were properly run, the spreadsheet's row labeled ""LOOP 1"" should display ""YES,"" and if not, ""NO."" By this, after the loop was done we can see which one should we check back again and do manually the work instead of checking then one by one on the website and check if it done properly.",https://forum.imacros.net/viewtopic.php?f=7&t=32056&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2357,How to use iMacros to add HTML tag values in new line or Edit existing tag,"Google Chrome: Version 105.0.5195.102 (Official Build) (64-bit)
iMacros: 2021
System: Windows 10 Pro 21H2 19044.1949

There's a dropdown selection box with multiple preset tag values for user selection. However, I need a tag value with my own range value=""30.0 To 70.0"">30.0 To 70.0 which is not available.

How can I add a new tag value (new line) OR edit the existing to modify my required range by using iMacros Browser.

I've read a few similar post & solutions by Guru Shugar but those are for adding ""value"" into an empty field and another is to remove attribute ""disabled"" which is not what i required.

Any help will be much appreciated. Thanks. 
Code: Select all	<p>Score PT</p>
			</p>
                           <select name=""ExpScorePT"">
                                <option class=""_def"" value=""-Any-"">-Any-</option>
                                <option value=""Above 5.0"">Above 5.0</option>
                                <option value=""Above 10.0"">Above 10.0</option>
                                <option value=""Above 20.0"">Above 20.0</option>
                                <option value=""Above 30.0"">Above 30.0</option>
                                <option value=""Above 50.0"">Above 50.0</option>
                                <option value=""Above 70.0"">Above 70.0</option>
                                <option value=""Above 90.0"">Above 90.0</option>
                                <option value=""Above 120.0"">Above 120.0</option>
                                <option value=""5.0 To 20.0"">5.0 To 20.0</option>
                                <option value=""20.0 To 50.0"">20.0 To 50.0</option>
                                <option value=""50.0 To 100.0"">50.0 To 100.0</option>
                                <option value=""Below 5.0"">Below 5.0</option>
                                <option value=""Below 10.0"">Below 10.0</option>
                                <option value=""Below 20.0"">Below 20.0</option>
                                <option value=""Below 50.0"">Below 50.0</option>
                                <option value=""Below 70.0"">Below 70.0</option>
                                <option value=""Below 90.0"">Below 90.0</option>
                                <option value=""Below 120.0"">Below 120.0</option>
                        </select>
                    </p>  ",https://forum.imacros.net/viewtopic.php?f=7&t=32046&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2360,How to extract dynamic table rows with loop & stop when reach a specific condition,"Google Chrome: Version 105.0.5195.102 (Official Build) (64-bit)
iMacros: 2021 
System: Windows 10 Pro 21H2 19044.1949

With Guru chivracq kind guidance from my earlier post viewtopic.php?f=7&t=32028, i'm able to extract required data with unique element style=""background-color: orange;"" by using 'Double Relative Positioning' instead of using Xpath.

Since the data table is dynamic with 15mins refresh interval, the total of rows will change from each refresh. I've tried to search for solution & study eval() from the forum posts (especially Guru chivracq solutions) but i'm still not able to grasp the technic somehow   

Here again, I hope to have more guidance to find the solution. 

Questions:-
1. Is there a way to make a loop instead of the way i'm using as per code below eg. 'Extract 1st rows, 'Extract 2nd rows and so on?
2. The extraction should be stopped when reached <tr class=""montage_atm_bottom even"", how can i achieve this?

My current code:-
Code: Select allSET !EXTRACT NULL
TAG POS=1 TYPE=TD ATTR=STYLE:""background-color: orange;""
TAG POS=R-1 TYPE=* ATTR=* 


'Extract 1st rows
TAG POS=R1 TYPE=TD ATTR=CLASS:descriptorcell
TAG POS=R1 TYPE=A ATTR=CLASS:montagelink EXTRACT=TXT
TAG POS=R1 TYPE=TD ATTR=CLASS:rightcelltd<SP>volcell EXTRACT=TXT
TAG POS=R1 TYPE=TD ATTR=CLASS:rightcelltd<SP>volcell EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop FILE=test_{{!NOW:yymmdd_hhnn}}.csv

'Extract 2nd rows
TAG POS=R1 TYPE=TD ATTR=CLASS:descriptorcell
TAG POS=R1 TYPE=A ATTR=CLASS:montagelink EXTRACT=TXT
TAG POS=R1 TYPE=TD ATTR=CLASS:rightcelltd<SP>volcell EXTRACT=TXT
TAG POS=R1 TYPE=TD ATTR=CLASS:rightcelltd<SP>volcell EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop FILE=test_{{!NOW:yymmdd_hhnn}}.csv

'Extract 3rd rows
TAG POS=R1 TYPE=TD ATTR=CLASS:descriptorcell
TAG POS=R1 TYPE=A ATTR=CLASS:montagelink EXTRACT=TXT
TAG POS=R1 TYPE=TD ATTR=CLASS:rightcelltd<SP>volcell EXTRACT=TXT
TAG POS=R1 TYPE=TD ATTR=CLASS:rightcelltd<SP>volcell EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop FILE=test_{{!NOW:yymmdd_hhnn}}.csv

'Extract 4th rows...
'Extract 5th rows...

",https://forum.imacros.net/viewtopic.php?f=7&t=32030&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2361,Xpath code to extract specific element in nested TR and TD,"Good day everyone. I need guidance to extract a dynamic data from a table with multi-tier nested TR and TD which can be found in tbody section.

Fortunately, my required data can be identified with a unique element style=""background-color: orange;""

I've tried with the followings but resulted with ""EANF"".

1. TAG XPATH=""//*[@id=""opt_montage_table""]/tbody//tr//td[@class='<SP>descriptorcell':[@style='background-color:<SP>orange;]'/div/span/a""] EXTRACT=TXT

2. TAG XPATH=""//*[@id=""opt_montage_table""]/tbody//tr//td[contains(@style,'background-color:<SP>orange;)]'/div/span/a"" EXTRACT=TXT

3. TAG XPATH=""//*[@id=""opt_montage_table""]/tbody/*[contains(@style,'background-color:<SP>orange;)]'/div/span/a"" EXTRACT=TXT

Any advise and guidance will be much appreciated. Thank you.

",https://forum.imacros.net/viewtopic.php?f=7&t=32028&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2362,-1000 Index is out of range.,"Hi. Pls help me. 

I want to extract some data in URL by loop. 
Loop is working well from 1st time to 60~66th times. 
But in 67th time, -1000 index error pop up as below. 
This error bother me.  

## error message ## 
2:33:19 AM: Error: ""-1000: Unspecified"" at line 40
SAVEAS TYPE=EXTRACT FOLDER=D: FILE=IKEA_PRODUCT_INFO1.csv
The index is out of range. The index must be non-negative and must be less than the size of the collection.
Parameter name: index


## My code ##
VERSION BUILD=2021.0
TAB T=1
TAB CLOSEALLOTHERS
SET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP NO
SET WAIT_TEMP 1

SET !LOOP 1
SET !DATASOURCE URL.csv
SET !DATASOURCE_LINE {{!LOOP}}

URL GOTO={{!COL1}}
WAIT SECONDS={{WAIT_Temp}}

EVENT TYPE=CLICK SELECTOR=""#pip-product-information-section>div:nth-child(1)>button"" BUTTON=0
WAIT SECONDS={{WAIT_Temp}}

EVENT TYPE=CLICK SELECTOR=""#product-details-compliance>div:nth-child(1)>button>span>span"" BUTTON=0
WAIT SECONDS={{WAIT_Temp}}

TAG SELECTOR=""#SEC_product-details-compliance>div:nth-child(1)>p"" EXTRACT=TXT
TAG SELECTOR=""#SEC_product-details-compliance>div:nth-child(6)>p"" EXTRACT=TXT
TAG SELECTOR=""#SEC_product-details-compliance>div:nth-child(7)>p"" EXTRACT=TXT
TAG SELECTOR=""#SEC_product-details-compliance>div:nth-child(8)>p"" EXTRACT=TXT
TAG SELECTOR=""#SEC_product-details-compliance>div:nth-child(12)>p"" EXTRACT=TXT
TAG SELECTOR=""#SEC_product-details-compliance>div:nth-child(13)>p"" EXTRACT=TXT
WAIT SECONDS={{WAIT_Temp}}

TAG POS=1 TYPE=link ATTR=rel:canonical EXTRACT=href
WAIT SECONDS={{WAIT_Temp}}

SAVEAS TYPE=EXTRACT FOLDER=D: FILE=RESULT.csv
WAIT SECONDS={{WAIT_Temp}}


## URL ##
https://www.ikea.com/kr/ko/p/lixhult-ca ... -60476518/
https://www.ikea.com/kr/ko/p/lixhult-ca ... -00476521/
https://www.ikea.com/kr/ko/p/rashult-tr ... -50445990/
https://www.ikea.com/kr/ko/p/rashult-tr ... -90490136/
https://www.ikea.com/kr/ko/p/raskog-tro ... -00466961/
https://www.ikea.com/kr/ko/p/hoegsma-ch ... -10425611/
https://www.ikea.com/kr/ko/p/hoegsma-ch ... -90425612/
https://www.ikea.com/kr/ko/p/langsted-r ... -00495176/
https://www.ikea.com/kr/ko/p/toftlund-r ... -20481017/
https://www.ikea.com/kr/ko/p/stavreby-d ... -60481015/
https://www.ikea.com/kr/ko/p/regnskur-p ... -40482638/
https://www.ikea.com/kr/ko/p/regnskur-p ... -10437949/
https://www.ikea.com/kr/ko/p/spiken-led ... -90469149/
https://www.ikea.com/kr/ko/p/skurup-pen ... -70489520/
https://www.ikea.com/kr/ko/p/skottorp-l ... -10509564/
https://www.ikea.com/kr/ko/p/ryet-led-b ... -20447617/
https://www.ikea.com/kr/ko/p/ryet-led-b ... -90447628/
https://www.ikea.com/kr/ko/p/ryet-led-b ... -40438725/
https://www.ikea.com/kr/ko/p/strafly-ch ... -10479208/
https://www.ikea.com/kr/ko/p/justina-ch ... -20472824/
https://www.ikea.com/kr/ko/p/malinda-ch ... -50479188/
https://www.ikea.com/kr/ko/p/kapkrusbae ... -20476817/
https://www.ikea.com/kr/ko/p/borstad-sp ... -00461218/
https://www.ikea.com/kr/ko/p/buskbo-pla ... -90455374/
https://www.ikea.com/kr/ko/p/stuk-box-w ... -40474426/
https://www.ikea.com/kr/ko/p/stuk-box-w ... -00474433/
https://www.ikea.com/kr/ko/p/stuk-box-w ... -10474437/
https://www.ikea.com/kr/ko/p/stuk-box-w ... -10474442/
https://www.ikea.com/kr/ko/p/stuk-box-w ... -20474446/
https://www.ikea.com/kr/ko/p/spantad-va ... -60488984/
https://www.ikea.com/kr/ko/p/jaettelik- ... -60490091/
https://www.ikea.com/kr/ko/p/tjog-stora ... -20477666/
https://www.ikea.com/kr/ko/p/tjog-stora ... -50474609/
https://www.ikea.com/kr/ko/p/tjog-stora ... -00473914/
https://www.ikea.com/kr/ko/p/tjog-stora ... -40474619/
https://www.ikea.com/kr/ko/p/tjog-stora ... -60477669/
https://www.ikea.com/kr/ko/p/dvala-fitt ... -60482449/
https://www.ikea.com/kr/ko/p/dvala-fitt ... -60482454/
https://www.ikea.com/kr/ko/p/gurli-cush ... -60479183/
https://www.ikea.com/kr/ko/p/tromma-wal ... -60454291/
https://www.ikea.com/kr/ko/p/taggad-wal ... -80466293/
https://www.ikea.com/kr/ko/p/plira-alar ... -10466296/
https://www.ikea.com/kr/ko/p/mala-scissors-70477616/
https://www.ikea.com/kr/ko/p/mala-apron ... -10485351/
https://www.ikea.com/kr/ko/p/boerja-tra ... -00213884/
https://www.ikea.com/kr/ko/p/boerja-fee ... -20199288/
https://www.ikea.com/kr/ko/p/fabler-3-p ... -10158189/
https://www.ikea.com/kr/ko/p/annons-pot ... -50298475/
https://www.ikea.com/kr/ko/p/annons-5-p ... -30207400/
https://www.ikea.com/kr/ko/p/aptitlig-c ... -40233427/
https://www.ikea.com/kr/ko/p/aptitlig-b ... -60233431/
https://www.ikea.com/kr/ko/p/proppmaett ... -60300350/
https://www.ikea.com/kr/ko/p/mopsig-16- ... -80343004/
https://www.ikea.com/kr/ko/p/ikea-365-o ... -30286732/
https://www.ikea.com/kr/ko/p/ikea-365-o ... -60286735/
https://www.ikea.com/kr/ko/p/vardagen-o ... -90289313/
https://www.ikea.com/kr/ko/p/vardagen-o ... -30289306/
https://www.ikea.com/kr/ko/p/korken-jar ... -10213548/
https://www.ikea.com/kr/ko/p/vardagen-j ... -20291927/
https://www.ikea.com/kr/ko/p/vardagen-j ... -60291925/
https://www.ikea.com/kr/ko/p/vardagen-j ... -80291929/
https://www.ikea.com/kr/ko/p/dyrgrip-re ... -00309301/
https://www.ikea.com/kr/ko/p/dyrgrip-gl ... -10309305/
https://www.ikea.com/kr/ko/p/foersiktig ... -00300206/
https://www.ikea.com/kr/ko/p/godis-glas ... -40174588/
https://www.ikea.com/kr/ko/p/godis-glas ... -20137846/
https://www.ikea.com/kr/ko/p/ikea-365-g ... -90278362/
https://www.ikea.com/kr/ko/p/ivrig-glas ... -30258324/
https://www.ikea.com/kr/ko/p/mjoed-beer ... -40179604/
https://www.ikea.com/kr/ko/p/oanvaend-b ... -10242032/
https://www.ikea.com/kr/ko/p/pokal-snap ... -60137849/
https://www.ikea.com/kr/ko/p/reko-glass ... -20137851/
https://www.ikea.com/kr/ko/p/svalka-win ... -40137812/
https://www.ikea.com/kr/ko/p/svalka-cha ... -80137805/
https://www.ikea.com/kr/ko/p/grunka-4-p ... -50174465/
https://www.ikea.com/kr/ko/p/direkt-3-p ... -60295400/
https://www.ikea.com/kr/ko/p/fullaendad ... -20393087/
https://www.ikea.com/kr/ko/p/fullaendad ... -60393090/
https://www.ikea.com/kr/ko/p/fullaendad ... -60392986/
https://www.ikea.com/kr/ko/p/fullaendad ... -00392994/
https://www.ikea.com/kr/ko/p/ikea-365-h ... -50161590/
https://www.ikea.com/kr/ko/p/ikea-365-h ... -20161582/
https://www.ikea.com/kr/ko/p/ikea-365-h ... -00161644/
https://www.ikea.com/kr/ko/p/ikea-365-h ... -50161585/
https://www.ikea.com/kr/ko/p/idealisk-f ... -50174516/
https://www.ikea.com/kr/ko/p/koncis-tur ... -40225960/
https://www.ikea.com/kr/ko/p/koncis-can ... -80166124/
https://www.ikea.com/kr/ko/p/staem-pota ... -40233253/
https://www.ikea.com/kr/ko/p/vardagen-b ... -40309827/
https://www.ikea.com/kr/ko/p/vardagen-c ... -70309816/
https://www.ikea.com/kr/ko/p/ikea-365-v ... -50175139/
https://www.ikea.com/kr/ko/p/ikea-365-v ... -60163602/
https://www.ikea.com/kr/ko/p/vardagen-p ... -40294717/
https://www.ikea.com/kr/ko/p/vardagen-c ... -60294721/
https://www.ikea.com/kr/ko/p/vardagen-c ... -20294723/
https://www.ikea.com/kr/ko/p/voerda-par ... -90289266/
https://www.ikea.com/kr/ko/p/voerda-uti ... -90289247/
https://www.ikea.com/kr/ko/p/voerda-cle ... -40289160/
https://www.ikea.com/kr/ko/p/voerda-coo ... -00289237/
https://www.ikea.com/kr/ko/p/voerda-chi ... -80289163/


## I installed below. ##
iMacros Professional Edition
iMacros Browser 2021.0
Windows10 Pro-64 (v.19044.1706)


Pls help me.
Thank you!

Jandi Lee",https://forum.imacros.net/viewtopic.php?f=7&t=31952&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2364,"I need to keep this site ""Open"" all time, please help!","Good day!

I'm working as a Manager in a restaurant. We need to monitor the availability of the restaurant to the customers since some of our employees, are turning it off or making it ""Busy"", even if we don't really have customers. So long story short, Google led me to iMacros, the problem is, my macro is missing something. It can ""Open"" our restaurant, the script is working, but I need it to just ignore if we're already ""Open"", so do not do something else, just ""Open"" if we are ""Busy"". The error is displayed like this:

RuntimeError: element SPAN specified by TXT:reopen was not found, line: 4

I want it to run indefinitely, so when someone sets our restaurant to ""Busy"", the macro will just press ""Reopen"" itself.

Here's what is inside my macro:

VERSION BUILD=1011 RECORDER=CR
URL GOTO=https://private.portal.restaurant/vendo ... ability-tb
TAG POS=1 TYPE=SPAN ATTR=TXT:Update
TAG POS=1 TYPE=SPAN ATTR=TXT:reopen
WAIT SECONDS=30
SET !ERRORIGNORE YES

Please see the images I attached, and I am hoping that someone will help me with my issue.

Sincerely,
Arwin T.",https://forum.imacros.net/viewtopic.php?f=7&t=31917&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2365,Code to Extract selected values in Dropdown menu,"Hi.
I am new to macro.

My url is as mentioned below.

https://sme.policybazaar.com/GHIPlans?e ... 1LUT09&v=2

I wanted to extract the the values already selected in drop down menu. (please note, I wanted to extract already selected value. I dont want to select the any new value) 

i.e. Value selected under Drop down menu 

1) ""Sum insured""
2) ""Maternity Benefits""
3) ""Room Rent Limits""
4) ""Pre Existing Disease""
Code: Select allVERSION BUILD=1011 
URL GOTO=https://sme.policybazaar.com/GHIPlans?enquiryid=QnQwMmVQcXNOYkp1QlBObXl4VE1LUT09&v=2
TAG SELECTOR=""#root>DIV>MAIN>DIV>DIV>DIV:*>DIV>DIV>DIV>DIV>svg>path"" EXTRACT=TXT
鈥楾AG SELECTOR=""#mb""
",https://forum.imacros.net/viewtopic.php?f=7&t=31909&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2372,I can't download pictures,"Hi everyone.
I'm using the free version of iMacros but I will purchase the full version if I can download pictures thanks to it.

The code is really simple, but it keeps says ""Error: only one ONDOWNLOAD command should be used for each download""

VERSION BUILD=1011 RECORDER=CR
ONDOWNLOAD FOLDER=* FILE=+_{{!NOW:yyyymmdd_hhnnss}} WAIT=YES
URL GOTO=https://mtgaddict.net/oracle/en/cs-core ... rit-dragon
BACK
ONDOWNLOAD FOLDER=* FILE=+_{{!NOW:yyyymmdd_hhnnss}} WAIT=YES

I need to download the picture, rename it as 1 then 2 then 3 etc.

Any help will be much appreciated.

Kind regards,
Seb",https://forum.imacros.net/viewtopic.php?f=7&t=31830&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2373,Check if a CSS Selector exists on a webpage,"Hi all,
Preface:
-	I鈥檓 a beginner at coding, so your patience is appreciated   
-	Apologies if this isn鈥檛 the right forum group in which to post this question. I wasn鈥檛 quite sure where to put it.
-	I鈥檝e been looking for the answer to this question for several hours last night and this morning and couldn鈥檛 find it. If this type of question is already answered, I apologize for creating a redundant thread and would appreciate the link.
-	I tried very hard to include all the information required, as mentioned in the 鈥淧lease read this before reporting problems鈥?thread. Please go easy on me if I neglected to include some important information. I鈥檒l happily provide anything else that鈥檚 needed.

iMacros v10.1.0 for FF, 'Free', FF94 (v94.0.1 (64-bit)), Win10_ENG.

I want my script to be able to check to see if there鈥檚 a specific CSS Selector on a webpage. If it doesn鈥檛 exist, I want the script to continue. If it does exist, I want the script to extract the HREF/URL link from the selector, open up a new tab, go to the URL extracted from the selector, and then close the tab and return to the first tab.

The CSS selector in question is:
Code: Select allbody > div:nth-child(15) > a:nth-child(1)

I wrote this code out:
Code: Select allSET !ERRORIGNORE YES
SET !TIMEOUT_STEP 1
TAG SELECTOR=""body > div:nth-child(15) > a:nth-child(1)"" EXTRACT=HREF
TAB OPEN
TAB T=2
URL GOTO={{!EXTRACT}}
WAIT SECONDS=1
TAB CLOSE
SET !EXTRACT NULL
TAB T=1

The problem with this code is that if the selector doesn鈥檛 exist, the script will open a new tab and then freeze (even though I have SET !ERRORIGNORE YES), presumably because it doesn鈥檛 know what to do with !EXTRACT, which is NULL. This is why I need the script to be able to check if the selector is on the page (unless there's another solution of which I'm not aware).

Additionally, I鈥檓 running the free version of iMacros, which only allows for three custom variables. I鈥檓 already using three variables to generate a random number earlier in the script:
Code: Select allSET !VAR1 1
SET !VAR2 3
SET !VAR3 EVAL(""var randomNumber=Math.floor(Math.random()*\""{{!VAR2}}\"" +\""{{!VAR1}}\""); randomNumber;"")

Is it possible to perform the check for the CSS selector without using any variables? If not, is it possible to generate a random number without the three variables so I can then use them for the check?

Thank you for reading and for your help.  ",https://forum.imacros.net/viewtopic.php?f=7&t=31816&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2374,Data extraction problem in tabular format imacros,"Hi.
I am using firefox 48
imacros 8.9.7
Windows 10 64 bit OS

I am using the below code to extract data 
Code: Select allVERSION BUILD=8970419 RECORDER=FX
TAB T=1
URL GOTO=https://www.nseindia.com/option-chain
TAG POS=1 TYPE=TH ATTR=TXT:CALLS
TAG POS=2 TYPE=DIV ATTR=TXT:CALLSPUTS<SP>OI<SP>Chng<SP>in<SP>OI<SP>Volume<SP>IV<SP>LTP* EXTRACT=TXT

But after extraction I am not getting the data in table format. is there any solution ?",https://forum.imacros.net/viewtopic.php?f=7&t=31796&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2377,Extract Last Word,"Firefox 52.9.0 (32-bit)
iMacros 8.9.7
Win-10 (64-bit)

Hi,
    I try open the link  by EVENT mode, then extract the two positions of  last  word and combine together save it as html page in specified folder.. But it saved file name as undefined__undefined.html. 

i want to replace the ""("" and "")"" of the 1st position of extracting  last word . 
 I don't idea of these...( I am gathering code from in Data Extraction and Web Screen Scraping  Forum page  as i need )  Please guide me.
Code: Select allVERSION BUILD=8970419 RECORDER=FX
TAB T=1

'URL GOTO=https://mnregaweb2.nic.in/netnrega/FTO/fto_sign_detail.aspx?lflag=local&flg=W&page=b&state_name=%e0%ae%a4%e0%ae%ae%e0%ae%bf%e0%ae%b4%e0%af%8d%e0%ae%a8%e0%ae%be%e0%ae%9f%e0%af%81&state_code=29&district_name=%e0%ae%a4%e0%ae%bf%e0%ae%b0%e0%af%81%e0%ae%b5%e0%ae%a3%e0%af%8d%e0%ae%a3%e0%ae%be%e0%ae%ae%e0%ae%b2%e0%af%88&district_code=2906&block_name=Thellar&block_code=2906015&fin_year=2021-2022&typ=fst_sig&mode=b&source=&Digest=h6f9h6YyIpuMzlkCjaGneQ
SET !LOOP 2
EVENT TYPE=CLICK SELECTOR=""#form1>DIV:nth-of-type(3)>TABLE:nth-of-type(4)>TBODY>TR:nth-of-type({{!LOOP}})>TD:nth-of-type(2)>A"" BUTTON=0 MODIFIERS=""ctrl""

TAB T=2
SET !EXTRACT_TEST_POPUP NO

SET !EXTRACT NULL
' Extract Panchayat Name
'TAG POS=1 TYPE=TD ATTR=TXT:TN-06-015-043-043/559-A<SP>(Sathapoondi) EXTRACT=TXT
TAG POS=1 TYPE=TD ATTR=TXT:TN-06-015-* EXTRACT=TXT
SET Pt_name {{!EXTRACT}}

SET Pt_name EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.indexOf('('); y=s.substr(x);if(x<0){z='No Name';} else{z=y;}; z;"")

PROMPT {{Pt_name}}

SET !EXTRACT NULL
' Extract FTO No.
'TAG POS=1 TYPE=B ATTR=TXT:Fto<SP>No.<SP>:<SP>TN2906015_010421FTO_1238 EXTRACT=TXT
TAG POS=1 TYPE=B ATTR=TXT:Fto<SP>No.<SP>:<SP>TN2906015_* EXTRACT=TXT

SET Fto_no EVAL(""var s='{{!EXTRACT}}'; var x,y,z; x=s.indexOf('T'); y=s.substr(x); if(x<0){z='No FTO';} else{z=y;};z;"")
PROMPT {{Fto_no}}

SAVEAS TYPE=HTM FOLDER=E:\Pt-Fto\ FILE={{!Pt_name}}{{!Fto_no}}.htm












Thanks & Regards,
S. Tamilselvan.",https://forum.imacros.net/viewtopic.php?f=7&t=31756&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2378,[SOLVED]: Variables in TAG command are not resolved,"Hello everybody,

we need to mark certain values in a checkbox list.

If I record a macro it is giving me lines like
Code: Select allTAG POS=1 TYPE=A ATTR=HREF:https://www.biconeo-aquascaping.de/mod/shop_admin/?sh_fn=article_edit&sub_fn=set_art_cat&cat_id=36&art_id=2323
TAG POS=1 TYPE=A ATTR=HREF:https://www.biconeo-aquascaping.de/mod/shop_admin/?sh_fn=article_edit&sub_fn=set_art_cat&cat_id=39&art_id=2323 
-> This example adds the categories 36 and 39 to the article 2323

We have the article Ids and the category Ids available, so I put them in a CSV file. Connecting to the CSV works, I get different values from the CSV in my script nicely. 

The problem comes with line:
Code: Select allTAG POS=1 TYPE=A ATTR=HREF:https://www.biconeo-aquascaping.de/mod/shop_admin/?sh_fn=article_edit&sub_fn=set_art_cat&cat_id={{!COL4}}&art_id={!COL5}}
This leads to the error ""Element not found"" where you can see that iMacro doesn't resolve the CSV columns to the desired values. 

I tried 
Code: Select allSET myString https://www.biconeo-aquascaping.de/mod/shop_admin/?sh_fn=article_edit&sub_fn=set_art_cat&cat_id={{!COL4}}&art_id={!COL5}}
TAG POS=1 TYPE=A ATTR=HREF:{{myString}} 
This comes to ""Error: ""-1300: ElementNotFound"" at line 28    TAG POS=1 TYPE=A ATTR=HREF:{{myString}} Could not find element"", in other words, again the variable is not resolved. 

Can anybody help?

All the best
Casi

Setup Information:
iMacros 2021.0, Personal Edition
iMacros Browser 2021.0 
Windows 10 German
...and I used the search function. If the solution is already described, just let me know and send a link please  ",https://forum.imacros.net/viewtopic.php?f=7&t=31750&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2379,Re: Conditional Action upon Message on the Page,"crash wrote: 鈫慦ed Aug 25, 2021 11:05 pm
hello i need help in my new project example:
Code: Select allTAB T=1
SET !VAR2 EVAL(""Math.floor(Math.random()*5000 + 1);"")
URL GOTO=mysite.com{{!VAR2}}

I need that when a message 'no found' appears on the site it ignores it and go do next page... when the message 'no found' does not appear it searches the site if it has 'name:' and saves this line is it possible?


Yeah...!, this one approved but the Quality is still as Low as your first 2 Duplicates that I previously disapproved with:
2 Threads disapproved, User Notified, Reason(s):
- 2 Duplicates.
- Thread Title not compliant with Forum Rules.
- User didn't read the Forum Rules...

@User, I was going to approve your 1st Thread and would have asked you to read the Forum Rules to then accordingly improve its ""Quality"", but as you go ""spamming"" the Forum directly without waiting for Approval, I prefer to disapprove them both, ... to give you ""the time"" to read the Forum Rules...

+ Correct Sub-Forum to recreate your Thread would also rather be the 'Data-Extraction' Sub-Forum...


=> But you still reopened your Thread in the 'General' Sub-Forum while I asked you to reopen it in the 'Data-Extraction' Sub-Forum, ah-ah...!    
But OK, no big deal about that part, I will move your Thread myself to the correct Sub-Forum...   
(EDIT: Done, => Thread moved to the 'Data-Extraction' Sub-Forum.)

=> You still didn't read the Forum Rules, which leads to:   
- Thread Title not compliant with the Forum Rules (again!) => Not DESCRIPTIVE, sorry but ""Need help with my Project"" + ""How i resolve this?"" are 2 completely useless Thread Titles for a Tech Forum...   
- CIM...!    (Read my Sig also...   )",https://forum.imacros.net/viewtopic.php?f=7&t=31734&sid=ddad572d7e36bd7d9ce3abd083a6d173,AI
2383,"iMacros skips some pages while extracting webpages, what should be the solution ?","I am using Browser Firefox 48.0
iMacros for Firefox 8.9.7
Windows 10 64-bit Operating system

I am using the below code to extract data from a website
Code: Select allVERSION BUILD=8970419 RECORDER=FX
TAB T=1
SET !DATASOURCE kd.csv
SET !LOOP 1
'Increase the current position in the file with each loop 
SET !DATASOURCE_LINE {{!LOOP}}
WAIT SECONDS=3
TAG POS=1 TYPE=A ATTR=TXT:{{!COL1}}
WAIT SECONDS=3
TAG POS=1 TYPE=TABLE ATTR=TXT:* EXTRACT=TXT
 SET !CLIPBOARD {{!EXTRACT}}
WAIT SECONDS=3



imacro test.png (7.63 KiB) Viewed 3664 times



I am copying data from the website, but I am getting an error, like if the website has 160 pages then while running the code skips some pages randomly. I thought that is because of internet speed and loading issue hence I have raised the waiting seconds into 3 still its happening. I am copying the data and storing in clipboard using ditto software. When clicking the next button the webpage don't load, it simply display a loading image and loads within a second. The website is made of JavaScript. What should be the change in the code so that I can extract all the pages ?? Please help",https://forum.imacros.net/viewtopic.php?f=7&t=31587&sid=117ef7144580822f4982137e21eddfc2,AI
2384,Looking for a person able to scrape Crunchbase VC information,"Hey Guys,
I am looking for a person/group of people, able to scrape Crunchbase Investor/VC Data. What exactly should the database look like? We need these fields: Company Name, Type of Investor, Url, Logo, Thesis, all past ticket sizes and rounds they participate in, Founded year, number of funds, number of investments, number of exits, number of lead investments. And info to all their companies in portfolio.

So If there is anyone who can do that, please hit me up with a questions and I will gladly go over it with you in detail (I can share the exact scraping steps with you over email later.)
Let's discuss the pricing over email as well. Reply here with your email and first name, and I will get back to you to open the discussion and clarify whether we can find a way to cooperate.

Thanks for your attention, have a great rest of the day!
Jan",https://forum.imacros.net/viewtopic.php?f=7&t=31572&sid=117ef7144580822f4982137e21eddfc2,AI
2385,communicating with telegram bot,"hello
I am trying to make basic macros to communicate with a telegram bot 
I success with everything but, 
bot time to time communicate with ""are you sure"" and I should respond with ""yes or no /or/ are you a robot or not"" otherwise the process will fail
my point is, is it possible for imacros to communicate with the bot and check for a specific message and then decide to respond with a specific message?

like:

imacros writes: hello (and waits for a response): if the response back was the word ""hello"", then write.. example ""How are you""... 
if what wroten is not the word ""hello"" .. ""then respond with some word else or wait 25 seconds and try again"" etc..)

let me know if my point is clear or I should send images to help
Thanks
Code: Select allimacros app- No browser extension
OS: Windows 10 - Imacros 2021.0.0.0 trial",https://forum.imacros.net/viewtopic.php?f=7&t=31554&sid=117ef7144580822f4982137e21eddfc2,AI
2387,Help to scrape this elusive content,"From this page, when you click on the first listed flight more text with fare options appears below. This is what I want to extract. I cannot see this in iMacros browser with IE emulation. Why not?   
Any advice?
thanks,
https://www.ryanair.com/gb/en/trip/flig ... onIata=ALC",https://forum.imacros.net/viewtopic.php?f=7&t=31490&sid=117ef7144580822f4982137e21eddfc2,AI
2390,extract text from Element with non-fixed location,"hi 
i use imacros browser 12.5 and windows 10 pro x64

i want to extract some text from a website
Description of text and square:
Every time I enter the site, the location of each square (only the squares, not the text inside them), which includes reinvest, withtraw, etc., changes.
The square location changes every time I enter the site and I can not extract the text correctly. I want to extract the number inside the square corresponding to doge. Inside the attached photo, it is written 0.825348080509353. This number is constantly increasing.
I used the Extraction Wizard feature and got the following code. (Click on number)
Code: Select allTAG POS=2 TYPE=H4 ATTR=CLASS:cloud-percent EXTRACT=TXT
This code works fine. As long as the location does not change.
But when its location changes and xmr, for example, takes its place, the number corresponding to xmr (returns 0.00000000000000)
I even tried to select the doge text as an anchor title so that
Code: Select allTAG POS=2 TYPE=H4 ATTR=CLASS:m-2 EXTRACT=TXT
Again, this code works as long as the location does not change
i update this code as following but get not found error
Code: Select allTAG POS=2 TYPE=H4 ATTR=CLASS:m-2&&TXT:DOGE EXTRACT=TXT
and this is the doge text html tag
Code: Select all<h4 class=""m-2"">DOGE</h4>
What is the solution now?
I hope the explanation is complete.",https://forum.imacros.net/viewtopic.php?f=7&t=31397&sid=117ef7144580822f4982137e21eddfc2,AI
2392,Conditional statement to check value of HTML tag,"Hi,

I would like to create a macro that contains a conditional statement to check the value of a HTML tag on a webpage:

Checks TAG POS=1 TYPE=DIV ATTR=ID:Vop11
If this = ""Booked""
Refresh webpage
Else
Continue running macro

I am using iMacros 12.5 Enterprise on Windows 10 and running the macro in iMacros Browser.

I am new to iMacros and any solutions would be appreciated.

Many thanks,

Paul",https://forum.imacros.net/viewtopic.php?f=7&t=31373&sid=117ef7144580822f4982137e21eddfc2,AI
2397,"How to grab variable in source code to extract? (script type=""text/javascript""> var abc = <payload>)","Hi there,

first off, let me include some information:
VERSION BUILD=12.6.505.4525, Trial Version on Windows 10 64-bit, English with Browser IE11. Demo scripts and another script that I built today work well.

I am trying to build a macro that will run through a series of weekends for a car rental company and will store the advertised price in a csv file. I will then later be able to look at the data, mark which weekends I find interesting and - that will be a future project - use a second macro to book those weekends.

The company's website is nice enough to include all the data I really want in one ""object"" up top. ""Object"", because here is where it gets tricky for me: I don't know how to call that, let alone grab it. It is not something that drives the display on the website, and I can't seem to use the right ""find"" option for the TAG command - however I wonder whether it would not be much more elegant go via xpath, if that is possible here.

How would I go about 
1) for beginners, and to understand how this works, grabbing everything inside the variable sxux?
2) to really get what I need, grabbing the part marked in green, i.e. the entire string in quotation marks right after ""prc_wc"" (our, put differently everything between prc_wc and prc_pp)?






Below is the code of the snippet I am trying to get.
Code: Select all<script type=""text/javascript"">
var sxux = {""user_id"":"""",""user_email"":"""",""agia"":"""",""uci"":""xxx"",""uda"":""16.06.2021"",""uti"":""12:00"",""rci"":""40272"",""rda"":""19.06.2021"",""rti"":""09:00"",""uliso"":""DE"",""lor"":""3"",""days_til_begin"":""306"",""ctyp"":""P"",""grp"":"""",
""class_name"":"""",""offers_extrema"":""MCMN|29.68|XXAX|209.52|P|53|DEUF3000|P|49|DER1E000"",""loginstate"":""Public"",""sx_res_tpl"":""offerselect"",""wakz"":""EUR"",""prpd"":"""",""sim_external"":""0"",
""layout"":""standard"",""view"":"""",""prl"":"""",""rType"":""P"",""rValue"":"""",""insu"":""E"",""fir"":""60"",""posl"":""DE"",""offerposl"":""FR"",""total"":"""",""total_gross"":"""",""prc_wc"":""S1:A|S2:A|S3:A|M1:A|M2:A|M3:B|L1:A|L2:B|L3:A"",
""u_d"":""desktop"",""prc_eq"":""MCMN:42.99:128.98|ECMR:43.66:130.98|CCMR:48.66:145.98|CDMR:49.66:148.98|CLMR:52.66:157.97|CWMR:53.66:160.97|IDMR:55.66:166.97|CLAR:56.32:168.97|
CPMR:56.66:169.97|CWAR:57.32:171.97|ILMR:57.99:173.97|IVMR:57.99:173.97|IWMR:58.32:174.97|IDAR:59.65:178.96|IFMR:59.65:178.96|SDMR:60.65:181.96|ILAR:61.99:185.96|
CPAR:62.32:186.96|IWAR:62.65:187.96|SWMR:62.32:186.96|SSMR:63.65:190.96|SFMR:64.65:193.96|FDMR:66.65:199.96|FWMR:68.32:204.95|FDAR:71.33:213.99|FWAR:73.33:219.98|
PDAR:78.99:236.98|PSAR:81.66:244.97|PWAR:82.32:246.97|LDAR:83.32:249.97|PFMR:83.99:251.97|LWAR:83.99:251.97|LSAR:88.66:265.97|SVMR:88.99:266.96|LFAR:92.32:276.96|S
VAR:98:293.99|CCAN::|SSAX::|FCAR::|FWAX::|XDAR:131.33:393.99|XFAR:132.99:398.98|LCAR::|XSAR:135.33:405.98|LPAN::|LWAX::|LFAE::|LDAN::|LWMR::|LFAJ::|XCAN::|XJAN::|XVAN::
|PXBR::|LFAN::|XLAN::|XWAR::|XFAN::|XCAR::|XJAR::|XXAX::"",""prc_pp"":"""",""prc_poa"":"""",""resn"":""0"",""sproducts"":""Car;MCMN,Car;ECMR,Car;CCMR,Car;CDMR,Car;CLMR,Car;CWMR,Car;IDMR,Car;
CLAR,Car;CPMR,Car;CWAR,Car;ILMR,Car;IVMR,Car;IWMR,Car;IDAR,Car;IFMR,Car;SDMR,Car;ILAR,Car;CPAR,Car;IWAR,Car;SWMR,Car;SSMR,Car;SFMR,Car;FDMR,Car;FWMR,Car;FDAR,Car;FWAR,Car;
PDAR,Car;PSAR,Car;PWAR,Car;LDAR,Car;PFMR,Car;LWAR,Car;LSAR,Car;SVMR,Car;LFAR,Car;SVAR,Car;CCAN,Car;SSAX,Car;FCAR,Car;FWAX,Car;XDAR,Car;XFAR,Car;LCAR,Car;XSAR,Car;LPAN,Car;
LWAX,Car;LFAE,Car;LDAN,Car;LWMR,Car;LFAJ,Car;XCAN,Car;XJAN,Car;XVAN,Car;PXBR,Car;LFAN,Car;XLAN,Car;XWAR,Car;XFAN,Car;XCAR,Car;XJAR,Car;XXAX"",""product_name"":"""",""pn"":
""Reservation-Pkw-Offerselect"",""lg"":""fr"",""ibe"":""PKW|Default"",""vat"":""VAT_Y|19"",""osl"":""79|61|18"",""wor"":""Wednesday"",""delcol"":""DEL_notset|COL_notset"",""pm"":"""",""cpc"":"""",""fdr"":""N"",""ic"":""insu:0"",
""bepc"":"""",""br"":"""",
""grp_p"":"""",""ex_p"":"""",""ec_f"":"""",""ec"":""""}
</script>

Here are two options I have tried, but to be honest, just piecing together things from random posts I find:
Code: Select allTAG POS=1 TYPE=INPUT ATTR=CLASS:sxux EXTRACT=TXT
TAG POS=1 TYPE=VAR ATTR=TXT:""sxux"" EXTRACT=TXT 


Thanks a bunch!",https://forum.imacros.net/viewtopic.php?f=7&t=31164&sid=117ef7144580822f4982137e21eddfc2,AI
2402,SOF: Extract Data using EVAL with Triple Separator.,"Some interesting Thread on SOF from 2 weeks ago, the User was ""supposed"" to open a parallel Thread on our Forum, but after asking several times and ""waiting"" with no Follow-up anymore since 10 days, I guess they probably managed to find a Solution, that they probably will never share, so I'm a bit ""tired"" of waiting and I'm creating this ""parallel"" Thread ""on their behalf""..., before they will eventually delete their Thread on SOF, which happens for more than 50% of Threads/Qt's related to iMacros on that Forum... 

Original Thread and Parts of the Comments...:
- Extract Data Using Eval
mizanrobi wrote:
Extract Data Using Eval

Asked 14 days ago
Active 12 days ago
Viewed 49 times

Code: Select allTAG POS=1 TYPE=H1 ATTR=ID:title  EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:acrCustomerReviewText EXTRACT=TXT
TAG POS=1 TYPE=A ATTR=ID:askATFLink EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\mizanrobi\Desktop FILE=product.csv

with above code I am able to extract 3 things
Code: Select allProduct title: PLUSINNO Telescopic Fishing Rod and Reel Combos Full Kit, Spinning Fishing Gear Organizer Pole Sets with Line Lures Hooks Reel and Fishing Carrier Bag Case Accessories

Customer reviews: 666 ratings

Question answered: 180 question was answered among 200

what I want

for customer review and question answered I want only first number: 666 & 180 not text or any other number

and for product title: The text part before first comma (,): PLUSINNO Telescopic Fishing Rod and Reel Combos Full Kit it can be either , or : or - so the product title will be cut off by first (, or : or -)

I know eval can help, but I can not implement.

Thank you.

javascript regex eval imacros

edited Apr 27 at 6:45
asked Apr 25 at 17:33
mizanrobi

... To which I commented...:
chivracq wrote:[...] Case about the Triple Separator for the 'Product title' Field is quite interesting actually, and you may want to open a parallel Thread on the 'iMacros' Forum if you don't get a Solution here from some 'regex' and/or 'javascript' Guru... 

(And half of all Threads in the 'Data Extraction' Sub-Forum on that same Forum contain the/a Solution for the 2 other Fields...)

... And then...:
mizanrodi wrote:thanks a lot. I have found solution for last two just need to solve product title issue. 
Code: Select allSET !var3 eval(""var s=\""{{!extract}}\""; s=s.split(\"" \"");s[0];"") working find to extract desire data. 

chivracq wrote:Yep, very good...!, even if I prefer a slightly different Syntax, easier to debug, the one you are using is not from me...:
Code: Select allSET !VAR3 EVAL(""var s='{{!EXTRACT}}'; var z=s.split(' '); z[0];"")
PROMPT EXTRACT:<SP>_{{!EXTRACT}}_<BR>VAR3:<SP>_{{!VAR3}}_

For 'Product title', you'll need to open a parallel Thread on the iMacros Forum (where I saw you created an Account yesterday) if you still need Help, I don't/rarely answer on this Forum, read my Profile... (Your Qt/OP doesn't comply (yet?) with my ""Quality Criteria"" anyway...)


I asked them several times to mention their FCI, but they only mentioned at some point:
mizanrobi wrote:... but working for my FF without issue.


And last Comment from me from ""yesterday""...:
chivracq wrote:10 days later, any Follow-up...?, or did you find a Sol by yourself...?

Hum, Compliment then, I'm ""nearly"" Impressed...! Please share it (on this Forum), and open also a Parallel Thread on the iMacros Forum like I had asked you, or I will open it for you as your Scenario is ""interesting"", but it would be ""handier"" if you were the ""Owner"" of that Thread...

>>>

... And I would of course have been ""curious"" to hear how this User implemented the desired Functionality to extract this ""Product title"" Field until the first of the 3 possible Separators, a bit like the Functionality in 'Excel' when importing/opening a '.CSV' File, with 5 or 6 possible Separators that can be combined together...

I had several possible Solutions/Implementations in mind, but I think if I had had to implement that Functionality for myself, I would have gone for 'indexOf()' for the 3 Separators, + 'push()' to an Array with a Check on 'indexOf()'>=0 (Char/String not found returns ""-1""), and then with 'Math.min()' or 'sort()' on the Array to return the lowest 'indexOf()' Value (>0) to reuse with 'substr()' or 'substring()' on the original Extract...   

And hum, with ""only"" 3 Separators, it might actually be simpler and shorter to handle all the Conditions with just several 'if'/'else' Statements...  ",https://forum.imacros.net/viewtopic.php?f=7&t=31007&sid=117ef7144580822f4982137e21eddfc2,AI
2405,Help needed with Keyword Assertion,"VERSION BUILD=10022823
iMacros 10 Browser
Windows 10

First of all, apologies for my naivety in this subject. I'm still pretty new to all this.
Basically I'm looking to write a code that goes to a user on a website, and if it finds the words 'Status: Online' it saves the username to an online.txt file, and if the username is offline it saves it to a offline.txt file, and then moves on to the next username, which would be given by {{!COL2}}.
Now I understand that the macro is searching for Status: Online or Status: Offline, but then the extract command is mutually exclusive regardless of whether or not it finds the text on the page anyway.
Thus when I run the script, it saves the username to both the Online file, and the Offline file.
Here's what I have so far:
Code: Select allSET !DATASOURCE filter.csv
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}
FILTER TYPE=IMAGES STATUS=ON

URL GOTO=www.test.com/user={{!COL1}}
 
SEARCH SOURCE=TXT:""Status: Online""
'TAG POS=9 is the location of the username
TAG POS=9 TYPE=FONT ATTR=* EXTRACT=TXT

SET !CLIPBOARD {{!EXTRACT}}
SAVEAS TYPE=EXTRACT FOLDER=""my filepath"" FILE=Online.txt
SET !EXTRACT NULL

SEARCH SOURCE=TXT:""Status: Offline""
'TAG POS=9 is the location of the username
TAG POS=9 TYPE=FONT ATTR=* EXTRACT=TXT

SET !CLIPBOARD {{!EXTRACT}}
SAVEAS TYPE=EXTRACT FOLDER=""C:\\Users\\cooki\\Desktop\\Users Online"" FILE=Offline.txt
SET !EXTRACT NULL

Any help to get me on the right path would be greatly appreciated. Thank you!",https://forum.imacros.net/viewtopic.php?f=7&t=30990&sid=117ef7144580822f4982137e21eddfc2,AI
2406,Triggering macro to accept or reject jobs (text lines with links) when they appear,"VERSION BUILD=12.0.501.6698
Microsoft Windows 10
IE 11 or Firefox 74.0

Hello, I'm working in remote as a contractor for a customer who has CST, while I have Greenwich time (a little different actually, but it's not important). I would like to activate a macro able to accept or reject new jobs when they pop up on the customer's website while I'm sleeping. I've already recorded macros to accept or reject jobs when they appear. A new job displays as a simple line of text on top of the screen, then I can run a macro like:

TAG POS=1 TYPE=SPAN ATTR=ID:cphMain_gvwWorkOrderList_Label1_0
TAG POS=1 TYPE=INPUT:SUBMIT FORM=ID:form1 ATTR=ID:cphMain_btnAccept (Firefox)

TAG POS=1 TYPE=SPAN ATTR=ID:cphMain_gvwWorkOrderList_Label1_0
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:ctl00$cphMain$btnAccept (IE)

What I need is something that triggers this only when a new job appears in the main window (accepted jobs remain there, only in a different color, like rows of a table, so they must not trigger the macro).",https://forum.imacros.net/viewtopic.php?f=7&t=30939&sid=117ef7144580822f4982137e21eddfc2,AI
2407,Instagram Bio Scrapping,"Hi,
I am a university student. I need a script for extracting Bios of instagram users whose Usernames are listed in a file(google doc or offline), I concentrated so much on similar scripts to develop and tailor them but there were 2 problems:
1.All of the scripts in my iMacros yield to outputs with error 鈥?EANF#鈥?instead of the bio鈥檚 text!
2.I couldn鈥檛 find a good tutorial for training how to code imacros scripts!
Please as I need it just for my personal use but so urgent, can u do a favor and write sth similar,plzz? if not would u plzzz recommend me a useful tutorial for writing it??
Yours sincerely
Morteza",https://forum.imacros.net/viewtopic.php?f=7&t=30895&sid=117ef7144580822f4982137e21eddfc2,AI
2410,Extraction issue of html attribute value,"Hi everyone especially to chivracq  

I am trying to extract attribute value for the attached html page. However, the  below imacros code only extract the last part of html code and write only to one column as an output. Could anyone help me what is wrong with my code ?


Code: Select allSET !ERRORIGNORE YES
SET !TIMEOUT_STEP 0
TAB T=1
SET !TIMEOUT_PAGE 15
SET !DATASOURCE input.csv
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}
SET !DATASOURCE_COLUMNS 1

URL GOTO={{!COL1}}
WAIT SECONDS=2
ADD !EXTRACT {{!URLCURRENT}}TAG POS=1 TYPE=DIV ATTR=CLASS:color-swatch-div* EXTRACT=HTM
SET ATTR EVAL(""'{{!EXTRACT}}'.match(/aria-label=[\""'](.+?)[\""']/)[1];"")
SET !EXTRACT NULL
ADD !EXTRACT {{ATTR}}
WAIT SECONDS=2

TAG POS=2 TYPE=DIV ATTR=CLASS:color-swatch-div* EXTRACT=HTM
SET ATTR EVAL(""'{{!EXTRACT}}'.match(/aria-label=[\""'](.+?)[\""']/)[1];"")
SET !EXTRACT NULL
ADD !EXTRACT {{ATTR}}
WAIT SECONDS=2

TAG POS=3 TYPE=DIV ATTR=CLASS:color-swatch-div* EXTRACT=HTM
SET ATTR EVAL(""'{{!EXTRACT}}'.match(/aria-label=[\""'](.+?)[\""']/)[1];"")
SET !EXTRACT NULL
ADD !EXTRACT {{ATTR}}
WAIT SECONDS=2

SAVEAS TYPE=EXTRACT FOLDER=* FILE=out.csv",https://forum.imacros.net/viewtopic.php?f=7&t=30914&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2411,"How do I timeout a ""SEARCH"" command?","System: macOS 10.14.6 with iMacros free on Google Chrome Version 78.0.3904.108 (Official Build) (64-bit)

I currently have this code: 
Code: Select allVERSION BUILD=1005 RECORDER=CR
SET !ERRORIGNORE YES
SET !TIMEOUT 100000
SET !EXTRACT_TEST_POPUP NO
SEARCH SOURCE=REGEXP:""([A-Z0-9]{5}-[A-Z0-9]{5}-[A-Z0-9]{5}-[A-Z0-9]{5}-[A-Z0-9]{5})"" EXTRACT=""$1""
SET !CLIPBOARD {{!EXTRACT}}

When I play it, it goes straight to ""RuntimeError: Source does not match to REGEXP='([A-Z0-9]{5}-[A-Z0-9]{5}-[A-Z0-9]{5}-[A-Z0-9]{5}-[A-Z0-9]{5})', line: 5"" on some pages. I want to timeout this script so that whenever its posted it'll copy it to clipboard... For some reason the code isn't timing out, even though its in the code...? 

Thanks in advance.",https://forum.imacros.net/viewtopic.php?f=7&t=30724&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2412,"On Behalf of @Focalav: SEARCH SOURCE=REGEXP ""Next target: <b>6 seconds</b>""","Alright...!, stg I very rarely do, but I'm posting/creating this Thread on Behalf of a ""New"" User who tried to post about 1 week ago and I disapproved their Post/Thread because they didn't read the Forum Rules and used a ""completely useless""    (=Non-Descriptive) Thread Title (=> 鈥渋macros question鈥? yep, very ""cute"" indeed, oops...!   ), and I ""thought"" they would reopen their Thread with the ""Minimum Required Quality"", but..., hum..., nope, ah-ah...!   

The overall/average Quality of Threads I disapprove is usually pretty Low, (and I'm not even talking about all Spam and Fake Posts from self-proclaimed SEO Experts who want to advertise for their Site in their Sig, ah-ah...!, but for ""Legit"" Posts I regularly have to disapprove, the 3 main Reasons are ""Wrong Sub-Forum"" + ""Non-Descriptive Thread Title"" + ""Screenshot(s) uploaded to some external Pix Hosting Site""), but this one contained some ""interesting"" Research by this User, so I'm still creating a Thread for them...   
Disapproved topic 鈥渋macros question鈥?written by 鈥淔ocalav鈥?for the following reason
禄

Post/Thread disapproved, User notified, Reason:
""Useless Thread Title...!""

""but help there is hit and miss""
=> Yep indeed, for Users who don't read the Forum Rules and don't use the Forum ""correctly""...

- You can reopen your Thread with a DESCRIPTIVE Thread Title. (This part is Blocking for Approval.)
- The 'Data Extraction' Sub-Forum would btw be more ""appropriate""/specific than the 'General' one that you chose. (This is now Blocking also as I've mentioned it...)
- Mention your FCI. (Not Blocking for Approval but helps bypass the ""hit and miss""...)

The rest of the Post is ""Good Quality"" and the Qt is interesting actually...

(And correct Spelling is ""iMacros""...)

>>>
Post details
imacros question
Posted by Focalav 禄 27/02/2020 - 18:46

HI all Im trying to do a data extract from a specific loaction, for a random bit of data using imacros.
Im trying to use a SEARCH SOURCE=REGEXP query but the query structure is a mystery to me.
Ive read: http://wiki.imacros.net/SEARCH
but this doesn't explain enough so that I can figure out how to make it useful in building my own query.

A sample bit of the source webpage is:
Code: Select all<table cellpadding=0 cellspacing=0><tr><td width=""220px""><div style=""width:200; height:25; background:url(/images/timedcombat-background.png); border:1px solid #000000;""></div></td><td>Next target: <b>6 seconds</b></i></td></tr></table>

So Ive tried: a whole bunch of stuff trying to figure out how the query structure works but I haven't go it.

Imacros Notes on using SEARCH REGEXP
Code: Select allSEARCH SOURCE=REGEXP:""target:..[^]"" EXTRACT=$1 | gives: undefined
SEARCH SOURCE=REGEXP:""target:..[^s]"" EXTRACT=$1 | gives: undefined
SEARCH SOURCE=REGEXP:""target:..[^0-9]"" EXTRACT=$1 | gives: undefined
SEARCH SOURCE=REGEXP:"".*target..([0-9]+)"" EXTRACT=$1 | gives: Source Doesnt match target
SEARCH SOURCE=REGEXP:""target..([0-9]+)"" EXTRACT=$1 | gives: Source Doesnt match target
SEARCH SOURCE=REGEXP:""target:..([0-9]+)"" EXTRACT=$1 | gives:Source Doesnt match target
SEARCH SOURCE=REGEXP:""target:..([^0-9]+)"" EXTRACT=$1 | gives:Source Doesnt match target
SEARCH SOURCE=REGEXP:""target:..([^0-9])"" EXTRACT=$1 | gives: b
SEARCH SOURCE=REGEXP:""target:.([^0-9])"" EXTRACT=$1 | gives: <
SEARCH SOURCE=REGEXP:""target:([^0-9])"" EXTRACT=$1 | gives: Blank, literaly, the space character I think..
SEARCH SOURCE=REGEXP:""target:?([^0-9])"" EXTRACT=$1 | gives: ;
SEARCH SOURCE=REGEXP:""target:?.([^0-9])"" EXTRACT=$1 | gives: <
SEARCH SOURCE=REGEXP:""target:?.([^0-9])"" EXTRACT=$1 | gives: Blank, literaly, the space character I think..
Code: Select allSEARCH SOURCE=REGEXP:""target:?.([^0-9]+)"" EXTRACT=$1
... =>  | gives: UGH,.. some part of the Imacros source code?
Code: Select allvar altKey;
var ctrlKey;
var metaKey;
if (window.event != null) {
c=String.fromCharCode(window.event.keyCode).toUpperCase();
altKey=window.event.altKey;
ctrlKey=window.event.ctrlKey;
metaKey=window.event.metaKey;
}else{
c=String.fromCharCode(e.charCode).toUpperCase();
altKey=e.altKey;
ctrlKey=e.ctrlKey;
metaKey=e.metaKey;
}
if (window.event != null)
target=window.event.srcElement;
else
target=e.originalTarget;
if (target.nodeName.toUpperCase()=='INPUT' || target.nodeName.toUpperCase()=='TEXTAREA' || altKey || ctrlKey || metaKey){
}else{
if (c == '

So Then I decided I need help. Yes, I'm on the imacros forums, but help there is hit and miss.
is there documentation of how exactly to form Search Source queries using imacros? Or does anyone use Imacros, and could explain this to me.

So, here it is...   
I won't ""try"" to answer it (even if there are some much easier/simpler Solutions) as the User didn't ""follow up"" on my Disapproval by reopening their Thread with some ""correct"" Quality, but I found the Research part with all different 'SEARCH' Statements and their Results quite interesting though   , and worth sharing on the Forum...   

And I've reformatted a bit the Post, especially about the very last 'SEARCH' Statement that I separated from the previous ones, which seems to retrieve some iMacros Core Code about the 'EVENT' Mode... Hum..., strange indeed...   

>>>

@OP (= @Focalav), you are still ""welcome"" to post in this Thread if you ""notice"" it, you won't be able to take Ownership of this Thread, but if you post in it, you'll be able to get Notifications..., and I might still help you if you follow up ""a bit correctly"" and also mention your FCI like I already asked you...   
(But don't try to create your own Thread now (about the same Qt), that would be a Duplicate, and I wouldn't approve it anyway, you had 1 week already to do so, now it's too late, sorry...   )",https://forum.imacros.net/viewtopic.php?f=7&t=30882&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2414,Extract dynamic content with iMacros,"I'm entirely new to iMacros, and I'm trying to get iMacros to extract the email address that appears when you click ""reply"" on ads on craigslist.

The only thing I have is:

VERSION BUILD=10021450
URL GOTO=https://losangeles.craigslist.org/sfv/r ... 77618.html
TAG POS=1 TYPE=BUTTON ATTR=TXT:reply

I have tried simply using wildcards, which doesn't work.",https://forum.imacros.net/viewtopic.php?f=7&t=30708&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2415,How to extract certain links and assign them to an array variable?,"Hi. I am new to iMacros.

 I use Windows 10, iMacros 12.0.1.171 Enterprise trial version, chrome 78.0.x

 Any help will be greatly appreciated!

 I want to do something very simple but don't know how to do this.


 I want to extract all the links having a certain word in a web page.

 For example, I want to extract all the links having 'JCC' like below:

  1) http://JCC.domain-name.com/any_number_or_letter_or_whatever
  2) http://JCC.domain-name.com/any_number_or_letter_or_whatever
  3) http://JCC.domain-name.com/any_number_or_letter_or_whatever

  ... and so on

 After extracting all the links above, I want to move to each web page one by one or randomly.

 I mean, after visiting the first page, I want do something, and then, I want to move to the 2nd web page,  and then wait for 7 seconds, using 'WAIT SECONDS=7' command, to load the web page completely, and I want to do something... and so on.

I must not visit the web page I had already paid a visit.

 Q1) How can I assign each link having a certain word to an array? How can I assign all the links to an array AT RANDOM or FROM TOP-TO-BOTTOM OF THE HTML PAGE?

 Q2) How can I move to each web page using the array variable?

 Sometimes, the web page doesn't show all the links having 'JCC's, so I might have to hit the F5 key to refresh the web page.

 Sometimes I get no new link, sometimes 5 or 10 new links which were not displayed before reloading the web page. 

 There is no way to know how many times I have to reload the web page but I guess 2 to 5 would be good.

 Q3) I want to add all the new links from the reloaded the web page to the previous array. How can I do this?


[Q4] I might have to use this macro later again. Then, I must NOT revisit the web page I'd already visited. How can I make iMacros do this?

 Thank you very much in advance. : )",https://forum.imacros.net/viewtopic.php?f=7&t=30692&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2418,iMacros 12 not recording keystrokes or extracting data,"I鈥檓 creating a Excel macro with iMacros and VBA.  I've done this before without too many issues on a number of web sites.  I started off using the iMacros browser.  I got about 1/2 way through entering data on the web page without any issues.  So far so good.

This particular web page has a number of sections, e.g., Summary Info, Claim Info, Parts, Operations, Expenses, Comments, and Customer (in this order).  When I got to the Parts, Operations, Expenses, Comments, and Customer sections, there is a toggle switch to hide / display the fields in that section.  The Parts, Operations, and Expenses sections all display a minus sign (to hide the fields) but no fields are displayed.  The Comments and Customer fields also display the minus sign but the fields in those sections are displayed.  I haven't seen anything like that before.

I then changed the code to use iMacros with the IE browser.  The good news is I can now see the missing fields in all of the sections now.  I kicked off the macro and it got part way through the existing code and then I had an issue where it didn't extract the header I was looking for.  I tried to use the sidebar but can鈥檛 use IMacros functionality, e.g., it won鈥檛 let me extract data and doesn鈥檛 record keystrokes like I normally can.  It's almost like it is frozen.  

Any suggestions would be greatly appreciated...鈥︹€︹€?.shaves

I'm using Office 365 (Excel 2016)
iMacros 12.0.501.6698
Windows 10
64 bit OS",https://forum.imacros.net/viewtopic.php?f=7&t=30582&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2419,Extracting a number from an ID,"Hello guys, I hope my topic finds you well.

I want to extract a specific number from an ID value,

here is the line:

Code: Select all
Code: Select allTAG POS=1 TYPE=INPUT ATTR=CLASS:markAll<SP>check EXTRACT=HTM

and here is the result I get after I run the previous line:
Code: Select all<input style=""outline: 1px solid blue;"" id=""raidListMarkAll1518"" class=""markAll check"" onclick=""Game.RaidList.markAllSlotsOfAListForRaid(1518, this.checked); ""type=""checkbox"">

For example, I want only to extract this number (1518) from this (id=""raidListMarkAll1518"").

I don't know how to make the EVAL command to achieve the result I want.

I hope to get help from you guys, I really appreciate it

Thank you so much.",https://forum.imacros.net/viewtopic.php?f=7&t=30612&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2422,Extract A Cell from Google Sheet or a CSV,"Firefox 49.0.2
iMacros VERSION BUILD=8970419
Windows 10 64 Bit

Hi there,
I know It's kinda bad question. But I really need this.I tried harder to create the code using iMacros 12 but I failed.
I tried to search forum but can't find a exact solution.

Let me show you What I need.

Here is a example of google sheet. --> https://docs.google.com/spreadsheets/d/ ... =drive_web

I wanna extract a cell value. 

Or,

I got a csv file like this google sheet example.
Same question there.

How Do I Extract a specific cell from a csv or from google shit??!!

I'm new to this stuff. 

Thanks  ",https://forum.imacros.net/viewtopic.php?f=7&t=29533&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2423,Extract List to CSV,"Firefox for Windows Ver 49.0.1 (32 bit)
iMacros for FF Ver 8.9.7
OS Windows 10

Dear Sir,

 I want to extract each item from  List;  to store CSV file in each column one by one.





I have tried but i could not extract continuously . Same item will extracted. 
Kindly help me. I have communication problem  . I don't know how to express my need. I will try  .

Code: Select allVERSION BUILD=8970419 RECORDER=FX
TAB T=1
SET !TIMEOUT_STEP 1
SET !TIMEOUT_PAGE 400


TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_CPHPage_ddblock EXTRACT=TXT
SET !VAR1 EVAL(""'{{!EXTRACT}}'.trim();"")
WAIT SECONDS=2
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_CPHPage_ddPanchayat EXTRACT=TXT
SET !VAR2 EVAL(""'{{!EXTRACT}}'.trim();"")
WAIT SECONDS=2
TAG POS=1 TYPE=SELECT FORM=ID:aspnetForm ATTR=ID:ctl00_CPHPage_ddVillage EXTRACT=TXT
SET !VAR3 EVAL(""'{{!EXTRACT}}'.trim();"")

SET !EXTRACT {{!VAR1}}[EXTRACT]{{!VAR2}}[EXTRACT]{{!VAR3}}[EXTRACT]
SAVEAS TYPE=EXTRACT FOLDER=* FILE=Firka.csv

SET !EXTRACT NULL

Thanks & Regards,
S.Tamilselvan",https://forum.imacros.net/viewtopic.php?f=7&t=30526&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2425,Get image info,"Hi
I'm on Win7x64, FF 52.7.3x64, iMacros 9.0.3

FF allows displaying of an image list with according image data, like on screenshot:

How to access all available image infos with iMacros? I would like to it  into CSV.",https://forum.imacros.net/viewtopic.php?f=7&t=28519&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2428,extract data from not specified class,"Hello,
I am wondering if it is possible to extract data (text) from unspecified TD or TR class?

this is the HTML:
<tr>
			<td><b style=""outline: 1px solid blue;"">Reward</b></td>
			<td align=""center""><b>540</b></td>
			<td align=""right""><b>20</b></td>
			<td align=""right""><b>80</b></td>
			<td colspan=""2"">&nbsp;</td>
			<td align=""right""><b>2</b></td>
			<td align=""right""><b>5</b></td>
			<td align=""right""><b>1</b></td>
		</tr>

I am attempting to extract number 540 (which changes daily) right after the word ""Reward""

iMacros for FF V8.9.7
FF v55.0.2
MacOS v10.13",https://forum.imacros.net/viewtopic.php?f=7&t=30271&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2429,iMacros for IE stops scraping data after 15-20 minutes while script runs,"I've been having an ongoing issue with iMacros for IE where, while running a script, the script will run fine for about 15-20 minutes (grabbing the data I need), but then stops grabbing the data and fills empty. I've tried time constraints to give the page enough time to upload, but this is not the issue.
I've isolated this problem to only IE, as it runs fine on Firefox (and even ran fine on the iMacros browser, which I no longer have). The only issue is, I need to run this script in IE (as it captures the data in such a way that makes it usable for me).

Is anyone else having this issue, and have they found a way around it?",https://forum.imacros.net/viewtopic.php?f=7&t=30229&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2434,"[Youtube]Subscribe, get video title and input it to a Spreadsheet","Hi guys

I have to do this following task
* I have a list of Youtubers (in a Google Drive spreadsheet)
* The spreadsheet has 3 columns ( Channel URL, Video Title, Video URL)
* Visit each of them
* After visit, subscribe to their channel
* Then, pick title and URL from a video of their channel
* And fill that info (title and URL) back to the spreadsheet 

I tried recording using iMacros but have had no result so far. 

Each time I run the code, it still go to first cell of the spreadsheet, and it can't copy/paste the title and URL of the video 

Here's the script from the recorded action

====
Code: Select allVERSION BUILD=10021450
URL GOTO=https://docs.google.com/spreadsheets/d/myspreadsheetexample

TAG POS=2 TYPE=SPAN ATTR=TXT:Open<SP>link
'right-click the cell and choose ""open link"" in Google Spreadsheet

TAG POS=1 TYPE=YT-FORMATTED-STRING ATTR=TXT:Subscribed<SP>*
WAIT SECONDS=2

TAG POS=14 TYPE=A ATTR=ID:video-title
TAG POS=1 TYPE=H1 ATTR=TXT:*
TAB T=1
TAG POS=2 TYPE=DIV ATTR=TXT:Separator:

===

i'm using iMacros for Firefox (v 10.0.2.1450) 
Firefox version 65.0.1 

Thank you so much in advance",https://forum.imacros.net/viewtopic.php?f=7&t=30168&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2435,SEO,"Hi,

Can anyone explain me how Imacro helps out for SEO work.  I am just an newbie to IMacro, i want to understand the powerful functionality of Imacro in SEO.

Thanks
Naveen











?",https://forum.imacros.net/viewtopic.php?f=7&t=10016&sid=a8b6b8836ef4b006067cba7caee672f3,AI
2438,EXTRACT and ONDOWNLOAD,"Hello folks!
Windows 7, iMacros for Firefox 8.9.7, Firefox 54.0.1 (32-bit)

I'm an iMacros beginner and I need your help.

I want to extract description of a file and then present it in a message box, after the file is downloaded.
I used EXTRACT=TXT command, but because of this command, iMacro doesn't download the file anymore:
Code: Select allTAB T=1
URL GOTO=http://tcmb.gov.tr/wps/wcm/connect/TCMB+EN/TCMB+EN/Main+Menu/STATISTICS/Tendency+Surveys/Business+Tendency+Statistics+and+Real+Sector+Confidence+Index/Data
ONDOWNLOAD FOLDER=C:\Temp\ FILE=BTS-ReportTable-Int.xls WAIT=YES
TAG POS=1 TYPE=A ATTR=TXT:Business<SP>Tendency<SP>Survey<SP>and<SP>Real<SP>Sector* EXTRACT=TXT
prompt File:<SP>{{!EXTRACT}}<SP>downloaded!
I doubled the entry (one without that EXTRACT=TXT command) and now it works:
Code: Select allTAB T=1
URL GOTO=http://tcmb.gov.tr/wps/wcm/connect/TCMB+EN/TCMB+EN/Main+Menu/STATISTICS/Tendency+Surveys/Business+Tendency+Statistics+and+Real+Sector+Confidence+Index/Data
ONDOWNLOAD FOLDER=C:\Temp\ FILE=BTS-ReportTable-Int.xls WAIT=YES
TAG POS=1 TYPE=A ATTR=TXT:Business<SP>Tendency<SP>Survey<SP>and<SP>Real<SP>Sector*
TAG POS=1 TYPE=A ATTR=TXT:Business<SP>Tendency<SP>Survey<SP>and<SP>Real<SP>Sector* EXTRACT=TXT
prompt File:<SP>{{!EXTRACT}}<SP>downloaded!
It's just, I don't think it is supposed to look that way.
I'd like to write codes as good as possible.
Could you possibly tell me how my code should look like?

Plus, is it possible to present just a part of extracted text?
I mean in this case: instead of prompting whole ""Business Tendency Survey and Real Sector Confidence Index (July 2017)"" iMacro would prompt just ""(July 2017)"".",https://forum.imacros.net/viewtopic.php?f=7&t=27826&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2439,Answer: Open multiple links same time,"Dear all !
I have this link
https://www.bandatnendongnai.vn/tin-tuc ... n-con-sot/
And i want Open link from hyperlink and images at slide bar
There are many links
1,2,3,... links open simultaneously in one browser tab 1, tab 2 ... Tab ....
I have not done     
Thank you so much",https://forum.imacros.net/viewtopic.php?f=7&t=30042&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2441,"Choose pdf name dynamically by variable does not work, datetime only","Dear friends, 

Imacros Version: 10.0.5
Chrome: Vers茫o 71.0.3578.98
OS: Windows 10 64 bits

I trying download some open pdf's in som sites, for example here I will put this site:
https://www.wdl.org/pt/item/18529/

So the title of this page is ""Val锚ncia. Mercado"", I want extract the title of many pages to put in pdf name when downloaded. 

My script working with dynmi datetime:
Code: Select allTAG POS=1 TYPE=H1 ATTR=ID:page-title EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
TAG SELECTOR=""#downloads>LI>A""
ONDOWNLOAD FOLDER=* FILE=+_{{!NOW:yyyymmdd_hhnnss}} WAIT=YES
TAG SELECTOR=""#downloads>LI>UL>LI:nth-of-type(2)>A""

So, when I download using this above code, the name of archive comes:  ""18529_20181223_223412"", this proof that the datetime dynamicaly works.

But when I try put the title page as filename dinamycaly (to work in another pages), dont work, see my script:

Code: Select allTAG POS=1 TYPE=H1 ATTR=ID:page-title EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}
TAG SELECTOR=""#downloads>LI>A""
ONDOWNLOAD FOLDER=* FILE=+_{{!VAR1}} WAIT=YES
TAG SELECTOR=""#downloads>LI>UL>LI:nth-of-type(2)>A""


I thinks must be some simple action to solve this (because the dynamic datetime works, some the name by variable must work by logical thinking), I try many things and dont work, someone can help with this?

thanks in advanced",https://forum.imacros.net/viewtopic.php?f=7&t=30034&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2444,Extracting data of JSON,"Hey all,

I'm trying to extract part of a JSON web page to find a specific part in it.

What I want to try to use, is this command :

SET N EVAL(""var json2 = JSON.parse('{{!EXTRACT}}');"")

But I received this error :
JScript statement in EVAL contains the following error: Variable 'JSON' has not been declared. Line 6: SET N EVAL(""var json2 = JSON.parse('{{!EXTRACT}}');"")

I am using iMacros Browser 12 on WIndows Server 2012 R2

Most of the example I have seen of the JSON.parse command seems like they were using iMacros for FF, so I'm wondering if it's only available on that.

Thanks

For now, my script is very simple.

URL GOTO=D:\iMacroApp\Scenarios\extract.json
TAG POS=1 TYPE=BODY ATTR=* EXTRACT=TXT
SET N EVAL(""var json2 = JSON.parse('{{!EXTRACT}}');"")
PROMPT {{N}}",https://forum.imacros.net/viewtopic.php?f=7&t=29803&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2445,Extracted Values Include Unwanted Labels,"Good Afternoon - 

I am trying to configure an iMacros job which will collect 2 numerical values from 4 web pages then save them into a CSV daily for import into a report.  I've gotten it to mostly work except for one small issue which I need help with, please.

I used the wizard to browse to and select the values, choosing to ""Add command"" after each.  Only the numbers were selected, but the ""Extracted"" box shows the labels next to the numbers as well.  I continued to see what the output would be and once I ran the job, it created a CSV which included both the label and value.  Below is a screenshot of just the value ""28"" selected, yet it shows ""Consumed 28 28"" in ""Extracted.""  The resulting CSV for that is also """"Consumed 28""


The macro string which captures the data is below.  
Code: Select allTAG POS=1 TYPE=LABEL ATTR=ID:Consumed_label EXTRACT=TXT

I've looked through online help, but cannot figure out how to just get it to grab the numerical value.  Any suggestions?

Bonus Question 
Once I get this figured out, I will need to find a way to import all 8 extracted values into an existing Excel report.  If anyone has any suggestions or links that would help, I'd really appreciate it.

Thank You!",https://forum.imacros.net/viewtopic.php?f=7&t=29939&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2446,Data Extraction help,"I'm far from an expert, but I've stumbled my way through many useful macros, greatly improving my workflow.

I want to take advantage of imacros web scraping, but I fail time and again. 

Is anyone out there good enough to get me started? I could really use this data. My goal is to scrape public permit data and dump it into and excel (or csv) file.

An example website is below. If someone could get me started, I can easily adapt it for the other websites.  Thank you very much in advance for any help!

https://www.velocityhall.com/accela/vel ... /index.cfm",https://forum.imacros.net/viewtopic.php?f=7&t=29900&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2447,Web Extracting imacros,"I am extracting data from URL using imacro... its working perfect inside URL html there is one User ID detail . i want to extract that, but don't know what should i write code for that

Here is my URL

https://www.justdial.com/mumbai/Aakar-A ... I5M1_BZDET

And here bellow is my imacro code
Code: Select allVERSION BUILD=10022823
TAB T=1
SET !DATASOURCE infos.csv
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}
SET !ERRORIGNORE YES
SET ABC {{!COL3}}

TAB CLOSEALLOTHERS
SET !TIMEOUT_PAGE 25
URL GOTO={{!COL3}}

SET !EXTRACT_TEST_POPUP NO

SET !TIMEOUT_STEP 0
TAG POS=1 TYPE=SPAN ATTR=CLASS:fn EXTRACT=TXT
TAG POS=3 TYPE=SPAN ATTR=CLASS:lng_add EXTRACT=TXT
TAG POS=1 TYPE=a ATTR=ID:whatsapptriggeer EXTRACT=HREF
TAG POS=3 TYPE= button ATTR=onclick:* EXTRACT=HREF
ADD !EXTRACT {{!URLCURRENT}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=Extract.csv

In this code I think there are some problems here:
Code: Select allTAG POS=3 TYPE= button ATTR=onclick:* EXTRACT=HREF

What I want to extract used ID. Please you can see in this image.
",https://forum.imacros.net/viewtopic.php?f=7&t=29902&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2450,EXTRACT <TD> WhitOut Class | Inside <tbody>,"Updated: I fixit error, thank for seeing post.
Hi everyone.
I have a problem, extract data from website to CSV file but in the website all data wanto get no have Class dev, all code inside <td>
I have a code in the below, please see and help me. 

Thank so much.
Code: Select all<tbody><tr>
					<td><b>Name: </b></td>
					<td>[b]ratedsegment[/b]</td>
				</tr>
				<tr>
					<td><b>Posts: </b></td>
					<td>0</td>
				</tr><tr>
				</tr><tr>
					<td><b>Position: </b></td>
					<td>Brand new</td>
				</tr>
				<tr>
					<td><b>Date Registered: </b></td>
					<td>[b]August 12, 2018, 10:56:23 AM[/b]</td>
				</tr><tr>
					<td><b>Last Active: </b></td>
					<td><b>Today</b> at 02:57:47 AM</td>
				</tr>
				<tr>
					<td colspan=""2""><hr class=""hrcolor"" width=""100%"" size=""1""></td>
				</tr><tr>
					<td><b>ICQ:</b></td>
					<td></td>
				</tr><tr>
					<td><b>AIM: </b></td>
					<td></td>
				</tr><tr>
					<td><b>MSN: </b></td>
					<td></td>
				</tr><tr>
					<td><b>YIM: </b></td>
					<td></td>
				</tr><tr>
					<td><b>Email: </b></td>
					<td>
						<i><a href=""mailto:cfeuer@chiuyu.org"">[b]cfeuer@chiuyu.org[/b]</a></i>
					</td>
				</tr><tr>
					<td><b>Website: </b></td>
					<td><a href=""""></a></td>
				</tr><tr>
					<td colspan=""2""><hr class=""hrcolor"" width=""100%"" size=""1""></td>
				</tr><tr>
					<td><b>Gender: </b></td>
					<td></td>
				</tr><tr>
					<td><b>Age:</b></td>
					<td>N/A</td>
				</tr><tr>
					<td><b>Location:</b></td>
					<td></td>
				</tr><tr>
					<td><b>Local Time:</b></td>
					<td>August 13, 2018, 02:57:47 AM</td>
				</tr><tr>
					<td colspan=""2""><hr class=""hrcolor"" width=""100%"" size=""1""></td>
				</tr>
				<tr>
					<td colspan=""2"" height=""25"">
						<table style=""table-layout: fixed;"" width=""100%"" cellspacing=""0"" cellpadding=""0"" border=""0"">
							<tbody><tr>
								<td style=""padding-bottom: 0.5ex;""><b>Signature:</b></td>
							</tr><tr>
								<td colspan=""2"" class=""smalltext"" width=""100%""><div class=""signature""></div></td>
							</tr>
						</tbody></table>
					</td>
				</tr>
			</tbody>

I want to get data i choose BOLD in the code.",https://forum.imacros.net/viewtopic.php?f=7&t=29689&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2453,How to delete point in extract imacros,"Hi, I need help

How to delete point in extract from imacros ? Example : 3.045 => 3045 (How to Delete this point ?)
I've tried deleting using a script like this can not too
Code: Select all'ADD !EXTRACT {{!URLCURRENT}}
'WAIT SECONDS=5
TAG POS=1 TYPE=DIV ATTR=TXT:*
SET !TIMEOUT_STEP 0
TAG POS=2 TYPE=STRONG ATTR=CLASS:* EXTRACT=TXT
WAIT SECONDS=1
'SET !EXTRACT EVAL(""\'{{!EXTRACT}}\'.replace(/./g, \'\');"")
PROMPT {{!EXTRACT}}

SAVEAS TYPE=EXTRACT FOLDER=* FILE=ScrapeNumber.csv
WAIT SECONDS=0.5


I'm at a loss and try run script this and not working  
Code: Select all'ADD !EXTRACT {{!URLCURRENT}}
'WAIT SECONDS=5
TAG POS=1 TYPE=DIV ATTR=TXT:*
SET !TIMEOUT_STEP 0
TAG POS=2 TYPE=STRONG ATTR=CLASS:* EXTRACT=TXT
WAIT SECONDS=1
'SET !EXTRACT EVAL(""\'{{!EXTRACT}}\'.replace(/9./g, \'9\');"")
PROMPT {{!EXTRACT}}
SET !EXTRACT EVAL(""\'{{!EXTRACT}}\'.replace(/0./g, \'0\');"")
PROMPT {{!EXTRACT}}
SET !EXTRACT EVAL(""\'{{!EXTRACT}}\'.replace(/1./g, \'1\');"")
PROMPT {{!EXTRACT}}
SET !EXTRACT EVAL(""\'{{!EXTRACT}}\'.replace(/2./g, \'2\');"")
PROMPT {{!EXTRACT}}
SET !EXTRACT EVAL(""\'{{!EXTRACT}}\'.replace(/3./g, \'3\');"")
PROMPT {{!EXTRACT}}
SET !EXTRACT EVAL(""\'{{!EXTRACT}}\'.replace(/4./g, \'4\');"")
PROMPT {{!EXTRACT}}
SET !EXTRACT EVAL(""\'{{!EXTRACT}}\'.replace(/5./g, \'5\');"")
PROMPT {{!EXTRACT}}
SET !EXTRACT EVAL(""\'{{!EXTRACT}}\'.replace(/6./g, \'6\');"")
PROMPT {{!EXTRACT}}
SET !EXTRACT EVAL(""\'{{!EXTRACT}}\'.replace(/7./g, \'7\');"")
PROMPT {{!EXTRACT}}
SET !EXTRACT EVAL(""\'{{!EXTRACT}}\'.replace(/8./g, \'8\');"")
PROMPT {{!EXTRACT}}


SAVEAS TYPE=EXTRACT FOLDER=* FILE=ScrapeNumber.csv
WAIT SECONDS=0.5
",https://forum.imacros.net/viewtopic.php?f=7&t=29612&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2454,extract dynamical prices,"Windows10/
iMacros v12 enterprise edition/

Good morning, 
I've been searching for so long, and I found answers on the forum, but it doesn't work for my case...I hope someone could help me.
I need to extract products transport cost on a website (such as amazon), but it depends on the number of products ordered.
When I create my macro on iMacros, I just underline the price with my mouse : TAG POS=1 TYPE=P ATTR=TXT:5,76<SP>鈧?But if I run my macro again and that this price changed, it still indicate 5.76 euros.
My question : how to extract in a better way dynamic informations ? Then I want this information in a csv.
template of my extraction : 
TAG POS=1 TYPE=P ATTR=TXT:5,76<SP>鈧?EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\UTILISATEUR\Documents\iMacros FILE=TSEfraisTransport1.csv

Best regards, Romane (from France)",https://forum.imacros.net/viewtopic.php?f=7&t=29557&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2462,Extract an URL,"Hello,

First, sorry for my english, I a french girl   

I am using iMacros and Javascript to grab information.
With some javascript actions, I got a list of URL which can be pdf, txt files or html page.
And I want to save these URL into a folder (as a pdf file for a pdf link, html file for a html link ....)
Do you know how it is possible ?  with iMacros ? with javascript ?

Thanks by advance for your help

Thais",https://forum.imacros.net/viewtopic.php?f=7&t=28499&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2463,How to retrieve the name of the downloaded PDF document?,"Hi,

I am using iMacros Version10 and want to retreive the name of a PDF document that is stored behind a short link: e.g. http://shortlinks.de/40tr points to https://www.upc.at/pdf/anleitungen/fibe ... n-0517.pdf

The macro in Excel writes the shortlink to the variable {linkname}. Beside downloading the PDF file I also want to hand over the name of the actual file (b2c-installationsanleitung-wlan-0517.pdf) to excel

Here is my macro performing the download, but I did not succeeded in writing the name of the PDF file to a variable:

VERSION BUILD=10002738
TAB T=1
TAB CLOSEALLOTHERS
Set !errorignore yes
ONDOWNLOAD FOLDER=C:\CHECK_Links FILE=* WAIT=YES
URL GOTO={{linkname}}
TAB T=1

I hope, that there must be some simple solution or trick for this task.

Thank you very much for your support!

Richard",https://forum.imacros.net/viewtopic.php?f=7&t=28494&sid=7ae4e60bc4f3ad89d3e546c9096547af,AI
2469,!WAITPAGECOMPLETE vs Wait Time,"I have a relatively slow loading set of pages that I am using to scrape data. Initially I had problems with the script getting ahead of itself and causing problems by advancing too quickly. I initially tried the Wait feature and that helped a lot. I later tried the !WAITPAGECOMPLETE command thinking that it would be a more efficient way of handling slow loading pages. Unfortunately, it doesn't work as well as putting in somewhat arbitrary Wait commands. I would still get some advancement to the next page when the first page wasn't complete yet. Am I missing something with the WAITPAGECOMPLETE command or am I better off using the Wait command?",https://forum.imacros.net/viewtopic.php?f=7&t=27044&sid=01bf0edf807fd3d4bb507a35b87155ad,AI
2471,iMacros Script to Extract Specific Text From Static Position,"Problem Description Information:
1. iMacros Version: iMacros for Firefox 8.9.7 (VERSION BUILD=8970419)
2. Windows 10 (64-bit) [English]
3. Firefox 45.9.0
4. Included demos work ok.
5. Included VBS sample scripts run ok.
6. Not applicable/no specific recording or replay fails on a specific website.
7. Not applicable/no test for encounter of the same problem with the: iMacros Browser, iMacros for Internet Explorer, iMacros for Firefox, and iMacros for Chrome.

Problem:
I am trying to write an iMacros script that checks a webpage, and in that webpage checks a specific webpage position for a specific text that is positioned always to the right of a naming text that always has the same position on that webpage (naming text is: ""Location:"") where the value of this specific text to the right of that naming text can change between 1 to 26 different values (say: A-Z), Once iMacros ""reads"" this specific text, then have iMacros take that specific text and look it up in a specific Excel file where that specific text has specific values (say: A to Z) located in column A and then have iMacros return the corresponding output value from column B (say: 101-126) on the same row.
How would I script this in iMacros?

So far, using the iMacros built in recorder I have made the following progress in iMacros (webpage image with iMacros recorder ""blue boxes"" listed below):
Script:
Code: Select allVERSION BUILD=8970419 RECORDER=FX
TAB T=1
URL GOTO=https://URL.aspx
TAG POS=1 TYPE=DIV ATTR=ID:SectionHeader
TAG POS=1 TYPE=SPAN ATTR=ID:tip_0
FRAME F=2
TAG POS=1 TYPE=A ATTR=TXT:Communication2
WAIT SECONDS=1
TAG POS=1 TYPE=TD ATTR=TXT:Location:
WAIT SECONDS=1
TAG POS=1 TYPE=TD ATTR=TXT:SPECIFIC-TEXT
WAIT SECONDS=1

I have pasted below an image of the webpage I am trying to script for; where it needs to read the ""specific text"" located to the right of the text ""Location:"" where that text is always in the same position on the webpage and always to the right of ""Location:"" - how would I script this in iMacros?
Webpage Image:







EDIT1: Thanks Chivracq, changes made.

EDIT2: Chivracq, thanks for posting in my PM thread. I wanted to PM you to ask you to update your reply on my previous thread, so as to edit your reply in that previous thread to requote my updated edited original post as there was some extraneous information that I needed to edit out that should have been edited out when I first posted that thread, so I updated that previous thread. If you would be able to edit your original reply to requote my updated edited original post; I can then post my solution to that thread, so others can benefit. I am thankful to you for helping me solve that issue in that thread. Thanks for all the help!",https://forum.imacros.net/viewtopic.php?f=7&t=28432&sid=01bf0edf807fd3d4bb507a35b87155ad,AI
2477,Problems with Tab opening and find the right atribute,"So, I'm kinda new to Imacros and have only basic knowledge. Long story short, from the  exhibitor list i have, on the exhibition site, i need to extract info about 'em.

https://ces18.mapyourshow.com/7_0/searc ... endrow=200 that's the list.

What i'm tring to do, is to make imacros open exhibitors page in a new tab, scrap the info i need, close it, and repeat with next inline 


 prim1 
 , but i can't find the way to do that, since href's or position arent specified. 
But that is just a part of the problem. Once in that page, i could tell imacros what to copy BUT the email, which i kinda need the most since it's position isnt specified either. 


 prim2 
 


 prim3 


Any suggestion would be a huge help.",https://forum.imacros.net/viewtopic.php?f=7&t=28304&sid=01bf0edf807fd3d4bb507a35b87155ad,AI
2480,how to save data with auto reload option,"Hi all i am looking to extract data from a page called random.php

http://www.quotationspage.com/random.php3

its has quotations by famous people 
if we reload a new set of quotes are displayed 
i want to extract all the quotes available in that please guide how to....
I have imacros installed in chrome

thanks in advance",https://forum.imacros.net/viewtopic.php?f=7&t=28298&sid=01bf0edf807fd3d4bb507a35b87155ad,AI
2487,"Help with undefined error, Eval & Regex","Imacros v8.9.7 on FF ver 55.0.3 32-bit, Win 7 32

Hello;
I use EVAL function and Regex to extract emails, I get undefined error
what am I doning wrong here?
I have tested the Regex code in a regex tester site, and it was OK.
Code: Select allTAG POS=1 TYPE=TEXTAREA ATTR=ID:txt_Dw EXTRACT=HTM
SET !VAR1 EVAL(""'{{!EXTRACT}}'.'.match([a-z0-9_\.-]+)@([\da-z\.-]+)\.([a-z\.]{2,6});"")                      
PROMPT {{!VAR1}}

I have tried another regex code for the EVAL function for the same purpose from the following thread, but it only returns one email.  
http://forum.imacros.net/viewtopic.php? ... 023#p64023

Appreciate your help,
Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=28066&sid=01bf0edf807fd3d4bb507a35b87155ad,AI
2488,Negative Relative Extraction Problem,"Hi All,
A total Newbie to iMacros and looking for some help from a more experienced user.

I have a problem with the negative relative extraction on eBay.com

Windows XP SP2
iMacros v6.2
Office 2003 - Using Excel VBA to control the iMacros Browser

I was trying to extract data from the ebay.com website.  The listings are in batches of 50.  I can extract the first 49 items and then it fails on the 50th.  I have tried displaying 100 auctions to a page and it extracts the first 99 items and fails on the 100th - - the 50th items extracts OK in this instance.  I have looked at the source of the eBay page and can see no difference in the formatting of the last item on each page.

To replicate, goto http://www.ebay.com and search for 'toner' (for example) -- at the time of writing there are 24,000+ items for 'toner' split over multiple pages.  I have created a sample macro which demonstrates the problem.  It extracts items 1, 2, 49, and 50 (note:  In my VBA code, I use a loop and pass a variable called Pos to iMacro, change the page and then repeat the process).  The logic I use is to find the time left on the page and then using negative relative positioning to extract the link which conforms to the following syntax - http://cgi.ebay.com/*.

Once you have navigated to the search results, run the simple macro below from within the browser.
Code: Select allTAG POS=1 TYPE=SPAN ATTR=CLASS:time*
TAG POS=R-1 TYPE=A ATTR=TXT:*&&HREF:http://cgi.ebay.com/* EXTRACT=HREF
TAG POS=2 TYPE=SPAN ATTR=CLASS:time*
TAG POS=R-1 TYPE=A ATTR=TXT:*&&HREF:http://cgi.ebay.com/* EXTRACT=HREF
TAG POS=49 TYPE=SPAN ATTR=CLASS:time*
TAG POS=R-1 TYPE=A ATTR=TXT:*&&HREF:http://cgi.ebay.com/* EXTRACT=HREF
TAG POS=50 TYPE=SPAN ATTR=CLASS:time*
TAG POS=R-1 TYPE=A ATTR=TXT:*&&HREF:http://cgi.ebay.com/* EXTRACT=HREF


The first 3 items - 1,2 and 49 extract correctly.  The last item - 50th - fails with #EANF#.

Any suggestions.  I doubt it is a bug with iMacros, but any pointers on my code to do the extract would be appreciated.

Cheers
Steve  ",https://forum.imacros.net/viewtopic.php?f=7&t=5197&sid=01bf0edf807fd3d4bb507a35b87155ad,AI
2489,"Extract Formatting, curious if there is a better way.","Hiya,

FF = 56.0 (64-bit)
IM = 9.0.3 for Firefox
OS = Windows 7 Professional SP1

I finally got this script to work (ish, under ideal conditions). The problem arises when there are gaps in the extracted data (missing tags), which is totally fine, but it messes up the formatting in the .csv. The columns are all shifted right by the number of tags that are missing.

URL is question: https://www.ontario.ca/environment-and- ... ll-records

Once a region is selected, the relevant records are listed, and then I loop this script for the number of records.

My Script:
Code: Select all
'VARIABLE SETUP
SET !EXTRACT_TEST_POPUP NO
SET !ERRORIGNORE YES
SET !TIMEOUT_PAGE 60
SET DELAY 0.2
SET !LOOP 1
SET STEP 7
SET WELLID EVAL(""{{STEP}}*{{!LOOP}}-6"")
SET DATE EVAL(""{{STEP}}*{{!LOOP}}"")
SET DEPTH EVAL(""{{STEP}}*{{!LOOP}}-1"")

'LOOPS THOUGH THE MAIN TABLE LIST
'=================================
TAG POS={{WELLID}} TYPE=TD ATTR=* EXTRACT=TXT
WAIT SECONDS={{DELAY}}
ADD !EXTRACT //
WAIT SECONDS={{DELAY}}

TAG POS={{DATE}} TYPE=TD ATTR=* EXTRACT=TXT
WAIT SECONDS={{DELAY}}
ADD !EXTRACT //
WAIT SECONDS={{DELAY}}

TAG POS={{DEPTH}} TYPE=TD ATTR=* EXTRACT=TXT
WAIT SECONDS={{DELAY}}
ADD !EXTRACT //
WAIT SECONDS={{DELAY}}

'RESET POSITION
TAG POS={{WELLID}} TYPE=TD ATTR=* 
WAIT SECONDS={{DELAY}}
'CLICK ON HTML RECORD
TAG POS=R1 TYPE=A ATTR=TXT:HTML
WAIT SECONDS={{DELAY}}

'PAGE LOAD ALLOWANCE
WAIT SECONDS=1.5 

'PULLS REST OF WELL DATA FROM HTML RECORD
'========================================
'Well Type (Domestic, Industrial, etc.)
TAG POS=1 TYPE=TH ATTR=TXT:Well<SP>Use
WAIT SECONDS={{DELAY}}
TAG POS=R4 TYPE=TD ATTR=* EXTRACT=TXT
WAIT SECONDS={{DELAY}}
ADD !EXTRACT //
WAIT SECONDS={{DELAY}}

'Well Status (Water Supply, Observation, etc.)
TAG POS=6 TYPE=P ATTR=* EXTRACT=TXT
WAIT SECONDS={{DELAY}}
ADD !EXTRACT //
WAIT SECONDS={{DELAY}}

'Water found at depth
TAG POS=1 TYPE=TH ATTR=TXT:Kind
WAIT SECONDS={{DELAY}}
TAG POS=R1 TYPE=TD ATTR=* EXTRACT=TXT
WAIT SECONDS={{DELAY}}
ADD !EXTRACT //
WAIT SECONDS={{DELAY}}

'Static Water Level
TAG POS=1 TYPE=TD ATTR=TXT:SWL
WAIT SECONDS={{DELAY}}
TAG POS=R1 TYPE=TD ATTR=* EXTRACT=TXT
WAIT SECONDS={{DELAY}}
ADD !EXTRACT //
WAIT SECONDS={{DELAY}}

'Pump Rate
TAG POS=1 TYPE=TH ATTR=TXT:Pumping<SP>Rate
WAIT SECONDS={{DELAY}}
TAG POS=R1 TYPE=TD ATTR=* EXTRACT=TXT
WAIT SECONDS={{DELAY}}
ADD !EXTRACT //
WAIT SECONDS={{DELAY}}

'Recommended Pump rate
TAG POS=1 TYPE=TH ATTR=TXT:Recommended<SP>pump<SP>rate
WAIT SECONDS={{DELAY}}
TAG POS=R1 TYPE=TD ATTR=* EXTRACT=TXT
WAIT SECONDS={{DELAY}}

'Save Record
SAVEAS TYPE=EXTRACT FOLDER=* FILE=WELL_RECORDS_{{!NOW:ddmmyyyy}}.csv
WAIT SECONDS={{DELAY}}

'RETURN TO THE MAIN TABLE
TAG POS=1 TYPE=A ATTR=TXT:Go<SP>Back<SP>to<SP>Map&&ONCLICK:MOECC_UI.goBacktoMap()

WAIT SECONDS=1.5

The only way I have found that kind of works is to append a 'spacer' (""//"") after each extract, which at least allows me to count back the number of columns to reformat with the missing info, and then deleting all the placeholder columns. This can get very tedious when dealing with hundreds of records. 

Is there not a better way?

Thanks in Advance,",https://forum.imacros.net/viewtopic.php?f=7&t=28009&sid=01bf0edf807fd3d4bb507a35b87155ad,AI
2492,extracted data saving to one cell?,"Hello everyone,

I recently just started using imacro. I am trying to develop a script that extracts a bunch of emails from Google and saves them to a .csv file. However, when it creates the .csv file all the emails are saved in one cell, the first one . Any help is appreciated, thank you!  ",https://forum.imacros.net/viewtopic.php?f=7&t=22565&sid=01bf0edf807fd3d4bb507a35b87155ad,AI
2495,"Different behaviour - scripting vs manual, with AJAX request","Hi all

Our ISP provides an online web site, where you can download a CSV of usage data.
I am attempting to write a tool that extracts this data for each of our customers only a regular basis, and injects it into a database so they can easily retrieve a graphical representation

There are two views of data - summary and detailed.
When I login to the website manually, the process is URL for login screen -> TAG enter details -> click A 'Detailed' -> click A 'Download Usage' -> click BTN 'Download'
The CSV that downloads shows detailed usage data.

I have built this behaviour into iMacros, using recording and some manual adjustments for additional waiting.
But the downloaded CSV is always only the 'summary' data - not the detailed.

I have confirmed by watching the script that iMacros does trigger the click on the A tag 'Detailed', which in theory is what 'switches' the behaviour of the Download button to download detailed data.
But the downloaded CSV is only ever summary view. I can even perform the same behaviour side-by-side, in a Chrome window manually on the right and iMacros 12 built-in browser on the left, and the output from iMacros is summary whereas the output from Chrome is detailed. Interestingly though - when I navigate the page using the iMacros built-in browser manually, it correctly registers that I've clicked on 'Detailed' view and downloads the full detailed CSV.

The website is very AJAX-heavy, to load in controls and data (hence having to build in plenty of additional WAIT commands). 
My thinking is that the built-in iMacros browser is not handling some form of local cache/cookie/session storage command from the website, whereas Chrome et al does.
As an example, in the Chrome developer when switching back forth between 'summary' and 'detailed' view (before downloading the CSV) this error is logged;
VM127:3 [Deprecation] 'window.webkitStorageInfo' is deprecated. Please use 'navigator.webkitTemporaryStorage' or 'navigator.webkitPersistentStorage' instead.

If I'm right, and the website is storing the 'current' view (detailed) in some kind of local storage, is there any way to replicate this behaviour in iMacros?

I've checked the 'Application' tab in Chrome developer console, and can't see any obvious setting stored in Local Session Storage or Cookies, just a SESSION_ID which doesn't help.
I've also tried to run my iMacros script in the main 3 browsers;
- IE 11 worked to load the web page, but no fields were ever filled in on the login page
- Chrome & Firefox were completely unresponsive when I click 'Play in -> ...' in the Script Editor

Only the built-in iMacros browser works, and it comes very close to what's needed by just refuses to 'remember' that it's clicked 'Detailed View' before it downloads
Note that I've tried clicking both the 'Detailed' and 'Download' buttons as both TAGs and EVENTs

I'm using iMacros 12 on Windows 10
If anyone has any suggestions relating to tricks for handling AJAX-heavy pages, ideas about how to force iMacros to 'fully register' a click on an AJAX page, or tips for getting iMacros to work with Chrome on Windows 10 I'd appreciate it

Regards,
Sam",https://forum.imacros.net/viewtopic.php?f=7&t=27904&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2496,Zooming,"New iMacros user here.  I have a web page that is being displayed differently on standard aspect monitors than on their widescreen counterparts (the components I'm using in the macro aren't even present in the trunctated version!).  I have tried various resizing to no avail, but have noticed that when I zoom out the web page is rebuilt with the components I need!  I've searched these forums and the wiki to no avail.  I have tried each recording method and none of them pick up my zooming (ctrl + / ctrl -).  Has anybody successfully gotten zooming to work with IE?

Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=27938&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2498,Extract email but only if not a duplicate in my CVS file?,"Hi I am going nuts trying to figure out how to extract email from a website but only if that email isn't already in my CVS file.  If the email is already there can I stop the iMacro and have it restart from the begging?

To extract and save I'm using and it works:
Code: Select allTAG POS=1 TYPE=P ATTR=CLASS:""tel-number"" EXTRACT=TXT

SAVEAS TYPE=EXTRACT FOLDER=/Users/Me FILE=cl-num.csv

How would I go about checking if the email is already in my CVS?  And if it is make the iMacro restart?

Thanks everyone 


.",https://forum.imacros.net/viewtopic.php?f=7&t=27867&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2499,Dropdown list not triggered,"Hi All

I'm new to this so please excuse any obvious errors. I'm trying to do something quite basic - get to a train schedule web page, which shows the delays for various lines. I've made a basic script with the iMacros record. When running, it reaches the page, but will not populate the dropdown list to select the line captured during the iMacros record process. I included a 5 second wait to allow any AJAX or dynamic code to load. Here's the script to select the Central line:
Code: Select allVERSION BUILD=9030808 RECORDER=FX
TAB T=1
URL GOTO=https://app.gometro.co.za/#/chooseline
WAIT SECONDS=5
TAG POS=1 TYPE=LI ATTR=TXT:Central


The page uses the React javascript framework (which I'm not familiar with) and the line selection code looks as follows:
Code: Select all<div class=""mdl-grid wrapper""><div class=""mdl-cell mdl-cell--12-col""><!-- react-empty: 860 --><div class=""main-heading"">LINE UPDATES - CHOOSE A LINE</div><div id=""choose-a-line"" style=""display: block;""><div class=""mdl-selectfield""><div class=""mdl-textfield mdl-js-textfield is-upgraded"" data-upgraded="",MaterialTextfield""><input class=""mdl-textfield__input"" id=""textfield-Selectaline"" readonly="""" value="""" type=""text""><label class=""mdl-textfield__label"" for=""textfield-Selectaline"">Select a line</label></div><i class=""mdl-selectfield__arrow""></i></div></div></div></div>


So I'd like to ask the forum members is anyone has suggestions for resolving this?

Many thanks
Cliff",https://forum.imacros.net/viewtopic.php?f=7&t=27850&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2500,Determine row in which a string is in an HTML table,"How can I determine in which row of a table a certain string is and save it into a variable using iMacros? (and use that position to later click a link with iMacros)

Thanks! ",https://forum.imacros.net/viewtopic.php?f=7&t=27649&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2501,NEWBIE : How to Loop and Click Next?,"first time using iMacros.

I want to run a loop 50x then when it is done, 
click the ""Next"" button and run the same loop again 50x, 
click ""Next""... and run until ""Next"" is no longer clickable.

so far i have this working for 1x loop.  
but i cant get it to run the text extraction+CSV export 50x.. THEN click NEXT.

Code: Select allSET !LOOP 1

TAG POS={{!LOOP}} TYPE=TD ATTR=CLASS:domain EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=Extract_{{!NOW:ddmmyy}}.csv

'click NEXT'
TAG POS=1 TYPE=A ATTR=TXT:Next

WAIT SECONDS=3",https://forum.imacros.net/viewtopic.php?f=7&t=27824&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2506,iimGetLastExtract() doens't work,"Hello,

My desktop is GNU/Linux debian testing, Firefox 45.9.0 and iMacros for Firefox 8.9.7.

I'm new in Imacros and I'm trying to extract -as a trainning- the data from a table on a website and different tabs.  I have encountered a problem with EXTRACT command combined with iimGetLastExtract(n).

The code can extract (PROMPT {{!EXTRACT}} show value) data and save in .csv file, but  iimGetLastExtract(n) only read null value for all data extracted, and the javascript logical operator's code doens't work.  But, if I insert SET !EXTRACTDIALOG YES javascript works fine but doens't save data in .csv file.
Code: Select all// Carga del website
var macro1 = ""CODE:"";
macro1 += ""TAB T=1\n"";
macro1 += ""URL GOTO=http://es.global-rates.com/estadisticas-economicas/inflacion/indice-de-precios-al-consumo/ipc/ipc.aspx\n"";
macro1 += ""SET !TIMEOUT_STEP 1\n"";

// Extracci贸n de datos	
var macro2 = ""CODE:"";
macro2 += ""SET !TIMEOUT_STEP 1\n"";
macro2 += ""TAG POS=1 TYPE=TD ATTR=TXT:{{i}} EXTRACT=TXT\n"";
macro2 += ""TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";
macro2 += ""TAG POS=R2 TYPE=TD ATTR=TXT:* EXTRACT=TXT\n"";
macro2 += ""SET !EXTRACTDIALOG YES\n"";
macro2 += ""SAVEAS TYPE=EXTRACT FOLDER=* FILE=VISORD_o.csv\n"";

// Ir a nueva URL	
var macro3 = ""CODE:"";
macro3 += ""TAB OPEN NEW\n"";
macro3 += ""TAB T=2\n"";
macro3 += ""URL GOTO=http://www.google.com \n"";
macro3 += ""TAG POS=2 TYPE=DIV ATTR=TXT:Google.es* EXTRACT=TXT\n"";
macro3 += ""SAVEAS TYPE=EXTRACT FOLDER=* FILE=VISORD_o.csv\n"";
macro3 += ""TAB T=1\n"";
macro3 += ""TAB CLOSEALLOTHERS\n"";

for (l=1;l<2;l++)
{
	iimPlay(macro1);
	
	for (n=1;n<8;n++)
	{
		var i = [""Alemania"",""Austria"",""Brasil"",""Corea del Sur"",""Dimamarca"",""Eslovaquia"",""Jap贸n""];		
		iimSet(""i"",i[n-1]);
		iimPlay(macro2);
		var date = iimGetLastExtract(2);
		//alert(date);
		if (date == 'mayo 2017')
		{
			iimPlay(macro3);
		}
	}
}

I have read the FAQ's and  SET !EXTRACTDIALOG YES is not supported by Firefox.

Would someone be able to have a look at this and see what I'm doing wrong?  Many thanks for your time and efforts.",https://forum.imacros.net/viewtopic.php?f=7&t=27628&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2507,Getting function Expected when tyring to use reverse(),"Configuration: Surface Pro 2, Win 10, Firefox 53, iMacros Standard Edition (x86) Version 11.0.246.4051
Hi,
I am trying to split out the house address from the total extract.
The Web page HTML has a line break<br> between the name line section and start of address, but when extracted it doesn't appear as extracted=HTM shows the two fields run together ""<SP>lastnamehousenumber<SP>"" with no delimiter. However, I was able to split them using regex on white space.
Since the address fields are always represented the same way my goal is to separate house address, reverse it and split by index, o being zip, 1 being state, 2 being city and 3 the address. That way I don't have to worry about complex address parsing.

Here is the code I have so far. 
Code: Select allVERSION BUILD=11.0.246.4051
TAB T=1
TAB CLOSEALLOTHERS
SET !EXTRACT_TEST_POPUP NO
'URL GOTO=https://www.protk.com/ProTeck.LocalVendor.Web/Home/Show?pageName=Cases
FRAME F=1
TAG POS=2 TYPE=TD ATTR=CLASS:ReportManagerTable_rows EXTRACT=TXT
SET Trim_spces EVAL(""var x,y,z; x=\""{{!EXTRACT}}\""; y=x.replace(/^\\s+|\\s+$/gm,'');y;"")
SET Split_spce EVAL(""var w,x,y,z; w = \""{{!EXTRACT}}\""; y = w.split(' ');y;"")
SET raw_address EVAL(""var x,y,z; x = '{{!EXTRACT}}'; y = x.replace(',',' ');y;"")
'***************************GET RID OF WHITE SPACE BETWEEN NAME LINE AND HOUSE NUMBER*********************************************
SET Sub_split_NME_NBR EVAL("" var a,x,y,z;  x = '{{Split_spce}}'; y=x.split(/\\s/g); z = y[1];z;"")
'**********TRIM WHITESPACES AT BEGINNING AND END OF STRING ADDRESS******************************************************
SET Trim_spces EVAL(""var x,y,z; x=\""{{Sub_split_NME_NBR}}\""; y=x.replace(/^\\s+|\\s+$/gm,'');y;"")
SET Rev_addr EVAL(""var x,y,z; x = '{{Sub_split_NME_NBR}}'; z = x.reverse(); z;"")

Here is the error code I get and it must have to do with the reverse() looking like a function call(?). I can't figure out how to get around it.
Code: Select allError -1250: JScript statement in EVAL contains the following error: Function expected. Line 23: SET Rev_addr EVAL(""var x,y,z; x = '{{Sub_split_NME_NBR}}'; z = x.reverse(); z;"")

I have attached a JPG with the ""PROMPT"" records for the !EXTRACT and variables.
Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=27634&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2508,Resume extraction though Firefox browser crash,"Hi everyone, i'm facing issue with Firefox browser. Once the browser crashes , I have to start all over with my extraction.

Is there a way for me to continue extracting from where it stopped rather than starting all over again from the beginning using JavaScript?",https://forum.imacros.net/viewtopic.php?f=7&t=27555&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2509,Extract to specific cells in excel,"TAG POS=5 TYPE=A ATTR=TXT:* EXTRACT=txt
SAVEAS TYPE=EXTRACT FOLDER=* FILE=test.csv


Gives extract perfectly and confirm in popup box. Only it places the extract in the A2 column in excel. For example I  need it to go to F20 and next time I run it, I need it to over write f20 again. Ultimately just always be in that specific cell. I have googled and googled and surprisingly do not see anyone else requesting this feature?

Any help would be appreciated.

I tried adding SET !EXTRACT {{!COL6} but that did not help.

I am using firefox 53.0.3
9.0.3 Imacro for firefox.
Win10 english
no other issues with imacros or any other macros ive been using.",https://forum.imacros.net/viewtopic.php?f=7&t=27544&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2511,"New to E-commerce website Extraction, where do i begin","Hello

I have a massive project with extracting products from E-Commerce websites

What i need to do

Select Each product in a given category to open Detail page and extract information(Text or Image)
Select pagination where there are more than one page in one category and continue on next page 

Are there any tutorial/video lessons available that explain process

I have knowledge with HTML/XPath etc., I just need to find some example how to cycle through an E-Commerce site

Any help will be appreciated

Tom",https://forum.imacros.net/viewtopic.php?f=7&t=27521&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2512,Capturing EXTRACT data,"iMacros Version V. 844
Browser Google Chrome versions is: 58.0.3029.110 (64-bit)
Operating System macOS Sierra V. 10.12.5
Included Demos work ok (all of them)

Also tried briefly on a Windows 10 system with Chrome with the same results. All demos worked.


Is there a way to put user-entered data into a variable?

I had been using PROMPT to capture user input data as a variable...

PROMPT ""Please enter text:"" !VAR1

Unfortunately, PROMPT no longer works with the latest Chrome update. 

I've come up with the following workaround that EXTRACTs the user input from an html form I created.  But I can't pass that data along because EXTRACT is not a variable and IEXTRACT does not work within a TAG.  I've spent a lot of time with this and I haven't found a way to do this.  Is there a way to load TAG EXTRACT'd data into a variable?

This is what I am trying to do...

URL GOTO=C:\Users\username\Desktop\FormTest.html
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:form1 ATTR=NAME:abc EXTRACT=TXT 

SET !VAR1 {{!EXTRACT}}

TAB OPEN NEW
TAB T=2
URL GOTO=http://www.forvo.com/word/{{!VAR1}}/#ar

No luck with this, even though I found some demo samples that said it should work.

Any suggestions would be appreciated. Thank you.",https://forum.imacros.net/viewtopic.php?f=7&t=27519&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2515,Extract links in separate lines,"Hello, how to save in file one link per line?
Code: Select allSET !EXTRACT_TEST_POPUP NO
SET !ERRORIGNORE YES
TAG POS=1 TYPE=A ATTR=CLASS:marginr EXTRACT=HREF
TAG POS=2 TYPE=A ATTR=CLASS:marginr EXTRACT=HREF
WAIT SECONDS=1
SAVEAS TYPE=EXTRACT FOLDER=D:\ FILE=links.txt


imacros 9.0.3, firefox, windows10 64",https://forum.imacros.net/viewtopic.php?f=7&t=27473&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2517,Web Scraping Table With SPAN Class,"Hello iMacros Community,

I'm brand new to iMacros, but I have spent a lot of time watching YouTube videos and browsing this forum. However, I haven't been able to figure out how to do this quite yet.

My company uses an eCRM system that loads via Internet Explorer 11, and I am able to get iMacros to go through all of the login process and get to the point where I'm ready to extract records. I can even get it to extract the first row of records. However, I cannot get it to go down to the next row, and extract it. Each of my attempts either involve it scraping the first line multiple times, or in an #EANF# error after the first row.

My code is below:
Code: Select allVERSION BUILD=11.5.498.2403
TAB T=1
TAB CLOSEALLOTHERS
FRAME NAME=WorkAreaFrame1

'Table fields to extract in order. [objectid] is the unique index for each record
TAG POS={{!LOOP}} TYPE=SPAN ATTR=ID:C17_W66_V67_V76_items_table[1].objectid EXTRACT=TXT
TAG POS={{!LOOP}} TYPE=SPAN ATTR=ID:C17_W66_V67_V76_items_table[1].createdon EXTRACT=TXT
TAG POS={{!LOOP}} TYPE=SPAN ATTR=ID:C17_W66_V67_V76_items_table[1].sender EXTRACT=TXT
TAG POS={{!LOOP}} TYPE=SPAN ATTR=ID:C17_W66_V67_V76_items_table[1].description EXTRACT=TXT
TAG POS={{!LOOP}} TYPE=SPAN ATTR=ID:C17_W66_V67_V76_items_table[1].status EXTRACT=TXT
TAG POS={{!LOOP}} TYPE=SPAN ATTR=ID:C17_W66_V67_V76_items_table[1].employee_concat EXTRACT=TXT
TAG POS={{!LOOP}} TYPE=SPAN ATTR=ID:C17_W66_V67_V76_items_table[1].group_concat EXTRACT=TXT
'TAG POS={{!LOOP}} TYPE=A ATTR=ID:C17_W66_V67_V76_items_table[1].ITEMTYPE EXTRACT=TXT
'TAG POS={{!LOOP}} TYPE=SPAN ATTR=ID:C17_W66_V67_V76_items_table[1].priority EXTRACT=TXT

SAVEAS TYPE=EXTRACT FOLDER=T:\TELESERV\Continuous<sp>Improvement<sp>Db<sp>Files\TCS-KNX<sp>Daily<sp>Digest FILE=Extract_{{!NOW:ddmmyy_hhnnss}}.csv

'This is the row selector
'TAG POS=1 TYPE=A ATTR=ID:C17_W66_V67_V76_ItemTree_sel_1-rowsel EXTRACT=TXT

'Forward button for next page of results. Need to push if exists, and grab next list.
'TAG POS=4 TYPE=SPAN ATTR=CLASS:th-clr-span EXTRACT=TXT

I do have some commented out lines, until I can figure out how to get the scraping of one list of results. The hard part is that this list can contain up to a maximum of 200 results. If there are more than 200 results, it would need to push the ""Next"" button (TAG POS at the very bottom of the code above), then extract those results as well until the ""Next"" button could not be pressed again.

Each row is indicated in the string at the very end, surrounded in []. So, Code: Select allTAG POS={{!LOOP}} TYPE=SPAN ATTR=ID:C17_W66_V67_V76_items_table[1].objectid EXTRACT=TXT would then go to the below for the next line, and so on. Code: Select allTAG POS={{!LOOP}} TYPE=SPAN ATTR=ID:C17_W66_V67_V76_items_table[2].objectid EXTRACT=TXT 
Below is a picture of how our eCRM GUI is set up. 



 eCRM Table Setup 

Although I'm not grabbing every field, the names from the headers should give you an idea of the text that exists within them.

Any help/direction you all could provide would be greatly appreciated.

Thanks!",https://forum.imacros.net/viewtopic.php?f=7&t=27445&sid=8ea127909c0d83c2fac4f53ec6c3d19a,AI
2520,TAG and SEARCH parameters,"And now for Part II of http://forum.imacros.net/viewtopic.php?f=7&t=27429
One possible outcome of a friend search on FriendlyFriends.com is that a friend was not found. In that case, the resulting web page contains the following:
<div id=""FriendListing"" class=""NoFriend"">
	<p class=""NoFriendSummaryText"">I'm sorry, but your friend was not found.
        </p>
</div>
If I get this result then I want to append the csv record with some relevant notation but I first have to detect this result instead of other possible results. I tried the following but neither work:
SEARCH SOURCE=TXT:""'NoFriendSummaryText'"" IGNORE_CASE=YES
TAG POS=1 TYPE=A ATTR=TXT:*sorry*
Both the macro fails at both cases at the above line.
I think either command will work and I think I just need help with the parameters. I found the above by searching the demo macros and command reference and forum but didn't find an exact solution. Thanks in advance for any help fixing the parameters (ideally for both options).
iMacros for FF v9.0.3, FF52-x32, Win7-x64",https://forum.imacros.net/viewtopic.php?f=7&t=27433&sid=41f2b858fcf7ca695bb39ee1420df70b,AI
2522,Scrapping articles via RSS,"I want to scrap a website (http://pkjobvacancy.com/). Everytime a new article is posted on this website I want to copy the text and image of that post, and send me via email on my email address. Please someone give me script for this..
Thank you",https://forum.imacros.net/viewtopic.php?f=7&t=27383&sid=41f2b858fcf7ca695bb39ee1420df70b,AI
2526,Trim leading and trailing spaces,"good morning friends,
I have the following code


The extraction of this page https://notariad1970.000webhostapp.com/ ... cipal.html
Code: Select allVERSION BUILD=844 RECORDER=CR
TAG POS=1 TYPE=INPUT:TEXT FORM=ID:form1 ATTR=ID:* EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:lblFechaSistema EXTRACT=TXT
TAG POS=1 TYPE=TD ATTR=TXT:Fecha<SP>de<SP>Registro
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
TAG POS=R1 TYPE=TD ATTR=TXT:* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\oz\Downloads FILE=1 

The result of the extraction is as follows
Code: Select all54001333300120140059801,""mi脙漏rcoles, 08 de febrero de 2017 - 09:14:55 p.m."",""
15 Sep 2015 
"",""
ENVIO EXPEDIENTE DESPACHO DE ORIGEN 
"",""
MEDIANTE OFICIO M-9186 SE DEVUELVE EL EXPEDIENTE AL JUZGADO DE ORIGEN. 
"",""

"",""

"",""
15 Sep 2015 
""


But I want to eliminate spaces and jumps, so that it stays this way
Code: Select all54001333300120140059801,""mi脙漏rcoles, 08 de febrero de 2017 - 09:14:55 p.m."",""15 Sep 2015"",""ENVIO EXPEDIENTE DESPACHO DE ORIGEN"",""MEDIANTE OFICIO M-9186 SE DEVUELVE EL EXPEDIENTE AL JUZGADO DE ORIGEN."","""","""",""15 Sep 2015""

I try this code eva, but it did not work

Code: Select allSET !EXTRACT ""    This value has leading and trailing spaces     ""
' Display the value within vertical bars to emphasize the spaces
PROMPT |{{!EXTRACT}}|
SET trimmedValue EVAL(""\""{{!EXTRACT}}\"".replace(/^\\s*|\\s*$/g, \""\"");"")
PROMPT ""|{{trimmedValue}}| - not anymore!""",https://forum.imacros.net/viewtopic.php?f=7&t=27237&sid=41f2b858fcf7ca695bb39ee1420df70b,AI
2528,"this.dataSource[(line - 1)] is undefined, line: 11 (Error co","Error: this.dataSource[(line - 1)] is undefined, line: 11 (Error code: -1001)
Firefox Plugin
Code: Select allSET !DATASOURCE X:\Test2.csv
SET !LOOP 1
SET !DATASOURCE_COLUMNS 1
SET !DATASOURCE_LINE {{!LOOP}}
SET !VAR1 {{!COL1}}
ADD !EXTRACT {{!LOOP}}
'--------- test2
SET !DATASOURCE X:\Test1.csv
SET !DATASOURCE_COLUMNS 2
SET !LOOP 1
SET !VAR3 {{!COL2}}
SET !VAR2 {{!COL1}}
URL GOTO=http://XXXXXXXXXXXX.com/xxxxx/{{!VAR3}}?xxxxxxx{{!VAR2}}{{!VAR1}}xxxxxxxxxx
ADD !EXTRACT {{!VAR2}}{{!VAR1}}
WAIT SECONDS=0.25
TAG POS=1 TYPE=P ATTR=CLASS:rateType EXTRACT=TXT
TAG POS=1 TYPE=P ATTR=CLASS:rateAmount EXTRACT=TXT
TAG POS=1 TYPE=DIV ATTR=ID:requestedRateText EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=X:\ FILE=Output_{{!VAR2}}.csv

So the run first work but on the second the error shows.
Test1.csv contain only 1 line with ""test123,test456""",https://forum.imacros.net/viewtopic.php?f=7&t=27228&sid=41f2b858fcf7ca695bb39ee1420df70b,AI
2530,TABLE: Extract Data Row by Row...,"iMacros: 10.4.28.1074
Firefox: 50.1.0
OS: Windows 8.1 Pro


I come up with a solution like this to extract the first 5 row of a table from this website: ""https://datatables.net/examples/data_so ... array.html""

My question: Is there a way to shorten the query like to extract the full row instead of every cell in a row?
Code: Select allVERSION BUILD=10.4.28.1074
TAB T=1
TAB CLOSEALLOTHERS
SET !EXTRACT_TEST_POPUP NO
URL GOTO=https://datatables.net/examples/data_sources/js_array.html
TAG POS=1 TYPE=TD ATTR=TXT:Airi<SP>Satou
TAG POS=1 TYPE=TD ATTR=CLASS:sorting_1 EXTRACT=TXT
TAG POS=2 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=3 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=4 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=5 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=6 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=2 TYPE=TD ATTR=CLASS:sorting_1 EXTRACT=TXT
TAG POS=8 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=9 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=10 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=11 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=12 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=3 TYPE=TD ATTR=CLASS:sorting_1 EXTRACT=TXT
TAG POS=14 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=15 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=16 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=17 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=18 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=4 TYPE=TD ATTR=CLASS:sorting_1 EXTRACT=TXT
TAG POS=20 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=21 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=22 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=23 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=24 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=5 TYPE=TD ATTR=CLASS:sorting_1 EXTRACT=TXT
TAG POS=26 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=27 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=28 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=29 TYPE=TD ATTR=* EXTRACT=TXT
TAG POS=30 TYPE=TD ATTR=* EXTRACT=TXT",https://forum.imacros.net/viewtopic.php?f=7&t=27207&sid=41f2b858fcf7ca695bb39ee1420df70b,AI
2535,Selecte text from paragraph,"Hi, 
I'm using the Firefox Extension(9.0.3) an Firefox 50.1.0

I have folowing HTML:
Code: Select all<p>
first line<br /> second line<br /> E-Mail: <a href=""mailto:test@web.com"">test@web.com</a><br>  <a href=""http://www.web.com"" target=""_blank"">http://www.web.com</a><br> <br />
</p>

I have following imacro:
Code: Select allTAG POS=1 TYPE=P ATTR=TXT:* EXTRACT=TXT

When I extract this - on position of the links the text reepats from top:
Code: Select allfirst line
second line
E-Mail:first line
second line
E-Mail:test@web.com
first line
second line
E-Mail:first line
second line
E-Mail:test@web.com
http://www.web.com


How can I avoid this? What am I doing wrong?

Thanks for your help",https://forum.imacros.net/viewtopic.php?f=7&t=27143&sid=41f2b858fcf7ca695bb39ee1420df70b,AI
2536,Need help with extraction from Airlines,"Can someone help me with the Data extraction?
Like: https://bestprice.lufthansa.com/?resolu ... nation=BER#
For example: 	
VIE
Vienna
BER
Berlin
Departing: Apr 22, 2017 - Returning: Apr 23, 2017
1 adult
Economy
from83 鈧?
Any idea? Thanks

EDIT: I will compare the price from different Airlines on there own Homepage.

On the LH Desktop Page some times 5, 6 ,7 Flights go. How to make the different times?
Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=27136&sid=41f2b858fcf7ca695bb39ee1420df70b,AI
2537,Navigating a Report Page,"While I'm still a rookie with iMacros, I've made some solid progress in navigating and extracting needed Web info. Now I'm stuck.
In particular, I am trying to simulate the advancement to the last page of a report. In the default recording mode, an image button is identified for generating the report. When I play the recording, the image button is not found. When I view the raw html, the image button is not present either. I've tried all of the possible iMacro recording options but nothing has worked. Any help is greatly appreciated.

Some details: I/E 11, iMacros 11, Windows 7
I'm using the standalone iMacro product.",https://forum.imacros.net/viewtopic.php?f=7&t=26922&sid=41f2b858fcf7ca695bb39ee1420df70b,AI
2540,Extracting piece of JSON from code,"Hey guys

This really is my last resort, as I tried all different combinations for getting this little script to work.

What I am trying is to extract the part of JSON that is between these brackets [], and including them.
I tried to find out what exactly regex code would look like, and visited regexr.com

code I'm working with:
Code: Select all<script>
	var profitChartData = [{""date"":2011,""profit"":570000,""income"":4578000},{""date"":2012,""profit"":50442000,""income"":127168000},{""date"":2013,""profit"":72790000,""income"":216381000},{""date"":2014,""profit"":18135000,""income"":211685000},{""date"":2015,""profit"":60730000,""income"":457759000}];
</script>
what regexr suggests as solution:
Code: Select allprofitChartData\s\=\s([^]+);

of course it doesn't work...

this is how my non-functional script looks like at the moment:
Code: Select allVERSION BUILD=9030808 RECORDER=FX
TAB T=1

URL GOTO=URL
WAIT SECONDS=5
SEARCH SOURCE=REGEXP:""profitChartData\s\=\s([^]+)"" EXTRACT=""$1""

info:
firefox version: 50.0
imacros for firefox: 9.0.3
windows7 64bit",https://forum.imacros.net/viewtopic.php?f=7&t=26974&sid=41f2b858fcf7ca695bb39ee1420df70b,AI
2541,Extracting when no Name: or ID:,"Is there any way for me to extract the number 16435628 from the code below from a website given there is no Name: or ID:?  Its the on the 5th line down below...  Thanks

Code: Select all<table class=""table table-bordered table-responsive"">
                <thead></thead>
                <tbody data-bind=""foreach: Bills"">
                    <tr data-bind=""css: { altrow: $index()%2 }"">
                        <td style=""background-color: magenta;"" data-bind=""html: DisplayBillNumber""><a href=""BillDetail.html?year=2016&billno=435628&parcel=25000710100000"">16435628</a></td>
                        <td data-bind=""text: DisplayBillType"">Secured Annual</td>
                        <td class=""textright"" data-bind=""html: DisplayLevyAmount""><a href=""DirectLevy.html?year=2016&billno=435628&parcel=25000710100000"">$193.94</a></td>
                        <td class=""textright"" data-bind=""text: DisplayBillAmount"">$1,448.88</td>
                        <td class=""textright"" data-bind=""html: DisplayTaxStatus""><a href=""BillDetail.html?year=2016&billno=435628&parcel=25000710100000"">Click Here</a></td>
                    </tr>
                </tbody>
               ",https://forum.imacros.net/viewtopic.php?f=7&t=26894&sid=41f2b858fcf7ca695bb39ee1420df70b,AI
2542,Scrape phone numbers from contact pages using CEO as keyword,"What I'm doing ? I'm scraping CEO phone numbers from company websites.

What I have ?
This is my code, it currently says when CEO word appear on page, extract next 2 elements,
BUT I want like this -> When CEO text appears in page, extract next element which has numbers.
Code: Select allSet !Timeout_Step 0
Set !ErrorIgnore Yes
Set !Extract_Test_PopUP NO
SET !DATASOURCE tj.csv

Tag Pos=1 Type=* ATTR=Txt:*CEO* Extract=Txt 
Tag Pos=R1 Type=* Attr=* Extract=Txt
Tag Pos=R1 Type=* Attr=* Extract=Txt
Set ceo {{!Extract}}
Set !Extract Null

Add !Extract {{ceo}}

SAVEAS TYPE=EXTRACT FOLDER=D:\imacro\ FILE=ceo.csv

Required answers:Code: Select all1. What version of iMacros are you using?
VERSION BUILD=8871104 RECORDER=FX  (Newest PaleMoon version)

2. What operating system are you using? (please also specify language)
Windows 8.1

3. Which browser(s) are you using? (include version numbers)
Palemoon Version: 26.5.0 (x64)

4. Do the included demo macros work ok? 
Yes

5. If reporting a problem with the Scripting Interface, please also test if the included VBS sample scripts run ok.
N/A

6. If recording or replay fails on a specific website: Can you please post the URL of the web page and/or the imacro that creates the problem? If you can not post the imacro or login data in the public user forum, please email N/A

7. Do you encounter the same problem with the iMacros Browser, iMacros for Internet Explorer and iMacros for Firefox? Note: If your question is specifically about iMacros for Firefox or iMacros for Chrome, please use their sub-forums. 
N/A

help appreciated, br. Mark
PS. Is this message in correct format for asking question ?",https://forum.imacros.net/viewtopic.php?f=7&t=26872&sid=41f2b858fcf7ca695bb39ee1420df70b,AI
2547,Extracting text with no html TYPE,"1. What version of iMacros are you using?
VERSION BUILD=8961227 RECORDER=FX


2. What operating system are you using? (please also specify language)
Windows 7 English

3. Which browser(s) are you using? (include version numbers)
Firefox 49.0.1

4. Do the included demo macros work ok?
Yes

5. If reporting a problem with the Scripting Interface, please also test if the included VBS sample scripts run ok.
N/A

6. Website: Code: Select allhttps://www.thredup.com/product/women-cotton-talbots-blue-long-sleeve-button-down-shirt/17919214
iMacros code I am using now:
Code: Select allTAG POS=1 TYPE=H1 ATTR=CLASS:brand-title EXTRACT=TXT
TAG POS=1 TYPE=H2 ATTR=CLASS:item-title EXTRACT=TXT
TAG POS=1 TYPE=H2 ATTR=CLASS:item-title<SP>item-size EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=CLASS:price EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=CLASS:compare-price EXTRACT=TXT
TAG POS=1 TYPE=DIV ATTR=CLASS:savings-percentage EXTRACT=txt
TAG POS=1 TYPE=DIV ATTR=CLASS:final-sale EXTRACT=TXT
TAG POS=1 TYPE=STRONG ATTR=TXT:Description EXTRACT=TXT
TAG POS=R1 TYPE=LI ATTR=TXT:* EXTRACT=TXT
TAG POS=R1 TYPE=LI ATTR=TXT:* EXTRACT=TXT
TAG POS=R1 TYPE=LI ATTR=TXT:* EXTRACT=TXT
TAG POS=R1 TYPE=LI ATTR=TXT:* EXTRACT=TXT
TAG POS=1 TYPE=STRONG ATTR=TXT:Measurements EXTRACT=TXT
>>> TAG POS=R1 TYPE=BR ATTR=TXT:* EXTRACT=HTM 
TAG POS=1 TYPE=STRONG ATTR=TXT:Materials EXTRACT=TXT
>>> TAG POS=R1 TYPE=TXT ATTR=TXT:* EXTRACT=HTM
TAG POS=1 TYPE=STRONG ATTR=TXT:Condition EXTRACT=TXT
>>> TAG POS=R1 TYPE=HTM ATTR=TXT:* EXTRACT=TXT

7. Do you encounter the same problem with the iMacros Browser, iMacros for Internet Explorer and iMacros for Firefox? 
Yes

Problem I am having is grabbing the text for Measurements, Materials and Condition on the website because it is not in span, div, etc. 
Source code is 
Code: Select all<strong>Description</strong><ul><li>Long sleeve</li><li>Blue</li><li>Solid</li></ul></div><div><strong>Measurements</strong><br><!-- react-text: 906 -->44"" Chest, <!-- /react-text --><!-- react-text: 907 -->25"" Length<!-- /react-text --></div><div><strong>Materials</strong><br><!-- react-text: 911 -->100% Cotton<!-- /react-text --></div><div><strong>Condition</strong><br><!-- react-text: 915 -->This item is gently used with minor signs of wear (minor stain).<!-- /react-text --></div>   

the number afterCode: Select all<!-- react-text: changes with every page so I can't use it",https://forum.imacros.net/viewtopic.php?f=7&t=26744&sid=91bf0dc741658a74aa70a7f66f53d097,AI
2549,Scraping email without opening email client,"Hello!

I am trying to scrape the email of Amazon reviewers who reviewed our products. However, there is just a ""Send an email"" link, and extracting from it only arrives at javascript:void(0). Only after clicking on this link, does it display the actual email of the reviewer. Then it is possible to scrape it.

The issue is that we need to scrape many emails and clicking on ""Send an email"" opens Outlook. I would need to either be able to extract the email without clicking on ""Send an email"", or make it so that iMacros clicks on it but it does not open the email client. I can't do that simply by changing the configuration in my computer, as this scraper would be used by many people. 

Here is an example page: http://www.amazon.com/gp/pdp/profile/A2 ... tbl_3_name

Extracting ""Send an email"" so that it doesn't follow the link does not work either, as I need to actually click on the link instead of just tagging it. Could you guys help?? Thank you very much!",https://forum.imacros.net/viewtopic.php?f=7&t=25897&sid=91bf0dc741658a74aa70a7f66f53d097,AI
2550,Getting data from the screen that is NOT in the source,"Hi

I am trying to get the data from a webpage. But the data doesn't exist in the page source. I am guessing it is generated by javascript/ajax or something. It is an email address. And in page source it looks like this:
Code: Select all<a id=""/gp/profile/A225AROJV5PA03"" class=""a-link-normal pr-email"" href=""javascript:void(0)"">Send an Email</a>

So, the link on the screen says ""Send an Email"". When you click on that link, it changes to an email address. But the email address is nowhere in the html.

Using iMacros, I can easily identify the link as id:/gp/profile/A225AROJV5PA03 and I can extract the text ""Send an Email""

But I want to extract the email address itself.

Does anyone know how to do this?",https://forum.imacros.net/viewtopic.php?f=7&t=26774&sid=91bf0dc741658a74aa70a7f66f53d097,AI
2553,Extraction not possible since 9.0.3,"Hi
Imacros 9.0.3, Win7x64, FF 48.0.2 in use.
Example imacros doesn't work: on running any of example iMacro i get errors like Code: Select allPage loading timeout, URL: null, line: 6 (Error code: -802) Internet works, all pages are loaded properly.

Since update to 9.0.3 extracting imacros which always have worked, don't work anymore: they evebn don't write eanf. Imacro i try to run is:
Code: Select allVERSION BUILD=9030808 RECORDER=FX
SET !TIMEOUT_STEP 0
SET !ERRORIGNORE YES
TAB T=1
SET !DATASOURCE videos.csv 
SET !DATASOURCE_COLUMNS 1
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}
SET !VAR1 EVAL(""var randomNumber=Math.floor(Math.random()*1 + 3); randomNumber;"")
URL GOTO={{!COL1}}
TAG POS=1 TYPE=DIV ATTR=CLASS:_ygd&&TXT:* EXTRACT=HTM      
ADD !EXTRACT {{!URLCURRENT}}
SET !EXTRACT EVAL(""decodeURI('{{!EXTRACT}}');"")
WAIT SECONDS={{!VAR1}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=+{{!NOW:ddmmyyyy}}.csv",https://forum.imacros.net/viewtopic.php?f=7&t=26704&sid=91bf0dc741658a74aa70a7f66f53d097,AI
2555,Website scrapping question is this possible?,What I'm looking to do is take data from Poe.trade a website for a game called path of exile. This site pulls information from the game that people indicated they would like to sell. What I'm looking for is something that will periodically pull data from the website analyse it then send me alerts when certain parameters are met. I was wondering if something like this can even be done.,https://forum.imacros.net/viewtopic.php?f=7&t=26660&sid=91bf0dc741658a74aa70a7f66f53d097,AI
2557,Loop through extraction using field name versus POS?,"Configuration: Surface Pro 2, Win 10, Firefox 47.0.1, iMacros Standard Edition (x86) Version 11.0.246.4051

Hi, 
I am looking for a way to reduce the amount of statements when extracting data. It would be ideal if I could get it down to 2 statements for the lot...since this is a small section of a larger macro I would prefer not to go to .js but handle in .iim files. I only want to extract the data from the fields that match certain field names.  I can't figure out how to loop through POS since the extract selection is based on ATTR=""field name"" not position on web page.

Here is what i have now:
VERSION BUILD=10.3.27.5830
SET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP YES
SET !TIMEOUT_STEP 0
'Subject GLA SF
TAG POS=1 TYPE=LABEL ATTR=TXT:Living<SP>Area
TAG POS=R1 TYPE=DIV ATTR=CLASS:field-value EXTRACT=TXT
'Subject Year built
TAG POS=1 TYPE=LABEL ATTR=TXT:Year<SP>Built
TAG POS=R1 TYPE=DIV ATTR=CLASS:field-value EXTRACT=TXT
'Subject Stories
TAG POS=1 TYPE=LABEL ATTR=TXT:Stories
TAG POS=R1 TYPE=DIV ATTR=CLASS:field-value EXTRACT=TXT
'Begin Collect Subdivision data
'year range
TAG POS=1 TYPE=LABEL ATTR=TXT:Year<SP>Built<SP>Range 
TAG POS=R1 TYPE=DIV ATTR=CLASS:field-value EXTRACT=TXT
'# with pool
TAG POS=1 TYPE=LABEL ATTR=TXT:With<SP>Pool
TAG POS=R1 TYPE=DIV ATTR=CLASS:field-value EXTRACT=TXT
'# single story
TAG POS=1 TYPE=LABEL ATTR=TXT:Single<SP>Story
TAG POS=R1 TYPE=DIV ATTR=CLASS:field-value EXTRACT=TXT
'# multi story
TAG POS=1 TYPE=LABEL ATTR=TXT:Multiple<SP>Story
TAG POS=R1 TYPE=DIV ATTR=CLASS:field-value EXTRACT=TXT
'Avg GLA SF
TAG POS=1 TYPE=LABEL ATTR=TXT:Sqft
TAG POS=R1 TYPE=DIV ATTR=CLASS:field-value EXTRACT=TXT
'Avg Lot SF
TAG POS=1 TYPE=LABEL ATTR=TXT:Lot<SP>Sqft
TAG POS=R1 TYPE=DIV ATTR=CLASS:field-value EXTRACT=TXT
'End Subdicision
'Deed Hstory-Last Sale date
TAG POS=1 TYPE=TH ATTR=TXT:Sale<SP>Date
TAG POS=R1 TYPE=TD ATTR=CLASS:collapsible-caption EXTRACT=TXT 
 'Deed Hstory- Last Sale price
TAG POS=1 TYPE=TH ATTR=TXT:Sale<SP>Price
TAG POS=R1 TYPE=TD ATTR=CLASS:cell-numeric EXTRACT=TXT
' Most recent Land value
TAG POS=1 TYPE=DIV ATTR=TXT:2017<SP>Prelim
TAG POS=R2 TYPE=SPAN ATTR=CLASS:htable-data EXTRACT=TXT
' Most recent taxes
TAG POS=1 TYPE=DIV ATTR=TXT:2015<SP>Final
TAG POS=R8 TYPE=SPAN ATTR=CLASS:htable-data EXTRACT=TXT
' Subject subdivision
TAG POS=1 TYPE=LABEL ATTR=TXT:Subdivision
TAG POS=R1 TYPE=DIV ATTR=CLASS:* EXTRACT=TXT
' Subject Legal Description
TAG POS=1 TYPE=SPAN ATTR=TXT:Description
TAG POS=R1 TYPE=LABEL ATTR=CLASS:monsoon-fielddata EXTRACT=TXT
' Subject Zoning
TAG POS=1 TYPE=DIV ATTR=TXT:City<SP>Zone
TAG POS=R1 TYPE=LABEL ATTR=CLASS:field-label EXTRACT=TXT
'Does subject have pool
TAG POS=1 TYPE=LABEL ATTR=TXT:Pool
TAG POS=R1 TYPE=DIV ATTR=CLASS:field-value EXTRACT=TXT

 Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=26654&sid=91bf0dc741658a74aa70a7f66f53d097,AI
2559,Extract portion of data,"Configuration: Surface Pro 2, Win 10, Firefox 47.0.1, iMacros Standard Edition (x86) Version 11.0.246.4051

Hi, 
I am trying to extract just the data highlighted in the CMA example. I have tried  both normal POS as well as relative. Both will work but the problem is that it ""extracts"" all three categories and saves as one set of data IE: 121119013434. I have tried many iterations of the extract statement to try and drill down to specific data( 1901) but to no avail. For what it is worth I have attached the web pages HTML. Is there any way to accomplish this?
Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=26555&sid=91bf0dc741658a74aa70a7f66f53d097,AI
2562,Extract text from multiple url's,"Good morning,

I am having a problem extracting text from multiple url's and saving to csv. I am using the code below, the url's are json returns and I am running in firefox.
Code: Select allVERSION BUILD=8031994
TAB T=1
URL GOTO=https://maps.googleapis.com/maps/api/distancematrix/json?origins=Balmain+East,+NSW,+Australia&destinations=Sydney,+NSW,+Australia&key=??
TAG POS=1 TYPE=BODY ATTR=* EXTRACT=TXT
URL GOTO=https://maps.googleapis.com/maps/api/distancematrix/json?origins=Balmoral+Lake+Macquarie,+NSW,+Australia&destinations=Tweed+Heads,+NSW,+Australia&key=??
TAG POS=1 TYPE=BODY ATTR=* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=+_{{!NOW:yyyymmdd_hhnnss}}

Each work perfectly on their own but when put together return only a small portion of the text.

Help would be much appreciated. Thanks!  ",https://forum.imacros.net/viewtopic.php?f=7&t=26553&sid=91bf0dc741658a74aa70a7f66f53d097,AI
2568,Extract A Certain Part of Text (Partial),"iMacros for Firefox 8.9.7 (Firefox 47)  , Win 7 x64
Extraction Dialog is created by Imacros 10 IE

The Actual Text is
Code: Select allDear xxxxxxx
Please log in using your login code below:
qpy6HW

This email was sent because someone tried to log in from a new browser or location with your correct email address and password.
If it wasn't you trying to log in, please change your password immediately. Please also consider changing your password on all services you are using this password for.

I want to extract the 
Code: Select allqpy6HW

This Code always changes but the location of the code is always the same

i am trying to store this Code Portion in !VAR
and later use it

in my example the code is Code: Select allqpy6HW
and i want to store this in !VAR

Is it Possible ?

I can Extract the entire Text
Its from an Email

I can help you get an idea if you need a live example . just tell me i will arrange something .",https://forum.imacros.net/viewtopic.php?f=7&t=26443&sid=91bf0dc741658a74aa70a7f66f53d097,AI
2571,"How to take a number, use it to select an image and use text","Hi,

In a form, the antispam works like this :

- a sentence gives a number between 1 & 5, and explains we must write the word corresponding of the image position, 1 to 5.

So for example :
Write the word corresponding of the 4th image
then we have the 5 images which are shown randomly but on the same line, in the same code, just the order changed. Even the name of the image doesn't change.
So we could have 1.gif - 3.gif - 4.gif - 2.gif - 5.gif 
The sentence ask me the 4th, so i must write the sens of the 2.gif image which could be a circle or a square.

So, how can i pick the number of the image, then use it to detect which image is at this position (1 to 5), then i extract the name of the image, in a sorte of table somewhere i stored that 1.gif is a circle, 2.gif a square, 3.gif an arrow, etc. So at the end imacro put the right answer in the field.

thank you,",https://forum.imacros.net/viewtopic.php?f=7&t=26380&sid=91bf0dc741658a74aa70a7f66f53d097,AI
2573,please someone help me,"haii..
i want to extract data to csv
can someone help me make the script?
i already try but i can't

here's the website
https://seller.shopee.co.id/portal/sale?type=toship

here's the first page


and then what i want to scrape is, when i click the ember. here's the landing page
https://2.bp.blogspot.com/-t66oeks3Fvg/ ... /test2.png

here's the data all i want to scrape

https://2.bp.blogspot.com/-Zvrc49RlbNY/ ... /test6.png

https://1.bp.blogspot.com/-EEupRZJvZ9Q/ ... /test3.png

https://3.bp.blogspot.com/-93Wlx3eb7Mg/ ... /test4.png

https://1.bp.blogspot.com/-43q5jQBh6hE/ ... /test5.png

and i want to play loop for all items at this page
https://seller.shopee.co.id/portal/sale?type=toship

can someone help me?
tq",https://forum.imacros.net/viewtopic.php?f=7&t=26374&sid=91bf0dc741658a74aa70a7f66f53d097,AI
2577,Extract email Body (GMail),"Hi , i try to extract a gmail body but gmail make random id each time!
i'm newbie   
can someone help me thanks",https://forum.imacros.net/viewtopic.php?f=7&t=26313&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2578,YouNow Date extract and close page by date result,"Hello,

(1)

how can I extract the Date / Time?

URL GOTO=https://www.younow.com/ItsNolyMon/channel
TAG POS=17 TYPE=DIV ATTR=CLASS:minutes-ago<SP>text-muted EXTRACT=TXT

URL GOTO=https://www.younow.com/Nochillmikeyyy/channel
TAG POS=15 TYPE=DIV ATTR=CLASS:minutes-ago<SP>text-muted EXTRACT=TXT

You can see ""TAG POS"" is not the same.

I need the first time under
TAG POS=1 TYPE=A ATTR=TXT:Broadcasts

How can I do this?

I try 
TAG POS=1 TYPE=A ATTR=TXT:Broadcasts
TAG POS=R1 TYPE=DIV ATTR=CLASS:minutes-ago<SP>text-muted EXTRACT=TXT
but this is not the searched result.

I don't find a element of 
TYPE=DIV ATTR=CLASS:minutes-ago<SP>text-muted EXTRACT=TXT
whitch can be the start point for:
TAG POS=R1 TYPE=DIV ATTR=CLASS:minutes-ago<SP>text-muted EXTRACT=TXT

iMacros 10
FireFox 41
win 8 / 64

(2)

As a result, the Text Extraction you get as ""13 days ago"" or ""17 minutes ago"". If the result is older than ""2 days ago"", I would like to close the page by iMacros. Otherwise, to remain open the page. How can I implement with iMacros?

best regards",https://forum.imacros.net/viewtopic.php?f=7&t=25874&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2580,First column of a table,"Hi,

I have to extract the 3rd column's cell of the first row table that has in the 1st column a certain name.

the table looks like:
Code: Select all<tr><td class=""name"">Name1</td><td class=""name"">Name2</td><td><span>ITEMtoEXTRACT</span></td></tr>

till now I've written this code:
Code: Select allTAG POS=1 TYPE=TD ATTR=CLASS:crest-player-name&&TXT:Name1
TAG POS=R1 TYPE=TD ATTR=TXT:*:*:* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}}

That's working fine, but it extracts the 3rd cell also if Name1 is in the second cell. As you see,the 1st and the 2nd cell have the same ATTR.

What should I do?",https://forum.imacros.net/viewtopic.php?f=7&t=26275&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2582,Need to Extract Current URLs from 2 Tabs and Save to CSV,"Hi

Noob here.  I am trying to create a macro that clicks on 2 links in Tab 1 (this I have figured out) ..and saves the 2 currenturls in CSV. (this is the problem)

This is how I've thought about this:

1. Tab 1
2. Find ""First Link"" click on it
3. Tab 2
4. Grab CURRENTURL
5. Go back to Tab 1
6. Find ""Second Link"" click on it
7. Tab 3
8. Grab CURRENTURL
9. Go back to Tab 1

The end result should be 2 different URLs saved in 1 CSV file
Code: Select allVERSION BUILD=8970419 RECORDER=FX
TAB T=1
TAG POS=1 TYPE=A ATTR=TXT:First<SP>Link<SP>*
TAB T=2
ADD !EXTRACT {{!URLCURRENT}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=urls.csv
WAIT SECONDS=0.3
SET !EXTRACT NULL
TAB T=1
TAG POS=1 TYPE=A ATTR=TXT:Second<SP>Link<SP>*
TAB T=3
ADD !EXTRACT {{!URLCURRENT}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=urls.csv
WAIT SECONDS=0.3
TAB T=1

^ This saves the url from Tab 2 TWICE in the CSV file, they should be different URLs (URL from Tab 2 and URL from Tab 3)

Any ideas?",https://forum.imacros.net/viewtopic.php?f=7&t=26257&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2586,Unable to handle Javascript popup,"hi Team,

In one of my site where I am performing screen scraping. I am unable to handle the code of a popup window.

Issue in detail: Once I click button 1(disable account), a popup is thrown asking for confirmation click button 2(are you sure?). This popup code is written in javascript and I am unable to handle that popup Imacros code. 

Generated code(no luck): 
TAG POS=1 TYPE=INPUT:SUBMIT ATTR=NAME:btnOnOff
WAIT SECONDS=1
TAB T=1
ONDIALOG POS=1 BUTTON=NO 

Let me know if you need any more details in this regard. Thank you. 

Regards,
Ashwin",https://forum.imacros.net/viewtopic.php?f=7&t=26184&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2589,Extracting data from div,"Hi all,

I have a div which is mentioned below.

File name	         Date	                 Size	              Status	        Download
Status_One	04/18/2016	1245	             Success	Download Button
Status_Two	04/18/2016	12314564	     Success	Download Button
Status_Three	04/18/2016	14587	     Success	Download Button
Status_Four	04/19/2016	789451	     Success	Download Button
Status_Five	04/19/2016	1256              Success        Download Button
Status_Six	        04/19/2016	021	             Success	Download Button
Status_Seven	04/19/2016	45214	     Success	Download Button


I want extract data row by row and after that download the individual file. Please help me.

waiting for the reply.

Thanks,
Sai",https://forum.imacros.net/viewtopic.php?f=7&t=26096&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2590,Extracting text from a readonly textbox that changes?,"Hi, I'm trying to create a macro where I can loop it and every certain number of minutes it extracts and saves the value.  I'm having some trouble because when I select the textbox and attempt to add the command, it'll fail to do it if the number changes. (Which it's supposed to.)
Code: Select allTAG POS=1 TYPE=INPUT:TEXT ATTR=VALUE:0.00001839 EXTRACT=TXT

this is the line of code that is created when i try to perform the ""extract text"" task. Whenever I try to repeat the entire process it won't save the file afterwards because it can't find the value it's looking for, when I just want it to look in that box and take whatever value is in there. 

I tried doing ATTR=VALUE:* but it still didn't save as either TXT or CSV.. Here is the full code.
Code: Select allVERSION BUILD=10022823
TAB T=1
TAB CLOSEALLOTHERS
PROXY ADDRESS=[Censored]
SET !EXTRACT_TEST_POPUP NO
SET !TIMEOUT_PAGE 120
URL GOTO=https://www.satoshidice.com/?secret=[Censored]
TAG POS=1 TYPE=LI ATTR=TXT:Invest
WAIT SECONDS=20
'text input activated
TAG POS=1 TYPE=INPUT:TEXT ATTR=VALUE:* EXTRACT=TXT
WAIT SECONDS=2
SAVEAS TYPE=EXTRACT FOLDER=* FILE=Extract_{{!NOW:ddmmyy_hhnnss}}.csv

Any help would be greatly appreciated, thanks.",https://forum.imacros.net/viewtopic.php?f=7&t=26064&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2591,Skip steps if text is found,"Hey everyone, I'm trying to find out if it's possible for a section to be skipped if a specific text is found in a webpage. For example, say I have a macro that is going to 10 websites and extracting 10 different items from each page. The first item is either ""skip to next page"" or ""extract all items"", just to be simple. If the ""skip to next page"" is present, instead of extracting the remaining 9 items, that section of the macro is skipped and the macro moves to the next page. If the ""extract all items"" is found, then the remaining 9 items are extracted and the macro would move to the next page. Basically, and IF argument to either continue the macro or skip steps.",https://forum.imacros.net/viewtopic.php?f=7&t=26075&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2592,Clickbank Imacros Script,"Hi everyone, I am pretty new to imacros but have basic knowledge about how it works. I am trying to create a script which will go to clickbank marketplace, copy the title and affiliate link of the product. I tried doing it but I don't know how to make imacros copy the url, title or save the details in csv

Can somebody please help me??

I would really appreciate.

Regards
IndeXer",https://forum.imacros.net/viewtopic.php?f=7&t=25913&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2593,Scroll Ajax and Extract data,"iMacros for Firefox 9.0.0b2
WIN 10 64
FF 45.0.1

Say I want to get some sneaker's price on a website and the result is dynamic, I have to scroll down if I want to get the data, I have attached my code.

My Code:

TAB T=1
TAB CLOSEALLOTHERS
SET !ERRORIGNORE YES
URL GOTO=https://stockx.com/sneakers/most-popular?view=list
WAIT SECONDS=2
TAG POS={{!LOOP}} TYPE=A ATTR=TXT:*&&HREF:*&&DATA-REACTID:.4.0.1.0.1.2.1.1.1.0:*
WAIT SECONDS=2
TAG POS=1 TYPE=TITLE ATTR=* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=Test.csv


If you the run the code it will go to the website and click into each sneaker and get the title and back and get next one and so on and so forth, but there's more sneakers in the list that one has to scroll to see more.

My question is how should I add in the scroll function to make it work?

URL GOTO=javascript:window.scrollBy(0,20000)

^^^this one doesn't work in the sense that it will only scroll down maybe 20 rows more but it doesn't help in the big picture.

Thanks for all of your helps and time and effort!",https://forum.imacros.net/viewtopic.php?f=7&t=26047&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2594,Scraping data from images? ( NOT captcha ),"Hi,

With the help if iMacros, I was able to download 1000+ .png images that contain CLEARLY written phone numbers.

I want to scrape the text ( numbers ) from the those images to send a mass SMS campaign. What is the best way/tool to get this done?

P.S - I understand that this question is probably irrelevant to iMacros but I am relying on the expertise of the community and hoping to get some recommendations.",https://forum.imacros.net/viewtopic.php?f=7&t=25685&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2599,Variable POS TAG in Loop,"MY CONFIG:
iMacros 8.9.6 for Firefox
Firefox 45.0
Mac OSX 10.10.5
Code: Select allTAG POS=1 TYPE=SPAN ATTR=TXT:Optionen
ONDOWNLOAD FOLDER=* FILE=+_{{!NOW:yyyymmdd_hhnnss}} WAIT=YES
TAG POS=36 TYPE=SPAN ATTR=TXT:Herunterladen
TAG POS=55 TYPE=I ATTR=TXT:

TAG POS=1 TYPE=SPAN ATTR=TXT:Optionen
ONDOWNLOAD FOLDER=* FILE=+_{{!NOW:yyyymmdd_hhnnss}} WAIT=YES
TAG POS=38 TYPE=SPAN ATTR=TXT:Herunterladen
TAG POS=55 TYPE=I ATTR=TXT:


I have that variable TAG POS in
Code: Select allTAG POS=36 TYPE=SPAN ATTR=TXT:Herunterladen

and want to build a loop. i dont know how i can solve that.",https://forum.imacros.net/viewtopic.php?f=7&t=25932&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2600,exclude extracted data from SAVEAS based on criteria ?,"OS: Windows 8.1 64 bit
iMacros 10.0.2

Good day,

My datasource is a .csv that contains the terms I am using to search in a website. The results of searches are individual pages, depending upon the search term.

It happens that for some of the search terms there exist no individual pages and the website where I am searching shows a  ""0 items exist"" message.

iMacros still extracts data according with the its tag positioning and the DOM structure of the ""0 items exist"" page or gets #EANF# in the cases  it does not find a tag match on the page.
Then, iMacros saves the extracted that in a row of the extraction csv.


I wish to exclude from Code: Select allSAVEAS TYPE=EXTRACT .csv the rows that belong to the ""0 items exist"" webpage (in other words my extracted CSV to show only the complete data extracted from the website).

Question:
 How do I prevent the rows from ""items do not exist""  to appear in the EXTRACT csv? 
What is the condition to be placed looking like?

Thank you.",https://forum.imacros.net/viewtopic.php?f=7&t=25893&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2601,"same elements ,different TAG POS=x across webpages. extract?","Question: How to extract data with iMacros from a website whose TAG POS=x of the same element is variable between different webpages?

Dear iMacros community,

I wish to extract data from a website that contains multiple webpages by searching in the website according to a list of keywords defined in a datasource .csv.

iMacros should enter sequentially in each individual page, grab certain elements on each webpage and save data in a csv. The elements to be extracted are the same in between all webpages.

My problem is that the TAG POS=x does not remain the same for an element when moving from webpage to webpage.

e.g on a page a HTML TAG element has TCode: Select allAG POS=95 TYPE=SPAN ATTR=* EXTRACT=TXT, while on other page same HTML TAG element changes to Code: Select allTAG POS=96 TYPE=SPAN ATTR=* EXTRACT=TXT
The only possibility I am thinking would be to pick the elements by their text attribute ( I mean their text).

Question:
Does the TXT parameter like TXT:Manufacturer (or eventually TXT:Manufacturer*) permits the selection without knowing the exact TAG POS=?

Is there other solution to make this kind of an extraction with iMacros?(variable position of the tag for the same html element across pages)

Thank you.",https://forum.imacros.net/viewtopic.php?f=7&t=25887&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2602,Extract information [DONE],"Hello,
i try to build something in imacro but i had now problems.
Before captcha Solving see i this http://i.imgur.com/ghPl7Ng.png and after successfull captcha solving I see this http://i.imgur.com/LwUMRJd.png

Now i want when the extraxt Text ""Your<SP>Investing<SP>account<SP>will<SP>be<SP>credited<SP>"" the same like this ""Your<SP>Investing<SP>account<SP>will<SP>be<SP>credited<SP>"" than there schould be a if than. 
But how work this with this code:
Code: Select all'SEARCH SOURCE=REGEXP:""greencheck.png\"" style=\""display: ([^;]+);"" IGNORE_CASE=YES EXTRACT=""$1""
TAG POS=1 TYPE=IMG ATTR=ID:profit-correct EXTRACT=HTM       [color=#00BF40]-> This i must change in TAG POS=1 TYPE=CENTER ATTR=TXT:Your<SP>Investing<SP>account<SP>will<SP>be<SP>credited<SP>wi* EXTRACT=HTM [/color]
SET !VAR7 {{!EXTRACT}}
SET !EXTRACT NULL
SET !VAR8 EVAL(""if ('{{!VAR7}}'.match(/style=\""display: block/)) {var x = \""1\"";} else {var x = \""2\"";} x;"")      [color=#00FF00]-> But here i do not know and need help[/color]
TAB OPEN
TAB T=2
URL GOTO=http://www.9kw.eu/index.cgi?source=imacros&action=usercaptchacorrectback&apikey={{!VAR2}}&correct={{!VAR8}}&id={{!VAR5}}
WAIT SECONDS=2
TAB CLOSE
TAB T=1 

After this i had an other question: 
I want realaize the same like up, but i want a if than loop when a number is greater than. How i can realize this.

I think that are for you profis very easy but for a dummy difficult. thx for helping!",https://forum.imacros.net/viewtopic.php?f=7&t=25167&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2603,Extract and save to .CSV + LOOP,"VERSION BUILD=11.0.246.4051
Windows 8.1
IE11
Included demo macros work ok
the included VBS sample scripts run ok
encounter the same problm with iMacros IE, Firefox, Chrome

Hello,

It's my 1st macro so being indulgent   
I didn't find solution in Wiki, FAQ and this forum.

My macro is about data extraction and saving into a .CSV
VERSION BUILD=11.0.246.4051
TAB T=1
TAB CLOSEALLOTHERS
SET !PLAYBACKDELAY 0.2
URL GOTO=http://annuaire.agencebio.org/resultats ... activite=0
TAG POS=1 TYPE=A ATTR=TXT:en<SP>savoir<SP>+
TAG POS=2 TYPE=DIV ATTR=CLASS:band_sqr EXTRACT=TXT SET !EXTRACT {{!COL1}}
TAG POS=1 TYPE=SPAN ATTR=CLASS:fn EXTRACT=TXT SET !EXTRACT {{!COL2}}
TAG POS=1 TYPE=DIV ATTR=CLASS:adr EXTRACT=TXT SET !EXTRACT {{!COL3}}
TAG POS=16 TYPE=SPAN ATTR=* EXTRACT=TXT SET !EXTRACT {{!COL4}}
TAG POS=1 TYPE=SPAN ATTR=CLASS:email EXTRACT=TXT SET !EXTRACT {{!COL5}}
TAG POS=1 TYPE=SPAN ATTR=CLASS:url EXTRACT=TXT SET !EXTRACT {{!COL6}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=Extract_{{!NOW:ddmmyy_hhnnss}}.csv
BACK
TAG POS=2 TYPE=A ATTR=TXT:en<SP>savoir<SP>+
TAG POS=2 TYPE=DIV ATTR=CLASS:band_sqr EXTRACT=TXT SET !EXTRACT {{!COL1}}
TAG POS=1 TYPE=SPAN ATTR=CLASS:fn EXTRACT=TXT SET !EXTRACT {{!COL2}}
TAG POS=1 TYPE=DIV ATTR=CLASS:adr EXTRACT=TXT SET !EXTRACT {{!COL3}}
TAG POS=16 TYPE=SPAN ATTR=* EXTRACT=TXT SET !EXTRACT {{!COL4}}
TAG POS=1 TYPE=SPAN ATTR=CLASS:email EXTRACT=TXT SET !EXTRACT {{!COL5}}
TAG POS=1 TYPE=SPAN ATTR=CLASS:url EXTRACT=TXT SET !EXTRACT {{!COL6}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=Extract_{{!NOW:ddmmyy_hhnnss}}.csv

etc. etc. (there is x15 "" TAG POS=* TYPE=A ATTR=TXT:en<SP>savoir<SP>+ "" per page)

This website is a directory. My goal is to extract contact informations for each company and save it into an excel file (1 company per line)   
I didn't find command to extract each text element, put each element into an excel column, and save all into a .csv

Many thanks for your help",https://forum.imacros.net/viewtopic.php?f=7&t=25818&sid=e8ccee866abcf29ebd57b704a290f61c,AI
2608,Commas in Text save as CSV File,"Hi All,
again, sorry for the newb questions.  I really cant find an asnwer on this one however it appears everyone has expereinced but doesnt appear to be solutions or people who answered their own post with a solution but didnt post the solution?  uuhhgg.

anyways,,,  

I am using the iMacros Browser V11.0.246.4051 to attemtp to figure any of this out.  Where I am at now is dealing with scraping a text title that has commas in it.  When it is saved as a CSV file of course it seperates out the words that has commas between them.  For example.

Precut Insulation, Door Skin Damper Kit, Inner

I need this in one column in the CSV File NOT three colums.

There has got to be a way around this?  can someone share a solution how to either eliminate the commas in the text or get iMacros to ignore the commas in the text when saving as CSV file.

Yes I know its comma deliminted,,, if I could change that?  wouldnt know where that option is to change the dleiminters?  even if that is possible?  would it make a difference exporting text to a database?  is that more difficult?   I would think this would be a real common problem and that there would be simple soljutions all over?  any help would be appreciated.  thanks...",https://forum.imacros.net/viewtopic.php?f=7&t=25740&sid=9d96e1268e5b272969ed509f53432633,AI
2609,Extracting some lines from a website,"Hello guys.
Need some help with a script. I want to extract this messages from this site.

I want to extract the email and the message and put them in csv file and should look like this: ""email@dasdasda.com E-mail address is valid"" Or if the email is invalid it will look like this ""email@dasdasda.com E-mail is invalid""

I tried to extract the text and save it to a file like this
""TAG POS=1 TYPE=TD ATTR=TD:* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=tanana""
but it didint work..
Any suggestions ? Thanks and have a great day you all.


",https://forum.imacros.net/viewtopic.php?f=7&t=25713&sid=9d96e1268e5b272969ed509f53432633,AI
2610,Extracting Page Source Information If Present,"VERSION BUILD=8961227 RECORDER=FX
Windows 8.1 Firefox 44.0

Okay, please forgive me for what may end up being very novice questions. I am fairly new to trying to use iMacros. Added it to Chrome a couple years ago but never really tried using it. I just added it to Firefox and am trying to find out if it can even do what I want it to do.

I am trying to use iMacros to search the page HTML source for a specific string. If that string exists I would like for it to extract that string along with the following 60 characters and date and time to append it to a CSV file. The thing is, that string could appear in the source 1 time, 20 times or perhaps not at all. I would then want the macro to wait 10 minutes, refresh the page and do the search all over again. I am looking for what may be subtle differences in if the string is found or not. Though I can determine the differences by analyzing the CSV file later. 

I am successfully able to search the page source with the below code. I have been able to get it to display a popup when the string is found. The problem is that it times out when the string is not found and the loop ends. Also, it is only prompting that the string exists. It is not logging that the string exists in a file for later analysis. I am also only able to extract the exact string, not the following 60 characters. 
Code: Select allVERSION BUILD=8961227 RECORDER=FX
TAB T=1
TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:SearchForm ATTR=*
TAG POS=1 TYPE=A ATTR=TXT:More<SP>Dates
SEARCH SOURCE=REGEXP:""(String_Text_To_Find)"" IGNORE_CASE=YES EXTRACT=$1
PROMPT {{!EXTRACT}}

I hope that I provided enough information for someone to be able to answer my question. I guess I want to know if this is even something that iMacros is capable of doing before I put a lot of time in to working through it to only find out it isn't possible. My knowledge of iMacros and the code is very rudimentary right now, but I hope to learn a lot here and reading any resources you could point me to.

Thanks in advance!",https://forum.imacros.net/viewtopic.php?f=7&t=25723&sid=9d96e1268e5b272969ed509f53432633,AI
2611,extract email from text,"Hi,
Thanks for Shugarhttp://stackoverflow.com/users/4270557/shugar in stackoverflow
Code: Select allSET testString ""bla bla bla xyz@mail.com bla bla bla""
SET email EVAL(""'{{testString}}'.match(/(([^<>()[\\]\\\.,;:\\s@\\\""]+(\\.[^<>()[\\]\\\.,;:\\s@\\\""]+)*)|(\\\"".+\\\""))@((\\[[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\])|(([a-zA-Z\\-0-9]+\\.)+[a-zA-Z]{2,}))/g)[0];"")
PROMPT {{email}}

http://stackoverflow.com/questions/3055 ... th-imacros",https://forum.imacros.net/viewtopic.php?f=7&t=24571&sid=9d96e1268e5b272969ed509f53432633,AI
2612,Search for a list of URLs in site and extract to file?,"I need to search my site for several old URLs so they can be updated.  Then, for each URL that's found I need to extract the page URL where it was found and the link text that's displayed.  This all needs to be written to a CSV file  for review.  I'm fairly sure the URL will only appear once on a page and there won't be multiple URLs found on a page, but I can't rule that out.  Ideally the macro could read the URLs I'm searching for from an excel or CSV file (in which case I would need the macro to list which URL was found with the other info mentioned above, but I'm willing to search them individually. I've tried checking the example macros, forums and wiki, but I'm having trouble fitting it all together (haven't used iMacros in a loooong time).  Help or a point in the right direction is appreciated!",https://forum.imacros.net/viewtopic.php?f=7&t=25311&sid=9d96e1268e5b272969ed509f53432633,AI
2613,[solved] Imacro does not extract,"hey all so I've spent hours and hours trying to solve tis but I couldn't...you guys are my last hope:

I want to extract the word style / label of hundreds of words in this dictionary: http://www.oxfordlearnersdictionaries.c ... antagonist

for example in the link above I want the ""(formal)"" to be extracted, but no matter what I do, it won't... here's my code:
Code: Select allURL GOTO=http://www.oxfordlearnersdictionaries.com/definition/english/antagonist
TAG POS=1 TYPE=SPAN ATTR=ID:antagonist__15&&REG:fml&&CLASS:reg EXTRACT=TXT 

I've written more specific ATTR's but were all failures and Imacro won't extract",https://forum.imacros.net/viewtopic.php?f=7&t=25629&sid=9d96e1268e5b272969ed509f53432633,AI
2616,How to extract the link after a specific div,"Hi,


I would like to create a list of link for a newspaper, so i built this macro :
Code: Select allVERSION BUILD=8940826 RECORDER=FX
TAB T=1
SET !ERRORIGNORE YES
SET !LOOP 1
URL GOTO=http://www.lemonde.fr/recherche/?keywords=keyword&qt=recherche_globale&page_num={{!loop}}
TAG POS=1 TYPE=A ATTR=CLASS:grid_3*alpha*obf* EXTRACT=HREF
TAG POS=2 TYPE=A ATTR=CLASS:grid_3*alpha*obf* EXTRACT=HREF
TAG POS=3 TYPE=A ATTR=CLASS:grid_3*alpha*obf* EXTRACT=HREF
TAG POS=4 TYPE=A ATTR=CLASS:grid_3*alpha*obf* EXTRACT=HREF
TAG POS=5 TYPE=A ATTR=CLASS:grid_3*alpha*obf* EXTRACT=HREF
TAG POS=6 TYPE=A ATTR=CLASS:grid_3*alpha*obf* EXTRACT=HREF
TAG POS=7 TYPE=A ATTR=CLASS:grid_3*alpha*obf* EXTRACT=HREF
TAG POS=8 TYPE=A ATTR=CLASS:grid_3*alpha*obf* EXTRACT=HREF
TAG POS=9 TYPE=A ATTR=CLASS:grid_3*alpha*obf* EXTRACT=HREF
TAG POS=10 TYPE=A ATTR=CLASS:grid_3*alpha*obf* EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=* FILE=links_lemonde.txt
WAIT SECONDS=3


Everything was good at the begining, but then i discover sometimes links are not formated always  with this attribute, so i search the commun symbole:
Code: Select all<article class=""grid_12 alpha enrichi mgt8"">
               <div class=""grid_11 conteneur_fleuve alpha omega"">
                  <a href=""/culture/article/2012/04/02/selection-cd_1679176_3246.html?xtmc=immobilier&xtcr=3463"" class=""grid_3 alpha obf""><img width=""147"" height=""97"" data-item-type=""article"" data-lazyload=""true"" alt=""S茅lection CD"" title=""S茅lection CD"" class=""lazy-retina"" data-src=""http://s2.lemde.fr/image/2007/03/10/147x97/881629_7_2fa4_les-journalistes-de-la-rubrique-musiques-du_72443f12f62ce8088d1db266472488c7.jpg"" src=""data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="" onload=""lmd.pic(this);"" onerror=""lmd.pic(this);""></a>
                  <div class=""grid_8 omega resultat"">
                                    <h3 class=""txt4_120 ""><a href=""/culture/article/2012/04/02/selection-cd_1679176_3246.html?xtmc=immobilier&xtcr=3463"">S茅lection CD</a></h3>
                     <span class=""txt1 signature"">LE MONDE | 9 avril 2012</span>
                     <p class=""txt3"">Chutes de type Niagara glauque (Flowing Down Too Slow), mirages psych茅d茅liques (Domeniche alla periferia dell'impero), corps fantomatiques (Nell'alto dei giorni immobili), trames effiloch茅es (The Nameless City), tous ces moments de d茅sagr茅gation se doublent paradoxalement d'une irr茅sistible force...</p>
                  </div>
               </div>
            </article>
            <article class=""grid_12 alpha enrichi mgt8"">
               <div class=""grid_11 conteneur_fleuve alpha omega"">
                                 <div class=""grid_11 omega resultat"">
                     <h3 class=""txt4_120 marqueur_restreint""><a href=""/a-la-une/article/2012/04/09/le-club-des-entrepreneurs-intouchables_1682627_3208.html?xtmc=immobilier&xtcr=3464"">Le club des entrepreneurs intouchables</a></h3>
                     <span class=""txt1 signature"">LE MONDE | 9 avril 2012</span>
                     <p class=""txt3"">Ils appartiennent 脿 la caste la plus m茅pris茅e d'Inde et pourtant, aujourd'hui, ils ont r茅ussi dans les affaires. ...Ces quelques centaines de patrons tissent leur r茅seau et revendiquent leur place parmi l'茅lite...Ce n'est pas pour sa moquette rouge 茅paisse, ses moulures au plafond et son volumineux...</p>
                  </div>
               </div>
            </article>
 

I founded that :
Code: Select all<div class=""grid_* omega resultat""><h3 class=""txt4_120 "">

Where  * = 8 or 11 

Here an example page http://www.lemonde.fr/recherche/?keywor ... ge_num=424

So i try to do this modification (and few others) :
Code: Select allTAG POS=1 TYPE=DIV ATTR=CLASS:grid_*omega*resultat&&TXT:HREF*
TAG POS=R1 TYPE=A ATTR=HREF:* EXTRACT=HREF 


But i don't succeed.

So, do you have an idea to help me with this macro?

Thank you",https://forum.imacros.net/viewtopic.php?f=7&t=25544&sid=9d96e1268e5b272969ed509f53432633,AI
2617,dynamic class; what do I do if the class changes?,"iMacros Version 11.0.246.4051
Browser FireFox 42.0
OS Windows 7 Home Premium - Service Pack 1


CAN I EXTRACT CLASS TYPE?

I don't even know the words to ask or research this as I don't even know how to explain it... 

When I click on one location it may have any of the five random images... it can return any of the following:

TAG POS=1 TYPE=DIV ATTR=CLASS:cell<SP>image<SP>yellow_things&&TXT:  EXTRACT=HTM
TAG POS=1 TYPE=DIV ATTR=CLASS:cell<SP>image<SP>brown_things&&TXT:  EXTRACT=HTM
TAG POS=1 TYPE=DIV ATTR=CLASS:cell<SP>image<SP>red_things&&TXT:      EXTRACT=HTM
TAG POS=1 TYPE=DIV ATTR=CLASS:cell<SP>image<SP>blue_things&&TXT:     EXTRACT=HTM
TAG POS=1 TYPE=DIV ATTR=CLASS:cell<SP>image<SP>green_things&&TXT:   EXTRACT=HTM
TAG POS=1 TYPE=DIV ATTR=CLASS:cell<SP>image<SP>orange_things&&TXT: EXTRACT=HTM

None of which will provde me with anything to extracted accept "" EXTRACT=HTM "" which returns (yellow_things) in the return string and that could be something that I could use if I new how to regex ""?????_things"" from a long HTM string.

Example return string
<div style=""outline: 1px solid blue;"" class=""cell image green_things""></div>

Question: should I try to regex ""??????_things"" out of the HTM?
Or is there a different method to count how many green_things are on this page?

What I am trying to do... is click ten of these images and extract how many of each colour was found in the list.

Sorry for sounding so dumb.. but I don't know where to begin... can someone provide me with what terms/words/keywords to research so I will know where to start when researching?

This is not on a public page... so I can not provide a link... sorry.",https://forum.imacros.net/viewtopic.php?f=7&t=25507&sid=9d96e1268e5b272969ed509f53432633,AI
2618,How to determine if a button is available,"I'm working on macro (Excel VBA) that will loop through a series of vouchers in PeopleSoft Accounts Payable. After the macro ""saves"" the voucher, it will click on the ""Next in List"" button. This works fine.

The vendor has over 5,000 vouchers. In PeopleSoft AP, only 300 records can be displayed at a time. 

What I want to do is validate the ""Next in List"" button to see if it is grayed out (and not available). If it is grayed out, then I will perform some other action to display the next 300 records.

Is this possible? Can iMacros determine if a button is ""available"" or not?

I'm using iMacros 10.3 and PeopleSoft Accounts Payable 9.1 and Internet Explorer 11.

Thanks for the help.....",https://forum.imacros.net/viewtopic.php?f=7&t=25513&sid=9d96e1268e5b272969ed509f53432633,AI
2620,How To Extract A Number From H3 TAG,"Code: Select all<h3 class=""n""> ""2.&nbsp;"" <a href=""/los-angeles-ca/mip/california-hospital-medical-center-457886157?lid=457886157"" data-analytics=""{""target"":""name"",""feature_click"":""""}"" rel="""" itemprop=""name"" class=""business-name"" data-impressed=""1""> text next to the numer i want to extract </a>

(2. Hospital Medical Center)

i want to extract number 2 and to it as anchor. and loop through lists exporting only text without numbers reference to exel file.

help would be appreciated thank you.
Code: Select allTAG POS=3 TYPE=H3 ATTR=TXT:{{!LOOP}}. EXTRACT=TXT
'SET !EXTRACT NULL

TAG POS=R1 TYPE=A ATTR=CLASS:business-name EXTRACT=TXT
TAG POS=R1 TYPE=SPAN ATTR=CLASS:street-address EXTRACT=TXT
TAG POS=R1 TYPE=SPAN ATTR=CLASS:locality EXTRACT=TXT
TAG POS=R1 TYPE=SPAN ATTR=ITEMPROP:addressRegion EXTRACT=TXT
TAG POS=R1 TYPE=SPAN ATTR=ITEMPROP:postalCode EXTRACT=TXT
TAG POS=R1 TYPE=DIV ATTR=CLASS:phones<SP>phone<SP>primary EXTRACT=TXT

SAVEAS TYPE=EXTRACT FOLDER=* FILE=test-2.csv

Tried this but in vain!!",https://forum.imacros.net/viewtopic.php?f=7&t=25493&sid=9d96e1268e5b272969ed509f53432633,AI
2622,Data Replace with EVAL doubt,"When I use only 1 EVAL code, all works fine. See below:

VERSION BUILD=6060703 RECORDER=CR
URL GOTO={{!URLCURRENT}}

SET !EXTRACT_TEST_POPUP NO

TAG POS=1 TYPE=H1 ATTR=TXT:* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}} 
SET !EXTRACT NULL

WAIT SECONDS=1

TAG POS=8 TYPE=P ATTR=TXT:* EXTRACT=TXT
SET !VAR2 {{!EXTRACT}} 
SET !EXTRACT NULL

WAIT SECONDS=1

TAG POS=4 TYPE=P ATTR=TXT:* EXTRACT=TXT
SET !VAR3 {{!EXTRACT}} 
SET !EXTRACT NULL

WAIT SECONDS=1

TAG POS=12 TYPE=P ATTR=TXT:* EXTRACT=TXT
SET !VAR4 {{!EXTRACT}} 
SET !VAR4 EVAL(""var s='{{!EXTRACT}}'; s.replace('Site: ', '');"")

WAIT SECONDS=1

TAG POS=1 TYPE=INPUT:TEXT FORM=ID_text={{!VAR1}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID_text={{!VAR2}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID_text={{!VAR3}} here the word ""Site:"" is replaced fine. All the rest is held. perfect!
TAG POS=1 TYPE=INPUT:TEXT FORM=ID_text={{!VAR4}}

However, when I use 2 or more EVAL, the text of the first EVAL comes together to the second   See below:

VERSION BUILD=6060703 RECORDER=CR
URL GOTO={{!URLCURRENT}}

SET !EXTRACT_TEST_POPUP NO

TAG POS=1 TYPE=H1 ATTR=TXT:* EXTRACT=TXT
SET !VAR1 {{!EXTRACT}} 
SET !EXTRACT NULL

WAIT SECONDS=1

TAG POS=8 TYPE=P ATTR=TXT:* EXTRACT=TXT
SET !VAR2 {{!EXTRACT}} 
SET !EXTRACT NULL

WAIT SECONDS=1

TAG POS=4 TYPE=P ATTR=TXT:* EXTRACT=TXT
SET !VAR3 {{!EXTRACT}} 
SET !VAR3 EVAL(""var s='{{!EXTRACT}}'; s.replace('City: ', '');"")

WAIT SECONDS=1

TAG POS=12 TYPE=P ATTR=TXT:* EXTRACT=TXT
SET !VAR4 {{!EXTRACT}} 
SET !VAR4 EVAL(""var s='{{!EXTRACT}}'; s.replace('Site: ', '');"")

WAIT SECONDS=1

TAG POS=1 TYPE=INPUT:TEXT FORM=ID_text={{!VAR1}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID_text={{!VAR2}}
TAG POS=1 TYPE=INPUT:TEXT FORM=ID_text={{!VAR3}} instead replace the prefix City: and held the city name, comes all the code City: name_of_the_city[EXTRACT] site_name 
TAG POS=1 TYPE=INPUT:TEXT FORM=ID_text={{!VAR4}} instead replace the prefix Site: and held the site name, comes all the code City: name_of_the_city[EXTRACT] site_name 

Wrong: City: name_of_the_city[EXTRACT] site_name
Correct: California
Correct: http://www.google.com

Could you, please, help me with some sample code?

Thanks! ",https://forum.imacros.net/viewtopic.php?f=7&t=25444&sid=9d96e1268e5b272969ed509f53432633,AI
2623,V11 Relative positioniong,"I am using V11 standard under Win 10 with Firefox 41.0.2.
I primarily use iMacros for form filling and this is the first time using the data extracting wizard. I have written a data extract macro using the wizard,  the macro extracts the data okay, but no matter what I have tried(creating anchor tag before extracting) the wizard generates absolute position.
What am I missing?
Ex:
VERSION BUILD=11.0.246.4051
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=https://mymonsoon.com/Parcel/Details?up ... 6512&fc=qs
'Set anchor
TAG POS=1 TYPE=SPAN ATTR=TXT:Property<SP>Information
TAG POS=2 TYPE=LABEL ATTR=CLASS:field-label EXTRACT=TXT
TAG POS=6 TYPE=LABEL ATTR=CLASS:monsoon-fielddata EXTRACT=TXT",https://forum.imacros.net/viewtopic.php?f=7&t=25267&sid=9d96e1268e5b272969ed509f53432633,AI
2626,Search and Extract,"Hello I'm trying to figure out how to search a an identifier and scrape an associated cost with it for example I want to search for the ISBN 9781111771577 and extract the amount of ""Total additional charges' I am playing around with the SEARCH SOURCE=REGEXP: Extract function but seem to be unable to use it correctly and normally get an error 'RuntimeError: Source does not match to REGEXP=''9781449635978'', line 7 (Error code: -926)' can anyone help?

Code: Select all<div class=""itemInfoWrapper cf"">
 <div class=""mobpad cf product-data"">
 <a href=""/textbooks/gardners-art-through-the-ages-vol-1-a-global-history-14th-edition/9781111771577"" class=""imglnk""><img src=""https://img.valorebooks.com/H90/97/9781/978111/9781111771577.jpg"" width=""60"" alt=""Gardner's Art through the Ages V..."" /></a>
 <div class=""itemdata"">
 <a href=""/textbooks/gardners-art-through-the-ages-vol-1-a-global-history-14th-edition/9781111771577"" class=""title"">Gardner's Art through the Ages V...</a>
 
[color=#FF0000][b] <span class=""product-data-label"">ISBN-13: 9781111771577</span><br/>[/b][/color]
 
 
 <span class=""product-data-label"">ISBN: 111177157X</span><br/>
 
 <span class=""product-data-label""><span class=""darkBlue"">Status:</span>
 <a class=""itemstatus"" href="""" target=""_blank"">Shipped</a></span><br/>
 <span class=""product-data-label""><span class=""darkBlue"">Shipping:</span> Standard</span><br/>
 <span class=""product-data-label""><span class=""darkBlue"">Condition:</span> Very Good</span><br/>
 
 <span class=""product-data-label clip clip-1-desktop clip-2"">
 <span class=""darkBlue"">Comments:</span>
 <span class=""""> High-quality textbook rentals since 2006. We do not ship to Alaska, Hawaii or any off-shore territories.
 <span class=""clipEllipsis"">&#133; <button class=""defaultLink moreBtn"">more</button></span>
 <button class=""defaultLink lessBtn"">less</button>
 <span class=""clipFill""></span>
 </span>
<tr class=""totalRow darkBlue"">
[color=#FF0000] [b]<td class=""labelCell"" colspan=""2"">Total additional charges:</td>
 <td>$28.92</td>[/b][/color]
 </tr>
 </table>
 <button class=""flat_button purple aclbclose"" type=""button""><span class=""icon-left-open""></span>Back</button>
 </div>",https://forum.imacros.net/viewtopic.php?f=7&t=25280&sid=9d96e1268e5b272969ed509f53432633,AI
2627,Getting house number from address,"Using V11 on win 10 and firefox 42
Trying to split off house number from US address and put in VAR1. US Address format : ""xxxxx E sunny ave"" No zip
I am new to java script so and I have messed with this till I'm out of ideas.
Here is code with URL but that is password protected and since extract pop-up show correct returns correct address I'm pretty sure problem is in Set format.
I get this error
Error -1100: Wrong format of SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s=s.split("" "",1);"") command, at line: 7

VERSION BUILD=10.3.27.5830
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=https://mymonsoon.com/Parcel/Details?up ... 1067&fc=qs
'Extract address
TAG POS=3 TYPE=DIV ATTR=CLASS:monsoon-fielddata EXTRACT=TXT
'Get house number
SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s=s.split("" "",1);"")
PROMPT {{!VAR1}}

Error -1100: Wrong format of SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s=s.split("" "",1);"") command, at line: 7",https://forum.imacros.net/viewtopic.php?f=7&t=25283&sid=9d96e1268e5b272969ed509f53432633,AI
2628,mouse right click script,"hello guys,

i need your help...i m looking for a mouse right click script.

details : 

when i right click on image then a ""view image"" option appears (which is in eight line of window). i want when i click on view image it should be open in new tab on firefox. if any website has 10 image on first page then all the images should be open in new tabs. 

plz guys help me out to get this imacro script.

script for example site : goodfon.su",https://forum.imacros.net/viewtopic.php?f=7&t=25359&sid=9d96e1268e5b272969ed509f53432633,AI
2629,Extracting string of numbers,"I'm using the iMacros Firefox plugin. I would like to be able to extract an order number from a webpage. The order number isn't contained within a tag, so I'm not sure how to do this. This is the bit of code that I'm looking at:
Code: Select all<br>Order Number:3732856<br>

How can I extract ""3732856"" from that?",https://forum.imacros.net/viewtopic.php?f=7&t=25352&sid=9d96e1268e5b272969ed509f53432633,AI
2637,Help needed scraping H2 and TD element,"Hi All,
Sorry if this is a newbie questions - hope someone can help me out here.
Already tried all suggestion solutions found online and here.

From product pages like this URL https://butik.multiline.dk/Enterprise%2 ... mID=100545 i need to scrape the Headline (H2) and the table values ""100545"", ""3 dnk x 5 ltr/krt"",""100545.CT""...

I tried things like 

""TAG POS=1 TYPE=H2 ATTR=CLASS:idMain&&TXT:* EXTRACT=TXT"" and
""TAG POS=1 TYPE=TD ATTR=CLASS:idSection EXTRACT=TXT""

but i keep getting the #ENF# error..
Been struggling with this for 2 hours now - hoping someone can help me..

Dannie",https://forum.imacros.net/viewtopic.php?f=7&t=25162&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2642,2 Step data extraction not working with loop,"Hi Friends,

I am just trying to extract the title and paragraph of each framework/tool etc.

Trying to extract from below website.
Can you guys help me in this? 
www.softdevtools.com

http://www.softdevtools.com/modules/web ... .php?cid=1
so, In softdev.csv there are number links to loop.
Here is the code which I am trying to do. Hope you understand
Example: 
Need to extract 

ColumnA :  StresStimulus    
ColumnB: StresStimulus is an enterprise-class load test tool for performance testing complex websites, web API and mobile. It provides a comprehensive solution to assess, predict, and improve the performance.

and so on like there are (2847) tools available on this website.

TAB T=1
SET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP NO
SET !DATASOURCE D:\softdev.csv
SET !LOOP 2
'SET !DATASOURCE_COLUMNS 1
SET !DATASOURCE_LINE {{!LOOP}}
URL GOTO={{!COL1}}
TAG POS=1 TYPE=TD ATTR=TXT:*** EXTRACT=TXT
ADD !EXTRACT {{!COL1}}
TAG POS=1 TYPE=DIV ATTR=TXT:*** EXTRACT=TXT
ADD !EXTRACT {{!COL2}}
TAG POS=2 TYPE=TD ATTR=TXT:*** EXTRACT=TXT
ADD !EXTRACT {{!COL1}}
TAG POS=2 TYPE=DIV ATTR=TXT:*** EXTRACT=TXT
ADD !EXTRACT {{!COL2}}
TAG POS=3 TYPE=TD ATTR=TXT:*** EXTRACT=TXT
ADD !EXTRACT {{!COL1}}
TAG POS=3 TYPE=DIV ATTR=TXT:*** EXTRACT=TXT

waiting for correct code.

Thanks
Satish",https://forum.imacros.net/viewtopic.php?f=7&t=25134&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2643,Need to copy and paste text value of CKEDITOR as source html,"I was hoping to find a way to paste the current value of CKEDITOR as the Source (html) of the CKEDITOR instance.

I've migrated thousands of records and the HTML is displaying as plain text with-in the CKEDITOR.  If I can copy the value, click the ""Source"" button, paste the value and then save - I'm golden.
Code: Select allVERSION BUILD=8920312 RECORDER=FX
TAB T=1
URL GOTO=https://na3.salesforce.com/knowledge/publishing/articleEdit.apexp?id=kA150000000097h
WAIT SECONDS=2
SET data
URL GOTO=javascript:data=CKEDITOR.instances['articleEdit:createDocumentForm:sectionRepeat:0:fieldRepeat:0:fieldName:textAreaDelegate_00N50000003iSzW'].getData();
TAG POS=1 TYPE=SPAN ATTR=ID:cke_17_label //clicks the Source button
URL GOTO=javascript:data=CKEDITOR.instances['articleEdit:createDocumentForm:sectionRepeat:0:fieldRepeat:0:fieldName:textAreaDelegate_00N50000003iSzW'].setData('###'+data);


The SET command is wrong, but the good news is that the getData and setData is working.  I can use one javascript URL but I'm not sure if there is a way to click the CKEditor ""Source"" button.

Any help or ideas are appreciated.",https://forum.imacros.net/viewtopic.php?f=7&t=25025&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2646,ReLoop File when end is reached,"VERSION BUILD=8920312 RECORDER=FX
firefox 39.0
WIN: 8.1
------------------------------------------------
Example Code
Code: Select allVERSION BUILD=8920312 RECORDER=FX
TAB T=1
SET !DATASOURCE test.csv
SET !DATASOURCE_COLUMNS 1
SET !DATASOURCE_LINE {{!LOOP}}
PROMPT {{!COL1}}

test.csv file
Test1
Test2


i want imacros when reach (test2) reloop again from (test1)... 
but i got that error
Code: Select allRuntimeError: Invalid DATASOURCE_LINE value: 3, line 5 (Error code: -951)
",https://forum.imacros.net/viewtopic.php?f=7&t=24962&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2647,A loop with an ahref,"Hello Everybody.

I use I macro for 3 years now, and it's first i have a matter.
I need to click an a link wich contains something.
I try to create a loop like this : 
TAG POS=1 TYPE=A ATTR=TXT:{{!COL1}} ==> Doesn't Work
I try 
TAG XPATH=""//a[contains(.,''{{!col1}}""\')]"" ==> Doesn't Work

Can I put dynamic Value in a Attr=something ?",https://forum.imacros.net/viewtopic.php?f=7&t=24957&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2651,Handling two drop downs,"I've been trying to scrape up data from a web page which has two drop downs, Upon submission of drop down box - a table appears which i wanna scrape.

URL I've been trying to scrape is 

https://tseamcet.nic.in [Works only on Internet Explorer]

In the home page, you will see "" College-wise allotment Details"" - click on it and it will take you to a page with two drop downs. 

[The whole site is protected from some sort of security measures, which prevents direct accessing of allotment page via URL || Only way is through HOME PAGE and then COLLEGE WISE ALLOTMENTS DETAILS]

On that page, I wanna select a college from drop down and all branches from second drop down and extract table which appears upon clicking ""Show Allotments"".

Eg: Lets say college i wanna select is ""JNTH - JNTU COLLEGE OF ENGG. HYDERABAD, HYDERABAD"" and branch is  ""COMPUTER SCIENCE AND ENGINEERING"" from two drop downs, upon clicking Show Allotments, a table appears with list of students. I wanna be able to scrape that table with two more extra columns containing COLLEGE NAME and BRANCH. Next I wanna just change branch say ""ELECTRONICS AND COMMUNICATION ENGINEERING"", i wanna again click show allotments and scrape table again. I wanna continue this process for each branch for a college. Upon mismatch of branch with college, I wanna prevent scraping. To cut long story short, I wanna

1. Input College in first drop down
2. Input all the possible branches in second drop down
    i.   Click ""Show Allotments""
    ii.  Scrape table with two more columns containing COLLEGE NAME and BRANCH
    iii. Save Extracted Data in a table
3. Upon mismatch of College Name and branch, Prevent scraping
4. Repeat the process for all the colleges in first dropdown

I've been using iMacros from a month and it has made my life a lot-lot simpler. This is the complex one, I've encountered for the first time. A little help here would be appreciated.",https://forum.imacros.net/viewtopic.php?f=7&t=24896&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2652,Extract ID from ATTR,"Hello

i'm trying to extract ID from ATTR

here the the code:
Code: Select allTAG POS=1 TYPE=A ATTR=DATA-HOVERCARD:/ajax/hovercard/user.php?id=050848560&extragetparams=%7B%22fref%22%3A%22grp_mmbr_list%22%7D&&DIR:ltr&&HREF:https://www.domain.com/username?fref=grp_mmbr_list


i want to extract ""050848560"" only into csv file

how i can do that?",https://forum.imacros.net/viewtopic.php?f=7&t=24874&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2653,"Extracting URL from webpage, and entering on Excel File.","Hello everyone!!

I have been working on creating a Macro for work over the past few days. 

The macro I need to create is very simple, and needs to preform these exact tasks:
1) Copy Current Web-Page's URL 
2) Open Google Sheet Designed to Hold all of the compiled URL's (Already Bookmarked)
3) Paste URL into next available cell. 

I have tried using the ""record"" feature to create this macro. However, the macro keeps using the first URL I had created rather than taking the url of the current page. 

Thanks for any help in advance!!

Jonah",https://forum.imacros.net/viewtopic.php?f=7&t=24876&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2654,3 Part Name,"I have a Name

Mr Billy Bunter

The only parts i need is Billy and Bunter = here is the code i have but i cant get it to work---

TAG POS=1 TYPE=H3 ATTR=TXT:* EXTRACT=TXT
SET FirstName EVAL(""var s=\""{{!EXTRACT}}\""; s.split(\"" \"", 1);"")
SET Index EVAL(""var s=\""{{!EXTRACT}}\""; s.indexOf(\"" \"");"")
ADD Index 1
SET LastName EVAL(""var s=\""{{!EXTRACT}}\""; var d=\""{{Index}}\""; s.substr(d);"")
PROMPT {{FirstName}}<SP>{{LastName}}

I WOULD REALLY LOVE FOR SOMEONE TO HELP ME PLEASE, rather than just give me the answer can you try to explain it also?

Thanks in advance

Mark",https://forum.imacros.net/viewtopic.php?f=7&t=24865&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2656,Please help me,"I want to create an Imacro's script for firefox based on an if else/ statement

I can't get this to work. I don't understand much that is written on the forum 

VERSION BUILD=8881205 RECORDER=FX
///IN THIS FIRST LINE, A WORD IS EXTRACTED
TAG POS=22 TYPE=TD ATTR=CLASS:maintxt EXTRACT=TXT

/// IN THIS SECOND LINE, A DECISION IS MADE. XX (TAG POS=XX) MUST BE REPLACED BY A NUMBER FROM 1 TO 8
/// BASED ON THE WORD EXTRACTED BEFORE. HOW CAN I DO THIS?
TAG POS=XX TYPE=TD ATTR=HEIGHT:25&&CLASS:hover
TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:NoFormName ATTR=TYPE:submit&&VALUE:Ik<SP>wil<SP>graag<SP>reizen.<SP>Stap<SP>in<SP>het<SP>vliegtuig!&&CLASS:nicebut&&NAME:travel




///IN THIS FIRST LINE, A WORD IS EXTRACTED
/// IN THIS SECOND LINE, A DECISION IS MADE. XX (TAG POS=XX) MUST BE REPLACED BY A NUMBER FROM 1 TO 8
/// BASED ON THE WORD EXTRACTED BEFORE. HOW CAN I DO THIS USING IMACROS FOR FIREFOX?",https://forum.imacros.net/viewtopic.php?f=7&t=24679&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2658,Splitting Text after a Comma and Extracting emails,"I am attempting to extra information from a large database of contacts in a format similar to this.

      Person: Dan Chilocoat, Manager
      Company: Reed & Thomas Electrical Contractors, Inc
      Contact details: (410) 239-9680, 1232 Ocean Drive, San Francisco, CA, dan@dantheman.org

I would like my iMacro script to do the following:
1. Grab the first and last name of the person and stop collecting after the comma, and place the name in one column of a csv file
2. If there is an email address under Contact details, place it in a second column next to the name. Otherwise skip to the next entry.
3. The position and order of these elements may change. Is there a way to tell iMacro to just grab the text after ""Person:"" and ""Contact details:"" ?

Here's what I have written so far:
Code: Select allVERSION BUILD=8920312 RECORDER=FX
TAB T=1
TAG POS=5 TYPE=TD ATTR=DIR:ltr EXTRACT=TXT
SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s.split(',')[0];"")

TAG POS=11 TYPE=TD ATTR=DIR:ltr EXTRACT=TXT
SET testString EXTRACT  
SET email EVAL(""'{{testString}}'.match(/(([^<>()[\\]\\\.,;:\\s@\\\""]+(\\.[^<>()[\\]\\\.,;:\\s@\\\""]+)*)|(\\\"".+\\\""))@((\\[[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\])|(([a-zA-Z\\-0-9]+\\.)+[a-zA-Z]{2,}))/g)[0];"")
PROMPT {{email}}

SAVEAS TYPE=EXTRACT FOLDER=/Users/bob/Desktop FILE=test2.csv

TAG POS=1 TYPE=IMG ATTR=BORDER:0&&ALIGN:MIDDLE&&SRC:/images/s-next.gif&&ALT:Next


SET !VAR2 EVAL(""var randomNumber=Math.floor(Math.random()*7 + 1); randomNumber;"")
WAIT SECONDS={{!VAR2}}

The code does not work. Specifically, iMacro ignores the split function I set up and returns TypeError: ""EXTRACT"".match(...) is null, line 8 (Error code: -1001) for the second part of the code. 

How can I edit this code to accomplish the above goals?

EDIT-- Problem solved!",https://forum.imacros.net/viewtopic.php?f=7&t=24652&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2660,Extract (horrible) paged table with AJAX using Javascript,"This is not a question topic, I just would like to share the solution I used to extract info from a page using iMacro FF plugin.
First - iMacro is a fantastic product! I don't need the paid suite so far but I ordered my company to buy it because I will need it sooner or later and it totally worths every penny.
Second - the amount of information in the forum, the tech support and the examples are also fantastic! I built a solution in 2 days from scratch by reading everyone's else issues and solution. That's why I want to share mine, hope it can help others.

The issue: I have to extract to csv all the entries from this website - https://www.compras.rj.gov.br/publico/ (which only works on IE - blergh!)
If you click on the right table on ""10775 concluidas"" it will show a table on the center with the 10775 Licitacoes I have to extract info from.

I used the User Agent Switcher add-on for FF (https://addons.mozilla.org/pt-br/firefo ... -switcher/) to pretend I was in IE. A great add-on, by the way. For some reason in my experience iMacro works better with FF than Chrome or IE, and it has the built-in support for scripting using javascript. So, FF is the way.

The information is based on a paged table with 10 entries per page. I need some info that is showed when I click on each link. The page was designed using Ajax and javascript, so when you click the link it shows the information I need on the same page and div. This wouldn't be a problem, except for the fact that when you click the ""Retornar"" button on the bottom of each detailed page to go back to the previous page with the 10 entries, it always goes to the very FIRST 10 entries. Horrible! I would have to navigate to the correct page each time I finished my 10 entries extraction. I was able to build a solution like that looping through JS and iMacro, but it would take around 20 days with more than 10,000 entries. Not sure if the reason for that is clear, one would have to test th page to better understand. For example, I started in the page 1 (1-10 entries) and extract the 10 entries as desired. Then I increment a variable (k) to run a 'pagechange' iMacro so I could get to the page 2 (11-20 entries). Issue is - when I click the ""Retornar"" button from the detailed entry 11, it does not takes me to Page 2, it takes me to Page 1! So, for instnce, in page 500, after I extract the details from entry 501, the ""Retornar"" button would take me again to page 1 and I would have to run the iMacro 'pagechange' 500 times to get to entry 502...hope the issue is clear.

As I said I developed a first solution using this option that would take 20 days. I will share if someone wants because it can be useful for some loop with JS learning:    
Code: Select allvar macro;
macro =""CODE:"";

macro +=""SET !EXTRACT_TEST_POPUP NO""+""\n"";

///Primeiro item da tabela
macro +=""TAG POS={{i}} TYPE=A ATTR=ONCLICK:javascript:ajaxIncludeDiv('licitacao_detalhe.asp?retorno=licitacoes_concluidas.asp&id=*','principal');&&HREF:javascript:void(0);""+""\n"";
macro +=""WAIT SECONDS=2""+""\n"";

///Salva htm da tabela
macro +=""TAG POS=1 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=2 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=3 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=4 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=5 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=6 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""SAVEAS TYPE=EXTRACT FOLDER=C:\\Users\\Admin\\Downloads\\ FILE=*""+""\n"";

///'Retorna pagina anterior
macro +=""TAG POS=1 TYPE=INPUT:BUTTON ATTR=ONCLICK:javascript:ajaxIncludeDiv('licitacoes_concluidas.asp','principal');&&CLASS:botao&&TYPE:button&&VALUE:<<<SP>Retornar""+""\n"";
macro +=""WAIT SECONDS=2""+""\n"";

var k=0;
for (var j=1;j<1078;j++) { ///loop pelo numero de paginas
	for (var i=1;i<20;i++)	{ ///loop pelo numero de registros da tabela de cada pagina
			if (k>0) { ///ajusta a paginacao porque o codigo sempre retorna para a primeira pagina
				for (var p=0;p<k;p++) { ///vai para a pagina da vez
					iimPlay(""pagechange"");
				}
			}
		iimSet(""i"",i)
		iimPlay(macro)
		i = i+1
			if (i==20) {
				k++;
		}
	}
}


The final solution: I used the Firebug add-on to find out how the paging was being handled. It turns out to be a javascript function with 3 parameters. Then I used the command URL GOTO in iMacro to go to the correct page each time I needed, using a variable to loop through the top entry of each page. Below is the final solution, not sure if there's an easier or more straigth forward solution for this site. I know this one is working great and I hope it can be useful for someone with similar pages to scrape. In the end I successfully scraped the 10775 in a couple of hours.
Code: Select allvar macro;
macro =""CODE:"";

macro +=""SET !EXTRACT_TEST_POPUP NO""+""\n"";
macro +=""URL GOTO=javascript:paginacao('licitacoes_concluidas.asp','principal',{{k}})()""+""\n"";
macro +=""WAIT SECONDS=5""+""\n"";

///Primeiro item da tabela
macro +=""TAG POS={{i}} TYPE=A ATTR=ONCLICK:javascript:ajaxIncludeDiv('licitacao_detalhe.asp?retorno=licitacoes_concluidas.asp&id=*','principal');&&HREF:javascript:void(0);""+""\n"";
macro +=""WAIT SECONDS=3""+""\n"";

///Salva htm da tabela
macro +=""TAG POS=1 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=2 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=3 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=4 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=5 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=6 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""SAVEAS TYPE=EXTRACT FOLDER=C:\\Users\\MMM\\Downloads\\ FILE=*""+""\n"";

///'Retorna pagina anterior
macro +=""TAG POS=1 TYPE=INPUT:BUTTON ATTR=ONCLICK:javascript:ajaxIncludeDiv('licitacoes_concluidas.asp','principal');&&CLASS:botao&&TYPE:button&&VALUE:<<<SP>Retornar""+""\n"";
macro +=""WAIT SECONDS=3""+""\n"";


var k=0;
for (var j=1;j<1078;j++) { ///loop pelo numero de paginas
	for (var i=1;i<20;i++)	{ ///loop pelo numero de registros da tabela de cada pagina
                iimSet(""k"",k)
		iimSet(""i"",i)
		iimPlay(macro)
		i = i+1
			if (i==20) {
				k=k+10;
		}
	}
}



I also did a rewing solution to scrape from the start to end, so I could finish it sooner. The rewind starts k with 10760 to go to the last page with 10 entries and the subtracts 10 on each loop:
Code: Select allvar macro;
macro =""CODE:"";

macro +=""SET !EXTRACT_TEST_POPUP NO""+""\n"";
macro +=""URL GOTO=javascript:paginacao('licitacoes_concluidas.asp','principal',{{k}})()""+""\n"";
macro +=""WAIT SECONDS=5""+""\n"";

///Primeiro item da tabela
macro +=""TAG POS={{i}} TYPE=A ATTR=ONCLICK:javascript:ajaxIncludeDiv('licitacao_detalhe.asp?retorno=licitacoes_concluidas.asp&id=*','principal');&&HREF:javascript:void(0);""+""\n"";
macro +=""WAIT SECONDS=3""+""\n"";

///Salva htm da tabela
macro +=""TAG POS=1 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=2 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=3 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=4 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=5 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""TAG POS=6 TYPE=TD ATTR=CLASS:cx_clara EXTRACT=HTM""+""\n"";
macro +=""SAVEAS TYPE=EXTRACT FOLDER=C:\\Users\\MMM\\Downloads\\ FILE=*""+""\n"";

///'Retorna pagina anterior
macro +=""TAG POS=1 TYPE=INPUT:BUTTON ATTR=ONCLICK:javascript:ajaxIncludeDiv('licitacoes_concluidas.asp','principal');&&CLASS:botao&&TYPE:button&&VALUE:<<<SP>Retornar""+""\n"";
macro +=""WAIT SECONDS=3""+""\n"";


var k=10760;
for (var j=1;j<1078;j++) { ///loop pelo numero de paginas
	for (var i=1;i<20;i++)	{ ///loop pelo numero de registros da tabela de cada pagina
                iimSet(""k"",k)
		iimSet(""i"",i)
		iimPlay(macro)
		i = i+1
			if (i==20) {
				k=k-10;
		}
	}
}



And, of course, the 'pagechange' macro from the first solution, not in use in the final solution anymore:
Code: Select allTAG POS=1 TYPE=IMG ATTR=SRC:https://www.compras.rj.gov.br/publico/imagem/icon_cx_seta-right.gif
WAIT SECONDS=1
",https://forum.imacros.net/viewtopic.php?f=7&t=24570&sid=7fd87bd5f8edabeec090c4a1f0d5b352,AI
2665,Unable to save 1 little box of text,"Hi All,

VERSION BUILD=8920312
Win7
Firefox 36.0.1

What I am trying to do is search for a product on a website, then simply extract and save in a csv file the text description for the product.

I have a bout 300 part numbers to search for.

This is the website it first starts at.
http://us.idec.com/Home.aspx

I then searches for product successfully, and lands at say this page: http://us.idec.com/Catalog/SearchResult ... 3G8JT22TFB*

You can then see the small text description on the page "" Touchscreen HMI 8.4 inch TFT 65K Color SVGA 800x600 Black Bezel Ethernet USB Port "".

This is the decsription I need to save for each searched product.

I have the looping all set, but can not get it to save that section of text.

Please help.

Here is the macro as i have so far.

########

VERSION BUILD=8920312 RECORDER=FX
TAB T=1
URL GOTO=http://us.idec.com/

SET !LOOP 1
SET !DATASOURCE IDEC.csv
SET !DATASOURCE_COLUMNS 1
SET !DATASOURCE_LINE {{!LOOP}}

EVENT TYPE=CLICK SELECTOR=""HTML>BODY>FORM>DIV:nth-of-type(3)>DIV>DIV>DIV:nth-of-type(3)>DIV>DIV:nth-of-type(3)>DIV>DIV>DIV>INPUT"" BUTTON=0
EVENTS TYPE=KEYPRESS SELECTOR=""HTML>BODY>FORM>DIV:nth-of-type(3)>DIV>DIV>DIV:nth-of-type(3)>DIV>DIV:nth-of-type(3)>DIV>DIV>DIV>INPUT"" CHARS={{!COL1}}
EVENT TYPE=MOUSEUP POINT=""(1003,86)""
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>FORM>DIV:nth-of-type(3)>DIV>DIV>DIV:nth-of-type(3)>DIV>DIV:nth-of-type(3)>DIV>DIV>DIV>INPUT:nth-of-type(2)"" BUTTON=0
EVENT TYPE=CLICK SELECTOR=""HTML>BODY>FORM>DIV:nth-of-type(3)>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV:nth-of-type(2)>DIV>DIV>DIV>DIV>DIV:nth-of-type(2)>DIV>DIV>DIV:nth-of-type(2)>P"" BUTTON=0 

TAG POS=1 TYPE=text ATTR=TXT:* EXTRACT=TXT


SAVEAS TYPE=TXT FOLDER=* FILE=idecscrape_{{!NOW:yymmdd_hhnnss}}.csv

WAIT SECONDS=5

I look forward to any help someone could give.",https://forum.imacros.net/viewtopic.php?f=7&t=24569&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2666,If else statement firefox,"I want the script to perform a certain action based on the information it finds on the page
How can I add if / else statements to imacros for firefox?


Thank you kindly for your time,",https://forum.imacros.net/viewtopic.php?f=7&t=24225&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2667,Extract URL of image,"I write code below in .iim,but failed....
what happen ???

The first is available, but the second returns ""      "" (nothing)

VERSION BUILD=4201129     
TAB T=1     
TAB CLOSEALLOTHERS     
URL GOTO=http://www.iopus.com/imacros/demo/v6/extract2/
TAG POS=1 TYPE=A ATTR=TXT:H*links* EXTRACT=HREF 
TAG POS=1 TYPE=IMG ATTR=SRC:*shark_thumbnail.jpg EXTRACT=HTM


iMACROS for FX 6.0.7.5 
FX 3.0.3",https://forum.imacros.net/viewtopic.php?f=7&t=5703&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2668,Can't save captcha image,"Hello
How i could get image from this?
Code: Select all<div style=""background-image: url(""/captcha.aspx?data=0.4835614321813405"");"" id=""CaptchaImageForget"">
                             <div onclick=""ReloadCaptcha('CaptchaImageForget')"" class=""RefreshCaptcha"">
                            </div>
                        </div>",https://forum.imacros.net/viewtopic.php?f=7&t=24421&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2669,[Solved] Saveas extract saves... nothing,"Hello, I have a simple script and a big problem I do not know how to deal with.

Part of (Java)Script macro:
Code: Select alliimPlay(""CODE: TAG POS=1 TYPE=DIV ATTR=CLASS:ImageContainer EXTRACT=HTM"");
iimPlay(""CODE: SAVEAS TYPE=EXTRACT FOLDER=* FILE=+*"");


The tag output is ok (checked via alert(iimGetLastExtract())) and extracts the html code of the ImageContainer div.
However, the saveas saves... nothing... just [BOM]""""[BOM]

Am I missing some command ?
I am using FF 37.0.2 and iMacros 8.9.2 for FF.

EDIT: However, it does work while not playing as javascript (without using iimPlay command - .iim file). I am stuck.",https://forum.imacros.net/viewtopic.php?f=7&t=24385&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2670,Extracting URL from Hyperlinks,"so heya all its-a me again.
using FF 3702, win8.1 and VERSION BUILD=10.4.28.1074 macros


so ive been wondering how to extract links from the table that is presented there in hte html i upladed. i understand that the link is made of ""www.njuskalo.hr/CATEGORY/name-of-the-pr ... sontheleft"". without the category this task would be ez pz with regex, but i cant figure out how to define the category. so thats why im asking how to get the url from the hyperlink 
using IE extract wizard
i get this
Code: Select allTAG POS=1 TYPE=A ATTR=TXT:MEM<SP>UFD<SP>8GB<SP>JF300<SP>TS EXTRACT=HREF
TAG POS=1 TYPE=A ATTR=TXT:USB<SP>memorija<SP>Transcend<SP>8GB<SP>JF560 EXTRACT=HREF
TAG POS=1 TYPE=A ATTR=TXT:PC<SP>HP<SP>280<SP>G1<SP>MT,<SP>K8K51ES EXTRACT=HREF
TAG POS=1 TYPE=A ATTR=TXT:NaviaTec<SP>Mouse<SP>USB<SP>1180 EXTRACT=HREF

which is useless if i want to go to the next page and do the same task of extracting the URL-s",https://forum.imacros.net/viewtopic.php?f=7&t=24373&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2672,Scraping business data of results pages,"Hi imacro-pros,

I麓ve very little experiences with imacros and would appreciate getting some help from you.

I want to scrape email addresses and URLs from search result pages. The thing is to get the data I need imacros has to klick on the first result, grab the data and go back to the results page and klick the next result. Ive been trying different approaches without any success.

here is how I started:

VERSION BUILD=8920312 RECORDER=FX
TAB T=1
URL GOTO=http://www.oldtimer.net/oldtimer-firmen ... katitem=64  search result page
TAG POS=1 TYPE=A ATTR=TXT:Werner<SP>Dreyer<SP>Kraftfahrzeuge<SP>GmbH                                          click on first result to see the ad
WAIT SECONDS=3                                                                                                                                             pause to laod the ad
TAG POS=1 TYPE=H1 ATTR=TXT:Firma<SP>Werner<SP>Dreyer<SP>Kraftfahrzeuge<SP>GmbH<SP>-*             copy and extract the business name
TAG POS=1 TYPE=P ATTR=TXT:Email:<SP>info@classic-truck.de<SP>Internet:<SP>http://www.c*                     copy and extract the email and URL

Then go back the the results page, click on the next result and continue extracting the data

I couldnt figure out how to jump from the first result to the second result after extracting the data of the first result.

I also tried 
TAG POS={{LOOP}} TYPE=A ATTR=TXT:*    but it didnt work

Id be very happy if somebody could help me out here.

Thanks in advance!

Manson",https://forum.imacros.net/viewtopic.php?f=7&t=24330&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2675,Beginner [HELP NEEDED],"Hello everyone...

i'm new to imacros and i'm not a developer at all. i just finished reading the iMacros command refference, but i'm needing some hints to develop some data extraction macro.

the issue that i'm frontin is that, bc i'm no developer i dont have the agility to think on every variable to use in the code. so what im asking is if can anyone help me, pointing the way? like you should read this or that. i'm not looking for completed codes. i want to get there myself in order to learn.

so, i would like to code a macro, that goes to a URL, and in that url there are several links. i need the macro to open a link at a time, and in the new page, extract the data needed. then i need it to loop until there is no more links available to enter.

i dont know if im being clear enough, but at this point any help is a blessing!

thanks in advance..

Cheers


Diogo",https://forum.imacros.net/viewtopic.php?f=7&t=24209&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2676,Looping through URL Cls,"Dear Sir any one can help me how can Loop through Multipal URL in Csv
Code: Select allVERSION BUILD=10022823

'
SET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP NO
SET !DATASOURCE ""C:\Users\Lalitpatel\Desktop\Book1.csv""
SET !DATASOURCE_COLUMNS 1
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}

TAB T=1
TAB URL GOTO={{!COL1}}
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:bus_name EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:bui_name EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:street_name EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:lan_name EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:are_name EXTRACT=TXT
ONDIALOG POS=1 BUTTON=YES
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:city_name EXTRACT=TXT
TAG POS=1 TYPE=SELECT ATTR=NAME:pin_name EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:sta_name EXTRACT=TXT
TAG POS=1 TYPE=A FORM=NAME:os_home ATTR=TXT:Contact<SP>Information
TAG POS=1 TYPE=SELECT ATTR=* EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:contact_person_name EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:contact_person_designation EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=ID:std_code EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:lan_num[] EXTRACT=TXT
TAG POS=4 TYPE=SPAN ATTR=* EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:mob_name[] EXTRACT=TXT
TAG POS=5 TYPE=SPAN ATTR=* EXTRACT=TXT
TAG POS=2 TYPE=INPUT:TEXT ATTR=NAME:mob_name[] EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:email_id EXTRACT=TXT
TAG POS=1 TYPE=INPUT:TEXT ATTR=NAME:website EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=Data.csv
WAIT SECONDS=1

I am getting Error 1200

as my URL is two long

thanks in advance",https://forum.imacros.net/viewtopic.php?f=7&t=24188&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2677,EVAL with multiple replacements,"CIM
Imacros version 101258883
Windows 8.1 English
Imacros browser

I'm trying to replace multiple text strings with a "","" but it's not working. Can someone tell me how do to multiple EVAL s.replace actions?
I can put a bunch of s.replace in a row, but only the last one ever gets executed.

CODE PORTION

SET !EXTRACT NULL
TAG POS=2 TYPE=DIV ATTR=CLASS:cont-wrap EXTRACT=TXT
SET Features1 EVAL(""var s='{{!EXTRACT}}'; s.replace('Food and drink', ','); s.replace('Things to do', ',');"")
PROMPT Features1:<BR><BR>_{{Features1}}_




Trying to do this at:
http://www.hotels.com/hotel/details.htm ... GT=1&YGF=0

Under the ""In the hotel"" part.

Thank you",https://forum.imacros.net/viewtopic.php?f=7&t=24107&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2678,Save as all links in a page,"Hello I'm new i'm the IMacros.
I'm using Linux/Debian Firefox imacro 8.8.9
I trying to download all links in a page but they imacros download only the first link.
link example ""href=""consinternetpro1a.csp?processo=19896100001685032"">0016850-32.1989.4.03.6100""
the amount of links and the links are dynamics being generated after a form

bellow is the section of the html with the links



<tbody><tr>
        <td bgcolor=""#CCCCCC"" width=""184""><p align=""center""><font face=""Verdana"" size=""1"">Processo</font></p></td>
        <td bgcolor=""#CCCCCC"" width=""184""><p align=""center""><font face=""Verdana"" size=""1"">Classe</font></p></td>
        <td bgcolor=""#CCCCCC"" width=""64""><p align=""center""><font face=""Verdana"" size=""1"">Secretaria</font></p></td>
        <td bgcolor=""#CCCCCC"" width=""184""><p align=""center""><font face=""Verdana"" size=""1"">Situa莽茫o</font></p></td>
        <td bgcolor=""#CCCCCC"" width=""129""><p align=""center""><font face=""Verdana"" size=""1"">Numera莽茫o Antiga</font></p></td>
        <td bgcolor=""#CCCCCC"" width=""184""><p align=""center""><font face=""Verdana"" size=""1"">Localiza莽茫o</font></p></td>
</tr>
<tr>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""Processo1"" bgcolor=""white"" width=""162""><a style=""outline: 1px solid blue;"" href=""consinternetpro1a.csp?processo=19896100001685032"">0016850-32.1989.4.03.6100</a><a></a></td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Classe1"" bgcolor=""white"" width=""184"">PROCEDIMENTO ORDINARIO</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""Vara1"" bgcolor=""white"" width=""64"">15a Vara</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Situacao1"" bgcolor=""white"" width=""184"">BAIXA - FINDO</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""NAntiga1"" bgcolor=""white"" width=""129"">89.0016850-9</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Localizacao1"" bgcolor=""white"" width=""184"">AG.ARQ em 23/07/2009</td>
</tr>
<tr>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""Processo2"" bgcolor=""#CCCCCC"" width=""162""><a href=""consinternetpro1a.csp?processo=19906100000059732"">0000597-32.1990.4.03.6100</a><a></a></td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Classe2"" bgcolor=""#CCCCCC"" width=""184"">IMPUGNACAO AO VALOR DA CAUSA</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""Vara2"" bgcolor=""#CCCCCC"" width=""64"">15a Vara</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Situacao2"" bgcolor=""#CCCCCC"" width=""184"">BAIXA - FINDO</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""NAntiga2"" bgcolor=""#CCCCCC"" width=""129"">90.0000597-3</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Localizacao2"" bgcolor=""#CCCCCC"" width=""184"">AG.ARQ em 07/08/2007</td>
</tr>
<tr>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""Processo3"" bgcolor=""white"" width=""162""><a href=""consinternetpro1a.csp?processo=19946100002859970"">0028599-70.1994.4.03.6100</a><a></a></td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Classe3"" bgcolor=""white"" width=""184"">CUMPRIMENTO PROVISORIO DE SENTENCA</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""Vara3"" bgcolor=""white"" width=""64"">15a Vara</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Situacao3"" bgcolor=""white"" width=""184"">BAIXA - FINDO</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""NAntiga3"" bgcolor=""white"" width=""129"">94.0028599-0</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Localizacao3"" bgcolor=""white"" width=""184"">AG.ARQ em 07/08/2007</td>
</tr>
<tr>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""Processo4"" bgcolor=""#CCCCCC"" width=""162""><a href=""consinternetpro1a.csp?processo=19956100002801566"">0028015-66.1995.4.03.6100</a><a></a></td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Classe4"" bgcolor=""#CCCCCC"" width=""184"">EMBARGOS A EXECUCAO FUNDADA EM SENTENCA</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""Vara4"" bgcolor=""#CCCCCC"" width=""64"">15a Vara</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Situacao4"" bgcolor=""#CCCCCC"" width=""184"">BAIXA - FINDO</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: right;"" id=""NAntiga4"" bgcolor=""#CCCCCC"" width=""129"">95.0028015-9</td>
        <td style=""font: 10px Verdana,Courier New,arial,helvetica; text-align: left;"" id=""Localizacao4"" bgcolor=""#CCCCCC"" width=""184"">AG.ARQ em 07/08/2007</td>
</tr>",https://forum.imacros.net/viewtopic.php?f=7&t=24124&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2679,Save URL to file with parameters,"Hi,

I just started using iMacro so maybe for the experts this might be simple.
But after I checked the forum for similar issues without success I try to find help here.

So, currently I use the iMacro Addon in Firefox and start it like this

Run, %firefoxExePath% %url% %iMacro% from AHK

there the iMakro looks like this

VERSION  BUILD=7500718 RECORDER=FX

SET !VAR1 {{!URLCURRENT}}
SET !EXTRACT_TEST_POPUP NO
TAB T=1     
WAIT SECONDS=3
URL GOTO={{!VAR1}}
SAVEAS TYPE=HTM FOLDER=C:\Workspace\Downloads FILE=temp1.htm

That works fine, but I have some questions.

1. Is it possible to pass parameters to the macro for the folder and the file?
So I would like to do something like

SAVEAS TYPE=HTM FOLDER={{!VAR2}} FILE={{!VAR3}}

2. If the filename or foldername include spaces I recognized some problems. Is theere a way to use spaces in the parameters?

Thanks in advance!

Kind regards

Sheldon 

.",https://forum.imacros.net/viewtopic.php?f=7&t=22992&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2680,SET command not working in under JavaScript when extracting,"Hello everyone,

At the company I'm working, we're (trying) to build an authomatic data extractor but it fails and I can't see a way to solve it.
It takes out information (the latest ID which is displayed in main page in our internal web application) and we need to store that information later on a third-party website (and we are not allowed to use any API from them)

We want to use the latest ID as the source for a loop in Javascript.
Once we open our web app the information shown is the latest customer order information (ie: 32000)

When I test the macro code without JavaScript it works but if I am planning to do it this way inside a Javascript file, it comes with an error.
wrong format of SET command, line 6 (Error code: 910)

Javascript file was done in NotePad (UTF-8), right now it is stored inside iMacros\Macros Folder (Same directory as samples)

I want to do that number as a variable for the loop (which I can't use it yet because it's failing)

I know the for loop is wrong at the moment but I cannot understand why a simple line works fine when I run it as iim file and when I try to use it inside JavaScript, it fails.

I am using Firefox 35.0.1 / Windows 8.1
iMacros: version 8.8.8

Does anybody know how can I do those two tasks?

Thanks in advance,
Code: Select allvar macro;
var i, retcode, errtext;

macro = ""CODE:""+""\n"";
macro += ""TAB T=1""+""\n"";
macro += ""URL GOTO=http//127.0.0.1/anual_reports""+""\n"";
macro += ""SET !EXTRACT_TEST_POPUP NO""+""\n"";
macro += ""TAG POS=1 TYPE=H1 ATTR=TXT:* EXTRACT=TXT""+""\n""+""\n"";
' Here we are supposed to store the latest obtained data (32000) but it throws an error.
macro += ""SET !VAR1 EVAL(\""var s=\""{{!EXTRACT}}\""; s=s.replace(\""#\"",\""\"");\"")"" + ""\n"";
macro += ""URL GOTO=www.externalwebsite.com/index.php?id={{!VAR1}}"";

for ( i = 1; i <= {{!VAR1}}; i++) {
	retcode = iimPlay(macro);
	
    if (retcode < 0) {          
        errtext = iimGetLastError();
        alert(errtext);
        break;
    }
}

Here goes the test I did with iim file (limited because later I think I can't use it for a loop):
Code: Select allVERSION BUILD=8881205 RECORDER=FX
TAB T=1
URL GOTO=http//127.0.0.1/anual_reports
SET !EXTRACT_TEST_POPUP NO
TAG POS=1 TYPE=H1 ATTR=TXT:* EXTRACT=TXT
SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s=s.replace(\""#\"",\""\"");"")
URL GOTO=URL GOTO=www.externalwebsite.com/index.php?id={{!VAR1}}",https://forum.imacros.net/viewtopic.php?f=7&t=24022&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2682,Not getting the right element,"iMarcro 8.8.5, Firefox 33, Windows 8.1 64-bit

I'm having a bit of trouble trying to tag the right element. When I run my script I tag the right thing buy when I try to find tag the forms that's related to my element its tags the forms related to the element next to my desire element.

My Script:
TAG POS=1 TYPE=H2 ATTR=TXT:*TROPICAL*GREEN* Extract=TXT
TAG POS=R1 TYPE=LABEL FORM=ID:product* ATTR=TXT:9.5
TAG POS=R1 TYPE=INPUT:RADIO FORM=ID:product* ATTR=ID:swatch-0-9-5
TAG POS=R1 TYPE=INPUT:SUBMIT FORM=ID:product* ATTR=NAME:button
WAIT SECONDS=1
TAG POS=1 TYPE=A ATTR=TXT:View<SP>cart

HTML:
<div id=""product-description"">
<h2>Hanon x Asics Gel Epirus 'TROPICAL GREEN'</h2> <---Anchor
<p id=""product-price"">
<span class=""product-price"" itemprop=""price"">$130.00</span>
</p>
<form id=""product-form-371757621"" data-money-format=""${{amount}}"" method=""post"" action=""/cart/add""> <----- Desire element to be selected
<script>
<script type=""text/javascript"">",https://forum.imacros.net/viewtopic.php?f=7&t=23525&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2683,IFRAME Extraction,"I have been trying to figure out this for 2 days, and keep failing

VERSION BUILD=8070701 RECORDER=CR
TAB T=1
URL GOTO=http://www.example.com/test-{{!LOOP}}
TAG POS=1 TYPE=A ATTR=TXT:Mp4uploadHD<SP>Video
TAG POS=1 TYPE=IFRAME ATTR=TXT:mp4* EXTRACT=HREF


The source
Code: Select all<div class=""player-area"">
					<div id=""embed_holder"" style=""position: relative; width: 657px; height: 349px; background: #000 url(/images/loading.gif) no-repeat;""><div class=""videoembed activeembed"" id=""embed-262635""><iframe title=""MP4Upload"" type=""text/html"" allowfullscreen=""true"" frameborder=""0"" scrolling=""no"" width=""660"" height=""370"" src=""http://example.com/embed-5wyhhu3nvi0c-650x370.html""></iframe></div></div>				</div>

I want to extract this part http://example.com/embed-5wyhhu3nvi0c-650x370.html

I am using latest macro addon for firefox.",https://forum.imacros.net/viewtopic.php?f=7&t=23809&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2690,#EANF# error when trying to scrap XML file,"Hello!
I am using iMacros V10, VERSION BUILD=10.3.27.5830, Windows 7, 64-bit, IE 11 and Firefox 34

I am trying to scrape data from an XML file, however I am having trouble getting the iMacro script to correctly identify the TAG text I want to scrape. I tried using the Extraction Tool for iMacro Internet Explorer but when I ran the script I get #EANF#. I also tried using relative positioning, but to no luck. Can someone offer a suggestion?
Code: Select allTAG POS=1 TYPE=MARSHA_CODE ATTR=* EXTRACT=TXT
TAG POS=1 TYPE=TOTAL_PURCHASES ATTR=* EXTRACT=TXT
TAG POS=1 TYPE=TOTAL_MAC_GUEST ATTR=* EXTRACT=TXT
TAG POS=1 TYPE=TOTAL_MAC_CONFERENCE ATTR=* EXTRACT=TXT
TAG POS=1 TYPE=TOTAL_MAC_PUBLIC ATTR=* EXTRACT=TXT
Code: Select all'TAG POS=0 TYPE=DATASET ATTR=TXT:<?xml<SP>version=""1.0""<SP>encoding=""UTF-8""?>@namespace<SP>html<SP>url(http://www.w3.org/199*
'TAG POS=R12 TYPE=MARSHA_CODE ATTR=TXT:<MARSHA_Code>PSPBR</MARSHA_Code>

Code: Select all<?xml version=""1.0"" encoding=""UTF-8""?>

-<DataSet xmlns=""https://secure.elevenwireless.com/2011/10/ElevenOs"">


-<xs:schema xmlns="""" xmlns:msdata=""urn:schemas-microsoft-com:xml-msdata"" xmlns:xs=""http://www.w3.org/2001/XMLSchema"" id=""NewDataSet"">


-<xs:element msdata:UseCurrentLocale=""true"" msdata:IsDataSet=""true"" name=""NewDataSet"">


-<xs:complexType>


-<xs:choice maxOccurs=""unbounded"" minOccurs=""0"">


-<xs:element name=""Table"">


-<xs:complexType>


-<xs:sequence>

<xs:element name=""Date"" minOccurs=""0"" type=""xs:string""/>

<xs:element name=""Brand"" minOccurs=""0"" type=""xs:string""/>

<xs:element name=""MARSHA_Code"" minOccurs=""0"" type=""xs:string""/>

<xs:element name=""Occupied_Rooms"" minOccurs=""0"" type=""xs:int""/>

<xs:element name=""Total_Purchases"" minOccurs=""0"" type=""xs:int""/>

<xs:element name=""Total_Mac_Guest"" minOccurs=""0"" type=""xs:int""/>

<xs:element name=""Total_Mac_Conference"" minOccurs=""0"" type=""xs:int""/>

<xs:element name=""Total_Mac_Public"" minOccurs=""0"" type=""xs:int""/>

</xs:sequence>

</xs:complexType>

</xs:element>

</xs:choice>

</xs:complexType>

</xs:element>

</xs:schema>


-<diffgr:diffgram xmlns:msdata=""urn:schemas-microsoft-com:xml-msdata"" xmlns:diffgr=""urn:schemas-microsoft-com:xml-diffgram-v1"">


-<NewDataSet xmlns="""">


-<Table diffgr:id=""Table1"" msdata:rowOrder=""0"">

<Date>2014-12</Date>

<Brand>Renaissance</Brand>

<MARSHA_Code>PSPBR</MARSHA_Code>

<Total_Purchases>9967</Total_Purchases>

<Total_Mac_Guest>4446</Total_Mac_Guest>

<Total_Mac_Conference>693</Total_Mac_Conference>

<Total_Mac_Public>754</Total_Mac_Public>

</Table>

</NewDataSet>

</diffgr:diffgram>

</DataSet>",https://forum.imacros.net/viewtopic.php?f=7&t=23864&sid=030d300656bab0b18f7fde81bb00e1ce,AI
2693,Extracting HTML from textarea break <BR> tags lost,"I want to extract html code that is inside of a textarea

However when I do so html that look like this.

<strong>A Place to Log Your Ideas for Configurator!</strong><br><br>Do you have recommendations

---------------------- The <br><br> tags are replaced with a line feed ---------------------------
<strong>A Place to Log Your Ideas for Configurator!</strong>

Do you have recommendations
-----------------------------------------------------------------------------------------------------

<br> and <p> tags are lost or replaced.
How do you extract html from a textarea and maintain all tags

Thanks for your help.
I am running 
VERSION BUILD=8820413 RECORDER=FX on firefox.

My code looks like this.

TAG POS=1 TYPE=TEXTAREA ATTR=TXT:* EXTRACT=TXT
SET !CLIPBOARD {{!EXTRACT}}",https://forum.imacros.net/viewtopic.php?f=7&t=23423&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2694,extracting text from DHTML Layer Popup for Errors and Alerts,"hello everyone...
I'm new to iMacros
I want to ask how to extract text from a web, especially the alert box, that usually appear with an exclamation sign in triangle

here is the example HTML that I got after I save the url page:
Code: Select all	<!-- This right here is the DHTML Layer Popup for Errors and Alerts -->
	<a name=""top_of_error_box""></a>
	<div id=""container_errors"">
		<div style=""visibility: visible;"" id=""error_box"">
			<div class=""shadow-outer"">
				<div class=""shadow-inner"">
					<div class=""error-header"">
						<i class=""icon-exclamation-triangle""></i>&nbsp;&nbsp;
						<strong>Looks like there's a problem...</strong>
						<i id=""form_error_box_close"" class=""icon-times float-right""></i>
					</div>
					<div style=""text-align:left; background:#fff; padding:10px;"">
						<div id=""error_box_msg_area"">
							<div id=""error_items""><div>this is the text that I want to be extracted and save to .csv file</div></div>								
							<!-- BEGIN LivePerson Noscript Button Code -->

I want to be able to save :
this is the text that I want to be extracted and save to .csv file

is there anyone in here that can help me ???",https://forum.imacros.net/viewtopic.php?f=7&t=23780&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2696,"Why my virtualBox, Firefox cannot install the iMacros","Why my virtualBox, Firefox cannot install the iMacros, it is xp sp3 system with .net 4.

I try to test some versions, firefox versions are 21 26 32 33, all are fail.

Why????",https://forum.imacros.net/viewtopic.php?f=7&t=23687&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2698,EXTRACTION CONFLICT,"i just want to extract this on excel by columns.. name, number, postal, address
any ideas?

another one.. some of the list has two numbers .. 
thats why the ""pos=no"" so each page is changing so i just do a manual click
of it then add command.. whew ..

still my result fails....
 -newbie

VERSION BUILD=10022823
TAB T=1
SET !EXTRACT_TEST_POPUP NO
TAB CLOSEALLOTHERS
URL GOTO=http://www.degulesider.dk/person/resultat/8832
TAG POS=1 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=1 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=2 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=2 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=2 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=3 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=4 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=3 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=1 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_land_line EXTRACT=TXT
TAG POS=5 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=6 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=4 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=2 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_land_line EXTRACT=TXT
TAG POS=7 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=8 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=5 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=3 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=9 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=10 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=6 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=4 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=11 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=12 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=7 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=5 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=13 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=14 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=8 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=6 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=15 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=16 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=9 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=7 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=17 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=18 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=10 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=8 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=19 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=20 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=11 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=9 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=21 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=22 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=12 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=6 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_land_line EXTRACT=TXT
TAG POS=23 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=24 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=13 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=7 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_land_line EXTRACT=TXT
TAG POS=25 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=26 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=14 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=10 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=27 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=28 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=15 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=8 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_land_line EXTRACT=TXT
TAG POS=29 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=30 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=16 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=11 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=31 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=32 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=17 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=12 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=33 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=34 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=18 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=13 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=35 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=36 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=19 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=14 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=19 TYPE=SPAN ATTR=CLASS:hit-street-address EXTRACT=TXT
TAG POS=38 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=20 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=15 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=20 TYPE=SPAN ATTR=CLASS:hit-street-address EXTRACT=TXT
TAG POS=40 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=21 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=16 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=41 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=21 TYPE=SPAN ATTR=CLASS:hit-postal-code EXTRACT=TXT
TAG POS=22 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=11 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_land_line EXTRACT=TXT
TAG POS=43 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=44 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=23 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=17 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_mobile EXTRACT=TXT
TAG POS=45 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=46 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=24 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=12 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_land_line EXTRACT=TXT
TAG POS=47 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=48 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=25 TYPE=SPAN ATTR=CLASS:hit-name-ellipsis EXTRACT=TXT
TAG POS=13 TYPE=SPAN ATTR=CLASS:hit-phone-number<SP>type-phone_normal_land_line EXTRACT=TXT
TAG POS=49 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=50 TYPE=DIV ATTR=CLASS:hit-address-line EXTRACT=TXT
TAG POS=1 TYPE=A ATTR=TXT:2
WAIT SECONDS=3
SAVEAS TYPE=EXTRACT FOLDER=c:\sample FILE=Extract_{{!NOW:ddmmyy_hhnnss}}.csv",https://forum.imacros.net/viewtopic.php?f=7&t=23326&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2701,Looping Part of Macro,"Hi,

I have a macro that I am running to extract data from a table. As you can see below, I need to advance the variable by 8 after each extraction, however, when looping, it will set the variable back to its original value. Is there a way to loop only part of a macro?

Thanks!
Code: Select allVERSION BUILD=10022823
TAB T=1
TAB CLOSEALLOTHERS

SET !EXTRACT_TEST_POPUP NO
SET !DATASOURCE_LINE {{!LOOP}} 

Set var1 100
Set var2 2
Set var3 3
Set var4 4
Set var5 5
Set var6 6
Set var7 7
Set var8 8


TAG POS=1 TYPE=TD ATTR=ID:ctl00_mainCopy_ScacGridView_tccell0_0 EXTRACT=TXT
TAG POS={{var2}} TYPE=TD ATTR=CLASS:dxgv EXTRACT=TXT
TAG POS={{var3}} TYPE=TD ATTR=CLASS:dxgv EXTRACT=TXT
TAG POS={{var4}} TYPE=TD ATTR=CLASS:dxgv EXTRACT=TXT
TAG POS={{var5}} TYPE=TD ATTR=CLASS:dxgv EXTRACT=TXT
TAG POS={{var6}} TYPE=TD ATTR=CLASS:dxgv EXTRACT=TXT
TAG POS={{var7}} TYPE=TD ATTR=CLASS:dxgv EXTRACT=TXT
TAG POS={{var8}} TYPE=TD ATTR=CLASS:dxgv EXTRACT=TXT

SAVEAS TYPE=EXTRACT FOLDER=* FILE=mytable_test.csv

Add var2 8
Add var3 8
Add var4 8
Add var5 8
Add var6 8
Add var7 8
Add var8 8",https://forum.imacros.net/viewtopic.php?f=7&t=23588&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2702,Extract data from 'itemprop' with iMacros,"Can someone please help me with this? I wrote a script (iMacros) to get data from a website, and it works well.
I am using iMacros for Firefox 8.8.2.
But, the only thing I can not seem to get right, is the extraction of 'itemprop' values. See below a piece of HTML data:
Code: Select all<div id=""data_profile_middle160"">
<h3><span>Information</span></h3>
<div><strong>Location:</strong>
<span itemprop=""address"" itemscope itemtype=""http://schema.org/PostalAddress"">
<a href=""http://thedjlist.com/world/Netherlands/Amsterdam/"" itemprop=""addressLocality"">Amsterdam</a>,
<a href=""http://thedjlist.com/world/netherlands/"" itemprop=""addressCountry"">Netherlands</a>


The iMacros script I am using is:
Code: Select allVERSION BUILD=6000328
TAB T=1

SET !EXTRACT_TEST_POPUP NO

SET !DATASOURCE sites.csv
SET !DATASOURCE_COLUMNS 1
SET !LOOP 1
SET !DATASOURCE_LINE {{!LOOP}}
URL GOTO={{!COL1}}
TAG POS=1 TYPE=DIV ATTR=ID:dj_name&&TXT:* EXTRACT=TXT  
TAG POS=1 TYPE=A ATTR=ITEMPROP:addressLocality&&TXT:* EXTRACT=TXT    
ADD !EXTRACT {{!URLCURRENT}}
SAVEAS TYPE=EXTRACT FOLDER=* FILE=+{{!NOW:ddmmyyyy}}.csv
WAIT SECONDS=1


The output I get is this:
Code: Select all""AFROJACK  Send Message"",""#EANF#"",""http://thedjlist.com/djs/AFROJACK/""


The URL is: http://thedjlist.com/djs/AFROJACK/

I'd like to extract the value 'Amsterdam' from the itemprop 'addressLocality'. The page is full of itemprop's so I really need to know how to get the values.

Thanks for help in advance.",https://forum.imacros.net/viewtopic.php?f=7&t=23557&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2706,[HOW TO] Extract Only Selected Text,"Hello Everybody =)) 
I dont know how to Extract Selected Text, But i know how to extract text
Example
Code: Select allEnter this code into your dashboard to verify your email address and activate password recovery:
LRBEeEkqTKG45sH2tpT6R8 
(WARNING! DO NOT SHARE THIS CODE WITH ANYONE)
https://launch.stellar.org/#/dashboard

You will not be able to recover your password if you do not activate this feature.

Keep this code SAFE.

Anyone with this code and your username can gain access to your account. If you lose both your password and your recovery code, you will lose access to your funds, so be safe!

Onward,
The Stellar Team
鈥斺€?
This message was sent by the Stellar Development Foundation,
PO Box 411486, San Francisco, CA 94141, USA.
If you would like to unsubscribe from further email, please notify us: https://api.stellar.org/unsubscribe?email=nage%40lackmail.net.
How to extract only the recovery code (LRBEeEkqTKG45sH2tpT6R8 ) I might add that these are the variables because the recovery code will be changed .

I using windows 8.1 Pro x64, Firefox 34.0, iMacros for Firefox 8.8.2  ",https://forum.imacros.net/viewtopic.php?f=7&t=23520&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2709,Extract content based on its proximity to a located image,"Hi.

I had a search through ther forum but i may not be able to search correctly for what i am looking for so please forgive me if this has ben asked before.

I am planning to use the find image on page function to identify the x and y location of the image. I then want to extract any text which is within a specific pixel distance from the image. or possibly just in the div above and the div below 

however this seems to have its own set of issues as the CSS may make a div appear in a different place than its location within the raw html.

hence i have turned to imacros.

So question is can i specify to extract text from within a certain area of the page ?",https://forum.imacros.net/viewtopic.php?f=7&t=23466&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2710,Split {{!EXTRACT}} data into csv file,"Hello all, I am having a hard time finding documentation on the ""split"" command.

All I would like to do is split the result I get from extraction into two fields of a csv file, one comtaining the email adress, the other all the rest of the extracted data.

I am running the lastest iMacros on FF 30 on a Linux Ubuntu 13.10 box. 

The code I have: 
Code: Select allVERSION BUILD=8820413 RECORDER=FX
TAB T=1


TAG POS={{!LOOP}} TYPE=SPAN ATTR=TXT:[in]
TAG POS=R1 TYPE=H3 ATTR=TXT:* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=*

Creates a file called extract.csv and has all the matches, but they are all on the first column. The data is in this format:
John, Smith johnsmith@somesite.com
or, 
John, Andrew Smith johnasmith@somesite.com

How can I slit the email out of there?? And make it be two fields in the csv file??

I have tried this:
Code: Select allVERSION BUILD=8820413 RECORDER=FX
TAB T=1


TAG POS={{!LOOP}} TYPE=SPAN ATTR=TXT:[in]
TAG POS=R1 TYPE=H3 ATTR=TXT:* EXTRACT=TXT
SET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s = s.split(\w@\w.\w); s;"")
SAVEAS TYPE=EXTRACT FOLDER=* FILE=*

Doesn't work ",https://forum.imacros.net/viewtopic.php?f=7&t=23464&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2712,Extracting always returns #EANF# with IE plugin (even demos),"Hi,

Got a strange situation with the latest IE plugin, on a windows 7 system with IE 8 (can't upgrade due to compatibility reasons with certain applications)

The macro keeps returning #EANF# when trying to extract text or html from a website ... it's not an issue with my macro as the included extract demo also keeps giving the #EANF#.
We have another system, also running windows 7 with the exact same version of IE 8 and also latest version of the imacros IE plugin (downloaded from this site a couple days ago) ... and on that system it works without a problem.

Does anybody have any idea what could cause this?
Could it be a certain security setting in IE that prevents the imacros IE plugin from extracting data?

Thanks in advance ",https://forum.imacros.net/viewtopic.php?f=7&t=23427&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2713,Search Term - Email Alert,"how do I setup imacros so an email alert is sent when a specific search terms appears on a website?

Ex. https://www.yahoo.com/
Keyword ""wrestling""

this is my first time posting, I tried to search, I am sure this has been discussed in the past so please link me if possible

Best Regards",https://forum.imacros.net/viewtopic.php?f=7&t=23406&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2717,Reload page after X loops?,"Hi guys,

I'm looking for a method of a loading a webpage after a certain number of looks e.g
if iLOOP = 1000 URL GOTO=http://.......

I know if statement aren't possible in imacros. 

Is there another way?

Cheers,",https://forum.imacros.net/viewtopic.php?f=7&t=23354&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2718,Problem to extract url with Pos=R1,"I tested much things, but everytime there is a problem that i cant solve. So here is the next, i thought it is very simple, but iam wrong...

I want to extract the url:
Code: Select all<a href=""/i,1162736/buecher/harry-potter-band-4-harry-potter-und-der-feuerkelch-joanne-k-rowling-gebundene-ausgabe"" 
This is the dom:
Code: Select all<div style=""outline: 1px solid blue;"" class=""list-item-inner"">
    [b]<a href=""/i,1162736/buecher/harry-potter-band-4-harry-potter-und-der-feuerkelch-joanne-k-rowling-gebundene-ausgabe"" [/b]title=""Harry Potter: Band 4 - Harry Potter und der Feuerkelch - Joanne K. Rowling [Gebundene Ausgabe]"" class=""productConversion"">
        <img style=""display: block;"" src=""https://dksw3hw5iu1be.cloudfront.net/1162736/thumbs/110.gif?lm=0"" data-original=""https://dksw3hw5iu1be.cloudfront.net/1162736/thumbs/110.gif?lm=0"" class=""cover"" alt=""Harry Potter: Band 4 - Harry Potter und der Feuerkelch - Joanne K. Rowling [Gebundene Ausgabe]"" height=""154"" width=""110"">    </a>
    <h2>
        <a href=""/i,1162736/buecher/harry-potter-band-4-harry-potter-und-der-feuerkelch-joanne-k-rowling-gebundene-ausgabe"" title=""Harry Potter: Band 4 - Harry Potter und der Feuerkelch - Joanne K. Rowling [Gebundene Ausgabe]"" class=""inverse productConversion ellipsis"">
            Harry Potter: Band 4 - Harry Potter und der Feuerkelch - Joanne K... </a>            
        <div class=""ellipsis"" title=""B眉cher"">
            <p class=""category-icon category-book"">B眉cher</p>        </div>
    </h2>
    <div class=""data"">
   <p class=""availability green"">
    3 Artikel auf Lager            </p>
   <p class=""price"">
<span>
 <strong class=""alpha"">2,69&nbsp;鈧?/strong>
 </span>
 </p>
<p class=""vat"">
                Preise sind Endpreise (kein USt-Ausweis gem. 搂25a UStG)<br> zzgl.
                [b]<a href=""/s/agb#versandkosten"" class=""deliveryCostsTooltip"">
                    Versandkosten</a>[/b]
                (Standardversand: 3,99 鈧?
            </p>

I tried this:
Code: Select allTAG POS=1 TYPE=div ATTR=class:list-item-inner*
TAG POS=1 TYPE=A ATTR=HREF:/* EXTRACT=HREF
and get http://www.rebuy.de/s/agb#versandkosten

i tried this:
TAG POS=1 TYPE=div ATTR=class:list-item-inner*
TAG POS=1 TYPE=A ATTR=HREF:/i,1162736/buecher/harry-potter-band-4-harry-potter-und-der-feuerkelch-joanne-k-rowling-gebundene-ausgabe EXTRACT=HREF

that works, but when i changed POS=1 to POS=R1
Code: Select allTAG POS=1 TYPE=div ATTR=class:list-item-inner*
TAG POS=R1 TYPE=A ATTR=HREF:/i,1162736/buecher/harry-potter-band-4-harry-potter-und-der-feuerkelch-joanne-k-rowling-gebundene-ausgabe EXTRACT=HREF

i get #EANF#

I dont understand why its not working.
I set an relative position und want to extract the following a href with Pos=R1...

Maybe this time anyone can help me.

Using
firefox V. 31.0.0.5310
imacros 10.0.0.2738
Windows XP",https://forum.imacros.net/viewtopic.php?f=7&t=23348&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2719,Need help with loop for js,"Hi.
Iam from Germany, so sorry for my bad english. 
On a website, i want to click 50 links. After each click extract a few values, go back, and klick the next one.
After this go to the next page and begin again ...
The first macro is this (erbuy.iim) :
Code: Select allSET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP no
SET !VAR1 {{!LOOP}}
ADD !VAR1 

TAG POS={{!VAR1}} TYPE=A ATTR=CLASS:productConversion*
TAG POS=R1 TYPE=A ATTR=HREF:* 
TAG POS=1 TYPE=SPAN ATTR=class:loud* EXTRACT=TXT
ADD !EXTRACT {{!URLCURRENT}}
TAG POS=1 TYPE=SPAN ATTR=class:key&&TXT:EAN*
TAG POS=R1 TYPE=SPAN ATTR=class:value* EXTRACT=TXT
TAG POS=1 TYPE=LI ATTR=class:price&&TXT:Preis* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Dokumente<SP>und<SP>Einstellungen\blub\Desktop\Macros\ FILE=erbuy-auslesen.csv
BACK

When i set up the repetitions on firefox to 50, it works fine.
Than the next macor should start (erbuy-weiter.iim) :
Code: Select allTAG POS=1 TYPE=A ATTR=TXT:weiter

I create a js file (erbuy-java.js)
Code: Select alliimPlay(""\erbuy.iim""); 
iimPlay(""\erbuy-weiter.iim""); 

It works, but because the first script starts every time at the beginning, it only extracted TAG POS=1.
So i tried something like this ( a part of the code )
Code: Select allvar jsNewLine=""\n""
var MyMacroCode

MyMacroCode = ""CODE:""
MyMacroCode = MyMacroCode+""SET !VAR1 {{!LOOP}}"" + jsNewLine
MyMacroCode = MyMacroCode+""ADD !VAR1 "" + jsNewLine
MyMacroCode = MyMacroCode+""TAG POS={{!VAR1}} TYPE=A ATTR=CLASS:productConversion*"" + jsNewLine
iimPlay(MyMacroCode)
 

That doesent work. I think i have read that js can not handle commands with {{!}}.

Now i dont know what to do next. Hope anyone can help me to find a solution.",https://forum.imacros.net/viewtopic.php?f=7&t=23328&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2720,EXTRACT & EVAL in Javascript,"I can only put the first part of the code as it consists of more than 1200 lines.
Code: Select allvar code = ""CODE:"";
                code += ""SET !ERRORIGNORE YES\n"";
		code += ""SET !TIMEOUT_PAGE 9\n"";
	        code += ""SET !EXTRACT_TEST_POPUP NO\n"";
	        code += ""URL GOTO=www.addmefastbots.site11.com\n"";
		code += ""TAG POS=1 TYPE=H1 ATTR=* EXTRACT=TXT\n"";
		code += ""SET !VAR1 EVAL(""var s= \""{{!EXTRACT}}\""; var d=s.replace(\""ml\"",\""\""); var e=d.replace(\""n\"",\""\""); var a=e.replace(\""e\"",\""\""); var c=a.replace(\""f\"",\""f.\"");"")\n"";
		code += ""SET !EXTRACT NULL\n"";
		code += ""TAG POS=1 TYPE=IMG ATTR=SRC:*head.png EXTRACT=ALT\n"";
		code += ""SET !VAR2 EVAL(""var s= \""{{!EXTRACT}}\""; var d= s.replace(\""e\"",\""\""); var h= d.replace(\""my\"",\""y/\"");"")\n"";
		code += ""SET !EXTRACT NULL\n"";
		code += ""TAG POS=1 TYPE=IMG ATTR=SRC:*pixel.gif EXTRACT=ALT\n"";
		code += ""SET !VAR3 EVAL(""var s= \""{{!EXTRACT}}\""; var d= s.replace(\""P\"",\""\""); var n= d.replace(\"" \"",\""\"");"")\n"";
		code += ""SET !EXTRACT NULL\n"";
		code += ""WAIT SECONDS=1\n"";
		code += ""ADD !VAR1 {{!VAR2}}{{!VAR3}}\n"";
		code += ""WAIT SECONDS=1\n"";
		code += ""URL GOTO={{!VAR1}}\n"";
		code += ""WAIT SECONDS=7\n"";
		code += ""TAG POS=1 TYPE=IMG ATTR=ID:skip_ad_button\n"";
		code += ""TAG POS=1 TYPE=INPUT:SUBMIT ATTR=VALUE:Me<SP>gusta\n"";
		code += ""TAG POS=1 TYPE=BUTTON ATTR=ID:u_*_*\n"";
		code += ""WAIT SECONDS=2\n"";
		code += ""TAB CLOSE\n"";	
	iimPlay(code);
	}


Browser: Firefox (Latest Version)
OS: Windows 7

",https://forum.imacros.net/viewtopic.php?f=7&t=23317&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2721,Repeat a path,"Hello, 

I have a certain imacro (""change"") to stract informatio from the site ""X"", and I need to use this same macro to other sites, such as ""Y"", ""Z""... Is there a way, imagining the hole thing in only one script (imacro), setting the original imacro (""change"") as some kind of variable that is repeated to each different site?

Thank you",https://forum.imacros.net/viewtopic.php?f=7&t=23316&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2722,extract data from varying page templates,"https://www.oxfordshire.gov.uk/cms/schools/list/all

hi I'm trying to extract school data from above link

the first school listed doesn't have fax no on its details page whereas the 3rd school listed does have it

this is a varying template. how do i handle this?

pls help",https://forum.imacros.net/viewtopic.php?f=7&t=23292&sid=b07178a8376ad918bdc99d345a2f0b97,AI
2726,copy selected text,"HI all 

How to copy ONLY selected Text 

example
Code: Select allGood day,

Somebody, probably you, signed up xxxxxxxxxxxxm at xxxxxxxxx to receive free digital currency coins every day.

This happened on 2014-08-19 20:08:50 UTC from IP address xxxxxxxx

If this was you, please verify your email address by clicking on the link below and follow 
the instructions on the screen to set a secure password. 

/activate

Please enter the following activation Code: 192f6a8bc7df95b260b3a3082b1c5334

If you did not sign-up, please let us know immediately so that we can cancel the registration.

Sincerely,

The  Team



How to extract Activity code??  ""192f6a8bc7df95b260b3a3082b1c5334""",https://forum.imacros.net/viewtopic.php?f=7&t=23261&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2727,How to extract the downloadable CSV-files from Google Trends,"Dear Community,

I am new to iMacro and did not find any helpful topics existing so far that could help me with my problem: For a research project we need to extract the CSV-Files that are available at Google Trends (www.google.com/trends/explore) for each combination of country and search term about a hundred times. I recorded the downloading process (you need to be logged in to gmail to download the CSV-files) with iMacros and chose the option ""open file"" (otherwise it will just rewrite the file every single time, because every CSV-file has the same name and I do not know how to rename it during iMacro's downloading process). However, when I replay the Macro I recorded, nothing happens. The macro gets stuck whenever it arrives at the first ONDOWNLOAD command. The error name is the following: ""RuntimeError: ONDOWNLOAD command was used but no download occurred. (Error code: -804)"".

Does anyone know how it is possible to download the CSV-files from the Google Trends website and then either open or save it?

Thanks a lot in advance for your help
Sarah",https://forum.imacros.net/viewtopic.php?f=7&t=22527&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2728,Web data extraction into Excel,"Hello folks!
How are you?

First of all, these are my configs:

1. What version of iMacros are you using
 iMacros Firefox 8.8.2 

2. What operating system are you using? (please also specify language)
Windows 7 Professional, Service Pack 1 - Portuguese (Brazil)

3. Which browser(s) are you using? (include version numbers)
Firefox 30.0

Well, I am new iMacros user, so I don't know much more than simple tasks, which iMacro offers. What I need to do, is run a macro to extract the price value of all airplane tickets from a research in a specific website and paste into excel.
Let me explain better, this is the website which I need to extract values: (It is located at the right column)

http://www.decolar.com/shop/flights/res ... 8-31/1/0/0

Then I need to paste these values into an excel column.

It is possible to do this? Beucase there will be days, which has 10 values, and days which has 7 values.
Thanks for helping.",https://forum.imacros.net/viewtopic.php?f=7&t=23143&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2729,ONDOWNLOAD extract url instead of download file,"I meet an interesting problem in which I want to extract the download url instead of download it with ONDOWNLOAD. 
More specifically, when I submit a form, it returns a download url, for example http://domain.com/file.zip, and I want to grab the url (extract url) instead of downloading the file. How can I do it?
Code: Select allTAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:F1 ATTR=ID:btn_download
' Now need to Extract the returned URL (not appearing in address bar as it's in zip type)


Thank you.",https://forum.imacros.net/viewtopic.php?f=7&t=23161&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2731,How to export email from tripadvisor,"Hi guys, I'm trying to find the solution for over four days and I've already tried everything I can. I lerned quite a few tricks on the way, btw 
Right, my issue: 
I'm trying to get an email address from this site:
Code: Select allhttp://www.tripadvisor.com/Hotel_Review-g35805-d87654-Reviews-The_Talbott_Hotel-Chicago_Illinois.html
The thing is I even can't get iMacros to click on the ""E-mail hotel"". Tried recording but the imacros show error when I tried to play it. Also tried to get it from the source code but I can't find the email in the code unless I click on the  ""E-mail hotel"". Could you please show me the way here?

Thanks for this forum as I found soo much help here already, especially thanks to chivracq.

Using iMacros for Firefox 8.8.2 on Linux Mint 16 (Petra)",https://forum.imacros.net/viewtopic.php?f=7&t=23130&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2732,Extracting only part of the text.,"Conf:
Firefox 30.0, iMacros for firefox 8.8.2, Windows 7 Professional SP1 x64

Hello everyone.

I'm trying to extract only part of the text.
This is an example-
This is my story:""We had a white house and I wanted to paint it black but he didn't allow it.""
His side!""I wanted a blue house.""

I want to extract ony the parts in bold.

We had a white house and I wanted to paint it black but he didn't allow it
I wanted a blue house


I was searching around and notice that everyone is mentioning EVAL.
And the only thing I came up until now is this:
Code: Select allSET !VAR1 EVAL(""var s=\""{{!EXTRACT}}\""; s.replace(\"".""\"",\""\"");"")

And I only get with that:
This is my story:""We had a white house and I wanted to paint it black but he didn't allow it

His side!""I wanted a blue house

How to remove:
This is my story:""
His side!""

???",https://forum.imacros.net/viewtopic.php?f=7&t=23101&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2734,LOOP to Next Loop when error is reached?,"hello...

i am relatively new to imacros and looking to scrape some data. i have a working script but would like to make one modification to it. the basic idea of the script is that it scrapes data from the 10 results from a search page on a site, then proceeds to page two to of the results. i am using the {{!LOOP}} command to go through a list of URLs i would like to scrape. however, some search results only have 1 or two pages of results, while others have 20+ pages. 

the way the current script is written, when the tag is no longer found on the page it continues to run when i would like it to stop for the current loop and move on to the next.

so i really have a two part question.

1. is there a way to tell the script to go to the next LOOP when an error is reached? if so, how?
2. what is the best way to scrape through multiple pages of search results? right now (as you will see in the code below) i simply copy the code over again and paste it below so it runs again on page 2 of the search results. surely, there has to be a better way to do this? (i am by no means a coder, just hacked some stuff together to get it to sort of work how i want it to - so any help is appreciated!)
Code: Select allVERSION BUILD=8820413 RECORDER=FX
TAB T=1
TAB CLOSEALLOTHERS
SET !ERRORIGNORE YES
SET !TIMEOUT_STEP 0
SET !DATASOURCE urls.csv
SET !DATASOURCE_COLUMNS 1
SET !DATASOURCE_LINE {{!LOOP}}
SET !EXTRACT_TEST_POPUP NO

URL GOTO={{!COL1}}

TAG POS=1 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=2 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=3 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=4 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=5 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=6 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=7 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=8 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=9 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=10 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

set !extract null

SET !VAR1 EVAL(""var randomNumber=Math.floor(Math.random()*3 + 10); randomNumber;"") 
WAIT SECONDS={{!VAR1}}

TAG POS=1 TYPE=A ATTR=TXT:Next

TAG POS=1 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=2 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=3 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=4 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=5 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=6 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=7 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=8 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=9 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

TAG POS=10 TYPE=A ATTR=CLASS:pull-left EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\Desktop\ FILE=test.csv

set !extract null

SET !VAR1 EVAL(""var randomNumber=Math.floor(Math.random()*3 + 10); randomNumber;"") 
WAIT SECONDS={{!VAR1}}

TAG POS=1 TYPE=A ATTR=TXT:Next


i am using windows 7, firefox 29.0.1,",https://forum.imacros.net/viewtopic.php?f=7&t=23006&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2735,Extract specific URLs,"Hello everyone, I hope you're doing well and thanks for reading my post. I'm not a total newbie to iMacros,and I've created a couple of simple imacro scripts in the past. Anyway now I face a task which I find beyond the scope of my abilities. I need to scrape urls from a certain page, but I only need to scrape the ones which have not been opened before. So the ones which are not present in the history, or the ones which are blue in color and not purple. 

The first way is very difficult, as many guys I've read say that you cannot browse your history with a script because of a privacy issue. So now I've turnt my head to the second solution. I need to find a way to firstly extract the color from the text and then to decide if it should be extracted or not. 

Any ideas or suggestions?

Thanks in advance, and kind regards.",https://forum.imacros.net/viewtopic.php?f=7&t=22945&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2738,Data Extraction and Use of Tabs [HELP] Firefox,"My first problem is that I use imacro to fill out forms on websites.
However, this specific website however obviously saves cookies and if I try fill out the form twice it'll tell me that I have already done it and I have to come back tomorrow, however if I use a private window I can do it as many times as I want as long as I manually close the window open a new private window and then start the macro again..

I was wondering isit possible to somehow get this macro to loop without having to always babysit it..
I have seen other people do this with the new skittles thing but they wont spill the beans on how they do it.. This guy shows most of his code if you want to take a look at it: https://www.youtube.com/watch?v=UNAJ-BU3dt0 (its the tab on the left I'm interested in)

Another problem I have is that is there anyway of extracting changing data from a yahoo email with a specific subject and then paste that data somewhere else?

Thank you for your help in advance!",https://forum.imacros.net/viewtopic.php?f=7&t=22953&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2739,驴Extract is not supported ?,"UnsupportedCommand: command EXTRACT is not supported in the current version, line 18 (Error code: -912)

I want to download all elements in the table and save in a csv format.

How can i do that ? I have to enter all commands like ""TAG POS=1 TYPE=IMG ATTR=SRC:*shark_thumbnail.jpg EXTRACT=ALT""   ?

I use Firefox 29 and the last version of imacros.

Thanks!

PD: I want to download this table http://zone-h.org/archive/special=1",https://forum.imacros.net/viewtopic.php?f=7&t=22931&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2740,How to Extract Specified Text,"i need u Extract an e-mail id while i am extracting the extracted text are ""Untitled document testmail@gmail.comtestmail@gmail.com"" how to extract only an one email id example  testmail@gmail.com someone give me the code for extracting only one email id...",https://forum.imacros.net/viewtopic.php?f=7&t=22903&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2745,Data Extraction to CSV Help,"Hi, 

I am new to iMacros and have been trying to figure out how to scrape the names, image source and links from this page to a CSV: http://app.traackr.com/campaigns/view/3957

This is what I currently have and its not working as intended. Any help would be much appreciated
Code: Select allVERSION BUILD=8031994
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO=http://app.traackr.com/campaigns/view/3957
TAG POS=1 TYPE=A ATTR=TXT EXTRACT=TITLE 
TAG POS=1 TYPE=A ATTR=TXT:H*links* EXTRACT=Title-Link 
TAG POS=1 TYPE=IMG ATTR=SRC:*shark_thumbnail.jpg EXTRACT=ALT    

TAG POS=2 TYPE=A ATTR=TXT EXTRACT=TITLE 
TAG POS=2 TYPE=A ATTR=TXT:H*links* EXTRACT=Title-Link 
TAG POS=2 TYPE=IMG ATTR=SRC:*shark_thumbnail.jpg EXTRACT=ALT   

TAG POS=3 TYPE=A ATTR=TXT EXTRACT=TITLE 
TAG POS=3 TYPE=A ATTR=TXT:H*links* EXTRACT=Title-Link 
TAG POS=3 TYPE=IMG ATTR=SRC:*shark_thumbnail.jpg EXTRACT=ALT   

TAG POS=4 TYPE=A ATTR=TXT EXTRACT=TITLE 
TAG POS=4 TYPE=A ATTR=TXT:H*links* EXTRACT=Title-Link 
TAG POS=4 TYPE=IMG ATTR=SRC:*shark_thumbnail.jpg EXTRACT=ALT   

TAG POS=5 TYPE=A ATTR=TXT EXTRACT=TITLE 
TAG POS=5 TYPE=A ATTR=TXT:H*links* EXTRACT=Title-Link 
TAG POS=5 TYPE=IMG ATTR=SRC:*shark_thumbnail.jpg EXTRACT=ALT   

'
'Relative extraction
'
'1. Mark reference (anchor) element
TAG POS=1 TYPE=TH ATTR=TXT:MyTable
'2. POS value is RELATIVE to the anchor element   
TAG POS=R3 TYPE=TD ATTR=TXT:* EXTRACT=TXT 


This is what I would like the first row to look like

User full name, link to user profile, user img source url",https://forum.imacros.net/viewtopic.php?f=7&t=22774&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2746,Need help extract correct data,"Hi.
I try to use imacros since a few days.
a few things has worked, but the most not   
Now i have a new problem. i think is it very simple, but i cant find a solution.
can anyone help me please?


I have the following source code:
Code: Select all<h1 style=""outline: 1px solid blue;"" class=""artikeldetail"">NHL 14 Ankauf</h1> (Spiele / Games - Xbox 360 Spiele - Sport)<br><strong>EAN: </strong>5035228111103<br><br><form action=""ankauf_warenkorb.html"" method=""post"">
  

I want to extract only the number: 5035228111103, and nothing else.
The only i have reached is to extract ""NHL 14 Ankauf"" with
TAG POS=1 TYPE=h1 ATTR=CLASS:artikeldetail EXTRACT=TXT",https://forum.imacros.net/viewtopic.php?f=7&t=22729&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2748,Follow two different paths based on condition,"I am using enterprise edition and using firefox 28.0 and am familiar with VB and java scripting. I am extracting some values from webpage and after that I want to compare the values from my database cells to extracted values if it passes I want IIM to follow certain links and if fails I want IIM to follow some other links. How do I place such condition?
2ndly, I am confuse because, in IIM file, everything goes by line number; How do I define paths based on conditions? How do I tell Macro to executes this if pass and execute that if fails. I will really appreciate if any of you may give some general examples.
Thanks",https://forum.imacros.net/viewtopic.php?f=7&t=22710&sid=7e74df639d022ccbaa0e92d884edcf30,AI
2751,Having Major Trouble Scraping Tables,"I am trying to rip table data from this page:

http://gametz.com/?A=MyItems&desire=wan ... mes&Next=0

I want the table of items to be extracted as either HTML or CSV, and to retain the following information in each row:
 1) ""del"" button link from second column
 2) Name of Item
 3) Platform

I have tried this many different ways - extracting the table, extracting rows, etc.  The closest I can get is a CSV file which includes the table with all the data.  However, the ""del"" link (and other links in that cell) gets extracted as plaintext and not as a link.  I have tried extracting to CSV and HTML.  When I extract to HTML the output HTML file includes everything above the table, and not just the table.

I have tried this using the Text Extraction Wizard from the demo of Pro, and despite using different outputs, none of them have given me the results needed.  Scripts are run under iMacros for Firefox 8.8.1.

Any help here is much appreciated.",https://forum.imacros.net/viewtopic.php?f=7&t=22688&sid=764a56e3c04defae9b390be083f6af7b,AI
2752,How to scrape url in this website ?,"Hi,

I want to scrape url of all video in this link : http://www.dailymotion.com/us/relevance ... /english/1. I tried many way but all failed.
So, please help me. Thank you very much !

This is my code:
Code: Select allVERSION BUILD=8810214 RECORDER=FX
TAB T=1
FILTER TYPE=IMAGES STATUS=ON
SET !EXTRACT_TEST_POPUP NO
SET !ERRORIGNORE YES
URL GOTO=http://www.dailymotion.com/us/relevance/search/english/1

TAG POS={{!LOOP}} TYPE=DIV ATTR=CLASS:psprite pmode8
TAG POS=R1 TYPE=A ATTR=TXT:* EXTRACT=HREF
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\ngong\Documents\iMacros FILE=Ollytest1.csv
",https://forum.imacros.net/viewtopic.php?f=7&t=22553&sid=764a56e3c04defae9b390be083f6af7b,AI
2753,Problem when extracting text,"Hi,

I am trying to create a macro to search a list of web pages and evaluate the text that is on them.  If a certain string of text is NOT on a given page, I need the macro to make a dinging sound.  I've been successful in getting this to work, but only for one page.  When the macro goes to the second page, and the third page, and so on, it always makes the dinging sound, whether the string of text is on the page or not.  Included below is the macro itself:

-------------------------------

VERSION BUILD=8601111 RECORDER=FX
TAB T=1
SET !ERRORIGNORE YES
SET !EXTRACT_TEST_POPUP YES

URL GOTO=http://shop2.gzanders.com/mb-930-jm-pro ... hetic.html
TAG POS=7 TYPE=SPAN FORM=ACTION:http://shop2.gzanders.com/checkout/cart ... YW5kZXJzLm* ATTR=* EXTRACT=TXT

SET !VAR1 EVAL(""if (\""{{!EXTRACT}}\"" == \""Out of stock\"") {var s = \""\"";} else {var s = \""C:\\\Windows\\\Media\\\chimes.wav\"";} s;"") 
'PROMPT !EXTRACT:<SP>{{!EXTRACT}}<br>!VAR1:<SP>{{!VAR1}}

URL GOTO=file://{{!VAR1}}

URL GOTO=http://shop2.gzanders.com/mb-930-jm-pro ... synth.html
TAG POS=7 TYPE=SPAN FORM=ACTION:http://shop2.gzanders.com/checkout/cart ... YW5kZXJzLm* ATTR=* EXTRACT=TXT

SET !VAR1 EVAL(""if (\""{{!EXTRACT}}\"" == \""Out of stock\"") {var s = \""\"";} else {var s = \""C:\\\Windows\\\Media\\\chimes.wav\"";} s;"") 
'PROMPT !EXTRACT:<SP>{{!EXTRACT}}<br>!VAR1:<SP>{{!VAR1}}

URL GOTO=file://{{!VAR1}}

---------------------

Why is it making a dinging sound on the second page (the last line of the macro) even though the text that is being extracted says ""Out of stock""?  How can I make it not do this?  I want it to make the dinging sound ONLY if it does NOT say ""Out of Stock"", and I want it to do this with a long list of web pages.  

This was tested on iMacros for Firefox (not sure how to find out which version), Firefox 27.0.1, Windows 8.  Any help would be greatly appreciated.  Thanks in advance.",https://forum.imacros.net/viewtopic.php?f=7&t=22650&sid=764a56e3c04defae9b390be083f6af7b,AI
2755,Wow...Nice challenge ahead. Need help scraping a site,"Hi,

I try to scape this site: https://www.duval.realtaxdeed.com/index ... 04/09/2014
I need the property information and the URL for each to be auctioned property.
I've succesfully created a macro to scrape one property and thought about making a loop for the others (see also http://forum.imacros.net/_uploads/extract ... action.htm and then 'press next' (still need to figure out that one).

The challenge:
The second property has another POS numbers EXCEPT for the URL. That has the same POS number as the first property: 1.

I'm playing using the trail-version and have a couple of weeks left. 
Also need to know if I need the 'extended' version or could do these kind of things with the basic version.

Would love to hear some pointers for this challenge 

Thanks!",https://forum.imacros.net/viewtopic.php?f=7&t=22641&sid=764a56e3c04defae9b390be083f6af7b,AI
2756,Finding with specific filters on web-page with iMacro,"Hello. I wonder if it's possible with iMacro to write into code a specific finding filters to make script, which will allow to find certain text on web-page with some criteria such as digit count and starting characters? For example I need to find all phrases which have 7 digits and start with ""14"", which also have it last digit equal to 8 on a web-page. I'd be nice if it can be copied at least into clipboard so that I could use it in further ways. Thanks in advance for any assistance.",https://forum.imacros.net/viewtopic.php?f=7&t=22623&sid=764a56e3c04defae9b390be083f6af7b,AI
2757,Extract Linkedin User ID,"Hi there, 

I've using a bot to send messages to linkedin group. Unfortunally, the bot doenst save the 1st users round of messafes, so i need to recover the users id who i send messages to avoid repeat send them the message again.

Does anyone have a script to extract user id from the users page?

Kind regards

Alexis",https://forum.imacros.net/viewtopic.php?f=7&t=22531&sid=764a56e3c04defae9b390be083f6af7b,AI
2758,How to open extracted table?,"VERSION BUILD=8031994
TAB T=1
TAB CLOSEALLOTHERS
URL GOTO =https://sellercentral.amazon.com/myi/se ... geOffset=0
WAIT SECONDS=10
TAG POS=1 TYPE=TABLE ATTR=CLASS:data-display<SP>manageTable&&TXT:* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=mytable_{{!NOW:yymmdd_hhnnss}}.csv
WAIT SECONDS=3

I am using this to extract a table,  It outputs a csv file that looks like this http://i.gyazo.com/dd306cf3666e063ef419b009e604b11d.png

for the life of me i cannot get openoffice calc to open this, Ive tried many combination of delimiters, 

did  I save it wrong? is there a way to specify csv it appears to be putting new line characters in, but i don't think openoffice or even excel can use a \n as a delimiter .",https://forum.imacros.net/viewtopic.php?f=7&t=22626&sid=764a56e3c04defae9b390be083f6af7b,AI
2760,iMacro painfully slow on Internet Explorer,"Hi,
i use imacro to extract links from wikipedia. The exactly same skript runs under Firefox in less the one minute but the IE (Enterise and free) version needs more than 15 Minutes (i killed it, was not finished).
I use Firefox 27 und IE 10 under windows 7 with the latest version of Imacro.
Code: Select allVERSION BUILD=8300326 RECORDER=FX
TAB T=1
TAB CLOSEALLOTHERS 
SET !TIMEOUT_PAGE 60
SET !TIMEOUT_STEP 0
SET !EXTRACT_TEST_POPUP NO
SET !REPLAYSPEED FAST
URL GOTO=http://www.jedipedia.de/wiki/Spezial:Letzte_脛nderungen?days=3&limit=600
WAIT SECONDS=20

TAG POS=1 TYPE=A ATTR=HREF:*/wiki/*curid*action=history*&&TXT:Versionen EXTRACT=TITLE
.........
TAG POS=600 TYPE=A ATTR=HREF:*/wiki/*curid*action=history*&&TXT:Versionen EXTRACT=TITLE

SAVEAS TYPE=EXTRACT FOLDER=d:\pub\WikiVX\Backups\jedipedia\ FILE=jedipedia.csv",https://forum.imacros.net/viewtopic.php?f=7&t=22587&sid=764a56e3c04defae9b390be083f6af7b,AI
2761,"Open Google Search results, extract text","Hi,

I think what I want to do is pretty simple, but I have never used iMacros for data extraction and am having some trouble. Hopefully I can explain it clearly.

What I want to do is:

1) start on a page with Google search results open
2) Click the first result
3) Copy the URL
4) Paste the URL into a CSV cell
5) Copy the text on the page
6) Paste the text into a CSV cell BESIDE THE URL CELL
7) Close the browser tab (go back to TAB 1 search results)
 Click the next result
9) Repeat steps 3-8 for all 10 results on the first search page
10) Go to the second page of results
11) Repeat entire process for this page

...and so on. So basically this macro would open every search result and save all of the text, until I stop it or there are no more results. I'm sure someone has done this, but I searched and nothing looked like what I want. Some looked 

I tried doing this myself, and I could not even get as far as opening the first search result. When I just click the result, it uses the code ATTR=TXT:[NAME OF SEARCH RESULT]. This will not work, I want it to click on the first search result no matter what the page title is, so that I can use the macro on every page.

If someone could walk me through this whole process I'd really appreciate it! I've watched a few tutorials here and there and read some threads, but most of it is addressed to someone who has some basic knowledge, and I have barely any. 

Please assume the latest version of Firefox and the latest version of iMacros - I am not currently at my home computer so I don't know the version number, but I'll install whatever version you know how to use if it means I can figure this out.",https://forum.imacros.net/viewtopic.php?f=7&t=22547&sid=764a56e3c04defae9b390be083f6af7b,AI
2762,How to loop macros with Javascript,"Hi, please help me, I want to do looping with my macros. I want to paste different data in my CSV file to a webpage. Let me explain what I want to do in codes:

I have 2 CSV files. 
First let's call it ""name.csv"":
Sam
David
Annie

Second, let's call it ""Country.csv"":
America
Indonesia
Canada

First macro has the ""DATASOURCE"" name.csv. Let's call this macro ""PasteData.iim""
Second macro has the ""DATASOURCE"" Country.csv. Let's call this macro ""AnotherPasteData.iim""

Here's what I want to do:
Code: Select alliimPlay (""LoginA.iim""); // I have an account so I have to login first
iimPlay (""PasteData.iim""); // This one should paste ""Sam"" (Here's the question, how do I do this?)
iimPlay (""AnotherPasteData.iim""); // This one should paste ""America"" (Here's the question, how do I do this?)

iimPlay (""LoginB.iim"");
iimPlay (""PasteData.iim""); // This one should paste ""David"" (Here's the question, how do I do this?)
iimPlay (""AnotherPasteData.iim""); // This one should paste ""Indonesia"" (Here's the question, how do I do this?)

iimPlay (""LoginC.iim"");
iimPlay (""PasteData.iim""); // This one should paste ""Annie"" (Here's the question, how do I do this?)
iimPlay (""AnotherPasteData.iim""); // This one should paste ""Canada"" (Here's the question, how do I do this?)


I can just write it like that all the way down but the macros will be too many to create. So if possible is there a simpler way to do that?",https://forum.imacros.net/viewtopic.php?f=7&t=22535&sid=764a56e3c04defae9b390be083f6af7b,AI
2763,extract macro in javascript problem,"I have following macro: TAG POS=1 TYPE=H2 ATTR=* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\....\ ....\ ....\ ....\ FILE=dane.csv鈥?鈥?
And works well

I鈥檇 like to put it in javascript : as below鈥?鈥?
readin = ""TAG POS=1 TYPE=H2 ATTR=* EXTRACT=TXT"";
readin +=""SAVEAS TYPE=EXTRACT FOLDER= C:\....\ ....\ ....\ ....\  FILE=dane.csv""
iimPlay (readin);鈥?adn I get an error:

[Exception... ""Component returned failure code: 0x80520001 (NS_ERROR_FILE_UNRECOGNIZED_PATH) [nsILocalFile.initWithPath]"" nsresult: ""0x80520001 (NS_ERROR_FILE_UNRECOGNIZED_PATH)"" location: ""JS frame :: resource://imacros/utils.js :: imns.FIO.openNode :: line 203"" data: no], line 199 (Error code: -991)鈥?
I don鈥檛 know why ? Could somebody help me ?",https://forum.imacros.net/viewtopic.php?f=7&t=22534&sid=764a56e3c04defae9b390be083f6af7b,AI
2764,Extract the data from javabsed webpage,"Hi there,

I want to extract Detail, location, facilities in csv file through imacro. But it JavaScript based. And i am unable to extract it. Please let me know how to create code for it. For manually it consume so much time.

http://www.avanta.co.uk/UK/offices/serv ... tin-friars

Regards,
James.",https://forum.imacros.net/viewtopic.php?f=7&t=22496&sid=764a56e3c04defae9b390be083f6af7b,AI
2765,copy data from website into excel file,"I'm new here and I have a question. 
I will try to describe the steps I want to make: 
1 - open this url - http://www.amazon.com
2 - write in search ""table"" 
3 - copy the entire page 
4 - Open a blank Excel file and paste what I copied into 
5 - go back to Amazon and go to page 2 
6 - copy the entire page again 
7 - paste what I copied after the previous piece in Excel 
8 - go to the third and so on until it reaches the last page 
Actually I want to create Excel Data Base from all pages of the Amazon search - ""table""

tnx",https://forum.imacros.net/viewtopic.php?f=7&t=22486&sid=764a56e3c04defae9b390be083f6af7b,AI
2766,Linkedin name scraping,"Hi,

I have created a macro to scrape peoples names from their Linkedin Profiles,  However when I scrape said names it is saved as their full name ex: ""John Doe"" I'm trying to figure out a way to just get either their first or last name.  Either ""John"" or ""Doe"".  I have a feeling that I could use the EVAL statement but i'm totally stumped.  Can you please help me out.   Thank you.


Code: Select allVERSION BUILD=9002379
TAB T={{!LOOP}}
SET !EXTRACT_TEST_POPUP NO
TAG POS=1 TYPE=SPAN ATTR=CLASS:full-name EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=names.csv",https://forum.imacros.net/viewtopic.php?f=7&t=22479&sid=764a56e3c04defae9b390be083f6af7b,AI
2767,Extract table from website,"Hi.

I'm using iMacros for chrome and i'm having some troubles extracting a table from the website.

For extracting the table, I've followed the demo ExtractTable.iim

I want to extract all the data from an website (many entries) and I'm failing to do so right at the first step.

I have to go to http://www.bedca.net/bdpub/index.php and at the top right corner, select Consulta and then click on Lista alfabetica and finally, at the letters, click on Todos. This will give me all the items that I have to extract the data. I want to click one item at a time and then, on the loaded page, extract the 3 table to a file.

This is what I'm doing for the first item but it's not doing nothing.
Code: Select allVERSION BUILD=8300326 RECORDER=FX
TAB T=1
URL GOTO=http://www.bedca.net/bdpub/index.php
TAG POS=1 TYPE=A ATTR=TXT:Consulta
WAIT SECONDS=3
TAG POS=1 TYPE=SPAN ATTR=TXT:Lista<SP>alfab茅tica
WAIT SECONDS=3
TAG POS=1 TYPE=A ATTR=TXT:Todos
WAIT SECONDS=3
TAG POS=1 TYPE=A ATTR=TXT:Aceite<SP>de<SP>algod贸n
WAIT SECONDS=3
TAG POS=3 TYPE=TABLE ATTR=TXT:* EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=* FILE=mytable_{{!NOW:yymmdd_hhnnss}}.csv


Im managing to open the item but the table it's not being created.

Can somebody point me in the right direction?

Regards,

kodiak",https://forum.imacros.net/viewtopic.php?f=7&t=21476&sid=764a56e3c04defae9b390be083f6af7b,AI
2768,How to paste a text into a text box,"Hello, I want to paste something into a text box but I can't figure out how to do that with imacros, 

Here is the source code of the text box I want to paste to:
save it in html
Code: Select all<br />
	<script type=""text/javascript"" src=""clientscript/vbulletin_quick_reply.js?v=387""></script>
	<form action=""newreply.php?do=postreply&t=689925"" method=""post"" name=""vbform"" onsubmit=""return qr_prepare_submit(this, 10);"" id=""qrform"">
	<table class=""tborder"" cellpadding=""6"" cellspacing=""1"" border=""0"" width=""100%"" align=""center"">
	<thead>
		<tr>
			<td class=""tcat"" colspan=""2"">
				<a style=""float:right"" href=""#top"" onclick=""return toggle_collapse('quickreply');""><img id=""collapseimg_quickreply"" src=""images/bluefox/buttons/collapse_tcat.gif"" alt="""" border=""0"" /></a>
				Quick Reply
			</td>
		</tr>
	</thead>
	<tbody id=""qr_error_tbody"" style=""display:none"">
	<tr>
		<td class=""thead"">The following errors occurred with your submission</td>
	</tr>
	<tr>
		<td class=""alt1"" id=""qr_error_td""></td>
	</tr>
	<tr>
		<td class=""tfoot"" align=""center""><span class=""smallfont""><a href=""#"" onclick=""return qr_hide_errors()"">Okay</a></span></td>
	</tr>
	</tbody>
	<tbody id=""collapseobj_quickreply"" style="""">
	<tr>
		<td class=""panelsurround"" align=""center"">
			<div class=""panel"">
				<div align=""left"" style=""width:640px"">
					<div class=""smallfont"">Message:</div>
					
		<script type=""text/javascript"">
		<!--
			var threaded_mode = 0;
			var require_click = 0;
			var is_last_page = 1; // leave for people with cached JS files
			var allow_ajax_qr = 1;
			var ajax_last_post = 1391172996;
		// -->
		</script>
		<table cellpadding=""0"" cellspacing=""0"" border=""0"" style=""width: 100%"">
<tr>
	<td id=""vB_Editor_QR"" class=""vBulletin_editor"" width=""100%"">
		
		<div id=""vB_Editor_QR_controls"" class=""controlbar"">
			<table cellpadding=""0"" cellspacing=""0"" border=""0"">
				<tr>
					<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_removeformat""><img src=""images/bluefox/editor/removeformat.gif"" width=""21"" height=""20"" alt=""Remove Text Formatting"" /></div></td>
					<td><img src=""images/bluefox/editor/separator.gif"" width=""6"" height=""20"" alt="""" /></td>
					
						<td>
							<div class=""imagebutton"" id=""vB_Editor_QR_popup_fontname"" title=""Fonts"">
								<table cellpadding=""0"" cellspacing=""0"" border=""0"">
									<tr>
										<td class=""popup_feedback""><div id=""vB_Editor_QR_font_out"" style=""width:91px"">&nbsp;</div></td>
										<td class=""popup_pickbutton""><img src=""images/bluefox/editor/menupop.gif"" width=""11"" height=""16"" alt="""" /></td>
									</tr>
								</table>
							</div>
						</td>
					
					
						<td>
							<div class=""imagebutton"" id=""vB_Editor_QR_popup_fontsize"" title=""Sizes"">
								<table cellpadding=""0"" cellspacing=""0"" border=""0"">
									<tr>
										<td class=""popup_feedback""><div id=""vB_Editor_QR_size_out"" style=""width:25px"">&nbsp;</div></td>
										<td class=""popup_pickbutton""><img src=""images/bluefox/editor/menupop.gif"" width=""11"" height=""16"" alt="""" /></td>
									</tr>
								</table>
							</div>
						</td>
					
					
						<td><img src=""images/bluefox/editor/separator.gif"" width=""6"" height=""20"" alt="""" /></td>
						<td>
							<div class=""imagebutton"" id=""vB_Editor_QR_popup_forecolor"" title=""Colors"">
								<table cellpadding=""0"" cellspacing=""0"" border=""0"">
									<tr>
										<td id=""vB_Editor_QR_color_out""><img src=""images/bluefox/editor/color.gif"" width=""21"" height=""16"" alt="""" /><br /><img src=""clear.gif"" id=""vB_Editor_QR_color_bar"" alt="""" style=""background-color:black"" width=""21"" height=""4"" /></td>
										<td class=""alt_pickbutton""><img src=""images/bluefox/editor/menupop.gif"" width=""11"" height=""16"" alt="""" /></td>
									</tr>
								</table>
							</div>
						</td>
					
					
						<td><img src=""images/bluefox/editor/separator.gif"" width=""6"" height=""20"" alt="""" /></td>
						<td>
							<div class=""imagebutton"" id=""vB_Editor_QR_popup_smilie"" title=""Smilies"">
								<table cellpadding=""0"" cellspacing=""0"" border=""0"">
									<tr>
										<td><img src=""images/bluefox/editor/smilie.gif"" alt="""" width=""21"" height=""20"" /></td>
										<td class=""alt_pickbutton""><img src=""images/bluefox/editor/menupop.gif"" width=""11"" height=""16"" alt="""" /></td>
									</tr>
								</table>
							</div>
						</td>
					

					<td><img src=""images/bluefox/editor/separator.gif"" width=""6"" height=""20"" alt="""" /></td>
					<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_cut""><img src=""images/bluefox/editor/cut.gif"" width=""21"" height=""20"" alt=""Cut"" /></div></td>
					<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_copy""><img src=""images/bluefox/editor/copy.gif"" width=""21"" height=""20"" alt=""Copy"" /></div></td>
					<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_paste""><img src=""images/bluefox/editor/paste.gif"" width=""21"" height=""20"" alt=""Paste"" /></div></td>

					<td><img src=""images/bluefox/editor/separator.gif"" width=""6"" height=""20"" alt="""" /></td>
					<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_undo""><img src=""images/bluefox/editor/undo.gif"" width=""21"" height=""20"" alt=""Undo"" /></div></td>
					<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_redo""><img src=""images/bluefox/editor/redo.gif"" width=""21"" height=""20"" alt=""Redo"" /></div></td>

					<td width=""100%"">&nbsp;</td>
					
					<td>
						<div class=""imagebutton"" id=""vB_Editor_QR_cmd_resize_0_99""><img src=""images/bluefox/editor/resize_0.gif"" width=""21"" height=""9"" alt=""Decrease Size"" /></div>
						<div class=""imagebutton"" id=""vB_Editor_QR_cmd_resize_1_99""><img src=""images/bluefox/editor/resize_1.gif"" width=""21"" height=""9"" alt=""Increase Size"" /></div>
					</td>
					
				</tr>
			</table>

			<table cellpadding=""0"" cellspacing=""0"" border=""0"">
				<tr>
					
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_bold""><img src=""images/bluefox/editor/bold.gif"" width=""21"" height=""20"" alt=""Bold"" /></div></td>
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_italic""><img src=""images/bluefox/editor/italic.gif"" width=""21"" height=""20"" alt=""Italic"" /></div></td>
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_underline""><img src=""images/bluefox/editor/underline.gif"" width=""21"" height=""20"" alt=""Underline"" /></div></td>
						<td><img src=""images/bluefox/editor/separator.gif"" width=""6"" height=""20"" alt="""" /></td>
					
					
						
							<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_justifyleft""><img src=""images/bluefox/editor/justifyleft.gif"" width=""21"" height=""20"" alt=""Align Left"" /></div></td>
							<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_justifycenter""><img src=""images/bluefox/editor/justifycenter.gif"" width=""21"" height=""20"" alt=""Align Center"" /></div></td>
							<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_justifyright""><img src=""images/bluefox/editor/justifyright.gif"" width=""21"" height=""20"" alt=""Align Right"" /></div></td>
						
						<td><img src=""images/bluefox/editor/separator.gif"" width=""6"" height=""20"" alt="""" /></td>
					
					
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_insertorderedlist""><img src=""images/bluefox/editor/insertorderedlist.gif"" width=""21"" height=""20"" alt=""Ordered List"" /></div></td>
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_insertunorderedlist""><img src=""images/bluefox/editor/insertunorderedlist.gif"" width=""21"" height=""20"" alt=""Unordered List"" /></div></td>
					
					
						
							<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_outdent""><img src=""images/bluefox/editor/outdent.gif"" width=""21"" height=""20"" alt=""Decrease Indent"" /></div></td>
							<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_indent""><img src=""images/bluefox/editor/indent.gif"" width=""21"" height=""20"" alt=""Increase Indent"" /></div></td>
						
						<td><img src=""images/bluefox/editor/separator.gif"" width=""6"" height=""20"" alt="""" /></td>
					
					
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_createlink""><img src=""images/bluefox/editor/createlink.gif"" width=""21"" height=""20"" alt=""Insert Link"" /></div></td>
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_unlink""><img src=""images/bluefox/editor/unlink.gif"" width=""21"" height=""20"" alt=""Remove Link"" /></div></td>
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_email""><img src=""images/bluefox/editor/email.gif"" width=""21"" height=""20"" alt=""Insert Email Link"" /></div></td>
					
					
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_insertimage""><img src=""images/bluefox/editor/insertimage.gif"" width=""21"" height=""20"" alt=""Insert Image"" /></div></td>
					
					
						<td><img src=""images/bluefox/editor/separator.gif"" width=""6"" height=""20"" alt="""" /></td>
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_wrap0_quote""><img src=""images/bluefox/editor/quote.gif"" width=""21"" height=""20"" alt=""Wrap [QUOTE] tags around selected text"" /></div></td>
						<td><img src=""images/bluefox/editor/separator.gif"" width=""6"" height=""20"" alt="""" /></td>
					
					
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_wrap0_code""><img src=""images/bluefox/editor/code.gif"" width=""21"" height=""20"" alt=""Wrap [CODE] tags around selected text"" /></div></td>
					
					
					
						<td><div class=""imagebutton"" id=""vB_Editor_QR_cmd_wrap0_php""><img src=""images/bluefox/editor/php.gif"" width=""21"" height=""20"" alt=""Wrap [PHP] tags around selected text"" /></div></td>
					
				</tr>
			</table>
		</div>
		

		<table cellpadding=""0"" cellspacing=""0"" border=""0"" width=""100%"">
			<tr valign=""top"">
				<td class=""controlbar"">
					<fieldset style=""border:0px; padding:0px; margin:0px"">
						
							<textarea name=""message"" id=""vB_Editor_QR_textarea"" rows=""10"" cols=""60"" style=""width: 100%; height:100px"" tabindex=""1"" dir=""ltr""></textarea>
						
					</fieldset>
				</td>
				
			</tr>
			<tr><td class=""controlbar"">
<fieldset id=""vB_Editor_QR_smiliebox"" title=""Smilies"">
	<legend>Smilies [<a href=""#"" onclick=""vB_Editor['vB_Editor_QR'].open_smilie_window(smiliewindow_x, smiliewindow_y); return false"" title=""Showing 15 smilie(s) of 60 total."">More</a>]</legend>
	<table cellpadding=""4"" cellspacing=""0"" border=""0"" align=""center"">
	<tr align=""center"" valign=""bottom"">
<td><img src=""images/smilies/fool.gif"" id=""vB_Editor_QR_smilie_94"" alt="":fl:"" title=""fool"" border=""0"" /></td><td><img src=""images/smilies/please.gif"" id=""vB_Editor_QR_smilie_79"" alt="":pls:"" title=""pls"" border=""0"" /></td><td><img src=""images/smilies/78.gif"" id=""vB_Editor_QR_smilie_19"" alt=""#gedsa43"" title=""567ffdsa"" border=""0"" /></td><td><img src=""images/smilies/51.gif"" id=""vB_Editor_QR_smilie_15"" alt=""df3@"" title=""sdf54"" border=""0"" /></td><td><img src=""images/smilies/shy-fart.gif"" id=""vB_Editor_QR_smilie_96"" alt="":sf:"" title=""shy-fart"" border=""0"" /></td><td><img src=""images/smilies/f500bfbcf4ce4a41acf41c169b964ce5.gif"" id=""vB_Editor_QR_smilie_45"" alt="":f500325"" title=""sad32"" border=""0"" /></td><td><img src=""images/smilies/77.gif"" id=""vB_Editor_QR_smilie_31"" alt="":77:"" title=""77"" border=""0"" /></td><td><img src=""images/smilies/shisha_goza.gif"" id=""vB_Editor_QR_smilie_67"" alt="":shisha_goza:"" title=""Shisha Goza"" border=""0"" /></td><td><img src=""images/smilies/icon_syria_man.gif"" id=""vB_Editor_QR_smilie_62"" alt="":icon_syria_man:"" title=""Icon Syria Man"" border=""0"" /></td><td><img src=""images/smilies/cigar-smoker.gif"" id=""vB_Editor_QR_smilie_97"" alt="":csm:"" title=""Cigar smoker"" border=""0"" /></td>
</tr>
<tr align=""center"" valign=""bottom"">
<td><img src=""images/smilies/68.gif"" id=""vB_Editor_QR_smilie_18"" alt=""#gedsafsd"" title=""sdf54ds"" border=""0"" /></td><td><img src=""images/smilies/banned.gif"" id=""vB_Editor_QR_smilie_83"" alt="":ban:"" title=""Ban"" border=""0"" /></td><td><img src=""images/smilies/Welcome.gif"" id=""vB_Editor_QR_smilie_21"" alt="":Welcome:"" title=""Welcome"" border=""0"" /></td><td><img src=""images/smilies/57.gif"" id=""vB_Editor_QR_smilie_16"" alt=""ds4"" title=""567ff"" border=""0"" /></td><td><img src=""images/smilies/icon_surprised.gif"" id=""vB_Editor_QR_smilie_58"" alt="":icon_surprised:"" title=""Icon Surprised"" border=""0"" /></td><td><img src=""images/smilies/84.gif"" id=""vB_Editor_QR_smilie_36"" alt="":84:"" title=""84"" border=""0"" /></td><td><img src=""images/smilies/clap.gif"" id=""vB_Editor_QR_smilie_42"" alt="":clap:"" title=""Clap"" border=""0"" /></td><td><img src=""images/smilies/icon_syria_lovely.gif"" id=""vB_Editor_QR_smilie_61"" alt="":icon_syria_lovely:"" title=""Icon Syria Lovely"" border=""0"" /></td><td><img src=""images/smilies/bye1.gif"" id=""vB_Editor_QR_smilie_40"" alt="":bye1:"" title=""Bye1"" border=""0"" /></td><td><img src=""images/smilies/65.gif"" id=""vB_Editor_QR_smilie_17"" alt=""#gedsa"" title=""567fsad"" border=""0"" /></td>
</tr>

	</table>
</fieldset>
</td></tr>
		</table>
	</td>
</tr>
</table>

<input type=""hidden"" name=""wysiwyg"" id=""vB_Editor_QR_mode"" value=""0"" />
<input type=""hidden"" name=""styleid"" value=""0"" />

<script type=""text/javascript"">
<!--
vB_Editor['vB_Editor_QR'] = new vB_Text_Editor('vB_Editor_QR', 0, '22', '1');
var QR_EditorID = 'vB_Editor_QR';
//-->
</script>
	

					<fieldset class=""fieldset"" style=""margin:3px 0px 0px 0px"">
						<legend>Options</legend>
						<div style=""padding:3px"">
							
							<label for=""qr_quickreply""><input type=""checkbox"" name=""quickreply"" value=""1"" id=""qr_quickreply"" accesskey=""w"" tabindex=""4"" />Quote message in reply?</label>
						</div>
					</fieldset>
				</div>
			</div>

			<div style=""margin-top:6px"">
				<input type=""hidden"" name=""fromquickreply"" value=""1"" />
				<input type=""hidden"" name=""s"" value="""" />
				<input type=""hidden"" name=""securitytoken"" value=""1391173028-57c4db6b44891d2a41eece2efcbc7884af65d158"" />
				<input type=""hidden"" name=""do"" value=""postreply"" />
				<input type=""hidden"" name=""t"" value=""689925"" id=""qr_threadid"" />
				<input type=""hidden"" name=""p"" value=""who cares"" id=""qr_postid"" />
				<input type=""hidden"" name=""specifiedpost"" value=""0"" id=""qr_specifiedpost"" />
				<input type=""hidden"" name=""parseurl"" value=""1"" />
				<input type=""hidden"" name=""loggedinuser"" value=""112994"" />
				<input type=""submit"" class=""button"" value=""Post Quick Reply"" accesskey=""s"" title=""(Alt + S)"" name=""sbutton"" tabindex=""2"" id=""qr_submit"" onclick=""clickedelm = this.value"" />
				<input type=""submit"" class=""button"" value=""Go Advanced"" accesskey=""x"" title=""(Alt + X)"" name=""preview"" tabindex=""3"" id=""qr_preview"" onclick=""clickedelm = this.value"" />
			</div>

			<div align=""center"" id=""qr_posting_msg"" style=""display:none; margin-top:6px"">
				<img style=""vertical-align: middle;"" src=""images/bluefox/misc/progress.gif"" alt=""Posting Quick Reply - Please Wait"" />&nbsp;<strong>Posting Quick Reply - Please Wait</strong>
			</div>
		</td>
	</tr>
	</tbody>
	</table>
	</form>

	
		<!-- Mozilla work around for focusing on QR in WYSIWYG mode -->
		<div id=""qr_scroll""></div>
	

	<script type=""text/javascript"">
	<!--
		// initialize quick reply
		qr_init();
	//-->
	</script>


Please tell me how to do it with imacros, thank you.",https://forum.imacros.net/viewtopic.php?f=7&t=22389&sid=764a56e3c04defae9b390be083f6af7b,AI
2770,Combine many scraped Texts in one .csv file?,"Hi,
i want to scrape specific elements on a site and want to safe them in a csv.
Code: Select allSET !EXTRACT_TEST_POPUP NO
TAB T=1
TAB CLOSEALLOTHERS
set !var1 0
add !var1 {{!loop}}
URL GOTO=http://MYQUERY-{{!var1}}-*
TAG POS=1 TYPE=H1 ATTR=* EXTRACT=TXT
TAG POS=1 TYPE=DIV ATTR=ID:answer EXTRACT=TXT
SAVEAS TYPE=EXTRACT FOLDER=C:\Users\LE_ME\Desktop\MY_FOLDER FILE=Extract_{{!NOW:ddmmyy_hhnnss}}.csv
WAIT SECONDS=5


I want the <h1>*</h1> and the ATTR=ID:answer extracted and saved in a .csv file.

At the moment my script goes to site number 1, saves both elements in one file, goes to site number 2 and repeats it... so i have many files that i can't combine with Excel or something else. I tried the solution with CMD but it's not really working.

Is there a possibility to save everything in a cache and save it then at the end of my macro?",https://forum.imacros.net/viewtopic.php?f=7&t=22357&sid=764a56e3c04defae9b390be083f6af7b,AI
2771,Using EVAL to remove spaces and rows from extracted text,"Dear all,

I would like to rise up the question of using the EVAL command for simple modifications of extracted text.

In my case when using the Code: Select allTAG POS=r2 TYPE=SPAN FORM=NAME:Main ATTR=* EXTRACT=TXT I get result in the form
Code: Select all""


the result is:

1234 Euro

thank you for searching""

I have studied the EVAL page in the wiki
but I guess because of my lack of knowledge of the java language, I am not able to understand how to ""say"" that I would like the spaces, new rows and text=""Euro"" or text=""the result is:"" or text=""thank you for searching"" to be replaced with """" (in other words I am interested in the 1234 value only).

I guess the key is in this function 
Code: Select alls.replace(\""USD\"",\""\"")

but I do not understand how to modify it to serve my needs.

Thank you in advance for your support and advise 

Kind regards,

Maxee",https://forum.imacros.net/viewtopic.php?f=7&t=21480&sid=764a56e3c04defae9b390be083f6af7b,AI
2772,Extracting text betwen brackets ( ),"hi .. i need some help
im trying to extract the email address   name@email.com   from this source :

-------------------------------------------------------------------------------------------------------------------------

We emailed you at (<span>name@email.com</span>) to make sure we have the right email address.

-------------------------------------------------------------------------------------------------------------------------

i tried this : 
SEARCH SOURCE=REGEXP:""you at \\(([^)]+)\\)"" EXTRACT=$1
what i got is  :
<span>name@email.com</span> 

how can i get the email address without span element ? ",https://forum.imacros.net/viewtopic.php?f=7&t=22318&sid=764a56e3c04defae9b390be083f6af7b,AI
2773,PDF to Excel: Multiple RegEx matches in multiple columns,"Hi everyone, 
I’m very new to UiPath and programming as a whole, so I don’t have a lot of experience in this. 
I’m trying to create a program, that reads a single, random PDF-file, datascrape using regex and writing the &hellip;",,
2774,Collect many Data in DataTable in type Array and for use later in program,"Hello, 
Here are  screen shot what I have already created in my program and what I want to create now. As you can see my robot ask me three times for all necessary data about order to send some emails to the customer. Af&hellip;",,
2775,UIpath API Http Request Problem,"Hi All 
I have reached a roadblock 
I have an api and I have put down the headers and all 
The request perfectly works in postman and in my own python program which uses requests library. 
I can’t work it out in UiPath. 
&hellip;",,
2776,Can UI Path bots replace batch programs?,"Hi, 
I am looking for a way to get rid of a few batch programs in my Web Application and use UI path. 
I work for a large company, and we can get the enterprise license, if I can prove that this solution will save money. &hellip;",,
2777,UiPath Cross browser Testing,"How to use the single script designed in uipath to run in multiple browsers? 

I will pass the browser variable in datasheet and it should run in that browser. 
2.The selector content for each browser seems to change as &hellip;",,
2778,Is it possible to extract the contents of PDF and include it into an email body to be sent?,"I have currently scheduled an automated report from a reporting server, however it always sends the report as PDF attachment. I would like to have the PDF content just immediately pasted into the email body and then forw&hellip;",,
2779,About the IT Automation category,"Everything about our IT Automation packages. 
As a first step in our IT Automation roadmap,  we’re excited to announce our VMware activities package is now available in public preview!  This initial set of 11 activities &hellip;",,
2780,Data Migration,"Is it possible to automate the data validation process, post data migration using UiPath?",,
2782,Automate google search with data from excel,"Hi all 
I have a database of words to Google and scrape the data that comes up. 
I can figure out the scraping portion but I am unable to automate the search process. 
So the flow would be like 
Extract word from databas&hellip;",,
2783,Read hand writing on PDF File - OCR,"Dear Friends 
Please help to extract this hand written PDF data to excel or word as part of automation process. 
You advices are highly appreciate 
  
Thanks",,
2784,Project is stopped and not create any log,"Everyday, Uipath project is running automatically, but sometimes when [login ] activity login into the other web, [login ] activity don’t start  and not create any log too. Although i used [try-catch] activity in this ca&hellip;",,
2785,Cannot access application 'pgmessagehosttext.exe.' Elevated privileges might be required. Try running the UiPath application as an administrator,"I’m trying to automate an RDP process. I started UiPath studio as admin on my local machine. From the Studio, I was able to RDP to a remote server, log in to the remote desktop with an account from a different domain. Th&hellip;",,
2786,IT Automation Activities for VMWare,"Update!

This activity is now available on the Official Stable Feed:



Learn more about it here :point_down:

As a first step in our IT Automation roadmap, we’re excited to announce our VMware activities package is now&hellip;",,
2787,Proxy script edge browser sessions,Hello. I am looking to find a way to automate and check the browser settings on Edge in the init stage of an RE Framework Process. The download settings and cookies and permission settings have to be manually set up to d&hellip;,,
2788,Need help to identify the variable type for the word having string with number,"Hi Team, 
I am a newbie to this UI_Path automation technology, I am trying to create logon sequence where I have to provide user name as string+number, Kindly suggest the appropriate variable type for this",,
2789,Comparing data in two different databases,"Hi, 
Here is my testcase that needs to be automated. 
There is a schema in oracle db that needs to compare with a table in Microsoft sql developer using UI path… 
Can anyone help me with this scenario? 
Note: i am a begi&hellip;",,
2790,Raise a ETL job Request in Tidal to load changes from Oracle to Sql DB's,"I have a scenario to be automated in UIPath studio Pro Community using “Test Automation” 
1)Raise a request to run the &#39;oracle to Sql &#39;DB–&gt;ETL job request raised in Tidal 
2)Verify the ETL job has completed normally–&gt;The&hellip;",,
2791,SAP Business One Automation with UiPath,"Any good links for studying SAP Business One Automation with UiPath. 
I am new to this actually, 
any good resources will help. 
thanks, 
varun.",,
2792,On image appear probleme,"hello guys I am trying to use the event on image appear within my automation but it doesn’t work. It acts as if the image selected wasn’t on the screen. 
I want to use this to tel the robot thaht when that picture appear&hellip;",,
2793,Can we hand;e CEF pages using UI Path?,"We have a windows desktop application where we have embedded CEF pages. We have some ribbn cordiantes which on clicking opens an embedded chrome page. Is automation of such pages possible using UI Path ? 
requesting you &hellip;",,
2794,How to pass variables dynamically in connection string,"Hi im using excel as automation and used OleDB connection and the below is my connection string 
“Provider=Microsoft.ACE.OLEDB.12.0; Data Source=“C:\Users\Muthulakshmi\Downloads\Requester_1.xlsx”; Extended Properties=‘Ex&hellip;",,
2795,Double Click Text :Cannot find the UI element corresponding to this selector,"Hello guys, 
I am new to uipath and facing an issue here, when I am trying to automate the SAP application. 
Scenario: 1) I am accessing  some T-code and then I click on Get variant icon. 
2) A new window(Get Variant) op&hellip;",,
2796,Web Automation for,"Hello Team, 
I have a website in which every week the new file is uploaded . Now I have to automate in such a way that my process should check for new file every week and upload it in the destination folder. 
Please help&hellip;",,
2797,Automate the web service now Application,"My requirements is to automate the Service now Web application,my bot should open the service now Application in Web portal.if I have a 5incidents in my quee and I need to Loop the incidents one by one, open the Frist in&hellip;",,
2798,Guidewire ClaimCenter 9 New Claim Selector Issue,"Hi All, 
Recently we are planned to Autoamate the Guidewire ClaimCenter 9 - Creating new claim process. but we stuck in the very beginning step. We are not able to select the Newclaim from Claim Tab. We tried many ways n&hellip;",,
2799,Window application - UI element not recognised. Entire window identified as single element,"Hello, 
I am trying to automate a window application but the entire application is identified as single element. Selectors doesn’t recognize each objects such as username, password etc. 
I tried changing the UI framework&hellip;",,
2800,Computer vision and ocr is available in mobile automation?,"Hello guys, 
I want to automate rdp in mobile app how to do that… Please let me know… 
Also I see in community edition there is no computer vision and ocr activities to automate rdp in Mobile 
Urgent please let me know",,
2801,If: Column 'Age' does not belong to table DataTable,"Hi Everyone, 
I am trying to do a simple Excel Automation wherein the robot would read and write data to an excel file, but when I run the robot I’m getting an error. Below is the screenshot. I hope you guys could help m&hellip;",,
2802,Can we automate SSRS reports with UiPath?,"Hi Folks, 
I need to automate a SSRS report which has a lot of data metrics in table form. Can we automate it with UiPath, if yes then how?",,
2803,After uipath update: Input string was not in a correct format,"Hello I updated Uipath to the latest version. The supplier had developed billing automation for us, but it is now bankrupt. But at the place of “convert” now stops. Maybe you can help how this can be fixed? Thank you in &hellip;",,
2804,Creating NuGet package during CI pipeline,"Hello, 
I am currently evaluating the use of UiPath for automated GUI-Tests. 
To create reuseable components we would like to group workflows into packages. 
Now i want to create a package on our build server everytime c&hellip;",,
2805,UiPath not able to recognize field element - Full pane highlight,"Hello UiPath Experts, 
I am facing a weird issue while trying to automate my process in SAP. When I try to use click activity or type into activity, the UiPath is not highlighted that specific field I want to use instead&hellip;",,
2806,IT Automation Activities for Active Directory Domain Services,"Update!

This activity is now available on the Official Stable Feed:



Learn more about it here :point_down:

As the next step in our IT Automation roadmap, we’re excited to announce our Active Directory Domain Service&hellip;",,
2807,Automation to create a powerpoint presentation,"Hi Team, 
Is there a specific activity or automation sequence that will allow me to create or build a powerpoint presentation? The logic is if i receive an email with a pre formatted template (excel sheet) on how many sl&hellip;",,
2808,Automating Micro Focus ALM,"I am attempting to automate usage of the Micro Focus Application Lifecycle Management (ALM) Quality Center (QC) system (Formerly from HP), version 12.55. I can successfully enter the user ID and password, the click Authe&hellip;",,
2809,Trying to get chatbot response and store data,i am trying to automate a chatbot in amazon lex . i am qetting sample questions and putting into a amazon lex chatbot .and trying to capture response for every question . is it possible ? . can we make the selector dynam&hellip;,,
2810,Automation,"Hello, 
I am a beginner in IT and in the RPA course. I have to automate a task, an email (from Outlook) must be registered in a CRM. Is it possible? Is there a tutorial for this? 
Thank you in advance. 
Gulbara",,
2811,How to save a file to a certain location after exporting,"Hi guys, 
My process exports(downloads) a file from a website, but he file is automatically saved to the downloads folder but i want to save it to another folder. 
how can i do it?",,
2812,Azure DevOps Automation - (NOT for UI Path project version control),"Hi All, 
Wanting to automate functions in Azure DevOps web app which is a different project. 
Such as: 

Creating new stories (work items) from data in excel
Updating existing stories / acceptance criteria etc from data &hellip;",,
2813,Windows Server Issue,"Hello, I’m having an issue with the UiPath UiAutomation everytime you try to go into select element mode, it throws this error: 
 
The machine is Windows Server 2012 R2, it has desktop experience and .net framework insta&hellip;",,
2814,How to Get an Process DataTable Items,"HI, That you in advance for the help here. I am currently building an automation and i am stuc not knowing how to proceed. I have a DataTable that essentially looks like this  
It is a list of orders . The way the table &hellip;",,
2815,ID changing every time in SAP application,"Hi, 
I am automating the SAP application using uipath. I am using basic recording activity in uipath. 
But thing is ID in SAP application is changing every time. i used UI explorer to find other attributes like Parent id&hellip;",,
2816,Excel sorting and removing duplicate,"Hi All, 
I am working on automation of Excel where I need to sort the data in descending order and remove the duplicate. 
The column on which I have to apply sort condition contains date. 
So all data will be sorted in d&hellip;",,
2817,DEVOPS for UIPATH- Updating Process,"I am exploring on DEVOPS features for UIPATH projects. I have used Jenkins and able to build and deploy the package. 
Once new package is deployed we need to update the process. Is there any way to automate or any comman&hellip;",,
2818,"IT Automation activities (Active Directory, VMWare, Exchange, AWS) are now on the Official Feed",":loudspeaker: IT Automation activities are now available on the Official Stable Feed
If you follow us closely, you might have noticed these five activities packages that were recently released for public preview: 


Act&hellip;",,
2819,Automation in MS Project,"Hello, 
is it possible to have the automation build on MS Project? I tried on excel activities but it’s not working as expected. 
Any suggestions or help? 
BR, 
Manisha",,
2820,UnEncountered popup issue in citrix environment,"Hi Team, 
We have done automation of the desktop application in citrix environment.While run the Bot on stage environment,we are getting desktop application error popup which is unknown for us and it can appear in any sc&hellip;",,
2821,New Virtual Machine Old packages were not included,"Hi, we had an issue for a new remote desktop, a robot was looking for an old package and resulting to an issue, but the update only gives the new package. what would be the automatic way to download/update the local of t&hellip;",,
2822,About the Automation Suite category,Help around our Automation Suite.,,
2823,Automation Suite and Openshift,"Hi Everyone, 
I would like to ask if i can install UiPath Automation Suite with Openshift or no also what is the best practice for robots VMs is to avail detected VM for every robot or add more than one robot on the same&hellip;",,
2824,Erro Login SSO Automation Hub - user domain did not match the list of configured allowed domainsallowed domains,"Guys, I have the error in the HUB using the automatic login (SSO) for all people who have the domain @usiminas.com working very well, but for the employees of one of the companies of the group with the domain @solucoesus&hellip;",,
2825,Reg RE Framework I need to terminate error transaction item and move to next item in queue,"About my automation -Using RE Framework I am getting input data from excel sheet and load them in queue and then executing each steps in application  . 
Scenario 
E.g. Let say i have 25 steps in my automation and in-betw&hellip;",,
2826,Generating test report once the test execution is completed,"Hi All, 
This might be the old topic but i find no concrete solution for the problem statement hence want to reiterate with the solution i tried. 
I am utilizing UIPATH purely for the test automation, to automate all the&hellip;",,
2827,Need to Perform some steps before RE Framework start,"Hi , 
I am new to RE Framework Automation and I want to run some Steps 
before my RE framework automation starts . Can someone help me how we can run my Automation steps ? 
I want to perform below one  time steps before &hellip;",,
2828,Debugging Active directory integration with kerberos,"User login to Automation Suite
When there are user login issues to Automation Suite even after following these steps, please follow the steps below to debug the issue. 

Check if Kerberos is configured successfully if y&hellip;",,
2829,Issue about setting Automation Suite: DNS Validation Failed from container,"Hello, 
There is an error in the Automation Suite installation step, please help me to fix it 
“DNS Validation Failed from container”",,
2830,HOW IS AUTOMATION IMPORTANT IN AI?,"The automation is important in RPA. If it is important so how could, I use it what were the preferences of automation suite. And how to download a suite in a laptop.",,
2831,Automation suite deployment with azure cloud,"Hello everyone 
does anyone have a video tutorial or a simple document that describes the steps for deploying a project on cloud azure with automation suite 
Thank you in advance",,
2832,Cloud Azure deployment,"Hello 
I want to use the automation suite to deploy my project. 
I’m using Cloud Azure 
Does it require the same prerequis as the on prem? 
The picture below has prerequists are they the ones needed in Azure deployment? 
&hellip;",,
2833,Apps Tab keep on loading Not Opening,"Dear Developers, 
I have a sudden problem showing up, I am using #build:automation-suite utomation-suite and since yesterday I am not able to access Apps page rest of the things working fine (Data service, Orch. integrat&hellip;",,
2834,"There is an error in XML document (2,16)","Hello, 
I’m unable to authenticate to my work Tableau server. I used my credentials but got error 
&#39;There is an error in XML document (2,16). I’m the first one in my office to embark on the journey to automate Tableau re&hellip;",,
2835,Automation suite - Deploy issue (Online single-node evaluation installation-Basic),"Hey all ! 
I am trying to deploy the Automation suite on the Linux machine, but I faced the below issue, appreciate any support please :grimacing: 
Linux: RHEL 8.4: 
Automation Suite: Manuel - Online single-node evaluati&hellip;",,
2837,Amazon Web Services (AWS) Activities are now in Public Preview,"Amazon Web Services (AWS) Activities are now in Public Preview


Update!

This activity is now available on the Official Stable Feed:



Description
UiPath.AmazonWebServices.Activities package offers activities for Amaz&hellip;",,
2838,"How to set KEY/Value pairs as ""x-www-form-urlencoded"" type in ""HTTP Request"" activity in uipath","How to set KEY/Value pairs as “x-www-form-urlencoded” type in “HTTP Request” activity in uipath? 
WE are making a API call using postman which works fine in postman as shown below: 
  
in the above postman request we are&hellip;",,
2839,IT Automation Activities for Microsoft Exchange Server,"Update!

This activity is now available on the Official Stable Feed:



Learn more about it here :point_down:

Description
UiPath.ExchangeServer.Activities package offers administration activities for Microsoft Exchange&hellip;",,
2841,Azure Active Directory (Azure AD) Activities are now in Public Preview,"Update!

This package is now available on the Official Stable Feed:
 


Azure Active Directory (Azure AD) Activities are now in Public Preview


Description
UiPath.AzureActiveDirectory.Activities package offers activiti&hellip;",,
2842,Need to click on an element on the 2 window,"Hi 
I am unable to click “All time” using click option 
I tried attach browser , recording but in vain 
I need to close the window too not the browser. “Close window” activity closes the browser itself",,
2843,Problem Creating Mailbox,"@ovidiuponoran 
After creating the user in AD (with user principal name) by using the Create User activity available inside the ActiveDirectoryDomainService Package successfully, the Create Mailbox (Exchange Server) acti&hellip;",,
2844,DU Framework - Skip waiting process for human validation,"I want the processing to continue without waiting for a validation.  What should I do? 
Best regards,",,
2845,UiPath NetIQ eDirectory Activities are now in Public Preview,":loudspeaker: UPDATE 
UiPath.NetIQeDirectory.Activities 1.0.0 are now available on the Official Feed 


Description
UiPath.NetIQeDirectory.Activities package offers cross-platform activities for LDAP (both eDirectory an&hellip;",,
2846,Google Cloud Platform (GCP) Activities are available for Public Preview,"Description

Update: :loudspeaker: :loudspeaker::loudspeaker: Google Cloud Platform activities are now available on the Official Stable Feed!
In addition to existing activities for managing virtual machine instances, it&hellip;",,
2848,"Pass a variable as condition in If activity, where variable contains complete condition as a string","Hi, 
I want to pass a variable as condition in If activity, where variable contains the complete condition as string. 
Like - 
My condition is assigned to the variable - 
variable1 = ‘row1.item(“Name”)=row2.item(“Name”)’ &hellip;",,
2849,Azure Activities now in Public Preview,"Update

This package is now available on the official feed!
 

Description
UiPath.Azure.Activities package offers activities for Microsoft’s cloud, Azure. 
This initial set of 50+ activities enables IT Departments to ea&hellip;",,
2851,Oracle script exection- PL/SQL procedure,"Hi Team, 
I need to execute the below script in uipath, connecting using connection string. 
Not able to get the output as expected: 
[11:11] Sait, Girish 
SET SERVEROUTPUT ON 
DECLARE 
CURSOR cur_wtn 
IS 
SELECT ref_id_&hellip;",,
2852,IT Automation Samples and Learning,"That’s a great news from Uipath… 
Expecting few more samples for easy learning. Which activity we can use for updating the details(for eg: phone number, address etc) while user creation? Plz share samples if possible. &hellip;",,
2853,How to establish a Connection for mainframe (QWS3270 Secure),"Hi, 
I am new to mainframe. Please help me to establish a connection in qws3270 for the terminal type IBM-3278-2. 
I have tried using ‘UiPath Internal’ option with hostname and port 
and “IBM ELLHAPI” option with winhlla&hellip;",,
2854,Query for multiple windows login,Is it possible to login unknown Windows machine (in or out of domain) with admin credential and take screenshots once checks on system is done?,,
2856,Download File activity failed to capture the file output,"This question is to be answered by UiPath architect team. 
@developers - please excuse. 
Issue - The ;download file’ activity is randomly (or occasionally) failing to capture the output file. We did all the possible ways&hellip;",,
2857,To check if row contains more than one code,"Hi, 
After scrapping the value and write it to excel file , I want to check if row(0) contains multiple code like (h1, h4,h6) i want to proceed . 
I’m not sure how to write in if condition row(0).toString={“h1”, “h4”} 
  &hellip;",,
2858,UiPath Orchestrator Not Connecting,"not able to connect to UIRobot , UIOrchestrator with my UIStudio. 
have done everything correct but still the same issue. 
does machine name,  username and domain name are case sensitive ? 
Couple of days back it &hellip;",,
2859,Cannot find HotKeyTrigger Activity under Monitor events activity in Latest version of UiPath 20.10 ( Manage Packages),"cannot find HotKeyTrigger Activity under Monitor events activity in Latest version of UiPath 20.10 
also tried looking in Manage Packages but all in pain… 
can anyone please help ?",,
2860,Uipath Get Mail Message from Speciak Mail Group,"Hello dear forum members, 
I would like to access outlook mail messages which comes to the specific company mail group. 
For example 
Main Mail 
a.Inbox 
b.Drafts 
C.Sent Items 
and so on. 
FactoryMailGroup 
a.Inbox 
b.D&hellip;",,
2861,Software Update,once the environment of the software changes so i i have to build activites again which i have created early for older environment?,,
2862,Read large text file,I have a large text which i want to read the size of the file is 1.5 MB. when i try to use the read text file activity i am getting error due to large size. The text file contain some meta data in the starting and later &hellip;,,
2863,ML/AI Integration with UIPath,"Hi Everyone, 
I want to know how can I integrate Uipath with ML/AI. What all packages can help in it? 
and Whether they can work in community version or need enterprise version .",,
2864,Outlook Automation - I want to automatically open locally saved outlook emails and save and export their zip file,"Hello! 
I currently have a locally stored folder of 50+ outlook emails which each have a zipped file I want to download, extract, and change the file type ***.unl to ***.dsv. 
I was able to build a basic workflow to wher&hellip;",,
2866,Check the attribute (aaname) of UiElement using If Activity,"There is an UiElement having selector: 



I want to check that this selector contains aaname value as that of entered by the user which stored in variable “WorkID” using the “IF” activity. Can anybody help me with this? &hellip;",,
2867,ML Activity issue with recognition,"Hi I am having a problem with ML Activity 
It can not see a dot when extracting number 
  
Is there a possibility to fix this somehow? 
Can you improve a AI model to include that item in training set of a model so the ne&hellip;",,
2868,Can I add image screenshot into the outlook email body?,Can I add image screenshot into the outlook email body?,,
2869,AD Server connection error,"Hi, 
for one of our project i was doing a POC, for which we needed to connect with the AD server and check the user exists or not. 
i’ve use AD Scope activity and provided all the details. But while executing i’ve been g&hellip;",,
2870,AI fabric - when it can be used with free community version,"Hi, 
AI fabric - when it can be used with free community version , right now it requires Enterprise licnese, if we go ahead with it, would we be able to still use community version for free ? 
Regards",,
2871,Error report connecting Bizagi and Uipath,"Hello, 
Recently, I’ve tried to connect Bizagi and Uipath, and I found the error because Uipath changed the way of setting tenant. 
If I regularly register tenant using orchestrator like adding service, Bizagi failed aut&hellip;",,
2872,Scrap informations from website to Exell,"Hi 
I’m searching for a solution to scrap statistical informations contained  on www.statfoot.be  to an Exell sheet. 
Can anybody help me?",,
2876,Help on Error HRESULT E_FAIL has been returned from a call to a COM component,"I am using ie 11 and want to click on the below link   
however it throws this error   
Error HRESULT E_FAIL has been returned from a call to a COM component 
i have tried all the solutions suggested from previous questi&hellip;",,
2877,Extracting data from a datatable,"Hi 
I am creating a procedure where i have a excel file containing hundreds of rows of data, however i only need access to a few rows, i have created a datatable reading in the necessary rows. 
The next stage is opening &hellip;",,
2878,Click text activity not working,I have list of acknowledgement numbers of different forms and have web page contains such numbers as hyperlink. I want to download one by one such forms but click text activity is not working. Please assist.,,
2879,Count no of words between comman separated words,"Datatable contains 
A,b 
A,c,b 
U,k,p,j 
Expected result 
A,b 
As it has only 2 letters 
My req is to fetch the row which has least no of letters",,
2880,Exit a job when a condition is met,"I want to exit a job when a certain condition is met. Not just simply exit a workflow but stop the robot complete. 
Kind regards 
Jos",,
2881,Multiple PDFs Image Data Extraction,"Hi team! 
I have some pdfs, all contain images in it. I want to extract the data from these images. I used Get OCR Text, Find Image, Click OCR Text, and different OCR engines like Teserract OCR, Google OCR, etc. But sinc&hellip;",,
2883,Important question,"Hi everyone. 
Plz help me for the below use case 
A folder contains some 300 excel files, and I need to append data of all excels together in one and below is my task to perform via RPA 

Open folder
Read first excel, cl&hellip;",,
2884,Read Range not reading a formula,"Hi there, 
I’m trying to add a “Read Range” activity in order to read from an excel that contains formula to check the remaining days based on the date from a different cell. Below is the Excel file that i’m trying to re&hellip;",,
2885,ALM Integration with UiPath Tool,"Hi All, 
Need help! 
I need to Integrate ALM with UiPath Tool. (basically to update test status - Pass/Fail) 
Can somebody help me how to do that? 
Thanks in advance. 
Regards, 
Sujeetkumar Albal 
9740561666",,
2886,How to create a reusable Libraries which returns data and i can use that data in Test scripts,"Hi, 
I’m new to UIPath Forum, In my Project we are maintaining Test data in Excel sheet. so to read that excel sheets we are creating a libraries and getting the data in Data Table but we are not aware how to Use that Da&hellip;",,
2888,Need help : Execution on webpage,"I have attached a webpage where I have to do certain operation. 
I have highlighted an icon where I have to click and provide essential 
information and save it. Same operation I have to do for each and every record. 
Si&hellip;",,
2889,Need to find new error line following the timestamp,"Hello All, 
I have a text file with large number of error details, I need to extract the current error based on timestamp. 
Whenever the file gets updated with a new error, We need to fetch the new error. 
I have attache&hellip;",,
2891,Need to Migrate from Resolve Systems to UiPath,"Hi, 
I’m looking to track down any documentation that details: 
A) Is it possible to migrate from Resolve Systems to UIPath 
B) How is it done 
Any feedback would be greatly appreaciated",,
2892,Citrix Activities are now in Public Preview,"Update
This package is now available on the official feed! 
 

Description
UiPath.Citrix.Activities package offers activities for XenServer 7.x and Citrix Hypervisor 8.0 virtualized infrastructures. 
This initial set of&hellip;",,
2893,"Automating Web page handling without VMs, just using DOM and HTML interpretation","Hello 
Im new to UiPath and im trying to find easily if its able to handle DOM/HMTL full interpretation without a VM. Allocating and deallocating VMs is a pain ,expensive and slow. 
I would like to ask directions on the &hellip;",,
2894,Data Extraciton from PDF tables,"Pls elaborate the best way to extract data from row in PDFs. Here issue is multiple PDFs generates by system and number of rows in each PDF are not constant,  but COLUMNs remain same.  ROWs count changing in each PDF. BO&hellip;",,
2897,[Task] - How to read a Word Document and write in the Application as per certain criteria,"Prerequisite: A Schedule is given on a Weekly basis where there are security guards present on particular sites for particular times. 
Chennai Port:  587-2076 
CSS	Sat	Sun	Mon	Tue	Wed	Thu	Fri 
Time/Hours	 12/03	12/04	12/&hellip;",,
2898,How to Filter Data Table Using A List of Values,"I have a data table that I wish to Filter based on a list of Customer values. Instead of listing each customer value separately, how would I code the Filter to use “Contains” and place customer values in a List.",,
2900,Single Node Installation - Help,"Hello everyone :smiling_face: 
I am facing an error during single node installation in linux machine. Help me to fix the issue. The issue details attached in below screenshot image.",,
2901,Automation Suite - Airgapped Install Restrictions in an Non-Airgapped environment,"Hi Team, 
Quick question 
If we do an airgapped installation in a non-airgapped environment (with internet), just for the sake of trying airgapped install procedures, can it still be considered a non-airgapped environmen&hellip;",,
2902,How to filter out/remove cells that contains empty,"Hello everyone, 
I was wondering how can I filter out and remove the cells that contains empty in an excel file? 
From 
 
To 
 
I have tried to use the filter wizard, but it would not filter and remove the cells out",,
