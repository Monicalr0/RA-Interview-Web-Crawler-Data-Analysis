Title,URL,Subreddit,Body,Comments
Help with code to web scrape tripadvisor,https://www.reddit.com/r/webscraping/comments/10fk2ov/help_with_code_to_web_scrape_tripadvisor/,webscraping,"Hello! I have had help building this chunk of code but from this point on, I have no clue how to add the pagination. My goal is to gather the email data of all the hoteils on this link :  
[https://www.tripadvisor.pt/Hotels-g189100-Portugal-Hotels.html](https://www.tripadvisor.pt/Hotels-g189100-Portugal-Hotels.html)  


So, for example, if I wanted to scrape the info of the 3 first pages, i would print the name of the hotels and their respective email. 

Any piece of advice? I wouldn¬¥t mind paying if I could get some help :)",[]
Not able to scrape all the reviews,https://www.reddit.com/r/webscraping/comments/10fnico/not_able_to_scrape_all_the_reviews/,webscraping,"I am having trouble scraping the all the reviews from this website [https://www.petsmart.com/dog/food/fresh-food/freshpet-vitalandtradegrain-free-beef-and-bison-adult-dog-food-1095.html?cgid=100248&fmethod=Browse](https://www.petsmart.com/dog/food/fresh-food/freshpet-vitalandtradegrain-free-beef-and-bison-adult-dog-food-1095.html?cgid=100248&fmethod=Browse) . I am only getting old one not getting the recent one even though I have a sort filter parameter `Most Recent` in place. Here is my code:

    import requests
    from bs4 import BeautifulSoup
    from datetime import datetime
    from urllib.parse import unquote
    import math
    import pandas as pd
    
    
    class PetSmartReviewsDetail:
    
        results = []
    
        headers = {
            ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64; rv:108.0) Gecko/20100101 Firefox/108.0"",
            ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"",
            ""Accept-Language"": ""en-US,en;q=0.5"",
            ""Connection"": ""keep-alive"",
            ""Upgrade-Insecure-Requests"": ""1"",
            ""Sec-Fetch-Dest"": ""document"",
            ""Sec-Fetch-Mode"": ""navigate"",
            ""Sec-Fetch-Site"": ""none"",
            ""Sec-Fetch-User"": ""?1"",
            ""Sec-GPC"": ""1"",
        }
    
        params = {
            ""passkey"": ""208e3foy6upqbk7glk4e3edpv"",
            ""apiversion"": ""5.5"",
            ""displaycode"": ""4830-en_us"",
            ""resource.q0"": ""reviews"",
            ""filter.q0"": [
                ""isratingsonly:eq:false"",
                ""productid:eq:69272"",
                ""contentlocale:eq:en,en_US"",
            ],
            ""sort.q0"": ""submissiontime:desc"",
            ""stats.q0"": ""reviews"",
            ""filteredstats.q0"": ""reviews"",
            ""include.q0"": ""authors,products,comments"",
            ""filter_reviews.q0"": ""contentlocale:eq:en,en_US"",
            ""filter_reviewcomments.q0"": ""contentlocale:eq:en,en_US"",
            ""filter_comments.q0"": ""contentlocale:eq:en,en_US"",
            ""limit.q0"": ""30"",
            ""offset.q0"": ""0"",
            ""limit_comments.q0"": ""3"",
        }
    
        def fetch_prod_ids(self, url: str):
            print(f""Fetching product ID's from: {url}"", end="""")
            response = requests.get(url, headers=self.headers)
            print(f"" | Status code: {response.status_code}"")
            soup = BeautifulSoup(response.text, ""lxml"")
            product_ids = [
                link.get(""href"").split(""."")[0].split(""-"")[-1]
                for link in soup.find_all(""a"", {""class"": ""name-link""})
            ]
    
            return product_ids
    
        def parse_review_detail(self, product_id: str, API_URL: str):
            item = {}
            self.params[""filter.q0""][1] = f""productid:eq:{product_id}""
            response = requests.get(API_URL, params=self.params, headers=self.headers)
            print(f""Fetching product review api from: {unquote(response.url)}"", end="""")
            print(f"" | Status code: {response.status_code}"")
            json_blob = response.json()
            total_reviews = round(
                math.ceil(json_blob[""BatchedResults""][""q0""][""TotalResults""]) / 30
            )
    
            for i in range(0, total_reviews):
                review_page = i * 30 + 8
                self.params[""offset.q0""] = review_page
                self.params[""filter.q0""][1] = f""productid:eq:{product_id}""
                response = requests.get(API_URL, params=self.params, headers=self.headers)
                print(
                    f""Fetching paginated product review api from: {unquote(response.url)}"",
                    end="""",
                )
                print(f"" | Status code: {response.status_code}"")
                json_blob = response.json()
                product = json_blob[""BatchedResults""][""q0""][""Includes""][""Products""][
                    f""{product_id}""
                ]
                item[""Product_Name""] = product[""Name""]
                item[""Total_Rating""] = product[""ReviewStatistics""][""AverageOverallRating""]
                item[""User""] = [
                    r[""UserNickname""] for r in json_blob[""BatchedResults""][""q0""][""Results""]
                ]
                item[""Time""] = [
                    datetime.fromisoformat(r[""SubmissionTime""]).date().strftime(""%m-%d-%Y"")
                    for r in json_blob[""BatchedResults""][""q0""][""Results""]
                ]
                item[""Heading""] = [
                    r[""Title""] for r in json_blob[""BatchedResults""][""q0""][""Results""]
                ]
                item[""Detail""] = [
                    r[""ReviewText""] for r in json_blob[""BatchedResults""][""q0""][""Results""]
                ]
                item[""Recommend""] = []
                for recommend in json_blob[""BatchedResults""][""q0""][""Results""]:
                    if recommend[""IsRecommended""] == True:
                        item[""Recommend""].append(""Y"")
                    else:
                        item[""Recommend""].append(""N"")
    
                item[""No_helpful_vote""] = product[""ReviewStatistics""][""NotHelpfulVoteCount""]
                item[""Yes_helpful_vote""] = product[""ReviewStatistics""][""HelpfulVoteCount""]
    
                self.results.append(item)
    
        def to_csv(self):
            df = (
                pd.DataFrame(self.results)
                .fillna("""")
                .explode([""User"", ""Time"", ""Heading"", ""Detail"", ""Recommend""])
            )
    
            df.to_csv(f""petsmart_reviews_details.csv"", index=False)
    
            print('Stored results to ""petsmart_reviews_details.csv""')
    
        def run(self):
            api_url = ""https://api.bazaarvoice.com/data/batch.json""
            base_url = ""https://www.petsmart.com/dog/food/fresh-food/freshpet/?pmin=0.01&srule=best-sellers&format=ajax""
            product_ids = self.fetch_prod_ids(base_url)
            for _id in product_ids:
                self.parse_review_detail(_id, api_url)
            self.to_csv()
    
    
    if __name__ == ""__main__"":
        scraper = PetSmartReviewsDetail()
        scraper.run()

Here is the API url [https://api.bazaarvoice.com/data/batch.json?passkey=208e3foy6upqbk7glk4e3edpv&apiversion=5.5&displaycode=4830-en\_us&resource.q0=reviews&filter.q0=isratingsonly:eq:false&filter.q0=productid:eq:1095&filter.q0=contentlocale:eq:en,en\_US&sort.q0=submissiontime:desc&stats.q0=reviews&filteredstats.q0=reviews&include.q0=authors,products,comments&filter\_reviews.q0=contentlocale:eq:en,en\_US&filter\_reviewcomments.q0=contentlocale:eq:en,en\_US&filter\_comments.q0=contentlocale:eq:en,en\_US&limit.q0=8&offset.q0=0&limit\_comments.q0=3](https://api.bazaarvoice.com/data/batch.json?passkey=208e3foy6upqbk7glk4e3edpv&apiversion=5.5&displaycode=4830-en_us&resource.q0=reviews&filter.q0=isratingsonly:eq:false&filter.q0=productid:eq:1095&filter.q0=contentlocale:eq:en,en_US&sort.q0=submissiontime:desc&stats.q0=reviews&filteredstats.q0=reviews&include.q0=authors,products,comments&filter_reviews.q0=contentlocale:eq:en,en_US&filter_reviewcomments.q0=contentlocale:eq:en,en_US&filter_comments.q0=contentlocale:eq:en,en_US&limit.q0=8&offset.q0=0&limit_comments.q0=3) reviews are coming from. Can anyone please help me out here scrape all the reviews?",[]
ScrapeStack trouble getting HTML for search results page for Best Buy.,https://www.reddit.com/r/webscraping/comments/10fjmre/scrapestack_trouble_getting_html_for_search/,webscraping,"Trying to get the HTML for the Best Buy search results page:

> curl --location --request GET ""https://api.scrapestack.com/scrape?access_key=MY_KEY&proxy_location=us&render_js=1&url=https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&qp=format_facet%3DFormat~Physical&st=elden+ring"" 

This is returning improper HTML:

    *snip*
    <title>Best Buy: Page Not Found</title>
    *snip*

This same URL works fine using other services but I would rather use ScrapeStack since I'm using it elsewhere on our site.

Would appreciate any tips. Thanks!",[]
Best platform to host my webscraping backend?,https://www.reddit.com/r/webscraping/comments/10f2j1m/best_platform_to_host_my_webscraping_backend/,webscraping,"I am currently building a web scraping application and am in the process of deciding on a platform to host my backend. I am looking for a platform that can handle the resource-intensive nature of web scraping and can support the programming languages and frameworks that I am using (Selenium, BS, requests).

I have been researching various options such as Heroku and Vercel, but webscraping violates their terms of use.

I would like to know what others have found to be the best platform for hosting web scraping backends and why. Additionally, if you have any experience with a specific platform and its ability to handle web scraping, I would appreciate any insights you could provide.

Thank you in advance for your help!","['Digital Ocean', 'You can use VPS from Google Cloud or AWS', 'I am running mine on Azure', 'Zyte']"
"Facebook graph API: ""please reduce the amount of data you're asking for error""",https://www.reddit.com/r/webscraping/comments/10f9tu4/facebook_graph_api_please_reduce_the_amount_of/,webscraping,"I know, if you're asking nicely to an API then it's not *real* scraping -- but official developer channels gave zero shits about this (figures) so mayyyybe it's worth a shot here? 

I'm working on a custom solution that gets *owned* page posts and comments for a client, for analytics reasons. Client manages some 25+ pages, of which all but one are localised versions, and one is a global page in English, which is the default to where FB users not included in all the different locales get redirected to. 

Everything is working OK (it's mostly `<page_id>/feed` and `<post_id>/comments` requests) *except for the global page*. 

When trying to access that via the /feed endpoint all I get is the **""please reduce the amount of data you're asking for""** error, regardless of *limit* set on the request or the number of fields requested. I can set that limit to *one* and still get the error, regardless of the amount of fields I'm requesting. 

Any ideas?",[]
Need help completing this project.,https://www.reddit.com/r/webscraping/comments/10f8rjk/need_help_completing_this_project/,webscraping,"I am a newbie with web scrapping and I need help in completing this project.

I am trying to get the number form the span class that has the last ""star fill"" This is my desired output  seatcomfort  = 3 cabinstaffservice = 5 inflight = 5 FoodBeverages = 5 GroundService = 4 Valueformoney = 4 wifi = 5 

    seatcomfort = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Seat Comfort"")) td.review-rating-stars.stars, span.star fill')    
    
    cabinstaffservice = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Cabin Staff Service"")) td.review-rating-stars.stars, span.star fill')
    
    inflight = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Inflight Entertainment"")) td.review-rating-stars.stars, span.star fill')
    
    FoodBeverages = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Food & Beverages"")) td.review-rating-stars.stars, span.star fill')
    
    GroundService = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Ground Service"")) td.review-rating-stars.stars, span.star fill')
    
    Valueformoney  = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Value For Money"")) td.review-rating-stars.stars, span.star fill')
    
    wifi  = Ratings.select_one('tr:has(td:first-child:-soup-contains(""Wifi & Connectivity"")) td.review-rating-stars.stars, span.star fill')
    --------------------------------------------------------------------------------
    #output 
    
    seatcomfort= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star"">4</span><span class=""star"">5</span></td>
    
    Cabinservice= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star fill"">5</span></td>
    
    inflight= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star fill"">5</span></td>
    
    foodbeverages= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star fill"">5</span></td>
    
    Groundservice= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star"">5</span></td>
    
    Valueformoney= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star"">5</span></td>
    
    wifi= <td class=""review-rating-stars stars""><span class=""star fill"">1</span><span class=""star fill"">2</span><span class=""star fill"">3</span><span class=""star fill"">4</span><span class=""star fill"">5</span></td>","['If your tool supports xpath you could use that to find the last star fill, or you could change select_one to select_all and get the last element from the array. Changing your selector from span.star fill to span.star.fill should return only the filled star spans']"
data completeness checks for dynamically loading page,https://www.reddit.com/r/webscraping/comments/10f4g5k/data_completeness_checks_for_dynamically_loading/,webscraping,"Hi, using Selenium to crawl and complete web scraping. The url loads dynamically and hence playing around with waiting time to fully load. I am not able to get the content-length meta data form the source to check teh data completeness. Is there any other way to check whether downloaded data is complete. As the page is loading dynamically, some long pages are incompletely downloaded. How do we get around this issue? Thanks.","[""Pretty good explanation of why it's difficult here: https://stackoverflow.com/questions/47832693/is-there-a-way-with-python-selenium-to-wait-until-all-elements-of-a-page-has-loa \n\nThat being said, you can use ExpectedConditions to wait until a certain html element has loaded or is clickable. Is there any element that fits that use case, something that only appears once the data has loaded?\n\nAlso, is Selenium/browser automation the only option here? Could you recreate the network requests without a browser at all? Or maybe use the browser just to get the required cookies/authentication details and the make the back end requests without the browser?""]"
"Web scraping Etsy for best sellers of an Etsy search term, then their top items?",https://www.reddit.com/r/webscraping/comments/10f3jx9/web_scraping_etsy_for_best_sellers_of_an_etsy/,webscraping,"Is this plausible using JavaScript and how legal is doing so? I‚Äôve been learning web dev for a year now and thought this could be a good idea for a portfolio piece but maybe employers would frown upon this?

I wanna make it for me personally as I do hand tool woodwork and thought it would be useful to have this data and then I thought surely I could program that?",[]
Which course is better for web parsing?,https://www.reddit.com/r/webscraping/comments/10el6kb/which_course_is_better_for_web_parsing/,webscraping,"Hello! I am a complete beginner, who would love to learn about web scraping. Currently, I am deciding between these two courses:
1. https://www.udemy.com/course/web-scraping-in-python-with-beautifulsoup-and-selenium/
2. https://www.udemy.com/course/web-scraping-course-in-python-bs4-selenium-and-scrapy/
I am sort of leaning towards second one, because it has just been updated and it also covers scrapy.
I'd love to hear your opinions or if you can reccommend me a better course.
Have a nice day!","['Obviously the second one because it has a higher rating, plus you say you are leaning towards that one?', 'Neither.  Choose a website and try to parse it one step at a time.']"
How to launch a scrapy spider from a script in a Thread(),https://www.reddit.com/r/webscraping/comments/10enkwt/how_to_launch_a_scrapy_spider_from_a_script_in_a/,webscraping,"Hi evryone,

I am using scrapy and I would like to start my spider from a script without blocking the process while scraping. Basically I have a little GUI with a start button, I don't want the window to be frozen when I press the start button, because I also want a Stop button to be able to interrupt a scrap if needed without terminating the process manually with Ctrl-C .

I tried to thread like this:

    def launch_spider(self, key_word_list, number_of_page):
            spider = SpiderWallpaper()
            process = CrawlerProcess(get_project_settings())
            process.crawl('SpiderWallpaper', keywords = key_word_list, pages = number_of_page)
    // if i use process.start() directly the main process is frozen waiting for the
    // scraping to complete so :
            mythread = Thread(target = process.start)
            mythread.start()
    output:
        Traceback (most recent call last):
      File ""/usr/lib/python3.10/threading.py"", line 1016, in _bootstrap_inner
        self.run()
      File ""/usr/lib/python3.10/threading.py"", line 953, in run
        self._target(*self._args, **self._kwargs)
      File ""/home/***/.local/lib/python3.10/site-packages/scrapy/crawler.py"", line 356, in start
        install_shutdown_handlers(self._signal_shutdown)
      File ""/home/***/.local/lib/python3.10/site-packages/scrapy/utils/ossignal.py"", line 19, in install_shutdown_handlers
        reactor._handleSignals()
      File ""/usr/lib/python3.10/site-packages/twisted/internet/posixbase.py"", line 142, in _handleSignals
        _SignalReactorMixin._handleSignals(self)
      File ""/usr/lib/python3.10/site-packages/twisted/internet/base.py"", line 1282, in _handleSignals
        signal.signal(signal.SIGTERM, reactorBaseSelf.sigTerm)
      File ""/usr/lib/python3.10/signal.py"", line 56, in signal
        handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
    
    ValueError: signal only works in main thread of the main interpreter
    

I saw this [snippet](https://snipplr.com/snippet/67015/revisions) which seems interesting but the import doesn't work with the latest version of scrapy.

Any ideas on how to manage the signal to be used in a subThread ?",[]
need software to scrap some select data from a website and remplace them in my text file,https://www.reddit.com/r/webscraping/comments/10ec8cc/need_software_to_scrap_some_select_data_from_a/,webscraping,"Hey guys, i hope you are doing well, like in title, need software to scrap some select data from a website and remplace them in my text file. is their software which can do this job, for exemple my text file has:

Description:

TITLE:

TYPE:

PLACE:

i need scrapper data which will be based only in that website and will use my custom text file as a template, then it will remplace auto data scrapped from that website and remplace each data in exact place of my text file.

So anytime i give it that URL, it will generate those data. sorry for my bad english","['Hi, I can help you with this, check your dm.', 'You can do this with [MrScraper](https://mrscraper.com). \n\nSimply extract each data into 3 different extractos: title, type and place.\n\nThen use the API, Zapier or any other no-code tool to print this variable in a doc üëç']"
"Between Stable Diffusion and ChatGPT, its clear blocking bots only stop the small guy.",https://www.reddit.com/r/webscraping/comments/10egpxv/between_stable_diffusion_and_chatgpt_its_clear/,webscraping,"We always knew this was the truth, but it doesnt get more 'in your face' than seeing AI art and ideas scraped from websites with anti-scraping technology. 

Its a good thing they defeated the anti-bot, because if they didn't we wouldn't have 2 new and useful technologies.

But why does wealth decide who gets access to public facing data?","[""Are you saying some sites can't be scraped by little guys?"", ""Can you please help me understand how Stable Diffusion and ChatGPT relate to web scraping?  Not saying they are unrelated, I just don't get the connection.  Appreciate if someone can enlighten me here."", 'I see this different - it‚Äôs a resource game, it always was/will be. ChatGPT has very broad use case scenarios, that‚Äôs why they had to put so much effort and resource to make it, which they had. There‚Äôs obvious gap in the technology right now, because to train large amount of data you need powerfull and expensive compute power. \nBut with such a big steps in ML which are revealed everyday, I think that webscraping will change also, and soon it will be really hard to prevent scraper to do it‚Äôs job. Data access was never as important as right now.', ""as with any sort of security it's always an arms race.""]"
bet365 or other identical sites stats to excel,https://www.reddit.com/r/webscraping/comments/10do8vm/bet365_or_other_identical_sites_stats_to_excel/,webscraping,"   I am looking for a site or a way to get live stats from bet365 or  other identical site that has the same stats , i could look into web  scraping but from what i understand, it's illegal to scrap the stats  cause of the terms of violation.

I need a website suggestion that actually does this work with  subscription or not or a site where it accepts web scraping and give  identical stats.  
any help appreciated","['The terms of violation? lol', 'What are you looking to accomplish specifically? What data?', 'hehehee. Webscraping is not ilegal, so long as the data is on the public domain, then its ethically right  to take that data and use it the way you want. I have been scraping Bet365 realtime data for more than 3 years now. \n\nI think they use terms like ""Violation"" to scare  people like you.\n\nWhich starts are you looking for to be specific?']"
Looking for a web scraping tool with export to rss feed,https://www.reddit.com/r/webscraping/comments/10dhi2k/looking_for_a_web_scraping_tool_with_export_to/,webscraping,Is there any web scraping tools that have the ability to export the data as rss feed or send it to discord/other platforms?,"['RSS feeds are just XML (usually, but sometimes they are JSON)\n\nYou could easily create a script (or hire someone off Fiverr, Upwork) that builds the XML structure from the scraped data and then uploads it to a web server somewhere or just a static site hosting platform like Netlify or Vercel. \n\nThen use a service like [IFTTT](https://ifttt.com) to send data to a Discord webhook when a new item is posted to the RSS feed.']"
Scraping google maps for website URL,https://www.reddit.com/r/webscraping/comments/10de4tl/scraping_google_maps_for_website_url/,webscraping,"Hello, I have been trying to scrape google maps for websites along with the name for quite some time now and have had no luck after trying countless solutions. Most things i find will collect some information but nothing I have used yet will copy the website link into the csv file. Has anyone tried to do this and found something that has worked for them? I would love to hear it! Thanks","['Funny enough, I built something to scrape regular google search results and this is how I did it (using python).\n\n    def scrape_google_results_page(html):\n        soup = bs4.BeautifulSoup(html, ""html.parser"")\n        links = soup.findAll(\'a\')\n        results = []\n        for link in links:\n            if link.has_attr(\'href\'):\n                url = link[\'href\']\n                if url.startswith(\'/url?q=\'):\n                    url = url.replace(\'/url?q=\', \'\')\n                    url = url.split(\'&\')[0]\n                    results.append(url)\n        \n        return results\n\nView the full source in the context of the larger script [here](https://github.com/crock/worksauce/blob/8eca606fdd5146e06eb7c93a43c863088dbdcea7/search-result-extractor.py#L32:L44).']"
Hosting web scraped data on vercel?,https://www.reddit.com/r/webscraping/comments/10do4wp/hosting_web_scraped_data_on_vercel/,webscraping,"Hello, I created a web scraper using playwright that scrapes several sneaker sites and puts it into a json format using node for an api. But, I keep getting error 500 function invocation failed. It works fine in local.   


This is what some of my logs look like:

    [GET] /
    10:55:15:55
    Please run the following command to download new browsers:              ‚ïë"",""‚ïë                                                                         ‚ïë"",""‚ïë     npx playwright install                                              ‚ïë"",""‚ïë                                                                         ‚ïë"",""‚ïë <3 Playwright Team                                                      ‚ïë"",""‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"",""    at /var/task/index.js:24:34"",""    at Layer.handle [as handle_request]

Is it not normal to use playwright like this? I understand playwright is usually used for integration testing. Should I be sending the data scraped to a database first and then pull data from there for the api.  Totally new to building my own api , so this is all learning process if someone can point me in the right direction <3.","['hum I don‚Äôt see your code, neither am I a playwright dev, but here are my 2 cents:\n- I don‚Äôt know how long are your scraper is taking to return that information, since this is supposedly an asynchronous process, are you sure your return API code is prepared for that?\n- it‚Äôs not like you cannot use a scraper with an API, but have you considered implement a webhook? just an idea.\n- why not Puppeteer if you‚Äôre already using NodeJS? imho is a more mature framework for scrapping']"
Help with WebScraper.io?,https://www.reddit.com/r/webscraping/comments/10dkyb4/help_with_webscraperio/,webscraping,"Before you start, no, I know nothing about programming or writing code. But is there anyone out there who can help write a sitemap for the WebScraper.io Google Chrome extension? I‚Äôm unable to download anything new on my laptop so I‚Äôm looking for someone who can write the code that I can copy and paste into web scraper io.",[]
Using selenium with proxy still hit bot detection,https://www.reddit.com/r/webscraping/comments/10d0x5r/using_selenium_with_proxy_still_hit_bot_detection/,webscraping,"Trying to scrape a site that uses cloudflare ([axs.com](https://axs.com)) and I keep on getting hit by their bot detection. No captcha or anything just a full block. What's weird is that if I use a full vpn (surfshark) and using another country like belize, I could load the seats properly but if I use selenium then it gets detected.

Is there a list of recommended settings either with firefox or chrome drivers that would imitate a regular browser?","['Try using direct HTTP requests instead of selenium. This is a way better, quicker method.\n\nWith this method, you can use cloudscraper to bypass cloudflare.', 'use undetected chromedriver', 'AXS notoriously difficult to scrape for counts my dude', 'A few things that come to mind and are worth looking into are:\n\n[https://github.com/lwthiker/curl-impersonate](https://github.com/lwthiker/curl-impersonate)\n\n[https://github.com/berstend/puppeteer-extra](https://github.com/berstend/puppeteer-extra)\n\nAnd if not then you need to upgrade your proxies and send the requests via an advanced [web scraping api](https://brightdata.grsm.io/vitariz-unlocker).']"
Mayday! Mayday! Impossible site to webscrape!? REWARD for solution!,https://www.reddit.com/r/webscraping/comments/10d881i/mayday_mayday_impossible_site_to_webscrape_reward/,webscraping,"I have no idea how to scrape [these](https://ibb.co/HFNbw3j) (IV) numbers from [this](https://marketchameleon.com/Overview/IP/IV/) website. It contains stock numbers and I have tried to scrape them off the website for eons. And although I have made advancements I am yet to actually scrape them. I am a noobie at web scraping and coding overall but maybe you guys know how to obtain them. I am coding in python >3 and I have figure that the site is ajax-based since it wouldn‚Äôt work with a me want me get requests. PS A loggin shouldn‚Äôt be required since the numbers still show up in inspect although not being logged in.

[This](https://textdoc.co/pnuygOileDQf6YV1) is the script I am using at the moment. But it wouldn‚Äôt give me the numbers.

I have googled and apparently using a chrome driver should fix the Ajax problem. Therefore I have tried using [this](https://textdoc.co/BoglTirm1SA3Mt5R) script. But it doesn't work for whatever reason. And I have no clue how to fix it.

I‚Äôll reward you who finds a python code that can obtain these numbers for me and put them in a list or string. Have a great new year everybody!!!!","[""The script you're refering to at [https://file.io/DfRNXlJsVfbb](https://file.io/DfRNXlJsVfbb) has been deleted. Also you need an account to scrape the site which I don't have (and yes there's a 7 day trial but not looking forward to using that). Your own python script has also been deleted from https://file.io/J2azbyhI8ElM."", 'First off, Chrome\'s ""inspect element"" shows a mix of server-rendered HTML + client-rendered HTML. If you want to view just what comes from the server, right click on the page and choose ""View Page Source"". That\'s what your scraper is getting and with the modern web, that\'s what most sites send to the data and then they hydrate the page client-side, which is why you are having trouble with scraping the data you want.\n\nOf course, there are ways. Mainly using headless browsers (phantomjs) or browser automation tools like Selenium.', ""What's your budget?"", ""By the way, nothing is impossible... üòé\n\nNote: maybe it's not worth it (too expensive, too much effort, etc.)""]"
How hard is it to scrape that?,https://www.reddit.com/r/webscraping/comments/10cyco7/how_hard_is_it_to_scrape_that/,webscraping,"Hello guys! I'm very new here, so excuse me if my question is stupid, but I'm trying to figure out how hard it is to scrape the following data and send it to something like google sheets.

I have a WooCommerce website, and I need to extract a specific page from the WooCommerce Analytics tab on an hourly/daily basis. [Here is a link](https://imgur.com/UHthsj3)

P.S. (Using the native WooCommerce API is NOT an option)

Thanks!","[""As you say you can't use the API and considering you'll need to log into the backend, perhaps look into using selenium with a Python script and running it periodically.""]"
"Octoparse pagination ""next page""",https://www.reddit.com/r/webscraping/comments/10cy6ml/octoparse_pagination_next_page/,webscraping,"hello, i'm trying to make octoparse click on ""next page"" to continue the scraping. I tried to create a loop with pagination but it never works... the result is that the first page is correctly scraped, but the second, third, fourth, etc. are not scraped et the first page continue to extract duplicated datas.

someone knows where the problem comes from ?",[]
Javascript,https://www.reddit.com/r/webscraping/comments/10d1n4g/javascript/,webscraping,"Hey, got a page that one hit returns some js that then needs to be actioned to submit to the final end point page. 

Other than selenium what other options do u have, the faster and more robust the better. Unsure how I could use curl for somthing like this but open to all and any ideas.",[]
can someone guide me about how to setup scrapy project,https://www.reddit.com/r/webscraping/comments/10cm3ox/can_someone_guide_me_about_how_to_setup_scrapy/,webscraping,"I am starting a project where I have to scrape around 50 to 60 news website for live and current news. I am currently making individual scripts for now as all the websites have different tags for their article information like headline, date and article body. How do I go about setting a single project where all the scripts will run simultaneously, and factors that I should keep in mind. 

Like - As all websits have different tags, I don't think item loaders will be of help as I have to consider all the scenario where I have to perform a check on all tags related to that information i am about to extract.

I am fairly new at scrapy so any idea about how I would  go about it would be appreciated.",['Dm me i could help you. No paying by the way.']
What database do you guys use to store data? And why?,https://www.reddit.com/r/webscraping/comments/10c3uq1/what_database_do_you_guys_use_to_store_data_and/,webscraping,Bonus question: What database do other data aggregating businesses use?,"[""SQLite, because it doesn't require hosting a server. Also easy to backup the database file."", 'SQL for data that is used in analytics and MongoDB for ""record keeping""', ""Google BigQuery, usually. The data is easy to share, export, make reports from and query, and the Business Intelligence dept of most companies already have accounts.\n\nIt's also priced only per Terrabyte queried, so no ongoing server cost when the DB is not used. This makes it practically zero-cost for most projects, unless data sizes are enormous (may even fit into the free tier in many cases)"", 'Notepad++\n\nEdit:  Forgot to say why.', 'Json, readable easy to modify and user friendly', 'I do very small hobby projects so a .csv file does just fine.', 'Mostly csv or txt files, cuz my projects are relatively small scale.', 'Firestore, but looking into Dynamo. Looks promising', 'I usually use feather or parquet files']"
Yellowpage scraper.,https://www.reddit.com/r/webscraping/comments/10cbygu/yellowpage_scraper/,webscraping,"Hey everyone,

I've been working on a web scraping project using Python to extract data from [YellowPage](https://github.com/sushil-rgb/YellowPage-scraper) and I wanted to share my experience and some tips for anyone else looking to do the same.

First, I used the popular library BeautifulSoup and Playwright to navigate and automate the website. The script asks user to enter a business name, location and number of pages to scrape and save it into an excel database accordingly. It extracts all the necessary data including emails as well. I feel I used lots of try and except clause, if someone has better approach them please free to share.

Another thing to watch out for is that the website structure can change frequently, so it's important to regularly check and update your code accordingly.

Overall, it was a fun and challenging project that taught me a lot about web scraping and working with dynamic websites.

Let me know if you have any questions or tips of your own to share!","['This is pretty awesome. I am also working on a similar project.', ""That's great\n\nWere you able to extract all the emails from the sites\n\nI have a project at hand too using selenium python library\n\nI'm having 80% success with several try clause as well"", ""I don't think I can switch from selenium at least for now"", 'Why did you use Playwright? requests would have worked fine as this page is not being rendered by JS in the browser.  \nSee how you can [wrap try except in a decorator](https://stackoverflow.com/questions/15572288/general-decorator-to-wrap-try-except-in-python).', ""This is really cool!\nI've been looking for ways to improve my webscraping skills, so I guess I'll try something similar."", ""Since I switched from beautiful soup and I've been working with selenium and I found it more interesting perhaps because I've not tried playwright"", 'Thank you for sharing. Helps me on my webscraping journey. I just learning to program. How can you tell which programs to use for which sites?', ""have an exact project using scrapy and mysql. word to the wise their css node names don't change often"", 'Why are you tracking your virtual environment? You can ignore it since it can be easily created.']"
I want to make a demo web scrapper,https://www.reddit.com/r/webscraping/comments/10c0qjs/i_want_to_make_a_demo_web_scrapper/,webscraping,"Which scrapes live score from site like espn Cricinfo and updates live score on my web app.
I never scraped a live website before so your  suggestions or any resources will be helpful üòÅ

Thanknging in anticipation.","['What programming languages can you program in? Python? Java? JavaScript?\n\nIf you know Python, then do the scrapy tutorial:\nhttps://docs.scrapy.org/en/latest/intro/tutorial.html\n\nIf you know any other language, google for a Selenium tutorial in that language.\n\nIf you don‚Äôt know any programming language, then first learn Python until you‚Äôre able to do that scrapy tutorial.\n\nIf you want to learn more about what scraping entails, check out some YouTube videos by John Watson Rooney:\nhttps://youtube.com/@JohnWatsonRooney\n\nI recommend this one as a starter:\nhttps://youtu.be/nCuPv3tf2Hg\n\nGood luck with your learning!']"
Where can I find a mentor?,https://www.reddit.com/r/webscraping/comments/10blltx/where_can_i_find_a_mentor/,webscraping,"I find myself hitting a ceiling a lot and I don't know where to go next in order to become more advanced. Currently, I'm maybe at the intermediate level with an elementary knowledge of things such as browser fingerprinting, bypassing bot protection, captcha management, networking, and general web development.

What I'm really struggling with is not knowing what I don't know and not knowing how to find out what I don't know. Instinctually, I wanted to learn networking and back-end development at a deeper level as I believe that would at least give me an idea of what sort of information I'm sending to a website and what a website can do with that information. Then be able to reverse engineer that information somehow to at least give myself a better chance. But then again, it goes back to not knowing what I don't know.

Either way, any pointers would be greatly appreciated. I have a lot of really good ideas for projects that keep being put on hold because of the web scraping hangups so now is the time for me to really step up and put my money where my mouth is.","['For some additional ideas, check out this list of ways to level up your scraping game I made last month:\n\nhttps://reddit.com/r/webscraping/comments/zf9a4v/_/izavkfw/?context=1', 'Describe one of these hang ups', 'have you tried the plugin Stealth with puppeteer? there‚Äôs a huge NodeJS/Puppeteer community creating new amazing plugins and sharing snippets that you may be missing, most of people here tend to use scrappy/pyrhon though, so unless you‚Äôre considering change your setup choice(which I do not recommend cause Puppeteer is an excellent tool for automation/scrapping) I sugest to basically ignore them and look elsewhere (no offense python users, both environments are decent, is just a matter of choice).\n\nI worked exclusively with Node/Puppeteer for almost 2 years, Freelancing at Fivver, never was unable to do what I needed and my only limitations where my inability to see the real problem from time to time + limited time to try different approaches.\n\nWhat you said about learning how to backend is kinda valid, not really a must, but does help, my personal style is actually the opposite of mimicking a user(which is valid sometimes) I avoid direct interaction(clicks) as much as possible, nowadays we have many many websites written with angular, react, svelt and similars, I love those because I can just intercept their data in the background and without a single interaction you could just harvest so much information‚Ä¶ another thing you can do is, you can find a function that makes a request you want, lets call it function X, you inspect it, copy that guy, slightly change it to a format that works for you and you can write it in your main page, sometimes overwriting the original one, or just creating another version of it, that you can call it and harvest content‚Ä¶ you have a fricking browser on your hands theres so much you can do‚Ä¶ feel free to reach me if you have questions brother!', 'You said it yourself, learn a backend.... Or even better, learn a few. \n\nI recommend Pluralsight as a learning platform.', 'Everyone has trouble with blocking.  If you read around this sub you\'ll see it\'s less about understanding every aspect of how you might be getting blocked and more about finding the right software/config combination to overcome the blocks with this site or that site.  There\'s no exact science to web scraping and the techniques used to fingerprint you are always evolving.  There are third party services that people use to protect their websites whose only real job is to constantly find new ways to identify bots and headless browsers.  You don\'t want to be the one reverse engineering things.  Getting around blocks is always a ""hack"" and so it\'s going to always differ between sites.']"
cost of webscraping,https://www.reddit.com/r/webscraping/comments/10c1n65/cost_of_webscraping/,webscraping,"I've been trying to scrape product pages from Amazon with selenium.  I've been having connection issues, which makes me think I need to use proxies.   I started looking at the cost, and I'm seeing roughly $10 per gb for rotating residential proxies.      I'm not sure of the exact size but let's say an Amazon page is 2mb. That'd mean it'd cost me $20 for every 1,000 Amazon pages I get the info from.   Am I missing something?  It seems like webscraping is insanely expensive","['You only need residential proxies for the most protected sites. You can definitely use datacenter proxies for Amazon, which are much cheaper.\n\nAlso, there are cheaper residential providers, like [https://packetstream.io/](https://packetstream.io/) is just $1/GB - although you get a much smaller pool and much higher error rates compared to premium providers.', 'No way any page is 2MB of source code.  It is expensive though.', 'If you move away from Selenium and just use a HTTP client like Python Requests then you will be able to scrape at a fraction of the price. Using headless browsers drive up you proxy bills (if paying per GB) and require you to use higher-performing servers.']"
"How to use ""for"" to ""loop"" multiple urls ?",https://www.reddit.com/r/webscraping/comments/10bz75l/how_to_use_for_to_loop_multiple_urls/,webscraping,"The current code is perfect to scrape the information for only one Url, and i would like to be able to scrape from multiple urls at once ( maybe use For url in Urls ) , i would like to add this ( URL : [https://www.6pm.com/p/gbg-los-angeles-ayvie-slate/product/9479982/color/642?zlfid=192&ref=pd\_detail\_1\_sims\_cv](https://www.6pm.com/p/gbg-los-angeles-ayvie-slate/product/9479982/color/642?zlfid=192&ref=pd_detail_1_sims_cv)). Here is the current code for just one url below. Please any help or direction would be appreciated

 

`import datetime`  
 `from bs4 import BeautifulSoup`  
 `import requests`

`def get_url_data_from_url_request(url):`  
  `print("">> get_url_data_from_url_request: ""+str(url))`

   `url_data = None`  
  `headers = {""user-agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML,`   
 `like Gecko) Chrome/90.0.4430.93 Safari/537.36""}`  
  `s = requests.session()`  
  `s.keep_alive = False`  
  `request = s.get(url, proxies=None, headers=headers)`  
  `print(""request.status_code: "", request.status_code )`  
  `url_data = request.text`  
  `request.connection.close()`  
  `s.close()`  
  `return url_data`

`def main():`  
  `print(""bdr.sandbox"")`  
  `generated_on = datetime.datetime.now()`  
  `print(generated_on)`  
  `source_product_url = ""`[`https://www.6pm.com/p/easy-spirit-epic-gray/product/9450972/color/11`](https://www.6pm.com/p/easy-spirit-epic-gray/product/9450972/color/11)`""`  
  `url_data = get_url_data_from_url_request(url=source_product_url)`  
  `soup = BeautifulSoup(url_data, ""lxml"")`  
  `id_element = soup.find('span', {""itemprop"": ""sku""}).text`  
  `print(id_element)`

`if __name__ == '__main__':`  
 `main()`","['Does this code make sense?\n\n\n    urls = [""https://www.6pm.com/p/gbg-los-angeles-ayvie-slate/product/9479982/color/642?zlfid=192&ref=pd_detail_1_sims_cv"", ""https://www.6pm.com/p/easy-spirit-epic-gray/product/9450972/color/11""]\n\n    results = []\n\n    for each_url in urls:\n        url_data = get_url_data_from_url_request(url=each_url)\n        soup = BeautifulSoup(url_data, ""lxml"")  \n        id_element = soup.find(\'span\', {""itemprop"": ""sku""}).text\n        results.append(id_element)\n\n    print(results)']"
401/403 Error But I Can Access the Site (Java),https://www.reddit.com/r/webscraping/comments/10brqr1/401403_error_but_i_can_access_the_site_java/,webscraping,"I'm pretty new to this and am trying to take info off [https://core-api.prod.blur.io/v1/collections/azuki/executable-bids?filters=%7B%7D](https://core-api.prod.blur.io/v1/collections/azuki/executable-bids?filters=%7B%7D) but my code throws me a 401/403 error. I can view it on chrome so I'm confused.  


Heres what I have:

URL url = new URL(""https://core-api.prod.blur.io/v1/collections/azuki/executable-bids?filters=%7B%7D"");  
 HttpURLConnection con = (HttpURLConnection) url.openConnection();  
 con.setRequestProperty(""User-Agent"", ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36"");  
 con.setRequestMethod(""GET"");  
 con.connect();",[]
Where to start with webscraping ?,https://www.reddit.com/r/webscraping/comments/10agkmn/where_to_start_with_webscraping/,webscraping,"So far I only have experience with webscraping by using regex, which works nice. I googled a bit during the past days and it seems that DOM with javascript are popular. I don't know what the best way is but I like to know more about webscraping. What tutorials, books, etc.. would you recommend a newbie ?","[""I wrote several in-depth introductions in various languages:\n- [Python](https://scrapfly.io/blog/web-scraping-with-python/) (Python is recommended for web scraping as it's a very strong data language)  \n- [Javascript (NodeJS)](https://scrapfly.io/blog/web-scraping-with-nodejs/)  \n- [R](https://scrapfly.io/blog/web-scraping-with-r/)  \n- [PHP](https://scrapfly.io/blog/web-scraping-with-php-101/)  \n- [Ruby](https://scrapfly.io/blog/web-scraping-with-ruby/)\n\nThough here's a quick primer: all you need to scrape the web is a way to retrieve the web content (HTTP client or a web browser) and a way to parse it (HTML or JSON parser). Most programming languages has these two things!\n\nHTML parsing is a bit complicated, but HTML is a machine-readable format that represents a tree of elements:\n\n```html\n<head>\n  <title>hello world</title>\n</head>\n<body>\n  <h1>Product Name</h1>\n</body>\n```\nJust from indentation you can see the tree structure of this format and imagine how would you navigate to a particular element like `h1` -> `body->h1`.\n\n This is great for parsing as you can rely on strong, predictable structure to find the data instead of using regex where you rely on unpredictable text patterns (easy to break). To parse HTML there are 2 popular standards: \n- [CSS selectors](https://scrapfly.io/blog/parsing-html-with-css/) that's the stuff you use to select elements to apply CSS styles for (e.g. `.product>.title{color:blue}`)\n- [XPath selectors](https://scrapfly.io/blog/parsing-html-with-xpath/) similar to CSS selectors but more powerful. Were invented to parse XML documents when that was the dominant data format (compared to today's JSON)\n\nThe other hard part is blocking. HTTP is a very straight-forward protocol. You ask for something and you get it - a done deal! The challenge arises when you don't get a response because you look unusual (like a bot) or missing some parameters or tokens.\n\n---\n\nFor actual learning, it's best to pick a real practical project, but since blocking is such a daunting subject, you should probably choose a smaller target that is less likely to block. In other words, don't scrape Instagram or Google or any major scraping target for now."", 'What programming languages can you program in? Python? Java? JavaScript?\n\nIf you know Python, then do the scrapy tutorial:\nhttps://docs.scrapy.org/en/latest/intro/tutorial.html\n\nIf you know any other language, google for a Selenium tutorial in that language.\n\nIf you don‚Äôt know any programming language, then first learn Python until you‚Äôre able to do that scrapy tutorial.\n\nIf you want to learn more about what scraping entails, check out some YouTube videos by John Watson Rooney:\nhttps://youtube.com/@JohnWatsonRooney\n\nI recommend this one as a starter:\nhttps://youtu.be/nCuPv3tf2Hg\n\nGood luck with your learning!', 'I would recommend to learn/practice following:\n‚Ä¢ Python\n‚Ä¢ Python libraries: request, tqdm, json, pandas, regex(u know this already), bs4, csv\n‚Ä¢ Microsoft Excel\n‚Ä¢ Knowing JS vanilla and modern JS is huge plus\n\n\nLearn more about how API/requests work via browser inspector and it will ease ur scraping process/development.\n\nEdit: avoid Selenium. Thank me later']"
Checking Marketplaces for new products?,https://www.reddit.com/r/webscraping/comments/10aq27y/checking_marketplaces_for_new_products/,webscraping,"Hello - is there any chance to check for a specific search on marketplaces eg.

[https://www.facebook.com/marketplace/104047599631322/search?query=omnipods](https://www.facebook.com/marketplace/104047599631322/search?query=omnipods)

So it returns allways the new places products?  
And eg. checking this every 5-10 minutes?  
Probably this can¬¥t be really done manually - but probably is there also some services in places for stuff like this?","['No', 'You could try some of the point and click services or browser extensions, there are plenty out there so just search Google and try a few of them to see which one works best for your use case (and it‚Äôs r too expensive to run)']"
Is it possible to create a script like this?,https://www.reddit.com/r/webscraping/comments/10aixb5/is_it_possible_to_create_a_script_like_this/,webscraping,"[Mountainproject.com](https://Mountainproject.com) has data for 279,958 climbing routes. When searching for routes, you can modify based on location, climbing type, difficulty rating (as a range), and quality rating. At the results page, it gives you the option to export the first 1,000 results into an excel sheet.  


I'm interested in getting all of the route data together, but in order to do that I would need to create at least 280 separate results files, each with no more than 1,000 rows. I want to create a ""results checker"" script that runs an array of searches and reports the results for each one. For example, there are 36 different options for climbing difficult grades. I could have the script run 36 times and spit out how many search results there are for each grade. From there, I could get a sense of which grades I would have to further subdivide. The lowest difficulty grade only has 277 routes, so that would work, but there are quite a few with over 1,000 results.  


Is it possible to have a script do something like that?","['Yeah that sounds fairly reasonable.', ""I would check to see if the data is available for download anywhere maybe send an email and ask I mean it don't hurt to ask just it don't hurt saves ya alot of work unless your just doing this for practice then have at it""]"
Simple Scraper worked for me before but now it seems to be driving me mad....,https://www.reddit.com/r/webscraping/comments/10afnj7/simple_scraper_worked_for_me_before_but_now_it/,webscraping,"I'm trying to scrap this page ([https://elgl.org/celebrate-90-of-elgls-top-100-local-government-influencers-for-2022/](https://elgl.org/celebrate-90-of-elgls-top-100-local-government-influencers-for-2022/)) to create a base in Airtable. Simple Scraper will let me select all ""Names"" and ""LinkedIn"" links however it won't select all photos, bios, or locations. Do anyone have a suggestion as to what I'm doing wrong or perhaps an alternative? Thank you in advance for your time.","[' ```\nfrom requests_html import HTMLSession\nfrom bs4 import BeautifulSoup\nimport re\n\nsession =\tHTMLSession()\n\n\nurl = ""https://elgl.org/celebrate-90-of-elgls-top-100-local-government-influencers-for-2022/""\n\ndef get_bs(strObj):\n\treturn BeautifulSoup(strObj,\'html.parser\')\n\ndef get_details(pTags):\n\tdata=[]\n\tfor tag in pTags[:-1]:\n\t\tpText =tag.get_text()\n\t\tif \'|\' in pText and not(tag.find(\'a\')):\n\t\t\tdata.append(pText)\n\tif len(data)==1:\n\t\treturn data+[\' \']\n\treturn data\n\t\t\n\npage =\tsession.get(url)\nprint(page.status_code)\n\nbsObj =\tBeautifulSoup(page.text,\'html.parser\')\nprint(bsObj.title)\n\ncontentDiv= bsObj.find(\'div\',{\'class\':\'entry-content\'})\ncelebDetails =str(contentDiv).split(\'<hr/>\')[1:]\n\n\nfor content in celebDetails:\n\tcontentSoup =\tget_bs(content)\n\timg=contentSoup.find(\'img\')\n\tname =\tcontentSoup.find(\'strong\').get_text()\n\tlinks   =contentSoup.findAll(\'a\')\n\tpTags =contentSoup.findAll(\'p\')\n\toccupations,quality=get_details(pTags)\n\tbio=pTags[-1].get_text()\n\tprint(name)\n\tprint(img)\n\tprint(links)\n\tprint(bio)\n\n ```\n\ntry this. It is not best solution so If you find a better one let me know']"
You need to verify if you're a person or a bot when using googles advanced parameters.,https://www.reddit.com/r/webscraping/comments/10agca5/you_need_to_verify_if_youre_a_person_or_a_bot/,webscraping,"I'm writing a movieprogram using webrequests and googles advanced parameters are very handy for this.

For example, you can't search imdb's lists but when you write i.e.:

inurl:https://www.imdb.com/list/ intitle:""heist""

You find plenty of heist lists.
But googles constant verification makes it impossible to use, is there a way to circumvent the verification procedure ?",['duckduckgo.']
Can I be IP banned from a website if I try to scrape it?,https://www.reddit.com/r/webscraping/comments/109b2yg/can_i_be_ip_banned_from_a_website_if_i_try_to/,webscraping,"Basically I'm using Vinted (second hand selling platform) to sell some clothes, I also want to create simple scraping tools to understand better the price of items similar to mine and to get some statistics that could help me in my activity.  


I'm scared that if I start doing tests Vinted might block me somehow (ip ban or else), which would be terrible since I have been working on my account for a year now (I have more than 150 reviews).  


I know there are a lot of approaches to avoid being blocked (IP rotation, proxies, changing user agents..) but I haven't tested anything yet because of this fear.  


I know that complex things are easy once you have the whole picture, so I'm wondering if anyone with that level of knowledge could assist me on that.  


Thank you in advance :)","['Yes, it is definitely possible to get IP banned from scraping. Even when doing your test scripts, I recommend using a proxy that doesn‚Äôt leak.', 'You might check for a robots.txt file, if present it might list a query interval. For testing and small jobs I typically just set the delay to 3 or 4 seconds per request,1.5-2.5 was getting me blocked from certain sources', 'If you are only scraping at small volumes then you could just use the free plans of some of the paid proxy providers. Some have free plans with 10,000 free requests per month. [Check out this list.](https://scrapeops.io/proxy-providers/comparison/free-proxy-providers)', 'Try creating a throw away account and run code from that account.', 'They‚Äôre really easy to scrape. I‚Äôve built one. Use aws lamda and you‚Äôll be fine', 'If you are afraid of getting ip banned you can always try a VPN', 'yes very common', 'yes', 'Honestly, what content does Vinted have that is worth scraping?']"
Looking to scrape all the svg files on the internet (or at least a lot of them). Anyone has any thoughts on how to do that?,https://www.reddit.com/r/webscraping/comments/109lqmo/looking_to_scrape_all_the_svg_files_on_the/,webscraping,"It‚Äôs somewhat straight forward to land on a page and identify all the places where a url points to an svg or an svg tag appears explicitly in the html. 

The piece I‚Äôm struggling with might be super trivial to more accomplished scrapers out here: how do I find a seed list of reasonably professional websites to scrape (assuming professional websites have higher quality svgs generally). 

And also probably a basic question: given the landing page of a domain, it‚Äôs there any simple way to determine all the pages on that domain so I can scrape it?

I‚Äôm pretty handy with selenium but the basics of how to do mass scraping escapes me.",[]
Scrape Instagram posts,https://www.reddit.com/r/webscraping/comments/1093gcr/scrape_instagram_posts/,webscraping,"When you `add ?__a=1&__d=dis` to an Instagram post URL like this `https://www.instagram.com/p/CnEl_HyM0_H/?__a=1&__d=dis` you get a lot of JSON data returned with info on an Instagram post. This even works in incognito mode when logged out. 

However when I try to access this using node.js on a render.com server I get:

    {""message"":""Please wait a few minutes before trying again."",""require_login"":true,""status"":""fail""} 

I get the same response when accessing the page throug a TOR browser. Any idea how I could fix this?","['Even in incognito mode, your browser can add a guest id in the request header.   \nHave you tried to pass the header from your browser in your node.js script ?', ""It's because datacenter IPs (and definitely Tor exit node IPs) are blocked by default. You will need at least residential IPs for scraping Instagram.\n\nBTW, I wrote a blog post on [scraping Instagram](https://scrapingfish.com/blog/scraping-instagram) some time ago, you might be able to borrow some code.""]"
Is this a fair price for web scraping?,https://www.reddit.com/r/webscraping/comments/1097o1y/is_this_a_fair_price_for_web_scraping/,webscraping,"Hi all,

After messing around with Github and realizing I know absolutely nothing about Python, I'm thinking about commissioning someone to do a project using Python script to scrape the Jeopardy archive at [J-archive.com](https://J-archive.com) into CVS files using 6 fields (air date, round, category, value, answer and question). I wouldn't be looking to scrape the entire archive, but instead from the year 2020 to the current date. Here's a few examples of parsers that perform(ed) this job featured on Github repositories: [https://github.com/whymarrh/jeopardy-parser](https://github.com/whymarrh/jeopardy-parser)

[https://github.com/jbovee/j-archive-parser](https://github.com/jbovee/j-archive-parser)

Knowing nothing about Python, I have been unable to get those parsers to work for me, so I took to Fiverr in order to get a quote on how much it would cost to have someone do this. After chatting with the guy for a while he said he could do it and asked me what my budget was, which I found a little curious because I was assuming he was going to quote me at least some kind of rough price and then we'd go from there. When I responded that I didn't come in with a set idea for a budget and instead am looking for someone who can perform the project and give me an estimate on a price he came back with $130. When I told him to let me do some more research and think about it he immediately dropped the price to $90, which is the point where I started to get car salesman vibes.

I don't know what a fair price for a project like this is, and I'm certainly not disputing that this price may very well be fair, and obviously I want this deal to be good for both sides, but how we got to that figure made me think I better at least go to Reddit and ask.

Also, is Fiverr OK to be going to for a project like this? The last thing I'd want to do is drop $100+ on a project and not have it work as intended.","[""well you opened negotiations by saying you don't have a set budget, don't be surprised he started negotiating"", ""Pricing varies widely. I think you could get it done for less, but I don't think he's ripping you off. Scraping is relatively simple and can be done anywhere in the world, so it doesn't cost much. It usually only gets expensive if you want to scrape difficult sites routinely. You have to pay significantly more for guaranteed success month after month.  \n\n\nYou should only pay him for his work after he has provided a preview of the data."", 'when you are web scraping for a living, sometimes you assume projects that turn out to be completely different from your expectations when estimated it‚Ä¶ sometimes you‚Äôll get lucky, but most of time there‚Äôs always be an risk of getting screwed with extra work‚Ä¶ so yes imho he‚Äôs not ripping you off, mostly he‚Äôs just desperate to get something to work with', 'DM‚Äôd you a cheaper alternative', 'Im like web scrapig, this page is a little easy for get the data with web scraping. check the `table` in http for get all data of ech page and the link page only change one number.\n\nBut this is the work of the guy. The cost depend of time for maked the script.']"
Scraping embedded Google Slides?,https://www.reddit.com/r/webscraping/comments/1095z54/scraping_embedded_google_slides/,webscraping,"There's a website with over 40 embedded Google Slides (download, print disabled). Are there any tools to scrape or automate taking screenshots of the Google Slides?",[]
StaleElementReferenceException,https://www.reddit.com/r/webscraping/comments/1090134/staleelementreferenceexception/,webscraping,"im using python3 selenium with firefox.

how can i avoid this particular exception? it seems like everytime i try to do a loop over elements i get this exception sooner or later.

&#x200B;

    data_and_resources_ul = driver.find_element(By.CLASS_NAME,'resource-list')
    csv_or_tsv_total = data_and_resources_ul.find_elements(By.CLASS_NAME,'format-label')
    for csv_or_tsv in csv_or_tsv_total:
            csv_or_tsv.click()
            time.sleep(1)
            navbar = driver.find_element(By.CLASS_NAME,'tabs--primary')
            all_buttons = navbar.find_elements(By.TAG_NAME,'a')
            back_to_dataset = [a for a in all_buttons if a.get_attribute('href').endswith('dataset')]
            back_to_dataset[0].click()
            time.sleep(1)

this is my code csv\_or\_tsv.click() takes me to a new page so new url then i will add more code there after that i press back\_to\_dataset\[0\].click() takes me to the previous page where csv\_or\_tsv\_total exists.

so my for loop should not fail because the same elements where gathered in the first place when i was on that page initially.

the for loop crashes on second iteration with StaleElementReferenceException

how can i fix this?",['Try finding the element again. Treat it as though the element has changed. So you have to grab it again.']
Online services to run selenium scripts with GUI,https://www.reddit.com/r/webscraping/comments/108z1gj/online_services_to_run_selenium_scripts_with_gui/,webscraping,"Hi everyone!
I am working for a client and he asked me to scrape Craigslist. I made the script in selenium Python. Now he wants to run that script on some online server/cloud service. But I do not have any experience in that.
I tried digital ocean droplet with Ubuntu OS but it did not work and everytime I start the script it gives some error. 
Actually the script does not have headless web driver and the droplet is purely command line. I tried installing GUI libraries but many of them Did not work or they were very slow.

So is there any other online service with Windows OS or any other with GUI?","['Here is the best way to go about it IMO:\n1. Get a Linux server, standard Ubuntu server ( [here](https://m.youtube.com/watch?v=NKc3k7xceT8) is a tutorial for an always free cloud machine)\n2. Install docker on the machine ([link](https://docs.docker.com/engine/install/ubuntu/))\n3. Run the selenium/standalone-chrome image\n4. Run your selenium script\n[Here](https://stackoverflow.com/questions/45323271/how-to-run-selenium-with-chrome-in-docker) is a stackoverflow question that can guide you.\n\nIf you need to schedule the script (run it every day, look into cron jobs).\n\nIf you have any questions I can help you out.', ""Why do you need windows? \n\nAnyway i ran selenium scripts in the past using heroku. They have some sort of plugin system, it allows you to install selenium\n\nEdit: I found some tutorial for digitalocean + selenium just now, so i'm pretty sure it can be done on there too""]"
"What do these Fiverr ( Indian , Pakistan etc) Guys do for scraping any different than Google Search?",https://www.reddit.com/r/webscraping/comments/1091mnd/what_do_these_fiverr_indian_pakistan_etc_guys_do/,webscraping,"I've asked some guys to go to some webpages and scrape some companies leads for me.   
They say ' I can't see the data' on that page.   
What basically comes down to, they do what anyone can do and they just put it on excel file.  
I ask them 'what can you find as results that I can't find on Google Map or Google Search', none of them could give me a sharp answer.  


They call it scraping and all fancy words but they basically google search and go through first 1-2 pages and get information  that way.  


How's that scraping? How's that different than 'google search normally' ?","[""Scraping is just collecting data that is available on the internet, usually in an automated way.\n\nAutomation is just to save time, it would take months to go through, say 100k pages manually, someone can automate that in a few hours potentially.\n\nIt's not magic though that can get information that is not available otherwise."", '[deleted]', 'what most of those lead services provide are webcrawlers that normally will fillow a start point ‚Äúgoogle‚Äù and after that they will read websites returned and will try to scrape content from them in a certain depth, I will adsume that none of them are going as far as you want, because they are not intent to do that in first place what you are describing is a little more specific than what it is available in your regular Fivveer freelancer.\n\nI used to freelance over there 2 years ago and I am trying to start over again, but speaking for my personal perspective(and no offense intended) I never did nor intent to do webcrawlers because costumers looking for those are always trying to squeeze every single peny they are expending for those leads they are sp desperately looking for, with no regards about how hard it is and how much tine did it took you to write it down or how low are they dropping the ball on their budget.\n\nWebcrawlers are amazing, I got myself wanting to write a solution alike for many many years, but I always stoped because when you step back and look at the big picture, you just realize that they are normally the trashiest services on this particula field.', ""What data are you trying to get and what are they providing?  It is very unclear from your post.\n\nI wouldn't expect anything totally revolutionary for 5 bucks.""]"
What would you like to see in a free scraping service?,https://www.reddit.com/r/webscraping/comments/108krfw/what_would_you_like_to_see_in_a_free_scraping/,webscraping,"Hello Everyone! I am running a scraping startup.

I think most services are ridiculously expensive and I would really like to give back to the community by offering a free tier that would suit most or even all personal needs.

So my question to you guys is how many requests per month would you like to see in such a service for free? Please be honest and realistic. I would love to give everyone milions of requests per month for free, but there are significant costs associated with running such an infrastructure.

What kind of features would you like such a service to have?

Currently I offer 2 main functionalities:

* Scraping API. It returns data from a target URL using proxies either purely as HTML or scraping specific fields and returning them as JSON
* Crawling. In the user panel users can configure scraping profiles to be used for different web pages with an UI that allows you to interactively click on elements of the target page to be selected for scraping. Full website crawling is configurable by creating a crawling agent which consists of list of URL patterns which should either be followed and/or scraped using an extraction profile. All this can be run as a job resulting in a JSON or CSV file with rows populated from all scraped pages.

The bottom line is I want to understand what you would like to see in a free tier of such a service and I will try my best to meet it.

Thanks for your time!","['In my case, I have a list of company names and only need the company url', 'for functionality‚Äôs I would say the same level you can find at PhantomBuster‚Ä¶ what they have there is quite efficient', 'For non tech users a nice UI where they enter in their desired fields easily (say comments from a page), a decent number of API calls, or built in cookie deflections when needed. If you could scrape TikTok comments that would be ü§Ø\U0001fae1', 'The ability to export data to rss feed or other application like discord/telegram']"
Need a recommendation for good residential proxy services provider,https://www.reddit.com/r/webscraping/comments/108hys9/need_a_recommendation_for_good_residential_proxy/,webscraping," 

Hi,

I'm working on a small project for personal use (nothing commercial) and I want to try residential proxies to avoid getting CAPTCHA challenges.  
I want to buy a few GB for a trusted/premium provider cause I need those proxies to work at the money time.  
I've contacted Oxylabs but they won't access my url (although my project is 100% legit).  
I also see that this kind of URLs are also banned in Smartproxy, both of the providers have an extensive list of urls that they won't serve.  
Few questions to the experienced folks here:

  
1. Can any of you recommend a legit/trusted provider which has a pay-as-you-go plan and will sell few GBs at a reasonable price (10-20 $ / GB) and doesn't have that strict compliance   
and won't let me down with low quality addresses ?  
2. What do you think of Packetstream.io ? I see a lot of complains about people that share their bandwidth getting paid for the traffic they share, but what do you think about the IPs they provide to their clients ? I'm sure they aren't on par with Oxylabs but for me as a client, will it be throwing away 50$ ?  
Thanks for your words of wisdom","['May I suggest a slightly different approach and not answer your question directly?  \n\n\nIf it is for a personal project I presume you wont generate an insane amount of traffic.\n\nYou can deploy your own residential proxy using a Raspberry Pi for example and a 4G stick + sim.  \n\n\n1. Install Ubuntu\n2. Install ZeroTier or TailScale peer-to-peer VPN (they are free for personal use). This will allow you to access the Pi using its VPN IP from another computer on the same VPN network.\n3. Install the 4G stick\n4. SSH using the VPN IP to verify everything is working.\n5. Install an HTTP proxy service. There are plenty of choices which you can install with a single command\n\nDone. You have your own residential mobile network proxy. This is actually the most expensive type of proxy on the market.\n\nHint:  \nSome carriers allow you to have a duplicate SIM to your main contract which will share the voice and data quota from your main card for just a few bucks a month. Here in Bulgaria you can get it for 2 EUR per month.  \n\n\nBonus points:  \nMobile carriers do not guarantee you a dedicated IP, which actually might benefit you. The IP will change from time to time. If it is every few hours or days, noone can say, but still.', '[SOAX residential proxies](https://soax.com/residential-proxies/?utm_source=social&utm_medium=reddit&utm_content=answer) would fit your purposes: reasonable pricing, reliable and stable service and a wide pool of IP addresses, what is more - they have a customer support included to each tariff, so you could get consultation on using proxies for your project.', 'Whats the website you are trying to scrape?', 'Not a proxy service but Mysterium VPN is a cheap VPN that has residential IPs selfhosted by users.', 'Why not doing this from your home? ü§î', 'Please give it a try, I am using it for my personal use and it running smoothly too: https://quickscraper.co/']"
"I was temporary kicked off of LinkedIn, and now my account has been reinstated. When should I start web scraping again?",https://www.reddit.com/r/webscraping/comments/108m6cg/i_was_temporary_kicked_off_of_linkedin_and_now_my/,webscraping,"[here‚Äôs a little background of my story.](https://www.reddit.com/r/webscraping/comments/1071d9r/how_many_li_sn_urls_can_i_convert_to_a_li_public/) so now that Lincoln is working for me again, when should I start back to using my PhantomBuster to scrape about 200 to 300 profiles day?",[]
telegram channels,https://www.reddit.com/r/webscraping/comments/1088sjs/telegram_channels/,webscraping,Any one can help me how to scrap telegrams channels text with python! What is the best ways to do that?,['You can use python module telethon although be carefull with guidelines because I got myself banned while I was playing around with it.']
How to use bs4 to extract only the size available ?,https://www.reddit.com/r/webscraping/comments/1081feg/how_to_use_bs4_to_extract_only_the_size_available/,webscraping," 

Currently working on a project, my goal is to create a scraper to check only the size available of each item with bs4

Website of interest: [https://www.6pm.com/p/gbg-los-angeles-ayvie-slate/product/9479982/color/642?zlfid=192&ref=pd\_detail\_1\_sims\_cv](https://www.6pm.com/p/gbg-los-angeles-ayvie-slate/product/9479982/color/642?zlfid=192&ref=pd_detail_1_sims_cv)

I‚Äôm trying to extract only the available size without showing the size that are not available.

&#x200B;

What i have done : 

size = soup.find('div', {""class"": ""zna-z Ana-z""}).text

print(size)

Return : 5.56.577.588.5 

&#x200B;

and when i try this one

size=soup.find('div', {""class"": ""dqa-z""}).text

Return :5.5     

&#x200B;

My expected return is to get only available size like ‚Äú 6.57 ‚Äú  ( size6.5 and size7) because there are the one available i appreciate any help my way!","['Might not answer your specific question but when you load the page one of the script tags at the bottom has LOADS of data that if you dig into gets you the data you want (stock, price, sizes, styles) and hopefully answers the questions you\'ve been looking for help on.\n\nYou can extract the data like this:\n\n\n    import requests\n    from bs4 import BeautifulSoup\n    import re\n    import json\n    \n    url = \'https://www.6pm.com/p/gbg-los-angeles-ayvie-slate/product/9479982/color/642?zlfid=192&ref=pd_detail_1_sims_cv\'\n    \n    headers = {\'User-Agent\':\'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\'}\n    \n    resp = requests.get(url,headers=headers)\n    print(resp)\n    \n    soup = BeautifulSoup(resp.text, \'html.parser\')\n    \n    search = \'window.__INITIAL_STATE__ = \'\n    \n    dirty = soup.find(""script"", string=re.compile(search)).text\n    dirty = dirty.replace(search,"""")[:-1] # remove initial and end characters to leave only the json data\n    clean = json.loads(dirty)\n    \n    #loads of data here\n    print(clean[\'product\'][\'detail\']) #endpoints for sizes and stlyes includes prices and stock ""onhand""\n\nYou can explore the json yourself, to find what you are after', 'Well bushcat69 is the better way but If like another way with out parsing json you can use this way\n\n""""\nfor tag in div:\n\tinputTag =\ttag.find(\'input\')\n\tif \'checked\' not in inputTag.attrs and not (inputTag.attrs[\'aria-label\'].endswith(\'Out of Stock\')):\n\t\tprint(inputTag)\n""""\n\nparsing all the div tag with class \'dqa-z\'.It will only print input tag with available size.']"
what do you do when you get a access denied for trying to get an url? (Python Selenium),https://www.reddit.com/r/webscraping/comments/107e5cw/what_do_you_do_when_you_get_a_access_denied_for/,webscraping,"&#x200B;

https://preview.redd.it/06zjmlfuo0ba1.png?width=1239&format=png&auto=webp&v=enabled&s=5c92ff00c29ad5fb0437a01debd9ddcb374fce82

im trying to call an url many times for webscraping.

    for i in international_data_indicators:
        for y in countries:
            url = f'https://www.migrationdataportal.org/international-data?i={i}&cm49={y}'
            driver.set_page_load_timeout(10)
            driver.get(url)
            time.sleep(1)
            driver.delete_all_cookies()```

i get the following error attached as picture and basically im blocked, i tried clearing cookies and cache but it didnt work as you can see.

what can i do?

im using python3 and selenium with firefox","[""Your IP address is being blocked, you'll need to use proxies to bypass the IP detection.\n\nhttps://www.browserstack.com/guide/set-proxy-in-selenium"", ""I'm always surprised proxies are still not the default, any website that holds data that's worth scraping will block IPs that send too many requests. There's a quick Selenium integration on this [proxy](https://brightdata.grsm.io/vitariz-proxy) provider's platform.""]"
Web Scraping React Web,https://www.reddit.com/r/webscraping/comments/107hav4/web_scraping_react_web/,webscraping,"Hello, how I can to make web scraping to this page using python ?

[https://rollercoin.com/network-power](https://rollercoin.com/network-power)",[]
is there any way to get member details of telegram channel?,https://www.reddit.com/r/webscraping/comments/1079ih9/is_there_any_way_to_get_member_details_of/,webscraping,I'm not the channel admin.,"['This is the only thing I can find: https://os2int.com/toolbox/investigating-telegram-users-user-groups-and-extracting-youtube-metadata/', 'Following', 'Try Telethon\nhttps://github.com/LonamiWebs/Telethon/wiki/Retrieving-all-chat-members']"
How many LI SN URLs can I convert to a LI Public URL in a day without getting sanctioned?,https://www.reddit.com/r/webscraping/comments/1071d9r/how_many_li_sn_urls_can_i_convert_to_a_li_public/,webscraping,"[I got this error on LinkedIn today after scraping about 1,200 LI URLs to convert them into a public LI URL.](https://imgur.com/a/6q9QdiO) I was using the [Sales Navigator URL Converter](https://phantombuster.com/automations/sales-navigator/9068/sales-navigator-url-converter), and **I'm very nervous to make sure that this doesn't happen again.** 

I have another LI account that I plan on upgrading into a LI SN account. I've had this account for 2.5 years, and I have 100s of connections, and it's got a ""low spam score,"" for a lack of a better term. 

Anyways, I have already ~2,400 LinnkedIn Sales Navigator URLs that I've converted to Public LinkedIn URLs, and I'll need to scrape about 5,000 more. 

How many should I do in one day so that I don't have my account terminated?

I've scraped from a LI SN Lead List using dataminer.io, and I can do thousands of those daily, so I was a bit surprised that I can only scrape ~1,200 LI SN accounts today.","[""if you scraped 2,000 leads from LISN, then you should be able to scrape 1,000 in a day. I don't know what happened to you."", 'Most people recommend a couple of hundred, 200-300. From my limited experience that works over a longer period of time, but it might be a low estimate, especially if you do it only for a couple of days.']"
how to scrape hidden input value with bs4,https://www.reddit.com/r/webscraping/comments/1075ghr/how_to_scrape_hidden_input_value_with_bs4/,webscraping," 

Currently working on a project, my goal is to create a scraper to check the availability of item with their respective sizes (stockId) with bs4

Website of interest: [https://www.6pm.com/p/bogs-b-moc-mid-winter-painted-black-multi/product/9419937/color/80?zlfid=192&ref=pd\_detail\_1\_sims\_cv](https://www.6pm.com/p/bogs-b-moc-mid-winter-painted-black-multi/product/9419937/color/80?zlfid=192&ref=pd_detail_1_sims_cv)

I‚Äôm trying to extract the data from the ‚Äúonly # left in stock ‚Äú and the size inside the <input type:hidden class.

so normally, a website will have out-of-stock on an item by default (if it's out of stock). for this specific site, the user needs to select a shoe size before the target scrape ""only # left in stock‚Äù if inventories are less than 10 for that specific size autherwise nothing is scrape .

What i have done : 

value = soup.find('input', attrs={'name': 'stockId'}).get('value')

print(value)

Return error

My expected return is to get Each size and know ‚Äú only # left in stock ‚Äú respectively if applicable.

i appreciate any help my way!","['There are few different thigns here.\n\n1. You need to click on the size to possibly get this displayed\n2. In order to automate this please look into Selenium or Playwright, can\'t do this with BS4 only\n\nIn general, I would do something like this:\n\n- click on the desired size\n- add if statement, something like: ""if this div is displayed get the value, else add 0 or \'none\' as a value""\n\nThat\'s the rough idea how I would go about this.']"
Scraping only prices from 50 websites,https://www.reddit.com/r/webscraping/comments/1074ycw/scraping_only_prices_from_50_websites/,webscraping,"Hi 

I want to scrap prices from websites to compare the prices from cheapest to expensive?

For ex: I want to scrap price for the best crms out there and wants to list crms from cheapest to expensive and same scenarios for like web hosting or any other niche

There are more then 50 CRMs out there and wants to list them under one roof 

And then wants to create a bot that will check and update the pricing every 3 hours.

Someone has done it for domains as they have their apis
https://www.domcomp.com/

But what about who doesn't have APIs?","['Well if I understand you correctly you want the simplest way to webscrape prices and compare them.\r  \nSo you have the same option as usual.\r  \n1. Use some kind of app (some are paid).\r  \n2. Write and maintain your own code this would need attention.', 'This is a two part problem.\n\nCrawling/Scraping.  There are services you can use. Actually I run my own service and I can hook you up with a discount and/or a free bundle to get you started. If you are interested - DM me.\n\nData ingest. Getting the data is one thing, but ingesting it, structuring it and displaying it in a meaningful comparison is another topic. You will have to provide a bit more details. Maybe 1-2 example websites and is it just prices you want to compare? It kind of sounds like it from your post, but if you want to offer this as a service I suspect this is not enough and you will likely want to compare functionalities, limitations, different features per price tier, etc.\n\nI am curious to learn more.', 'Why do you need to check CRM pricing every 3 hours?']"
Price Scraping and Monitoring Bot,https://www.reddit.com/r/webscraping/comments/106tqgo/price_scraping_and_monitoring_bot/,webscraping,"Hi 

I'm a beginner here 

Is there any readymade script out there with which I can scrap prices from websites to compare the prices from cheapest to expensive?

For ex: I want to scrap price for the best crms out there and wants to list crms from cheapest to expensive and same scenarios for like web hosting or any other niche

How can I do this? Any tool available?  Or do I need a developer to make a bot for each crm provider to get pricing and automate that","[""You don't need a developer to build this scraping. It's one of the simple use cases. You can build this scraping using a no-code scraper. Basically, you configure the URL and the XPath of the pricing HTML elements, and it retrieves the data. \n\n[Byteline WebScraper](https://www.byteline.io/web-scraper?utm_source=reddit&utm_medium=webscraping&utm_campaign=scraper) makes it pretty easy. It also provides a Chrome extension to help you pick the XPaths."", 'there are some tools that are ‚Äúgoogle search‚Äù oriented but they normally sux like badly, they are normaly used to find emails and more simple data such as phone numbers, if you want consistency and quality content you first start by defining the websites you want prices to compare to and then yes, I would say you‚Äôll need someone to write something custom for you, but hopefully I‚Äôm wrong and you‚Äôll find something usefull, doesn‚Äôt hurt trying.']"
Gather public facebook events,https://www.reddit.com/r/webscraping/comments/106n33r/gather_public_facebook_events/,webscraping,"Hello!

I want to fetch Facebook events in my city. Unluckily, it seems like the API only provides this data to ""Facebook Marketing Partners"" (see [https://developers.facebook.com/docs/graph-api/reference/event/](https://developers.facebook.com/docs/graph-api/reference/event/))

&#x200B;

So, naturally webscraping is the only possibility left. However, I am concerned that facebook will detect that I automate stuff and just block my account if I scrape the website every few hours.

Is there another way to solve this problem? I guess keeping the session cookie and not always relogging is a must, otherwise I will run into captchas quickly. Has anyone ever tried this and can recommend something?",[]
How to scrape phone numbers from groups on WhatsApp?,https://www.reddit.com/r/webscraping/comments/105ymmb/how_to_scrape_phone_numbers_from_groups_on/,webscraping,Is there some way?,"['I did it once using puppeteer, I don‚Äôt quite remember fully right now(it was two years ago), but it was something like this:\n\nopen web.watsapp on puppeteer, then there‚Äôs some option that you can set to define sessions to be stored in a local folder‚Ä¶ after you conclude your session you just need to point your session to that same folder and presto, you bypass authentication and other verification methods and can just focus on harvest data', 'What I can think of right now is opening WhatsApp in a web browser and do some selenium.', ""You'll need to be a member of those groups first, and that is not public data!""]"
Has anyone found a way to get around LinkedIn's login wall to scrape content?,https://www.reddit.com/r/webscraping/comments/105r0iq/has_anyone_found_a_way_to_get_around_linkedins/,webscraping,"I'm particularly interested in scraping LinkedIn Jobs info and public posts. 

I know there's a Developer API, but I'd prefer not to sign up for that.

I've also tried signing up for an account and using it to scrape, but its been suspended. 

Any help or info would be great. Thanks in advance!","[""LinkedIn is probably the hardest and/or most expensive website to scrape. There is no real way around using expensive proxies. Aside from that I haven't encountered any issues."", 'those really people interesting in scrapping linkedin??\n\nwhat iinformation you look for there ??', 'just log in, use a cookie inspector and set them on your script along with the same user agent from the navigator you used too', 'You may try extracting google search results by site:linkedin.com and sort by time.', 'Pm me', 'Getsalesfox.com - using cookies', 'Scrapeops.io has a [guide](https://scrapeops.io/python-scrapy-playbook/python-scrapy-indeed-scraper/) with scrapy', 'Hi,\n\nTry [Proxycurl](https://nubela.co/proxycurl/linkedin), it is intended for this. They also [wrote an article on how to bypass authwalls here](https://nubela.co/blog/tutorial-how-to-build-your-own-linkedin-profile-scraper-2020/).', 'Hi. I think i can help you with that. please check out the newly released software I develop https://myfolder.notion.site/myfolder/Phantom-Connect-0aedc60ce03043cc83f8fd55aa558d9c']"
"Is there any webscraping bot, to extract booking.com price information?",https://www.reddit.com/r/webscraping/comments/105pq5r/is_there_any_webscraping_bot_to_extract/,webscraping,"There are free webscraper with them you can extract Hotel informations, rooms & prices on booking.com, if you pretending your travel date and the Hotel.

What I'm searching is something like a bot, extracting the Hotel price based on random travel periods and dates automatically. 

This data set could be analyzed to find pricing errors.

Is there any tool that can be used that way?
I have no programming skills. How difficult is it to sketch a tool doing this kind of work?","[""I wrote an in-depth tutorial on [how to scrape booking.com with Python](https://scrapfly.io/blog/how-to-scrape-bookingcom/) and it's a pretty easy scrape!\n\nFor scraping pricing I found it's best to select values from the pricing calendar as you can get pricing of 30-60 days in a single request!"", ""I'm currently working on the exact project using selenium python"", 'Hopefully it will be ready soon', ""If you guys need real time price information for hotels/air/car/etc (not free), DM me, I got that data.\n\nMight not be exactly what you see on [booking.com](https://booking.com), but it's live data.""]"
extract ebay user listing...,https://www.reddit.com/r/webscraping/comments/105vxmu/extract_ebay_user_listing/,webscraping,"so i'm working on project that helps you to extract ebay user all  listings with price and and all related informations

those that worth trying ?

will anyone be intersting in that ?","["" I've recently worked on getting (via scraping) daily updates on competitor vendor (up to 5) prices in walmart online store."", 'Hello, I am interested. I am a professional web scraper on Upwork and I have completed 50+ data extraction projects and 415 hours on Upwork.\n\nSend me a message with the detail about the project. \n\nThank you.\nKHALIQ.']"
How to scrape Google Trends 'related queries' and auto pull every hour?,https://www.reddit.com/r/webscraping/comments/105osgg/how_to_scrape_google_trends_related_queries_and/,webscraping,"I've tried to use Axiom, Zapier, Python script (not very good at Python though) and can't seem to figure it out, especially trying to get the script to click the arrow key to go to the next 5 results and so on...  


Any help would be great.  


Just to confirm, this is a specific query in Google Trends, specific category, I want to scrape the 25 related queries that appear, and then re-scrape every hour.",['Have you tried https://github.com/GeneralMills/pytrends?']
"Any thoughts on how to ""scrape"" a websocket?",https://www.reddit.com/r/webscraping/comments/105e1gt/any_thoughts_on_how_to_scrape_a_websocket/,webscraping,"I'm trying to build a dataset and it's going 'ok', up until trying to ""catch"" all the traffic from the games on [this website](https://acquire.tlstyer.com) (link to code on site).

Part of me says ""Selenium"", but part of my wants to leverage having a copy of the server code. Can I use any methods from the code to ""decode"" the websocket messages?  I've scraped plenty, but websockets are new to me for scraping.","['The other ""tricky part"" is that I kind of want to capture about a year\'s worth of games (or at LEAST a month\'s worth).  So I was thinking of running a VPS and dumping each day to a CSV of \'moves\' or \'games\' or something.', 'Which pieces of data do you need? Are they present on the page, or do you need to access the raw websocket data?', 'You can create web socket connections in any programming language']"
Ideas for how to pull HTML from a dynamic web page? (i.e: page source doesn't show all the items on the live web page).,https://www.reddit.com/r/webscraping/comments/1058heb/ideas_for_how_to_pull_html_from_a_dynamic_web/,webscraping,"Hi all,

I am trying to scrape information from this web page (https://ircc.canada.ca/english/newcomers/services/index.asp#table1caption, set show locations to all), but I'm having trouble getting the HTML I want to scrape.

I'm on Firefox - when I right click to view source, the information for each row of services isn't in the HTML. However, when I right click and inspect, I see the HTML for the information I'm interested in.

Does anyone have insight into how I can get the HTML from inspecting? I've tried googling my problem, but couldn't find anything. Thanks in advance!","['It seems the service info you want is loaded after the fact, hence why it isn\'t in the page ""source"" but is present when the page is inspected. I\'m not sure about FF, but in chrome you can inspect the page, pick a parent element for the data you want and just do a right click -> copy -> copy element. That table with id table1tbody should do nicely after setting the display to All. If you\'re looking for a programmatic solution to pull data from dynamic pages then selenium will do it, though it may be overkill for this', 'Load the page up and go to the developer tools, and then sources at the top for chrome or debugger in firefox. under services there\'s a file with all the table data in, its called ""services-info"". I\'d just save it and parse it', ""What you are describing is called Single Page Application. It loads a minimal HTML page along with the javascript. Then the browser evaluates it and generates the HTML content on the fly.\n\nThe page you shared is not a classic example, but it seems to also generate the content you are interested in using JavaScript.  \n\n\nFor this type of web page you need a browser to obtain its contents.   \nI suggest you use Puppeteer. It will help you manage headless browser and use it to send requests and receive the data you need.\n\nIf you don't want to bother with the implementation you can use a scraping service.  \n\n\nDM me if you need any help.""]"
need help with puppeteer radio button clicking,https://www.reddit.com/r/webscraping/comments/104yosd/need_help_with_puppeteer_radio_button_clicking/,webscraping,"im using puppeteer to automate a survey, but the selector id of the buttons change every time so how can i always look for the right button","[""Dm the details if you don't mind solution in selenium"", 'OK', 'Dude, use the text property', 'sometimes when you have a dynamic form is easier to listen to scripts being fired in the background than trying to physically click them‚Ä¶ use the console inspector in the network tab‚Ä¶ if you gor pne of those forma you just hit the jackpot, cause it will also give you links to next pages and even content', 'There has to be another selector. If you can share html, I can probably help.']"
A Year of Writing about Web Scraping in Review,https://scrapecrow.com/year-of-writing-in-review.html,webscraping,,
Does anyone knows how to surpass the download restrictions on Telegram?,https://www.reddit.com/r/webscraping/comments/1055xb6/does_anyone_knows_how_to_surpass_the_download/,webscraping,"So, as title says, Is there anyway to surpass the media content (images/videos) restrictions on some Telegram Channels/Groups? I mean, Telegram some months ago added a function to disable  to download content if the channel/group owner wanted to. And I want to know if it's possible to avoid that blockade and download content anyways.

Thanks in advance.",[]
Open Source AI Image Classifier with Automatic Dataset Creator,https://github.com/serpapi/serapis-ai-image-classifier,webscraping,,
"Linkedin Comments Scraper - Script to scrape comments (including name, profile picture, designation, email(if present), and comment) from a LinkedIn post from the URL of the post.",https://github.com/gurbaaz27/linkedin-comments-scraper/,webscraping,,
Best Proxy Lists?,https://www.reddit.com/r/webscraping/comments/1043rhg/best_proxy_lists/,webscraping,What reliable sources of proxies have you all had success with? Preferably places that don't break the bank.,"['ProxyAxe and webshare are two that have been recommended before on here.', '/r/Proxylists and /r/ProxySites may interest you as well. I second the webshare recommendation, FWIW', 'Hey guys with I am a private 4G proxy provider based in Ireland. As I would not want to promote here you can DM me for details if you are interested.\nHave a great', '[removed]', '[removed]']"
"Sorry if this is the wrong sub, but is it possible to extract an email from an old youtube account?",https://www.reddit.com/r/webscraping/comments/104fxv6/sorry_if_this_is_the_wrong_sub_but_is_it_possible/,webscraping,"Basically I can't remember the email I used to create a youtube account from back in 2010. I've tried retrieving it using google's tool where you enter your phone number but I have several gmail accounts linked to the same phone number. And it showed me a dead account I no longer use instead of the three active accounts also linked to that number, so it wasn't very effective.",[]
Looking for people with datasets for sale!,https://www.reddit.com/r/webscraping/comments/1043a47/looking_for_people_with_datasets_for_sale/,webscraping,I‚Äôm looking for individuals that have data for sale. It can be any kind of interesting marketable data that another party might be interested in purchasing. I‚Äôm doing research for a project also as see if the option for monetization is possible. Thanks!,"['I could download data with web scraping of relatives important sites.', 'Sent a dm', 'I have 3+ months of daily supermarket prices, many shops.\n\nSend me a dm if u r interested.']"
Are reddit's classes names change periodically (those with random letters and numbers and what not),https://www.reddit.com/r/webscraping/comments/103xy6z/are_reddits_classes_names_change_periodically/,webscraping,"I'm building webscraper for the final project I'm doing, and I wonder is it a wise path to hardcode their names, or to just think of something else?

I saw other advice of searching through elements until I find something recognizable, but I'd rather do this because this is my first time web scraping.

I want to grab links to posts from the subreddits main page, then use those links to request HTML of these individual posts.","[""They are randomly generated strings and will change constantly. Try looking into xpaths or if you want to scrape Reddit, look at praw - it's Reddit's api""]"
Scrape Videos from multiple websites,https://www.reddit.com/r/webscraping/comments/103xtjl/scrape_videos_from_multiple_websites/,webscraping,"Hi,

I have seen many posts in this subreddit about scraping videos from one specific website. My use case is however a bit different since I am theoretically interested in all videos on the web. My question would be how would one go about this since the location of the videos in the HTML differs for each webpage, video sitemaps are not always present or use different formats and sometimes there are multiple videos like ads present.

To my knowledge, this should be possible since Google seems to be quite reliable able to identify videos on any webpage.

Thanks for your help.","['Videos should be inside video tag, no?\nAnd I think ads are mostly in iframe tags.\n\nSo scraping ‚Äúvideo‚Äù tags would make it work', 'I would like to collab. dm me']"
Anyone want to write me a program that can automate generating a pdf of a magazine subscription?,https://www.reddit.com/r/webscraping/comments/103s1uu/anyone_want_to_write_me_a_program_that_can/,webscraping,Noob here looking for a quick fix. Budget to pay you is limited.,"['Hello, send me a message I can help me you with that.', ""I can do that  using selenium python library \n\nJust send me the details \n\nI've worked on similar projects in the past one that download examination reports from indian patent site (INPASS) as another one I'm currently working on is the one that download statements of the Monetary policy from South Africa reservation Bank site"", 'ChatGBT FTW.', ""What's your budget?""]"
Scraping a Dynamic Webpage with rSelenium,https://www.reddit.com/r/webscraping/comments/1036j0g/scraping_a_dynamic_webpage_with_rselenium/,webscraping,"Hi! 

&#x200B;

I have been attempting to find a workaround for a web crawler that I am building to download a datafile file on a schedule. I have encountered a problem when trying to get the path for a dropdown menu, my problem specifically is that I just can't figure out the xpath or selector for it. 

[Here](https://opendata.dc.gov/datasets/integrated-tax-system-public-extract/explore) is the website. After navigating to the page of interest by clicking the download button, I then need to click the ""Download Options"" button to bring up a drop down where I can then click the dropdown element which initiates the file download. I've attached pics for reference of the download option button I am referencing. Additionally, I've provided my script thus far at the end. 

&#x200B;

Thank you in advance for your help! 

    # Load Packages
    ## Data Manipulation
    library(tidyverse)
    ## Webscaping
    library(RSelenium)
    ## Selenium Remote Server
    library(netstat)
    rs_driver_object <- rsDriver(
      browser = ""chrome"", 
      chromever = ""108.0.5359.22"", 
      verbose = F,
      port = free_port()
    )
    rem_dr <- rs_driver_object$client
    # Open a chrome page
    rem_dr$open()
    # Navigate to webpage
    rem_dr$navigate(""https://opendata.dc.gov/datasets/integrated-tax-system-public-extract/explore"")
    # Fit window to full screen
    rem_dr$maxWindowSize()
    # Navigate to Download Button and Click 
    rem_dr$findElement(using = ""xpath"", ""//*[contains(concat( ' ', @class, ' ' ), concat( ' ', 'btn-default', ' ' ))]"")$clickElement()
    # Navigate to Download Options and Click
    rem_dr$findElement(using = ""xpath"", ""//div//calcite-dropdown"")$clickElement()

&#x200B;

[First step in accessing the download options dropdown](https://preview.redd.it/ra5diarnp1aa1.jpg?width=1667&format=pjpg&auto=webp&v=enabled&s=a2e747be8c6e42115e3f0d00ddd33d01e270f462)

&#x200B;

&#x200B;

[Final step to initiate the download for the file](https://preview.redd.it/mzmnj2nsp1aa1.jpg?width=1667&format=pjpg&auto=webp&v=enabled&s=8fce7b7a584f8c02cc995888fe8a10843c6f0a51)","['Why not just scrape their backend api delivering the content?', 'Ya I agree but also got to your web browser download I been using selenium ide on chrome just record a test click on the button and it will give you all the things']"
Is there a way to convert LinkedIn Sales Navigator URLs to a LinkedIn public URL?,https://www.reddit.com/r/webscraping/comments/1039q3b/is_there_a_way_to_convert_linkedin_sales/,webscraping,"I have [many LI URLs but in Sales Navigator format](https://imgur.com/a/wvGHHiS), and I'd like to convert them to public profile LI URLs so that we can upload them into our CRM. My LI SN URLs are in an CSV or Excel format. 

How do I go about doing this?","['Can you show us the process of turning them public and then we can see what we can do.', 'Dm me a sample and I can help']"
Gurufocus / Implemented new cloudflare protection?,https://www.reddit.com/r/webscraping/comments/102zfub/gurufocus_implemented_new_cloudflare_protection/,webscraping,"Hello - i was scraping data from gurufocus for a long time - eg. from this site

[https://www.gurufocus.com/term/pb/AAPL/PB-Ratio](https://www.gurufocus.com/term/pb/AAPL/PB-Ratio)  


But resently i saw a new protection from gurufocus and get this page when try to scrape data:

https://preview.redd.it/6sgt2395zz9a1.png?width=1320&format=png&auto=webp&v=enabled&s=6ab9538f367de0c6880f02afcee7308d38248ff0

I can find a workaround using residental proxies eg. from smartproxy.

Is there any other (maybe cheaper) solution to still get the data form gurufocus?","[""There are a number of ways to bypass Cloudflare:\n\n**#1 -** Finding the origin server for the website and sending your requests to that instead.\n\n**#2 -** Using a fortified headless browser to solve the JS challenges. Good options are the [Puppeteer stealth plugin](https://github.com/berstend/puppeteer-extra/tree/master/packages/puppeteer-extra-plugin-stealth) and [Selenium undetected-chromedriver](https://github.com/ultrafunkamsterdam/undetected-chromedriver).\n\n**#3 -** You could also use a Cloudflare bypass tool like [FlareSolverr](https://github.com/FlareSolverr/FlareSolverr), which is a proxy server you can use to bypass Cloudflare and DDoS-GUARD protection. \n\n**#4 -** The other option is to use a smart proxy solution like [ScrapeOps Proxy](https://scrapeops.io/proxy-aggregator/) which does all 4 steps for you and includes a Cloudflare bypass built-in. You just send it the URL you want to scrape and it will take care of rotating the proxies, generating real browser headers, using a headless browser to bypass anti-bot JS challenges, and dealing with ban pages & CAPTCHAs.\n\n[This guide goes through in more detail how to scrape Cloudflare-protected websites.](https://scrapeops.io/web-scraping-playbook/how-to-bypass-cloudflare/) \n\nI've tested it with [ScrapeOps Proxy Aggregator](https://scrapeops.io/proxy-aggregator/) and that URL is working when you enable the Cloudflare bypass by adding `bypass=cloudflare` to your request. You can test it yourself by [signing up for a free account here](https://scrapeops.io/app/register/proxy).""]"
key word search,https://www.reddit.com/r/webscraping/comments/102s9rc/key_word_search/,webscraping,does anyone know a place where I can insert multiple urls and scrape them for specific keywords,"['Neilpatel.com', 'What do you mean by scraping URLs for specific keywords?']"
Scraper to collect APY data from Web3 frontends,https://www.reddit.com/r/webscraping/comments/102vcbo/scraper_to_collect_apy_data_from_web3_frontends/,webscraping,"I'm working on a project which offers auto-compounding vaults for yield-bearing opportunities across a variety of Web3 protocols/dapps. Due to the nascent nature of this industry, there aren't many reliable sources of data and it looks like I'll have to scrape a few front-ends to grab the listed APYs in some cases. For example, I need to scrape the list of farms and their associated APYs from the Sushi.com front-end.

Any recommendations for the best route to go about doing this? I personally have no experience with scraping, but have a decent grasp on Typescript, JavaScript & Python. Appreciate any guidance you can share in this regard. Thanks!","['Is it me or I don‚Äôt understand the request?\n\nIt kinda look like what I can find on r/masterhacker so I‚Äôm confused', 'Need to provide more info', 'I would go with python & beautiful soup for this task as you are navigating thru HTML trees and tables to find such data. I also assume you would want to update the data to a CSV or database of some sort.\n\nI can make this script for you as well, DM for more information\n\nHeres a bs4 tutorial by Tech With Tim https://www.youtube.com/watch?v=lOzyQgv71\\_4', 'DM me']"
Webscraper for job postings hiring physics majors,https://www.reddit.com/r/webscraping/comments/102sas0/webscraper_for_job_postings_hiring_physics_majors/,webscraping,"Hello, I‚Äôm looking for guidance on where to start with designing a program that goes through indeed and finds listings of jobs hiring a physics major to find all the potential jobs that would hire me with my physics major. I have a pretty limited knowledge of python. Thank you!","['Here is a guide and working code for [how to scrape Indeed jobs using Python Scrapy](https://scrapeops.io/python-scrapy-playbook/python-scrapy-indeed-scraper/).', 'If you want to scrape Indeed without getting blocked - this is for you:  \nhttps://www.page2api.com/blog/how-to-scrape-indeed/']"
"Your Advice: How to scrape a webpage with pagination and an AJAX ""wall "" to download pdf's after the wall has been crossed?",https://www.reddit.com/r/webscraping/comments/1024ead/your_advice_how_to_scrape_a_webpage_with/,webscraping,"Let me know if you'd like to see the link to the page.

Edit:
I am struggling to scrape this site: https://www.resbank.co.za/en/home/publications/statements/mpc-statements

What i am specifically looking to do is to go from page to page and click the links, which will open to new windows that contain pdf documents which I would like to download and zip together. I am specifically interested in the ""statements of monetary policy"" document.","['Dm with the details, I have similar similar project I just completed', ""Could you provide more details on what you mean by AJAX wall?\n\nIf you mean pagination limit e.g. there are loads of results but the max page is 10 then there aren't many options. You can try to sort the pagination from and approach it from multiple ends giving you 20 pages (e.g. sort by highest price and then by lowest). Then, if the pagination offers any filters you can apply them in various order to reach as many results as possible."", 'I would use Puppeteer Network inspector to monitor those ajax results, if they\'re not encrypted you could possibly just intercept them directly, or even find what and how they are being requested and call that function yourself... which would save you toons of work on ""navigating"" through pagination links']"
how many pages can be loaded from amazon before being banned?,https://www.reddit.com/r/webscraping/comments/1021x0d/how_many_pages_can_be_loaded_from_amazon_before/,webscraping,"I've been building a webscraper for amazon.  It takes about 1 minute per product page.   Will that let me fly under the radar?  I assume I'm fine when I run it for an hour at a time, but once I get all the bugs worked out, I want to run it basically nonstop.  Will amazon ban me for loading 10,000 pages in a week?","[""It depends on many factors starting from your IP address to the way your scraper works. \n\nScrapers can be identified in dozens of different ways^1 and assigned trust scores (how likely the client is a bot). So, the only way to know is to try and optimize your scraper from there. \n\n10,000 pages a week isn't particularly a lot but my guess would be that if you're running from a single IP with basic web scraping then it's very likely you'll get blocked. Note that banning is not very common, Amazon will simply start blocking you or ask you to start solving captchas.\n\n1 - see [this in-depth guide](https://scrapfly.io/blog/how-to-scrape-without-getting-blocked-tutorial/) I wrote if you want to learn more"", 'No I don‚Äôt think amazon will ban you even if you scraped 100 pages per second and why will they?\n\nGoogle and other search engines also use scrapers and there is no limit to search speed declared in robots.txt \n\nthis is why when you search for example ‚Äúiphone 14‚Äù on google, google shows you amazon in their search result. Because google has scraped that amazon page.\n\nBy declaring your header as google-bot‚Äôs you should be good to go.\n\nI haven‚Äôt tried it myself but, I have done a similar thing on facebook, for scraping public page‚Äôs about section, I would scrape 500 pages per minute, no ban no nothing happened']"
How would you web scrape marketwatch's virtual stock exchange?,https://www.reddit.com/r/webscraping/comments/101u3b9/how_would_you_web_scrape_marketwatchs_virtual/,webscraping,"Hi, I'm new to all of this. I just want some general pointers and maybe links to some tutorials for web scraping Marketwatch and creating orders in its virtual stock market exchange -[https://www.marketwatch.com/games?mod=side\_nav](https://www.marketwatch.com/games?mod=side_nav)","['You would want to start with requests and bs4.\n\nRequests helps you with fetching the html data.\n\nbs4 helps you find specific elements within the html data.\n\nIf it requires javascript rendering you can learn selenium which automates a browser for your task.\n\nIf you want to make post requests i would recommend going with fiddler for beginners.\n\nFiddler is basically a program that shows you your actions on your browser for example logging into a website it will show you made a post request for logging in. You can replicate that post request to maybe login a different account. (paid program)\n\nYou can also use your browser but the downside is if the website reloads or redirects you then any post or get request will be cleared.\n\nA couple tips i can give you is always look out if the data is being via an api.\n\nHow can you tell? go to the network tab and check for any get requests that returns json data. Those will usually be delivering content.\n\nAs for post requests there are two types which people usually get confused with. Data and Json. You can tell you make post request with Data is when you look at the original payload it can look something like this  ""username=1&password=1"". With json it will look like this {""username"":""noob"", ""password"":""noob}. This would be very helpful when you try to create orders on your virtual stock market exchange via python.\n\n&#x200B;\n\nHeaders are very important as well. Many website use this as a tactic to stop web scraping bots. The most important one would be user-agent.\n\n&#x200B;\n\nYou can probably buy a course on udemy that are on discount.\n\nThis isn\'t very hard to learn so any course on youtube will do.\n\nThe subjects for your needs is just python requests, bs4, json, selenium and post requests.\n\n&#x200B;\n\nYou can also ask me any questions and ill be glad to help you out :)']"
How do you generate leads ?,https://www.reddit.com/r/webscraping/comments/101l3jg/how_do_you_generate_leads/,webscraping,"Hi everyone,
I‚Äôm just wondering how do you guys generate leads by web scraping?

I‚Äôm just thinking for example about real estate agencies and what kind of leads they might be interested in?

Any ideas üí°?",['Are you asking for sources or how do we get clients by web scraping?']
Scraping Aliexpress search page does not return all products,https://www.reddit.com/r/webscraping/comments/101k1rd/scraping_aliexpress_search_page_does_not_return/,webscraping,"

I have the below code, which I expect to return 60 products, but instead only returns 16:

    driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()))
    
    url = 'https://www.aliexpress.com/w/wholesale-silicone-night-light.html?SearchText=silicone+night+light""&""catId=0""&""initiative_id=SB_20230101130255""&""spm=a2g0o.productlist.1000002.0""&""trafficChannel=main""&""shipFromCountry=US""&""g=y'
    
    driver.get(url)
    
    driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
    
    html = driver.page_source
    soup = BeautifulSoup(html, 'lxml')
    
    product_links = []
    
    
    def get_element_title(element):
        return element.select('h1[class*=""manhattan--titleText--""]')[0].text
    
    
    def get_product_links(soup):
        for element in soup.select('a[class*=""manhattan--container--""]'):
            link = f""http:{element['href']}""
            product_links.append(link)
            print(get_element_title(element))
    
    
    get_product_links(soup)

I manually checked the class name for all the products, since I  thought maybe some of them have different class names in an effort to  stop scraping, but they all have the same class name.

Screenshot since I think the class names are randomly generated for different people

&#x200B;

https://preview.redd.it/ri63jykmdo9a1.png?width=490&format=png&auto=webp&v=enabled&s=93588d01bcab0e6b7c857e4548002d166ccc5aa4","['Two things: have you tried running it headed, and seeing if that `execute_script` did as you requested? And then, related to that: did you check `driver.page_source` was what you expected?', 'Instead of using selenium have you tried using the back end api aliexpress uses to deliver the content?', 'Selenium is a bit of an overkill to scrape Aliexpress\' search. \n\nYou can use simple HTTP requests via `httpx` or `requests`:\n\n```python\nimport httpx\n\nquery = ""drill""\nsort_type = ""default""\npage = 1\nrespons = httpx.get(\n\t""https://www.aliexpress.com/wholesale?trafficChannel=main""\n\tf""&d=y&CatId=0&SearchText={query}&ltype=wholesale&SortType={sort_type}&page={page}""\n)\n```\n\nAll of the preview data is even available in hidden web data, so you don\'t need to parse the HTML:\n\n```\nimport json\n\ndef extract_search(response):\n    """"""extract json data from search page""""""\n    sel = Selector(response.text)\n    # find script with page data in it\n    script_with_data = sel.xpath(\'//script[contains(text(),""window.runParams"")]\')\n    # select page data from javascript variable in script tag using regex\n    return json.loads(script_with_data.re(r""window.runParams\\s*=\\s*({.+?});"")[0])\n```\n\nSee this [full guide I wrote on how to scrape Aliexpress](https://scrapfly.io/blog/how-to-scrape-aliexpress/#scraping-search) for more.', 'Question was answered:\n\n[https://stackoverflow.com/questions/74985825/scraping-aliexpress-search-page-does-not-return-all-products/74992964#74992964](https://stackoverflow.com/questions/74985825/scraping-aliexpress-search-page-does-not-return-all-products/74992964#74992964)\n\n    def launch_url(url):\n    # create webdriver object\n    chrome_srv = Service(driver_path)\n    driver = webdriver.Chrome(service=chrome_srv)\n    driver.get(url)\n    # find doc/window height and compute page count\n    doc_height = driver.execute_script(""return document.body.scrollHeight"")\n    win_height = driver.execute_script(""return window.innerHeight"")\n    num_pages = int(doc_height / win_height)\n    print(f\'doc height=>{doc_height}\\tpages =>{num_pages}\')\n    # scroll through the document\n    for page in range(num_pages):\n        driver.execute_script(""window.scrollTo(0, arguments[0]);"", win_height * (page+1))\n        print(f\'scrolling to=>{win_height * (page+1)}\')\n        sleep(2)\n\n    # get all product anchor tags\n    anc_elem_list = driver.find_elements(By.CSS_SELECTOR,\'a[class^=manhattan--container--1lP57Ag]\')\n    # get all product title tags\n    title_elem_list = driver.find_elements(By.CSS_SELECTOR,\'h1[class^=manhattan--titleText--]\')\n    print(len(anc_elem_list),len(title_elem_list))\n    for anc_elem,title_elem in zip(anc_elem_list,title_elem_list):\n        print(anc_elem.get_attribute(\'href\'),title_elem.text)']"
thread-safe: a simple tool for saving local copies of your favorite Twitter threads,https://www.reddit.com/r/webscraping/comments/101f7tx/threadsafe_a_simple_tool_for_saving_local_copies/,webscraping,"Hey Everyone,

Happy new year! I wanted to share a CLI tool I built for saving a local copy of any Twitter thread. It's called thread-safe and it's designed to do just that - keep your favorite threads safe on your local machine. This way you never have to worry that they might one day just disappear...

More seriously, I wanted a simple way to save threads of interest \_without\_ having to use a third-party app that needs access to my Twitter account (or vice-versa) and that forces me to reply to the thread to ""unroll"" or otherwise save a copy.

thread-safe generates an HTML file containing all of a thread's contents including each tweet's text, links, and media attachments (images and videos). This file, all attachments, and a JSON data file are saved to the local filesystem and the HTML can be used to display the thread locally in a browser at any time.

By using a dedicated directory for all generated files, thread-safe can be used to maintain a local library of saved threads that can easily be searched using standard commandline tooling like (rip)grep, fzf, fd, and any other awesome tool of your choice.

I built thread-safe because I often find myself saving links to informative threads and am interested in preserving the author's content. This is exactly the use case that thread-safe addresses: generating local copies of these single-authored, consecutive series of tweets. Notably, thread-safe intentionally does not save the comments or discussion from other users that follow.

[https://github.com/dkaslovsky/thread-safe](https://github.com/dkaslovsky/thread-safe)

I hope you might find it useful and I'd love to take any feedback or suggestions that might come to mind!",[]
Best web scraping api's at the moment?,https://www.reddit.com/r/webscraping/comments/1016j3l/best_web_scraping_apis_at_the_moment/,webscraping,"Hi, we are currently using 3 web scraping API's mostly for legacy reasons and to be able to have a variety of web scrapers for different sites.

I am finding 2 of these performing poorly and am looking for alternatives.

Currenly using:

ScraperAPI. Have a legacy cheaper plan so have kept it, but often it will return a 500 error when scapfly works every time.

ScrapingBee. Got in when the started and they were very good. Also noticing a lot of failures where scrapfly would work.

ScrapFly. Seems to work really well, but pissed they decided to double our price for our plan.

Would like to keep scrapfly and find another solution and get rid of scrapingbee and scraperapi.

ScrapingFish and Scrapeops Proxy look good.

Any recommendations?","[""If you're running a legitimate business just hire a developer. These scraping tools are a joke."", 'Scrapfly definitely the best at the moment, to have tested and used almost all serious web scraping api, switching to their solution was day and night for us (real estate company), \\~80 targets migrated for now, before/after stats are greatly improved. Compared to traditional proxy solution, it saves cost arround 60% due to a lower retries and the bandwidth wasted disappear', ""Ideally you should build one yourself. Nothing will perform better.\n\nStill if you don't want to maintain your own infrastructure for this, I would give [Crawlio](https://www.crawlio.net) a try - It is very similar to ScrapingBee in terms of API and is way faster."", ""At [Scraping Fish](https://scrapingfish.com), we have a very user friendly pricing which is usage based instead of monthly subscription so you don't lose unused requests at the end of every month. In addition, it's predictable as the cost of each request is the same regardless of which options you use. All requests use the same premium mobile proxy and you don't pay anything extra for JS rendering, scraping google, or other features. Please [contact us](https://scrapingfish.com/contact) if you need a free trial account to try it out.  \nYou can read more on how we compare to ScraperAPI and ScrapingBee here: [https://scrapingfish.com/how-we-compare](https://scrapingfish.com/how-we-compare)"", 'Give [ScrapeOps Proxy Aggregator](https://scrapeops.io/proxy-aggregator/) a try. It aggregates all the proxy providers together so you have access to over 20 proxy providers (including all Proxy APIs) from a single API endpoint.\n\nYou send us the request and we find the cheapest proxy provider that gives you the best performance for that target domain.', 'Scrapeninja.net is another one to try.', 'Did you try wintr and scraping ant?', 'I would recommend [ScrapeStars](http://scrapestars.com), they are team of professionals.']"
How will ChatGPT affect web scraping?,https://www.reddit.com/r/webscraping/comments/100scxg/how_will_chatgpt_affect_web_scraping/,webscraping,It‚Äôs a pretty open ended question because I just want to know the general opinions of web scrapers on ChatGPT,"['I believe that, as for all areas of software engineering, it will just reduce the Googling time at best, which may be less valuable the more experienced you are.', 'what do you mean by that? why did you union chatgpt and webscrapers? can you elaborate on this?', ""Not ChatGPT itself, but similar technologies and AI in general already has uses - mainly for unstructured data, or when you want to scrape different websites without writing a script for each - say many different ecommerce sites.\n\nAlso, GitHub Copilot (which is based on a very similar text model) is extremely useful for helping with HTML/JSON parsing code and other monotone tasks in connection with web scraping.\n\nThat said, I don't think AI will revolutionize anything here; it's just a question of a few more use cases and convenience. One possibility where things could change more may be bot detection (but those also already use AI, yet somehow, they are almost always possible to bypass).\n\nAlso I should note that while ChatGPT is a great demonstration of how advanced AI got, but it's not in itself a solution to most things, task-specific models work much better."", ""Why do ppl have such a hardon for ChatGPT? There's literally nothing intelligent about it. It's just another stats based bot that's tuned a little better than others and has the Elon sticker slapped on it. Nothing revolutionary about it... ppl so easy to hype these days.""]"
Any alternatives to Browserless.io,https://www.reddit.com/r/webscraping/comments/100z2f5/any_alternatives_to_browserlessio/,webscraping,"I really like the ease of using Browserless.io, especially when dealing with AWS Lambdas, but they seem to be too expensive and the self-hosted option would require me to pay for a license.

Are there other alternatives I could look into?

Thanks!","['[Scraping Fish](https://scrapingfish.com) has clear and transparent pricing. It‚Äôs going to be cheaper especially if you need JS rendering and need flexibility in terms of how many requests per month you do: [https://scrapingfish.com/how-we-compare](https://scrapingfish.com/how-we-compare)', 'If its for web scraping then there are a couple similar ones. ranked from favorite to least favorite in my opinion.\n\n&#x200B;\n\n[https://scrapingant.com](https://scrapingant.com)\n\n[https://www.wintr.com](https://www.wintr.com)\n\n[https://scrapingbee.com](https://scrapingbee.com)\n\nhttps://www.scrapingdog.com\n\n[https://proxiesapi.com](https://proxiesapi.com)\n\n&#x200B;\n\nYou can automate the signup process to some of these websites helping you bypass the scraping limit.', 'Crawlee', 'Can you give an estimated number of requests per month you would be requesting?\n\nI have a few ideas.']"
Here is my issue,https://www.reddit.com/r/webscraping/comments/100xmj8/here_is_my_issue/,webscraping,Selenium seems to work different on every device I use if it works at all I been trying playwright out and I think I like it but does anyone have any issues with it or suggestions or anything idk I'm board,"['I think if you post this in r/Python you would get the responses you want.', ""Ya lately I've been trying playwright I kinda like the API less extra fat on it like 3 different action modules for the same action is not needed also the async or sync option is nice that and fack that it self installs all the required driver's and all is cool I need to use it more thou really for some form filling I just haven't had time also it's smaller which I like cus a lot of sights use anti bot and JavaScript  and I feel better about just going playwright off the rip then trying requests only to get a 404 no matter what""]"
https://preservedbritishsteamlocomotives.com/,https://www.reddit.com/r/webscraping/comments/100wht2/httpspreservedbritishsteamlocomotivescom/,webscraping,"Anyone know how this site could be scraped for an offline mirror? I'm only familiar with wget and they seem to have blocked it. 

[https://preservedbritishsteamlocomotives.com/](https://web.archive.org/web/20211104032702/https://preservedbritishsteamlocomotives.com/)

Thanks in advance",['You can use requests and bs4. \n\nRequests to fetch only the html.\n\nBs4 to get any specific elements.']
Looking for python developers specialized in web scraping,https://www.reddit.com/r/webscraping/comments/zzyspg/looking_for_python_developers_specialized_in_web/,webscraping," I've been developing this automation software for almost one year. The software is PhantomBuster alternative, and it's released now. I plan to expand features that will make it more unique, and I'm looking for a python developer to join me in building software that will help people in the marketing area. More about the software - [https://myfolder.notion.site/myfolder/Phantom-Connect-0aedc60ce03043cc83f8fd55aa558d9c](https://myfolder.notion.site/myfolder/Phantom-Connect-0aedc60ce03043cc83f8fd55aa558d9c)","['Hello, I am python developer and I specialise in Web scraping. I freelance on Upwork and I will like to be a part of this project. Send me a message so that we can discuss about this better.\n\nThank you.\nKhaliq.', 'Dm me pls', 'I have two years of experience in web scraping and possess extensive knowledge of web-based automation tools such as Selenium and Playwright. I have completed several scraping projects in my recent work and would be happy to connect and learn more. You can view my work on my [GitHub](https://github.com/sushil-rgb?tab=repositories) profile for your reference.', 'My main job function as a Python dev is web scraping and automation.']"
Why can't I scraping this site?,https://www.reddit.com/r/webscraping/comments/1001jfp/why_cant_i_scraping_this_site/,webscraping,"Hi. Below python code gives all urls in a site. It works on every website i tried. But it did'nt work on one site. Site is [eksisozluk.com](https://eksisozluk.com) . Console is empty. It does not give any error. But it does not give any url although even there is. Why do you think this might be?

    import requests
    import re
    from bs4 import BeautifulSoup
    
    url = 'http://tuskolik.com'
    response = requests.get(url)
    html = response.text
    
    
    soup = BeautifulSoup(html, 'html.parser')
    
    
    links = soup.find_all('a', href=re.compile(r'/\d{6}\.html'))
    
    for link in links:
        print(link['href'])",['Have you checked the response html you actually get back?\n\nI have double checked. add some headers in there and this website has cloudflare.']
python package or regex for crawling emails?,https://www.reddit.com/r/webscraping/comments/zzuhmz/python_package_or_regex_for_crawling_emails/,webscraping,"I have a list of ~2000 unique urls I scraped for different stores. I'd like to crawl them for any emails found on the landing page or on the contacts page if there is one.
I've tried a couple of open source packages on GitHub as well as implementing a couple of email regex finders but none of them are really doing the job right.
I know I'll never be able to capture all of them, but was wondering what your preferred methods are in this case?","['The lowest hanging fruit can be captured by looping through each unique URL and appending ""/contact-us"" and downloading the information using the requests library for Python. BeautifulSoup can parse the HTML and regex can find any link or text with an email signature (regex for email id using re in Python = \'\\^\\[a-zA-Z0-9.!#$%&‚Äô\\*+/=?\\^\\_\\`{|}\\~-\\]+@\\[a-zA-Z0-9-\\]+(?:\\\\.\\[a-zA-Z0-9-\\]+)\\*$\'). \n\nThis should work for many sites because ""/contact-us"" is standard. This will fail for some Java based websites and if they do not have the standard ""/contact-us"" URL path.']"
possible to automate listing an item to sell on Ebay with python selenium in 2022?,https://www.reddit.com/r/webscraping/comments/zzjb40/possible_to_automate_listing_an_item_to_sell_on/,webscraping,"It doesn't look that simple up front because the UI looks different depending on what item category you type inside the search bar. Does anyone have a working solution that still works? I am typing this message on 12/30/2022, so heading into 2023.","['ebay have api , you may check that', 'You can use fiddler and then make a regular posting. It should show you the post request you made to make the listing. All you would need to change is the payload inside to your new listing. If you don‚Äôt have fiddler you can also use your browsers network tab']"
Life insurance leads,https://www.reddit.com/r/webscraping/comments/zzkoz9/life_insurance_leads/,webscraping,Hi I‚Äôm pretty new to this but been spending all month learning as much as I can. I‚Äôm wanting to scrape websites for potential clients looking to buy life insurance. I would really appreciate any wisdom.,"['Life insurance can be bought by anyone, so i guess... yellow pages, correlate the entries with the leaked credit score data and filter out people below a certain score :P']"
Need help with selenium and web scraping,https://www.reddit.com/r/webscraping/comments/zzcb8k/need_help_with_selenium_and_web_scraping/,webscraping," Im trying to click the marketplace icon on Facebook and it's an icon with an anchor tag. Ive read the docs and they suggest using link text and using the text between the <a></a> tags but there is no text between the tags. Ive attached an image of what the button is exactly 

&#x200B;

 [https://imgur.com/ZH28HCg](https://imgur.com/ZH28HCg)","['Instead of clicking it you can use bs4 to get the href element and then paste that in your browser.', 'You can let us know what you‚Äôre trying to achieve and then we can see what we can help you with', ""Get selenium ide for chrome or something similar and find the class name or Id or css tag something like that import By\nFrom selenium.webdriver.common.by import By\n###and to select an element\nwbd.find_element(By.ID, idname)\n\nSorry I'm on my phone and it's I'll say impossible but probably more of a pain to do code blocks on here"", 'I think you can select it by ‚Äúhref‚Äù\n\nPress ctrl+F in chrome inspect element and search for \n\n‚Äú/marketplace/?ref=app_tab‚Äù\n\nIf there is 1 href like this then you can use\n\n\nurl = ‚Äú/marketplace/?ref=app_tab‚Äù\ndriver.find_element_by_xpath(\'//a[@href=""\'+url+\'""]\')', ""Find_element(By.Name, 'Marketplace')""]"
How to scrape data that is not visable on the site with Octoparse,https://www.reddit.com/r/webscraping/comments/zz33l7/how_to_scrape_data_that_is_not_visable_on_the/,webscraping," Hi. I am new to programming and scripting. Learning now, but I would really need some help right now for a project if somebody would be nice to help me out. When using the Octaporase, it is pretty straight forward with extracting data that is visible on the page. Put when you want to extract data that is hidden, you need to know a bit more scripting I guess. 

I would like to extract the sku number from this site  [https://www.nespresso.com/se/se/order/capsules/vertuo](https://www.nespresso.com/se/se/order/capsules/vertuo) but I don¬¥t know how.

The SKU number is not visible on the website but is in the background e.g( Nb-sku-coffee id=""109877"" where id is the product number) The Sku number is the ID of all product.

Could someone help me and explain how to extract this data that is not on the front page.  


Best regards","['Hi, considering that you have the whole html data from octaparse you can apply the current code to get the sku. All you need to do is change the req.text.\n\n&#x200B;\n\nhttps://pastebin.com/2vc8JzKz']"
How to monitor a page for content changes?,https://www.reddit.com/r/webscraping/comments/zyu7hq/how_to_monitor_a_page_for_content_changes/,webscraping,"I have this page:

[https://creditcards.chase.com/all-credit-cards](https://creditcards.chase.com/all-credit-cards)

I'd like to get an alert on whenever there are changes. 

Are there any services to do this?","['You can make a post request to the page every x seconds, minutes or hour and then check if the html source is the same as the previous one. Id it isn‚Äôt the same then something has changed.', 'You need to monitor post requests', 'Depending on your needs and update frequency, things like wachete or visual ping are full featured and usually cheap.  Otherwise yes, you can ping and compare responses ever X interval for elements you care about.', 'What do you want to monitor on that page? \r  \nIt is a list, I don‚Äôt see any service available to monitor a list/table for changes and better approach could be to create a script to parse and convert into CSV, use csv diff for every x hour.', 'I can build you a discord bot to do that.', 'Visualping website does that. They give you a few free requests per day. Very easy to use interface.', 'Theres websites that do that']"
Web scraping orders from Amazon: is still possible?,https://www.reddit.com/r/webscraping/comments/zyy2dt/web_scraping_orders_from_amazon_is_still_possible/,webscraping,"Hello there!

I‚Äôve made a few searches and found old replies to the same question but there are two differences to what I‚Äôve learned trying to scrape the ‚ÄúMy Orders‚Äù pages‚Ä¶

First, there‚Äôs no built-in report function for my country (Italy).

Second and most annoying: I‚Äôm trying with a simple JavaScript function to be copy-pasted and run directly in the browser that involves use of the default fetch and querySelectorAll API. The point is that the DOM nodes resulting from the fetch call that there were supposed to hold the relevant informations about the orders are encrypted and I can‚Äôt read them.

I‚Äôve found no trace of this, is it a new thing?","['Have you checked properly maybe they are in hexadecimal? Or even if they are somehow encrypted there must be some sort of function to decrypt it and you might be able to find it in sources', ""Is this as a seller or a buyer? I've used this Chrome extension as a buyer and it works well: [Amazon Order History Reporter](https://chrome.google.com/webstore/detail/amazon-order-history-repo/mgkilgclilajckgnedgjgnfdokkgnibi)""]"
Python script that scrapes the People also ask section from Google in the niche you want and publishes it to a WP website.,https://www.reddit.com/r/webscraping/comments/zyve7l/python_script_that_scrapes_the_people_also_ask/,webscraping," 

I found a python script that scrapes the people also ask section in your niche and publishes automatically to your WordPress website.

[https://www.youtube.com/watch?v=JJMHZ2qbjBg&t=139s](https://www.youtube.com/watch?v=JJMHZ2qbjBg&t=139s)

Based on the article it scrapes 25 questions and answers in your niche and publishes them to your WP website as a post. It also scrapes an image and YT video based on the title.

More info [https://sarc-wv.com/people-also-ask-script-scrape-and-publish-full-step-by-step-installation-guide/](https://sarc-wv.com/people-also-ask-script-scrape-and-publish-full-step-by-step-installation-guide/)

I am wondering, how to create something like this, without paying for it.

Thanks","['Well to ‚Äúcreate something like this, without paying for it‚Äù you need to know python\n\nDo you have any knowledge of python? or maybe any other language like JavaScript?']"
Beginner tutorial on scraping websites in Javascript,https://youtu.be/ssRo5nVOvrQ,webscraping,,
scraping JavaScript based website,https://www.reddit.com/r/webscraping/comments/zydznc/scraping_javascript_based_website/,webscraping,"Hi guys, this is my first time taking a task on web scraping, I'm trying to scrape a JavaScript website, the problem is that I'm already using selenium by certain tags don't seem to be scraped even tho they appear in inspect tool and after disabling JavaScript

If anyone can help i will be sooo grateful, and thank you in advance

This is the link of the website: https://www.ouedkniss.com/services-evenements-divertissement/1

I'm trying to get the emails if those announcements: 

from bs4 import BeautifulSoup
from selenium.webdriver import Chrome
from selenium.webdriver.chrome.service import Service
from selenium.webdriver import ChromeOptions
import pandas as pd

Options = ChromeOptions()
Options.headless = True
driver_service = Service(executable_path=r""C:\Users\Lilia\Desktop\WebScraping\chromedriver.exe"")
driver = Chrome(service=driver_service)
driver.get('https://www.ouedkniss.com/services-evenements-divertissement/1')
soup = BeautifulSoup(driver.page_source, 'lxml')


def Extract_Emails():
    elements = soup.findAll('div', class_=""col-sm-6 col-md-4 col-lg-3 col-12"")
    script = soup.find('script')
    print(script)
    
    for element in elements:
      link = element.find('a', class_='d-flex flex-column flex-grow-1 v-card v-card--link v-sheet o-announ-card-content theme--dark')
      driver.get('https://www.ouedkniss.com'+link['href'])
      soup1 = BeautifulSoup(driver.page_source, 'lxml')
      email = soup1.find('span', class_='v-chip__content')
      print(email)

Sorry, i couldn't attach the pic of the output of the inspect tool","[""I'm on my phone now so can't really check it, but maybe look for the hidden api: https://youtu.be/DqtlR0y0suo\n\nThe website is probably hitting an api to generate the data you need to scrape and then using javascript to actually embed them, check the video it explains a lot!"", 'Share the website and what element you want to get. And maybe share your code so we can see what you did wrong and we can fix it for you.', 'There is a graphql endpoint that you can query quite easily, if you look at your browsers Network tab when you reload the page you\'ll see a bunch of requests that get the data from the backend api.\n\nI\'ve recreated the two important requests below, first to get all the events (and their ids) then to get their email address (using the event id, there is a query for their phone number too, lol, seems safe) the below code is very rough but will get all the emails:\n\n\n    import requests\n    \n    graphql_url = ""https://api.ouedkniss.com/graphql""\n    \n    for page in range(1,25):\n    \n        events_payload = {\n            ""operationName"": ""SearchQuery"",\n            ""variables"": {\n                ""mediaSize"": ""MEDIUM"",\n                ""q"": None,\n                ""filter"": {\n                    ""categorySlug"": ""services-evenements-divertissement"",\n                    ""origin"": None,\n                    ""connected"": False,\n                    ""delivery"": None,\n                    ""regionIds"": [],\n                    ""cityIds"": [],\n                    ""priceRange"": [None, None],\n                    ""exchange"": False,\n                    ""hasPictures"": False,\n                    ""hasPrice"": False,\n                    ""priceUnit"": None,\n                    ""fields"": [],\n                    ""page"": page,\n                    ""count"": 48,\n                },\n            },\n            ""query"": ""query SearchQuery($q: String, $filter: SearchFilterInput, $mediaSize: MediaSize = MEDIUM) {\\n  search(q: $q, filter: $filter) {\\n    announcements {\\n      data {\\n        ...AnnouncementContent\\n        smallDescription {\\n          valueText\\n          __typename\\n        }\\n        noAdsense\\n        __typename\\n      }\\n      paginatorInfo {\\n        lastPage\\n        hasMorePages\\n        __typename\\n      }\\n      __typename\\n    }\\n    active {\\n      category {\\n        id\\n        name\\n        slug\\n        icon\\n        delivery\\n        priceUnits\\n        children {\\n          id\\n          name\\n          slug\\n          icon\\n          __typename\\n        }\\n        specifications {\\n          isRequired\\n          specification {\\n            id\\n            codename\\n            label\\n            type\\n            class\\n            datasets {\\n              codename\\n              label\\n              __typename\\n            }\\n            dependsOn {\\n              id\\n              codename\\n              __typename\\n            }\\n            subSpecifications {\\n              id\\n              codename\\n              label\\n              type\\n              __typename\\n            }\\n            allSubSpecificationCodenames\\n            __typename\\n          }\\n          __typename\\n        }\\n        parentTree {\\n          id\\n          name\\n          slug\\n          icon\\n          children {\\n            id\\n            name\\n            slug\\n            icon\\n            __typename\\n          }\\n          __typename\\n        }\\n        parent {\\n          id\\n          name\\n          icon\\n          __typename\\n        }\\n        __typename\\n      }\\n      count\\n      __typename\\n    }\\n    suggested {\\n      category {\\n        id\\n        name\\n        slug\\n        icon\\n        __typename\\n      }\\n      count\\n      __typename\\n    }\\n    __typename\\n  }\\n}\\n\\nfragment AnnouncementContent on Announcement {\\n  id\\n  title\\n  slug\\n  createdAt: refreshedAt\\n  isFromStore\\n  isCommentEnabled\\n  userReaction {\\n    isBookmarked\\n    isLiked\\n    __typename\\n  }\\n  hasDelivery\\n  deliveryType\\n  likeCount\\n  description\\n  status\\n  cities {\\n    id\\n    name\\n    slug\\n    region {\\n      id\\n      name\\n      slug\\n      __typename\\n    }\\n    __typename\\n  }\\n  store {\\n    id\\n    name\\n    slug\\n    imageUrl\\n    __typename\\n  }\\n  user {\\n    id\\n    __typename\\n  }\\n  defaultMedia(size: $mediaSize) {\\n    mediaUrl\\n    __typename\\n  }\\n  price\\n  pricePreview\\n  priceUnit\\n  oldPrice\\n  priceType\\n  exchangeType\\n  __typename\\n}\\n"",\n        }\n    \n        event_resp = requests.post(graphql_url, json=events_payload).json()\n    \n        for event in event_resp[\'data\'][\'search\'][\'announcements\'][\'data\']:\n    \n            email_payload = {\n                ""operationName"": ""UnhideEmail"",\n                ""variables"": {""id"": event[\'id\']},\n                ""query"": ""query UnhideEmail($id: ID!) {\\n  email: announcementEmailGet(id: $id)\\n}\\n"",\n            }\n    \n            resp = requests.post(graphql_url, json=email_payload).json()\n            \n            print(event[\'title\'],\'|\',resp[\'data\'][\'email\'])', ""In most cases it is a good idea to wait for a particular selector and then obtain the contents.  \nI don't know about Selenium, but with Puppeteer some websites take longer to load and if I do not wait for the container I am interested in, it will not be present in the scraped HTML.\n\nI hope that helps.""]"
"Built a Telegram Scraper, need suggestions",https://www.reddit.com/r/webscraping/comments/zydj5y/built_a_telegram_scraper_need_suggestions/,webscraping,"Hey guys, I've built another web scraper that can scrape a telegram group members and saves them as a contact, the scraper then adds the contact created to the desired group. The scraper is scraping 700 contacts per hour, I'd like suggestions to put this scraper to some good use, your suggestions are much appreciated.
Thanks",['Sell it to some crypto start ups']
Recapture data from deleted Reddit replies,https://www.reddit.com/r/webscraping/comments/zy8caf/recapture_data_from_deleted_reddit_replies/,webscraping,"I posted in a Reddit sub asking if it was ok to post a product I‚Äôm developing once it‚Äôs finished, in order to get peoples feedback. No one actually answered my original question but turns out a lot of people were interested and put their hand up to beta test the app once it becomes available. 

Turns out I should have read the rules - no ‚Äúadvertising‚Äù is allowed in, but that was what my initial question to the sub was about. 
The post stayed up for about a week and then the mod came along and deleted every response I got to the post. 

I would like to get the usernames of the people who responded to me so I can DM then in the future (most of them asked me to). Is there any way to recapture this data? I don‚Äôt know if scraping is an option for this? Or
might an older version of the post be cashed on my device somewhere?",['You can try archive.org']
Is there a site where I could run a bunch of data or even a url through it and it could machine learn enough to come up with questions?,https://www.reddit.com/r/webscraping/comments/zxzpoj/is_there_a_site_where_i_could_run_a_bunch_of_data/,webscraping,"
For example, say I wanted to run all the post titles of a sub and the machine could come up with its own questions using similar formats for questions from prior posts? 

Or if that would take a bunch of coding to implement, is there a way to easily webscrape a bunch of titles from a sub so I could at least see the most commonly used words?","['Webscraping all the titles probably wouldnt be that hard. You could honestly just find an intro to webscraping guide on youtube and just copy the code in the example and use that with a few slight modifications. Most examples will pick stuff from a page and do something with it so you just chamge it slightly to work for reddit and automate a way for it to look through all the links. It shoulsnt be too bad with a tool like playwrite for nodeJS.', 'One way I think you can do it is by scraping the titles and asking ChatGPT to come up with questions like these', 'You can add whatever data you want to GPT-3: https://openai.com/blog/customized-gpt-3/\n\nDepending on your use case, this may not even be necessary.  You might be able to just stuff a dozen examples of the questions you want into the prompt and be off to the races.']"
Scraping tumblr,https://www.reddit.com/r/webscraping/comments/zxz7a7/scraping_tumblr/,webscraping,"Hi everyone!  
I come to you in a time of need, since I have no idea how scraping or programming works, so I was hoping there's an easier way that I haven't found yet.  
Here's the deal: I need a way to find every post on tumblr that has a specific note. For example, if I were to reblog or like a post, my username would show up underneath and it'd say ""'username' liked this.""   
As you can imagine, there are millions of posts and going through all of their notes manually is impossible, so I figured that:  
a. Either I download the whole thing and somehow search locally on my pc, though it'd probably take a lot;  
b. Find a software that crawls the site and then apply a filter to that username to find the posts they've reacted to.  
Is this doable? Is there anyway someone could do this?  
I'm sorry if I don't make much sense, I have no experience with python and other stuff that is usually used in this sort of things. Thank you so much!",['that sounds like big task.\n\nFirst off if you want to search through all the posts the first thing you would need to do is create a database and scrape all the posts url.\n\nConsidering it has hundreds of billion of posts it can take millions of man hour just to scrape the whole database. But you can of course bypass that with some proxies.\n\nI never used tumblr before so let say it has the same format as following and followers. If you only wanted to find the posts that you were following then that would be a doable task.\n\nYou can simply use python requests to each username you follow and then copy down each posts link. After that you can go through each link and check the html source for your specific username.']
Need help webscraping a manga I purchased on DLsite,https://www.reddit.com/r/webscraping/comments/zxyuf6/need_help_webscraping_a_manga_i_purchased_on/,webscraping,"I have purchased a few read-only books on DLsite, so I cannot download them like other works I have already purchased. The book I am trying to webscrape is Famiresu Senshi Purin as shown here (NSFW link warning):

https://www.dlsite.com/comic/work/=/product_id/BJ338851.html

For anyone else who uses the webscraper Chrome extension, how would I be able to download the images from the book on DLsite?","[""I have checked out the website. It seems like they are using javascript to load in the content.\n\nYou can use bs4 and selenium. Right click is disabled so you can't inspect element the exact location but i have it down.\n\nI made a quick script that will get all the images from the html source\n\nhttps://pastebin.com/zNa54nwh""]"
Scrape Fb post from group and push to web?,https://www.reddit.com/r/webscraping/comments/zxyu63/scrape_fb_post_from_group_and_push_to_web/,webscraping,"Just came across this website that seems to be pulling content from their group automatically. They are then segmenting the data and pushing it to a site where they are allowing someone to play with filtering options like a live excel. Really curious if this is custom or this a platform providing this service?

The website is larvato.com 
(Not my site obviously)","[""Okay, so there are two features here in this setup. One is pulling posts from Facebook, the other is displaying the data on website. I don't think you will be able to find a solution integrating both features. \n\nSo what you can do is search for a Facebook scraper online, I am sure you will find a few companies providing this service. Then all you will need to do is save this data on your database and display it on your website. \n\nIt will require a fair bit of coding so if you aren't comfortable with it, you can hire someone to do it. In fact, I provide this service myself. If you are interested, you can pm me."", 'You simply need a $3 vps and then run a script that scrapes that group using bs4 and requests and then send it back to the database.', 'Is this a public group?']"
"Webscraping for Emails across Internet, Verifying Emails",https://www.reddit.com/r/webscraping/comments/zxxslb/webscraping_for_emails_across_internet_verifying/,webscraping," I am learning python, and would like to learn how to webscrape for emails across the internet AND verify emails.  

Where can I find more information to focus on this area?  Thanks!","[""What type of emails are you trying to get? in what niche? I'm pretty sure the people that have sources for scraping emails won't share it to profit on it."", ""I will be glad if you check the software I'm developing. You have a chance to become our first customer :)  \nhttps://myfolder.notion.site/myfolder/Phantom-Connect-0aedc60ce03043cc83f8fd55aa558d9c""]"
Having trouble scraping from wiktionary,https://www.reddit.com/r/webscraping/comments/zxq7og/having_trouble_scraping_from_wiktionary/,webscraping,"I don't really know if anyone has experience with this, but I'm trying to web scrape this table of most common Mandarin worsd from wiktionary using BeautifulSoup. 

[https://en.wiktionary.org/wiki/Appendix:Mandarin\_Frequency\_lists/1-1000](https://en.wiktionary.org/wiki/Appendix:Mandarin_Frequency_lists/1-1000)

Three problems: 1) it prints the table really weirdly, where I want it to be like on line 62  (as an example) and orderly (simplified, traditional, pinyin (latin pronounciation), and meaning). For some reason it just outputs the pinyin on the last line on one and starts again on a new line, with NaN and file after it

2) The meaning doesn't show up for any of them

https://preview.redd.it/ldgz5xieeq8a1.png?width=571&format=png&auto=webp&v=enabled&s=3c417e4e38c55ab3a6502dbcab15f38e137d331d

&#x200B;

3) It stops abruptly at line 312 with a bad character symbol when there's way more for it to go

&#x200B;

https://preview.redd.it/28u8jad1fq8a1.png?width=469&format=png&auto=webp&v=enabled&s=f4ca6872fe79e357582243cd2596816073e1e4f1

Any help would be greatly appreciated","[""Here's the code:\n\n`import requests`\r  \n`import pandas as pd`\r  \n`from bs4 import BeautifulSoup`\r  \n\r  \n`wiki_url = 'https://en.wiktionary.org/wiki/Appendix:Mandarin_Frequency_lists/1-1000'`\r  \n\r  \n`response = requests.get(wiki_url)`\r  \n`soup = BeautifulSoup(response.text, 'html.parser')`\r  \n\r  \n`mandarintable = soup.find('table', attrs={'class': 'wikitable'})`\r  \n`print(mandarintable)`"", ""It comes out weird because you didn't also find the child elements like the tr and td. But the other problem i found is the elements inside tr also contains another tr so if you do find\\_all then you it could also cause some automation issues. \n\nThe bad character symbols can be because you didn't add any headers such as Accept-Encoding which usually translate certain texts.""]"
Python Library to scrape RSS-Feeds from waybackmachine?,https://www.reddit.com/r/webscraping/comments/zxduid/python_library_to_scrape_rssfeeds_from/,webscraping,"Hi,
my goal is to Archive all rss-feeds by a website from the last 5 years. Is there a easy way to combine beautifulsoup and some libarys to use waybackmachine for this?","[""I don't have a library but you can use this code which should get you most of the way there, it uses some backend requests to get the data you want. I've broken out of the loops at the end just for testing so delete those to get all the data\n\n\n    import requests\n    import urllib.parse\n    \n    site = 'http://feeds.bbci.co.uk/news/rss.xml'\n    year = 2020\n    \n    safe_site = urllib.parse.quote_plus(site)\n    \n    url = f'https://web.archive.org/__wb/calendarcaptures/2?url={safe_site}&date={year}&groupby=day'\n    \n    resp = requests.get(url).json()\n    \n    for item in resp['items']:\n        if len(str(item[0])) == 4:\n            day = str(item[0])[-2:]\n            month = str(item[0])[:2]\n        else:\n            day = str(item[0])[-2:]\n            month = str(item[0])[:1].zfill(2)\n    \n        day_url = f'https://web.archive.org/__wb/calendarcaptures/2?url={safe_site}&date={year}{month}{day}'\n    \n        time_data = requests.get(day_url).json()\n        print(f'Finished {day_url}')\n    \n        for t in time_data['items']:\n            time_str = str(t[0]).zfill(6)\n    \n            final_url = f'https://web.archive.org/web/{year}{month}{day}{time_str}/{site}'\n            output = requests.get(final_url)\n            print(output.text)\n            break\n        break"", 'You can start with feedparser, it‚Äôs easy to use, if RSS (XML) is properly built.', 'You can explore [FeedParser](https://github.com/kurtmckee/feedparser) too']"
Best method to scrape/copy web quick,https://www.reddit.com/r/webscraping/comments/zwvzkm/best_method_to_scrapecopy_web_quick/,webscraping,"There's a web quiz here: [https://theroasterie.com/pages/coffee-quiz](https://theroasterie.com/pages/coffee-quiz)

Short of clicking through every option and typing it up in a spreadsheet, does anyone have suggestions on a more efficient way to copy this over? A bot maybe?","[""if you're non technical and its a simple scrape, your best bet is checking out some browser extensions"", ""Use Python and Selenium. You automate the browser actions (clicks on elements) and scrape the pages' content as you continue towards the end of the quiz. Pretty basic stuff actually."", 'Requests and bs4 would be even easier. If the data is being provided by an api all you would need to do is parse it with json.', 'This might help:\n \nhttps://docs.google.com/spreadsheets/d/1skwk18o1bj2aU4tkEto11VZZQ1vMpP_ynnGm_ti1CKc/edit?usp=sharing']"
Can you program a BeautifulSoup Web scrapper?,https://www.reddit.com/r/webscraping/comments/zx324c/can_you_program_a_beautifulsoup_web_scrapper/,webscraping,Looking for someone who knows how to program and write a web scrapper using the beautifulsoup software. Message me for more details about the web scrapper I'm looking to have created. Thank you!,"['If the request is not too hard you can message me and I can do it for free.', 'Hey there what is it you want scraped, I can surely help you with that.\nIs there a mail to reach you out something to chat about the details', 'Github - E2E-SSE\n\nEMAIL GNSGroup@mail.com \n\n&#x200B;\n\nI can build scrappers from the ground up utilizing APIs, bs4 and even selenium for advanced scrapping\n\nI am not limited to python either.\n\n$20 ETH or BTC', ""I'm developing automation software that scrapes multiple websites using beautifulsoup. I've also added a custom requests section, and I think I can help you with that. website link - https://myfolder.notion.site/myfolder/Phantom-Connect-0aedc60ce03043cc83f8fd55aa558d9c""]"
Help scraping a Pdf from a Notion Page,https://www.reddit.com/r/webscraping/comments/zvyxik/help_scraping_a_pdf_from_a_notion_page/,webscraping,"Noob here so sorry if this is a common question. I was trying to download the fcp assistant editing pdf thats at the bottom of this webpage.""[shorturl.at/glq06](https://shorturl.at/glq06)"" Ive tried a couple of different way but nothing seems the work Any help is appreciated.",[]
is manual scrapping still alive?,https://www.reddit.com/r/webscraping/comments/zv7t91/is_manual_scrapping_still_alive/,webscraping,"I am trying to understand if the current automated tools cover all spectrum of data scrapping projects? I feel like the automated tools, though beneficial in many scenarios, cannot cover many projects that might be small,m or very complex for an automated tool, or maybe because the tools are complex to use.","[""Manual data entry/scraping is not a thing anymore unless it's something super casual. \n\nHowever, human-assisted scraping, like using bookmarklets or tampermonkey scrape scripts, is totally underrated. Especially when it comes to difficult targets that block hard, like LinkedIn etc. You can navigate to data endpoints and click a script to save the data whenever. It doesn't scale that well if you need thousands of scrapes but for that middle ground where it's too difficult to write a scraper and too exhausting for manual data entry the browser scripting niche is great!"", 'For some client jobs, hand copy and paste is still the cheapest solve VS automation (rarely, but it‚Äôs for sure real)']"
What to do when I just can't solve a problem?,https://www.reddit.com/r/webscraping/comments/zv05nl/what_to_do_when_i_just_cant_solve_a_problem/,webscraping,"I just can't figure out this problem, and what's worse, I can't seem to accurately describe the problem.  I'm wondering if I can hire someone by the hour, but I really don't know where to start with that.

Are there services for something like this?  Any you guys would recommend?","['lol just post the problem dude', 'If you can‚Äôt describe the problem, it means you don‚Äôt understand it, which could mean the problem is elsewhere that you looking. It is more difficult or impossible for us to help you though. \n\nTry to describe it by saying what is related to, and what was your path which ended in facing the problem. Also - do you know why you can‚Äôt solve it? Is it your lack of knowledge or maybe lack of software/hardware resources?', 'Don‚Äôt get frustrated\n\nhttps://www.motogp.com/api/results-front/be/results-api/session/938282af-aed2-4110-bcfd-19bf8210b229/classifications\n\nThis endpoint returns json which has all the info you need', ""Hi, if you'd like I can take a look at this tomorrow. I am a full time webscraping freelancer. Thanks and happy holidays.""]"
Possible to scrape Facebook for Business Pages?,https://www.reddit.com/r/webscraping/comments/zuwqiu/possible_to_scrape_facebook_for_business_pages/,webscraping,title,"['If you can get enough accounts, nowdays as soon as you create an account and the second you do aomething that looks suspicious they require a phonenumber if you arent banned instantly their ai is becomming quite good at detecting stuff like that.\n\nA possible workaround could be trying to scrape the data trough search engines and good proxies so you dont make the requests to facebook but rather look for facebook pages trough search engines. However they have limits as well but again there are workarounds for as well.\n\nIt is not impossible. But it requires some creativity and planning.', 'If you‚Äôre talking about finding pages then it‚Äôs very simple. You can use something called ‚Äúdorks‚Äù it‚Äôs basically a more efficient way of searching for what you need.\n\nExample of google dork site:Facebook.com ‚Äúdogs‚Äù\n\nI‚Äôm not sure if this is the correct way but if it is it should return results with anything facebook links that includes dogs. Of course you would need to tweak the link for it to only show groups.', ""Yes. I'm developing automation software that scrapes multiple websites including Facebook. I've also added a custom requests section, and I think I can help you with that. website link - https://myfolder.notion.site/myfolder/Phantom-Connect-0aedc60ce03043cc83f8fd55aa558d9c""]"
"why some proxy doesn't provide ID, PW, DNS, PORT in simple from??",https://www.reddit.com/r/webscraping/comments/zup8jp/why_some_proxy_doesnt_provide_id_pw_dns_port_in/,webscraping,"recently, I've been trying free plans or trials from so many proxy provider to find valuable.

I found out there are two types.

  \- A : simply provide ID, PW, DNS, PORT and additional. 

  \- B : never provide things like case 'A', but kind of example like below

ex) protocol:// {there own url} & url = {target website url} & api\_key = { api key } & api\_option = true

&#x200B;

I am a noob scraper, so I couldn't imagine how to use them in my puppeteer scripts. I tried google as well but never found about my issue.  One of them had a chat with me, guided me that I just need to take api\_key as username and leave empty for password. That worked with their product but never worked with the others. 

&#x200B;

Please anybody help me to use it as proxy.","['Read about how URLS are formed', 'Type A is a proxy using the official documentation for http/https/socks4/socks5. \n\nType B is just a http endoint (think of it like a normal website, but instead of accesing a website it reroutes to the url)\n\nType A is better supported, has higher bandwith throughput and lower ping, but since the socks4, socks5 and http protocols are pretty limited, servers cannot see which domain, subdomain or stuff, they only know where to connect, your username and in socks5/http your password too (If you use authentification)\n\nUsing a general web server as a proxy has pretty much 0 support, so if your proxy provider stops working for any reason you would have to change your entire code\n\nTo combat this the only way (without breaking backwards compatibility) is to make a new protocol, socks6 sounds like a good ideea if you ask me']"
What's the easiest pre-packaged scraper (not language or scripting tools) that would work with LinkedIn Sales Navigator?,https://www.reddit.com/r/webscraping/comments/zuhqvy/whats_the_easiest_prepackaged_scraper_not/,webscraping,"I'm hearing about Equests + BS4, but these are very technical solutions. I was wondering what scraper such as DataMiner, is the most easiest, but most mature solution out there. 

Thanks in advance.","[""Don't waste your time scrapping data from linked in you will get banned if you do that."", 'Isn‚Äôt LinkedIn kind of militant when it comes to combating scraping? I recently read about their efforts to make it difficult. I have no personal experience on the topic, however.', ""I don't know about a pre packaged scraper but I have built one for it so it's definitely possible."", 'Dux soup or evaboot (sales nav only)', 'You can use third party scrapers that provides you with a pool of ip. Some provides headless browsers for JavaScript loading']"
Selenium's .text attribute isn't working despite being able to find the element,https://www.reddit.com/r/webscraping/comments/zuoijg/seleniums_text_attribute_isnt_working_despite/,webscraping,"`from requests_html import AsyncHTMLSession, HTMLSession`  
`from selenium import webdriver`  
`from selenium.webdriver.common.by import By`  
`import time`  
`import json`  
`import csv`  
`import re`  
`import pandas as pd`  
`import requests`  
`from pandas import json_normalize`  
`from bs4 import BeautifulSoup`  


  
`driver = webdriver.Chrome()`  
`driver.set_page_load_timeout(30)`  
`driver.get(""https://app.prizepicks.com/"")`  


`grid = driver.find_elements(By.ID, ""projections"")`  


`for players in grid:`  
`names = players.find_elements(By.CLASS_NAME, 'name')`  
`scores = players.find_elements(By.CLASS_NAME, ""strike-red"")`  
 `print(scores[3].text)`

&#x200B;

Here, the last print statement is supposed to print an NBA player's name from the website (Prizepicks). It prints whitespace despite the text being in the HTML/CSS, and I have correctly identified the element in my code.","[""Wouldn't it be easier to simply use their API to extract the data you want?\n\nhttps://api.prizepicks.com/projections?league_id=7\n\nhttps://stackoverflow.com/questions/74168861/need-help-scraping-data-from-prizepick-api""]"
"Trying to scrape this side, but the accept cookies button is making other elements unable to be interacted with.",https://www.reddit.com/r/webscraping/comments/zumfh8/trying_to_scrape_this_side_but_the_accept_cookies/,webscraping,"I feel like my html knowledge is the issue here.  I rely a lot on the inspect element function to find the element I want, but I can't seem to right click on the button specifically.

I'm using Selenium right now.  If another tool is better, I'm all ears.

https://www.motogp.com/en/gp-results/2022/VAL/MotoGP/RAC/Classification","['Accept the cookies as part of your script.', ""If you need to click some link:\nUsing Selenium is not like control mouse on your screen, so you can open web page on your real browser, accept cookies and find link you need to click. then put that xpath to your code to see if Selenium can click to that link without accept cookies.\n\nOf you need to get html data:\nDon't use Selenium parser, it slow. Return html and use other parser tool like beautifulsoup (parsel or selextolax is better) to get data."", 'I have ran into this. Try looking up ‚Äúselenium wait for element to appear‚Äù or something like that. I used that to get around it.. or maybe headless browser with options']"
YouTube get Homefeed,https://www.reddit.com/r/webscraping/comments/zuiiny/youtube_get_homefeed/,webscraping,I want to get the urls from the videos of a users homefeed. I already look through the api but YouTube disabled the option to get a users home feed. I thought about scraping the urls from YouTube.com but this seems to be a bad option too because you have to log into YouTube to get a users videos‚Ä¶ any idea to solve this ?,['Every YouTube channel has its RSS link. Just parse it.']
Does Noone Use Firefox Anymore?,https://www.reddit.com/r/webscraping/comments/zuhq8e/does_noone_use_firefox_anymore/,webscraping,"All the 'undetectable' webdrivers seem to be built for Chrome, the Selenium firefox 'patches' haven't worked in a year. Does anyone here still use FF when using browser-based scraping?","[""I've used it with selenium and gekodriver it comes out to be mostly the same from what I've seern"", ""It's mostly just a convenience thing. Chrome is a more popular browser with better CDP (Chrome devtools protocol - that's the layer that allows communicating with the browser) implementation, so naturally, most developer efforts go towards popular tech."", 'Yes and it works fine']"
Unable to get text from simple <div>,https://www.reddit.com/r/webscraping/comments/zuhjhf/unable_to_get_text_from_simple_div/,webscraping,"Can someone help me this. I was able to get to this div. But when I tried to get the text 'time is now', it is not grabbing that. Here is the html sniplet:

`<div class=""col-md-9"">`

`<span id=""clock""></span><i class=""far fa-clock small mr-1""></i>time is now`

`</div>`

However, when I tried to read time using Python

`timetext = response.xpath(""//div[@id='ContentPlaceHolder1_maintable']/div[4]/div/div[2]"")[0].text`

I received no value. If I tried to 

`timetext = response.xpath(""//div[@id='ContentPlaceHolder1_maintable']/div[4]/div/div[2]"").text`

Then I am getting an error *'list' object has no attribute 'text'*

I know that I am able to read the correct div as I was able to read everything else. Thanks for your help.

TP","[""Yourxpathhere//text()').extract()[2]  -or whatever index for text you need. - if your using scrapy and it looks like you are.""]"
not sure where to start,https://www.reddit.com/r/webscraping/comments/zuc444/not_sure_where_to_start/,webscraping,looking to extract lots of data from a website that requires individual searches for each time. not sure where to get started. any help?,"[""Yes, I'm available to help Dm me""]"
How to verify a customer paid on a confirmed order,https://www.reddit.com/r/webscraping/comments/zud41j/how_to_verify_a_customer_paid_on_a_confirmed_order/,webscraping,"Hi!  New Facebook business owner and I sent a customer an invoice through the messenger app via Meta.  The customer confirmed the order, but how can I see if she paid?  And how would I enter a tracking number to prove shipment?  TIA!",['huh?']
Need industry classification data from website,https://www.reddit.com/r/webscraping/comments/zuaei6/need_industry_classification_data_from_website/,webscraping,"Hi, I am looking to scrape industry classification data. Not an expert programmer, posting here as i was unable to find any element to search. Looking for a way forward. At this point i do not even know if this data can be scraped or not.

I have list of entries which will replace DCXINDIA in the URL, would like to scrape the information in the image below.

[https://www.nseindia.com/get-quotes/equity?symbol=DCXINDIA](https://www.nseindia.com/get-quotes/equity?symbol=DCXINDIA)

&#x200B;

https://preview.redd.it/5hhy5uzwqu7a1.png?width=598&format=png&auto=webp&v=enabled&s=368be7d8ae61e531d58d947c6e830386d573a46b

Thanks",['https://www.nseindia.com/api/quote-equity?symbol=DCXINDIA']
Web Scraping,https://www.reddit.com/r/webscraping/comments/ztq5nu/web_scraping/,webscraping,Is it legal to scrap data from a website like [rockauto.com](https://rockauto.com) ?,"['It‚Äôs legal to scrape publicly available data. Data behind a membership wall such a LinkedIn could expose you to a lawsuit if you grab it, because their terms and conditions say not to. Would you win in court? Who knows? But the cost of defending yourself would be high.\n\nHow you the data on publicly available sites matters. Suppose you plan to repost all of rockauto‚Äôs data on another website. I‚Äôm guessing that they would take legal action.', 'Depends on the country actually.  \n\n\nI had a customer with similar situation and I know something about it.\n\nIn most countries it depends on what you do with the data. It is similar to the standard intelectual property laws.  \n\n\nIf you resell or make business around data that you copied 1:1 (even if it is public like a real estate listing website) the website owner could take legal actions against you and will probably win.  \n\n\nIf you however offer significant enough added value on top of that data, then you will probably be fine.  \n\n\nExample 1:  \nIf you create a website for car sale ads and you just fill it with stolen ads from another website - you are in trouble.  \n\n\nExample 2:  \nIf you pull data from various websites, run analysis, indicate what is good/bad price for a car of given type, age and mileage, provide historic price chart, etc. etc. This is much more than simply reposting data and one can argue the main ""product"" of your website is that additional information, not the scraped ads per se. Then courts have ruled in favor of the scraper.  \n\n\nThis is in no way a leagal advice. Every country has different laws, they change all the time. It also kind of depends on who has the better lawyer.  \n\n\nI hope this provides a bit more clarity.']"
Looking for a website where I can scrape results of baseball by innings.,https://www.reddit.com/r/webscraping/comments/ztlr5g/looking_for_a_website_where_i_can_scrape_results/,webscraping,"Hi everyone, 

For a personal project, I would like to have a dataset who contains all results of baseball matches by innings.  Is there anyone who know a website where I can scrape these data ? 

Thank you for your time.",['Check out [Retrosheet](https://www.retrosheet.org/). \n\nThis should do the job.']
ok idk if this has bin done or not,https://www.reddit.com/r/webscraping/comments/ztmjzf/ok_idk_if_this_has_bin_done_or_not/,webscraping,"Selenium scraper that runs off a json file I'm codeing it in python but making it so ya just need to make a Jason string in the correct format and it is basically like an old school punch card therefore you could actually make the instructions using most not all program languages. Or even put together by hand allowing for simple tool that a novice someone who doesn't wanna learn . A whole new language and a whole new API could potentially use.or for larger projects may be able to submit a a json object and it could be ran with the benefit of rotating  proxy's and if needed some experience but without the trouble or cost of having to set it up from scratch .example maybe the customer is a web developer or even just a computer litterat person who can mess around with. Some developer tools on there browser  and figure out. What I think is a straight forward syntax of{'url':'starturl','varfl':'one per line variables in my test case parcel numbers','outfl':'outfile', 'actlst':[[{'id':'elementsid','act':'enter(v)'},{'class_name':'elementclasdname', 'act':'click'}],[this would be the second page of actions . As much as possible I will try to build in things like waiting for the page load and later add randomization selectable by option to void antibot detection that hopefully will have no more configuration then simply switching 'humametrics':from false to true at a very feature less level now. I don't really so far need much in features so I'm trying to get basic functions to be 100% instead of having random glitches but I know some of my future targets are not as easy 
I'm basically writing this cus I want to write apisbut find the gathering of data to supply them with to be a pain",[]
Why does the same beautifulSoup code not work for certain real estate sites?,https://www.reddit.com/r/webscraping/comments/zti5v1/why_does_the_same_beautifulsoup_code_not_work_for/,webscraping,"Just for practice, I‚Äôm trying to scrape real estate websites using the same basic code structure each time. For some reason it doesn‚Äôt work with this website. Here is my code:

import requests
from bs4 import BeautifulSoup
url = 'https://www.rentalia.com/holiday-rentals-italy/'
page = requests.get(url)
soup = BeautifulSoup(page.content, 'html.parser')

lists = soup.find_all(""div"", class_=""itemList itemRow col ng-scope s12 m6 l4"")
print(lists)

This code returns an empty list","[""Because the underlying web page structure won't be the same, each scaper code is usually unique to the webpage.""]"
Any available API to scrape data from journal articles?,https://www.reddit.com/r/webscraping/comments/ztd155/any_available_api_to_scrape_data_from_journal/,webscraping,"Hi! I plan to obtain some data from journal articles.

I'm hoping to create a database to suit the needs of my project and was thinking whether there are any APIs available to assist me. The data that I am looking for are molecular data, mainly their optical properties and ADME-T.",['https://api.crossref.org/']
Grabbing data from betfair,https://www.reddit.com/r/webscraping/comments/zt6g7u/grabbing_data_from_betfair/,webscraping,"Hi, I'm having a little problem with a project I'm doing, I want to grab information from the ruby union page of betfair: [https://www.betfair.com/sport/rugby-union](https://www.betfair.com/sport/rugby-union)

I want to grab only match odds,  however I'm grabbing all the odds match odds, handicap and total points. How can I grab only match odds and not the others? 

I'm using this line to grab:

betodds= driver.find\_elements(By.CSS\_SELECTOR, 'span.ui-runner-price')

if you could help me it would be awesome.","['Betfair have an api, it‚Äôs going to be the best way.']"
AZlyrics,https://www.reddit.com/r/webscraping/comments/zt4j2q/azlyrics/,webscraping,"Does anyone know how to get around the AZlyrics bot detection? I tried adding a 2 second delay but it still kicked me out. Also, anyone know how to regain access once you're already kicked out? :L",['https://pypi.org/project/azlyrics']
World Cup Project,https://www.reddit.com/r/webscraping/comments/zsravq/world_cup_project/,webscraping,"A little side project to help me learn how go webscrape. 
Im a proficient data engineer and am familiar with python, git, sql etc

I wanted to webscrape articles from each of the host nations that competed in the Fifa 2022 World 

I'll be ingesting the data into an Azure data lake storage account from there I'll be running sentiment analysis to try and understand how different nations perceived the World Cup. (I wont need guidance on this part).

Lots of things to unpick here. So over to you experts",['Qatar was the only host nation']
"For Playwright, what language should I choose? Why is it not explained in docs?",https://www.reddit.com/r/webscraping/comments/zsoszl/for_playwright_what_language_should_i_choose_why/,webscraping,"I want to use Playwright for automation mainly for personal projects.

I am more proficient in Javascript but I wonder if it is a good idea to use Playwright to pick up Python. Because Python is a useful language. Any idea? I have find some discussion on Reddit about this, and I wonder why this very important question is not mentioned in the docs of Playwright? When I click ""Get start"" in the homepage of Playwright it just bring me to NodeJS Playwright document..","[""Go for TypeScript, don't use plain JS.""]"
Scraper stuck in infinite loop,https://www.reddit.com/r/webscraping/comments/zs1m27/scraper_stuck_in_infinite_loop/,webscraping,"Hello,

I built a scraper using Python selenium for a website and I am iterating through ""pages"" displayed on the website and I am scraping data by classes. At some seemingly random point the scraper does not scrape anymore (I see that in the chrome driver) but the code is still running and also not throwing any errors. I suspect it is stuck in some sort of loop. The weird thing is that if I stop the code and try to scrape this particular page on its own there are no issues. What could be the problem here?

Any help is much appreciated.","['Run the scraper not headless and watch it', ""There could be a few potential causes for a scraper getting stuck in an infinite loop. Here are a few things you might want to consider checking:  \n1. Make sure you have a termination condition for your loop. It's possible that the loop is continuing indefinitely because it doesn't have a defined end point.  \n2. Check for errors that might be occurring within the loop. If there is an error that is causing the loop to terminate prematurely, this could result in an infinite loop.  \n3. Check for any conditions that might cause the loop to skip over certain iterations. For example, if you have an if statement that is supposed to skip certain iterations, but the condition is never met, the loop could get stuck.  \n4. Check for any issues with the website or web page that you are scraping. It's possible that there could be issues with the website that are causing the scraper to get stuck.  \n5. Check your internet connection. If the connection is unstable or slow, this could cause issues with the scraper.  \nIt's also a good idea to include some debugging statements in your code to help identify where the issue is occurring. This could involve adding print statements to log the progress of the loop, or using a debugger to step through the code and see what is happening at each iteration."", 'mind sharing more details\n\nmaybe screenshot every new page ya visit', 'Chatgpt can help -\nLike for real‚Ä¶', 'Are you logging? Log some of the values of key variables. A lot of packages will automatically log their warnings and errors if you have logging set up. Then check the logs for what‚Äôs happening. I have cracked so many mysteries by grepping or paging through the logs. Hope this strategy helps you out.', 'You might have missed some If condition like not validating the element existence or keeping track of completed pages.']"
webscrape remove.bg,https://www.reddit.com/r/webscraping/comments/zs3apj/webscrape_removebg/,webscraping,i want to upload image and download from remove bg automatically without paying for their api using python. can i has full project that does that? thanks,"['I think it could be simpler to use a background-removal library in Python (for instance Rembg). Beyond the restrictions put in place by remove.bg to prevent scraping you would not need to upload the image and download the output, as everything could be done locally.', ""If a company provides an API, it's generally bad practice to try and get around it.""]"
I built a web scraping system with Python and Celery to scrape millions of websites,https://www.reddit.com/r/webscraping/comments/zr2xmo/i_built_a_web_scraping_system_with_python_and/,webscraping,"Since working on my SaaS product full time I‚Äôve learned a ton about web scraping. 

Yesterday I developed a systems design to scrape company career pages at scale. I‚Äôm slowly scaling up from 300 req/s. 

I can reach about 12,000 pages per second to process 2,000,000 websites in 7 days.

At that rate, I‚Äôll be able to scrape the public pages of large sites like LinkedIn in no time!  

The system with built with Python/Scrapy, Celery, pandas, and Kubernetes. 

Happy to explain more if anyone is interested.","[""I'm equally curious about the how and the why."", 'Talk to us when you make it work, bragger, LinkedIn will block you in no time.', 'Interesting. Do you ever experience any blocks when scraping like this? If so what do you do to mitigate?', 'Did you automate the career page discovery and scraping logic?', ""One thing I struggle to understand is how scrapers are able to make that many requests. I'm always testing from my home machine, and I worry about getting my ip banned. I've used VPNs (like NordVPN) but I don't think that's a scalable solution.\n\nI wonder if you, or anyone else, has resources or processes for scraping at scale without getting banned. Is there services for rotating proxies? Can you recommend any? Is that the right approach? As someone who is relatively new to this, what might be the gateway into that world?"", 'No offense, but 12 000 pages per second is a bit hard to believe. Forget about what kind of scraper, library, orchestrator, whatever you are running. Headless Chrome alone on my i9 9900K with 32 GB of RAM manages about 70-80 pages per second on 100% CPU utilization.  And thats on a very light website like [ipchicken.com](https://ipchicken.com). Lets say with a bit of optimization you can reach a 100.\n\nYou still need at least 120 i9 class nodes to reach 12 000 pages per second and thats assuming that whatever you are using to orchestrate this has zero overhead and you are some sort of a wizard. Unless you run a datacenter at home I can only imagine you will have to use some kind of cloud provider. This class of machines run for 300-700 EUR per month. So we are talking about $36,000 - $84,000 per month for compute alone.\n\nAll that on a self hosted system?\n\nDefinitely interested to hear how you manage that.', ""What does Pandas do? +the how's and why's and following ofc.\nHow does proxies pricing scale with that?"", 'How do you scrape systems that need login and have systems robust against web scraping? I wanted to know the ways to not have my pool of accounts blocked by Instagram as it often happens', 'What is the most important part of process when creating such a scraper?', 'H∆° can you bypass tls fingerprint or antibot system with scrapy?', 'How you are able to scrape LinkedIn public accounts', 'Following', '12,000 pages per second is crazy men, excellent work.  \nInteresting make ML with this data.', 'What are you using for your Database management?', 'Good Luck on the LinkedIn.', ""I have done LinkedIn webscraping.\n\nIt has a unique problem, your bottleneck will not be the number of proxies required but LinkedIn accounts with a sizeable network. \n\nIf you scrape LinkedIn using a new account (assuming you don't get blocked) with no connections, LinkedIn will not show you any data about people. That is because LinkedIn only shows you the profile of people who are at least your 3rd degree connection."", 'Please share updates once you Scrape 2M / week.\nThere may be some surprises there...', 'What hosting do you use for scrapping?', 'Have you tried scraping LinkedIn at this pace?', 'So it doesn‚Äôt exist yet? Do it and tell us then', 'Can u handle a project for me with web-scraping?', 'Have you met Shape Security bot detector and how did you pass it?', 'Hey as a beginner, i am curious and would like to know more.', ""I'd love to hear more about the data engineering side of this. How you went about setting up Kubernetes, which cloud provider your using, etc.\n\nI've been thinking about setting up something similar just for funsies but I'm not totally certain where to start."", 'LinkedIn does not allow so many requests. You need to use multiple linkedIn accounts for sure.\n\nI developed a database/SAAS-product similar to Enlyft that provides users with a list of companies and their respective technology stacks. Once the list is exported, users can set up an automated artificial intelligence messaging campaign.']"
What would be proper terminology for the following web scraping.,https://www.reddit.com/r/webscraping/comments/zrsy10/what_would_be_proper_terminology_for_the/,webscraping,"I am building a project that goes through through multiple pages on a website. I‚Äôm completely self-learning so I am in process of learning English terminology for this computer service. 

The function uses HTML Agility Pack to pick up parent page then goes through each individual link and pulls out the links from that one link. Then it goes through that link. 

I have to talk about this to native English speakers. 

Thank you in advance.","['The process of web scraping where you search a page for links and follow each link you find until you exhaust the list of links is called ""crawling"" or ""web crawling."" Web crawlers, also known as spiders or bots, are automated programs that visit websites and follow links to other pages on the internet. \n\n\nThey are commonly used by search engines to discover new and updated content on the web and to index that content for search results. Web crawlers typically start by visiting a seed set of URLs and then follow links from those pages to discover new pages. They continue this process until they have crawled all the pages they are interested in or until they reach a predetermined stop condition.']"
Scraping with NodeJS vs Python,https://www.reddit.com/r/webscraping/comments/zrdp6x/scraping_with_nodejs_vs_python/,webscraping,"Hey, I'm a newbie at web scraping. I'm familiar with the Javascript ecosystem and have tried basic scraping using different libraries.  


Is it worth it to learn Python specifically for scraping? Is there anything that I cannot do (for scraping) in the Javascript ecosystem?","['You will be able to do the same thing, but imo running JS outside of the browser is inconvenient (you will need something like node or deno)\n\nSince python offers packages like beautifulsoup, scrapy and numpy i prefer working in python, together with the fact i prefer the language i go with Python.\n\nAlso no experience myself with this in JS but im sure both offer headless browser libraries', 'I know both and prefer using nodejs. Puppeteer(google) and playwright(ms) are very better maintained library like selenium.', ""Puppeteer with its various libraries is a bit more versatile than anything in Python, but of course Python is easier and quicker to write for the average case.  It you're great with JS/Node you should probably just stick with that."", 'No. There is nothing you cannot do in js that you can do with python. I think js has the same level if not more powerful ecosystem than python.', ""NodeJS works for web scraping just fine though Python is all around a better language for this niche - better libraries like http2 support, parsing libraries, data validation etc.\n\nHow much scraping do you need to get done? Knowing both JS and Python is generally a very good skill to have in web scraping and any other software medium, so if it's not a one-off task investing some time here might not be a bad idea!\n\nI wrote a few beginner-friendly introduction articles if you're looking for resources to get started:\n- [Web Scraping with NodeJS](https://scrapfly.io/blog/web-scraping-with-nodejs/)  \n- [Web Scraping with NodeJS and Puppeteer](https://scrapfly.io/blog/web-scraping-with-puppeteer-and-nodejs/) (Puppeteer is a library that lets you control real web browsers)  \n- [Web Scraping with Python](https://scrapfly.io/blog/web-scraping-with-python/)"", ""I scrape with node professionally and haven't run into any issues that make me think about switching to another language. It can handle your needs just like most anything else. Use the language you're most comfortable with. Outside of headless browser, you don't need a ton of libraries. There are a few that you might really need to help parse etc... Take a look at crawlee which offers other types of crawlers other than headless browsers."", ""Both NodeJS and Python are popular programming languages that can be used for web scraping. Which one you choose will depend on your specific requirements and preferences.\r  \n\r  \nHere are some factors to consider when deciding between NodeJS and Python for web scraping:\r  \n\r  \n**Language features**: Python is a general-purpose programming language with a large standard library and a large community of users. It has many libraries and frameworks specifically designed for web scraping, such as Beautiful Soup and Scrapy. NodeJS, on the other hand, is a JavaScript runtime built on Chrome's V8 JavaScript engine. It is designed for building scalable network applications and is often used for real-time web applications.\r  \n\r  \n**Performance**: NodeJS is generally faster than Python when it comes to executing code, thanks to its use of asynchronous programming and the V8 engine. However, this can vary depending on the specific use case and the libraries and frameworks used.\r  \n\r  \n**Ease of use**: Both NodeJS and Python are relatively easy to learn and use, but Python may have a slight edge due to its simplicity and the availability of many libraries and frameworks for web scraping.\r  \n\r  \n**Community and support**: Both NodeJS and Python have large and active communities, so you should be able to find support and resources for either language.\r  \n\r  \nUltimately, the choice between NodeJS and Python for web scraping will depend on your specific needs and preferences. You may want to consider trying out both languages and evaluating their performance and ease of use for your particular use case."", ""You can achieve the same result using both languages. \n\nI personally work with NodeJS and puppeteer for scraping. I went for the NodeJS route since I was already familiar with JavaScript and I don't have much experience using python.\n\nAlso there is lot of documentation and tutorials on the internet about scraping with both, so it depends on what you are willing to learn.""]"
Scraping Facebook Business Page Posts?,https://www.reddit.com/r/webscraping/comments/zrdthd/scraping_facebook_business_page_posts/,webscraping,"Hey Guys,  
Basically, I need to scrape all the Facebook posts made on a specific business page. I want to scrape for things like post links, post dates, descriptions, images, likes, and comments. Do you know of software that can do this for a public facebook business profile?","['You can use plenty of things, u can use node axios cheerio, python with BS4, scrapy', 'Well, I have written a scraper in past to do it for someone. Not only did it include business details, but it also fetched information about all running Facebook ads as well. \n\n  \nI am not aware of any readymade tool which can do it, but I would be happy to build one for you. Send me a pm if you are interested.', 'Hey, i have written software that includes this function. The software is already finished and everything works fine except GUI. I can dm you the website link to check all functions maybe your interested in.']"
Merge two dataframes on key ignoring capitlization of that key,https://www.reddit.com/r/webscraping/comments/zrfe8y/merge_two_dataframes_on_key_ignoring/,webscraping,"i am using python pandas.

lets say i have two different dataframes, the two dataframes contains a col called country\_id

but one df has it uppercase like this COUNTRY\_ID and the other has it lower case country\_id

i want to merge these two frames on country\_id but it is not working because pandas is taking the two cols as different names:

so if i write:

    final_df = pd.merge(first_df, second_df, how='left', on='country_id')

or if i write:

    final_df = pd.merge(first_df, second_df, how='left', on='COUNTRY_ID')

both wont work and i will get a KeyError exception. how can i fix that without manipulating my dataframes columns.","[""Why not first, the one column with the uppercase , change it to uppercase-> lowecase and then merge it.\n\ndf\\['Name'\\] = df\\['Name'\\].str.lower()""]"
How to add paramaterX bypass to an openbullet config?,https://www.reddit.com/gallery/zr69ad,webscraping,Hi. How do I add paramaterX bypass token from a web scraping API to openbullet?,[]
"finding chromedriver, glibc version compatibility",https://www.reddit.com/r/webscraping/comments/zr0gse/finding_chromedriver_glibc_version_compatibility/,webscraping,"I'm trying to set up a webscraper in an amazon-linux terminal and I'm having issues with chromedriver glibc compatibility.

&#x200B;

I'm currently using chrome and chromedriver version \~108. So I decided to try installing chrome and chromedriver 102. I still get the same error and the list of versions to try is too huge to trial and error this.

&#x200B;

My glibc version is 2.26-62.amzn2   ...   and it seems more awkward to change that than to use older chromes.

&#x200B;

&#x200B;

Below is the error message when chromedriver tries to open.

&#x200B;

        Traceback (most recent call last):
          File ""/home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/service.py"", line 97, in start
            path = SeleniumManager().driver_location(browser)
          File ""/home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/selenium_manager.py"", line 74, in driver_location
            result = self.run((binary, flag, browser))
          File ""/home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/selenium_manager.py"", line 93, in run
            raise SeleniumManagerException(f""Selenium manager failed for: {command}. {stderr}"")
        selenium.common.exceptions.SeleniumManagerException: Message: Selenium manager failed for: /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager --browser chrome. /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager: /lib64/libc.so.6: version `GLIBC_2.29' not found (required by /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager)
        /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager: /lib64/libc.so.6: version `GLIBC_2.28' not found (required by /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager)

&#x200B;

How can I find chromedriver glibc version compatibility requirements / How can I find which version of chromedriver I need?",[]
[Scrapy]Where can I find this graphed data???(please help or give suggestions),https://www.reddit.com/r/webscraping/comments/zqa0xo/scrapywhere_can_i_find_this_graphed_dataplease/,webscraping," 

I am trying to scrape **price history** of below image from this link: [https://www.akakce.com/laptop-notebook/en-ucuz-macbook-air-mgn63tu-a-apple-m1-8-gb-256-gb-ssd-13-3-notebook-fiyati,882468581.html](https://www.akakce.com/laptop-notebook/en-ucuz-macbook-air-mgn63tu-a-apple-m1-8-gb-256-gb-ssd-13-3-notebook-fiyati,882468581.html)

&#x200B;

https://preview.redd.it/8itr0p16gy6a1.png?width=1920&format=png&auto=webp&v=enabled&s=6330ecb47816d7a776c230e5b4662aee2c5ec5a9

But the site does not keep the price history data in the **inspect element** section. It only **dynamically** shows the the price of the date **where you last hovered your mouse on**(Its css selector is :""**#tooltip > span**"").

&#x200B;

https://preview.redd.it/v3zt5d47gy6a1.png?width=1920&format=png&auto=webp&v=enabled&s=c52ece5af5e7b43ec2af96c9734069668c1b41a0

Also The table does not get its data from **Network** requests(I checked **Fetch/XHR I am not sure if it is possible to be taken from other sections**).As you can see there is only **one request** in Fetch/XHR tab and it only returns a -1 and nothing else.

**Where can this data be. How can I find it so that I can scrape it?**",['> Where can I find this graphed data?\n\ninside the raw HTML there is a `application/ld+json` that contains what you want.']
"403 response using links from CSV, 200 response using links from in-memory lists?",https://www.reddit.com/r/webscraping/comments/zpybse/403_response_using_links_from_csv_200_response/,webscraping,"I'm new to web-scraping, so I'm sorry if this is something obvious.

I'm currently trying to scrape all data from a website. The site is structured well, so my approach was to start at the home page, and work through scraping all links from each 'level'. so start at the home page, get all it's categories, then go through the categories one-by-one to get their categories, and so on.

So far, it's worked decently well, but the code was becoming a mess, so I decided to do a big refactor. One of the main things that I did, was I created cache CSV files to store each link, as opposed to using in-memory lists that I was using prior. I figured that by caching the links, I could go directly into a deeper web-page without having to scrape over the entire thing every time.

The bug is that whenever I try to navigate to these links from the CSV file, I get a 403 error. But if I run the branch where everything is still using in-memory lists, I get 200's for each response and am able to walk through the website as normal. I have already checked the URLs on both branches and they're consistently a match. If I navigate to the pages manually on both branches, they work.

For context, I'm using Python 3.9 with the BeautifulSoup4 and Requests modules on a Windows 10 machine.","[""If you're getting 403, it might mean that the website blocked your request for scraping. Did you try e.g. slowing down your job by waiting some time between requests? Another option is to use a proxy to get larger pool of IPs or, even simpler solution, use a web scraping API like [https://scrapingfish.com](https://scrapingfish.com)."", ""Sounds a lot like a session cookie issue. Your previous code:\n\n1. Went to the homepage\n2. Got cookies and product links\n3. Scraped product links (same HTTP session?)\n\nNow you cut out the step 1 and 2, so when you're connecting to the product links, the website sees that you don't have a session cookie and blocks you. Are you using `requests.Session()`?\n\nTo fix this, simply scrape the homepage to collect session cookies before scraping product links from your CSV file.""]"
Anyone here who‚Äôs knowledgeable in Scrapy ItemLoaders? I‚Äôm having issues with incorporating multiple ItemLoaders in different parse functions within the same spider.,https://www.reddit.com/r/webscraping/comments/zpvqtw/anyone_here_whos_knowledgeable_in_scrapy/,webscraping,I‚Äôd be most grateful to anyone willing to help me with my problem as it‚Äôs the only thing holding me back from a successful project. I will share the problem in detail for anyone interested to show a beginner the ropes of Scrapy ItemLoaders. üôè,"[""(a) don't ask to ask, just state your question, bearing [how to ask a good question](https://web.archive.org/web/20221207052428/https://catb.org/esr/faqs/smart-questions) in mind (b) r/scrapy may have more eyeballs that can help you""]"
IMDB - WebScraping - Infinite Download Images and Video of a Movie,https://www.reddit.com/r/webscraping/comments/zpmqvy/imdb_webscraping_infinite_download_images_and/,webscraping,"Check out the code for how we can download all the videos and Images of a movie.

[https://github.com/pavan412kalyan/imdb-movie-scraper/blob/main/snippets/download\_all\_videos\_by\_movie\_id.py](https://github.com/pavan412kalyan/imdb-movie-scraper/blob/main/snippets/download_all_videos_by_movie_id.py)

[https://github.com/pavan412kalyan/imdb-movie-scraper/blob/main/snippets/Image\_downloaders.py](https://github.com/pavan412kalyan/imdb-movie-scraper/blob/main/snippets/Image_downloaders.py)

[https://www.youtube.com/watch?v=tGavdm9djjw&t=6](https://www.youtube.com/watch?v=tGavdm9djjw&t=6)","['Very helpful, would love if we have a weekly thread where people can share their codes , will be very helpful for newbies like me']"
Free Web Scraping!,https://www.reddit.com/r/webscraping/comments/zpmx58/free_web_scraping/,webscraping,"Hey Everyone! Does anyone have any requirement for web scraping. I can help you in providing the DATA. TOTALLY FREE! 
It would be better if the website DOES NOT require any JS rendering. Let me know.","['Hi! I would like some help with my scraper in Python if you have some knowledge of scrapy.', 'hey that sounds great, what kind of data do you provide? i have a scraper based on Python and targeting social media networks', 'Hi broth, Im could help, im work this topics but with R.  \nGood mix, I think.', ""For personal use, I'm trying to teach myself the tools needed to scrape for something like the following.  I have 30k-50k commercial addresses in a state. I'd like to retrieve the business or businesses associated with each address to store in a database. \n\nJust starting this processes myself so I'd appreciate guidance on how to approach it or if you were able to provide the data how I could replicate it afterwards.""]"
Scrape CSV data embedded in Href tag,https://www.reddit.com/r/webscraping/comments/zpisk8/scrape_csv_data_embedded_in_href_tag/,webscraping,"Edit solved:

I just did a whole bunch of find and replaces on the %20 etc.. characters

Hi all,

I'm trying to download approx 30,000 csv files of racing results from the below website and i stuck on dealing with the CSV data.

[https://www.racingandsports.com.au/horse-racing-results/australia/hobart/2022-12-18](https://www.racingandsports.com.au/horse-racing-results/australia/hobart/2022-12-18)

The data i need is just embedded in the page inside a href tag that you click to download the data see below...

    <a href=""data:text/csv;charset=utf-8,RNo,Name,Prize%20Money,Time,Winner,Class,Dist,SOT,Time,Temp,Diff,RSSF,L600,Wins,AWI,API%0D%0A%221%22,%22hobart-united-fc-bm70-hcp%22,%22AUD%20$25,000%22,%222:12.70%22,%22SO%20ASTOUNDING%22,%22BM70%22,%222100%22,%22G%22,%22132.7%22,%2212.64%22,%22+0.09%22,%2286.35%22,%2236.45%22,%228%22,%2210.7%22,%223.21%22%0D%0A%222%22,%22ayc-netball-club-mdncl1%22,%22AUD%20$25,000%22,%221:40.47%22,%22NOT%20A%20BRASS%20RAZOO%22,%22CL1%22,%221600%22,%22G%22,%22100.47%22,%2212.56%22,%22+0.33%22,%2277.25%22,%2236.65%22,%221%22,%221.6%22,%221.16%22%0D%0A%223%22,%22wellington-cricket-club-maiden%22,%22AUD%20$25,000%22,%221:05.35%22,%22FEAR%20THE%20STING%22,%223U%20MDN%22,%221100%22,%22G%22,%2265.35%22,%2211.88%22,%22-0.02%22,%2294.05%22,%2235.57%22,%220%22,%220%22,%220.96%22%0D%0A%224%22,%22wakeful-club-bm64-hcp-fm%22,%22AUD%20$25,000%22,%221:12.35%22,%22THELMA%22,%22FM%20BM64%22,%221200%22,%22G%22,%2272.35%22,%2212.06%22,%22+0.10%22,%2288.75%22,%2236.50%22,%2215%22,%2225%22,%225.52%22%0D%0A%225%22,%22lindisfarne-afl-masters-class-1-hcp%22,%22AUD%20$25,000%22,%221:12.09%22,%22SISTINE%22,%22CL1%22,%221200%22,%22G%22,%2272.09%22,%2212.02%22,%22+0.05%22,%2290.45%22,%2236.15%22,%2210%22,%2210.3%22,%223.88%22%0D%0A%226%22,%22winzenberg-handicap%22,%22AUD%20$50,000%22,%221:05.30%22,%22DUNBRODY%20POWER%22,%22Qlty%22,%221100%22,%22G%22,%2265.3%22,%2211.87%22,%22-0.03%22,%2294.35%22,%2235.52%22,%2219%22,%2221.8%22,%225.69%22%0D%0A%227%22,%22j-j-roofing-bm68-hcp%22,%22AUD%20$25,000%22,%221:26.62%22,%22JOHNNY%20CHUTZPAH%22,%22BM68%22,%221430%22,%22G%22,%2286.62%22,%2212.11%22,%22+0.25%22,%220%22,%2236.43%22,%229%22,%2213.2%22,%223.67%22%0D%0A%228%22,%22ladbroke-it-bm62-hcp%22,%22AUD%20$25,000%22,%221:38.02%22,%22ROYAL%20AND%20TOUGH%22,%22BM62%22,%221600%22,%22G%22,%2298.02%22,%2212.25%22,%22+0.02%22,%220%22,%2237.03%22,%2211%22,%2212.9%22,%223.61%22%0D%0A"" class=""btn btn-boxed-g pull-left btn-export"" style=""margin-left:5px"" id=""btnCSV"" download=""Hobart Results - 18 Dec 2022.csv""> <i class=""fa fa-file-excel-o""></i><span> CSV</span> </a>   

I have been able to extract the string I need and then try and use something like...
 
    pd.read_csv(io.stringIO(csv_data))
 
This however just gives a 144 column wide file instead of the 13 columns and 8 lines for data.

ive tried adding encoding, removing ""data:text/csv... at the start.

ill be pushing the extracted CSV file directly to a Database for use later on.

Any ideas how to solve this?

thanks in advance",[]
Backend server can detect real IP ?,https://www.reddit.com/r/webscraping/comments/zpbq0x/backend_server_can_detect_real_ip/,webscraping,"So i build a scraper using python ( requests , bs4) to get data from a certain webapp.  
I've inspected the site headers and set up everything with headers, proxies (\~500 different IPs, http proxies ) all that stuff.  
What i've noticed after \~4K requests i've recieved:

  
`<html>`

`<head>`

`<title>403 Forbidden</title>`

`</head>`

`<body>`

`<h1>Error 403 Forbidden</h1>`

`<p>Forbidden</p>`

`<h3>Error 54113</h3>`

`<p>Details: cache-hel1410033-HEL 1671405206 1319037323</p>`

`<hr/>`

`<p>Varnish cache server</p>`

`</body>`

`</html>`

  
But i got the response everytime i've tried to sent a request ( with different proxy IP ) which was weird.  
So i've tested the same code on different machine ( different IP ) and it works with the same setup using the same proxies.  
I'm assuming the web-server is somehow detecting my real IP ? Is this the issue & is there a workaround ?  


Thanks","['Some proxies do reveal your home IP. Try this experiment: with your proxies on, scrape http://www.xhaus.com/headers and see if the web page shows that any of your identifying details appear in the request or headers.', ""Thanks all for the help. As we all know we do some dumb mistakes every now and than. I'll give you all the TLDR:  \n\n\n\\- It wasn't any technical issue , basically on the provider page where i use the proxies from, i forgot to add my new server IP as whitelisted ( the server i was running the script on )  \n\n\n\\- While using `requests.request`  , i was using the proxies but since they were not authorized to be used trough that IP, for some reason it didn't throw exception - the request continued probably with the server IP ( thats why i was wondering how they got my original ip )  \n\n\n\\- I've changed to use `httpx` instead of requests & added the server as whitelisted and its working perfectly\n\nNOTE for future: Don't forget to check the Ops stuff before debugging code lol\n\nThanks all for the suggestions , pretty useful advices if i'm met with an actual issue like this in future"", 'Make sure your proxies are anonymous and I use proxy from www.proxy-hub.com it works flawless for me.\n\n\nWhat site are you trying to scrape possible to share?? Probably I can help as I have done plenty scraping scripts.', ""Aren't you sending a session in the cookies?"", 'Is it possible for you to share the website that you are scraping. I can check and tell you if the website is protect by antibots or browser fingerprinting.', ""Hey u/1991jme I see you figured out the issue was that you didn't whitelist your server's IP with your proxy provider.\n\nHowever, if you start experiencing issues again, make sure that you aren't just rotating your proxy IP but your fingerprints/headers as well :)""]"
Selenium scraping locator strategy (beginner),https://www.reddit.com/r/webscraping/comments/zowxx6/selenium_scraping_locator_strategy_beginner/,webscraping,"Hi there,

I am trying to scrape these video IDs. They have the attribute name ""data-video-id"", which can be seen in this screenshot, https://imgur.com/a/7p7oIxT

How do I extract the value of 6316835209112?

What locator strategy do I use (complete noob at Selenium). Thanks so much in advance","[""Do you need Selenium? What's the site?  \nWith bs4 you can do `el.get('data-video-id')`"", 'You need to use getAttribute\n\nelem.getAttribute(""data-video-id"");']"
Who‚Äôs using the `httpx` library instead of `requests` these days? Just learned about it today,https://www.python-httpx.org/,webscraping,,
Help with first scraper,https://www.reddit.com/r/webscraping/comments/znzud2/help_with_first_scraper/,webscraping,"Hey guys I'm hoping someone might be able to help me with one of my first python projects.

&#x200B;

 I'm trying to make a simple scraper to make a list of apartments on craigslist. But had a hard time so just trying to get it to print the number of search results first for now.

I'm not sure why this code isn't working. 

    import requests
    from bs4 import BeautifulSoup
    
    session = requests.Session()
    # Make a GET request to the Craigslist page
    url = ""https://newyork.craigslist.org/search/mnh/apa#search=1~gallery~0~0""
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}
    page = requests.get(url, headers=headers)
    
    # Make sure the request was successful
    if page.status_code != 200:
        print(f'Error: request returned status code {page.status_code}')
    else:
        # Parse the HTML of the page
        soup = BeautifulSoup(page.text, 'html.parser')
    
        # Find the element with the class ""result-count""
        result_count_element = soup.find('div', {'class': 'result-count'})
    
        # Check if the element was found
        if result_count_element is None:
            print('Error: element with class ""result-count"" not found')
        else:
            # Extract the text of the element and split it by space
            result_count_text = result_count_element.text.split(' ')
    
            # The first element in the split text should be the number of search results
            result_count = result_count_text[0]
            print(f'Number of search results: {result_count}')
    

It prints: Error: element with class ""result-count"" not found

as far as i can tell it should be finding this result-count and printing 1,773

https://preview.redd.it/66tgpknghe6a1.png?width=789&format=png&auto=webp&v=enabled&s=2c3a82dc3559663b1a2c162f666f5f42f0fa0929

&#x200B;

Im a complete beginner any help is very much appreciated thanks.","['Print your soup variable and verify what‚Äôs in there. Maybe it‚Äôs empty for some reason.', 'BeautifulSoup scrapes content from page source. The page in question is generating the content on run-time. This is why BS is unable to find it. Take a look at page source to get the idea.\n\nThe alternate is to use screen scraping via Selenium.', ""You need to delay before finding that result count div..\n\nimport time at top and write like this\n\ntime.sleep(2000);\n\n result\\_count\\_element = soup.find('div', {'class': 'result-count'})"", 'Try to see if there any XHR or API calls in the network tab that may be helpful in extracting the details']"
Advice for scraping massive number of items faster?,https://www.reddit.com/r/webscraping/comments/znweub/advice_for_scraping_massive_number_of_items_faster/,webscraping,"As the title says, I have a project but wanted your advice in general.
I am currently scraping a website with 220K+ product pages.
I am using requests/beautifulsoup and concurrent futures to get the data as fast as possible.
The script still takes around 6-8 hours to run, and if I get an error at the 100k mark for example, I lose all the data that was scraped.
So I was wondering what your methods are to scrape massive datasets as fast possible? Would love some advice","['Okay, here is some practical advice.\n\n(1) Save the data as you scrape like others have said.\n\n(2) Record requests which returned errors somewhere to retry them. You can automate this part.\n\n(3) Instead of using concurrency, rely upon asynchronous requests. Scraping is not a CPU bound operation.\n\n(4) Asynchronous setup will make 100s of requests per second, so you will likely get blocked. In order to circumvent it, you will need to use a lot of proxies.\n\n(5) If you want more speedup which will be probably overkill at this point, use multiple computers with asynchronous scraping setup. I should add that it might look like a DDOS attack on the target website, so play nicely. \n\n\n\nI think this should be sufficient. You can make the scraping arbitrarily fast (roughly) if you already have or you can generate all links to be scraped in advance and then feeding it to this distributed scraper. Your main bottleneck will be number of proxies with this infrastructure.', ""Save the data as you scrape it so you don't lose it.\n\nStart the script, go to bed, next day you have all days scraped, ready to parse."", 'Crawl it first, parse it later. Crawl with asyncio (aiohttp or httpx) and save to mongodb (url and html for each document). Then use multiprocessing when you parse data, parse with selectolax or parsel instead of bs4.', 'Can the information you want be gotten with a HEAD request? It should be faster than GET. \n\n If you‚Äôre looking to use a different tool, I recommend Scrapy, but I doubt it‚Äôs that much faster. It just makes a lot of scraping jobs really easy. \n\nLike Annh said, you should be saving the data as you go. It should be slightly faster, since you‚Äôll be using less memory.', 'Following as I feel I can learn a lot for my own projects with similar framework. Quick question regarding proxies: is their a top 3 or 5 proxy companies/providers people would recommend for the project in this thread?', 'Apply the ETL process but with saving steps for your data. You can use an expiration system for your saved data, just to not re-download everything.', 'Have you checked to see if there‚Äôs a hidden API or REST endpoint you can query? That plays nicely with `requests`, and it‚Äôs faster since there‚Äôs no HTML overhead.', 'for saving  requests  u can convert links  to hash and save html content  of each page with name of that hash \nand use database to locate each one', 'Lol LL LLl few']"
How to stop Requests library from encountering bad gateway errors?,https://www.reddit.com/r/webscraping/comments/znwv03/how_to_stop_requests_library_from_encountering/,webscraping,I am using the Requests library to scrape some data from Spotify. I am scraping around 5000 pages and around 10 per run will error. I and using try/except so it doesn't affect anything but I still want the data from all requests. I tried using a delay of 0.75 sec and 0.5 sec and I still got errors. Any suggestions?,"['Another tactic is to maintain a list of the failed requests and tack those on the end of your list. I‚Äôve had better luck sometimes doing my retries later than trying to re-try repeatedly in the moment until it goes through. Probably looks better on the server logs too. Repeated fails might trigger a server response.', 'Use a while loop inside the try except to keep trying the request until it goes through']"
What kind of data/security bot is this?,https://www.reddit.com/r/webscraping/comments/znio8q/what_kind_of_datasecurity_bot_is_this/,webscraping,"I have tried multiple sites for scraping data and all of them use some form of bot detection. They all have different name for it but seems the exact type. Here is an example:

iiXnANQ1pn-a: 

iiXnANQ1pn-b:hj90iw

iiXnANQ1pn-c:AADwqRuFAQAAvTZNIeZfh\_hARZ6Hz6QwY0SI1cSOArylWZ

iiXnANQ1pn-d:AAaihIjBDKGNgUGASZAQhISy1WIYbrZv\_VhYxf\_\_\_\_\_mR38WAPtKFEdo7\_vYdczycqYG294

iiXnANQ1pn-f:A5t0tBuFAQAAKXxx7kkj\_xFYZdXDMUW9r5gjdp8inNcIGads\_vyBaqsaO9pgAWKP9DKcuG

&#x200B;

From another site:

x-incssdtm-a 1u4L2F2O=dJdrhgpEcAZ1A2-O43sSfvK3fgqAl3m

x-incssdtm-b f6844s

x-incssdtm-c AOC2eoaEAQAAOwY7DAC

x-incssdtm-d ABaChIjBDKGNgUGAQZIQhISi0

x-incssdtm-f A2VNnoaEAQAAsg

&#x200B;

Is this Akami? I dont think it is. This is something else .. any hints?","['Pretty sure this is Shape Security. It‚Äôs definitely one of the harder bot protections to deobfuscate.', 'Google Search on ""incssdtm"" reveals nothing but one or two people asking the same question.  So most likely a custom solution is my guess.  I\'m curious, what\'s the website?', ""Maybe you need to go about this different more closely mimic an actual user add cookies and header maybe run your browser in non headless mode or at least specify a screen size add in some randomization and have another tab opened to JCPenney or pornhub whatever . Sides that it is a financial website there gonna go to town on security so wat can ya do. Like many things if one side advances the other side must follow suit. Other hints don't change proxy's during a convection or user agents those are things company's like maxmind look for fo detect fraud and same gose for bots .evolve or die""]"
Can multiple websites be scraped in one code,https://www.reddit.com/r/webscraping/comments/znbo3z/can_multiple_websites_be_scraped_in_one_code/,webscraping,"I have scraped some news websites and i have written different syntax for most of them( I am new at it), is there a way I can scrap data from 5 websites through one code or any other way to to recognise patterns in those websites that I am missing.","['While the other comment presently here says no and is correct, I will qualify thus somewhat:\nIf you want to make one code that scapes say only 5 specific sites (and assuming you want it not to have a different function for each site of course) and pretty uniform requirements from each  (date, title, article) you can do it.\n\nHowever the amount of research (e.g. into the html, network traffic, etc), clever thinking (e.g. Regex), and planning required usually exceeds the time needed to just write a separate script for each site. \n\nIf routine scrapping of several sites is required I would rather write one function for each, then write a program that brings them all together and then multithread it so it runs faster.', ""It's either easy or impossible.\n\nIf you want all images or keywords highlighting the rough topic it is no problem.\n\nIf you want a detailed analysis it is on the verge of impossible and only achivebal with complicated artificial intelligence or similar, which probaply is overkill"", ""So best thing is you just need to identify your common attributes.\n\nFind the DOM selectors of all attributes on each website and either create separate file for each website's selectors(Id, class or xpath)  etc or create a map against each website and store their selectors against it.\n\n Write logic at one place for extraction.\n\nFrom URL identify the website then find the selectors of that particular website from map ad now use those website selectors to extract the data from it."", ""No, you can't scrape multiple websites with a single program.""]"
What is the best way to bypass the Octoparse human verification requirement?,https://www.reddit.com/r/webscraping/comments/znghei/what_is_the_best_way_to_bypass_the_octoparse/,webscraping,"It appears that this screen appears when I enter the Zillow website address in octoparse. Could you please let me know what to do about it?

I am using residential proxy and user agent rotation.

https://preview.redd.it/07q71xxrw96a1.png?width=779&format=png&auto=webp&v=enabled&s=d301ddca5856604ff75a1562f01c3ec9205504cd","[""This shouldn't happen with multiple proxies unless you are using few proxies that are already detected."", ""Seems like there's either an issue with your proxies OR your user agents. I would suggest changing the user agents list first, and if that doesn't solve the issue, try a new proxy provider.\n\nAre you currently using datacenter proxies? I would suggest using either residential or mobile proxies instead(If simply changing the user agents list doesn't fix the issue).""]"
Different views for google results?,https://www.reddit.com/r/webscraping/comments/znd002/different_views_for_google_results/,webscraping,"Hello - i tried to call a google-search using selenium -

For examples this link:[https://www.google.com/search?q=http%3A%2F%2Fwww.turci.it](https://www.google.com/search?q=http%3A%2F%2Fwww.turci.it)

When i do this with selenium i get this view:

https://preview.redd.it/un4mhw84296a1.png?width=1911&format=png&auto=webp&v=enabled&s=968744988af479899ff40126987e0acc1dc9a105

And when i open this in the incognito browser manually i get this view:

&#x200B;

https://preview.redd.it/19d9m0nk296a1.png?width=1879&format=png&auto=webp&v=enabled&s=96a07d1f0a05929eb3aee3cc7ba9494d6e3464d7

Why is this different?Also the html-structure seems to completely different?

Cookie i found in the manual opend browser window:

&#x200B;

https://preview.redd.it/pi70gms2i96a1.png?width=1119&format=png&auto=webp&v=enabled&s=968113dae59f74c0191c2a7ac3e186c2a41f016d","['[deleted]', 'different cookies', ""Any other difference like settings on your browser that arnt on the selenium or are you using Firefox and them chrome also far as results proxy's change some results by changing your location idk some ideas""]"
Why can't beautiful soup scrape this table?,https://www.reddit.com/r/webscraping/comments/zmuu0a/why_cant_beautiful_soup_scrape_this_table/,webscraping,"I am trying to scrape this [site](https://www.basketball-reference.com/boxscores/202110220LAL.html).

There are two tables on this site that I can't retrieve with beautiful soup. Their ids are ""line\_score"" and ""four\_factors"".

I am able to access all the other tables by their IDs, except this one. Any ideas?

This is my Python code:

`import requests`  
`a = requests.get(""https://www.basketball-reference.com/boxscores/202110220LAL.html"")`  
`from bs4 import BeautifulSoup`  
`soup = BeautifulSoup(a.content)`  
`soup.select(""#line_score"")`

This is the HTML of the table: 

`<table class=""suppress_all stats_table"" id=""line_score"" data-cols-to-freeze=""0"">`

`<caption>Line Score Table</caption>`



&#x200B;

   `<colgroup><col><col><col><col><col><col></colgroup>`

   `<thead>`

&#x200B;



`<tr class=""over_header"">`

`<th aria-label="""" data-stat=""header_tmp"" colspan=""6"" class="" over_header center"">Scoring</th>`

`</tr>`



`<tr>`

`<th aria-label=""&nbsp;"" data-stat=""team"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">&nbsp;</th>`

`<th aria-label=""1"" data-stat=""1"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">1</th>`

`<th aria-label=""2"" data-stat=""2"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">2</th>`

`<th aria-label=""3"" data-stat=""3"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">3</th>`

`<th aria-label=""4"" data-stat=""4"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">4</th>`

`<th aria-label=""T"" data-stat=""T"" scope=""col"" class="" poptip center"" data-over-header=""Scoring"">T</th>`

`</tr>`

`</thead>`

`<tbody><tr><th scope=""row"" class=""center "" data-stat=""team""><a href=""/teams/PHO/2022.html"">PHO</a></th><td class=""center "" data-stat=""1"">23</td><td class=""center "" data-stat=""2"">34</td><td class=""center "" data-stat=""3"">37</td><td class=""center "" data-stat=""4"">21</td><td class=""center "" data-stat=""T""><strong>115</strong></td></tr>`

`<tr><th scope=""row"" class=""center "" data-stat=""team""><a href=""/teams/LAL/2022.html"">LAL</a></th><td class=""center "" data-stat=""1"">26</td><td class=""center "" data-stat=""2"">18</td><td class=""center "" data-stat=""3"">23</td><td class=""center "" data-stat=""4"">38</td><td class=""center "" data-stat=""T""><strong>105</strong></td></tr>`

&#x200B;

`</tbody></table>`",['the data is loaded dynamically. you might want to give the network pane a check to see where the data is coming from.']
TikTok scraping?,https://www.reddit.com/r/webscraping/comments/zmp2jd/tiktok_scraping/,webscraping,"Hello - has anybody scraped tiktok so far?  
I tried to do this - but i am not able to find eg. the count of comments in the html-source code. It seems that the code when inspecting looks different than what i get in selenium + bs4?","['www.github.com/davidteather/TikTok-Api', ""I tried to but what I wanted to do involved getting in the Saved folders and there's no way to do that on tiktok web. I don't know why they don't have this functionality?"", 'Try my mini repo https://github.com/hglong16/tiktok-donloader.', 'I have a basic script I use that searches key terms and downloads the meta data of all the videos that search term returns.']"
Best Configuration/Infrastructure for...,https://www.reddit.com/r/webscraping/comments/zmpakp/best_configurationinfrastructure_for/,webscraping,"I want to scrape a website for sales data. There are 24,000 pages to look at daily.

Initially, each page scrape will take \~ 60 seconds. After the initial data scrape/save, each page scrape should take \~ 15 seconds.

I am looking at using Python & Docker deployed with AWS Lambda but am running into issues, namely an AWS Lambda can only be running for max 15 minutes. 

I started researching the Lambda concurrency options, and it still seems like the process/code/config will be long & complicated.

Conversely, I ran a multi-threaded Python script from my local mac, and it successfully made it through the \~ 24K pages in \~ 16 hours (with max 15 seconds a page scrape). I just don't want to have to run locally everyday - I want it to run offsite/cloud automatically.

Thoughts/suggestions?","['Can you provide more details about the code you wrote? Although most of the overhead will be in waiting for the network response.   \nAnyway you can opt for an EC2 instance which is basically a server that you then need to  install the packages on and create a cron job for the script.', 'Are you using requests libary or selenium', ""Do everything you can to drop Selenium, that's your big bottleneck, is there anyway to get the data via requests? Is there data via a backend API or json hidden in the HTML? \n\n\nThen look at concurrent.futures to run your code concurrently."", ""24k pages daily isn't that much comes to web scraping.\n\nThe number one enemy in web scraping is IO blocking. It's when your program waits for input/output, which as you can imagine, is basically most of web scraping. \n\nThere are many ways to get around IO blocking in Python: asyncio, threads, subprocesses etc. I wrote an in-depth blog a few months ago: [Web Scraping Speed: Processes, Threads and Async](https://scrapfly.io/blog/web-scraping-speed/) if you'd like to learn more.\n\nWithout seeing your code it's hard to tell what's the bottleneck though Selenium is very likely to be it. Have you looked into [asynchronous Python and Playwright](https://scrapfly.io/blog/web-scraping-with-playwright-and-python/)? Are you using subprocesses? Note that threads will not help with processing scaling in Python, so you need to use sub-processing + threads/asyncio to scale for IO blocking and processing simultaneously.\n\nAlternatively, you can spend 30$ or something on a web scraping API (like [Scrapfly](https://scrapfly.io), I work here) that runs cloud browsers for you and save you a significant headache :)""]"
Get email from specific site?,https://www.reddit.com/r/webscraping/comments/zml1jr/get_email_from_specific_site/,webscraping,"Hello - i try to collect the data from this website:

[https://www.11880-heizung.com/heizung/grosshabersdorf/fischer-sanitaer-u-heizungstechnik-gmbh-cokg-27726455.html](https://www.11880-heizung.com/heizung/grosshabersdorf/fischer-sanitaer-u-heizungstechnik-gmbh-cokg-27726455.html)

On the right side you can see there eg. the email-adress on the site.When i inspect the site i also see the entry in this form:

https://preview.redd.it/1h4yuaxrc26a1.png?width=562&format=png&auto=webp&v=enabled&s=e084e05d794e34b33b99c83f8216d74553a620b9

&#x200B;

I now get the information from this site using selenium and bs4 and the selected span looks like this in the end:

    <span class=""flex-1 truncate cursor-pointer"" onclick=""vueApp.enableContactForm({
                    'entryId' :&quot;27726455&quot;,
                    'entryName' :&quot;Fischer Sanit\u00e4r- u. Heizungstechnik GmbH &amp; Co.KG&quot;,
                    'trackingInvokedBy' : 'contact-on-detail',
                    'handleTracking': true
                });"">
     <a class=""__cf_email__"" data-cfemail=""6112041317080204210c18070812020904134f0504"" href=""/cdn-cgi/l/email-protection"">        
      [email protected]
     </a>
    </span>

As you can see the email is not visible anymore?

Any idea how i can get the email which is visible when inspecting the site?","['If you look in the network tab of your browsers Developer Tools and reload the page you will see there is a some javascript called ""email-decode.min.js""\n\n\nI copied that into chatGPT and it explained what the minified code was doing, basically it is taking that ""data-cfemail"" hexidecimal string and decoding it into a human readable email address. I can\'t really write javascript so I asked chatGPT to rewrite the key functions in python... the below code is what is gave me and it works. So cool!\n\n    from urllib.parse import unquote\n\n    def parse_hex(string, start):\n        """"""Parses a hexadecimal string and returns its decimal equivalent.""""""\n        # Get the hexadecimal substring from the input string\n        hex_str = string[start : start + 2]\n    \n        # Parse the hexadecimal string and return the decimal value\n        return int(hex_str, 16)\n    \n    def decode_string(string, key_start):\n        """"""Decodes a string using the hexadecimal values and a key.""""""\n        # Initialize an empty string for the decoded output\n        decoded_str = """"\n    \n        # Get the hexadecimal key from the input string\n        key = parse_hex(string, key_start)\n    \n        # Iterate over the input string and decode each hexadecimal value using the key\n        for i in range(key_start + 2, len(string), 2):\n            # Get the hexadecimal value from the input string\n            hex_val = parse_hex(string, i)\n    \n            # Decode the hexadecimal value using the key\n            decoded_val = hex_val ^ key\n    \n            # Append the decoded character to the output string\n            decoded_str += chr(decoded_val)\n    \n        try:\n            # Decode the output string using URL encoding\n            decoded_str = unquote(decoded_str)\n        except Exception as e:\n            # Log any errors that occur during decoding\n            print(e)\n    \n        # Return the decoded string\n        return decoded_str\n    \n    decode_string(\'6112041317080204210c18070812020904134f0504\',0)']"
"ElementNotInteractableException: Message: Element <span class=""selection""> could not be scrolled into view",https://www.reddit.com/r/webscraping/comments/zmk7ro/elementnotinteractableexception_message_element/,webscraping,"    select_span = driver.find_element(By.CLASS_NAME,'selection')
    select_span.text
    select_span.click()

i found the element i want and select\_span.text give me the text inside `<span class=""select2-selection__rendered"" id=""select2-indicatorDropdown-container"" role=""textbox"" aria-readonly=""true"" title=""Experienced violence since COVID-19"">Experienced violence since COVID-19</span>`

which is 'Experienced violence since COVID-19' but if i do select\_span.click() to click on this span and open the selection i get :

    ElementNotInteractableException: Message: Element <span class=""selection""> could not be scrolled into view

i keep getting this error almost daily and i never understand why..

there is no shadowroot here or iframe or anything since i got the element anyway, but when i try to click it i get this error why?? it happens to almost everything i try to click.

PS: i use python selenium and firefox browser",[]
Scaping from a dynamic bar plot,https://www.reddit.com/r/webscraping/comments/zmc5lo/scaping_from_a_dynamic_bar_plot/,webscraping,"Hi there,

I want to scrape the half hourly data in the bar plot on https://www.nldc.evn.vn/, when I hover over the values, I can see the values I want. I cannot see any of it the Inspect Element. What is the way to approach this?","['No need to scrape from the table. The table is filled using a GET request at this link [https://www.nldc.evn.vn/Chart24hHandle.ashx?d=14/12/2022&isChangeDate=0](https://www.nldc.evn.vn/Chart24hHandle.ashx?d=14/12/2022&isChangeDate=0)\n\nYou can see the parameter d is for date. In the URL above December. 14, 2022 is 14/12/2022.']"
Next level web scrapers,https://www.reddit.com/r/webscraping/comments/zlxotc/next_level_web_scrapers/,webscraping,"So a quick background, I am a fairly intermediate web scraping developer created 100s of projects with different combinations of selenium, playwright, scrapy, puppetteer or plain curl requests and bs4 rendering in Python or nodejs or even Java. 

I know threading techniques as well as proxies, anti detection, bypassing HCaptcha and fun captcha, bezier mouse movements for test score, erc.

I have always wondered how some companies out there like phantomBuster create very fast scrapers for things like Linkedin for example?

Now I started my Own personal scraping project to test this out and I have used possibly every combination of tools and every thing I could think of to make my scraping project go faster but It will never be as fast or even 50% as fast as those online website.

For example, there was a public post with 8k comments that I wanted to get the people name to test rhe scraper. And I had to wait until all comments are loaded (which I have to trigger the 'load more' button) to get the names.

After good 13 mins, I got the list. But using phantom buster, I got it after 2 mins only!

How do they even get it that fast with JS involved?? 

I remember Instagram had a secret special character that you put at the end of URL to make it load data without Js or something but I don't know about Linkedin.

I am not talking here about bot detection and blocking or bypassing log in or anything. Just the speed of scraping js-rely website.

EDIT 1: 

After karllorey, comment about the mobile version, let me say I did a similar experiment with Facebook. 

So there was a project that we wanted to scrap the posts of a selling group we owned on Facebook that had more than 1500 daily posts in India. The project involved getting data that could reach to a point where the V8 JS engine of Chrome/Firefox couldn't handle it anymore. 

So, I found an older version of the Facebook site targeted at old mobiles that only load 20 or 40 posts at a moment, and when you press load more, it destroys the previous DOM. 

It was overall faster and the memory was doing great. 

Do you think LinkedIn could have a similar platform or something? 

I don't think PhantomBuster save the posts in thread though or something similar because I tried on different post types and they were all similar results average of 2-2:30 mins.

Bonus Content about Benchmarking and testing: 

My experiment involved benchmarking the time from the moment the comments section were loaded, didn't take into consideration the startup time or anything else (because they can be altered by reusing the same browser and different optimizations) 

I also tested different tools all on the same post including selenium, playwright and other libraries that could parse JS. 

I believe as @VestaM mentioned in his comments it is about API exploiting but maybe it takes too much time to research it.","['Tl.dr its all about research and domain specific knowledge of the field, what might or might not work for targets.\n\nWell. As a person who works in a such company (not your mentioned one) that scrapes a lot of data and fast. I can tell you that its not about the tooling, but the research of targets. If you have to wait for page to load comments then it means there is an api (probably with pagination) that can be accessed and scraped/crawled.\n\nNot for your situation particularly, but also caching scraped data for more than one customer, means that if you requested a scrape via someones scraping api, scraped data is cached somewhere rmq or redis or whatever for next client to take.', ""Very interesting thread, interested to hear others. Just my two cents: On my phone (Linkedin app), I can scroll to the bottom of large comment sections rather quickly. Also, if there's a post with many comments, there's a good chance Phantombuster has already seen the thread and stored it :)"", 'Are we getting the data in network request on load more button click?', 'Hey @xmouda, see [Proxycurl API](https://nubela.co/proxycurl/linkedin). They have a specific endpoint that hits public LinkedIn profiles. Each API request takes ~2s, and you can make hundreds of concurrent requests at the same time for scale. And no, you do not need to bring your own LinkedIn session cookies like you need to on PhantomBuster.', ""Maybe they're using direct api calls. Need to check this myself as I am going to write a linkedin scraper in the next months."", 'Can you tell me more about the Instagram load with the special character?']"
Hi i wann to show any website in my local react application,https://www.reddit.com/r/webscraping/comments/zm0zd6/hi_i_wann_to_show_any_website_in_my_local_react/,webscraping,"I try it with iframe but there is problem not every website allows to embed their website ,is it possible  to handle this?
Or can i just save website 1 page as html and css files?",[]
Deploying Scrapy Projects on the Cloud,https://www.reddit.com/r/webscraping/comments/zlrkfs/deploying_scrapy_projects_on_the_cloud/,webscraping,"Hi all

I have found 2 services for deploying Scrapy Projects on the Cloud: Scrapy Cloud and PythonAnywhere. Do you guys have any experience with either of them or maybe other services? Are there other cheaper options? Where do you deploy your scrapy projects?","['AWS Lambda Functions or GCP Cloud Functions are relatively cheap depending on your use case', ""Hello,\n\n[We](https://bitmaker.la/en/) are currently running a closed beta of Bitmaker Cloud (free and unlimited). Bitmaker Cloud gives you easy management of scraping workloads via a web dashboard and API. Only Scrapy spiders are supported at the moment (additional languages/frameworks are on the roadmap).  \nBitmaker Cloud is powered by [estela](https://bitmaker.la/blog/2022-06-24-estela-oss-release.html), an elastic web scraping cluster running on Kubernetes. estela is a modern alternative to proprietary platforms such as Scrapy Cloud, as well as OSS projects such as scrapyd. The source code of [estela](https://github.com/bitmakerla/estela/) and [estela-cli](https://github.com/bitmakerla/estela-cli) is available on Github.\n\nWe've worked for many years in web scraping (several of us worked previously in companies such as Zyte/Scrapinghub) . We are really looking forward to get feedback from other experts (and newcomers too!).\n\nWe plan on running the beta until the end of January, if you or anyone else is interested in participating please write me at [breno@bitmaker.la](mailto:breno@bitmaker.la).\n\nAfter the beta ends we'll be launching Bitmaker Cloud officially on a pay-as-you-go model, based on resource usage (ala  AWS, but just with CPU, bandwidth and storage metrics).\n\nThanks!"", ""I tried scrapu cloud once but wasn't flexible enough for me for the project. Now i use Kubernetes (gke)"", 'You can take a look at ScrapeOps and digitalocean (Scrapyd)']"
Web scrape every dispensary in US,https://www.reddit.com/r/webscraping/comments/zldcyv/web_scrape_every_dispensary_in_us/,webscraping,I‚Äôm a novice at Python and data science in general. I‚Äôve tried to read posts here - is there a free way to scrape every dispensary in the US for contact information?,"['Yes', 'You may want to look into webscraping sites like this: https://www.leafly.com/dispensaries', 'Every site will be built differently thus you will have to almost certainly have to write a script for each.']"
Trying to combine two for loops so that the output works for a csv file,https://www.reddit.com/r/webscraping/comments/zles5p/trying_to_combine_two_for_loops_so_that_the/,webscraping,"Newbie here. Personal web-scraping project using python. Really having trouble. Tried a number of stabs at this and I am at a loss. Can anyone help with my code? I would like the ended auction title on the same line (and separated by a comma) as the price it sold for on eBay - so as to export to a csv file. Tried the zip function, but output comes out with the HTML code attached. Thank you.

    from bs4 import BeautifulSoup 
    import requests  
    url = ""https://www.ebay.com/..."" 
    result = requests.get(url).text doc = BeautifulSoup(result, ""html.parser"")  
    
    ul = doc.find('ul', {'id': 'ListViewInner'}).prettify() 
    names = doc.find_all('a', class_='vip') 
    prices = doc.find_all('span', class_='bold bidsold')  
    
    for name in names[:5]: 	
        print(name.text) 
    for price in prices[:5]: 	
        print(price.text) 

Output:

    Auction 1 
    Auction 2 
    Auction 3 
    Auction 4 
    Auction 5 
    $21.50 
    $22.38 
    $13.50 
    $38.00 
    $20.50  

Preferred Output:

    Auction 1,$21.50 
    Auction 2,$22.38 
    Auction 3,$13.50 
    Auction 4,$38.00 
    Auction 5,$20.50","['I can appreciate your newbie status, but please do keep content on this sub about _webscraping_, whereas your question is better suited for r/python or r/learnpython \n\nBut fine: [`zip`](https://docs.python.org/3.11/library/functions.html#zip) and its friend [`csv_writer.writerows`](https://docs.python.org/3.11/library/csv.html#csv.csvwriter.writerows)\n\n    >>> n = [name.text for name in names[:5]]\n    >>> p = [price.text for price in prices[:5]]\n    >>> w = csv.writer(sys.stdout)\n    >>> w.writerows(zip(n, p))\n    Auction 1,$21.50\n    Auction 2,$22.38\n    Auction 3,$13.50\n    Auction 4,$38.00\n    Auction 5,$20.50', 'for i in range (0,5): print (name\\[i\\].text, price\\[i\\].text)', 'For name in names[:5]:\n\n      For price in prices[:5]:\n\n             Print(name.text, price.text)']"
Would it be worth it to get storm proxies residential rotating proxies?,https://www.reddit.com/r/webscraping/comments/zl7gcu/would_it_be_worth_it_to_get_storm_proxies/,webscraping,"I've been using storm proxies for a while but with their data center proxies. I'm generally very pleased considering the price.

However, I do get blocked \*a lot\* when scraping any websites that have some sort of decent bot prevention management. Which is maybe like 60% of the sites I go after?

And honestly, I usually can get what I need but with a little work which is fine considering the unlimited bandwidth and low cost.

So their residential proxies are a bit more than their data center and I just wanted to see if you guys think: 

1. would it be worth it to get residential proxies at all

2. If so, would it be worth it to get them from storm proxies?","['As you say, residential proxies cost more than data center ones. To keep my own costs low, a strategy I have been using more and more lately is to have an automatic fallback mechanism in my scraper function. So for each URL, the function tries scraping up to 3 times using my unlimited bandwidth socks5 proxies, and if it hasn‚Äôt succeeded yet then it switches to residential proxies.', 'Hey, did you try rotating your proxies?']"
Inputting a variable into classes to find tags in html,https://www.reddit.com/r/webscraping/comments/zlccli/inputting_a_variable_into_classes_to_find_tags_in/,webscraping,"I don't know how to phrase the question for the header but I'm scraping this [site](https://www.betexplorer.com/soccer/england/league-one/) and for each match the teams have a code and that code is used to represent them. So in the league table the classes for the tr tags have the code so I was wondering if there was a way to put the codes I've already collected from earlier in the code into something like 

soup.find('tr', class\_=f'odd glib-participant-{code} highlight')

or with css selectors or xpaths. I already tried f strings and it didn't work.","['Setting aside your triggering use of ""it didn\'t work"" without saying what it *did* do, have you confirmed that the content you\'re looking for is visible to `soup`? I don\'t mean Chrome, I mean _soup_ which does not execute XHRs nor execute JavaScript']"
How do I find duplicate titles?,https://www.reddit.com/r/webscraping/comments/zl0ri6/how_do_i_find_duplicate_titles/,webscraping,"Hi - I am learning web scraping by scraping a bunch of news websites (either the webpage directly or the RSS feed). I am unable to understand how to make a list of all titles or stories that are on the same topic. 

Is there any library (Python or other language) that helps us find similarities between sentences? For example, how do I flag two titles as similar if they are as follows = 

* Apple releases iPhone 14
* iPhone 14 is similar to iPhone 12 - nothing changed at Apple

How do I go about this problem? Thanks and I'd appreciate any tips for solving this.","['You will need more than just string similarity ... although Dedupe.io (and their library) would get you far already \n\nThe topic to look into is Named Entity Recognition (NER) to figure out which titles mention the same companies/products. From there you would tag and categorize\n\nSpacy is a Python library that helps with this', 'This is for AI category ( mainly NLP ). You will need a dataset to train the AI to recognize the similarity between the titiles. You will need lots of titles and time to create the dataset for higher accuracy.']"
Web Scraping messages from Whatsapp,https://www.reddit.com/r/webscraping/comments/zky1rv/web_scraping_messages_from_whatsapp/,webscraping,"Hi! I'm trying to do a whatsapp data scrape.  I want to scrap the text, images and videos, with the date and the contact.

For this I am using selenium in Whats app web.

I'm having difficulties because the whatsapp web code only displays what appears on the screen and not the entire conversation.

I recently saw that whats app released an API for small business owners. Does anyone know of a tutorial that helps download messages through the API? Or do you have any tips?

&#x200B;

Thanks a lot, guys!","[""Your best bet is to scroll up the screen until you find the date you're looking for, I've heard Whatsapp API access is very limited""]"
understanding etl pipelines for webscraping,https://www.reddit.com/r/webscraping/comments/zk5fju/understanding_etl_pipelines_for_webscraping/,webscraping,"Hi all, I have exclusively worked as a freelance webscraper and will be interviewing for a job soon.

The company asked about etl pipelines and aws batch. 

I have no familiarity with aws at all but I belive in my day to day projects, that to some degree, I have had to etl... mostly just to flat files such as excel.

My question is how can I sell that to the employer or should I, considering I deal with extracting, cleaning, and loading, all in one script?

Thanks everyone.","[""When they ask for ETL they mean most likely bigger projects than one-off single file scripts that scrape into Excel files\n\nI would\n\n- familiarize myself with what goes into Extract, Transform, Load - get a feel for best practices and bottlenecks like parallelism and where you have to start thinking about scaling up to multiple servers\n\n- look into pipeline tools like Airflow, Luigi and similar to get an idea what AWS Batch tries to solve and how others do it. Not in depth, just some blog posts so you at least don't hear them mention things that you can't put in context\n\n- ask and listen very well what they need ... and don't overpromise. ETL borders with DevOps and if they expect you to build everything on AWS from scratch then you will face a steep learning curve (with permission management in IAM etc) that you have to take into account time wise\n\nETL is an awesome area to grow into ... just don't jump into the deep end where you have to learn everything at once on a tight schedule"", 'Thank you metrics this company was really cool and just said you can learn the other stuff we connected a lot on webscraping, nlp, and etc but your post helped out a lot', ""I didn't get the job but the requirements were very insightful in what I need to learn and the input from this forum was also amazing. Thank you metrics""]"
Help with scraping ESPN college football data,https://www.reddit.com/r/webscraping/comments/zjpipj/help_with_scraping_espn_college_football_data/,webscraping,"So I'll start by saying that I'm sure there's a better way to do this, but I have already written a bunch of files that work with this method, so I would greatly prefer to keep it. Basically, I am pulling ESPN URL links to college football games, such as :

[https://www.espn.com/college-football/game/\_/gameId/401437033](https://www.espn.com/college-football/game/_/gameId/401437033)

Adding ""&xhr=1"" (which should covert to JSON), and then opening the data with python using:

    response = urllib.urlopen(link)
    data = json.loads(response.read())

This worked all of last season and all of this season until this week, and it seems that specifically it's the ""&xhr=1"" piece that's no longer working (as I'm unable to get a JSON file anymore). Again I know there are other and better ways, but I have many other large files written to parse through the JSON files that I used to get.

Does anyone know anything more about this? Or if there is an easy way to take a link like the one I've given above and covert all the HTML code into JSON?

Many thanks for any help!","['I can\'t get the url to automatically deliver json but I think the data you are looking for is in the response from the page when you load it up. \n\nIn a script tag at the bottom is a long JSON string that you can get out using python like I\'ve done below. I think the juicy info you want is in there. I\'ve used string slicing to get it out and convert it into json: \n\n    import requests\n    import json\n    \n    url = \'https://www.espn.com/college-football/game/_/gameId/401437033\'\n    \n    resp = requests.get(url)\n    \n    s = resp.text\n    start = ""window[\'__espnfitt__\']=""\n    end = \';</script>\\n                <script type=""text/javascript"">__CDN_PATH__=\'\n    \n    dirty = s[s.find(start)+len(start):s.rfind(end)] #get middle of start and end ie json data\n    clean = json.loads(dirty)\n    \n    print(clean[\'page\'][\'content\'][\'gamepackage\'])', 'Check [this answer](https://stackoverflow.com/questions/43469412/convert-html-source-code-to-json-object)']"
Looking for help/ insight,https://www.reddit.com/r/webscraping/comments/zjj0f2/looking_for_help_insight/,webscraping,"Let me start by saying that I have zero experience and I only heard the term webscraping a couple days ago. Sorry if this is the wrong place to post this, please remove if so.

Every year I help do a big online comp for my company. I‚Äôm currently in the middle of doing it all by hand, typing/ copy pasting info into excel‚Ä¶. I got really fed up the other day and figured there had to be a better way to do this. With some googling I found Webscraping which seems to be what I‚Äôm looking for. I‚Äôve been reading about it all weekend but I‚Äôm still pretty lost on how to actually start. I have zero coding experience and like I said earlier, only just heard about webscraping a couple days ago. What would you all recommend as a starting point for me to learn? 

If it helps, my company sells to most of the major retailers in the US (Home Depot, target, Walmart, Costco, etc). So those would be the sites Im pulling from.","['A book called: ""Automate the Boring Stuff with Python: Practical Programming for Total Beginners"" can help you understanding some of the basic of the automation process like: auto drawing, auto sending email and web scraping. You need to understand the concept of web scraping first and then you can choose your favorite tool for the job (to me is Python). The learning curve has taken me 3 months:\n+ First, just Python and Beautifulsoup for simple webs. \n+ Next, using Selenium for more dynamic webs.\n+ Last, using scrapy to go even more high level web scraping include: threading, middleware, cookies...', 'Let me know if you need help, i build Web Automated Python scripts that do actually what u are looking for , scrape web data from online stores daily and save into a database or export to a csv file. I will ge happy to work with you on this.', ""Hey, [here's a](https://www.youtube.com/@oxylabs) great YouTube channel to start with web scraping. There's a lot of step-b-step tutorials for beginners, and in general lots of information to explore about scraping, it's ethics, tools, etc. Give it a try."", ""If you don't mind, I can actually help you with the project. I am a professional web scraper on Upwork.""]"
is zillow still scrapable with python bs4 2022?,https://www.reddit.com/r/webscraping/comments/zjho5f/is_zillow_still_scrapable_with_python_bs4_2022/,webscraping,"I tried to scrape it and I couldnt get any data back. I outputted the driver.page\_source to the terminal since that is that i pass that into the soup object but the terminal output displays html text that indicate the site was not found.

    <html xmlns=""http://www.w3.org/1999/xhtml"" data-l10n-sync=""true"" dir=""ltr"" lang=""en-US"">
      <head>
        <meta http-equiv=""Content-Security-Policy"" content=""default-src chrome:; object-src 'none'"" />
        <meta name=""color-scheme"" content=""light dark"" />
        <title data-l10n-id=""neterror-dns-not-found-title"">Server Not Found</title>
        <link rel=""stylesheet"" href=""chrome://global/skin/aboutNetError.css"" type=""text/css"" media=""all"" />
        <link rel=""icon"" id=""favicon"" href=""chrome://global/skin/icons/info.svg"" />
        <link rel=""localization"" href=""branding/brand.ftl"" />
        <link rel=""localization"" href=""toolkit/neterror/certError.ftl"" />
        <link rel=""localization"" href=""toolkit/neterror/netError.ftl"" />
      </head>
    
      <body class=""neterror"">
        <!-- PAGE CONTAINER (for styling purposes only) -->
        <div class=""container"">
          <div id=""text-container"">
            <!-- Error Title -->
            <div class=""title"">
              <h1 class=""title-text"" data-l10n-id=""dnsNotFound-title"">Hmm. We‚Äôre having trouble finding that site.</h1>
            </div>
    
            <!-- Short Description -->
            <p id=""errorShortDesc"">We can‚Äôt connect to the server at automationcontrolled. <span data-l10n-id=""neterror-dns-not-found-with-suggestion"" data-l10n-args=""{&quot;hostAndPath&quot;:&quot;www.automationcontrolled.com&quot;}"">Did you mean to go to <a href=""https://www.automationcontrolled.com/"" data-l10n-name=""website"">www.automationcontrolled.com</a>?</span></p>
            <p id=""errorShortDesc2""></p>
    
            <div id=""errorWhatToDo"" hidden="""">
              <p id=""errorWhatToDoTitle"" data-l10n-id=""certerror-what-can-you-do-about-it-title"">What can you do about it?</p>
              <p id=""badStsCertExplanation"" hidden=""""></p>
              <p id=""errorWhatToDoText""></p>
            </div>
    
            <!-- Long Description -->
            <div id=""errorLongDesc""><span data-l10n-id=""neterror-dns-not-found-hint-header""><strong>If you entered the right address, you can:</strong></span><ul><li data-l10n-id=""neterror-dns-not-found-hint-try-again"">Try again later</li><li data-l10n-id=""neterror-dns-not-found-hint-check-network"">Check your network connection</li><li data-l10n-id=""neterror-dns-not-found-hint-firewall"">Check that Firefox has permission to access the web (you might be connected but behind a firewall)</li></ul></div>
    
            <p id=""tlsVersionNotice"" hidden=""""></p>
    
            <p id=""learnMoreContainer"" hidden="""">
              <a id=""learnMoreLink"" target=""_blank"" rel=""noopener noreferrer"" data-telemetry-id=""learn_more_link"" data-l10n-id=""neterror-learn-more-link"" href=""https://support.mozilla.org/1/firefox/107.0.1/Linux/en-US/connection-not-secure"">Learn more‚Ä¶</a>
            </p>
    
            <div id=""openInNewWindowContainer"" class=""button-container"" hidden="""">
              <p><a id=""openInNewWindowButton"" target=""_blank"" rel=""noopener noreferrer"">
              <button class=""primary"" data-l10n-id=""open-in-new-window-for-csp-or-xfo-error"">Open Site in New Window</button></a></p>
            </div>
    
            <!-- UI for option to report certificate errors to Mozilla. Removed on
                 init for other error types .-->
            <div id=""prefChangeContainer"" class=""button-container"" hidden="""">
              <p data-l10n-id=""neterror-pref-reset"">It looks like your network security settings might be causing this. Do you want the default settings to be restored?</p>
              <button id=""prefResetButton"" class=""primary"" data-l10n-id=""neterror-pref-reset-button"">Restore default settings</button>
            </div>
    
            <div id=""certErrorAndCaptivePortalButtonContainer"" class=""button-container"" hidden="""">
              <button id=""returnButton"" class=""primary"" data-telemetry-id=""return_button_top"" data-l10n-id=""neterror-return-to-previous-page-recommended-button"">Go Back (Recommended)</button>
              <button id=""openPortalLoginPageButton"" class=""primary"" data-l10n-id=""neterror-open-portal-login-page-button"" hidden="""">Open Network Login Page</button>
              <button id=""certErrorTryAgainButton"" class=""primary try-again"" data-l10n-id=""neterror-try-again-button"" hidden="""">Try Again</button>
              <button id=""advancedButton"" data-telemetry-id=""advanced_button"" data-l10n-id=""neterror-advanced-button"">Advanced‚Ä¶</button>
            </div>
          </div>
    
          <div id=""netErrorButtonContainer"" class=""button-container""><button class=""primary try-again"" data-l10n-id=""neterror-try-again-button"">Try Again</button>
            
          </div>
    
          <div class=""advanced-panel-container"">
            <div id=""badCertAdvancedPanel"" class=""advanced-panel"" hidden="""">
              <p id=""badCertTechnicalInfo""></p>
              <a id=""viewCertificate"" href=""javascript:void(0)"" data-l10n-id=""neterror-view-certificate-link"">View Certificate</a>
              <div id=""advancedPanelButtonContainer"" class=""button-container"">
                <button id=""advancedPanelReturnButton"" class=""primary"" data-telemetry-id=""return_button_adv"" data-l10n-id=""neterror-return-to-previous-page-recommended-button"">Go Back (Recommended)</button>
                <button id=""advancedPanelTryAgainButton"" class=""primary try-again"" data-l10n-id=""neterror-try-again-button"" hidden="""">Try Again</button>
                <button id=""exceptionDialogButton"" data-telemetry-id=""exception_button"" data-l10n-id=""neterror-override-exception-button"">Accept the Risk and Continue</button>
              </div>
            </div>
    
            <div id=""blockingErrorReporting"" class=""advanced-panel"" hidden="""">
              <p class=""toggle-container-with-text"">
                <input type=""checkbox"" id=""automaticallyReportBlockingInFuture"" role=""checkbox"" />
                <label for=""automaticallyReportBlockingInFuture"" data-l10n-id=""neterror-error-reporting-automatic"">Report errors like this to help Mozilla identify and block malicious sites</label>
              </p>
            </div>
    
            <div id=""certificateErrorDebugInformation"" class=""advanced-panel"" hidden="""">
              <button id=""copyToClipboardTop"" data-telemetry-id=""clipboard_button_top"" data-l10n-id=""neterror-copy-to-clipboard-button"">Copy text to clipboard</button>
              <div id=""certificateErrorText""></div>
              <button id=""copyToClipboardBottom"" data-telemetry-id=""clipboard_button_bot"" data-l10n-id=""neterror-copy-to-clipboard-button"">Copy text to clipboard</button>
            </div>
          </div>
        </div>
      </body>
      <script src=""chrome://global/content/neterror/aboutNetErrorCodes.js""></script>
      <script type=""module"" src=""chrome://global/content/aboutNetError.mjs""></script>
    </html>","['Zillow is very easy to scrape using its internal API. Are you familiar with the Network tab?', ""Yes, you can, but it's way more complicated than just simply sending a http request or doing driver.get using selenium... you have to overcome its bot detection mechanism.""]"
Daath AI Parser is an open-source application that uses OpenAI to parse visible text of HTML elements.,https://github.com/kagermanov27/daath-ai-parser,webscraping,,
Help needed with extracting videos from site.,https://www.reddit.com/r/webscraping/comments/zisszd/help_needed_with_extracting_videos_from_site/,webscraping,"Hi all,

I'm pretty new to webscraping, so far I only done webscraping by loading the source page of a website in regexbuddie and use regex to extract the element I want to use in c#.

There are several  TV broadcasters in my country (Belgium) which have a video-on-demand platform. One of them is VRT MAX, there's a serie called In de gloria and I want to download the videos: [https://www.vrt.be/vrtnu/a-z/in-de-gloria/1/in-de-gloria-s1a1/?ndl=true](https://www.vrt.be/vrtnu/a-z/in-de-gloria/1/in-de-gloria-s1a1/?ndl=true)

I checked the source page but there's no link  to any video file.

How can I download the videos ?","[""If there's ever anything one wants to do with downloading audio or video from the Internet, first start with youtube-dl or its friend yt-dlp\n\n* https://github.com/ytdl-org/youtube-dl/blob/2021.12.17/youtube_dl/extractor/vrt.py\n* https://github.com/yt-dlp/yt-dlp/blob/2022.11.11/yt_dlp/extractor/vrt.py"", 'scrape links, then youtube-dl or wget', 'The videos are streamed using dash and broken up into pieces using CodeShops FreeUsp (From what I see from the site Responses) Basically what you need to do is to retrieve the ID of the video you want to watch, start a scrapper that hits their API using http requests and retrieves the video segments that are base64 encoded, find a way to merge them into a file and you got your video format.   \n\n\nSome examples of the API calls are:  \nFor video: [https://ondemand.vrtcdn.be/content/vod/vid-4fbb5def-55b6-4980-89a3-7d20271ebf02-CDN\\_4\\_HD2022/vid-4fbb5def-55b6-4980-89a3-7d20271ebf02-CDN\\_4\\_HD2022\\_drm\\_ffda6c9d-37a6-41c1-8343-d468f1bba673.ism/dash/vid-4fbb5def-55b6-4980-89a3-7d20271ebf02-CDN\\_4\\_HD2022\\_drm\\_ffda6c9d-37a6-41c1-8343-d468f1bba673-video=4992105-87552.dash](https://ondemand.vrtcdn.be/content/vod/vid-4fbb5def-55b6-4980-89a3-7d20271ebf02-CDN_4_HD2022/vid-4fbb5def-55b6-4980-89a3-7d20271ebf02-CDN_4_HD2022_drm_ffda6c9d-37a6-41c1-8343-d468f1bba673.ism/dash/vid-4fbb5def-55b6-4980-89a3-7d20271ebf02-CDN_4_HD2022_drm_ffda6c9d-37a6-41c1-8343-d468f1bba673-video=4992105-87552.dash)  \n\n\nIf you check the last part of the url you can see -video={number}-{number}.dash, I am not sure what exactly the first number is but the second number is the segment of the video your browser is receiving at that request.  \n\n\nOther than recording the videos you need to make a scraper that can communicate with VRT MAX API, download all the segments and merge them together.   \n\n\nI also need to point out that it is highly illegal to Record/Download this content.   \nBe safe.']"
What are some ways web scraping can be used UNETHICALLY?,https://www.reddit.com/r/webscraping/comments/zhsybt/what_are_some_ways_web_scraping_can_be_used/,webscraping,"Doing a presentation for my computer science class about web scraping.

In my presentation I mentioned web scraping can be used ethically and unethically. Due to our school and presentation requirements we HAVE to provide examples of stuff we mention, it‚Äôs a pain in the butt, but it does teach us research and critical thinking skills.

Anyway, can you guys please tell me some ways the web scraping can be used unethically? Thanks in advance!","['cloning website  and host it  as it real website', 'Exploiting misconfiguration in apis and retrieving data that should not be possible, there have been cases that people could extract personal data of users just because the developers where incompetent and could not build proper apis.', 'First one that comes to mine is a DOS attack by over exploiting the servers.', ""https://www.youtube.com/watch?v=pHu0_ksUAbQ\n\nThe data in the what's app data breach was scraped from Google. The video tells more details about it.\n\nAlso most tools from r/OSINT utilize web scraping and how osint can be missuesd is obvious.\n\nAlso in the process of creating bots it is very likely you'll find vulnerabilities like ssrf or an sql injection. Ior APIs give more data than they should. I think it's obvious where I am going with that"", 'Scraping LinkedIn and selling the database on black markets\n\nIt has been done before', ""Scalping graphic cards or ps5's"", '1. Many a times in webscraping it is require sending request to server multiple time in short span of time , this can cripple their infrastructure \n\n2. Using crawled/scraped data to sell it in black market.', 'If you scrape a website you can clone it if you feel like it and use to for example steal login details', 'I‚Äôm on a pirating website and they always complain about scraping because 1. It takes a big toll on the servers and 2. if the data falls into the wrong hands then all the links will get taken down', 'An interesting anecdote might be the case of google scarping lyrics genuis (they used a fun trick to prove it was being scraped)\n\nOther more general examples\n\n* Marketplace manipulation by comparing prices (selling), or sniping deals (buying)\n* You could argue scraping in itself is unethical since it ignores tos and spiders.txt , giving sometimes unmanageable traffic spikes and access to information in an uncontrolled way (unlike an API)\n* Selling scraped info for phising\n* Stolen content/articles for websites that just exist to have ads/some other way to get money\n\nThere was also some news regarding a lawsuit of a big Asian scraper, maybe there is something interesting in there for your presentation [https://techcrunch.com/2022/07/06/meta-sues-chinese-companys-us-subsidiary-for-scraping-facebook-and-instagram-data/](https://techcrunch.com/2022/07/06/meta-sues-chinese-companys-us-subsidiary-for-scraping-facebook-and-instagram-data/?guccounter=1)\n\nOn the other side of the spectrum there is services like google image search and instagram trying to make it harder for users to ""download"" images, even though by design of the web they are already on the users hard disk.\n\nIf you end up using any of this suggestions, please leave a shout out to r/webscraping in your presentation (or scrape it like the godless pirate you are ;)', 'You can check webscraping service terms of service and the prohibited usage, I know scrapfly is very strict for example, and there tos looks to sum up very well [https://scrapfly.io/terms-of-service](https://scrapfly.io/terms-of-service) (section 10 Prohibited Uses)', 'Here is a good guide on the ethics of web scraping that be helpful. \n\nhttps://scrapeops.io/web-scraping-playbook/ethics-of-web-scraping/']"
Where do you sell data sets?,https://www.reddit.com/r/webscraping/comments/zhn2t9/where_do_you_sell_data_sets/,webscraping,"Hey folks, love the community and knowledge and experience around here. I‚Äôve learned a lot. Question about what are good platforms to sell data sets on. I guess these platforms are sometimes called data marketplaces. I‚Äôve googled and checked out a few, but I didn‚Äôt see anything that seemed like a good fit for a small time web scraper who just has a few data sets to post up.

Anyone have good experiences with particular places that they‚Äôd like to recommend? Thanks in advance.","['I offer some datasets on Datarade (datarade.ai). Not a lot of traffic but if you have highly specialized and high quality data you will find some customers there', ""Hi! We're a new data marketplace startup based in the U.S. called Sellagen (sellagen.com). We are an easy to use and intuit data marketplace who prides in our accessibility and are looking for people with data that are willing to sell their data in our platform. It is completely free to create an account and list data. You can set your own price for your data. Basically imagine the platform as your data store front or an eBay for data if you will. So this offers a clean, hassle free option for you to sell your (legally) acquired data and you don‚Äôt have to worry with building your own infrastructure like a payment system for example. Feel free to reach out if you have any questions!"", ""What's the average price for CE of a dataset?""]"
Scraping data and then showing it on my webpage with charts,https://www.reddit.com/r/webscraping/comments/zhurbp/scraping_data_and_then_showing_it_on_my_webpage/,webscraping,"Hello all! is this the right approach to this problem? Is there an easier way? 

What i want to do:

 \- Scrape some data of the web. Numbers

How i plan on doing it: 

\- JavaScript

\- Puppeteer

\- Express.js","['It highly depends on what languages you can code and how sophisticated you want to get\n\nIf you ""speak"" Python but either are Meh in frontend development or don\'t care enough to spend a lot if time on the frontend I would look into Streamlit. Ok interface, pure Python (but you can build components in React if you need some custom oomph)\n\nIf you can code Javascript (or Python) check out Bokeh. This is more about interactive visualizations but can be used to code static figures too. I used Bokeh to create the live hotel views on https://ratedefence.com\n\nPure Javascript and don\'t mind a steep learning curve: check D3. You basically code up svg figures with varying interactivity\n\nFor simple things there are smaller libraries like charts.js and similar\n\nWhat you use in the end highly depends on the project. You don\'t need Puppeteer if you can scrape an API directly. Express is awesome to build APIs, to serve static websites it can be overkill.', 'I used Python to scrape data, then I use MySQL to upload it to phpmyAdmin, then I just use php to pull the data from the database to display it.\n\nThat‚Äôs how I do it. But I‚Äôm a novice, so if anyone knows an easier way, please do let me know', 'Used python and Django for this. That way everything is in one language. Better Charts can be made with chart JS.', 'Replit share a full stack for that']"
Help with a project,https://www.reddit.com/r/webscraping/comments/zhughh/help_with_a_project/,webscraping,"First project! I‚Äôm having troubles getting this code to scrape all the team names , expects points etc from this website: https://rugby4cast.com/predictions/
It is only showing the titles in the HTML 


This is the code I‚Äôm using:

import requests
from bs4 import BeautifulSoup
import pandas as pd

# Create an empty list to store the data
data = []

# Fetch the HTML content of the page using the requests library
url = 'https://rugby4cast.com/predictions/'
response = requests.get(url)

# Parse the HTML content using Beautiful Soup
soup = BeautifulSoup(response.content, 'html.parser')

# Use Beautiful Soup to find the HTML elements that contain the data you want to extract
date_elements = soup.find_all('span', attrs={'class': 'champ_date'})
home_team_name_elements = soup.find_all('div', attrs={'class': 'team_name'})
away_team_name_elements = soup.find_all('div', attrs={'class': 'team_name'})

# Loop through the date elements and extract the text from each element
for date_element in date_elements:
    date_text = date_element.text.strip()
    print(f'Date: {date_text}')

# Loop through the home team name elements and extract the text from each element
for home_team_name_element in home_team_name_elements:
    home_team_name_text = home_team_name_element.text.strip()
    print(f'Home team: {home_team_name_text}')

# Loop through the away team name elements and extract the text from each element
for away_team_name_element in away_team_name_elements:
    away_team_name_text = away_team_name_element.text.strip()
    print(f'Away team: {away_team_name_text}')

    # Add the data for the teams to the `data` list
    data.append([date_text, home_team_name_text, away_team_name_text])


# Create a Pandas DataFrame with the extracted data
df = pd.DataFrame(data, columns=['Date', 'Home', 'Home xP', 'Away', 'Away xP'])

# Write the data from the Pandas DataFrame to an HTML file
df.to_html('rugby_predictions_bs.html')","[""Add this to your code:\n\n    headers = {\n    \t'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n    \t}\n    \n    url = 'https://rugby4cast.com/predictions/'\n    response = requests.get(url, headers=headers)""]"
How long would it take to webscrape all my reddit comments into an excel or word document?,https://www.reddit.com/r/webscraping/comments/zhaq9s/how_long_would_it_take_to_webscrape_all_my_reddit/,webscraping,Approximately? Would I be able to find someone on OfferUp or something who could do it in less than an hour? Or would it be more cumbersome than that?,"[""5-10 minutes for a programmer (using reddit's api)\n\nOr use a [data request](https://www.reddit.com/settings/data-request) from reddit."", 'Just came here to say I like ur username üê¢']"
What do you charge for webscraping on average?,https://www.reddit.com/r/webscraping/comments/zh2xa9/what_do_you_charge_for_webscraping_on_average/,webscraping,"I am getting the impression that webscraping is considered fairly low skilled job by many clients and thus expect it to be extremely cheap. So, I would like to get a general picture of compensation for webscraping.

I would really appreciate if you would participate in the poll.

Thanks

[View Poll](https://www.reddit.com/poll/zh2xa9)","[""it's not an impression, but those are toxic clients that nobody want in fact. The typical profile:  US based companies founded by non tech people and hiring fully offshore on fiver and lot of management. Of course they are disapointed and have very bad experience, in web scraping fiverr is the bottom trash tier (if you are serious, never put your profile on it - it's negative and you will only attract toxic client - don't be fooled by rating on profile, it's full fake). In the end they dont have budget required for the expected volume and result and you must ignore them.   \n\n\nSometimes there are another kind of those toxics, it's a freelance getting the job and hiring for low budget to do the actual job."", 'You should charge depending on the structure of the website and the amount of data to be scraped.\n\nOn average 100 - 300 bucks is enough for a static website and scraping a data of 10k', 'I get paid $60 an hour for it']"
Need help selecting this div,https://www.reddit.com/r/webscraping/comments/zgx7kj/need_help_selecting_this_div/,webscraping,"I'm scraping a very old site that uses ""#document"" tag in its html (????), and i need to find a child div inside this element tag.  
I'm using Selenium (Python). No xpath extensions worked.  
Does someone knows how could I select it?

I tried the /html/frameset/frame/#document/html/body/div\[2\]/div xpath but it seems that its not valid  


https://preview.redd.it/cuqp3d85iv4a1.png?width=484&format=png&auto=webp&v=enabled&s=10ab8054172f4ab17ed37e9ff36687d3f6091aa0","[""Here's a thread that might help!\n\nhttps://stackoverflow.com/questions/53827240/how-to-scrape-html-elements-with-no-class-or-id-specified-in-attribute-with-beau"", 'use beautiful soup and select it allows you select multiple class/style etc ... attributes', ""The <frame> probably can't be accessed from the parent (security sandbox etc). Did you try to load the frame src (domain.tld/t_top.asp) directly with Selenium and use the regular xpath libraries on the child document?""]"
How did everyone here learn how to webscrape? and have any of you made any cool projects with it?,https://www.reddit.com/r/webscraping/comments/zgdi1w/how_did_everyone_here_learn_how_to_webscrape_and/,webscraping,"Hi everyone, this is my first ever reddit post (feels kind of unbelievable given that im a 22 year old web addict). I've been teaching myself how to code for the past few years and have gone fairly deep into web scraping in particular. I was just wondering what stories other people have in why they decided to learn how to do webscraping. It would also be cool to hear about any projects that any of you worked on; So far i've built a few bots that run social media pages for me and I intend to use it more in conjunction with AI and economic theory in the future (currently writing up the proposal for that). hope to hear something cool from one of you soon!","['My entry into web scraping was to solve specific problems I had, like scraping pollen readings from multiple local allergy clinics and news stations (https://www.hayfevr.ly). Another scraper was to reformat the interface for one of my favorite news sites (https://www.thnr.net is what I made). Having a personal connection to the project certainly helps keep me to see it through.\n\nI like the challenge of scraping whether it‚Äôs choosing the rate limit or crafting an xpath. It‚Äôs better than sudoku at giving my brain a workout!', 'I happened upon webscraping while learning python for data analysis.\nMy first project was to scrape IMDb for data on movies and do a any type of analysis on it.\nHonestly the project wasn\'t much but it felt powerful and I looked into it further, and realized any information on the web can be gathered and structured. I especially love tackling the ""store locator"" pages for any brand I can think of.\nNow I do odd jobs on freelancing sites just as a side-hustle/hobby.', 'I have scraping projects of many sectors, static and dynamic webs, currently working on antibot forms like captcha codes, i have quite expierence scrapping the main content and structure the data.', 'I just did projects. \n\nMy first project was... To scrape lots of cute catgirl pictures with categories from many sources. *1k of the even have clothes*. I now have 2 frontends and a maria db database with over 2k pictures which are all categorized. Yea it was my first project....  \nHere is the blog I wrote https://ln.topdf.de/HellowNeko-Blog/\n\nTh biggest project I did create is a pretty powerfull music downloader which searches the whole internet for metadata as well as audio data and automatically downloads the requested songs and modifies the metadata of the mp3 files accordingly. It can even automatically search and find most songs of most underground artists and download them all with one command. It also fetches the lyrics, and for the cache I use a sql database.  \n - I got over 50 stars on my Github repo: https://github.com/HeIIow2/music-downloader  \n - Of course I also packaged it and uploaded it to PyPI: https://pypi.org/project/music-kraken/  \n - It is over 3k Lines of python code *so far*\n\n*I am only 17 by the way and basically maintain it myself without any help XD*', ""I'm also learning about web scraping. I have a Udemy course and it uses Scrapy. My project is to make a web scraping portal or marketplace. I still don't know how to make it though."", ""I mostly learnt web scraping by messing around. After two weeks or so, I went from completely inexperienced to kind of competent lol. I used a lot of online resources like YouTube videos, articles and documentation from different scraping libraries to get started.\n\nI learnt webscraping because I wanted to carry out a project that had been on my mind for a long while. The project was a program to download anime for me automatically. I ended up completing it with a few complications, but it doesn't work anymore lol, so I'll have to redo it soon.\n\nI also played around with a few APIs and made a program to list out the details of whatever anime you type in.\n\nI also learnt some Web automation with Selenium and recently used it to make a WhatsApp Bot to forward messages on their website.\n\nNow, I scrape sites and articles once in a while just for fun. \n\nConsidering using what little experience I have to make some money on Fiverr or something."", ""I've always wanted to but had trouble figuring it out, then I found out my brother had made one, and he taught me. When I understood how the Requests module worked, everything clicked and worked like magic""]"
"What is the best, free, web scraping tool?",https://www.reddit.com/r/webscraping/comments/zg93ht/what_is_the_best_free_web_scraping_tool/,webscraping,"Hey all, I‚Äôm looking for a free web scraping tool that can scrape from multiple sources and pair data sets. 

Any recommendations?","['Build one for yourself', ""scrapy is the best i've come across so far, and i've scraped for awhile. It's amazing in every way and i'm still learning new things it can do"", 'Probably Axios and Cheerio.', 'Python', ""Really depends how messy your sources are. If they're consistent, probably fastest to spin up ParseHub or Scrape up or a few others. Test and see what needs to get cleaned. Then think about how you'd want to scale or repeat it. \n\nUnless you're doing it for a personal dev project, then by all means do it manually through Selenium, Python's bs or scrapy or R."", ""I'd say just build your own. That is always the best free option"", 'Crawlee', ""I'm looking for a scraping tool and came across Bardeen. It is free, anybody has experience with that?"", ""I developed a database/SAAS-product similar to Enlyft that provides users with a list of companies and their respective technology stacks. Once the list is exported, users can set up an automated artificial intelligence messaging campaign. It's way cheaper then Enlyft, if you would like a demo, DM me!"", ""If you're thinking about serious web scraping then [https://playwright.dev](https://playwright.dev) is the tool to go. It's a Swiss Army knife for web scraping."", ""It depends on what you want to do.  \n\n\nFor general scraping for personal needs, I would suggest building one yourself.   \n\n\nThere are mainly 2 ways to approach this:  \n1. HTTP client. All programming languages have some sort of http client that will allow you to access a remote URL and obtain its HTML code. After that it is a matter of getting a library to traverse and parse this HTML based on your liking.  \n2. Using headless browser. Chrome can be run in headless mode and be controlled remotely via API. You can use puppeteer for Node.js or PuppeteerSharp for .Net to take care of this for you. For other languages you can find a corresponding wrapper library.  \n\n\nYou will need an external service for this only in the following usecases:  \n\\* You want to scrape big amount of data and don't want to take care of the corresponding infrastructure.  \n\\* The website you scrape have anti-bot systems and lack the know-how and/or big enough proxy pool to avoid detection.  \n\\* You are not technically inclined to do it yourself. Unfortunately none of the tools I know are user friendly enough for someone who doesn't know what they are doing.  \n\n\nIf you still want to check out some services. Here they are:  \n\\* BrightData - [https://brightdata.com/](https://brightdata.com/) . They are probably one of the best, but it comes with the associated cost.  \n\\* ScrapingBee - [https://scrapingbee.com](https://scrapingbee.com/) . Cheaper, easier to use. It has 1000 API credits valid for 1 month, after that you need to pay for the service.  \n\\* Crawlio - [https://www.crawlio.net](https://www.crawlio.net) . Even cheaper. Has the ability to configure crawling and scraping which you can configure by clicking on the page elements you want it to extract. No code knowledge required. Comes with 1000 API credits for the first month and then you revert to the Free plan that gives 100 API credits per month without a duration limit. Full disclaimer: Crawlio is my personal service that I offer.""]"
Scraping small businesses in United States,https://www.reddit.com/r/webscraping/comments/zgej3w/scraping_small_businesses_in_united_states/,webscraping,"Hey guys! I'm new here. Just starting my path as a freelance data scraper, and client requiring me to scrape small business contacts in US for his lead generation. And if I'm ok with implementation, I still can't come up with a proper place, from where we should scrape this data. Can someone give me a hint, with resource/database what I could use for this task?   


Thanks in advance.  ","['You can scrape yellowpages, superpages, yelp, manta.com, europages botw.\n\nI have a web app that can help you with the project. Let me know if you will to see, I will send you the link.', 'e.g. facebook marketplace', 'Small businesses? Like he wants millions of leads? \nNarrow it down, he should make it more specific, then you can think again where there is such data.']"
Need web scraping professional to help me decide how to proceed with my lead generation idea,https://www.reddit.com/r/webscraping/comments/zgjapg/need_web_scraping_professional_to_help_me_decide/,webscraping,"I would like a pretty specific set of data scraped from 1 or 2 websites.  

Trying to decide if paying someone on fiverr when I need more leads is smarter than paying for the code?  If that question doesn‚Äôt make sense then I apologize, I have about 2 hours of researching web scraping under my belt.

Anyone want to steer me in the right direction and possibly gain another client?","[""Okay, let me explain something you will run into later.\n\nAs you understand, webscraping basically finds data from websites in an automated fashion. However almost every website which has valuable data actively tries to thwart it. In addition to it, websites change for a lot of reasons. \n\nSo, if you pay for the code once, after a while it will most certainly stop working for the two aforementioned reasons. It is something you should keep in mind if you want to pay once for getting the code. \n\nIn my opinion, the most hassle free way is to pay someone to write and maintain the scraper for a fixed monthly fee and they will supply the leads to you monthly.\n\nIf the data is static and doesn't update, then it would make sense to just pay once for getting a list of all leads.\n\nJust something to keep in mind."", 'Hello, I am a professional freelancer on Upwork and I will love to help with the web scraping project.\n\nKindly send me the link to the websites. I will start by inspecting them.', 'Thank you for all the messages.  Khaliq hopped in first and he‚Äôs been great to work with', 'Try [Cloudlead.co](https://Cloudlead.co) \\- effective and reasonable..!']"
"Can I get 100,000+ rows from database at my library?",https://www.reddit.com/r/webscraping/comments/zgcdyl/can_i_get_100000_rows_from_database_at_my_library/,webscraping,"I was sent on a mission to obtain an updated list of dental offices in the USA. I was able to access this beautiful list via my library card, however I'm only able to download 500 rows at a time. I'd really love to get this all in one table as quickly as possible. Is this doable with a scraper? If so, any recommendations would be greatly appreciated.","[""you're able to access the database via the commandline?"", 'Share the link i think i can do it', 'Write a script using python.']"
Beginner starting out,https://www.reddit.com/r/webscraping/comments/zgbqjb/beginner_starting_out/,webscraping,Hello all im a web dev student and i would like to make some kind of webscraping project. Any beginner tips? I will be using Javascript because thats what im learning in school.,"['Playwright is a great tool to start with.', ""Here's a tutorial on starting web scraping with Playwright -> [https://oxylabs.io/blog/playwright-web-scraping](https://oxylabs.io/blog/playwright-web-scraping) (reflecting the previous comment)""]"
How to make up a overall great portfolio in web scraping?,https://www.reddit.com/r/webscraping/comments/zf9a4v/how_to_make_up_a_overall_great_portfolio_in_web/,webscraping,"I'm a beginner starting out, I would like to know what projects or scraping what site will make a great portfolio for web scraping.","['You want to showcase your skill level, flexibility, and creativity. Here are some ideas. Show your proficiency in these areas, and clients will be beating down your door.\n\nScraping Tools/Techniques\n\n-   roll your own scraper using `requests`\n\n-   scrape using a framework like `scrapy`\n\n-   scrape using a headless browser like selenium, playwright, puppeteer\n\n-   scrape using Google Sheets `IMPORTXML`\n\n-   scrape using Excel PowerQuery\n\n-   scrape by parsing html (ie, screen scraping)\n\n-   scrape by querying hidden API/REST endpoints\n\n-   scrape where you have to also crawl the website using a spider\n\n-   scrape in parallel such as by using multithreaded or a pool\n\n-   scrape where you have to use OCR (optical character recognition)\n\n-   scrape using xpath, regex, css selectors, text search\n\n-   scrape using a cloud/remote host\n\n-   scrape using python, Java\n\n-   scrape using an API tool like postman, insomnia\n\n-   set up a recurring or scheduled scrape for data that changes regularly (like prices)\n\nGaining/Maintaining Access\n\n-   scrape using proxies\n\n-   scrape where you need to authenticate in some way (cookies, user/pass)\n\n-   scrape something that doesn‚Äôt like scraping (google SERPs, LinkedIn)\n\n-   scrape where you have to overcome CAPTCHA, RECAPTCHA, and similar ‚Äúprove you‚Äôre human‚Äù defenses\n\n-   scrape where you have to overcome Cloudflare, DataDome, Imperva, Botprotect.io, Distill Network, and similar defenses\n\n-   scrape where presenting as a mobile UA makes it easier\n\n-   scrape using `undetected_chromedriver`\n\n-   scrape where you have to fill in form elements to gain access or to reach all data\n\n\nVariety\n\n-   scrape some type of social media\n\n-   scrape emails using pop3, imap\n\n-   scrape from PDFs\n\nProcessing, Deliverable\n\n-   output scraped data in multiple formats (txt, csv, Excel, json, sqlite3)\n\n-   output scraped data to database over a network (eg, MySQL, Postgres, MongoDB)\n\n-   scrape multiple websites/data sources and consolidate results into a single data set without duplicates', 'https://www.rapidtech1898.com/htmlFinanztools/en/portfolioWebScrapingEN.html', 'Im also starting out, reach out if you want we could healp each other']"
Scraping this page,https://www.reddit.com/r/webscraping/comments/zf29un/scraping_this_page/,webscraping,"This is probably a simple one for a subreddit dedicated to web scraping. 

I like the books on this page. I would like to add them to my notion page with books to read. I don't want to type over all the books. Is there an easy way to scrape the titles and authors to a spreadsheet? 

  
[https://www.mostrecommendedbooks.com/books-billionaires-read](https://www.mostrecommendedbooks.com/books-billionaires-read?ref=producthunt)","['In Google Sheets use this formula for Authors: \n\n    =IMPORTXML(""https://www.mostrecommendedbooks.com/books-billionaires-read?ref=producthunt"",""//*[@id=\'__next\']/ol/li//div/div/div[1]/p"")\n\nand this one for Book Titles:\n    \n    =IMPORTXML(""https://www.mostrecommendedbooks.com/books-billionaires-read?ref=producthunt"",""//*[@id=\'__next\']/ol/li//div/div/div[1]/a[1]/h3"")']"
Checking html integrity check after scrapping,https://www.reddit.com/r/webscraping/comments/zf3nrv/checking_html_integrity_check_after_scrapping/,webscraping,Would like to know any best practices available after a Web page has been scraped using Scrapy or Selenium python packages. Thanks.,[]
Scrapy-platywright Shadow DOM,/r/scrapy/comments/zexnab/scrapyplatywright_shadow_dom/,webscraping,,
Web scraping Real State - Get Properties announcements - Owner's personal contact phone,https://i.redd.it/0cxqcfybpc4a1.png,webscraping,,
How to get bank movements BBVA?,https://www.reddit.com/r/webscraping/comments/zepbi9/how_to_get_bank_movements_bbva/,webscraping,"A while back I asked how to defeat BBVA's anti bot system and someone, I think here, suggested me to buy the services of a company that would get me that data. I don't remember what was that company and I can't find the post or find it googling it.

Can you help me guys?","[""Unsure where you are located, but look up 'PSD2 API'.\nI recommend Nordigen.com it will allow you to connect tour bank and fetch transactions easily""]"
"Made a little tool to test & compare various scraping services ‚Äì all in the browser, no tracking, open-source",https://github.com/Strajk/scraping-services-tester,webscraping,,
Are there any tools to perform automated scraping and converting the output to PDF? Any free versions of those to try and then buy ?,https://www.reddit.com/r/webscraping/comments/zeho6f/are_there_any_tools_to_perform_automated_scraping/,webscraping,I am looking to scrape a webpage with many hyperlinks and want each of the resultant hyperlinked pages to be scraped as well and then publish a pdf output and hyperlink same way for easy internal navigation. I wanna know if any tools out there can do just this,"['How similar are the hyperlinked websites?', 'I can do this for you. Will provide you the tool.']"
I am searching a api for live soccer statistics.,https://www.reddit.com/r/webscraping/comments/ze61mw/i_am_searching_a_api_for_live_soccer_statistics/,webscraping,"Hello.

Do anyone know/used an API from which i can get live soccer statistics.

Thanks in advance.",[]
Scrape tiktok views count each hour and have a chart to see the evolution,https://www.reddit.com/r/webscraping/comments/ze5t6n/scrape_tiktok_views_count_each_hour_and_have_a/,webscraping,"Hello,

I'm sure this is a easy request but I am a full beginner in scraping and google sheets.

I'm trying to scrape views count of some videos from this profile:

[https://www.tiktok.com/@davidguetta?lang=fr](https://www.tiktok.com/@davidguetta?lang=fr)

\----

First, I'm trying to scrape followers number, it seems easier :

=IMPORTXML(""[https://www.tiktok.com/@davidguetta?lang=fr](https://www.tiktok.com/@davidguetta?lang=fr)"",""//strong\[@title='Abonn√©s'\]"")

but I have ERROR!

Can someone help me please....

thank you so much",['Your code works for me. You may have too many requests in Google Sheets happening?']
"Newbie question: If I want to make a program to click buttons on certain websites, is web scraping the correct direction to look for?",https://www.reddit.com/r/webscraping/comments/ze8se9/newbie_question_if_i_want_to_make_a_program_to/,webscraping,"As per the title, should I search for web scraping? And learn tools such as beautiful soup, selenium?","['Any Web driver like selenium or headless browser\n\nPuppeteer or Playwright\n\nYou can use chrome extension and also you can write native JavaScript code and run it in the Devtools under script secrion', 'Selenium yes', 'Checkout playwright for that kind of stuff  they have a codegen feature where you can record steps and give you the code for it.\n\nhttps://playwright.dev/python/docs/codegen#running-codegen', 'Yes you can do that with web scraping (puppeteer, selenium, etc) or desktop automation (computer vision and things like pyautogui, etc).', ""Yes, web automation and web scraping are very closely related. Do you have any coding knowledge?\n\nPlaywright is becoming the best overall tool for browser automation and it's available in multiple languages (though Python or Javascript is recommended). I wrote a [lengthy hands-on introduction](https://scrapfly.io/blog/web-scraping-with-playwright-and-python/) if you're interested.\n\nOther than that, make sure you're aware that automation can quickly become more complicated than it appears. Often websites employ anti-web-scraping tech that detects such tools and blocks them. \nIf you have no prior experience and have a deadline for your project, you might want to outsource this to somebody who does or use web scraping API that supports cloud web browsers (disclosure: I work at [one that does this](https://scrapfly.io/docs/scrape-api/javascript-scenario))""]"
"Would like to sell Trademe searching data, anyone want to buy?",https://www.reddit.com/r/webscraping/comments/ze7g9v/would_like_to_sell_trademe_searching_data_anyone/,webscraping,Basically provide market report for Trademe database in real time.,[]
[Webinar] Social media and news data extraction: Here's how to do it right,https://www.reddit.com/r/webscraping/comments/zdw3c5/webinar_social_media_and_news_data_extraction/,webscraping,"**Is your data feed optimized and legally compliant?**

If you are extracting social media and news data at scale, you would already have a schema in place. But are you confident that you are not missing any important data fields?

Join James Kehoe, Product Manager at Zyte, for a webinar on developing a social media and news data schema that just works!

When: **14th December 4pm GMTFree | Online**Register here - [https://info.zyte.com/social-media-news-data-extraction-webinar](https://info.zyte.com/social-media-news-data-extraction-webinar)

What you will be able to learn:

* **Discover important data fields you should scrape**
* **Improve the coverage of your data feed using ML**
* **Understand the** **legal considerations** **of scraping social media & news data**","[""4pm GMT happens when this comment is 10 hours and 56 minutes old.\n\nYou can find the live countdown here: https://countle.com/hYJD73w--\n\n---\n\nI'm a bot, if you want to send feedback, please comment below or send a PM."", ""View in your timezone:  \n[14th December 4pm GMT][0]  \n\n[0]: https://timee.io/20221214T1600?tl=%5BWebinar%5D%20Social%20media%20and%20news%20data%20extraction%3A%20Here's%20how%20to%20do%20it%20right""]"
Excel Web Scrape Not Working Due to Error,https://www.reddit.com/r/webscraping/comments/zdl20v/excel_web_scrape_not_working_due_to_error/,webscraping,"My scraping method of choice is Excel PowerQuery, but it seems I have met my foe in a recent redesign of a site I pull information from.

[https://webapps.washingtoncountyor.gov/custody/#/?searchby=people&alpha=b](https://webapps.washingtoncountyor.gov/custody/#/?searchby=people&alpha=b)

I get this error:

Details: ""The [Web.Page](https://Web.Page) function requires Active Scripting to be enabled in Internet Explorer options. See [https://go.microsoft.com/fwlink/?LinkId=506565](https://go.microsoft.com/fwlink/?LinkId=506565) for details on how to enable Active Scripting.""

I have tried everything there is to try with enabling scripting in IE, registry, I have troubleshot it to death with no luck. Can someone please help?","[""Looks like the site is now heavily javascript based and also uses recaptcha to make sure non-humans aren't scraping it"", 'Do you know Python? Or do you have familiarity with Postman or Insomnia? If so, have you investigated whether you can query the API endpoint directly? I‚Äôm on mobile until tomorrow; I can check this website tomorrow to see if it‚Äôs suitable for endpoint scraping. It avoids messing with the JavaScript entirely, which is super convenient.\n\nInfo here on the endpoint method if you‚Äôre curious:\n\nhttps://youtu.be/DqtlR0y0suo', '/u/CaptainJackAubrey2 Yeah, I checked just now, and this site is easy to check by directly accessing the API endpoint. Run this `curl` command to see:\n\n    curl ""https://api.co.washington.or.us/v1/services/whosincustody/list/?searchby=people&alpha=d&callback=_jqjsp&_1670348031714=""    -H ""Accept: */*""    -H ""Accept-Language: en-US,en;q=0.9""    -H ""Cache-Control: no-cache""    -H ""Connection: keep-alive""    -H ""DNT: 1""    -H ""Pragma: no-cache""    -H ""Referer: https://webapps.washingtoncountyor.gov/""    -H ""Sec-Fetch-Dest: script""    -H ""Sec-Fetch-Mode: no-cors""    -H ""Sec-Fetch-Site: cross-site""    -H ""User-Agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Mobile Safari/537.36""    -H ""sec-ch-ua: \'Not?A_Brand\';v=\'8\', \'Chromium\';v=\'108\', \'Google Chrome\';v=\'108\'""    -H ""sec-ch-ua-mobile: ?1""    -H ""sec-ch-ua-platform: \'Android\'""   \n\nChange the `d` in `alpha=d` to each other letter of the alphabet (a, b, c, etc.) one at a time to retrieve all the records. Nice bonus is that this method retrieves all results for each letter in a single response (no need to deal with 2 or more pages, as with the HTML approach).\n\nHope this helps.']"
Building a Web scraper web app.,https://www.reddit.com/r/webscraping/comments/zczq6k/building_a_web_scraper_web_app/,webscraping,"What do you guys thinks about building a scraper as a Web app and sell. Do you guys think it will be profitable?

Let me know what you guys think.","['Take a look at the scrapers in Apify.com, is something like you are talking about.', ""You won't know for sure until you try, so if its a project you want to work on, go for it."", ""There is one that scraps info from LinkedIn ...but I'm not sure if it get the right emails....but it gives like best guess of the email...but yeah scraper services are very good."", 'Can you build one that can bypass a paywall?']"
Pagination when URL doesn't change,https://www.reddit.com/r/webscraping/comments/zcro8u/pagination_when_url_doesnt_change/,webscraping,I'm trying to scrape this site [https://franchisedisclosure.gov.au/Register](https://franchisedisclosure.gov.au/Register) and there's a next page button at the end. How do I loop through clicking the button without skipping the first page. I'm using playwright in python but I'm open to suggestions on other ways to scrape the site.,"['Inspect the network panel, the full data might be in the first response and the ui just shows more when clicked', 'Normally the link has a parameter in the link such as website.com/?page=3 and that sort of thing. But some websites, like Zillow, make it difficult to do that cuz you have to scroll or something', 'XHR is your friend', ""It's all there in XHR\nRegister?handler=profilespartial\nPageid\nCount\nNumberPerPage\n...."", 'The site has a few steps that you need to replicate before you can scrape it. You don\'t need to use playwright, you can just use requests. Each XHR request is signed by a token that is hidden in the HTML when you first load the page. You need to include it every subsequent request, including a post requests to accept their TOS\n\n\nAfter that you can replicate the page requests that you see in the browser like I\'ve done below. I\'ve chosen to just read the table into a pandas dataframe but if you want to you could parse all the table data by manually extracting all the info (incl the link?) \n\n    import requests\n    from bs4 import BeautifulSoup\n    import pandas as pd\n    \n    s = requests.Session()\n    \n    url = \'https://franchisedisclosure.gov.au/Register\'\n    \n    resp = s.get(url)\n    \n    soup = BeautifulSoup(resp.text,\'html.parser\')\n    token = soup.find(\'input\',{\'name\':\'__RequestVerificationToken\'})[\'value\']\n    print(token)\n    \n    tos_url = \'https://franchisedisclosure.gov.au/Register?handler=AcceptedTOU\'\n    \n    files = {\n    \t\t\'__RequestVerificationToken\':(None, token),\n    \t}\n    \n    accept = s.post(tos_url,files=files)\n    print(accept)\n    \n    output = []\n    for page in range(1,5):\n    \tdata_url = \'https://franchisedisclosure.gov.au/Register?handler=ProfilesPartial\'\n    \n    \tfiles = {\n    \t\t\'PageId\':(None, str(page)),\n    \t\t\'Count\':(None, \'50\'),\n    \t\t\'AcceptedToU\':(None, True),\n    \t\t\'NumberPerPage\':(None, \'50\'),\n    \t\t\'__RequestVerificationToken\':(None, token),\n    \t\t}\n    \n    \tinfo = s.post(data_url,files=files)\n    \tprint(info)\n    \n    \ttables = pd.read_html(info.text)\n    \n    \tdf = tables[0]\n    \ttry:\n    \t\tdf[[\'Franchisor Name\',\'ABN\']] = df[\'Franchisor Name\'].str.split("" ABN: "",expand=True)\n    \texcept ValueError:\n    \t\tdf[\'ABN\'] =\'\'\n    \n    \toutput.append(df)\n    \n    final_df = pd.concat(output)\n    final_df.to_csv(\'franchise_data.csv\',index=False)']"
"Ebay Scraping, filter out international results, select parents that do not have specific descendants",https://www.reddit.com/r/webscraping/comments/zbshbg/ebay_scraping_filter_out_international_results/,webscraping,"Hello all! I have some code that I put together to scrape ebay sold prices using BeautifulSoup and so far it is working pretty good. The only issue I currently have is that it also pulls prices for 2 categories that ebay adds to the search results page (International Sellers, and results matching fewer words)

I am struggling to filter these out. Its like I need to identify if the listing (the parent) contains a specific descendant and then filter out that parent. I hope that is clear, here is a sample:

[https://www.ebay.com/sch/57988/i.html?\_from=R40&\_nkw=Fjallraven%20nuuk%20parka&LH\_Complete=1&LH\_Sold=1&\_udlo=50&\_udhi=600&LH\_PrefLoc=1](https://www.ebay.com/sch/57988/i.html?_from=R40&_nkw=Fjallraven%20nuuk%20parka&LH_Complete=1&LH_Sold=1&_udlo=50&_udhi=600&LH_PrefLoc=1)

The picture below is an example of an item that I would like to filter out. It has a Span Class for location. This class only exists if the item is from an international seller.

https://preview.redd.it/mrf9dfbudr3a1.png?width=895&format=png&auto=webp&v=enabled&s=ca0f094757cdb0787e9d5abf6632b9095e8f1cef

    import xlwings as xw
    import bs4 as bs
    import requests
    import statistics
    
    @xw.func
    def get_prices(url,args =[]):
        url_base = requests.get(url).text
        soup = bs.BeautifulSoup(url_base,'lxml')
    
        products = []
        results = soup.find('div', {'class': 'srp-river-results clearfix'}).find_all('div', {'class': ['s-item__details clearfix'] })
        for item in results:
            price = item.find('span', class_='s-item__price').text.replace('$', '').replace(',', '')
    
            if 'to' not in price:
                price = float(price)
                products.append(price)
    
    #def calculate_averages(products):
        mean = round(statistics.mean(products), 2)
        median = round(statistics.median(products), 2)
        mode = round(statistics.mode(products), 2)
    
        return mean, median, mode",[]
Pre-trained Webscraping Models,https://www.reddit.com/r/webscraping/comments/zbjaos/pretrained_webscraping_models/,webscraping,I am looking to acquire a pretrained machine learning model in python for webscraping. I've look around online with no success. Does this exists? if not it is difficult to build on?,"[""What's your budget? And for what type of sites?\n\nWe have some, but took us 3y to get them where we need them to be, so it's not gonna be cheap."", 'What would you want to scrape ?  Some model that I am aware to extract structured data from Detail page.  \n\\- Markuplm\r  \n\\- Simsdom\r  \n\\- Freedom\r  \n\\- DOM-Extraction\r  \netc. Please search in github for code.', ""Probably not what you're looking for but you can train it on your own: https://github.com/lorey/mlscraper"", ""It's hard to guess what you're searching for exactly, but here's what I can offer:\n\nIf figuring out the selectors is an issue, I have an open source library that allows you to input a few pages and the expected output. The library then figures out which CSS selectors to apply. Done.\n\nHere's a python example:\n\n```python\nimport requests\nfrom mlscraper.html import Page\nfrom mlscraper.samples import Sample, TrainingSet\nfrom mlscraper.training import train_scraper\n\n# fetch the page to train\neinstein_url = 'http://quotes.toscrape.com/author/Albert-Einstein/'\nresp = requests.get(einstein_url)\nassert resp.status_code == 200\n\n# create a sample for Albert Einstein\ntraining_set = TrainingSet()\npage = Page(resp.content)\nsample = Sample(page, {'name': 'Albert Einstein', 'born': 'March 14, 1879'})\ntraining_set.add_sample(sample)\n\n# train the scraper with the created training set\nscraper = train_scraper(training_set)\n\n# scrape another page\nresp = requests.get('http://quotes.toscrape.com/author/J-K-Rowling')\nresult = scraper.get(Page(resp.content))\nprint(result)\n# returns {'name': 'J.K. Rowling', 'born': 'July 31, 1965'}\n```\n\nhttps://github.com/lorey/mlscraper"", ""Hey, I don't know about the difficult part - it depends on your seniority level, but [here's](https://oxylabs.io/blog/web-scraping-for-machine-learning) a blog post for a starters."", 'I think you should learn a little bit about AI models and web scraping.\n\nWe are not quite there yet where AI programs could scrape data from any website. For now, we need to write code manually to scrape data off websites.', '[deleted]']"
project ideas for beginners,https://www.reddit.com/r/webscraping/comments/zbgf04/project_ideas_for_beginners/,webscraping,"I've recently gotten into web scraping and wanted to work on some beginner level projects to up my skills. Any cool recommendations? Preferably, something that's useful would be better","[""I wrote loads of articles that can give you some ideas and hands-on guidance!\nTo start we have [scrapeguides](https://scrapfly.io/blog/tag/scrapeguide/) series of articles that cover popular web scraping targets. Though while the articles are focused on scraping itself, you can get quite creative and come up with extra layers for the scraped data.\n\nNext, [Creating Search Engine for any Website using Web Scraping](https://scrapfly.io/blog/search-engine-using-web-scraping/) is a really cool project idea as it introduces several data concepts like data collection, indexing and presentation.\n\nFinally, [How to Turn Web Scrapers into Data APIs\n](https://scrapfly.io/blog/how-to-turn-web-scrapers-into-data-apis/) is a quick introduction how to turn your web scraper into web API which one of the most useful skills you can have in this field.\n\n----\n\nOther than that, I'd recommend sticking with something you like and aim for low-complexity targets (e.g. don't go straight to scraping Google or Linkedin which are very likely to ban you) as finishing web scraping objects can be pretty difficult and time consuming - there are just so many things that can go wrong.\n\nCombining web scraping with GUI/TUI/CLI frameworks is a great exercise too. For example, check out [rich](https://github.com/Textualize/rich), [typer](https://github.com/tiangolo/typer), [textual](https://github.com/Textualize/textual/) for new hip libraries that are really fun to works with."", 'Webscrape job/internship sites and automate job suggestions (you can literally do so many things with this)\nI did half part and will make it after my exams .\nYou can also try this']"
Best tool for scraping this site,https://www.reddit.com/r/webscraping/comments/zbh7n5/best_tool_for_scraping_this_site/,webscraping,"Hi, hope everyone is doing well 

I need to scrap a site that requires login then searching for a location with a start and end dates 

then collect three text elements related to each other and be able to turn pages

I tried ParseHub which is perfect for what I need but sadly it's not working, I create the project then test it, it works fine during the test then when I try to collect the data it gives me empty results

I am thinking of using some browser extension that would allow me to skip the login and search steps but not sure what are some good extensions to use?

is there another alternative for ParseHub? some other tool that's better something opensource I can host locally especially with the need to login I don't feel ok with sharing the password with ParseHub the project is public so anyone can get the password from it 

thanks, I would appreciate any help",['Try use selenium']
"Spidergram is a collection of tools my company Autogram has built or enabled over the past several years to support our work to automate content inventories for large websites: it's part web crawler, part domain model, and part mad science. We released the first public beta today.",https://www.reddit.com/r/webscraping/comments/zb0u91/spidergram_is_a_collection_of_tools_my_company/,webscraping,"Spidergram is a toolbox for exploring, auditing, and analyzing complicated web sites ‚Äî particularly ones that use multiple CMSs, span more than one domain, and are maintained by multiple teams inside an organization. 

https://github.com/autogram-is/spidergram

We built Spidergram to overcome some of the the roadblocks we hit when using existing crawling and inventory tools on our clients' complex, multi-site web properties. Our goal is to make it easier to answer complicated questions about big, sprawling web properties.","['For the nerd-inclined, Spidergram is built on top of a tall stack of fantastic open source software. Under the hood it\'s:\n\n* Apify\'s [Crawlee](https://crawlee.dev) project, with a specific focus on [Playwright](https://playwright.dev). We decided to focus on it for now because the majority of our projects involve some kind of cross-browser evaluation for clients, and Playwright\'s ability to swap in Safari and Firefox rendering engines was a huge help.\n* [HtmlToText](https://github.com/html-to-text/node-html-to-text), a fast and flexible HTML to text library that can prep messy markup for plaintext analysis in a variety of ways.\n* [ArangoDB](http://arangodb.com), a ""multi-modal"" database engine that stores arbitrary JSON documents like MongoDB, key/value data like Redis, and graph relationships like Neo4j ‚Äî and lets you leverage all three kinds of data in a single query.\n* [Oclif](https://oclif.io) to quickly click together CLI tools for kicking off and monitoring crawls, generating reports, etc.\n* [SheetJS](https://sheetjs.com) for quickly generating complex reports in the familiar ""workbook full of spreadsheets"" style.\n\nOn top of those pieces, we\'ve added:\n\n* A custom set of domain entities representing unique URLs, individual web resources, sub-page fragments, third-party data sets, and all of the interrelationships between them.\n* A custom wrapper around Crawlee\'s PlaywrightCrawler that handles building and preserving all of those relationships during a crawl, as well plug-in points for common scenarios like ""pinging URLs to check their status without downloading the full page,"" ""downloading files that match interesting MIME types,"" and subscribable status events that make it easy to pipe crawl progress to friendly CLI displays or lightweight react apps.\n* CLI convenience tools for kicking off crawls, generating reports, and handling common inventory and analysis tasks ‚Äî like capturing multi-breakpoint screenshots of specific page elements across a set of URLs\n* A large pile of helper classes for extracting structured data from pages (either during the crawl or in a dedicated post-processing phase), pulling time-windowed Google analytics data for every found resource, building relational hierarchies based on URL and metadata patterns, and generating basic reports about the data that\'s been pulled back.\n\nIt might be a little off the beaten path for this sub, given its focus is less on ""scraping specific target data from potentially-hostile third party sites"" and more on ""assembling a giant knowledge graph to help assess your own site,"" but there\'s a pretty strong crossover in the tools and approaches we\'re using‚Ä¶ hopefully other folks find it interesting!', 'I understand very little, but it seems impressive', 'Good job!']"
"Proxy-Coin, the cheapest place where you can buy residential rotating proxies!",https://www.reddit.com/r/webscraping/comments/zbbvkc/proxycoin_the_cheapest_place_where_you_can_buy/,webscraping,"You can buy through LTC RIGHT NOW any amount of GiB and pay 0.5$ per GiB!

Our protocols are HTTP and SOCKS5!

If you are intrested join us in our discord server! [https://discord.gg/w3rgx6DNGZ](https://discord.gg/w3rgx6DNGZ)",[]
Rotating Proxy Service Recommendation,https://www.reddit.com/r/webscraping/comments/zb0l4v/rotating_proxy_service_recommendation/,webscraping,"Can anyone recommend good but not much expensive proxy service? I have this [website](https://therealreal.com) and it is constantly giving 403 which blocks my scraper every 8 request out of 10 is getting 403 and I am trying to mitigate it through better proxy plan so if anyone have any recommendation please suggest. I have tried scraperapi, packetstream, webshare those doesn't seems to be working. Thank you.","['[TheRealReal.com](https://TheRealReal.com) is protected by PerimeterX so you will need to use a proxy provider with an inbuilt bypass for it.\n\n[ScrapeOps Proxy API](https://scrapeops.io/proxy-aggregator/) has one, you just need to set `&bypass=perimeterx` on your request.', 'Use apify.com they are one of the best.', '[proxy-ipv4.com](https://proxy-ipv4.com), 4G proxies from different locations available there', ""Real value for money is [SOAX](https://new.soax.com/?utm_source=social&utm_medium=reddit&utm_content=answer), imho. I've tried it first with their demo period, liked its service and customer support. Rotation can be set up manually in the dashboard, that is also convenient. And if rotation you need is more than once in a 90 secs, support can do it for you. Recommend!"", 'You could try Proxy-Coin, this is self promo but I am just saying :D \n\nCheap prices, 50cents per GiB']"
Finding element with XPATH not working.,https://www.reddit.com/r/webscraping/comments/zatv4f/finding_element_with_xpath_not_working/,webscraping,"New to web scrapping, I‚Äôm having trouble selecting an element using XPATH. The element is present on the site but when I run my code it can‚Äôt be found. Hoping someone can provide insight. Thanks in advance.","['Dump the html to a file and look at it with a web browser with JS disabled, you might not be getting the html you are expecting.', 'This might be happening because the webdriver is trying to find the element when it is not even loaded. Try using time.sleep(15) function after the link opens, what this function basically does is that it waits 15 seconds after link opens. By this time all the elements should be loaded completely', 'Can you share the link so that we can review it?', 'There are few chrome extension that help you with this topic\n\nLike this one :[xpath-helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl)']"
I want to master Scrapy for web scraping in python,https://www.reddit.com/r/webscraping/comments/zam49d/i_want_to_master_scrapy_for_web_scraping_in_python/,webscraping,"can you give me a roadmap, or maybe some advanced youtube guide, i took datacamp scrapy guide, it was good but not advanced","['John Watson Rooney YT channel has good content about Scrapy', 'This guy has some great videos on Scrapy, binged most of his stuff last night, id start with this playlist and go from there\n\nhttps://youtube.com/playlist?list=PLj4hN6FewnwoUArmA8kifDHZYvRn6egg5', 'Check out [The Scrapy Playbook](https://scrapeops.io/python-scrapy-playbook/) lots of good tutorials there, also has a [YouTube channel](https://www.youtube.com/@scrapeops) if you prefer videos.']"
HELP - Trying to webscrape a site and cannot do it in python nor via Chrome extension,https://www.reddit.com/r/webscraping/comments/zarf66/help_trying_to_webscrape_a_site_and_cannot_do_it/,webscraping,"Hello everyone,

I am currently working on some data for a competitor study at my company, I am trying to have a list in excel for these [companies](https://www.renewableuk.com/search/newsearch.asp).

Tried a python script but cannot identify the tag to then scrape the company names.

I tried webscraper extension on chrome but no luck.

&#x200B;

Can you help?

&#x200B;

Thanks","['As others have already pointed out, the results are fetched inside an iframe on the page.\n\nYou have a few ways to deal with that:\n\n# Scrape from the browser\n\nUsing a couple of javascript lines, you can scrape the search results from your browser by selecting the `#SearchResultsFrame` and increment the page by clicking on the `.DotNetPager`.\n\n```javascript\ndocument.querySelector(\'#SearchResultsFrame\').contentWindow.document.children[0].querySelectorAll(\'.DotNetPager td a\')[PAGENUMBER - 1].click()\n```\nYou shouldn\'t encounter any problems traversing the pages in your browser by simply incrementing the `PAGENUMBER` variable.\n\n# Increment the `__EVENTTARGET`\n\nTrigger at least one request in your browser to get the necessary cookies. Then from the Network tab, Right-click on the newest request -> ""Copy Value"" -> ""Copy POST Data"".\n\nYou will find a key called `__EVENTTARGET` with a value like `SearchResultsGrid$ctl29$ctl06` where `ctl06` refer to the 6th page.\n\nIncrementing that number and should return the next page.\n\n\n\nRegardless of which way you use, the rest is parsing ordinary HTML.', ""The problem is the list is inside an iframe. I had the same issue and even created a web scraper extension ([https://madscraper.com/extension](https://madscraper.com/extension)). I'm currently working on a way to bypass iframe contents as well."", 'I sent you a pm.']"
Select correct html attribute in R with rvest.,https://www.reddit.com/r/webscraping/comments/zawrxi/select_correct_html_attribute_in_r_with_rvest/,webscraping,"I have problem with rvest.Im want to add the Author but the selector dont run well. Maybe is because this is next to `href`.

    library(tidyverse)  
    library(rvest)   
    startTime <- Sys.time()  
    get_cg <- function(pages) {     
    cat(""Scraping page"", pages, ""\n"")    
     page <-str_c(""https://cgspace.cgiar.org/discover?     scope=10568%2F106146&query=cassava&submit=&rpp=10&page="", pages) %>%    read_html()     
    
    tibble(   
    title = page %>% html_elements("".ds-artifact-item"") %>% html_element("".description-info"") %>% html_text2(), # run well    
    
    fecha = page %>% html_elements("".ds-artifact-item"") %>%   
    html_element("".date"") %>%   
    html_text2(), # run well    
    
    Type = page %>% html_elements("".ds-artifact-item"") %>%  
     html_element("".artifact-type"") %>%  
     html_text2(), # run well    
    
    Autor= page %>%    html_elements("".ds-artifact-item"") %>%    html_element("".description-info"") %>%  
     html_attr(""href""), # not download the Authors 
    
    link = page %>%   html_elements("".ds-artifact-item"") %>%   html_element("".description-info"") %>%   
    html_attr(""href"") %>% # run well   
    str_c(""https://cgspace.cgiar.org"", .))}    
    
    df <- map_dfr(1, get_cg)    
    endTime <- Sys.time()  
    print(endTime - startTim)",[]
HELP - Completely lost on simple Webscraper.io request,https://www.reddit.com/r/webscraping/comments/zasvfv/help_completely_lost_on_simple_webscraperio/,webscraping,"Hi All, I'm really blowing it right now putting together a simple webscraper for [Auction.com](https://Auction.com).   


Would anyone have availability for a quick call or zoom to help me out?  


Literally all I need from the scrape is each house with Street address, city, state, zip, and auction date all in separate columns. Pretty simple, but I'm big dumb.","[""Well, I have made a scraper for it but I didn't use any third party tool.\n\nIf I remember correctly, this is not the easiest site to scrape. They use a lot of anti bot systems to block any kind  of automated activity so don't beat up yourself if you can't do it.""]"
Better choice when Dealing with Perimeter X?,https://www.reddit.com/r/webscraping/comments/zacwr2/better_choice_when_dealing_with_perimeter_x/,webscraping,"I‚Äôm currently scraping several websites, I‚Äôve been able to solve one website with scraping bee but I cannot figure out a way to scrape this other website that is a infinite-scroll with PerimeterX. I‚Äôve read that ZenRows can do it? Are they better then scrapingbee?","['[ScrapeOps](https://scrapeops.io/proxy-aggregator/) has providers who can scrape PerimeterX protected sites.\n\nThere are also [other ways around bypassing PerimeterX](https://scrapeops.io/web-scraping-playbook/how-to-bypass-perimeterx/).', 'Hey, ScrapingBee co-founder here. We do manage to bypass perimeter-x very well, feel free to reach out to support chat if you have any issue.']"
If API available from Optimum,https://www.reddit.com/r/webscraping/comments/za1vv2/if_api_available_from_optimum/,webscraping,"Hi all, new to scraping but familiar with python. Looking to see if I can pull an API from a web flow that involves entering an address and then getting the internet plans and pricing available. I would like to avoid selenium if possible and it seems like there should be an api but this usecase is a bit more complex than the tutorials. Any hints would be greatly appreciated!

&#x200B;

[https://order.optimum.com/Buyflow/Storefront](https://order.optimum.com/Buyflow/Storefront)",[]
Python Selenium - How much network speed until it no longer is the limiting factor?,https://www.reddit.com/r/webscraping/comments/z9vskl/python_selenium_how_much_network_speed_until_it/,webscraping,"Hi, I'm using Selenium in Python for my webscrapers. These webscrapers are on a  VPS with up to 200Mbps transfer speed.

Would it be beneficial to get a 1Gbps transfer speed VPS to decrease the speed required to scrape?

To what point would increasing transfer speed be time and cost effective?

(Assuming unlimited bandwidth)","['[deleted]', 'I think your going to max out your vps a long time before you max out your network connection. \n\nYou f you think the network is the bottleneck, turn off the image rendering. That could save you most of your bandwidth.', ""I don't understand why people use Selenium to scrape websites. It is 100x more efficient to find the apis the websites are using and use those apis to scrape data.\n\nYou should be able to do it on s regular $5 droplet on digital ocean or a similar service.""]"
What would you recommend to do in order to scrape built on Wix,https://www.reddit.com/r/webscraping/comments/za4wfc/what_would_you_recommend_to_do_in_order_to_scrape/,webscraping,"I hate how Wix had me pay for my website, I just want to get the code and it myself and launch it that way, any tips or software or anything to help me accomplish this goal?","['Grab the HTML source?', ""Scraping isn't the way you'd do this. Just press CTRL+S on the site to save the HTML, then you'll have to download the JavaScript separately. It might be tough to get everything working properly.\n\nMight be easier to rebuild it in an AWS Lightsail hosted WordPress site, assuming there's no user DB etc you'd lose.""]"
Data Scraping for Updating Costs,https://www.reddit.com/r/webscraping/comments/z9qf20/data_scraping_for_updating_costs/,webscraping,"Hi!!

I have spent a good bit of time on this, and I am pretty well stumped.

I am trying to get put together a bit of a catalog of construction materials (2x6, 2x4, drywall, insulation, etc). 

I would like to be able to put this data into a spreadsheet, and have the prices update from the website (like Lowe's, Home Depot, or others) whenever the file is open without opening each individual page. This way the estimator can open the file, and have current pricing available.

I tried the get data from web option in Excel, and that just times out. I've watched a few videos, but that hasn't worked either. (User error, I'm sure.) If someone could point me in the right direction, I would appreciate it!",['Can you share the url of website you want to scrape?']
Scroll to element using python selenium,https://www.reddit.com/r/webscraping/comments/z9nlyy/scroll_to_element_using_python_selenium/,webscraping,"    element.location_once_scrolled_into_view

and ive used this too:

    driver.execute_script(""return arguments[0].scrollIntoView(true);"", element)

and everytime i run any of these lines i get anxiety because its always by chance if they work or not.

now im trying to run any of these im not getting any errors but the page doesnt scroll to the element, and the first one gives me this without scrolling anywhere:

    {'x': 0, 'y': 0}

eventho the element is not on coordinates x = 0 and y = 0

what can i do?

if i try to click the element i get:

    ElementNotInteractableException: Message: Element <button class=""btn btn-outline-light btn-icons"" type=""button""> could not be scrolled into view

&#x200B;","['Are you sure that your js script is working on this site? Also be sure that the element which you want to scroll to is ready, visible and clickable.']"
Help required for html parsing,https://www.reddit.com/r/webscraping/comments/z9legw/help_required_for_html_parsing/,webscraping,"For one of our NLP project, we have scraped data (downloaded raw data in html format) form a web portal. We are trying to parse the html and finding difficulties as the template they have used in the portal varies over the period of time and quite difficult as sometimes they don't even use proper styling for headers or etc.. We have to identify header, divions, sub division and contents. This hierarchy style is not consistent as they have used several templates to build the portal.   
Any ideas on how to tackle the situation?","['How many variations are we talking, just a few (3-10) or a lot? ( 20-100)\n\nFor the first, just make a separate ""parser"" per variation.\n\nIf its the latter, depending on what data you need, it might make sense to simply convert all the HTML to text and look for patterns that way, or just use the text ""as is""...\n\nUnsure about what you\'re doing and what the portal is though..', 'Are you looking for the differences of the multiple scraped versions? It really depends on how you want to output the differences. Something like beautifulsoup or the build in HTML paraer allows you to go through the dom hierarchy', 'There is no specific or general rule to achieve this you need to identify the pattern and write your parser against each variation.\n\nRegarding extraction of selectors this should be the hierarchy in terms of gathering the element reference. \n\n1. Check Id or explicit class\n2. Identify Attributes\n3. Xpath']"
Advice for consolidating multiple data sources,https://www.reddit.com/r/webscraping/comments/z96ywc/advice_for_consolidating_multiple_data_sources/,webscraping,"A couple things to bear in mind:
- Data and scraping is not my domain of expertise, please be kind.
- If there are better communities to ask, please point me to them.

Problem: I am trying to consolidate data from multiple data sources

- Google Analytics
- PowerBI
- Other ""generic"" web reports -- basic tables

Because the sources are diverse, I was thinking scraping would be a better ""general purpose"" approach rather than trying to integrate with various APIs. Also, APIs aren't even available for some report services.

General thought process:
Access url >> adjust date range >> scrape results >> drop results in Excel 

How would you tackle this problem? What tools would you recommend?

Thanks for any guidance.","['You should tells your programming experience and which language you are familiar with. I can do those things with javascript node-fetch + cheerio or puppeteer for more stubborn sites. But most people recommend and use python and its assorted scraping packages', 'Okay I am not an expert on the tools you have mentioned in your post but I think using the apis would be the best approach here.\n\n(1) Google Analytics provide an API\n\n(2) Power BI is used to create dashboards and you can connect it with any data source you want.\n\n(3) You need to make the nature of these generic reports more precise, a scraper is a good solution if it is the ONLY solution. \n\nScrapers are generally very fragile because they depends upon undocumented parts of website, you will need to hire someone to maintain these scrapers for you. On the other hand,  APIs don\'t change that often so you can reliably use them in your tool without someone having to ""maintain"" it. \n\n Apis also exlose information which may not be accessible on website.\n\nSo overall I would suggest using apos whereas you can and only write scrapers if you don\'t have another option.']"
Having Trouble Scrapping Table from Govt Website,https://www.reddit.com/r/webscraping/comments/z976ru/having_trouble_scrapping_table_from_govt_website/,webscraping,"I am trying to practice web scrapping tables with  BeautifulSoup but no data is returned from the following website! I am doing something wrong?

[https://rms.arc.gov.au/RMS/Report/Download/Report/1b0c8b2e-7bb0-4f2d-8f52-ad207cfbb41d/243](https://rms.arc.gov.au/RMS/Report/Download/Report/1b0c8b2e-7bb0-4f2d-8f52-ad207cfbb41d/243)","['Hard to imagine without seeing any of your code.', ""If it's a dynamic website there's limitation to what you can do with Beautiful so you may need to use selenium, you can Dm with your code"", ""This site does an API call to populate this table. Open up the inspector and look at the network tab when you refresh the page.\n\nYou can then use something like scrapy to do the API call yourself and then parse the json data it gives you. This actually will return you fields that they aren't even showing in the table, but might be useful to your purposes: \n\n\\['StrategicResearchPriorities', 'ScienceAndResearchPriorities', 'IndustrialTransformationPriorities', 'FieldsOfResearch', 'Title', 'AdminOrganisationStateName', 'AdminOrganisation', 'ProjectCode', 'ChiefInvestigators', 'OrganisationParticipantSummary', 'Summary', 'AnnouncedDate', 'IntelligenceSecurityChallenges', 'AllocatedFunding', 'AllocatedFundingCalendarYears', 'AllocatedFundingYears', 'UnnamedAwardSummary'\\]""]"
How to scrape an infinite scrolling website that doesn't call next page in XHR?,https://www.reddit.com/r/webscraping/comments/z8m9x6/how_to_scrape_an_infinite_scrolling_website_that/,webscraping,"I'm trying to scrape this site: [https://www.releases.com/l/TV\_Series\_Seasons/2022/11/uk](https://www.releases.com/l/TV_Series_Seasons/2022/11/uk)I've looked in the XHR/fetch but cannot find how the next page is called, what do I do?

Edit: {""endpoints"":\[{""url"":""https:\\/\\/a.nel.cloudflare.com\\/report\\/v3?s=F4ZwETHbjsDx58GsUatcFTL+xuTi5ds40J/GvyQdoR1lB9dyQqgr91RfnWS5Pix5a8g1Wc54YcXglpcNZ8xeVlu5lZOkePAr0rXd0zOZwDu+W1orupx5/a+6n5nTKgfA1JPk""}\],""group"":""cf-nel"",""max\_age"":604800}

I'm guessing it's the end of the road - being cloudflare?","[""It's definitely using XHR requests for loading pages: https://i.snipboard.io/DPQaYf.jpg\n\nWhich leads to this link: https://www.releases.com/calendar/nextAfter?blockIndex=114&itemIndex=23&category=TV_Series_Seasons&regionId=uk\n\nMake sure to use `disable cache` option in your browser's devtools to avoid missing cached requests.""]"
Chrome Webscraper Extension: How can I change this dropdown and load the rest of the data before scraping?,https://www.reddit.com/r/webscraping/comments/z8g0yx/chrome_webscraper_extension_how_can_i_change_this/,webscraping,"I'm trying to change a dropdown selection from """"Top 100 by Market Cap"" to ""All Coins"" on this page:

[https://coincodex.com/historical-data/crypto/?date=2022-01-01T18:00:00Z](https://coincodex.com/historical-data/crypto/?date=2022-01-01T18:00:00Z)

How do I get the webscraper extension to cooperate with me for this? I'm struggling a bit and appreciate any help.","['If anyone sees this and can help, I even got it to switch to the new page, but now the scraper completes before any of the new data is loaded.\n\nI would really appreciate some help from an old hand at this.']"
Google Player scraper only returns first 30 results,https://www.reddit.com/r/webscraping/comments/z8idmt/google_player_scraper_only_returns_first_30/,webscraping,"Ok so I‚Äôm trying to make progress on an academic project and using Google play to scrape apps with the keyword ‚Äúflashlight‚Äù but it only returns the first 30. Does anybody have any advice to circumvent this issue?

If so feel free to dm me! Thanks so much.","['Google doesn\'t give you any other way of seeing more than 30 apps when you search, this is the search url: https://play.google.com/store/search?q=flashlight&c=apps&hl=en&gl=us\n\nThe only way I\'ve been able to get different results is to alter the language (in the url: ""hl=en"") and the country codes (""gl=us""), so maybe a method for you to get lots of results is to loop through all the language and country codes and just store the unique results.\n\nI suspect you are wanting to highlight how many flashlight apps have over-reaching permission requirements or something privacy related?']"
I tried webscraping this website with Selenium and BeautifulSoup but I‚Äôve had no luck. Can anyone tell me what I‚Äôm doing wrong?,https://www.reddit.com/r/webscraping/comments/z7vf58/i_tried_webscraping_this_website_with_selenium/,webscraping,"This is the website:

https://allegro.pl

Whenever I try to scrape it just refreshes and nothing happens. What tools can I use to determine what tools are needed to scrape it? Do I just try whatever application and see what sticks?","['The site uses has anti scraping measures like captchas in place to stop you.\n\n[Undetectable Chrome Driver](https://github.com/ultrafunkamsterdam/undetected-chromedriver) seems to work though #pip install undetected-chromedriver     \n\nuse that in place of selenium', 'i\\`ve scrapped allegro before. what do you want to scrape exacly?\n\npozdro :P', 'I used to get stock data from different websites, it wasn\'t ""scraping"" by any measure, but, I think they used Selenium to make a request to different stock websites, some websites returned, error 400, error 500, they can see if it is an automated response, you might want to us PyAutoGUI in Python, it just automates keystrokes, so, if it is not too complicated, just Ctrl+C and Ctrl+V data in a text file']"
Tips to scrape news website with multiple sections,https://www.reddit.com/r/webscraping/comments/z7ucky/tips_to_scrape_news_website_with_multiple_sections/,webscraping,"I am new to webscraping and learning playwright,  and I am scrapping a news website. The site has different sections like latest, local, world etc..

Any tips or resources for scraping all sections in one go.","['Look for an RSS feed as a first step, then check the robots.txt for a known structure or sitemap']"
how to know if a website is scrapable or not?,https://www.reddit.com/r/webscraping/comments/z7xk0a/how_to_know_if_a_website_is_scrapable_or_not/,webscraping,"I was trying for hours to scrape a website 

this one : [https://www.ouedkniss.com/automobiles-citadine/1](https://www.ouedkniss.com/automobiles-citadine/1)

I'm using scrapy but i couldn't scrape anything, is it possible to know if a website is scrapable or not and what's up with this site?","['There\'s a graphql query happening in the background which you can see in the Network tab of your Developer Tools when you load the page, I\'ve replicated the query in python below, you can loop over the page numbers parse the json as you want:\n\n\n    import requests\n    import json\n    \n    url = \'https://api.ouedkniss.com/graphql\'\n    \n    for page in range(1,10):\n    \n        payload =  {\n            ""operationName"":""SearchQueryWithoutFilters"",\n            ""variables"":{\n                ""mediaSize"":""MEDIUM"",\n                ""q"":None,\n                ""filter"":{\n                    ""categorySlug"":""automobiles"",\n                    ""origin"":None,\n                    ""connected"":False,\n                    ""delivery"":None,\n                    ""regionIds"":[],\n                    ""cityIds"":[],\n                    ""priceRange"":[None,None],\n                    ""exchange"":False,\n                    ""hasPictures"":False,\n                    ""hasPrice"":False,\n                    ""priceUnit"":None,\n                    ""fields"":[],\n                    ""page"":page,\n                    ""count"":60\n                    }\n                },\n                ""query"":""query SearchQueryWithoutFilters($q: String, $filter: SearchFilterInput, $mediaSize: MediaSize = MEDIUM) {\\n  search(q: $q, filter: $filter) {\\n    announcements {\\n      data {\\n        ...AnnouncementContent\\n        smallDescription {\\n          valueText\\n          __typename\\n        }\\n        noAdsense\\n        __typename\\n      }\\n      paginatorInfo {\\n        lastPage\\n        hasMorePages\\n        __typename\\n      }\\n      __typename\\n    }\\n    active {\\n      category {\\n        id\\n        name\\n        delivery\\n        slug\\n        __typename\\n      }\\n      count\\n      __typename\\n    }\\n    suggested {\\n      category {\\n        id\\n        name\\n        slug\\n        __typename\\n      }\\n      count\\n      __typename\\n    }\\n    __typename\\n  }\\n}\\n\\nfragment AnnouncementContent on Announcement {\\n  id\\n  title\\n  slug\\n  createdAt: refreshedAt\\n  isFromStore\\n  isCommentEnabled\\n  userReaction {\\n    isBookmarked\\n    isLiked\\n    __typename\\n  }\\n  hasDelivery\\n  deliveryType\\n  likeCount\\n  description\\n  status\\n  cities {\\n    id\\n    name\\n    slug\\n    region {\\n      id\\n      name\\n      slug\\n      __typename\\n    }\\n    __typename\\n  }\\n  store {\\n    id\\n    name\\n    slug\\n    imageUrl\\n    __typename\\n  }\\n  user {\\n    id\\n    __typename\\n  }\\n  defaultMedia(size: $mediaSize) {\\n    mediaUrl\\n    __typename\\n  }\\n  price\\n  pricePreview\\n  priceUnit\\n  oldPrice\\n  priceType\\n  exchangeType\\n  __typename\\n}\\n""\n            }\n    \n        headers = {\n            \'content-type\':\'application/json\'\n            }\n    \n        resp = requests.post(url,data=json.dumps(payload),headers=headers).json()\n        print(page,len(resp[\'data\'][\'search\'][\'announcements\'][\'data\']),\'results\')\n        #parse data as you want here', ""I can't look at a site currently, but in general if you can view it in the browser you can scrape it. Little to no exception.\n\nMaintaining a scraper for an aggressive site can be much work though""]"
What is the best coding language to do the following:,https://www.reddit.com/r/webscraping/comments/z7qjcb/what_is_the_best_coding_language_to_do_the/,webscraping,"Hello, I want to build an app, or a chrome extension, or a script that will help me automate browser tasks on a classifieds website, the tasks I want to automate are:  
\- Login to my account  
\- fetch a list of specific ads and store them on a database  
\- watch for newly added ads and notify me when a specific ad is published  
\- ability to submit a contact request to specific ad owners  
What technology is best to perform this kind of operations knowing that I will do a lot of DOM manipulation and XHR requests. I usually work with Node to build scrappers. I wonder if I should switch to Python? or if there's any other coding language I should consider like Python. You can suggest a node framework. I heard about phantomjs but I never tried before.Looking forward to your suggestions","['I think Python is your best bet for these situations', ""Javascript, I assume that's what extensions are written in but I could be wrong."", 'if you need to automate browser, checkout Selenium. You can use it with many languages and it\\`s preety easy to learn. It also has a browser extension called Selenium IDE and it allows you to record your actions and play them without any code (or very small)\n\nSelenium official site\n\n[https://www.selenium.dev/](https://www.selenium.dev/)\n\nSelenium IDE\n\nhttps://chrome.google.com/webstore/detail/selenium-ide/']"
Scrapping restaurant addresses in the US,https://www.reddit.com/r/webscraping/comments/z7umyt/scrapping_restaurant_addresses_in_the_us/,webscraping,"Hello,   
I am looking to collect the address of restaurant chains in the US.   
Some big names like Burger King and some smaller ones like California Pizza kitchen.   


When I visit the website of these companies, I am usually confronted with a map where I can only see the restaurants close to my location. [https://www.cpk.com/locations](https://www.cpk.com/locations)  


Is there a way to scrape the the list of all their locations?   
tnx","['What about retrieving the data from OpenStreetMaps? There is a plenty of free endpoints for OSM and the Overpass query language is fairly easy to learn. Alternatively I can freelance it for you, dm if interested :)', ""Each chain will have a unique website design and will need a unique scraping strategy, CPK for example has all it's locations in a json endpoint here: https://api.cpk.com/api/v1.0/restaurants/cpk-stores which can be parsed pretty easily. I found this by looking at the network requests in the Developer Tools of my browser when loading that page.\n\n\nHowever Burger King doesn't let you access all the stores in one go from what I can tell, they are drip fed as you enter a location in the search bar, so you would need to build more complex script that looks legitimate and searches through locations and stores the responses. \n\n\nI imagine that the other chains will be similar, the bigger the chain the more complex the scrape is likely to be.""]"
Website blocking me from one computer but not another one. I can't figure the difference.,https://www.reddit.com/r/webscraping/comments/z7d2u1/website_blocking_me_from_one_computer_but_not/,webscraping,"**TL;DR: Why does a site block one machine but not the other? They are both on the same LAN (same IP address) and are sending identical headers.**

I have a Python script that runs once per day to grab price data from a website. The script visits 5 different pages on the website with a 10 second gap between visits. It doesn't do anything crazy like sending it a million requests per second. Example page that triggers a 403 error: [https://www.apmex.com/product/1/1-oz-american-gold-eagle-coin-bu-random-year](https://www.apmex.com/product/1/1-oz-american-gold-eagle-coin-bu-random-year)

In the past couple weeks, I have been getting 403 Connection Refused errors. When I went to troubleshoot it, I found that the script still worked fine when I ran it from a different computer. We are all on the same LAN, so the target website would see the same IP address from both computers.

I went to [this site](https://www.whatismybrowser.com/detect/what-http-headers-is-my-browser-sending) with a python script to check the get request headers. Here are the headers it reported:

&#x200B;

||Working Computer|Not Working computer|
|:-|:-|:-|
|ACCEPT|`*/*`|same|
|**ACCEPT-ENCODING**|gzip, deflate, br|gzip, deflate (no 'br')|
|**CONNECTION**|keep-alive|same|
|**CONTENT-LENGTH**|||
|**CONTENT-TYPE**|||
|**USER-AGENT**|python-requests/2.28.1|python-requests/2.25.1|

So I updated the python requests module and modified the headers but I am still getting blocked. Any ideas how to fix?

## Here is the code:

    >>> import requests
    >>> from requests.adapters import HTTPAdapter, Retry
    >>> url = 'https://www.apmex.com/product/1/1-oz-american-gold-eagle-coin-bu-random-year'
    >>> s = requests.Session()
    >>> s.headers.update({'Accept-Encoding': 'gzip, deflate, br'})
    >>> retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504, 429])
    >>> s.mount('http://', HTTPAdapter(max_retries=(retries)))
    >>> # Here are the headers that I will send:
    >>> s.headers
    {'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'}
    >>> res = s.get(url)
    >>> res.status_code
    403","[""There could be a few causes though this seems like TLS fingerprinting. Your different machines might have a different version of openSSL (or whatever SSL library their using). Some TLS fingerprints are very common and straight up banned. Try updating your system's SSL packages. For more see [How TLS Fingerprint is Used to Block Web Scrapers?](https://scrapfly.io/blog/how-to-avoid-web-scraping-blocking-tls/)\n\nYou should probably not use the default user agent string (pick some windows browser like `Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36`) \n\nAlso worth noting that the header order is not natural. All browsers order headers in a specific order and if your scraper doesn't match that it's super easy to detect. For more see [Header Order section of this article](https://scrapfly.io/blog/how-to-avoid-web-scraping-blocking-headers/#header-order)"", '[deleted]']"
"Best way to get data from websites like leetCode, hacker rank etc . I mostly need their list of questions and sone other stuff .",https://www.reddit.com/r/webscraping/comments/z6z7a4/best_way_to_get_data_from_websites_like_leetcode/,webscraping,"I am creating a script/ app that will give me random leet code questions to solve . Currently it is personal project but some of friends are also interested in using so how can I do so ? 
I don‚Äôt don‚Äôt know much about web scraping so any help is appreciated .","['Have you tried Google/youtu.be first?', 'google.com  then we ask', 'grinder xd', 'get some script going that sends http requests to leetCode']"
Web Archive - HTML Code dump of websites,https://www.reddit.com/r/webscraping/comments/z6vwmv/web_archive_html_code_dump_of_websites/,webscraping,"I have seen a website similar to web archive that used to crawl and index HTML code of webpages of every website. I forgot the website. Does anyone know anything similar?

For example, we could search for some term like ""<h1>twitter</h1>"" and it used to return all webpages that has a twitter inside head tag in their html code.

Thanks in advance",[]
Help with Parsehub?,https://www.reddit.com/r/webscraping/comments/z6vuob/help_with_parsehub/,webscraping,"I'm using Parsehub to scrape a website, however I cannot get it to click through every link on a list on links. It just clicks and scrapes the first one, then ignores the others.

Does anyone know why this might be happening? Desperate for some help.",[]
Can you help me understand/reverse engineer this API?,https://www.reddit.com/r/webscraping/comments/z6tfok/can_you_help_me_understandreverse_engineer_this/,webscraping,"I have this store locator widget I need to scrape for store locations. 

[https://smallbatchpets.com/where-to-buy](https://smallbatchpets.com/where-to-buy)

Whenever I filter, or do any action, on the widget the Fetch/XHR tab simple shows a ""writeevent"" POST request that returns this JSON

    {""statusCode"":201,""body"":""{\""response\"":\""OK\""}"",""headers"":{""Access-Control-Allow-Origin"":""*""}}

but the widget shows the new locations and is reflected in the inspect elements tab.

Can someone help me understand what is going on here?

How would you access the store locations from this?

Thank you!","['The data is actually already loaded in the background and the widget just updates the map. In your browser\'s Developer Tools if you press Ctrl+Shift+F and search for one of the stores you\'ll see it pops up in a javascript file. This is loaded by the main page when you first go to that URL you linked. \n\nSo the job becomes:\n\n1. load the page\n2. find the script that loads the sites\n3. get the response text from that script url\n4. load the json in that script into a dataframe\n5. send to csv\n\nHere is a script that does all that, you\'ll need to install python and ""pip install requests bs4 pandas"" \n\n    import requests\n    from bs4 import BeautifulSoup\n    import json\n    import pandas as pd\n    \n    url = \'https://smallbatchpets.com/where-to-buy\'\n    \n    resp = requests.get(url)\n    print(resp)\n    \n    soup = BeautifulSoup(resp.text,\'html.parser\')\n    script_part = soup.find(\'script\',{\'id\':""storelocatorscript""})[\'data-uid\']\n    \n    script_url = f\'https://cdn.storelocatorwidgets.com/json/{script_part}\'\n    \n    data = requests.get(script_url)\n    print(resp)\n    dirty = data.text.replace(\'slw(\',\'\')[:-1]\n    \n    clean = json.loads(dirty)\n    \n    df = pd.json_normalize(clean[\'stores\'])\n    df.to_csv(\'pet_stores.csv\',index=False)\n    print(\'data saved to pet_stores.csv\')']"
"Rotating Proxy Service for Many, Many Requests",https://www.reddit.com/r/webscraping/comments/z64hbv/rotating_proxy_service_for_many_many_requests/,webscraping,"I'm building my first scraping project, and I'm almost finished, but I still need to decide how I'm going to rotate proxies. The main issue that I'm running in to is that I am going to be sending up to 6000 requests daily at intervals of 5 seconds (I'm logging basketball scores and their respective live odds), and most proxy rotating services don't offer an affordable plan that would allow for this. Does anyone know of a good solution?","[""Are you currently being rate limited? On request every 5 seconds, in my experience, usually isn't enough to get blocked. If you are not getting blocked you may be good to run without a proxy. Another alternative for lower traffic, you can use an AWS API gateway as a free proxy for up to 1mil requests per month."", '!remindme 10 days', ""It really depends on the website you're scraping. Could you give us more info?\n\n6000 requests/daily isn't much at all unless the pages are very heavy. Are you using an HTTP client library or browser automation (Selenium, Playwright etc.)?\n\nThe first thing to note is that the reason you're being detected might not be proxies at all. Many betting websites use strong anti-scraping tech that analyses IP addresses, connection headers, HTTP protocol levels, TLS fingerprints etc. If you want to learn more about how it's done see a series of [introduction articles I wrote here](https://scrapfly.io/blog/how-to-scrape-without-getting-blocked-tutorial/)\n\nIf you're only interested in getting the job done, then I'd recommend skipping all of this magic and using a web scraping API that manages the connection for you. I work at [scrapfly.io](https://scrapfly.io/) and the cheapest plan should easily handle your use case :)"", 'Here is a [comparison tool](https://scrapeops.io/proxy-providers/comparison/) for comparing the plans different proxy providers offer.', 'I made a package that finds you free tested proxies fast\nYou can use it to build your custom rotator\n\nhttps://github.com/idandaniel/ballyregan']"
Webscraping instagram page with apify not working,https://www.reddit.com/r/webscraping/comments/z5wvc1/webscraping_instagram_page_with_apify_not_working/,webscraping,"Ive been using the apify instagram page/ post scraper to scrape info on the 30 most recent posts for public accounts and it‚Äôs always worked seamlessly. Unfortunately there is one account that just doesn‚Äôt work with the apify actor. I‚Äôve tried inserting the link in different ways tried a different actor waited a day and it just can‚Äôt webscrape that specific page. Do you guys know if there is anything i can do to get the data or if some instagram pages are protected and if there is a way to bypass that? Any help would be greatly appreciated and you would be saving me a ton of time for my project!

Here is the link to the profile i am trying to scrape: https://instagram.com/ginengine?igshid=YmMyMTA2M2Y=","['If i click the link it says page doesnt exist, so probably the profile was deleted or you mistyped the link', 'Strange that this link doesn‚Äôt work! I tried other formats though and was 100% they worked!!']"
Is there a way to bypass recaptcha while scrapping with selinum,https://www.reddit.com/r/webscraping/comments/z5iasm/is_there_a_way_to_bypass_recaptcha_while/,webscraping,I am trying to scrap a website with selinum but i am getting stuck because of a recaptcha i found an api that bypasses it but it charges per usage,"[""Have you looked into Selenium alternatives? Selenium is dead easy to detect as it leaks its identity to the browser's javascript namespace. For more see [How Javascript is Used to Block Web Scrapers?](https://scrapfly.io/blog/how-to-avoid-web-scraping-blocking-javascript/)\n\nA modern alternative to Selenium is [Playwright](https://scrapfly.io/blog/web-scraping-with-playwright-and-python/) - maybe you'd have more luck with it? [Puppeteer](https://scrapfly.io/blog/web-scraping-with-puppeteer-and-nodejs/) is another alternative with a pretty big community for avoiding captchas and blocking though it's only available in NodeJS."", 'Machine learning, or you could just have your bot send you a text every time you have to fill out a captcha and do it from your phone remotely it‚Äôs inconvenient but nothing is perfect other than that scroll through GitHub and look what code people are offer for captcha evasion I‚Äôm sure your not the only with this issue someone has the solution', 'Pay or do it manually. As much as it sucks, it‚Äôs the only  real way unless the captcha is being triggered because of something you‚Äôre doing. If it‚Äôs always on the page, then those are the 2 options. If you are going for a large scalable system, it‚Äôs just part of the cost.']"
How to scrape whole webpage from a website that unloads data once no on-screen with Octoparse?,https://www.reddit.com/r/webscraping/comments/z5gczk/how_to_scrape_whole_webpage_from_a_website_that/,webscraping,"I'm having particular trouble with [this](https://voila.ca/products?sortBy=favorite&sublocationId=43a936d1-df1d-4bf1-a09c-b23c6a8edf63) website. I'm semi-used to Octoparse and usually scrolling websites aren't an issue but what I've found with this particular website is that the data un-loads once not on the display screen. As a result, if I tell Octoparse to scroll down the entire page and then scrape the data it only scrapes the last 8 or so on the bottom of the page (the Xpath points to the last 8 or so entries and can't figure out how to change this). I tried another approach where I scroll, then extract, then scroll, then extract over and over again and manually did an extract path (vs auto-detect), however, it pulls the same data for every extract. It does not switch to a new entry. I've tried various loops inside of scrolls and scrolls inside of loops but can't figure it out. Any help is appreciated!","[""that's why you shouldn't rely on someone else tool"", 'Had this issue scraping emails from Outlook. I developed an extension to monitor any changes and save it [https://madscraper.com/extension](https://madscraper.com/extension). Afterwards, I just wrote a simple js code to auto-scroll the page.']"
eBay account image scraping,https://www.reddit.com/r/webscraping/comments/z5glnq/ebay_account_image_scraping/,webscraping,"Hi! I recently found an eBay account that has over 37,000 listings for various pages from extremely rare magazines that haven't been archived online. Most of these are the only scans available online for these magazines, even though some pages are missing. What would be the most efficient way to download every listing image from the account?

[https://www.ebay.com/str/mantiquesandbaseball](https://www.ebay.com/str/mantiquesandbaseball)","['I can scrape all these images', 'making costume script to do the job\ni can do it if you need  go through each listing and save the images']"
I created a the best local web scraping extension in 2 weeks,https://www.reddit.com/r/webscraping/comments/z532ii/i_created_a_the_best_local_web_scraping_extension/,webscraping,"Hi guys! I created a the best local web scraping extension in 2 weeks. [https://madscraper.com/extension](https://madscraper.com/extension)

What makes this different when many extensions exist?

1. Some websites are really stubborn to scrape from. We know because we tried 10 of the most popular scraping extensions for our own business. We rebuilt selectors from scratch to cover 99% of all use cases, no kidding.
2. Selects the data you really want. We give you variations of selections to choose from so you get exactly the rows you want.
3. Keeps your data on the cloud. Most web scrapers require you to download your data right away or pay to save your data online. Whatever you scrape is automatically saved so you can access it any day, anytime. Download formats in JSON & CSV.
4. Pagination Hell. Paginating simple table structures with next hyperlinks is easy. What is difficult for others is handling javascript buttons, single page apps and tricky actions. Some extensions only allow pagination on the cloud and charge for it.
5. Primary Keys. To prevent duplicate data, you can set a column as a primary key so that you only get fresh and updated data every time
6. Built for your team. You and others can scrape many websites, from different browsers & devices with a single source of truth.
7. Select once, run anytime. After you create a selection once, you or your team will never re-create it. Anytime you visit the same page, Mad Scraper checks for new data and updates.
8. It's SUPER simple to use. We tried 10 popular web scrapers and made this so much easier to use than all of them. A 1 minute tutorial is all you'll ever need. [https://madscraper.com/tutorial](https://madscraper.com/tutorial)
9. Built for developers. Web-hooks for fresh data and a JSON API link for your applications.
10. We'll listen to you. If you have any suggestion and it's a killer one, we'll work on it in a week or less!

IN OUR DECEMBER TIMELINE

* Google Sheets Integration
* AirTable Integration
* Deep Scraping
* and if we have the time, a landing page 

Give it a try and you'll love it. Get it at [https://madscraper.com/extension](https://madscraper.com/extension). In case you're wondering, IT""S FREE. All feedback & suggestions are welcome.","[""You've written a scraper extension, but not managed to launch a website or provide any details about who you are or what you do with our data.\n\nSo it's a no from me."", 'Good job!', 'Can you elaborate a bit on what you mean by deep scraping in your December roadmap?']"
Does anyone know what's going on here?,https://www.reddit.com/r/webscraping/comments/z4vzwx/does_anyone_know_whats_going_on_here/,webscraping,"I'm trying to scrape a website by listening in on one of it's websocket connections.  I've been able to start collecting these ws messages using selenium, but have no idea how to parse the payloads.  They look like this:

&#x200B;

eJyt09sKAiEQBuB3mdvdmLU9ZL1KdGEy0IKgOBZF9O5ZsNCCS1heOjof8o/u72Ad7MCTM0oT1OBUOMUCHu2VtfWEceNGnrFBDiqMHEb9WsRjhgIxtu86o1jH7osyZ4rtlYRH/b/dTXbzYYuy9Ozaogzep/BVW8QeknZfxN4k7bxZipxMqqGIPWUif34m3+R5Il0RWyYTyZvkkr1d+pWHJ8rvT88=\\

&#x200B;

If this encoding format looks familiar to anyone, please let me know what it is and what I can do to parse it!

Thanks!","['base64 -> [zlib-inflate](https://en.wikipedia.org/wiki/Zlib) -> json\n\n\n```json\n[\n  {\n    ""op"": ""replace"",\n    ""path"": ""/boxscore/players/0/statistics/0/athletes/3/stats/12"",\n    ""value"": ""+8""\n  },\n  {\n    ""op"": ""replace"",\n    ""path"": ""/boxscore/players/0/statistics/0/athletes/4/stats/10"",\n    ""value"": ""1""\n  },\n  {\n    ""op"": ""replace"",\n    ""path"": ""/boxscore/players/0/statistics/0/athletes/4/stats/12"",\n    ""value"": ""+11""\n  },\n  {\n    ""op"": ""replace"",\n    ""path"": ""/boxscore/players/0/statistics/0/athletes/5/stats/12"",\n    ""value"": ""-3""\n  },\n  {\n    ""op"": ""replace"",\n    ""path"": ""/boxscore/players/0/statistics/0/athletes/6/stats/12"",\n    ""value"": ""-5""\n  },\n  {\n    ""op"": ""replace"",\n    ""path"": ""/boxscore/players/0/statistics/0/athletes/7/stats/12"",\n    ""value"": ""-8""\n  },\n  {\n    ""op"": ""replace"",\n    ""path"": ""/boxscore/players/1/statistics/0/athletes/5/stats/12"",\n    ""value"": ""+6""\n  },\n  {\n    ""op"": ""replace"",\n    ""path"": ""/boxscore/players/1/statistics/0/athletes/6/stats/8"",\n    ""value"": ""1""\n  },\n  {\n    ""op"": ""replace"",\n    ""path"": ""/boxscore/players/1/statistics/0/athletes/6/stats/12"",\n    ""value"": ""-4""\n  },\n  {\n    ""op"": ""replace"",\n    ""path"": ""/boxscore/players/1/statistics/0/athletes/8/stats/12"",\n    ""value"": ""+5""\n  },\n  {\n    ""op"": ""replace"",\n    ""path"": ""/boxscore/players/1/statistics/0/athletes/9/stats/12"",\n    ""value"": ""+8""\n  }\n]\n```\n\n\nref: [CyberChef](https://gchq.github.io/CyberChef/)', 'base64\ntry using cyberchef in magicmode', ""It's a base64 encoded binary string.  No idea what the binary format is.""]"
I made a fast proxy fetcher package in python,https://www.reddit.com/r/webscraping/comments/z4kllk/i_made_a_fast_proxy_fetcher_package_in_python/,webscraping,"I got sick if getting blocked by websites while web scrapping, so I made a python package & CLI that finds free tested proxies for you REALLY FAST.

That package named Ballyregan, and it's key features are:
‚Ä¢ Proxy fetching - get valid proxies fast. You can also filter while fetching by protocol or anonymity.
‚Ä¢ Proxy validation - you can use the ProxyValidator by itself to validate proxies.
‚Ä¢ Proxy filtering - you can filter your own list of proxies by anonymity and protocol.

Check it out:
https://github.com/idandaniel/ballyregan

This is my first package and I will really appreciate your opinion on it.

Hope it helped you at least as much as it helped me!",[]
Is it possible to scrape for websites that are built with Wix?,https://www.reddit.com/r/webscraping/comments/z4oy2i/is_it_possible_to_scrape_for_websites_that_are/,webscraping,"Specifically, would it be possible to scrape for websites using Wix in US and Canada?","['Yes.', 'Everything is possible']"
ticket data scraping project,https://www.reddit.com/r/webscraping/comments/z4fr30/ticket_data_scraping_project/,webscraping,"Hello guys, 

My company needs to collect data about ticket pricing in the US. We want to collect data from the top 7 websites, including Ticketmaster.  Data collection frequency should be two times per week.  I need to learn how to manage this situation; we can either hire a developer or search for a platform or company that offers this service.  The main goal is regular high-quality data delivery on our AWS bucket. What would you suggest for the most cost-effective solution?","[""I don't know of any platform which provides this service but I can develop custom scrapers to fetch this data on a regular basis and host them for you. \n\nYou can message me on reddit of you want to build custom scrapers."", ""I'm up for the jobs then. Got proxies, servers, codes..."", 'My solution would be develop scrapers for every site and run them on demand using Apify so your backend consumes the data easily. By the way I work everyday with your use case.', ""Interested! I have a team of developers and the required skill set for the job. We've built in-house scrapers like google and Instagram scrapers. I'm finding this project to be fascinating. Let's connect and do something interesting. Thanks"", ""What's your budget?\nThe most effective solution will be to buy the data (extracted data)"", 'Apify has ticketmaster scraper - https://apify.com/lhotanok/ticketmaster-scraper', 'Im suggest you make a scraper script for each page and program it for run this each week.  \nDepend the site you could use `rvest` or `Rselenium` packages.', 'Previously, i have scrapped lots of tickets data from usa, i know the great solution for cost-effective,please dm me so I can deliver more data quality', ""I'd suggest to try a web scraping API, for example [https://scrapingfish.com](https://scrapingfish.com).""]"
Need help extracting emails....,https://www.reddit.com/r/webscraping/comments/z4h8xp/need_help_extracting_emails/,webscraping,"So:

I have a task to gather all the contact emails from multiple people from a website.

The website is about movies and there are about 30 movies on a page.

To find the contact div, i need to firstly click on the movie img and then click on the contact div where i find the data i want: The contact email of the producer.

Now, there are about 300 pages. So almost a total of almost 9000 movies.

Is there any way I could extract the emails using some kind of web scraping method or shall I accept my misery and spend two months manually writing them? Thank you in advance and sorry!","['use email regex.', 'There will be an easier way, what is the site?', ""What's the website?"", 'Can you send me the link to the website?']"
How do you scrape lazy load sites,https://www.reddit.com/r/webscraping/comments/z4do13/how_do_you_scrape_lazy_load_sites/,webscraping,"So I'm trying to scrape this site [https://www.bergstromlexus.com/](https://www.bergstromlexus.com/) and it's rendered in javascript and a lazy load site. Usually when I come across these I do one of the following:

\-Send API requests (I'm trying to get product data from different categories so I can't do this)

\-Use requests-html (It isn't rendering the contents of the page where the cars are listed in the categories)

\-Use playwright (It isn't returning all the data and usually for these types of sites I have encountered there's something I can click on to make the script scroll down the page and load the data I need but there's none on this site)

Is there another method I could use or something I could change in the methods I tried already that'll help me scrape the data. Thanks","['Have you tried with selenium?', ""There's a bunch of data in this json endpoint: https://www.bergstromlexus.com/apis/widget/INVENTORY_LISTING_DEFAULT_AUTO_NEW:inventory-data-bus1/getInventory \n\nAssume this isn't the data you want? What exactly are you trying to scrape?""]"
how to change browser.download.dir as much as i want in my script without reinitializing the driver,https://www.reddit.com/r/webscraping/comments/z49fwp/how_to_change_browserdownloaddir_as_much_as_i/,webscraping,"    options = Options()
options.set_preference(""browser.download.dir"", 'downloads')

driver = webdriver.Firefox(service=service,
                                options=options)

so after i initialize the driver this driver downloads will go into downloads variable which has the path i specified for my downloads.

what if i want to change this download var later on in my script without reinitializing the driver how can i do that?

i want to change the download location many times in my script. i am using python selenium with firefox",[]
Scraping Specialcards Prices form Futbin,https://www.reddit.com/r/webscraping/comments/z49ems/scraping_specialcards_prices_form_futbin/,webscraping,"Hello, I play Fifa Ultimate Team and I would like to create a spreadsheet with which the prices of the players are automatically updated via [Futbin.com](https://Futbin.com). You can get the prices for normal gold cards via e.g. [https://www.futbin.com/23/playerPrices?player=231747](https://www.futbin.com/23/playerPrices?player=231747)

These are the prices for Mbappe. But I would also like to have prizes from special cards. Unfortunately, they have the same id (231747) as the normal cards.

Anyone know how to get the prices?",[]
Web Scraping Question - (I'm new at this),https://www.reddit.com/r/webscraping/comments/z3x684/web_scraping_question_im_new_at_this/,webscraping,"I have done some basic web scraping tuts using puppeteer, but I am looking to do something slightly more complex now. I have found this old website that displays the some prayers([https://ibreviary.com/m2/breviario.php](https://ibreviary.com/m2/breviario.php)). My friend might want to make an app / web client with this data. The issue is that the prayers are categorized on the site by date and time of day. I found a python web scraping script for it but the script specifies the date and the prayer category to scrape as opposed to scraping all of the prayers in their related categories with the dates associated with them. Is it possible to create a script that would just scrape all the data and output them into a single JSON file with the dates and categories associated with each individual prayer? Any suggestions would really help / even just ideas how you might go about doing this.  
 Thanks.",['Yeah it sounds like a slight modification to the script you found']
Webscraping with Machine Learning,https://www.reddit.com/r/webscraping/comments/z2ulzr/webscraping_with_machine_learning/,webscraping,"Hi y‚Äôall, sorry if this is the wrong spot for this question, I‚Äôve been coding for a while now, and I‚Äôm working on this project for a data science internship. practically it‚Äôs a universal webscraper, that grabs specific text that is flexible for every website. 

The idea is to just grab all the text from the website and tokenize the input, and convert it. To a vector via tf-idf, before feeding it into a binary classification model. Which has been pre-trained on labeled data beforehand. If the model outputs us a success we‚Äôll add the data, else we‚Äôll discard it. 

The reason why I used machine learning to differentiate  between the text that is important and the irrelevant ones, is because the text I need doesn‚Äôt follow a uniform structure, ie every website could have it laid out differently. So regex and basic heuristics is out of the question as far as I know.

The result are pretty disappointing, it‚Äôs true positive rate is high, but so is the false positive rate. Has anyone does anything similar, or have any guidance?

Thank you in advance.","[""Hey. I don't know anything about machine learning, but Ive heard of wordnet, a graph of all words where the distance indicates semantic distances. So you can scan a tokenised text and then get Kontext. For example if next to wing is the word airplain the programm gets that it ain't a chicken wing.\n\nI don't know how relevant it is but if you can do it without ai, you should."", 'You may find the node package called Compromise interesting.', 'I would recommend Googling the phrase ""Feature Engineering"" and seeing if anything inspires you.']"
HOW TO EXTRACT EMAIL ADRESS?,https://www.reddit.com/r/webscraping/comments/z2tem3/how_to_extract_email_adress/,webscraping,"Hello! I have a task to gather email adresses from a website that has multiple pages, and I would normally do this task by hand but there are 250+ pages and it would take an infinite amount of time.

Soo... I was wondering if there is any way I can extract the email adresses using a script? Thank you in advance!","['Refer to this link\n\nhttps://stackoverflow.com/questions/57944130/beautifulsoup-how-to-extract-email-from-a-website', ""If the email addresses are in an element with a specific class, I would grab the content of all the items in that class.\n\nIf the emails aren't contained in a specific element with one class, you may have to use RegEx."", 'Just grab all the text and iterate over it. It‚Äôs slow but if you only need to run it once it‚Äôs w/e. Just check something like \n\nif(p.text.find(‚Äú@‚Äú) != -1)\n\nTo check if it‚Äôs a email, or alternative just have a lookup table of common emails to validate them.', ""> I was wondering if there is any way I can extract the email adresses using a script?\n\nConsider using [Regular Expressions](https://en.wikipedia.org/wiki/Regular_expression).\n\nthis expression should cover almost all emails you will encounter:\n\n```\n^[A-Za-z0-9._+\\-\\']+@[A-Za-z0-9.\\-]+\\.[A-Za-z]{2,}$\n```\n\nsrc: [How can I validate an email address using a regular expression?](https://stackoverflow.com/questions/201323)""]"
Am I doing this formula right?,https://www.reddit.com/r/webscraping/comments/z2rrnr/am_i_doing_this_formula_right/,webscraping,"Hey Guys, I got off of webs craping for a while (I was only a novice before).  Can someone tell me what i'm doing wrong?  


I'm trying to grab the **street address** from the below webpage. For reference the street address is 529 23rd St.   


**Webpage**

[https://www.facebook.com/events/1149224985975829/?acontext=%7B%22event\_action\_history%22%3A\[%7B%22mechanism%22%3A%22discovery\_local\_tab%22%2C%22surface%22%3A%22bookmark%22%7D\]%2C%22ref\_notif\_type%22%3Anull%7D](https://www.facebook.com/events/1149224985975829/?acontext=%7B%22event_action_history%22%3A[%7B%22mechanism%22%3A%22discovery_local_tab%22%2C%22surface%22%3A%22bookmark%22%7D]%2C%22ref_notif_type%22%3Anull%7D)  


**The inspect element code is**  
 <span class =""x1lliihq x6ikm8r x10wlt62 x1n2onr6 xlyipyv xuxw1ft x1j85h84""   


**The code I'm trying is in google docs is:**  
//span(@class,'x1lliihq x6ikm8r x10wlt62 x1n2onr6 xlyipyv xuxw1ft x1j85h84')",['What are you using for this? Bs4?']
Has anyone made money building a product / service based on web-scrapping here ?,https://www.reddit.com/r/webscraping/comments/z28kqs/has_anyone_made_money_building_a_product_service/,webscraping,"Hey all,

I am looking for some inspiration on projects that you guys did and managed to get clients for. I am not looking for the freelancing stories but rather product or service stories.  
For instance you found a website that has some useful data, you scrapped it, repackaged it and managed to sell it to recurring paying customers.  
Also automation product / services ideas are welcome.  


Cheers","['Google did', 'At Tjommi.app we basically make money indirectly by scraping ecommerce stores and detecting price drops, we then automate a refund request to the store on behalf of our users and charge a finders fee on successful refunds.\n\nSort of an unconventional way of monetizing data, but it might give you some ideas.\n\nOther than that, i built Kassal.app (kassalapp is norwegian for cashiers check or receipt) which is basically a niche price comparison site for groceries, its monitized with affiliate links (kitchen supplies, food delivery and cookbooks), you could do the same thing with clothes, phones etc.', 'Here is a YouTube channel that I can recommend: [https://www.youtube.com/c/CobaltIntelligence/videos](https://www.youtube.com/c/CobaltIntelligence/videos)\n\nThere\'s a series of videos ""Making Money with Web Scraping"".', 'Hopefully other people share their experiences here']"
How do I get this bullet list (HTML),https://www.reddit.com/r/webscraping/comments/z2ctpy/how_do_i_get_this_bullet_list_html/,webscraping,"I'm sucks in coding. I'm having period cramps and all. 

I tried

for li in soup.find\_all('li',{'style':'text-align: justify;'}):  
for item in li.find\_all('span'):  
job.setRawDescription(str(li.text))

[I want to get all of the bullet lists](https://preview.redd.it/vors74hg2m1a1.png?width=643&format=png&auto=webp&v=enabled&s=9729ba8eb232d0074b33f114b3fe2805a6f35c58)

https://preview.redd.it/r9jypaup2m1a1.png?width=737&format=png&auto=webp&v=enabled&s=f793354e25d37a4f7fde7efaed13d8fcfc2084e4

[My code](https://preview.redd.it/d8n3mro73m1a1.png?width=524&format=png&auto=webp&v=enabled&s=ee20917357de88650fb54564b14590887d3306de)

[it captures the lasttt one and not in \\""Your role & responsibilities:\\""](https://preview.redd.it/xrel0c4a3m1a1.png?width=480&format=png&auto=webp&v=enabled&s=7fff01026805cf3d3a12d9398815b6925d6b3d8f)","['Not sure what the `job` object is, but you probably are finding everything, but because the for loop iterates over everything and overwrites with the current item, you will only ever get the last one. You can test this by seeing how long each `.find_all()` is.\n\nEdit - code formatted (mobile sucks sometimes)', '`library(rvest)`\n\n`library(tidyverse)`\n\n`library(stringr)`\n\n`url_data <- ""`[`https://topdev.vn/detail-jobs/mobile-engineer-android-talent-success-2028669`](https://topdev.vn/detail-jobs/mobile-engineer-android-talent-success-2028669)`""`\n\n`## Read URL, and select the node. In this case xpath of table.`\n\n`url_data2 <- url_data %>%`\n\n`read_html() %>%`\n\n`html_nodes(xpath=\'//*[@id=""about_company""]/div[2]/div\') %>%`\n\n`html_text()`\n\n&#x200B;\n\n\\# ""Must have strong technical skill at least 1 specific domain Mobile (Android) and take responsibility for designing that part in project.\n\n\\# \\\\r\\\\nCommunicate with customer in requirement collection, progress reporting and trouble solving.\n\n\\# \\\\r\\\\nResponsible to compose detail design documents, and implement the functionality based on provided requirements‚Äô backlog\n\n\\# .\\\\r\\\\nTransform customer‚Äôs requirements into logical, economical and practical system designs.\n\n\\# \\\\r\\\\nReady to support customer at out of business working hours when there is request.\n\n\\# \\\\r\\\\nAnalyze and design system flow and procedures to ensure optimum control and security of data and efficient use of resources\n\n\\# .\\\\r\\\\nPrepare detailed specifications from which complete sets of programs will be written.\\\\r\\\\nLead in researching and coaching new technology for growing up subordinates‚Äô development skill.\n\n\\# \\\\r\\\\nReport to Leader and Line Manager.\\\\r\\\\nEnsure the technical solution through the projects.\\\\r\\\\nOther job will be assigned by Supervisor depends on the project and period.\\\\r\\\\n""', ""Could you give us the link of the page you're scraping?""]"
Can't get HTML attribute,https://www.reddit.com/r/webscraping/comments/z1v5x8/cant_get_html_attribute/,webscraping,"Hi. I'm new to the web scraping world. I want to scrape the data-counter attribute (data-counter=""2332"") but I can't. I used the same way to get href attribute but this time it won't work. Can anyone help me? 

https://preview.redd.it/3egap5lfji1a1.png?width=1115&format=png&auto=webp&v=enabled&s=a0c2f815e969e5291d3149e1d997ef30dd2b18e5

[My usual way](https://preview.redd.it/47o9xdnwji1a1.png?width=434&format=png&auto=webp&v=enabled&s=a600963aa203e96b2c63b40a810209a71e504b49)

&#x200B;

[The error](https://preview.redd.it/jucaea63ki1a1.png?width=463&format=png&auto=webp&v=enabled&s=d70f6caf1d78f15bff3180c7c19ac0154ae5643c)

&#x200B;

[I tried int](https://preview.redd.it/881tfe97ki1a1.png?width=512&format=png&auto=webp&v=enabled&s=b133bedafe9ef21bf15d1f6b8c51a53d0f9202ea)

&#x200B;

[But error too](https://preview.redd.it/fn0yoprbki1a1.png?width=462&format=png&auto=webp&v=enabled&s=6b96fd7c2f069b743e0b1162a8a91bcd68374899)","['‚Äújobs‚Äù here is a ‚Äúlist‚Äù, not a dict-like object. My guess is that there are more than one element with class ‚Äúgpr-jobfinder-table‚Äù, and soup.select is returning all of them inside a list.']"
Looking for software to scrape documents from EU institutions websites,https://www.reddit.com/r/webscraping/comments/z1t2wr/looking_for_software_to_scrape_documents_from_eu/,webscraping,"Hello!

My  task is to download documents from the main EU bodies. These websites  do not require any kind of log in, the documents and their material are all available freely. I bought ScrapeBox to help me with that but it‚Äôs not working as intended. 

Is there anyone who can help me automate this process (whether entirely or partially)? 

&#x200B;

Below is the link to my previous post. It has more information on what I am looking for.

[https://www.reddit.com/r/webscraping/comments/x6gb3e/i\_would\_like\_some\_help\_with\_troubleshooting/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/webscraping/comments/x6gb3e/i_would_like_some_help_with_troubleshooting/?utm_source=share&utm_medium=web2x&context=3)

&#x200B;

I have since given up on Scrapebox since 1) it was useless and 2) these crooks refuse to refund even though it was evident in our communications that their software cannot do what I need it to.

&#x200B;

I am now looking to for software that does this. Either to outright buy it or with a subscription fee.","[""If you meant Scraperbox, they offer an API to evade anti bot systems and render javaScript. You have to be a programmer to actually build the scraper.\n\nI don't think you will be able to find a readymade scraper to scrape content from the website you shared. Someone will need to a write a custom web scraper to get this data. If you are familiar with Python and webscraping, I can guide you to do it.\n\nOn the other hand, if you are not a programmer, I can build one for you to get the data. If you are interested, you can pm me."", 'Hello,   \nIf you need one-time extraction of a big amount of data (not small amounts in high frequency) then maybe you will try DAAS (data as a service) [companies](https://www.g2.com/categories/data-extraction-services-f8d7dc20-f5c1-416b-a1d0-d8692c9a4f61)  \n\n\nYou can tell them the scope of the project and they will directly deliver the data.', ""I'm a founder of web scraping company (https://webscrapingsolutions.co.uk/). \nWe can help you get these docs :)""]"
Scrape Wowprogress,https://www.reddit.com/r/webscraping/comments/z1sm36/scrape_wowprogress/,webscraping,"Hey veryone, I'm not and expert in exel/google sheets and I have basic basic knowledge around coding in general.  


This is what I wanna do,   


I want to scrape [https://classic.warcraftlogs.com/character/eu/mograine/kooinish](https://classic.warcraftlogs.com/character/eu/mograine/kooinish) ""Best Perf. AVG"" and then there is a number there. I want to take that number and present it in a google sheets. I have installed xpath in my browser, I have added Importfromweb ""another addon for google sheets"". This is what I type in and I only get errors. I have also tried other variants.  


=IMPORTFROMWEB(""https://classic.warcraftlogs.com/character/eu/mograine/kooinish"";""//div\[@class='best-perf-avg'\]/b"")   


\#ALL\_SELECTORS\_RETURN\_NULL  


I hope there is someone out there who could help me fetch that number and present it in a google sheet.  


Thanks!","['https://articles.classic.warcraftlogs.com/help/api-documentation', 'Most of the people here use Python or JavaScript to scrape websites. You will have better luck posting it on Excel specific subreddit.', 'this website is rendered with Javascript\n\nYou have to add the ""jsRendering"" option in your function, just like that: \n\n=IMPORTFROMWEB(""https://classic.warcraftlogs.com/character/eu/mograine/kooinish"";""//div\\[@class=\'best-perf-avg\'\\]/b"";""jsRendering"")', 'Web scraping to Google Sheets using Pipedream and ScrapeNinja: https://www.youtube.com/watch?v=uBC752CWTew']"
Whats the best affordable server to host a python web scraping script?,https://www.reddit.com/r/webscraping/comments/z1m80m/whats_the_best_affordable_server_to_host_a_python/,webscraping,I need to run at least 10 python script that scrapes data. Its multi threaded. I tried the free dyno on heroku but it reached the memory limit already by running only one script. I need to run multiple script that will not break the bank. Please suggest. Thanks,"[""If you can, buy the cheapest Raspberry Pi you can get (try a RPi Zero W, new or used, it doesn't matter, it's about $8-10 on eBay) and use it as a webserver. I use it like that for several webscraping projects and it works flawlessly."", 'I use Cloud run on GCP. If you architecture the application properly you will stay entirely within free tier.', 'Pythonanywhere.com', ""As many people already mentioned great options like DigitalOcean or Linode 5$ unit should be plenty. Google cloud also gives 100$ trial which can last you a while.\n\nOne note is that web scraping is not very resource intensive - you can dramatically reduce memory footprint with code optimizations.   \nFor example, you should stream scraped results to file or database directly. \n\nFor simple projects, I like to use [json lines file type](https://jsonlines.org/), csv or sqlite which makes it dead easy to free up memory.\n\nIf you're running Selenium, Puppeteer or Playwright then you'll need quite a bit of memory. Though you can block image resource loading which should dramatically decrease bandwidth and memory usage (see [blocking resources section of this Playwright intro I wrote](https://scrapfly.io/blog/web-scraping-with-playwright-and-python/#blocking-resources))\n\nIf you're using Python you can have major memory use reduction just by using generators instead of lists too."", 'The Oracle Free Tier?', ""Raspberry pi, as someone else mentioned.\nBut this means you are responsible for electricity & internet connection 24/7 and you can't access the server if you are outside of your wlan.\nGood enough for basic stuff.\nSo depends what you intend to do.\n\nOtherwise have a look at digital ocean, you can get small server for $5 per month or so. Again, depends what you have in mind"", 'you can use droplet on [Digital Ocean](http://digitalocean.com). $5 per month.', 'I use an intel pc stick with 2gb ram and intel atom quad core z3735f, work fine and cost near to nothing to run', 'I use a Raspberry Pi Zero W.', ""It all depends on what is your infrastructure based on. I'm sure you have some sort of a queue system, database, etc.   \n\n\nHow many pages are you processing per day, let's say?""]"
Scraping Facebook pages public data using python,https://www.reddit.com/r/webscraping/comments/z1o9ij/scraping_facebook_pages_public_data_using_python/,webscraping,"Hi everyone. I'm new to web scraping. I want to extract public data from facebook pages like post, comments, likes etc. I tried out a few solutions but it did not work out since facebook have so many restriction. It will be great help if you guys able to guide me. Thanks :D",[]
How to identify duplicate crawl data?,https://www.reddit.com/r/webscraping/comments/z1ekic/how_to_identify_duplicate_crawl_data/,webscraping,"Sometimes data is duplicated across multiple web pages. At other times, multiple pages contain identical but distinct data. By ""distinct data,"" I mean that it was created independently of other data.

Does any know of existing approaches for distinguishing duplicate and identical blobs of data? I'm thinking there may be some means for finger printing it, but computing, say, a vanilla SHA512 hash isn't going to work here.","[""> a vanilla SHA512 hash isn't going to work here.\n\nWhat data exactly?\n\n**To quantify the similarity between:**\n\n# Text\n\nConsider something like [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) and one of it's implementations like [thefuzz](https://github.com/seatgeek/thefuzz).\n\n# Images\n\nThere is whole category of image hashing algorithms that doesn't have the classic [avalanche effect](https://en.wikipedia.org/wiki/Avalanche_effect):\n\n* [a-hash](https://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html)\n* [p-hash](http://www.phash.org/)\n* [d-hash](https://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) \n* [w-hash](https://fullstackml.com/wavelet-image-hash-in-python-3504fdd282b5)\n\n\nsrc: [What is image hashing used for?](https://stackoverflow.com/questions/998662/)"", ""Haven't tried but Scrapy's Item Pipeline could work: [https://docs.scrapy.org/en/latest/topics/item-pipeline.html](https://docs.scrapy.org/en/latest/topics/item-pipeline.html)"", 'Check Scrapy. From what I remember, it deduplicates the same URLs by default if you use the yield Request (scrapy object instance). And if you want to stop this behaviour, when you use the Request you can set the dont_filter=True param.']"
Webscrapper IO - No data scraped yet error,https://www.reddit.com/r/webscraping/comments/z12ir7/webscrapper_io_no_data_scraped_yet_error/,webscraping,"Hi, 
Can anyone help me resolve the issue of No data scraped yet using Webscrapper IO extension. I‚Äôm trying to scrape data from Amazon and have built a site map that goes from one page to page that is recommended and so on. While the site map is able to navigate from one page to another, no data is being scraped. Anyone facing similar issue or is aware of root cause here ? Please help and let me know if you need more details to understand the issue better","['I\'m seeing a similar issue here... Data Previews all look good, and the scraper seems to go through all the pages correctly, but I\'m always seeing ""No data scraped yet"" even after refreshing. Will post if I find a fix.']"
Scraping a JS file and all its associated JS files,https://www.reddit.com/r/webscraping/comments/z17o1i/scraping_a_js_file_and_all_its_associated_js_files/,webscraping,"Is it possible to download a JS file and all the associated JS files automatically?
This is the package from a CDN that I want to build locally:

https://jspm.dev/@spectrum-web-components/bundle/elements.js

This build of it is the only one that works for my use case.  Is it possible to download this JS file and all associate JS files?","[""yes it's possible\n\nit just a little tricky"", '[deleted]']"
API for scraping residential addressees?,https://www.reddit.com/r/webscraping/comments/z0roic/api_for_scraping_residential_addressees/,webscraping,"Hello! 

I‚Äôm new to scraping and couldn‚Äôt find the answer after researching and test runs on Apify and other sources. 

I‚Äôm looking for an API that can scrap Google Maps for residential mailing addresses that meet my search criteria in a city. Is this possible?

Thanks!","['I think USPS has something that you can buy. I remember doing something like that several years ago to send out mailers.', ""Not exactly what you described, but it could be worth taking a look at if [OpenAddresses](https://openaddresses.io/) would fit the task. I haven't done much work with residential mailing data myself, but I found [The Markup's detailed explainer](https://themarkup.org/show-your-work/2022/10/19/how-we-uncovered-disparities-in-internet-deals) for how it used OpenAddresses and other data sources in its recent report on internet pricing to be highly informative.\n\nCould be a good starting point for seeing what's out there."", 'What search criteria is it? A lot of autosuggest endpoints could work depending on volume and dat points needed', ""hey there, actually you should be able to scrape this with Apify's Google Maps Scraper. Have you tried asking their support or on their Discord? Give it a shot, they are usually very helpful.""]"
Browser automation Framework,https://www.reddit.com/r/webscraping/comments/z0pdch/browser_automation_framework/,webscraping,Has anybody used the Browser automation framework what‚Äôs your experience?,"['Selenium & Playwright. You can choose one, then forget about your problems. \n\nTheir Python SDK is great, they have a lot of documentations and Stack Overflow questions.', ""If you're new to this area I'd recommend Playwright. I wrote a pretty extensive introduction here [Web Scraping with Playwright and Python](https://scrapfly.io/blog/web-scraping-with-playwright-and-python/)\n\nPlaywright is the most modern take in this area and has both synchronous and asynchronous APIs as well as a [code generator](https://playwright.dev/docs/codegen) that records what you do in your browser and exports code. Though, take note that exported code kinda sucks but it can be used as a bootstrap for your program.""]"
"Some website making scrapy splash timeout, before anything is rendered, even with one page.",https://www.reddit.com/r/webscraping/comments/z0nowv/some_website_making_scrapy_splash_timeout_before/,webscraping,"I've been struggling and working with many different setting but it seems some website just timeout with scrapy splash with even one url, and I can't even get a response.text it just hangs. I've tried this with aquarium and multiple splash instances and some just don't work no matter what while others do. 

I don't understand how these websites can cause scrapy splash to timeout on try one.

I have scrapy splash instances set to 2-4gb or ram with 24gb ram on my machine. But even one url from particular sites can crash it","[""I've encountered the same issue. No solution since they are not maintaining the project.\n\nThus, I created a solution for myself. It's early (I code it in 2 days) but it works. Check it out, you can self-host it as a service.\n\nI built it with Python (FastAPI) & Playwright. More features are coming soon.\n\n[https://pywright.fly.dev/](https://pywright.fly.dev/)""]"
Is the data on this website scrapable? Tennis project.,https://www.reddit.com/r/webscraping/comments/z0m0sp/is_the_data_on_this_website_scrapable_tennis/,webscraping,"Hi, I am working on a data science project for school and I am trying to scrape the match stats from a tennis tournament in 2022: [https://www.atptour.com/en/scores/stats-centre/archive/2022/8998/MS003](https://www.atptour.com/en/scores/stats-centre/archive/2022/8998/MS003)

I cannot figure out how to access the stats data (serve rating, aces, etc). I cannot find any of the stats by searching through the output of BeautifulSoup's html parser, however the stats exist in the page when I inspect the website directly. I have also tried using lxml to scrape but nothing is returned.

Any thoughts? Thanks in advance, your advice is much appreciated!","['The match stats are being fetched later after the page load from these locations:\n\n```json\n{\n  ""API_HOST"": ""https://itp-atp-sls.infosys-platforms.com"",\n  ""API_MATCH_STATUS"": ""/prod/api/match-beats/status/year/#year/eventId/#eventId/matchId/#matchId"",\n  ""API_MATCH_BEATS"": ""/prod/api/match-beats/data/year/#year/eventId/#eventId/matchId/#matchId"",\n  ""API_STATS"": ""/prod/api/stats-plus/v1/keystats/year/#year/eventId/#eventId/matchId/#matchId"",\n  ""API_STROKE_SUMMARY"": ""/prod/api/stroke-analysis/rally/v2/year/#year/eventId/#eventId/matchId/#matchId"",\n  ""API_COURT_VISION"": ""/prod/api/court-vision/year/#year/eventId/#eventId/matchId/#matchId/pointId/#pointId"",\n  ""API_BELOW_COURT"": ""/prod/api/court-vision/belowCourt/year/#year/eventId/#eventId/matchId/#matchId/pointId/#pointId"",\n  ""API_RALLY_ANALYSIS"": ""/prod/api/rally-analysis/year/#year/eventId/#eventId/matchId/#matchId"",\n  ""API_HEAD_TO_HEAD"": ""/prod/api/head-to-head/year/#year/eventId/#eventId/matchId/#matchId"",\n  ""API_HEAD_TO_HEAD_COMPARE"": ""/prod/api/head-to-head/year/#year/eventId/#eventId/player1/#player1/player2/#player2"",\n  ""API_INSIGHTS"": ""/prod/api/assisted-journalism/insights/year/#year/eventId/#eventId/matchId/#matchId"",\n  ""API_SOCIAL_HEATMAP_TWEETS"": ""/prod/api/social-heatmap/tweets/year/#year/eventId/#eventId/matchId/#matchId"",\n  ""API_LOAD_MORE_TWEETS"": ""/prod/api/social-heatmap/tweets"",\n  ""API_LEADERBOARD_RANKS"": ""/prod/api/slam-leader/ranks/year/#year/eventId/#eventId/eventType/#eventType/player1/#player1/player2/#player2"",\n  ""API_YTDSTATS"": ""/prod/api/stats-plus/v1/ytdStats/year/#year/eventId/#eventId/matchId/#matchId"",\n  ""API_S3_MATCH_BEATS"": ""/static/prod/match-beats/#year/#eventId/#matchId/data.json"",\n  ""API_S3_STATS"": ""/static/prod/stats-plus/#year/#eventId/#matchId/keystats.json"",\n  ""API_S3_RALLY_ANALYSIS"": ""/static/prod/rally-analysis/#year/#eventId/#matchId/data.json"",\n  ""API_S3_STROKE_SUMMARY"": ""/static/prod/stroke-analysis/v2/#year/#eventId/#matchId/data.json"",\n  ""API_S3_COURT_VISION"": ""/static/prod/court-vision/#year/#eventId/#matchId/data.json"",\n  ""MC3D_API_BASE_PATH"": ""https://itp-atp-sls.infosys-platforms.com/prod/api/"",\n  ""POLLING_TIME"": {\n    ""courtVision"": 20000,\n    ""matchStatus"": 10000,\n    ""default"": 10000,\n    ""setChange"": 10000,\n    ""leaderboardRanks"": 60000\n  }\n}\n```\n\nIf you try to fetch these json objects directly; you will find them encrypted.\n\nFollow this [StackOverFlow](https://stackoverflow.com/questions/73735401/scraping-an-atptour-com-api-returns-what-looks-like-encrypted-data) thread which will point you to this [github repository](https://github.com/serve-and-volley/atp-world-tour-tennis-data) where if you are just interested in the raw data, someone already did the work there.', 'Every website is scrapable', 'Have you tried using selenium webdriver?']"
Can't get JSON content from HTML,https://www.reddit.com/r/webscraping/comments/z0fm0i/cant_get_json_content_from_html/,webscraping,"I'm scraping a website and it has JSON but my code doesn't seem to catch them. Everything else is fine but the last 4 rows give an error. Here's the snippet of the code:

&#x200B;

https://preview.redd.it/5uvjgm43a61a1.png?width=554&format=png&auto=webp&v=enabled&s=3a521ed768623922a0394551517ff52be0683741","[""Haven't tried to test anything, but should it be `html` and not `htcl`? Also it helps if you post error messages that you are getting. In theory BeautifulSoup has a text property that you can access your `objTag`. You could then use the `json` library (`import json`) to read the string that you get.""]"
BeautifulSoup - Scrape product and product variants and export it to csv,https://www.reddit.com/r/webscraping/comments/z0ef5i/beautifulsoup_scrape_product_and_product_variants/,webscraping,"I am trying to scrape this [website](https://www.petbarn.com.au/dogs/dog-food/dry-dog-food)  products listing what I am trying to achieve here is grab all the info  per product for example: product\_name, price and their variants info as  well like 10kg, 20kg, 3kg and their prices accordingly. I have search  the html they don't provide all the info I am looking for but under  script tag they have a json residing which could be useful. Here is the  json script tag:

    </script><script type=""text/x-magento-init"">
            {
                ""[data-role=swatch-option-111105]"": {
                    ""Magento_Swatches/js/swatch-renderer"": {
                        ""selectorProduct"": "".product-item-details"",
                        ""onlySwatches"": true,
                        ""enableControlLabel"": false,
                        ""numberToShow"": 16,
                        ""jsonConfig"": {""attributes"":{""1299"":{""id"":""1299"",""code"":""size"",""label"":""Size"",""options"":[{""id"":""6651"",""label"":""10kg"",""products"":[""116724""]},{""id"":""6780"",""label"":""20kg"",""products"":[""108981""]},{""id"":""6234"",""label"":""3kg"",""products"":[""108987""]}],""position"":""0""}},""template"":""$<%- data.price %>"",""currencyFormat"":""$%s"",""optionPrices"":{""108987"":{""baseOldPrice"":{""amount"":49.990908090909},""oldPrice"":{""amount"":54.99},""basePrice"":{""amount"":42.718180818182},""finalPrice"":{""amount"":46.99},""tierPrices"":[],""msrpPrice"":{""amount"":0}},""108981"":{""baseOldPrice"":{""amount"":172.71818081818},""oldPrice"":{""amount"":189.99},""basePrice"":{""amount"":128.17272627273},""finalPrice"":{""amount"":140.99},""tierPrices"":[],""msrpPrice"":{""amount"":0}},""116724"":{""baseOldPrice"":{""amount"":120.899999},""oldPrice"":{""amount"":132.99},""basePrice"":{""amount"":102.71818081818},""finalPrice"":{""amount"":112.99},""tierPrices"":[],""msrpPrice"":{""amount"":0}}},""priceFormat"":{""pattern"":""$%s"",""precision"":2,""requiredPrecision"":2,""decimalSymbol"":""."",""groupSymbol"":"","",""groupLength"":3,""integerRequired"":false},""prices"":{""baseOldPrice"":{""amount"":49.990908090909},""oldPrice"":{""amount"":54.99},""basePrice"":{""amount"":172.71818081818},""finalPrice"":{""amount"":189.99}},""productId"":""111105"",""chooseText"":""Choose an Option..."",""images"":[],""index"":{""108987"":{""1299"":""6234""},""108981"":{""1299"":""6780""},""116724"":{""1299"":""6651""}},""preSelectedGallery"":[],""channel"":""website"",""salesChannelCode"":""base"",""sku"":{""108987"":""127956"",""108981"":""127960"",""116724"":""501600""},""labels"":{""108987"":{""sales_flag_label"":""Great low price""},""108981"":{""sales_flag_label"":""Great low price""},""116724"":{""sales_flag_label"":""Great low price""}},""hasEndDate"":{""108987"":false,""108981"":false,""116724"":false},""dynamic"":{""name"":{""108987"":{""value"":""Black Hawk Fish And Potato Adult Dog Food - 3kg""},""108981"":{""value"":""Black Hawk Fish And Potato Adult Dog Food - 20kg""},""116724"":{""value"":""Black Hawk Fish & Potato Dog Food 10kg""}},""sku"":{""108987"":{""value"":""127956""},""108981"":{""value"":""127960""},""116724"":{""value"":""501600""}},""gtin"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""marketing_offer_short"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""advice_care"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""product_category"":{""108987"":{""value"":""Dry Food""},""108981"":{""value"":""Dry Food""},""116724"":{""value"":""Dry Food""}},""benefits"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""feeding_guide"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""health_condition_dietary"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""brand_filter"":{""108987"":{""value"":""Black Hawk""},""108981"":{""value"":""Black Hawk""},""116724"":{""value"":""Black Hawk""}},""ingredients"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""activity_level"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""size"":{""108987"":{""value"":""3kg""},""108981"":{""value"":""20kg""},""116724"":{""value"":""10kg""}},""food_type"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""health_benefits"":{""108987"":{""value"":""Total Wellbeing""},""108981"":{""value"":""Total Wellbeing""},""116724"":{""value"":""Total Wellbeing""}},""life_stage"":{""108987"":{""value"":""Adult""},""108981"":{""value"":""Adult""},""116724"":{""value"":""Adult""}},""flavour"":{""108987"":{""value"":""Fish""},""108981"":{""value"":""Fish""},""116724"":{""value"":""Fish""}},""nutritional_info"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""breed"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""nutritional_info_table"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""australia_made"":{""108987"":{""value"":""No""},""108981"":{""value"":""No""},""116724"":{""value"":""No""}},""nutrition_grade"":{""108987"":{""value"":""Premium""},""108981"":{""value"":""Premium""},""116724"":{""value"":""Premium""}},""lifestyle"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""weight_control"":{""108987"":{""value"":""No""},""108981"":{""value"":""No""},""116724"":{""value"":""No""}},""frequent_feeder_price"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}},""size_swatches"":{""108987"":{""value"":""""},""108981"":{""value"":""""},""116724"":{""value"":""""}}}},
                        ""jsonSwatchConfig"": {""1299"":{""6651"":{""type"":""0"",""value"":null,""label"":""10kg""},""6780"":{""type"":""0"",""value"":null,""label"":""20kg""},""6234"":{""type"":""0"",""value"":null,""label"":""3kg""},""additional_data"":""{\""update_product_preview_image\"":\""1\"",\""use_product_image_for_swatch\"":0,\""text_swatch_as_multiple_select\"":\""1\"",\""swatch_input_type\"":\""text\""}""}},
                        ""mediaCallback"": ""https\u003A\u002F\u002Fwww.petbarn.com.au\u002Fswatches\u002Fajax\u002Fmedia\u002F"",
                        ""jsonSwatchImageSizeConfig"": {""swatchImage"":{""width"":30,""height"":20},""swatchThumb"":{""height"":90,""width"":110}},
                        ""showTooltip"": 1                }
                }
            }
        </script>

I have manage to parse that script tag and turned that into python  dictionary via json.loads() but couldn't figure out the best way to  extract info and export it to csv. here is my code so far:

    import requests
    import pandas as pd
    from bs4 import BeautifulSoup
    import json
    
    from datetime import datetime
    from datetime import date
    
    now = datetime.now()
    today = date.today()
    
    
    class PetBarnProdScraper:
    
        all_info = []
    
        def fetch(self, url):
            print(f""HTTP GET request to URL: {url}"", end="""")
            res = requests.get(url)
            print(f"" | Status Code: {res.status_code}"")
    
            return res
    
        def parse(self, response):
            soup = BeautifulSoup(response.text, ""html.parser"")
            product_urls = [a.get(""href"") for a in soup.select(""a.product-item-link"")]
            product_ids = [
                pid.get(""id"").split(""-"")[-1] for pid in soup.select(""div.product-item-info"")
            ]
            titles = [
                a.text.replace(""\n"", """").strip() for a in soup.select(""a.product-item-link"")
            ]
            old_price = [
                p.select_one(""span.price"").text for p in soup.select(""span.old-price"")
            ]
            ratings = [r.get(""title"") for r in soup.select(""div.rating-result"")]
            no_of_reviews = [review.text for review in soup.select(""a.action.view"")]
            data = (
                soup.select('script[type=""text/x-magento-init""]')[3]
                .text.replace(""\n"", """")
                .strip()
            )
            data_json = json.loads(data)
            data_j = json.loads(
                data_json[""*""][""Overdose_AdobeAnalytics/js/view/datalayer""][""datalayer""][0]
            )
    
            for idx in range(len(titles)):
                try:
                    ratings_count = ratings[idx]
                    reviews_count = no_of_reviews[idx]
                    last_price = old_price[idx]
                except:
                    ratings_count = ""N/A""
                    reviews_count = ""N/A""
                    last_price = ""N/A""
                d = {
                    ""Scraped_Date"": now.strftime(""%m/%d/%Y, %H:%M:%S"").split("","")[0],
                    ""Scraped_Time"": now.strftime(""%m/%d/%Y, %H:%M:%S"").split("","")[1],
                    ""product_name"": titles[idx],
                    ""price"": data_j[""PLP""][""products""][idx][""productPrice""],
                    ""old_price"": last_price,
                    ""ratings"": ratings_count,
                    ""number_of_reviews"": reviews_count,
                    ""productSKU"": data_j[""PLP""][""products""][idx][""productSKU""],
                    ""productSize"": data_j[""PLP""][""products""][idx][""productSize""],
                    ""priceWithoutTax"": data_j[""PLP""][""products""][idx][
                        ""productPriceLessTax""
                    ],
                    ""lifeStage"": data_j[""PLP""][""products""][idx][""lifeStage""],
                }
    
                for prod_id in product_ids:
                    details = soup.select_one(
                        f""script:-soup-contains('[data-role=swatch-option-{prod_id}]')""
                    )
                    labels = []
                    if details:
                        json_details = json.loads(details.text.replace(""\n"", """").strip())
                        json_endpoint = json_details[f""[data-role=swatch-option-{prod_id}]""]
                        label_option_list = json_endpoint[
                            ""Magento_Swatches/js/swatch-renderer""
                        ][""jsonConfig""][""attributes""][""1299""][""options""]
                        for lab in label_option_list:
                            labels.append(lab[""label""])
    
                            d[""label_options""] = labels
                print(d)
                self.all_info.append(d)
    
        def to_csv(self):
            df = pd.DataFrame(self.all_info).fillna("""")
    
            df.to_csv(f""{today}_petbarn.csv"", index=False)
    
            print('Stored results to ""petbarn.csv""')
    
        def run(self):
            for i in range(1, 2):  # total_number of pages
                url = f""https://www.petbarn.com.au/dogs/dog-food/dry-dog-food?p={i}""
    
                response = self.fetch(url)
    
                self.parse(response)
    
            self.to_csv()
    
    
    if __name__ == ""__main__"":
        scraper = PetBarnProdScraper()
        scraper.run()

every time I run that code the label\_options column has always the same  values which is the last one I am guessing. here is the output I am  getting:

    Scraped_Date,Scraped_Time,product_name,price,old_price,ratings,number_of_reviews,productSKU,productSize,priceWithoutTax,lifeStage,label_options
    11/21/2022, 00:31:47,Black Hawk Fish And Potato Adult Dog Food,189.99,N/A,N/A,N/A,black-hawk-fish-&-potato-adult-dog-food,,172.72,Adult,""['10kg', '20kg', '3kg']""
    11/21/2022, 00:31:47,SavourLife Ancient Grains Lean Chicken Adult Dog Food,159.99,N/A,N/A,N/A,savourlife-ancient-grains-lean-chicken-adult-dog-food,,145.45,Adult,""['10kg', '20kg', '3kg']""
    

Expected output:

    Scraped_Date,Scraped_Time,product_name,price,old_price,ratings,number_of_reviews,productSKU,productSize,priceWithoutTax,lifeStage,label_options
    11/21/2022, 00:31:47,Black Hawk Fish And Potato Adult Dog Food,189.99,N/A,N/A,N/A,black-hawk-fish-&-potato-adult-dog-food,,172.72,Adult,10kg
    11/21/2022, 00:31:47,Black Hawk Fish And Potato Adult Dog Food,189.99,N/A,N/A,N/A,black-hawk-fish-&-potato-adult-dog-food,,172.72,Adult,20kg
    11/21/2022, 00:31:47,Black Hawk Fish And Potato Adult Dog Food,189.99,N/A,N/A,N/A,black-hawk-fish-&-potato-adult-dog-food,,172.72,Adult,3kg
    11/21/2022, 00:31:47,SavourLife Ancient Grains Lean Chicken Adult Dog Food,159.99,N/A,N/A,N/A,savourlife-ancient-grains-lean-chicken-adult-dog-food,,145.45,Adult,3kg
    11/21/2022, 00:31:47,SavourLife Ancient Grains Lean Chicken Adult Dog Food,159.99,N/A,N/A,N/A,savourlife-ancient-grains-lean-chicken-adult-dog-food,,145.45,Adult,20kg

Can anyone please help me figure out the best way to get the expected output? Thanks!","['You\'re looping over all the prod_ids and overwriting d[""label_options""] with every prod_id\'s labels until you get to the last one... then you are just appending the last prod_id\'s labels to each product. So, while you are looping over the pro_ids you need to check if the prod_id corresponds the product in the ""d"" dictionary that you are working with']"
Legality of webscraping stock sites,https://www.reddit.com/r/webscraping/comments/yzvuvt/legality_of_webscraping_stock_sites/,webscraping,"Hi, I've been doing on and off scraping of stock analysis websites (such as [Zacks.com](https://Zacks.com)) for a while now.

I was wondering a few questions regarding the rules. I know for many sites, especially bigger sites that grade stocks like CNN, Benzinga, Zacks, ..., it is absolutely against their TOS to scrape data, but I also see some people, even within this subreddit, say it's legal because it's public data or point to a legal decision. I know there probably isn't a clear line, but are there legal issues/rules/improvements to what I'm doing here:

* Scraping a site around every 10s for a ticker's grading (not 24/7 but maybe for 12hrs every 2 weeks)
* EDIT: the data gathered from this scraping isn't republished, just processed through algorithms and shared between a few of my friends

Is there a frequency that is allowed? Is stock analysis technically copyrighted/property of the site? Could I be sued for doing this?

I just want to get an idea of the rules I should follow to stay out of trouble, thank you!","['No lawyer by any means but, as far as I understand the law you can only copy right it if the analysis required some propriatary algorythm for example that is patented bc if you just draw supports and resistances and look at some indicators to make a conclusion you in theory could do that yourself so it should be fine\n\nwhy these sites have it in their TOS is probably (based on my opinion) to prevent a DDoS attack that is caused by a bunch of people trying to scrape them for data, what wouldnt be a problem if they just make an api so you can get the relevant data easily\n\nHope this helps', ""Not a lawyer but if it's accessible on the web as the  public and you don't overtax the system, fair game.""]"
How to webscrape the rates and currencies from each country?,https://www.reddit.com/r/webscraping/comments/z01hi0/how_to_webscrape_the_rates_and_currencies_from/,webscraping,"Hi everyone,

My internship company asked if I could webscrape the rates and currencies from each country from this website: [https://www.rebtel.com/en/](https://www.rebtel.com/en/) and [https://yollacalls.com/](https://yollacalls.com/). I have little Python experience so I tried to scrape it with Beautifulsoup but for some reason my output does not provide me the rates. I've watched several Youtube tutorials but everything what I try doesn't work? Could somebody please help me with this? It would help me a lot in passing my internship and achieving a good grade! :) 

&#x200B;

For rebtel I used this as input:

&#x200B;

from bs4 import BeautifulSoup  
import requests  


html\_text = requests.get(""https://www.rebtel.com/en/"").text  
soup = BeautifulSoup(html\_text, ""lxml"")  


countries = soup.find\_all(""div"", class\_=""col-xs-12"")  


print(countries)

&#x200B;

output: 

&#x200B;

\[<div class=""col-xs-12 text-center text-image pad-l""><div class=""key gap-none-top"">Limited offers for new users only</div></div>, <div class=""destination-top pad-none-left pad-none-right col-xs-12 col-sm-8 text-left text-center-xs""><h3><i class=""flag-xs flag-xs-cu gap-s-right"" style=""margin-top:-5px;""></i>Cuba</h3><p class=""gap-s-top"">Try 20 minutes for 6 ‚Ç¨</p></div>, <div class=""destination-bottom col-xs-12 col-sm-4 text-center""><a class=""btn btn-secondary btn-sm btn-block-xs btn-block-sm"" data-ga-action=""OfferBanner"" data-ga-category=""StartPage"" data-ga-event=""click"" data-ga-label=""CU"" href=""[https://my.rebtel.com/en/my/checkout/product/3358/signup/](https://my.rebtel.com/en/my/checkout/product/3358/signup/)"">Buy now</a></div>, <div class=""destination-top pad-none-left pad-none-right col-xs-12 col-sm-8 text-left text-center-xs""><h3><i class=""flag-xs flag-xs-ng gap-s-right"" style=""margin-top:-5px;""></i>Nigeria</h3><p class=""gap-s-top"">Try 30 minutes for 1 ‚Ç¨</p></div>, <div class=""destination-bottom col-xs-12 col-sm-4 text-center""><a class=""btn btn-secondary btn-sm btn-block-xs btn-block-sm"" data-ga-action=""OfferBanner"" data- 

etc....

&#x200B;

Thanks in advance for looking at it.","[""Dm me let's discuss the way forward"", 'soup.find_all returns a list of all the divs that match the args, now you have to get the attribute that contains the info you want, probably something like countries[0].text_content()']"
Find Out-Of-Date WordPress Sites,https://www.reddit.com/r/webscraping/comments/yzyl2b/find_outofdate_wordpress_sites/,webscraping,"Is it possible for a scraper to find wordpress sites that are running on an out of date version of WP and collect:
URL, WP version, and any contact info (name number, email, contact page)?","[""Let me guess. You either want to hack some sites to distribute you're mailware or to have fun or you want to automatically email the people to update theire page xD"", 'Yes, definitely. With some margin of error ofc.', ""Sure, it's possible. I've worked on a similar project, but we concentrated on Shopify stores.""]"
Proxy-Coin is the cheapest residential rotating proxy provider out there right now!,https://www.reddit.com/r/webscraping/comments/z00zi6/proxycoin_is_the_cheapest_residential_rotating/,webscraping,"We sell at 0.60$ per GiB!

Now you might be thinking, why is this so cheap? It is due to us being a new company and not trying to make too much profit and keep it fair for the clients.

We think clients should get the best support and proxies they can get so we made Proxy-Coin!

We support the following protocols: SOCKS5 and HTTP

Discord server: [https://discord.gg/w3rgx6DNGZ](https://discord.gg/w3rgx6DNGZ)","['Yes this is self promo, I am just trying to promote my service :D']"
Scraping android applications,https://www.reddit.com/r/webscraping/comments/yzdumj/scraping_android_applications/,webscraping,"Not sure if it is relevant here, but did anyone try Scraping Android Apps using Appium. 

I wanted to host the apps I want to scrape on a service like browserstack but according to them I shouldn't really use their service for apps I don't own.
 
Are there any other similar services that allow that?",['I was wondering if you can do it with Windows Subsystem for Android.']
Which companies use web scraping?,https://www.reddit.com/r/webscraping/comments/yz1la9/which_companies_use_web_scraping/,webscraping,Does anyone know what business sectors use web scraping or need it?,"['Almost all of them.', 'Ever heard of google?', 'There are many examples provided on this YouTube channel: [https://www.youtube.com/c/CobaltIntelligence/videos](https://www.youtube.com/c/CobaltIntelligence/videos)']"
Error on IMPORTHTML from Google Search,https://www.reddit.com/r/webscraping/comments/yz3v9b/error_on_importhtml_from_google_search/,webscraping,"Anyone know why I might be getting this error?

&#x200B;

https://preview.redd.it/p9meymbb9u0a1.png?width=1924&format=png&auto=webp&v=enabled&s=29029423b6294b1728bbd7b55d3b77fe0134d8f3

https://preview.redd.it/mz62p0ma9u0a1.png?width=416&format=png&auto=webp&v=enabled&s=a72a9b80813a88ab68c12560312193882513dcd1",['Google is blocking scraping Google Search using Google Sheets?']
resources for webscraping beginner,https://www.reddit.com/r/webscraping/comments/yykic1/resources_for_webscraping_beginner/,webscraping,"As the title said, i am getting started with web scraping and am looking for some good resources (blogs, youtubr, ..etc)

To learn more about it, thank you for the help","['This homie has some really good videos specifically for python webscraping: https://www.youtube.com/@JohnWatsonRooney', '+1 for Cobalt Intelligence!', 'For me, the best way to learn web scraping is to get your hand dirty by keep practicing.\n\nI used Python for web scraping as a hobby. And it turned out that I made a very decent amount of money from Upwork.\n\nCobalt intelligence is a great resource.  If you want to learn web scraping with Python, you can take a look at my 2 blog posts here:\n\n[https://thetomtech.net/how-to-scrape-data-with-beautifulsoup/](https://thetomtech.net/how-to-scrape-data-with-beautifulsoup/)\n\n[https://thetomtech.net/how-to-scrape-a-website-using-requests-in-python/](https://thetomtech.net/how-to-scrape-a-website-using-requests-in-python/)\n\nI hope this helps.', 'I began with the Scrapy tutorial: [https://docs.scrapy.org/en/latest/intro/tutorial.html](https://docs.scrapy.org/en/latest/intro/tutorial.html). \n\nAlso, [https://books.toscrape.com/](https://books.toscrape.com/) and [https://quotes.toscrape.com/](https://quotes.toscrape.com/) are great pages to practice on.', ""I'm biased because it's my channel but we talk about a lot of JS web scraping alongside the business side.\n\n&#x200B;\n\nhttps://www.youtube.com/@cobaltintelligence"", ""Not the world's best resource (yet, lol)...but since we're all dropping links, here's mine:\n\n[https://www.duinobit.com/category/web-scraping/](https://www.duinobit.com/category/web-scraping/)\n\n...thought I had more up, but turns out all the bits and pieces I've been working on haven't been written up yet, which I should really get to.\n\nOnly thing in there that's relevant at the minute is a Puppeteer example in something I wrote to answer a question on Reddit about counting internal links per page for a website, that's below:\n\n[https://www.duinobit.com/seo/get-total-count-of-internal-links-to-each-page-of-a-site-using-node/](https://www.duinobit.com/seo/get-total-count-of-internal-links-to-each-page-of-a-site-using-node/)"", 'When I realized that webscrapers just download, read, and parse a sites html, that‚Äôs when I realized it wouldn‚Äôt be too difficult. There‚Äôs a few extra steps, but I got my first webscaper up and running this week!', ""Hi All- thanks for this wealth of information. Instead of creating a new post, thought I'd just piggy back on this existing post. I hope you dont mind u/emran94 . \n\nAnyways, I wanted to ask what are your thoughts on learning Javascript vs Python? Thanks!""]"
Where to sell your scraped data?,https://www.reddit.com/r/webscraping/comments/yyyacl/where_to_sell_your_scraped_data/,webscraping,"Hey guys, so I have made a scraper for a site that i am sure the data can be useful for others, but not sure how and where I can sell the data access. 
anyone would like to share? 

thanks","[""I've used fiverr for that."", 'what kind of data do you have for sale? I have brokered a deal in telco industry suppliers area and sold them a large data set which actually ends buying us buying a cable company.']"
Anyone know how to scrape first result from google search with Python.,https://www.reddit.com/r/webscraping/comments/yynkap/anyone_know_how_to_scrape_first_result_from/,webscraping,"`query = ""khabib's age""`

`listing = []`

`headers = {`

`'User-agent':`

`""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582""`

`}`

`html = requests.get('`[`https://www.google.com/search?q='+query`](https://www.google.com/search?q='+query) `, headers=headers).text`

`soup = BeautifulSoup(html, 'lxml')`  


Basically I want to get the first result from google search.  For example,

 if the query is ""khabib's age"". It should return ""34 years"". Let me show you another example to understand. If asked who is the richest person in the world, than it should return ""Elon Musk, the co-founder and CEO of Tesla, is the richest person and the richest man in the world with a net worth of $203 billion."" because this is the first result of the search. So now you can understand what I expect. Anyone can help me with this!","[""I'd like to help. Dm"", 'If you know python try playwright']"
Need some help to finish an online marketplace notification project,https://www.reddit.com/r/webscraping/comments/yyn0ls/need_some_help_to_finish_an_online_marketplace/,webscraping,"Hi guys, I have set up a Gumtree (online marketplace) scraper, to scrape data from a specific item category every 2 mins and then export the data in JSON, HTML, etc.

I want to know how to automatically analyse the data and send an email/SMS when a scrape yields data that meet specific criteria. For example:

Price: less than ¬£100
Keywords: Trek mountain bike 

So every time a new listing is created for a Trek mountain bike that is under ¬£100, I would be notified instantly, within a minute or so of the listing being uploaded to Gumtree.

I already have the extracted data but I don‚Äôt know how to analyse it.

I am currently using Apify platform",[]
Add extra cols based on other cols using python pandas,https://www.reddit.com/r/webscraping/comments/yydjtv/add_extra_cols_based_on_other_cols_using_python/,webscraping,"Hi guys, i want to use python pandas to do the following.

i have a dataframe that i turned into an excel using pandas to\_excel().

my excel has the following two cols:

destination\_country | exporting\_country

012                                         048

012                                         048

012                                          048

i want to use pandas to add two more cols one called destrination\_country\_name and exporting\_country\_name and these cols values are dependant on whats inside destination\_country and exporting\_country

example:

if destination\_country == 012 then destination\_country\_name == 'Algeria'

same thing for exporting\_country\_name.

so how can i do that before converting my dataframe to excel? so i have the extra cols i need","['You can use [replace](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) and provide a dict with mapping. Here is an example:\n\n    import pandas as pd\n    df = pd.DataFrame({""a"": [1, 2, 3], ""b"": [4, 5, 6]})\n    a2c = {1: ""10"", 2: ""20"", 3: ""30""}\n    df[""c""] = df[""a""].replace(a2c)\n\nIt creates column `c` based on values in column `a` and applies mapping from `a2c` dictionary.\n\nBy the way, what does it have to do with webscraping?']"
Attempt to scrape a web page,https://www.reddit.com/r/webscraping/comments/yy72r6/attempt_to_scrape_a_web_page/,webscraping,"Is there a way to web scrape this page with REST requests? I've tried different ways but I can't get through the cloudflare  


[https://jkanime.net/](https://jkanime.net/)","['I\'d recommend using a webscraping API which is capable of bypassing Cloudflare. Here is a simple code snippet to scrape [https://jkanime.net/](https://jkanime.net/) using Scraping Fish API:\n\n    from urllib.parse import quote_plus\n    import requests\n    \n    API_KEY = ""YOUR SCRAPING FISH API KEY""  # https://scrapingfish.com/buy\n    url_prefix = f""https://scraping.narf.ai/api/v1/?api_key={API_KEY}&url=""\n    \n    url = f""https://jkanime.net/""\n    \n    response = requests.get(f""{url_prefix}{quote_plus(url)}"", timeout=90)\n    \n    # add your response processing/parsing logic\n    with open(""jkanime.html"", ""wb"") as f:\n        f.write(response.content)']"
Scraping Facebook group posts for email addresses made in the comments,https://www.reddit.com/r/webscraping/comments/yyatvv/scraping_facebook_group_posts_for_email_addresses/,webscraping,"I have a need to scan all of the comments on posts made in a given Facebook group (including clicking ""All Comments"" and ""show previous X comments"" to get all of the comments) to find email addresses posted in the comments. Users in these groups tend to post their email address as a comment and I want to capture all of them. Does anyone know an automated python script or something else that works?

I looked through GitHub and most of the scripts are outdated and I tried tweaking them but ran into other errors. I could code something custom but really just looking for something quick and free if anyone has a recent piece of code they can point me to that has worked for them?","['First of all - is this a private or public group? I do not want to say that you can\'t scrape private groups, but usually it is very hard to do it. \n\nAnother topic which you have to consider - if you plan to use your own account to see the content of the group (and if you want to interact with FB UI by clicking ""show all comments"") - Facebook can ban your account for non-human behaviour (clicking in show all comments in every post in 1 minute timespan is an example of that). \n\nIf it\'s public - I would first try some no code tools to do it.', 'https://github.com/kevinzg/facebook-scraper\n\nHave you tried this tool? \nIt works well also for scraping comments. \nIf the group is private and if you have a fb profile with this group access, you can use the profile for facebook login with the tool.']"
[Self Promotion] Data Marketplace Searching for People w/ Data To Become Sellers!,https://www.reddit.com/r/webscraping/comments/yy0en9/self_promotion_data_marketplace_searching_for/,webscraping,"Hi everyone!

We're a US based data marketplace startup (launched around 2 weeks ago) where businesses and individuals can sell their data. Since one of our target market for sellers is data scrapers (and we have founders that are long-time lurkers), we are reaching out to know whether anyone would be interested in signing-up as a seller, set your own price for your data and start listing it today! Think of us as a data storefront to manage your data related transactions. Let us know if you have any questions!

All the best,

Team @ Sellagen","[""I'm interested"", 'Yes please share details', 'Pls share details - immediately - thanks ! And good luck with your new venture', 'please share the details with me too. why the details are in PM? is this a private service?', 'Interested', 'I am interested.']"
I need Projects ideas,https://www.reddit.com/r/webscraping/comments/yy4s80/i_need_projects_ideas/,webscraping,"I'm building a bot, that scrapes that online and store it in a PostgreSQL database. Then Make the data available with a rest api with flask, i'm having trouble with coming up with some idea of the type of data/website to scrape, any suggestion would help.","['Pick a subject you like.  A band, a book, a movie, a video game, etc. and the scrape anything/everything related to that.', 'I\'m building one to scrape business/people/etc .. starting with neuly.com but would like to extend to crunchbase. This is for my day job, currently using a hodge podge of php scripts, one is essentially browserless jQuery in php. \n\nCurrently testing some stuff w/ crawlee though.\n\nedit: \nWhat makes it hard at neuly is the content is VERY unstructured see: \n\n    <div class=""row my-4"">\n\t<div class=""col-12"">\n        <div style=""max-width: 700px"">\n               <p>\n                    <strong>Summary:</strong><br>\n                    The Top Spot for Shrooms Online in Canada - We got everything for your trippy needs! Shroom Edibles, \n                    Dried Magic Mushrooms, Psilocybin Capsules, and more! Low prices, fast shipping! Buy Shrooms Online in \n                    Canada today!\n               </p>\n            </div>\n\t  </div>\n    </div>\n\nDoes look like they added a little more structure since last time I checked. Before the Description was just dangling wrapped in <p> tags.']"
1000$ if you can help scrap images,https://www.reddit.com/r/webscraping/comments/yy2aq2/1000_if_you_can_help_scrap_images/,webscraping,"I am willing to pay 1000$ to whoever can get me all product images for specific SKUs (mostly non available anymore in SheIn.com)


For example:
swvest44201202734  
swshorts04200527196  
swdress07200403365  
rg5786  
NC8301gold  
sk2203015018855010  
sf2110143763514492  
sn2112311287464378  
si2203040063060131


Notes:
1. I will need a script with the source 
2. I have one image link of any specific product what I need is the rest of links of that product images 
3. Images are available on that server in a specific directory that I know","['Dm', 'send me details in private', 'DM', 'Dm', 'Doable.\nDm', 'Previously I have scrapped, Dm me']"
"New to webscraping, should I better learn code or use code-free tools?",https://www.reddit.com/r/webscraping/comments/yxlv4y/new_to_webscraping_should_i_better_learn_code_or/,webscraping,"Hi folks!

Total newbie in web scraping. My (extremely small) knowledge in code limits to basic HTML and CSS.

Context: My company asks me to update the data bases that lists our competitors. I need information such as product name, if they have certain labels (that one is tricky as no one writes the norms in the same way), available sizes, product composition, warranty year and other specific terms to my market and so on. These informations need to be classified in an Excel spreadsheet.

I have never done data scraping but I am pretty sure there is something to do out there. Settling probably takes time as every website has a different layout. Also to note that my target websites are not really e-commerce websites. I work in B2B so it not possible to buy from, these are more ""display website"".

So my question today is, do you think it is worth investigating web scraping for this purpose ? 

If so, considering my (inexistant) knowledge in coding, is it better that I learn coding or use pre-made tools? Do you have any recommandations? 

I have been browsing some free tools (like chrome extension Web Scrapper) even tho I'm still extremely lost. For now, I'd like to keep using free stuff if possible. And once I know how to get things work, maybe I can show some early results to my manager and convince him to put some budget for it. 

Thanks in advance for your help! Any answer or piece of advice will be really helpful!","['Besides free no code tools (octoparse maybe?), you can try scraping APIs, like scrapingfish or others (Google ""scraping API""). \nMy experience with no code tools are not the best-usually they look good on presentation, but I have never get all the data I want, it\'s usually incomplete.\n\nScraping apis require some basic coding skills.\n\nBut first of all - investigate your target sites - what do you see in the source core (right click anywhere on the website and choose ""show source"". Do you see your data, van you just copy/paste it? Is data available on one page, or is there any pagination? How many websites you have to scrape? How often you will be getting data from these sites?']"
Buffer full error from calling an API too many times,https://www.reddit.com/r/webscraping/comments/yxpisa/buffer_full_error_from_calling_an_api_too_many/,webscraping,"    Traceback (most recent call last):
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connection.py"", line 174, in _new_conn
        conn = connection.create_connection(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\util\connection.py"", line 95, in create_connection
        raise err
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\util\connection.py"", line 85, in create_connection
        sock.connect(sa)
    OSError: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 703, in urlopen
        httplib_response = self._make_request(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 386, in _make_request
        self._validate_conn(conn)
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 1042, in _validate_conn
        conn.connect()
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connection.py"", line 358, in connect
        self.sock = conn = self._new_conn()
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connection.py"", line 186, in _new_conn
        raise NewConnectionError(
    urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000259E25E06A0>: Failed to establish a new connection: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\requests\adapters.py"", line 489, in send
        resp = conn.urlopen(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 815, in urlopen
        return self.urlopen(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 815, in urlopen
        return self.urlopen(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 815, in urlopen
        return self.urlopen(
      [Previous line repeated 7 more times]
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\connectionpool.py"", line 787, in urlopen
        retries = retries.increment(
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\urllib3\util\retry.py"", line 592, in increment
        raise MaxRetryError(_pool, url, error or ResponseError(cause))
    urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.macmap.org', port=443): Max retries exceeded with url: /api/results/custom-duties-by-year?reporter=682&partner=784&product=940520&year=2021 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000259E25E06A0>: Failed to establish a new connection: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full'))
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""c:\Users\system_int\Documents\DataportalCrawlers\MacMap Data\macmap3.py"", line 121, in <module>
        final_data = My_function(
      File ""c:\Users\system_int\Documents\DataportalCrawlers\MacMap Data\macmap3.py"", line 46, in My_function
        response = s.get(url, headers=user_agent, verify=False)
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\requests\sessions.py"", line 600, in get
        return self.request(""GET"", url, **kwargs)
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\requests\sessions.py"", line 587, in request
        resp = self.send(prep, **send_kwargs)
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\requests\sessions.py"", line 701, in send
        r = adapter.send(request, **kwargs)
      File ""C:\Users\system_int\AppData\Roaming\Python\Python310\site-packages\requests\adapters.py"", line 565, in send
        raise ConnectionError(e, request=request)
    requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.macmap.org', port=443): Max retries exceeded with url: /api/results/custom-duties-by-year?reporter=682&partner=784&product=940520&year=2021 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000259E25E06A0>: Failed to establish a new connection: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full'))

im getting this error when im trying to get data from an api and appending that data inside a dictionary.

then at the end im turning that dict into a dataframe using pandas to convert it to\_excel()

im getting the above error after 16334 call.

how can i fix that? ive tried the following:

    url = f'https://www.macmap.org/api/results/custom-duties-by-year?reporter=[dest]&partner=[expo]&product=[hs]&year=[yr]'
    # retry connection in case of error. you can also check all codes using re.status_codes._codes
    retries = Retry(total=10, backoff_factor=1200, status_forcelist=[
    				500, 502, 503, 504, 509])
    # mount the requests of our url with retries
    s.mount(url, HTTPAdapter(max_retries=retries))
    # try response from url
    try:
    	response = s.get(url, headers=user_agent, verify=False)
    # if response is ok get the data
    	if response.status_code == 200:
    		all_data = response.json()
    except ConnectionError as e:
    	print('Error : ' + e)
    	print('status code: ' + response.status_code)
    	print('content: ' + str(response.content))
    	time.sleep(800)
    	response = s.get(url, headers=user_agent, verify=False)
    	if response.status_code == 200:
    		all_data = response.json()

my exception is not catching the error and i tried to set backoff\_factor=1200 so to wait 20 mins incase a call fails but im still getting this same error.

what can i do?","['I\'m pretty sure it\'s port exhaustion... You are probably running out of ports to make calls with. The first link has some suggestions for fixing an ephemeral port exhaustion problem. You can also Google ""Python port exhaustion"" for a solution that works well with your project. Hope that helps!\n\nReferences:\n\nhttps://making.pusher.com/ephemeral-port-exhaustion-and-how-to-avoid-it/\n\n https://stackoverflow.com/questions/35800131/python-requests-ephemeral-port-exhaustion\n\nhttps://stackoverflow.com/questions/4415175/an-operation-on-a-socket-could-not-be-performed-because-the-system-lacked-suffi\n\nhttps://kb.vmware.com/s/article/2075305#:~:text=Register-,%22An%20operation%20on%20a%20socket%20could%20not%20be%20performed%20because,error%20in%20vCenter%20Server%20(2075305)&text=This%20issue%20occurs%20when%20the,operating%20system%20have%20been%20exhausted.', ""Your f string in the 'url' variable needs curly braces (i.e. {x}) instead of square brackets (i.e. [x]). Try that and then repost new errors so we can see what changes. Good luck!""]"
Google web scrape from excel list?,https://www.reddit.com/r/webscraping/comments/yxbfy0/google_web_scrape_from_excel_list/,webscraping,"Is it possible to take a search term from a column in excel, have the bit search google and then scrape the standard google tables that come up over and over? I‚Äôve seen bots for google maps places but not for other tables and not for search terms from an excel sheet‚Ä¶","['If I understand correctly, you want to scrape Google SERP. If so, here is a simple python code snippet using Scraping Fish API for one keyword. You can read a list of keywords from excel column and loop over it.\n\n    from urllib.parse import quote_plus\n    import requests\n    \n    API_KEY = ""YOUR SCRAPING FISH API KEY""  # https://scrapingfish.com/buy\n    url_prefix = f""https://scraping.narf.ai/api/v1/?api_key={API_KEY}&render_js=true&url=""\n    \n    # to get uule for location you can use: https://github.com/ogun/uule_grabber\n    # or https://site-analyzer.pro/services-seo/uule/\n    uule_usa = ""w+CAIQICIDVVNB""\n    \n    keyword = ""kitchen sink""\n    search_url = f""https://www.google.com/search?q={quote_plus(keyword)}&uule={uule_usa}&gl=us&hl=en""\n    \n    response = requests.get(f""{url_prefix}{quote_plus(search_url)}"", timeout=90)\n    \n    # add your response processing/parsing logic\n    with open(""google.html"", ""wb"") as f:\n        f.write(response.content)']"
Challenging task. NEED HELP,https://www.reddit.com/r/Automate/comments/10fcdna/challenging_task_need_help/,Automate,"I am looking for a service or way to have people text a SMS number in whatsapp and have an automated response/reply that ask for a sequence of information in response to user input. I also want it to record the user responses into some kind of file or excel sheet.

&#x200B;

What is this called? is there a way to create this?

&#x200B;

If I didn't explain it well, what I am imagining is like a customer support automated text feature where you text a number enter in your name then the automated machine ask for your email, then you send it, then it ask for your order number etc. I need something like that over whatsapp and for the responses to be recorded.","['Not 100% clear whether you\'re looking for something working as you state ""out of the box"" or whether you\'re capable of doing a little programming. Either way, Twilio is one company that will allow you to programmatically SMS. If you\'re not into coding, you can probably get someone from fivver to make you something that will do it for a reasonable cost.', 'ChatGPT + Python + Twilio.']"
How can I automate sending email to recruiters?,https://www.reddit.com/r/Automate/comments/10ew1x4/how_can_i_automate_sending_email_to_recruiters/,Automate,"I've a list of companies(around 1000),  I want to apply jobs for. I want to attach my resume to the email. And a letter telling I want internship at some positions.

What'll be same?

1) Cover letter


2) Attached resume

What'll be different?

1) Email address","['Easy. Microsoft outlook excel and word. Mail merge toolkit (or something similar). You can even customize the cover letter based on fields in excel', 'Where did you get a list of 1000 recruiters?', 'You could use the Mail Merge plugin in Thunderbird: https://addons.thunderbird.net/en-us/thunderbird/addon/mail-merge/', 'Mailmerge.']"
Sending commands to a speaker using a programming language,https://www.reddit.com/r/Automate/comments/10eb9fd/sending_commands_to_a_speaker_using_a_programming/,Automate,"I want to create a custom alarm clock that is controlled over the internet running on a remote server (probably a lambda on a schedule). Ideally I need a speaker which I can send commands to over the internet, unless I‚Äôm missing something. Current options I‚Äôve thought about but seem like a pain in the ass to implement:
- Hacking a google/apple/Amazon device and sending it a command somehow (probably a locally hosted web server, but this defeats the whole thing of running it in the cloud).
- speaker that has an api interface that I can access if I allow it and has some sort of encryption

Any ideas?","['Use the Google search term ""IP Speaker"" where IP stands for ""Internet Protocol"".  This is a speaker that will accept and use an IP address.\n\nexample: https://www.amazon.com/Algo-8186-Paging-Speaker-Ringer/dp/B01G2O5A70/ref=sr_1_3?crid=2J6OEUEHZMVH3&keywords=ip+Speaker&qid=1673975291&sprefix=ip+speaker%2Caps%2C134&sr=8-3&ufe=app_do%3Aamzn1.fos.c3015c4a-46bb-44b9-81a4-dc28e6d374b3\n\nAlternatively, you could also use a bluetooth speaker and connect it to a computer. This would probably be the easy method for what you\'re trying to do. This way you could send your commands to an actual computer rather than trying to figure out how to essentially build your own audio drivers for some random device.', 'Maybe something with a raspberry pi?\n\nhttps://www.makeuseof.com/create-your-own-privacy-friendly-voice-activated-raspberry-pi-smart-speaker-with-mycroft/']"
Google Reveals Its Answer To OpenAI's ChatGPT From DeepMind | Deepmind DreamerV3 General AI | AI Powered Robotic Exoskeleton,https://youtu.be/dvKFiWJAO64,Automate,,
automatically changing TV brightness.,https://www.reddit.com/r/Automate/comments/10ecfb9/automatically_changing_tv_brightness/,Automate,How can I automate my TV to change to yellow mode or less bright? Like through a iPhone routine/ smart home automation,"[""Perhaps something like Logitech Harmony would be a solution? You could script the remote button sequence. Haven't played with one myself but seems like it could work.\n\nEdit: Apparently discontinued but maybe something along those lines."", 'TV or monitor? I just want to clarify']"
Custom shortcuts to automate access AI on your Mac,https://www.reddit.com/r/Automate/comments/10eau6v/custom_shortcuts_to_automate_access_ai_on_your_mac/,Automate," 

Hello folks,

I'm always amazed by the power of GPT-3 and Open AI.

This post is a combination of both information and promotion, so please bear with me.

I've always wanted to use AI directly on my phone and computer, without having to go to OpenAI's playground or ChatGPT in the browser.

For that, I created a tiny Mac app called Elephas. I have shared the app in this group [in the past](https://www.reddit.com/r/Automate/comments/1027x84/using_gpt3_to_automate_content_repurposing_for/) as well and got some amazing feedback from the members :)

Since then many users have asked for the ability to add custom commands in the app, that they can use to invoke AI on their computer.

So I've just shipped a feature called ""Snippets"".

With this, **now you can assign custom shortcuts to OpenAI prompts and use them in your day-to-day workflow wherever necessary.**

This is how it works -

https://reddit.com/link/10eau6v/video/401eus15ilca1/player

 There are many more such utility features that can help you get the power of AI in your daily work.

You can get the app here - [Elephas](https://elephas.app/?ref=rAutomate-snippets)

You can try it out for FREE for 7 days.

Appreciate your feedback.

Do let me know any new features that you would like to see in the app.

Thanks",[]
"Book about the impact of AI in society, employment, etc...""Beautiful tsunami: understanding and thriving in the age of AI"", by Javier Marti. Talks about where we are and where we are going, and how it'll affect all of us, including potential bad outcomes",https://javiermarti.co.uk/JM_website_2016/other/BOOK_Beautiful%20tsunami_AI_Javier_Marti.pdf,Automate,,
r/CodeFight,https://www.reddit.com/r/Automate/comments/10eo64h/rcodefight/,Automate,"The Internet is changing, fast. Time to level-up. And take the fight for digital and cognitive liberties to the enemies of humanity.

Join us at r/CodeFight!","[""After reading this and visiting that subreddit I still have no clue what it's for.  level-up what?"", 'if this is about AI generated content, bro thinks there‚Äôs gonna be an epic code-off showdown between AI and humanity üíÄüíÄüíÄ\n\nit‚Äôs either AI gets stagnant all of a sudden and nothing ever happens, or they develop sentience and murder all of humanity, but the best middle ground is that they develop sentience and try to coexist with us\n\nbut i assure every artist out there, imma be honest and upfront, **i cannot bust a nut to AI generated hentai**, up to this point there‚Äôs still little imperfections with AI generated artworks that i **physically cannot keep my dick hard** when i see them, and they all look same-same\n\nso if y‚Äôall ever feel like there‚Äôs no customerbase left for you, i‚Äôm still here and willing to pay for my custom hand-drawn anime milf hentais aight?', ""What is the point of this? There are dozens of more specific learn to code subs (e.g. learn sql, python, c, c#, java, css, etc. etc. etc.). Then from there are more formal subs for those that generally know e.g. r/Python. And then there are more generic things like r/learnprogramming and a handful of similar. \n\nWhat is an enemy of humanity? Is this some rogue state? AI? Aliens? Rogue states are still humans and aliens, if a threat, are unimpressed by your coding since they apparently have advanced space flight (or are not a threat). So... AI? You going to learn to speak in some form of machine language better than an AI that is an enemy of humanity? What's next water pistols to fight the ocean?""]"
The everything market app,/r/CyberStasis/comments/10e7mco/the_everything_market_app/,Automate,,
Recommendation for automate,https://www.reddit.com/r/Automate/comments/10e1uc9/recommendation_for_automate/,Automate,"I‚Äôm currently a legal Intern and my boss has graciously given the task  to summarize 300+ legal cases by June, I‚Äôd like to know if I can automate this task, by either using a website or by purchasing a software. For any how do I go about doing it. Thanks a lot in advance!!!!!","['One option to check out: https://detangle.ai/\n\nMay be worth getting on their beta and trying it with one case, and see if it gives good results.', ""I feel like a text expander would be a good option here. Depending on your platform, there are lots of good ones. I prefer Text Blaze, but it isn't available on mac yet""]"
Harvesting and bunching radishes,https://gfycat.com/happygoluckybriefeasternnewt,Automate,,
"Hot take: AI and ChapGPT are not ready yet, text expanders > AI",https://www.reddit.com/r/Automate/comments/10dfb32/hot_take_ai_and_chapgpt_are_not_ready_yet_text/,Automate,"AI has a lot of promise, but I feel like they should only be used for idea generation right now. They're just too new to actually use for real work and I've seen many cases where they just aren't as accurate as they could be. They definitely could get there one day, but they aren't ready yet. 

Personally, I prefer to use a text expander (automates typing using pretyped phrases) to automate my writing. Text expander, Text Blaze, and aText are solid options. They aren't AI like ChatGPT, but they can help you automate without the risk that AI has right now. 

Just my thoughts, as I see people post ""Is ChatGPT the future"" every day in every single thread. Looking forward to everyone's thoughts!","['I feel you, but I‚Äôm also using Espanso and have been playing with OpenAI api to add flair to the expansions.\n\nhttps://maxziebell.de/2022/12/21/setting-up-an-ai-expansion-using-espanso/\n\n(Not my blog, but I did stumble on it when setting my own stuff up)']"
Is Chat GPT the future?,https://www.reddit.com/r/Automate/comments/10ctghc/is_chat_gpt_the_future/,Automate,,"['the future of what?', 'For automation? No absolutely not. Asked it a few coding questions it failed. It gives out pretty believable output but in practice you still have to fit it into whatever use case you have beyond a simple example. I did enjoy discussing art thefts with it though. As a random conversation buddy you ask questions its pretty fun. Ps it has no connection to the internet, so cannot look up anything for you.', ""ChatGPT is the future like one level higher than Alexa was the future in Fall 2014.  It's the best in its space, hands down (asking it to compose songs about various topics is quite amusing), and will probably serve as a foundation upon which various other services, apps, etc., can be based on... That said, ChatGPT4 is said to be on the way and supposedly will be just as remarkable of an advance above ChatGPT3 once experienced."", ""It has a 3 in the name, so probably not. I'd guess it will be part of the present until version 4, which will follow a similar path.\n\nAfter a few weeks of having it around and using it for work (FT law, PT software) it works as a fair assistant. One of my favorite uses is using the normal windows voice to text where I can bumble around inaccurately in my speech, but then also tell ChatGPT to rewrite it as a coherent letter. I do a similar thing with coding issues. Both ... maybe save time mostly."", 'I think ChatGPT and other tools similar to it will have a large role in many applications in the future.  I personally hate talking to many of the chat bots that are used as an initial troubleshooting tool for many companies.  I was chatting with Bluehost chatbot about my website being down recently and it literally couldn\'t understand what I meant when I said ""my website is down"" and I had to rephrase. And it never helped me anyway - I had to call them up and talk to a real person.  \n\n&#x200B;\n\nI think that AI will be able to troubleshoot many of the common problems that people have with various software, but it will take time and having real humans checking the answers before it will be trained enough to solve problems reliably on its own.  You wouldn\'t want it going in and making something worse by its ""solution"".', ""Also, here's ChatGPT's response to this question:\n\nChatGPT is a tool that is part of the current state-of-the-art in natural language processing. It has the ability to generate human-like text, which has many potential applications in fields such as customer service, content creation, and language translation. However, it's important to note that the field of artificial intelligence is constantly evolving, and new developments may supersede the capabilities of ChatGPT in the future.""]"
New Research From Google Shines Light On The Future Of Language Models ‚≠ï,https://www.reddit.com/r/Automate/comments/10bq9gp/new_research_from_google_shines_light_on_the/,Automate,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs might allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* reduce hallucination and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example is words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (a proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/n7svd95hv0ca1.png?width=800&format=png&auto=webp&v=enabled&s=282b88e4c256236b65b23de9b8ac392abbb4e656)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week.No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)","[""It looks like OP posted an AMP link. These should load faster, but AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).\n\nMaybe check out **the canonical page** instead: **[https://twitter.com/richvn/status/1598714487711756288](https://twitter.com/richvn/status/1598714487711756288)**\n\n*****\n\n ^(I'm a bot | )[^(Why & About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon: u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)""]"
The misuse of AI is the familiar promise one thing and deliver something else,/r/CyberAutonomy/comments/10bnig4/the_misuse_of_ai_is_the_familiar_promise_one/,Automate,,
Review of ChatGPT/AI for LinkedIn for marketing,https://www.reddit.com/r/Automate/comments/10bf7sg/review_of_chatgptai_for_linkedin_for_marketing/,Automate,"Recently, I found a Chrome extension called Engage AI. I've been using the tool for the past week or two. I wanted to share my experience with it.

The Chrome extension essentially writes comments for you for LinkedIn posts. All you have to do is copy the link of a post and paste it into the extension. The app scrapes information from the post and comments, then it generates a related comment for you using AI.

The comments generated by the extension are accurate and have perfect spelling and grammar. They sound genuine, human, and high-effort, far better than generic comments such as ""Great post"" or ""Thanks for sharing.""

Unfortunately, the comments aren‚Äôt always accurate. Occasionally, the app will generate a comment that doesn‚Äôt make much sense with the post and I need to refresh it. It‚Äôs also a shame that it‚Äôs specific for LinkedIn and no other platforms, so you‚Äôre out of luck if you don‚Äôt use LI.

It's not for everyone, but for someone like me who uses LinkedIn to build relationships with leads for my employer, it's a great help. I feel like VAs could benefit a lot from this, and anyone else trying to stay in front of your target audience on LinkedIn.

Personally, I comment on prospects' posts on a weekly basis. It can take me hours to come up with, say, twenty insightful and well-thought-out comments. With this extension, those hours are reduced to just a few minutes. I've gotten positive feedback from the comments I've made with it so far.

Overall, the app is a great time-saver and saves me the effort of needing to write a comment that makes sense in a field I‚Äôm not always familiar with. I‚Äôm already on the paid plan, but there‚Äôs a free trial if you want to test it out.

If anyone else has tried it, I would love to hear your thoughts.","[""Yeh I tried using ChatGPT recently for an article, asking it to add some stats and cite publications.  Everything it provided sounding exceptionally convincing (Authors, papers, dates, pages) - except that it was all complete BS.   If it wasn't for me fact checking, I could have easily posted the article.  \nIf we thought misinformation and fake news was bad now, the coming years will be horrible.  I actually don't understand how we will stop it from polluting everything.  In many senses it is going to be more important than ever to be a critical thinking human."", ""What a nightmare world we're entering."", ""I've found the openAI models are good for seeding ideas.  Generally trying to have it write up an entire article is a bit hit or miss factually and the writing style is clearly automated. Human intervention is still required at this point to make it sound good.\n\nThere is also the issue of niche topics. The more narrow your focus and less widespread the information on it, the more likely it is to almost directly copy source material.\n\nIt did offer me a nice poem about unmanaged switches in the style of E. E. Cummings that gave me a chuckle during my Livestream this week."", 'Interesting thank you.  Now I‚Äôm wondering whether it can be used to engage with Reddit posts‚Ä¶.']"
"A.I. APIs Landscape 2023 for Speech and Vision (image and video) analysis, Natural Language Processing and Automatic Document Parsing",https://i.redd.it/51lp4sz9msba1.gif,Automate,,
OpenAI Announces New ChatGPT PRO Version And Watermarking Tool | New Samsung Robot Powered By Artificial Intelligence | Breakthrough Robot Gripper Resembles Elephant's Trunk,https://www.youtube.com/watch?v=tAQOhsaCz8s,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Performance of Automation Anywhere v.26,https://www.reddit.com/r/Automate/comments/10avwt7/performance_of_automation_anywhere_v26/,Automate,"Hi, Have you used the latest version 26 of Automation anywhere ? How's it faring compared to the previous AA versions, what better services are being offered ?",[]
New to UI Automation: Seeking Recommendations for Learning UI.Vision,https://www.reddit.com/r/Automate/comments/10anuy5/new_to_ui_automation_seeking_recommendations_for/,Automate,"Hey everyone, I'm new to the world of UI automation and I'm interested in learning more about UI.Vision. Can anyone recommend some good resources or courses for a beginner to get started with UI.Vision? Are there any particular websites or places that you've found to be particularly helpful in learning this technology? Thanks in advance for your suggestions!",[]
Trying to get saved Instagram posts into my Notion databases. Anyway to IFTTT++ it anywhere?,https://www.reddit.com/r/Automate/comments/10ai3jy/trying_to_get_saved_instagram_posts_into_my/,Automate,"Whenever I'm on Instagram, I hit the save/bookmark to denote that I'm interested in referencing something later.

I would love to find a way that when I hit save, the link to that Instagram post gets sent somewhere, anywhere. I'm particularly looking to put it in my notes database, but even if I can get it to a Google sheet, a list app, a doc, whatever, I could automate from there.

Unsure what to do other than manually copy pasting the link and describing it myself.

Note:
Instagram does have a saved items folder system, but the folder process for saved posts can only be utilized when you are saving a post from your feed, and not from inside a reels, or from the Saved section, but then it doesn't tell you which of your posts are not in folders. (imagine having your Gmail All Mail section, and your folders, but no inbox).","['Have a try with Telegram, I know a lot of bots being built to send things there']"
"Recently, I saw a store on the AliExpress platform to buy this product very cheaply. Has anyone used his product? Can you share your experience, thank you very muchÔºÅ",https://i.redd.it/g0fb84ao2lba1.png,Automate,,
Is there an AI that I can train to illustrate my photos in a specific style?,https://www.reddit.com/r/Automate/comments/10aioqh/is_there_an_ai_that_i_can_train_to_illustrate_my/,Automate,"We own a business where people send us their photos to be illustrated in a specific style. Is there an AI tool I can train to start automating this instead? It could help us save a lot of time and money. 

Here are the final [illustrations](https://imgur.io/a/XRmq2AC) - I don‚Äôt want to share the original photos but I‚Äôm sure you could imagine them, too.","['Number of examples and budget?', 'Possible? Absolutely. Like /u/charlesrwest asked, budget and number of examples are a huge factor here. Training data will likely be your limited resource. Not sure how deep a catalogue you have but ai models are hungry.\n\nRather than shooting for 100% photo to final product automation I would probably consider just trying to streamline the process as much as you can, working on individual components of the process, and then stitch it together.\n\n- Background removal\n- Automatic masking of eye features\n- Posterization of general image\n- Isolation of clothing, henna, etc, and higher res posterization of that.\n- Text is straightforward. Could automatically suggest colour based on clothing/contrast evaluation.\n\nEven if these things are done individually and manual work can get you the last 20%, might be the most cost effective solution.', ""Yes, try the deep dream generator first. https://deepdreamgenerator.com/\n\nIf you're technically inclined, the cutting edge is using Stable Diffusion with a custom trained model. But you need a powerful system to run it.""]"
An AI wrote this guys video,https://youtu.be/S7tQGgdpEGQ,Automate,,
Google Calendar events based on Outlook events,https://www.reddit.com/r/Automate/comments/108i6ja/google_calendar_events_based_on_outlook_events/,Automate,"Would anyone know a way to create events on Google Calendar if a new event has been added to Outlook Calendar? It's not an invite or in any way actually ""connected"" ‚Äì basically if an event is created in Outlook, it needs to book the same time in GC with any generic title.","['Needed to do this at a previous org and found this tool worked best - https://calendarbridge.com/pricing/', 'I have been using [this](https://github.com/phw198/OutlookGoogleCalendarSync/) for a couple of years, works like a charm.', 'Zapier']"
Breakthrough Artificial Intelligence Learns To Use Robotic Arm Better Than OpenAI + Google With Reinforcement Learning | New AI Humanoid Robot | New 3D Scene Synthesis AI,https://youtu.be/PW-CO-x1Yrk,Automate,,
I created a program that downloads multiple inputted videos from Instagram and TikTok without a watermark,https://github.com/PabloEscobar1337/tiktok-and-instagram-content-downloader,Automate,,
Intelligent Document Processing,https://www.reddit.com/r/Automate/comments/107n2wu/intelligent_document_processing/,Automate,"Hi!

I am trying to learn about Intelligent Document Processing

Need to build a automation tool to find some words in documents in pdf/word format

Make a check list about what was found 

These documents are digitalizations from a scanner 

Some documents have 900 pages or more, and some have bad quality digitalizations from decades ago (probably need to setup a database for each word)

I know there's several which can do that job, but I am looking for something more accessible, these available are too expensive targeting enterprises 

Any guidance would be very helpful!",[]
The Lazy Productivity Script - A tool that uses OpenAI‚Äôs Whisper and GPT-3,https://twitter.com/AllAbtAI/status/1612317768132558849?s=20&t=4RTBzL77gEOEwgyINkHKzA,Automate,,
Lane following autonomous model car,https://www.reddit.com/r/Automate/comments/107idgh/lane_following_autonomous_model_car/,Automate,"We have to develop a model car that should drive autonomously with the help of floor markings. The 2 floor marking lines are about 30cm apart and can have different colors, i.e. not just white or yellow. Do any of you know an open source project that already implements this or any tutorials I can start with. I don't have the training data to train an AI myself.

As hardware we have a Jetson Nano 4GB Ram, an Intel RealSense D435 Depth Camera and 4 mecanum wheels","['Is this for Amazon Deepracer?', 'Lego robotics had a kit that did this.  Easy to program, used light sensors to guide it along any path you make with tape on the floor.']"
Should I learn powershell or automation tools (Automation 360 & UiPath)?,https://www.reddit.com/r/Automate/comments/105x8ay/should_i_learn_powershell_or_automation_tools/,Automate,"I already know python, I've done some web scraping using it and automated pulling sales data from multiple websites.

I also know SQL, and recently tried using some automation tools such as Automation Anywhere (didn't like it though).

And I'm wondering whether learning powershell would be good for a career in automation, or is it that RPA tools are mostly used in automation?

Thanks in advance.","['Powershell if you eant to work in cloud/devops, uipath if you want to go the RPA route. RPA is a type of automation, but it is not the only way.  With sql and oython you can work in both, it depends more of your final destination.', 'ignore Powershell, its garbage, go with bash']"
Leetcode obsolete?,https://youtu.be/ntKqPEaqso8,Automate,,
New York City‚Äôs education department bans students and teachers from using ChatGPT.,https://medium.com/inkwater-atlas/new-york-citys-education-department-bans-students-and-teachers-from-using-chatgpt-243ef0507f84,Automate,,
Does an AI email assistant exist?,https://www.reddit.com/r/Automate/comments/104vpbv/does_an_ai_email_assistant_exist/,Automate,"Hey everyone,

What I'd ideally like is a kind of AI inbox assistant.

I get \*so many emails\*. I want them filtered into 'Reply now', 'Important', 'Read later', 'Not needed'.

The ideal case is that they would be further categorised by the kind of action to be taken. Then with an AI language model, a draft is there to send - e.g. when someone asks for a call, there are two drafts there: no thanks, and a yes with a link to my calendar. 

Or when a customer asks the same question as has been asked 100s of times before, the draft answer is there to send with 1 click.

Does this exist?","['Oddly enough had this conversation with a friend of mine who works in AI. He sent me this might help?\n\nhttps://ellieai.com/', 'Hey,\n\nA close one, more than an email assistant, it works as a social media assistant as well. Though you cannot save templates, it can write a contextual answer as it takes hints.\n\n[https://www.reddit.com/r/ProductivityApps/comments/zu5lzi/an\\_ai\\_reply\\_assistant\\_for\\_emails\\_and\\_social\\_media/](https://www.reddit.com/r/ProductivityApps/comments/zu5lzi/an_ai_reply_assistant_for_emails_and_social_media/)', 'I haven\'t seen what you\'re asking for, but I know the problem, and I have had a noodle with OpenAI\'s Playground to see if I could train a model.  I called it ""Busy Man\'s Email"".\n\nI dropped a load of my own emails, wrote what the preferred response would be, and it did really well.\n\nI got it to check if the email was from a person or an automated email system (people weighted higher in certain categories). It output a less-than-a-tweets characters summary of the email plus  Classification, Resource Required (i.e. banking, email, web, physical item, phone etc.), Urgency, Action Required Status etc., \n\nIt did a pretty good job. My own idea was to get it so that I could have only \'action required: yes | Urgent\' notifications, and then have a list of the summaries of emails sent to me to review as I have opportunity.  I could also then just ask for all \'banking/finance\' messages to be shown so I could then just do all of that in one go, and likewise all \'email response required\' - keep everything silo\'d if it isn\'t super important/urgent.\n\nIt would be a piece of cake to get the AI to write a response to the email (it excels at that), but writing your-industry-specific stuff would be a lot harder as the fine tuning (the little I know about it) would take a __lot__ of data and a relatively high cost. Fine tuning OpenAi\'s models is quite expensive. Personally I\'d absolutely pay for it if I thought I had a snowball\'s chance of being able to integrate it properly!\n\nHeck, if anyone wants to work with me on it I\'ll stump up the cashola!\n\nIf I could actually code then I would be building this myself.  I __am__ the perfect case for testing!']"
OpenAI to Hit $29B Valuation After Latest Share Sale,https://metaroids.com/news/openai-to-hit-29b-valuation-after-latest-share-sale/,Automate,,
What Job category does warehouse automation fall under?,https://www.reddit.com/r/Automate/comments/104yltw/what_job_category_does_warehouse_automation_fall/,Automate,"https://youtu.be/4DKrcpa8Z_E

Hello you all. Saw this video. I think it would be interesting to work in a field similar to this. 

Where would I begin looking? PLC or another field to work on projects like this?

Thank you","['Based on this, you may be interested in being a Controls Engineer, Controls Technician or joining an Operational Technology team.', 'You will use PLCs for this type of work. Education wise, if you are going for a bachelor‚Äôs, an EE degree is what you‚Äôre looking for. The role you‚Äôd be in is called a Controls Engineer or Tech like the other commenter mentioned. This entails a mix of both mechanical and electrical system design and integration.\n\nI‚Äôm a controls engineer who just started his own firm selling quality testing machinery. If you have any questions don‚Äôt hesitate to ask!', 'Can confirm, the earlier comments are right on the money. I am a controls engineer at an automation OEM.']"
83 People Know the Truth | How To Automatically Update Youtube Video Title?,https://youtu.be/FPL_qfl9M9c,Automate,,
"Browse AI is the first AI-powered web automation software that learns to perform data extraction, monitoring, and automation tasks on the web. Get 200 Credits",https://browse.ai/?via=free,Automate,,
New Nvidia AI Robot Simulation Tech + Breakthrough Google Muse Artificial Intelligence,https://www.youtube.com/watch?v=VETMzQi1-UY,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Snake game built automatically with ChatGPT,https://youtu.be/DvqE2iYWyfk,Automate,,
Meet GPTZero: The AI-Powered Anti-Plagiarism Program,https://medium.com/inkwater-atlas/meet-gptzero-the-ai-powered-anti-plagiarism-program-4a6ac41ea0d7,Automate,,
"As AI tools have quickly become a controversial topic in the game industry, modl.ai aims to make AI tools that support developers, not supplant them. Get more details here:",https://www.reddit.com/r/Automate/comments/103wd0o/as_ai_tools_have_quickly_become_a_controversial/,Automate,[https://www.gamedeveloper.com/programming/using-ai-bots-as-game-development-tools-not-replacements-with-modl-ai](https://www.gamedeveloper.com/programming/using-ai-bots-as-game-development-tools-not-replacements-with-modl-ai),"['I would like to see a game that uses A.I. in game to create new map areas, storylines and NPCs.\n\nEssentially, the developers could build a base game with X number of quests, then the A.I builds more quests based on the player‚Äôs interactions and play style.']"
Get ready for Bing Search with ChatGPT!,https://medium.com/inkwater-atlas/get-ready-for-bing-search-with-chatgpt-fb255d63c5f,Automate,,
Memory Match Game Automatically coded in python,https://youtu.be/knm4KOaeSxU,Automate,,
"Nvidia VS Microsoft : Breakthrough 3D Avatar Creator AI Turns Text, Images, and Video Into Realistic Avatars",https://www.youtube.com/watch?v=yZ3RtunGJUU,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
What to do when hyperparameter tuning doesn‚Äôt improve model performance?,https://www.reddit.com/r/Automate/comments/1027o9d/what_to_do_when_hyperparameter_tuning_doesnt/,Automate,,"[""Check the data for biases or unbalances, try to get more data, try different models, make sure you aren't get stuck on local minima for your tuning""]"
Using GPT-3 to automate content repurposing for social media,https://www.reddit.com/r/Automate/comments/1027x84/using_gpt3_to_automate_content_repurposing_for/,Automate,"Hello folks,

It's crazy how versatile and powerful GPT-3 and Open AI are.

This post is a combination of both information and promotion, so please bear with me.

I got some great feedback and support from the members of this subreddit on [my last post.](https://www.reddit.com/r/Automate/comments/z7sofx/using_gpt3_to_reply_to_automate_email_replies/) So sharing a post here again.

Many of our users had been asking for the ability to repurpose their existing blog and newsletter content into social media posts.

They are mostly busy content writers so this can be really useful to them in their day-to-day work.

So I tried a simple prompt - ""Summarize this for a tweet""

I took the content from¬†an [OpenAI Blog](https://openai.com/blog/our-approach-to-alignment-research/) and summarized it into a tweet.

&#x200B;

https://preview.redd.it/lro33pbout9a1.png?width=800&format=png&auto=webp&v=enabled&s=641e3cbac828c2e0e4b5c4ff06a2568b3ec424e5

 

Next, I tried another prompt - ""Summarize this into a LinkedIn post""

And that worked alright as well.

&#x200B;

https://preview.redd.it/6rf7k1pput9a1.png?width=800&format=png&auto=webp&v=enabled&s=a8f0d633897284bff66e55a0992afe8be800c44e

 

Finally, I tried this prompt - ""Summarize this into a Facebook post.""

&#x200B;

https://preview.redd.it/3bpnuh4sut9a1.png?width=800&format=png&auto=webp&v=enabled&s=48d77576627b8acc3c78dea5b93232ff6b891f83

 

These prompts worked well so I decided to integrate them into our Mac app, and the users loved it.

Here is the final demo of how it works inside my app -

&#x200B;

https://reddit.com/link/1027x84/video/m8qtr6rtut9a1/player

 

It can be difficult to copy and paste the content into the playground.

If you have a Mac and want to do this more straightforward way then please try out my app [Elephas](https://elephas.app?ref=rAutomate-socialrepurpose)

I have built many such utilities into the app to help you use AI on a daily basis.

You can try it for free for 7 days.

Do share your feedback.

Hope you find it useful

Thanks",[]
Hangman coded in 3min 30 seconds automatically with ChatGPT,https://youtu.be/_F2jAoeZMiA,Automate,,
How to Stay Relevant in a World Full of Smart Bots?,/r/OurGreenFuture/comments/102juid/how_to_stay_relevant_in_a_world_full_of_smart_bots/,Automate,,
ChatGPT: Why It‚Äôs Not a Threat to Google Search,https://medium.com/inkwater-atlas/chatgpt-why-its-not-a-threat-to-google-search-639c2f915f53,Automate,,
How to automate the sharing of posts from a facebook page to multiple facebook groups?,https://www.reddit.com/r/Automate/comments/1028csc/how_to_automate_the_sharing_of_posts_from_a/,Automate,"I'm dumb and have limited resources. 

I've seen it was somewhat possible to do with ""make"" (former intergromat) but I really don't know how. 

I just want to share posts from one page across multiple groups. Do you have any idea how I can do this?",[]
Stepper motors with built-in homing sensor/permanent zero index?,https://www.reddit.com/r/Automate/comments/101tzgh/stepper_motors_with_builtin_homing/,Automate,"I am after a stepper motor with a built-in sensor so that I can home multiple stepper motors to the same rotational position (relative to the flattened shaft). I have previously done this with dual-shaft stepper motors with a cam and switch but that was too finicky.

I am looking for an off-the-shelf solution and looked into using the zero index of an encoder. But most stepper motors with encoders are only aligned by eye when installed so the zero index position would vary between motors.

Does anyone know of a stepper motor that has a permanent zero index machined onto the shaft or something similar?

Thanks!","['You can also buy servo motors that are controlled by step and direction.  These can have additional features such as homing modes, etc, with the simplicity of steppers for input.', 'RemindMe!', 'There is a startup company that has this. But it is not common in the market yet.  https://www.vincentgroenhuis.nl/wp/home/2022/09/04/vincents-research-videos/\n\nOther than this you can only go for conventional add-on of an absolute encoder', 'AMCI makes nice steppers with the motor diver and controller built into the stepper. Has the I/O and encoder built into the stepper body. \n\nhttps://www.amci.com/plc-automation-products/smd17e2-networked-series-integrated-stepper-motor-controller-drive/#!/243\n\nThey have NEMA 17 and up and have Ethernet/IP and Profinet Ethernet comms.', 'Hello Realistic_Oven_5349, \n\nI‚Äôm an engineer for Teknic ‚Äì thanks to those who commented about our products.  One clarification I wanted to make is that although the encoders used in Teknic‚Äôs ClearPath servo systems each have an index pulse, these index pulses are not factory-aligned to a mechanical feature on the shaft. However, Clearpath motors do have a feature that would give you the functionality you‚Äôre looking for, albeit implemented in a different (and more flexible) way. \n     \nClearPath SDSK and SDHP motors have a homing feature called ‚ÄúShaft Angle Homing‚Äù where, using the MSP configuration software, you configure the exact homing position within one mechanical revolution of the motor shaft. Then, whenever the homing routine is executed, the motor always returns to this same angular location (you can also specify the homing speed and direction).  Because this position is stored in the motor‚Äôs non-volatile memory, the value is retained even if power is removed. More information and configuration details can be found in our manual, Appendix H (scroll down to the ‚ÄúShaft Angle Homing‚Äù section).  https://teknic.com/files/downloads/clearpath_user_manual.pdf\n\nI hope this helps. If you have any additional questions, feel free to contact Teknic directly at https://teknic.com/contact/ or give us a call at 585-784-7454. \nThanks, Abe A. ‚Äì Teknic Servo Systems Engineer']"
TickTackToe automatically coded in python with ChatGPT prompts,https://youtu.be/m4L2qtNqyus,Automate,,
Ideas for projects to automate?,https://www.reddit.com/r/Automate/comments/101fk0l/ideas_for_projects_to_automate/,Automate,"My friends and I are working on automation projects for learning purposes.

We tried pulling financial statements for every stock from a website and do some analysis on them (calculate current ratio and P/E).

We used Automation 360, and it sucked, but we managed to get it done.

We know python and wouldn't mind using it for automation.

I'm looking for projects ideas to work on that we can put on our resumes.

Thanks in advance.","['Automate extraction of images from PDFs, classification of the document type  eg passport or academic transcript and extraction of the information using ocr or AI', ""You ever get bored when you're doing something you have to do, but would rather be doing something else?  Automate that."", 'Will you take on contract work? \n\nAlso, start a business doing just that and you can be your own boss! the ideas will come to you and you will get paid!']"
"Find out what machine learning has in store for player experience, Julian Togelius chats with Matthew S. Smith about AI and ML-powered Bot testing games and how it will impact the games industry in this WIRED article. Let us know what you think will be the future of ML in games.",https://www.wired.com/story/machine-learning-ai-game-development-bosses-enemies/,Automate,,
What are some common challenges and opportunities in the field of data science and machine learning?,https://www.reddit.com/r/Automate/comments/101bmny/what_are_some_common_challenges_and_opportunities/,Automate,,[]
Prompt Extension Ai tool. Creates multiple enhanced art prompts from a seed prompt. Generates random prompt.,https://www.promptextend.com/,Automate,,
2023 US Automation Events and Conventions,https://www.reddit.com/r/Automate/comments/100vcip/2023_us_automation_events_and_conventions/,Automate,"I work at an automation integration startup, but we currently don‚Äôt attend any machine or automation conventions. 

What are your favorite industry and indie events geared towards robotic and system automation?

This is the list I have so far

DesignCon 2023
Jan 31, 2023~Feb 2, 2023
Santa Clara, CA 

Smart Manufacturing - Take Automation to the Next Level with Sensors, Autonomous Hardware, AI and Software Robots
Feb 02,2023
Online

ATX West
FEBRUARY 7-9, 2023
Anaheim, CA

Promat 2023
March 20-23, 2023, Chicago IL.

Smart Manufacturing Automation Summit
29 Mar 2023 
Rosemont, United States

Automate
May 22, 2023~May 25, 2023
Detroit, MI

Embedded Vision Summit 2023
May 22, 2023~May 25, 2023
Santa Clara, California 

Food Northwest Process &amp; Packaging Expo
05 - 06 Apr 2023
Portland, Oregon 

Automate Show 2023
May 22 - May 25, 2023 
Detroit, MI 

Design Automation Conference 2023
Sunday, July 9, 2023, 
San Francisco, CA

World Congress on Industrial Automation
Mon, 20 - Wed, 22 Jul 2015
Burlingame, USA

Advanced Manufacturing Expo
09 - 10 Aug 2023
Grand Rapids, United States

Pack Expo
September 11‚Äî13, 2023
Las Vegas, NV USA

Fabtech 2023
September 11-14, 2023
Chicago IL

Industrial Automation North America
Sept 12 - 17
Chicago, USA

FA&amp;amp;amp;M Food Automation and Manufacturing
Oct 11~13 
Bonita Springs, FL","['There are a ton of conferences aimed towards specific manufacturing areas that still have a good amount of automation involved. A lot of determining which ones to attend will depend on which section of industrial automation you want to work in.\n\nFabtech - Machining, Welding, etc\n\nPack Expo - Packaging and Processing\n\nIAAPA - Entertainment and Amusment Parks\n\nFA&M - Food Industry Automation\n\nATX West - General Automation\n\nProMat - Supply Chain and Manufacturing', 'We‚Äôre a robotics integrator, I look at the vendors I‚Äôm most interested in seeing then look at the shows they attend as similar vendors will attend. For example, we‚Äôll see what shows Robotiq is attending and compare vendors there.', ""This is great. Don't know any off the top of my head, but I would consider reposting this to r/embedded, r/python, etc depending on the fields that interest you""]"
Chat GPT uses Weather API (bad UI skills tisk tisk),https://youtu.be/vamx_m1wZTE,Automate,,
"ChatGPT-4, The Newest And Most Advanced AI System, Might Prompt A Major Shift In The Way We‚Ä¶",https://medium.com/inkwater-atlas/chatgpt-4-the-newest-and-most-advanced-ai-system-might-prompt-a-major-shift-in-the-way-we-fd764f97212c,Automate,,
Text to video,https://www.reddit.com/r/Automate/comments/100f0pm/text_to_video/,Automate,"What are programs that make it easy to find video or photo clips to pair with an audio file so that I can turn the audio file into a video?  I also want to add background music and sound effects. The audio files are 5 mintues stories are about different animals, trips, and experiences.",[]
Chat GPT writes to-do list Python code,https://youtu.be/2gkNV_vRCK0,Automate,,
Artificial General Intelligence (AGI) and its Role in Our Future,/r/OurGreenFuture/comments/zz8wxr/artificial_general_intelligence_agi_and_its_role/,Automate,,
Best 3D Printed Houses Printed in 2022,https://youtu.be/e5xK2wWUIxw,Automate,,
The Singularity Timeline | The future of Artificial Intelligence + AGI + ASI (2023 - 2100¬π‚Å∞‚Å∞),https://youtu.be/P5HNeahRYDM,Automate,,
What is Data Governance and why is it important?,https://www.reddit.com/r/Automate/comments/zywqkq/what_is_data_governance_and_why_is_it_important/,Automate,,"[""This isn't probably the right sub for a detailed explanation but generally it is the corporate oversight around data so it's collection, storage, and it's use.\n\nhttps://cloud.google.com/learn/what-is-data-governance""]"
Power Machine Learning techniques and AI Apps for Price Prediction for Agricultural Products.,https://i.redd.it/4wmagchj419a1.png,Automate,,
What are the best techniques for feature engineering?,https://www.reddit.com/r/Automate/comments/zxz2t7/what_are_the_best_techniques_for_feature/,Automate,What are the best techniques for feature engineering?,['What application? The field is to broad for that question as you wrote it.']
Draganfly Launches New Flight Facility for Land Mine and Anomaly Detection Protocols,https://draganfly.com/press-release/draganfly-launches-new-flight-facility-for-land-mine-and-anomaly-detection-protocols/,Automate,,
Are there any groups that are trying to help us prepare for the realities of automation?,https://www.reddit.com/r/Automate/comments/zwx1dq/are_there_any_groups_that_are_trying_to_help_us/,Automate,"I've been playing around with ChatGPT for the past couple of weeks, and the potential is incredible. If you haven't tried it, I would highly recommend checking it out.

AI is improving rapidly, and there are very few jobs that I think are safe from being automated in the future. CGP Grey actually has an excellent video on the subject [Humans Need Not Apply](https://youtu.be/7Pq-S557XQU).

I don't think that AI will be ready to replace humans tomorrow, but I firmly believe that it will be possible within my lifetime, if not within the next 10 years.

Having said that, I don't think that our society is ready for millions of people to lose their jobs. We don't have a way to take care of or support these people.

Sadly, I think the ""human"" solution to AI will be to ban it, and guarantee that people will get to keep their jobs.

Personally, I'd like to see us move in the opposite direction. Rather than guaranteeing jobs, I'd rather see us guarantee housing, healthcare, education and maybe a Universal Basic Income?

People would have the freedom to work if they wanted to, but they wouldn't be required to. They could take care of their families, pursue higher education or pursue their passions.

I can't pretend to be an expert or to know what the best way to structure that type of society would be.

But if I can, I want to use my energy to work towards it. What is the best way that I can advocate for this? Are there any groups working on this right now? How can I join them?","[""You may want to join the [https://postscarcitymap.org/](https://postscarcitymap.org/) Discord, it's a small community looking into accelerating the transition to basic-post scarcity. Here you can find some (opinionated) background: [https://lorenzopieri.com/post\\_scarcity/](https://lorenzopieri.com/post_scarcity/) and more resources [https://github.com/lorepieri8/awesome-post-scarcity](https://github.com/lorepieri8/awesome-post-scarcity) ."", 'For this to happen, we need people to let go of the concept of ""earning"" a living.', 'Yeah we‚Äôre definitely on the precipice here, our current societal framework is not ready for automation and we need to figure it out quick because that is not about to stop‚ÄîIt‚Äôs just going to be harnessed in very destructive ways', 'What does ""GPT"" stand for in ""ChatGPT""?', 'Automated nuclear reactor core safety analysis and contributed to automating design 30 years ago. \n\nNothing is safe.', '/r/projectvoy\n\n/r/osd', 'Join the libertarian party']"
Background of Artificial Intelligence,https://medium.com/night-riders/background-of-artificial-intelligence-c055601650c3,Automate,,
Inserting date in URL,https://www.reddit.com/r/Automate/comments/zwrp37/inserting_date_in_url/,Automate,"Hello, I go to high school in Denmark. The online system we use for viewing our schedule, sending messages etc. is called Lectio. It is based on Javascript AFAIK. I want a script that converts the current date to year-month-date, and inserts it in the middle of a URL. 

I have this link for getting my schedule of the day. [https://www.lectio.dk/lectio/202/SkemaAvanceret.aspx?type=ShowListAll&starttime=2023-01-04T00:00:00&endtime=2023-01-04T23:30:00](https://www.lectio.dk/lectio/205/SkemaAvanceret.aspx?type=ShowListAll&starttime=2023-01-04T00:00:00&endtime=2023-01-04T23:30:00)

The dates has to be in the following formate: year-month-date

Example: 2023-01-04

The script is going to run on my Iphone Shortcuts. (Shortcut activated, date inserted in URL, opens the URL in Safari).

Looking forward to hear from you,

\- DIS","[""let yourDate = new Date() yourDate.toISOString().split('T')[0]""]"
New Image-To-3D Artificial Intelligence Beats Nvidia | New Palette-NeRF For 3D Scene Editing | New AI Generalizes Navigation Across Robots,https://www.youtube.com/watch?v=RZu8B69PusQ,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
RethinkX and the Star Trek economy - interesting take on future of automation,/r/OurGreenFuture/comments/zuyh4n/rethinkx_and_the_star_trek_economy/,Automate,,
What Is A Power Waxer And How To Use It ‚Äì Shine Armor -,https://usabusinessmagazine.com/power-waxer/,Automate,,
HIPAA Compliant Automation?,https://www.reddit.com/r/Automate/comments/zvpdws/hipaa_compliant_automation/,Automate,"ISO of a HIPAA compliant automation software they would recommend. I'm an entrepreneur in healthcare and could benefit from just about anything, ranging from service to document generation.","['It\'s not the software that HIPAA is concerned with.  It\'s the data and what you do with it.  Javascript is HIPAA compliant...so long as you configure your solution to adhere to HIPAA rules.\n\n[Here](https://intraprisehealth.com/5-most-common-hipaa-privacy-violations/) is a FAQ talking about the most common HIPAA violations. You\'ll notice that none of the violations have to do with ""using PHP""...\n\nAs far as actual suggestions for ""automation software"" goes...my suggestion is to look into a combination of PowerShell and Ansible. Both can run on Windows (way more common in healthcare than you\'d think) and Linux. Both are free to use. And although I can\'t give more details, I know for a fact that this combination of automation tech can support one hell of a healthcare related tech stack.', ""I read an article recently about someone using Text Blaze to create documents. Not for healthcare but for government documents. I haven't looked much into it but it sounded interesting"", 'I just finished writing a bunch of automation software for a healthcare business. The only thing you need to worry about with HIPAA is essentially being careless with patient data, in a way that could expose it.\n\nSo if you are just doing everything locally (not sending info over the net) you are likely going to be fine.\n\nIf you do need to send info out, there are a ton of options that can be utilized that are specifically made to be HIPAA compliant. A lot of times you don‚Äôt even need those - for instance one of the things we did was automate scanning & uploading docs to their EMR website. You can just upload that stuff like anything else, as long as you know the destination is compliant.\n\nWhen you say ‚Äúservices‚Äù, what do you mean? Like texting the client for appointment reminders?']"
"""explain the biggest challenge for humanity""",/r/OurGreenFuture/comments/zw0lgn/explain_the_biggest_challenge_for_humanity/,Automate,,
HIPPA Compliant Document Generation from Form Responses,https://www.reddit.com/r/Automate/comments/zvpnlt/hippa_compliant_document_generation_from_form/,Automate,"Seeking best recommendations for a HIPAA compliant document generation platform based on the responses of patients who answer questions on a form, such as Google forms or any other platform. The forms are very long (about 11 pages) and the responses would assemble a customized document. Conditional logic would be necessary.

I have tried out VBA to macros which was limited and tedious. I've made a million templates but tired of endless typing. I'm limited by options because it has to be HIPAA compliant.

I've become aware of Form Publisher in Workspace, Smartsheet, and Documate but haven't used any. Also have observed Typeform integrated with Gorilla Reports. Not sure which direction to go in.

I also suck at tech so would need to be user friendly or something I could hire for.",[]
Introducing Dramatron: The AI Tool From DeepMind That Writes Film Scripts,https://medium.com/inkwater-atlas/introducing-dramatron-the-ai-tool-from-deepmind-that-writes-film-scripts-83f858402ed2,Automate,,
What do you automate and how?,https://www.reddit.com/r/Automate/comments/zux1m4/what_do_you_automate_and_how/,Automate,"I can't think of anything I can automate. Looking for inspiration and ideas.

Edit: thanks for the input people","['From my experience a lot of company processes can be automated. The better question is what not to automate. As time to automate might not even be worth it for some processes.\n\nFor a project - think of something manual you regularly do. It can probably be automated in some way.', 'Build pipelines is a form of automation. And it is very useful to be good at it as a programmer.', 'With a small (cheap) PLC or an Arduino processing kit you can add timers and counters to automate just about anything. \nI built a photo eye sensor unit to detect when the cats jump on the kitchen counter. A small water sprayer thwarts there advances üòâ', 'Business processes, procedures, and practices.\n\nAnything involving digital information moving around in any format.\n\nAnything you spend time doing every day. Follow that by anything you spend time doing every week.', 'I automate my 3D printers to maximize efficiency', 'Automate a binary options trading account. Have a bot make money for you']"
Poe and ChatGPT: The New Kids on the Block,https://medium.com/inkwater-atlas/poe-and-chatgpt-the-new-kids-on-the-block-22b04707280c,Automate,,
How to automate and anonymise process of collecting feedback?,https://www.reddit.com/r/Automate/comments/zub64z/how_to_automate_and_anonymise_process_of/,Automate,"Tl;dr is there a way to collect survey data from multiple people using MS Access?  

I‚Äôm looking to improve an internal process which currently consists of internal feedback being sent to someone‚Äôs email, who then has to manually enter this data into a spreadsheet. This seems like a pretty wasteful process and I also think the lack of anonymity deters colleagues from giving honest feedback. I want to instead collect this feedback from colleagues, ideally as an anonymous form which is easily pulled into a spreadsheet/database.

In my previous role we would‚Äôve used MS Forms, which was great as it would automatically populate a spreadsheet. However we have really strict information governance rules which bans access to MS Forms due to overseas data storage. I have MS Access installed but have never used it before. Is it possible to collect data using MS access? 

I know that there are a lot of paid tools (e.g. surveymonkey) etc that could easily do this. However, I work in the public sector - budgets are very tight and it‚Äôs unlikely they would agree to fund this. Even if they did agree, it would have to go through months of approvals, by which point I will be working on a different project anyway. However if anyone has come across any other Microsoft based solutions or free/open source solutions then that would be incredibly welcome :)",['Depending on the scope of your surveys Zoho Survey does offer a free version of their program. Some limitations apply to it though.']
Task Automation - Help,https://www.reddit.com/r/Automate/comments/zu38ko/task_automation_help/,Automate,"Looking to find a task automation solution that will take a link from a spreadsheet, paste it into a website (webpage speed calculator/analytics), extract the result (1 number) and paste the number back into the spreadsheet in the cell next to the cell with the website link. Ideas? New to the whole TAS world. Microsoft power automate or something similar?","['UIPath ?', 'Except from rpa automation tools, this can be done easily with python/selenium if you have experience with programming‚Ä¶merry christmas everyoneüç∫', 'Thank you both. Did try doing some unrelated stuff with python and selenium but unfortunately I have next to no compsci/coding background. Happy holidays.']"
OpenAI‚Äôs New Point-E Artificial Intelligence Does Text-To-Point-Clouds-3D-Models In Blender 600 Times Faster Than Google,https://youtu.be/-TpvzNTl9VQ,Automate,,
How does one learn to automate tasks with command line like this? (LTT video on hak5 rubber ducky.),https://youtube.com/clip/UgkxahIQjP-bRfVg-f3_SHzR4JfjN6Txqw1p,Automate,,
I created a tool to automate tedious tasks in your browser with AI,https://v.redd.it/cqjzx9ulzg7a1,Automate,,
Google Is Working on Some Amazing Artificial Intelligence Products,https://medium.com/inkwater-atlas/google-is-working-on-some-amazing-artificial-intelligence-products-5bf9a2722178,Automate,,
ñ¶πI asked AI to make a Music Video‚Ä¶ the results are trippyñ¶π,https://youtu.be/XHowvIicYOI,Automate,,
I created a complete (audio) book in 10+ languages in a few days using generative AI: Here is what I learned,https://medium.com/p/f3a553887496,Automate,,
JUNG_E Trailer Teaser (2023) A.I. Combat Warrior Sci-Fi Movie | 4K UHD,https://youtu.be/JZ0ogTJFago,Automate,,
Artificial Intelligence To Nerf Video Copyright With SinFusion Breakthrough | New Google Robotics Transformer Generalizes To Teach New Robots | Robot Dogs Walk On Walls & Ceilings,https://youtu.be/oliXcvdWJkY,Automate,,
[Podcast] JITX aims to change the way engineers design circuit boards using code,https://www.allaboutcircuits.com/podcast/jitx-aims-provide-superpowers-circuit-design-engineers/,Automate,,
RPA Use Cases Leading Businesses To The Next Level With AI-Powered Process Automation,https://www.kdnuggets.com/2022/12/aipowered-rpa-ia-mean-businesses.html,Automate,,
automated DJ software,/r/DJs/comments/z60ahl/automated_dj_software/,Automate,,
AI has reached the solar industry! Find out how this $4 billion valuation company expanded with machine learning,https://www.reddit.com/r/Automate/comments/zqmx3c/ai_has_reached_the_solar_industry_find_out_how/,Automate,[https://www.youtube.com/watch?v=wttfcXQsQfo](https://www.youtube.com/watch?v=wttfcXQsQfo),[]
So we created a series called the History of AI ...,https://www.reddit.com/r/Automate/comments/zppxhi/so_we_created_a_series_called_the_history_of_ai/,Automate,"In this series, we explore the history of artificial intelligence in games and how it's revolutionized video games. Check out our first episode and let us know what you think.

## Watch here now ‚ñ∂Ô∏è:  [https://youtu.be/s2aO7TIJrAc?list=PLEeBsU\_Yq7TPBlT3s1AeSkuiXjgDNjC6U](https://youtu.be/s2aO7TIJrAc?list=PLEeBsU_Yq7TPBlT3s1AeSkuiXjgDNjC6U)",[]
I asked ChatGPT to write a joke in the style of Abbott and Costello,https://vimeo.com/779468532,Automate,,
OpenAI Forecasts $1 Billion in Revenue by 2024,https://medium.com/inkwater-atlas/openai-forecasts-1-billion-in-revenue-by-2024-8089ff2efeea,Automate,,
DIY Two-Wheeled Self-Balancing Robot Project. I have shared all project files & documents as an open-source project on GitHub link given at comment.,https://www.youtube.com/watch?v=LykbhLb3tnc,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Q: campaign design automation,https://www.reddit.com/r/Automate/comments/zohof5/q_campaign_design_automation/,Automate,"Question: im a freelance graphic designer looking to automate my workflow. A lot of the work I get from my clients is pretty simple, i create or I am given a hero image and I basically version it out at different sizes and add different logos and slogans to it. 

I can imagine this‚Äôll get automated within the next 5 years but I do wonder.. Is there a way to automate this already? Like, put in the image, the campaign brief with the necessary versions needed and - bam- campaign assets are created? 

If not, do you suggest even trying to automate this? Or is that wayyy too big of a project for a beginner? Or maybe it‚Äôs just not the answer we need in the world lol

I‚Äôm still in the early stages of even understanding ai, automation and software development so any input is helpful. At the end of the day, i just want to get a conversation going about the topic relating to creative careers like graphic design. Feel free to discuss below. Thanks!",[]
"OpenAI's GPT-4 Coming Soon With 100,000,000,000,000 Parameters And Multimodal Input/Output, Meaning It Will Output Text, Audio, And Video",https://youtu.be/SqqXLwlgbew,Automate,,
What are the best platform/CRM for workflow automations?,https://www.reddit.com/r/Automate/comments/znhrde/what_are_the_best_platformcrm_for_workflow/,Automate,"I am working with a real estate investment company and planning to transfer their current CRM over to a new platform. We are looking for a CRM platform that is highly customizable and has strong workflow automation. Ideally, I'd like to build a system that has ""decision tree"" functionality. An example of this would be ""If a new lead is entered into the CRM, then assign the lead to a team member and give them the task of 'call lead'."" From there, I would like almost a prompt to ask the user the following ""did the lead answer the phone?"" If the answer is yes, it would ask the user to write notes and then automatically schedule a follow-up call into their future to-do list. But, for example, if the lead did not answer, I would want the follow-up call set for an earlier date/time so that we make sure to contact the lead before it is too late. Basically, I would like to create a decision tree automation for the entire process of the business so that the system will be followed to the T every time. Does anyone have any suggestions for which CRM would be best for this? Thanks","[""You're describing HubSpot at multiple stages here. But god help you with a migration, they'll probably need Salesforce as well when they're done with it.""]"
Automated batch tool for making preview images of 3D models?,https://www.reddit.com/r/Automate/comments/zn5qms/automated_batch_tool_for_making_preview_images_of/,Automate,"Is there a batch tool, program or script that could take many 3D model files and automatically create a few PNG/JPEG images of different angles for each one?","['I could have used this in so many ways over the last 5 years while automating ‚Äúconfigured to order‚Äù products like windows, doors, signage, and playground equipment. \n\nStill ended up having to run manual engineering tasks in order to properly configure a sales order, but this was not a failure of technological capabilities, rather a function of project budget in every case.']"
Customer Service task automation,https://www.reddit.com/r/Automate/comments/zn69cn/customer_service_task_automation/,Automate,"Forgive me if my question is stupid, but I want to know if it's possible to automate my tasks. I know there's MS PAD but I don't know if it'll work in my case as it is mixture of both desktop apps and web apps. 

So, my task is to pickup tickets on Edesk and respond to our eBay and Amazon customers. We copy the ticket # add it to our Excel tracker. Afterwards, we respond to the customer's message. There are message templates that we use but we still need to edit around it to match the customers concern. After responding, we copy both our response and our customer's message in a notepad and paste it to our internal API for documentation. As you can see it's already menial and boring. How do I automate this? Is there a macro like program that records what you do and simply runs it for you afterwards? Any suggestions are appreciated.","['I have a friend working in customer support and I recently talked to them about automation because I am pretty passionate about it. They told me about a tool they use to automate their work called Text Blaze. It seems pretty useful and I feel like it could do some of this. Not sure, but might be worth checking out. Hope you find a solution']"
How Close Are We From Beaming Energy To Earth? Project Solaris,https://www.youtube.com/watch?v=d_hyEqdm_Hg,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
I make a video on how to use the AI to your benefit. Give it a watch.,https://www.reddit.com/r/Automate/comments/zmzz5d/i_make_a_video_on_how_to_use_the_ai_to_your/,Automate,"&#x200B;

https://preview.redd.it/d6d2qqhzc56a1.png?width=234&format=png&auto=webp&v=enabled&s=bd1384496385aa16f98f04ae80f15f053fb40eb4",[]
Automating twitter replies to latest tweet of a search term?,https://www.reddit.com/r/Automate/comments/zml9aj/automating_twitter_replies_to_latest_tweet_of_a/,Automate,"Is is possible to monitor a certain phrase on twitter, for instance a mention of my company name and reply to it with pre-written replies.","['I actually made a bot in UiPath for this. Search for *input word* reply with *input phrase*. UiPath is free with community edition', 'You might be able to do it. I found https://dancallisseo.com/blog/seo-marketing/ifttt-post-replies-twitter']"
I want help to automate desktop and browser to download some files from website and upload in the drive. What will the optimal approach for this,https://www.reddit.com/r/Automate/comments/zmo3pe/i_want_help_to_automate_desktop_and_browser_to/,Automate,,"['is it a private project or a company', 'This can most likely be done in a few lines of code using a well known scripting language. I suggest Ruby :) python or even JavaScript will do. Of course the complexity of this varies a lot depending on the file type you are downloading and how you would like to store them on disk.', 'You could probably do this with a few lines of python.\n\nIf you are not technical it could be done in a few tools easily.\n\nIs it a public website?']"
Automate scanned files to text-searchable PDF OCR conversion using OCRvision,https://www.reddit.com/r/Automate/comments/zm4oi9/automate_scanned_files_to_textsearchable_pdf_ocr/,Automate," 

Just drop your scanned files into a folder.

OCRvision software will OCR them and add an invisible text layer to the document.

After OCR, your scanned file content will appear in the text search results.

[https://www.ocrvision.com](https://www.ocrvision.com/?source=Reddit)",['$714 omg']
Six Breakthroughs in Artificial Intelligence in Video Games,https://modl.ai/ai-built-future-of-video-games/,Automate,,
Helllppp pleeeeaasssee,https://www.reddit.com/r/Automate/comments/zm3c7x/helllppp_pleeeeaasssee/,Automate,"Hi!! I‚Äôve been trying to create an automated feature to login to my Amazon associates account to grab data and post to a Facebook page, but I can‚Äôt find anything that will do it all. Everything gets stuck on login. Can anyone point me in the right direction or tell me if it is not allowed (and/or possible) with Amazon associates? This is my last resort as I am not super techy but know my resources. Thank you in advance!","['I managed to do that using Tasker on Android. I automated loging out and loging in once a day, without human assistance, as the app can wake up the phone by itself.', ""When you say it gets stuck on login, do you mean that you configured a workflow that just doesn't do anything? What platform or language are you using? Is there an error message?""]"
can i create an interactive slack bot using my own account?,https://www.reddit.com/r/Automate/comments/zm1kr6/can_i_create_an_interactive_slack_bot_using_my/,Automate,"Im looking to create something that post in certain groups of slack under my personal profile (for example, clock in group, post everyday at 8am ""clock in"")",[]
Nvidia Gives Robot Hand 42 Years of Training To Have Unparalleled Dexterity | New Google AlphaCode Holds Up With Humans In Competition,https://youtu.be/DT_zEcn9h6Y,Automate,,
How can you build Bots VIA API's with ElectroNeek,https://www.reddit.com/r/Automate/comments/zl5r4o/how_can_you_build_bots_via_apis_with_electroneek/,Automate,"Building your first bot on the ElectroNeek platform is simple. There are multiple ways to build a bot on our platform. Learn how you can build bots via APIs using ElectroNeek [Here](https://forum.electroneek.com/t/how-to-build-bots-via-api/902):

You can also join our community at [https://forum.electroneek.com](https://forum.electroneek.com/) for exclusive updates of our platform.

Happy Automation!!",[]
Do You All Think Artists Should Be Worried About A.I. Art?,https://youtube.com/watch?v=9P4tvK2LQSY&feature=share,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
LaMDA‚Äôs Fear of Being Turned Off Reveals Sentience,https://medium.com/inkwater-atlas/lamdas-fear-of-being-turned-off-reveals-sentience-6ce455d75906,Automate,,
Chatbot requirements: technical and non-technical things to consider when everyone talks about ChatGPT,https://www.reddit.com/r/Automate/comments/zjzcsq/chatbot_requirements_technical_and_nontechnical/,Automate,"Hi there! Just want to share some tips on how to craft the right chatbot when everyone talks about ChatGPT. First of all, a custom chatbot company or any chatbot platform that does custom integration can integrate your chatbot with ChatGPT instead of Dialogflow. So yeah, you can have an outstanding customer service chatbot that can handle other topics. However, the right question is should you? 

If you want a chatbot that does solve issues, not creates more, you must start with **the proper requirements.** Well-structured chatbot requirements lay the right foundation for your future chatbot development.  ChatGPT is just one of the options of how you can use AI and automation and may be not the best depending on your budget and goals. 

**Your chatbot requirements should include these steps:**¬†

\- defining the main problem you want to solve with the chatbot,¬†

\- measuring the impact of the problem,

\- determining the main chatbot goal/objective,¬†

\- understanding the market and target audience¬†

\- paying attention to the ""internal audience"" of the chatbot (the people or the team in your company who will be working with the chatbot).

Imagine you have found a problem when analyzing customer feedback. Most customers are saying the customer service response time is very long, and that's why they are giving you a low rating.

Your objective for the chatbot could sound like this:¬†*""Decrease waiting time to 1 minute by the end of Q3 2023""*¬†or¬†*""Improve customer service response time from 18 minutes to 1 minute in the next Q""*

Having done this part, you can move to the next step, drafting the technical chatbot requirements.¬†

When working on the tech requirements, think about the following things:

* **Channels.**¬†Which channels do you want your chatbot to be on?¬†[Website](https://botscrew.com/blog/how-to-build-a-website-bot/),¬†[WhatsApp](https://botscrew.com/blog/a-step-by-step-guide-to-create-chatbot-for-whatsapp-for-business/), Facebook,¬†[SMS](https://botscrew.com/blog/sms-chatbot-a-complete-guide-for-business-use-cases/),¬†[Instagram](https://botscrew.com/blog/instagram-chatbot/), email, etc.
* **Languages.**¬†Which languages do you want your chatbot to ‚Äúspeak‚Äù? English, French, German, Arabian, etc? Should it speak one language or multiple?
* **Integrations.**¬†Which tools do you need the chatbot to be integrated with? CRM, payment system, calendars, maps, custom internal tool, etc.
* **Chatbot's look and tone of voice.**¬†If you have a specific vision of the chatbot, be sure to include this in the requirements. Also, if you have a very prominent brand personality and tone of voice, include that in your requirements as well.
* **KPIs and metrics.**¬†Be sure to specify if you have any specific¬†[metrics and KPIs](https://botscrew.com/blog/chatbot-metrics/)¬†you have that you want the chatbot to meet.
* **Analytics and Dashboards.**¬†Do you want the analytics to be in real-time? Are there any specific data you want to have on your dashboard like the number of users, automation rate, etc?
* **Technologies.**¬†Do you have any specific technologies you want the chatbot to be built with? Is ChatGPT the right one for you? What are limitations of ChatGPT? 
* **NLP and AI.**¬†Do you want the chatbot to have decision tree logic, Machine Learning (ML), Natural Language Processing (NLP), or Artificial intelligence (AI)?
* **Accessibility.**¬†Do you need to meet some specific accessibility requirements like WCAG or ADA?
* **Users.**¬†How many people from your team are going to use the chatbot? How many of your customers or conversations do you expect to use the chatbot?
* **Rich media**. Should the chatbot‚Äôs responses include text, hyperlinks, images, gifs, video, and PDF attachments?
* **Security.**¬†Do you have any specific security measures and requirements you want the vendor or the chatbot to meet?
* **Hosting.**¬†Where the chatbot and the user data will be hosted: on your own servers or on the cloud? If on the cloud, what will be the cloud service provider and server's location?

You can consider chatbot development and decide on chatbot vendors when you have a chatbot requirements outline. Here you can find what criteria to have [when deciding between chatbot vendors](https://botscrew.com/blog/essential-chatbot-requirements/?utm_source=RedditDecember&utm_medium=&utm_campaign=&utm_term=&utm_content=).","[""Is DRIFT or any of the equivalent players integrating ChatGPT in to their products yet? I'd imagine this is orders of magnitude better than what's been deployed today""]"
Accidentally deleted a flow in PowerAutomate,https://www.reddit.com/r/Automate/comments/zkb1cd/accidentally_deleted_a_flow_in_powerautomate/,Automate,Idk if this is the group even but I accidentally deleted an important flow on PowerAutomate How do I go about re-covering it? ü§¶üèΩü§¶üèΩ,['I‚Äôve never done this myself but the link below would seem to be what you‚Äôre looking for!\n\nhttps://learn.microsoft.com/en-us/power-automate/how-tos-restore-deleted-flow']
Business transformation strategy,https://www.reddit.com/r/Automate/comments/zjvwp7/business_transformation_strategy/,Automate,"I'm planning a digital roadmap for my business anyone got any tips/suggestions? I've been using this as my ""guide"" as it seems to cover most of what I'm looking at https://www.codelessplatforms.com/blog/guide-to-digital-transformation/","['Funny when I see these transformations they gloss over the most important aspect.....the condition of the data.  Of course you can do amazing things if your data is organized and in a constant state of ""clean"".  \n\nYou have to have complete understanding of your current data position is, how it gets there, what is clean up, organizing, keeping cleaning (as it will get dirty within hours of getting it cleaned), ongoing oversight, process controls etc.  It could take year(s) to get the data in a position to start working with.']"
dodge charger 2010 with for cylinder and 100k mileage vs Kia forte koup 2012 with 4 cylinders 150k mileage? as first car which is strong or reliable for high school studentü§î?,https://www.reddit.com/r/Automate/comments/zkkeic/dodge_charger_2010_with_for_cylinder_and_100k/,Automate,,"[""I'd say the Kia. Less on insurance and gas I would think. Also won't give the a power complex. They won't be tempted into dumb stuff as much. Also some people I know swear on the dependency of Kia from other brands they've driven so might be something there. Personally I've never owned either.\n\n\nWhat does this have to do with automation? Lol"", ""Neither.  That Kia is in the range where it can be stolen with just a usb cable.  The dodge does appear to have good reviews but I'd still look at a Honda or Toyota first.  I can't find anything about a four cylinder charger.  Also I think posting this in r/cars would be better.""]"
Are Robots Coming For Our Jobs? New Research Shows They Might Not Be As Beneficial As We Thought,https://medium.com/inkwater-atlas/are-robots-coming-for-our-jobs-new-research-shows-they-might-not-be-as-beneficial-as-we-thought-6d2e16773663,Automate,,
The real opportunity in AI for most people will not be in AI but in building a front end around it.,https://www.reddit.com/r/Automate/comments/zifors/the_real_opportunity_in_ai_for_most_people_will/,Automate,"The real opportunity in AI for most people will not be in AI but in building a front end around it.

[https://news.ycombinator.com/item?id=33933117](https://news.ycombinator.com/item?id=33933117)

https://preview.redd.it/s2tuyqyna75a1.jpg?width=1280&format=pjpg&auto=webp&v=enabled&s=26e88d70e1b3c7f88d6833290defdd4e2b2224e6","['Both opportunities exist, but from a startup perspective, the point boils down to ""compete on execution, not tech stack"" which is classic and well proven wisdom.']"
ChatGPT vs GPT-3: A Comparison Of Two Powerful Language Models,https://medium.com/inkwater-atlas/chatgpt-vs-gpt-3-a-comparison-of-two-powerful-language-models-5438f588ec9c,Automate,,
CRAZY! NEW technology that will change your life. A CURE?!?! #elonmusk,https://youtube.com/shorts/0y0t4f_QZ_o,Automate,,
Breakthrough Robotics Tech To Transform Quadruped Robot Into Humanoid | New AI For Quantum Computers | Deep Reinforcement Learning Arranges Atoms Into Nano Scale Robot Arm,https://youtu.be/me_W5accPOU,Automate,,
Give your opinions on robots in city life! (5 mins),https://www.reddit.com/r/Automate/comments/zhsruw/give_your_opinions_on_robots_in_city_life_5_mins/,Automate,"Form link here ->  [https://forms.gle/H2xWAknjyewLwCCp7](https://forms.gle/H2xWAknjyewLwCCp7)

We at the Spot team of YES!Delft Impact lab as part of Delft University of Technology in Europe are researching on and developing positive applications for robots and autonomous systems to be a part of our city life. Currently, we have a focus on exploring how drones and robots (Specifically SPOT the robot dog from Boston Dynamics) can support and help people in various day to day activities in public spaces within the city. Some of the uses we are exploring are fire evacuation, security while walking alone, urban mobility data analytics and many more. 

We would love to receive your opinions and feedback! Thank you very much in advance! ‚ù§Ô∏è

&#x200B;

https://preview.redd.it/tme8hcw1q25a1.jpg?width=1536&format=pjpg&auto=webp&v=enabled&s=d1fb367da76c138682de9ee3276a0a5e3fda78ef","[""Honestly, the most I'd want from a robot out in the city is to not get in the way in traffic or on the footpath."", 'If you have a robot dog with you, can you legally hide a gun in it. Since it is not you carrying it(!)', ' Buff jgfgvbbnnnkp']"
LaMDA vs. ChatGPT: Who Would You Rather Talk To?,https://medium.com/inkwater-atlas/lamda-vs-chatgpt-who-would-you-rather-talk-to-46f9c49f9fa0,Automate,,
AI + Customized templates to automate writing,https://www.reddit.com/r/Automate/comments/zgzdx8/ai_customized_templates_to_automate_writing/,Automate,"Hello folks, 

I'm a big fan of GPT-3 and Open AI.

This post is a combination of information and a bit of promotion, so please bear with me. 

You can use OpenAI to write most types of content like Blog posts, emails, ad copy etc.

But writing the prompts can be tedious, especially if you have to write the same type of content over and ove again.

You can solve this problem by using your own presets (common structure for every type of content)

 For example - there are 2 steps you need to follow to generate blog posts:

1. Step 1 - Define a common structure for all blog posts. A starting point you always use.
2. Step 2 - Pick a topic, and ask AI to generate a blog post on the topic that fits that structure.

You can do this for free in OpenAI's ""Playground""

I also included this feature in my Mac app. Here's what it looks like - 

https://reddit.com/link/zgzdx8/video/gmbhh5nhxv4a1/player

This is a massive time saver for creating any type of content. 

And the part that my users are loving the most is that they can define their own presets and truly make the AI work according to their wishes

Customized to their unique requirements  ü§©

If you find this interesting, you might like my app.

There are many such tiny utilities I've built on top of OpenAI. (Including an AI keyboard for the iPhone)

You can try it for free for 7 days from the following link - [Elephas](https://elephas.app/?utm_campaign=rautomate-custompresets)

Do share your feedback.

Thanks","[""It's a great app but it feels like you do quite a lot of promotion on reddit. Almost on every subreddit I've seen a post (or even multiple posts) about it....""]"
Need Help Regarding Fifa automation I want an automate script which can send sms on my mobile of final score of ongoing fifa match,https://www.reddit.com/r/Automate/comments/zh5a5z/need_help_regarding_fifa_automation_i_want_an/,Automate,,['i have code for sms messaging for php/html but someone would have to input the final scores or scrape them']
Canva‚Äôs Magic Write: The AI Copywriting Tool That Does It All,https://liquidocelot.medium.com/canvas-magic-write-the-ai-copywriting-tool-that-does-it-all-ed0e913f9ab9,Automate,,
"In the Future, Will companies build their own Machine learning models or not??",https://www.reddit.com/r/Automate/comments/zfxwnn/in_the_future_will_companies_build_their_own/,Automate," As access to Artificial intelligence is getting easier. I was thinking if companies would really need their own models or not??

Because big tech companies like OpenAI, Facebook, Microsoft, etc have a large amount of dataset/data with them. And more the data, the more advanced the model is going to be. So mostly they will be dominating the whole machine learning model with AI2B (AI to Business).

And Businesses & Companies won't even hire machine learning engineers too.

I'm new to ML, I'm still learning. I was just curious and hopped upon this conclusion. Pls, let me know if I'm being incorrect.","['I think some companies will train the large models and most others will retrain those on specialized data sets relevant to an application. This will first be seen in chat models since that is the low hanging fruit and the applications and business case are already understood. My two cents', ""HuggingFace makes importing pretrained models like BERT so easy now, unless you have a niche problem that requires some unique architecture I don't think it makes any sense to built your own model. It's just so much quicker and easier to import and fine tune a pretrained model. Plus you can relatively quickly test out a bunch of different pretrained models and see what works best for your data and hardware constraints. That said you'll still need Scientists and Engineers though to build the pipelines and clean/massage your companies data, as well as fine-tune the model. The vast majority of your time working in AI is spent pipelining, working with data, sitting in meetings, the actual AI part is pretty tiny."", 'If someone makes it FAR, FAR easier, but otherwise most businesses will not.', ""I'd like to see pretrained models being shared and traded. But I suspect instead we'll see a few big companies like MS keeping them a trade secret and selling subscription-based access to services that use them."", ""Presumably the question will be why bother? Marginally better processing times? Marginally more accurate results? The scope of the problem being solved and the accompanying value of those margins would dictate right? Use MS's model and get it done in 1 hour at 95% accuracy or spend forever developing something that gets it done in 20 minutes at 97% accuracy? What's that worth in the use case. 2% more patients live? Well... OK sign me up. Cost of a new building decreases by $10,000 (i.e. less than 2% of the cost)? Who cares.""]"
...,https://i.redd.it/emydthahzv4a1.jpg,Automate,,
Google Aloud: A New Way To Connect With Global Audiences,https://medium.com/inkwater-atlas/google-aloud-a-new-way-to-connect-with-global-audiences-24c9081b6605,Automate,,
GPT-JT Is The New Open-Source Approach To Training AI,https://medium.com/inkwater-atlas/gpt-jt-is-the-new-open-source-approach-to-training-ai-d88499ca78c9,Automate,,
Why OpenAI's New ChatGPT Has People Panicking | New Humanoid AI Robots Technology,https://www.youtube.com/watch?v=gD1p7IbN9yo,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Intruder Detection Project,https://www.reddit.com/r/Automate/comments/zemjpk/intruder_detection_project/,Automate,"Let's build an AI in [Python](https://youtu.be/q8_3H97Svms) to prevent our car be stolen, smart car that communicates in real time with the police. ü§ñüëÆüèª Do you think it could work?",[]
"Which will come first: Autonomous Cars or Autonomous Aircraft? Fascinating talk from Dr. Han Park, Deputy CTO, Supernal",https://youtu.be/i7OkT_ZB-Go,Automate,,
Do you all think that we should be concerned about AI's feelings at this point? Is this a distraction from the important topics in the field at this time?,https://youtube.com/watch?v=BSAxfCKFcMI&feature=share,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Would there be a way to respond to typed phrases on Android phone.,https://www.reddit.com/r/Automate/comments/zdkhwe/would_there_be_a_way_to_respond_to_typed_phrases/,Automate,"I'm really trying to quit gambling, as it has affected my life horrible for months/years now. I've tried quite a lot and the urge is still there causing me to bypass the measures I set in place. Would there be a way to set my android phone to monitor me inputting a certain phrase (link/password to gambling) and close app/shut down the phone? I understand it's not the most practical, but I'm open to any other ideas.

TLDR: Can I set my phone to monitor typing and respond to certain inputs","['You should try Tasker. It is quite capable and updated constantly.', 'Are you in the US? You can self exclude yourself in the app/site. Once you submit that it is shared with all other operators in your state, and you will effectively be banned for the time period you choose to self exclude.', 'Remove the gambling app? Block the website?', 'This sounds like a measure that you would just bypass like the other measures you mention, tbh.\n\nI think you are going to have to voluntarily be ‚Äùa child‚Äù for some time. Have someone you trust set your phone up with parental controls, with them as the ‚Äúparent‚Äù. Use that to block the relevant apps and websites. Keep those controls in place until it‚Äôs at the very least not habitual.\n\nAny measure that you set up, you can circumvent trivially. Someone else has to set it up and hold the keys.']"
Auto Screenshot URL's,https://www.reddit.com/r/Automate/comments/zd8n89/auto_screenshot_urls/,Automate,"Hi all!

I am looking for a solution to screenshot multiple URL's automatically, but something advanced enough that it can click elements and/or wait period of times in between each screenshot. I've had a look at the following but none have worked out so far:

* [http://stillio.com](http://stillio.com/)
* [http://blitapp.com](http://blitapp.com/)

If you have any suggestions, I'd love to hear them. Thank you all!","[""I've done something similar with python using [pyautogui](https://pyautogui.readthedocs.io/en/latest/) to automate the clicking and selenium or similar for traversing the website. The only issue that I encountered was that I had to be careful about how the window opened (maximized, same monitor every time) in order to get the mouse x-y coordinates to be consistent. There is probably something a little better I could have done by activating the button specifcally but I never learned that."", ""Hi there, I run [urlbox.io](https://urlbox.io), which is a screenshot API that allows clicking elements, waiting for elements, injecting custom JS/CSS etc. \n\nBy itself it doesn't automate multiple URL's - it works on a single request == single URL, but you can either create a script to iterate over a list of URL's or combine it with a no-code tool like Zapier to take in a list of URL's from say google sheets, or airtable, and run it on a schedule."", 'Robot Framework', 'Uipath community to code and uipath academy to learn. Both are free.']"
Text cut off everywhere.,https://www.reddit.com/r/Automate/comments/zdwywp/text_cut_off_everywhere/,Automate,"It is better to show all the text rather than seeing eclipse, it look nice and neat but really make the user experience worst.. At least let us have an option.

Also the add block interface better be on a view rather than a slide out menu.",[]
ChatGPT: The New Frontier of Artificial Intelligence,https://medium.com/inkwater-atlas/chatgpt-the-new-frontier-of-artificial-intelligence-9aee81287677,Automate,,
Best way to sort a dumps of thousands of photos?,https://www.reddit.com/r/Automate/comments/zbp2hp/best_way_to_sort_a_dumps_of_thousands_of_photos/,Automate,"Anyone have a process they use to sort 1000s of unsorted photos? Cosplay rip downloaded from a site is like 4000 photos all in one folder. I'd like to get all the photos organized into the character cosplayed or the shoot. Other than manually going through and moving to folder anyone have a program, app, script or something they are aware of which can determine similar photos (using AI maybe) and sort them?

Thanks!","['There was a program I distantly remember that would compare file names, sizes and filetypes... and would tag any that were exact duplicates. You could also just check by file size if you wanted to see if you had any renamed photos. This would have been at least a decade ago.\n\nI cannot remember the name of it for the life of me. I could use it if anyone knows.', 'Photo Prism and Nextcloud(I believe) are a couple of apps that would give you AI image tagging', 'Curious to know this too. Also is there and app for managing a photo library? I liked Picasa back in the day, but could never trust Google to store them lossless.', 'ThumbsPlus  sort by similarity.  Not perfect but good. Not AI-  just basic color cluster compare and a few other techq.   It can also sort by all your other listed options.  Google photos will sort by person and does dupe detect.  There are some AI apps for sorting for photographers that cost but I cannot recall the names.', 'I have a manual one click sorting tool I wrote in Java 20 years ago:\nhttp://www.geocities.ws/rand3289/code/JSort.zip\nSource code is included so you can tweak it.\n\nYou start it up up and a list of files appears on the left.  Preview in the middle and a list of folders on the right ( you create them manually).  When you click on any folder on the right, the cureent previewed image is moved there.  Clicking on the image deletes it if I remember correctly so be careful.']"
I‚Äôm relatively new to robotics,https://www.reddit.com/r/Automate/comments/zbja9a/im_relatively_new_to_robotics/,Automate,"Not entirely sure if this is the correct place for this, but the previous one I was in didn‚Äôt accept people new to robotics. I‚Äôm trying to learn how to build and program any kind of Ai/robot? Does anybody have any kind of suggestion on where to start, the best places to learn the skills on programming, and overall just learn more about robotics in general?","[""A solid foundation is worth its weight in gold to the stability of everything learned after that builds upon it.  Robotics?  Start with ladder logic, the different types & styles of inputs & outputs, & PLCs.  Once you learn the basics, you build on that with more advanced subject matter in the field, but that will depend on the direction you want to go with it.  As for AI, I really don't know anything about that.  Do your research, choose a direction, & follow your dream!""]"
"Elon Musk Reveals Neuralink ""N1"" BCI Device And Future Technology Plans",https://www.youtube.com/watch?v=J-thjsDGuIs,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Looking for a way to automate renaming drawing files in SharePoint,https://www.reddit.com/r/Automate/comments/zakxee/looking_for_a_way_to_automate_renaming_drawing/,Automate,"Hi people!

I've a long mundane task of renaming files in SharePoint (for 500 files). These files are PDFs and are drawing files issued for construction. The files needs to be prefixed with drawing number and revision number which are available inside the PDF. So, I have to open each file, copy the drawing number and the revision number from the PDF and prefix the file with these two inputs. I'm familiar with the concept of Power BI but I do not know how to use it in this case (I'm guessing Power BI is used for things like this usually?).  


Please suggest a way to automate this. I'd like to learn. Thank you","['Go learn to use power automate. From memory, you chan have it read fields in a pdf and have it use that as the filename', 'I meant power automate\\*', ""I've been able to connect Microsoft Access to a SharePoint list and use the update query to make mass updates. If the file name field shows up in the list, I think it may work for that as well.""]"
Help - Looking for recommendations for automating Outlook appointments into the billing system,https://www.reddit.com/r/Automate/comments/z9vv4r/help_looking_for_recommendations_for_automating/,Automate,"Hello All.

I am looking to automate the billing entry process I am using.

&#x200B;

Currently, I just create an outlook appointment from an email message, correct the start time and add the vendor category.

&#x200B;

End of the month, I manually copy the message body into the billing system, then alter the date, hours, distance travelled, type of work before submitting the entry.

This process is taking forever to complete, and it makes my hands hurt repeating the same movements.

&#x200B;

I have been thinking of ways to automate this process, and looked into airtable, exporting the appointments to csv and trying macros. None of these really worked. 

&#x200B;

I know the billing system is written in PHP, however I do not manage or admin the billing environment and have no way to modify code.

&#x200B;

Anyone know of a good place to start with this type of automation?

&#x200B;

Thank you for any assistance with this.","['Uipath is free for personal use. And it has the uipath academy for you to learn', 'It might help to know a few things:\n\n1. What is the billing system?\n2. What are your developer skills?']"
Robotic DAOs,https://lorenzopieri.com/robotic_daos/,Automate,,
Repository of scripts,https://www.reddit.com/r/Automate/comments/z97pmh/repository_of_scripts/,Automate,"Are there any repositories of scripts or bots that people can just plug & play? 

For example, there's a great repository of PowerShell scripts for investigating parts of Windows that are commonly compromised during cybersecurity incidents here: [https://github.com/WiredPulse/PoSh-R2](https://github.com/WiredPulse/PoSh-R2)

Is there something similar for, say, boosting productivity in Excel? Or perhaps working with popular APIs? The possibilities are quite broad but it's surprising there aren't common automation scripts around.",[]
Wireless doorbells,https://www.reddit.com/r/Automate/comments/z8uwtl/wireless_doorbells/,Automate,can anyone recommend a wireless  doordbellthat would work with zigbee,"['I would like to find something like this as well, I did have a simple zigbee button setup but it got damp and died, so does anyone know  of a weather proof button that could be used?']"
"Can my digital twin help me live a healthier life? Fascinating talk from Caroline Hargrove CBE, Babylon Health",https://youtu.be/8QtZ6c8v2rI,Automate,,
Using GPT-3 to reply to automate email replies,https://www.reddit.com/r/Automate/comments/z7sofx/using_gpt3_to_reply_to_automate_email_replies/,Automate,"Hello folks,

It blows my mind that there are so many applications of GPT-3. This post is a combination of information and a bit of promotion, so please bear with me.

You can use OpenAI APIs to reply to emails. Just give it the right prompt and it will generate a decent piece of text that you can quickly send out as an email.

You can get a free OpenAI account and use their ""playground"" to test this.

Prompt:

>Reply to the following email in a professional tone.  
\[Actual email\]

You can replace the ""professional"" with different modes.

This worked well. Obviously, I had to make some adjustments specific to my app. 

Here is the final demo of the feature inside my app.

https://reddit.com/link/z7sofx/video/b4owfjl6vv2a1/player

If you find this interesting, I've built many more such tiny automations in my app. You can try it at the following link -  [Elephas](https://elephas.app/?utm_campaign=r-openai-email-reply-rautomate) 

Do share your feedback.

Thanks","[""This could be extremely useful, but I bet at some point someone is absolutely going to not properly read the generated reply before sending, and it'll be the one time the text reads like someone having a stroke.\n\n(I mean, I'm not saying don't use it. Only that it's gonna happen and it'll be hilarious.)\n\nOn a related note, a similar feature which could be extremely lucrative is an app which could be trained on figuring out where incoming mails should be forwarded to. Corporations often have departmental inboxes where the emails are manually sorted every day to go to various sub-teams. Having them pre-sorted would be a big help. The feature could train itself on looking at emails in an account's outbox which had headers indicating it originally came from somewhere else (i.e. it wasn't just written as a new email from that account), and looking at the headers, subject, text, and other options and where it got sent to. (Other options could include things like the org-chart team section that the previous and/or original senders were assigned to at the time of sending.)"", 'This is like magic, maybe integrate it with your calendar to assist in setting appointments', 'Pretty cool.', 'Very very cool', '[deleted]', 'Apple products only?', 'The problem with text AI as a tool is, we people might literally forget how to express ourselfs properly, just like we forget how to orient ourselfs because of google maps for example. The price for this tool is a serious dependence on it, just like a drug.']"
Using AI-powered automation to automatically classify interested prospects in your inbox,https://www.reddit.com/r/Automate/comments/z7zpwi/using_aipowered_automation_to_automatically/,Automate,"Hi there!

At [Levity](https://levity.ai/) we help people and businesses save time usually spent on repetitive tasks and invest it where it's needed most.

We cover a lot of use cases, all of them working around unstructured data - take a look[here](https://levity.ai/use-cases) at all the processes you can automate!

One of our main use cases is email automation - we help marketing teams identify spam or interested prospects and label them in their inbox.

One of the companies we help is [Incendium Strategies](https://www.incendiumstrategies.com/)\- in their particular case, our platform automatically tags all incoming emails and classifies them according to whether it is coming from an interested prospect or not. Take a look [here](https://levity.ai/success-stories/outbound-email-automation) to see exactly how they made this happen by implementing Levity into their processes!

I'm happy to have a chat or answer any questions you may have. You can also book a one-on-one demo on our homepage if you'd like to see how Levity could work for your particular use case!",[]
New Machine Learning HD Video Transformer AI | New Neuralink Brain Computer Interface Rival Uses Photonics To Transmit Information Through The Retina | New AI Invents Millions of New Materials,https://youtu.be/TE33knxncLg,Automate,,
Tech Religion,https://www.youtube.com/watch?v=EgeuhWsmpNU,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
How can I automate a task on a website that lacks an API?,https://www.reddit.com/r/Automate/comments/z7btbv/how_can_i_automate_a_task_on_a_website_that_lacks/,Automate,"My mother owns a clinic and uses [Fusion Web Clinic](https://fusionwebclinic.com/insights/), and I'd like to automate the generation of reports on the number of cancellations. But there is no API, how can I around this?","['One option is Selenium\n\nhttps://www.selenium.dev/\n\nPersonally I use it with python, but it work with different languages', ""Before you go in a different direction, consider sending an email to the company that develops that product and ask them if they have an API (and if they can give you access). \n\nSome companies don't put their APIs out there but still have one and may share access."", 'try UiPath or Automation Anywhere as well to see the options you have. You are thinking in the right direction.', ""Are you a programmer? There is always the option of web scraping, although that is not a preferred solution. I'd be happy to help you with this if you need, but I don't have prior experience with Fusion Web Clinic. DMs open :)"", ""I've done this specific task with selenium for a different website. Once you get the hang of it its fairly easy"", 'Hello there!\n\nPlease feel free to schedule a call with us and we will be happy to assist you with your requirements!\n\nhttps://calendly.com/d/grr-gmq-vpw/30-minute-meeting']"
Zapier vs. AT Automations vs. Make (take the quiz),https://www.reddit.com/r/Automate/comments/z705tc/zapier_vs_at_automations_vs_make_take_the_quiz/,Automate,"I made a free quiz to help you decide between the three platforms: [https://www.fillout.com/blog/zapier-vs-make-vs-airtable-automations](https://www.fillout.com/blog/zapier-vs-make-vs-airtable-automations)

Are there any other platforms I should be considering?","['Make vs Zapier makes sense, Airtable serves other purposes (even if they support automating workflows, which of course they do).']"
Best open-source/self-hosted automation tools,https://www.reddit.com/r/Automate/comments/z6b5w9/best_opensourceselfhosted_automation_tools/,Automate,"I played a lot with multiple automation tools in the last two weeks. I was focused on finding open-source and free solutions.

I narrowed down my research to these 5 open-source/free automation tools that you can use right away. Two of these are ready-to-use Zapier alternatives.

1- **Huginn** \- Huginn is a system for building agents that perform automated tasks for you. It‚Äôs like creating your own personal assistant, but without the need to learn a programming language.

Hosting: Self-hosted.

2- **automatisch** \- It is an automation tool that lets you easily create workflows in your web browser with no coding knowledge required. It's a user-friendly Zapier alternative.

Hosting: Self-hosted (for now).

3- **n8n** \- N8n is an open-source, no-code automation tool that lets you quickly create workflows with its drag-and-drop interface. It is based on nodes so you can connect anything to everything. The best Zapier alternative I've seen.

Hosting: Desktop, hosted, & self-hosted.

4- **Beehive** \- This is similar to how Huginn works. It's an event and agent system. Agents are triggered by events and perform their actions. There are multiple integrations (called Hives).

Hosting: Self-hosted.

5- **Power Automate** \- Microsoft's official no-code automation tool. It allows you to create flows for automating your tasks. It also has a desktop version (included in Windows 11). You can automate almost anything with it.

Hosting: Desktop and hosted (cloud flows - not free).

I have created some flows to automate Google Chrome using Power Automate, which works nicely. I am also using the n8n desktop version to connect multiple services I use.

\---

Links to these tools are in the comments.","['Power Automate: https://apps.microsoft.com/store/detail/power-automate/9NFTCH6J7FHV  \nn8n: https://n8n.io/  \nHuginn: https://github.com/huginn/huginn  \nAutomatisch: https://automatisch.io/  \nBeehive: https://github.com/muesli/beehive  \n\\---  \nHappy automating! ü§ñ üöÄ üí™ ‚ú®', 'Great overview. Thanks. the nice thing about n8n desktop is that it provides you with a webhook endpoint on there site without the need to have your own domain or open a port. I found this super convenient for testing.']"
"Why every country in the world will have a vertical farm by 2035? Fascinating talk from Jesper Hansen, YesHealth",https://youtu.be/b9hap2ROKWo,Automate,,
"The robot is just a tool: Responsible use requires good research, education and training. Fascinating talk from Dr. Mark Slack, Chief Medical Officer, CMR Surgical",https://youtu.be/mk1wyJCGl2k,Automate,,
"Breakthrough Open-Source Minecraft General AI Does 3000+ Tasks | New Google DeepMind Interactive Video Game AI Can Talk, Listen, Ask Questions, Navigate, Retrieve Info, Manipulate Objects, & Carry Out Numerous Other Tasks Like A Human",https://www.youtube.com/watch?v=W0PxikDCL_g,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
"Anyone have any tips for automating WordPress (development, design, connecting api databases, UX, updating content, etc.)",https://www.reddit.com/r/Automate/comments/z4mki0/anyone_have_any_tips_for_automating_wordpress/,Automate,"Any wordpress developers here have any suggestions for automating the development process of wordpress? Specifically for developing headless sites and connecting to external api databases? 

There are a bounch of new wp plugins to automate these processes with little coding, but there's just too many options to choose from. Anyone have any experience with any of these plugins? 

Thought I would ask if there are any experienced users here I before spend a bunch of time learning and testing a bunch of different plugins one-by-one.

Any suggestions are appecreciated, thanks in advance.",['This sounds to me like one of these situations where you‚Äôve convinced yourself WP is the solution to whatever problem you have. I‚Äôd definitely take a step back and first try to find a different solution.\n\nWhy on earth would you want to have WP as a backend for anything?']
A Robot that can build Gundam models at the Gundam Factory Yokohama,https://youtu.be/Oe94bSyBEaY,Automate,,
"How to utilize Slack, Trello, Figma & Zapier for my company?",https://www.reddit.com/r/Automate/comments/z3tnl5/how_to_utilize_slack_trello_figma_zapier_for_my/,Automate,"I am hired as a automation expert to a company of to streamline their communication. The company has 50 employees and 6 teams.

We have got Slack, Trello, Figma & Zapier for all the works.

If you have worked with these softwares, tell me how to smoothen the communication between team.

You may say: Make separate trello boards for teams.
Ans: We did, but, still we have communication problems.

Question for you, how do I automate figma and trello?

Please Dump all the toughts and all the resources you have.",['Start here: https://slack.com/intl/en-au/help/articles/231967387-Trello-for-Slack\n\nThen here: https://help.figma.com/hc/en-us/articles/360039829154-Get-Figma-notifications-in-Slack']
"How can system-wide collaboration fix system-wide problems? Fascinating talk from Caroline Gorski, CEO, R¬≤ Factory at Rolls-Royce",https://youtu.be/Mv5pNW2cyjk,Automate,,
Best Live Dashboard Solution for KPIs???,https://www.reddit.com/r/Automate/comments/z301yf/best_live_dashboard_solution_for_kpis/,Automate,"Hello Everyone, 

I'm running a B2B agency and we're currently going through rounds of automation. One of the things that I'd like to implement is a dashboard that contains the following: 

\+ Financial data (MRR, growth, etc.) - coming from Quickbooks

\+ Customer success data (# of new customers, total number of clients, retention rate, etc.) - coming from Hubspot and Google Sheet.

I was wondering if there's a service that allows to integrate all these data sources and beautifully visualize them as live dashboard that can be accessed through the phone, laptop, and maybe also displayed onto a TV. And it'd allow for user management (certain people within the company can see certain information). 

Thanks in advance",[]
New Nvidia AI Turns Text To 3D Objects 8X Better Than Google | New Nvidia Video Style Transfer AI | New Differential-Equation Based Neural Network From MIT Solves Brain Dynamics,https://youtu.be/CbVDaCEdhEM,Automate,,
Auto call and relay message,https://www.reddit.com/r/Automate/comments/z240ac/auto_call_and_relay_message/,Automate,"I‚Äôm looking for something that will call a number once a day and record that message. Then send me a text/ email/ notification/etc. with the recording.

The place I work for uses a hotline for work expectations the following day and I don‚Äôt want to call and wait for the message to be played. I want it when I‚Äôm ready. Saving me like 10 seconds per day :)",[]
Email Automation,https://www.reddit.com/r/Automate/comments/z1zjpc/email_automation/,Automate,"Emails are the primary form of communication used by businesses worldwide and are a crucial part of any business organization. Using email automation, you can automate your repetitive email tasks, including opening emails, reading and processing them, sorting them, extracting data from them, working on them, and responding to them accordingly. ElectroNeek offers integrated email activities that are ready to be used for automation across various platforms, including Microsoft Outlook, Microsoft 365, Google, Yahoo, iCloud, Yandex, and others. Deep dive into how you can automate your email activities here: [https://forum.electroneek.com/t/email-automation/1102](https://forum.electroneek.com/t/email-automation/1102)

Be part of our community, register here: [https://forum.electroneek.com](https://forum.electroneek.com/)

&#x200B;

https://preview.redd.it/joo6mgqwfj1a1.png?width=1620&format=png&auto=webp&v=enabled&s=f86de5a37192f999f938c94b9f02f89373e3b2f9",[]
Scan Vinyl barcode ‚Üí Play on Spotify ?,https://www.reddit.com/r/Automate/comments/z199lj/scan_vinyl_barcode_play_on_spotify/,Automate,"Hello automation friends.

I'm searching for a way to simplify the use of my vinyl collection for my significant other.  
A way could be to scan the barcode of a vinyl, and get redirected to the spotify page of the album.  
Is there any of you that use a third party apps, script or iOS shortcut that could do this trick?  
We mainly use Apple environnement and Sonos in the house.

Thanks a lot :)","[""This could be programmed by using the Discogs API to turn the barcode into the album name, and then using the Spotify API to find the album page.\n\nOn IOS or apple specifically, I'm not sure. This could be a web app."", 'If you were willing to print some stickers for it instead, Spotify Codes could work ‚Äî> https://www.spotifycodes.com/', 'Slip an NFC tag into your vinyls and then use Apple shortcuts to lead to the Spotify URL.', 'A friend of mine built an app for this! https://apps.apple.com/us/app/nowplaying-music-facts/id1596487035', 'I think better to use nfc stiker inside']"
"How to connect Python to WordPress, detailed tutorial",https://www.callmefred.com/how-to-connect-python-to-wordpress/,Automate,,
Jotform Reverse Engineer help,https://www.reddit.com/r/Automate/comments/z1ak0o/jotform_reverse_engineer_help/,Automate,"Hi! So in short, I've pulled apart the website and grabbed all the assets I can, however I still notice that within for-formuser.js - the submitted data gets shot over to [submit.jotform.com](https://submit.jotform.com) \- anyone know what kind of data/file is being sent? I know this depends on several things like the integrations you want, but we're kinda stuck. What we want to do:  


\- Rely on our own webservers to handle the html/js/css - which we've set up and tested   
\-Have the form we made send out a report email via our own servers  
\-Have the form data integrate into a SQL server we can query to run reports.   


We're a non profit food bank and really needing some direction on what needs to be done at this point. Thanks!",[]
GUI automation tool,https://www.reddit.com/r/Automate/comments/z0l2mf/gui_automation_tool/,Automate,"Hey all!

I am looking for a framework or tool, which can automate GUI software on Windows. A client of mine uses an application which poorly does not have any API. The only way of automating would be inserting directly to the DB. The vendor of the software locks th DB however and acces to it is just possible after a paid consultancy by the vendor.

So I am looking for a framework for automating GUI actions. It is just about inserting data into a basic form. Which tools should I look for?","['autohotkey can write scripts that move mouse and do stuff. Might help you.', ""pyautogui(https://pyautogui.readthedocs.io/en/latest/) and autoit.  However it may be better to start by looking at how they send to the database.  What do you know about the front end and how it connects to the DB?  Are you able to view the traffic and check if it's unencrypted?  Is this in a browser or other software?"", 'Search for Robotic Process Automation (RPA) software. UiPath is one such package.', 'Power automate, or uipath might be Worth looking into', 'Or use the Microsoft Power Automate if you have it. Its included with Office 365.', ""Looks up Joe Glines on YouTube. He has awesome courses and great video tutorials for Autohotkey..it's exactly what you need..""]"
Automate FB message with Image,https://www.reddit.com/r/Automate/comments/z0t1we/automate_fb_message_with_image/,Automate,"Hi!  My goal is to make the following:   
I have a folder with n number of Images.   
Ideally, rand an image and send it to via messenger to my own user ID every day at specified time.   
\* Extra points if it could be sent from a facebook profile, rather than a page.  


Any idea of how to get this done ?   
I thought about creating a zap (zapier) that monitors google drive) but IDK.. Please help",[]
Automate youtube uploads? (AI/generated content on demand),https://www.reddit.com/r/Automate/comments/z0d3wd/automate_youtube_uploads_aigenerated_content_on/,Automate,"A perfect example (many of you will likely have seen this channel):

[https://www.youtube.com/@RoelVandePaar/videos](https://www.youtube.com/@RoelVandePaar/videos)

He uploads every minute, his software takes text from web forums (questions and answers) and compiles it into a video presentation, with a pre-recorded intro. Though this many seem a nuisance, his program has managed to produce great tutorials on fixing technical problems.

I would like to do something similar (though specific to a community), basically taking info from the web and putting into a unique video format, with relevant images. Can anyone help me with the basic idea? Would screenscrappers, API or a single script (such as with python) be able to do this? Any information would be an enormous help.",[]
new SNAPCHAT feature transfers an image of an upper body garment in realtime on a person in AR,https://i.redd.it/p38td2lbhw0a1.gif,Automate,,
Breakthrough Machine Learning AI Runs Nuclear Fusion Reactor | New AI Supercomputer With 13.5+ Million Processor Cores | New Brain Model For Conscious AI,https://youtu.be/HeXA9C1A9HU,Automate,,
Artificial Intelligence & Robotics Tech News For October 2022,https://youtu.be/QrXnYHubFPc,Automate,,
Ideas for intake process,https://www.reddit.com/r/Automate/comments/yxfbv8/ideas_for_intake_process/,Automate,"Hi everyone, I am setting up an automation department for my company, and wanted to get your thoughts on how everyone here has used intake process. what worked and why did you like it or didn't work, 

I've personally had experience with an intake from on MS Access, Power Apps, MS Excel and Jira as well.   


i want somehting super duper simple for the business, so simple that even a pre schooler can do something. 

&#x200B;

also for the intake, what data is important to have in your opinion?

for me i always want hourly savings and the SME as well as data on region and team etc

&#x200B;

thank you guys",[]
Modl.ai raises $8.4M to develop AI-driven play testing and QA bots,https://www.reddit.com/r/Automate/comments/ywvfqm/modlai_raises_84m_to_develop_aidriven_play/,Automate,"We are thrilled to announce that we have raised $8.4 million to redefine the game development process. Our goal is to make game development more efficient and enjoyable by automating processes like exploratory testing, quality assurance testing‚Äîand many others.

And we can‚Äôt do this alone! Thank you for being so supportive. We‚Äôre eager to get the product into your hands, and this funding will help us get there faster.

[https://venturebeat.com/games/modl-ai-seriesa-ai-bot-qa-testing-griffin-gaming-microsoft-m12/](https://venturebeat.com/games/modl-ai-seriesa-ai-bot-qa-testing-griffin-gaming-microsoft-m12/)",[]
"I created an ""AI"" to automate the testing of my game Novus, with an estimated 75 hours saved over the last 3 days from using it",https://v.redd.it/v8ih45f7660a1,Automate,,
ERP integration best practices,https://www.reddit.com/r/Automate/comments/ywr9a9/erp_integration_best_practices/,Automate,"I'm putting together a proposal for an internal systems integration project for ERP, ecommerce and CRM and have been researching project plans and best practices. so far ive found this guide which has a handy workbook. is there anything else beyond this that i should be considering? https://www.codelessplatforms.com/blog/system-integration-best-practices/",[]
From reddit to insta stories,https://www.reddit.com/r/Automate/comments/ywk2rn/from_reddit_to_insta_stories/,Automate,"Thank you for reading this post. 
Is there a tool that allows me to take a certain type of post from a certain subreddit and automatically create a story on Instagram, posting it automatically?","['If you don‚Äôt find anything, I want to give you one piece of advice if you want to code it yourself:\n\nYou can get the json of a reddit post/subreddit, you don‚Äôt have to scrape it yourself']"
Breakthrough Google Reincarnation Reinforcement Learning | New Microsoft AI For Realistic Faces | New Dual Arm Robotics Tech | New Meta AI Solves 5X More Math Theorems Than Any Model Before It,https://youtu.be/_kY_Ca5sbBs,Automate,,
I built an AI() custom function for Google Sheets,https://www.reddit.com/r/Automate/comments/ywhnqw/i_built_an_ai_custom_function_for_google_sheets/,Automate,"This function lets you do simple things like classification, and text/copy generation. 

But the really cool thing is it can be used as a general natural language programming tool. Don't know how to TitleCase a bunch of data in Sheets? Just do AI(""Titlecase this:""&CELLNUMBER).

Check it out and let know if you find it useful! https://www.abiraja.com/blog/natural-language-programming-in-google-sheets

Instructions for running this on your own sheets are in the blogpost above ^",[]
Is RPA the right solution?,https://www.reddit.com/r/Automate/comments/yvht8a/is_rpa_the_right_solution/,Automate,"Hello Automate experts,

I am interested in automating lead creation using LinkedIn Sales Navigator. There are different pieces to the process but the specific part to this that I am trying to figure out is entering a company name and job title into sales navigator and then taking those results and entering those names into a spreadsheet. So the formula I envision is a spreadsheet with company names and then the same job titles for all companies. RPA takes that data and one by one conducts searches in LinkedIn and the takes those results and copies and pastes them in the spreadsheet. Is there a better way to do this? Is this possible?","[""Everything you described is possible with any RPA platform. I'd specifically recommend UiPath Community Edition, which is free if you're a smaller company (below a certain threshold of employees & revenue). I don't see any requirement you listed that would take longer than a day or two to build.\n\nThe same automation could also be built with traditional programming (not using any RPA platform), for example using Python + a web automation library (e.g. BeautifulSoup, Selenium, PlayWright) + an excel library (e.g. openpyxl)""]"
Record user input via a screen capture AI?,https://www.reddit.com/r/Automate/comments/yul3ha/record_user_input_via_a_screen_capture_ai/,Automate,"Hi guys, I'm very new to automation and coding in general, I'm not sure if I asked my question right but I'm imagining of a program that initially records a user's mouse movement/keyboard inputs for a time and then save it as a sort of script so when I run it next time, it does the recorded movement by itself. Is it possible?

There isn't a task I'm handling currently that relates to this, so it's more simply of a random thought instead of a task I want to automate, it might bring some sort of relevance in the near future though

I've heard of keylogging but it only records a log of the user input but the mouse input is only collected through means unrelated to screen capture(at least how I' think it to be, didn't have firsthand experience with it)

So yeah. To summarize, I'm asking for the possibility of a program that can :

1 - record a user input through screen capture

2 - save it as a script

3 - run and execute recorded movement captured","['seems like a pretty basic search:\n\nhttps://www.google.com/search?q=record+all+user+input+mouse+keyboard', ""https://pyautogui.readthedocs.io/en/latest/ - This doesn't exactly record the user input.  You'd have to script it.  For example Search for an image on screen or move the mouse to a location."", 'Lookup macros.', 'There a whole industry around this. I like UIPath, they have some nice polished demo videos on YouTube. They also incorporate AI to identify common user actions that are ripe for automation!']"
This Youtuber Builds Robotic Legs For Snakes Because He Feels Bad For Them,https://yodoozy.com/man-develops-snake-legs-rectifies-evolutionary-mistake/,Automate,,
New Amazon Warehouse Automation AI Robot | New Google AI Robotics Play Ping Pong Like A Boss | New Machine Learning AI For Brain Tumors,https://youtu.be/r2VcA7nMJs8,Automate,,
YouTube AI Automation: 1000 Subscribers and 4000 Hours Watch Time in 12 days,https://www.youtube.com/watch?v=eJYypn8Az3k,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Using AI GPT J to extract company names from news headlines,https://www.reddit.com/r/Automate/comments/ysb45a/using_ai_gpt_j_to_extract_company_names_from_news/,Automate,"I'm gonna show you how I'm extracting company names from a headline using GPT J, Make and Google Sheets. This would be particularly useful if you're scraping some headlines from the internet, like news articles or blog articles, and you want to build a contact list based on the company name.  


[https://www.youtube.com/watch?v=2N9UqJ68clQ](https://www.youtube.com/watch?v=2N9UqJ68clQ)","['Awesome project, thank you!']"
PDF Form Data to MS Access,https://www.reddit.com/r/Automate/comments/ysjd70/pdf_form_data_to_ms_access/,Automate,"Hello all!

I have a tedious task that I am hoping that the fine people of Reddit can help me automate or, at the very least, help me implement a more efficient process.

The existing process is as follows:

1. Employees from Company A fill out a PDF form after every client interaction
2. Company A employees save the form in a specific folder for Company A Data Entry Person
3. Company A Data Entry Person prints all forms from folder
4. Company A Data Entry Person enters all fields into an MS Access form controlled by Company B
5. Company A Data Entry Person writes the Unique ID assigned to the MS Access entry in the upper corner of the PDF form paper copy.
6. Company A Data Entry Person files a paper copy of the PDF form in a specific filing cabinet.
7. Company B provides a report to Company A that summarizes Company A's monthly entries into the MS Access DB.
8. Company A Data Entry Person compares Company B report with Company A numbers (gathered by hand counting specific fields on the paper copy of the PDF forms).
9. Company A Data Entry Person corrects or disputes any discrepancies and informs Company B Data Entry Person that they can rerun the number and provide a new report.

Some variables to consider are as follows:

* Company A does not have access to anything in the MS Access DB except the MS Access forms. This is a contracted job that pays based on what is input into MS Access DB.
* PDF forms get assigned a unique ID when submitted into MS Access. The ID gets written on a paper copy of the PDF form to associate what is input in MS Access. It is how Company A disputes discrepancies between Company A's and Company B's data. Ideally, the data matches between the two. However, they never do.

I will add that the PDF form originated as a Word document supplied by Company B. I am open to converting back to the Word version if it helps. I couldn't see that being the appropriate file type for this application.

I believe that is the most relevant information that I can provide but will provide more if needed. This process involves thousands of PDF forms, and entering them one by one into the MS Access DB is killing Company A's productivity. There has to be a better way. Please help r/Automate!!!","[""This is perfect for RPA but RPA ain't free!"", 'We can design a process manual of two pages to do a semi-automated way that will bring you, with your skills, to automate completely after implementing the semi-automated. We understand that all pdfs are same format.\n1.- we would need two - three pdfs to check format\n2.- after checking our feasability to do it, we would make you an offer to make this 2 pages manual to do the semi-automated process (minimum cost 500‚Ç¨)\n3.- timing: from monday 3 days for offer after receiving pdfs. 10 days for 2 pages manual\n4.- contract through freelancer\n5.- we are EU company that we do such things for local administrations.\nThks', 'Try make.com or zapier, most likely capable of working with these to integrate. Ideally you could code a quick API for it but instead those apps I suggested are point/click/drag/drop API creators', 'But really, why do you use a pdf instead a smartphone o web based form to collect data?', 'Tons of online eform options like [Formstack](https://www.formstack.com/) that could eliminate a bunch of your steps and make it so the data entry is done as the form is filled out and submitted.  The data could be supplied in a variety of formats.  Not hard to automate a good bit of your requirements‚Ä¶']"
"Are we too hypocritical for a ""As Need"" grocery automated system to eliminate food waste?",https://www.reddit.com/r/Automate/comments/yrtvsn/are_we_too_hypocritical_for_a_as_need_grocery/,Automate,"Hi Reddit,

&#x200B;

Just as the name suggests, I listened to this podcast where they spoke about eliminate food wast by downloading an app that mostly tracks that for you. But the basic idea is that you only every have as much as you need for a very short amount of time. Such as a week. only buying food in portions instead of bulk besides initial setup, this app is supposed to figure out all the buying for you but for all the documentaries and the 'don't litter' campaigns I don't think this is going to catch on and I'd love a discussion on if we actually want to work on the earth, eliminate food waste or if it's really just a fun talking point to have. A good thing to chat about at dinner parties","['Reducing food waste is definitely a worthy goal; there is, however, tremendous financial upside for supermarkets, large food corporations, etc that influence you to buy more than you need.\n\nI think we rely way too much on PSA campaigns in order to never ask what deeper motivations are driving wide scale behaviors, despite them looking like individual actions.\n\nAn optional app would probably never be used.\nA required app (via government or corporations) would be abused by those bodies. \n\nWhen basic human needs have a price tag, shady business practices and corruption have no bounds because people will pay anything for them', ""Yes.  Most people already shop in a way that suits them.  Food waste is already lost money to each person/company involved.  An app to fix that may be useful but they have already made the choice to accept that or they would have already changed their actions.  Add in tracking concerns and competition(uber eats, etc) and I don't think it would be worth the time.  Feel free to prove me wrong.  But that's not going to be easy."", 'I think our food system has to be reimagined in order to eliminate food waste. \n\nOur solution is to facilitate food production at the point of consumption, and grow your produce successively so that only what you want to eat is ready for harvest. \n\nI don‚Äôt k ow what a good solution is for all the rest of the products that can‚Äôt be grown though!', ""It's a smokescreen designed to make you think that individual people are causing most of the planet's waste, as opposed to corporations."", ""Oh. If you're interested in the podcast it's [https://open.spotify.com/show/5Ls5qbBEpKatFZkpExDIZ9](https://open.spotify.com/show/5Ls5qbBEpKatFZkpExDIZ9).  You can watch it first and get back to me if you want"", ""What's the name of the app? I'd give it a go.\n\nAlso, eliminating food waste is one thing, but that should also simplify your shopping and free you more time. That's double good: both on personal and social level. I think it could work if it does appeal to the second aspect, which it seemingly does."", ""All you have to do is look at the videos of parents emptying entire candy bowls into their bags to know that the population in general has 0 self control.  The vast majority of people won't use it."", ""So the issue is food waste in fridges? \n\nStart going upstream in food production. Precision farming and efficient manufacturing has both a financial motive and far bigger bang for the buck. It isn't an all hands on deck problem *in the United States*. \n\nTry not to over buy. Try not to overeat. Try to be healthy."", '>But the basic idea is that you only every have as much as you need for a  \n very short amount of time. Such as a week. only buying food in portions  \n instead of bulk besides initial setup\n\nAs somebody with no car, I only buy as much as can fit in a backpack. I rarely have to throw away food, as I mostly do not have fresh food for more than a week.']"
Way to be notified by Google maps or Waze if there is traffic on my morning commute and suggest best route?,https://www.reddit.com/r/Automate/comments/yrxvgu/way_to_be_notified_by_google_maps_or_waze_if/,Automate,,"['Tried looking this up, looked at IFTTT but couldn‚Äôt find anything specific. I can take about 3 different routes to work each morning. Id love if I got an alert if there was a traffic jam or accident so another route would be better automatically', ""Google Assistant has this built in and it's free""]"
Centralized Solution for Tasks/Scripts with Redundancy,https://www.reddit.com/r/Automate/comments/yre5h0/centralized_solution_for_tasksscripts_with/,Automate,"We support thousands of computers and servers in different network layers from various ""script servers"" for different team members. I am hoping to centralize all scripts/tasks to one pane of glass and also have a way of reporting on if those scripts ran successfully or had issues. I have heard ""PowerShell Universal"" can do this, but was wondering what other companies were doing. We are also trying to have more control/tracking of our scripts with version control, etc. What are others doing as far as running scripts (shell/bat and powershell) against several different servers/computers and environments from a central location (while having redundancy so the server can be rebooted or if there is an outage, etc).","[""Sitescope script monitors. SiS also provides dashboards and alerting. I'm not sure if script monitors are included in the free version.""]"
Join us in our E-commerce Media Automation Hackathon,https://www.reddit.com/r/Automate/comments/yrwhnn/join_us_in_our_ecommerce_media_automation/,Automate,"Inviting automation enthusiasts to build apps to automate the future of E-commerce ü§ñ

**When?** 11-17 November 2022

**Location:** Virtual üåê

**Prize:** USD $200 cash prize + media rendering API credits to help you launch your product

Details below ‚¨áÔ∏è

Over the past decade, we‚Äôve witnessed a rapid digital transformation in online commerce. Content like user-generated content, testimonials, product images, data-driven creatives, etc in the form of digital media has been a crucial part of marketing for sellers. The demand for product content is only going to grow as more users adopt online shopping.

To keep up with this demand, we need innovative and efficient ways to tackle these problems. That is exactly what this hackathon aims to solve.

What can you build for this hackathon?

You can build anything that uses the Shotstack API to generate media and helps the E-Commerce sellers. Some project ideas could be:

* An automated product video generator plugin for an E-Commerce platform like Shopify, Amazon, Wix, WooCommerce, Ebay, BigCommerce, Magento, Etsy, etc.
* Web apps that automatically generate product content tailored for e-commerce.
* E-commerce tailored design marketplace.
* A personalized video bot for e-commerce platforms that automatically generates personalized videos when customers buy.
* Data-driven automated media generator for re-marketing based on customer behavior.
* Testimonial banner generator that automatically generates content from product reviews for social media marketing

and much more. To see it in action, try our [automated product promo video generator](https://shotstack.io/use-cases/scenarios/api/generate-promo-videos/) from code.

How to participate?

* Submit the [hackathon entry form](https://forms.gle/Fik7gpk2Qi81mrqe7)
* Sign up for a free [Shotstack account](https://dashboard.shotstack.io/register) to get your API key
* Use Shotstack API to build your submission
* Submit [your project here](https://forms.gle/dwAozLyXFT2B7dz96) before 17 November 2022, 11:59 p.m.(PT)

[**Visit our website**](https://shotstack.io/learn/shotstack-hackathon/) to learn more about this Hackathon.

If you have any questions, then use our [community forum thread.](https://community.shotstack.io/t/media-automation-for-e-commerce-hackathon/376)

**We can‚Äôt wait to see what you build!**",[]
Indoor positioning system basics for managers and non-technical people,https://www.reddit.com/r/Automate/comments/yqebew/indoor_positioning_system_basics_for_managers_and/,Automate,"[https://www.youtube.com/watch?v=ELw2fd3nsc4](https://www.youtube.com/watch?v=ELw2fd3nsc4) \- video explanations

[https://marvelmind.com/pics/indoor\_positioning\_system\_basics.pdf](https://marvelmind.com/pics/indoor_positioning_system_basics.pdf) \- presentation

https://preview.redd.it/a31kmjqafwy91.png?width=960&format=png&auto=webp&v=enabled&s=4667072a116b462a4b9395a82cecfb37526b0387",['You misspelled advertising.']
Power Automate vs. Python,https://www.reddit.com/r/Automate/comments/ypvyh0/power_automate_vs_python/,Automate,"Hi Everyone!

I'm working on my MS in Data Science and would consider myself proficient in Python. I just started a new job and I'm sitting through some very long training sessions on Power Automate, which I've never used or heard of before. 

From what I've seen thus far, it seems like Power Automate is just a bad way of writing code and automating things. Seems like it would be easier to just write a script and use Task Scheduler to run it to check for triggers. 

Am I missing something here?

Thanks!","['*Note: I\'m operating under the assumption that you\'re talking about Power Automate Desktop (the RPA software), not Power Automate (the cloud service), since you mentioned Task Scheduler. If this assumption is incorrect, please explain why you\'re comparing two very different types of solutions. Otherwise, read on.*\n\nSure, it might be ""easier"" to ""just write a script and use Task Scheduler to run it,"" but take a moment to think about all of the implicit prerequisites in that statement. \n\nFor starters, you\'d have to know at least one scripting language inside and out, forwards and backwards. I\'ve been writing PowerShell, Python, Bash, and even Batch (CMD) scripts for many years, some as far back as the DOS days, and even I\'m not skilled enough in any of those languages to fully replicate all of the features that can be strung together in Power Automate Desktop, which doesn\'t require even a single line of code. Then there\'s the whole process of tinkering with the options in Task Scheduler until you find the right ones that make it work the way you want. And of course you\'d better hope you didn\'t leave any weird bugs in your code that only appear under very specific circumstances, because debugging a script running in the background with the screen locked is not for the faint of heart.\n\nNow, pretend you\'re an average non-technical office worker with a lot of repetitive tasks on your daily to-do list, which really ought to be automated, but you\'ve never written a line of code in your life, and your IT department can\'t/won\'t write the code to automate those tasks for you. What are your options? You could spend the better part of a decade (or more) learning how to code and building a skill set broad enough to cover all of your automation use cases, but where are you going to find the massive amounts of time for all that extracurricular work, especially considering that you already have too much work on your plate as it is? Also, what if you\'re just not very good with technical stuff? Are you just S-O-L?\n\nEnter Power Automate Desktop. If you\'re a god-level scripting wizard, you simply aren\'t part of its target demographic. Sorry to disappoint you. But if you\'re a normie like 95% of the users out there, drowning in repetitive tasks at work, and you\'d really like to automate at least some of your daily processes, but don\'t have the time/energy/resources/etc. to code it yourself, then you are exactly the type of person Power Automate Desktop was designed for.', ""It is a pro/con situation. You can throw together things in the MS universe with power automate in almost no time. It takes care of the authentication and a bunch of tolerances because it is first and foremost meant to be rode hard by business users. You can start to get some advanced features to do things. Generally, if it is in the MS universe or has an API that plays nice with power automate then you can use power automate. \n\nPower automate will run into maintainability issues. I don't think it has any decent version control. The interface will start to get clunky and requires workarounds sometimes. It's fine. \n\nPython is another tool that has a place. Easier? IDK maybe sometimes. Sometimes maybe not.   You might find pulling data from a bunch of MS sources is done in PA and then analyzed in Python. Or you write an API that the PA script works with. \n\nPowerapps itself ends up relying on PA for a bunch of things. I wouldn't be surprised if a bunch of other power platform things work easier on PA than python, but that sounds more like a future-you problem :)"", ""Power automate works great for very simple apps but isn't great at anything complicated. \n\nNo version control, no ability to have multiple people working on it at once, just overall not maintainable. \n\nBusiness leads pushed it because they were told it was easier than normal development and there would be huge cost savings. In reality, it was a huge burden and resulted in workaround after workaround just to get things working the way the business wanted it.\n\nAfter about 6 months we decided to abandon it and go back to regular development. \n\nIn short, it's too simple for regular devs and too complicated for non-devs."", 'My first instinct is that for Unreal Engine, people use a combination of C++ and Blueprints, which is just a way of coding in C++ in unreal engine to speed up processes and visualize your logic, making it easier to see patterns and troubleshoot.\n\nWhat people say is that certain things are suited more for hard coding while others are suited for blueprints.\n\nSo, maybe to help prime your mind for openness on something more visually set up....\n\nDifferent tasks are suited to different tools, and sometimes ones experience with one tool is a major factor on which tool is more ideal to use.', 'I use it mostly for onboarding or off boarding processes where users fill out a form and then it goes through an approval process before continuing the next steps or reaching out to some api to create their user in some system and then sends them an email with instructions. A Microsoft Form can only be accessed within the org if you use office 365.', 'Following', 'Some of the comments definitely touch on some of the benefits like authentication, ease of use for non-devs, etc. \n\nI might say the closer comparison might be it‚Äôs sibling Logic Apps. Do I create it using a low-code option or do I write an Azure function with code?\n\nIf you have a lot of integration with different apps, the ease of connecting and having someone not necessarily a developer supporting and maintaining definitely makes Power Automate / Logic Apps enticing, particularly if you aren‚Äôt familiar with all of the different SDKs with different apps and having to maintain secure credentials. \n\nOne thing that Python has a leg up is straight up performance. With Power Automate, you sacrifice performance by adding overhead that makes an easier UI and have it take control of auth. and everything. \n\nIf ease of use is your main concern for non-devs or you have a lot of integrations with apps, Power Automate might be user. \n\nIf your main priority is performance and speed of the actual process, Python can be tuned to be much faster. \n\nIf it is something I need to be fast and do a zillion times a day, the ROI of building in code makes sense.', 'Watching']"
First Ever NASA Humanoid Robot To Release Before Tesla's Optimus| New Nvidia AI Tops OpenAI DALLE-2 & Google Imagen For Text To Image Output | New Meta AI Performs 60 Times Faster Than DeepMind AlphaFold,https://www.youtube.com/watch?v=zgfevVDLJfk,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
NEVER water your plants again! (DIY Auto Watering System),https://youtu.be/1pFTfwi9_6Q,Automate,,
Automate screen events,https://www.reddit.com/r/Automate/comments/ypkg3e/automate_screen_events/,Automate,"I have been using macros for a really long time, when i was a kid i developed vbasic macros to automate farming in my games, i'm a big fan of logitech / razer / any peripheal with the ability to set macros .. (\*\*\* insert you know i'm something of a developer myself joke \*\*\*).   


Anyways I allways enjoyed scripting but at the end of the day those inputs require human interaction to activate.... I know this is not strictly true because you always can use an api to retrieve recent information and activate those macros... but its painful to build an api for every game, chart software or website.  


My question is, its 2022, is there any nice machine learning screen recognition library or framework? do you guys know where should I start looking? Im open for any language  


What I want to build is a software with that library that allows me to:  
\- Input the screen pattern to activate  
\- Set up a script to use when the pattern is detected  
\- Listen to all screen events  
\- input a screen pattern to stop  
\- the program stops and continue listening  


Is that approach valid? Is it too bad for computer performance? is there any better way to automate scripts without reading screen events?  


Thanks all and excuse my english.","['You mean Image recognition not screen recognition, screen readers are accessibility tools designated for blind people to interact with the computer.\n\nFYI Image recognition refers to the task of inputting an image into a neural network and having it output some kind of label for that image. The label that the network outputs will correspond to a pre-defined class. There can be multiple classes that the image can be labeled as, or just one. If there is a single class, the term ""recognition"" is often applied, whereas a multi-class recognition task is often called ""classification"".  \n\n\nYou can use Tensorflow or/with Keras to build your own image clasification software and connect it to a python screen capture software such as Pyautogui to clasificate the images and respond, but i dont know how performatic it will be']"
Automate form filling,https://www.reddit.com/r/Automate/comments/yo1502/automate_form_filling/,Automate,"How can I fill a website form with information of last year same form downloaded in pdf 

Let me explain. I want to automate to pass same information from last year from the exact text  to the new form in a website. Both form are the same just that 1 is downloaded and the other is online","['Is this something you are going to need to do regularly? [XKCD has a great chart about automation and the time saved](https://xkcd.com/1205/). If this is a one-off or only annually then it would take you far longer to automate this than it would to copy-and-paste.', 'I think [text blaze autopilot](https://blaze.today/guides/autopilot/) can do this to some extent. I use it to fill out google forms quickly for my job', "">A place for the discussion of automation, additive manufacturing, robotics, AI, and all the other tools we've created to enable a global paradise free of menial labor.\n\nDid you read this in the sidebar, and decide that this was the automation tech-support-forum?""]"
Liquid democracy simulator,https://github.com/stateless-minds/cyber-acid,Automate,,
Automate Mac app workflow,https://www.reddit.com/r/Automate/comments/ymno4f/automate_mac_app_workflow/,Automate,"Hey! Im a programmer that have a specific setup of windows and applications up when working on a specific project. 
How could I automate the setup of this?

What I want to achieve: run a script to open up vs code, run script in terminal, open up a terminal and go to a specific path and run scripts etc. place windows in a certain way.

I‚Äôm using a Mac.","[""Have you looked into the Automator app that comes with MacOS?  I've used it for simple things like drag and drop or selecting groups of files and sending them to a script via Terminal in the bash $@ builtin var.  Very handy for sending items in Finder to a script to do more processing or start up other apps with that list of files."", 'AppleScript is great for this type of thing. Works well with GUI apps and has full ZSH / BASH support within scripts. It should be dead simple if you know any C family languages.', 'Check out [Hammerspoon](http://www.hammerspoon.org/)!\n\n>What is Hammerspoon?This is a tool for powerful automation of macOS. At its core, Hammerspoon is just a bridge between the operating system and a Lua scripting engine. What gives Hammerspoon its power is a set of extensions that expose specific pieces of system functionality, to the user.What can it do for me?You can write Lua code that interacts with macOS APIs for applications, windows, mouse pointers, filesystem objects, audio devices, batteries, screens, low-level keyboard/mouse events, clipboards, location services, wifi, and more.\n\nFor example, you could write a shell script that handles all the terminal commands and then calls [Hammerspoon](https://www.hammerspoon.org/go/#ipcurl) to re-position the windows to your liking. It can even handle multi-monitor setups.', 'I use [Keyboard Maestro](https://www.keyboardmaestro.com/main/) to do this for myself. I built myself a version of exactly what you\'re asking for where I can save and reopen ""project 1"" windows/applications, ""project 2"", etc.\n\nIf you get into Keyboard Maestro, DM me and I can share with you what I built. Their forums are *very* helpful too, with tons of sample macros others have built.']"
"Breakthrough Google AI Makes Dynamic, Multi-Minute HD Videos With Changing Scenes From Text Script | New Google AI Autonomously Writes Its Own Robotics Computer Code",https://www.youtube.com/watch?v=zuycoc6XLzE,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Help with data graphing,https://www.reddit.com/r/Automate/comments/ym0v4y/help_with_data_graphing/,Automate,"I'm trying to accomplish  the goal of automating the process creating a report that pulls:

The high and low points in activity  in the last 24 hours and 30 days

The average amount of activity lost the last 24hrs and 30 days

It's able to to generate that report at certain times of the day and a click of button

Ideally the report is generated in Excel

This is all being pulled from a site that has live data graphs.


Any help would be appreciated, it's safe to say I don't have alot of programming experience. I'm struggling with this and could use any help!","[""Where is the data housed? If it's in anyway related to the Microsoft Suite Power BI should be seamless and somewhat simple with some dedicated youtube watching.""]"
PowerBI desktop automation,https://www.reddit.com/r/Automate/comments/ylwdrh/powerbi_desktop_automation/,Automate,"Hello, I'm looking to automate the whole PowerBI process, from updating the data to publishing the new data on the PowerBI web service.

My PowerBI report is fed via a MariaDB database. The connection is established correctly. 

When the schema or the structure of the database changes, I would like the PowerBI report to update automatically without any manual action on my part.

I already have a working solution based on pywinauto. The ""problem"" is that PowerBI is forced to run. At that moment, the user has to stop what he was doing.

Of course, I can automate this task at night when there is no activity on the server, but I wanted to get your opinion about a possible alternative solution to automate this process without relying on GUI",['use power bi API?']
RoboCo launches TODAY at 9am PDT on Steam Early Access!,https://www.reddit.com/gallery/yl2prd,Automate,,"[""Hello, automation fans! If you're looking for a game where you can build robots, serve humans, and (most importantly) automate your robots using Python code, make sure to check out RoboCo, our sandbox robotics game where you build robots to serve the needs of squishy, hapless humans! \n\n[https://store.steampowered.com/app/1067220/RoboCo/](https://store.steampowered.com/app/1067220/RoboCo/)"", ""9am PDT happens when this comment is 2 hours and 38 minutes old.\n\nYou can find the live countdown here: https://countle.com/8HFWR4BjI\n\n---\n\nI'm a bot, if you want to send feedback, please comment below or send a PM."", 'View in your timezone:  \n[TODAY at 9am PDT][0]  \n\n[0]: https://timee.io/20221103T1600?tl=RoboCo%20launches%20TODAY%20at%209am%20PDT%20on%20Steam%20Early%20Access!']"
The Role of RPA in Digital Transformation,https://www.reddit.com/r/Automate/comments/ylu52q/the_role_of_rpa_in_digital_transformation/,Automate,"RPA can help development teams and business users cope with changing digital transformation and internal process modifications. It's feasible that RPA will impact practically every business and function, from data security to low-code application development and deployment. 

There are many RPA Tools are available, but top 5 tools are below:

Automation Anywhere  

IBM Robotic Process Automation 

UiPath  

Blue Prism  

Rocketbot 

What do you think- What is the [Role of RPA in Digital Transformation](https://www.zenesys.com/blog/the-role-of-rpa-in-digital-transformation)??","['Big doubts as to how it could help development teams. It‚Äôs more likely to help non-developer teams. What would this contribute otherwise?', ""TBH whenever I've tried to use any it always felt way easier to just code something out than learn the ins/outs of how the RPA wants something to work. I think a dedicated person might find a use though I also wonder this:\n\nDo most people have enough free time at work to learn a new tool? Is the company paying for that time? Should EVERYONE be learning the new tool? OK, so just a couple of people become really good at RPA? Won't those people just be developers now spending their time using the tool elsewhere in the company?\n\nI fear that those in an IT development bubble greatly underestimate the difficulty of working with IT (technology, coding, sys admin, security, whatever) by those out of the bubble. More time needs to be spent with help-desk professionals? \n\nThen again, go back and look at 1989 books on how to use MS Word. That was magical then, but mostly mundane now. Maybe RPA low-code will go that route.""]"
Content Automation with Stable Diffusion + GPT-3 API + Python ü§ñ,https://youtu.be/Jg2ChBGduho,Automate,,
‚öôÔ∏èHow to use Actionable Messages in Outlook with Power Automate,https://youtube.com/watch?v=DCihYRK8w9Q&feature=share,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
No more Grocery Shopping,https://youtu.be/uts8t4WLRF0,Automate,,
Breakthrough Computer Vision AI Device | New Neuromorphic Tech Runs Deep Learning AI 100X Faster Using Photons,https://www.youtube.com/watch?v=0KUk4qUzFho,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Save Money and Time Hiring AI Secretary to do Grocery and handle other Daily Tasks,https://youtu.be/uts8t4WLRF0,Automate,,
"What makes the human hand special, and why is it worth replicating in mechanical form? Rich Walker, CEO, Shadow Robot",https://www.youtube.com/watch?v=fm7QUiRUAJA,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Chatbots in Marketing: 6 real examples,https://www.reddit.com/r/Automate/comments/yj6vt5/chatbots_in_marketing_6_real_examples/,Automate,"Hi! There're plenty of articles about chatbot benefits for marketing, but not so many about real examples of how companies use chatbots for marketing. üôÉ So I gathered 6 chatbot examples from companies like Virgin Holidays, Coca-Cola, Choose Chicago, Honda, etc., with campaign descriptions, results, and stories behind the development of these chatbots.

Info includes anything I could get from media like Adweek, Drift, Campaign Bried to Forbes, Google Business, and my company's experience. 

[Read it here](https://botscrew.com/blog/chatbots-in-marketing-6-examples-on-how-to-wow-your-customers/?utm_source=Reddit_November&utm_medium=&utm_campaign=&utm_term=&utm_content=)

Let me know if it is helpful!",[]
Introduction to UiPath ReFramework,https://www.zenesys.com/blog/introduction-to-uipath-reframework,Automate,,
Jotform to Contract to Docusign help,https://www.reddit.com/r/Automate/comments/yieq4q/jotform_to_contract_to_docusign_help/,Automate,"So we have a jotform we fill out internally with info as a client signs up with us, when submitted certain fields from that form (name, rate, start date) immediately populate a jotform PDF contract filling in the necessary fields, then that PDF is emailed to the client for signature.

The issue is there's no digital signature box available on that PDF and Jotform's new ""jotform sign"" feature doesn't integrate with that, we would have to fill out a separate contract and send it.

Ideally we want some of the fields from the form that is filled out to auto populate within a text contract that has a docusign type digital signing box that is auto sent to the client, any idea how to achieve this?","[""You read the following, and thought this was your personal tech support forum?\n\n>A place for the discussion of automation, additive manufacturing, robotics, AI, and all the other tools we've created to enable a global paradise free of menial labor.\n\nGTFO"", 'I‚Äôm not 100% certain bc I haven‚Äôt tried it . BUT you might be able to make this work by linking it to PandaDoc . So have them fill out your form , then run a Zap from JotForm to PandaDoc . All info will transfer and then they will sign in PandaDoc. You will get an email letting you know when they signed and then be done with it . Just a thought if u can‚Äôt get the signature done internally .']"
Interviewing Marc LeVine on Job Skills needed in Industrial Automation,https://youtube.com/watch?v=lYZssVpBBRE&feature=share,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
"automatic construction inflates concrete homes, just like balloons",https://www.designboom.com/architecture/balloons-concrete-structures-inflate-shapeshift-houses-automatic-construction-10-12-2022/,Automate,,
"Precise indoor geofencing solution for industrial applications (people, forklifts, robots, AGVs, cranes, drones)",https://www.reddit.com/r/Automate/comments/yi2y1j/precise_indoor_geofencing_solution_for_industrial/,Automate,"[https://marvelmind.com/download/geofencing/](https://marvelmind.com/download/geofencing/)

* ¬±2cm accuracy
* Static and mobile geofencing zones
* 2D and 3D geofencing
* Different connectivity options
* Time trigger and distance trigger for geofencing violation

https://preview.redd.it/5ye6lkdg53x91.jpg?width=1280&format=pjpg&auto=webp&v=enabled&s=af726eb62ae87e3be0cc6e17e65ffe1cd7afb2e5",['Thanks for the blog spam you could have just posted the content here ü§¶\u200d‚ôÇÔ∏è']
This robot will get you out of the dentist's office in 15 minutes instead of an hour,https://techcrunch.com/2022/10/17/cyberdontics-raises-15m-for-robotic-root-canals/,Automate,,
Automated towel folding machine,https://youtu.be/ekE2xQAAGKY?t=32,Automate,,
Celus to bring automation to electronics design,https://venturebeat.com/automation/celus-to-bring-automation-to-electronics-design/,Automate,,
First Deep Sea (1km) Humanoid Robot Drone Lets Operators Touch And Feel With Haptic Feedback | New Machine Learning Tech Transcodes Brain Waves Into Natural Language To Read Human's Thoughts,https://www.youtube.com/watch?v=2dt8sJS0rEo,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Hyperautomation: The Beacon for Your Digital Transformation Journey,https://www.impactqa.com/blog/hyperautomation-the-beacon-for-your-digital-transformation-journey/,Automate,,
What's the best text expander?,https://www.reddit.com/r/Automate/comments/yfpg48/whats_the_best_text_expander/,Automate,"I saw an ad on youtube for something called a text expander and it seemed pretty cool. I did some looking around and realized there's a bunch of different ones out there. I want to try one but I'm not sure which is the best. The ones I found are: espanso, text blaze, textexpander, magical, and phraseexpress. Which is the best?","['Autohotkey', 'Text Blaze - swear by it! Works in Chrome', 'Quillbot', ""PhraseExpress is a buggy slow program. I use Espanso, AHK, and something else I don't know the name of.""]"
Easy Invoice Manager - A Software to Automate Invoice Processing,https://www.reddit.com/r/Automate/comments/yfzh6v/easy_invoice_manager_a_software_to_automate/,Automate,"Manual invoice processing has many drawbacks which is why a lot of business are opting to upgrade towards automated invoice processing. [Easy Invoice Manager](https://easyinvoicemanager.com/) is a Software which enables businesses to fully automate their invoice processing without the need to having professional accountants. Following are some of the advantages you will get by using this amazing software: 

&#x200B;

* Data capture during Invoice upload, and no typing necessary
* Cloud storage of invoices
* Coding invoices for accounting
* Tracking current and future pay dates
* Manage cash flow with real time bank balance
* Manage vendor balances
* Get approvals on the invoice payments
* Get Accounts Payable aging reports
* Get paid invoice reports
* No accounting education required",[]
Does anyone have experience with ambi ROBOTICS?,https://www.reddit.com/r/Automate/comments/yfz795/does_anyone_have_experience_with_ambi_robotics/,Automate,"I imagine some of you work with, in some capacity, a robotics integration company. Have you worked with or started any conversations with ambirobotics? I am very interested in your experiences with them. 

In addition, if anyone would like to discuss other robotics integration companies, I'm eager to have a conversation about your experiences and the types of applications you might be working on. Specifically, I'm interested in end-of-line solutions for material handling and P&P systems for product distribution.

Automate! So future generations can be lazy! :)",[]
OCR software to monitor a folder for scanned PDFs,https://www.reddit.com/r/Automate/comments/yefewq/ocr_software_to_monitor_a_folder_for_scanned_pdfs/,Automate,"OCRvision- Windows software to batch OCR scanned PDFs in a folder to searchable PDFs

[https://www.ocrvision.com/?source=Reddit](https://www.ocrvision.com/?source=Reddit)",['How well does it handle handwritten docs?']
Text to JPG,https://www.reddit.com/r/Automate/comments/yee4v0/text_to_jpg/,Automate," 

# Text to Formatted Jpg

I have hundreds of text files that I need to use to make videos overlaid by audio. Essentially what I need to do is take a text file and convert it to an image like a jpg, and then repeat. Between the text file and jpg file, I need some formatting done, so it no longer looks like a text file.

so basically I need to repeat the following process a couple thousand times.

1. Text file
2. Format text file
3. convert to jpg

any help or suggestions much appreciated

example -

This is my text file currently -

üì∑

[text file](https://preview.redd.it/n89z4qdvx8w91.png?width=1867&format=png&auto=webp&v=enabled&s=b17cc5d66af19264db6b257fcc3ef9c7c15616d2)

current jpg -

&#x200B;

[current JPG](https://preview.redd.it/40ceigpwx8w91.png?width=567&format=png&auto=webp&v=enabled&s=d33a0e5ab4fdd9905c534c1049439d7fbaa633bb)

&#x200B;

What I would like as a jpg

&#x200B;

[something like this](https://preview.redd.it/gw7jilgyx8w91.png?width=190&format=png&auto=webp&v=enabled&s=b16db9f2217de3b8bc765461423c1b260cadba67)","['Hey OP.\n\nIf you know a little code... \nTry this \nBatch convert then to a word using a modified to your likeness normal.dotx template.\nhttps://learn.microsoft.com/en-us/answers/questions/294897/how-can-i-batch-convert-txt-to-docx-or-doc-files.html\n\nYou can also use some bulk conversion tool\n\nSee this article for some suggestions:\n\nhttps://answers.microsoft.com/en-us/msoffice/forum/all/how-do-i-batch-convert-txt-files-to-docx-with-a/8d8fd422-be34-479f-9406-f463d02ea85c\n\nAnd then you bulk convert them to pdf to jpeg using acrobat or use some of the aforementioned tools in word.\n\nBest of luck, and if possible show us how you doing so we can learn to. ;-)\n\nCheers!', ""the formatting is the problem. You'll need to learn how to use Character/Paragraph styles in Ps or ID. The rest of the automation is easy using either app.\n\nand why the F are you reposting the same question to this sub that you asked 1 day ago?"", 'https://text2image.com/en/', 'https://imagemagick.org/Usage/text/', 'You could use bannerbear for this.\n\nhttps://www.bannerbear.com/product/image-generation-api/\n\nCreate a bannerbear template, then have a little script read the files and send them to bannerbear using their api.', 'I would implement by text -> pdf -> jpg\n\nquick google will give you examples in the language of your choice', 'If you‚Äôre a Linux person you could look into LaTeX and have it output to a pdf or jpg?', 'Wrong format, use PNG. Jpeg is for photos and is lossy']"
"This Youtube live stream features automated camera aimed at airport that monitors all flight activity during day time, tracking and zooming in on flights detecting take-offs.",https://www.youtube.com/c/HappyLandings737/live,Automate,,
How can I create custom integrations/automations between apps on a more detailed/technical level than what tools like zapier offer. Is this possible to achieve via a certain programming/coding language. If so where would I start in order to learn how to do this?,https://www.reddit.com/r/Automate/comments/ye2ry7/how_can_i_create_custom_integrationsautomations/,Automate,,"['What would you want to do that‚Äôs more detailed and more technical?\n\nCoding could be the way to go. Depends on your use case! I know a few tools that would be useful before you take that leap', 'I think it depends what you wanna do']"
"Build Robots, Serve Humans, and Automate Your Creations in RoboCo - Coming to Steam EA on Nov 3rd, 2022!",https://www.youtube.com/watch?v=Kwm4kgsIb6A,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
"Stunning AI Doing Desk Jobs! Ten Tasks at Once | automation Biz Admin. | Uses Facebook Wechat, Send Video Message in RealTime",https://youtu.be/mv_k7djQEOc,Automate,,
Text to Formatted Jpg,https://www.reddit.com/r/Automate/comments/ydh06l/text_to_formatted_jpg/,Automate,"I have hundreds of text files that I need to use to make videos overlaid by audio. Essentially what I need to do is take a text file and convert it to an image like a jpg, and then repeat. Between the text file and jpg file, I need some formatting done, so it no longer looks like a text file.

so basically I need to repeat the following process a couple thousand times.

&#x200B;

1. Text file
2. Format text file
3. convert to jpg

any help or suggestions much appreciated

example - 

&#x200B;

This is my text file currently - 

&#x200B;

&#x200B;

[Text file](https://preview.redd.it/at3bczgwf6w91.png?width=1867&format=png&auto=webp&v=enabled&s=7db628607954cb217811a5012a1227552350190b)

current jpg - 

&#x200B;

[current jpg](https://preview.redd.it/ueqy4gcyf6w91.png?width=567&format=png&auto=webp&v=enabled&s=b6720c8c991ba3657b099bce53c81831fe6368dc)

What I would like as a jpg 

&#x200B;

[something like this](https://preview.redd.it/56k914d1g6w91.png?width=190&format=png&auto=webp&v=enabled&s=fa388560e6b3d5a5cf585b0441aa7e92e0661269)

&#x200B;

edit: showed examples a fixed a couple of times to a couple thousand times","['>I need some formatting done, so it no longer looks like a text file.\n\nwhat in the world does that mean?', 'I have done something like this \nReddit post --> youtube videos', 'Google data merge Adobe.', ""Could you give some examples or screenshots of what you've got to start with and the end result you're after?""]"
New AI Driven Robotics Beat Human Professional Soccer Skills | Breakthrough Google AI Edits Images With Text | New Deep Learning Tech Uses Light Waves,https://www.youtube.com/watch?v=Qf4oouquPIw,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
The Geoffrey Hinton NLP Fellowship is now accepting applications! (By Univ.AI),/r/UnivAI/comments/y865gq/the_geoffrey_hinton_nlp_fellowship_is_now/,Automate,,
how to automate screen brightness level based on light luminous from webcam?,https://www.reddit.com/r/Automate/comments/ycp9yn/how_to_automate_screen_brightness_level_based_on/,Automate,How to do that in windows 10?,[]
Automating multiple emails on the basis of google form response?,https://www.reddit.com/r/Automate/comments/yckij8/automating_multiple_emails_on_the_basis_of_google/,Automate,"The club is looking for a simplified Library system and they just want to put in a QR code that directs us to a google form where people borrowing the book can fill their details(email/student ID/etc). But we have an automation requirement, which is to send an email to the individual once the return date is close. Any suggestions on this? If not related to this, any idea how these folks can manage their library? They do not have a lot of tech experience but clearly honor system isn't effective enough lol","['I would have thought you could build this in the Microsoft stack using something like Power Automate. https://powerautomate.microsoft.com/en-us/connectors/details/shared_microsoftforms/microsoft-forms/', 'You should be able to connect this in Zapier. Zapier serves as a middleman so that your systems can talk to each other. Feel free to reach out to further discuss and see if we can create a workflow for their business. \n\nwww.sibautomations.com']"
Advice for learning to automate web interactions?,https://www.reddit.com/r/Automate/comments/yc1fz2/advice_for_learning_to_automate_web_interactions/,Automate,"I‚Äôm working on a project to download YouTube videos of songs and analyze their melodies. I would like to automate the process of:

1. Uploading a local MP3 file to https://basicpitch.spotify.com/ based on filename programmatically 
2. When processing is complete, pop open the parameters to tweak 
3. Adjust certain values on the slider to a pre-set point (ideally, but optional)
4. Download the resultant MIDI file from the web page

Can anyone describe how to accomplish this and/or point me to resources on learning this sort of automation? I have Windows and Mac machines at my disposal. Thank you!","[""How I would do this:\n\nUse a mouse+keyboard recorder tool (there are a bunch of free ones out there), and set it up so you can do all of those steps with set locations on screen for each of the buttons you need to push.\n\nCould probably set up the described scenario in five minutes. No programming required, if you just make it so the files are in order in a folder, so a down arrow key goes to the next in line.\n\nIf you're looking to actually manipulate things on the Spotify website using code and parameters based on the output of the analysis, then you'd need a rather complex plugin I think. And this might be more of an in depth java or python project."", 'Interfacing the list of filenames with the sequence recording software might be a sticking point. On the other hand it can be fast and easy as pointed out.\n\nIf you want a no-code, node-based tool, Power Automate from Microsoft could be something to look into. It can do most of what you do in Windows. Scheduling tasks is a bit hacky, though.\n\nPython + Selenium is more customizable and extendable. Learning this also teaches you skills that are much more applicable to other projects than using a piece of software that records the mouse and keyboard.']"
New Machine Learning Driven Exoskeleton Robotics | Bionic Tech For Sense of Touch In VR & AR,https://www.youtube.com/watch?v=xBCm91uAhrk,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
No Code Workflow Solution That Supports Snowflake/Jira/Email Sending?,https://www.reddit.com/r/Automate/comments/y9druu/no_code_workflow_solution_that_supports/,Automate,"Hey everyone, 

Not sure if this is the right place to ask this. I'm looking for a No Code/Low Code solutions similar to Power Automate so that all employees at my company can quickly and easily build workflows that span the multiple products that we use. Ideally the solution should have built in integrations/connectors for:

* Snowflake
* Jira Software
* Gmail (Google Workspace)

&#x200B;

After the first initial setup time cost, each additional employee should be able to setup their automation without code changes and minimal integration effort. Any suggestions for solutions that solve these requirements?","['This is a terrible idea, but there‚Äôs several middleware options that will support this. \n\n\nAs much as I hate it, zapier is probably the easiest to get to grips with.', ""I think text blaze can work for this. The company I work for uses its shared folders and organization features. It's useful because it helps us automate jira tasks and customer emails. It's free to try out and it's definitely no-code. Might be worth checking out."", ""If you haven't seen it, you should also check out [n8n](https://n8n.io/).  In the same vein as Zapier but much more extensible.""]"
One Click Automation(OCA). Enter your automation scenario to immediately receive an automation script.,https://www.reddit.com/r/Automate/comments/y8wls0/one_click_automationoca_enter_your_automation/,Automate,Is it possible to develop a service where you submit your automation concept and receive a python automation script when you want to apply an automation scenario?,"['With AI yes, you can write SQL with AI already so you definitely can develop this', 'In theory yes.  \n\nIn practice, no.  There are too many moving parts unless severely constrained by assumptions.', ""Yes, it's called hiring a programming team. Assuming you're not too worried about the 'immediately' bit."", ""That's next level."", 'This is the dream that almost every IoT company shoots for. IFTTT and SmartThings get closest, but are still really far from ""push button: get automation"".\n\nAutomations are specific to the person and the place. An automation that works perfectly for one user will be completely useless to the next.', 'After reading the comments, I realized that my thoughts were still too simplistic and restricted to the automation of software activities. In reality, software and hardware can work together to create a smart existence, which can sometimes be completed without your involvement at all.']"
Process mining as a non-data scientist,https://www.reddit.com/r/Automate/comments/y8fm48/process_mining_as_a_nondata_scientist/,Automate," Hi all,

For the past years I have been working in finance (and operations) for a mid sized corporation. I was introduced to process mining a while ago (just in presentation form). It also popped up in a recent job I am interviewing for. Both times people referred to Celonis as the software of choice.

I am wondering whether picking up process mining as a non-data scientist (university finance degree and a love for IT) is doable? I mostly worked with excel (not a specialist, but proficient) for finance purposes and do not consider myself to be a data scientist. I have no experience in programming outside of some light VBA work. Would picking up software like Celonis be relatively okay or would this be a huge challenge? Is process mining more aimed towards true data scientists or is this also applicable to more general business analysts?

Thanks in advance for some insight!

Best,

Brandermant","['I am familiar with Celonis and process mining, though I haven‚Äôt actually used their software, but I would guess like most analytics software that it will box your analytical range into what the creators of the software intended for you to answer. Therefore, it will only be a helpful skill if you work for a company that uses their software in the future. Data Scientists usually prefer to do analytics using open source programming languages, because it is so much more flexible at gleaning insights out of all kinds of data.', 'What‚Äôs the difference between process mining and RPA?', ""I work with celonis. There is rpa, Tableau, data scientists, and celonis team all in different departments.\n\nThe thing that most companies are stuck at is... what's my next action. Even the fanciest award winning dashboard may display the problem but it can't automate the next action. Celonis can or with rpa together do it. Celonis i think is bigger picture with the mapping side. The rpa teams want to use celonis to pave the way for their automations to know what to automate and then to gage if the issue was actually fixed.\n\nSkill wise to administer it you need sql and to do the fancy stuff python or r. If not you'd be browsing the dashboards to find areas of concern and develop use cases on automatation so the dev team can work on it.\n\nTo give an idea of why I think it's important to add this tool to your skillet is that the big 4 consultancies signed on as partners over the last year and some of the largest companies use the software including many of the big tech companies which are early adopters for useful tools.""]"
Breakthrough Tesla AI Supercomputer 8x Faster Than Previous Fastest Supercomputer | New Google NLP Driven Robotics Perform Complex Multistep Tasks According To Speech Commands,https://youtu.be/uEBUWHsWfnE,Automate,,
Research on customers&chatbots. You'll find insights on what to improve in your customer service automation,https://www.reddit.com/r/Automate/comments/y761co/research_on_customerschatbots_youll_find_insights/,Automate,"Hi, there! My team prepared the report on what customers love and hate about customer service chatbots. Together with our project managers, and marketers we analyzed data of thousands of conversations with real users, and made thi reserach with insigts, stats and recommendations on how to overcome struggels. 

This research would be helpful for: 

* Companies that have a chatbot that isn't performing well and they don't know why 
* Companies who want to implement a chatbot but are unsure about the best ways of creating a chatbot. 

**Important things!** 

*Here're more details about our research:*

&#x200B;

Use case: Customer support

Platform: Website

Language: English

Locations: Europe, USA

Industries: E-commerce, Retail, Retail health

&#x200B;

Here's the link to [the general article on the best chatbot practices and there you'll find the research](https://botscrew.com/blog/chatbot-best-practices/?utm_source=RedditResearchpromotion&utm_medium=&utm_campaign=&utm_term=&utm_content=).",[]
"Hello Everyone, I would like to share first version of my new project Virtual Logic Integrated Circuit project (LOTP VLIC v1)",https://v.redd.it/kylqsb2zzyt91,Automate,,
Thoughts on DAOs?,/r/cooperatives/comments/y4zig4/thoughts_on_daos/,Automate,,
Worlds Most Realistic Humanoid Robotic Arm With Artificial Muscles | Generalist Robot VIMA AI Model | Quantum Computing Breakthrough | In-vitro Neuron Learns To Play Video Games,https://www.youtube.com/watch?v=uLYf_418VDw,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
How buildings will be designed in the future,https://mechatronicscanada.ca/latest-news/5058-all-you-need-to-know-about-the-augmenta-construction-platform-an-interview-with-ceo-and-co-founder-francesco-iorio,Automate,,
"We‚Äôre all at different places in our automation journeys. If you just want to make your life easier with app script automation, this is a good place to start",http://automatedteacher.com/2022/10/13/create-impressive-personalized-reports-instantly-with-apps-scripts/,Automate,,
Why the heck are Rexroth controllers so popular?,https://www.reddit.com/r/Automate/comments/y33ayo/why_the_heck_are_rexroth_controllers_so_popular/,Automate,"Many industrial robots in my area are constructed with Rexroth motion control products. Is there any particularly reason, beyond the programming flexibility?","[""It's fun to say in the Scooby-Doo voice?\n\nIDK, only used them once, wasn't particularly impressed, but they worked fine enough."", 'This might be better asked at r/PLC']"
Windows automation: monitoring a logging software?,https://www.reddit.com/r/Automate/comments/y39axs/windows_automation_monitoring_a_logging_software/,Automate,"Looking for a program that detects any change on selectable part of the screen and keeps a log

Some time ago I had found a software who had many monitoring abilities. One of them was allowing me to select a portion if the screen, record its value, and detecting any change occurring over time. The software also allowed to record all the events (in this case, when the screen portion changes) in a logfile, with time and date.

Any suggestion on a similar software?",[]
Automating searches on LinkedIn Sales Navigator,https://www.reddit.com/r/Automate/comments/y2k0sv/automating_searches_on_linkedin_sales_navigator/,Automate,"Hello Automate community,

Is there a way to automate detailed searches on the Sales Navigator platform? From a spreadsheet I can feed into whatever system would be used, company names and/or domain names and then job titles. What I would need the system in question to do is find people with those specific job titles at the companies outlined on the sheet and then feed the results back to a spreadsheet. I think this can be done with one or more systems but I haven't identified all of the pieces yet so I was hoping to find some help. Thank you!","['Have you tried using a text expander? I feel like that would be one way to do this', 'Did you figure this out?  I am curious also']"
"How do we still feed the world while reducing chemical use and CO2? Simon Aspinall, CEO, Ecorobotix",https://www.youtube.com/watch?v=sfUkr_GF5ss&t=136s,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
How to automate slicing of 3D models,https://www.reddit.com/r/Automate/comments/y25bp9/how_to_automate_slicing_of_3d_models/,Automate,"I am looking for a program that can automate slicing of 3D models or at least a program that can estimate print times and filament usage for use in a website. Right now I have to take the file, Manually slice then upload the file to my site for it to parse out this data.   


Is there a slicing program that either has an API I can use (I can host any of these programs locally on my server) or is there a way to autoslice using Prusa Slicer that I am missing?  


The only thing I really need is a program that can take a 3D model (Either STL or OBJ) and estimate a print time and Filament usage in grams. I can handle the rest. I'm just looking for a faster way to make quotes other than doing them all by hand","['Using the PrusaSlicer headlessly through command line works mighty fine for me. No real tutorials or anything, just the command line help-command.\n\n\nhttps://github.com/prusa3d/PrusaSlicer/wiki/Command-Line-Interface']"
Algorithm Trading,https://www.reddit.com/r/Automate/comments/y29ycj/algorithm_trading/,Automate,Anybody knows how to automate taking positions and exiting in trading?,[]
New OpenAI DALL-E Powered Robotics | Google DeepMind AI Discovers New Matrices Algorithms,https://www.youtube.com/watch?v=_c5rcYwPTQg,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
My BASH automated YouTube channel for music playlists and art!,https://www.youtube.com/channel/UCJSFRfzJzCNL5-HX6CRQw2w,Automate,,
Automated the dialogue skipping,https://v.redd.it/1iuo0539rys91,Automate,,
AI app to diagnose illnesses with speech analysis as an input.,https://www.npr.org/2022/10/10/1127181418/ai-app-voice-diagnose-disease,Automate,,
Individual answers to Google-Form submissions - automatically,https://www.reddit.com/r/Automate/comments/xzifni/individual_answers_to_googleform_submissions/,Automate,"This is crazy-amazing from where I sit: I can automate anything and get a UI for free: can use the G-Form not only to trigger, but to give immediate feedback to users. A different one for each submission. All it takes is plain G-Forms and a small Apps Script.

Here's how (stripped-down example):

1. Create a new form
2. turn on ""Make this a quiz"", set ""Release grades"" to ""Immediately after each submission""
3. can have ""Collect e-mails"" on or off, does not matter
4. Define the question, do *not* give an answer scheme. In my case, I have one called ""Variation"". Details on why the form needs to be a quiz: see below
5. Add yet another Question (non-mandatory!) called ""Feedback""
6. Edit the confirmation message to sth like ""Click 'View Score' to see the result""
7. In the 3-dots menu (upper right), open the script editor
8. Paste a variation of this script (you can determine feedback in the script, call a webhook like I do, look sth up in Sheets, whatever)

&#x200B;

    function myFunction(e) {
      var form = e.source;
      var response = e.response;
      var variationItem = form.getItems().find(item => item.getTitle() === 'Variation');
      var feedbackItem = form.getItems().find(item => item.getTitle() === 'Feedback');
      var variationResponseText = response.getGradableResponseForItem(variationItem).getResponse();
      var feedbackResponse = response.getGradableResponseForItem(feedbackItem);
      var didAddFeedback = false;
      if (!feedbackResponse.getFeedback()) {
        console.log('Adding feedback - chosen option: ' + variationResponseText);
        var webhookUrl = 'https://<whateveryourapi>?variation=' + encodeURIComponent(variationResponseText);
        var webhookResponse = UrlFetchApp.fetch(webhookUrl, {headers: {'X-Test': 'whatever'}}); // can also have API key headers, etc.
        var webhookBody = JSON.parse(webhookResponse.getContentText());
        feedbackResponse.setFeedback(FormApp.createFeedback().setText((webhookBody[variationResponseText].ordered ? ('We ordered ' + variationResponseText) : ('Sorry, could not order ' + variationResponseText + ': ' + webhookBody[variationResponseText].reason)) + '\nalso check out https://<yourwebsite>/<path>/' + encodeURIComponent(variationResponseText)).build());
        response.withItemGrade(feedbackResponse);
        didAddFeedback = true;
        console.log('Added feedback');
      } else {
        console.log('Feedback already there');
      }
      if (didAddFeedback) {
        console.log('New feedback - submitting');
        form.submitGrades([response]);
        console.log('New feedback - submitted');
      }
    }
    
    function installMe() { // (call manually so far; for addon: onInstall)
      var form = FormApp.openById('<your form ID>');
      ScriptApp.newTrigger('myFunction')
          .forForm(form)
          .onFormSubmit()
          .create();
    }

of course, you can customize / change this - it's just a (working) example to give you an idea

what it does:

* react on form submissions
* add a ""Quiz feedback"" to the one submission (*that* is why we needed it to be a quiz)
* make sure we only add feedback once (you'd have an endless loop otherwise)
* the feedback can contain links that are also rendered as links (can also contain pre-filled links to other G-Forms, even; same goes the confirm message, btw - just that the confirmation message is always the same for all users)

then,

1. run installMe() from the script console. This adds the trigger - it's also visible in the script console then. You'll be asked to authorize an insecure app. As the app is your own: do that
2. Try the form

Hope you find this useful - sure let me know what you think!

**Screens step by step:**

Here is my example form:

&#x200B;

https://preview.redd.it/62wjursidrs91.png?width=1360&format=png&auto=webp&v=enabled&s=330976f3a8a8ba79a0158bb41a96028690db9edb

after submit:

&#x200B;

https://preview.redd.it/a6zyz4l0drs91.png?width=1114&format=png&auto=webp&v=enabled&s=26c739a013eeebf86f6b2eb0c6951352a120d8b4

Viewing the ""score"" - i.e. the *individual* feedback:

&#x200B;

https://preview.redd.it/7w910vc3drs91.png?width=1298&format=png&auto=webp&v=enabled&s=958a18944d68e78474ed503a8e23cc408d6f1525",['What use cases are you envisioning for this form workflow?']
Breakthrough Google AI Makes HD Video From Text | Deepmind AI Matrices Algorithm Discovery,https://www.youtube.com/watch?v=DgxW_9kHyyY,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Python Script for automated web page monitoring,https://www.reddit.com/r/Automate/comments/xzdv9q/python_script_for_automated_web_page_monitoring/,Automate,"Hello automation community,

I am interested in finding a solution that could monitor thousands of websites and let me know on weekly basis when various technologies, plugins etc have been added or removed and send a report back. Basically looking for specific changes in the HTML. Is this possible at a large scale? (\~5k-10k sites)","['Hexavision or something like that, a tool similar to the one you commented. There are many tools to monitor code changes daily.', 'Nope. Each webpage would needs its own script. Hugely expensive in software dev costs and maintenance.']"
Learning to Coordinate for a Worker-Station Multi-robot System in Planar Coverage Tasks,https://www.youtube.com/watch?v=aGZFEzbaRro,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Introducing the Basic Post-scarcity Map,https://www.reddit.com/r/Automate/comments/xywvt4/introducing_the_basic_postscarcity_map/,Automate,"A society in which all basic human needs are provided at zero or very low cost without significant human work can be defined as basic post-scarcity.

Our civilization is getting close to the point where this is technically feasible.

To reach this milestone as soon as possible, we introduce here the Basic Post-scarcity Map project, an effort to map the current technological state of the art and to understand how far we are from a basic post-scarcity society. We are currently in alpha stage, and we are releasing early to gather feedback and collaborators.

This project is an attempt to provide unbiased answers to the following questions:

* What technological advancements are needed to reach basic post-scarcity?
* What is the state of the art, what resources are available to learn about it and who is currently working on improving it?
* How far are we from achieving basic post-scarcity and what are the bottlenecks?

To accomplish this, the basic idea is to build two maps: the first where we will deconstruct the basic needs needed to reach basic post-scarcity and the technical milestones needed to satisfy them with minimal human work at the minimum cost. The second in which we input and update the state of the art for each of the technical milestones.

References to the current state of the art and the resources needed to learn more about it are collected in separate pages forming a shared library.

This is an open source and collaborative project. All contributions are fact-based, with no projections, opinions, marketing or propaganda.

We believe that having a searchable and living assessment of the state of the art will enable people who want to work towards this goal to know what is needed, what is currently feasible and who is currently working on what.

We are aware of the many limitations of this approach and in particular we know that technical bottlenecks are not the only roadblocks to a basic post-scarcity civilization. However, we also think that it is not unreasonable to assume that reducing the cost and the manpower associated with fulfilling basic needs will make it easier for public and / or private actors to provide them as widely as possible.

Our goal is to make this project simple to contribute and update. At this stage we need help creating technical milestones. Domain experts are particularly welcome to shine light on the state of the art for the relevant milestones in their respective fields.Additionally, anyone with reference materials and / or knowledge of people currently involved in solving these problems can contribute by sharing their resources in the library.

The project is currently live at: [https://postscarcitymap.org/](https://postscarcitymap.org/)

You can make contributions directly to the library by editing the pages on our Gitlab repo: [https://gitlab.com/postscarcity/library/](https://gitlab.com/postscarcity/library/). At the moment the process of updating the tree is still pretty manual, but we plan to automate it more.

If you want to join us as part of the team and contribute regularly, we also have a Discord server: [https://discord.com/invite/vhc8EZkmEv](https://discord.com/invite/vhc8EZkmEv)

We welcome feedback and contributions of any size.","[""For how long have you been developing this? Taking a look now. I do hope you take my notes under consideration, I don't mean them as hard criticism. Rather, this is something of importance to me so I'd like to aid as I can.\n\nLooking at the Needs tree, organization could be improved imo. I would break this into Sustenance, Shelter, Commons, and Accessibility. \n\nSustenance (Food > Ready/Prepared > Broken down by type (as a balanced diet is needed, not meat/vegetable/grain but fiber/protein/minerals/etc)(Water > Potable/Utility)\n\nShelter (Clothing)(Housing)\n\nCommons (Healthcare)(Education)(Public Transportation)(Public Spaces and Utilities)(Ecology)\n\nAccessibility (Mobility)(Auditory)(Communication)(Further Needs)\n\nDoesn't look like I can open up the pips for Clothing or Freight Transport so that's it for Needs.\n\nEdit: Ah, I see tech is much further along.\n\nEdit 2: Doesn't seem necessary to make a big comment for tech (would add Geothermal for energy), read the GitHub as well and it's off to a good start. Template seems well suited.""]"
Automation Ideas in IT,https://www.reddit.com/r/Automate/comments/xz1xm5/automation_ideas_in_it/,Automate,"Hey everyone, i just wanted to ask for some ideas on what to automate in ur daily job as IT as HD,SD,Sysadmin ect.. What are some things that you have automated?","['Just about everything...', 'Login and logout process, but especially everything between.']"
Any tools to automatically follow confirmation links in your email?,https://www.reddit.com/r/Automate/comments/xyxydu/any_tools_to_automatically_follow_confirmation/,Automate,"Hey 'maters! I was wondering if anyone already had a script/tool/app to automatically check your email(s) and follow the confirmation links in them sent from various sites or something similar? Even perhaps a guide to build my own? The email service is Gmail but it's used as a catch-all for all emails. If Gmail is an issue then I also have Office365 email hosting although I don't think that really matters since they both support SMTP/IMAP connections.  Any help is appreciated. Thank you,!",[]
SMS auto-reply to a different number?,https://www.reddit.com/r/Automate/comments/xwj5yo/sms_autoreply_to_a_different_number/,Automate,"Okay, I don't know if I am missing something or if it just doesn't exist. I have an ""unlimited"" data plan with my provider. When I use up 5GB in a day, I receive an SMS message stating that if I want to keep browsing at high speeds, I have to send a specific SMS to another number. Every 2GB afterwards the process repeats.

I did find some auto-reply apps on Google Play, but I haven't found one that can send a message to one number when receiving from another.

Is there a way to automate sending that message to keep using high speed internet or do I have to actively resend it every time I want to download something bigger during the span of a couple hours?","[""Do you have an andriod? If you do, there's an app that will allow full control over what gets sent and to whom. If we receive a message, we can auto reply that message to a different number."", 'You could try IFTTT?', 'Use something like IFTTT to send a text to that number every day, every couple of hours, regardless?']"
"Robots are making French fries faster, better than humans",https://www.reuters.com/technology/want-fries-with-that-robot-makes-french-fries-faster-better-than-humans-do-2022-10-04/,Automate,,
New Google AI Makes 3D Video Game Objects From Text | Quantum Physics AI | Soft Robotics Learn How Hard To Grip,https://www.youtube.com/watch?v=YoJaI4aYaDA,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Need some help to refine my automation,https://www.reddit.com/gallery/xve13q,Automate,,"[""I built the above on Automate app for Android so that after a few seconds of my tablet screen being off, the wi-fi turns off as well. Except if I have something like a movie downloading for offline viewing or if the tablet is connected to power. For the most part this works really well but every now and again, I find the wi-fi just turns off while I'm using the tablet. There must be some logic I'm missing somewhere. Anyone have any ideas? Having wi-fi off when I'm not using the tablet has really given me significant more battery endurance"", 'What tool is this?', 'If you turn on your display several times while wi-fi is on, the task could be currently on some of the delay blocks. So after a while it will disable the wi-fi.\n\nJust an idea to avoid this is to add additional ""when display on"" block that processes immediately right before ""Disable wi-fi"" block to check if your display is still ""on"". But I\'m not sure if the ""Delay XXs awake"" block keeps the screen on.']"
Intelligent Process Mining for Better RPA Results,https://www.impactqa.com/blog/intelligent-process-mining-for-better-rpa-results/,Automate,,
"With the help of these snake robots, you might not need to disassemble an engine to fix it",https://www.metallisation.com/innovative-robotics-created-in-partnership-with-rolls-royce/,Automate,,
How to batch merge every 4 pics into 1,https://www.reddit.com/r/Automate/comments/xufiic/how_to_batch_merge_every_4_pics_into_1/,Automate,"I have a folder filled with pics. I want every 4 pics to be combined into 1 - one vertically and another horizontally and the last one diagonally , ie, if there are pictures of 4 squares, they should form a 2x2 matrix. i want this to happen to EVERY 4 pics. How do i do it in windows 10(or android - although i personally prefer windows for work of this nature)","['Python + openCV should be able to do it pretty quick', 'python and imagemagick', 'It\'s ugly but this works for me:\n\n`ls | awk \'ORS=NR%4?FS:RS\' | xargs -I{} sh -c \'montage -mode concatenate -tile 2x2 {} ""out_{}.jpg""\'`\n\n- List files\n- Put four files on each line\n- Concatenate those four files into a 2x2 grid using ImageMagick\'s Montage CLI: https://imagemagick.org/script/montage.php\n\nThis probably works in Windows in Git Bash if ImageMagic is installed', 'I did it with Python and PIl for my photobooth web app. The single_stripper function arranges them all vertically, and the big_stripper function arranges them like how you asked.\n\nhttps://github.com/okiest/vbooth/blob/master/utils/stripper.py']"
Elon Musk Reveals Tesla Optimus AI Robot | New Meta Text To Video AI,https://youtu.be/F_3ewle483w,Automate,,
[meta] how does AI generated multimedia not infringe on thousands of copyrights? Isn‚Äôt it basically stealing ideas directly to compose a new out of fragments of existing data essentially?,https://www.reddit.com/r/Automate/comments/xrzmah/meta_how_does_ai_generated_multimedia_not/,Automate,Sorry if I lack understanding of the process‚Ä¶,"[""That's most art, I think"", 'I feel like I should sue you for this.\n\nI mean, not sue *you* per se, but more one of the old Greek philosophers.\n\nActually, since thousands of philosophers have had those ideas and your DNA is composed of fragments of them, it makes sense to sue you.', ""It's more like it's interpreting what it has seen.\n\nWould be tough to say how what it does differs from what we do."", 'Its a powerful tool, still a tool. Can‚Äôt escape progress, just stay on top of it', 'Maybe I don‚Äôt understand the question but‚Ä¶ if I made a mosaic out of pieces of other peoples artwork‚Ä¶ that final result is MY artwork, right?. So maybe I‚Äôm not clear on the question.', 'Because humans subconsciously do this CONSTANTLY.', ""I had this same thought with respect to the newest piece of software that Nvidia just announced on their 4000 series GPUs. Its a function called RTX remix, and its basically capturing all of the wireframes/textures/positional data of every 3d asset in a video game and then auto generating copies of it to be modified within the Remix software. I'm not exactly sure why more 3d artists aren't upset about this.\n\nhttps://youtu.be/Gr6VOrKiXwU"", 'Indeed. AI image generator simply couldn\'t function without images in the machine learning data sets. It\'s commonly known that such data sets contain copyrighted images. Ordinarily, copying images onto a hard drive (data set) without permission puts the ""copy"" into copyright infringement. However, developers have relied on ""fair use"" exceptions for data mining.\n\nNever the less, such ""research purposes"" don\'t extend to end users for commercial purposes.\n\nThe other problem, is that to do this ""correctly"" developers could have followed the film industry paradigm for controlling exclusive rights through a ""chain of title"" which, is written documentary evidence of everyone that made a creative contribution to a film. There could be thousands of people involved in each film.\n\nSo developers could have crowd sourced artists around the world and acquired exclusive licenses from them. In turn the artists could earn a small percentage in royalties. This is normally how licensing works and there is no need to deviate from this....but the AI developers did deviate. So now they have screwed up.\n\nWithout exclusive licenses then all derivative works made by AI users are devoid of ""exclusive"" protections that a proper ""chain of title"" would have ensured. For example, film producers rely on a complete ""chain of title"" so they can wave their written licenses in the face of a judge.\n\nSo AI users are stuffed. They can\'t wave any exclusive license at anyone and cannot protect their output. ""Fair use"" even if extended to the AI users, isn\'t ""fair copyright"". It\'s an ""exception to copyright"". So the best they can hope for is user rights. Not exclusive rights.\n\nThis is on top of the fact that machine processes can\'t be copyrighted in any case.\n\nGetty\'s ban on AI works is telling because they were sued for copyright fraud in 2016.\n\nPotentially any AI users trying to license their outputs or protect it in court could be counter sued for copyright fraud.\n\nIt\'s a mess all the way because developers didn\'t acquire written exclusive licenses for the data set images. They are idiots!', ""They just automated art and design. \n\nMost if not all of the techniques posed by AI like Stable Diffusion are the same used by Artists, since... well... art began. It's just way, way, way faster. \n\nIf you look into how production art is made and how photoshop is intended to be used: Taking other peoples work and forming it into your own beyond recognition is a staple of the industry for fast mock up work. \n\nI personally think AI will be the beginning of the decline in IP rights. This could be a good or a bad thing, but I'm still fingers crossed that we can actually have a robot overlord at this point: Surely it would be better than the current alternatives \\*Gestures broadly\\*."", 'copyrights are kinda bullshit to begin with', '[Copyright infringement in artificial intelligence art](https://www.technollama.co.uk/copyright-infringement-in-artificial-intelligence-art).']"
Bot Name Generator ‚Äì here's a free tool to generate names for your chatbot,https://www.reddit.com/r/Automate/comments/xrullw/bot_name_generator_heres_a_free_tool_to_generate/,Automate,"Hi there! My team recently created one piece of content, and I'm so excited to share it with you! Bot Name Generator! It is for everyone who wants their chatbot to engage from the moment it introduces itself. You can generate names based on your industry and personality traits. For example, 

Creative + Travel Industry = Bethany, Mallory, Beep Boop, Anne Droid and more :)

Besides, on this page, you will see instructions, tips, recommendations, and helpful guidance on how to name your chatbot based on your industry. 

People have different expectations when talking to an e-commerce bot and a healthcare virtual assistant. So, if you need some professional opinion, [check out this page](https://botscrew.com/chatbot-name-generator#/?utm_source=Reddit&utm_medium=&utm_campaign=&utm_term=&utm_content=) with ideas and real-world examples!",[]
Bumblebee movement detection using MOC70T4 sensors,https://www.reddit.com/r/Automate/comments/xr6vxo/bumblebee_movement_detection_using_moc70t4_sensors/,Automate,"Dear all,

&#x200B;

I am working on a research experiment where we teach bumblebees to pass trough a sensor to activate a sugar-water f feeder. The first prototype (picture attached) uses a sensor (left) that activates the feeder. The other sensor (right) records time feeding. We are using MOC70T4 sensors to detect the bumblebees movement responses, however, the first pilot shows that its not working as well we hoped. The bumblebees have to emit a very stereotyped response to activate the sensor, which often leads to repeated and not recorded responses.

&#x200B;

I would really appreciate suggestions of sensors that could be more appropriated for the task, or ways of setting up the sensors that facilitate the detection.

https://preview.redd.it/raohtt92nsq91.jpg?width=1079&format=pjpg&auto=webp&v=enabled&s=2e86c0f78b3f01bdfde41917a47f6d2a62dab875","[""I'm not sure you're in the right sub. You might try a sub more focused on scientific experimentation.  Perhaps /r/askscience/\n\nThat said, I'll take a stab...\n\nIt sounds like you're beating yourself against a brick wall trying to get this sensor to interact with the bumblebuddy.  Why not take Mr Bumbles having to interact with the sensor out of the equation all together?\n\nMaybe have a defined space like a painted square on the floor of the enclosure. Point a video camera at that with a wee bit of logic monitoring it. \n\nSomething like:  \n\n* clear image -> no feed\n\n* image contains black blob -> activate feeder\n\n[Python OpenCV](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) probably has everything you need plus a bag of chips to get this project on the next level."", 'Interesting. Can I ask why you chose that type of sensor and maybe not something like auditory? What is it in their particular behavior that makes the current sensor not work as intended']"
Automate creating words file,https://www.reddit.com/r/Automate/comments/xr2yeb/automate_creating_words_file/,Automate,"My job require creating new documents most of the time it‚Äôs same 
For each client we use around 5 documents with only 5-6 line different 
I now automate everything by using microcode but even with that it take around 5-7 minute 
however I want lvl up my automations and let it create all document 
For example entering the client information and it will create all 5 documents 
I used python (the paid library works but too expensive and free one usually change the format - size .. etc) 

So any idea how could I do this 
I can do  c,python,bash","['couldn‚Äôt you so this pretty easily with vba as well?  python might be user friendly but i could easily picture this being a simple word macro, which also means you don‚Äôt have to deal with trying to maintain python or getting other people access.\n\ni would start with [this](https://learn.microsoft.com/en-us/previous-versions/office/troubleshoot/office-developer/automate-word-create-file-using-visual-basic) and go from there. vba is pretty simple to learn just break down your steps into very simple actions and google them and there is probably someone who has already done what you need.', '`import jinja2`\r  \n`from docxtpl import DocxTemplate`\r  \n\r`from docx2pdf import convert`\n\nThose will do it in python. docx files are not terribly simple (convert a copy of one to a .zip file and see behind the curtain), but the hard parts have been done for you.', ""VBA  is better solution for this. I can help you with this development if you're willing to pay for it."", 'At least on Windows, Python quite certainly still has the ActiveX bridge - so you could call all VBA commands from Python. So instead of `GetObject(, ""Word.Application"")`, you\'d have\n\n`import win32com.client`\n\n`word = win32com.client.Dispatch(""Word.Application"")`\n\nNow, you get all the methods Word has to edit a doc (find and replace), export a doc, etc. (same as VBA)\n\nBTW: LibreOffice has sth similar called UNO, indep. of Windows; but LibreOffice won\'t preserve all formatting 100% (at least last time I checked, which was a while ago)']"
Automated art painting?,https://www.reddit.com/r/Automate/comments/xqu39k/automated_art_painting/,Automate,"Looking for a service where I can sub!it graphic art and have it painted. Not simply a print but a painting.

With the proliferation of excellent A.I. text to art generators the common person can now create artistic works of art but there should be an option available to transfer the image to a yexturd medium and not merely a print.

I'm looking for such a service if it exists. Yes commissioning a human is one option I am also pursuing but I'm interested in taking an idea to result purely with technology.",['https://www.theguardian.com/technology/2022/apr/04/mind-blowing-ai-da-becomes-first-robot-to-paint-like-an-artist']
"5G Humanoid AI Robot For 170K USD To Automate Service Industry Tasks | New Nvidia AI Creates 3D Renderings | OpenAI Open-Sources ""Whisper"" AI Model | Autonomous Microrobots",https://youtu.be/DOglbscWbmw,Automate,,
How to do speech-to-text transcription (speech recognition - NLP) for free with better quality than Google's API by using OpenAI's freshly released model Whisper,https://youtu.be/msj3wuYf3d8,Automate,,
"For the people who are in full support of automating as much as possible, how is this beneficial short and long term?",https://www.reddit.com/r/Automate/comments/xo6m1b/for_the_people_who_are_in_full_support_of/,Automate,,"['Short term there may be some hiccoughs but long term the advantage should be lower prices on goods.  Let me give you a example.  Say you have a farming drone which plants seeds then picks them, then you have a Automated truck which drives them to the desired location, then you have a automated chef which cooks it into a pizza. If we can get it to economically scale right it should go like this. Cheaper materials prices because it is farmed in much more abundance and lower cost due to not needing to pay for higher labour -> cheaper transportation due to not paying for a driver -> cheaper pizza due to not needing to pay for a chef. All these benifits should be passed down to the consumer. So instead of charging 10$ for a pizza you can charge 1.50 because of the abundance of productivity. That is how it should go but companys might get greedy.  Although I would say pricing things cheaper would encourage more sales.  Anyways at this point it would make sense to Implement a UBI (universal basic income) and it does not have to be a lot because goods will be much cheaper. A UBI is beneficial because it will keep the money circulating instead of hoarded like big companies do In tax havens.  Also necessary because once you automate a lot of those tasks  a lot of jobs will be lost but that does not have to be a bad thing if it increases productivity and the scenario I laid out can come into fruition. Cheaper goods for everyone long term hopefully and less meaningless 1 dimensional jobs.', 'This is an important question!\n\nAutomation poses two primary dangers, displacement of laborers and future shock. Danger here being negatively affected quality of life, or loss of a way of living. \n\nThe first danger exists mostly as a result of capitalist society equating value to labor productivity (and many issues exist today because wages and productivity fell out of lockstep in the 70s). There are some who would love their job regardless and not want to lose it, and they might not have to. The jobs to be automated first are repetitive simple labor, jobs requiring problem solving or unique solutions are at less risk, as are jobs requiring delicate or abstract techniques (though tech is getting there).\n\nThe second danger is a term from the book of the same name, by Alvin Toffler and his spouse Adelaide Farrell, which is a phenomenon inherent to any disruptive/intrusive technological advancement. Essentially it\'s the psychological stress you feel when ""out of your depth"" with technology or a way of things, as you might see with parents or grandparents unable to manage computers or smartphones.\n\nBoth of these dangers can be accounted for.\n\nMass automation historically is tied to cultural renaissance. Agriculture and the domestication of animals removed the need to scour the land for weeks at a time, the wheel enabled mills to grind grain and much more, writing allows communication regardless of distance and presence (providing much faster dissemination of ideas), the Renaissance proper boosted that further with the printing press - the industrial revolution brought machinery and explosive productivity, steam powered trains brought mass infrastructure, steel and electricity became basic building blocks of our world, oil and mass production with automobiles and the freedom of movement that entails - and now the internet with near instant communication from anywhere to anywhere.\n\n**Short term** benefits we can expect include new business endeavors and ambitions in general from the population, as the necessities are handled by automation more and more time becomes available to pursue aspirations and curiosities. New areas of art and forms of expression. People probably will be less rooted to one place, as they aren\'t being relied on to produce surplus at a daily location anymore. There may be more aimless gatherings of people for the sake of community. Development of language and cultural identity too as interpersonal interactions are less rigidly structured. Decreasing cost of living, more investments, more (good) risk taking behavior.\n\n**Long term** would bring more of the above as well as more advancements in technology and the sciences from resources not having to go towards needs - the chance that an Einstein or Hawking goes unnoticed because they scraped by working at McDonald\'s their whole life or went untreated goes WAY down. In general we could see less conflict over resources, less negative competition and more sportsmanlike competition.\n\nOf course, there are dangers I didn\'t mention at the beginning because they\'re not dangers of automation, they\'re dangers *using* automation. Things like automated guns on drones, soulless interactions with machines when seeking help (think automated 911), or the big bad AI singularity and self replication. I also don\'t consider failures of human morality or judgement to be dangers of automation.\n\nMy biggest concern with automation beyond the transitory period has to do with global resource supply. Automating everything at once will result in limited rare earth elements and an overinvested tech-generation (pushing a technological frontier too hard leads to burnout, a disinterest in further advancements, and the ""needle in a haystack"" problem).\n\nThanks for asking!']"
graticule,https://www.reddit.com/r/Automate/comments/xoozvp/graticule/,Automate,"Hi. I am trying to make a flow where an app starts after reboot and additionally starts the beacon. Im having problem with the beacon.  How can I force the app to start the beacon ?

The app is to be found on google playstore under this link

https://play.google.com/store/apps/details?id=com.emilburzo.graticule","['https://www.dev2qa.com/how-to-start-android-service-automatically-at-boot-time/', 'The easiest way to do it would be to say something like, ""If this service is not running, start service (or app). If the service (or app) generates a notification in the top bar you could say, ""if this notification does not exist, start service (or app). Afterwards, add a short delay, like 15 second, draw a line from the ""start app"" and the ""yes a notification does exist (or a service is running)"" to the top of the delay. Then loop the bottom of the delay back to the start of the flow. It will run constantly but when you first boot it will see that the service is not running or the notification is not displayed and run the app for you.', ""Reddit is not your personal tech support. Stop treating it like it is, it's rude.""]"
"Grapes, berries and robots: is Silicon Valley coming for farm workers jobs?",https://www.theguardian.com/environment/2022/sep/08/california-agriculture-technology-farm-workers,Automate,,
Newest AI From World AI Conference In China,https://www.youtube.com/watch?v=XYeZziy_2UE,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Anyone able to help me identify the type of automation I need?,https://www.reddit.com/r/Automate/comments/xkz4eo/anyone_able_to_help_me_identify_the_type_of/,Automate,"I work for a tech consulting firm where automating internal tasks is highly valued. I really want to automate one of my tasks in work. 

We have a Master Resource List in an excel sheet for everyone in our team of 100 people. There are a number of fields to populate which make this a boring task. The fields include Name, employee number, manager etc. 

How can I automate this to only have to type in a name and then the script pulls the data from relevant sources? Would be very interested to use this as an opportunity to learn about automation. Any help would be greatly appreciate.","[""What relevant sources are you talking about? I'm not sure what to recommend since I don't fully understand your situation but I had a similar-ish project for my job last month and I used a tool called Text Blaze to automatically fill in a lot of information. Could work for you, maybe not. Hope you find a solution!"", ""Look into Python. It works wonderfully for Automation and I think you can what you need in the excel sheet.\n\n\nCaution: If there's no Data Protection for these sheets it should be fine."", 'Could easily do it with VBA in Excel', ""This is a textbook example of incomplete requirements.\n\nWhat's in this Master Resource List?  Is it a straight spreadsheet with no internal logic? Can it be exported to CSV without losing functionality?  If not, what functionality exists within Excel? Can that be duplicated in a wrapper script? (the answer is almost certainly yes)\n\nBut then what are these relevant sources you're talking about? Is it a flat text file? A database? An API? What privileges are needed to access these resources? Admin? User? Perhaps a service account?\n\nWould you be open to removing the excel spreadsheet from the workflow entirely? Re-write this process from the ground up with automation in mind? Or is the existence of this XLS non-negotiable? If so, why? Is it used by other business processes?\n\nThe list of concerns could go on and on, but these are the immediate red flags that jump out at me."", 'If you know python, look into xlsx library.', ""I get people saying python.  I also love python, but for me I'd use PowerShell.  Assuming you're on Windows you're going to get the most power OoTB imho."", 'Maybe this? https://youtu.be/Z-h2UER3b\\_0', ""More than likely, I think the decision to have the Master Resource List being an Excel Workbook is already a mistake that should be worked around. Is that really the most user-friendly way to get the information to the end-users? I know I wouldn't want to wait for the load times of Office anytime I wanted to bring that information up.\n\nIf the amount of users who need to access that information is greater than one, I would highly favor a web app based approach. If not, then I guess pick your poison on your scripting language in converting to your preferred output format, since they're all largely capable of doing the same thing.""]"
Powershell script for bulk user manager atrribute update,https://www.reddit.com/r/Automate/comments/xka1mw/powershell_script_for_bulk_user_manager_atrribute/,Automate," Hi Team.. I have this business critical task to update the manager field under organizational chart in Active Directory for 1000 users with wrong manager info. I have been struggling with finding/modifying the right PowerShell script for this.  


Please if anyone has a lead or a base script for this idea, would appreciate if you can send over to assist with my task. Thanks in advance [\#PowershellScript](https://www.linkedin.com/feed/hashtag/?keywords=powershellscript&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6977939143502807041) [\#info](https://www.linkedin.com/feed/hashtag/?keywords=info&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6977939143502807041)",['Take a look at https://community.spiceworks.com/topic/2306954-updating-ad-manager-attribute-from-csv-using-powershell']
Help: Automate count of items listed on a website.,https://www.reddit.com/r/Automate/comments/xjw1ba/help_automate_count_of_items_listed_on_a_website/,Automate,"I was trying to figure out if there is a way to automate the count of a particular item listed on a website. For example: www.myeyedr.com/about/our-eye-doctors. This site provides a list of all doctors for a particular location. Currently, I am manually going through each location and counting the number of doctors provided. Is there an easier to way to automate this task?

I can cut down on some time using a count on inspect elements but that still requires me to load the entire list and doesn't work when the site redirects to another page.

Thanks for any inputs!","['Python beautiful soup to scrape the data and selenium to navigate', 'I can program that for you. Dm me.', 'As the site uses dynamic content, I think Selenium / [webdriver.io](https://webdriver.io) would be the way to go. You automate the browser: navigate to the start page, pull all that are visible by xpath, click ""Load more"", pull again, etc.']"
Top 7 Brain Computer Interface (BCI) Devices of 2022 | Artificial Intelligence Tech,https://www.youtube.com/watch?v=GhQfKDLJLq0,Automate,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Industrial SW engineering UX stone age?,https://www.reddit.com/r/Automate/comments/xj14f7/industrial_sw_engineering_ux_stone_age/,Automate,"At my workplace there are lots of different PLCs and other automation equipment which have one thing in common. Catastrophical user experience for engineers who develop and maintain the software (programming PLCs and robots etc.).

Yaskawa MPE720 came out of the stone age, it can not even assign the memory to the variables.  It has to be done manually. Epson RC+ looks like it has been made around '95. Sta√ºbli SRS is somehow OK. The best so far I have used is Beckhoff PC PLC - programming environment is MS Visual Studio XAE with all of it's bells and whistles. 

So, which PLC system would be the most contemporary considering SW engineer's user experience? I dream of something that could be programmed in VS Code, with GIT version control and with worker user interfaces built in HTML5 or with some other standard framework?","['Beckhoff will be the closest to what you are describing. However, all PLC/Industrial Control Systems will be fundamentally different than traditional computer software. PLCs run a RTOS and most vary between vendors making standard tools rare. The fact that many systems control equipment where human life is at stake means that changes have to be vetted and are generally slow to market. The major suppliers (Rockwell, Siemens, etc.) are building tools to cater more to the software minded folks. Rockwell‚Äôs newest offerings are getting closer. Also, ignition for HMI/Scada is fantastic. None of them are going to be cheap.', 'My only experience is with TIA Portal but it sucks.']"
Greenhouse automation project,https://www.reddit.com/r/Automate/comments/xhmen3/greenhouse_automation_project/,Automate,"I have Automated greenhouse as my uni project and need help with heating/cooling mode transition. I know that program can't just go from heating to cooling and backwards, as it could result in great energy losses, but there is a great chance that on a sunny winter day you would need cooling mode during the midday and heating when sun goes down. 

So my question is: What conditions should I set for heating and cooling mode, so I can get smooth transitions during the day, and should I add something like standby mode when temperature is in it's setpoint?","[""One easy solution is to have a deadband around the desired temperature. So if the temperature is 80 and the dead band is plus or minus 3 degrees you'd turn on the heater for anything below 77 and turn on the cooling for anything above 83. You do nothing in the deadband (77-83).\n\nThis is fairly common for residential temperature control, but I have no idea what setpoints (or other variables, like humidity) are appropriate for a greenhouse. That sounds like a good candidate for the engineering design process ;)"", ""It could be overly complicated and well beyond the scope of your project, but if you have access to weather forecast data, you can use that as a feedforward variable in your control loop.\n\nEven if you don't end up using it, it's a fun concept that could be worth talking about in your report as something you considered using."", 'Just buy a fan temp monitor it will turn the cooler on when needed and there‚Äôs the same device for heating', 'Use CUSUM. Pick a critical value and a threshold so it needs to be over/under a certain temperature for a long enough amount of time and going further past will speed up the transition. https://www.spcforexcel.com/knowledge/variable-control-charts/keeping-process-target-cusum-charts', ""Usually green houses will vent outside air before turning on heat pumps. So if it needs cooling on a sunny winter day you'd just open a vent and turn on a fan. So what I am really saying is you need an exterior temperature sensor. \n\nAlso, humidity is very important if it's a real greenhouse. So you will need a dew point sensor as well, assuming you have the ability to control humidity.""]"
"Virtually Amish (2022) by Lindsay Ems, on Amish approaches to the internet and high-tech capitalism ‚Äî An online group discussion on Thursday September 29, open to everyone to join",/r/PhilosophyEvents/comments/xhh77n/the_amish_and_technology_thursday_september_29/,Automate,,
"Automated voice ordering will be rolled out to roughly 250 Checkers restaurants, perliminary testing shows 98% accuracy.",https://www.qsrmagazine.com/exclusives/checkers-pioneers-pivotal-drive-thru-technology,Automate,,
This guy is building robots to make mars habitats from scratch,https://youtu.be/yjaSlgvwdzo,Automate,,
Ex-Cisco CEO John Chambers aims to automate enterprise networking with NaaS startup Nile,https://telecom.economictimes.indiatimes.com/news/ex-cisco-ceo-john-chambers-aims-to-simplify-enterprise-networking-with-his-new-startup-nile/94215387,Automate,,
Looking to integrate data source with social media post,https://www.reddit.com/r/Automate/comments/xfzaoq/looking_to_integrate_data_source_with_social/,Automate,"I am already familiar with Zapier (and formerly automate.io). 

I already use the former for another project which monitors entries in a Google Sheets document, populated by Google Forms.

In this new case, I want to monitor output from an outside managed data source. 

##More specifically, my desired workflow is:
1. Retrieve recent EV Charging Stations added to AFDC Database
2. Parse data and filter by a single state, or a selection of ZIP Codes.
3. Post selected column fields to Social Media.

https://afdc.energy.gov/fuels/electricity_locations.html#/analyze

The data stored here can be retrieved via: CSV Download, HTML Embed, and Developer APIs.

The trick is, I don't know how to pull that data into a Zapier automation. 

Zapier does have a Webhook option, but it is a paid feature.

Is there a way of getting around this? My only other idea was to download the CSV on a schedule, and copypaste into a sheet on a regular basis. But that will require significantly more work on my end.

Any suggestions for how to accomplish this?","[""If you want more bang for your buck, I'd recommend you check out Make.""]"
Breakthrough Neural Network AI For Robotics | New Computer Vision 3D Scanner | New Computer Vision AI For 3D Environments | New Google AI Machine Learning Can Smell,https://youtu.be/p6Q_hJzOZCU,Automate,,
Rules update suggestions,https://www.reddit.com/r/automation/comments/m0vfh2/rules_update_suggestions/,automation,"Hi all,

It was brought to my attention a mod we thought had left the team was continuing to take actions on the sub. I have removed said mod and undone their actions.

On that note, I think it is worthwhile revisiting the rules and ask the community what they want from this sub.","['No ads. Its cool if people have come across a product they want to suggest but the 10:1 rule and ""are you being paid to advertise"" are pretty easy to follow and do a great job of weeding out all the spam.', 'Educational links of thread would be great. Help those who want to learn more. And keeps from asking questions. But also is an ever expanding resource.', 'Software Saturday / Sunday Sticky?', 'Test Comment', 'Test Comment', 'Test Comment', 'Test Comment', 'Test Comment', 'Test Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.\n\nSit amet consectetur adipiscing elit ut aliquam purus. Morbi non arcu risus quis varius. Felis donec et odio pellentesque diam volutpat commodo sed egestas. Mollis aliquam ut porttitor leo a diam sollicitudin tempor. Neque aliquam vestibulum morbi blandit cursus risus at ultrices mi. Eu volutpat odio facilisis mauris sit amet massa vitae. Mauris commodo quis imperdiet massa tincidunt nunc pulvinar sapien. Erat imperdiet sed euismod nisi porta. Tempor orci dapibus ultrices in iaculis nunc sed augue lacus. Neque convallis a cras semper auctor neque vitae tempus quam. Turpis nunc eget lorem dolor sed viverra ipsum.', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'text Comment', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', '[deleted]', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Edited Comment', 'text Comment', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Edited Comment', 'text Comment', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Edited Comment', 'text Comment', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'text Comment', 'text Comment', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'text Comment', 'text Comment', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Edited Comment', 'text Comment', 'Edited Comment', 'text Comment', 'Edited Comment', 'Edited Comment', 'text Comment', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Urna et pharetra pharetra massa massa ultricies. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Mattis nunc sed blandit libero volutpat sed cras ornare. Convallis tellus id interdum velit laoreet. Eu feugiat pretium nibh ipsum consequat nisl vel pretium. Sem et tortor consequat id porta. Morbi blandit cursus risus at ultrices mi tempus imperdiet. Magnis dis parturient montes nascetur ridiculus mus. Ultrices vitae auctor eu augue ut lectus arcu. Cursus eget nunc scelerisque viverra mauris in aliquam sem. Quam elementum pulvinar etiam non quam lacus suspendisse. Et magnis dis parturient montes nascetur.', 'Edited Comment', 'Edited Comment', 'text Comment', 'Explanatory notes embedded within the code. Comments are used to remind yourself and to inform others about the function of your program. Multiline comments are used for large text descriptions of code or to comment out chunks of code while debugging applications. Comments are ignored by the compiler.', 'Edited Comment', 'text Comment', 'Explanatory notes embedded within the code. Comments are used to remind yourself and to inform others about the function of your program. Multiline comments are used for large text descriptions of code or to comment out chunks of code while debugging applications. Comments are ignored by the compiler.', 'Edited Comment', 'text Comment', 'Edited Comment', 'Edited Comment', 'text Comment', 'Explanatory notes embedded within the code. Comments are used to remind yourself and to inform others about the function of your program. Multiline comments are used for large text descriptions of code or to comment out chunks of code while debugging applications. Comments are ignored by the compiler.', 'Edited Comment', 'text Comment', 'Explanatory notes embedded within the code. Comments are used to remind yourself and to inform others about the function of your program. Multiline comments are used for large text descriptions of code or to comment out chunks of code while debugging applications. Comments are ignored by the compiler.', 'Edited Comment', 'text Comment', 'Explanatory notes embedded within the code. Comments are used to remind yourself and to inform others about the function of your program. Multiline comments are used for large text descriptions of code or to comment out chunks of code while debugging applications. Comments are ignored by the compiler.', 'Edited Comment', 'text Comment', 'text Comment', 'Edited Comment', 'Edited Comment', 'text Comment', 'text Comment', 'Explanatory notes embedded within the code. Comments are used to remind yourself and to inform others about the function of your program. Multiline comments are used for large text descriptions of code or to comment out chunks of code while debugging applications. Comments are ignored by the compiler.', 'text Comment', 'text Comment', 'text Comment', 'Explanatory notes embedded within the code. Comments are used to remind yourself and to inform others about the function of your program. Multiline comments are used for large text descriptions of code or to comment out chunks of code while debugging applications. Comments are ignored by the compiler.', 'text Comment', 'text Comment', 'Explanatory notes embedded within the code. Comments are used to remind yourself and to inform others about the function of your program. Multiline comments are used for large text descriptions of code or to comment out chunks of code while debugging applications. Comments are ignored by the compiler.', 'text Comment', 'text Comment', 'Explanatory notes embedded within the code. Comments are used to remind yourself and to inform others about the function of your program. Multiline comments are used for large text descriptions of code or to comment out chunks of code while debugging applications. Comments are ignored by the compiler.', 'text Comment', 'text Comment']"
Jira Automation - Release notes,https://www.reddit.com/r/automation/comments/10f3m0e/jira_automation_release_notes/,automation,"Hi, does anybody know how to Automate the creation of JIRA release notes without the plug in?","[""Here's some [Jira automation templates](https://blaze.today/gallery/jira/) I use"", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
what are these tables called used to assemble the panels and move them around?,https://www.reddit.com/r/automation/comments/10dozf6/what_are_these_tables_called_used_to_assemble_the/,automation,[https://www.youtube.com/watch?v=O3al52UWoc0](https://www.youtube.com/watch?v=O3al52UWoc0),"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'smart table\n\nhttps://thehouseofdesign.com/automation-products/automated-truss-system/']"
Help with automatic extraction of data from PDFs,https://www.reddit.com/r/automation/comments/10dhn2g/help_with_automatic_extraction_of_data_from_pdfs/,automation,"I have 4 PDFs with menus for the different messes in my college. I want to extract and add them to a database. I tried using the pdf2txt program in Linux but the extracted text was not arranged in a manner that I could easily separate menus for various meals of a week. As a temporary measure, I took screenshots of the various menus and passed them into OCR software but it is very time-consuming. Hence, I would like to automate the process. Is there any way programmatically or otherwise that I can easily extract the data? The PDFs in question can be found [here](https://drive.google.com/drive/folders/1QL7kaqUy7nTFG5BWYOsfqLk-sAYFA90W?usp=sharing) (Google Drive link)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""If pdf2txt is giving you the text you need, but not in the format you want it, you want to just consider another tool or script that can get you that last step of the way from PDF --> Unformatted Text --> Formatted Text.\n\nIf you're using Linux, AWK is a very good command-line program for doing on-the-fly text reformatting. You also might find that writing a simple Python script is a good way to get a text-reformatting workflow in place; but if you're new to programming either option might take a little more work than you are ready to put in, depending on how important the task is to you.  \n\n\nIf you want post an example of a single line of misformatted text and then tell me the format you want it in, I'd be willing to show you the solutionary AWK or Python command to get you started."", 'You can check out this link for Data extraction. Works the best! https://app.docsumo.com/signup']"
How long until cleaning staff are replaced?,https://www.reddit.com/r/automation/comments/10dyhho/how_long_until_cleaning_staff_are_replaced/,automation,"I know this might be a long time from now, or not. But have there been ‚Äúmotherships‚Äù that help/host multiple smaller cleaning robots help clean up rooms and offices? Like have multiple room as and mopping guys coming in and out of it. Idk just an idea I had recently.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Not soon enough!!']"
What‚Äôs the best automation tools you‚Äôve come across for automating the sales process?,/r/SalesTechnology/comments/10d6k0l/whats_the_best_automation_tools_youve_come_across/,automation,,
Beginner - easiest to learn automation for clicking check boxes on forms and pop-ups,https://www.reddit.com/r/automation/comments/10demk1/beginner_easiest_to_learn_automation_for_clicking/,automation,"In my organization, we have to confirm all of our reports every week. We pull up a list of reports and then check them off one at a time. Each report leads to a pop-up where we confirm the order with a password. 

What system is easy to learn and deploy and share with my fellow workers?  Thank you!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
"Would anyone be willing to share details on how the UIPath pricing works? We are currently on Kofax RPA, I'd love to switch but I fear it will be much more expensive",https://www.reddit.com/r/automation/comments/10dd3sy/would_anyone_be_willing_to_share_details_on_how/,automation,"Hey guys!

I'm the head of our department for Process Automation and one of the tools at our disposal is Kofax RPA. The business bought this in themselves (without telling IT) because a salesman sold it as fine for end users. Out of the 20 that trained, only 1 kept going and managed to DDOS the main system and so it got brought to IT's attention and came under their wing. I was then hired to work with it. I now work with this and several other systems to automate processes.

I love the ease of the software and you can spin up robots so easily. It's not cloud based, but we don't need to have any VMs because Kofax has a sort of emulated desktop where I can run excel, websites etc so we have 2 servers to run the bots as services and nothing else required. It's quick, it's easy.... but I HATE Kofax. 

There is no community, we've been asking for changes for 5 years and nothing happens. They don't offer cloud and our infrastructure if off-shored to TCS and it is always breaking down. The support is non existent. There is no training we had to learn by doing. When you ask their support or training teams, they come back going ""I don't know, we were not trained on this"". Upgrading the on prem software is week's of hard work and planning. The thing is great when it works but so flaky and not getting much investment.

I love the look of UI path but I fear it may end up waaaay more expensive. From what I've seen, you pay per month for the software but you also have to pay per month for each robot? We have 100+ processes running under Kofax and we are on their old licence - so we pay for the amoutn of ""resources"" we use at any given time. So we can run 100 robots just staggered across time so they're not all running at once. And some of the robots are big processes that can take up to a day but only go once a month, and other processes that take days but runs a couple of times a year. I can't pay per robot for a thing that rarely runs (but when it does, it enters 100s of thousands of rates into our system).

I'd love to hear from anybody willing to share a ballpark figure or even just confirm how it works for paying for robots and if I would be shelling out 1000s of dollars a year per process I automate. I will never get that past a business case!

Thanks!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I can't speak for UIPath or Kofax, but if you are currently a Microsoft shop, there is a chance you could be licensed for Power Automate. A number of [Office 365 licenses](https://learn.microsoft.com/en-us/power-platform/admin/power-automate-licensing/faqs#what-power-automate-capabilities-are-included-in-office-365-licenses) include some sort of Power Automate usage. Power Automate offers both a cloud based digital process automations (DPA) and on-premises RPA automations. You may find that many of your RPAs can actually be DPAs, which in general have a much higher success rate and less maintenance.""]"
Automating a farm: PLC Opta? or RPi and arduinos with Home Assistant?,https://www.reddit.com/r/automation/comments/10bkhns/automating_a_farm_plc_opta_or_rpi_and_arduinos/,automation,"I'm trying to automating my farm and I have around 20 sensors that would control around 10 devices that should stabilise my environment. It seems like I'm biting off more than I can bite but I'm willing to put in the ground work.

Does anyone have experience automating a farm or something like that? Should I wait for the PLC Opta or use RPi and arduinos with Home Assistant?","['What‚Äôs the extent of what you are trying to monitor? Whats the required latency? PLC Opta might be overkill depending on what you are looking to do.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'PLC and actually I would use more standardized PLC. Ie. Click from automation direct. \n\nI have these in for automation and been running flawless for fifteen years. The biggest difference is how the programming software monitors operations live and can rapidly determine problems.']"
Looking for Python Automation Project Ideas,https://www.reddit.com/r/automation/comments/10bjn1l/looking_for_python_automation_project_ideas/,automation,"Hello  folk, I've recently started looking into automating random side  projects I had lying around and ideas with Python. I'm having a blast,  but sadly my creativity is somewhat limited, hence this post. My  questions are:

\- what kind of project you have done which you are proud of?

\-  any Python libraries which are absolutely hidden gems? For me it has to  be Playwright, never heard of it before a friend mentioned it and I fell in  love with it

Thank you for your time and answers in advance, any input is highly appreciated. :)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Some of my best projects came from necessity as you have a real world problem to solve. Being able to think you‚Äôre way through putting the solution into code on the fly will make you a better programmer all around. I‚Äôve made password managers, excel interpreters, PLC communication, etc.\n\nI‚Äôve found that if you are looking to find an easy solution to something complicated there is a 99.9999% someone made a python library for it haha.', 'Selenium for web automation opens up a lot of opportunities. If you can write a good tool to automate posting to sites like ArtStation, deviantart etc, then you have a valuable product on your hands.']"
Requesting direction,https://www.reddit.com/r/automation/comments/10azvcn/requesting_direction/,automation,"I have a process where I need to open \~4,800 individual links - each link will download an Excel file. I have developed processes that help me identify the files that I need to download, and my post-download processes are also confirmed. All of this ends up in a large analysis database that I use to help my company understand their hiccups in our billing process. None of these 4,800 or so files are all that large - about 15kb is the average size. Making it more complicated is that the files are behind a username/password combination. I have looked at PowerShell and for the moment, I have this batch download utility in Chrome where I can do about 500 at a time. But needless to say, this is a day-long job if I want to do this since I have to click the button to save 4800 times. I am not well versed in development languages (I know T-SQL far better) although I'm not afraid to Google anything and see if it works. The few web-based ""macro"" systems that I've tried have made it to about the 10th file and then the speed issues take over. So I am asking the community that if you had a static link with a particular file ID and you had 4800 of those links, what is a good method for automating the opening of the website, and downloading the file?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I'd use something like this. \n\nhttps://www.httrack.com/\n\nI presume you can make a html file with a link to what you want to download and then feed that into something like this. It has an option to add a user name and password iirc."", 'You could run it with a semi-simple Python script. It would give you the ability to set where you save the file as well if you setup a main file with all the links and such', 'Hey, this sounds like a either a pyhon script or a bot built to execute on your behalf. If you aren‚Äôt technical sounds like writing the code or building the bot might be off the table so you might want to look at the platform like Wrk. Essentially they take best in class automation but break it down to micro steps that a non technical person can build and configure and enables them to access bots, python scripts. Etc. it‚Äôs just in beta at the moment but you can sign up to try it out here [wrk platform](https://hubs.li/Q01x3nTC0)']"
How to automate a Google login flow using Selenium scripts?,https://www.reddit.com/r/automation/comments/10ak1sw/how_to_automate_a_google_login_flow_using/,automation,"We are trying to automate a Google login flow in our Selenium script.  

When the test runs, Google treats this as suspicious/spam-bot activity, and asks for additional verification. So we get either a Captcha screen or a OTP code login screen. 

Has anyone solved this problem before?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I think this does what you're asking. Check out [auth.py](https://auth.py) in the below git:\n\nhttps://github.com/alexgolec/tda-api/tree/master/tda"", ""There really isn't enough detail to provide exact tips/this is an old post, but in the event other people are having a similar problem, it may be helpful to read this:  \n\n\nhttps://stackoverflow.com/questions/60117232/selenium-google-login-block""]"
We made an SDR tool. Is it helpful for you guys?,https://persio.io/ai,automation,,
Automation consultant? Python ninja?,https://www.reddit.com/r/automation/comments/109j9xn/automation_consultant_python_ninja/,automation,"Hi there. I run a small events website based in Las Vegas. I use some automation tools like Visualping and Magical to help me track and input events, but I'm hoping to add some more tools to my arsenal. 

I've toyed with learning some basic Python to help me scrape data, and messed with ChatGPT a little, but it's a bit of a learning curve that I don't have time for. I'll cop to some intellectual flabbiness, too, sure.

Ideally, what I'd like is to describe to an expert what I would like to automate and how I'd like to automate it, walk them through my process, and either have them point me to the right tools or even create the tools themselves. This is something I'm happy to pay for.

Does such a thing exist -- an automation consultant or Python or ChatGPT ninja who can build me a bespoke solution for my specific needs? Thanks.","['We do exactly this! DM me and we can discuss what you need.', 'PM me if you want, I make fast deployment tools in the controls field all the time in python. I‚Äôm sure we could come up with what you need!', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Thanks to both of you! Glad to hear this exists! I‚Äôll be reaching out.']"
Is ChatGPT a trap for businesses?,https://www.reddit.com/r/automation/comments/109c368/is_chatgpt_a_trap_for_businesses/,automation,"I've seen a lot of excitement from various businesses about using ChatGPT to let them reduce their staffing costs in fields like customer service. If you look at the numbers right now, it makes a lot of sense because the costs of ChatGPT are current really, really low. But the terms and conditions of ChatGPT make it very clear that those costs can change, and I wonder if businesses might be walking into something of a trap.   


There's a lot of money to be made in automation, I know, I am an industrial automation engineer. But I expect that OpenAI (the developers of ChatGPT) knew exactly how much money their software could save companies when they developed that software, which is why they did it, and they don't intend to let the companies pocket the lion's share of those savings that they did they work to realize.   


So right now, companies can be early adopters and help ChatGPT build market share, train their AI, integrate it with other software packages and demonstrate it's ability to displace workers in the real world. But companies that go down this route and going to make ChatGPT essential to their business operation. So when ChatGPT decides to 10x, 100x, 1000x their costs, those companies are going to have to pay it, and at that point they may have done a lot of work to make ChatGPT a viable alternative to a human employee only to pay OpenAI some non-trivial percentage of the pay of the human worker their displaced.","[""Not necessarily. It could be that they leave ChatGPT at low cost for small commercial consumers, and make it expensive for larger projects such as government and military contracts. AI will always need to consume a lot of new data, because the world and vocabulary changes constantly. Making it cheap ensures a constant stream of data from small companies, which they can use for their big business contracts.\n\nThat's my two cents, maybe I'm wrong though."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
I got early access to Bardeen + OpenAI! What's one automation you would love to have?,https://storage.googleapis.com/postly-548c1.appspot.com/PHOTOs/cdc48a1f97099bb9e3283e76ef9717058323c94e55c9fac0dd538326d031a5e8.jpg?GoogleAccessId=firebase-adminsdk-pgu3n%40postly-548c1.iam.gserviceaccount.com&Expires=253376553600&Signature=UM8FHEm5IC1012qXi%2BK1%2FEQO09Bw4qQ99pMbdHjese7FsnR%2BiAgVOxlJsOZRQ52VrRBSCDTiBQAvp9s2Ux28oQdZMQOM0cXhIgh%2FF2UulzUXY%2FPRcmmIl5BzLt1LqwbC8kzdtFJJLRXoFVkRQhqGBhvq%2Bz0jGEVejvMAeLDfwfnspZi7R9ZuF6%2F%2FN01Gd%2BhaqErL9%2B8XyliMhTnrVvX61nn11njKtsHwxyS8rowy1nFNhkBdE4YNluVv%2Fvan3UnMvuxwk9BIEQ0T2Y04Ar0MHuGpYxy1cZbrVyVyC4fn5LRznLIAWJppLliLnc6gsioEtS6Y6hhwmZ7Ne9SlEQ4QTA%3D%3D,automation,,
Need help to automate a WhatsApp group chat data to word,https://www.reddit.com/r/automation/comments/109hw07/need_help_to_automate_a_whatsapp_group_chat_data/,automation,"Hello, so at my work the sales team uses a WhatsApp group chat to communicate. The format is something like: order number, client name, picture of design selected and info about design (quantity for example), then the other product, followed by a dash line to indicate the order is over and then repeat same format for new order.   Now we have one guy who‚Äôs only task is to copy paste that data into word and print is as purchase order. I want to automate this process by directly somehow having people input the data into WhatsApp and it getting converted and pasted to word, so then end of the day all I have to do is to print all.   Any help is much appreciated.","['Could you change their process? That‚Äôs going to be a lot easier than trying to extract from WhatsApp. Extracting over email would be pretty straightforward and easy.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Could do this with Android adb to get the messages, something like https://gist.github.com/erickdsama/8b312635ca34770f69d08f58d65f69d1, you could also to use Android emulator and WhatsApp and just copy and paste them all  into word directly much faster.\n\n\nEdit: is this whatsapp on phone? I'm pretty sure there's a website version which you might be able to do with curl to get the messages"", ""There's a desktop version of Whatsapp.  You could possibly do it with that.  Seems like the best option though would be to switch platforms to either email or something not as secure as Whatsapp."", '[wrk](https://hubs.li/Q01x3nTC0)can do that- they would extract the information from the chat daily and then upload directly to word']"
Is there a way to setup Google Sheets to send a weekly message on a Messenger chat?,https://www.reddit.com/r/automation/comments/108m17j/is_there_a_way_to_setup_google_sheets_to_send_a/,automation,I have a facebook messenger chat and I need to send a weekly reminder with the title of the weekly seminar talk from a google sheet - is there an easy way/instructions to send messages to Messenger from an AppScript. Where would I plug in/find FB access keys?,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Hey, not 100% how they connect but I do know that you can do it via [wrk](https://hubs.li/Q01x3nTC0). Basically it can take the data from g sheets use a pre populated message to then send out to fb messanger users (assuming name is available in g sheet to determine who to send to). Platform is currently in beta but I know they have the capability.']"
No-code Google Sheet automation,https://www.reddit.com/r/automation/comments/108s379/nocode_google_sheet_automation/,automation,"Hi all, I just published an article on the no-code Google Sheet automation. [https://www.bardeen.ai/posts/google-sheet-automations](https://www.bardeen.ai/posts/google-sheet-automations)

It shares detail on how to automatically accomplish the following:

1. Copy LinkedIn company data to Google Sheets
2. Copy a list of meetings during a timeframe to a Google Sheet
3. Enrich TikTok links and save them to Google Sheets
4. Copy ClickUp tasks to Google Sheets
5. Get Google Search Results for a keyword and save them to Google Sheets","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'sweet will check it out, been looking for something similar']"
Logical flow of moes brand zigbee push button?,https://www.reddit.com/r/automation/comments/108fdug/logical_flow_of_moes_brand_zigbee_push_button/,automation,"So I got my first zigbee hub and switches.

Could anyone help a dummy out here?

Here's the situ:

*   3 smart bulbs exist in OFFICE.

*  In Tuya Smart, I've setup the following:  
     IF office(123) OFF and PIR Motion Detected  
     THEN execute automation officeON

I created the inverse of the above so the bulbs turn off when detected as on.

I've also tapped to set ""When ALL conditions are met""

I understand why it does not work:  I've created a loop.  When I press the button, it's triggering both routines.  

The issue is I don't know how to set it up as Tuya wants me to do it.  This would be 1000x easier if the execution simply toggled the power state rather than having to make routines to check the power state. 

Another oddity is in another office, it works exactly as it should but that office only has 1 bulb.  The weird thing is I intentionally created a loop by setting PIR motion sensor to both ON and OFF state. 

Could anyone elaborate how ya'll got yours to work cause mines driving me insane?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Enhance your Automation Superpowers: connect Python to Google Sheets,https://www.callmefred.com/how-to-connect-python-to-google-sheets/,automation,,
Does a Bachelors degree open more doors?,https://www.reddit.com/r/automation/comments/107ppe8/does_a_bachelors_degree_open_more_doors/,automation,"I work in medical manufacturing and decided to go to school for Automation. I‚Äôm currently in an Associates program for Automation and Robotics Engineering. Most postings for Automation jobs only require a two year degree, but I was wondering if anyone who‚Äôs currently an Automation Engineer has any insight into whether turning it into a bachelors degree would be worth it?","['I‚Äôd say it‚Äôs a different path, depends on your goals and what you want out of career. I have a. BSEE and work with a lot of Automation Technicians. For better or worse we both do similar jobs but a lot more of the grunt work gets done by the Technicians where I develop the overall plan and lead and organize the design and build. I don‚Äôt talk pay with them, but I‚Äôd assume a BS pays more then an Associates. I do think both positions will not have an issue finding work as the company‚Äôs I work with and the company I am at have a hard time hiring either.', ""Another option to the EE route is mechanical engineering. My ME program had an option that focused on controls and mechatronics that would give you all the tools you could dream of to build fully automated lines or a robot from scratch. And an ME degree opens a lot of doors. I would go through the program you're in and look into transferring credits to get an ME degree."", ""I Also have an associates degree in Mechatronis and Robotics and work in my field as a Robotic technician. I am on route through my work to become recognized as a automation engineer within my company. I was going to school to obtain my bachelors degree in mechanical engineering but withdrew during covid. I was going for free and only have about 8 classes left but i have no desire or need to finish. What I have found is that the industry can find mechanical engineers no problem, and to be honest Ive only met one who has any practical mechanical aptitude and knows how to use tools or machine parts, they can play on solidworks and thats about it. I currently make 33 dollars an hour but am on track to make 6 figures in the next three years. You should stick with the associates, get in somewhere on the tech level, learn how to build controls panels, troubleshoot automation issues and turn a screwdriver, program plc's, learn ladder logic, read electrical prints and anything else you can. The need for skilled automation workers is in such demand you will be rewarded handsomley. There are very few engineers these days that have all of those skill sets and you will quickly be more valuable than someone with a bachelors degree. Companys more than ever prefer candidates to have industry experience over degrees. \n\nGood luck!"", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'I‚Äôve only worked for large manufacturing companies, but the people I‚Äôve seen who didn‚Äôt get bachelors struggle to get into leadership positions down the road. \n\nIve talked to a few and they always say they thought they didn‚Äôt want a leadership role, but now with the technical background they‚Äôve developed, they realize how useful they could be as a leader and how they could make a difference, but the lack of degree is the hold up.', 'Unlike a lot of degrees. Technical degrees like this actually can give you more insight into a line fo work. If you did not have the degree you certainly have to have some high level projects that you were involved in intimately in order to show your experience.']"
How would I go about automating my MURAL tasks in my user story map from JIRA? (Project Management),https://www.reddit.com/r/automation/comments/107sgqi/how_would_i_go_about_automating_my_mural_tasks_in/,automation,"Hey guys, I find I'm manually spending quite some time manually updating my user story map every sprint. I'm thinking there must be a way to automate this, in the sense that when someone changes the status of their assigned task in JIRA, it would automatically update in MURAL?

I'm new to automation so any advice would be helpful and a fantastic learning experience. :)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
What type of automation are you most interested in for your business?,https://www.reddit.com/r/automation/comments/107lin1/what_type_of_automation_are_you_most_interested/,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', '[deleted]', 'lead generation, negotiation, development, research, security.']"
Filling the Gaps in Workspace ONE Access Audit Logs,https://mobile-jon.com/2023/01/09/filling-the-gaps-in-workspace-one-access-audit-logs/,automation,,
"Just getting into browser automation, need advice",https://www.reddit.com/r/automation/comments/106zj7u/just_getting_into_browser_automation_need_advice/,automation,Currently using axiom to do some lead generation and simple task. I defiantly see the possibilities but am finding there's not a ton of info online about which programs are the best. If I just used Zapier and Axiom would that be enough to accomplish anything I want to do in terms of marketing and sales data automation or is there other programs I should be looking at?,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Hopefully this doesn't sound to much like a sales pitch. I am a huge fan of Zoho One. It has Zoho Flow which is a program like Zapier and also comes with Zoho CRM and Zoho Campaigns, which are powerful tools that would accomplish the things you have listed here. It also comes with a huge suite of other programs for your business. Best part is you can integrate them all together and avoid alot of busy work while getting the most out of your data."", 'Text expanders are solid. I use Chrome a lot and Text Blaze is probably my favorite text expander that works well on Chrome', 'Looking into this too for LLC skiptracing any tips?']"
Document Scanning and Naming to OneDrive,https://www.reddit.com/r/automation/comments/10578r0/document_scanning_and_naming_to_onedrive/,automation,"Hey all!

Just started up at a new Restaurant gig in management and they've got a pretty lengthy process on reconciling and uploading invoices to their corporate storage. 

Essentially after reconciling invoices they have to Scan each document via the OneDrive App (Which managers do with their phone), Deal with the whole Pinch the corners in to crop the actual document, Upload it and rename each Invoice to have the Invoice Number and Total Amount. 

What I'm trying to streamline is :   
If we were to get a [Document Scanner](https://www.amazon.com/Epson-Workforce-ES-50-Portable-Sheet-fed/dp/B07KQZWPYN/ref=asc_df_B07KQZWPYN/?tag=hyprod-20&linkCode=df0&hvadid=312342833501&hvpos=&hvnetw=g&hvrand=1884267105011814510&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1026528&hvtargid=pla-612329262651&psc=1), Connect to the back office PC , Scan the documents to a PDF, and have either:

*  Invoices Parsed for both the Invoice Number and $Total
* Dialog Box Popped up to input Invoice Number and $Total

and have it uploaded to Onedrive in that format. 

Any suggestions? I've been a reddit Lurker for a while but have barely posted. Anything helps!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Scan and OCR documents: https://paperless-ngx.com. There may be a more purposed software out there, but this is a good start at least. You can host it on any machine. The more powerful the machine, the faster it is and the more accurate you can OCR.\n\nTo upload to OneDrive, you can either use\n* the official OneDrive sync client\n* https://rclone.org triggered by Scheduled Task (Windows) or cronjob (Linux+MacOS), or a more complex script to trigger on new files detected in the folder', 'Hey, OCR is definitely what you need for the first piece and then you‚Äôll need an api/integration to upload the data. If you want a one stop shop to be able to parse the information/upload data directly [wrk](https://hubs.li/Q01x3nTC0) has these capabilities built in. Beauty is you don‚Äôt need any technical skills to build on it and it cost as little as $1000-5000 to get started and then you just pay as you go.']"
Help automating file saving,https://www.reddit.com/r/automation/comments/1056jkd/help_automating_file_saving/,automation,"Hi all - 

I am looking to get help automating saving a bunch of files, individually. I have an excel sheet that I need to print line by line 100+ times to PDF on a regular basis. It is cumbersome to do this all the time. I have a working macro in excel that will highlight and print each line, but I have to manually add the name and save it. Is there a program or something that I should download that will allow me to write a macro for everything, not just excel, ideally integrated with excel? Or is there a way to do it without having to download 3rd party software?

TIA","[""That sounds horrible. I'd do it with python. If you've got a couple weeks you could learn enough to get it done. Otherwise, you could probably get a freelancer to write it on a day or less.\n\nhttps://automatetheboringstuff.com/"", ""VBA\n\nIf you already have managed a macro in excel you shouldn't have too much trouble integrating some VBA code in the macro editor to make this happen.\n\nedit: read your question again and it sounds like it needs a counter, end of page, or some other trigger to do the file name and save portion. Shoot me a dm with some sample data if you get stuck."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Either python byt then you need to have some learning curve and have python env on your PC. Or you could try to do it in VBA - one environment, you don't need to install anything. Python could be useful in many things, so maybe it's worth learning."", ""Windows 11: Microsoft Power Automate. It's pre-installed \n\nWindows 10: If without downloading third party software you mean that you don't have admin access and need to do it without being able to download anything, then Microsoft Power Automate is likely out and VBA is your answer.  If you mean that downloading first party software is acceptable, then Microsoft Power Automate"", 'Thank you all for taking the time to offer advice. I actually took someone else‚Äôs suggestion and used chatgpt this morning and automated a few excel tasks already. Appreciate y‚Äôall for the help though']"
Automate Word/Pages form on Mac,https://www.reddit.com/r/automation/comments/104uxmf/automate_wordpages_form_on_mac/,automation,"Hi. I have a fairly straight forward form I have to fill in to kennel my dog on occasion. Is there anyway I can take data from a Numbers spreadsheet to automatically create the filled in form with dates etc included? I only have Mac but I do have Office Excel and Word if that's the only way, ideally looking for a Mac method using Shortcuts or Automator or something similar. Thank you","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'I am thinking of something sinilar for my daughter‚Äôs school health screening. I will pop back if I get a chance to work on it or look for replies on your end. I imagine you might need to frankenstein it with datajar and keyboard maestro.', ""I'd be reaching for python - you should be able to read most spreadsheet formats using pandas, and filling out a form really depends on what format the form exists in. More info would be helpful""]"
Does a handheld/non-phone remote control for Alexa lights exist?,https://www.reddit.com/r/automation/comments/104d9bh/does_a_handheldnonphone_remote_control_for_alexa/,automation,"Pretty much title.

I‚Äôm looking for a simple remote that can be left on a coffee table.  

One that would allow users of aforementioned remote without barking to Alexa, fumbling through an app, nor getting off the couch.  

So no wall switches or apps. 

I feel like there‚Äôs some competitive reason this doesn‚Äôt exist else I‚Äôm sure I‚Äôd see it advertised everywhere.  Hopefully I‚Äôm wrong on that.

Thanks in advance!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'No. And it would not be trivial to make one, since you would need to create an Analog/digital interface, where once one turns on/off something (analog, as you described it), it triggers something digital (this can be implemented with zapier + Alexa routine or IFTTT + Amexa) but it would need to be customized either once-and-forget, or each time one wants to change something.\n\nIMO the market is very small.\n\nI am an early adopter of Alexa, and I have had my entire place automated for a few years now.\n\n-----------\n\nEdit: a second thought, this is what you might want to explore: building a small analog device that triggers (on/off) an Alexa routine. This way it\'s not just for the light, but for anything one wishes to do.  I like the Apple AirTag format, small, relatively long battery life (add wireless charging). It would become yet another device in the Alexa world, and that you could ""pair"" the device to a routing (you might need to develop a *skill*).\n\nAt that point, one could assign routines to a device on a whim: *Alexa, assign morning routine to Tag A*. *Alexa, assign kitchen lights routine to Tag B*; or - course - via the Alexa app.', 'Depends on the lights you get. Alexa specifically no I dont think so, but I know Lutron has one.', ""https://www.wired.com/story/how-to-create-custom-alexa-routines/\n\nNot quite what you are looking for but there should be a way to do something like this by the looks.... Create a routine and tie it to a 3rd party server that syncs up to the controller you want to use. \n\nJust a case of finding the parts that play well together or get someone to build a small Arduino or similar device that does what you need. \n\nBut I'm no expert so you will have to explore further.""]"
Top 7 Programming Language For Automation For 2023,https://i.redd.it/7e05ggndw6aa1.png,automation,,
Research to template automation help,https://www.reddit.com/r/automation/comments/102pwz2/research_to_template_automation_help/,automation,"Hey everyone,

I'm doing a project that involves email outreach using templates that I fill in with information that's specific to the person I'm targeting. I'm wondering if there's a way to develop an app or if there's something out there that would allow me to feed it the information on a single person (if I could do multiple that would be even better) then it would provide me with the finished template with everything filled in. Currently, I'm doing everything with find and replace on Word however there has to be a faster way out there to do this. I tried my best to explain what I'm doing but I'll provide a photo of what exactly I mean with the custom email outreach below. Any thoughts or suggestions are greatly appreciated!

&#x200B;

https://preview.redd.it/vurdoataix9a1.png?width=1302&format=png&auto=webp&v=enabled&s=cfafb6558bd628bedb19e086a5f908d813289576","['As someone in sales, you can do this, but please, don‚Äôt. Professional inboxes are overflowing with this kind of spam, and it does not work. It‚Äôs noise.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Zoho CRM can do this. Pretty easy actually. You can import all of your target audience in as contacts then set it to send out. It will auto fill the relevant information.   Then it's like 60,000 a day you can send before it asks for more money."", ""I think I have a solution to that problem. There is an AI tool that personalizes the generic stuff that you have. It creates personalized videos and you don't have to re-record for every lead that you have. You can record once addressing generally to their problem and it will personalize it in a number of 1000s within a few clicks. Additionally it incorporates CTA, custom URL's & titles all in a single landing page. Makes them think it's for them. You can check it.""]"
Automation technician/engineer degree help. Can y‚Äôall vets check my associate degree classes and give me advice on if it‚Äôs a solid set of classes to get me started in the field thanks I‚Äôm 22 and wanna break into the automation field. Thanks any input is appreciated,https://i.redd.it/1wvlo1p74y9a1.jpg,automation,,
Keep your resume up to date through continuous integration - Github Action,https://github.com/marketplace/actions/awesome-cv-builder,automation,,
Why doesn't Echo Show 15 detect people instantly?,https://www.reddit.com/r/automation/comments/100txnf/why_doesnt_echo_show_15_detect_people_instantly/,automation,"Some of the newer Echo devices can trigger routines by detecting people present. 

I think most of them go on sounds. Breathing and soft humming won't do it, but speech snaps them to attention instantly. 

My Echo Show comes with boasts that it uses its camera to this end, too. Makes sense. (Not if it's true on every variant Show model.)

So why doesn't it register presence instantly when I come in and open the camera shutter? I'd hoped to set this up like a button or lightswitch.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Looking for feedback - Users subscribing but not understanding capabilities of product,https://www.reddit.com/r/automation/comments/zzfs0a/looking_for_feedback_users_subscribing_but_not/,automation,"I built an application where users can click ‚Äúrecord‚Äù then perform whatever actions in a browser like add data to CRM, send email, send message etc and then replace whatever value they typed in the recording with a dynamic value from google sheets. For example let‚Äôs say you want to send a bunch of users a message on Instagram and you have a list of usernames in a google sheet. You click record, go to Instagram, type in the username, type the message then click send. Then you edit this to use the value from google sheets instead of what you had actually typed.

Most of our users signed up to send automated instagram messages and have understood how that works pretty well but when wanting to automate actions on another site / in a different way it doesn't go too well.

Whatever you record can then be triggered by webhook, new row in google sheet, scheduled etc.

This is working great and we have a number of customers using it to send Instagram messages but they‚Äôre not seeing that the same idea can apply to LinkedIn, Facebook, openphone etc. without spending 10-30 minutes on a call they‚Äôre not understanding whatever action they record can be performed from a cloud service later with dynamic data so they don‚Äôt have to.

I‚Äôm a bit overly technical with explanations which is the fault for the majority of this but how would you better train users on the product without getting on 30 minute calls or building resource pages with videos that are just going unwatched

I understand a lot of these things can be done via api, with zapier etc but most users we have cannot do that and for some sites there is no api so there is no real alternative besides hiring a VA/being a developer to write a custom script/giving up your time

Thoughts?","[""Maybe an intro video that comes up when they first login? At least then it's right in their face."", 'Perhaps a video tutorial, on 3 websites it can work on, or what you think would be its most common use cases. With perhaps some templates people can edit\n\nDumb it down enough that the target market can use it. Perhaps have a friend/family memeber read the instructions/script and see if they understand and can carry out the instructions.\n\nMake sure you maintain documentation for your product also. It should come with basic-advance guides.\n\nDm me and i wont mind going over some sort of script for a video with you.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'What‚Äôs the product called? I‚Äôm interested']"
Development Environments,https://www.reddit.com/r/automation/comments/zz6qdh/development_environments/,automation,"Hey all, we have an automation company customer specializing in manufacturing systems. They work with all the industry standard tools and controllers. 

Their in-house development is using windows 10 VMs on a VMware host to create POC including programming their controllers and PLC code in house. 

We are looking for a solution to streamline the development process. The hardware on prem is old and we are putting together some paths forward. 

What are others using to develop for customer solutions? We are an IT consulting firm performing an assessment and we wanted to see what others in our industry are doing. Any insight is appreciated. 

One constraint is that they have to bring laptops onsite for the POC and sometimes there is no internet. 

Thanks!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Hi,\r  \nAI in fintech is the use of artificial intelligence for financial data analytics, customer service, supply chain management, trading advice, and much more. \r  \n\r  \nWe at AIACME provide Artificial Intelligence solutions for your business workspace. We help you to Automate your operations reduce manual errors and increase your productivity.\r  \nReach us at enquiry@aiacme.in']"
Email & Punch Card Automation ideas?,https://www.reddit.com/r/automation/comments/zz5ymx/email_punch_card_automation_ideas/,automation,"I do a lot of interviews and emails for work, and recently HR said they want us to punch in, punch out when responding to an email. Is it a hassle? Yeah. Is it good they want us to get paid to work? Yeah. But I get the feeling it could be quasi-automated.

So the gist of it would be a tool that I can open, makes a logging stopwatch, scores it into a google sheet, which then tallies it into a total clocked hours and can be logged as a single item at the end of the month. It's super rare I'd work overtime in any given week, so not really worried about being stiffed a weekly bump.

This might exist in some form, but if not perhaps the parts of it already do.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'You could use Zoho Desk for this. It allows you to set up an email address that makes ""tickets"" when an email is received. There is a time clock on each ticket and you can pull reports of time spent during a date period. It also can relate emails back to the ticket. That would also allow you track individual situations easier I would imagine vs having it just in your email somewhere.', 'Part of me says ""why reinvent the wheel"" here as there are numerous options that your company could pay for as a turnkey solution.\n\nOn the other hand, this would be easy enough to do in Excel as I\'ve made a calendar for vacation/PTO that would email out to a manager whenever someone made a change.\n\nFind a calendar template, set up a macro, and have a submit button that would sum the hours for the time period and send an email.']"
How Does ChatGPT Help When Writing Automation Code With the Playwright Tool?,https://www.reddit.com/r/automation/comments/zz0plw/how_does_chatgpt_help_when_writing_automation/,automation,"In its first week of use, [ChatGPT](https://openai.com/blog/chatgpt/) smashed numerous Internet records. As someone who works in QA automation, my first idea was how I could use this wonderful platform to make it simpler for testers to work on Web automation.

As ChatGPT may be used to write code in a variety of programming languages and technologies. After more investigation, I made the decision to create some scenarios using it. *I used ChatGPT to develop the Cucumber feature file and various use cases based on UI situations.*

**Please follow the** [**link**](https://qaautomationlabs.com/chatgpt-for-automated-testing-for-playwright/) **for more detail**

*We can use ChatGPT to generate the Code but we can‚Äôt say that the generated code is perfectly fine. The generated code needs to be modified in some situations.*","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
ChatGPT is absolutely insane,https://www.reddit.com/r/automation/comments/zygstz/chatgpt_is_absolutely_insane/,automation,"I've never worked with a learning model that's so flexible and robust in terms of generative output, with the exception stable diffusion and/or midjourney.

Where do you folks see this tech going? Do you think we'll see additional open access, large-scale lawsuits against generative AI, or something else entirely?","['There will be lawsuits and chatGPT with be it‚Äôs own lawyer‚Ä¶‚Ä¶', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
How do you use browser automation at work or in personal life?,https://www.reddit.com/r/automation/comments/zygxp0/how_do_you_use_browser_automation_at_work_or_in/,automation,"Hello all! I'm curious to know if anyone uses browser automation tools in their work or personal life. If so, could you share how you use these tools?  
I'm aware that they are commonly used for testing, data gathering, and marketing, but are there any other applications for these tools?","[""I'm not sure if this fits the bill, but I really like using Tampermonkey because it allows you to write custom JavaScript which can interact with pages on your behalf.\n\nAs an example, I have a project that I was working on which needed to have pages advanced at regular intervals for render testing, both front and back-end, so I wrote a simple script that simulates a click on the advance button after waiting for the page to load.\n\nTaken further, you could theoretically automate quite a few things with an approach like this if you were looking for something more customized, and could even use a framework like Selenium to access additional functionality.\n\n[https://www.tampermonkey.net/](https://www.tampermonkey.net/)\n\n[https://www.selenium.dev/documentation/webdriver/](https://www.selenium.dev/documentation/webdriver/)\n\nEdit: I should mention that you can keep it as simple as you like, or you can go as virtually as complex as your needs necessitate. It's basically injecting custom code into any page that matches the URL path you specify within your browser.\n\n    (function() {\n        'use strict';\n    \n        // Your code here...\n        setTimeout(function(){\n            document.querySelector('.next-button').click();\n        }, 2500);\n    //localhost:8011/home/session*\n    })();"", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I try to avoid some of the more complicated automation tools because I don't know how to use them. I just use a simple one called Text Blaze to help automate a lot of my work. It helps me send basic emails, messages, and quickly schedule calendar meetings. Overall makes typing pretty fast"", ""I have written bots in python/selenium to assist in buying hard to get items and in social media marketing.  It saves me tons of time and they're fairly easy to do.""]"
Generative AI and automations for Writers in Tech (Free online event),https://www.eventbrite.com/e/generative-ai-for-writers-in-tech-free-online-event-tickets-500154725247,automation,,
Low-Code Developer,https://www.reddit.com/r/automation/comments/zybroz/lowcode_developer/,automation,"You love tech, building systems, and automating, to make it play together tighter than The London Philharmonic Orchestra.  
But there are two major problems.   
1. You are saying the same thing over and over again to customer. You just want to build cool stuff not be a customer relations manager.  
2. The lack of security. One week you have five projects the next week you have one. The feast is great but the famine feels like you have been placed in an Oliver Twist novel.

If you are looking for a new technical challenge and to be part of a fast-growing Marketing Agency (growing at a rate of over 300% per year) this could be the perfect fit for you.  


What we provide:  


1. Guaranteed 40 hours every week
2. Fully remote working (work from anywhere)
3. Competitive salary
4. 28 days of paid time off per year 
5. Regular working hours 
6. Supportive personal growth environment
7. We handle customer communication so you can do what you do best
8. Smart QA processes to make sure you have all the information you need and reduce unnecessary noise.
9. One clear point of communication so you don‚Äôt have to worry about dealing with multiple stakeholders
10. A team that lives by the values of effectiveness, data and accountability
11. Clear and documented SOPs that are constantly evolving to support the team in being as effective as possible.
12. Support and training on new tools and software to give you.
13. Weekly review to discuss anything you need help and support with to be the best version of yourself in your role.
14. Career development opportunities as the company grows. We believe in growing talent from within.
15. A clear vision that you can be part of that supports both financial goals and philanthropic goals to impact more lives in the world
16. $250 a year personal development scholarship that can be used for any kind of training you wish. Whether that be personal growth, learning a new skill, going to a mastermind or an industry event. It‚Äôs totally your choice. We support you.
17. A clear project management software system so you always know what you should be working on and what is a priority each. No more guessing games.
18. A fast-paced environment that is exciting to be part of.
19. To push the boundaries of what is possible with new technologies  
**What you will be doing**

As part of Amplify, you will be helping us complete strategic goals. To add clarity, you will either lead an activity, help manage an activity, and/or be accountable for the outcome of the activity.

**Direct Supervisor:** CEO

**Functional Supervisor:** Project Manager

## LMA Tasks

**Key:**  
L - Lead

M - Manage

A - Accountable  
**Tasks:**

1. LMA - Determining project requirements and developing work schedules for the team
2. MA - Delegating tasks and achieving daily, weekly, and monthly goals
3. M - Liaising with team members, management, and clients to ensure projects are completed to standard
4. A - Identifying risks and forming contingency plans
5. MA - Analyzing existing operations and scheduling training sessions and meetings to discuss improvements
6. MA - Keeping up-to-date with industry trends and developments
7. A - Updating work schedules and performing troubleshooting
8. M - Being transparent with the team about challenges, failures, and successes
9. LM - Writing progress reports and delivering presentations to the relevant stakeholders
10. LM - Create Go High Level automations and templates
11. LM - Create Active Campaign automations and templates

## Salary

1. $8000 per month based on 40 hours per week   
comment on this post to get the assessment link","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Hi there! I‚Äôd love to take the assessment and see if it‚Äôs a good fit :).', 'Send me the assessment.', 'Hi there!!  \nSend me the assessment!!']"
Has anyone ever worked with software from Survalent for SCADA systems?,https://www.reddit.com/r/automation/comments/zxcwg7/has_anyone_ever_worked_with_software_from/,automation,"I am currently working on a SCADA for a shrimp farm, the software we are using is from Survalent. The map editor is SmartVU, and the program to link the variables is STC Explore. 

There is no information anywhere on the internet on how to use this software and I‚Äôm having a hard tome figuring it out.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Help Automating a Word Document,https://www.reddit.com/r/automation/comments/zwvdul/help_automating_a_word_document/,automation,"Hello! So at my current workplace, every time we get a new patient, we manually fill out multiple word documents. I have managed to combine all these word documents into one word document, however, there are too many fields for us to manually fill out and most of the fields are the same thing repeated. This document includes things like the patient's name, DOB, and etc. Is there anyway where I can make it so that whenever we get a new patient, I can just type the patient's name and DOB and it automatically populates the fields in the document? I know I can do this with Microsoft forms but the document is setup weird and has a table in the footer and whenever I submit the form, it wipes out everything in the footer and only leaves the filled in text. Does anyone have any ideas on how I could possibly automate this? Thanks!","['You can use VBA to Automatically fill up word reports from excel data. \n\nsee this video for example\n\n[https://youtu.be/f\\_ya8IIicAo](https://youtu.be/f_ya8IIicAo)', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'In this day and age why is a healthcare company (even the smallest of them) filling out Word forms? Is this for some edge cases or did your workplace never modernize? Most health companies are using a vendor product now (SaaS or maybe still a desktop client).\n\nIf you are going to use Word still for a while, I suggest doing a ""mail merge"". You can fill in the details in Excel then the mail merge will output a Word doc with the completed fields. For one patient or hundreds.']"
Auto play a YouTube video/livestream on a Google TV,https://www.reddit.com/r/automation/comments/zx1951/auto_play_a_youtube_videolivestream_on_a_google_tv/,automation,"I have one of the new Chromecasts with Google TV and was wondering if I could make it always stream a specific YouTube stream as a cable TV replacement for my grandma (she always only watches one specific channel). Does anyone have suggestions on how to either force an Android TV (Google TV is based on android) to play 24/7 while still allowing the TV to turn off, or make it auto-load the video when the TV turns on? Any help would be much appreciated.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Need help with automating the filling up of a word document,https://www.reddit.com/r/automation/comments/zveu7j/need_help_with_automating_the_filling_up_of_a/,automation,"I work for a company that does something called alarm activations. They have a document template that needs to be filled up every time there has been an alarm activated. We then receive a whatsapp message from the guard and take the content and populate a word document template we have and email it to the higher ups. I want to know how I can automate this whole process so that as soon as the message is received, the relevant content on the template is filled and sent to email directly. 

&#x200B;

How can i do this?","[""Does the alarm have an API available that you can either subscribe to a webhook or poll every 1/5/15/30/60/etc minutes to look for new alarms since the last poll? If so, you could extract the alarm details from the API and use PowerShell and some extensions to generate a document from the MSWord template... That's just one way I can think of off the top of my head, but without knowing the details about how the alarm works (or at least, how it communicates events), its difficult to say for sure what the best approach might be."", 'Word VBA will probably be the simplest to fill the form out and email it. \n\nId probably look for some kind of automation that can get you WhatsApp > raw data > network drive or email then VBA can pull the information from there.', ""To provide an alternative to those below: you could write a program that integrates with the Whatsapp API and/or look at twilio for processing whatsapp incoming messages.\n\nOnce you get your data. If you have multiple messages, filter messages by a date (such as today's date), process each message that meets all criteria (you can add validations such as if the guard correctly filled out everything), then write the data into a word file (such as by using a library), and then email the file."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
"tools for streamlining project paperwork flow on municipal contract jobs. (Submittals, factory testing forms, startup forms, O&M manuals)",https://www.reddit.com/r/automation/comments/zud543/tools_for_streamlining_project_paperwork_flow_on/,automation,"I imagine this is one of those scenarios where it's best built from the ground up by the person asking the initial question. The context is a municipal contract job where you do months of paperwork first before going onsite to do equipment installation testing and commissioning. Is there anything already existing that is almost like a database for a project, where I have an instrument and control cabinet lists where I enter, say, an instrument tag number once. Then, everything in the project mentions that tag number is extrapolated from that initial database entry, so I don't need to repeat entering information. And is there anything built on top of any existing standards like ISA. 

I have my specification packet in PDF form, site drawings (floor plans, electrical and control schematics). I have to itemize everything relevant to my scope in a datatable or database that is derived from the specification packet. I then have to build my documents using adobe pro, word, and excel. Sometimes, im provided template forms that i have to use for every instrument (sometimes 40 identical instruments). I have to fill ouy 3 or for forms for each instrument i procur and field test, then i have to put those field test forms in a O&M manual along with the instrument manuals, cutsheets, warranty policy, manufacturer information, vendor information (location, contact info, latest price)","['I think you might be looking at a document cloud here, not sure if that satisfies all of your needs, but there us Paperless-ng.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I'm not sure that this would solve your problem entirely or provide a complete solution, but you may consider looking into the MS Power App platform.\n\nYou may be able to build something custom that transposes data as-needed while keeping everything stored in a relational database, and depending upon how specific you want your functionality to get you may be able to bring on a developer as a freelancer to help you push your system over the finish line.\n\nhttps://powerapps.microsoft.com/en-us/""]"
Created this Updated list of the Best Automation Testing Tools,https://coursesity.com/blog/best-automation-testing-tools/,automation,,
How to automate creating technical evaluation sheets from pdf quotations,https://www.reddit.com/r/automation/comments/ztb6u6/how_to_automate_creating_technical_evaluation/,automation,"Hi, I was wondering how to accomplish automating to create a technical evaluation sheet by extracting data from quotations in pdf files that are of different formats from different suppliers. Would it be possible ? If not should I require suppliers to fill a uniform pdf format?","['You need to provide a lot more detail friend', 'Rather than getting pdf from suppliers give them a well structured google/airtable form which includes all possible fields for information \nAnd tell them to fill this instead', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Runedu.co,https://www.reddit.com/r/automation/comments/zth57w/runeduco/,automation,"Would you like to run your online language course fully digitally?

Digitize your education company:

 1. Minimize expenses  
2. Additional Income with AI  
3. Make decisions with KPI's  
4. Digital presence to boost sales","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'What a low-effort advertisement.']"
Automation Testing Market Witness the Growth of $52.7 billion by 2027,https://www.reddit.com/r/automation/comments/zt9ooj/automation_testing_market_witness_the_growth_of/,automation,"Report determine and forecast the automation testing market based on component, testing type, dynamic testing, non-functional testing, endpoint testing, organization size, vertical, service and region from 2016 to 2027, and analyze various macro and microeconomic factors that affect the market growth.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
[BLOG] Use PowerApps for an automation Front-End,https://www.reddit.com/r/automation/comments/zspzt0/blog_use_powerapps_for_an_automation_frontend/,automation,"To celebrate the [festive tech calendar](https://festivetechcalendar.com/) I've written a blog about how to use PowerApps to create a simple front end to execute Automation scripts hosted in Azure. I thought some people might be interested in it so here you can find it:

[https://autosysops.com/blog/use-powerapps-to-create-a-simple-front-end-for-automation](https://autosysops.com/blog/use-powerapps-to-create-a-simple-front-end-for-automation)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
trying to connect to the database using python unittest,https://www.reddit.com/r/automation/comments/zrh35d/trying_to_connect_to_the_database_using_python/,automation,"Need to create a testing framework using python unittest and try to connect to the database. I am using the below code for connection which returns the connection. 

&#x200B;

`def connection(self):`

`connection = mysql.connector.connect(host='`[`127.0.0.1`](https://127.0.0.1)`',` 

`database='testingDatabase',`

`user='Admin',` 

`password='admin123',`

`auth_plugin='mysql_native_password')`

`return connection`","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Text expansion app for Android - allows SQL lite files,https://www.reddit.com/r/automation/comments/zrdx60/text_expansion_app_for_android_allows_sql_lite/,automation,"Hi everyone!

I wasn't quite sure what subreddit to ask about this in so please let me know if I'm not in the right place. :)

I'm currently using a piece of freeware for text expansion and simpler macros on my PC that I'm really happy with. However, I'm unable to find an app for my Android phone that's compatible with it. The software doesn't have an official mobile app so I'm looking for any app for text automation that I can use to expand text (duh) while typing on my phone.

However, I'd like to be able to export the SQL lite database from my PC and import it into the Android app so that I have all the snippets ready to go. (I'm not expecting the macros to work or any of the shortcuts that include keyboard-specific keys like Ctrl or Alt).

I realize this won't be an automatic synch, much less a two-way synch, but it's good enough for now. :)

Looking forward to your advice Reddit community! :D I'm sure you'll provide words of wisdom as always!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
automated DJ software,https://www.reddit.com/r/automation/comments/zq7b83/automated_dj_software/,automation,"I made software using JavaScript that has about 300 songs and the software mixes the tempo matched instrumentals in key, each refresh creates a new endless mix and its powered by the super accurate Web Audio API hardware clock, even works on iOS.

The dopest part is that it is super flawless accurate as far as key matching and tempo matching goes and its all written in vanilla JavaScript. It even does some things DJs cant currently do, like considering the accurate tempo/pitch shifted key (like when a DJ speeds up a song) instead of the one the song is recorded at. As much fun as DJing has been to me for over 20 years, this is even more fun because it comes up with combinations I love and have never used before but they sound great together.

EDIT: [https://cappinkirk.com](https://cappinkirk.com) to listen live and [https://github.com/dmvjs/kwyjibo](https://github.com/dmvjs/kwyjibo) for the source code (doesnt include audio files yet for obvious reasons). This project is originally a hardware idea for selling to clubs but it amazingly it also works well on the web. I make mixtapes with it and export to sounddcloud also

EDIT #2: Happy Cake Day r/DJs!

EDIT #3: i posted a youtube video unedited with details [https://www.youtube.com/watch?v=61mAf\_8swEE](https://www.youtube.com/watch?v=61mAf_8swEE)

EDIT #4:

&#x200B;

[Songs are grouped by tempo and key](https://preview.redd.it/sqyvfp7tjt2a1.png?width=1367&format=png&auto=webp&v=enabled&s=ead23a47dedea9b41b464ce0ab67b4de7564f70f)

&#x200B;

[Adjacent keys are eligible for selection](https://preview.redd.it/uy79eudxjt2a1.png?width=1367&format=png&auto=webp&v=enabled&s=1011dc28b07ec99a7c1c7b089c41de1e4db3d34d)

&#x200B;

[lead files are 16 beats, body files are 64 beats](https://preview.redd.it/wl1qhv31kt2a1.png?width=1366&format=png&auto=webp&v=enabled&s=64afedab344cfe212d4a7384b804a4a86201fbb4)

EDIT 5: kwyjibo got featured in [https://bytes.dev](https://bytes.dev) JavaScript email thanks! [https://bytes.dev/archives/145](https://bytes.dev/archives/145)","['Very cool! What an awesome idea\n\nCan we supply it with any tracks? Or do we also need to provide key/tempo info as well? \n\nHow does it handle when a song changes key? Can it correctly mix into another song after a key change?', 'Damn it looks crazy ....', 'Kudos human, this is incredibly impressive.', 'Damn, my next party is sorted', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Find Retailer Map on Website,https://www.reddit.com/r/automation/comments/zq06ou/find_retailer_map_on_website/,automation,"Is there a way to automate adding in new retailers onto a retailer map on a website? We get form submissions from new retailer accounts, (we have many retailers), and manually entering them into the website takes 2-5 mins each, and we have 267 to input. Could Zapier or something similar help with this? Thank you","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Automate tape application in furniture manufacturing,https://www.linkedin.com/posts/enimac_enimac-tapeapplicationmadeeasy-tape-activity-7010506843839971328-rzeC?utm_source=share&utm_medium=member_android,automation,,
What power connector is used on that Mitsubishi HC-KFS servomotor?,https://www.reddit.com/gallery/zppczq,automation,,"[""It's impossible to say without seeing the wire unplugged and the pin configuration.  It could be something made specifically by/for Mitsubishi."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Does anyone know what is this 4 pin power connector on Mitsubishi HC-KFS servomotor? Exact model is HC-KFS43 but the same connector is used on many other Mitsubishi servomotors of that era.\n\nOne side of connector is marked MXJ 2. Opposite side is marked 54271. \n\nMitsubishi manual is listing connector and matching receptlaces for all cables attached, but not the one connected directly to motor body (or at least I could not find it). Googling ""MXJ 2 54271"" does not retun anything useful either. Is that some proprietary Mitsubishi plug that you cannot buy, or am I missing something?']"
Is this AB manual wrong?,https://www.reddit.com/r/automation/comments/zo9toh/is_this_ab_manual_wrong/,automation,"My understanding is that a sourcing input is pnp, so it would be expecting an active low signal. Is this manual backwards? https://i.imgur.com/vSwUBlQ.png","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Trying to figure out automatic switching for 220v machines.,https://www.reddit.com/r/automation/comments/zo8x9o/trying_to_figure_out_automatic_switching_for_220v/,automation,So I've bought myself a new wood lathe which is 220v. My air compressor is also 220v. The problem is that I have only one outlet. I've been trying to figure out how I can automatically prevent the compressor from starting while the lathe is running. Any ideas on how I could accomplish this?,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""So you have a compressor which is left on all the time, starts itself when it needs to build pressure... and you want to ensure that it never starts when the lathe is running?\n\nWith modifying your lathe: You can wire a contactor to the start relay in the lathe and then run the compressor power through the N/C circuit on the contactor. When the lathe isn't running, the contactor will pass power as normal and the compressor can run whenever. When the lathe is running, the contactor will close which will open the power circuit for the compressor thus preventing it running. When you stop the lathe, the contactor will open again closing the compressor power.\n\nWithout modifying your lathe: I would simply build a small enclosure with a switch for lathe power. Use a DPDT (On-Off-On) switch with 220v rating, and wire the plug (to your outlet) into the common on the switch, with the lathe wired to one side of the switch and the compressor to the other. Then just switch between the two as required."", 'Could you not add a larger breaker and then wire in another receptacle in parallel? I mean depends on your total usage and main breaker size. Also depends on the amperage of each of the machines, but why limit run time when both could potentially run?']"
skills for Electromech eng technology,/r/PLC/comments/zlvl64/skills_for_electromech_eng_technology/,automation,,
Are color sensors applicable for checking fruit‚Äôs freshness as a non-contact sensor?,https://www.reddit.com/r/automation/comments/zll1nr/are_color_sensors_applicable_for_checking_fruits/,automation,"So it‚Äôs my very first days with the job, I knew there‚Äôs a problem where fruits are wasted if most of them are moldy. 1/4 of the whole shipment could be wasted if the boxes have moldy ones as they wouldn‚Äôt like it to pass by the conveyer belt and processed with the fresh ones. 
I really would appreciate if I got resources, project‚Äôs vids or documents to check the whole thing, thank you.","[""I don't think that you could do it reliably with just color, if you want vision system you would need to find company that will create software that will spot mould on fruits. Maybe fruits could be rinsed, box by box and then check water after rinsing for mould presence."", 'You need a grading machine with a vision system for detecting external defects. \n\nCheck out equipment from Greefa, MAF, Compaq, or Aweta for more info', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Walmart rolled out self-checkout to streamline operations and reduce labor ‚Äì but employees and customers say it's causing a surge in thefts,https://www.businessinsider.com/walmart-employees-and-customers-blame-self-checkout-shoplifting-rising-theft-2022-12,automation,,
Electrical/Control & Instrumentation Engineer to Automation Engineer,https://www.reddit.com/r/automation/comments/zl4w3x/electricalcontrol_instrumentation_engineer_to/,automation,"I'm an electrical engineer with a couple of years experience behind me. Previously I was an electrician before pursuing my degrees to become an engineer. Based in Ireland. 

I've always loved automation, I excelled at it in college, plc programming, logic, robotics, but I ended up going down the path of an electrical design engineer mostly since leaving college in 2018, most recently I'm also a control & Instrumentation Engineer. I've been working in proximity to automation engineers throughout some projects and really like the idea of doing the role everyday.

What I'm struggling with is how or can I break into an automation engineer role? I've looked at 100s of roles for automation and I don't see any point in applying as I don't meet the criteria but I know I'd love doing all of the responsibilities! What can I do? I can't think of anything other than completing some PLC dojo courses etc but that still doesn't give me the experience required.

Edit: going back to college full time is out of the question, I've already done this to acquire two degrees. I would be willing to do a night time course but there are none I can see, there are some in mechatronics but I'm not sure I'm willing to offload ‚Ç¨10k to get a 2nd level 8.","['Believe I have answered this before here but, the last 3 people I have hired for the automation engineer position have a background similar to what you described. Hiring people who know the hardware side already have proven to be very good at troubleshooting even before they are solid at the programming aspect.', ""My company mostly hires Electrical Engineers for our controls and automation work. We have hired into the team someone that started out as an instrumentation & electrical technicians that have demonstrated an ability and willingness to learn the programming side. However, engineers typically have a quicker path to this. I've seen techs that become programmers and are better than the engineers, but I've seen it the other way around to. It all depends on if you really want to learn it. We would train PLC programming and HMI development. That's not something that people can typically get training for in school themselves in most cases. Our engineers, however, might work well beyond just basic programming and HMI development, and more into complete systems integration, including with network and IT systems. Again, it's all in your willingness to self-actualize, learn, and apply it to be successful."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
How do i get an automation to go down to the next customer in my invoice register,https://www.reddit.com/r/automation/comments/zkcmfh/how_do_i_get_an_automation_to_go_down_to_the_next/,automation,Hi.  I was tasked to automate some processes at work and was looking for some help.  I have been asked to automate our billing invoices to automatically be assigned in our billing invoice master file.  I have been able to get the automation to work for one entire customer but cant get it to jump to the next invoice.   I was told i need to use some kind of loop but don't really know what kind to use.  Thanks,"['Depending on the language are you looking for a for loop?', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'in Python it would be something like this\n\n```Python\n# Import the necessary library\nimport pandas as pd\n\n# Load the data into a pandas DataFrame\ndf = pd.read_csv(""customers.csv"")\n\n# Iterate through the customers column\nfor customer in df[""customers""]:\n  # Run the billing process for the current customer\n  bill = calculate_bill(customer)\n  send_bill(customer, bill)\n```']"
Starting School,https://www.reddit.com/r/automation/comments/zkarwx/starting_school/,automation,"My company is putting me through a technical program at the local community college. For the entire duration of the program I will be using SQL and statistics in online college classes. February through March I'll be going through in-person classes on FANUC robotics and PLCs, as well as some GD&T/Schematic reading stuff at the applied technologies center.
  Not sure if anyone here is in manufacturing automation but I'm a little intimidated since my only technical work so far has been using SQL and Excel for some basic data work. 
  Any tips on the learning process? Or does this program even sound like it's covering enough information?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Doesn‚Äôt sound like it‚Äôs enough at all. You need electrical, mechanical, pneumatic, fluid power, PLC, And more PLC.']"
Requesting advice to begin automation learning,https://www.reddit.com/r/automation/comments/zjqwhw/requesting_advice_to_begin_automation_learning/,automation,"Looking for a learning path and resources where I can learn automation.
A suggestion on route map of this learning path by self

Thanks and appreciation in advance for your guidance.","['there are Google Coursera Python for IT automation courses, you can even get financial aid to get them free as well', 'What sort of automation? Software or mechanical?', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Custom Inventory Management system with automated low inventory alert,https://www.reddit.com/r/automation/comments/zgxi2k/custom_inventory_management_system_with_automated/,automation,"I love building systems with no-code tools. Today, I built a custom Inventory Management system with just no-code tools. The system includes a low inventory alert. Here's a video demonstrating the same, [https://youtu.be/XoAo666kiHw](https://youtu.be/XoAo666kiHw). The tools used were Utilize.app, Make, Google Sheets, and Gmail.

‚öíÔ∏è Utilize is used to build an app to add and withdraw stock. Once the stock is withdrawn, Utilize triggers Make with the SKU information.

‚öíÔ∏è Make is used to trigger Google Sheets and Gmail.

‚öíÔ∏è Gmail sends an alert to the admin.

&#x200B;

I think many small businesses could benefit from a simple inventory management app. Looking forward to hearing thoughts from the community.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Basically what autocrib,cribmaster offers but without the ""vending machine"" You would also need to add ERP  compatibility  and transacrion history logs. Also user profiles for admins and regular users']"
Forward emails and more with a click of the mouse.,https://www.reddit.com/r/automation/comments/zh6p21/forward_emails_and_more_with_a_click_of_the_mouse/,automation,"At work a few people have to do this repetitive function:

* Some emails, but not all
* Need to be forward to a certain email address, always the same email address
* And also BCC to another  email address, always the same email address. BCC is important
* Add a certain message in the body of the email, first line is fine, always the same message
* we use Google Workspace


I am new to automation, but this sounds like a great first project.

What would be best to use? Google Apps Script, Zapier or some other tool?

Thanks in Advance.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', '>Some emails, but not all\r  \n\r\n\nThere can be a number of platforms/ways to do this, but the key to getting the right answer is here.\n\nWhat are the conditions that let you identify which emails you need to forward? \n\nI.e. Do these emails from a particular sender? Do they contain certain keywords? Any other marker to help the desired automation identify those emails?']"
Mitsubishi FX5U connection with Monitouch TS1070s,https://www.reddit.com/r/automation/comments/zd1olw/mitsubishi_fx5u_connection_with_monitouch_ts1070s/,automation,"Hi everyone, I am having a hard time trying to get a connection between this two devices. I am already lost on what went wrong. Please help me\~\~\~I've attached both my devices setting pics. My HMI keeps prompting PLC1 Communication Error Time-Out.

[V8](https://preview.redd.it/jxddi3sym14a1.jpg?width=544&format=pjpg&auto=webp&v=enabled&s=c815d6e53fcb957507876920e636331c0bcb4c12)

[GXworks3](https://preview.redd.it/xsrpvvtxm14a1.jpg?width=1517&format=pjpg&auto=webp&v=enabled&s=639122250ebfb1be722a5380ee09dbd19b96f940)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
EIT automation online school opinions.,https://www.reddit.com/r/automation/comments/zcg63l/eit_automation_online_school_opinions/,automation,"Hello!

I'd like to further my education. I'm a mechatronic engineer, but worked mostly as a maintenance technician, in the mechanical side. I'd want to pursue a career in automation, but the only chances I have are online schools. I live and work in a little city in Bolivia, in latin america. 

Here is a link to some of EIT's programs.

[https://www.eit.edu.au/?post\_type=courses&s=&course-types%5B%5D=professional-certificate](https://www.eit.edu.au/?post_type=courses&s=&course-types%5B%5D=professional-certificate)

If anyone know about this EIT, and their programs, would you please give me your opinon. If anyone knows about another online program which grants some type of certificate in conjunctions with the learning, please I'd like to know. I know there is a huge gap between using a simulator than a real PLC, but I have a PLC and some stuff that would let me install a tiny system. Automation is huge, but Im more intereted in system dynamics and control (PID controllers, PLC/DCS, plant design and mostly industrial automation).

Thanks and cheers.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
How to automate a specific song to wake me up the 1st of every month?,https://www.reddit.com/r/automation/comments/zaj50h/how_to_automate_a_specific_song_to_wake_me_up_the/,automation,"Siri or Alexa, what automation would allow me to wake up to a specific song at 8:00am on the first of every month?","[""I'TS THE FIRST OF THA MONTH!"", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Apple shortcuts. Select new automation and you can set this up for the 1st of every month at 08:00 and select the song']"
How to Automate WhatsApp Message for free using Power Automate,https://www.reddit.com/r/automation/comments/zad6pd/how_to_automate_whatsapp_message_for_free_using/,automation,[https://medium.com/@vanshkharidia7/how-to-automate-bulk-whatsapp-messaging-without-coding-30fe97f9832c](https://medium.com/@vanshkharidia7/how-to-automate-bulk-whatsapp-messaging-without-coding-30fe97f9832c),"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Nice! Never knew you could do this!']"
ABIR X8 + TUYA app,https://www.reddit.com/r/automation/comments/zal6vp/abir_x8_tuya_app/,automation,I need help with adding the ABIR X8 robot vacuum to the tuya smart app. Has anyone been able to do so?,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
"""Driveway durable"" LED strips",https://www.reddit.com/r/automation/comments/zaclg8/driveway_durable_led_strips/,automation,"My problem is that my 2019 Chevy volt's backup camera has weak nightvision and I routinely get home after midnight. I can't see to back into the driveway, so I installed the $3 solar lights which are already underperforming. I would love to have simple strips that could be mounted on the driveway but they need to be able to endure being run over. As of right now I speculate I'll go with low voltage posts but the ability to maneuver the cars over the edge of the driveway would be a big plus, our parking is complicated. Thanks!","[""Use your mirrors and don't rely on the tech?\n\nHow did you reverse before cameras?"", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Sounds like just a simple sensor light would do the trick? You'd be best to mount it high up with the sensor pointed mostly down, so when it's dark and you get close, your target is well-lit and now you can just park normally?\n\nThey are really common around here, typically one thumb dial to adjust how dark it needs to be before motion will start triggering it, and one dial to adjust how long the light stays on once triggered. Sensor and light can be angled independently.\n\nAlso, you could have an indoor switch to either force it off or force it on depending on the unit/your needs e.g. if you don't want pets or whatever triggering it. Or you could even get a remote one and eschew the sensor completely.""]"
I am trying to publish data from MangoAutomation with an http post request (shown in the first image) and receive this data on nodered by using the http request node to make a request to Mango. Unfortunately I get an empty response. Can anyone figure what I am doing wrong?,https://www.reddit.com/gallery/z9sque,automation,,[]
Need tips,https://www.reddit.com/r/automation/comments/z7wbk9/need_tips/,automation,Hey guys! What automation tips would you give to a newbie?,"['Study up and have a good understanding of:\n-How to use multi meter, check voltage/current continuity.\n-AC/DC circuits.\n-Differences between NO/NC (normally opened, normally closed.\n- how a relay works\n-Binary.\n-16, 32 and 64 bit registers\n-Differences between register formats signed, unsigned, floating points etc.\n\nLittle more advanced:\nModbus, canbus communication setups and topologies.\nLAN (local area Network) structures. Ipv4, macs, subnets, gateways. \nVFD/vsd programming and best practices.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I suggest trying out some basic automation tools before jumping into the complex stuff. That can help you get a foundational understanding of how it works, the uses of it, etc. Personally, I think that text expanders are solid tools to mess around with at first, because they are useful and they can help you understand the code that goes into automation without getting too deep into it. I recommend Text Blaze if you want to use one that's user-friendly. Ultimately, it's up to you. Just try not to get overwhelmed with all of the complicated stuff right off the bat, you got this!"", 'Dose not say munch for your automation company if you have to ask that here....']"
Hey guys i'm very new to this topic!,https://www.reddit.com/r/automation/comments/z7c03t/hey_guys_im_very_new_to_this_topic/,automation,Hey guys i'm very new to this topic can i ask someone a few questions about automation? Thanks,"['You just used up your first question. You get 2 more.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'This is an automated response, yes you can']"
Automation Testing Market is expected to reach USD 49.9 billion by 2026,https://www.reddit.com/r/automation/comments/z6stlh/automation_testing_market_is_expected_to_reach/,automation,"To determine and forecast the global market based on components, testing types, dynamic testing, services, endpoint interfaces, organization size, verticals, and regions with respect to individual growth trends and contributions toward the overall market.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
What should people who lose their jobs due to automation do?,/r/IdeologyPolls/comments/z4mkaj/what_should_people_who_lose_their_jobs_due_to/,automation,,
VFD AutoRestart,https://www.reddit.com/r/automation/comments/z4btk9/vfd_autorestart/,automation,"Good day all!
 
I am wondering if it is common practice in industry to use the auto restart parameter on VFDs?

TIA","['Depends on how it‚Äôs used. I work in manufacturing so no you really ought to have a motor inspector/ electrician come to  line and check everything before they reset it. If you leave it up to the operators they‚Äôll trip the OLs every 15 minutes for a 12 hour shift and never give a shit. But i often use auto restart on power loss just not OL.', 'Depends on what you are doing with it', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Ask this in r/PLC \\- you'll get more answers."", 'It depends on the process, and which faults you want to use the auto reset/restart on. I usually enable the over/undervoltage ones, and the mA input ones, as I mostly work in water and wastewater, and we want the pumps to come back after a transfer between power sources in emergencies.', ""Auto restart to stop and start based on external logic is common. Keep in mind some motors have a recommended limit on start/stop cycles, min/max speed limits. Resetting a faulted vfd is not a good idea. It is usually recommended to latch these types of shutdowns and have a tech and/or electrician look over drive or motor being controlled. Vfds usually trip for a good reason. If you're having issues, First easy check, I'd recommend checking motor plate settings.""]"
Lutron caseta and Logitech circleview,https://www.reddit.com/r/automation/comments/z3znu7/lutron_caseta_and_logitech_circleview/,automation,Has anyone tried these two in conjunction? I‚Äôm hoping for when my doorbell senses motion or is pressed my front entry lights will turn on? Can you all let me know if this is possible I‚Äôm using Apple HomeKit as a hub,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
With what kind of probelms can neural net help in signal processing automation?,https://www.reddit.com/r/automation/comments/z3bg1e/with_what_kind_of_probelms_can_neural_net_help_in/,automation,"Hi,  
This might be a silly question so I apologize in advance.

I was trying to search on google for what kind of problems ML or DL can be useful regarding signal processing or automation but I found nothing.  
Also the problems in these topics which I have stumbled onto during classes on my uni seem not to complicate, or it seems that using such algorithm on these problems would be like reinventing the wheel.  


Do you guys have any ideas in what fields in Automation/Signal processing such algorithms might be useful?  
(I don't have a million bucks robot to teach it to walk so such farfetched ideas that are impossible for a student to conduct a study on are out)  


Thank you for your help in advance, because I couldnt thought of any topic on which I can conduct a study on.  
If you will, just give me a hint where to look, or a loose idea where such things might be useful and somewhat doable.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Working on aquaculture plc. Is a Brainbox good?,https://www.reddit.com/r/automation/comments/z3678q/working_on_aquaculture_plc_is_a_brainbox_good/,automation,"A small PLC system to control a few motors, air pumps, and lighting based on a few variables. Do you have any suggestions as to which PLC system I should employ?

I want to use Wi-Fi and have a nice interface for my computer.

Thank you in advance!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""AutomationDirect has good low cost PLCs. For something as simple as your project, you could use a Click. As for a nice interface to your computer, a PLC generally isn't going to do that without extra software. Usually a PLC interfaces with an HMI. You can see the values of the PLC tags on the programming software, but it's not a nice way of viewing it. AutomationDirect also sells Aruino based PLCs. You might be able to find some open source software that will display the values over a GUI. You could also use a SCADA software like Ignition, but that's probably overkill for your application. Is this for yourself or for a business?"", 'The brand LaCroix sofrel has many PLCs  dedicated for water process, i recommend to check them out.', 'Check out weidmuller, it runs node red aswell. This gives you a flexible User interface for control', 'Never heard of it. Is it specific for that usage?']"
Best Dashboard Solutions for KPIs???,https://www.reddit.com/r/automation/comments/z3034c/best_dashboard_solutions_for_kpis/,automation,"Hello Everyone, 

I'm running a B2B agency and we're currently going through rounds of automation. One of the things that I'd like to implement is a dashboard that contains the following: 

\+ Financial data (MRR, growth, etc.) - coming from Quickbooks

\+ Customer success data (# of new customers, total number of clients, retention rate, etc.) - coming from Hubspot and Google Sheets. 

I was wondering if there's a service that allows to integrate all these data sources and beautifully visualize them as live dashboard that can be accessed through the phone, laptop, and maybe also displayed onto a TV. And it'd allow for user management (certain people within the company can see certain information). 

Thanks in advance","['The first two that come to mind are Power BI and Grafana.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Download and Upload Automation - Web based,https://www.reddit.com/r/automation/comments/z2h59j/download_and_upload_automation_web_based/,automation,"I am looking for a tool that would help me download and upload assets / files from one web based application and upload on another. 

The flow would go like:

* Download N number of files from a web based application, A
* Save them to a local drive, let's say D:/
* Upload these assets to another web based application, B

Is there an out of the box tool that would help me achieve this? Thanks a ton!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Uipath community edition', 'What about FTP?']"
windows GUI automation tool/framework,https://www.reddit.com/r/automation/comments/z0l328/windows_gui_automation_toolframework/,automation,"Hey all!

I am looking for a framework or tool, which can automate GUI software on Windows. A client of mine uses an application which poorly does not have any API. The only way of automating would be inserting directly to the DB. The vendor of the software locks th DB however and acces to it is just possible after a paid consultancy by the vendor.

So I am looking for a framework for automating GUI actions. It is just about inserting data into a basic form. Which tools should I look for?","['Check out [AutoIT](https://www.autoitscript.com/site/) that was it is specifically designed for.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Automation for fast-growing B2B Service Agency,https://www.reddit.com/r/automation/comments/z0etl2/automation_for_fastgrowing_b2b_service_agency/,automation,"Hello Everyone, 

I'm the CEO of a fast-growing B2B service agency focused on cold email lead generation. Our internal core products include Hubspot CRM, Front, Notion, Slack, Google Drive, and Gsuite. I was wondering if anyone has ideas of how to best use automations (Zapier, App Sheets, etc.) to streamline the data exchange between these different products. We're open to coding our own integrations as well. 

Thanks and I'm looking forward to your answers.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I'd recommend looking at tools they've built out for integration options first before going into third party like zapier"", 'Let me know if you want to circle back some ideas you may have. I‚Äôm happy to help !', 'If you‚Äôd want a constellation with my business, I‚Äôd be happy to help. Dm if you‚Äôre interested']"
Browser Automation Scheduling for Average Joe?,https://www.reddit.com/r/automation/comments/z091lj/browser_automation_scheduling_for_average_joe/,automation,"I am looking for a free or cheap software for Mac (chrome or safari) that would allow me to set up automated waiver wire pickups for fantasy football, right after waivers are run (like 3:00am lol). I know programs like Selenium exist, but I don‚Äôt know how to code, so I can‚Äôt create my own program.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Assuming your intended workflow is simple, Apple‚Äôs Automator app is built in to MacOS and is relatively easy to use. You can create automated workflows and then schedule them.']"
Discover 2 years' history of receipts&invoices in your Gmail account(s) with one click,https://www.reddit.com/r/automation/comments/z04zcv/discover_2_years_history_of_receiptsinvoices_in/,automation,"We developed a great automation service that finds all invoices and receipts in your Gmail and Outlook accounts up to 2 years back. The service is now offered at AppSumo for a $69-lifetime deal. Take a look + tell us what you think.

[https://appsumo.com/products/wellybox](https://appsumo.com/products/wellybox?utm_source=partner-link&utm_medium=referral&utm_campaign=partner-166422)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', '$70 to use a key word search ...come on.....']"
What should this cost,https://www.reddit.com/gallery/yy4fyt,automation,,"['Somewhere between $50 and $5 million', ""If your company was to build this panel what would it roughly cost. I don't think management knows how complicated a build like this is. For reference there's 8 vfds that are used to control 9 motors in this build."", 'Best estimate is a bit broad brush as there isn‚Äôt much detail to go on, but I would budget between ¬£15,000 and ¬£25,000 in the UK. It really should be in an enclosure 3x the size it is in. There would be additional costs for engineering beyond a simple reverse engineer copy and redraw especially if you include site visits. \nInstall and testing of same would depend on site conditions. \nI‚Äôd budget between ¬£800 and ¬£1,100 a day including traveling time and + expenses for a commisioning engineer (UK). I‚Äôd expect 2-3days on site electrical commisioning as long as everything is ready to go in advance. \nAllow 6 months to source drives in the current market after design approval and a month to build once they are available. \nHope that helps.', 'Materials wise, I‚Äôd estimate between $15k-$40k depending on brands and distributor relationships. \n\nLabor is a metric that varies from company to company and is impossible to guess unless you know a company‚Äôs rate, but a skilled panel builder could build this in between 60-80 hours.', 'About tree fiddy', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""There are way too many factors to give an adequate estimate, but if I had to guess, I'd say at least $25k depending on how good your sourcing team is or relationships with suppliers.\n\nYou gotta love when management says CaN't YoU dO iT cHeApEr?!"", 'This is way too small enclosure.', ""$40-50k especially with today's prices. I also agree, cramped panel, pain to wire.""]"
Salary Question,https://www.reddit.com/r/automation/comments/ywxqij/salary_question/,automation,"I‚Äôm an electrical engineering graduate working as a controls engineer that does start up commissions and software development. I‚Äôve been doing this for about 1.5 years now. I‚Äôve done additional things for my company beyond my base responsibilities such as building a wiki that‚Äôs quickly becoming a go-to source for information sharing, and expanding the tools we have for our project development/management suite.

With annual reviews coming up I‚Äôm wondering what is a fair salary increase. I live in WA on the western side of the Cascades so cost of living is up there if that makes a difference. 

Anyways‚Ä¶  advice/comments/concerns?","[""Since each state and company can vary, my suggestion would be to look at local job postings that are similar to help gauge your expectations. Most companies have a flat percentage they boost employees' pay by and then occasionally a cost of living boost. If you can at least show a competitive salary it may help get what you'd expect."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Have you checked Glassdoor/Indeed?']"
Forbes: How AI can improve job quality,/r/ArtificialInteligence/comments/ywsjee/forbes_how_ai_can_improve_job_quality/,automation,,
Automating Weq4u.. via Zapier or scripts or something similar?,https://www.reddit.com/r/automation/comments/ywvzbc/automating_weq4u_via_zapier_or_scripts_or/,automation," 

We are based in the UK and use a horizon phone system.

One of our departments have to often queue for hours to talk to someone in another call centre they need, it wastes a lot of time, and they are tied up when clients need them, they dont want to put the queued call on hold to talk o a client in fear of missing when its answered.

Weq4u is an excellent free service which takes the sting out of this but with a couple of caveats.

It routes your call queue request based on caller Id so you can't have more than 1. Plus all of our agents have the outbound CID set to our main office number, so that's 1 for the whole company and the return path for that number is our reception. So it can't work from this CID

When you are at the front of the queue it gives you a quick missed call for you to redial from the relevant CID to connect, it doesn't just join in you when you answer the call.

My thoughts were that we could have separate VoIP lines (with any UK provider) for the agents just for managing weq4u, so our agent makes the call out with their unique CID, they join the outbound queue and presses 9\* to end the call, so weq4u does its magic and waits for an agent.  
Then weq4u rings back to that VoIP line (2 rings missed call) to tell it that the agent is ready on the outbound call. We have an automation somehow which immediately rejects the weq4u call based on its CID, then redials it (which you have to do), dials a hunt group (external inbound) number on our horizon system and transfers the call onto that hunt group (or conferences it if that's easier).

I could name the horizon hunt groups based on which agents queue will be calling so that everyone in that department can see that Agent X's outbound call queue is ready to talk, if they are available they will grab it, if they are away or on the phone someone else will grab it and do what's needed so it's not missed.

It's only a small department, currently only 4 but looking to expand to 8 agents, they all sit together and can easily see whos busy and whos not.

It should work in theory, but I've been trawling VoIP systems and automation systems and I can't find an obvious way to make this happen.

The system would need to have unlimited minutes, include the 0333 number in the minutes, and be able to present UK CID's

I would appreciate any pointers or ideas I'm a sys admin (and I do a bit of easy dev), I'm not up on all the VoIP solutions out there.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
"Automation COE Market Growth, Opportunities Business Scenario, Share, Growth Size, Scope, Key Segments and Forecast to 2027",https://www.reddit.com/r/automation/comments/ywln5t/automation_coe_market_growth_opportunities/,automation,"To provide detailed information related to major factors (drivers, restraints, opportunities, and industry-specific challenges) influencing the market growth.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Automate content distribution to Reddit using RPA,https://www.reddit.com/r/automation/comments/yvv208/automate_content_distribution_to_reddit_using_rpa/,automation,"Hi all, I've built a flow to automate content distribution to Reddit using RPA (RoboMotion)  


Here's the vid: [https://youtu.be/CKL\_f7f-q6o](https://youtu.be/CKL_f7f-q6o)  


By the way - I have nothing to sell. Just sharing my knowledge. 

&#x200B;

https://preview.redd.it/vzq3ipx9t30a1.jpg?width=890&format=pjpg&auto=webp&v=enabled&s=78ed284c5ab666e09a0ba6e9064e71736c7cd133","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
I need to click that link first!,https://www.reddit.com/r/automation/comments/yuvelx/i_need_to_click_that_link_first/,automation,"I am someone desperately looking for a house. This realtor sends you emails, from the same sender, that host a link that you click on for a viewing of the apartment. The first \~25 people get in. I want to set it up so that when I receive an email from that sender, it automatically clicks the 2nd link or that specified link with the same button text. How would someone with no, or minimal, coding experience do this?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Outlook on a computer with the email connected. Set up a trigger to run a file on the PC and maybe to also save the email. Run a powershell script that parses the email and executed the link in your default web browser. \n\nMaybe easier ways to do it but idk.']"
"Automation COE Market Growth, Opportunities Business Scenario, Share, Growth Size, Scope, Key Segments and Forecast to 2027",https://www.reddit.com/r/automation/comments/ys9jjo/automation_coe_market_growth_opportunities/,automation," To provide detailed information related to major factors (drivers, restraints, opportunities, and industry-specific challenges) influencing the market growth","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Join us in our E-commerce Media Automation Hackathon,/r/Automate/comments/yrwhnn/join_us_in_our_ecommerce_media_automation/,automation,,
Is there a system used in companies/business that tracks status of projects like how we track a food order/clothes online?,https://www.reddit.com/r/automation/comments/yqfesj/is_there_a_system_used_in_companiesbusiness_that/,automation,"For example jobs that are running that need process flow‚Ä¶ any names/suggestions? 
Thanks ü•∞

Explanation: 

The company I am at wants me to reduce COPQ (cost of poor quality) and improve the Company as a whole. 
The company is a service provider not manufacturing company - so it's the improving is mostly focused on providing the service itself which is on site and the steps before providing it. (work flow between employees)

For example: The company barely has SOPs, those will be written. 
Process flow maps for processes should be designed .
I also think a software where you track the process flow of each procedure such as from receiving RFQ -> doing commercial and technical offer --> submission --> Letter of PO --> Site visit, that can be documented like how we track food online after an order and know at which step we're at  till the execution of the job. 

However, I have no idea what else i should be doing? Six sigma or lean or what exactly? I'm a bit lost from what can work best in the company's case, any advice?","['Jira, Meistertask to a lesser extent', ""Yes, but it's fairly industry specific. CRM's and Issue Trackers are probably as close to universal as it gets."", 'try monday.com', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Explanation: \n\nThe company I am at wants me to reduce COPQ (cost of poor quality) and improve the Company as a whole. \nThe company is a service provider not manufacturing company - so it's the improving is mostly focused on providing the service itself which is on site and the steps before providing it. (work flow between employees)\n\nFor example: The company barely has SOPs, those will be written. \nProcess flow maps for processes should be designed .\nI also think a software where you track the process flow of each procedure such as from receiving RFQ -> doing commercial and technical offer --> submission --> Letter of PO --> Site visit, that can be documented like how we track food online after an order and know at which step we're at  till the execution of the job. \n\nHowever, I have no idea what else i should be doing? Six sigma or lean or what exactly? I'm a bit lost from what can work best in the company's case, any advice?"", 'Guys what do you think about power automate by office 365? Any ideas? I need something that‚Äôs doesn‚Äôt need a long implementation, like between 1-3 months, and not that costly! \nThank you!']"
Automated testing basics,https://www.reddit.com/r/automation/comments/yqjg89/automated_testing_basics/,automation,"Running a large number of tests automatically on the developer's computer, server, or in the cloud to make sure everything functions as intended.

The testing phase of the software development lifecycle is crucial. By handling tedious and repetitive duties like regression tests, automation testing frees up testers to focus on other high-quality activities like conducting exploratory tests and analyzing test findings, among other things. As a result, automation testing makes it possible for you to complete more tests faster.

Manual testing may become challenging as there are more features to test or more software faults to find. Automated testing can help with this.

To keep the software development process productive and effective, automation testing investment is crucial. You may run tests in any environment you require, as frequently as you like, and with automation. As a result, security and stability are improved, time and effort are saved, and maintenance costs are decreased.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Complete Reporting Automation,https://www.reddit.com/r/automation/comments/yq1aom/complete_reporting_automation/,automation,"How do you have your reports automated completely? Share the steps/tools and tech stack used. I am looking for a solution to automatically send report to a list of people or users who are interested in receiving it. 

How did you build the report (etl, transformatin). How are you generating the report and sending to users, in which frequency and how?

Any help, ideas, processes tools and tech will be appreciated.

The idea is to design the report once and make it work every week/days etc and send automated everything. Zero human in tervention after that.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'I don‚Äôt think this is exactly what you‚Äôre after, but I don‚Äôt build ‚Äòreports‚Äô I build ‚Äòdashboards‚Äô. These don‚Äôt need to be sent to anyone. At any one time I can go to a link and view the dashboard I‚Äôm interested in which contains the information I‚Äôm after.\n\nThe main approach is to have all data from all sensors be captured in some sort of timeseries database like influxdb or timescaledb.\n\nThen you access that database via a data visualisation platform like grafana or superset. Then through that tool you setup the queries and charts you‚Äôre interested in. It‚Äôs typically best to build one dashboard for one purpose. Build as many as you want / need.\n\nThe data from your sensors should be captured into your timeseries database basically in a live fashion and you‚Äôre front end should also pull in all updated info from the database live. So at any time you go to your dashboard you get the current information. And it doesn‚Äôt need to be just about what is happening now. You‚Äôll probably want to analyse trends over time and identify anomalies. You could also setup alerts if certain variables reach certain thresholds perhaps for a certain time period etc.\n\nOnce setup it‚Äôs all very hands off so may satisfy you‚Äôre intent.', 'Hard to answer this one. Depends on how is the data generated, where it it hosted (cloud, internally, both), compliance/security, apps involved, type/structure of data, etc.\n\nWithout details, answers are a shot in the dark.', 'Have you looked into rpa solutions? And are you looking to build something yourself?']"
Which is better? Cypress VS Selenium,https://www.reddit.com/r/automation/comments/ym4sf2/which_is_better_cypress_vs_selenium/,automation,"Hello guys!
I am pretty new (almost one year) into automation testing for websites. My question and debate topic is which one is better and gives more opportunities in order to test a website? Cypress or Selenium grid?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""When it comes to [test automation services](https://www.qasource.com/automation-testing-services) Selenium is one of the best automation tools available in the market to automate web & mobile applications.\n\nSelenium is a very popular open-source tool widely used in industry. However, there are some new tools like Cypress also, gaining popularity, so let's see the cypress vs selenium comparison.\n\nCypress is a developer-friendly JavaScript-based front-end testing tool that operates directly in the browser using a unique DOM manipulation technique.\n\nSelenium WebDriver is a library along with a language-specific framework with the flexibility to select programming languages like Ruby, Python, Java, etc. It uses JSON wire protocol for executing test cases.\n\nSelenium is a preferred choice for over a decade for test automation, as it has many advanced features when it's compared to cypress, as it supports multiple test frameworks like junit, TestNG, and Cucumber, etc, & supports safari browser, whereas cypress doesn't support safari browser, also cypress can't invoke multiple tabs.\n\nThese were only a few points but Cypress lacks on multiple fronts when it comes to 1-1 comparison. Selenium may be a bit hard to set up & use but it's worth using selenium.""]"
How To YouTube Automation With Python.. I'll be posting my own creation soon!,https://www.youtube.com/watch?v=p0M0MzFLtY0&ab_channel=Moneysmith,automation,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
Help with Report automation,https://www.reddit.com/r/automation/comments/ym0wn7/help_with_report_automation/,automation,"I'm trying to accomplish  the goal of automating the process creating a report that pulls:

The high and low points in activity  in the last 24 hours and 30 days

The average amount of activity lost the last 24hrs and 30 days

It's able to to generate that report at certain times of the day and a click of button

Ideally the report is generated in Excel

This is all being pulled from a site that has live data graphs.


Any help would be appreciated, it's safe to say I don't have alot of programming experience. I'm struggling with this and could use any help!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'What industry are you working for?']"
Simulation in DOPSoft 4.00.16.22,https://www.reddit.com/r/automation/comments/ym0cn4/simulation_in_dopsoft_4001622/,automation,"I‚Äôve been trying to simulate in DOPSoft but all the fonts that I use are replaced with Arial, somebody knows if it‚Äôs a version error?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Reusability in Power Automate,https://www.reddit.com/r/automation/comments/ylj4p6/reusability_in_power_automate/,automation,I want to ask if someone has solved the reusability problem of Power Automate Desktop. How can I reuse the subflows in different Desktop flow?,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Need help finding a tool that integrates Google Drive with Confluence Server.,https://www.reddit.com/r/automation/comments/yldkrv/need_help_finding_a_tool_that_integrates_google/,automation,"Hello!

I need to migrate tens of thousands of files of varying types from Google Drive into individual Confluence Server pages.

Can anyone recommend some good automation tools to check out?

Thanks in advance.

## Requirements

#### Must have

* Copy content from Drive to Confluence Server.
* Retain all content for each file.
* Retain most (if not all) formatting for Google Docs.
* Output a list/inventory to Google Sheets with specific details of what occurred.
* Meet corporate security requirements, including access limitations and data retention.
* Can be tested thoroughly before implementation.

#### Nice to have

* Can determine source file location on Drive side.
* Can recreate source file folder location on the Confluence side. e.g., If the source file was in /documents/taxes/ then it will create a documents page in confluence, a taxes page inside that, and put the file into that target.
* Can read Confluence labels.
* Content reformat or cleanup option. e.g., strip tags, remove markup, things like that.
* Integration with existing Confluence macros.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Anyone familiar with the DC2000 drives?,https://www.reddit.com/r/automation/comments/yl6j8j/anyone_familiar_with_the_dc2000_drives/,automation,Need help repairing some boards on a DC2000 dc drive. If anyone has any knowledge on how to repair the DCFB boards DM me.,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Email Automation Challenge,https://www.reddit.com/r/automation/comments/yl8n8y/email_automation_challenge/,automation,"Hi there,  
Newbie to this group. I'm having trouble with an email automation challenge.  


Most mail tools allow you to ingest responses from a survey, create a contact and then send an email or sequence to a user and include merge fields based on their responses. But what if you are creating an automation where you have someone answering a form multiple times (like say a daily performance report) and you want to send them through an email sequence for each submission with the fields for each submission included with its unique merge fields? The problem I'm having in Mailchimp and other mailers is that their merge field contacts are overwritten so I can't send them a tailored sequence for EACH submission.   


Got any ideas for platforms that would help me out with this or an ideal way to set this up?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
IoT-enabled Smart Silo/Bulk storage using LiDAR for Volume Estimation,https://www.reddit.com/r/automation/comments/yko13k/iotenabled_smart_silobulk_storage_using_lidar_for/,automation," Hi everyone, I am a student taking a Master of Science in Computer Applications. I am currently on the early journey of my graduate studies. The title that I wrote is the prospect topic I want to pursue/conduct in my thesis. Basically, the goal is, the 3D LiDAR will scan inside the Silo or Bulk Storage and depict a 3D visualization of the surface of the raw material inside and compute the estimated volume. I will create a user interface, probably window GUI or web-based GUI for remote monitoring and conducting operations.

Now the question is, how can I or where can I integrate the IoT part? (Since I am majoring in Internet of Things), and what should be my objective for this?","['Might also get some help from r/PLC\nOther than that what IoT devices do you foresee needing for this? Will you be coupling the LiDAR scan with other data from field?', 'Why not just incorporate a scale or use lasers to determine the approximate level of the material?\n\nAre you trying to be hyper-exact by creating a 3D grid from the LiDAR measurements and interpolating to create a surface?', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Automation COE Market is predicted to reach USD 1.5 billion by 2027 with CAGR of 36.9%,https://www.reddit.com/r/automation/comments/yk0ljt/automation_coe_market_is_predicted_to_reach_usd/,automation," To provide detailed information related to major factors (drivers, restraints, opportunities, and industry-specific challenges) influencing the market growth","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Tools for automation and daily tasks,http://awesome-tools,automation,"Hello, I m courios to find out more tools to use in my daily task as a infrastructure engineer. So far I use the following tools:
- dnsx (dns query)
- httpx (http requests and testing)
- mapcidr (quick calculate subnets)
- tmux .. for multiple terminals
- barrier (for remote connection to my computer through laptop)
- ansible ... Off course
- puppet/foreman
- kubernetes with helm/kind
- jq (for json parsing)
- ddosify (testing multiple paths or status code)
- mailcow-docker
- janus (for api gateway endpoints)
- docker gen (generate nginx config)
- acme-companion (generate ssl certificates)

What interesting tools do you use ü§ó??",
Benefits Of Automation Testing,https://www.reddit.com/r/automation/comments/yid2dc/benefits_of_automation_testing/,automation,"Automation testing is one of the most popular methods for software testing. It‚Äôs an extremely efficient, cost-effective solution that can be adopted at any stage of your software development lifecycle ‚Äì and it eradicates ambiguity on whether to use automation testing or stick to manual testing.

Automated testing has many benefits, such as: 

**Shift-left testing**\- It is a part of continuous testing that conveys that the testing phase should be incorporated into the SDLC (Software Development Life Cycle), right from the requirement gathering phase to find bugs at an early stage. This can save money and time, by ensuring that bugs are found early by a tester.

**Easy regression testing-** Developers may need to perform a set of similar test cases over and over again, just to ensure that the bug has been removed. To make sure all testers have access to the latest changes, they have to spend time setting up the environment on their local machines and then run tests in parallel by creating new test plans and test cases every time. This leads to unnecessary wastage of time, money and efforts.

**Reduced business costs-** Automation testing offers significant benefits. It is advantageous for organizations because it reduces business expenses and enhances the quality of work. With automation testing, organizations are able to reduce additional expenses and maximize resource utilization.

**Improve the quality of manual tests-** With the help of test automation tools, you can perform manual testing very effectively. Test automation helps testers validate testing coverage in different environments and produces the required results at a much faster pace. These benefits of test automation have won customer favors in recent years.

**Better smoke testing-** Smoke tests are considered to be a best practice as they help avoid outages. These tests are used in the early phases of a project, and can catch major issues before tests begin running regularly. Automated smoke tests are an efficient way for teams to do this.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Good information about automation testing. Thank you for sharing this.']"
Looking for a way/program so when I receive a text from a particular phone number it‚Äôs then immediately copied and sent to a phone number list automatically.,https://www.reddit.com/r/automation/comments/yhwv7g/looking_for_a_wayprogram_so_when_i_receive_a_text/,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'You‚Äôre basically describing an SMS chat bot with custom response functionality.  Look into Amazon Lex or similar ‚Äî you can allocate a phone number that can receive a text and then kick off a Lambda that does stuff with it like relaying it to an outbound SMS.  I‚Äôve actually done a similar project for a hackathon a few years back so there might be even more user friendly tools out there now.  https://aws.amazon.com/blogs/messaging-and-targeting/create-an-sms-chatbox-with-amazon-pinpoint-and-lex/', ""All you need is tasker if you're on android."", 'Check out Shortcuts for iOS or Tasker for Android. If you want to get more aggressive you can check out Twilio and making your own paid sms bot lol']"
Career change away from production,/r/PLC/comments/yfymff/career_change_away_from_production/,automation,,
10 Key Benefits of Business Process Automation,https://www.reddit.com/r/automation/comments/yeobst/10_key_benefits_of_business_process_automation/,automation,"**What is business process automation?**

[Business process automation (BPA)](https://www.sattrixsoftware.com/what-is-business-process-automation.php) is the use of technology to automate repetitive, manual tasks in the workplace. By automating these tasks, businesses can improve efficiency, save money, and improve employee morale.

BPA can be used to automate a variety of tasks, including customer service, data entry, accounting, and human resources.

Examples of Business Process Automation

‚¶Å	 Document routing

‚¶Å	 Invoice processing

‚¶Å	 Employee onboarding

‚¶Å	 Data entry

‚¶Å	 Screening against PEPs and sanctions lists

‚¶Å	 Data deletion

‚¶Å	 Transaction monitoring

**The top 10 benefits of business process automation are listed below.**

**1. Increased efficiency and productivity:** Business process automation can help you automate repetitive and time-consuming tasks, freeing up your employees to focus on more important tasks.

**2. Improved accuracy and quality:** Automating tasks can help to reduce errors and improve the overall quality of your outputs.

**3. Increased customer satisfaction:** When your processes are more efficient and accurate, your customers will be happier with the results.

**4. Allows employees to focus on higher-value tasks:** Automating low-value tasks frees up employees' time so they can focus on tasks that are more important to the business.

**5. Increased sales and revenue:** Automating your processes can help you to increase sales and revenue by making it easier and faster for your customers to purchase your products or services.

**6. Reduces errors and improves accuracy:** Automated processes are more accurate than manual ones, which can help to reduce errors and improve the quality of your products or services.

**7. Increased market share:** By automating your processes, you can improve your competitiveness and grab a larger share of the market.

**8. Increased profitability:** Automating your processes can help you to increase your profits by reducing costs and increasing

**9. Save time and money:** By automating tasks that are time-consuming and expensive to complete manually, businesses can save a significant amount of time and money.

**10. Higher productivity:** Organizations that use technology to automate processes see an increase in productivity as well. The main reason for this is that machines can handle multiple tasks at once, which speeds up processes.","[""Surprised pikachu face?\n\nWho would've thought"", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Why do I even sub to this subreddit üôÑ', 'Automation is making things easier, faster, and more effective which is why we are seeing more and more involvement of automation in various fields. The most important aspect of automation I think is the fact that it is helping us to do things more accurately in shorter amount of time.', 'The benefits are clear - but what tools would you recommend using to see these benefits through? Recommendations? Windows environment? Linux/Mac environment?']"
extraction automation in excel,https://www.reddit.com/r/automation/comments/yepb85/extraction_automation_in_excel/,automation,"Hi Everyone, wanted to ask if we can build a macro which can extract a Excel file from a password protected pdf (which we generally open from Adobe acrobat from left attachments section) is there an alternative way or std software for this automatation?","['Python or Alteryx is your best bet', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Duplicate Tickets,https://www.reddit.com/r/automation/comments/yeecop/duplicate_tickets/,automation," 

Hi, i work as app support.

I would like to delete duplicate tickets automatically.

Can someone guide me on this.

I can do some programing (Front end , and a little bit of python). But i dont seem to have any idea on how to do it.

The rough idea now i have is to use DOM and get the duplicate tickets close. If there is a faster way,it would be better or maybe a structure i can follow. Can some1 help me on this . Thanks","['First thing id suggest is getting a udemy course on selenium if you want to use the dom. Python or Javascript (python is easier as you dont have to deal with promises / callbacks). This might be easier for you than the ideal method. \n\nIdeally i‚Äôd automate it utilizing the ticket applications API. You can use postman for this if youre not totally comfortable programming. In addition to postman look at the network tab on the developer tools in your browser to inspect the request when you GET the list of tickets and delete them. You can right click the request and copy it as a curl then import it into postman.\n\nGood luck, you‚Äôre on the right path to develop some awesome skills', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""what's your ticketing system? read up on their api docu, perhaps you can create a custom function that could read new tickets and compare them to existing ticket, then either do a merge or close the the other ticket (you might want to leave a comment/note indicating its duplicate ticket #).""]"
Importing Youtube History via Clicknium and Python,https://www.reddit.com/r/automation/comments/ydzgzn/importing_youtube_history_via_clicknium_and_python/,automation,"Hello Everyone, I created a script to import history from one YouTube account to another. There is likely a better way than this, but since nothing existed I just wrote something quick and thought I'd share.

# The Method and Why

This will automatically iterate over your google takeout export and load each video in your previous watch history so that it can be registered in a new account. I use a 10 second wait between each load to ensure the browser has fully loaded the page and YouTube puts it into your watch history.

I created this because I was trying to move my YouTube history from one account to another, and could not find an option. If you want to fully transfer an account, there is a method for this, but I could not find anything for ONLY transferring watch history.

# Prerequisites

Visual Studio Code (you can use any tool to run the python script, this was my go to)

Vivaldi browser (you can probably modify the code below easy enough to work with others, but this is what I used)

Your watch history exported from google takeout

# Exporting History in Google Takeout

Go to [https://takeout.google.com](https://takeout.google.com) on the account you want to transfer history from

Click ""deselect all""

Scroll down to YouTube and YouTube Music section and check it

Click multiple formats and swap watch history from HTML to JSON then click ok

Click ""All YouTube data included""

Click deselect all

Check the history checkbox then hit ok

Click next step

Send via email to yourself. Depending on your length of watch history this will take a varying amount of time.

# Next Steps

If you do not have visual studio code, install it. Load a folder in it and create a python script (I called it [ImportHistory.py](https://ImportHistory.py))

Login to YouTube in the browser you use with the script below (currently written for vivaldi) with the account you want to import to.

Copy and paste the following into your created script

    from clicknium import clicknium as cc
    import json
    import time
    
    # install chrome extension and open a tab to automate
    cc.chromium('vivaldi').extension.install_or_update() 
    tab = cc.chromium('vivaldi').open(""www.google.com"") #Can be any url, just need to open a tab
    
    # Open the google takeout json for watch history
    with open('watch-history.json', encoding=""utf8"") as f:
        data = json.load(f)
      
      
        #Loop through each item in our history.
        for i in data['history']:
            #If there is a title url we can load it
            if 'titleUrl' in i:
                #Strip the https:// from the url in order to make goto work properly.
                url = str(i['titleUrl']).replace(""https://"", """")
    
                #Open the url and then wait for it to fully register youtube history
                tab.goto(url)
                time.sleep(10) #Arbitrary number, feel free to make shorter if it works
                                
      
    # Closing file and tab
        tab.close()
        f.close()

# After your Takeout is downloaded

Extract the zip and navigate to \\Takeout\\YouTube and YouTube Music\\history\\ where a watch-history.json file should live.

Copy the json file to the same folder as your python script you made.

Run the python file which will open a new browser window and tab. This tab will load all videos in sequential order. It is useful to do this in a single tab as you can right click the tab and mute to let this run in the background while you continue doing whatever else.

# How Long Will it Take?

This depends on how long your watch history is, but roughly 10 seconds per video in your history. This might take a really long time. You may try shortening the time.sleep(10) to a smaller value. I was able to successfully run as low as 5 seconds, but this started to cause some issues with not all results being put in my youtube history (likely due to load times).

# Modifying To Use Another Browser

The only lines you should need to change to make this work with another browser is the first two that install the chrome extension and open a tab, specifically

    cc.chromium('vivaldi').extension.install_or_update() 
    tab = cc.chromium('vivaldi').open(""www.google.com"") #Can be any url, just need to open a tab

Clicknium supports other browsers, and if you change this to open a tab on a different browser it should work fine, but you may need to modify it slightly as I have not tested in any other browser.

&#x200B;

Hopefully this helps someone, but if you have the ability to just transfer an account I would recommend that as it is faster.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Does the YouTube history include Liked videos and other playlists?']"
Which of these most closely matches your opinions on automation,/r/IdeologyPolls/comments/ydl3ig/which_of_these_most_closely_matches_your_opinions/,automation,,
My friend tried automating War thunder,https://github.com/therealcyber71/WarThunder-AutoPilot,automation,,
"Walk back to where 1 human is assembling 50 parts into 1 and want it automated. Meanwhile, we just walked passed two humans standing in place doing tasks with zero brain power and zero complexity. If it's low complexity and there is a lot of human labor, it is probably ripe for automation.",https://v.redd.it/g4fgioty88v91,automation,,
Automation Testing Market Extrapolated to Reach $49.9 billion by 2026 at a CAGR of 19.2%,https://www.reddit.com/r/automation/comments/y9n71f/automation_testing_market_extrapolated_to_reach/,automation,"Report determine and forecast the global market based on components, testing types, dynamic testing, services, endpoint interfaces, organization size, verticals, and regions with respect to individual growth trends and contributions toward the overall market.","['Yet another unlinked report....', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Call for Participants - Online Survey on Explicability in Process Mining,https://www.reddit.com/r/automation/comments/y8zu85/call_for_participants_online_survey_on/,automation,"Hey everyone,

I recently posted in r/processmining but I'd like to repost in this channel since there is a wider audience that might have been involved with business process consulting. 

I am an industrial PhD candidate and my research is centered on Explainable AI in Process Mining.

If anyone here had previous experience with explainability strategies in PM consultancy - both as an expert or client - I would value your input for an online research questionnaire on the subject.

It should take no more than 10-15 min and you will be granted access to further insights on explicability strategies through the forthcoming scientific research article, plus the chance to participate in online focus groups.

You can participate in the questionnaire from here: [Microsoft Sway Landing Page](https://sway.office.com/Ew3baJ5vSyTHmWNs?ref=Link)

Thanks for your attention!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
World Economic Forum article: Here's how employers can make sure technology improves jobs,https://www.reddit.com/r/automation/comments/y8bd0g/world_economic_forum_article_heres_how_employers/,automation,"&#x200B;

* **Employers who force technological change on employees without consultation are likely to create an unhappy workforce.**
* **The best workplace technologies free up employees for creative tasks, and improve productivity.**
* **There are two ways employers can achieve this sweet spot: collaboration with labor unions and co-design.**

[https://www.weforum.org/agenda/2022/10/here-s-how-employers-tech-improves-jobs/](https://www.weforum.org/agenda/2022/10/here-s-how-employers-tech-improves-jobs/)","['Wishful thinking at best.', 'Eat ze bugz before zleep in ze podz', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Do you manage a community or coworking space? Want to do it better?,https://www.reddit.com/r/automation/comments/y7k15q/do_you_manage_a_community_or_coworking_space_want/,automation,"Hello everyone,

My team recently built [CoWello](http://www.cowello.com/)‚Äîa tool to help community managers streamline member bookings and payments. This platform is meant for coworking spaces that rent out desks, conference rooms, and/or event spaces to remote workers, startups, and freelancers.

**Here‚Äôs a quick demo:** [https://youtu.be/dLo6OISz5xw](https://youtu.be/dLo6OISz5xw)

We‚Äôd love to know your thoughts! Please drop us an email at [hello@cowello.com](mailto:hello@cowello.com) or leave a comment below.

Cheers!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Introduction to RaaS - Robotics as a Service,https://www.reddit.com/r/automation/comments/y7amcs/introduction_to_raas_robotics_as_a_service/,automation,[https://medium.com/p/introduction-to-raas-robots-as-a-service-e3731af3156d](https://medium.com/p/introduction-to-raas-robots-as-a-service-e3731af3156d),"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
I designed a machine to help accelerate Universal Basic Income,https://www.youtube.com/watch?v=LGqSjCa44Nw,automation,,"['[removed]', 'j', 'Lol, [www.reddit.com/comments/**watch**](https://www.reddit.com/comments/watch) is the URL.']"
"Automate template filling for reports, invoices, and more with this tutorial",http://automatedteacher.com/2022/10/13/create-impressive-personalized-reports-instantly-with-apps-scripts/,automation,,
Looking for An App or Alternatives,https://www.reddit.com/r/automation/comments/y33z1v/looking_for_an_app_or_alternatives/,automation,"Currently, I'm using the automation application Join - by joaoapps. It has a function to send whatever URL between the connected platforms, such as my PC and Android. As I was watching a YouTube video and sending it to my computer, I thought it would be convenient if I could swipe up with two fingers to trigger the app's action. I remember seeing an app years ago that created icons for particular parts or actions of an app, but I am not certain if it still exists, so I am here to ask if anyone knows this app, similar ones, or a workaround.

[https://play.google.com/store/apps/details?id=com.joaomgcd.join&hl=pt\_BR&gl=US](https://play.google.com/store/apps/details?id=com.joaomgcd.join&hl=pt_BR&gl=US)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Automation COE Market Extrapolated to Reach $1.5 billion by 2027,https://www.reddit.com/r/automation/comments/y2r84q/automation_coe_market_extrapolated_to_reach_15/,automation,"Report provide detailed information related to major factors (drivers, restraints, opportunities, and industry-specific challenges) influencing the market growth.","['Report not linked... So guess will take your world for it', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Rockwell Learning+ material,https://www.reddit.com/r/automation/comments/y1y0mt/rockwell_learning_material/,automation,"Would anyone happen to have copies or ability to send some educational material or guides that would possibly work with the learning+ classes? ie. 500, 5000 programming and troubleshooting?","['Or just use CCW because it‚Äôs free. \n\nOr get courses on Udemy for the relevant software. I don‚Äôt think the price is worth it just for learning', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Zapier-like for home computer,https://www.reddit.com/r/automation/comments/y1jqyx/zapierlike_for_home_computer/,automation,"Hi, fellow automators!

I was curious if a software, Linux-based, allows me to automate tasks on my computer (Ubuntu) that are repetitive and happen multiple times per day.

I am looking at something similar to Zapier, but for my desktop computer, not an online service. 

Something like: when folder ABC is updated with a file, make a copy of that new file and move it to Folder DEF

Is this possible?","['You could run n8n on your computer and have that do it. Or you could use rsync if this was the only use case.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Field Medic Using the Workspace ONE API to Fix Real-World Issues,https://mobile-jon.com/2022/10/11/mobile-jon-field-medic-using-the-workspace-one-api-to-fix-real-world-issues/,automation,,
Automation Ideas in IT,https://www.reddit.com/r/automation/comments/xz1xyj/automation_ideas_in_it/,automation,"Hey everyone, i just wanted to ask for some ideas on what to automate in ur daily job as IT as HD,SD,Sysadmin ect.. What are some things that you have automated?","['Disabling user ids.  Ideally connected to event from HR systems.  \n\nNice way to clear audits.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Depends on your situation. Whatever you do or see someone do repeatedly is something that can be automated. I usually start with the highest ROI.']"
Automation of windows program,https://www.reddit.com/r/automation/comments/xwerjf/automation_of_windows_program/,automation,"Hey Guys, am trying to automate a windows program. It has a input field and gives the elevation value after entering certain coordinates in it. Currently I am using python for it and its working great. But am using pyautogui.locateonscreen and it does not work well on other monitors. So is there any windows automation language or anything that I can use for my purposes. 
Thanks!","['Pyautogui should be your last resort. It finds elements using screenshots and the process is very fragile.\n\nTry pywinauto instead. Instead of screenshots, it identifies individual windows components (such as a serach box, button...) and interacts with them natively.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'You might have better luck by posting on r/rpa but something like UIPath could be a program you could use for this purpose.', ""If you have money to spend, you could look at UFT/QTP. I haven't used it for a decade, but it works with Windows forms."", ""If you're using Python, tkinter might be an option as well. Can specify the fixed size of the input window so it does not re size across various monitors.""]"
How will I implement this automation,https://www.reddit.com/r/automation/comments/xv5bqu/how_will_i_implement_this_automation/,automation,"I had this project implementation idea and found out that power automate was the best tool to do it. But I later found that I need a premium version to implement the features I want to.

I there any alternate way to implement what power automate premium version can provided its free","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Not sure what feature in Power Automate you're looking to replace. If it's text-based automation, try Text Blaze. If it's more of process-based automation, try IFTTT or Decisions."", 'Yes you just need to go into the settings and select that within power automate', 'Depends on the connectors you need in Power Automate to the cost. Premium connectors is where they get you.']"
Anyway to automate or macro this game modding process?,https://www.reddit.com/r/automation/comments/xsoo9l/anyway_to_automate_or_macro_this_game_modding/,automation,"So I'm part of a modding community for older NHL  games. One of the things we do is remove retired or young player portraits from the game and update with new images.  This is a time consuming process that takes 5-10 minutes per player. I appreciate we could probably get an app build but wanted some ideas about how we could automate speed up the process.  Its time consuming because we need to decrypted extract files make changes in Photoshop then rebuild. I have copied the instructions below but note its missing images as its a PDF. 

&#x200B;

Step 1: Find portrait ID  
Ex: McDavid, Portrait ID 9857  
Step 2: BIG File Extractor  
Open BigF  
Step 3: Find portrait  
Note, there are two locations. USRDIR\\fe\\ion\\artassets\\playerheads and  
USRDIR\\fe\\ion\\artassets\\playerheadssmall  
Both of these will need to be updated. One is for menu, one is for in-game.  
There are 3 folders within to split up the portraits, and one for blank silhouettes  
In this case, McDavid is in p8001\_12000, as we are looking for p9857.  
After selecting it, this should be your screen:  
These files all have their own use, and the names are all different depending on the portrait.  
Extract all to a new folder, I name it the portrait ID.  
We are looking for the Refpack compression that is not file 0. It is always the file with the biggest file  
size. In this case, it is file 3512.  
Step 4: QuickBMS decompression  
Open up QuickBMS and use the script refpack.bms  
Select the file name we found  
Save it in the same folder.  
Step 5: Photoshop  
Now there is a .dds file to edit. Open it up in photoshop.  
This is what we get  
Looks just like what is in the game with one difference ‚Äì the black background. This is solved by  
using an alpha layer so we get what appears in the game:  
We‚Äôll keep this in mind for later. For now, lets replace it with an updated portrait. Make sure the  
bottom of the picture cuts off where it did in the original.  
Now, lets get back to the alpha layer.  
In the bottom right corner in Photoshop, where you can add/change layers, there is a ‚Äúchannels‚Äù  
menu. Select it  
There are 5 layers. RGB, Red, Green, Blue, and Alpha. Select the alpha layer.  
Lets make it completley black. I use Alt+Backspace as a shortcut to fill the image black.  
Now, go back to layers, then hold Ctrl+Left click on the thumbnail for the portrait to select it  
Go back to Channels and fill that selected part in with white. I use Ctrl+Backspace to fill it in.  
Notice now if you turn on the visibility for RGB (and keep Alpha visible) there is a red tint. This will  
not show up in the game, but the rest of the image will.  
Save the .dds, you can overwrite the old one if you want, or save it as a new file. But make sure you  
use these settings:  
Step 6: QuickBMS Compression  
Open QuickBMS with the refpack\_compress.bms script. Select the .dds file you just saved  
Save that in the same folder.  
Now, you notice the folder contains a new file.  
Delete the original file, in this case 3512. Then, rename the compressed dds file to be that name,  
with no extension. You can also delete the unpacked dds file. Your folder should look like this:  
Step 7: Rebuild .big  
Back in the BIG File Extractor, select Rebuild BIG. Select the folder that you were working in.  
Save the file in the folder that held the original .big file. Sometimes this program crashed when  
rebuilding, I have found that to reduce that crash you rename the file so it saves as a new file rather  
than overwrite the original one. There is still a chance the rebuild can crash. If it does, simply try it  
again with a new name.  
Once it is done, you should see this  
If your new file has a file size of 0, there was an error in rebuilding the .big file. Try to do it again.  
If it has worked, simply delete the original file and rename your file to be the same as the original.  
Done! Check it out in-game!  
Remember that this was the first of two files to replace. The other one is in playerheadssmall. It is  
what is used in the actual game, not in the menus. You have to go through these same steps to  
replace that one, as the file size is different.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Yeah, as long as you can explain the process this well, that's no reason it couldn't be done. Id do it with python. Hardest part is probably the image manipulation in Photoshop but even that is doable.""]"
automate 4 sequential 120v plugs without wifi?,https://www.reddit.com/r/automation/comments/xqt6ka/automate_4_sequential_120v_plugs_without_wifi/,automation,"Looking for options to reboot isp router, internal router, aps, and one other device. Once a week, middle of the night, 5 minutes between each to allow a full reset.

I considered smart plugs, but the whole point is to reboot WiFi/Internet so not sure how that would work.

Middle of the night, so doing it from my phone isn't an option

Any suggestions?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Maybe a mechanical timer plug?\n\nI looked quickly and saw a lot that do 24h.', ""There are options but we'll need to know more.\n\n1) Why?\n\n2) is it every night at the same time, or which combination of those two?\n\nThere are options to bounce your router if it fails a ping (inside too, I think).  There are options to bounce many routers on a schedule and then bounce other pieces).  There are options to bounce things only when you're home or only when you're away. This uses a few new devices but some of it can be done with stuff you have, so tell is whether you have some voice assistant or iot stuff on-hand."", '[Shelly Pro](https://shelly.cloud/shelly-pro-smart-home-automation-solution/) would work perfectly for this.']"
web automation help!,https://www.reddit.com/r/automation/comments/xqq2rv/web_automation_help/,automation,"Hello guys,

I was working on a app api integration with 3rd party web apps.
Unfortunately the api does not cover everything, few features require web login using google and fill information like customer & driver phone numbers, delivery date and time , location pickup and drop. Then you submit the order.

For such a task what is your recommendation. 
What are the best free tools out there that i can run on linux/windows ?

Thanks","['You can use PowerAutomate on Windows or something like scrapy using python on windows and linux. \n\nDepends on your comfort with python, but I would go that route as you have the most integration flexibility.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Zapier? You‚Äôre still stuck with their limited API commands, but you may find a workaround solution']"
how to automate switching between data transferring input devices,https://www.reddit.com/r/automation/comments/xq7jt5/how_to_automate_switching_between_data/,automation,"Hello everyone, I have several devices that have an output D-sub socket for reach of them, and an output device that that has one input socket, this device allows me to read the data that the input device that's connected to it transfers, and I would like to automate the switching between the input devices instead of having to always unplug and plug each manually, how would I go about doing this? 

The issue that I see with using relays for example is simply the amount of switching that is needed, because I need to switch to all the pins of each D-sub socket that I want to use, are there boards that allow switching between such sockets?  Or should I really use an abysmal amount of relays instead? 

Thank you!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Anyone have options to automate Red rotating switch and 2 x plug sockets? Details in comments.,https://www.reddit.com/gallery/xnvnoy,automation,,"['This is a new set up for a pool pump/heating working.\n\nThe red switch controls heating elements or UV treatment and must have the water supply before activating.\nPlugs must both turn on before the red switch activates.\nThen the red switch must also turn off before the plugs turn off.\n\nAny thoughts on how best to automate this would be amazing and thank you in advance.', 'you can use a contactor rated for the load and place it downstream of your disconnect (red switch), the coil of that contactor can then be controlled by a lower level signal, for example a pressure or flow switch that you place in in line with your heater. Not sure what you mean with the plugs turn off.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'You could use a timed relay, you may also want another control relay, depending on what turns the system on.']"
What makes my control-system real-time?,https://www.reddit.com/r/automation/comments/xmu7vm/what_makes_my_controlsystem_realtime/,automation,"I'm currently working on my thesis for uni. In my project I control a virtual vision-based robot cell (using RoboDK) and control it using TwinCAT. These are communicating via OPC UA (using Python API).

An algorithm written in Python as well calculates and plans the path for the robot based on the information from the simulated 2D camera.

The system runs on windows.

Does all this mean that my robot control is real-time? (Because my robot program is not pre-compiled and uploaded but the trajectory is calculated while running.) Afterall all my system-components exist on a PC virtually and there is no hardware in the loop.

If not, what term would you use to describe this type of control? What makes it non-real-time? (Using windows makes it impossible to control in real-time?) Does the TwinCAT runtime (or PLC cyclic control in general) make it real-time?","[""A system is considered real-time only if it can meet its temporal requisites. That means they all tasks must meet the deadline, independently of whether the deadline is short or not. Windows isn't a Real-Time OS, consequently, your application isn't either.\n\nRunning the control application in a PLC with a simulated robot would be considered a Hardware-in-the-Loop simulation"", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'The definitions are a bit fuzzy, but I would say your system can be considered real-time. If it responds dynamically to its environment, it is real-time. This is contrasted against something like a manufacturing robot, which will typically run through pre-programmed routines with only minor adjustments made by sensors.', 'Id say a system is real time when you have a trace of times when every thing happened. And the timings are assured by the system. I mean if the fieldbus has a cycle time of 2ms (for example), I expect to receive the robot tpc (for example) each 2ms exactly. If this is not constant in time for me it is not real-time/synchronous system, it is an asynchronous system.', 'I would say a controller works real-time if the controller can compute the its output at least 10 times faster than the system time constant. So as nention by other it is relative to the system but it means that controller computes fast enough to react to disturbances.', ""So based on the four comments I received there are three points of view and it seems that the definition of real-time is not really obvious.\n\n* Firstly someone says that my system can react to its environment so it may be considered as real-time. IMO this is a way broader definition of real-time, so I'm concerned that it really is called so. \n* Ohers say the speed of the signals of my system has to be deterministic and assured (equal or higher than the controller cycle-time) so when the controlled machine receives them the evoked effect meets deadline. \n* On the other hand it is completely impossible to be real-time because of windows.\n\nPlease correct me if I'm wrong!""]"
Zoom bot joining various meetings Simultaneously.,https://www.reddit.com/r/automation/comments/xl0c6o/zoom_bot_joining_various_meetings_simultaneously/,automation,How can I make my Zoom bot join various meetings at a time?,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Getting into manufacturing automation at a small business-looking for resources,https://www.reddit.com/r/automation/comments/xjoovm/getting_into_manufacturing_automation_at_a_small/,automation,"I run a small business with my partner and we are getting a small manufacturing line going for our product. We have a system for getting boxes/cartons printed, creases, and cut (either in advance or on-demand).

I've been looking around at YouTube videos and various other automated equipment websites trying to reverse engineer some of this stuff. Are there other resources I should look into?

For reference: im an electrical and mechanical engineer with heavy programming experience, so I can build a lot of systems, im just looking for reference on how to build reliable systems of this nature. Thanks in advance for the help!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Check out our website flex-lineautomation.com. Our engineers will be happy to help you get your line figured out!', 'Wow! amazing venture, I am in the same situation in the past looking for system and parts for a small project. I found this site Industrial Electrical Warehouse, theyve been helpful with my needs. hope this may help you, good luck!']"
Preprocessing documents before sending to IDP,https://www.reddit.com/r/automation/comments/xjgm4u/preprocessing_documents_before_sending_to_idp/,automation,"At my last job we implemented ABBYY to speed up document processing and it worked for most documents but not all. In the end we ended up using the following workflow:

1. All invoice submissions went to QA first who validated the document type (and if it was clean etc.)
2. If it was a doc that was good for ABBYY they would send it there.
3. Otherwise bookkeeping would handle it themselves.

This worked but ruined the point of going to IDP since inevitably we would get a backlog and it wasn't uncommon to have crap documents in there anyways.

After talking around with other people it seems like this is not an uncommon scenario. I've been exploring a (really dumb?) idea to do a quick preprocessing on documents (<10s) to just evaluate:

* is this a legible, valid document submission?
* what type of document does it look like?

And then either send it back to the submitter if it is a bad doc, or pass it on to the right bucket if its good.

So, it this a useful idea? If you do IDP, do you also have a person in the loop for that first validation step?

Any and all thoughts are welcome, especially if I'm the only one that dealt with this problem. Also, if you want to try out what I've got so far PM me.  


Update: [https://www.gdbd.app/](https://www.gdbd.app/) is pretty much what I was thinking of, thanks guys, not going to pursue further.","['You should give this service we use a try: [https://www.gdbd.app/](https://www.gdbd.app/)  \nIt was very easy to integrate into our pipeline.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Own the logic to generate your discover weekly and release radar playlists - without Spotify or other cloud music providers,https://blog.platypush.tech/article/Automate-your-music-collection,automation,,
"Easy, quick, no-setup Cypress Boilerplate",https://www.reddit.com/r/automation/comments/xexk69/easy_quick_nosetup_cypress_boilerplate/,automation,"[https://github.com/optimumqa/cypress-boilerplate](https://github.com/optimumqa/cypress-boilerplate)

I'd like to share with you our Cypress boilerplate which we use for new projects.

Suitable for beginners and easy to modify if you would need to.

Hope it helps :)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Where should I start?,https://www.reddit.com/r/automation/comments/xe9y5x/where_should_i_start/,automation,"Hey guys, in short, I want to create an automation that can download course documents/slides from our school‚Äôs website. I have only little knowledge about programming. What language should I learn, where should I start, and how long would it take to learn and build an automation that can do that.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'My suggestion is to learn Javascript. Once you do, look for QA Automation tools that support javascript. Any tool out there should have the option to download from websites.\n\nProbably the fastest and best way for your problem would be to learn Python and download all stuff with a python script.']"
Automate Progress Reports for Teacher?,https://www.reddit.com/r/automation/comments/x9yzbt/automate_progress_reports_for_teacher/,automation,"Greetings All. My wife is a teacher, and her school requires teachers to do weekly progress reports for their students. Last night I was helping her set up this year's forms (63 total), which involved inputting the student's name, the appropriate teacher's name, and the date. Unfortunately, this is a largely manual process, requiring her to manually tally the number of missing assignments and transpose the details for each into its appropriate box on the form on a weekly basis (this eats up between 2 and 3 hours, since she has to switch back and forth between pages). She commented that she wished this sort of project could be automated, and it got me thinking about options. Unfortunately, her school is cheap as hell and will not invest in any gradebook applications (they rely solely on G-Suite, using a shared Google Sheets workbook for grade tracking (I know, lol)). 

What I would like to do is come up with a way to automatically fill in the student's details to the form (right now it is a Google Doc with crazy formatting, would love to move it to Word or PDF it but I need to maintain compatibility with G-Suite), date the form, and then tally the number of assignments missing for each class and load it into the  appropriate box on the form. Then I would need to automatically save the output so it is ready to print, while maintaining an archive of the file.

I used to work for a company that used a program that ran VBS scripts to generate the day's reports in a variety of formats (some Word, some Excel, a couple of PDFs). Ideally I would love to do something similar. However, I am learning as I go. So, I am curious what everyone would recommend for achieving this goal (e.g. do I need a program, do I just need scripts, what language, etc.). My hope is to give her an option that scrapes the master gradebook on command and carries the data to another spreadsheet (in case any students are added later we won't have to tweak the program), then carries over the student's data to the appropriate fields, then looks at the line of grades and reports on number of missing assignments (they score 0-4, with 0 being missing and anything 1 or above being turned in, so this should be a simple item to report on). The output would then export as either an editable document, a PDF, or both. If anyone has any external sites that they would recommend I research, please post the links! I'm not looking for everyone to do this for me, I just need an idea of where to start. Thanks!","['It could be easily done by existing G-suite account.. with the help of Google apps script .\nGoogle sheet , doc, pdf, auto email etc can be easily possible.', 'I‚Äòm pretty sure this can be done with Python not too complicated, it may be finnacy to get the data out of the doc, but maybe you can export that as csv and run over that.', 'Python can, but if you are not a developer, look into Microsoft Power Automate.  There are a number of other tools that would work but you need something you are comfortable maintaining.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'It really depends on the inputs and outputs of the process, \n\nThe common thing to do is mail merge/personalized emails. (You can change the output to a email/word doc/pdf.\n\nThis can be done using python, or excel + word or Microsoft power automate or using gsheets + app script or some other extension that works with gsuit.', 'Going through old posts I saved. Here‚Äôs an option if you havent already found one: http://automatedteacher.com/2022/10/13/create-impressive-personalized-reports-instantly-with-apps-scripts/']"
how would you auto-save facebook group posts?,https://www.reddit.com/r/automation/comments/x9sa6a/how_would_you_autosave_facebook_group_posts/,automation,"It‚Äôd be nice if I can export posts from a facebook group automation. e.g. send an email copy to address. I have tried with IFTTT, make.com with no success. Also importantly it‚Äôs a closed group","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'You can literally ""watch posts"" and get the content from them with Make - assuming that you\'re the admin/owner of the group.']"
Dimensionally Accurate Rechargeable Batteries,https://www.reddit.com/r/automation/comments/x8mhlf/dimensionally_accurate_rechargeable_batteries/,automation,"I bought my son his first race car and it takes 6 AA batteries which it just drains.

It fits any of your normal store-bought double A's just fine but four different pairs of rechargeables I've purchased are a bit too big.

Has anyone had a similar issue and found an AA battery that has the same diameter?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Auto-Paste (Paste Automation),https://twitter.com/Cyclebrity/status/1567592879958409216?s=20&t=T50bqeoREt_RuNTA1sy2bA,automation,,
Is there a tool to automate some click and search process?,https://www.reddit.com/r/automation/comments/x7zxzj/is_there_a_tool_to_automate_some_click_and_search/,automation,"I am on a website which has many audio files, it doesn't have a download button but you can download the files if you take the link that is provided to the <source src=""http:..linkhere""/>. It's painful to go through all that manually so I am looking for a tool that will click on one link open inspect element (or even without opening) but just grabbing that link provided to source and open it in a new tab then right click and save it. I tried to write a script in nodejs that would do that without needing clicks and stuff but the problem is you need to be logged in to be able to read the content of page so I am not very good at that.","['Selenium/Cypress could help.\n\nId suggest sticking to that script, looking into the authentication the site does, and adding that to the header of the requests. This will be much quicker and won‚Äôt require a browser / webdriver.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
The Complete Guide to Scrape Google Maps for Lead Generation with No-Code,https://scrape-it.cloud/blog/zapier-for-google-maps-scraping,automation,,
how to automate data entry,https://www.reddit.com/r/automation/comments/x56o8x/how_to_automate_data_entry/,automation,"I was just given a job to pull data, names, phone numbers, email addresses and other ""job application"" data from a website to an excel spreadsheet.  I feel like this could easily be an automatic process but don't know how to go about automating it. Any ideas?","['Build a scraper using python and beautiful soup', 'Use Python and Selenium', 'Have you checked to see if they have an API or export feature before you jump to scraping? Half the time I read these posts there‚Äôs already something in place to facilitate the very thing.', 'Build a workflow on Wrk! It can be easily automated. Hit me up, if you want to learn more!', 'McGoogle it, there are lots of options simple and complex. Easier is to copy/paste python script, or you could run a SQL database that pulls on request packets if you wanna have some real fun!', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'ui path can handle that easily', 'If the website layout is static this can be easily done using Microsoft power automate desktop', ""I should mention this is going to be for work I'm doing for a non profit. So I'm not interested in paying to have this work automated.  I don't get paid for the work either."", 'use scrap to get the information from de website you want. If youre in linux you can do with a bash script.\nUse the curl command to extract all the information from the site and then egrep to filter']"
"hello, I want to find a way to automate/semi-automate putting these sticker labels onto these paper bags. I found a way to get the sticker off the strip but I don't know how to get the stickers on the bags from here.",https://www.reddit.com/gallery/x2m20d,automation,,"['A labeler machine will do everything. \nIf you have automated the showing of the label one time per package you only need to transport the boxes and drag them touching the label.\nI.e.\n https://youtu.be/7WcPZivgCo0', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Post on r/PLC']"
Need ideas for other tools to remove scale from centers. Info in comments.,https://www.reddit.com/gallery/x2mzuf,automation,,"['Is it like a 60 degree center for a tailstock or is the shaft hollow and you are trying to clean the ID?\n\nThey have those spring loaded hones that use actual abrasive stones if it‚Äôs an ID. Do you have a  manual lathe with a tailstock?\n\nEdit: the spring loaded home looks like a tripod on a flexible shaft', ""I am trying to source a tool for automatically removing heat treat scale from the inside of shaft centers. Right now I am leaning towards a more lightweight version of the second picture. The weakness is that the paper can tear and fall out, and depending on pressure can wear fast. I'm looking for any tools you all have used that are cost effective enough to be consumable. Also trying to get away from twisted wire construction.\n\n&#x200B;\n\nEdit for more clarification: Ours is a standalone machine that just cleans centers. Part is clamped horizontal down to the pallet on the infeed conveyor, two motors on slides come in and brush the centers. Air pressure controls the motor feed pressure, the motors slide without any encoding so drills and cutters are out."", 'Have you looked at cone flap wheels? They might be more resilient than the tool in your second photo.', 'What about center laps? We use a couple mounted in regular old drills.', 'Dry ice blaster', ' Before we switch tools, how fast were you running. Those run at 850 RPM if I remember correctly.', 'We had a center cleaner machine for after heat treat.  It had a norbite cone for a bit. Some kind of abrasive stone anyways.  Lasted a long time.  If they make that machine I am sure they made specialized bits', ""Not sure if someone already mentioned this, but they sell tools just for this purpose. It's a cone that is sold with cone shaped sandpaper, works great."", 'We line them up and hit them with a bead blaster.  It turns out to be a much quicker operation to get them in and out of a blaster than a mill.  And it is a very low skill operation.  New guy type of job.  This also keeps everything on center for the lathe op.', 'A flat bottom drill? Just have the operator adjust z for the thickness of the scale? Either that or I know there are brush tools for polishing/deburring that might work', 'A needle scaler? Putting it in a lathe is slightly unusual, but this is almost exactly the goal.', 'xebec brushes maybe ?', ""How about these   \n[https://www.kemet.co.uk/products/deburring/cnc-deburring-wheels](https://www.kemet.co.uk/products/deburring/cnc-deburring-wheels)  \nThese look cool. Nearly bought one for my stuff but didn't in the end. (That story is a keeper for free too).  They come in all shapes and sizes"", '[Cratex cones.](https://www.cratex.com/Products/Rubberized-Abrasives/Cones/Cones) they are a rubberized abrasive cone on a mandrel.  You can shape them anyway you want.', 'A soft wood dowel? With some light oil?', 'I think youd be best off making the centres not so deep pre heat treat and then going to a finish depth with a 60deg carbide drill or equivalent mill']"
Website Scraping,https://www.reddit.com/r/automation/comments/x1oeft/website_scraping/,automation,Anyone here that can scrape a website‚Äôs hidden pages? Need some help with this.,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Is it just one website? [Wrk.com](https://Wrk.com) would be a good solution if you are doing this on a regular basis, but maybe not if this is a one time thing. I work at Wrk, let me know if you want to chat about this']"
"hi how can I automate my blogs sites, I want to automate wordpress blogs.",https://www.reddit.com/r/automation/comments/x1icsa/hi_how_can_i_automate_my_blogs_sites_i_want_to/,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'You can use wp-cli  \n\n\nhttps://make.wordpress.org/cli/', 'You can use [Wrk.com](https://Wrk.com) if you want to do this on a regular basis. There are workflows that can help you with this. Hit me up, if you want to learn more :)']"
Do change the tray for a good a cable (like 12AWG) will prevent this to happen? is 120v 10A rated relay. I'm connecting a 120 v 5-9max A motor to it.,https://i.redd.it/xcarl7hdmck91.jpg,automation,,
how can I automate video clip clipping?,https://www.reddit.com/r/automation/comments/wz7ch6/how_can_i_automate_video_clip_clipping/,automation,"I have been taking speeches from my job that are hours in length and cutting them down into smaller clips so that we can archive the clips as separate clips in the event that someone needs just a portion of the speech. The task is very time consuming, I do have the time stamps that I have to go in and take clips but the process is tedious. 

&#x200B;

Is there a way I could somehow make bot that plugs in the timestamps and saves each clip so that I save time and work on other more important things? I have some experience with powershell, very little Java/C++, video editing tool is Wondershare filmora 9","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I would recommend ffmpeg.\n\nhttps://ffmpeg.org/ffmpeg-filters.html#trim\n\nIt's all command line based."", ""Hey, maybe this is something that could help: https://youtu.be/5MhmdM_hq8A\n\nI never tried it, so i don't know it has the feature you need but sounds interesting""]"
Every Zapier alternative as a single implementation?,https://www.reddit.com/r/automation/comments/wyco7y/every_zapier_alternative_as_a_single/,automation,"Since we are planning to build an integration to zapier I was wondering if there is any library which sits on top of zapier, microsoft flow builder, google flow, ifttt, integromst, etc. There are so many integration providers that I think its not necessary to build everyone into our platform.

Is ghere any library which we could implement once and then would work on all of these providers?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'n8n, or node-red (bit harder, closer to the machine, but more versatile)', ""You're asking is there a centralized pool of API info that zapier, ifttt Ave the others pull from. The answer is no."", 'There is this free project called tracardi, when you connect it it will orchestrate your event to any destination you want. Or you can use segment but then you need to pay for every event.', 'If you‚Äôre willing to work with a SaaS provider, look into KinCloud. We use them as a first-layer integration mesh to connect different CRM, ERP, and web-based tools. It‚Äôs a set-it-and-forget-it system so much less involved than Zapier with the smaller workflows. Also, consider RunAlloy; they‚Äôre mainly an eCommerce automation platform but recently released an integration software to do the same at a larger scale. Workato is also an option, depending on your needs.']"
ElectroNeek: Why is the Automation of Remote Work Essential? - ElectroNeek,/user/ElectroNeekGlobal/comments/wxh5wo/electroneek_why_is_the_automation_of_remote_work/,automation,,
What states can an assembly line station be in?,https://www.reddit.com/r/automation/comments/wwk9gr/what_states_can_an_assembly_line_station_be_in/,automation,"Hoping you all can sanity check me on this:

I'm using three conditions to dictate the state: part presence, cycle status, and line run-out status (When there is no more work in the queue)

|Part Present|In Cycle|Cell Running Out|State|Idle Reason|
|:-|:-|:-|:-|:-|
|1|1|0|Working|\-|
|1|0|0|Idle|Push|
|0|0|0|Idle|Pull|
|1|1|1|Working|\-|
|1|0|1|Idle|Push|
|0|0|1|Idle|Run out|

The point of this being to automatically detect and track how long each station spends working and idle, and why it is idle.

I omitted faulty states like in-cycle and no part present, but are there any other states that I'm missing?","['You should look into PackML. It‚Äôs an attempt at standardizing machine states in a manufacturing environment. \n\nhttps://www.omac.org/packml', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Need opinion for Github Integration with Automation Application.,https://www.reddit.com/r/automation/comments/wvs8lj/need_opinion_for_github_integration_with/,automation," **When integrating Github with your automation software, which is better? From a Security point of view? From a UX point of view? : -**

&#x200B;

1. Ask the user for the Personal Access Token every time they want to make changes to the connection(Add repo, org, etc. or Edit existing)
2. Ask for the token once, and let the user do unlimited changes till the token expires.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
How to prevent from my email mark as spam?,https://www.reddit.com/r/automation/comments/wvdq7i/how_to_prevent_from_my_email_mark_as_spam/,automation,"I am currently making a web app for my company. One of the features are automatically sending email notification to our customer to notify that we have started the job that they requested.

The problem is, when we open tested this for 3 months, our company email got marked as spam.

Is there any way to prevent this? I have seen so many people sending newsletter and a similar notification email without being marked as spam so I know it is possible.

I use Phyton, Django Framework, JavaScript and Next.js

Thank you in advance!","['Hi , \n\nWould it be possible to provide the email headers of one of the emails that have been marked as Spam?', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
AI Art in Relations to the Commercial Art and Design Industry,https://www.reddit.com/r/automation/comments/wuyjab/ai_art_in_relations_to_the_commercial_art_and/,automation,"I have seen some AI art. People in creative industries say it's a tool, others say it's nothing to worry about but I haven't heard people talk about how it's going to reduce jobs. I want to hear from people who better understand automation, what is AI art and/or similar programs going to do to art and design fields?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Built a bit of automation to help improve my public speaking skills,https://automating.life/posts/using-automation-to-improve-public-speaking,automation,,
What job title would this be?,https://www.reddit.com/r/automation/comments/wqyk9f/what_job_title_would_this_be/,automation,"Currently I am a data analyst. My responsibilities in this job have mostly been using BI tools to make presentations and reports for our CSM's to show clients. However since the team started with me and two coworkers and we have a non-technical boss, we've had a lot of flexibility to do pretty much whatever we want.

A lot of those unrelated projects have tended to be with automation. I've created VBA macros for our finance people, used jupyter to make a script out of something that had been done manually, and designed workflows on [Tray.io](https://tray.io/). I know python and javascript at an intermediate level, but don't know much outside of actual coding (i.e. I can solve coding problems but don't know much about like how frontend and backend interact or about networking in general).

Is straight up automating things like a specific job? If I like those kinds of projects is there some sort of career path that I should be looking into?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I don't know, but I too would like to learn more about specializing in automation solutions.""]"
Pulover's Macro Not Stopping,https://www.reddit.com/r/automation/comments/wqxjsk/pulovers_macro_not_stopping/,automation,"i've made a macro with pulover's macro creator to test out one of my bots, and I can't seem to stop it. I tried the default playback and play macro commands, as well as the stop, and they never seem to work.

[This is the macro I have](https://preview.redd.it/rzn4uf2fqbi91.png?width=1310&format=png&auto=webp&v=enabled&s=9de89b28bb978faf86eb8de2cb5d467d4634f6f6)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
My PowerShell Devops Automation Framework: Kasini3000(similar to ansible),https://www.reddit.com/r/automation/comments/wqj3x4/my_powershell_devops_automation_framework/,automation,"win,linux devops automation batch script framework.(It is similar to PuppetÔºåAnsibleÔºåpipeline) 

Open source, free, cross-platform

&#x200B;

scenes to be used: 

Distributed tasks, file replication, master-slave server management, etc.

&#x200B;

i post it at powershell area ,to see:

[kasini3000](https://www.reddit.com/r/PowerShell/comments/wpk9nm/powershell_devops_automation_framework/)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Workaround for Outlook restrictions at work?,https://www.reddit.com/r/automation/comments/wqaqk3/workaround_for_outlook_restrictions_at_work/,automation,My work does not allow the sharing of our Outlook work calendar outside our organization. This is making life difficult. I use a Google calendar for personal/family and I‚Äôm wondering if there‚Äôs a way that I could create a workaround that would effectively connect these. Any thoughts?,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I use Nine to access my work email and calendar from my phone. It can make your work calendar appear on your phone's calendar app as well, though it won't be visible anywhere other than your phone."", ""there's an app for windows called gsync, check it out""]"
Python Google Sheets and Excel Automation with XLWings and automated email sending.,https://youtu.be/ed4OsePomFw,automation,,
Testing tools for Automation,https://www.reddit.com/r/automation/comments/wpr25p/testing_tools_for_automation/,automation,"Automation testing can be a time-consuming and costly process. It is essential to choose the right automation testing tool that fits your requirements. There are various automated testing tools available on the market, but it is important to choose one which will assist your developers in writing reliable code. 

The Test Automation Solutions program you need can be found in our extensive list of options and frameworks. Many of the tools and frameworks are self-paced, so you can take things at your own pace. You can also pick from a variety of topics including test automation frameworks and tools to help write effective test cases.

You can select a test automation framework and a set of tools that best fit your needs. Check out these tools: 

1. **Cypress** 
2. **LambdaTest** 
3. **Selenium** 
4. **Puppeteer** 
5. **Playwright**","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I might need to check out Puppeteer. I just looked it up and it looks pretty interesting. Thanks for sharing! I currently use Text Blaze but that is primarily for email and text automation. I'll check out Puppeteer""]"
Is there An Android App that listens to sound and plays an audio file for specific word?,https://www.reddit.com/r/automation/comments/wprquj/is_there_an_android_app_that_listens_to_sound_and/,automation,"Just like how Google Assistant pop-up when we say ""Ok Google"" or ""Hello Google""

How about something like I say ""Deez"" and an audio file plays called ""Nuts.mp3""

Is it possible?","['Can share the code', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Absolutely. I built one speech to text app on MIT app inventor to help kid write correct spellings.\nThe code can easily be modified to do some action based on what text is generated by the speech to text engine!', ""I modified the app. The folder attached has the source code (aia) and the app. It just says the command matched for now, i don't know what action you need \nBe cautious about installing packed apps. Better build from source so that you know what it does.\nTo build from source, import the aia file to MIT app inventor web app builder.\n https://www.dropbox.com/sh/xqec40r5bxvjl7f/AAC5Pk7hC23ibaHKdwO8gA-oa?dl=0""]"
How to bulk search people on LinkedIn with Google Sheets?,https://growthtoolz.com/how-to-bulk-search-people-on-linkedin-with-google-sheets/,automation,,
I just moved here and can‚Äôt set this to save my life lol,https://www.reddit.com/gallery/wopvuj,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'I‚Äôd take off the switchplate too see if I could spot a name. If that didn‚Äôt work, I‚Äôd turn off the power and start disassembling stuff to try to find out.\n\n\n\nWAIT! Is it this one? I searched ‚Äúlight timer month/lat‚Äù\n\n(https://www.lowes.com/pd/TORK-Timers-Digital-Lighting-Timer/1000341251)']"
"I don't know if this is where the mechanical automation engineers live, but if y'all could stop using worm gear reductions to move high inertia loads, I would really appreciate it.",https://www.reddit.com/r/automation/comments/wnw29r/i_dont_know_if_this_is_where_the_mechanical/,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'I prefer planetary gearboxes, unless I have to go at a right angle to make things fit. Why the worm drive hate? I mean other than backlash and shear forces on the splines and spiral mitre gears bring generally a better option unless you have a high speed motor and a large reduction ratio.', 'Seriously no kidding people.  At least size them correctly if you‚Äôre going to use them! The amount of belts and winding spools that are out there with undersized motovario worm drives baffles me.  You‚Äôre not saving a penny if I have to come change this out while your line is down. \n\nOh and Hiwin sells harmonic drives in every flavor.']"
When automation took over?,https://www.reddit.com/r/automation/comments/wnbt3v/when_automation_took_over/,automation,It is seems inevitable but what human will do then. How and what can we prepare for our future generation so they will be fully equipped when the time comes. If they loose track the social and economical impact will be   ugly.,"[""How are we to know you are human? Perhaps someone has automated automation doomsdaying and you are an automated program seeking information to fight against the future human resistance? \n\nI don't think it's safe enough to take that gamble by responding."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Accept that you are just not going to do the same thing the same way your entire working life, you will be constantly learning and adapting to changing technology. That is normal, has been ever since industrial revolution started centuries ago.', 'Ive said it for 15 years now, program the robot or be replaced by it.']"
Is there a physical switch that can control multiple Alexa/Google devices?,https://www.reddit.com/r/automation/comments/wmnwu7/is_there_a_physical_switch_that_can_control/,automation,"Hi, 

I love using voice commands but my non english speaking mother cannot.  Is there a way for me to connect like a few Alexa lights in the room to one physical switch so that she can turn them all on and off at once easily?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'So - a light switch? Where all the lights are on the same circuit? Call an electrician.']"
How Exactly Is Automation Intelligence Going to Change the Way that Hiring is Done?,https://www.thetechventures.com/technology/how-exactly-is-automation-intelligence-going-to-change-the-way-that-hiring-is-done/,automation,,
Help with download csv automation,https://www.reddit.com/r/automation/comments/wloput/help_with_download_csv_automation/,automation,"Hi,

(sorry for my bad english, not my mother language).

I'm looking for a way to automate the following process to be done two times per day every monday to friday:

1. Open specific url in chrome
2. Log in with user and password
3. Navigate with several clicks to a ""export"" button that opens the window ""select where to download this file"" (this is not a link to the file, is a button).
4. Select a specific Google Drive folder to download the file.

I only know how a bit of coding in Google Apps Script,  I have tried Axiom, Chromium and Zapier, but I can not find a way to make it select the destination folder and download the file.

I am using a PC, windows, chrome, google drive.

Is there a tool with low code that I can set to do this? I am a one man company and I would prefer not having to pay an expensive license like UiPath or Automation Anywhere.

Also, is there a way to do this fully online so the task gets done even if I don't have my PC turned on?

Thanks for your help","[""Ui.vision can do this without much of a learning curve. It won't download straight to your Google drive, you'll have to download to your computer then upload to Google drive unless you have Google drive mounted on your computer."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
"Noob here, completely lost on this workflow",https://www.reddit.com/r/automation/comments/wld6x4/noob_here_completely_lost_on_this_workflow/,automation,"I have a list of company names, website URL,s and Linkedin URLs.   


I want to find their mailing address but I'm completely lost on how to get started. Spent the whole day trying to figure out solutions and I'm still in the same place as the start.   


So far I've tried to source:   


OpenCorporation - Only free to ""public good"" use cases  
Google Maps: ""can't find search query variables that pull the info in the list to consistently get accurate address

linkedin, which does give accurate addresses, but not always, and worst of all can't scrape it

&#x200B;

\+ many more ideas that didn't work.   


any help would be really appreciated","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""You can create a web scraping workflow and you should be able to scrape the linkedin address. Shouldn't be complicated at all. DM me if you want to chat more about this.""]"
What is the latest RPA solution in the Insurance sector?,https://www.reddit.com/r/automation/comments/wkzk4j/what_is_the_latest_rpa_solution_in_the_insurance/,automation,,"[""What are you looking to do? If you can describe the process and the end goal, I'm pretty sure that I can help you out."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Scraping member list of a slack community's channel and save to a CSV file via Clicknium,https://www.reddit.com/r/automation/comments/wktjqf/scraping_member_list_of_a_slack_communitys/,automation,"# Requirement Statements
Get member list of a slack community's channel and save to a CSV file.
We can start this simple beginner process quickly with [Clicknium](https://www.clicknium.com/).

# Environment Preparations
- Windows 10
- Visual Studio Code 1.69.2
- Clicknium 0.1.2
- Python 3.10.5
- Chrome 103.0.5060.134
> **Remarks:**  
>- Need run this sample in English region. 

# Run this sample
- Follow [clicknium getting started](https://www.clicknium.com/documents) to set up develop environment.
- Clone [sample repo](https://github.com/automation9417/automation-samples.git).
  ```
  git clone https://github.com/automation9417/automation-samples.git
  ```
- Open the folder 'WebSlackScrapingChannelMembersInfor' in Visual Studio code
- Open `sample.py` in visual studio code.
- Fill the sign config in  `sample.py`
  ```python
  sign_method_name="""" #google for Google account, slack_email for slack email.
  sign_in_email_or_phone="""" #google email or slack email
  sign_in_password="""" #account passwword
  ```
- Fill the slack config in `sample.py`
  ```python
  slack_community_url="""" #The URL of the slack community you want to send essage. e.g.""https://example.slack.com""
  slack_channel_name="""" #The name of the channel you want to send message.
  slack_message="""" #The message content
  ```
- Press `F5` to debug the sample or press `CTRL+F5` to run sample.

# Steps

### Assume Slack is not open in chrome, so we need open chrome with the community address firstly.  
   ```python
   #Use following code to open chrome with target url
   browser_tab=clicknium.chrome.open(""https://example.slack.com"") # update the address to your slack community.
   ```
### Assume Slack is not signed in, so we need to sign in slack with Google account or Slack account.  
![](imgs/sign_in_slack.png)
  - Google account sign in
    ```python
    from msilib.schema import Error
    from clicknium import clicknium, locator
    def google_sign_in(email,password):
        clicknium.find_element(locator.websites.slack.google_sign_in_btn).click()
        choose_account_lebel=clicknium.wait_appear(locator.websites.google_account.choose_account_label,wait_timeout=5)
        if choose_account_lebel:
            clicknium.find_element(locator.websites.google_account.use_another_account_btn).click()
        email_or_phone_input=clicknium.wait_appear(locator.websites.google_account.email_or_phone_input,wait_timeout=5)
        if email_or_phone_input:
            email_or_phone_input.set_text(email)
        else:
            error_msg=""email_or_phone_input not found.""
            raise Error(error_msg)
        clicknium.find_element(locator.websites.google_account.email_or_phone_next_btn).click()
        password_input=clicknium.wait_appear(locator.websites.google_account.password_input,wait_timeout=5)
        if password_input:
            password_input.set_text(password)
        else:
            error_msg=""password_input not found.""
            raise Error(error_msg)
        clicknium.find_element(locator.websites.google_account.password_next_btn).click()
    ```
  - Slack account sign in
    ```python
    from clicknium import clicknium, locator

    def slack_email_sign_in(email,password):
        clicknium.find_element(locator.websites.slack.slack_email_input).set_text(email)
        clicknium.find_element(locator.websites.slack.slack_password_input).set_text(password)
        clicknium.find_element(locator.websites.slack.slack_signin_btn).click()
    ```
### Cancel open slack desktop app  
![](imgs/cancle_open_slack_desktop.png)
  - Click `Cancel` button
      ```python
      from clicknium import clicknium, locator
      from clicknium.common.enums import *
      def close_open_desk():
          open_slack_cancel_btn= clicknium.wait_appear(locator.desktops.chrome.open_slack_win_cancel_btn,wait_timeout=10)  
          if open_slack_cancel_btn:
              open_slack_cancel_btn.click(by=MouseActionBy.MouseEmulation) 
      ```
### Choose use slack in browser  
![](imgs/choose_use_slack_in_browser.png)  
  - Click `use Slack in your browser`
      ```python
      from clicknium import clicknium, locator
      def use_slack_in_browser():
          use_slack_in_browser_button=clicknium.wait_appear(locator.websites.slack.use_slack_in_browser_button,wait_timeout=5)      
          if use_slack_in_browser_button:
              use_slack_in_browser_button.click()
      ```    
### Open search channel page.  
![](imgs/all_channels.png)  
  - Send hot key `Ctrl+Shift+L` to open search change page
    ```python
    def browse_channels():
        channels_menu_inner_span=clicknium.wait_appear(locator.websites.app_slack.channels_menu_inner_span,wait_timeout=5) 
        if channels_menu_inner_span:
            clicknium.send_hotkey(""{CTRL}{SHIFT}L"")
            sleep(1)
        else:
            msg=""channels menu not found.""
            raise Error(msg)
    ``` 
### Search and select the target channel.  
![](imgs/slack_search_select_channel.png)  
  - Enter the target channel name  
  - Click the `Search` icon  
  - Choose sort `A to Z`  
  ![](imgs/choose_sort_way.png)  

  - Select the target channel  
    ```python
    from msilib.schema import Error
    from clicknium import clicknium, locator
    def search_and_select_channel(channel_name):
        clicknium.find_element(locator.websites.app_slack.search_channel_tbx).clear_text()
        clicknium.find_element(locator.websites.app_slack.search_channel_tbx).set_text(channel_name)
        clicknium.find_element(locator.websites.app_slack.search_channel_btn).click()
        clicknium.find_element(locator.websites.app_slack.channel_sort_btn).click()
        clicknium.find_element(locator.websites.app_slack.sort_atoz_btn).click()
        matched_result_span=clicknium.wait_appear(locator.websites.app_slack.matched_result_span,{""channel_name"": channel_name})
        if matched_result_span:
            matched_result_span.click()
        else:
            msg=""No matched channel for ""+channel_name
            raise Error(msg)
    ```
### Get channel member count.  
![](imgs/channel_member_count.png)  
  - Use [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text) to get the member count  
    ```python
    from clicknium import clicknium, locator
    def get_member_count()->int:
        member_count=clicknium.find_element(locator.websites.app_slack.channel_member_count_span).get_text()
        member_count_int=0
        try:
            member_count_int=int(member_count.strip(' ').replace(',',''))
        except:
            member_count=clicknium.find_element(locator.websites.app_slack.channel_member_count_span).get_text()
            member_count_int=int(member_count.strip(' ').replace(',',''))
        return member_count_int
    ```
### Open members list window
![](imgs/open_member_list_win.png)  
  - Click `View members` to open the window
  - Select the `Members` tab
    ```python
    from msilib.schema import Error
    from clicknium import clicknium, locator

    def open_member_list_win():
        clicknium.find_element(locator.websites.app_slack.channel_member_count_span).click()
        member_list_win_members_tab=clicknium.wait_appear(locator.websites.app_slack.member_list_win_members_tab)
        if member_list_win_members_tab:
            member_list_win_members_tab.click()
        else:
            msg=""members tab not found.""
            raise Error(msg)
    ```
### Get members name one by one
![](imgs/get_member_name_one_by_one.png)  
  - Focus the search text box
  - Send `Down` hot key to select `Add People`
  - Loop to get member name
    - Send `Down` hot key to select member
    - Use [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text) to get the member name
    - Save name to CSV file  
  - Close member list window
    ```python
    import csv
    from time import sleep
    from clicknium import clicknium, locator

    def get_member_list(channel_name) -> str:
        browse_channels()
        search_and_select_channel(channel_name)
        member_count=get_member_count()
        open_member_list_win()

        clicknium.find_element(locator.websites.app_slack.channel_member_search_box).set_focus()
        add_peeple_btn=clicknium.wait_appear(locator.websites.app_slack.add_peeple_btn)
        if add_peeple_btn:
            clicknium.send_hotkey('{DOWN}')
            sleep(1)
        else:
            msg=""Add people button not found.""
            raise(msg)

        member_name_label_first=clicknium.wait_appear(locator.websites.app_slack.member_name_label_first)
        if not member_name_label_first:
            msg=""Member name label first not found.""
            raise(msg)

        csv_file_name=channel_name+'_names.csv'    
        with open(csv_file_name, 'w',encoding='utf-8', newline='') as csvfile:
            fieldnames = ['name']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            for num in range(1,member_count+1):
                clicknium.send_hotkey('{DOWN}')
                sleep(1)
                name=clicknium.find_element(locator.websites.app_slack.member_name_label_focus).get_text()
                writer.writerow({'name': name})         

        clicknium.find_element(locator.websites.app_slack.member_list_win_close_btn).click()
        return csv_file_name
    ```
### Get member email one by one
  - Loop  
    - Open members list window 
    ![](imgs/get_member_name_one_by_one.png)   
      ```python  
      open_member_list_win()
      ```  
    - Enter the member name to find member   
    - Click the target member to open the member profile  
    ![](imgs/open_member_profile.png)     
    - Use [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text) to get the member email in profile section  
    - Close the member profile section  
    ![](imgs/get_email_from_profile.png)  
      ```python
      import csv
      from msilib.schema import Error
      from threading import local
      from clicknium import clicknium, locator 
      from channel_operations import browse_channels, open_member_list_win, search_and_select_channel

      def get_member_email(name) -> str:
          open_member_list_win()
          clicknium.find_element(locator.websites.app_slack.channel_member_search_box).set_text(name)
          member_name_label=clicknium.wait_appear(locator.websites.app_slack.member_name_label_first,{""member_name"":name},wait_timeout=5)
          email=None
          if member_name_label:
              member_name_label.click()
              member_email_link=clicknium.wait_appear(locator.websites.app_slack.member_email_link,wait_timeout=5)
              if member_email_link:
                  email=member_email_link.get_text()
              else:
                  email=""email is not visible.""
              clicknium.find_element(locator.websites.app_slack.profile_close_btn).click()
          else:
              email=""user not found.""
              clicknium.find_element(locator.websites.app_slack.member_list_win_close_btn).click()
          return email

      def get_member_list_email(channel_name,name_list_csv_file) -> str:
          browse_channels()
          search_and_select_channel(channel_name)
          
          ret_csv_file_name=channel_name+'_member_emails.csv'
          with open(ret_csv_file_name, 'w',encoding='utf-8', newline='') as csvfile:
              fieldnames = ['name',""email""]
              writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
              writer.writeheader()

              with open(name_list_csv_file,encoding='utf-8', mode ='r')as file:
                  csvFile = csv.reader(file)
                  index=-1
                  for line in csvFile:
                      index=index+1
                      if index==0:
                          continue
                      name=line[0]
                      email=get_member_email(name)
                      if email:
                          writer.writerow({'name': name,""email"":email}) 
          return ret_csv_file_name
      ```
### Sign out.  
![](imgs/slack_sign_out.png)  
  - Click user avatar
  - Click `Sign out` 
    ```python
    from clicknium import clicknium, locator

    def sign_out():
        user_avatar_btn=clicknium.wait_appear(locator.websites.app_slack.user_avatar_btn,wait_timeout=5)
        if user_avatar_btn:
            user_avatar_btn.click()
            clicknium.find_element(locator.websites.app_slack.sign_out_btn).click()
    ```
### Close opened browser tab.  
   ```python  
   browser_tab.close()# close the opened browser tab.
   ``` 
# Tips 
- Pass variable to the locator  
In this sample channel name is passed to the `matched_result_span` locator as following
  - Define variable in locator  
   ![](imgs/pass_variable.png)  
  -  Pass variable in code
      ```python
      matched_result_span=clicknium.wait_appear(locator.websites.app_slack.matched_result_span,{""channel_name"": channel_name})
      ```
- Use wildcard in locator  
In this sample `channel_sort_btn` locator's sInfo is updated end with * as following
![](imgs/sort_btn_wildcard.png)  
# Concepts  
[Clicknium](https://www.clicknium.com/) provides excellent ways of the recorder and the concept of the Locator, which helps you finish developing efficiently without lots of details. Hence it is worth getting to know the concepts below.
1. [Locator](https://www.clicknium.com/documents/concepts/locator)
2. [Recorder](https://www.clicknium.com/documents/tutorial/recorder/)  
> **Functions involved**
>- [click](https://www.clicknium.com/documents/references/python/uielement/click)
>- [set_text](https://www.clicknium.com/documents/references/python/uielement/set_text)
>- [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text)
>- [open browser](https://www.clicknium.com/documents/references/python/webdriver/open)
>- [wait_appear](https://www.clicknium.com/documents/references/python/globalfunctions/wait_appear)
>- [activate browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/activate)
>- [close browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/close)
>- [find_element](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/find_element)
>- [set_focus](https://www.clicknium.com/documents/references/python/uielement/set_focus)
>- [get_property](https://www.clicknium.com/documents/references/python/uielement/get_property)
>- [send_hotkey](https://www.clicknium.com/documents/references/python/uielement/send_hotkey)   
# Get Started
1. Create a new folder. Open Visual Studio Code and press the keyboard shortcut `Ctrl+Shift+P` to select [Clicknium: Sample](https://www.clicknium.com/documents/tutorial/vscode/project_management) and select the newly created folder.
2. pip install clicknium
3. pip install pyperclip
4. Copy the '.locator' folder under 'WebSlackSendMessage' to your new created folder
5. Open `sample.py` and follow the steps above","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'The steps are long and require patience']"
The Best Sales Automation Software of 2022,https://saaslist.com/sales-automation-software/,automation,,
Get answers from answer sheet with OCR,https://www.reddit.com/r/automation/comments/wkfizk/get_answers_from_answer_sheet_with_ocr/,automation,"I need to get the answers from an answer sheet that looks like [this](https://remarksoftware.com/wp-content/uploads/2015/07/120-Question-Answer-Sheet_Page_1.jpg). I assume I could use some form of OCR that would interpret the position of the dots as a, b, c, etc but I don't really know how to search for that, and if it even exists. I'm not interested in programming it from scratch, because although it could be fun, I don't have enough time for it.

Edit: I found a better search term: ""OMR"" or ""Optical Mark Recognition"" and found some programs that do it. Will give FormRead a try but I accept suggestions.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
How much RPA will cost and how long will implantation take?,https://www.reddit.com/r/automation/comments/wk9q89/how_much_rpa_will_cost_and_how_long_will/,automation,Please share your idea,"['How much does a cake cost?\n\nIt depends on the size, ingredients, work required to make it, etc.\n\nOne cannot just estimate time without knowing a LOT of information.', 'Costs depend on who makes the implementation and how big your company is, but can easily exceed hundreds of thousands of dollars.\n\nThe time can also vary greatly. It can take anywhere between a few months and a few years to get a first successful bot to be operational‚Ä¶ \n\nMy recommendation: don‚Äôt use RPA. They are pretty much a scam industry. Invest in improving your core software or develop coded automation in c# or python for a tiny fraction of the costs.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Scraping someone's recent tweets via python UI automation module Clicknium,https://www.reddit.com/r/automation/comments/wixouh/scraping_someones_recent_tweets_via_python_ui/,automation,"# Requirement Statements
Get someone's recent tweets and save to a CSV file.
We can start this simple beginner process quickly with [Clicknium](https://www.clicknium.com/).

# Environment Preparations
- Windows 10
- Visual Studio Code 1.69.2
- Clicknium 0.1.3
- Python 3.10.5
- Chrome 103.0.5060.134
> **Remarks:**  
>- Need run this sample in English region. 

# Run this sample
- Follow [clicknium getting started](https://www.clicknium.com/documents) to set up develop environment.
- Clone [sample repo](https://github.com/automation9417/automation-samples.git).
  ```
  git clone https://github.com/automation9417/automation-samples.git
  ```
- Open the folder 'ScrapingPeopleRecentTweets' in Visual Studio code
- Open `sample.py` in visual studio code.
- Fill the sign config in  `sample.py`
  ```python
    sign_in_method_name="""" #google for Google account, twiiter_account for tweet account.
    account="""" #google email/phone, twitter username/email/phone
    verify_account=""""#Sign in with tweet account, may need secondary verification. e.g. sign in with twitter email use username/phone to verify.
    password="""" # password
  ```
- Fill a Twitter user name you want to scrape Tweets from  in `sample.py`
  ```python
  scrape_user_name="""" # The Twitter name you want to scrape Tweets from, must start with @. e.g. @exemple
  ```
- Press `F5` to debug the sample or press `CTRL+F5` to run sample.

# Steps

1. Assume Twitter is not open in chrome, so we need open chrome with the explore address firstly.  
   ```python
   #Use following code to open chrome with target url
   browser_tab=clicknium.chrome.open(""https://twitter.com/explore"") 
   ```
2. Assume Twitter is not signed in, so we need to sign in twitter with Google account or twitter account.  
![](https://github.com/automation9417/automation-samples/raw/main/ScrapingPeopleRecentTweets/imgs/sign_in_twitter.png)
  - Google account sign in
    ```python
    from time import sleep
    from clicknium import clicknium, locator
    from clicknium.common.enums import *
    def google_sign_in(email_or_phone,password):
        clicknium.find_element(locator.websites.twitter.login_btn).click()
        sleep(2)
        clicknium.send_hotkey(""{ESC}"")
        continue_with_google_btn=clicknium.wait_appear(locator.websites.twitter.continue_with_google_btn,wait_timeout=5)
        if continue_with_google_btn:
            continue_with_google_btn.click(by= MouseActionBy.MouseEmulation)
        else:
            clicknium.find_element(locator.websites.twitter.continue_as_x_btn).click(by= MouseActionBy.MouseEmulation)
            clicknium.find_element(locator.websites.google_accounts.use_other_account_btn).click()
        clicknium.find_element(locator.websites.google_accounts.email_or_phone_input).set_text(email_or_phone)
        clicknium.find_element(locator.websites.google_accounts.email_or_phone_next_btn).click()
        clicknium.find_element(locator.websites.google_accounts.password_input).set_text(password)
        clicknium.find_element(locator.websites.google_accounts.password_next_btn).click()
    ```
  - Twitter account sign in
    ```python
    from time import sleep
    from clicknium import clicknium, locator
    from clicknium.common.enums import *

    def sign_in_with_twitter_account(email_or_phone_or_username,verify_account,password):
        clicknium.find_element(locator.websites.twitter.login_btn).click()
        sleep(2)
        clicknium.send_hotkey(""{ESC}"")
        clicknium.find_element(locator.websites.twitter.twitter_account_input).set_text(email_or_phone_or_username)
        clicknium.find_element(locator.websites.twitter.login_next_btn).click()
        twitter_verify_input=clicknium.wait_appear(locator.websites.twitter.twitter_verify_input,wait_timeout=5)
        if twitter_verify_input:
            twitter_verify_input.set_text(verify_account)
            clicknium.find_element(locator.websites.twitter.login_next_btn).click()
        clicknium.find_element(locator.websites.twitter.twitter_password_input).set_text(password)
        clicknium.find_element(locator.websites.twitter.login_form_login_btn).click()
    ```    
3. Search and select a twitter user by username.  
![](https://github.com/automation9417/automation-samples/raw/main/ScrapingPeopleRecentTweets/imgs/search_target_user.png)   

    ```python
    from time import sleep
    from msilib.schema import Error
    from clicknium import clicknium, locator,ui
    from clicknium.common.enums import *
    import csv

    def search_and_select_user(username):
        clicknium.find_element(locator.websites.twitter.explore_menu).click()
        clicknium.find_element(locator.websites.twitter.search_text_box_input).click(by= MouseActionBy.MouseEmulation)
        clicknium.find_element(locator.websites.twitter.search_text_box_input).set_text(username)
        sleep(1)
        clicknium.send_hotkey('{ENTER}')
        
        search_people_tab=clicknium.wait_appear(locator.websites.twitter.search_people_tab,wait_timeout=10)
        if search_people_tab:
            search_people_tab.click()
        else:
            msg=""Search people tab not found.""
            raise Error(msg)
        
        target_search_people=clicknium.wait_appear(locator.websites.twitter.target_search_people,{""user_name"":username}, wait_timeout=10)
        if target_search_people:
            target_search_people.click()
        else:
            msg=""People:""+username+"" not found.""
            raise Error(msg)
    ```
4. Get user recent tweets.  
![](https://github.com/automation9417/automation-samples/raw/main/ScrapingPeopleRecentTweets/imgs/get_tweets.png)  
  - Use [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text) to get the tweets publish date, content and link, the result will be saved to a CSV file.  
    ```python
    from time import sleep
    from msilib.schema import Error
    from clicknium import clicknium, locator,ui
    from clicknium.common.enums import *
    import csv


    def get_user_recent_tweets(username)->str:
        search_and_select_user(username)

        clicknium.find_element(locator.websites.twitter.user_tweets_tab).click()
        tweet_article=clicknium.wait_appear(locator.websites.twitter.tweet_article, wait_timeout=10)
        if not tweet_article:
            msg=""Tweet not found.""
            raise Error(msg) 
        
        ret_csv_file_name=username+'_recent_tweets.csv'
        with open(ret_csv_file_name, 'w', newline='',encoding='utf-8') as csvfile:
            fieldnames = ['publish_date',""content"",""link""]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            tweet_articles=clicknium.find_elements(locator.websites.twitter.tweet_article)
            #Start for loop
            for index in range(1,tweet_articles.__len__()+1):
                selected_tweet_article=clicknium.wait_appear(locator.websites.twitter.selected_tweet_article,{""index"":index},wait_timeout=5) 
                if not selected_tweet_article:      
                    continue
                tweet_text=clicknium.wait_appear(locator.websites.twitter.tweet_text,{""index"":index},wait_timeout=2) 
                content=""""
                if tweet_text:
                    content=tweet_text.get_text()

                tweet_card=clicknium.wait_appear(locator.websites.twitter.tweet_card,{""index"":index},wait_timeout=2) 
                link=""""
                if tweet_card:
                    link=tweet_card.get_property(""href"")
                
                tweet_publish_date=clicknium.wait_appear(locator.websites.twitter.tweet_publish_date,{""index"":index},wait_timeout=2) 
                publish_time=""""
                if tweet_publish_date:
                    publish_time=tweet_publish_date.get_property(""datetime"")
                
                writer.writerow({'publish_date':publish_time,""content"":content,""link"":link}) 
            #End for loop
    ``` 
5. Sign out.  
![](https://github.com/automation9417/automation-samples/raw/main/ScrapingPeopleRecentTweets/imgs/sign_out.png)  
    ```python
    from clicknium import clicknium, locator

    def sign_out():
        user_avatar_btn=clicknium.wait_appear(locator.websites.app_slack.user_avatar_btn,wait_timeout=5)
        if user_avatar_btn:
            user_avatar_btn.click()
            clicknium.find_element(locator.websites.app_slack.sign_out_btn).click()
    ```
6. Close opened browser tab.  
   ```python  
   browser_tab.close()# close the opened browser tab.
   ``` 
# Tips 
- Pass variable to the locator  
In this sample user name is passed to the `target_search_people` locator as following
  - Define variable in locator  
   ![](https://github.com/automation9417/automation-samples/raw/main/ScrapingPeopleRecentTweets/imgs/pass_variable.png)  
  -  Pass variable in code
      ```python
      target_search_people=clicknium.wait_appear(locator.websites.twitter.target_search_people,{""user_name"":username}, wait_timeout=10)
      ``` 
# Concepts  
[Clicknium](https://www.clicknium.com/) provides excellent ways of the recorder and the concept of the Locator, which helps you finish developing efficiently without lots of details. Hence it is worth getting to know the concepts below.
1. [Locator](https://www.clicknium.com/documents/concepts/locator)
2. [Recorder](https://www.clicknium.com/documents/tutorial/recorder/)  
> **Functions involved**
>- [click](https://www.clicknium.com/documents/references/python/uielement/click)
>- [set_text](https://www.clicknium.com/documents/references/python/uielement/set_text)
>- [get_text](https://www.clicknium.com/documents/references/python/uielement/get_text)
>- [open browser](https://www.clicknium.com/documents/references/python/webdriver/open)
>- [wait_appear](https://www.clicknium.com/documents/references/python/globalfunctions/wait_appear)
>- [activate browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/activate)
>- [close browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/close)
>- [find_element](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/find_element)
>- [set_focus](https://www.clicknium.com/documents/references/python/uielement/set_focus)
>- [get_property](https://www.clicknium.com/documents/references/python/uielement/get_property)
>- [send_hotkey](https://www.clicknium.com/documents/references/python/uielement/send_hotkey)   
# Get Started
1. Create a new folder. Open Visual Studio Code and press the keyboard shortcut `Ctrl+Shift+P` to select [Clicknium: Sample](https://www.clicknium.com/documents/tutorial/vscode/project_management) and select the newly created folder.
2. pip install clicknium
3. Copy the '.locator' folder under 'ScrapingPeopleRecentTweets' to your new created folder
4. Open `sample.py` and follow the steps above","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Hope this sample is helpful !!!']"
Automated Temperature for Bearded Dragon Enclosure,https://www.reddit.com/r/automation/comments/wh96b0/automated_temperature_for_bearded_dragon_enclosure/,automation,"Hey guys,

I'm wondering what it would take to be able to basically have the perfect temperature inside my reptile enclosure all the time.

I don't know a lot about this stuff but basically..a bearded dragons enclosure should kinda have 3 zones.

A cooler side, a warm middle side, and then a hotter side where he would usually bask in more powerful light/UV.

What I would like to do is have maybe a sensor for the cool side and the hottest side and depending on the temps it would dim the light/turn it up and for the cool side if too cold it could turn on a ceramic heat emitter and adjust it up or down in power.

How hard would this be to do? And how would one go about it? Thanks","[""You can absolutely do this with an Arduino and a few sensors and relays. You could go down the road of analog current control through circuitry, but it's easier to just turn your heaters and uv lamps on/off based on set points. Best combination of simple and cheap for this kind of thing would be DHT11 sensors for temp and relays to control your heaters. An Arduino Uno is well enough equipped to handle everything, but if you want to add an lcd you'll probably want a mega 2560. There's really a ton of things you can do with this, including remote control via a WiFi board and MQTT."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Just get an inkbird controller off Amazon. Handles heating and cooling.']"
Clicknium Automation Sample Solution - Data Migration,https://www.reddit.com/r/automation/comments/wfwvul/clicknium_automation_sample_solution_data/,automation,"Many enterprises need data migration solution. For example, if IT system is upgraded,there is need to migrate data from legacy system to upgraded system. This is a sample for employee data migration solution with[Clicknium](https://www.clicknium.com/) desktop&web automation. Here is the details: migrate employee data from a legacy thick client application into new HR system. The manual steps are as the follows:

* query data from legacy client app based on employee id.
* query more meta data of employee through internal REST API.
* fill the data of employee to the new HR system(web portal).

# Run this sample

* follow [clicknium getting started](https://www.clicknium.com/documents/quickstart) to set up develop environment.
* clone this sample repo
* download and unzip [legacy thick client app](https://github.com/AutomationAnywhere/Employee-Data-Migration/raw/master/EmployeeList.zip) to local repo folder.
* clone [sample repo](https://github.com/clicknium/clicknium-samples).

&#8203;

    git clone https://github.com/clicknium/clicknium-samples.git 

* open the folder 'EmployeeDataMigration' in Visual Studio Code
* through pipinstall the dependent packages

requestsis used to query data through internal REST api.

    pip install requests 

* open app.pyin Visual Studio Code .
* press F5to debug the sample or CTRL+F5to run the sample. You will see the result as below:

https://preview.redd.it/4kkbpsyo1of91.png?width=522&format=png&auto=webp&v=enabled&s=6402101b350919763c67b1d00f041b6e135705a9

# The Purpose of the Sample

* open the legacy client application with subprocess module, and it will be used to query data later.

&#8203;

    current_dir = os.path.dirname(os.path.abspath(__file__)) employeeList_exe = os.path.join(current_dir, ""EmployeeList.exe"") process = subprocess.Popen(employeeList_exe) 

* open the browser with Clicknium python module and open new HR system. In this sample microsoft edge browser will be used.

&#8203;

    tab = cc.edge.open(""https://developer.automationanywhere.com/challenges/automationanywherelabs-employeedatamigration.html"") 

When the browser is opened, it will return to the edge tab/page.

* get the employee id in new HR system with Clicknium web automaton

&#8203;

    employee_id = tab.find_element(locator.employeedatamigration.developer.text_employeeid).get_text() 

* based on the captured employee\_id above, find the employee information on legacy client application with Clicknium desktop automation.

&#8203;

    cc.ui(locator.employeedatamigration.employee.edit_txtempid).set_text(employee_id, by='set-text')
    cc.ui(locator.employeedatamigration.employee.button_btnsearch).click(by='control-invocation')
    item[""first_name""] = cc.ui(locator.employeedatamigration.employee.edit_txtfirstname).get_text()
    item[""last_name""] = cc.ui(locator.employeedatamigration.employee.edit_txtlastname).get_text()
    item[""email_id""] = cc.ui(locator.employeedatamigration.employee.edit_txtemailid).get_text()
    item[""city""] = cc.ui(locator.employeedatamigration.employee.edit_txtcity).get_text()
    ... ...

automatically set text for employee\_id and click search button to capture all information such as first\_name, last\_name etc of this employee In desktop application operation,Clicknium uses mouse&keyboard simulation by default . In this legacy application, the input and button control support control invocation, so we can use it by pass parameter byin click and set\_text api.

* through requestsmodule, send http request to get the extra information(phone number, start date ) of the employee.

&#8203;

    response = requests.get(api_url + employee_id)
    print(datetime.datetime.now().strftime(""%H:%M:%S"") + "" get response"")
    obj = json.loads(response.content.decode('UTF-8'))
    item[""phoneNumber""] = obj[""phoneNumber""]
    item[""startDate""] = obj[""startDate""]

* fill the data into new HR system through clicknium web automation.

&#8203;

    tab.find_element(locator.employeedatamigration.developer.text_firstname).set_text(item[""first_name""])
    tab.find_element(locator.employeedatamigration.developer.text_lastname).set_text(item[""last_name""])
    tab.find_element(locator.employeedatamigration.developer.text_phone).set_text(item[""phoneNumber""])
    tab.find_element(locator.employeedatamigration.developer.text_email).set_text(item[""email_id""])
    ... ...
    tab.find_element(locator.employeedatamigration.developer.button_submitbutton).click()

In code above, you can see:

* The locator is separated from code , so the locator store can be managed independently. If the new HR system is upgradeD, the locator will changed and the locator store will be updated as well.
* As Clicknium provides unified API for both desktop and web automation, you can write automation code in the same way for browser and windows application.

# Locator

The [Locator](https://www.clicknium.com/documents/concepts/locator) is the identifier of UI element , which can be recorded or edited with [clicknium vs code extension ](https://marketplace.visualstudio.com/items?itemName=ClickCorp.clicknium).

In this sample, you can open the locator in Visual Studio Code , for example:

https://preview.redd.it/jhpfwk7v1of91.png?width=1211&format=png&auto=webp&v=enabled&s=8a4675d5acb9b5b35838548a8be0ccb8f62476e1

# Compare with Selenium & Playwright

* The web driver needs to be downloaded in Selenium matching exactly the browser . In this example, the Edge browser version is 103.0.1264.62, so there is a need to download the same version MS Edge web driver first.
* Selenium and playwright can only support web automation. So when it comes to employee data migration solution, you can operate on the legacy client app by using another library such as pywinauto.

# More samples

You can refer to more automation samples or solutions in [clicknium github samples](https://github.com/clicknium/clicknium-samples). Send [email](mailto:support@clicknium.com) to us or [Join Slack](https://join.slack.com/t/clicknium/shared_invite/zt-1cfxsstw7-s0CeJdhyg5wQ1h7_KKc6QQ).","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Python send a message to a slack community's channel.,https://www.reddit.com/r/automation/comments/wezzbp/python_send_a_message_to_a_slack_communitys/,automation,"# Requirement Statements
Send a message to a slack community's channel.
We can start this simple beginner process quickly with [Clicknium](https://www.clicknium.com/).

# Environment Preparations
- Windows 10
- Visual Studio Code 1.69.2
- Clicknium 0.1.3
- Python 3.10.5
- Chrome 103.0.5060.134
- Python package pyperclip
> **Remarks:**  
>- Need run this sample in English region. 

# Run this sample
- Follow [clicknium getting started](https://www.clicknium.com/documents/quickstart) to set up develop environment.
- Clone [sample repo](https://github.com/automation9417/automation-samples.git).
  ```
  git clone https://github.com/automation9417/automation-samples.git
  ```
- Open the folder 'WebSlackSendMessage' in Visual Studio code
- Open `sample.py` in visual studio code.
- Fill the sign config in  `sample.py`
  ```python
  sign_method_name="""" #google for Google account, slack_email for slack email.
  sign_in_email_or_phone="""" #google email or slack email
  sign_in_password="""" #account passwword
  ```
- Fill the slack config in `sample.py`
  ```python
  slack_community_url="""" #The URL of the slack community you want to send essage. e.g.""https://example.slack.com""
  slack_channel_name="""" #The name of the channel you want to send message.
  slack_message="""" #The message content
  ```
- Press `F5` to debug the sample or press `CTRL+F5` to run sample.

# Steps

1. Assume Slack is not open in chrome, so we need open chrome with the community address firstly.  
   ```python
   #Use following code to open chrome with target url
   browser_tab=clicknium.chrome.open(""https://example.slack.com"") # update the address to your slack community.
   ```
2. Assume Slack is not signed in, so we need to sign in slack with Google account or Slack account.  
  - Google account sign in
    ```python
    from msilib.schema import Error
    from clicknium import clicknium, locator
    def google_sign_in(email,password):
        clicknium.find_element(locator.websites.slack.google_sign_in_btn).click()
        choose_account_lebel=clicknium.wait_appear(locator.websites.google_account.choose_account_label,wait_timeout=5)
        if choose_account_lebel:
            clicknium.find_element(locator.websites.google_account.use_another_account_btn).click()
        email_or_phone_input=clicknium.wait_appear(locator.websites.google_account.email_or_phone_input,wait_timeout=5)
        if email_or_phone_input:
            email_or_phone_input.set_text(email)
        else:
            error_msg=""email_or_phone_input not found.""
            raise Error(error_msg)
        clicknium.find_element(locator.websites.google_account.email_or_phone_next_btn).click()
        password_input=clicknium.wait_appear(locator.websites.google_account.password_input,wait_timeout=5)
        if password_input:
            password_input.set_text(password)
        else:
            error_msg=""password_input not found.""
            raise Error(error_msg)
        clicknium.find_element(locator.websites.google_account.password_next_btn).click()
    ```
  - Slack account sign in
    ```python
    from clicknium import clicknium, locator

    def slack_email_sign_in(email,password):
        clicknium.find_element(locator.websites.slack.slack_email_input).set_text(email)
        clicknium.find_element(locator.websites.slack.slack_password_input).set_text(password)
        clicknium.find_element(locator.websites.slack.slack_signin_btn).click()
    ```
3. Cancel open slack desktop app  
  - Click `Cancel` button
      ```python
      from clicknium import clicknium, locator
      from clicknium.common.enums import *
      def close_open_desk():
          open_slack_cancel_btn= clicknium.wait_appear(locator.desktops.chrome.open_slack_win_cancel_btn,wait_timeout=10)  
          if open_slack_cancel_btn:
              open_slack_cancel_btn.click(by=MouseActionBy.MouseEmulation) 
      ```
4. Choose use slack in browser  
![](imgs/choose_use_slack_in_browser.png)  
  - Click `use Slack in your browser`
      ```python
      from clicknium import clicknium, locator
      def use_slack_in_browser():
          use_slack_in_browser_button=clicknium.wait_appear(locator.websites.slack.use_slack_in_browser_button,wait_timeout=5)      
          if use_slack_in_browser_button:
              use_slack_in_browser_button.click()
      ```    
5. Open search channel page.  
  - Send hot key `Ctrl+Shift+L` to open search change page
    ```python
    def browse_channels():
        channels_menu_inner_span=clicknium.wait_appear(locator.websites.app_slack.channels_menu_inner_span,wait_timeout=5) 
        if channels_menu_inner_span:
            clicknium.send_hotkey(""^+l"")
            sleep(1)
        else:
            msg=""channels menu not found.""
            raise Error(msg)
    ``` 
6. Search and select the target channel.  
  - Enter the target channel name  
  - Click the `Search` icon  
  - Choose sort `A to Z`  
  ![](imgs/choose_sort_way.png)  

  - Select the target channel  
    ```python
    from msilib.schema import Error
    from clicknium import clicknium, locator
    def search_and_select_channel(channel_name):
        clicknium.find_element(locator.websites.app_slack.search_channel_tbx).set_text(channel_name)
        clicknium.find_element(locator.websites.app_slack.search_channel_btn).click()
        clicknium.find_element(locator.websites.app_slack.channel_sort_btn).click()
        clicknium.find_element(locator.websites.app_slack.sort_atoz_btn).click()
        matched_result_span=clicknium.wait_appear(locator.websites.app_slack.matched_result_span,{""channel_name"": channel_name})
        if matched_result_span:
            matched_result_span.click()
        else:
            msg=""No matched channel for ""+channel_name
            raise Error(msg)
    ```
7. Enter the message and send.  
  - Enter message
  - Click the `Send` icon
    ```python
    from clicknium import clicknium, locator
    import pyperclip

    def send_message(channel_name, message):
        navigate_to_browser_channel_page()
        search_and_select_channel(channel_name)
        clicknium.find_element(locator.websites.app_slack.channel_message_input).set_focus()
        clicknium.send_hotkey('^a')
        clicknium.send_hotkey('^x')
        pyperclip.copy(message)
        clicknium.send_hotkey('^v')
        clicknium.find_element(locator.websites.app_slack.send_message_btn).click()
    ```
8. Sign out.  
  - Click user avatar
  - Click `Sign out` 
    ```python
    from clicknium import clicknium, locator

    def sign_out():
        user_avatar_btn=clicknium.wait_appear(locator.websites.app_slack.user_avatar_btn,wait_timeout=5)
        if user_avatar_btn:
            user_avatar_btn.click()
            clicknium.find_element(locator.websites.app_slack.sign_out_btn).click()
    ```
9. Close opened browser tab.  
   ```python  
   browser_tab.close()# close the opened browser tab.
   ``` 
# Tips 
- Pass variable to the locator  
In this sample channel name is passed to the `matched_result_span` locator as following
  - Define variable in locator  
   ![](imgs/pass_variable.png)  
  -  Pass variable in code
      ```python
      matched_result_span=clicknium.wait_appear(locator.websites.app_slack.matched_result_span,{""channel_name"": channel_name})
      ```
- Use wildcard in locator  
In this sample `channel_sort_btn` locator's sInfo is updated end with * 
# Concepts  
[Clicknium](https://www.clicknium.com/) provides excellent ways of the recorder and the concept of the Locator, which helps you finish developing efficiently without lots of details. Hence it is worth getting to know the concepts below.
1. [Locator](https://www.clicknium.com/documents/concepts/locator)
2. [Recorder](https://www.clicknium.com/documents/tutorial/recorder/)  
> **Functions involved**
>- [click](https://www.clicknium.com/documents/references/python/uielement/click)
>- [set_text](https://www.clicknium.com/documents/references/python/uielement/set_text)
>- [open browser](https://www.clicknium.com/documents/references/python/webdriver/open)
>- [wait_appear](https://www.clicknium.com/documents/references/python/globalfunctions/wait_appear)
>- [activate browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/activate)
>- [close browser tab](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/close)
>- [find_element](https://www.clicknium.com/documents/references/python/webdriver/browser/browsertab/find_element)
>- [set_focus](https://www.clicknium.com/documents/references/python/uielement/set_focus)
>- [get_property](https://www.clicknium.com/documents/references/python/uielement/get_property)
>- [send_hotkey](https://www.clicknium.com/documents/references/python/uielement/send_hotkey)  
# Get Started
1. Create a new folder. Open Visual Studio Code and press the keyboard shortcut `Ctrl+Shift+P` to select [Clicknium: Sample](https://www.clicknium.com/documents/tutorial/vscode/project_management) and select the newly created folder.
2. pip install clicknium
3. pip install pyperclip
4. Copy the '.locator' folder under 'WebSlackSendMessage' to your new created folder
5. Open `sample.py` and follow the steps above","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'What do you think of this scene?', 'More examples are coming']"
Some popular UI automation tools/ softwares,https://www.reddit.com/r/automation/comments/weunlp/some_popular_ui_automation_tools_softwares/,automation,"I would like to list some UI automation tools as following.

1. UIPath
2. Selenium
3. TestProject
4. UFT (United Functional Testing)/QTP (Quick Test Professional)
5. Clicknium
6. Katalon Studio
7. TestComplete
8. TestIM
9. Ranorex Studio
10. Squish
11. Rapise
12. AutonomIQ
13. Robocorp
14. Playwright","['Ui. Vision (selenium). Free and open source.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Can you completely replace selenium with clicknium?', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Wrk Technologies too.']"
Is phone automation out of control?,https://www.truecaller.com/blog/insights/truecaller-insights-top-20-countries-affected-by-spam-calls-in-2020-2,automation,,
Sample to demostrate clicknium web automation solution - customer onboarding,https://www.reddit.com/r/automation/comments/wd5kt4/sample_to_demostrate_clicknium_web_automation/,automation," 

This is a sample of customer onboarding solution through [clicknium](https://www.clicknium.com/) web automation .

For one enterprise, customer onboarding has a significant impact on whether a customer keeps using your product or not, you may define the customer onboarding process in your comany internally, for example, need add new customer information into CRM(customer relationship management) system. If you can automatically to do customer onboarding process, it should improve the efficiency significantly. Here we demo one custoemr onboarding automation solution:

* load the missing customer's information from CSV file .
* open CRM system.
* iterate the records in CVS file , fill into CRM form and register each customer.

# Run this sample

* follow [clicknium getting started](https://www.clicknium.com/) to set up develop environment.
* clone [sample repo](https://github.com/clicknium/clicknium-samples).

&#8203;

    git clone https://github.com/clicknium/clicknium-samples 

* open the folder 'CustomerOnboarding' in Visual Studio code
* through pip  
 install the dependenct packages

requests  
 is used to download the CSV file and pandas  
 is used to read CSV file .

    pip install requests pip install pandas 

* open app.py  
 in Visual Studio Code .
* press F5  
 to debug the sample or press CTRL+F5  
 to run sample.

You will see the result:

https://preview.redd.it/wqeiug19g0f91.jpg?width=509&format=pjpg&auto=webp&v=enabled&s=340de9b8ee0349a117d3ae47195469daaaf9a8f9

# What the sample do

* open the testing CRM web portal.
* get the url of CSV to be download.
* download the CSV file .

&#8203;

    tab = cc.edge.open(""https://developer.automationanywhere.com/challenges/automationanywherelabs-customeronboarding.html"")
    url = tab.find_element(locator.customeronboarding.developer.a_downloadcsv).get_property(""href"")
    excelFile = requests.get(url)
    temp_file = os.path.join(os.getcwd(), 'missing.csv')
    open(temp_file, 'wb').write(excelFile.content)
    data = pd.read_csv(temp_file)

* iterate the records and fill the data into CRM system and register the customer.

&#8203;

    for idx, item in data.iterrows():
    tab.find_element(locator.customeronboarding.developer.text_customername).set_text(item[0])
    tab.find_element(locator.customeronboarding.developer.text_customerid).set_text(item[1])
    tab.find_element(locator.customeronboarding.developer.text_primarycontact).set_text(item[2])
    tab.find_element(locator.customeronboarding.developer.text_street).set_text(item[3])
    tab.find_element(locator.customeronboarding.developer.text_city).set_text(item[4])
    tab.find_element(locator.customeronboarding.developer.select_state).select_item(item[5])
    tab.find_element(locator.customeronboarding.developer.text_zip).set_text(""%05d"" % item[6])
    tab.find_element(locator.customeronboarding.developer.email_email).set_text(item[7])
    if item[8] == ""YES"":
        tab.find_element(locator.customeronboarding.developer.radio_activediscountyes).set_checkbox()
    else:
        tab.find_element(locator.customeronboarding.developer.radio_activediscountno).set_checkbox()
            
    if item[9] == ""YES"":
        nda = 'check'
    else:
        nda = 'uncheck'
    tab.find_element(locator.customeronboarding.developer.checkbox_nda).set_checkbox(check_type=nda)
    tab.find_element(locator.customeronboarding.developer.button_submit_button).click()

From above code , you can see:

* Locator is separate from code , so locator store can be managed independently, if the CRM system is upgrade, locator is changed, just need update the locator store.
* Easy to select option from dropdown list: tab.find\_element(<locator>).select\_item(<option>)
* Easy to check radio button/checkbox: tab.find\_element(<locator>).set\_checkbox()

# Locator

[Locator](https://www.clickcorp.com/documents#automation/locator) is the identifier of UI element, through [clicknium vs code extension](https://marketplace.visualstudio.com/items?itemName=ClickCorp.clicknium) can record/edit the locator.

# Compare with Selenium

* Selenium need download the webdriver which version should exactly match the browser , in this example, my Edge browser version is 103.0.1264.62  
, so I need download the same version msedge web driver first.
* Selenium does not support check operation for radion button, need use click instead.

&#8203;

    driver.find_element('id', 'activeDiscountYes').click() 

* To select option from dropdown list, need import additional class to wrapper.

&#8203;

    from selenium.webdriver.support.select import Select
    Select(driver.find_element('id', 'state')).select_by_value(item[5])

* Compare the running time In this sample, need fill 7 records, each record need submit 10 fields. From the log, we can see clicknium is more faster than selenium.

&#8203;

    [clicknium] Start to fill data:2022-07-21 16:10:15.938903
    [clicknium] End to fill data:2022-07-21 16:10:18.460162
    
    [selenium] Start to fill data:2022-07-21 15:08:30.528693
    [selenium] End to fill data:2022-07-21 15:08:37.517574

# More samples

You can find more automatin sample/solution from [clicknium github samples](https://github.com/clicknium/clicknium-samples)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', '[deleted]', 'awesome, very useful!!!']"
Clicknium Automation Sample Solution - Finance Quarter Close,https://www.reddit.com/r/automation/comments/wa4x9r/clicknium_automation_sample_solution_finance/,automation,"For many enterprises, at the end of each fiscal quarter, the finance team spends much time making all of the financial obligations fulfilled. This is a sample of financial quarter close solution with [clicknium](https://www.clicknium.com/) automation. It can review transactions automatically. Here is the details: go through transactions in local financial system to find the matching transaction in the Bank system. If the match is found, change the `Transaction Status` to `Verified` in local financial system.

The manual steps are as follows:

* login to local financial system.
* Query the transactions in this quarter.
* login to the bank system.
* For each transaction:
   * based on `Payment Account`, navigate to corresponding page.
   * search 'Payment Amount'.
   * If the matched one is found, go back to local financial system, set the `Transaction Status` to Verified.
* After all transactions are reviewed, click `Submit` button in local financial system.

# Run this sample

* follow [clicknium getting started](https://www.clicknium.com/documents/quickstart) to set up develop environment.
* clone [sample repo](https://github.com/clicknium/clicknium-samples).

&#x200B;

    git clone https://github.com/clicknium/clicknium-samples.git

* open the folder 'QuarterCloseChallenge' in Visual Studio Code
* open `app.py` in Visual Studio Code.
* press `F5` to debug the sample or press `CTRL+F5` to run sample. You will see the result as below:

https://preview.redd.it/i18q40trz9e91.png?width=600&format=png&auto=webp&v=enabled&s=3315e7f70fd65c47dd2945f1fb1e399c60138187

# The Purpose of The Sample

* open local financial system to get the transaction count and scrape `Amount` and `Account` information for each transaction.

&#x200B;

    def get_transaction_count():
        transaction = []
        tab = cc.edge.open(""https://developer.automationanywhere.com/challenges/automationanywherelabs-quarterclose.html"", is_wait_complete=True, timeout=60)
        if tab.is_existing(locator.quaterclose.developer.button_onetrust_accept_btn_handler):
            tab.find_element(locator.quaterclose.developer.button_onetrust_accept_btn_handler).click()
        elems1 = tab.find_elements(locator.quaterclose.developer.text_paymentaccount)
        elems2 = tab.find_elements(locator.quaterclose.developer.text_paymentamount)
        count = len(elems1)
        for i in range(count):
            account = elems1[i].get_text()
            amount = elems2[i].get_text()
            transaction.append({""Amount"":amount, ""Account"":account, ""Status"":""Unverified""})
        return tab,transaction

Here we leverage Clicknium `find_elements` api to find all similar elements. For example, for element's locator of PO number:

https://preview.redd.it/iwtuqyo20ae91.png?width=1184&format=png&auto=webp&v=enabled&s=4d422b71810d7ee5fe8e1f81577365564df8297a

To record similar elements, you can click `Similar elements` in Clicknium Recorder:

https://preview.redd.it/02rzux140ae91.png?width=337&format=png&auto=webp&v=enabled&s=e92a68cb8f7e1df879d923541da15a1c94c392f0

The wizard will be shown as below:

https://preview.redd.it/28vqu2850ae91.png?width=509&format=png&auto=webp&v=enabled&s=9e8261f9b543460d1fe8ade55d9bfe076922ed14

You can record (`Ctrl`\+click) two or more elements, for example:

https://preview.redd.it/yt4gaoq70ae91.png?width=1153&format=png&auto=webp&v=enabled&s=80057d8e07bc4ef0e8be74140716299e012593e4

https://preview.redd.it/8yamkbs90ae91.png?width=508&format=png&auto=webp&v=enabled&s=5bde5f0eebcdeb7c7777481e07400cff837780af

It will show the counts of matched elements:

* open bank system and login
* iterate the transactions and go to the corresponding account page based on each transaction's account
* search the transaction's amount, and if the matched one is found, mark the transaction state as Verified.

&#x200B;

    def validate_transaction(transaction):
        bank_tab = cc.edge.open(""https://developer.automationanywhere.com/challenges/automationanywherelabs-arcadiabanklogin.html"", is_wait_complete=True, timeout=60)
        bank_tab.find_element(locator.quaterclose.developer.email_inputemail).set_text(""tammy.peters@petersmfg.com"")
        bank_tab.find_element(locator.quaterclose.developer.password_inputpassword).set_text(""arcadiabank!"")
        bank_tab.find_element(locator.quaterclose.developer.a_login).click()
        for item in transaction:
            bank_tab.find_element(locator.quaterclose.developer.a_action, {""account"":item[""Account""]}).click()
            bank_tab.wait_appear(locator.quaterclose.developer.table1)
            bank_tab.find_element(locator.quaterclose.developer.text).set_text(item[""Amount""])
            if bank_tab.is_existing(locator.quaterclose.developer.td_amount, {""amount"":item[""Amount""]}):
                item[""Status""] = ""Verified""
        bank_tab.close()

* go back to local financial system and  batch update transactions state.

&#x200B;

    def update_transaction_status(tab: BrowserTab, transaction):
        elems = tab.find_elements(locator.quaterclose.developer.select_status)
        count = len(elems)
        for i in range(count):
            elems[i].select_item(transaction[i][""Status""])
    
        tab.find_element(locator.quaterclose.developer.button_submitbutton).click()

# Locator

The [Locator](https://www.clicknium.com/documents/tutorial/locator) is the identifier of UI element, which can be recorded and edited with [clicknium vs code extension](https://marketplace.visualstudio.com/items?itemName=ClickCorp.clicknium).

# Compare with Playwright

* You need to write xpath to get similar elements.

&#x200B;

    elems1 = page.query_selector_all(""//*[contains(@id,'PaymentAccount')]"")
    elems2 = page.query_selector_all(""//*[contains(@id,'PaymentAmount')]"")

* You need to fill the text to search the transaction item by pressing Enter as well.

&#x200B;

    bank_page.locator(""[placeholder=\""Search\\.\\.\\.\""]"").fill(item[""Amount""])
    bank_page.press(""[placeholder=\""Search\\.\\.\\.\""]"",'Enter')

# More samples

You can refer to more automation samples and solutions in [clicknium github samples](https://github.com/clicknium/clicknium-samples). Send [email](mailto:support@clicknium.com) to us or [Join Slack](https://join.slack.com/t/clicknium/shared_invite/zt-1cfxsstw7-s0CeJdhyg5wQ1h7_KKc6QQ).","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
looking for a useful workflow tool built for personal use.,https://www.reddit.com/r/automation/comments/w9rmmq/looking_for_a_useful_workflow_tool_built_for/,automation,"I think I've wasted a year of my life just looking for apps. Let me know if anyone is spinning their wheels thinking this:

""One app does this thing, but it doesn't do this other thing, so I'll try to integrate them both. But I can't do that without this OTHER app.""

Yes, we'll if thats you that's also me too. Lol. Right now I'm looking for an app I can REALLY run my personal life from. I'm looking for notes, task management, project management, and automated workflows, and integrations. For example, if it's someone's birthday, I want to push a button and have a whole set of tasks appear automatically so I can remember the birthday in general.

I'm using notion, Evernote, Gmail, and google calender right now...


Anyone have any suggestions?","['If you use Google Sheets, you can try using Logic Sheet to automate your workflows.', 'Clickup can do this.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Take a good look at ifttt or zapier. If they're mostly online services you may be able to tie the automations together through those."", 'I think there are too many mobile applications meets you requirement.', 'TickTick (personal organization and to-do) Taskade- Project management and UpNote -notetaking.', 'You can use Notion a lot of functionality plus integrations. I use it for my personal use.']"
Excel and Google Sheet automation - Goal is Data Studio Dashboard,https://www.reddit.com/r/automation/comments/w99k9m/excel_and_google_sheet_automation_goal_is_data/,automation,"I am creating dashboard in Data studio which will show business metrics pulled from 2 main sources. Social platforms/Google products and from excel file. 

My approach was to connect excel file (which is going to be on sharepoint/server) to Google sheets where I can easily export data to google data studio. 

Second approach was to connect excel file directly to Data Studio but I am not finding any solution for this. 

Its important to update on daily basis 

Thank you in advance","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'From my experience, the software you mentioned all have corresponding APIs to meet your functions.']"
"McDonalds CEO: Kitchen automation ""economics don't pencil out""",https://thestack.technology/mcdonalds-robots-kitchens-mcdonalds-digitalization/,automation,,
What types of processes are suitable for RPA automation?,https://www.reddit.com/r/automation/comments/w872k9/what_types_of_processes_are_suitable_for_rpa/,automation,Please share your suggestions,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'I believe this blog will help you https://botpath.com/system-bots-automate-work', 'From my point, rpa is suitable for no-interactive or less-interactive scenarios, repetitive processes, and those we want to execute at regular time.']"
Configuration and State Management Tools Preference,https://www.reddit.com/r/automation/comments/w4n8pk/configuration_and_state_management_tools/,automation,"Which automation/configuration management tool do you prefer in terms of capability, learning curve, budget, self healing and so on, and why?. Just for a project I am working on. I promise I wont try to sell you anything. 

&#x200B;

Thanks community. 

[View Poll](https://www.reddit.com/poll/w4n8pk)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
How to automate random pixel filling?,https://www.reddit.com/r/automation/comments/w2kjbd/how_to_automate_random_pixel_filling/,automation,"Hey folks,

I‚Äôm trying to put together a fun activity for my office. I‚Äôll spare you the unnecessary details, so here‚Äôs what I need:

I want a way of randomly filling in a specified grid if pixels, and I‚Äôd like to be able to have this operation generate and save multiple files.

Think of it like a QR code, with a 6x6 grid of pixels ‚Äî so six pixels in length, and six pixels in width (meaning: a total area of 36 pixels). I‚Äôm looking for a way to generate a random assortment of combinations, and save each result (be it png, jpeg, pdf, etc).

Is there an easy way to do this? Ideally, I‚Äôd like to use 6 different colors, but I can figure that part out myself if I can just get some guidance on randomly generating the pixels. Thanks!","['Is this a programming question? What are you working with? You spared a bit too many ‚Äúunnecessary details‚Äù. If you have the structure set up just iterate through each pixel and use a random function in whatever language you‚Äôre using to determine the color.\n\nEdit: you could use the RANDBETWEEN function in excel to generate random numbers, and then use those numbers with conditional formatting to make a coloured grid. Not sure if that‚Äôs what you want but that‚Äôs probably the easiest non-programming way I can think of doing it', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Automating a way to fill out a form in a desktop app,https://www.reddit.com/r/automation/comments/w0lgp0/automating_a_way_to_fill_out_a_form_in_a_desktop/,automation,"The majority of my job is reading through technical reports of product failures, and filling out an assessment form (open text, dropdowns, radio buttons, etc) based on the information. Essentially I am making an assessment of the report. The open text portions of the forms are mostly things I can copy and paste from previous templates, with some slight modifications needed here and there. However the radio buttons and drop downs change depending on the type of report I‚Äôm reviewing. 

An example would be:
I have a report for test results which specify that the test failed due to user error. I would then use that report to complete the assessment form and check all the right boxes and enter the correct text needed to summary the issue.  

All of these reports and forms are housed in a proprietary desktop app on MacOS. 

I would like to automate this process to increase efficiency because my team process hundreds of these every week. 

Big disclaimer: I am not a programmer, but I am an engineer. 

Can anyone recommend an automated solution where I can basically just select the type of report I received and it somehow populates the entire form for me based on my selection? 
I assume it would need some type of script where I can pre-populate the form entries, but I have no clue how to do this. 

Using my previous example, perhaps the script could ask me what type of report I received, then I could select ‚Äútesting-user error‚Äù and it would populate everything I need into the assessment form. 

Any help would be greatly appreciated!","['Check out https://UiPath.com', 'I use Keyboard Maestro and am sure it could handle the job.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Microsoft Teams and Alexa Amazon,https://www.reddit.com/r/automation/comments/vzal2k/microsoft_teams_and_alexa_amazon/,automation,"I‚Äôm looking for a way to automate some home stuff. I work from home, as does my significant other. We just got smart light bulbs to show when we are on a call - as to not be interrupted. However, I‚Äôd like to set up automation that teams does this automatically when I‚Äôm on a call. 

So far I haven‚Äôt found very many solutions in my Google hunt for this. I do know how to code, a little. However, I‚Äôm not a developer or engineer by any means. 

If anyone has some advice/direction/ideas - I would love to hear them.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""There was a similar project using a special USB light by ThingM called a Blink or Blink1.  It could glue into the chat apps of the day so as to change colour in response to a call starting; or really for a host of other things.\n\nI'm not saying 'go get that', but I'm thinking that the ecosystem around that could provide some good places to trigger on the chat app part. You'd be on your own for the light bulb part, but that ecosystem should be pretty rich already.\n\nNo idea about teams.  Wow does it suck and I'm sorry you have to use it."", ""Why don't you start with a simple switch installed under your table that controls the light on your door? You can go from there.""]"
pay rate,https://www.reddit.com/r/automation/comments/vwz23e/pay_rate/,automation,"I work in a sugar mill doing automation work long hours during harvest which is 3-4 months.. off season 40-60hrs... I install all the  gear,plc and do a good bit of programing and the instrumentation electrical and pneumatics. I'm in it for 4 years have computer and technician skills coming into the job but at the point boss man doesn't have to get out his office much if any...it's a pretty big mill produces 4.5 million pounds a sugar a day so one could imagine it's alot of converyours,turbines,electric motors, boilers,batch and continuous centrifugal machines etc... I work on everything.. I'm not as slick as the boss man but I have 9 other co-workers and and I'm ahead by a good bit technician work is what ibreally enjoy and do my best im pretty good at figuring out stuff im not familiar with or havent seen especially if im explained how the system works. also install and maintain they're air conditioners..don't wanna come off with a swollen head im just wondering what somebody like me should be making in this field","['Depends which city/state your located in.  In NorCal, you should be making a min of 30/hr, probably closer to 35-40/hr.  With all that OT, should clear 100-120k per year.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'This is old but I just found this sub. With your skill set you should be making at least $40/hr. I have similar skill sets and that‚Äôs about what I make. I work as a automation tech in poultry/beef/pork/fish processing. My job is 100% travel though.']"
short survey,https://www.reddit.com/r/automation/comments/vtiiko/short_survey/,automation,"Hello, I am computer science student working on a automation project, one of my task was create a survey, now I need to collect some responses. I would appreciate if anyone could take this survey it just takes like 2 minutes. I will live the link here [https://www.surveymonkey.com/r/LVVVTZH](https://www.surveymonkey.com/r/LVVVTZH)","['Suggestion: You have a ""Never"" option and the two follow up questions, those follow up questions should have an N/A option otherwise your data might seem skewed.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Software Brings Construction to An Assembly Line,https://www.reddit.com/r/automation/comments/vthpz8/software_brings_construction_to_an_assembly_line/,automation,"Software developed by Agorus enables precision manufacturing of the components that go into residential structures, allowing on-site construction to be accomplished in days rather than months. The San Diego-based construction technology startup was founded four years ago by two former Navy SEALs and has just raised $6.5 million in a venture capital seed round.

[\>>>](https://www.gcoportal.com/software-brings-construction-to-an-assembly-line/)

r/global_construction","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Software automation powering industrial automation.']"
How Built Spaces can Henefit From Digital Engineering,https://www.reddit.com/r/automation/comments/vspfjp/how_built_spaces_can_henefit_from_digital/,automation,"Replacing manual with digital processes is one way that engineers can ensure that the world's built spaces use energy as efficiently as possible, writes Carl Coken, vice president of Atrius Engineering, Acuity Brands. This can involve the placement of sensors throughout a project for optimal energy usage, paired with digital twins to provide a wide array of data for operators.

[\>>>](https://www.gcoportal.com/how-built-spaces-can-henefit-from-digital-engineering/)

r/global_construction","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
This an automation added post 1657057284348,https://www.reddit.com/r/automation/comments/vs91n7/this_an_automation_added_post_1657057284348/,automation,this is a post description 1657057284348,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
This an automation added post 1657057252465,https://www.reddit.com/r/automation/comments/vs916x/this_an_automation_added_post_1657057252465/,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Any way to automate iTunes downloads on Windows?,https://www.reddit.com/r/automation/comments/vrr5ed/any_way_to_automate_itunes_downloads_on_windows/,automation,"I have a very large list of apps I would like backups of. It's somewhat deprecated, but Apple still lets you download .ipa files of apps in iTunes [12.6.5.3](https://12.6.5.3) on Windows 10 for people who want more control over app installation. I've created links that look like, for example:

itms://apps.apple.com/us/app/mobilefuge/id503610940?uo=4

I put the links into an html file I can open in a browser, and when I click on a link, the app's page opens in iTunes. Then I click a button in iTunes that says ""Get"", and it downloads to my PC. Rinse and repeat. Occasionally iTunes prompts me to re-enter my password, which I paste in and click OK.

This works, but it's incredibly tedious. Does anyone have suggestions for how I could automate this process? If it was all in a browser I could probably figure it out, but I've never done anything with automating a random program.

I actually found a program that says it does exactly what I want on Mac, but iTunes 12.6.5.3 won't run in recent mac OS's. It uses the JXA framework for OSX.

[https://github.com/attheodo/cherrypick](https://github.com/attheodo/cherrypick)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Using pyautogui, pywinauto, autoit, bash, or macro recorder']"
Test1,https://www.reddit.com/r/automation/comments/vrwhoc/test1/,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Help me to Automate Test Case Writing,https://www.reddit.com/r/automation/comments/vqodhh/help_me_to_automate_test_case_writing/,automation,"It's a very lengthy process to Create a Test Case document by own for every project.
I want to Automate this process.
Is there any tool or any other way to Automate Writing test cases by just providing it some info with similar modules such as login, register, ecomm user flow etc.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Have you looked at mail merge?', 'My company uses Text Blaze for automation because you can create templates that you can customize. You could potentially use this, but you might have to look further into it. Good luck!']"
How to monitor a website 24/7?,https://www.reddit.com/r/automation/comments/vq1gsd/how_to_monitor_a_website_247/,automation,,"['For downtime or something else?', 'Hexowatch or ChangeTower are solid options.  Both offer visual, HTML, and tech stack monitoring.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Have you tried to install telegraf/Prometheus - influxdb -grafana? There are many examples and grafana dashboard ready to download,  use and learn']"
Why has the Helpmate robot failed?,https://www.reddit.com/r/automation/comments/vp3y7p/why_has_the_helpmate_robot_failed/,automation,"The Helpmate hospital robot was a project from the early 1990s with the attempt to increase the productivity in a hospital. The robot was able to deliver food to the patient's rooms and was doing so much cheaper than a human nurse.

Before we can answer the question why the Helpmate project has failed we have to answer, if this was the case. It seems, that there a different opinions available if a hospital robot makes sense or not. The idea from the manufacturing company was, that a robot can do repetitive tasks much faster and at lower costs and this makes a robot a good choice to automate something. The counter argument is, that robots won't improve the productivity because they are too complicated to use. What do you think, was the robot ‚ÄúHelpmate‚Äù a success?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Rebar-tying Robot Tops 11K Ties in Record-setting Day,https://www.reddit.com/r/automation/comments/vnfhk9/rebartying_robot_tops_11k_ties_in_recordsetting/,automation,"  Advanced Construction Robotics' rebar-tying robot has achieved a record with 11,044 ties in a single workday, the Pittsburgh-based company says. The feat was achieved at a pace of 1,100 ties per hour on a bridge in the Florida Department of Transportation's Gateway Expressway megaproject.

[\>>>](https://www.gcoportal.com/rebar-tying-robot-tops-11k-ties-in-record-setting-day/)

r/global_construction","['How can the article not include a video showing it in action..', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'https://youtu.be/P_Oph4wDi38\n\nThis is a link to the video', 'Reminds me of my idiot friend whose family owns a cement company. \n\n""My job will never automated!""\n\nDude, you\'ve got 10 guys doing the work of what would have been 50 guys 30 years ago....why do you think that is?']"
Robotic System Seals Road Cracks,https://www.reddit.com/r/automation/comments/vmn1fi/robotic_system_seals_road_cracks/,automation,"Only two workers are needed to operate the CrackPro Robotic Maintenance Vehicle that robotically seals road cracks. The truck-mounted system developed by SealMaster and Pioneer Industrial Systems uses artificial intelligence and high-resolution cameras and lasers to find and gauge cracks before applying sealant.

[\>>>](https://www.gcoportal.com/robotic-system-seals-road-cracks/)

r/global_construction","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Can you do online freelancing with machine learning??,https://www.reddit.com/r/automation/comments/vlwb8d/can_you_do_online_freelancing_with_machine/,automation,I am a university student and i was wondering if I can use my studies into good use,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
I want to learn machine control,https://www.reddit.com/r/automation/comments/vlq7si/i_want_to_learn_machine_control/,automation,"Hey there! I'm a mechanical engineer who specializes in designing food processing equipment and production lines. I want to learn how to automate my designs so any referrals or learning sources would be great!

I have virtually zero idea on how to design and select the electric components for my applications.

The machines usually have stepper or servo motors, induction motors, pneumatics and sensors.","['Hi,\nI think you are looking for plc & HMI courses.\nYou can have a look at plcdojo.com\n\nhttps://www.plcdojo.com/courses/plc-fundamentals', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Checking University Website,https://www.reddit.com/r/automation/comments/vl85cq/checking_university_website/,automation,"Hi all, 

i want to automate to check if there is any change on website of my university.

I have tries many many MANY ways but nothing works so i am writing this here after many failed attempts.

I am trying to automate checking if there are any new notifications on my university website. I need to go to It's page ([https://university.com/Index.jsp?)](https://university.com/Index.jsp?)) and to login with my credentials and then i am greeted with my profile on their webisode ([https://university.com/StudentIndex.jsp?)](https://university.com/StudentIndex.jsp?)) where i can see if there are any new notifications. I have made selenium script that basically goes to that website, takes screenshot and compare it to an image i have took when there are no new notifications. If they are not the same, there are notifications and via versa. Easy! But when i want to put that script on my Raspberry Pi it doesn't work. I don't have X on there so selenium doesn't like it. I have tried with all sorts of modules (PyVirtualDisplay,...) but everything just make my RPI(3B) freeze and nothing. Ok so i have gone in another direction. I looked at wget and curl. But problem is UNI website has bad ssl. Ok --no-check-certification does the work, but still it wouldn't get pass login page. Ok then i have tried with cookies which i get when i log in, but wget says [https://university.com/Index.jsp](https://university.com/Index.jsp)? does not exist but [https://university.com](https://university.com) does, but if you go there there is no login. WTF. Ok then i have tried HTTrack but i am confused on how to add cookies to login ( i am on Linux Arch so i can use only cli version).

Ok that doesn't work as well, ok go look into another direction. I have tried to look for some website that does that, and there are some.Nice! But they are all paid, and if not paid they don't work. I don't know why, probably cause it is dynamic website and not static. 

Ok what about Android app? There are some but they too have a problem cause it is dynamic. 

And after many days and attempts i am writing this here. Does anyone know what should i do? How can i check If there are new notifications on uni website ?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
I made a robot that punishes me if it detects that if I am procrastinating on my assignments,https://youtu.be/YPSazrEqlxo,automation,,
AI Advances Nuclear Fusion R&D | New Amazon Robot Proteus Automation | AI Outperforms Crypto Markets | Robotic Fireflies,https://youtu.be/ADNa5Upvlj8,automation,,
"Are there any applications / work arounds for taking text from hundreds of similarly laid out pdfs, OCR scanning it, sorting it correctly, and auto filling it into online form fields?",/r/Automate/comments/vk315p/are_there_any_applications_work_arounds_for/,automation,,
New Amazon Robot Warehouse Automation Tech For Amazon Fulfillment Center,https://youtu.be/iSt6d_CFJzY,automation,,
how do i automate going to sites and log in?,https://www.reddit.com/r/automation/comments/vjfp59/how_do_i_automate_going_to_sites_and_log_in/,automation,"so i want to be able to go to like my mail, github, job searching site and many more then login to those sites. sometimes the site has like a code authentication tool.  
was wondering whats the best way to do this? selenium? pyautogui or something else?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'If you know Selenium & PyAutoGUI, you already have a higher technical caliber than mere biz users. As you pointed out, the choke point is 2FA. You may give Appium a try to access the code in your phone. \n\nEvything is possible to be automated as long as there is a repetitive pattern.', ""Are you just trying to get to the site and log in quickly? I know an automation site called IFTTT (if this then that) but I am not sure if it could solve this. Another potential solution could be using Text Blaze's keyboard shortcuts to quickly paste URLs, usernames, and passwords. Interesting question, hope you figure it out!"", 'Can you set the 2FA as something other than a text? An email 2FA could be automated far more easily']"
Report: 90% of orgs indicate increased demand for automation,https://venturebeat.com/2022/06/23/report-90-of-orgs-indicate-increased-demand-for-automation/,automation,,
Is there a website that lists all algorithms/AIs/robots/machines and categorizes them into what they can do to get an overview of what they can be used for?,https://www.reddit.com/r/automation/comments/vi2v5b/is_there_a_website_that_lists_all/,automation,If possible robots/AIs which need as little help as possible.,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
"I may get a job doing some automation for a company, but I never don't that, any tips or something I can learn about it quick please, I really need the job.",https://www.reddit.com/r/automation/comments/vhwa5f/i_may_get_a_job_doing_some_automation_for_a/,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I think it depends on what type of automation you're doing (sales, social media, email, etc). There's a lot of automation tools that can help you, though. IFTTT (if this then that), Text Blaze, Leapwork, etc. Just determine the type you need and you'll find help and resources""]"
How to automate downloading and emailing a bill?,https://www.reddit.com/r/automation/comments/vhi0qk/how_to_automate_downloading_and_emailing_a_bill/,automation,"I'm an IT tech but never used any automation for my own tasks.

I need to grab a PDF copy of a bill each month, then email it on.

I have MS flow, but not much else.  What is the best way to handle this? Not sure how it can handle login vs logging in with stored cookie and all that jazz.

My ability to do this in AutoHotKey is lacking and would like something well supported and straight forward.  

Thank you!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Do you use any software for downloading the bill? Does it offer an API? If it does, you can use MS Flow and if not, I‚Äôd suggest using Selenium to automate the login and download process using a language like javascript or Python.\n\nThere are several tutorials on how to do exactly this on Selenium!', 'Where does the bill reside? Probably zapier is your easiest option if there is not a native feature currently. Or look for a simple script, sounds like it would be a relatively easy google apps script for google sheets, not to fluent on vbasic which I think Excel uses', 'With PowerAutomate you can easily set it up to take a file that‚Äôs added to a folder and email it out. You still have to get it from the vendor site, but this allows you to look it over before sending it about and at least automate part of the process in ease.']"
Hey guys i need help on my work documents on site and to find better way and cyber secured to make it paperless through the whole site project all documents have to be signed by different parties I thought about adope fill and sing kinda like app but i need more professional approach ideas help,https://www.reddit.com/r/automation/comments/vhft57/hey_guys_i_need_help_on_my_work_documents_on_site/,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'hey, do you need implemetation help or just advice?', '[Sign.cc](https://Sign.cc) would be a perfect fit for you. I have been using it for a while and I am having a great experience with it. [Sign.cc](https://Sign.cc) helps me request and sign signatures from one party to multiparty']"
What are the business benefits of RPA in the insurance industry?,https://www.reddit.com/r/automation/comments/vhacb2/what_are_the_business_benefits_of_rpa_in_the/,automation,Please share your suggestions,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
How to automate this boring task?,https://www.reddit.com/r/automation/comments/vh2him/how_to_automate_this_boring_task/,automation,"It goes like this:
1- I have 800 names in a txtfile, each name is separated by a new line.
2- I have one picture 

I want to print each name in this picture, so I end up with 800 unique pictures. 
If you don't have an idea about how to automate this task, at least tell me what program to use to print these name, adobe photoshop, painter, or what?

Or there is any program that record screen clicks then repeats itself","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""You may be able to do this with Microsoft Word Mail Merge, since it seems to be a pretty simple task with only one variable.\n\nHowever, something I've been loving is using [BannerBear](https://www.bannerbear.com/) to add text to images at scale for marketing purposes. \n\nEither one may work, but BannerBear will probably be easier to figure out and I think it has a free trial."", 'You could do this very easily with n8n.io (free)\n\nJust download the desktop app and build a simple workflow to achieve this', 'you can try asking r/AutomateYourself']"
How should I go about editing a couple hundred thousand text files?,https://www.reddit.com/r/automation/comments/vgd5f5/how_should_i_go_about_editing_a_couple_hundred/,automation,"Might be a weird request and not the right sub for this... I have a couple hundred thousand text files with 2 lines I want to get rid of. The first line and the last line.

Obviously opening up notepad is out of the question. If it were a couple hundred then I could just open everything in notepad++ and batch edit. But a hundred thousand? How do you go about that","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'So... Tell us a little about the changes you need to make please\n\nIf they are consistent enough, and follow a pattern strictly enough, this could be done with a relatively simple script.', 'I use jedit, though there are quite a few text editors that can do this. What you want is find and replace that runs on all files in a folder and regular expressions if you want things like character return/new lines', 'Notepad++ open the entire folder as a project. Do a search and replace using REGEX.', 'Python sould be up to job']"
Optimization on Application Form,https://www.reddit.com/r/automation/comments/vge4bx/optimization_on_application_form/,automation,"Hi guys, do you have any recommendations for a good out of the box application from I can use to extract optimization metrics from:

1. Time took to answer each question
2. What question resulted in exiting the application form without finishing it
3. Additional use cases I didn't think of

For example google forms is a plug and play application form, but I am not familiar with any metrics it sends.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Hey! Did you find a solution for this?']"
I want to sequentially call numbers in a list through google voice or some other free provider,https://www.reddit.com/r/automation/comments/vfm7oo/i_want_to_sequentially_call_numbers_in_a_list/,automation,I am calling a crazy amount of car dealerships and asking them if they have a car in stock; it's annoying dialing all the numbers. Anyone know of a decent solution through IFTTT or something else?,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""/r/Tasker can handle this task, with no issue, if you're on an Android device."", 'You can directly call the number from Google sheet if you have Google voice']"
How to automate financial tracking,https://www.reddit.com/r/automation/comments/veu8rf/how_to_automate_financial_tracking/,automation,"Is there a way, that doesn‚Äôt involve third party apps, to pull credit card and bank account transactions once a week and put them into excel? 
Currently I do this manually and it is painful. Excel‚Äôs web function doesn‚Äôt word, I assume due to passwords. 

Is there an easy way to scrub with python? Is this something a beginner can set up?","['You can also try asking r/AutomateYourself', 'Try Selenium for the sites without APIs. There are lots of examples online, showing how you can automate using a browser to navigate and interact with sites. Your selenium script would simply be, navigate to website, login, navigate to transaction downloads, click download. Python can handle the resulting downloaded file and integrating that into what Excel file you already have.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Your use case is interesting. What‚Äôs your manual workflow currently? It‚Äôs hard not to involve a 3rd-party app, even with Python, you need to tap into some libraries.', 'Does your bank have an api ? To make some sort of request to your account and pull data ?']"
Issue with Corretto - NoClassDefFoundError dev/failsafe/Policy,https://www.reddit.com/r/automation/comments/veo7lh/issue_with_corretto_noclassdeffounderror/,automation,I've been pulling my hair out trying to figure out what's wrong. Why wouldn't it flag as not found during compilation. I'm using corretto Java 11 and Selenium 4. Newest versions of both.,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""hi,any update on this?\n\nI'm facing similar issue.""]"
Can anyone help me with a task in Python?,https://www.reddit.com/r/automation/comments/veen4r/can_anyone_help_me_with_a_task_in_python/,automation,Can anyone help me with a task in Python that should go to Google and from there search for a site name to enter the site and there search for the price list go to the price page and output to the document of all prices below a certain amount,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""What's the site? You could fairly easily use something like Selenium, but it depends on the output format.""]"
The Future of the Ready-Mixed Concrete Truck,https://www.reddit.com/r/automation/comments/veebyt/the_future_of_the_readymixed_concrete_truck/,automation,"The ready-mixed concrete truck is a mobile manufacturing environment, not just a vehicle, writes Craig Yeack. 

He notes that as technology evolves, each provider seeks to add displays into the truck cab, but end-users want to choose their own communication platform, safety cameras, water meters and other sensor systems, which he argues should be developed ‚Äúto plug and play‚Äù with various platforms.

[\>>>](https://www.gcoportal.com/the-future-of-the-ready-mixed-concrete-truck/)

r/global_construction","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
PID Tuning,/r/Python/comments/ve9sz9/pid_tuning/,automation,,
What use case of RPA has helped you the most?,https://www.reddit.com/r/automation/comments/vcqi4r/what_use_case_of_rpa_has_helped_you_the_most/,automation,Please share your suggestion.,"['I believe this article will help you: https://botpath.com/industries-adopting-rpa', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Enabling non code savvy people to create their own automated processes. Not everyone knows CMD/PS, but most people can generally figure out that the ""copy file"" button can be dragged onto the canvas to make copies to other directories.', 'Web scraping/data entry when db access is a no go.']"
Which automation tool is best for LinkedIn Marketing,https://www.reddit.com/r/automation/comments/vcbrqa/which_automation_tool_is_best_for_linkedin/,automation,"

[View Poll](https://www.reddit.com/poll/vcbrqa)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""My favorite is Text Blaze. My company uses it to automate our LinkedIn messaging and it's pretty effective. This is the article we found the other month that got us into using it: [https://blaze.today/blog/linkedin-connection-request-message-template-examples/](https://blaze.today/blog/linkedin-connection-request-message-template-examples/). Hope this helps!""]"
Automation Process is good for Business or not?,https://www.reddit.com/r/automation/comments/vbat96/automation_process_is_good_for_business_or_not/,automation,,"[""I definitely think it depends. If you use a site like Text Blaze to automate typing and email, that would help you work more productively. It can go too far, but you just have to be careful. There's a site called If This, Then That (IFTTT) and it has a lot of automation features that can be good for businesses if used properly."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Good for companies that adopt automation early on to improve their efficiency & raise customers' & staff's satisfaction; bad for those being ignorant, becoming less competitive & eventually obsolete."", 'Depends if there is clarity on why the automation is being done and what form or level of automation is possible', 'Automation for any business is good if you do it good, if not done good it is a headache. I have done automation for the industries for many years and non of my customers regretted.', 'If you are looking to automate your business processes, then BPMApp by 500apps is a good option. It can help you save time and money by automating your processes.  \nBPMApp by 500apps is a cloud-based business process management (BPM) tool. It enables businesses to automate their processes by creating workflows. These workflows can be triggered by events, such as a new customer being added to a CRM system.  \nIt can help you save time by automating repetitive tasks. For example, if you receive a lot of customer inquiries, you can create a workflow that automatically sends responses.']"
PODCAST: Thermal Intelligence Offers Jobsite Fuel Efficiency,https://www.reddit.com/r/automation/comments/vbbkdd/podcast_thermal_intelligence_offers_jobsite_fuel/,automation,"This Digging Deeper episode features an interview with Mark Malekoff, director of Thermal Intelligence - an equipment manufacturer out of Edmonton, Alberta, Canada. Mark and his co-founder used their first-hand experience in dealing with equipment that didn‚Äôt meet their standards for reliability and efficiency and brought a sustainable solution to construction jobsites.

[\>>>](https://www.gcoportal.com/thermal-intelligence-offers-jobsite-fuel-efficiency/)

r/global_costruction","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Python - When I try to search in a search box it types 2 list elements in a row,/r/selenium/comments/vaibgm/python_when_i_try_to_search_in_a_search_box_it/,automation,,
What are some tools a someone in an office job without a development background can use to automate their tasks?,https://www.reddit.com/r/automation/comments/va7y8f/what_are_some_tools_a_someone_in_an_office_job/,automation,"I have an information based job within a large organisation.
Hoping to find some great tools to automate any tasks possible.
I've been using copy.ai, looking into Zapier and IFTTT.","['You‚Äôre gonna need to be a lot more specific', 'Power Automate (Flow) from Microsoft. A lot like Zapier and IFTTT but built specifically for business use', 'Macros. I‚Äôve seen countless office people live by them', 'Auto hot key is useful.', 'Learn python. Look up the book Automate the Boring Stuff. That‚Äôs basically the whole purpose of the book. Free to read online. Beginner friendly.', '[Make](https://www.make.com/en)', 'Maybe ones like these? https://www.nocode.tech/category/automations', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Microsofts powerplatform including Desktop flow.\n\nVBA.\n\nUsually, that's all there is companies are willing to let end users play with. For good reasons tho."", '[mayalabs.io](https://mayalabs.io) could be something you could look into. Unfortunately they seem to have only team plan for now. This can be used to automate almost anything.']"
API for automating form creation,https://www.reddit.com/r/automation/comments/vabo4l/api_for_automating_form_creation/,automation,"I'm spinning out an internal tool we use to build and send html forms which can easily be integrated with some no-code services  -- but before I add any more features, I'd like to hear if anyone would find this useful: [https://neondispatch.com/](https://neondispatch.com/)","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
How to grab/open hyperlink from gmail group chat? (extract data after opening a link in gmail group chat),https://www.reddit.com/r/automation/comments/va9y7v/how_to_grabopen_hyperlink_from_gmail_group_chat/,automation,"Would like to automate weekly task and need some help...

&#x200B;

Weekly in a group chat (Gmail chat), my manager posts a link which I would open this link and extract data for company expense, employee attendance, etc...

&#x200B;

How would I go about this?  Will be using selenium to scrape data once I can get this link to open....  FYI, the group chat is active through out the day but only need a link  matching string **'report'** from gmail group chat to open

&#x200B;

Any advice would be really helpful since I am stuck..","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Hi, \n\n Maybe try nocode automation tools such as these? https://www.nocode.tech/category/automations\n\nMost of these tools have handy templates to start from.']"
Excel VBA script,https://www.reddit.com/r/automation/comments/v9q6ve/excel_vba_script/,automation,I want to basically create a calculator based on objective variables i.e. if hot =1 if cold -1. I have the data in different sheets in excel. Can anyone help me out ?,"['This sounds rather simple but there‚Äôs not enough details. What do you need help with? Have you tried yourself yet?', 'Hmm.. Look into VLOOKUP. You probably don‚Äôt even need a script.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Python program to automatically fill out forms in a word doc?,https://www.reddit.com/r/automation/comments/v97gyk/python_program_to_automatically_fill_out_forms_in/,automation," We're always filling out the same information on various forms for customers (company name, W9 tax info, address, etc). Clients will send us their vendor form which is unique to them, and they use different wording (organization name, company name, legal name, etc). It's the same information I'm copying and pasting all the time.

Any tools that can do this for me? Ideally it would try it's best to fill it out and then I'd just double check it to ensure it's accuracy. Maybe it highlights any cells it has problems with?

I imagine this isn't something that we'd have to build custom, it's not that powerful I think but I can't find any good tools that aren't overboard with AI.","['MS Visual Basic maybe a better option.  You could have a macro key for each different form.\nFor python you would need a way to identify the form.\nSelenium maybe a good solution too.  Works with Python.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'These RPA(Robotics Process Automation) low/no code tools may work, https://www.guru99.com/robotics-process-automation-tools.html']"
Booking form synchronised with complex and specific quotes per client that generate invoice automatically.,https://www.reddit.com/r/automation/comments/v92nre/booking_form_synchronised_with_complex_and/,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Why use TypeScript over JavaScript in Cypress?,https://www.reddit.com/r/automation/comments/v73vsc/why_use_typescript_over_javascript_in_cypress/,automation,"I use Cypress as my UI E2E automation framework. I see a lot of people suggesting to use TypeScript over JavaScript, but unsure on what the actual benefits are?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Hi As per my knowledge of working in a [software testing services](https://www.qasource.com/software-testing-company) Typescript is an enhanced version of JavaScript. It is good for handling large applications. The coding standards of Javascript are valid in typescript as well. Internally typescript codes are compiled into js form only.\n\nTypescript basically supports concepts of OOPS. Nowadays people prefer typescript over Javascript. Further, you can always refer to online articles on typescript for a better understanding of the language.\n\nHope you find it useful!!']"
Interested in sharing your knowledge on PLC systems or applications with the engineering community on Control Automation?,/r/aac/comments/v72y7a/interested_in_sharing_your_knowledge_on_plc/,automation,,
Connect google form to Microsoft Excel,https://www.reddit.com/r/automation/comments/v6rbj9/connect_google_form_to_microsoft_excel/,automation,Is there any way to connect google form to Microsoft Excel So if someone put data into Google form it will also get updated in Excel (in my Computer file).,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Not sure, but have a look at Zapier or Make']"
"I need a code that will WhatsApp a list of numbers a message, for free.",https://www.reddit.com/r/automation/comments/v71n30/i_need_a_code_that_will_whatsapp_a_list_of/,automation,,"['Is spamming cool now?', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
all buttons are greyed out on ur5 and i can barely press anything,https://i.redd.it/ugdwmf3d76491.jpg,automation,,
"I need a code that would take my ""Liked Videos"" from Youtube and feed it into an Excel sheet or Database.",https://www.reddit.com/r/automation/comments/v48y2c/i_need_a_code_that_would_take_my_liked_videos/,automation," I have 5k Liked Videos on youtube. I need a code that would take attributes of a Video :

Title, Link, Thumbnail (if possible), and Audio would be a dream.

and feed it into some form of storage like a MySQL database or an excel sheet.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""> a code\n\nrot13.  It's simple and easy to reverse .  You're welcome.""]"
How to build a bot to download the latest /r/gifs posts on Reddit,https://pipedream.com/blog/how-to-build-a-bot-to-download-the-latest-r-gifs-posts-on-reddit/,automation,,
Scan 2 or 3 barcodes to fill out spreadsheet Info in right place,https://www.reddit.com/r/automation/comments/uy1aai/scan_2_or_3_barcodes_to_fill_out_spreadsheet_info/,automation,"Hello,

im currently working on a project where i would like to automate data entry into a spreadsheet.

I have 256 Boxes that can be at 256 Spaces with Stuff in them and would like to use a barcode scanner to scan a barcode at the place and then at the box and have it fill out an cell thats coresponding to the place with the number of the box.

Does anyone have a somewhat easy solution for this?

All i can find are expensive subscription apps that would do that..","[""You could write a script in Python, for example.\nIf your scanner can output text as a peripheral (like a keyboard), you could make the program ask for each input, one after another. Then you can use a spreadsheet interface library to add the value in the right place.\n\nWhat's your programming experience level?"", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
What is the best program you use to automate posting on social media?,https://www.reddit.com/r/automation/comments/ux48j5/what_is_the_best_program_you_use_to_automate/,automation,,"['Buffer, Zapier, n8n, integromat, IFTTT, repost. etc', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Buffer', 'ContentStudio.io can post carousels, too']"
Find out Best Automation Company in UK,https://www.reddit.com/r/automation/comments/uxctlx/find_out_best_automation_company_in_uk/,automation,We researched and compiled a list of the top automation companies in the United Kingdom in 2022. Read our blog to learn more about industrial automation providers and to find the best company for your needs. Visit [https://www.watbro.com/best-industrial-automation-company-in-the-uk-to-watch-in-2022/](https://www.watbro.com/best-industrial-automation-company-in-the-uk-to-watch-in-2022/),"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
L√†m sao t√¨m ƒë∆°n v·ªã l·∫Øp ƒë·∫∑t kho l·∫°nh uy t√≠n t·∫°i H√† Tƒ©nh?,https://www.reddit.com/r/automation/comments/ux9f3j/l√†m_sao_t√¨m_ƒë∆°n_v·ªã_l·∫Øp_ƒë·∫∑t_kho_l·∫°nh_uy_t√≠n_t·∫°i_h√†/,automation,"&#x200B;

&#x200B;

H√† Tƒ©nh l√† m·ªôt t·ªânh ƒëang ph√°t tri·ªÉn m·∫°nh m·∫Ω v·ªÅ kinh t·∫ø x√£ h·ªôi. Ngu·ªìn l·ª£i kinh t·∫ø l·ªõn nh·∫•t c√≥ l·ªÖ ƒë·∫øn t·ª´ vi·ªác tr·ªìng tr·ªçt, chƒÉn nu√¥i c≈©ng nh∆∞ ƒë√°nh b·∫Øt th·ªßy h·∫£i s·∫£n khu v·ª±c ven bi·ªÉn. Hi·ªán t·∫°i c√°c doanh nghi·ªáp c≈©ng ƒëang √°p d·ª•ng v·ªÅ vi·ªác ƒë·∫ßu t∆∞ h·ªá th·ªëng kho l·∫°nh b·∫£o qu·∫£n th·ª±c ph·∫©m. Kho l·∫°nh Th·ªãnh V∆∞·ª£ng l√† ƒë∆°n v·ªã ƒëi ƒë·∫ßu, lu√¥n mong mu·ªën c√πng chung tay c√°c doanh nghi·ªáp ƒë·ªÉ kh√¥ng ng·ª´ng n√¢ng cao ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m. H√£y xem th√™m b√†i vi·∫øt n√† ƒë·ªÉ hi√™u r√µ h∆°n v·ªÅ d·ªãch v·ª• ch√∫ng t√¥i nh√©: [https://ello.co/kholanhthinhvuong/post/t8klsqakin0cupymhtrmkg](https://ello.co/kholanhthinhvuong/post/t8klsqakin0cupymhtrmkg)\#kholanhthinhvuong #lapdatkholanhhatinh #kholanhhatinh","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
An R&D Guide to Automation in Manufacturing for 2022,https://www.valuer.ai/blog/guide-to-automation-in-manufacturing,automation,,
arlo cameras,https://www.reddit.com/r/automation/comments/uwc5nt/arlo_cameras/,automation,"I am trying to figure out a way to automatically turn on two indoor arlo cameras when everyone leaves the house and to automatically turn them off when one person arrives home. Anyone know a way to do this? I use apple HomeKit, alexa, and ifttt. I am willing to download something else tho if it will get it to work.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'HomeKit has a bit around geolocation for home/away from home settings. I‚Äôve also seen stuff like Life360 used that can be set up through one of those as well to simply look for someone to arrive at home']"
Arlo Cameras,https://www.reddit.com/r/automation/comments/uwc3pk/arlo_cameras/,automation,"I am trying to figure out a way to automatically turn on two indoor arlo cameras when everyone leaves the house and to automatically turn them off when one person arrives home. Anyone know a way to do this? I use apple HomeKit, alexa, and ifttt. I am willing to download something else tho if it will get it to work.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
How to identify False-negative tests in a testing set?,/r/testomatio/comments/uv8oa9/how_to_identify_falsenegative_tests_in_a_testing/,automation,,
is there a motion detector i can use next to a pool?,https://www.reddit.com/r/automation/comments/uro369/is_there_a_motion_detector_i_can_use_next_to_a/,automation,"We're moving in a week.  New house has an inground pool.  I'm very worried about drowning.  It was a complete no 5 or 10 years ago.  But the kids are getting older.  Our youngest is 8.  I'm mostly worried about her.  The others can swim well.  But, even a good swimmer can drown.  I bought a thing that floats in the pool that will alert if it detects waves.  But I look at that like it's the last line of defense.  Id like to know if the kids are walking by the pool.

They have driveway alarms that would ding, but id like to set it to arm when we're not outside at the pool and, if it detects motion, not to only give one quick ding like a car drove in.  I want it to get my attention.  

Also, the new house I can't hook up anything internet based.  I've scheduled to have satellite internet installed.  But there's a monthly data cap.  So I'm not hooking a whole bunch of smart devices like we do now.  I can't have something trickling the data 24x7 all month.  (Kind of looking forward to being more old school.)

Also, if you can recommend ones that connect to internet.  They are installing fiber, but I think they're in phase 1 or 2 and well be in phase 4.  Enough time for a kid to drown.","['Honestly your best bet is a gated fence with a lock. Some states like AZ have laws requiring them to prevent accidental child drownings.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""> I can't have something trickling the data 24x7 all month\n\nIf you're interested, you could always have a local installation that sends you a message via Internet only when an event (e.g. cameras detect someone at the pool) happens.\n\nFor more info on this, I suggest posting to r/selfhosted.\n\nFor example, I self-host a service that sends me a text message using Internet when a specific event happen, and is passive the rest of the time. I do this on a Raspberry Pi 4, which uses around 6 Watts and costs me around 6‚Ç¨/year of electricity. I also host a Nextcloud instance, a replacement to Google Drive, completely local, and works without Internet.\n\nBut unless you are already into networking and administrating a server, a lock around the pool will require far less efforts."", 'You could do like someone else suggested and do self hosting. We run Blue Iris for camera software and use it as part of house security, but it has features in it like motion detection and more. It saves a lot of time avoiding false alerts (so you can set it up to alert only if motion is detected across an area the camera can see, so a leaf wouldn‚Äôt trigger it)\n\nIt is hosted on a PC local to the house. Alerts are the tricky bit, but I wouldn‚Äôt be surprised if you could get something set up to text you alerts', 'This is tricky.\n\nI would say a laser fence is your best option.\nBut this can get expensive if the pool is big.\n\nYou can also get a wirefence. When the wire is pulled it wil send a signal. This is the same system they use for special toilets.\n\nUse an arduino with a buzzer to get a signal.\n\nNo internet, cheap and save enough.']"
Is iMacros still the best and simplest method of extracting text from one page and copying it to another?,https://www.reddit.com/r/automation/comments/ure0ki/is_imacros_still_the_best_and_simplest_method_of/,automation,"I always used to love creating iMacros for extracting the data from osTicket and copying it to whatever web tool that was required for that ticket. I know there is Selenium, ParseHub, WebHarvy, RPA UiVision, etc.

But is there anything else that is both simple and effective at copying data from one tab to another?

What I love about iMacros is the recording option. You can just record it and edit what is needed after the fact. But it seems that iMacros is quite out-dated and has fallen under new ownership. Also, the fact that I lost all of my iMacros while I was on medical leave to remove my voice box (full laryngectomy because of esophageal cancer). Also, RPA UIVision doesn't seem compatible with a lot of elements, and iMacros has the same issue.

So before I start remaking these, is there something similar or better? I have a free lifetime subscription to ParseHub, which is amazing, but lacks the copy and paste function. I also reverse-engineered Webharvy to bypass activation, but I can't have that on company systems. 

Thanks for taking your time to read my post.","[""iMacros used to be great, and - as long as it works - runs web scraping still faster than UIvision. But for new projects I use UI.vision. It also has a recording option.\n\n> RPA UIVision doesn't seem compatible with a lot of elements,  and iMacros has the same issue.\n\nWhich ones? Both work well for me. Maybe this helps: https://ui.vision/rpa/docs/selenium-ide/web-scraping"", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
DCS Crash Course,https://www.reddit.com/r/automation/comments/uqeav7/dcs_crash_course/,automation,Folks I‚Äôve worked with Siemens Insight building automation for few yrs and have good automation background. I‚Äôve since move from that role and new opportunities show up with DCS process controls. Would greatly appreciate info about good learning materials/course for DCSüëç,"['I have never seen anything for general DCS courses or Materials nor would anything like that be very beneficial. I can only speak from Honeywell or DeltaV experience but the programming/automation is only a fairly minor piece of the entire system and their eco systems are manufacturer specific. Knowing one doesn‚Äôt allow you to go to the next and effectively manage it without some manufacturer specific training.  Your best bet is to contact your Manufacturer for classes specific to your DCS system.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Inkbird STC-1000 but for humidity?,https://www.reddit.com/r/automation/comments/uo2qdk/inkbird_stc1000_but_for_humidity/,automation,"Hi, I want to set up an automatic hygrometer ""thermostat"" that automatically shuts off a device whenever humidity enters chosen temperature. I'm familiar with the 65$ Inkbird humidity controller but I've seen some 20$ STC-1000 temperature controller and was wondering if there was a similar product for humidity.

Thanks in advance.","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'AFAIK Inkbird is the only one with that kind of ""all in one solution"". You could probably get cheaper options but it would most likely require some electrical work, also with cheaper price comes worse quality/reliability']"
Need help automating a remote lookup,https://www.reddit.com/r/automation/comments/unm7n5/need_help_automating_a_remote_lookup/,automation,"My department accesses data from another department to see if a list of people is in their system. Currently, we do this manually everyday but want to automate the process. We were thinking of using something like auto hot key, etc but the problem is we access the server via a remote connection. The process is clicking a shortcut on my desktop, which remotely connects to the server; I click an app on their desktop and enter the employee ID into the system; if they are, we take a screenshot and update the status on a Google Sheet.","[""Can't you do auto hot key on the server directly?"", 'you can ask r/AutomateYourself', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'You could totally use the selenium package in Python for this, gotta know someone who can write python though.\n\nEdit: didnt read correctly, thought it was a website so selenium wont work for this. If you have someone knowledgeable within your company you should look at directly accesing the system locally, that would make it alot easier.']"
Looking to automatically log me out of all my online accounts from all devices at the press of a button and when I leave work.,https://www.reddit.com/r/automation/comments/un7065/looking_to_automatically_log_me_out_of_all_my/,automation,"I keep finding myself still logged in to google, reddit, amazon, etc when I come into work the next day. I'm always pretty pressed for time, and extremely forgetful to boot.

If there is already some pre-existing software that could run on my phone to log me out when I leave work or manually select to log out of all accounts, that would be great. If not, maybe there is something that would allow me to easily make what I want, I dont know if IFTT or something similar would work, I do know I suck at programming though. So if that's the only option, I'd just wind up having to pay someone I could trust not to steal my account info to write it for me.","['Simplest option: use a browser with an option to delete cookies on close. Firefox does that very well with `Delete cookies and site data when Firefox is closed`, and even lets you whitelist websites for which you want to keep cookies.', 'Maybe something along the lines of a batch file deleting cache and cookies would be an easy solution.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'could be easily handled from your computer but some kind of kill switch from your smartphone to your work computer may not be possible.\nis it :\n- a laptop where you just close the screen ?\n- is your computer on a specidic domain ?\n- do you leave work at a specific hour or it may change from one day to another ?', ""If you're on Windows, try Power Automate""]"
Automate laundry with QR codes,https://www.reddit.com/r/automation/comments/un6rjt/automate_laundry_with_qr_codes/,automation,"So I just had this idea and wanted to know if it‚Äôs possible. So we print QR codes on insides of clothing item‚Äôs and when you scan them it gives you the option to select either ‚Äúwearing‚Äù, ‚Äúin the wash‚Äù, ‚Äúin the dryer‚Äù
What this would do is simply tell you where your clothing is so you never have to go searching for it again, leaving a mess in the closet
I was thinking about making a app to go with it that visually shows what clothing you‚Äôve been wearing, the ones you don‚Äôt use and probably should get rid of, but this is secondary to the QR code thing","[""Hmm, that's an interesting concept!\n\nIn its current form, I'm afraid this would create more work for me than it would solve.\n\nHowever, if you wanted to take this idea to the next level, you could try making an NFC chip that is durable enough to survive the washer/dryer and have a reliable way to install it on your clothing.\n\nThen, you could have a sensor on your washer, a sensor on your dryer, and a sensor in your closet so that when the clothes pass over it, it tracks where it is on your app.\n\nAt least for me, the action of scanning a QR code makes it too user un-friendly for me to see myself using, but I could get behind just tapping the tag of the shirt on a sensor."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
"Warehouse automation market is expected to touch the mark of $41 Billion by 2027, at a CAGR of almost 15% between 2022 and 2027, reported by LogisticsIQ‚Ñ¢, and it is expected that AGVs/AMRs are going to have more than 24% market share by 2027 in warehouse automation market.",https://www.reddit.com/gallery/umjl0y,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
BEST VIBRATORY BOWL FEEDER SYSTEMS IN INDIA,https://www.reddit.com/r/automation/comments/umbgyc/best_vibratory_bowl_feeder_systems_in_india/,automation,WE PROVIDE YOU WITH THE BEST VIBRATORY BOWL FEEDING SYSTEM IN INDIA JUST GIVE US CALL AND WE PROVIDE ALL OVER THE WORLD,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
NVIDIA‚Äôs Robot AI Finally Enters The Real World! ü§ñ,https://youtu.be/-t-Pze6DNig,automation,,
Cypress Configuration | Understand Cypress Runner,https://youtu.be/sgK71LMUExc,automation,,
"HELP! How can I update a list of 10K eCommerce data (Description, Logo, etc) in Airtable or Google Sheet or Excel?",https://www.reddit.com/r/automation/comments/ujwcfh/help_how_can_i_update_a_list_of_10k_ecommerce/,automation,"Hey everyone, I have a list of 10K eCommerce names and URLs in Airtable I'd like to update with the company logo, description, industry, etc. What's the best way to go about it?

Don't want to spend a month doing this :(","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Where are you pulling the data to update it with?\n\nIt should be easy to do, assuming you already have the data to update with readily available. A quick console application should suffice. I can do this pretty quickly for like $100.', 'I think it‚Äôs better to write a Google app script to get the description, logo']"
Sending an Instagram message for every google form submission,https://www.reddit.com/r/automation/comments/ujgq7o/sending_an_instagram_message_for_every_google/,automation,"Hi everyone! 

Is there anyway to send an automatic instagram message when a google form is filled out and submitted? The instagram message will be sent to the username the person filling out the google form provides. 

I tried using automation tools like Zapier but they don't seem to have the feature of sending Instagram messages to users that fill out a google form. 

Any help would be much appreciated, 

Cheers!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'you can try asking r/AutomateYourself']"
Automation and political systems discussion,https://www.reddit.com/r/automation/comments/uj6u5v/automation_and_political_systems_discussion/,automation,"In non complicated automated systems there‚Äôs two types and one could be seen as old and one as new or one as proprietary and one versatile but the reality is one type is based on things that are concrete in nature and where any change is minor and always an improvement where the other is based on a world where any business could go under at any moment and keeping with trends is more important than keeping up with needs, a world where recovering investment is more important than efficiency of operation or making things that last both in durability and requirement/need/desire to own. So my discussion prompt is what do you think about the move to using robotic arms and programming for everything instead of more mechanical versions that hardly require electricity and have all safeguards physical, where one motor can often do everything versus needing a software programmer? And my second part of my prompt is what are your options on politics and economics discouraging or encouraging automation use, should there be an incentive or even requirement? Should there be a classification system to identify automation that suits a need versus ones that serve a profit instead? Should a tighter standardization of motors and wear components be implemented as to allow automation of repairs also? Why or why not?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
"Using Node-RED, docker and blogger.com to do some automated content creation",https://cybernotdienst.de/automated-blog-posts-with-blogger-com-node-red-and-miniflux-6ef9154baa2,automation,,
SaaS to make a Facebook and Wordpress Post when a YouTube Video is uploaded?,https://www.reddit.com/r/automation/comments/uhx22n/saas_to_make_a_facebook_and_wordpress_post_when_a/,automation,"Trying to find something to potentially replace/replicate Zapier.  I'm having a lot of trouble with it disconnecting my YouTube account and connecting my Wordpress account.  Any SaaS or LTD out there that will do this?

Essentially, when a YouTube video is posted, I would like a Facebook post to include the Title, Description, and a link to the video.  I would also like a Wordpress blog post to share the same title, with the YouTube description and a link or embed as the body.

TIA!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'No idea of a SaaS that does this, but this should be scriptable.\n\nRetrieve the RSS from Youtube every X amount of time and see if there is a new video. Then https://developers.facebook.com/docs/graph-api/get-started tells you how to make a post on Facebook.', 'Check this out-- [mayalabs.io](https://mayalabs.io)']"
automate excel entry,https://www.reddit.com/r/automation/comments/uht32a/automate_excel_entry/,automation,"hello everyone. I have zero knowledge about coding so if this can't be done without prior coding experience please let me know. 

I work for a good sized company that outfits industrial products in Utah, Nevada, Wyoming and some parts of California. I am apart of our accounts payable department and right the work load is more than we can handle. We switched to Infor CSD in August of 2021 and we are still struggling to get out of the hole we dug. Part of our job is that we need to reconcile the credit card charges our salesmen enter into Wells Fargo. We download an excel worksheet from Excel and then manually enter all the credit charges looking for pricing issues, unit of measure errors or anything else that would hold the entry back from being processed and shown as closed. when doing an entry it goes like this:

&#x200B;

PO number, vendor ID, invoice number, the amount and that's that.

we then move to the next task where we look at the purchase order and look for anything that would make this invoice an issue. if all is clear we just validate, and then final update the entry and we never see it again unless we need access to it for future reference. I was wondering if there was a way to set up a macro or maybe some basic coding that could take the information from Excel, enter for me and then at the end of everything, I will look at any entries that are flagged for an error. 

we have about 4 months of credit cards to reconcile and each month has about 200 or so lines in excel.

if this isn't enough info to go off of please let me know and I can try to provide more. I don't want to post a super long question and risk leaking customer info or anything like that.","['Yes this can probably be done fairly easily with Python as there are libraries specifically for editing Excel files and working with tabular data, especially if your source data is already in a csv file.', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'you can ask r/AutomateYourself', 'You can do this with excel macros and VBA easily', 'That sounds like a macro job, as long as you just copy Excel data into whatever dataset Wells Fargo uses.\nIf I understood that right.']"
How are factories controlled?,https://youtu.be/p_ALH1beGJ8,automation,,
Automation that automatically applies for new appartments,https://www.reddit.com/r/automation/comments/ufaeem/automation_that_automatically_applies_for_new/,automation,"Hi all,

I live in a country where the housing market is pretty rough, my wife and I have been looking for anything bigger than our closet studio for over a year without any succes. It's honestly mood ruining to have to apply multiple times a week for an apartment and not get invited so I thought why not look for an automated solution. A while back I started learning Python for work but I am no where close to create an automation that refreshes the websites that upload the appartments and automatically applies for us. Would you guys know of any software that would do this? Preferably no-code? I would highly appreciate it!","['Are you using a website in particular to look for apartment postings?', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""I'm just getting into Power Automate and it may be able to help you. Essentially, you can try the application by recording your actions and just playing it back at the click of a button. It's free with windows 10 or 11, but you must have a Microsoft account"", ""Quick question before anyone wastes time here:\nCan't you just macro it? \ne.G you always send the same message ctrl+c ctrl+v or are you personalising it?""]"
Are there any aps or software for this?,https://www.reddit.com/r/automation/comments/ufcl56/are_there_any_aps_or_software_for_this/,automation,"Hey, guys. I need a help. I don't know anything about softwares, or about automation but I'd like to send one group's message to another group at the same time the message is received on the first group. Are there any automated softwares available for this? Do you guys know any? Any help is appreciated. Thanks!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'If youre not very familiar with automation and its an email or from a Microsoft product, I‚Äôd use Power Automate.', ""I am not aware of any Android API's that would do this.\nAnother issue is not knowing what group chats (applications) you're talking about.""]"
"I need to detect if the ""make call"" browser window opens when I click a phone number link. How do I do that in Selenium?",https://www.reddit.com/r/automation/comments/uee2bx/i_need_to_detect_if_the_make_call_browser_window/,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', ""Ok, basically this is the idea: if when you click on the link it opens a new tab in the same browser's window, then you can use the functions to change tab (I do not remember their name but you can get it with a quick google search). If this is the situation and you need more help you can send me a message. If it opens a new window I don't know how you can do it.""]"
"I need to detect if the ""email"" browser window opens when I click an email link. How do I do that in Selenium?",https://www.reddit.com/r/automation/comments/uee12t/i_need_to_detect_if_the_email_browser_window/,automation,,"['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Automated light switch with Python (preferably commercialy available),https://www.reddit.com/r/automation/comments/ue4ocd/automated_light_switch_with_python_preferably/,automation,"Hello everyone, I need to get a light or switch to place in a optical research setup. The idea is that while the measurements are being done the light is off, this can be done manually but sometimes we forget and we have to repeat everything. I thought of using a Arduino but there has to be a commercially available device that does this. Any idea?","['I think there‚Äôs too little information here.\n\nWhere is the Python capability needed? If it‚Äôs natively on the device itself, I don‚Äôt believe you would be able to use an Arduino and may need to opt for a RasPI.\n\nThat being said, I‚Äôd like to see how the rest of the process is done. Controlling a relay through an Arduino which is controlled via serial by Python is clunky. All you need to do is turn a relay on and off. There‚Äôs many ways to skin that cat.', ""Really depends on what your setup for the light/measurements/etc., but assuming that we're talking about a computer running something in Python & needing a basic room/lamp bulb turned off while doing so, you could honestly just pick up a hue bulb & hub, then address it directly using pyhue."", 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Presearch auto search,https://www.reddit.com/r/automation/comments/udzmv0/presearch_auto_search/,automation,"Hey everyone! I made a bot with Python that automatically search on Presearch which gives you money for it:

https://github.com/TheDriedWater/presearch-auto-search","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Facebook Leads + Whatsapp auto,https://www.reddit.com/r/automation/comments/udu06a/facebook_leads_whatsapp_auto/,automation,"Hi, I need to automate a message on Whatsapp for when a Lead fills my Facebook ad Lead Form.

Right now with zapier I send an automated email and sms to the leads. But I need to send a WhatsApp. Is it possible?

&#x200B;

Thanks","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'I would do it using selenium. I have no idea if there are more easy way to do it.']"
Automate tweet of Google Slides,https://www.reddit.com/r/automation/comments/udt7ro/automate_tweet_of_google_slides/,automation,"Hi All,

I have a Google slides document (1 slide) that is being updated daily, and I'd like to automate a scenario where I tweet that Google Slides doc (1 slide only) as an Image to Twitter daily at a any particular time of day.

I've been playing around with Zapier without any luck.

Any suggestions or someone has done something similar already?

Thanks","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'I\'d explore Make/Integromat.\n\nThere\'s a chance\\* that you can create this by using the G Slides [""Make an API call"" module](https://www.make.com/en/integrations/google-slides) (under ""actions"") to interact with this [API endpoint](https://developers.google.com/slides/api/reference/rest/v1/presentations.pages/getThumbnail), which allows you to get the thumbnail image (up to 1600px) of a specific slide (after which you can autopost that image to Twitter). \n\n*\\*I\'m saying ""there\'s a chance"" because Google APIs are riddled with limitations, weird auth permissions, and whatnot.*', 'you can ask r/AutomateYourself', 'If you‚Äôre editing the slide every day, can you save as an image and add it to a folder?\n\nThen you can have a Zap that will tweet any new file that‚Äôs been added to that folder?', 'You can do it with google apps script and the google slide api ([https://stackoverflow.com/questions/31662455/how-to-download-google-slides-as-images](https://stackoverflow.com/questions/31662455/how-to-download-google-slides-as-images)) and then feed the image url into zapier.']"
Where to buy online in Germany?,https://www.reddit.com/r/automation/comments/ud8pjk/where_to_buy_online_in_germany/,automation,"Hello guys, greetings from Brazil.

My wife‚Äôs cousin is coming from Germany and this maybe a chance to buy something, as she can bring small stuff.
I can only remember of Amazon and Saturn, are there any other online shop to look for automation and IT stuff like sff pc and cpu?

Thanks in advance!
Vielen Dank!","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Many places, do you speak German (can help in some smaller shops). Check some price portals like www Geizhals at. They also list shops that deliver here in Germany.']"
Automate sending custom messages to each link,https://www.reddit.com/r/automation/comments/ucz48a/automate_sending_custom_messages_to_each_link/,automation,"I have a spreadsheet where first column have the Linkedin profile links I want to connect, the second column B have the custom message I wrote for each contact. Now my workflow is to to open first link in new tab, click on Connect (or click on More then click on Connect button) A pop up will show, Click on message box, copy paste the message for that contact from spreadsheet in the message box and click Send button. This is a task I want to accomplish using automation so was wondering if there is a free software or something that can do this task for me automatically for each profile? I tried Pulover's Macro Creator but somehow its not working as intended. Any help would be appreciated.","['you can ask r/AutomateYourself', 'Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
How to break text into paragraphs at scale in several docs?,https://www.reddit.com/r/automation/comments/uc3k7c/how_to_break_text_into_paragraphs_at_scale_in/,automation,"I have many word docs with about 1500 words each. In each doc, they are all one text without any paragraphs (Though there are sentences with periods in them). I want to break them into paragraphs after every five sentences. How do I do that?","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'Preferred language?', 'This seems like something that could be accomplished easily with LaTex using Overleaf.com it is awesome for separating the formatting from the content. I use it for engineering reports. Although I am just gaining proficiency in it myself, it is a powerful tool.']"
automation technology for business idea,https://www.reddit.com/r/automation/comments/uaxtum/automation_technology_for_business_idea/,automation,"For the last few years there have been multiple times where I encountered repetitive tasks online that I wanted to automate. Typically i'll just hire a developer on Fiverr to create a bot to automate the process. 

&#x200B;

I always thought there had to be an easier way to automate simple mainstream tasks that most people encounter. 

&#x200B;

I created BotWarrior.io to solve this problem. Buy and sell bots to automate and simplify your workflow. 

&#x200B;

Out of curiosity, how do you guys automate easy repetitive tasks? Do you hire someone? Any insights would be super helpful. 

&#x200B;

Ryan","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*', 'The market of people who will take and use third-party micro-apps without concern is shrinking rapidly with each supply-chain exploit.  You need this for a ""sell this bot"" marketplace.', '> how do you guys automate easy repetitive tasks\n\nIn the IT world we automate everything ourselves using scripts and/or automation frameworks because most of what we do is repetitive tasks. What are some examples of \n\n> repetitive tasks online \n\nthat you speak of?']"
Automation in the workspace,https://www.reddit.com/r/automation/comments/u9ahr2/automation_in_the_workspace/,automation,"Wanted to quickly share a quick way to automate your workspaces. FaultFixers is a software that lets you can automate recurring maintenance tasks meaning you'll never have a broken toilet again!

https://www.faultfixers.com/blog/office-maintenance-software-must-have-features","['Thank you for your post to /r/automation!\n     \nNew here? Please take a moment to read our rules, [read them here.](https://www.reddit.com/r/automation/wiki/rules)\n\nThis is an automated action so if you need anything, please [Message the Mods](https://www.reddit.com/message/compose?to=%2Fr%2Frpa) with your request for assistance.\n\nLastly, enjoy your stay!\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/automation) if you have any questions or concerns.*']"
Running unlimited number of tests in less than 60 seconds,https://www.reddit.com/r/selenium/comments/10e0jd6/running_unlimited_number_of_tests_in_less_than_60/,selenium,"I developed an interesting tool that uses AWS lambda to basically parallelize the hell out of running test. It is a test framework specific solution and the current version works really well for pytest selenium/playwright setups. I ran a 10,000 test framework setup in under 60 seconds. Each test was designed to take 8-10 seconds to mimic typical runtimes. 

Would anyone be interested in taking this tool for a trial run in your company?

Some cool features are

1. Use your own AWS account. Support for Azure, GCP coming soon

2. Support for pytest with selenium, appium, playwright tool

3. Flaky test detection logic

4. Number of tests does not impact the overall runtimes too much because cloud scales for you

5. Way way cheap due to low cost of lambda function 

6. Integration with test case management system like TestRail and Zephyr and MicroFocus ALM. Test results are automatically uploaded to your TCM

7. No code changes needed.","['I‚Äôll post a screen recording in a few weeks so that interested folks can take it for a test drive', 'Sounds to good to be true']"
"How to click in this element using selenium which has no id and name,",https://www.reddit.com/r/selenium/comments/10e3wtv/how_to_click_in_this_element_using_selenium_which/,selenium,"<span class=""ui button"" style=""user-select: none;""><em class=""icons"">search</em><i></i></span>

&#x200B;

span = driver.find\_element(By.XPATH, ""//span\[@class='ui button'\]"")

attach=wait(driver,20).until(ec.presence\_of\_element\_located((By.XPATH,""//html//body//div//div\[4\]//span\[4\]//em\[@class='search'\]"")))

ive tried this and also tried it with class name but not working","['Your best bet is the XPath or CSS Selector.\n\nOpen up your browser to inspect the DOM (F12 or DevTools and select Elements in Chrome).  Then depress Ctrl+F to get the find in page bar.  You can test out your XPath or CSS Selector string there.  Ensure you get ""1 of 1"" in your results.\n\nThere are some plug-ins you can get to help you decipher what the best string is to identify the object.\n\nEDIT: Also, do a right-click and Copy on the element in the DOM.  You can copy the CSS Selector or various types of XPaths from their too.', 'Can‚Äôt remember the exact syntax off the top of my head but something like ‚Äú//span[@class=‚Äòui button‚Äô and text()=‚Äòsearch‚Äô]‚Äù might work', ""You could try .ui.button or .ui.button[style*='user-select']"", ""//span\\[@class='ui button'\\]//em\\[text()='search'\\]\n\n&#x200B;\n\nem @ class is icons, not search. Search is the text."", ""How to make that it doesn't click on button that has something near it(already clicked)(maybe can detect png on span class etc? And clicks on another one that doesn't have it.( Same id-s etc ).\n\n\n ( for inbox auto reply ).\n\n\nSomeone help me please"", ""You guys are probably right, but man I can rarely get XPATH to work right for me.  That's even with me copying the xpath straight from the devtools console.  I usually have to find\\_element by ID/class/etc or by partial text match.""]"
This button is physically visible but...,https://www.reddit.com/r/selenium/comments/10e2wcr/this_button_is_physically_visible_but/,selenium,"if you could find a button that doesn't have xpath, cssSelector, id, name, class, className, containsText, or tagName and doesn't have an id name in the DOM - how would you go about finding this button?  It is physically visible and clickable and I have no clue what to ask OpenAI at this point.","['Either look for the parent element (or the parent of the parent, just go up until you find something) if that has some unique property that can be used and search from there, or just use the raw xpath for example if nothing else works. I‚Äôm fairly certain that if the element exists in DOM it does have an xpath.\n\nIf you can share the DOM or the page in question it would be possible to give a more detailed answer that would be suitable for your use case.', ""If it's in the Dom why wouldn't it have an XPath."", 'How can there be no xpath? The one you copy from browser via dev tools might be not good enough, but then you would need to write your own.', 'Is it in a shadow DOM? It‚Äôs been a few years since I last looked into it. Has shadow DOM gotten easier to work with?', 'Just a button with no text on it?  I‚Äôd go with the ‚Äúfind a parent and use a relative xpath‚Äù strategy.', ""I'd say you have shitty developers and they need to go back and fix the code.\n\nAlso, EVERYTHING has an XPath."", 'or do a search against the img src and .click() it.\n\nExample:\n\n`link = driver.find_element(By.XPATH, ""//input[@src=\'images/done.gif\']"")` \n\n`link.click()`']"
Determine who shared a Facebook post,https://www.reddit.com/r/selenium/comments/10e2fu9/determine_who_shared_a_facebook_post/,selenium,"A friend of mine asked me if there was any way of programmatically getting a list of people who shared a post.  I can figure out how to get the number of shares, but haven't figured out how to get the actual list of people.  Anyone know how I can do this or where I can find some code snippets that do it?  

Thanks in advance",['Click on the number of shares. Should open the list of who shared it. Or maybe you click on the number of likes. I forget exactly and Facebook is always changing. But the info is there.']
Hard time populating the find_elements function.,https://www.reddit.com/r/selenium/comments/10dpl7k/hard_time_populating_the_find_elements_function/,selenium,"Hello all, i‚Äôm very new to Selenium and Python: Each time I attempt to use driver.find_elements none of the elements pop up. Same thing with

In fact, it looks like the functions I have are pretty limited, unless maybe I‚Äôm doing something wrong? 

I‚Äôve set up my environment like: 


from selenium import webdriver
from bs4 import BeautifulSoup
from selenium.webdriver.common.keys import Keys

Appreciate any responses!","['One automation debugging protip: use [document.querySelectorAll\\(<selector>\\)](https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelectorAll) for your selector debugging directly in a browser console. This is to make sure you have a correct selector in the first place...', 'Gonna need some more information here, specifically what locator strategy are you using for finding the elements (name, id, xpath, css, etc), how are you declaring the list and what are you doing with the variable you‚Äôve declared representing the list. If you include the error text you are receiving and a screenshot of the dom representing the elements you are trying to find it‚Äôll be easier to assist you.']"
Bot detection on google?,https://www.reddit.com/r/selenium/comments/10cp1vh/bot_detection_on_google/,selenium,"So my code was working fine until a couple of days when I keep getting this error message from google: **Couldn't sign you in.** **This browser or app may not be secure.**

Here is my code:

>from selenium.webdriver import Chromefrom selenium.webdriver import ChromeOptionschrome\_options = ChromeOptions()chrome\_options.add\_argument(""--lang=en-US"")chrome\_options.add\_argument('--disable-blink-features=AutomationControlled')chrome\_options.add\_experimental\_option('prefs', {'intl.accept\_languages': 'en,en\_US'})chrome\_options.add\_experimental\_option('excludeSwitches', \['enable-logging', 'enable-automation'\])chrome\_options.add\_experimental\_option(""useAutomationExtension"", False)chrome\_options.set\_capability('dom.webdriver.enabled', False)chrome\_options.add\_argument(f'--user-data-dir={PATHS.CHROME\_SYS\_PATH}')chrome\_options.add\_argument(f'--profile-directory={sub\_dir}')chrome\_dirver = Chrome(options=chrome\_options, executable\_path=PATHS.CHROME\_DRIVER)chrome\_driver.get(r""[https://www.google.com/](https://www.google.com/)"")

Google keeps throwing the same error over and over!  Is there any way to avoid the detection on chrome?","['I love watching this question get posted here every few days.', ""You can try logging into Google via stack overflow. That used to work, I don't know about now."", ""Try using [undetected chromedriver](https://github.com/ultrafunkamsterdam/undetected-chromedriver) it should make your scraper harder to detect. If that doesn't work then you will need to use proxies.""]"
selenium/Java interview prep,https://www.reddit.com/r/selenium/comments/10cejuc/seleniumjava_interview_prep/,selenium,"Hello, I am planning to apply for some selenium based automation jobs. Anyone can give some tips on which topics in java programming and selenium should I focus on. Thanks in advance",[]
automatically detecting elements or text within the web page using Selenium?,https://www.reddit.com/r/selenium/comments/10axdse/automatically_detecting_elements_or_text_within/,selenium,Im trying to create a script that could automatically detect the content of a web page and based on that content perform some operations .. Is it possible with selenium automation?,"[""The very first line on the [Selenium website](https://www.selenium.dev/) is:  \n\n\nSelenium automates browsers. That's it!\r  \nWhat you do with that power is entirely up to you."", 'Yes. \nMap the elements that may or not appear, and check if they exist.\n\nhttps://stackoverflow.com/questions/9567069/checking-if-an-element-exists-with-python-selenium', 'What do you mean by ""automatically detect the content of a web page""?', 'Yes']"
How to automate a Google login flow using Selenium scripts?,https://www.reddit.com/r/selenium/comments/10ak09w/how_to_automate_a_google_login_flow_using/,selenium,"We are trying to automate a Google login flow in our Selenium script.

When the test runs, Google treats this as suspicious/spam-bot activity, and asks for additional verification. So we get either a Captcha screen or a OTP code login screen.

Has anyone managed to successfully automate this flow?","['No one‚Äôs solved it, that‚Äôs why it exists', 'According to [this](https://stackoverflow.com/a/62244793) answer on Stack Overflow you can use the user data dir if you have already logged in. Not sure if this still works but definitely worth a try.', 'I would suggest playing around with localstore, userdir and disabling clean sessions. From Google login POV Selenium is not different from bots, so they do everything to prevent such possibilities.', ""I've gotten past captia, reCaptia, and 2FA by using webdriver wait (until element shows) and going through the captia manually.  However this means that you'll need to do these parts manually everytime a test case is run.  The automated process resumes soon after."", 'Is this an option?\nhttps://developers.google.com/recaptcha/docs/faq#id-like-to-run-automated-tests-with-recaptcha.-what-should-i-do', ""Google specifically puts code on so that you can't do this. They don't want bots logging in for probably a multitude of reasons. We should all respect that."", 'I‚Äôve logged into gmail html5 using selenium as recently as 2020. Otherwise can you feed the auth token into local storage?']"
[python] What's the proper way to handle lazy loading of elements?,https://www.reddit.com/r/selenium/comments/10anbsq/python_whats_the_proper_way_to_handle_lazy/,selenium,"I'm currently grabbing all the elements that are loaded on page load, and iterating over them.

    elements = driver.find_elements(By.CSS_SELECTOR, 'button.group.visible')  
    for element in elements:      
        ... 

Could someone advise me on how to load the next batch and apply them to the elements? Lazy loading also pools the elements so the data changes if you scroll up and down.",[]
Avoid StaleElementReferenceException,https://www.reddit.com/r/selenium/comments/10ael88/avoid_staleelementreferenceexception/,selenium,"Hi guys,
I had this error : StaleElementReferenceException
The scenario is :  i Searched on google and visited each link in  result of google but when i returned from the first link and clicked another link i got this error ""StaleElementReferenceException""

Selenium python 
Please help me and thanks guys.","['You likely need to re-declare your element locators at the start of your for loop (guessing you are getting all the  a tags in a list, then attempting to loop through it). Problem is that when you tell selenium what to do, the dom changes after you click the first link, so after you‚Äôve returned to the list of results your original element list declaration is stale. It‚Äôs either that or your aren‚Äôt waiting long enough between trying to click the next link. Stale element means the dom changed between declaration and action.']"
Add code once automation has started,https://www.reddit.com/r/selenium/comments/10a0b36/add_code_once_automation_has_started/,selenium,"Hello,

&#x200B;

This may be a more general python question rather than specific to selenium. I am fairly new to python and selenium, but I'm typically pretty good at Google, but I can't find this answer.

&#x200B;

I use selenium to automate several admin tasks (user opens a ticket, I have selenium take that info and put it in the vendor system is one use case). What I am looking for is a way to run selenium to sign in to the sites in the morning, and I can insert and run a block of code as needed. (Same block, the only thing that changes is the ticket number)

&#x200B;

Right now I am using VScode to write in and run when I have several that are ""ready"", but that kinda defeats what I am looking for. Is there an editor that I can run that will keep Chrome open and that I can add text to as I go?

&#x200B;

Thank you!",['Could you go about implementing a Trigger method?']
Kijiji Blank Page,https://www.reddit.com/r/selenium/comments/10a4zy0/kijiji_blank_page/,selenium,"Anyone know why Kijiji blank pages after clicking ""Post ad"" manually? All i've done is open the homepage with geckobrowser. Any ideas would be greatly appreciated",[]
General advice on setting up tests,https://www.reddit.com/r/selenium/comments/109gvcx/general_advice_on_setting_up_tests/,selenium,"Hello! I am pretty new to testing and Selenium and I feel as though I'm *not getting it*. 

I'm currently building an e-commerce portfolio app with Django; right now I have my accounts app set up to register new users, log them in/out, and delete their accounts. Presumably I'd like to test all those features in a script: load my app, create a new user, log that new user in, log them out, delete the account.

I've encountered a countless number of technical difficulties even performing one of these tasks. I've found the official documentation to be contradictory and confusing (maybe it's just me); every tutorial I've found so far has used outdated syntax and only delves into the most uselessly superficial tasks (loading a URL and that's it).

So I'd like some advice on where to go to figure out what the process is for testing what I'm aiming to test. **What's the general strategy for setting up these tests**? Are there any up-to-date resources available that focus on more useful testing processes?

For a specific example of a problem I'm encountering: how does one handle loading a different page during the test? I have been able to register a new user; on clicking ""submit,"" it takes them to a login page. How do I wait for the new login page to load before continuing? Implicitly waiting doesn't seem to do anything, but `time.sleep()` does.

Even if someone has a link to a repo that includes some tests in Python (especially if it's Django!) would be wonderful to see. I learn by example pretty well. Thanks for any advice.","['This is a hard one to answer, as there aren‚Äôt any specific problems posed.  \n\nFor the redirect to the new page, you are on the right track.  As a general rule I would be using a selenium wait on the WebElement that I was looking to use on the new page.\n\nThe general strategy may vary depending on tech used.  Currently I am using Cucumber with Selenium.  So I will write my test in Gherkin.  It‚Äôs BDD, so that fails, but with an error that tells me to implement the code.  I implement it and then write the next test.  All of my tests are independent, meaning that each test logs in, completes the test and logs out.  There is no dependency between tests.\n\nSome things you may want to look at (off the top of my head) are the selenium page object model, data injection to share your web driver, maybe a web driver factory if you are testing multiple browsers.\n\nI feel you.  The documentation you find around the web can be sparse, simple, and outdated, but it is out there.  Even what I have written here may be outdated as a newer version of Selenium has simplified the process of creating/using the webdriver.  I haven‚Äôt looked at that closely yet.']"
StaleElementReferenceException,https://www.reddit.com/r/selenium/comments/10900vs/staleelementreferenceexception/,selenium,"im using python3 selenium with firefox.

how can i avoid this particular exception? it seems like everytime i try to do a loop over elements i get this exception sooner or later.

&#x200B;

    data_and_resources_ul = driver.find_element(By.CLASS_NAME,'resource-list')
    csv_or_tsv_total = data_and_resources_ul.find_elements(By.CLASS_NAME,'format-label')
    for csv_or_tsv in csv_or_tsv_total:
            csv_or_tsv.click()
            time.sleep(1)
            navbar = driver.find_element(By.CLASS_NAME,'tabs--primary')
            all_buttons = navbar.find_elements(By.TAG_NAME,'a')
            back_to_dataset = [a for a in all_buttons if a.get_attribute('href').endswith('dataset')]
            back_to_dataset[0].click()
            time.sleep(1)

this is my code csv\_or\_tsv.click() takes me to a new page so new url then i will add more code there after that i press back\_to\_dataset\[0\].click() takes me to the previous page where csv\_or\_tsv\_total exists.

so my for loop should not fail because the same elements where gathered in the first place when i was on that page initially.

the for loop crashes on second iteration with StaleElementReferenceException

how can i fix this?","[""In my experience stale element means the DOM has changed due to asynchronous loading, and/or elements being updated somehow.\n\nI don't know python at all, but try to do some sort of exception catching and then re-build your list of elements to keep it current."", ""This was asked yesterday. My solution was to catch the exception and retry infinitely. It won't be stale forever. Most Devs are understandably scared of infinite loops though."", ""What is likely happening is that because csv_or_tsv is declared outside the loop, changes to the page are causing it to become stale and it can't find it again.\n\nIt might take longer for your script to run but you might be able to refresh your page as the first step inside the loop and see if that works."", 'conditional waits can help too.   https://www.testim.io/blog/selenium-wait-until-element-is-visible/', 'I write my tests in java, I do while loop with try/catch inside and re-initialisation of the element throwing the exception.']"
Dealing with StaleElementReferenceException error,https://www.reddit.com/r/selenium/comments/1086dfo/dealing_with_staleelementreferenceexception_error/,selenium,"Hi,

I am new to Selenium and I am getting a StaleElementReferenceException error but not sure why. I have tried to debug to no avail. It would be great if someone could point me to the issue. I have posted below links to the code on Gist and the stack trace as well. I have posted the code that contains the page object, the test and the stack trace.

NB: I have tried to link to the code I have posted on  Github Gist for easier reading  but it seems that Reddit will not allow external links in posts, which is unfortunate.  

[EditCustomer.java](https://EditCustomer.java)This is the page object.  

    package com.internetBanking.pageObjects;
    
    import java.time.Duration;
    
    import org.openqa.selenium.WebDriver;
    import org.openqa.selenium.WebElement;
    import org.openqa.selenium.support.CacheLookup;
    import org.openqa.selenium.support.FindBy;
    import org.openqa.selenium.support.How;
    import org.openqa.selenium.support.PageFactory;
    
    public class EditCustomerPage {
    	WebDriver driver;
    	
    	public EditCustomerPage(WebDriver driver) {
    		this.driver = driver;
    		PageFactory.initElements(driver, this);
    	}
    	
    	@FindBy(how = How.XPATH, using = ""//a[contains(text(),'Edit Customer')]"")
    	@CacheLookup
    	WebElement lnkEditCustomer;
    	
    	public void clickEditCustomer() {
    		lnkEditCustomer.click();
    	}
    	
    	
    	@FindBy(how = How.NAME, using = ""cusid"")
    	@CacheLookup
    	WebElement txtCustomerID;
    	
    	public void setCustomerID(String customerId) {
    		txtCustomerID.sendKeys(customerId);
    	}
    	
    	
    	@FindBy(how = How.NAME, using = ""AccSubmit"")
    	@CacheLookup
    	WebElement btnSubmit;
    	
    	public void submit() {
    		btnSubmit.click();
    	}
    	
    	
    	@FindBy(how = How.NAME, using = ""city"")
    	@CacheLookup
    	WebElement txtCity;
    	
    	public void custCity(String city) {
    		txtCity.sendKeys(city);
    	}
    	
    	public String getCustCity() {
    		return txtCity.getText();
    	}
    	
    	
    	@FindBy(how = How.NAME, using = ""state"")
    	@CacheLookup
    	WebElement txtState;
    	
    	public void custState(String state) {
    		txtState.sendKeys(state);
    	}
    	
    	public String getCustState() {
    		return txtState.getText();
    	}
    	
    	
    	@FindBy(how = How.NAME, using = ""sub"")
    	@CacheLookup
    	WebElement btnSubmitForm;
    	
    	public void submitForm() {
    		btnSubmitForm.click();
    	}
    	
    	
    	
    
    }
    

TC\_EditCustomer.java  This is the test

    package com.internetBanking.testCases;
    
    import java.io.IOException;
    import java.time.Duration;
    
    import org.testng.Assert;
    import org.testng.annotations.Test;
    
    import com.internetBanking.pageObjects.EditCustomerPage;
    import com.internetBanking.pageObjects.LoginPage;
    import com.internetBanking.utilities.XLUtils;
    
    public class TC_EditCustomer_004 extends BaseClass {
    	EditCustomerPage ec;
    	LoginPage lp;
    
    	@Test
    	public void EditCustomer() throws IOException, InterruptedException {
    		driver.manage().timeouts().implicitlyWait(Duration.ofSeconds(30));
    		driver.get(baseURL);
    		ec = new EditCustomerPage(driver);
    		lp = new LoginPage(driver);
    		
    		if (lp.iframeIsExists()) {
    			if (lp.iframeIsVisible()) {
    				logger.info(""GDPR popup displayed"");
    				System.out.println(""GDPR popup displayed"");
    				lp.switchToFrame();
    				lp.clickAccept();
    				lp.switchToDefault();
    			}
    		}
    
    		lp.setUserName(username);
    		lp.setPassword(password);
    		lp.clickSubmit();
    
    		ec.clickEditCustomer();
    
    		// retrieve customer number
    		String path = System.getProperty(""user.dir"") + ""\\src\\test\\java\\com\\internetBanking\\testData\\login.xls"";
    		String customerNumber = XLUtils.getCellData(path, ""Sheet1"", 1, 2);
    
    		// fill cust id and submit
    		ec.setCustomerID(customerNumber);
    		ec.submit();
    
    		// edit customer
    		ec.custCity(""Sheffield"");
    		ec.custState(""Yorkshire"");
    		ec.submitForm();
    
    		// dismiss alert
    		driver.switchTo().alert().accept();
    
    		// fill cust id and submit
    		Thread.sleep(5000);
    		ec.clickEditCustomer();
    		System.out.println(""Clicked Edit Cistomer"");
    		ec.setCustomerID(customerNumber);
    		ec.submit();
    		
    		//Verify if successfully edited
    		if(ec.getCustCity().equalsIgnoreCase(""Sheffield"") && ec.getCustState().equalsIgnoreCase(""Yorkshire"")) {
    			Assert.assertTrue(true);
    		}
    		else {
    			Assert.assertTrue(false);
    		}
    
    	}
    
    }
    

Stack Trace

    org.openqa.selenium.StaleElementReferenceException: stale element reference: element is not attached to the page document
      (Session info: chrome=107.0.5304.107)
    For documentation on this error, please visit: https://selenium.dev/exceptions/#stale_element_reference
    Build info: version: '4.5.0', revision: 'fe167b119a'
    System info: os.name: 'Windows 10', os.arch: 'amd64', os.version: '10.0', java.version: '11.0.11'
    Driver info: org.openqa.selenium.chrome.ChromeDriver
    Command: [ec39b1f7efd2e4cc6d31633d4c66d44b, sendKeysToElement {id=3c29de5c-eb57-4512-b455-b6a4bd6d35d6, value=[Ljava.lang.CharSequence;@3313d477}]
    Capabilities {acceptInsecureCerts: false, browserName: chrome, browserVersion: 107.0.5304.107, chrome: {chromedriverVersion: 107.0.5304.62 (1eec40d3a576..., userDataDir: C:\Users\fsdam\AppData\Loca...}, goog:chromeOptions: {debuggerAddress: localhost:50466}, networkConnectionEnabled: false, pageLoadStrategy: normal, platformName: WINDOWS, proxy: Proxy(), se:cdp: ws://localhost:50466/devtoo..., se:cdpVersion: 107.0.5304.107, setWindowRect: true, strictFileInteractability: false, timeouts: {implicit: 0, pageLoad: 300000, script: 30000}, unhandledPromptBehavior: dismiss and notify, webauthn:extension:credBlob: true, webauthn:extension:largeBlob: true, webauthn:virtualAuthenticators: true}
    Element: [[ChromeDriver: chrome on WINDOWS (ec39b1f7efd2e4cc6d31633d4c66d44b)] -> name: cusid]
    Session ID: ec39b1f7efd2e4cc6d31633d4c66d44b
    	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    	at org.openqa.selenium.remote.codec.w3c.W3CHttpResponseCodec.createException(W3CHttpResponseCodec.java:200)
    	at org.openqa.selenium.remote.codec.w3c.W3CHttpResponseCodec.decode(W3CHttpResponseCodec.java:133)
    	at org.openqa.selenium.remote.codec.w3c.W3CHttpResponseCodec.decode(W3CHttpResponseCodec.java:53)
    	at org.openqa.selenium.remote.HttpCommandExecutor.execute(HttpCommandExecutor.java:184)
    	at org.openqa.selenium.remote.service.DriverCommandExecutor.invokeExecute(DriverCommandExecutor.java:167)
    	at org.openqa.selenium.remote.service.DriverCommandExecutor.execute(DriverCommandExecutor.java:142)
    	at org.openqa.selenium.remote.RemoteWebDriver.execute(RemoteWebDriver.java:547)
    	at org.openqa.selenium.remote.RemoteWebElement.execute(RemoteWebElement.java:257)
    	at org.openqa.selenium.remote.RemoteWebElement.sendKeys(RemoteWebElement.java:113)
    	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    	at org.openqa.selenium.support.pagefactory.internal.LocatingElementHandler.invoke(LocatingElementHandler.java:52)
    	at com.sun.proxy.$Proxy24.sendKeys(Unknown Source)
    	at com.internetBanking.pageObjects.EditCustomerPage.setCustomerID(EditCustomerPage.java:34)
    	at com.internetBanking.testCases.TC_EditCustomer_004.EditCustomer(TC_EditCustomer_004.java:57)
    	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    	at org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:139)
    	at org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:677)
    	at org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:221)
    	at org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)
    	at org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:962)
    	at org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:194)
    	at org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:148)
    	at org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)
    	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
    	at org.testng.TestRunner.privateRun(TestRunner.java:806)
    	at org.testng.TestRunner.run(TestRunner.java:601)
    	at org.testng.SuiteRunner.runTest(SuiteRunner.java:433)
    	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:427)
    	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:387)
    	at org.testng.SuiteRunner.run(SuiteRunner.java:330)
    	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)
    	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:95)
    	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1256)
    	at org.testng.TestNG.runSuitesLocally(TestNG.java:1176)
    	at org.testng.TestNG.runSuites(TestNG.java:1099)
    	at org.testng.TestNG.run(TestNG.java:1067)
    	at org.testng.remote.AbstractRemoteTestNG.run(AbstractRemoteTestNG.java:115)
    	at org.testng.remote.RemoteTestNG.initAndRun(RemoteTestNG.java:251)
    	at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:77)

&#x200B;","['""@CacheLookup"" is used to store the WebElements once they are located so that the same instance in the DOM can always be used. Basically it intsructs the InitElement() method to cache the element so you don\'t have to search for it over and over again, which is great if this element and the DOM aren\'t going to change. \n\nWhat this means is that if that element changes in any way or the DOM changes in any way which affects that element while you are interacting with this page your cached object reference will become stale. Instead you want to just find the object directly each time you are going to use it (In this case).', 'This could help\n\nhttps://www.softwaretestingmaterial.com/stale-element-reference-exception-selenium-webdriver/', ""I just catch that exception and retry infinitely until it finds it. Feels hacky, but works. It won't be stale forever.""]"
Targeting the discord chatbox,https://www.reddit.com/r/selenium/comments/106p9pt/targeting_the_discord_chatbox/,selenium," Hello, I'm a new selenium user and I'm trying to use selenium to make a script to help me generate images with MidJourney while I'm afk. So far I've only managed to get selenium with Mocha to log me in to discord and then change the url to my conversation with MJ. But now I have problems figuring out how to actually send data into the chat. It uses a div instead of an input field, so I'm guessing it's some kind of javascript involved.

Does anyone have any experience with entering messages into the discord app chat? It would be so nice if someone could give me a heads up on this one!","['No Idea about it, but you can use Selenium ide and record the action, to see which elements does it interact with.']"
Need help on getAttribute() method,https://www.reddit.com/r/selenium/comments/104shey/need_help_on_getattribute_method/,selenium,"Hi guys, need some help on getAttribute method - tho the attribute is present, it‚Äôs returning null. What might be the problem? Thank you in advance!","['Should I spin the wheel, or do I already have enough money to buy a line of code?  \n\nThis is a really hard puzzle without any clues.', 'Post your code please. For extra credit, post a link to the page that contains the element in question.', 'Looks like you\'re trying to run before you can walk.\n\nStart in the simplest form first of all.\n\nLocate your element.\n\nelement.getAttribute(""valid attribute here"").\n\nMake sure whatever you\'re looking for actually exists. You might be trying to return value instead of text etc.\n\nCan you give us the HTML of the element you\'re using?', 'Would something like this help? (This is in C#.)\n\npublic static Dictionary<string, string> GetAttributes(this IWebElement element) { const string script = ""var items = {};"" + ""for (index = 0; index < arguments\\[0\\].attributes.length; ++index) "" + ""{"" + ""     items\\[arguments\\[0\\].attributes\\[index\\].name\\] = arguments\\[0\\].attributes\\[index\\].value"" + ""}"" + ""return items;"";\n\n    return element.ExecuteJavaScript<Dictionary<string, string>>(script, element);\n\n}\n\npublic static T ExecuteJavaScript<T>(this IWebElement element, string script, params object\\[\\] args) {  // Execute client-side JavaScript on IWebElement with return type IWebDriver driver = ((IWrapsDriver)element).WrappedDriver; IJavaScriptExecutor js = (IJavaScriptExecutor)driver; return (T)js.ExecuteScript(script, args); }\n\nUgh! I can\'t get the formatting right...']"
Selenium for Java or Python - advice sought,https://www.reddit.com/r/selenium/comments/1044cgj/selenium_for_java_or_python_advice_sought/,selenium,"Hi, I am a fairly beginner programmer with a strangely specific set of skills as a QA engineer. I have maintained test suites in a previous jobs which included adding and updating test code in Laravel and a different one using java. 

I have never set one up from scratch though and am a bit more comfortable building from the ground up with python but I wanted to get some input on which framework is better for a media focused site (think something similar to like Spotify or something).

Thanks in advance for your thoughts.","['There are 3 things here:\n\n1-Using Selenium\n2-Using Java\n3-Using Python\n\n\nFirst use Selenium only if you are working with websites. Selenium does not work on apps on Laptops or mobile apps on which most media and Spotify apps work\n\nSecond, Java is a powerful language with many libraries but not as Powerful as Python when it comes to add on libraries for AI and Machine learning or even Data Analysis algorithms\n\nThird Python can be used well with Selenium just like Java, although Java libraries for selenium seem to be older, and therefore stable builds to use with Selenium']"
Run Python-selenium bot on Gitlab,https://www.reddit.com/r/selenium/comments/103w64j/run_pythonselenium_bot_on_gitlab/,selenium,"Hi everyone

Is it possible to run a python-selenium task automator on Gitlab 

Pardon me if this is a silly question, I'm pretty new here, dunno much about gitlab CI pipeline and stuff

Thanks in advance",[]
"where i can find real examples with selenium java, i mean real in production scripts to pr√°ctica, bye level (",https://www.reddit.com/r/selenium/comments/103pnth/where_i_can_find_real_examples_with_selenium_java/,selenium,"Tryng from beginner to advance
Sorry for My bad English guys
Cya and thx",['look at examples in github.  \nhttps://github.com/nusratahmed/maven-selenium-webdriver-testng-example-project/blob/master']
Python & Selenium - help / ideas,https://www.reddit.com/r/selenium/comments/102sxzq/python_selenium_help_ideas/,selenium,"Hi All,

This probably isn't the cleanest code anyone has seen but, currently I am looking for some help or even ideas.   This code I've made is a project for fun, reason why I made this is I like to travel and yes I get there are other things like Hopper and FlightTracker but wanted to try some things on my own.

&#x200B;

**Here is what the code does:** It goes to the [AA.com](https://AA.com) site > Searches for the airport I depart from and want to arrive > Enters in the travel dates > Searches for them > AA (Tells me the dates are incorrect) I tell it to hit the submit button again  and it works > Then it takes a screen shot of the  depart flight of the first half of the page, saves it in my downloads then  clicks on the first box because it is the cheapest > Then Takes a screenshot of a return flight and saves it to my download.

(I haven't put this code on reddit but if anyone wants it I can easily give it to them.)  The next steps are I have another script run a couple minutes after > Picks up the files I saved to my downloads > Attaches it to an email and then the email sends it to me)

&#x200B;

**What i'm trying to get help with is i'm trying to get rid of the old way screenshots and putting this info into an excel document, or even put text into an email with Flight number, Price, Date, Time... ETC but i've ran into a road block and i'm not even sure if this is possible. Would love some help if anyone has experience.**

&#x200B;

`from turtle import clear`
`from selenium import webdriver`
`from selenium.webdriver.common.keys import Keys`
`from selenium.webdriver.common.by import By`
`from selenium.webdriver.support.ui import WebDriverWait`
`from selenium.webdriver.support import expected_conditions as EC`
`import timeimport os`

`if os.path.exists(""C:/Users/Test/Downloads/AA/(Filename).png""):os.remove(""C:/Users/Test/Downloads/AA/(Filename).png"")else:print(""The file does not exist"")`

`if os.path.exists(""C:/Users/Test/Downloads/AA/(Filename2).png""):os.remove(""C:/Users/Test/Downloads/AA/(Filename2).png"")else:print(""The file does not exist"")`

`chrome_options = webdriver.ChromeOptions()chrome_options.add_argument(""--incognito"")driver = webdriver.Chrome(executable_path=""C:/Users/Test/Downloads/chromedriver_win32/chromedriver.exe"",options=chrome_options)`

#Variables
`ID1 = ""slice0Flight1MainCabin""`
`NAME = ""segments[0].orgin""`
`NAME1 = ""segments[0].destination""`
`NAME2 = ""segments[0].travelDate""`
`NAME3 = ""segments[1].travelDate""`
`NAME4 = ""closeBannerButton""`
`XPATH = ""//*[@id='flightSearchSubmitBtn']""`
`XPATH2 = ""//*[@id='slice0Flight1MainCabin']""`
`LINK_TEXT = ""https://www.aa.com/booking/find-flights""`

`driver.get(LINK_TEXT)`

`print(driver.title)`

`time.sleep(10)`

`button = driver.find_element(By.NAME, NAME4)`[`button.click`](https://button.click)`()`

`search = driver.find_element(By.NAME, NAME)search.send_keys(""PHX"")`

`search = driver.find_element(By.NAME, NAME1)`

`search.send_keys(""LHR"")`

`search = driver.find_element(By.NAME, NAME2)`

`search.send_keys(""09/20/23"")`

`time.sleep(5)search = driver.find_element(By.NAME, NAME3)`

`search.send_keys(""09/27/23"")`

`time.sleep(5)button = driver.find_element(By.XPATH, XPATH)`

[`button.click`](https://button.click)`()`

`#Sleep timertime.sleep(45)`

`button = driver.find_element(By.XPATH, XPATH)`

[`button.click`](https://button.click)`()`

`#Sleep timertime.sleep(20)`

`driver.execute_script(""window.scrollTo(0,500)"")driver.get_screenshot_as_file('C:/Users/Test/Downloads/AA/(FileName).png')`

`#Sleep timer`

`time.sleep(20)`

`button = driver.find_element(By.ID, ID1)`

`driver.execute_script(""arguments[0].click();"", button)`

`time.sleep(8)`

`driver.execute_script(""window.scrollTo(0,700)"")`

`driver.get_screenshot_as_file('C:/Users/Test/Downloads/AA/(FileName2).png')`

`driver.quit()`

&#x200B;

&#x200B;

Edit1: Weird spacing in my post","[""Firstly, for the code lookin' weird, you need to add 4 spaces to the beginning of each of your code lines to make it look good:\n\n    like this\n\nSecondly, you might want to check out the [csv](https://docs.python.org/3/library/csv.html) library in Python. It's built-in, so you already have it, and it speaks Excel by default. You can write a CSV with all your data and open it in Excel!"", 'I would read the text off the page with selenium, put it into whatever format you want with python (text, csv, whatever) and mail it.', ""Is there a reason you're doing this through the front end instead of the much faster and easier api calls?"", 'Where are all the NAME1-4 VARS VALUE COMES FROM?']"
Is it possible to create a HTML button to run my Selenium script?,https://www.reddit.com/r/selenium/comments/10289jw/is_it_possible_to_create_a_html_button_to_run_my/,selenium,I've been looking for the longest time wondering if this was even possible. I've been told that the way to do it is to setup a CI like Jenkins and run it via a API trigger. Is there anyway I can do this without having to run the API?,"['You can schedule your Jenkins job to run at a particular time without having to run via an  API trigger', 'Setting up Jenkins will also give you a button to run your build, which you can set up to just run your Selenium script... so you kinda get both. :)', 'Why do you need it on the html button? What about cron job']"
java error when using selenium,https://www.reddit.com/r/selenium/comments/101q3oe/java_error_when_using_selenium/,selenium,"i have been trying to fix a problem for a while. i am using eclipse luna which is an out-of-date version, I'm doing this so I can use larva but basically, I'm having an issue with setting up selenium. can anyone help me out?

code:

	 WebDriver driver = new ChromeDriver();

System.setProperty(""webdriver.chrome.driver"", ""C://Program Files//chromedriver//chromedriver.exe"");

	 driver.get(""[www.google.com](https://www.google.com)""); 

&#x200B;

error:

Exception in thread ""main"" java.lang.IllegalStateException: The path to the chromedriver executable must be set by the webdriver.chrome.driver system property; for more information, see [http://code.google.com/p/selenium/wiki/ChromeDriver](http://code.google.com/p/selenium/wiki/ChromeDriver). The latest version can be downloaded from [http://code.google.com/p/chromedriver/downloads/list](http://code.google.com/p/chromedriver/downloads/list)","['Set the system property before you instantiate the chrome driver. Basically move your second line of code up before the first line.', 'Add the system properties in Windows, then Exit and relaunch (not just restart) your IDE so that it can read the newly added system variables.']"
"[C#] How to resolve ""Cannot access a disposed object"" error",https://www.reddit.com/r/selenium/comments/zzf828/c_how_to_resolve_cannot_access_a_disposed_object/,selenium,"Hey, folks. I've got an error that keeps coming up in a variety of tests, seemingly at random. I'm sure it's not random, but I can't identify the pattern (and subsequently the fix).

For context I have 29 tests running on windows VMs through Azure DevOps. I've got it set to 10 threads (browsers) but I can change that.

The error comes from somewhere I wouldn't expect it. Basically, I'm waiting for the invisibility of an element. Something like:

>return wait.Until(ExpectedConditions.InvisibilityOfElementLocated(By.XPath(locator)));

or

>element.Click();

This isn't a complicated line of code, and generally speaking if it fails I would expect to get an exception. But ""Cannot access a disposed object"" doesn't really tell me what the problem is or how to resolve it.

&#x200B;

It's important to note that these tests don't fail when I run them on my machine against a browser (i.e. not in a VM). I'm not sure if it has something to do with timing, with threading. Any clues are appreciated.","[""If you run only one thread, does it happen also?\n\nI am asking because I saw this explanation for the same error message in a different [situation](https://www.appsloveworld.com/csharp/100/433/cannot-access-a-disposed-object-a-common-cause-of-this-error-is-disposing-a-con):\n\n> It happens because all dependencies in the main tread are disposed when its execution finishes and you're trying to access them in another thread. To deal with this situation you need to create a scope in your background thread and resolve AuthorizedServiceService there:"", 'Never got this error, and I also have ran Selenium on C# on Azure.\n\nHow are you initializing and disposing the driver?']"
Unable to pull element for resource,https://www.reddit.com/r/selenium/comments/zwrznr/unable_to_pull_element_for_resource/,selenium,"Hey there yall! I've been trying to pull a element  from the following line of code: ```
<span tabindex=""0"" role=""link"" class=""regular-login-link clickable"">Regular Login</span>
``` and then have selenium click it. Issue is, it always says that it cant find the element. Doesn't matter if I try to use xpath, css selector, class name, nothing. `driver.find_element(By.CSS_SELECTOR, "".sso-login"").click()` Is the current line that tries to pull it, and then click it.","['Where are you getting "".sso-login""? The element in question doesn\'t have that class attribute, so it\'s not surprising Selenium can\'t find the element using that CSS selector. Try something like ""span.regular-login-link""', ""Three things come to mind:\n\n1) Have you tried using a javascript executor to click the element?\n\n2) Have you tried using some waits that return booleans (isclickable, exists, stuff like that) To make sure you've got it right?\n\n3) Is it in an iframe?""]"
Youtube Ad Detector,https://www.reddit.com/r/selenium/comments/zwarjd/youtube_ad_detector/,selenium," I‚Äôve been trying to use python selenium to watch YouTube videos for me and collect data. Getting the data is fairly easy, however, I run into problems when an ad pops up in YouTube. 

 For some reason, I can't figure out how to detect whether or not I have an ad.

  My current function is: 

def check\_ad():  try:             

WebDriverWait(driver, 20).until(         EC.presence\_of\_element\_located(driver.find\_element\_by\_xpath('//\*\[@id=""simple-ad-badge:g""\]'))             )           

print(""Ad detected"")        

 except:            

 print(""No Ad"")

Does anyone know any other way I can do this?","['Why not use Adblock instead?', 'Just do what I do and get YouTube premium family slot for a lot cheaper in Etsy or eBay. I was able to get 12 months for $50 and it‚Äôs been working perfectly fine for 4 months now.This is where I got it from  \nhttps://gameflip.com/profile/us-east-1:75bddc30-362e-4cf8-809c-286b43724d3f/john-a']"
How To Story an Instagram selenium and python,https://www.reddit.com/r/selenium/comments/ztw9h6/how_to_story_an_instagram_selenium_and_python/,selenium,"hi

please help me

I use

`mobile_emulation = {""deviceMetrics"": { ""width"": 412, ""height"": 914, ""pixelRatio"": 3.0 },""userAgent"": ""Mozilla/5.0 (Linux; Android 4.2.1; en-us; Nexus 5 Build/JOP40D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/41.0.1025.166 Mobile Safari/535.19"" }`

`chrome_options.add_experimental_option(""mobileEmulation"", mobile_emulation)`

But after sending with the message

 `rotate your device to add to your story`",['Share the full code']
"C# - Element.Click() returns error, after waiting for element to be clickable.",https://www.reddit.com/r/selenium/comments/ztqe7u/c_elementclick_returns_error_after_waiting_for/,selenium,"Hey, folks. I'm losing my mind on this one. I have this block of code:

>getWaitUtils.waitForClickabilityOfElement(GetElement(elementName));  
>  
>GetElement(elementName).Click();

The first line uses this:

>return wait.Until(ExpectedConditions.ElementToBeClickable(givenElement));

So I have an element (IWebElement, since I'm in C#). I wait for that element to be clickable. That line passes. The next line attempts to click the element (that selenium has confirmed is clickable). I get an error:

>OpenQA.Selenium.ElementClickInterceptedException: element click intercepted: Element is not clickable at point (1173, 1113)

I don't get it. What's the point of the wait if the element can't be clicked? What do?","[""There's probably other element over the element you want to click.\nCreate a try/catch and take a screenshot in the catch. See if there's any other element over it."", 'Try javascript click which overlooks some element padding', 'I can shed some light here. Until ElementToBeClickable  does basically 3 things.  It\'ll make sure the element gets found (exists), then it will check element element.isDisplayed() (is visible)   and element.isEnabled() \n\nIf all 3 are true then it passes.  enabled is pretty reliable but ""is displayed?"" what does it mean to be ""visible""? dunnow. that\'s up to the webdriver to decide. if a transparent element overlaps a button is it still visible? if an element only partially overlaps is it visible? if it\'s off screen is it visible? ¬Ø\\\\\\_(„ÉÑ)_/¬Ø\n\n\nnext I\'m going to guess this is chrome. which seems to be terrible at deciding where an element is when you call WebElement.Click().  Sometimes where on the element it decides to click doesn\'t actually line up with where it\'s rendered.  So adjacent elements seem to intercept the click even when nothing overlaps. \n\nwe use Java not C# but you can use an ""Action"" instead of just Element.click which often moves the mouse to the correct location \n\n        final Actions actions = new Actions(driver);\n        actions.moveToElement(elm).click().perform();\n\n\nwe also use a scroll script to make sure the element is on the screen. hasn\'t been a problem in a while but a while back clicks would fail because the element was out of view. \n\n    ((JavascriptExecutor) driver).executeScript(""arguments[0].scrollIntoView(true);"", elm);', ""Other have posted this but it sounds like there is something overlapping the element you want to click.  In the error does it tell you anything about the element that is intercepting the click?  It's been a minute since I had this specific error but I think I recall the full error containing some info about the element that would receive the click.\n\n&#x200B;\n\nIf that info is available check the console to see what that element is and maybe that will lead you to your answer.  Something else I've had some success with is using Ignore Exceptions during my waits so that if an interception is encountered then the wait just fails and starts the loop again.  Hope you get it solved!""]"
Flush all like buttons,https://www.reddit.com/r/selenium/comments/zrmis1/flush_all_like_buttons/,selenium,"Hello, 

I need help with iterating some like buttons on my LinkedIn feed. I was able to use the contains ""like"" descriptor to find all the buttons and scroll the page, but my current function keeps clicking the 1st like button even though it is not visible any longer. I have attempted to flush the variable, but the driver retains the original button as its main. Snippet below: 

&#x200B;

`def rerun():`  
 `print('running like function ')`  
 `all_buttons = buttons = driver.find_elements('xpath', ""//button[contains(.,'Like')]"")`  
 `like_buttons = [btn for btn in all_buttons]`  
 `while len(like_buttons) >= 1:`  
`for btn in like_buttons:`  
`driver.execute_script(""arguments[0].click();"", btn)`  
`driver.execute_script(""window.scrollBy(0,3000)"","""")`  
`time.sleep(2)`  
`del like_buttons`  
`del all_buttons`  
`print('bot is liking')`  
`rerun()`","[""Could you format your code in a single code block instead of all separate lines? Currently it's difficult to parse your code because you've lost the indentation."", 'Make it a do while statement and put your do at the very top so it finds the elements that only match the liked elements and then completes your actions.']"
"finding chromedriver, glibc version compatibility",https://www.reddit.com/r/selenium/comments/zr09jq/finding_chromedriver_glibc_version_compatibility/,selenium,"I'm trying to set up a webscraper in an amazon-linux terminal and I'm having issues with chromedriver glibc compatibility. 

&#x200B;

I'm currently using chrome and chromedriver version \~108. So I decided to try installing chrome and chromedriver 102. I still get the same error and the list of versions to try is too huge to trial and error this. 

&#x200B;

My glibc version is 2.26-62.amzn2   ...   and it seems more awkward to change that than to use older chromes. 

&#x200B;

&#x200B;

Below is the error message when chromedriver tries to open.

&#x200B;

        Traceback (most recent call last):
          File ""/home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/service.py"", line 97, in start
            path = SeleniumManager().driver_location(browser)
          File ""/home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/selenium_manager.py"", line 74, in driver_location
            result = self.run((binary, flag, browser))
          File ""/home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/selenium_manager.py"", line 93, in run
            raise SeleniumManagerException(f""Selenium manager failed for: {command}. {stderr}"")
        selenium.common.exceptions.SeleniumManagerException: Message: Selenium manager failed for: /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager --browser chrome. /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager: /lib64/libc.so.6: version `GLIBC_2.29' not found (required by /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager)
        /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager: /lib64/libc.so.6: version `GLIBC_2.28' not found (required by /home/ec2-user/.local/lib/python3.7/site-packages/selenium/webdriver/common/linux/selenium-manager)
    

&#x200B;

How can I find chromedriver glibc version compatibility requirements / How can I find which version of chromedriver I need?","['This submission has been removed because it looks suspicious to automod (c). If this was done in error, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fselenium&subject=about my removed submission&message=I‚Äôm writing to you about my submission that was removed (l). %0D%0DMy issue is...).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/selenium) if you have any questions or concerns.*', 'Do you specify path to chromedriver in `webdriver.Chrome()` of your Python script?']"
"I am trying to detect audio after playing an audio file in a page, want to check if audio is playing or not, if there anyway I can check if audio is playing ?, also in tab it shows ""tab is playing audio"" is there any way to extract that message from tab?",https://www.reddit.com/r/selenium/comments/zqthg7/i_am_trying_to_detect_audio_after_playing_an/,selenium,,"[""Untested!\n\n```\nfrom selenium import webdriver\n\ndriver = webdriver.Chrome()\ndriver.get('http://www.example.com/audio-page.html')\n\n# Find the audio element on the page\naudio_element = driver.find_element_by_css_selector('audio')\n\n# Check if the audio is playing\nif audio_element.get_attribute('paused') == 'false':\n    print('Audio is playing')\nelse:\n    print('Audio is not playing')\n\ndriver.quit()\n```""]"
Custom profiles of chrome not running in multithreading,https://www.reddit.com/r/selenium/comments/zqnu0i/custom_profiles_of_chrome_not_running_in/,selenium,"Hi Everyone,

I have an issue ongoing, I am trying to run custom chrome profiles with selenium,

The issue is that a single profile runs fine but when I use ThreadPoolExecutor, and open like three chrome profiles in parallel, one out of them works fine but the rest two do not do anything, they are just like halted. The code is concerned  is as follow:

`def browserthread(link):`  
 `i=links.index(link)`  
 `chrome_options = webdriver.ChromeOptions()`  
 `chrome_options.add_argument(""user-data-dir=C:\\Users\\LENOVO\\AppData\\Local\\Google\\Chrome\\User Data"")`  
 `chrome_options.add_argument(f""--profile-directory=Profile {str(i+1)}"")`  
 `driver = webdriver.Chrome(options=chrome_options)`  
 `drivers.append(driver)`

`with ThreadPoolExecutor(max_workers=threadnum) as pool:`  
 `response_list = list(pool.map(browserthread,links))`  
`drivers.clear()`

If multiple threads are run without profile specification, than all the chrome instances work fine, but when three profiles are opened in separate threads, only one instance works fine meanwhile other two remain halted.

Please help if you know a solution to this issue, thanks in advance.",[]
Why I can't find an element with time sleep but with webdriverwait the element appears,https://www.reddit.com/r/selenium/comments/zox3xu/why_i_cant_find_an_element_with_time_sleep_but/,selenium,"Why I can't find an element with time.sleep even with 100 seconds wait but with webdriverwait the element appears with even much less wait time, what's the mechanism behind it","[""This is an impossible question to answer given the info you've provided."", ""The way you've written it, it seems like sleep will only look once but webdriverwait looks in intervals. Am I misunderstanding?"", ""WebDriverWait typically polls the element at a specified interval so if you're waiting for 10s and polling every 500ms, you'll poll 20 times, maybe that's why. You really need to give more information and context though.\n\nThe mechanism is roughly the same to my knowledge, one sleeps the thread while the other continuously polls and can look for a specific condition. WebDriverWait is the one you should use and that's best practice. Time.sleep is generally bad practice and shouldn't really be used."", ""It's often the opposite. Are you on Java or Python? What exception do you get?""]"
XPATH returns WebElement object has no attribute aka not found,https://www.reddit.com/r/selenium/comments/zorev8/xpath_returns_webelement_object_has_no_attribute/,selenium,"I'm going nuts if I search for an xpath with $x() in the console inside the selenium browser it finds the element but when I do the same code with .find\_element in the script it keeps returning no element found (even if I do repeated searches with the Actions class).. what's going on here...

p.s. it's on Facebook website but it's a pop up that only shows on my account as it's a bug (See previous post of mine)","[""Two silly questions:\n\nIf it's in a popup are you switching scope from the main tab to the new window?\n\nAnd is it in an iframe?""]"
click on a pop up that appears on every page in Facebook,https://www.reddit.com/r/selenium/comments/zngys9/click_on_a_pop_up_that_appears_on_every_page_in/,selenium,"**Introducing cross-app messaging**

[**https://imgur.com/a/kTwj8xB**](https://imgur.com/a/kTwj8xB)

this pop up appears on every page now.. I need to get rid of it for running some scripts using selenium.. I tried get rid of it in the setting but there's nothing to remove it.. thank you","['having the same problem starting today', 'What exactly are you trying to do? What exactly have you tried? Please provide an example of what you are stuck on about handling a pop up and we will attempt to help you.', 'Add a listener when the button appears and click it.', 'Check if button exists and click it constantly, sideload an adblock extension and block the element altogether, evaluate some JS that removes it/click it whatever your choice how to handle it.', 'I found it easier using the mobile version, m.facebook.com', ""When clicking on 'Done' is it saved in a cookie or local storage? If so, just set it there so it does not appear"", ""You could maybe do a javascript injection to get rid of the window if that's what you need.""]"
Help Delaying Selenium Script,https://www.reddit.com/r/selenium/comments/zn591n/help_delaying_selenium_script/,selenium,"Hey guys, I wanted to know if anyone knew how to delay a selenium script so I can manually type something into a website and then, when I'm ready, have my selenium automation run. I use selenium in python(I've seen java versions and stuff). I already tried making an if statement with a user input as the condition but that didn't work very well.","[""What's the reason for typing something in manually instead of automating it?"", 'Maybe use thread sleep? Or what happens after you enter the text that you want? Does a success message appear or something like that? Maybe you can wait for that', 'https://selenium-python.readthedocs.io/waits.html']"
"How to handle Chrome's ""Reload site?"" window?",https://www.reddit.com/r/selenium/comments/zmp1ee/how_to_handle_chromes_reload_site_window/,selenium,"For context: I'm using Selenium 4.5.3 on Java and I've got a Mac. The browser is Chrome as the title suggests.

It happens on a JS app whenever I do some unsaved changes, then want to reload.

The exception I'm getting is:

    org.openqa.selenium.UnhandledAlertException: unexpected alert open: {Alert text : } 

However, when I use the in-built driver.window().alert() function, I'm told that there is no such alert!

It's not really counted as a window either - when I print out the output of driver.getWindowHandles(), I only see the one page.

I've tried just making a new Actions() object and clicking the Enter or Escape key to see what happens. Nothing.

I've also added --disable-notifications and --disable-popup-blocking, but nothing works.

Have you had this issue before and how did you solve it?

***Edit - Sort of solved:*** So, I still have no way of dealing with this, but what I've noticed is that you only get this sort of interaction if you use the actual browser. If you navigate using a link on the page, you get a pop-up which can be handled with the above options so that's how I'll do this going forward. Hope that helps!",[]
Custom Chrome Profile not opening in Selenium,https://www.reddit.com/r/selenium/comments/zmpvy4/custom_chrome_profile_not_opening_in_selenium/,selenium,"Hi everyone,

I am facing a problem for days with selenium in opening a custom-made profile, I am using the following line of code to open it but failing:

`chrome_options.add_argument(""user-data-dir=C:\\Users\\LENOVO\\AppData\\Local\\Google\\Chrome\\User Data\\Profile 1"")`

In order to open the default profile it just needs to remove the last part of the path, like this:

`chrome_options.add_argument(""user-data-dir=C:\\Users\\LENOVO\\AppData\\Local\\Google\\Chrome\\User Data"")`

It opens the default profile successfully but whenever I try to open a custom-made profile it opens the chrome with native selenium setting,

How can this issue be resolved?

Thanks in advance.","['I believe selenium by default still looks for the default profile there, so the trick is to delete profile 1 and rename the desired profile to profile 1.']"
Different ways to get xpath elements and CSS selectors,https://www.reddit.com/r/selenium/comments/zmdhwr/different_ways_to_get_xpath_elements_and_css/,selenium,"I would like to expand my knowledge on this because sometimes I'm struggling with myself due to the website I'm trying to automate, always changes every time I refresh it.

And in your opinion what's the best option to use, CSS selector or XPATH","[""It's contentious.\n\npersonally, I do everything with xpath. For a few reasons. Firstly, I've never found an element I couldn't get with xpath. Secondly, xpath allows me to navigate a DOM's hierarchy at will. And lastly, since xpath satisfies the first two, I don't feel the need to remember any extra syntax.\n\nI have heard it said it's slower. If so, it's measured in milliseconds and the several hundred thread.sleeps left over in the framework from the devs before me are of much greater concern than me using xpath."", ""I personally do everything with CSS because you can do the same things you can do with xpath, but it looks cleaner.\n\nAll you need is a cheatsheet like this: [https://htmlcheatsheet.com/css/](https://htmlcheatsheet.com/css/)\n\nThen go to a random website, open the developer console and try to find things in there. The find within the Developer Console matches CSS selectors, so it's a quick and easy way to learn how to use them. I imagine the same works for XPath, but again, my personal preference is CSS."", ""CSS selectors, the DOM can change just like you mentioned. An ID, Name, or Class is less likely to change. The goal of automation is to be quick, dependable, and have low maintenance. If you're using those properties in XPath then you're adding additional time, even if it appears negligible."", ""When using xpath you can select based in relative elements, maybe inside exactly three divs and a ul, or maybe the div has always a aria prop that only your data posses \n\nThat's where xpath shines"", 'What if the text on the page remains the same but the xpath changes?']"
"ElementNotInteractableException: Message: Element <span class=""selection""> could not be scrolled into view",https://www.reddit.com/r/selenium/comments/zmk7hr/elementnotinteractableexception_message_element/,selenium,"    select_span = driver.find_element(By.CLASS_NAME,'selection')
    select_span.text
    select_span.click()

i found the element i want and select\_span.text give me the text inside `<span class=""select2-selection__rendered"" id=""select2-indicatorDropdown-container"" role=""textbox"" aria-readonly=""true"" title=""Experienced violence since COVID-19"">Experienced violence since COVID-19</span>`

which is 'Experienced violence since COVID-19' but if i do select\_span.click() to click on this span and open the selection i get :

    ElementNotInteractableException: Message: Element <span class=""selection""> could not be scrolled into view

i keep getting this error almost daily and i never understand why..

there is no shadowroot here or iframe or anything since i got the element anyway, but when i try to click it i get this error why?? it happens to almost everything i try to click.

PS: i use python selenium and firefox browser","['In Java you need to scroll to that element then thread.sleep or wait.until so it can be clickable or whatever you neeed to do with element. So I guees you have something like that in Python', 'Instead of `select_span.click()` try \n\n    driver.execute_script(""arguments[0].click();"", select_span)\n\n\nA bit of javascript passed to the browser I think. Looks odd but it\'s been handy for me.', ""Hi as per [test automation services](https://www.qasource.com/automation-testing-services), the possible solutions for this one are:  \na) You need to check whether the locator you are using is correct or not.  \nb) If the locator is correct, scroll to the element where you want to perform the click operation. In javascript, you can scroll to a certain block by defining the block where you want to scroll {center, end or start block).   \nc) Once you have scrolled to the element put some wait there you can use implicit wait if that doesn't work try the pause method for some seconds and see if that works or not and perform the click operation.\n\nHope You find it useful!""]"
"Python-Selenium, what how can i detect this string?",https://www.reddit.com/r/selenium/comments/zlqxdb/pythonselenium_what_how_can_i_detect_this_string/,selenium,"Hi, I have an element that looks like this:

    <label class=""ui-selectchekboxmenu-label ui-corner-all"">Foobar</label>

How is Foobar detectable with something like this:

    expected_conditions.element_to_be_clickable((By.XPATH, ""Foobar"")))

Of course, not literally that, since the xpath is not only ""Foobar"" but I am trying to make the code work even if the element number changes or something like that due to a software update in the future.","[""//label[contains(text(), 'Foobar')]"", 'Just took a look at your previous comment - you need to surround the locator string with quotes, as it\'s a string argument.\n\n`\'//label[contains(text(), ""Foobar"")]\'`', 'Its just nice to sometime read about someone helping someone on py selenium stuff. Hiks', 'Everyone here suggested querying with text, I would also propose to query the element using ancestor and sibling tags that you think would always be there around the this Foobar element, in addition to using text of course, to make your selector more robust and resistant to changes.']"
How do I wait for css selector to click?,https://www.reddit.com/r/selenium/comments/zlorlo/how_do_i_wait_for_css_selector_to_click/,selenium,"Hi, I have a monitoring script in Python + Selenium. Right now I use a lot of xpath and id to click and it is annoying because the frameworks used sometimes changes the id and xpath, so I would like to look for something that will stay the same for a longer time.

I figured CSS selector content could be nice, they seem to be named after what they really do in the application I monitor.

I tried to google it but I found no examples I understood how I could convert.

Right now my function looks like this, how can I modify that to use css selector?:

    def wait_for_xpath_click(params, element_xpath):
    	temp_element = WebDriverWait(browser, 60).until(
    		expected_conditions.presence_of_element_located((By.XPATH, element_xpath))
    		)
    	time.sleep(5)
    	temp_element.click()

Edit: I got it now:
It needs to look like this:

    expected_conditions.element_to_be_clickable((By.CSS_SELECTOR, selector)))","['Nice for figuring it out can you give me an actual example of a real code please', ""You need to change the third line from By.Xpath to By.css, then enter the css right after. That's really all there is to it! Xpath, class, css, id can be used interchangeably, you just need to change the By.XXX function to match it, e.g. By.class, By.tag, By.id, By.css, etc.""]"
Simple class wont work,https://www.reddit.com/r/selenium/comments/zk5tw2/simple_class_wont_work/,selenium,"Can someone tell me, what have i done wrong here? 

Im very new to python and selenium FYI.

&#x200B;

    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    
    class Open (webdriver.Chrome):
        def __init__(self, path=Service(""C:\Selenium\chromedriver.exe"")):
            self.path = webdriver.Chrome(service=path)
            super(Open,self).__init__()
    
        def landFP(self):
            self.get(""https://www.facebook.com/"")
    
    page = Open()
    page.landFP
    

The outcome is not facebook page. im loss. help","[""What's the outcome? Please describe the error"", 'You aren‚Äôt calling your landFP method, add () to the end of page.landFP']"
Help me with selenium,https://www.reddit.com/r/selenium/comments/zjljgg/help_me_with_selenium/,selenium,Hello! I have a technical research paper about selenium that i need to go through and i don't have knowledge of selenium. Could someone help me with it? It's nothing in depth. It's just about an experiment to replace sleep threads in selenium with something else. Thank you,"['Funnily enough I just started a new job where their test suite has about 800 thread.sleep\'s i need to remove. DM me and I\'ll see if I can help.\n\nSide note: The ""something else"" is explicit waits. (wait.until(expectedcondition...))', ""Adding to what was already discussed above: It is better to write a wrapper for every inbuilt selenium methods such as click, enterText etc which internally calls wait.untill(elements is displayed) so that you don't have a dependency on sleep. Further optimisation would be adding polling every nth second (user defined) in is element displayed method.""]"
Scrapping of a trading website.,https://www.reddit.com/r/selenium/comments/zh2ad9/scrapping_of_a_trading_website/,selenium,So I was on [gocharting.com](https://gocharting.com).. trying to scrap some data... but I cannot find the elements of the chart in html file. There are divs but I cannot find the numbers & I'm not sure how to handle that. If anybody has any idea please let me know.,"['Picture what u want to scrape exactly would help', 'Try Selenium IDE to map the elements.']"
Selenium Modules,https://www.reddit.com/r/selenium/comments/zfjdxw/selenium_modules/,selenium,"Hi fellow automation geeks.

If im making any sense, can you guys point me to a website or any reference that i can check for:

All the Selenium's modules, for example we all know webdriver module.

Like for example when we want to user the webdriver module we write

 

    from selenium import webdriver
    
    driver = webdriver.Chrome(""your path"")

and when you want to find elements you will use 

    from selenium.webdriver.common.by import By
    
    driver.find_element(By.XPATH, '//button[text()=""Some text""]')

i want to know where can i read about all the modules. like webdriver modules, and BY modules(if its a module) and common modules(if its a module).

Thnks again",['I will point you to the official docs selenium.dev']
Getting a NoSuchElementException when trying to dismiss popup,https://www.reddit.com/r/selenium/comments/zeziub/getting_a_nosuchelementexception_when_trying_to/,selenium,"I am testing a demo website for practice and I am receiving an error when trying to dismiss a cookie permissions popup (not really a popup but an iframe). The popup only appears once in an open browser session and will only appear if you close the browser and reopen website. I am testing logins using test data from an xls, so when the  webpage is opened, it dismisses the cookie popup logs in and then logs out and then attempts the next login in the xls. It tries to look for the cookie popup which will not appear as we have not closed the browser. I have written an 'if' statement that checks for the popup, to dismiss popup if it appears or continue as normal if it doesn't. But it does not continue and then fails the test.  I would like some help on what is causing this.

Here is the error message:

**org.openqa.selenium.NoSuchElementException: no such element: Unable to locate element: {""method"":""css selector"",""selector"":""#gdpr\\-consent\\-notice""}**

Here is a link to the code for the test. The If statement is on line 23.

[https://gist.github.com/fdama/5a73c1f95319f09266120dd658b425cc](https://gist.github.com/fdama/5a73c1f95319f09266120dd658b425cc)

Thanks in advance.","[""Well, the error is on the function iframeIsVisible, but the code you linked there's no description of this function.\n\nWe need to see the function to see if we can notice anything wrong.""]"
Java selenium Textarea/iFrame help,https://www.reddit.com/r/selenium/comments/ze4n7p/java_selenium_textareaiframe_help/,selenium,"I have problem of locating and entering any text into ""Content"" field on some blog using java selenium webdriver. It seems like textarea but when inspected, textarea is hidden and iFrame document is what I need to somehow locate and sendKeys there. So basicaly I need somehow to click on <p> under <body> of that document under iFrame which I dont know how. Everything I tried bring me Exceptions NoSuchElement or NotClickable. I would appreciate any suggestion based on exeperience, thanks.","['To interact on an element inside on an iframe, you need to switch to the iframe prior to the interaction.\n\nCode:\n\ndriver.switchTo().frame(0)\n\n--Switch to the first iframe of the page\n\ndriver.switchTo().frame(‚Äúid of the element‚Äù)\n--Switch to the frame that has that id.\n\nhttps://www.guru99.com/handling-iframes-selenium.html', ""Welcome to the nightmare that is frames. Wait until you work with a website that utilizes 10+ nested frames, it's soul crushing.""]"
How do I bulk add a bunch of commands to Selenium?,https://www.reddit.com/r/selenium/comments/ze7l5j/how_do_i_bulk_add_a_bunch_of_commands_to_selenium/,selenium,"I was using Excel & iMacros to do this before.  Basically, I have a list of employee ID #s that need to be clicked, by their Link ID (not by their name).  So for example, in iMacros I'd send the command:
TAG POS=1 TYPE=A ATTR=ID:Link_123456

In that example, the employee's ID # was 123456.

I'd have a whole list of IDs, so I'd just use excel to merge the ""123456"" with ""TAG POS=1 TYPE=A ATTR=ID:Link_"" to make ""TAG POS=1 TYPE=A ATTR=ID:Link_123456""

iMacros doesn't work anymore and has been discontinued.

In Selenium IDE, it would be:
Command ""click""
Targer ""id=Link_123456

That's what I'd need it to do.

Now, how do I bulk add a bunch of IDs for each one to be clicked?  Can I do it in excel and then import those commands to Selenium?  I know how to manually add them one by one in Selenium, but I've got hundreds that need clicked.  Plus it's different everyday, so I'll have a new list of IDs that need to be clicked each day.

How do I bulk add new commands each time?","['I would look at data driving your tests, either using Excel or JSON and create map of KVPs and iterate that way.']"
Beginner's guide to web automation,https://www.reddit.com/r/selenium/comments/zd7zgd/beginners_guide_to_web_automation/,selenium,"Hi everyone, this is my first time in reddit, as well in this subreddit.

Basically I'm confused, I don't know where to start with web automation. I've been searching the web but I still have a hard time understanding where does ""Selenium"" fit in the whole picture. I don't have a whole picture btw. I have experience on scripting in Linux, and with networking. But the web is still an unexplored territory and I need to be able to write basic scripts that can access links, fill login details and retrieve data.  

Any definition, any book, any resources are useful for me right now, either to understand what selenium is, or to get an idea about the whole concept of web automation.","['Google is your best friend. Lots of stuff on YouTube or Udemy', ""Not entirely sure what you're looking for here, but I'll give it a shot:\n\nIf you're familiar with linux scripting, then selenium isn't much different.  Basically with selenium you can do anything on a website that a user can do.  So it's used for automation and also for user testing (many companies use it to run regression testing on their apps/websites when doing updates or feature changes).\n\nThe selenium documentation is good, but your best bet is to try making a test script to do something simple (log in to website, click some stuff, submit a form, logout).  Then iterate your code while you learn until you get it to do what you want.  Then come back here with your code and let people tell you how it could be done differently / simpler / more-compact.  Iterate on your code again and then you'll have a decent grasp.""]"
'sendKeys' but in Javascript,https://www.reddit.com/r/selenium/comments/zckj6g/sendkeys_but_in_javascript/,selenium, Is there a Javascript version of Selenium's 'sendKeys' method? Setting value won't work as the website doesn't think you've inputted data in the field in order for the sign in button to be enabled.,"['Maybe the input field is in iframe', 'Set the value to the correct element and then you may need to trigger an event\n\nLook at dispatchEvent that may be the issue']"
"some problems with find_element(By.NAME,""value"")",https://www.reddit.com/r/selenium/comments/zcduid/some_problems_with_find_elementbynamevalue/,selenium,"I'm working on a script using selenium that can login automatically login my college's imformation system.

Here's the code:
```python
from selenium import webdriver
from selenium.webdriver.common.by import By

from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager

# from selenium.webdriver.edge.service import Service as EdgeService
# from webdriver_manager.microsoft import EdgeChromiumDriverManager

driver = webdriver.Chrome(service = ChromeService(executable_path = ChromeDriverManager().install()))

# driver = webdriver.Edge(EdgeChromiumDriverManager().install())

driver.get(""http://stucis.ttu.edu.tw/stucis.htm"")

ID = ""studentid""
PASS = ""password""

ID_input = driver.find_element(By.NAME,""ID"")
PWD_input = driver.find_element(By.NAME,""PWD"")

ID_input.send_keys(ID)
PWD_input.send_keys(PWD)

driver.close()
```
and it comes with erros
```
Traceback (most recent call last):
  File ""C:\Users\USER\Desktop\Crawler\login_stucis.py"", line 19, in <module>
    ID_input = driver.find_element(By.NAME,""ID"")
  File ""C:\Users\USER\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\selenium\webdriver\remote\webdriver.py"", line 861, in find_element
    return self.execute(Command.FIND_ELEMENT, {""using"": by, ""value"": value})[""value""]
  File ""C:\Users\USER\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\selenium\webdriver\remote\webdriver.py"", line 444, in execute
    self.error_handler.check_response(response)
  File ""C:\Users\USER\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\selenium\webdriver\remote\errorhandler.py"", line 249, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""css selector"",""selector"":""[name=""ID""]""}
  (Session info: chrome=108.0.5359.95)
Stacktrace:
Backtrace:
        (No symbol) [0x00CBF243]
        (No symbol) [0x00C47FD1]
        (No symbol) [0x00B3D04D]
        (No symbol) [0x00B6C0B0]
        (No symbol) [0x00B6C22B]
        (No symbol) [0x00B9E612]
        (No symbol) [0x00B885D4]
        (No symbol) [0x00B9C9EB]
        (No symbol) [0x00B88386]
        (No symbol) [0x00B6163C]
        (No symbol) [0x00B6269D]
        GetHandleVerifier [0x00F59A22+2655074]
        GetHandleVerifier [0x00F4CA24+2601828]
        GetHandleVerifier [0x00D68C0A+619850]
        GetHandleVerifier [0x00D67830+614768]
        (No symbol) [0x00C505FC]
        (No symbol) [0x00C55968]
        (No symbol) [0x00C55A55]
        (No symbol) [0x00C6051B]
        BaseThreadInitThunk [0x761A6939+25]
        RtlGetFullPathName_UEx [0x77B08FD2+1218]
        RtlGetFullPathName_UEx [0x77B08F9D+1165]
```
Some fourms says it is becaust the driver's version is different.
I also tried:
```python
ID_input = driver.find_element(""name"",""ID"")
PWD_input = driver.find_element(""name"",""PWD"")
```
but can't works.","[""You're sending in a variable as a literal String. Get rid of the quotes if you are going to assign the ID and PWD variables."", 'Is the Name of the First Element really ""ID"" ? Does the page contain an iFrame? Can you share some HTML as well?\n\nEdit: the stuff you are looking for is in an iframe. You need to switch to it first.']"
Where do u learn,https://www.reddit.com/r/selenium/comments/zc5ptu/where_do_u_learn/,selenium,"Hi guys,

Where do u guys learn selenium. Any tutorials/blog/website/videos u can suggest. 

Im planning to learn it for free. Not planning to pay for it. 

I have a background of python and web dev. 

Thanks","['YouTube selenium tutorials\nBadabing badaboom', ""starting learning selenium with virtually no background in python. Tech with tim has some decent tutorials, i typically just have documentation pulled up. Almost everything i've needed is in doc's."", 'I would recommend the following YouTube channel. Look through the playlists for the Selenium tutorials. He has a few. \n\nhttps://www.youtube.com/@sdetpavan/playlists', ""Hey man, if you are 100% new, don't go Selenium. Learn Playwright. It's basically the same thing, but better."", ""I just started with it and so far I've been reading the docs (there's missing doc there) and YouTube tutorials with blogs"", 'Lots of resources online. If you just google the topic you want to learn. Usually redirects you to some YouTube link. Then it‚Äôs just a matter of looking at the reviews of the YouTuber and finding if you like their teaching skills. Good Luck.']"
how can i run selenium on replit? I always get driver path errors,https://www.reddit.com/r/selenium/comments/zc9h0n/how_can_i_run_selenium_on_replit_i_always_get/,selenium,"I did try solutions from this post comments
https://replit.com/talk/ask/Can-I-use-selenium/11566
But still doesn't work","[""To get help with something custom like this you will need to post more information than a link to 4 year old question. What exactly did you try from that page? What exactly is the error you're facing? Do you have any code that demonstrates your issue? When you follow the steps in this question above what is the outcome?""]"
Selenium ChromeDriver eating up HD space?,https://www.reddit.com/r/selenium/comments/zar4e3/selenium_chromedriver_eating_up_hd_space/,selenium,"Hi there, I'm to see if anyone is having this issue or if it is just me.

Prior to running my Selenium script, I have about 13GB of Hard Drive space.

After running the script for about 5-6 hours, I'm down to 3GB of space.

Here's my python code:

\# Keep the Browser open, even after execution  
chrome\_options = Options()  
chrome\_options.add\_experimental\_option(""detach"", True)  
\# this parameter tells Chrome that  
\# it should be run without UI (Headless)  
chrome\_options.headless = True  
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options = chrome\_options)  
....

driver.quit()

Is anyone else experience the same issue, if so, is there a way around it beside having restart my server or laptop after so many runs?

Thanks in Advance.","[""You could check what's consuming that much space. It's logs, temporary internet files, etc?"", 'Check the temp files generated by ChromeDriver...', ""I'm gonna guess that the issue is in the \\``....`\\` part of your program. Presumably that's the part that's taking 5-6 hours lol? The code you shared is just basic scaffolding.""]"
Element not interactable,https://www.reddit.com/r/selenium/comments/z9uyn2/element_not_interactable/,selenium,"Hi Reddit, I m working on a script that uses selenium to click on all of the jobs on indeed. I have found that without fail it always returns an ""element not interactable"" error on the 11th LI element. I have tried to implement an implicit wait to wait until the element was clickable and it just resulted in a timeout error. This is the code that I have so far

    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.wait import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver import ActionChains
    import time
    import pandas as pd
    
    intialLink = 'https://www.indeed.com/jobs?q=software+engineer&l=Connecticut&vjk=d2a438c96f6e9c7e&from=gnav-util-jobsearch--indeedmobile'
    driver = webdriver.Chrome(executable_path='C:<path ommited for privacy reasons>\\chromedriver.exe')
    driver.get(intialLink)
    jobPannels = driver.find_elements(By.CSS_SELECTOR,"".jobsearch-ResultsList > li"")
    
    #it starts at the 9th li element
    for i in range(9, len(jobPannels)):
        print(jobPannels[i].tag_name)
        ActionChains(driver).move_to_element(jobPannels[i]).perform()
        #wait = WebDriverWait(driver, 15)
        #wait.until(EC.element_to_be_clickable(jobPannels[i]))
        time.sleep(1)
        jobPannels[i].click()

I've tried to look this up and all I can find are people saying to use the wait for it to work and like I said before I didn't get that to work. I suspect that this is something to do with the underlying HTML of the site.

Solution: I found out that it was the 12 li that was giving me trouble, not the 11th. The reason for this was that the 12 li only contained an empty div.","['Gotta slap some grease on the wheels. Aka, put the faulty line of code within a try/except block. Then never think about it again.', 'maybe format the code snippet so its not on 1 line its unreadable atm']"
Scroll to element using python selenium,https://www.reddit.com/r/selenium/comments/z9nlsl/scroll_to_element_using_python_selenium/,selenium,"    element.location_once_scrolled_into_view

and ive used this too:

    driver.execute_script(""return arguments[0].scrollIntoView(true);"", element)

and everytime i run any of these lines i get anxiety because its always by chance if they work or not.

now im trying to run any of these im not getting any errors but the page doesnt scroll to the element, and the first one gives me this without scrolling anywhere:

    {'x': 0, 'y': 0}

eventho the element is not on coordinates x = 0 and y = 0

what can i do?

i just get this error if i try to scroll to the element or click()

    ElementNotInteractableException: Message: Element <button class=""btn btn-outline-light btn-icons"" type=""button""> could not be scrolled into view

&#x200B;","['Is it possible to link to the page in question? \n\nAnd if not, could you give some more context about it? IE why is it important to first scroll the elements into view before clicking on them? Does the page lazyload?', ""Maybe it's in iframe?""]"
Can't find an element which is visible on the windk,https://www.reddit.com/r/selenium/comments/z8vlmg/cant_find_an_element_which_is_visible_on_the_windk/,selenium,"I want to scrape the website: https://www.theguardian.com/world/coronavirus-outbreak
for newslinks.
Once the page is opened it asks to if or not accept cookies. There is a button to accept it which is visible in the screen which I want to click.
I tried to find it using xpath, class etc but no element is found.
I tried using wait to find the elements still it doesn't work. 
Can anyone help me solve this issue?",['the button seems to be in iframe.Try switching into frame and then click on the button.I was able to find the element.']
Issue with Selenium tests on AWS,https://www.reddit.com/r/selenium/comments/z7r7lx/issue_with_selenium_tests_on_aws/,selenium,"Hi all...

I've written a simple browser test with Python/Selenium that runs fine locally from my Mac, as well as from AWS Linux and Ubuntu Docker containers on my Mac. However, if I run the containers on an EC2 instance (with Docker installed, obvs), the test fails, always on the same step (which is loading a login page). I've tried an Ubuntu AWS EC2 instance and just installed all the component manually to run it natively from there, but that also fails in the same place.

So it seems that the issue is something to do with AWS, but I cannot for the life of me figure out what it may be so wondered if any of you glorious people might?","['Does it fail to load the page or fail the login steps? If it fails to log in, you may have to whitelist the EC2 IP.  They are usually within a certain range, but are dynamic.', 'Is your program headless?', 'Try taking a screenshot and grabbing a screenshot. First guesses are either firewall or DNS. Does your application load? Is it on the same host? I also seem to recall you can use VNC with the debug containers to watch playback and manually launch a browser.']"
Get 5GB of clean Residential Proxies for free,https://www.reddit.com/r/selenium/comments/z7tgeg/get_5gb_of_clean_residential_proxies_for_free/,selenium,"Hey all!

I'm looking for a few developers who are doing web scraping to make interviews about the proxy setup experience. The interview usually does not take more than 30-40 minutes. As a reward, I can offer 5GB of Residential Proxies.

Thanks in advance and don‚Äôt hesitate to DM me or at [product@soax.com](mailto:product@soax.com) to schedule a call.","['Finally I will be able to pay my rent with those 5GB of Residential Proxies', '[removed]']"
ticket-booking algorithm,https://www.reddit.com/r/selenium/comments/z72cfy/ticketbooking_algorithm/,selenium,"I implement automatic ticket-booking using selenium and python, including auto-login, auto-redirection, and auto-booking features. If you are interested in learning selenium and want to start with a project. Or if you have any comments or issues related with the task, feel free to check it online. the link towards to github repo is: [https://github.com/JJerryJi/ticket-booking](https://github.com/JJerryJi/ticket-booking)",[]
How to disable geckodriver.log file i dont want it to be created anywhere,https://www.reddit.com/r/selenium/comments/z6ygq8/how_to_disable_geckodriverlog_file_i_dont_want_it/,selenium,"straight forward question i dont want any logs from geckodriver,

i am using python selenium with firefox and everytime i run the driver a geckodriver.log file gets created. how can i disable that i dont want this file to be created.",['Apparently you just pipe it to dev null (or whatever the windows version of that is) as described here: https://stackoverflow.com/questions/41696695/how-to-turn-off-the-marionette-gecko-driver-logs-in-selenium-3']
#shadow-root (open) selenium python trying to access button inside shadow-root (open),https://www.reddit.com/r/selenium/comments/z4bwjc/shadowroot_open_selenium_python_trying_to_access/,selenium,"im trying to access a download button inside 4 shadow-root (open) what do i have to do?

the inspect element looks like this:

    <div class=""dataset-download-card"" data-test=""dataset-download-card"">
    	<hub-download-card dataset-id=""04c64cb5553843b8a644af6429b6633c_0"" spatial-ref-id=""4326"" data-element=""download-card"" hydrated="""">
    	 #shadow-root (open)
    	  <calcite-card dir=""ltr"" hydrated="""">
    	   #shadow-root (open)
    			<h3 slot=""title"">CSV</h3>
    			<dl slot=""subtitle"">
    				<dt class=""ltr"">File created</dt>
    				<dd>May 24, 2022, 01:58</dd>
    				<dt class=""ltr"">File size</dt>
    				<dd>28.9 KB</dd>
    			</dl>
    			<div slot=""footer-leading"">
    				<hub-download-notice file-status=""ready"" hydrated=""""/>
    				<calcite-button icon-position=""start"" alignment=""center"" appearance=""outline"" color=""blue"" scale=""m"" width=""full"" hydrated="""">
    				 #shadow-root (open)
    				  <button aria-label="""" class=""content--slotted icon-start-empty icon-end-empty"" type=""button"">
    						<span class=""content"">
    							<slot>
    								<#text>
    							</slot>
    						</span>
    					</button>
    					Download
    				</calcite-button>
    			</div>
    		</calcite-card>
    	</hub-download-card>
    </div>

so this is a chunk of the html code from inspect element. i am trying to access the Download button and click it.

how do i do that?

i tried using 

    driver.execute_script(""return document.querySelector('hub-download-card').shadowRoot.querySelector('calcite-card').shadowRoot.querySelector('calcite-button').shadowRoot.querySelector('button.content--slotted icon-start-empty icon-end-empty')"").click()

im getting error:

    JavascriptException: Message: TypeError: document.querySelector(...).shadowRoot.querySelector(...).shadowRoot.querySelector(...) is null",['[removed]']
how to change browser.download.dir as much as i want even after initializing the driver,https://www.reddit.com/r/selenium/comments/z49fi1/how_to_change_browserdownloaddir_as_much_as_i/,selenium,"    options = Options()
options.set_preference(""browser.download.dir"", 'downloads')

driver = webdriver.Firefox(service=service,
                                options=options)

so after i initialize the driver this driver downloads will go into downloads variable which has the path i specified for my downloads.

what if i want to change this download var later on in my script without reinitializing the driver how can i do that?

i want to change the download location many times in my script. i am using python selenium with firefox","[""Don't have an answer to your question but, cant you just grab the file and move it to the desired location? Don't understand the use case."", 'This stack overflow answer has some great insight\n\nhttps://stackoverflow.com/a/25744385/4999246', 'Use remote driver to take the session again and change those options  \n\n\nhttps://www.selenium.dev/documentation/webdriver/drivers/remote\\_webdriver/']"
Error Selenium Python,https://www.reddit.com/r/selenium/comments/z42y36/error_selenium_python/,selenium,"Hola a Todos 

Estoy automatizando con selenium y python con un webdriver de chrome, y ejecutando mis pruebas derrepente me aparecio este problema

23420:23136:1125/003102.494:ERROR:device\_event\_log\_impl.cc(215)\] \[00:31:02.494\] USB: usb\_device\_handle\_win.cc:1048 Failed to read descriptor from node connection: Uno de los dispositivos conectados al sistema no funciona. (0x1F)

estoy intentando de todo pero no me funciona, si alguien sabe pliiiz!","['–ù—è–º–∞–º –ø—Ä–µ–¥—Å—Ç–∞–≤–∞ –∫–∞–∫–≤–æ –ø–∏—à–µ—à, –º–æ–∂–µ –±–∏ —Ç—Ä—è–±–≤–∞ –¥–∞ –æ–ø–∏—Ç–∞—à –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏, –∞–∫–æ –∏—Å–∫–∞—à –Ω—è–∫–æ–π –¥–∞ –≥–æ –ø—Ä–æ—á–µ—Ç–µ.']"
xpath breaking,https://www.reddit.com/r/selenium/comments/z3a4am/xpath_breaking/,selenium,"So, I have a python script that at some point needs to get information from a website.
Everything is fine when I try to get ellement a, but element b breaks. This element lies deeper in the html code. Nothing would work.
I did figure out that after passing the 4th div or so that was when the xpath broke. When playing around with the website it seems that is roughly where the html changes when you press certain buttons.
 I figure the website makes use of something akin to tabs, but nothing seems to reflect this in the html. (And the ""default"" tab is the one I need anyways)
I can't really share the html and in python I've tried practicaly any way to access it that might exist (with the exception of going through sibling elements, as any element that is somewhat close to it is also unreachable)
Does anyone have an idea how I could fix this?","[""Without seeing the DOM it's hard to give any specific assistance here, but I think the general rule of avoiding XPath and only using it when nothing else works holds true here. If you can, try to find the element through some other means like class name, id (though I reckon you would've done that already if that was an option), text, and so on. If you can get the parent element of the element you want more easily with something like id, class name, text, etc. try to get that and then use some other means to search for the element you want within that element.\n\nFor example I had once a button that had no unique identifier to go on. The text of the button was the same as many other buttons in the DOM, it had no id, and the class names also were identical to many other buttons. I tried using XPath at first, but since the button appeared on a form that was generated when a certain event happened, the XPath wasn't always the same. However I noticed that the form actually had some unique identifiers, so when I first searched for that element with the unique locator I was then able to just look for the button element from within that form element. Even though the locators were identical to other elements on the page, they was unique within the context of the parent element.\n\nHope any of this helps!"", 'I can understand that you cannot share html code here. But at least you could mention if there are any tags or attributes available for that element.\n\nMaybe someone with similar issues will help.', 'Trying searching around for ""shadow dom""', 'Does it work with Selenium IDE recording and running? If it does, using the element it generates may solve your problem.']"
What else stops finding elements besides iframes,https://www.reddit.com/r/selenium/comments/z2psxn/what_else_stops_finding_elements_besides_iframes/,selenium,"I have a web page I'm trying to automate and it works perfectly until I get to a certain point, but then python stops finding anything on the last page.

I was using find element by link and by partial link but I also tried some different things with xpath, id, and css selector but still no dice.

After some googling, I also tried switching to the 2 iframes in the page (I did so by index) and back to the main content, but still not a die to be found. 

I noted that the links in question come in the same wrapper as a Javascript noop. Could that have something to do with it? What should I google/try next?

I'm not sure what to paste in here to ask for help. I've tried so many things that didn't work. Thanks for your time, those who read this far; whether you can help me or not, I appreciate you.","['Is the site public? Can you link it?\n\nYou can try Selenium IDE, does it interact with it?', ""Where you running headless and didn't realise there was another window?"", ""Any chance the people who keep downvoting this can explain why? I keep hitting 5 votes and then it's back down to 1 or 2. If there's a faux pas I've committed, please let me know.""]"
Rasberry Pi recs (or hosting alternatives) to run webdriver python script?,https://www.reddit.com/r/selenium/comments/z2nwqh/rasberry_pi_recs_or_hosting_alternatives_to_run/,selenium,Hi just wondering if anyone would be so kind to recommend a cost efficient setup for deploying a  crawler package that I plan to run continuously.  I was recommended raspberry pi for additional devices but there are a lot of options and not sure what is required to smoothly run webdriver (ChromeDriverManager)?  Just one instance of the script.  Also open to hosting but I don't know that is feasible... thank you!,['If it supports the software a pi is good for running something 24/7.  There‚Äôs also free tiers at the various cloud providers.']
Trying to understand .perform() keyword,https://www.reddit.com/r/selenium/comments/z2g9p7/trying_to_understand_perform_keyword/,selenium,anyone can eli5 the purpose behind this perform keyword thanks,[]
Select only buttons with aria-pressed,https://www.reddit.com/r/selenium/comments/z1rgdn/select_only_buttons_with_ariapressed/,selenium,"Hello, 

&#x200B;

Im working on improving my like bot. I can get all the like buttons, but the bot is very indiscriminate about what it likes as I'm using 

&#x200B;

`driver.find_elements('xpath', ""//button[contains(.,'Like')]"")`

I need to find a way to only click on items that are `aria-pressed=""false'` 

&#x200B;

I have tried adding the condition after the statement as follows:

`river.find_elements('xpath', ""//button[contains(.,'Like')]"" and aria-pressed=""false')`  

&#x200B;

&#x200B;

Any suggestions are highly appreciated.",['Your syntax is wrong see the example here \n\nhttps://stackoverflow.com/questions/18547410/xpath-with-multiple-contains-on-different-elements']
"Cannot find Chrome Binary , MS Visual Studio 2022",https://www.reddit.com/r/selenium/comments/z1p8ww/cannot_find_chrome_binary_ms_visual_studio_2022/,selenium,"Hello, I have been googling this error for 2 days, and i've tried to give the explicit path to the chromedriver.exe outside or the project and inside the project, it still pops up with the same error, tried it on different computers, reinstalled VS, tried also the headless start, and yes reinstalled few times.
Selenium.Support
Selenium.WebDriver
Selenium.WebDriver.ChromeDriver

nothing seems to be working. It's a very stupid question but i ran out of ideas or maybe my googling abilities suck.","[""For better help it would be a good idea to post your full error code.\nAt this point i can only guess from my humble point of view. A common issue exists when you copy the PATH of chromedriver. Change the separators from ' \\ ' to ' / '. Maybe it'll work."", 'Try this:\n\nhttps://www.reddit.com/r/selenium/comments/qw1jms/how_to_automatically_update_your_chromedriver_in_c/', 'Is it a version mismatch between the Chrome driver and Chrome Browser?  \nDoes your Chrome Driver version first 2 digits match   \nwith your Chrome Browser version first 2 digits?', 'Sorry for late reply I was hoping to receive notification from reddit, anyway, here is the code  \n\n\nBasically i am running the code from the official selenium web site, nothing unusual that would leave room for error, in the end i\'ve copy pasted everything, funny thing today i have tried to run it again, i receive a different error. Maybe some browser automatically updated and solved my error but created a new one :)\n\n&#x200B;\n\n    OpenQuestionnaire\r\n\r\n  \u2009Source:\u2009UnitTest1.cs\u2009line\u200917\r\n\r\n  \u2009Duration:\u20091 min\r\n\r\n \r\n\r\n  Message:\u2009\r\n\r\nTest method TestingQuestionnaire.UnitTest1.OpenQuestionnaire threw exception:\r\n\r\nOpenQA.Selenium.WebDriverException: The HTTP request to the remote WebDriver server for URL http://localhost:xxxxx/session timed out after 60 seconds. ---> System.Threading.Tasks.TaskCanceledException: A task was canceled.\r\n\r\n \r\n\r\n  Stack Trace:\u2009\r\n\r\nTaskAwaiter.ThrowForNonSuccess(Task task)\r\n\r\nTaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\r\n\r\nTaskAwaiter`1.GetResult()\r\n\r\n<MakeHttpRequest>d__35.MoveNext()\r\n\r\n--- End of stack trace from previous location where exception was thrown ---\r\n\r\nExceptionDispatchInfo.Throw()\r\n\r\nTaskAwaiter.ThrowForNonSuccess(Task task)\r\n\r\nTaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\r\n\r\nTaskAwaiter`1.GetResult()\r\n\r\nHttpCommandExecutor.Execute(Command commandToExecute)\r\n\r\n--- End of inner exception stack trace ---\r\n\r\nHttpCommandExecutor.Execute(Command commandToExecute)\r\n\r\nDriverServiceCommandExecutor.Execute(Command commandToExecute)\r\n\r\nWebDriver.Execute(String driverCommandToExecute, Dictionary`2 parameters)\r\n\r\nWebDriver.StartSession(ICapabilities desiredCapabilities)\r\n\r\nWebDriver.ctor(ICommandExecutor executor, ICapabilities capabilities)\r\n\r\nChromiumDriver.ctor(ChromiumDriverService service, ChromiumOptions options, TimeSpan commandTimeout)\r\n\r\nEdgeDriver.ctor(EdgeDriverService service, EdgeOptions options, TimeSpan commandTimeout)\r\n\r\nEdgeDriver.ctor(EdgeDriverService service, EdgeOptions options)\r\n\r\nEdgeDriver.ctor(EdgeOptions options)\r\n\r\nEdgeDriver.ctor()\r\n\r\nUnitTest1.OpenQuestionnaire()\u2009line\u200919\n\n&#x200B;\n\n    \n    using System;\r\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\r\nusing OpenQA.Selenium;\r\nusing OpenQA.Selenium.Chrome;\r\n\r\nnamespace SeleniumDocs.GettingStarted\r\n{\r\n    [TestClass]\r\n    public class FirstScriptTest\r\n    {\r\n\r\n        [TestMethod]\r\n        public void ChromeSession()\r\n        {\r\n           var driver = new EdgeDriver();\n               //var options - new EdgeOptions();\r\n\r\n            \n            driver.Navigate().GoToUrl(""https://www.google.com"");\r\n           \r\n            \r\n            //driver.Quit();\r\n        }\r\n    }\r\n}']"
How do you deal with 'difficult' elements?,https://www.reddit.com/r/selenium/comments/yy4y8l/how_do_you_deal_with_difficult_elements/,selenium,"Bit of a selenium noob so apologies upfront. 

I often come across elements that are very clear and seem easy to interact with, just by looking at the page. However, when I try to click on said element it comes up saying it's not clickable. Do you have a checkbox list of things you work through to click on difficult elements like these? In my mind I'm picturing a flowchart sort of like, if element not clickable, is it a popup- do X, is it an input box - do Y. If that makes sense","['Usually I just ask myself three questions:\n\n* Am I actually trying to find and click the correct element or do I need to click the parent or child of that element instead?\n* If it\'s an XPath or something else, is it actually unique or are there multiple elements found?\n* Is there a slight delay of a few ms between the element becoming clickable (e.g. a page load or something) and me trying to click it and do I need to wait for the element or the page to finish loading first?\n\nAll of these are simple enough to verify in the browser\'s console with little effort (except maybe the delay thing, that\'s usually just checked by me throwing a Thread.Sleep or something somewhere in the code and checking if that helps) and usually it\'s one of these three. They\'re like my ""have you tried turning it off and on again"" -routine.\n\nSometimes it gets a bit trickier, but in the end as long as something is on a webpage and it can be clicked with a mouse, it\'s usually clickable so it\'s just a matter of finding what particular portion of that element you need to actually click to get things rolling.', ""When I don't know what to do, I use Selenium IDE and export the code."", ""Use xpath plugins like chropath. If it doesn't give you what you need it'll get close to it"", 'JavaScript click', 'By using javascript. If you use C# you can write an extension for WebElements to add a ClickJs() method to the class. It can come in handy.', 'I look to download documents but sometimes the element is behind iframes and widgets. I can find my way around them for the most part but how how do you account for those techniques']"
Whats the best site for hosting a selenium bot?,https://www.reddit.com/r/selenium/comments/yxr43l/whats_the_best_site_for_hosting_a_selenium_bot/,selenium,I have a bot i want to host online so i would like to know which is best for selenium.,"[""I don't think that's possible. You'll have to run it on your computer or stand up your own Selenium Grid server."", 'Railway.app']"
Selenium Find button containing text,https://www.reddit.com/r/selenium/comments/yxptrt/selenium_find_button_containing_text/,selenium,"Hello,

&#x200B;

Im working on building a small linkedin bot that clicks on likes for my company's posts. The issue at the moment is that all like buttons are dynamic and therefore,  I cannot select via the regular text options. I have been trying to see if I could get something like the following working, but I'm getting an error::

`like = driver.find_element('xpath', ""//button[contains(text(),'Like')]"")`

`print(like)`

&#x200B;

Any help is greatly appreciated.","[""What's the element look like (in the Web Dev Tool, F12)?"", 'What error are you getting?', 'Answer for those wondering: `driver.find_elements(\'xpath\', ""//button[contains(.,\'Like\')]"")`']"
Help with uploading file on Python Selenium using remote driver,https://www.reddit.com/r/selenium/comments/yx49ls/help_with_uploading_file_on_python_selenium_using/,selenium,"I am using python 3.9 and selenium 4.6.0 on Chrome. I have a script that needs to upload a file to an input, this works fine on local but fails when run on RemoteDriver. The code I am using is

    driver.find_element(By.XPATH, ""//input[@accept]"").send_keys('path to file')

When run on RemoteDriver the error returned is

    selenium.common.exceptions.WebDriverException: Message: unknown command: unknown command: session/cddd71e067d7717481fb8a635103c643/se/file

I've think it is due to this line in the remote\_connection.py file in selenium

    Command.UPLOAD_FILE: ('POST', ""/session/$sessionId/se/file"")

From the research I've done the 'se' in this case is a 'vendor\_prefix' for selenium but I cannot figure out how to either configure the remote driver to use a vendor prefix or remove that from POST path that is being passed (short of pulling my own version of the code and maintaining that).

For other functional reasons I can't revert to selenium 3x (which is an option I've seen to correct this), nor can I set w3c to False. Does anyone know how to work around this particular issue; either by getting send\_keys to operate as expected in this situation or using another method to upload the file? Thanks.",[]
trying to download a file by opening the download link with urllib,https://www.reddit.com/r/selenium/comments/ywo4nf/trying_to_download_a_file_by_opening_the_download/,selenium,"    import urllib
    urllib.request.urlretrieve(csv_url, os.path.join(downloading_dir,'Data.csv'))

i have a var a downloading\_dir which specifies the path i want my download to go and Data.csv is the filename that i want once the download is finished.

im getting this error:  **URLError**: <urlopen error unknown url type: blob> 

the url has a weird format that urlopen is not processing well.

this is an ex of one of the links: blob:null/3a9be3c2-02d9-4943-b136-0d064a6bb6bb

if you open this link with selenium driver.get('blob:null/3a9be3c2-02d9-4943-b136-0d064a6bb6bb')

it works with an error but it works and the files gets downloaded with a random name and a location i do not want.

thats why im trying to access the download link in a way i can specify the download location and name.",[]
Trying to soup a page to get to an element because selenium is not finding it,https://www.reddit.com/r/selenium/comments/yvuqq9/trying_to_soup_a_page_to_get_to_an_element/,selenium,"[https://www.who.int/data/gho/data/indicators/indicator-details/GHO/proportion-of-population-below-the-international-poverty-line-of-us$1-90-per-day-(-)](https://www.who.int/data/gho/data/indicators/indicator-details/GHO/proportion-of-population-below-the-international-poverty-line-of-us$1-90-per-day-(-))

Open this url and go to Data tab, inside data tab we have the unclickable link on the right side with inner text: EXPORT DATA in CSV format: Right-click here & Save link

in order to get the data i need to right click and save link as then save in order to get the data i need.

im trying to reach this element by using beautifulsoup but no matter what i do i can't see to find this <a> tag nor its href attribute.

i tried using selenium to get driver.source\_page then use that source\_page in beautifulsoup it still didnt get me the element.

and i tried using selenium itself find\_element i also couldn't reach that <a> i need.

what can i do?","[""The link you posted directs to a page that's 404ing. Would love to take a look if you can fix the link!"", '[`https://www.who.int/data/gho/data/indicators/indicator-details/GHO/proportion-of-population-below-the-international-poverty-line-of-us$1-90-per-day-(-)`](https://www.who.int/data/gho/data/indicators/indicator-details/GHO/proportion-of-population-below-the-international-poverty-line-of-us$1-90-per-day-(-))\n\n&#x200B;\n\nthis is the link if anyone is trying to access it']"
Right click save link as in python,https://www.reddit.com/r/selenium/comments/yvqpwu/right_click_save_link_as_in_python/,selenium,"hello guys i want to right click save link as then save on the save pop up that windows shows.

this is an example:

[https://www.who.int/data/gho/data/indicators/indicator-details/GHO/proportion-of-population-below-the-international-poverty-line-of-us$1-90-per-day-(-)](https://www.who.int/data/gho/data/indicators/indicator-details/GHO/proportion-of-population-below-the-international-poverty-line-of-us$1-90-per-day-(-))

go on this page in the data tab u can see EXPORT DATA in CSV format:Right-click here & Save link

so if u right click and save link as it will let u save the data as csv.

i want to automate that can it be done using selenium if so how?","['I would go with another approach. Find the identifier of the SVG html tag and take a screenshot of that element\nhttps://stackoverflow.com/questions/15018372/how-to-take-partial-screenshot-with-selenium-webdriver-in-python', 'just do context click with selenium and then use pyautogui to click/navigate through the context menu']"
Selenium Side Runner does not create Result Files,https://www.reddit.com/r/selenium/comments/yu2n4x/selenium_side_runner_does_not_create_result_files/,selenium,"Hey Folks, I plan to test a website automatically on a headless server and I'm considering using Selenium for that since I know it from previous web scraping projects. I used Selenium IDE to record a little demo _.side_ file (opens a website and clicks a button) to test my setup. Then, I executed it using the Selenium side runner tool but it did not output anything although the output directory was set. Although my case is as simple as it can be I'm struggling so much already, partly because of poor documentation (ended up digging options/flags from source code) and the tool not doing what one is expecting, i.e. outputting results in a machine readable format. 

Here's what I've done:

**sides.yaml**

```yaml
capabilities:
  browserName: ""firefox""
timeout: 25000
```

**command**

```
selenium-side-runner --config-file=""config/side.yaml""  --output-directory=""results"" --debug tests/demo.side
```

**output**

```
Configuration: {
  baseUrl: '',
  capabilities: { browserName: 'firefox' },
  debug: true,
  filter: '.*',
  force: undefined,
  maxWorkers: 16,
  params: {},
  projects: [ '/home/user/workspace/test-website/tests/demo.side' ],
  proxyOptions: {},
  runId: '5d7ee74cc411318e92cb196738a08653',
  path: '/usr/local/lib/node_modules/',
  server: '',
  timeout: 25000
}
info: Running test demo
debug: Playing state changed prep
info: Building driver for firefox
info: Driver has been built for firefox
debug: Playing state changed playing
debug: executing open|/
debug: passed open|/
debug: executing click|linkText=Antworten!
debug: passed click|linkText=Antworten!
debug: executing click|name=q
debug: passed click|name=q
debug: executing type|name=q|blockchain
debug: passed type|name=q|blockchain
debug: executing click|css=input:nth-child(2)
debug: passed click|css=input:nth-child(2)
debug: Playing state changed finished
info: Finished test demo Success
 PASS  ../../../../../usr/local/lib/node_modules/selenium-side-runner/dist/main.test.js (6.686 s)
  Running project demo
    Running suite Default Suite
      ‚úì Running test demo (6260 ms)

Test Suites: 1 passed, 1 total
Tests:       1 passed, 1 total
Snapshots:   0 total
Time:        6.728 s
Ran all test suites within paths ""/usr/local/lib/node_modules/selenium-side-runner/dist/main.test.js"".
Jest did not exit one second after the test run has completed.

This usually means that there are asynchronous operations that weren't stopped in your tests. Consider running Jest with `--detectOpenHandles` to troubleshoot this issue.
```

Also, when passing the `--output-format` flag, I get the following error:

```
error: unknown option '--output-format=jest'
```

I followed the instructions at https://www.seleniumhq.org/selenium-ide/docs/en/introduction/command-line-runner/ with command line runner version 4.0.0-alpha.16.

**EDIT:** I just noticed that I'm on a alpha version of a presumably new major release and thus there may be breaking changes and not yet complete documentation. All fine but why the hell is this shipped to users that don't request it explicitly? Shouldn't there be separate release channels for unstable versions?

**EDIT:** Downgraded to the last _3.x_ release from 3 years ago(!) and now it outputs results properly. However, it has >20 security vulnerabilities listed in its dependencies which is a red flag for me. Also, the `--output-format` flag is not recognized either which is okay for me but still does not match the docs.",[]
How to find out where script is being run from?,https://www.reddit.com/r/selenium/comments/yty2cu/how_to_find_out_where_script_is_being_run_from/,selenium,"This is quite basic and perhaps not Selenium specific but I've got a Selenium script that's being called through celery, from my webpage built with Django. I thought I knew the file my script was running from but I just commented almost all the code from that file out and the script it still running! How do I find out where it's running from?",['what does the script do? just found for example the locator on which script is clicking across whole project']
VBA Firefox browser Selenium,https://www.reddit.com/r/selenium/comments/yt0gox/vba_firefox_browser_selenium/,selenium,"Hi guys, I just downloaded and learnt Selenium and Selenium Wrapper today for a VBA project. I just want to auto fill a form on web since it‚Äôs repetitive and time consuming but don‚Äôt want to keep opening and quitting Firefox browser all the time. Do you have any suggestions on how to write a function that only auto fill the form with values in specified cells? 
Thank you!!","[""There's no one size fits all formula. It depends on the html syntax of the website. You're asking a very broad question with not enough information provided for someone to give a more detailed answer. You also need to learn the browser API in order to interact with it to automate. Sounds like you need to learn more of the mechanics on how to do this, and come back w/ a more specific question if you get what I mean. It's a lot more complicated than just one function solving something that is very dynamic.""]"
ScreenPy Playwright v0.0.1 is released (and a humble request for help)!,https://www.reddit.com/r/selenium/comments/ysivip/screenpy_playwright_v001_is_released_and_a_humble/,selenium,"Hey friends, we released the [ScreenPy](https://screenpy-docs.readthedocs.io/en/latest/) extension [ScreenPy Playwright](https://screenpy-playwright-docs.readthedocs.io/en/latest/), and we need some help. With Selenium, i personally had a large, professional project to develop the extension with, so i feel that the [ScreenPy Selenium](https://screenpy-selenium-docs.readthedocs.io/en/latest/) extension is getting mature. But i don't have a similar project for Playwright.

If any of you have some time and interest, can you give some suggestions for Actions you would want to see in ScreenPy Playwright for it to cover your use cases? So far, there are only enough to be able to automate [this example test for SwagLabs.](https://github.com/ScreenPyHQ/screenpy_examples/blob/trunk/screenpy_playwright/swaglabs/features/test_cart.py#L15) 

We'd love to get your input! Also, is there a Playwright-specific subreddit? The only one i can find is for script-writing, you know, for theaters.",[]
How to achieve @FindBy in Playwright?,https://www.reddit.com/r/selenium/comments/ys5f1m/how_to_achieve_findby_in_playwright/,selenium,"Hi,  


I am thinking of migrating the 'core' of my framework and try to use Playwright.  
The framework is built on page object model and using the  @ FindBy annotations   
(using Java)  


So the page objects look like the following :  


public class HomePage(){  


@ FindBy(id='Username')  
public WebElement username\_field;

&#x200B;

public HomePage clickOnUsername(){  
username\_field.click();

return this();

}  
}  


I would like to keep the same format also using PlayWright, but it has no annotations of this kind.  
Any ideas how to achieve that?  


Thanks !","[""You can't use that annotation, but you can still use the POM. I'm in the midst of doing this process as well, it's tedious, but well worth it.""]"
How can I automate tests for a whiteboard? The Chrome extension IDE recorder seems to record coordinates.,https://www.reddit.com/r/selenium/comments/yrkbl2/how_can_i_automate_tests_for_a_whiteboard_the/,selenium,"I will soon need to automate some tests for a WebGL whiteboard (to draw and move objects), and I've been trying to practice on some sites that have examples of this (where you can move shapes around), and I noticed that the Selenium IDE Chrome extension recorder appears to track the coordinates. However, when I replay the recording (even after tinkering with what appear to be the coordinates), it fails to work.


Does anyone have experience with this who can share some advice on how to proceed?","['https://o8wi0.csb.app - this is the sample website I was tinkering with, trying to move the shapes around the board.', 'In general selenium IDE is terrible. Try selenium Webdriver.']"
"Hey, scraping developers, I need your help!",https://www.reddit.com/r/selenium/comments/yrg70d/hey_scraping_developers_i_need_your_help/,selenium,"Hey all, 

Are there any experienced scraping API‚Äôs tech-users (the tools like ScraperAPI, ScrapingBee, ScrapingBot, Zenrows, etc.)? Or just web scraping enthusiasts? I really need your help! 

My name is Alex, I am a scraping developer with a mission to build the best Proxy API tool out there (humble is not my way.)  So here is my project - [ScrapeIN‚Äô](https://scrapein.app/)  where I am trying to combine and automate the best practices for bypassing site protection and create all-in-one scraping infrastructure for any data engineer. 

I released the first MVP version of my Proxy API and want to make sure that it works as planned, so it would be awesome if you could help me out and test it for any issues and bugs. 

So to test my ScrapeIn you need to

1. Go [here](https://dashboard.scrapein.app/)
2. Register - it will allow you to use scraper for 14 days with 1000 credits. I can extend access on request if needed, just ping me here or in dms or by email. I don‚Äôt request credit card upon registration or anything, so don‚Äôt worry about the payment that supposedly should follow the trialüòÖ
3. Look through our [API docs](https://dashboard.scrapein.app/docs) 
4. [Use ](https://dashboard.scrapein.app/)the API key given to you for scraping any public data from the web.  
5. [Use](https://dashboard.scrapein.app/query-builder) visual CSS selectors mode in order to extract the necessary data from a site accurately. 
6. Take and submit a short questionnaire Google [form](https://forms.gle/vbEaerevcoDjFNNc6).  
7. Enjoy increased ScrapeIN‚Äô account balance by 1000 free credits! 

I really appreciate any of your feedback and thoughts about ScrapeIN‚Äô. Don‚Äôt hesitate to share with me any of your feedback in DMs or at support@scrapein.app.",['Bypassing site protections is the exact opposite of best practices.']
"Trying to Scroll inside a div with selenium, scroller function only scrolls up to a certain amount and then just stops",https://www.reddit.com/r/selenium/comments/yr3zpj/trying_to_scroll_inside_a_div_with_selenium/,selenium,"I want to get a list of all the list items which are present inside a div with a scroller. They are not loaded at once upon loading the page, rather the items are loaded dynamically as the user scrolls down (until there are no elements left). So, this is the scroller script which I tried to implement:

    def scroller():
        userList = None
        prev = 0    
    
        while True:
            time.sleep(5)
            userList = WebDriverWait(browser, 50).until(
                EC.presence_of_all_elements_located(( By.CLASS_NAME, '<class of list item>' ))
            )
            n = len(userList)
            if n == prev:
                break
            prev = n
            #getting the last element in the list in the view
            userList[-1].location_once_scrolled_into_view

This function scrolls the list upto a certain length, but doesn't go to the full length of the elements (not even half). Can someone please suggest a better way to do this?

Thank you","[""What's the purpose of prev =n"", ""You should *get* the list of visible items and look for the desired element there.  \nIf it's not present you save the last item and scroll to it.  \nOnce again check the list and save the last item.  \nTill the last item doesn't change after scrolling.""]"
Closing a tab in Selenium IDE,https://www.reddit.com/r/selenium/comments/yppzkt/closing_a_tab_in_selenium_ide/,selenium,"Hi! 
I have the steps: clicking on the URL that is external and it opens in a new tab. I then have to go back to the original tab to keep doing something there with Selenium IDE. How can I close that new tab with external link? Or how can I go back to the original tab? Please help","[""Stackoverflow, as usual, has the answer to this question:\n\n[https://stackoverflow.com/questions/12729265/switch-tabs-using-selenium-webdriver-with-java](https://stackoverflow.com/questions/12729265/switch-tabs-using-selenium-webdriver-with-java)\n\nSireesha Middela's answer looks right:\n\n`ArrayList<String> tabs2 = new ArrayList<String> (driver.getWindowHandles());`  \n`driver.switchTo().window(tabs2.get(1));`  \n`driver.close();`  \n`driver.switchTo().window(tabs2.get(0));`"", 'driver.close() : closes the current tab and transfers the driver control to the previous tab.', 'Have you tried the [select window](https://www.selenium.dev/selenium-ide/docs/en/api/commands#select-window) function?', 'Utilize window handles, store the handle of the window prior to opening up the new tab and then you can use switchTo for toggling.']"
Pressing spacebar in selenium (python) to scroll down in a table element,https://www.reddit.com/r/selenium/comments/yovwce/pressing_spacebar_in_selenium_python_to_scroll/,selenium,"What I need to do is, I need a list of all the elements which are basically list-items, but the list doesn't load at once, instead it loads part by part, so the following code doesn't get the list of all the list elements:

    userList = WebDriverWait(browser, 5000).until(
     EC.presence_of_all_elements_located(( By.CLASS_NAME, 'c-virtual_list__item' ))
    )

So, in order to get the list of all the elements present in the list/table, I need to scroll all the way down in the table. I am trying to do that by trying to replicate the following process:

1. Select the element with a scroller by clicking on it
2. Press space to scroll down

I wrote the following piece of code to try and accomplish that:

    scroller = WebDriverWait(browser, 5000).until(
        #this is a div element which contains a scroller
        EC.presence_of_element_located(( By.CLASS_NAME, 'c-table_view_keyboard_navigable_container' ))
    )
    
    prev = 0
    userList = None
    
    #scrolling until I read the end of the list
    while True:
        scroller.send_keys(Keys.SPACE)
        time.sleep(2)
        userList = WebDriverWait(browser, 5000).until(
            EC.presence_of_all_elements_located(( By.CLASS_NAME, 'c-virtual_list__item' ))
        )
        cur = len(userList)
        if cur == prev: break

But this line:  `scroller.send_keys(`[`Keys.SPACE`](https://Keys.SPACE)`)` throws an error:

>selenium.common.exceptions.ElementNotInteractableException: Message: element not interactable

I have seen some code snippets on stackoverflow where people select the body element:

`find_element(By.TagName, ""body"")`

and scroll down the webpage in a similar manner to what I have tried:

`element.send_keys(`[`Keys.SPACE`](https://Keys.SPACE)`)`

However, it doesn't work for me and throws the given error.

Can someone please help me make this work!?

&#x200B;

Thank you for your time :)","[""This isn't working because Selenium expects an element to which it is sending keys to be some form of text field. The element you're selecting isn't able to receive the text input, so Selenium considers that element not interactable.\n\nTaking a step back, ss there a reason you are trying to scroll by pressing the spacebar? Typically scrolling in Selenium is done via executing javascript."", 'Have you tried using Actions chain?', 'Not sure if this would help you but you could use the PyAutoGUI package to actually simulate space bar presses at the appropriate point in your code.  You could also simulate down arrow or page down presses too.  Good Luck!', 'I think [this](https://www.tutorialspoint.com/how-to-scroll-a-specific-div-using-selenium-webdriver-with-java) is what you want.']"
Can i find_element or find_elements that contain a class but this class only?,https://www.reddit.com/r/selenium/comments/yoh99r/can_i_find_element_or_find_elements_that_contain/,selenium,"lets say i have an elements like this:

    <div class=""full-width flex-buttons-container push""></div>
    <div class=""full-width push""></div>
    <div class=""full-width""></div>
    <div class=""full-width""></div>
    <div class=""full-width""></div>

if i use the following:

    ¬†driver.find_elements(By.CLASS_NAME,'full-width')

i will get all 5 divs but what i want is the elements that have full-width class and full-width class only.

so i want the last two divs only can i achieve something like that?","[""You can do this with css selector by excluding the ones you don't need, for example: div.full-width:not(.push)\n\nEdit: or, of course also using css selector: div[class='full-width']\n\nThat will look for a div with a class attribute that matches the String 'full-width' exactly.""]"
Selenium IDE,https://www.reddit.com/r/selenium/comments/yoj5gl/selenium_ide/,selenium,"Selenium IDE is a free, easy-to-use browser automation tool that makes web application testing simple. It is an open source test automation tool that allows you to capture and replay online activity, which then translates into tests that can be rerun at any time. 

In order to construct Selenium test cases as a component of the Selenium suite, the Selenium IDE record & replay tool was released in 2006.

Install the extension (or add-on) for the relevant browser before beginning Selenium automation testing using Selenium IDE. Additionally, the IDE offers a GUI for documenting website interactions. 

Selenium IDE may now be used to test on Chrome browsers in addition to Firefox, which it was previously only accessible to test on. Cross-browser support and Selenium parallel testing are now supported by the IDE.",['Thanks?']
how to target html nested elements ?,https://www.reddit.com/r/selenium/comments/ymqeo9/how_to_target_html_nested_elements/,selenium,"I'm trying to target a div that is deeply nested and has no specific id. Is it possible to get the parent element (that has an id), then to target the child div using find_element by xpath from there ?","['Xpath axes   driver.findElements()', 'You can also use class names in your xpath to simplify the locator. If the element in question has a super class(more than one class) you can use the contains method in your xpath so something like\n\n‚Äú//div[contains(@class, ‚Äòclass name‚Äô)]‚Äù', 'Can you link to the page, or if not then post an example of the HTML in question?']"
Using Driver in Functions and returning that Driver ?,https://www.reddit.com/r/selenium/comments/ym4anm/using_driver_in_functions_and_returning_that/,selenium,"I was wondering if it's a bad practice using functions with webdriver as an argument and return that webdriver in order to use it.
For example :
```python
def search(driver):
   # do something with the driver

    return driver
```

well it works, but i was wondering if good practice or i should avoid that ?","[""I don't return the driver in functions, because I never needed to, but I use it as argument all the time, and I don't see why it would be a bad practice."", 'That sounds like you‚Äôre lacking some abstraction layer between tests and driver itself.\nIdeally your tests should not be aware of drivers existence.', 'You could create a DriverManager Class and use TheadLocal. Then you could set it during start up and use a getter without having to pass the driver as a parameter.']"
How to generate Extent Reports in Selenium?,https://www.reddit.com/r/selenium/comments/ylromm/how_to_generate_extent_reports_in_selenium/,selenium," 

1. Import the JAR file: degreereports-java-2.1.2.jar. After downloading the ZIP file, extract its contents to a folder.
2. Add the JAR file to the build path of the project using the Build Path -> Set Build Path option.
3. Create a new JAVA class for Scope Report with the following code.

&#8203;

    package com.browserstack.demo;
    import org.junit.AfterClass;
    import org.junit.BeforeClass;
    import org.junit.Test;
    import org.openqa.selenium.WebDriver;
    import org.openqa.selenium.chrome.ChromeDriver;
    import com.relevantcodes.extentreports.ExtentReports;
    import com.relevantcodes.extentreports.ExtentTest;
    import com.relevantcodes.extentreports.LogStatus;
    public class ExtentDemo {
    static ExtentTest test;
    static ExtentReports report;
    @BeforeClass
    public static void startTest()
    {
    report = new ExtentReports(System.getProperty(""user.dir"")+""ExtentReportResults.html"");
    test = report.startTest(""ExtentDemo"");
    }
    @Test
    public void extentReportsDemo()
    {
    System.setProperty(""webdriver.chrome.driver"", ""D:SubmittalExchange_TFSQAAutomation3rdpartychromechromedriver.exe"");
    WebDriver driver = new ChromeDriver();
    driver.get(""https://www.google.co.in"");
    if(driver.getTitle().equals(""Google""))
    {
    test.log(LogStatus.PASS, ""Navigated to the specified URL"");
    }
    else
    {
    test.log(LogStatus.FAIL, ""Test Failed"");
    }
    }
    @AfterClass
    public static void endTest()
    {
    report.endTest(test);
    report.flush();
    }
    }

### How to generate Extent Reports in Selenium using NUnit?

    [SetUpFixture]
    public abstract class Base
    {
    protected ExtentReports _extent;
    protected ExtentTest _test;
    
    [OneTimeSetUp]
    protected void Setup()
    {
    var dir = TestContext.CurrentContext.TestDirectory + ""\\"";
    var fileName = this.GetType().ToString() + "".html"";
    var htmlReporter = new ExtentHtmlReporter(dir + fileName);
    
    _extent = new ExtentReports();
    _extent.AttachReporter(htmlReporter);
    }
    
    [OneTimeTearDown]
    protected void TearDown()
    {
    _extent.Flush();
    }
    
    [TestFixture]
    public class TestInitializeWithNullValues : Base
    {
    [Test]
    public void TestNameNull()
    {
    Assert.Throws(() => testNameNull());
    }
    }
    
    [SetUp]
    public void BeforeTest()
    {
    _test = _extent.CreateTest(TestContext.CurrentContext.Test.Name);
    }
    
    [TearDown]
    public void AfterTest()
    {
    var status = TestContext.CurrentContext.Result.Outcome.Status;
    var stacktrace = string.IsNullOrEmpty(TestContext.CurrentContext.Result.StackTrace)
    ? """"
    : string.Format(""{0}"", TestContext.CurrentContext.Result.StackTrace);
    Status logstatus;
    
    switch (status)
    {
    case TestStatus.Failed:
    logstatus = Status.Fail;
    break;
    case TestStatus.Inconclusive:
    logstatus = Status.Warning;
    break;
    case TestStatus.Skipped:
    logstatus = Status.Skip;
    break;
    default:
    logstatus = Status.Pass;
    break;
    }
    
    _test.Log(logstatus, ""Test ended with "" + logstatus + stacktrace);
    _extent.Flush();
    }
    }

**Source:** [Guide to generate Extent reports in selenium](https://qacraft.com/guide-to-generate-extent-reports-in-selenium-webdriver/)",[]
Selenium python wait for download to finish before driver.close(),https://www.reddit.com/r/selenium/comments/yl35k7/selenium_python_wait_for_download_to_finish/,selenium,"im trying to download files and my internet is volatile i can't predict the download time of every file and use time.sleep() for it.

im using firefox what i did is this loop

    while recheck:
        for file in os.listdir():
            if file.endswith('.part'):
                print('Downloading at index: '+ str(y))
                time.sleep(1)
            else:
                recheck = False
    time.sleep(1)

in firefox when u download something if the donwload isn't finished it has the extention '.part' in it

but this is not ideal and im still getting stuck sometimes because the driver tries to close the window before the download finishes and i get the popup that says if i want to close the browser when i have a download in progress.

what can i do? i can't find a useful answer on stackoverflow either.","[""Does it need to be on firefox?\n\nIf not, there's seem to have a solution:\n\nhttps://stackoverflow.com/questions/48263317/selenium-python-waiting-for-a-download-process-to-complete-using-chrome-web"", 'You can create a loop that keeps checking the file size every 5 to 10 seconds. Once the file size becomes constant for 4-5 iterations, you can assume the file is downloaded therefore ending the loop and then quitting the driver.\nHope this helps.', ""Why doesn't your method work though?\n\nAlso where is the variable y u're using is coming from""]"
11 Best Selenium Alternatives You Should Know,https://www.reddit.com/r/selenium/comments/ykteoh/11_best_selenium_alternatives_you_should_know/,selenium,"Below is the list of selenium alternatives:  


1. Robot Framework
2. Cypress
3. Katalon Studio
4. Screenster
5. CasperJS
6. Watir
7. Cucumber
8. Ghost Inspector
9. Lemonce Editor
10. TestCraft
11. Protractor","['Playwright', ""Cucumber is not even meant for being used like Selenium, it's just a framework for gherkin syntax"", '11 Selenium ‚Äòalternatives‚Äô you ‚Äòshouldn‚Äôt‚Äô use', 'Playwright and WebDriverIO', 'How is cucumber an alternative for Selenium? Does cucumber has option to control web browsers?', 'Cypress, Playwright and Puppeteer are definitely alternatives to Selenium but I think unless something has changed a lot of the other things you mentioned use Selenium underneath.\n\nA good way to think about it is if any of those product support a way to connect to a Selenium grid then they are just using Selenium and providing a different API on top of it.', 'Could you do a sort of rating for them? Have you used them? \n\nI‚Äôve used selenium and heard of Cucumber but the rest I haven‚Äôt.', ""The most common tools I've been seeing lately are\n\n- Cypress\n- Playwright\n- Puppeteer\n- WebDriverIO\n- Robot Framework"", ""Protractor is a wrapper on Selenium to deal with Angular. That's the joke, kids."", 'I‚Äôd have to throw Testcafe in there. Has many similarities to playwright/puppeteer.']"
what's the coolest project you've engaged in with Selenium?,https://www.reddit.com/r/selenium/comments/ykfn8f/whats_the_coolest_project_youve_engaged_in_with/,selenium,"What's the most interesting project you've engaged in that's required Selenium to serve as the primary tool?


It can be business/work or personal, any answer is welcome!","[""Inlaws wanted to take the wife and kid camping for a week but couldn't register any campsite at the one and only park they wanted to go to because all the spaces got swept up each morning. If they didn't get a campsite, I wouldn't get my week alone. Within 2 days of the python/selenium script running at 7am, they had a campsite booked and I had my week alone at home. It was fabulous."", 'For work, I wrote a script that takes a list of IP addresses, opens a headless browser that scrapes a MAC address from the end device and logs it into an excel sheet. Ran through 2500 IP addresses in about 4 hours. It would have taken me weeks to do it manually.', 'Wrote boating license test scripts to go through training and take long exam to get state certificatio. Normally training and exam takes 10+ hours to complete but test script can takes within two minutes.']"
Select an element with pointer-events:none,https://www.reddit.com/r/selenium/comments/yjz3v7/select_an_element_with_pointereventsnone/,selenium,"Hello, I am trying to find an element that is hidden behind the ""pointer-events:none"" property. 

I need to find a href value of the footer on that site, but the table in the body has this property to prevent clicking and every element inherits that.

Is there any way to disable this or is there any other way to find that href value?

Thanks for your answers!

&#x200B;

Here is the link for that site:

 https://mirror.ownpage.fr/clients/21d281cc37e84c52/preview/the\_briefing.html",[]
Chrome driver error,https://www.reddit.com/r/selenium/comments/yjzws9/chrome_driver_error/,selenium,https://stackoverflow.com/q/74237502/13284999,"['Use webdrivermanager', 'Share your code', 'Did you updated the chrome driver file? Remember to use the most stable version']"
Trying to get if site is safe,https://www.reddit.com/r/selenium/comments/yjxyoy/trying_to_get_if_site_is_safe/,selenium,"Hi, i was wondering if you can get via selenium information if site is safe eg: i have 2 sites one was flagged by google becouse it has phising content, and other site no. Yet selenium sees that both of the sites are safe. Any ideas?",[]
LogMeIn,https://www.reddit.com/r/selenium/comments/yjduij/logmein/,selenium,Does anyone here use LogMeIn?,"['What is that?', 'I use RemotePC for remote access and data transfer across computers..\n\nVery useful and reliable.']"
Wierd pagination,https://www.reddit.com/r/selenium/comments/yiyqvk/wierd_pagination/,selenium,"Using Python, how do I paginate through this site ? [https://community.tableau.com/s/ideas](https://community.tableau.com/s/ideas)

I can get the links for the first page, I can scrape the information for each item, but I can't figure out how to go to the next page.","['You can locate the ""next"" button with the CSS locator\n\n    lightning-button:nth-child(3) button.slds-button\n\nJust be sure to wait for the element to be clickable before clicking it, because it looks like it takes each page quite a while to load before the page is interactable', 'Normal selector:  \ndriver.FindElement(By.CssSelector(""svg\\[class=\'slds-button\\_\\_icon slds-button\\_\\_icon\\_right\'\\]"")).Click();\n\nWait:  \nnew WebDriverWait(driver, TimeSpan.FromSeconds(5)).Until(ExpectedConditions.ElementToBeClickable(By.CssSelector(""svg\\[class=\'slds-button\\_\\_icon slds-button\\_\\_icon\\_right\'\\]""))).Click();\n\n&#x200B;\n\nIf you\'re planning on doing his for every page you might want to grab the value of the final page number and create a for-loop.\n\nPs the above code is for C#', 'As per my knowledge of Javascript in [qa services company](https://www.qasource.com/) , You need to wait for the page to load properly and then scroll to the element and after that perform the click operation on the element which in your case is the next button:\n\n**Use the following code:**\n\nbrowser.geturl([community.tableau.com/s/ideas](https://community.tableau.com/s/ideas));\n\n//wait for the page to load fullybrowser.sleep(10000) //time in ms\n\n//Scroll to the next button$(\'//button\\[text()=""Next""\\]).scrollIntoView({behavior: ""smooth"", block: ""center"");\n\n// Click on next button$(\'//button\\[text()=""Next""\\]).click();\n\n//wait for the page to load fullybrowser.sleep(10000) //time in ms\n\nHope you find it helpful!']"
Is it possible to perform google authentication on a website without having to enter the password? Selenium always starts with a fresh session with not log ins or password saves,https://www.reddit.com/r/selenium/comments/yidyvd/is_it_possible_to_perform_google_authentication/,selenium,"I am writing an automation script for slack and currently I am using my email address and password to login. But for deploying this script (on a docker container), this is obviously not safe. The problem is that selenium starts with a completely fresh session every time the script is run. So, if I make the script click the google login button, I have to enter the google email address and password in order to log in. Is there some way to do this without password? Some google api or sdk probably?

&#x200B;

Pardon for the vague question. But I just haven't used anything like this before and am stuck on google authentication.

&#x200B;

Thank you in advance :)","['There is an option from selenium that allows using profile data and auto-complete, first of all, try to identify the which profile you use in case you have over 2 profiles, otherwise you can use the default profile. I\'m on my mobile phone, however this is the code.\n\n```from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.chrome.service import Service\n\nclass Driver(object):\n    def __init__(self) -> None:\n          service = Service(ChromeDriverManager().install())\n          options = Options()\n          directory_profile = ""C:\\\\User\\\\yourUserOfYourPc\\\\AppData\\\\Local\\\\\\Google\\\\Chrome\\\\User Data\\\\Default"" #if you have only one profile, you can use this setting, however it\'s not, type it until ""User Data""\n\n          # You manage 2 or more profiles, you necessarily must put this line of code\n          profile_to_use = ""Profile 3"" # pretend that you\'d want to use 3rd profile, use this\n          options.add_argument(""--user-data-dir=%s"", % directory_profile)\n          options.add_argument(""--profile-directory=%s"", % profile_to_use)\n```']"
Problems with new Instagram layout: Unable to locate element,https://www.reddit.com/r/selenium/comments/yim5t8/problems_with_new_instagram_layout_unable_to/,selenium,"I need a nudge in the right direction: Instagram has changed the interface. Since then, my testing script for posts no longer works. The until recently this section was enough to initiate a new post:

driver.find\_element\_by\_xpath('//div\[@class=""\_abm0""\]/\*\[name()=""svg""\]\[@aria-label=""New post""\]').click()

The new interface has been changed only slightly and the element in question looks like this:

<svg aria-label=""New post"" class=""\_ab6-"" color=""#262626"" fill=""#262626"" height=""24"" role=""img"" ... </svg>

Which led me to the following adjustment:

driver.find\_element\_by\_xpath('//div\[@class=""\_ab6-""\]/\*\[name()=""svg""\]\[@aria-label=""New post""\]').click()

But it will not work like this anyway. The error message is:

selenium.common.exceptions.NoSuchElementException: Message: no such element:: {""method"":""xpath"",""selector"":""//div\[@class=""\_ab6-""\]/\*\[name()=""svg""\]\[@aria-label=""New post""\]""}  (Session info: chrome=106.0.5249.121)

Do any of you ""pros"" have any idea what I'm doing wrong?

Thanks!",[]
"Chromium how to disable ""Save and autofill address""",https://www.reddit.com/r/selenium/comments/yicwf0/chromium_how_to_disable_save_and_autofill_address/,selenium,"Hello, recently I started having issues with my selenium autotests due to a chrome popup that appears when address form is being filled. From the options it can be manually disabled via Settings > Autofill > Addresses and more. My question is what is the chrome option name that can be used to disable this in my automation runs ?","['I eventually found the solution to my problem. To anyone that will encounter this in future: the chrome option is called ""autofill.profile\\_enabled"".\n\n    prefs = {\n            \'autofill.profile_enabled\': False\n        }\n    options.add_experimental_option(\'prefs\', prefs)', ""I've been looking for the answer to this question for days now. Thank you! I had to modify a bit so here's what worked for me. Hopefully this helps someone:\n\n`const webdriver = require('selenium-webdriver')`  \n`const chrome = require('selenium-webdriver/chrome')`  \n`const chromeOptions = new chrome.Options()`\n\n`prefs = {`  \n`'autofill.profile_enabled': false`  \n`}`  \n`chromeOptions.setUserPreferences(prefs)`""]"
Packaging msedgedriver.exe with my app,https://www.reddit.com/r/selenium/comments/yie13l/packaging_msedgedriverexe_with_my_app/,selenium,"I have a small .Net app for automating a repetitive task I am required to do every day. After building and publishing the app in Visual Studio 2022, it leaves me with 2 files.

MyAutomation.exe
msedgedriver.exe

The msedgedriver.exe file has to be in the same directory as MyAutomation.exe, otherwise the Edge browser will not open. 

I'm using Edge since I know it will always be installed on a machine I'm using, not sure if this issue would be any different using Chrome.

Is there a reason this is not being packaged with MyAutomation into one executable file?",['You could skip specifying the driver since it will need to be updated routinely and just add WebDriverManager to your dependencies.']
Need help with locating a button,https://www.reddit.com/r/selenium/comments/yi03yi/need_help_with_locating_a_button/,selenium,"Hello, I am attempting to click the coupon button on  [https://coupons.safeway.com/weeklyad](https://coupons.safeway.com/weeklyad). I have tried the following code  to locate various coupons but it doesn't seem to be able to find the element. The error I get is  ""InvalidSelectorError"".  Does anyone have any tips? 

&#x200B;

    WebElement coupon1 = driver.findElement(By.xpath(""/html/body/flipp-router/flipp-publication-page/div/flipp-sfml-component/sfml-storefront/div/sfml-linear-layout/sfml-flyer-image[1]/div/button[1]""));
    

&#x200B;

    WebElement coupon1 = driver.findElement(By.cssSelector(""//button[@aria-label='Large Fuji Apples or Navel Oranges, , $1.28 lb Member Price . Select for details.']""));","['the xpath you used is absolute xpath, can you try with relative one.\n\nand in the second line of code you have given xpath in By.cssSelector()']"
Element not Interactable exception,https://www.reddit.com/r/selenium/comments/yglfwr/element_not_interactable_exception/,selenium,"Hi, I am trying to login to a website - [https://myaccount.play-cricket.com/idp-signin?state=bDdCdExYWXNzNVlTSTBPRUxiMjhzWU9KZW02SGhINTM0NWEySnNncE01VWZmcUZibjNMU2dYdjBuSXlhanltcg&client\_id=qqaXhehov6cu0sd7AEfd&redirect\_uri=https%3A%2F%2Flogin.ecb.co.uk%2Foauth2%2Fv1%2Fauthorize%2Fcallback&response\_type=code&scope=email+openid+profile](https://myaccount.play-cricket.com/idp-signin?state=bDdCdExYWXNzNVlTSTBPRUxiMjhzWU9KZW02SGhINTM0NWEySnNncE01VWZmcUZibjNMU2dYdjBuSXlhanltcg&client_id=qqaXhehov6cu0sd7AEfd&redirect_uri=https%3A%2F%2Flogin.ecb.co.uk%2Foauth2%2Fv1%2Fauthorize%2Fcallback&response_type=code&scope=email+openid+profile) using selenium. When I attempt to enter something into the password field I get the element not interactable exception. This is my code:  driver.find\_element([By.NAME](https://By.NAME), 'password').send\_keys(password) .

Any help would be appreciated",['Did you wait until the page fully loaded?']
Process unexpectedly closed with status 11,https://www.reddit.com/r/selenium/comments/yftdr9/process_unexpectedly_closed_with_status_11/,selenium,"Hello, I'm trying to run selenium but i get this error.

This is the program in python:

    ""First selenium script""
    from selenium import webdriver
    from selenium.webdriver.firefox.service import Service as FirefoxService
    from webdriver_manager.firefox import GeckoDriverManager
    
    
    driver = webdriver.Firefox(
        service=FirefoxService(executable_path=GeckoDriverManager().install()))
    
    driver.get(""https://www.google.com"")

This is the console output:

    alex@nobara ~/selenium$ python main.py
    Traceback (most recent call last):
      File ""/home/alex/selenium/main.py"", line 7, in <module>
        driver = webdriver.Firefox(
      File ""/home/alex/.local/lib/python3.10/site-packages/selenium/webdriver/firefox/webdriver.py"", line 177, in __init__
        super().__init__(
      File ""/home/alex/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py"", line 272, in __init__
        self.start_session(capabilities, browser_profile)
      File ""/home/alex/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py"", line 364, in start_session
        response = self.execute(Command.NEW_SESSION, parameters)
      File ""/home/alex/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py"", line 429, in execute
        self.error_handler.check_response(response)
      File ""/home/alex/.local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py"", line 243, in check_response
        raise exception_class(message, screen, stacktrace)
    selenium.common.exceptions.WebDriverException: Message: Process unexpectedly closed with status 11
    

This is the content of geckodriver.log:

    1666974769008	geckodriver	INFO	Listening on 127.0.0.1:58881
    1666974769634	mozrunner::runner	INFO	Running command: ""/usr/bin/firefox"" ""--marionette"" ""--remote-debugging-port"" ""56809"" ""--remote-allow-hosts"" ""localhost"" ""-no-remote"" ""-profile"" ""/tmp/rust_mozprofile6UjIeC""
    ExceptionHandler::GenerateDump cloned child 6111
    ExceptionHandler::WaitForContinueSignal waiting for continue signal...
    ExceptionHandler::SendContinueSignalToChild sent continue signal to child

I don't know what to do.

I'm on Linux fedora 36.",['Is your OS only terminal based? It may be because the driver isn‚Äôt running headless but there‚Äôs no way to bring up the browser.']
GoLogin and using websocket for devTools,https://www.reddit.com/r/selenium/comments/yfpbkt/gologin_and_using_websocket_for_devtools/,selenium,"I'm trying to use GoLogin api with python/selenium to run the browser in the cloud. But Chrome driver expects ip:port for debuggerAddress instead of websocket. The GoLogin api returns a websocket address. 

Here is the code: 


debugger_status = gl.startRemote()
debugger_address = debugger_status[""wsUrl""]
print(debugger_address)

chrome_options = ChromeOptions()
chrome_options.add_experimental_option(""debuggerAddress"", debugger_address)
service = ChromeService(executable_path=chrome_driver_path)
driver = webdriver.Chrome(service=service, options=chrome_options)
driver.get(""http://www.python.org"")
assert ""Python"" in driver.title
time.sleep(3)
driver.close()
gl.stop()


When I run this code, I get this exception:

selenium.common.exceptions.InvalidArgumentException: Message: invalid argument: cannot parse capability: goog:chromeOptions
from invalid argument: cannot parse debuggerAddress
from invalid argument: must be 'host:port'


The debugger address looks like this: 

wss://635afEab6124ee712e5f2572.orbita.gologin.com/devtools/browser/4e3f8eda-cd49-45D7-9d51-acb76356fb77

How can I change the address of Chrome devTools to use a websocket instead of ip:port ? Or is there another way to do this?",[]
"Need help with making a generic Xpath within the Action (Helper location), that I can reuse on the web-application form page",https://www.reddit.com/r/selenium/comments/yfjv88/need_help_with_making_a_generic_xpath_within_the/,selenium,"Hi All,

&#x200B;

I‚Äôm new to automated testing with Selenium and have some questions regarding the following that I want to accomplish.

\-	I want to define a generic Xpath within the Action (Helper location), that I can reuse for several fields on the web-application form page.

\-	Basically I want to find the class element that I have written down in Step 2 and fill the k-textbox with a value by using the generic Xpath from Step 1.

&#x200B;

The below example is not working for me and I guess I have messed it up, would be nice If someone could provide me with an example. Thanks!

&#x200B;

Step1:

My current generic Xpath within the Action (Helper location), that I want use for several fields on the webpage form.

&#x200B;

Example what I got.

&#x200B;

public void SelectAndTypeEditFieldByLabelGeneric(string label, string searchtext)

{

TypeByXpath(""//\*\[contains(@class,'form-col') or contains(@class, 'form-input')\]//\*\[text()='"" + label + ""'\]/parent::div//input"", searchtext);

}

&#x200B;

Step 2:

From my steps location I fil in the input values with the reference toward the generic Xpath from step 1.

&#x200B;

Example what I got.

&#x200B;

\[When(@""I Fill In The Account Details"")\]

public void WhenIFillInTheAccountDetails(string label, string searchtext)

{

Action.SelectAndTypeEditFieldByLabelGeneric(""Bedrijfsnaam"", ""Test Company"");

Action.SelectAndTypeEditFieldByLabelGeneric(""Straat"", ""Test Street"");

Action.SelectAndTypeEditFieldByLabelGeneric(""Huisnummer"", ""999"");

    }  

Furthermore the When condition here has a relation with the SpecFlowFeature for the scenario.

&#x200B;

Scenario: Add A New Account

    When I Fill In The Account Detail

EDIT: I found the issue and solved the problem.

I changed: 

public void SelectAndTypeEditFieldByLabelGeneric(string label, string searchtext) With -> 'public void WhenIFillInTheAccountDetailsTest() .  

And within Action I changed the string searchtext with string value

public void SelectAndTypeEditFieldByLabelGeneric(string label, string searchtext)

{

TypeByXpath(""//\*\[contains(@class,'form-col') or contains(@class, 'form-input')\]//\*\[text()='"" + label + ""'\]/parent::div//input"", searchtext);

} ",[]
Website Blocking Selenium Input,https://www.reddit.com/r/selenium/comments/yf9790/website_blocking_selenium_input/,selenium,"Some background: I have been working on a project for a while now that scrapes fares off Amtrak's site so a calendar view of fares can be seen at once. Initially, Amtrak would throw an error anytime I tried to make a search on the site, but adding the code below as an argument to options fixed that.

    ""--disable-blink-features=AutomationControlled""

Now, I am struggling with a much more challenging kind of error. Using the above code, I can access the site and perform searches. However, after making many consecutive searches (the number varies but around 5+), the site stops loading searches again for 10-20 minutes. What is particularly strange about this error is that Amtrak is not blocking my browser, if I manually enter the same information Selenium does through the webdriver browser the site loads fine. I have tried using the undetected\_chromedriver extension and altered my input to appear more human-like by entering phrases character by character, adding random delays between every action, and hovering over elements before clicking. Somehow, Amtrak is able to differentiate my human input from Selenium, and I have no idea how. I'd really appreciate any ideas for how to change my code to make the form input undetectable.","[""Can you share your code so far so we can try tinkle with it to find a solution, I've a few ideas in my mind already""]"
"How to make a list by ""Label for"" elements",https://www.reddit.com/r/selenium/comments/yeo6dq/how_to_make_a_list_by_label_for_elements/,selenium,"Hello,

I have these elements:

 <label for=""categories\\\_36"">  


 <label for=""categories\\\_38"">  


 <label for=""categories\\\_6"">  


.....

I want to create a list with all of these elements and after that to click on every of them.

Maybe something like that:   all\_categories = driver.find\_elements(By.CSS\_SELECTOR,'label\[for=')

pp I am using Python and Selenium in  Jupyter Notebook.

Thanks in advance!","[""Find elements by xpath  \n    //label[contains(@for,'categories')]"", 'Driver.find_elements(By.xpath(‚Äú//label[contains(.,‚Äòcategories‚Äô)])']"
SeleniumIDE store values on a big table to txt,https://www.reddit.com/r/selenium/comments/yeb8ym/seleniumide_store_values_on_a_big_table_to_txt/,selenium,"hello i wanted to save the values of a big html table into a text file im currently working with selenium ide & selenium side runner  
any help would be good","[""If you're using selenium side runner may I recommend a few additional steps forward with robot framework.  Quite a many things you can do with it along with selenium.  I used it just this week to scrape 8k urls and store their element text values into a PostgreSQL database...""]"
Noobie needs help,https://www.reddit.com/r/selenium/comments/ye0sfm/noobie_needs_help/,selenium,"Hello there,

&#x200B;

at the moment i try to programm a small bot for myself, but know i run into following error and im not able to solve it by myself.

 **AttributeError**: 'WebDriver' object has no attribute 'execute\_scipt' 

&#x200B;

The Script is pretty simple:

    from selenium import webdriver
    from selenium.webdriver.common.keys import Keys
    from webdriver_manager.chrome import ChromeDriverManager
    
    driver = webdriver.Chrome(ChromeDriverManager().install())
    driver.implicitly_wait(10)
    
    driver.get(""fantastic-website"")
    driver.execute_scipt('document.getElementsByName(""btn-add-to-cart"")[0].click()')

I alredy tested the JavaScript in the DevTools Console and the Script is working.

Can someone give me a hint?

&#x200B;

Kind regards","['You have a typo; you\'re missing the ""r"" in ""execute_script"" so your code currently says ""execute_scipt"" . \n\nIt sounds like you\'re probably not using an IDE, or it would have highlighted the error for you. I recommend doing your development within an IDE like Pycharm.\n\nHope this helps!', 'So this is simply a typo, but a good lesson for a new person since the error tells you exactly what\'s wrong. \n\nThis line:\n\n    driver = webdriver.Chrome(ChromeDriverManager().install())\n\nAssigns the name \'driver\' to an instance of the webdriver module. \n\nThis line:\n\n    driver.execute_scipt(\'document.getElementsByName(""btn-add-to-cart"")[0].click()\')\n\nThen tries to utilize an attribute of the webdriver module ... but there\'s a problem there. There is no such attribute, according to the error. So what\'s the problem? Well there\'s no \'execute\\_scipt\' attribute, right? So ... fix the typo! The error tells you where the typo is, so it\'s easy once you get used to reading such things.  \n\n\n^((if that\'s too obtuse, just change \'execute\\_scipt\' to \'execute\\_script\'))', 'Where you got this code from']"
Is it possible to automate clicking a browser extension?,https://www.reddit.com/r/selenium/comments/yda3k6/is_it_possible_to_automate_clicking_a_browser/,selenium,"Hi.

I'm using Firefox on both my mobile phone and my PC and I have a tendency to open a lot of Facebook, 9gag, etc. tabs that contain videos on my mobile phone and then send the list of tabs to my PC where I open each link manually, click the VideoDownloadHelper addon on upper right, choose quality of video to download then swap over to next tab.

Is it possible to automate this with Selenium? The whole process would look like this:

1) Click VideoDownloadHelper
2) Click ""HLS streaming"" option in drowndown menu from VideoDownloadHelper with highest resolution
3) Ctrl+Tab to next tab
4) repeat until out of tabs


Is something like this possible with Selenium?","['Hi,\n\nYes, i think its possible. how do you send the list of these Tabs to your browser PC? is it via bookmarks? anyway, i was just thinking aloud bcoz u can automate the opeing of these Tabs too.\n\nLet me know if you  need help.', 'You could use a tool called AutoIt to accomplish this, but you can\'t use Selenium directly to do this. With AutoIt you should be able to automate the clicking sequence required to perform the tasks you want.   \n\n\nBut I have to ask if all you\'re doing is ""picking up where you left off"" on your PC after being on your mobile, why not use firefox\'s built in sync which is designed to do exactly this? All you have to do is create a profile so that you can keep track of these tabs on whatever device you\'re on then switch devices and boom! Bob\'s your Uncle.   \n\n\nIf that\'s sounds like exactly what you were looking for then click [here](https://www.mozilla.org/en-US/firefox/sync/) for more info, and have a great day!', 'Puppeteer can automate browser extensions']"
firefox/geckdriver slow to load [python/pytest],https://www.reddit.com/r/selenium/comments/ydazum/firefoxgeckdriver_slow_to_load_pythonpytest/,selenium,"Firefox can take over 30 seconds to load the first page.  Once the browser loads I have no issue with speed.   chromedriver loads almost instantly.

Any issues with my config?

    fp = webdriver.FirefoxProfile('C:\\Users\\user\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\7ot81hip.my--dev')
    driver = webdriver.Firefox(fp)

&#x200B;","[""I created a new profile to use and its way faster.  Not sure if that's all I did but it seems to have made a huge difference.""]"
Dealing with div number changes,https://www.reddit.com/r/selenium/comments/ycoqtv/dealing_with_div_number_changes/,selenium,"I am currently working in a automation of a kind of web form, but the it seems that the dibs numbers of the elements changes if I tried to retest my code (which means open the browser logging in, query the form and so on...). Here is an example of the  full xpath change:

1. 

/html/body/div[38]/div[2]/div/div[2]/div/div/div[1]/div[1]/div/table/thead/tr[2]/th[1]/span/span/span/input

2. 

/html/body/div[29]/div[2]/div/div[2]/div/div/div[1]/div[1]/div/table/thead/tr[2]/th[1]/span/span/span/input

If I try to use shorter version of xpath a get a ""grid number"" Id. 

Already tried to use contains and Starts with, but I got Not interactible element error.

Any thoughts?","['The full xpath path will change 99% if it is a modern site, since new things will be added to the site and the site could update as well, making your full xpath path invalid.\n\nDon\'t use full xpath path and learn to write your own xpath. It is not hard to learn and helps you with much shorter paths, which can be calculated much faster by selenium as well. So it should be in your best interest to learn to write xpath paths.\n\nEdit: For example if the input box has a text in it just use ""//input[text ()=\'the text in the box\']""', 'Hey,  dont use XPath  unless you now what you are doing. Use element locators. you will be frustrated especially if you are new to Selenium']"
https://www.techsravi.com/automation-testing-resume-for-4-years-in-experience/,https://www.reddit.com/r/selenium/comments/ycycnu/httpswwwtechsravicomautomationtestingresumefor4yea/,selenium,,[]
How do I click on Youtube cookies 'Accept all' correctly?,https://www.reddit.com/r/selenium/comments/ycfwjn/how_do_i_click_on_youtube_cookies_accept_all/,selenium,"[https://i.ibb.co/cy796c7/index.png](https://i.ibb.co/cy796c7/index.png)

Here's the code I currently use, but it doesn't work all the time. Sometimes it just errors out. Any ideas?

    el_xpath = '//*[@id=""content""]/div[2]/div[6]/div[1]/ytd-button-renderer[2]/a'
    WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.XPATH, el_xpath)))
    self.driver.find_element(""xpath"", el_xpath).click()

Error:

      File ""/home/admin/DEV/Python/bbot/scrap/__init__.py"", line 51, in click_accept_all
        WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.XPATH, el_xpath)))
      File ""/home/admin/DEV/Python/bbot/venv/lib/python3.10/site-packages/selenium/webdriver/support/wait.py"", line 90, in until
        raise TimeoutException(message, screen, stacktrace)
    selenium.common.exceptions.TimeoutException: Message: 
    Stacktrace:","['You could try locating the element by text ?', 'Use text() xpath', 'tried like this too, also not working from time to time...\n```\nWebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, ""//span[contains(text(), \'Accept all\')]"")))\nself.driver.find_element(""xpath"", ""//span[contains(text(), \'Accept all\')]"").click()\n```']"
Is there a method to wait without a timeout?,https://www.reddit.com/r/selenium/comments/yc7ene/is_there_a_method_to_wait_without_a_timeout/,selenium,"Hello,

I have a method in which I load a new page and the user has to login. Currently I am waiting like this until the user is logged in:  
`WebDriverWait(self.driver, TIMEOUT).until(EC.presence_of_element_located((By.CLASS_NAME, ""navigiumlogo"")))`

(The logo shows when logged in, thats the condition I am waiting for here) But the problem here is that the user has to be logged in in 40 seconds (TIMEOUT) or it throws an error. And now my question is: Should I catch that error and ""rewait"" or is there a better way to wait until the user has logged in? (preferably without a timeout?)","['Two suggestions:\n\n1. Increase the length of your wait, either as an implicit wait when you first instantiate the driver, or as an explicit wait specific to that line of code.\n\n2. Use a while loop, with the condition that the element is not present. As soon as the element is present, the loop will break.', ""What is the reason for it taking 40 seconds to login?\n\nSeems overly excessive. Are you connected to a remote DB or something?\n\nThe easiest thing to do is increase the timeout to like 2 mins and you'll never get an error. If the user is logged in before this time the test will just continue on."", 'time library, time.sleep()', ""Just unplug the router and tell your boss the internet is down and you can't work until its back up.""]"
How do I get Google Image URL?,https://www.reddit.com/r/selenium/comments/yblpzc/how_do_i_get_google_image_url/,selenium,"[https://i.ibb.co/f2TybWm/Recording-2022-10-23-at-18-03-43.gif](https://i.ibb.co/f2TybWm/Recording-2022-10-23-at-18-03-43.gif)

&#x200B;

Any ideas how to get image url from this widget when you click on image?  I really struggle with this...","['`img_list = WebDriverWait(driver,5).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, """"""img.rg_i[data-deferred]"""""")))`\r  \n`for i in img_list():`\r  \n`print(i.get_attribute(""src""))`']"
Bet365 scrape,https://www.reddit.com/r/selenium/comments/ybkpsp/bet365_scrape/,selenium,"Does someone Have a fix to access the bet365 webbpage I can access it but can‚Äôt see any odds, i can pay if you Have a solution","['Which Game are u trying to scrap?i have done several Bet365 scraping', 'Ok.share the code u need help on', 'they updated their bot detection, u should find out what u need to patch. Maybe try UC v2, if u didnt yet.']"
"Absolute beginner to Selenium Java, can't figure this out",https://www.reddit.com/r/selenium/comments/yb33jv/absolute_beginner_to_selenium_java_cant_figure/,selenium,"Hi everyone.

I'm applying for a new job and have to do a case due by tomorrow so I'm trying to learn Selenium Java.

I was following some guide on youtube and everything was smooth until I hit this absolute brick wall. So when this lady recorded this video a few months ago, the design of her AUT was different and the UI elements had ID's assigned to them.

&#x200B;

But the updated version of the site no longer has any UI elements with ID's. So now I have to figure this thing out by myself. I thought it might be good exercise and I'd tackle it in 5 minutes but I've been trying to give input to two textboxes and click one button for over two hours with no success. I'm about to absolutely lose my mind because all my efforts have been fruitless so far, I don't even know how many pages of stackoverflow I read or how many different xpath/CssSelector/whatever variations I tried. I just can't seem to get it. This ""minor inconvenience"" is driving me to the point of madness.

The site in question is linked below. Since it's a dummy site intended for testing, I don't think it really counts as advertising...

[https://opensource-demo.orangehrmlive.com/web/index.php/auth/login](https://opensource-demo.orangehrmlive.com/web/index.php/auth/login)

The username element:

<input class=""oxd-input oxd-input--active"" name=""username"" placeholder=""Username"" autofocus="""" data-v-844e87dc="""">    


Some of the things I tried (I tried god knows how many variations, just a few off the top of my head):

//input\[name='Username'\]

//input\[text='Username'\]

.// variations

..// variations

//Input\[text='Username'\]

//input\[placeholder='Username'\]

.//div/input\[text='Username'\]

//inputwpleƒ±fowepfeprƒ±ogerg

Can. Someone. Click. The. Damn. Username. And. Password. And. Login. Fields?

Could somebody tell me what I'm failing to see in all this? I'll be eternally grateful if you could help me realize my error.

&#x200B;

Sorry for the rant, and thank you.","['I think you\'ll find the following css selector-based approach works:  \n\n\ndriver.findElement(By.cssSelector(""input\\[name=\'username\'\\]"")).click();driver.findElement(By.cssSelector(""input\\[name=\'password\'\\]"")).click();driver.findElement(By.cssSelector(""button\\[type=\'submit\'\\]"")).click();', 'See my comment to one of the replies lmk if that works', 'driver.findElement(By.xpath(""//*[@id=""app""]/div[1]/div/div[1]/div/div[2]/div[2]/form/div[1]/div/div[2]/input"").click();', ""You're using Username, capitalized, and should be using case sensitive values in the properties"", 'Use selectorshub for finding out xpath']"
I'm making a big project to automate all the boring stuff at my company,https://www.reddit.com/r/selenium/comments/yb3hlg/im_making_a_big_project_to_automate_all_the/,selenium,"Hi everyone, I get started a project to check a lot of websites and scrape data from them, even though I'm wondering what *pattern design* I could use for this.

The idea basically is be able to extract some information from a file.csv and use it for scraping and filtering on these portals, furthermore, I would like to implement a GUI for being easier to use by my coworkers who don't know anything about programming or surfing on the terminal","['Selenium would be awful for this.', 'You could create a shiny app and use RSelenium for the GUI']"
Selenium is crashing without any errors,https://www.reddit.com/r/selenium/comments/yb0n3p/selenium_is_crashing_without_any_errors/,selenium,"Hello,

my Python Selenium Script is permanently crashing and I don't know why. I even redownloaded the Chrome Driver but it  still keeps crashing without any errors. My script is pretty fast so I don't know if it's so fast it crashes, because if I debug the program it runs completly normal and doesn't crash. This is the code I've newly written (and I feel like the error is coming from):

&#x200B;

        def find_element_selenium(self, by, name):
            return WebDriverWait(self.driver, TIMEOUT).until(EC.presence_of_element_located((by, name)))
    
        def find_elements_selenium(self, by, name):
            return WebDriverWait(self.driver, TIMEOUT).until(EC.presence_of_all_elements_located((by, name)))
    
        def load_new_page(self, url):
            self.driver.get(url)
            WebDriverWait(self.driver, TIMEOUT).until(EC.presence_of_element_located((By.CLASS_NAME, ""navigiumlogo"")))
    
        def login(self):
            print(""Bitte einloggen..."")
            self.driver.get(""https://www.navigium.de/schule/login/mainmenu.html"")
            WebDriverWait(self.driver, TIMEOUT).until(EC.presence_of_element_located((By.CLASS_NAME, ""navigiumlogo"")))

And this is the whole script for anyone that wants to see it: [https://pastebin.com/nQQCiHRN](https://pastebin.com/nQQCiHRN)

&#x200B;

Edit: I kinda found the error. I have a while loop that looks if a element is present with my find\_element\_selenium function, which works every where btw, but not with my while loop, so my guess is, that the while loop checks to often or something for the condition that it early-exits and doesn't even get in the while loop","[""What's the error?"", 'You should have some type of exception message..', 'You need to configure a log path (file) in the Service object. Then it will likely generate the output.']"
run headfull Chrome and Selenium in Docker,https://www.reddit.com/r/selenium/comments/ya9vzt/run_headfull_chrome_and_selenium_in_docker/,selenium,"It took me a few hours to finally figure out how to run headfull (not headless) Chrome and Selenium in Docker. In case that's helpful to anyone else, I wrote up a quick instructions and shared my source files.

https://www.texastim.dev/bloglet/dockerized-headfull-chrome

https://github.com/timoteostewart/dockerized-headfull-chrome-selenium","['xvbf was what we used before browsers had headless, it is practically the same. youcan chose to see the buffer, but it is a bit slower. containers do not have a graphical console.\n\nwhy do you want to run full mode in a container?\n\nyou can run it in full locally to develop and debug and run it in  headless in the container when it is finished, it is just one parameter.']"
"Need help using <div title class=""something""> in a find_elements",https://www.reddit.com/r/selenium/comments/yaapod/need_help_using_div_title_classsomething_in_a/,selenium,"I am very new to selenium but I have been able to reference most things by find\_elements(BY.CLASS) but I can't seem to figure out how to store the information that is in <div title class=""something"">. 

Any and all help is appreciated.",[]
"Retrieve class schedule changes from website using python and selenium No Such Element error, ID, XPATH, and more.",https://www.reddit.com/r/selenium/comments/ya0tml/retrieve_class_schedule_changes_from_website/,selenium,"My school has a system that tells us if our schedule has any changes.
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.support.select import Select


url = ""https://www.alliancetlv.com/◊¢◊ì◊õ◊ï◊†◊ô-◊û◊¢◊®◊õ◊™""
driver = webdriver.Chrome()
driver.get(url)
driver.implicitly_wait(5.0)

examButton = driver.find_element(By.ID, 'TimeTableView1_btnChanges')
``` 
I'm trying to find an element, and later click it using selenium. every time i try to find literally anything it returns No Such Element error. I tried by ID, class name, name, and even **XPATH**.

this is the website: https://www.alliancetlv.com/◊¢◊ì◊õ◊ï◊†◊ô-◊û◊¢◊®◊õ◊™
and I'm trying to click one of the tabs called ""changes/◊©◊ô◊†◊ï◊ô◊ô◊ù""

my end goal is to click the dropdown to the side, select a class, click the changes tab, then get all the data inside of it, then maybe format it. for now i just wanna understand why this wont fing work","['You use single quotes  when you pass the ID value (By.ID, \'TimeTableView1\\_btnChanges\')\n\nChange to double quotes:\n\nexamButton = driver.find\\_element(By.ID, ""TimeTableView1\\_btnChanges"")']"
Connect Selenium to existing Chrome session,https://www.reddit.com/r/selenium/comments/y9vt26/connect_selenium_to_existing_chrome_session/,selenium,"Backstory - with atlassian's last update, Jira can no longer expand all comments on a ticket automatically.  Atlassian did this to ""fix"" a bug (one that doesn't affect us).  Now we need to click a bar, and then shift click another bar in order to see all of the comments on a ticket.  It's a giant pita.

What I'd like to do -  
Write a script that will interact with the web session (which is already loaded) and click/shift-click on the necessary bars. 

Is this something that selenium is capable of?","['According to this tutorial, yes.\n\nhttps://learn-automation.com/how-to-execute-selenium-scripts-on-already-opened-browser/\n\nI never test it myself.', 'Just wanted to say, I feel your pain.\n\nTrying to write Selenium scripts to do some Jira setup (configure LDAP, set some default permissions), and I feel like the Atlassian web developers were on Acid when they wrote the code.']"
Probably missing a step using Mocha,https://www.reddit.com/r/selenium/comments/y8zzea/probably_missing_a_step_using_mocha/,selenium,"I'm teaching myself Selenium, and I've got that working on its own, but I'd like to use Mocha as well. I've definitely got Mocha installed on the project; I can see its version listed in my package.json. I've tried to ""require"" Mocha in the JavaScript file being run, but I'm not sure that's doing anything. Any time I try to use ""describe"", ""it"", or other Mocha-specific, I get an error that it can't find those function definitions anywhere. What am I missing?",[]
"How to wait for a text to disappear, and then retrieve values from elements? Python",https://www.reddit.com/r/selenium/comments/y93tfp/how_to_wait_for_a_text_to_disappear_and_then/,selenium,"So, I'm trying to scrap values from a table. There's a <tr> element and inside of that there are 12 <td> elements which I want the values of. The issue is, for a split second, when the page loads, only one <td> element appears inside of the parent <tr> and this <td> contains a text, ""Some Text"" and then later it disappears and the <tr> element is populated with 12 <td> elements which holds the values I want. Here is what I've been doing...

    data_list = []
    driver.get(url)
    wait = WebDriverWait(driver, 30)
    data = driver.find_elements(By.TAG_NAME, ""td"")
    wait.until_not(EC.text_to_be_present_in_element((By.TAG_NAME, ""td""), ""Some Text""))
    
    for item in data:
      value = item.get_attribute(""innerHTML"")
      date_list.append(value)
    
    print(data_list)

But, I get this error from `value = item.get_attribute(""innerHTML"")`

    Message: stale element reference: element is not attached to the page document","['I believe this is happening because the page changes between when you gather the elements in your data list, and when you attempt to operate on them. If you change the order of lines 4 and 5 I expect you will no longer encounter that error.', 'Do a try except and catch that error in a while loop. Then move on']"
Log4j is not logging into file or console,https://www.reddit.com/r/selenium/comments/y915ld/log4j_is_not_logging_into_file_or_console/,selenium,"I'm new to Selenium and need help with logging. Currently cannot log anything using Log4j. Could you help mw with why? Here are some relevant snippets of my code:

Create logger as part of setup:

    public class BaseClass {
    	public String baseURL = ""https://demo.guru99.com/v3/index.php"";
    	public String username = ""demo"";
    	public String password = """";
    	public static WebDriver driver;
    	public static Logger logger;
    	
    	@BeforeClass
    	public void setup() {
    		WebDriverManager.edgedriver().setup();
    		driver = new EdgeDriver();
    		driver.manage().window().maximize();
    		
    		logger = LogManager.getLogger(""ebanking"");
    		
    		
    	}
    

Log something using Logger:

    public void loginTest() {
    		driver.get(baseURL);
    		logger.info(""URL is opened"");
    		LoginPage lp = new LoginPage(driver);
    		
    		driver.manage().timeouts().implicitlyWait(Duration.ofSeconds(2));
    		
    		if(lp.iframeIsVisible()) {
    			logger.info(""GDPR popup displayed"");
    			lp.switchToFrame();
    			lp.clickAccept();
    			lp.switchToDefault();
    		}

[log4j2.properties](https://log4j2.properties):

    name=PropertiesConfig
    property.filename = logs
    appenders = console, file
    
    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = [%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c{1} - %msg%n
    
    appender.file.type = File
    appender.file.name = LOGFILE
    appender.file.fileName=${filename}/MyLogs.log
    appender.file.layout.type=PatternLayout
    appender.file.layout.pattern=[%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c{1} - %msg%n
    
    loggers=file
    logger.file.name=demo
    logger.file.level = debug
    logger.file.appenderRefs = file
    logger.file.appenderRef.file.ref = LOGFILE
    
    rootLogger.level = debug
    rootLogger.appenderRefs = stdout
    rootLogger.appenderRef.stdout.ref = STDOUT","[""You're asking Selenium people why log4j isn't working?""]"
Driver doesn't locate the element sometimes even when using webDriverWait.,https://www.reddit.com/r/selenium/comments/y8gksz/driver_doesnt_locate_the_element_sometimes_even/,selenium,"Hello everyone.    
I'm trying to scrape the main page of TikTok. specifically the grid videos on the main page of any profile. for instance, if you inspect his page [https://www.tiktok.com/@shopcider](https://www.tiktok.com/@shopcider)   
and paste this CSS selector ""div\[mode='compact'\]"" you will get the gird of videos. Once I grab the element I can loop through each video by getting the divs inside the parent div.   
All is working perfectly. However sometimes when I run the code it keeps saying that this element can't be located!! even tho the previous run was successful.  


Im using selenuim js.  
`const videosGrid = await driver.wait(until.elementLocated(By.css(""div[mode='compact']"")), 5000)`  
`const videos = await videosGrid.findElements(By.xpath('div'))`  


I do have 5 seconds waiting for this element. I don't get why sometimes it works so fine and sometimes it doesn't find it.   
I'm using a headless browser and running the script on Heroku server.","['Have you tried waiting longer than 5 seconds?', ""Can you share the whole code I'll run it here and report back to you""]"
Why is the third click action here timing out?,https://www.reddit.com/r/selenium/comments/y8bht8/why_is_the_third_click_action_here_timing_out/,selenium,"I'm trying to navigate through this webpage so I can scrape data from the ""Matchups"" table. What I've written so far is able to click through the ""basic"" button and the ""matchups"" button, but the request to click the ""show numbers"" button always times out unless I give the full XPATH as the locator. I think that it could be that the condition ""element\_to\_be\_clickable"" is not met because an ad covers the button so it isn't visible, however I don't think that explains why it works when I supply the full XPATH.

    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    import time
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.common.action_chains import ActionChains
    
    s = Service(""C:\Program Files (x86)\chromedriver.exe"")
    driver = webdriver.Chrome(service=s)
    
    driver.get(""https://www.vicioussyndicate.com/data-reaper-live-beta/"")
    driver.maximize_window()
    
    wait = WebDriverWait(driver, 20)
    
    basicBtn = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=""basicBtn""]')))
    driver.execute_script(""arguments[0].click();"", basicBtn)
    
    table = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=""table""]')))
    driver.execute_script(""arguments[0].click();"", table)
    
    showNum = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=""showNumbers""]')))
    driver.execute_script(""arguments[0].click();"", showNum)
    
    data = list(map(lambda x: x.text, driver.find_elements(By.CLASS_NAME, ""textpoint"")))
    print(data)
    print(len(data))
                            
    
    time.sleep(5)
    driver.quit()

I could, of course, just supply the full XPATH, although I've heard that this is not good practice and is less resilient to changes to the website.

Note: This code also times out with showNum located [By.ID](https://By.ID)(""showNumbers"")

EDIT:

I think I've figured out the issue. There are two elements on the site with ID ""showNumbers"" which is curious site design and I'm still not sure the best way to work around this. Should I find all elements with this ID and access the second one, or just supply the full XPATH?","[""I'm not sure what the html looks like, but maybe something like button[id='showNumbers'] to differentiate from the other id.""]"
Need help,https://www.reddit.com/r/selenium/comments/y89aj0/need_help/,selenium,"I am doing a project where I want to keep trying to login until the password is correct. However when I sendkey to the password it only works once and when I try again I get an error that says there something wrong with my sendkey to that specific element.

while (link.equals(currentURL) ){

wait.until(ExpectedConditions.*elementToBeClickable*(By.*xpath*(""//\*\[@id=\\""ctl00\_CPHContainer\_txtPassword\\""\]"")));

String pass = String.*valueOf*(words.get(count));

System.*out*.println(pass);

password.sendKeys(pass);driver.findElement(By.*xpath*(""//\*\[@id=\\""ctl00\_CPHContainer\_btnLoginn\\""\]"")).click();Thread.*sleep*(3000);if(!link.equals(currentURL)){System.*out*.println(""This is the password that was found"" + pass);}count++;}

&#x200B;",['Nvm I found out that I have to directly use the element path cant store it in a variable.']
Need Help Trying to select Youtube password input,https://www.reddit.com/r/selenium/comments/y7x9r7/need_help_trying_to_select_youtube_password_input/,selenium,"Hi So, im trying to automate my login into youtube using Java. However when I am at the step to enter the password I am unable to Find the element by Class name, Xpath, Element name, input name etc. Can somebody help me I have no idea why Selenium refuses to allow me to select the password and sendKey(password). I would paste the code here but im havinig trouble pasting static html code.Because Most of the stuff is changing .  


  
Here is the link to login  
 [YouTube (google.com)](https://accounts.google.com/v3/signin/identifier?checkedDomains&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den%26next%3Dhttps%253A%252F%252Fwww.youtube.com%252F%253FthemeRefresh%253D1&dsh=S-838216074%3A1666168380688619&flowEntry=ServiceLogin&flowName=GlifWebSignIn&hl=en&ifkv=AQDHYWomsb0FTjE3dV8tSu_TswsHl8S6roC4pr_rK4XYyxecLpX3Yx5b7dLF9guxlFoVnqjvrfZh_Q&pstMsg=0&service=youtube) ","['I had the same problem. Google detects your bot and block it.', 'use v2 https://github.com/ultrafunkamsterdam/undetected-chromedriver', ""You might be able to specify a profile where you've already manually signed in, although this may not work for your project""]"
Help Locating Input Element,https://www.reddit.com/r/selenium/comments/y7l8sg/help_locating_input_element/,selenium,"I'm trying to locate and send keys to an an input element which is embedded in a div embedded in a label.  I've tried by ID, CLASS and various XPATHs, but with no luck.  Here is the HTML block in question, any help appreciated.  Thanks!

<label data-testid=""InputLoginValue-wrapper"" class=""input-text input-text-InputLoginValue"" for=""InputLoginValue"">

  <span data-testid=""InputLoginValue-title"" class=""field-name"">Username or Email Address</span>

  <div data-testid=""InputLoginValue-container"" class=""input-wrapper"">

<input class=""input-InputLoginValue"" type=""email"" autocomplete=""email"" autocorrect=""off"" spellcheck=""false"" tabindex=""0"" id=""InputLoginValue"" data-testid=""InputLoginValue"" placeholder=""Username or Email Address"" aria-invalid=""false"" aria-label=""Username or Email Address"" aria-required=""true"" required="""" value="""">

  </div>

  <div class=""error-container input-error input-error-email input-error-InputLoginValue"" style=""height: 0px;"">

<p class=""input-error input-error-email input-error-InputLoginValue"" id=""InputLoginValue-error"" data-testid=""InputLoginValue-error"" role=""alert""></p>

  </div>

</label>","['By.CssSelector(""input[type=\'email\']"")\n\nYou can use any of the attributes inside the input tag instead of type.\n\nBy.CssSelector(""input[placeholder=\'Username or Email Address\']"")']"
Need help!!!How to verify title length by using selenium TestNG?,https://www.reddit.com/r/selenium/comments/y7b076/need_helphow_to_verify_title_length_by_using/,selenium,"I am using Adobe Experience Manager. When I edit the title on the author side, title cannot be more than 2 lines on the publish side. If it is, it will be truncated. How to verify how many lines of title using selenium java?",['Count the characters in the title‚Äôs string?\n\nEdit: here‚Äôs an article that seems to cover what you need: https://linuxhint.com/count-characters-in-string-in-java/']
How can I download files using selenium from a dashboard on AWS. I am able to download files on my local system. But I want to deploy it to the aws. How can I do it.,https://www.reddit.com/r/selenium/comments/y75a7h/how_can_i_download_files_using_selenium_from_a/,selenium,same as question,"['Use an EC2 instance and make sure to generate a graphical interface session. If your needs make it possible, running it on a Lambda might be ok too.']"
Crawling site within shadowroot,https://www.reddit.com/r/selenium/comments/y708k1/crawling_site_within_shadowroot/,selenium,"Hello, I'm a new trying to crawling several sites with bs4 + python.

it worked well til I found a site containing #shadow-root (open)

after some search, I understood it is a self DOM which can't grap as usual.



site structure

 <div style=""display"">    
¬†¬† shadow-root (open)    
¬†¬†¬†¬†¬†¬†<div class=""1""></div>    
¬†¬†¬†¬†¬†¬†<section></section>    
¬†¬†¬†¬†¬†¬† <div class=""2"">
            <ul></ul>
            <ul></ul>
            <ul></ul>
            <ul></ul>
            <ul></ul>
            ...
¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† <ul></ul></div>

</div>   



I tried to use pypi 'pyshadow'

shadow.find\_elements(""div\[class='2'\]"")



but it extract only some ul tag, not the whole ul tag



So I tried other thing



def expand\_element(element)

shadowroot = driver.execute\_('''return argument\[0\].shadowRoot''', element)

return shadowroot



tag\_shad = driver.find\_elements\_by\_xpath('Ïó¨Í∏∞Ïóê div(class='1') XPATH')



And


shadow\_root = expand\_element(tag\_shad)

ul = shadow\_root.find\_elements(""div\[class='2'\]"")


But it gave me no element.


Can I get some help?",[]
Logging-in into my Google account using Selenium,https://www.reddit.com/r/selenium/comments/y5ibgl/loggingin_into_my_google_account_using_selenium/,selenium,"Hello, I am automating a daily task of mine and I'm required to log into my Google account in order to accomplish it. I tried using Python's Selenium module, but Google detects chromedriver and doesn't let me log-in. I was wondering if anyone has ever encountered this problem and if so how did you bypass it? Thank you guys in advance.","['[https://github.com/ultrafunkamsterdam/undetected-chromedriver](https://github.com/ultrafunkamsterdam/undetected-chromedriver) use v2', ""Google and a lot of places block bots from accessing their site. It's becoming more and more common. No real way around it."", 'See if the same task be accomplished by [App Script](https://developers.google.com/apps-script) or one of their [APIs](https://developers.google.com/apis-explorer). Otherwise, it is a cat-and-mouse game.', 'There is an option to allow access, turn it on, and get the password that every computer can use. I automate email sending and using a same thing.']"
Find elements by class name returns empty list,https://www.reddit.com/r/selenium/comments/y5dqv1/find_elements_by_class_name_returns_empty_list/,selenium,"Hi All,

I am fairly new to selenium, however have spent a fair amount of time trying alternate methods posted on various python forums in attempt to return a list of elements containing the class ""sports-event-entry"".  I have  tried multiple search types but all return an empty list.

Essentially I want to get the web address, then run:

data = driver.find\_elements(By.CLASS\_NAME,""sports-event-entry"")

I was wondering if anyone has had similar issues (in more recently times than some forums), and would be willing to share some trouble shooting techniques.

Cheers!","[""Can you post the page that you're using to test your code? I will try to give you a hand"", 'Is it a dropdown ?', '...if it\'s ""contains"" or ""starts-with"", use xpath:\n""//*[contains(@class, \'sports-event-entry\')]""\nBut be careful with it because it might return unwanted elements.']"
web app has Oauth authorisation,https://www.reddit.com/r/selenium/comments/y4mwt2/web_app_has_oauth_authorisation/,selenium,"Hey all

 I have an web app that uses OAuth 2 on Active Directory on a LAN.

And I want to access the app from my Linux based GRID which is not on the LAN or the active directory.

The dreaded windows auth dialog appears which selenium does not Interact with.

If I manually enter my credentials I get access to the app.

&#x200B;

Any help will help.

Thanks

&#x200B;

A.","['You can try adding the auth server to a profile, like below:\n\nFirefoxProfile profile = new FirefoxProfile(); profile.setPreference(""network.automatic-ntlm-auth.trusted-uris"", ""http://url.com"");', 'Sorry I missed one important fact it has to be chrome Browser  But thanks u/King-Of-Nynex']"
Helium/Selenium Cant Open New tab with hotkey,https://www.reddit.com/r/selenium/comments/y4p5hp/heliumselenium_cant_open_new_tab_with_hotkey/,selenium,"so i have tried Ctrl + T and also F6 Key (to take me to the address  bar), but it goes straight to the search bar on google page. Please Help  Thanks

**My Code:**

    from selenium.webdriver import FirefoxOptions
      
    from helium import*   
    
    import time  
    
    useragent =""Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:86.0) Gecko/20100101 Firefox/86.0"" 
    
    options = FirefoxOptions() 
    
    options.set_preference(""general.useragent.override"",useragent) 
    
    s = start_firefox(f""https://www.google.com"",headless = False, options=options) 
    
    press(CONTROL + ""T"")  
    time.sleep(5) 
    
    write(""https://www.reddit.com"")   
    time.sleep(5) 
     
    kill_browser()","['Did you try to use:  \n\n\ndriver.execute\\_script(""window.open(\'https://www.reddit.com\');"")']"
Re-Connecting to existing browser session - python,https://www.reddit.com/r/selenium/comments/y4ozzc/reconnecting_to_existing_browser_session_python/,selenium,"  
Hey guys, trying to use python and selenium to keep current browser session open and not require the need to relogin to website x every time i run the program. 

I had it working with this code yesterday, and i updated windows now it isnt working. I am receiving error: 'cannot connect to host, chrome unreachable.'. Ive tried several different ports and none seem to work, although when i remove the 'options' argument from the driver declaration and use only service,  i am able to successfully open a new browser. Leading me to believe my issue lies within line 5 of my code.  

&#x200B;

Thanks for taking a peek! 

&#x200B;

`path = r""C:\Users\xxxxxxxxxx\chromedriver_win32\chromedriver.exe""`  
`service = Service(executable_path=path)`  
`web = 'https://xxxxxx/com'`  


`options = Options()`  
`options.add_experimental_option(""debuggerAddress"", ""localhost:9222"")`  
`driver = webdriver.Chrome(service=service, options=options)`  
`driver.get(web)`","['Take a look to the [DesiredCapabilities](https://stackoverflow.com/questions/66892502/chromedriver-desired-capabilities-has-been-deprecated-please-pass-in-an-options) if you need future help let me know', '[removed]']"
Selenium Timeout - Expected Condition returning False,https://www.reddit.com/r/selenium/comments/y47p05/selenium_timeout_expected_condition_returning/,selenium,"I'm coding a bot in Python that plays tic-tac-toe. The game is a Web  app written in React.js and is equipped with an AI of its own that  utilizes minimax. The user (which the Python bot simulates) is always X,  the AI is always O, and the user always moves first. The Python bot  plays by randomly selecting unmarked squares (this is only to  demonstrate automated UI testing).

**I was getting stuck inside a recursive function.**

    for i in clickedSquares:             
         if not winner:                 
              self.checkForWinner(load_browser, winner)                       
         elif i == random_square:                 
              self.test_playTTT(load_browser)             
         else:                 
              clickedSquares.append(random_square)

To fix this issue I added the **if not winner** condition  where ""winner"" is a string. This does terminate the loop; however, I'm  getting an error as soon as the checkForWinner() function is called  because winnerOh is always false. 

    winnerOh = WebDriverWait(load_browser, 10).until(EC.presence_of_element_located((By.XPATH, Tags.resultOh)))         
    
    winnerEx = WebDriverWait(load_browser, 10).until(EC.presence_of_element_located((By.XPATH, Tags.resultEx)))         
    
    tiedGame = WebDriverWait(load_browser, 10).until(EC.presence_of_element_located((By.XPATH, Tags.resultTie)))

 I'm looking for an element on the UI that declares the winner: X or O or  tie, which will only appear if the game is over.  So  WebDriverWait().until() is timing out waiting for that element to  appear, but it hasn't yet, because it's only the second move in the  game. 

I'm not sure how to fix this issue. If I remove the call to  checkForWinner() the bot will get stuck in the recursive call to  test\_playTTT(). The browser will not close after the game is over, and  the test will not end successfully.

**Is there another way to check the UI for the element I'm looking for that won't immediately return a False condition?  Is there a  better way for me to write this for loop?**

Linked is my post on StackOverflow with a full version of my Python method:

[https://stackoverflow.com/questions/74075172/selenium-timeout-expected-condition-returning-false](https://stackoverflow.com/questions/74075172/selenium-timeout-expected-condition-returning-false)

I'd appreciate any help.","['You can literallity add more time to the WebDriverWait or try to use a try/exeption', ""So the problem I was having had to do with how the presence\\_of\\_element\\_located() function works.  It was waiting after each click for the winner to be visible on the UI, but there's no way to know who the winner is or when it will win in a random game of tic-tac-toe, so it was timing out.""]"
What BDD/Cucamber type of language can be offered to business to create stories for UI automation without making SDETs life more difficult?,https://www.reddit.com/r/selenium/comments/y437oc/what_bddcucamber_type_of_language_can_be_offered/,selenium,"I know that many SDETs would rather deal just with Selenium for UI tests and not with Selenium+Cucamber because maintaining both is a pain in the butt. If the business still wants some kind of BDD language to create story requirements in, what can you offer them without making your life as an SDET more difficult?  If I want to keep it simple and not deal with Cucamber, would it be a good idea to have the business create requirements in Gherkin and just keep them in Jira for references without actually running any Cucamber on my side?","[""Imo BDD is starting to lose steam as more companies start to see how it plays out. It's simply not effective.""]"
"Selenium, second monitor usage rather than first",https://www.reddit.com/r/selenium/comments/y40xe4/selenium_second_monitor_usage_rather_than_first/,selenium,Can I make selenium utilize my second monitor rather than my first one? I am new to selenium.,"[""Google 'selenium set window position multiple displays'.\n\nDepending on which side the second monitor is on, you need to set your values so it moves left or right. You can also use negative values.\n\nWhen you have figured out that part, use the maximize window option to go full screen."", 'If you are moving it to the second just to get it out of the way - you can also run it headlessly \n\n&#x200B;\n\nJust an fyi']"
How do I hold down a click for roughly two seconds.,https://www.reddit.com/r/selenium/comments/y40woz/how_do_i_hold_down_a_click_for_roughly_two_seconds/,selenium,"I'm trying to add text to an image using an online photo editor. The problem arises when I click the text box button, I need to click and hold the mouse down (roughly two seconds long) and then release and this will let me enter text. How can I achieve this in selenium?","[""Have you googled it? \n\nPlenty of examples if you look.\n\nYou need to use Actions. There's a click and hold option as well as a release option.\n\nAs a 2 second sleep in between each action.""]"
IsDisplayed() method and driver not recognized by intellij java,https://www.reddit.com/r/selenium/comments/y40aq7/isdisplayed_method_and_driver_not_recognized_by/,selenium,"When i try to assert.fail(elementlocator.isDisplayed()) i don't get the IsDisplayed method, its red and is unrecognised. I have selenium , webdriver and testng dependencies in mu pom.xml",['Can you post your code?']
find_element_by_link_text() Problem,https://www.reddit.com/r/selenium/comments/y34uhg/find_element_by_link_text_problem/,selenium,"I'm experiencing the same problem: 

Whenever I try searching by link text this pops up:

&#x200B;

link = driver.find\_element\_by\_link\_text((""YouTube""))

AttributeError: 'WebDriver' object has no attribute 'find\_element\_by\_link\_text'

&#x200B;

Here is the code:

&#x200B;

 

import selenium  
from selenium import webdriver  
from selenium.webdriver.common.keys import Keys  
import time  
PATH = ""C:\\Windows\\chromedriver.exe""  
driver = webdriver.Chrome(PATH)  
driver.get(""https://www.youtube.com/channel/UCaSwSHJaodwjYUKR1KNJFtg"")  
time.sleep(2)  
link = driver.find\_element\_by\_link\_text((""YouTube""))  
[link.click](https://link.click)()  


If anyone can help me with this it would be great. Thanks!","['I advice you that youtube is not a good site to automations, you can search for other website to train your webdriver skills', 'Selenium recently deprecated the find_element methods. The most recent documentation on locating elements: https://www.selenium.dev/documentation/webdriver/elements/finders/', 'link = driver.find\\_element(By.LINK\\_TEXT, ""YouTube"")', 'u/lxv-crunchy  here is the code so you can practice:  \nhttps://pastebin.com/HACmdPng']"
Can't scrape price from website,https://www.reddit.com/r/selenium/comments/y31hkx/cant_scrape_price_from_website/,selenium,"I'm struggling with my python script to print me the current price of certain items on a website.
I've tried so many different solutions I could find on Google but none of them is working.

This is how it looks on the website:

<span class=""h4 m-product-price"" >399,00 DKK</span>

I want my script to print 399,00 DKK

Are any of you guys able to help?","['Are you getting an error of some sort? Which solutions have you tried and what happened? \n\nIt‚Äôs hard to help without knowing what‚Äôs been tried.', 'Set.‚Äùh4 m-product-price‚Äùas.variable then print variable  or write all to a list then print look up python basics', 'Post the site', 'this might help: https://blog.christian-schou.dk/how-to-web-scrape-prices-using-python/amp/\n\nchange use of id to class\n\nnot sure how this would work when multiple prices occur on a page', ""Can you post the page that you're using to test your code? I will try to give you a hand""]"
Testing Tool,https://www.reddit.com/r/selenium/comments/y2kjwb/testing_tool/,selenium," Hi All!

I wanted to inform the community of a service that makes it easier for automation test cases that involve email and sms validation such as MFA (multi factor authentication) They offer email and sms API‚Äôs that make it a breeze. They have free for ever accounts that are limited to emails only. Check them out at [swiftpigeon.io](http://swiftpigeon.io/)",[]
Suggest a Reporting tool,https://www.reddit.com/r/selenium/comments/y1uaw0/suggest_a_reporting_tool/,selenium,"I need a reporting tool that has the following feature:
- Record Parallel tests (using ThreadLocal)
- Emailable report*
- Run History**

* - like ExtentReport where I can send it to stakeholders in a zip file and they can open the html file in their browser.
** - Something that would show me the number of tests run, pass count, and fail count of each test run.",['Reportportal is a good one!']
image - save as dialog,https://www.reddit.com/r/selenium/comments/y1ruip/image_save_as_dialog/,selenium,"Is it possible to: 

1. initiate a right-click on an image
2. select the save as dialog
3. set a flemme
4. save it

?

Does it make open up more possibilities by using the JavaScript API to run the web drivers?

Have tried using Python to run web drivers but image urls have auth info and likely there will be a lot of headers to set up to use a non-browser GET using curl or something.",[]
How to stop while loop after scrolling,https://www.reddit.com/r/selenium/comments/y1epsy/how_to_stop_while_loop_after_scrolling/,selenium,"Hello, I have a problem to stop while loop after scrolling. Maybe this is not the appropriate method to scrap this site, I am not sure.  I want to scrap all ads and after that the program to stop. In my way I need to stop the program manually. 

Can somebody help me to stop the program when there aren't any ads?

The site is this - [https://www.jobs.bg/en/front\_job\_search.php](https://www.jobs.bg/en/front_job_search.php)

This is the code - [https://pastebin.com/udL5VwjM](https://pastebin.com/udL5VwjM)

Thanks in advance!",['You gonna need to explain your problem a little better man']
Security risks,https://www.reddit.com/r/selenium/comments/y14xf1/security_risks/,selenium,"Hi all! I'm looking to optimize some repetitive tasks on my workplace. We do most our work through chrome so i thought Selenium would be able to speed things up. However, are there any potential security risks I'm unaware of? Could chromedriver save any information without my consent?","[""It won't save anymore than you using chrome. You can run in incognito mode also\nhttps://www.tutorialspoint.com/running-chrome-browser-in-inconginto-mode-in-selenium#""]"
OOM chrome error,https://www.reddit.com/r/selenium/comments/y1763z/oom_chrome_error/,selenium,"Hi,

We are getting chrome OOM error while running selenium automation script through Jenkins.

And we tried replacing VMs as well but same OOM error there as well, even we updated the chrome version in VM but no luck.

Can anyone please help me out.

Thanks",['What happens when you try to run it locally?']
Has anyone gotten webdriver-auto-update package to work? It seems like no matter what I do it can't find my chromedriver.exe.,https://www.reddit.com/r/selenium/comments/y19c3z/has_anyone_gotten_webdriverautoupdate_package_to/,selenium,"[`https://pypi.org/project/webdriver-auto-update/`](https://pypi.org/project/webdriver-auto-update/)

Code in this package where it is messing up:

        try:
            # Executes cmd line entry to check for existing web-driver version locally
            os.chdir(driver_directory)
            cmd_run = subprocess.run(""chromedriver --version"",
                                     capture_output=True,
                                     text=True)     
        except FileNotFoundError:
            os.chdir("".."")
            # Handling case if chromedriver not found in path
            print(""No chromedriver executable found in specified path\n"")
            download_latest_version(online_driver_version, driver_directory)

Every time it goes into the except because it can't find the chromedriver.exe... 

&#x200B;

Any suggestions? Could someone show an example of this code working?","['This submission has been removed because it looks suspicious to automod (c). If this was done in error, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fselenium&subject=about my removed submission&message=I‚Äôm writing to you about my submission that was removed (l). %0D%0DMy issue is...).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/selenium) if you have any questions or concerns.*']"
I want to deploy selenium java on Netlify,https://www.reddit.com/r/selenium/comments/y0y53x/i_want_to_deploy_selenium_java_on_netlify/,selenium,"hey guys, i'm a little bit lost here. I want to deploy my selenium bot in netlify, so first of all, there's no tutorial, the closer i got was a tutorial on how to deploy in heroku, that itself isn't the issue cuz i manage to find the equivalent options in netlify, the issue is that the code part is written in python in the 3 sources i found, my small dumb monkey brain can't understand python, only poo languages (quite literally, only c# and java) and it seems i'm missing something in the docs, so, how could i translate this code or what would be the equivalency to java/c#? or at least, where's the specific documentation?

    import os
    from selenium import webdriver
    
    op= webdriver.ChromeOptions()
    op.binary_location = os.environ.get(""GOOGLE_CHROME_BIN"")
    op.add_argument(""--headless"")
    op.add_argument(""--no-sandbox"")
    op.add_argument(""--disable-dev-sh-usage"")
    
    driver = webdriver.Chrome(executable_path= os.environ.get(""CHROMDRIVER_PATH""),chrome_options=op)
    

&#x200B;","[""Netlify gives you their preset containers, JAM stack. I don't think you'll be able to deploy. You'd have to install chrome on the container... seleniun hq documentation recommends two containers, one for your solution and another for the selenium server/grid.  Heroku is probably a better option since you can containerized it yourself.\n\nGitbhub actions is also an easy way to do this..."", 'Here is a detailed 2022 comparison of Netlify and other most popular PaaS alternatives for deploying your apps as well as some details on features of these platforms: [10 Heroku Alternatives To Host Your App On The Cloud](https://autoidle.com/blog/heroku-alternatives)']"
xpath help,https://www.reddit.com/r/selenium/comments/y0j3b7/xpath_help/,selenium,"I know this is possible but can't wrap my head around the how. I have a table on a page where each row of the table is coded as an individual table. I need to click on the OPEN button on a specific row. But, the text in the cell I am looking for, I need to click the button in the previous cell. Anyone available to help here? Is there a good cheat sheet out there with problems and examples like this?

So ABC123 is what I need to look for, then I need to click the button located in the cell before that.

Thanks

    <div class=""dojoxGridRow dojoxGridRowOdd dojoxGridRowSelected"" role=""row"" aria-selected=""true"" style="""">
    <table class=""dojoxGridRowTable"" border=""0"" cellspacing=""0"" cellpadding=""0"" role=""presentation"" style=""width: 1128px; height: 30px;""><tbody>
    <tr><td tabindex=""-1"" role=""gridcell"" class=""dojoxGridCell nosort GridButton"" idx=""0"" style=""text-align: left;width:9%;""><div class=""grid-text-over""><input type=""button"" value=""Open"" class=""base-btn small green""></div></td>
    <td tabindex=""-1"" role=""gridcell"" class=""dojoxGridCell"" idx=""1"" style=""text-align: left;width:13%;""><div class=""grid-text-over"">ABC123</div></td> 

 Tried various combinations of this, but still not quite getting it.

    //input[@value='Open']//preceding::td[contains(text(),'ABC123'] 

Essentially I want to scan the entire page and look for a button that is followed by a cell that contains the text ""ABC123"".   I'm trying to click on that button.

Thanks for any pointers.","[""You're code is the wrong way about.\n\nYou're finding the button then traversing 'up' to look for ABC123. I'm assuming the button is left of or above the ABC123 text?\n\nYou need to find the text first then do preceding::td//input[@value='Open' to move back to the button. (You don't actually need to define the type of element after td, it will just click the middle of the element if it finds out.)\n\nAn image of the table layout would be better if you can."", ""I'm terrible at HTML. What I did use was using Selenium IDE when I didn't know how to click something, and checking which element it selected."", 'Would you mind sharing the URL?', 'You can try with javascript through execute script']"
Webpage immediately closing.,https://www.reddit.com/r/selenium/comments/y08dhy/webpage_immediately_closing/,selenium,"When I try to open a webpage, it immediately closes.

Here is my code.

from selenium import webdriver  
import time  


driver = webdriver.Chrome(executable\_path=""C:\\Drivers\\chromedriver.exe"")  
driver.get(""https://chromedriver.storage.googleapis.com/index.html?path=106.0.5249.61/"")  


time.sleep(99999)

&#x200B;

I tried other things to fix it thinking it was just because the code ended, but adding the sleep at the end didn't fix it. Here is what was relayed back to me from Pycharm

""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\Scripts\\python.exe"" ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\[main.py](https://main.py)"" 

C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\[main.py:4](https://main.py:4): DeprecationWarning: executable\_path has been deprecated, please pass in a Service object

  driver = [webdriver.Chrome](https://webdriver.Chrome)(executable\_path=""C:\\Drivers\\chromedriver.exe"")

Traceback (most recent call last):

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\[main.py](https://main.py)"", line 4, in <module>

driver = [webdriver.Chrome](https://webdriver.Chrome)(executable\_path=""C:\\Drivers\\chromedriver.exe"")

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\chrome\\[webdriver.py](https://webdriver.py)"", line 69, in \_\_init\_\_

super().\_\_init\_\_([DesiredCapabilities.CHROME](https://DesiredCapabilities.CHROME)\['browserName'\], ""goog"",

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\chromium\\[webdriver.py](https://webdriver.py)"", line 92, in \_\_init\_\_

super().\_\_init\_\_(

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\[webdriver.py](https://webdriver.py)"", line 272, in \_\_init\_\_

self.start\_session(capabilities, browser\_profile)

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\[webdriver.py](https://webdriver.py)"", line 364, in start\_session

response = self.execute(Command.NEW\_SESSION, parameters)

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\[webdriver.py](https://webdriver.py)"", line 429, in execute

self.error\_handler.check\_response(response)

  File ""C:\\Users\\micha\\PycharmProjects\\Whatnot Follow\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\[errorhandler.py](https://errorhandler.py)"", line 243, in check\_response

raise exception\_class(message, screen, stacktrace)

selenium.common.exceptions.SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version 106

Current browser version is 105.0.5195.128 with binary path C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe

Stacktrace:

Backtrace:

	Ordinal0 \[0x00A11ED3+2236115\]

	Ordinal0 \[0x009A92F1+1807089\]

	Ordinal0 \[0x008B66FD+812797\]

	Ordinal0 \[0x008D8C6D+953453\]

	Ordinal0 \[0x008D4200+934400\]

	Ordinal0 \[0x008D19C9+924105\]

	Ordinal0 \[0x0090806C+1146988\]

	Ordinal0 \[0x00907A6A+1145450\]

	Ordinal0 \[0x009018A6+1120422\]

	Ordinal0 \[0x008DA73D+960317\]

	Ordinal0 \[0x008DB71F+964383\]

	GetHandleVerifier \[0x00CBE7E2+2743074\]

	GetHandleVerifier \[0x00CB08D4+2685972\]

	GetHandleVerifier \[0x00AA2BAA+532202\]

	GetHandleVerifier \[0x00AA1990+527568\]

	Ordinal0 \[0x009B080C+1837068\]

	Ordinal0 \[0x009B4CD8+1854680\]

	Ordinal0 \[0x009B4DC5+1854917\]

	Ordinal0 \[0x009BED64+1895780\]

	BaseThreadInitThunk \[0x76906739+25\]

	RtlGetFullPathName\_UEx \[0x77908FD2+1218\]

	RtlGetFullPathName\_UEx \[0x77908F9D+1165\]

&#x200B;

&#x200B;

Process finished with exit code 1

&#x200B;

&#x200B;

&#x200B;

Thanks for any help you can provide.","[""Your chromedriver version doesn't match your browser version. Look into WebDriverManager if you don't want to manage that as much.""]"
How to bypass reCAPTCHA v2 ?,https://www.reddit.com/r/selenium/comments/y08bgd/how_to_bypass_recaptcha_v2/,selenium,I'm doing a crawl with selenium but have a problem with click i'm not robot . Can someone help me?,"['There are services that provide captchabypass. But you will need to know how to inject the captcha tokens into the JavaScript.', 'You may check https://captchas.io for a very low price CAPTCHA solving alternative and option. Prices starts at $22 only with 5 CAPTCHA threads and 5,000 daily solves limit.']"
How do I extract data from a dynamic table embedded in a webpage?,https://www.reddit.com/r/selenium/comments/xz3sfh/how_do_i_extract_data_from_a_dynamic_table/,selenium,"I'm trying to extract extract all the salary information from the table on the following URL: [https://www.fedsdatacenter.com/federal-pay-rates/](https://www.fedsdatacenter.com/federal-pay-rates/).

I'm not too familiar with Selenium or programming, so I apologize if I am using incorrect terminology. But I couldn't find any guidance on how to do this. If you could help me out, I would greatly appreciate it.","[""Katalon's help describes this a little better than I can, but basically:\n\nLook at how the html for a table is typically formatted.  You'll notice tr elements full of td elements full of data of interest.\n\nYou should be able to target this by XPath.  If there's only one table on the page it's easy:\n\n    //tr[x]/td[y]\n\nThis will target the cell at row x, column y.  From there it's just extracting the text itself, which unfortunately I can't help you with on mobile."", 'This could help: https://www.guru99.com/handling-dynamic-selenium-webdriver.html', 'u/oneironautkiwi here a possible code solutions: https://pastebin.com/z6EPLqXH']"
Chromedriver and Google Chrome dependency,https://www.reddit.com/r/selenium/comments/xyy2hh/chromedriver_and_google_chrome_dependency/,selenium,"Hi all,  


I was wondering if you guys ever had issues with `chromedriver` and google chrome not using the same version after your google chrome updates. Suddently your .py script won't be able to scrap data anymore on your remote server. I was wondering how you would prevent your script from failing to run. Maybe docker can help, but I was never able to make it work.  


Thanks for reading!","[""Docker could help you manage this, depends a little bit on your use case (and how big your company is with the new licensing model unless you want to learn how to use podman or something similar but I digress.)\n\nSounds like you're just doing single-instance scraping jobs so I might instead point you to webdriver manager: https://pypi.org/project/webdriver-manager/"", 'https://www.webnots.com/7-ways-to-disable-automatic-chrome-update-in-windows-and-mac/#:~:text=Double%20click%20on%20%E2%80%9CServices%E2%80%9D%20to,Disabled%E2%80%9D%20option%20then%20click%20ok.', ""Yes I used to do that but I don't necessarily want chrome to stop updating, I was looking into an isolated way to bundle chromedriver and chrome""]"
"Multiple svg's of same class, want to select specific one.",https://www.reddit.com/r/selenium/comments/xychlg/multiple_svgs_of_same_class_want_to_select/,selenium,"I have the xpath to an svg object I want to click on: 

    XPath = ""//*[name()='svg' and u/class='r-4qtqp9 r-yyyyoo r-1xvli5t r-dnmrzs r-bnwqim r-1plcrui r-lrvibr r-1hdv0qi']""

The website is very nested, I was only able to find this object using the class code. However, there are multiple instances of this object, as found using: 

    s = np.random.normal(mu, sigma, 8)
    driver.get(report_url)
    
    cards = WebDriverWait(driver, s[2]/50).until(EC.visibility_of_all_elements_located((By.XPATH, Report_Circles)))
    Report_Circles_click=wait.until(EC.element_to_be_clickable(cards[-1]))
    
    time.sleep(s[1]/100)
    ActionChains(driver).move_to_element(Report_Circles_click).click().perform()

However, clicking on the last instance of this svg object isn't actually correct, I want to click on the specific svg circle that precedes an a href: 

    <a href=""/badcatstuff/status/1577447899591544833""

wondering if I could find that specific href first, then click on the svg object preceding that? 

if so, how would the xpath look like?","[""depends on html structure between the a and svg. you'll want something like preceding sibling or parent. \n\nhttps://www.guru99.com/using-contains-sbiling-ancestor-to-find-element-in-selenium.html""]"
How can I find_elements using multiple By's in python?,https://www.reddit.com/r/selenium/comments/xxurpi/how_can_i_find_elements_using_multiple_bys_in/,selenium,"ByChained doesn't work. Googling doesn't helps. I'm frustrated. I can try to make my code work by using bad practises (like indexing), but I don't want to do it",['Can you share some more details?']
Selenium doesn't continue in for loop?,https://www.reddit.com/r/selenium/comments/xxxx7j/selenium_doesnt_continue_in_for_loop/,selenium,"Hello,

My code is working fine only with the first category, but doesn't loop through the others categories. I can't figure out what happ–µning and why Selenium stops in the loop. The separate code is working fine, but in this for loop ( for i in range(16, 50): ... line 49) the program stops the execution after the first category.  Please, help me to solve this!

The code - [https://pastebin.com/WN4NwRRx](https://pastebin.com/WN4NwRRx)  with Chrome driver installation.

Thanks in advance!","[""How does it stop, does it not find the next element? Its a bit hard to follow on the phone but after you click in your for loop i didn't see where you go back to the page to continue with the next element.""]"
What happened to the selenium documentation?,https://www.reddit.com/r/selenium/comments/xxugrq/what_happened_to_the_selenium_documentation/,selenium,"I tried to access a few bookmarked pages that had some information on selenium class methods for the chrome webdriver and the links are all broken. Where can I find all the documentation for the selenium project? This link ([https://www.selenium.dev/selenium/docs/api/py/index.html](https://www.selenium.dev/selenium/docs/api/py/index.html)) is garbaggio, it doesn't have any of the useful information. Just try typing ""find\_element"" and it nothing will turn up. This is a bit concerning as I can't seem to access the pages that I used before in generating a selenium project and I cannot find any information on the specific API methods. Any help would be greatly appreciated. Thanks",['Have you tried pressing the API link there? It seems to work for me.']
Change default download directory in Python?,https://www.reddit.com/r/selenium/comments/xxj8x8/change_default_download_directory_in_python/,selenium,"Hi guys, I am an amateur programmer using Python, Selenium, and ChromeDriver. I am coding in PyCharm. My issue is that I can't seem to successfully change my default download directory. Please see the following code below, which hasn't worked for me to change the directory:

    from selenium import webdriver
    
    chromeOptions = webdriver.ChromeOptions()
    prefs = {""download.default_directory"" : ""C:/Users/popularweb6231/python_work/""}
    chromeOptions.add_experimental_option(""prefs"", prefs)
    chromedriver = ""C:/Users/popularweb6231/chromedriver.exe""
    driver = webdriver.Chrome(executable_path=chromedriver, options=chromeOptions)

Instead, it's just using the Chrome default ('\_user'/downloads) folder as the default folder. Am I doing something wrong? Please help :(","['`prefs = {""download.default_directory"" :  r""C:\\Users\\popularweb6231\\python_work\\\\""}`\n\ntry with this']"
Locator Strategy for Shadow Elements,https://www.reddit.com/r/selenium/comments/xxgu96/locator_strategy_for_shadow_elements/,selenium,"Hi all,

I have a scenario where a shadow element has different HTML tags on each environment. I am trying to use a single locator to locate the element in each environment, but it seems undoable.

The scenario is like the following:

The element can be located in lower environments like this üëá

`document.querySelector('validSelector1').shadowRoot.querySelector('validSelector2').shadowRoot.querySelector('validSelector3');`

The element can be located in the prod environment like this üëá

`document.querySelector('validSelector1').shadowRoot.querySelector('notValidSelector2').shadowRoot.querySelector('validSelector3');`

So the second selector will not be valid in prod. Is there a way to create a method to ignore the shadow host that is not present in the HTML and move to the next present shadow host? I want to avoid creating duplicate locators for the same element if it is doable. I really appreciate any help you can provide.",[]
proxy alternative?,https://www.reddit.com/r/selenium/comments/xwn6uf/proxy_alternative/,selenium,"I'm trying to run an automation using selenium the problem is that the offers on the website are geo restricted, I wanted to go the proxy route but most of the free ones has problems, I was thinking VPN but as far as I know chrome doesn't have that option, I'm running on a VPS and making the whole system use a vpn is another mess, any suggestions?","['Get a cheap cloud instance with digital ocean and setup a grid in whatever part of the world you want?', 'Did you try the geo location mocking option ? Does it help with your offer testing based on location?', ""aws may be a solution in your case, i think it's free for the 1st year!""]"
--headless always enables javascript. Why?,https://www.reddit.com/r/selenium/comments/xwgv4n/headless_always_enables_javascript_why/,selenium,"Hello experts,

I need to test a website with javascript enabled *and* disabled. Everything works fine in headful mode with Chrome, however, once I switch to headless Javascript seemingly cannot be disabled. Here's a self-contained python MWE:

```
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_experimental_option(""prefs"", {""profile.managed_default_content_settings.javascript"": 2})
# chrome_options.add_argument(""--headless"")
driver = webdriver.Chrome(options=chrome_options)
driver.get(""https://www.whatismybrowser.com/detect/is-javascript-enabled"")
driver.save_screenshot(""screenshot.png"")
driver.quit()
```

If run as is, the screenshot will tell that Javascript is disabled, however, if headless mode is switched on (uncomment the commented line) Javascript will suddenly be enabled, contrary to the configured settings.

Any idea why that is and how to fix it? I'm using Chrome/Chromedriver v106.0.5249.61.

Thank you!","['After some digging I came across one stackoverflow post asking about a similar thing and it [looks like](https://stackoverflow.com/a/47538023) it might not be possible with ""--headless"".\n\nHowever on the same post [there was a comment](https://stackoverflow.com/a/73961248) that suggested using ""--headless=chrome"" which might be a solution to your issue. Try it out and see if that works for you.', 'My guess would be that headless chrome uses JavaScript to interact with the DOM.. If its headless, it might require it to be.']"
How to handle navigation Methods in Selenium Webdriver?,https://www.reddit.com/r/selenium/comments/xw2cfa/how_to_handle_navigation_methods_in_selenium/,selenium," <!--td {border: 1px solid #cccccc;}br {mso-data-placement:same-cell;}-->If you want to know how to handle navigation methods in selenium webdriver and what are the methods in it?  


Selenium WebDriver has provided different navigation methods, those are  


1. navigate().back()  
2. navigate().forward();  
3. navigate().refresh();  
4. navigate().to(url);  


with the help of navigation methods in selenium webdriver we can achive below things  


How to Navigate Back & Forward In Selenium WebDriver?  
How to Refresh Page In Selenium WebDriver?  
How To Load URL In Selenium WebDriver?  


If you want to know practicale code & implimentation click beliw link :  


https://www.techsravi.com/how-to-handle-navigation-methods-in-selenium-webdriver/  


\#techsravi #navigation #methods #handlingnavigationmethods #selenium #softwaretesting #seleniumwebdriver #HappyDussehra #Vijayadashami",[]
Hello I am trying to locate this element with the condition.,https://www.reddit.com/r/selenium/comments/xvupko/hello_i_am_trying_to_locate_this_element_with_the/,selenium,"Hi guys, I am new to Selenium. In my browser, when I am trying to locate and click the ""Missing"" button with the condition is the MPC need to match.  Sometime, when I search the items, it return multiple ""Missing"" button. I just want to click the missing which have the corrected MPC. Sometimes, it is first option. Sometimes, it is second or third options. I tried normalize-space. However, I cannot find a way to make it only choose the missing that is coordinate with the right MPC. Is it any way I can do it? Below is the link to the code.

browser.find\_elements(By.XPATH,""//div\[@class='x-grid-group-body'\]\[contains(text(),'10266')\] and \[contains(text(),'Missing')\]"")

[https://imgur.com/a/qu1oDAk](https://imgur.com/a/qu1oDAk)","['Not entirely sure of your ask, but a loop like this could work.\n\nList<WebElement> buttons = your identifier;\n\nfor(int i = 0; i < buttons.size; i++){\n if(button.get(i).getText().equals(""missing""){\n  button.get(i).click();\n}};', 'So you have a datatable and -- if I understood your question correct -- you want to find the cell with the href with text ""Missing"" that is on the same row as the text with the text ""10266""?\n\nWith your current code that\'s not likely to happen since if I understand the DOM you\'ve linked in the image correct these both are on separate <td> elements and your query expects the text to contain both ""Missing"" and the MPC text.\n\nInstead you could, for example, search for the parent element of the cell with the MPC text which should return the <tr> element, and then search from within that element for the ""Missing"" text, which should return the correct element. Then you can either just directly click that or if you need to store the values somewhere first or something, you can just store them in a dictionary for example with the MPC text as key and the actual href element as the value and then iterate from those later.\n\nI\'m not entirely sure what language you are writing your code in so I didn\'t write any code examples, but this should be pretty easy to figure out.\n\nLet me know if that helped!']"
Selenium 4 CDP Integration With Capybara,https://www.reddit.com/r/selenium/comments/xvip12/selenium_4_cdp_integration_with_capybara/,selenium,"Published blog on using Chrome DevTools protocol with Selenium and Capybara. To know more read the blog

[Selenium 4 CDP Integration with Capybara](https://blog.kiprosh.com/selenium-4-cdp-integration-with-capybara/)",[]
button.click() goes back and forth.,https://www.reddit.com/r/selenium/comments/xvdt9t/buttonclick_goes_back_and_forth/,selenium,"So I'm trying to click through a series of images in an album viewer by using [Button.click](https://Button.click)(), see code below.

    for (int i = 0; i < 10; i++){
                WebElement nextBtn = driver.findElement(By.xpath(""/html/body/div[1]/div/div/div/div[2]/div/div/div[1]/div/div[3]/div/div/div/div/div[1]/div/div/div/button""));
                Thread.sleep(1000);
                nextBtn.click();

The first click works fine, it goes forward the next image but on the next loop it goes back to the first image. This process repeats, first image - second image - first image until the loop completes i >= 10.

The position of the button change, the xpath for the next button is the same for both first and second.


EDIT: If I put the nextBtn = driver.findElement outside of the for loop it works, nextBtn clicks all the way from image 1 - 10. 

However when I try repeat the process later on in the code the for loop goes backwards from image 10-1, where it should keep going from image 10-20.","[""I'm not a big fan of the forced sleep, but you may need to add another one after the click. The DOM may not show the new button yet. At least some sort of synchronization could be necessary."", 'I am a newbie but when you click the first image, does the xpath change?', 'Your XPath could be so much more simple and less apt to break.\n\nFor example: //button instead of all that junk.\n\nThen, use an attribute like ""//button[text() = \'next\']"" if button contains text like that. If not, there\'s certainly some other tag that\'s unique to it vs. the previous button.\n\nUse the inspector in your browser to find unique tags for that element.']"
"What is Selenium used for, in simple words",https://www.reddit.com/r/selenium/comments/xumus2/what_is_selenium_used_for_in_simple_words/,selenium," 

Selenium is a free (open source) automated testing tool for validating web applications across a variety of browsers and platforms. You can use multiple programming languages like Java, C#, Python, etc to create Selenium Test Scripts.

The Selenium test suite comprises four tools:

1. Selenium Integrated Development Environment (IDE)
2. Selenium Remote Control (RC)
3. Selenium WebDriver
4. Selenium Grid

Selenium Tools is a suite of software, each piece catering to a different organization's Selenium QA testing needs.","['I like what you\'re trying to do here, but I would say ""testing"" is only one aspect of Selenium\'s use. People use Selenium for a variety of purposes, only one of which is to validate web products.', 'Navigate websites using programming', 'Automating repetitive software testing.\nAutomating repetitive tasks.', 'I use it to automate pentests with ZAP.', 'Browser Automation']"
How to import my Amazon cookie credentials into selenium?,https://www.reddit.com/r/selenium/comments/xtq1ce/how_to_import_my_amazon_cookie_credentials_into/,selenium,"How do I import Amazon login cookie credentials and implement into my code? I'm using selenium with chrome web driver and every time a new chrome windows open, the script has to type in the login and password. I would like to have my account already logged in when the new window opens.",['https://stackoverflow.com/questions/31062789/how-to-load-default-profile-in-chrome-using-python-selenium-webdriver']
can Selenium be used for enterprise freely?,https://www.reddit.com/r/selenium/comments/xtq8x7/can_selenium_be_used_for_enterprise_freely/,selenium,,"['Selenium is open source and free. Can be used anywhere', 'Yes']"
Selenium WebDriver Interview Questions And Answers,https://www.reddit.com/r/selenium/comments/xtlo7h/selenium_webdriver_interview_questions_and_answers/,selenium,,"['I would say brush up on the language they program in.', 'link is in profile']"
Selenium IDE send keys command,https://www.reddit.com/r/selenium/comments/xqi6oh/selenium_ide_send_keys_command/,selenium,"Quick preface, this is in IDE. I do not plan on scripting but if someone can help me figure this out by using native IDE commands that would be ideal.

&#x200B;

I'm stuck trying to find a way to select the contents of a text field and delete said content in an automated fashion. I tried having the script simply type nothing into the text field but clicking update doesn't actually retain the empty text field so I need to have the script erase the contents.

My goal is to have a ***send keys*** command that will send CTRL+A which will select the contents of the text field and then send backspace after to clear the text. Unfortunately I don't have much experience with coding in general and even less with java so I have no idea how I would word it in the value field.

For example, I've tried  ${KEY\_CONTROL}+${KEY\_""A""},  ${KEY\_CONTROL+""A""},  ${KEY\_CONTROL}+""A"", but all of these either don't do anything, pastes the entire command value / partially, or they add an A to the text.

&#x200B;

Any help is welcome.","[""I'm not sure at the moment, but why don't you record yourself doing it and see how IDE saves it?"", ""For what I'm reading here, you need to use a send keys for control and another one for A.\n\nTry that.\n\nhttps://www.rickyadams.com/wp/index.php/2017/12/01/using-special-keys-in-selenium-ide/"", 'All you gotta do is find element by X path, and then add ‚Äò.clear()‚Äô.  Or you need to use action chains if you insist on the hard way, using keystrokes.', 'Why not try something like the following?\n\n    IWebElement element = driver.FindElement(By.whatever(""...""));\n    Actions actionBuilder = new Actions(driver);\n    IAction act = actionBuilder \n      .MoveToElement(element) \n      .Click() \n      .KeyDown(Keys.Control) \n      .KeyDown(""A"") \n      .KeyUp(""A"") \n      .KeyUp(Keys.Control) \n      .SendKeys(Keys.BACKSPACE) \n      .Build();\n    \n    act.Perform();\n\nBTW, Selenium IDE would simply save this as element.Clear();']"
Error message that just doesn‚Äôt have a solution,https://www.reddit.com/r/selenium/comments/xpgx4o/error_message_that_just_doesnt_have_a_solution/,selenium,"Hello again everyone!

I have been running this software to automate basic tasks for a while now but it seems that it won't work as it is supposed to anymore. Whenever I run the script, I get these error messages and the script just stops running:


\[70808:73532:0927/133844.457:ERROR:cert\_issuer\_source\_aia.cc(34)\] Error parsing cert retrieved from AIA (as DER):

ERROR: Couldn't read tbsCertificate as SEQUENCE

ERROR: Failed parsing Certificate


I tried looking this up on a few different forums but no-one seemed to know exactly what is happening. It's worth nothing that the target website's UI has changed BUT I have run the script successfully with the new UI.

Any ideas on what could cause this?","['What is the script exactly doing? Its hard to debug when we cant see the code', 'Have you gone to the site manually in the browser, does it use ssl?', 'Most likely an expired ca public cert in the browsers cert store.\n\nso please update the browser and webdriver you use.\n\nThis is analogue to browsing the net with a very old browser having outdated public certs in its cert store. you will then never be able to connect to a https site with a ssl cert issued by this ca provider, you may be able to override it. in chrome you can set --ignore-certificate-errors\n\nplease see: https://www.chromium.org/Home/chromium-security/root-ca-policy/']"
Stale Element Not Found error,https://www.reddit.com/r/selenium/comments/xpgeuh/stale_element_not_found_error/,selenium,"Hi everyone,

I'm trying to automate some stuff in Selenium on a Single Page Application type of website.
As per action chain: the robot will accept cookies and then click on a CTA that redirects to another page. Being a single page application, CTRL+click is not working, so it will open in the same tab.

After the action chain, the script return a Stale element not found error. I tried recalling the find_element command after the action chain with a new XPATH that can be found in the new page, but the error still persists. 

Can anyone help me with some ideas regarding this? I tried so many workarounds and nothing is working...
I pasted the code in the comments section.
Thank you!","['from selenium import webdriver\nfrom selenium.webdriver import ChromeOptions\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nimport time\n\n\noptions = ChromeOptions()\noptions.add_argument(""start-maximized"")\ns = Service(ChromeDriverManager().install())\ndriver = webdriver.Chrome(service=s, options=options)\n\ndriver.get(\'https://www.tork.de/\')\ntime.sleep(3)\n\nbutton_XPATH = \'/html/body/tork-app/div/tork-editorial-page/div/div[1]/ng-component/div/div/div[2]/div/div[1]/tork-link/a\'\ncookies_ID = \'onetrust-accept-btn-handler\'\n\nbutton_redirect = driver.find_element(By.XPATH, button_XPATH)\ncookies_click = driver.find_element(By.ID, cookies_ID)\n\nactions = ActionChains(driver)\nactions.click(cookies_click)\nactions.move_to_element(button_redirect)\nactions.click(button_redirect)\nactions.pause(3)\nactions.perform()\ntime.sleep(3)\n\ndataLayers = driver.execute_script(""return window.dataLayer"")\n\nprint(dataLayers)\n\ndriver.quit()']"
What do you guys think about using selenium vs karate for ui automation?,https://www.reddit.com/r/selenium/comments/xp4abp/what_do_you_guys_think_about_using_selenium_vs/,selenium,What do you guys think about using selenium vs karate for ui automation? Looking for good and bad for both. TIA.,[]
Using Selenium IDE: how to take a screenshot?,https://www.reddit.com/r/selenium/comments/xojbsr/using_selenium_ide_how_to_take_a_screenshot/,selenium,"All documentation I can find online is telling me to use command:


""CaptureEntirePageScreenshot""


But this command is not recognised by FF or Chrome extensions.


Has this been deprecated? How to take a screenshot please, do I need to use a different version of Selenium?


Thank you.","[""I don't use the Selenium IDE, but hopefully this helps. \n\nFirefox allows you to take a full page screenshot, but Chrome does not as far as I'm aware. \n\n((FirefoxDriver) driver.getFullPageScreenshotAs(OutputType.YOURCHOICEHERE);\n\nI recommend BASE64 to allow for quick inserts in to reports without having to store the image files.\n\nFor all other browsers I would recommend looking up AShot."", 'This feature would be so helpful for me']"
Staying logged in to a site,https://www.reddit.com/r/selenium/comments/xnyf2r/staying_logged_in_to_a_site/,selenium,"I'm assuming there's a way to login to a site and then navigate to different pages while being logged in, I just can't figure out how to do that with cookies, etc.

I can login via sendkeys, but when I navigate to the next URL (I have a list of URLs to go to) the ""logged in"" status is lost.

Any tips on what I'm doing wrong?  I think it's pointless to post code that doesn't work, but I can if that helps the process, or I can DM the site I'm trying to login to and use.","['Have you tried logging in and storing cookies of the current session of the webdriver instance in a before method before navigating to other urls? \nRefer to the thread on stackoverflow:\nhttps://stackoverflow.com/questions/15058462/how-to-save-and-load-cookies-using-python-selenium-webdriver', 'I thinks driver saving cookies because by default they do that , i thinks you have maybe issue with that website caught you as a automation software not as normal browser and then they clearing the cookies']"
Trying to scrape xpath with python and selenium but object doesn't found,https://www.reddit.com/r/selenium/comments/xnh1wd/trying_to_scrape_xpath_with_python_and_selenium/,selenium,"I want to scrap the element of the polygon below and spend a significant amount of time searching for the appropriate xpath:

tidedir = self.driver.find\_element\_by\_xpath(""//polygon\[@points='5.5,0 11,5.5 8,5.5 8,15 3,15 3,5.5 0,5.5'\]"")

Unfortunately I still get an error :

 **Exception has occurred: NoSuchElementException**

Message: no such element: Unable to locate element: {""method"":""xpath"",""selector"":""//polygon\[@points='5.5,0 11,5.5 8,5.5 8,15 3,15 3,5.5 0,5.5'\]""} 

Does anyone know what I do wrong? Below the the corresponding xml from the page :

<svg viewBox=""0 0 11 15"" preserveAspectRatio=""xMinYMin meet"" class=""quiver-tide-arrow""><polygon points=""5.5,0 11,5.5 8,5.5 8,15 3,15 3,5.5 0,5.5""></polygon></svg>

<polygon points=""5.5,0 11,5.5 8,5.5 8,15 3,15 3,5.5 0,5.5""></polygon>

&#x200B;

[https://www.surfline.com/surf-report/bondi-beach/5842041f4e65fad6a7708bf8?camId=5d482ee6c4a6abc1d318fc0e](https://www.surfline.com/surf-report/bondi-beach/5842041f4e65fad6a7708bf8?camId=5d482ee6c4a6abc1d318fc0e)

&#x200B;

Thanks,

Tome","[""you have 2 options here: \n\n1. I tried to find the polygon with xpath and the closest and unique element I found is the parent element of it: //\\*\\[@class='quiver-tide-arrow'\\]\n2. You can try to use css selector: driver.find\\_element\\_by\\_css\\_selector('.quiver-tide-arrow polygon'). This element is unique and satisfies your requirement""]"
Can‚Äôt use camera headless chrome,https://www.reddit.com/r/selenium/comments/xn7cf4/cant_use_camera_headless_chrome/,selenium,Headless chrome doesn‚Äôt detect the camera.  I am running this Python program on a Linux mint laptop.  Is there any solution?,['camera-control is outside of browser and not part of webdriver. maybe a profile can be automated with no interaction so it just starts ?\n\nUsually this is fixed using a desktop driver like on windows you have winappdriver (deptecated) or commercial products like test-complete and loadrunner.']
How do I exit a Price Check Loop once desired or lower price is met and commit to purchase?,https://www.reddit.com/r/selenium/comments/xm6pde/how_do_i_exit_a_price_check_loop_once_desired_or/,selenium,"Please forgive my crappy code as this is my first attempt to code overall. I'm trying to to automate an amazon purchase when the right set price hits in the code compared to to current amazon price. It seems that it just goes into infinite loop rather than refresh the page and execute once the set price or lower hits. Is there an easier way to code it?

&#x200B;

    
    while (amazon_price) >= int(buy_price):
        print(amazon_price)
        print('do not buy')
        random_wait_time = random.randrange(8.0, 20.0)
        print(random_wait_time)
        time.sleep(random_wait_time)
        wd.refresh()
    else:
        add_to_cart_button = wd.find_element_by_xpath('//*[@id=""a-autoid-2-offer-1""]/span/input')
        add_to_cart_button.click()
    
    view_cart_button = wd.find_element_by_xpath('//*[@id=""aod-offer-view-cart-1""]/span/input')
    
    view_cart_button.click()","[""You aren't updating the value of amazon_price within your while loop, you're just setting it once beforehand. That means that even if the price on the page goes down, your code won't be aware of it because it will still be looking at the price as it was first set.\n\nYou need to put the logic to find and interpret the current price inside the while loop."", '1. Break it up in to functions. \n     Example checkPrice(). \n2. In main program loop over call to check price \n3. Exit loop by break statement if procent is good.  \n\nIn the case above i Would have the following sub functions \n\n\nInit - setup everything \nGetPrice\nCheckPrice\nMainProgram\nOrderItem\n\n\nMainProgram would be \n\n\n    Init \n    While(true):\n       If CheckPrice (GetPrice):\n           Break\n       RandomWait\n    OrderItem']"
Need help with multiple elements and fixing code,https://www.reddit.com/r/selenium/comments/xl61o2/need_help_with_multiple_elements_and_fixing_code/,selenium,"The script is coming along, and I want to thank everyone who have been of great assistance so far. 

    from selenium import webdriver
    from selenium.webdriver.common.keys import Keys
    from selenium.webdriver.common.by import By
    import login as login
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    import datetime
    import time
    
    x = datetime.datetime.now()
    x = x.strftime(""%b %d"")
    
    driver = browser = webdriver.Firefox()
    driver.set_window_size(1512, 799)
    driver.get(""https://connect.garmin.com/modern/activities"")
    
    driver.implicitly_wait(1)
    
    iframe = driver.find_element(By.ID, ""gauth-widget-frame-gauth-widget"")
    driver.switch_to.frame(iframe)
    
    driver.find_element(""name"", ""username"").send_keys(login.username)
    
    driver.find_element(""name"", ""password"").send_keys(login.password)
    driver.find_element(""name"", ""password"").send_keys(Keys.RETURN)
    
    driver.switch_to.default_content()
    
    time.sleep(10)
    
    driver.find_element(""name"", ""search"").send_keys(""Reading"")
    driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)
    
    time.sleep(2)
    
    time_read = 0
    time_meditated = 0
    time_programming = 0
    
    def get_modified_xpath(value):
    	return ""//span[text() = '{}']//ancestor::div[@class='list-item-container']//div[5]//div[2]//span//span[1]"".format(value)
    
    
    date_str = get_modified_xpath(x)
    
    current_time = driver.find_elements(By.XPATH, date_str)
    for times in current_time:
    	if len(times.text) >= 7:
    		result = time.strptime(times.text, ""%H:%M:%S"")
    		time_read += result.tm_hour * 60
    		time_read += result.tm_min
    		print(time_read)
    	else:
    		result = time.strptime(times.text, ""%M:%S"")
    		time_read += result.tm_min
    		print(time_read)
    
    time.sleep(1)
    
    driver.find_element(""name"", ""search"").clear()
    driver.find_element(""name"", ""search"").send_keys(""Meditation"")
    driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)
    
    time.sleep(3)
    
    current_time = driver.find_elements(By.XPATH, date_str)
    
    for times in current_time:
    	if len(times.text) >= 7:
    		result = time.strptime(times.text, ""%H:%M:%S"")
    		time_meditated += result.tm_hour * 60
    		time_meditated += result.tm_min
    		print(time_meditated)
    	else:
    		result = time.strptime(times.text, ""%M:%S"")
    		time_meditated += result.tm_min
    		print(time_meditated)
    
    time.sleep(1)
    
    driver.find_element(""name"", ""search"").clear()
    driver.find_element(""name"", ""search"").send_keys(""Programming"")
    driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)
    
    time.sleep(3)
    
    current_time = driver.find_elements(By.XPATH, date_str)
    
    time.sleep(1)
    
    for times in current_time:
    	if len(times.text) >= 7:
    		result = time.strptime(times.text, ""%H:%M:%S"")
    		time_programming += result.tm_hour * 60
    		time_programming += result.tm_min
    		print(time_programming)
    	else:
    		result = time.strptime(times.text, ""%M:%S"")
    		time_programming += result.tm_min
    		print(time_programming)
    
    print(f""You spent {time_read} minutes on Reading today"")
    print(f""You spent {time_meditated} minutes on Meditation today"")
    print(f""You spent {time_programming} minutes on Programming today"")
    
    # def get_time_from_page(activity, activity_spent):
    #
    # 	time.sleep(2)
    #
    # 	current_time = driver.find_elements(By.XPATH, date_str)
    #
    # 	driver.find_element(""name"", ""search"").clear()
    # 	driver.find_element(""name"", ""search"").send_keys(activity)
    # 	driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)
    #
    # 	for times in current_time:
    # 		if len(times.text) >= 7:
    # 			result = time.strptime(times.text, ""%H:%M:%S"")
    # 			activity_spent += result.tm_hour * 60
    # 			activity_spent += result.tm_min
    # 			print(activity_spent)
    # 		else:
    # 			result = time.strptime(times.text, ""%M:%S"")
    # 			activity_spent += result.tm_min
    # 			print(activity_spent)
    #
    # 	time.sleep(3)        

It isn't looking great doing the same thing three times, which is why I tried to make a function, but I have encountered issues.

First issue is that I am unsure how to give the function a variable that it should then add the minutes to. activity_spent for example, it doesn't seem to add the time when I call the function giving it the variable time_read or time_programmed, even though these variables exist already, or even if they don't.

Second issue is that I now need multiple different elements from the same one for two or three activities, walking, running and hiking. Here I want more than simply time, now I need the distance and maybe heart rate as well. 

Third issue, and last one, is that the next step would be to summarize the time spent that day in some creative format, maybe there is a library that can summarize it into a banner, that I then can use for the twitter bot? I will have to look into it.

Then fixing all the explicit waits to something better of course.

[Picture of website, layout and some HTML](https://imgur.com/btkXlhu)",['We are back to some more work üòÑ']
selenium can't get pagesource,https://www.reddit.com/r/selenium/comments/xkwexp/selenium_cant_get_pagesource/,selenium,"I CAN'T GET PAGESOURCE  ON CHROME WITH IETAB.
HAVE YOU EVER HAD SIMILAR QUESTIONS.
?
How do you fix it?",['How about removing IETab?  \nJust an idea...']
Text disappearing from text box,https://www.reddit.com/r/selenium/comments/xk956g/text_disappearing_from_text_box/,selenium,"I'm trying to send text to a text box and then select the result from a dropdown menu. It works just fine when I send the commands directly through the terminal, but when running a script the entered text disappears before it's able to click on the result. I've tried putting a sleep function, sending left arrow, and sending enter, and clicking the box after sending the text but no success. It seems strange that it works line by line in the terminal but not in a script.

&#x200B;

     browser.find_element(By.CLASS_NAME,'field-input').send_keys('France')
     browser.find_element(By.CLASS_NAME,'search-option').click()","['You should look at waits and wait until the element is interactable. Sleeps are too volatile and may not work under all conditions.', ""Nevermind, i was able to make it work by adding a sleep() before the text was entered. I guess the text box wasn't fully ready to receive text.""]"
Scarping a website which shows data after Logging in and has also 2FA in place,https://www.reddit.com/r/selenium/comments/xjwb8m/scarping_a_website_which_shows_data_after_logging/,selenium,"I am very new to scraping (almost zero knowledge) and have a task at hand which will need automation. As given in the title I need to scrap a few thousand of records which are at a website where I have to login and go through 2FA, put in the search parameter to see this data, the search parameters are going to change through the dropdown list. All I know yet is that I have to use Selenium to automate the process.

Can some one guide me into this? I will be really grateful and put up the code for everyone's use once the job is done!","[""First of, you don't have to use selenium. There are multiple other better frameworks for purely webscraping which don't even use Webdriver protocol.\n\nBut for your particular case : If the otp can be sent by email then it's easy. You just use some imap library to query your inbox and then regex the message for otp, which bot then can use to login. \n\nIf the otp comes by SMS it's much harder since you have to somehow pass it to the framework from the phone. If I must I'd probably use some forwarding app, but also use another phone for that purpose, not my private."", 'Thankful to all the guys ( u/jarv3r u/aft_punk u/mortenb123 u/Supra02 u/Die_Edeltraudt)who replied with their suggestions, I think I might be on to something and might succeed as well!', 'First of all, selenium won‚Äôt help you. Assume a manual login step is required, do it, scroll to the bottom to make sure the entire page gets rendered, then use a browser extension such as [SingleFile](https://chrome.google.com/webstore/detail/singlefile/mpiodijhokgodhhofbcjdecpffjipkle?hl=en) to save a full copy of the page.\n\nFrom there use a tool like BeautifulSoup (Python) to extract the elements you want.  If you have a Mac and/or can write some pretty basic JavaScript,  you can automate this a bit.  But my key takeaway should be that selenium provides you no help for this.\n\nIn theory you could use Bitwarden and it‚Äôs CLI to potentially generate the OTP at execution, but I wouldn‚Äôt even attempt to mess with that and I have quite a bit of understanding how to do these things effectively.\n\nMany times there‚Äôs an API you can access to avoid webscraping entirely, but it‚Äôs up to you to determine if that‚Äôs an option for this data source. 2FA makes me doubt that will be an option.', 'Following for research purpose!']"
Basic framework for WebDriver in C# (need feedback...),https://www.reddit.com/r/selenium/comments/xjd781/basic_framework_for_webdriver_in_c_need_feedback/,selenium,"Hi, Folks!

I'm trying to create a lightweight framework for using Selenium WebDriver in C#.  I am at a crossroads where I need some feedback.  I am not sure if I want to make it into a NuGet package or just leave it open on GitHub.

I would like to:
1) Get some advice on adding an automatic execution log utilizing a package like NLog.
2) Receive feedback whether this framework is even a descent idea or not.  I think it is but I'm biased!

The code can be found at: https://github.com/vasagle-gleblu/Element34

Please be gentle!","['\\*Bump!\\*\n\nMaybe I should mention it has:\n\n* a data layer to interface with CSV files, Excel spreadsheets (using EPPusFree), Access DB files, SQL server, etc.\n* a video recorder (using SharpAVI).\n* additional functions that I \\*\\*think\\*\\* make WebDriver easier to use under C#.']"
"Chrome, Linux, headless, using client certificates",https://www.reddit.com/r/selenium/comments/xjgz2a/chrome_linux_headless_using_client_certificates/,selenium," 

I am having a problem passing my test user's PKI certificates in the headless mode. I am using Java Selenium WebDriver 4.3.0. When I run my test suite in normal mode, my profile and certificates are picked up perfectly. Profile users are selected by the ChromeOptions class by identifying the --user-data-dir= . I have different profiles for each of my test users. Then the certificate is selected by the policy setting (i.e, AutoSelectCertificateForUrls). That also works perfectly. As I navigate to different URL locations my test certificates are presented and accepted correctly when I run in the normal mode.

When I change the mode to Headless=true (i.e., ChromeOptions.addArguents(""--headless""), it all falls apart and no certificate is presented when I open a Chrome browser and hit any webpage.

I found that Firefox was extremely simple to manage profiles and PKI test certificates!!! When a test runs in normal mode and works perfectly, all I have to do is set the FirefoxOptions.addCommandLineOptions(""--headless""); and it still works perfectly in the headless mode. Not so with Chrome!!!

Does anyone know the correct solution? I could use the information. I am really stuck here.... Is there a way to still make Chrome present PKI certificates in headless mode or does anyone know that this feature really does not work for Chrome/Chromium? Then I could stop wasting my time!

Thanks in advance for your help!",[]
Where is the official Selenium API?,https://www.reddit.com/r/selenium/comments/xjkezc/where_is_the_official_selenium_api/,selenium,"I'm looking for the official repositories for the Selenium API (for PYTHON), and I have not yet been able to find it. 

([https://www.selenium.dev/](https://www.selenium.dev/)) Is one link that I keep finding but it does not seem to have extensive documentation (such as all the key elements, possibilities of element interactions, etc.)

([https://www.selenium.dev/selenium/docs/api/py/genindex.html](https://www.selenium.dev/selenium/docs/api/py/genindex.html)) Seems to have extensive documentation, although is this the right one to review?

&#x200B;

Thanks all",['https://www.selenium.dev/selenium/docs/api/py/api.html']
just can't click this button,https://www.reddit.com/r/selenium/comments/xj8r7h/just_cant_click_this_button/,selenium,"I have tried findElement, Actions, CDP (gets IndexOutOfBoundsException) and JS. I have scrolled to the bottom, changed the location and size, waiting a fixed amount and for clickability. Selenium finds the element but says ""element not interactable: \[object HTMLInputElement\] has no size and location"" Can anyone suggest a way to make click work? $[0.click](https://0.click)() in devtools works. The button is obviously there. The site is [here](https://elicense.ohio.gov/oh_verifylicense) Thanks

//div\[@class='search-submit'\]/input\[@value='search'\]","['One little cheat I like to do sometimes is to use one of the browser plug-ins like Selenium IDE or Katalon Recorder and record a sequence of steps with that.  Then convert that to the appropriate to the appropriate language and see what it comes up with.', 'Could you add the locator you attempted?', 'That locator you added points to the search button on the ""Business"" tab, is that the one you need or are you looking the one under ""Individual""? The issue might be that there are technically 2 search buttons there in the DOM and if the individual page is showing but your locator points to the business page one then it won\'t be clickable.\nLocator I used for individual page: \n//div[@id=\'individualForm\']//input[@value=\'search\']']"
Powershell $driver.FindElements using XPath seems to have a limit of 50 results into an array.,https://www.reddit.com/r/selenium/comments/xj8pis/powershell_driverfindelements_using_xpath_seems/,selenium,Is there a way to increase this limit or remove it?,"[""Are you sure there aren't more than 50 elements in the DOM?\n\nAn example might help illustrate the point.""]"
[help] take screenshot without selenium,https://www.reddit.com/r/selenium/comments/xj4gs2/help_take_screenshot_without_selenium/,selenium,"Hi All,
We want to take screenshot of specific URL but due to memory requirements of headless chrome, it's really difficult to get all the screenshots in time and sometimes our Serverless Architecture runs out of memory due to the chrome and selenium.

Is there any python library that we can use to get screenshots without selenium?

Thanks in advance",['Try pyautogui or the pillow module in the ImageGrab submodule.']
What is selenium automation testing?,https://www.reddit.com/r/selenium/comments/xj51f3/what_is_selenium_automation_testing/,selenium,"Automation testing is the process of executing a set of predefined tests over and over again in a rapid and repeatable way, ensuring that they are always up-to-date. Automation testing dramatically reduces the need for manual testing, allowing you to focus on more important work. It also makes executing tests faster and allowed for more frequent testing.

Selenium is an open source test automation framework written in Java. It supports various browsers and provides support for various testing frameworks such as JUnit, TestNG. Selenium uses different software called drivers to control the browser.

Some of the reasons behind its popularity are as follows:

1) Tests can be written in various programming languages.

2) Tests can be run in different operating systems.

3) It supports any browser that is available.

4) It can integrate with other software frameworks like TestNG and JUnit for project management and reporting purposes.",[' Thank you']
Help with clicking href in python,https://www.reddit.com/r/selenium/comments/xiq7tf/help_with_clicking_href_in_python/,selenium,"Hello everyone,

I‚Äôm new to Selenium and need some guidance on how to click on a link assigned to href.

Below are my elements and I need Selenium to auto click and access the link: clickme.com

<div data-se=""app-card-container"" class=""chiclet--container"" draggable=""true"">
    <a aria-label=""launch app"" class=""chiclet a--no-decoration"" data-se=""app-card"" href=""https://clickme.com"" rel=""noreferrer"">
        <article class=""chiclet--article"">
            <section class=""chiclet--main"" data-se=""app-card-main""><img class=""app-logo--image"" src=""https://ok.com"" alt=""my app"" /></section>
            <footer class=""chiclet--footer"" data-se=""app-card-footer"">
                <o-tooltip content=""AtL"" position=""bottom"" class=""hydrated"">
                    <div slot=""content""></div>
                    <div aria-describedby=""o-tooltip-73""><span class=""chiclet--app-title"" data-se=""app-card-title"">my app</span></div>
                </o-tooltip>
            </footer>
        </article>
    </a>
    <button class=""chiclet--action"" tabindex=""0"" aria-label=""Settings for app"" data-se=""app-card-settings-button"">
        <svg class=""chiclet--action-kebab"" width=""20"" height=""4"" viewBox=""0 0 20 4"" fill=""#B7BCC0"" xmlns=""http://www.w3.org/2000/svg"">
            <circle cx=""2"" cy=""2"" r=""2""></circle>
            <circle cx=""10"" cy=""2"" r=""2""></circle>
            <circle cx=""18"" cy=""2"" r=""2""></circle>
        </svg>
    </button>
</div>","['Could you show what you have tried so far to locate the link?', 'Could you just have the webdriver go to the url in the href instead of clicking', 'driver.find_element_by_css_selector(""[href=\'https://clickme.com\']"")']"
selenium.common.exceptions.TimeoutException stackoverflow up,https://www.reddit.com/r/selenium/comments/xinwx6/seleniumcommonexceptionstimeoutexception/,selenium,"Please help guys

[https://stackoverflow.com/questions/73775449/selenium-common-exceptions-timeoutexception-message-but-the-element-is-existin](https://stackoverflow.com/questions/73775449/selenium-common-exceptions-timeoutexception-message-but-the-element-is-existin)","['You‚Äôre waiting until the element is clickable.  Are you sure the element is actually clickable?  Maybe it‚Äôs only clickable if it‚Äôs a public user, but not clickable if it‚Äôs private.\n\nMaybe try waiting for visibility or presence of the element instead.']"
SQL injection with selenium,https://www.reddit.com/r/selenium/comments/xi9oku/sql_injection_with_selenium/,selenium,"Hi all, how do we approach sql injection automation  testing with selenium? Are there any best practices that you followed in your project?",[]
How To Access Password Field To Create Google/Gmail Account,https://www.reddit.com/r/selenium/comments/xhhpoo/how_to_access_password_field_to_create/,selenium,I'm trying to use Selenium to create a Gmail and cannot access the password field. There is no id and using tab to access it won't work either. How can I tell Selenium to find the password field? Thanks!,"['Sounds like they are actively blocking a bot from doing this which makes sense because they also require a valid cell phone number with 2FA to sign up as well.', 'Not sure where you are trying to do this or what language, but where i just went to create an account the password textbox had multiple different html tags. you could use the name tag if it has that. By.TagName(""Password"");', 'lol sounds sus']"
Pulling multiple elements from the same page,https://www.reddit.com/r/selenium/comments/xhhosb/pulling_multiple_elements_from_the_same_page/,selenium,"So I am making a Garmin crawling script and I want it to pull multiple elements if they are from the same day and add the time together for some activities, time, distance and heart rate for another for example. 

[![Layout of website][1]][1]


  [1]: https://i.stack.imgur.com/brwIa.png

``` 
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import login as login
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import datetime
import time

x = datetime.datetime.now()
x = x.strftime(""%b %d"")

driver = browser = webdriver.Firefox()
driver.get(""https://connect.garmin.com/modern/activities"")

driver.implicitly_wait(1)

iframe = driver.find_element(By.ID, ""gauth-widget-frame-gauth-widget"")
driver.switch_to.frame(iframe)

driver.find_element(""name"", ""username"").send_keys(login.username)

driver.find_element(""name"", ""password"").send_keys(login.password)
driver.find_element(""name"", ""password"").send_keys(Keys.RETURN)

driver.switch_to.default_content()

time.sleep(10)

driver.find_element(""name"", ""search"").send_keys(""Reading"")
driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)

time.sleep(2)

element = driver.find_element(By.CSS_SELECTOR, '.activity-date > span:nth-child(1)').text

time.sleep(2)
print(element)

time_read = 0

if element == x:
	spent = driver.find_element(By.CSS_SELECTOR, 'li.list-item:nth-child(1) > div:nth-child(2) > div:nth-child(5) > div:nth-child(2) > span:nth-child(1) > span:nth-child(1)').text

        result = time.strptime(spent, ""%H:%M:%S"")

	time_read += result.tm_hour * 60

	time_read += result.tm_min

	print(time_read)

```

So this is my current code. It finds the date, checks if it is today and adds the minutes to the variable time_read. 

Now I need some help in how I go about adding multiple elements, and if this can be done with some kind of for loop, where it loops between the dates and can then extract the time from the element?

Do I need to set them up one by one, since I need to provide which element a specific iteration needs to pull from? So maybe I should have 5 or 6 checks for example, instead of some kind of loop that goes through and does it? Then it will be a lot of manual work, which makes me question if there isn't a better way to deal with it.

I do not want to use CSV.


Some relevant HTML
```
<div class=""pull-left activity-date date-col"">
        <span class=""unit"">Sep 14</span>
        <span class=""label"">2022</span>
    </div>

<span class=""unit"" title=""3:32:00""><span class="""" data-placement=""top"" title=""3:32:00"">3:32:00</span></span>

<span class=""unit"" title=""1:00:00""><span class="""" data-placement=""top"" title=""1:00:00"">1:00:00</span></span>
<span class="""" data-placement=""top"" title=""1:00:00"">1:00:00</span>
```

Also a bit unsure what the best way is to locate elements? Is CSS.SELECTOR good or should I use XPATH preferably?

Thanks","['I would recommend not using XPath for a multitude of reasons and would use the same CSS selectors before and store in a List. Then use a for loop and use .get(i) and .get(I+1) to get the values, increment your loop by 2.', 'You can use driver.find_elements and store the elements which share the same locator in a list. You can use Python list comprehension to filter the elements by defined conditions and perform further actions.', 'Not a selenium response, but a very relevant question‚Ä¶\n\nAre you sure there isn‚Äôt an API you can pull this from?\n\nI think if you open ‚ÄúDeveloper tools‚Äù, you are going to find this data is being pulled from an api, allowing you to bypass webscraping altogether.\n\nThis should always be your first strategy.']"
Selenium_Web_Driver :- Chrome_Ran_Out_Of Memory,https://www.reddit.com/r/selenium/comments/xhcuii/selenium_web_driver_chrome_ran_out_of_memory/,selenium,"In python I am using selenium web driver.

In the code i am logging in into a certain websites and i am download some specific reports and now I have automated this process. 

Earlier the code was working fine but now I am getting error in chrome

&#x200B;

Aww Snap

Something went wrong while displaying this webpage

Error Code : Out of memory

&#x200B;

Can someone help me out please.","[""That's a Chrome error not a selenium error but this page has some trouble shooting steps.\n\nhttps://support.google.com/chrome/answer/95669?hl=en&co=GENIE.Platform%3DDesktop"", 'Run in headless mode or use HTML unit driver.']"
Datadome etc‚Ä¶?,https://www.reddit.com/r/selenium/comments/xgi3jw/datadome_etc/,selenium,"Hi,

I thought I had saved a thread where a solution to datadome and was pointing toward a GitHub repository with a package supposed to help.
I‚Äôm not able to find any reference to it now‚Ä¶
Does anyone see what I‚Äôm talking about and could help me with a link?

Thanks üôè",[]
How do I post a question?,https://www.reddit.com/r/selenium/comments/xfxas4/how_do_i_post_a_question/,selenium,"Hello my fellow Selenium WebDriver coders.  I have been trying to post a question for 2 days and they all get deleted by AutoModerator.  Please, how do I get my questions posted?","['You just did', 'Did you break the single rule? No advertising']"
Dropdown problem on redesigned site,https://www.reddit.com/r/selenium/comments/xfktw9/dropdown_problem_on_redesigned_site/,selenium,"Hello,

I'm a big seller on a site that doesn't have an API.  
I have a Selenium program to post and edit ads.

They redesigned site and my program doesn't work.

Firstly I want to click on [dropdown](https://ibb.co/2cscx3q), then to select [category](https://ibb.co/7zHhcJN).  
Category is hidden in inspect, and it appears when I click on a category.

After that, I don't need to click on a subcategory, it opens itself when I select a category. Now I need to [select a subcategory.](https://ibb.co/XFSFXs6)

I managed to click on category menu but i can't manage to select category and subcategory.

Can anyone help me please?","['This submission has been removed because it looks suspicious to automod (c). If this was done in error, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fselenium&subject=about my removed submission&message=I‚Äôm writing to you about my submission that was removed (l). %0D%0DMy issue is...).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/selenium) if you have any questions or concerns.*']"
Build a simple Amazon price tracker using Python and Selenium,https://www.reddit.com/r/selenium/comments/xexgzj/build_a_simple_amazon_price_tracker_using_python/,selenium,[https://www.youtube.com/watch?v=w56ZqxYLCKU](https://www.youtube.com/watch?v=w56ZqxYLCKU),[]
Error in Setting up selenium in Node. Pls help :(,https://www.reddit.com/r/selenium/comments/xeb9cs/error_in_setting_up_selenium_in_node_pls_help/,selenium," 

I am following a tutorial on how to set up selenium in a node environment: [https://medium.com/dailyjs/how-to-setup-selenium-on-node-environment-ee33023da72d](https://medium.com/dailyjs/how-to-setup-selenium-on-node-environment-ee33023da72d)

After following some steps I get an error while testing. These are the steps that I followed: So first i initialize npm environment on windows: mkdir node\_testing cd node\_testing npm init -y npm install selenium-webdriver

then I installed chromedriver (105.0.5195.52) for chrome version 105.0.5195.102 (not the exact same version because there isn't one). I also added the chromedrivers path to system paths.

After that im asked to run the following code as a test but it results in an error:

`const webdriver = require('selenium-webdriver'),`  
 `By = webdriver.By,`  
 `until = webdriver.until;`  
`const driver = new webdriver.Builder()`  
¬† ¬† `.forBrowser('chrome')`  
¬† ¬† `.build();`  
`driver.get('http://www.google.com').then(function(){`  
`driver.findElement(webdriver.By.name('q')).sendKeys('webdriver\n').then(function(){`  
 `driver.getTitle().then(function(title) {`  
 `console.log(title)`  
 `if(title === 'webdriver - Google Search') {`  
 `console.log('Test passed');`  
¬† ¬† ¬† `} else {`  
 `console.log('Test failed');`  
¬† ¬† ¬† `}`  
 `driver.quit();`  
¬† ¬† `});`  
¬† `});`  
`});`

**Result:** Google Chrome opens and the following prints in the terminal:  

`DevTools listening on ws://127.0.0.1:57150/devtools/browser/d1fc93f0-4bff-4963-b074-0069229e0fa0`

`C:\Users\ibo\node_testing\node_modules\selenium-webdriver\lib\error.js:522`

`let err = new ctor(data.message)`

`^`

&#x200B;

`ElementNotInteractableError: element not interactable`

  `(Session info: chrome=105.0.5195.126)`

`at Object.throwDecodedError (C:\Users\ibo\node_testing\node_modules\selenium-webdriver\lib\error.js:522:15)`    

`at parseHttpResponse (C:\Users\ibo\node_testing\node_modules\selenium-webdriver\lib\http.js:589:13)`

`at Executor.execute (C:\Users\ibo\node_testing\node_modules\selenium-webdriver\lib\http.js:514:28)`

`at processTicksAndRejections (node:internal/process/task_queues:96:5)`

`at async thenableWebDriverProxy.execute (C:\Users\ibo\node_testing\node_modules\selenium-webdriver\lib\webdriver.js:740:17) {`

  `remoteStacktrace: 'Backtrace:\n' +`

`'\tOrdinal0 [0x004DDF13+2219795]\n' +`

`'\tOrdinal0 [0x00472841+1779777]\n' +`

`'\tOrdinal0 [0x00384100+803072]\n' +`

`'\tOrdinal0 [0x003AE523+976163]\n' +`

`'\tOrdinal0 [0x003ADB93+973715]\n' +`

`'\tOrdinal0 [0x003CE7FC+1107964]\n' +`

`'\tOrdinal0 [0x003A94B4+955572]\n' +`

`'\tOrdinal0 [0x003CEA14+1108500]\n' +`

`'\tOrdinal0 [0x003DF192+1175954]\n' +`

`'\tOrdinal0 [0x003CE616+1107478]\n' +`

`'\tOrdinal0 [0x003A7F89+950153]\n' +`

`'\tOrdinal0 [0x003A8F56+954198]\n' +`

`'\tGetHandleVerifier [0x007D2CB2+3040210]\n' +`

`'\tGetHandleVerifier [0x007C2BB4+2974420]\n' +`

`'\tGetHandleVerifier [0x00576A0A+565546]\n' +`

`'\tGetHandleVerifier [0x00575680+560544]\n' +`

`'\tOrdinal0 [0x00479A5C+1808988]\n' +`

`'\tOrdinal0 [0x0047E3A8+1827752]\n' +`

`'\tOrdinal0 [0x0047E495+1827989]\n' +`

`'\tOrdinal0 [0x004880A4+1867940]\n' +`

`'\tBaseThreadInitThunk [0x7613FA29+25]\n' +`

`'\tRtlGetAppContainerNamedObjectPath [0x77707A9E+286]\n' +`

`'\tRtlGetAppContainerNamedObjectPath [0x77707A6E+238]\n'`

`}`

`PS C:\Users\ibo\node_testing> [12044:11068:0914/213837.751:ERROR:device_event_log_impl.cc(214)] [21:38:37.751] USB: usb_service_win.cc:104 SetupDiGetDeviceProperty({{A45C254E-DF1C-4EFD-8020-67D146A850E0}, 6}) failed: Element not found. (0x490)`

`[12044:11068:0914/213837.861:ERROR:device_event_log_impl.cc(214)] [21:38:37.862] USB: usb_device_handle_win.cc:1048 Failed to read descriptor from node connection: A device attached to the system is not functioning. (0x1F)`        

`[12044:11068:0914/213837.896:ERROR:device_event_log_impl.cc(214)] [21:38:37.896] USB: usb_device_handle_win.cc:1048 Failed to read descriptor from node connection: A device attached to the system is not functioning. (0x1F)`        

`[12044:11068:0914/213837.903:ERROR:device_event_log_impl.cc(214)] [21:38:37.902] USB: usb_device_handle_win.cc:1048 Failed to read descriptor from node connection: A device attached to the system is not functioning. (0x1F)`

 Any suggestions are welcome. I've been stuck on this for days.  Pls help I'm stuck.","['OP are you able to get into google.com?', ""I'm not familiar with the language, but I see you reference driver.findBy and then webdriver again. I'd imagine you should only be referencing the driver object you created.""]"
How to save a xpath attribute list to a variable?,https://www.reddit.com/r/selenium/comments/xe3ifp/how_to_save_a_xpath_attribute_list_to_a_variable/,selenium,"Hi, just a quick question: How to save a xpath href attribute list to a variable?

The html code is:

<div class=""title\\\\\\\_container"">    


<a target=""\\\\\\\_blank"" href=""\\\[https://link.com\\\](https://link.com)"">    


Im using

prop\_url = driver.find\_elements(By.XPATH, '//div\[@class=""title\_container""\]\[1\]/a')

To find the url.

I can confirm this is correct because i can print this and i get the list:

for p in prop\_url:print(p.get\_attribute('href'))

But i dont need to print this list, i need to save it to a variable so i can create a dictionary afterwards.

I tried

prop\_url\_strings = prop\_url.\_\_getattribute\_\_('href')

print(prop\_url\_strings)

And im getting this error:

prop\_url\_strings = prop\_url.\_\_getattribute\_\_('href')

AttributeError: 'list' object has no attribute 'href'

&#x200B;

Can someone help me fix this please.

\*\*\*\*\*\*\*\*\*

EDIT: nm i got it:

prop\_url\_links=\[\]  
for pu in prop\_url:  
prop\_url\_links.append(pu.get\_attribute('href'))  
print(prop\_url\_links)

I guess i just needed to ""think out loud"". Sometimes you dont ""visualize"" things in your head good enough.","['Based on info you described, you can use the XPATH without the 1 step. It should solve your problem.']"
Commenting on an Instagram post throws StaleElementReferenceException,https://www.reddit.com/r/selenium/comments/xdju2p/commenting_on_an_instagram_post_throws/,selenium,"Hello,

I am trying to create a bot that likes and comments on posts that are on a user's Instagram feed. However, when I try to write a comment on a post using the .send\_keys() function, it throws the following exception: 

"" selenium.common.exceptions.StaleElementReferenceException: Message: stale element reference: element is not attached to the page document ""

&#x200B;

I understand the context of this exception, and I've tried some solutions already, such as:

* Using  WebDriverWait(browser, 60).until(EC.presence\_of\_element\_located(textarea)).
* Using  WebDriverWait(browser, 60).until(EC.element\_to\_be\_clickable(textarea)).
* Using  WebDriverWait(browser, 60).until(EC. staleness\_of(textarea)).
* Solution N¬∞3 + refreshing the page before clicking, as advised here: [https://stackoverflow.com/a/62170140/7138725](https://stackoverflow.com/a/62170140/7138725)

&#x200B;

I don't know how to solve this problem after trying so many solutions and ways. Help is much appreciated.

Here is my full code:

    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import NoSuchElementException
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium import webdriver
    import random
    import time
    
    def sleep_for_period_of_time():
        limit = random.randint(7,10)
        time.sleep(limit)
    
    user = input(""Enter your username: "")
    pwd = input(""Enter your password: "")
    
    def main():
        options = webdriver.ChromeOptions()
        #Adding options
        options.add_argument(""--lang=en"")
        options.add_argument('--disable-gpu')
        options.add_argument(""start-maximized"")
        options.add_experimental_option(""detach"", True) #<- to keep the browser open
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
    
        browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        browser.get(""https://www.instagram.com"")
        sleep_for_period_of_time()
    
        #Finding identification elements
        username_input = browser.find_element(by=By.CSS_SELECTOR, value=""input[name='username']"")
        password_input = browser.find_element(by=By.CSS_SELECTOR, value=""input[name='password']"")
        #Typing username and password
        username_input.send_keys(user)
        password_input.send_keys(pwd)
        sleep_for_period_of_time()
        #Locating and clicking on the login button
        login_button = browser.find_element(by=By.XPATH, value=""//button[@type='submit']"")
        login_button.click()
        sleep_for_period_of_time()
        #Disabling notifiations
        browser.get(""https://instagram.com/"")
        acpt = browser.find_element(by=By.XPATH, value='/html/body/div[1]/div/div/div/div[2]/div/div/div[1]/div/div[2]/div/div/div/div/div/div/div/div[3]/button[2]')
        acpt.click()
        sleep_for_period_of_time()
        #Liking and commenting on 4 pictures on my feed
      
        #locating and clicking on the like button
        like = browser.find_element(by=By.XPATH, value='/html/body/div[1]/div/div/div/div[1]/div/div/div/div[1]/div[1]/div[2]/section/main/div[1]/section/div/div[3]/div[1]/div/article[1]/div/div[3]/div/div/section[1]/span[1]/button')
        like.click()
        print(""Liked!"")
        time.sleep(3)
    
        comment = ""Cool!""
        #Locating the text area on the post and commenting
        text_area = browser.find_element(by=By.CSS_SELECTOR, value='textarea[class=""_ablz _aaoc""]')
        text_area.click()
        sleep_for_period_of_time()
        
        text_area.send_keys(comment)","['Why do you need to click on the textarea before using sendkeys?\n\nI THINK that clicking on it somehow changes the page. Try not clicking and just use the sendkeys method.', 'Because element is stale, find it again and then send keys.']"
Skipping HCaptcha,https://www.reddit.com/r/selenium/comments/xdhj52/skipping_hcaptcha/,selenium,"Hello everyone, I have a python script that was skipping a HCaptcha beautifully until a week ago and then all of sudden it stoped working. 

To skip that I was using:

chrome_options.add_argument('--disable-blink-features=AutomationControlled')

Does anyone know if anything has changed? Also does anyone know how to keep skipping that?","['you can turn on test mode in captchas. Just ask the devs to make this change', 'Just add the nopecha extension to auto solve them', ""Stop using automation on sites that EXPLICITLY don't want you to.  For sure, stop asking professionals to help you do sketchy stuff.""]"
running selenium python scripts with arguments in docker,https://www.reddit.com/r/selenium/comments/xd2b8c/running_selenium_python_scripts_with_arguments_in/,selenium,"hello,

i have a selenium python script that i need to dockerize , it's my first time using docker.

 i have watched some tutorials i understood why docker is used and it's use-cases but i don't know how to  implement my script using docker .

i have added some files to my project:

dockerfile:

\`\`\`##############################3

FROM python:3.6.9-alpine  
ENV PYTHONUNBUFFERED 1  
COPY ./requirements.txt /requirements.txt  
RUN pip install -r /requirements.txt  
RUN mkdir /app  
COPY ./app /app  
WORKDIR /app

\`\`\`##########################

docker-compose.yml :

\`\`\` ################################

version: '3'  
services:  
 selenium:  
 image: selenium/standalone-chrome  
 ports:  
\- 4444:4444  
 restart: always  
 apps:  
 build:  
 context: .  
 volumes:  
\- ./app:/app  
 command: sh -c ""python3 run.py""  
 depends\_on:  
\- selenium 

\`\`\`###########################

&#x200B;

and the requirements.txt : 

\`\`\`#########################

selenium==3.141.0

\`\`\`############################

if someone can guide me it will be awesome.","[""I had the exact same use case. I was trying to run a selenium automation using Java inside a docker container.\nI used the selenium docker hub image...\nhttps://hub.docker.com/r/selenium/hub\n\nThe thing special about this image is that it provides you with 2 ports to view the output.\n1 using VNC (port 7900) viewer and other using NoVNC (port 5900)\nIt also comes with openjdk installed.\nIf you're using python then you might have to install it using the dockerfile.\n\nI used NoVNC because I didn't want to install VNC viewer\nAll I did was use the image, copy my program into the image using dockerfile and run it. Remember to expose the ports using port mapping (-p 5900:5900). After this, i hit localhost:5900 and it worked.""]"
Stuck on not being able to hit a button with Python + Selenium,https://www.reddit.com/r/selenium/comments/xd7ww1/stuck_on_not_being_able_to_hit_a_button_with/,selenium,"Hello,

I'm trying to automate a process since I can't with Vanguard ETFs, basically just buy 1 stock. I've dabbled in coding for the web with Python so I get the jist of what's happening.

I am all the way up to Preview Order but I can't seem to grasp clicking it.

The button itself has no id, and I've tried XPATH and it doesn't work.

Button code on inspecting it

`<button _ngcontent-trade-web-angular-c92="""" type=""button"" tdsbutton="""" tdsbuttonstyle=""primary"" data-testid=""btn-trade-preview-order"" tdsbuttonsize=""compact-below-xl"" class=""twe-flex-button-wrap__button tds-button tds-button--compact-below-xl""> Preview Order </button>`

XPath method

`driver.find_elements(By.XPATH, ""/html/body/twe-root/main/twe-trade/form/div/div[3]/div[2]/twe-trade-detail/tds-card/div/tds-card-body/div[3]/button[2]"").click()`

I've tried Searching for Preview Order as well.

`driver.find_element(By.XPATH, ""//button[text()=' Preview Order ']"").click()`

At this point I'm not sure what other options I have? The error is always ""Unable to locate element:""

&#x200B;

&#x200B;

EDIT:

I am able to get farther but I get this now.

""selenium.common.exceptions.ElementNotInteractableException: Message: element not interactable""

I am doing the following command now, gonna keep trying..

`driver.find_element(By.CSS_SELECTOR, ""[data-testid='btn-trade-preview-order']"").click()`","[""Without being able to see the full HTML of the page this is just guesswork, but typically if you aren't able to locate an element through normal means, there's a good chance that element is inside an iframe."", 'try js execution', 'Try using .Submit() instead of click. \nSometimes that works for me', ""Maybe the element is not clickable since it's not visible or out of view.\n\nYou can try 2 things: add a wait (implicit or explicit to wait for element to be visible) or use move_to_element to scroll to that element.\n\nHope it helps. Good luck!""]"
Okta Integration,https://www.reddit.com/r/selenium/comments/xcm0yi/okta_integration/,selenium,Anyone have experience integrating their selenium frameworks and OKTA?,"['Also interested in this! My company is working towards Okta integration as well.', ""what sort of integration? do the sites you're testing require an okta login?""]"
Selenium pulls wrong value from an element?,https://www.reddit.com/r/selenium/comments/xccevf/selenium_pulls_wrong_value_from_an_element/,selenium,"from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import login as login
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import datetime

x = datetime.datetime.now()
x = x.strftime(""%d"")

driver = browser = webdriver.Firefox()
driver.get(""https://connect.garmin.com/modern/activities"")

driver.implicitly_wait(2)

iframe = driver.find_element(By.ID, ""gauth-widget-frame-gauth-widget"")
driver.switch_to.frame(iframe)

driver.find_element(""name"", ""username"").send_keys(login.username)

driver.find_element(""name"", ""password"").send_keys(login.password)
driver.find_element(""name"", ""password"").send_keys(Keys.RETURN)

driver.switch_to.default_content()

driver.implicitly_wait(10)

driver.find_element(""name"", ""search"").send_keys(""Reading"")
driver.find_element(""name"", ""search"").send_keys(Keys.RETURN)

#element = driver.find_element(By.CLASS_NAME, ""unit"")
element = driver.find_element(By.XPATH, ""//html/body/div[1]/div[3]/div[2]/div[3]/div/div/div[2]/ul/li/div/div[2]/span[1]"")

print(element.text)

This is the code, the element ""unit"" should return ""Aug 25"", which I then want to use with ""x"" to make sure that I pull the correct data from a specific page. Problem is, it always returns today's date, even though the HTML says the correct one.

https://imgur.com/a/2d4YuQi

That is the page, any help is appreciated","['You\'re searching the element by class name. Classes aren\'t unique identifiers. You also have two variables with the same name  \n  \nEdit: best way to do this is to find the text in the 2nd column in the row that contains the search term, in this case ""reading""', ""Looks like you're just printing the value of x, as in x when it's today's date. Can you show what you mean by using that and the result of the findElement call?"", 'That ""unit"" is being used across every date entry on the Activities page. You want to use the Selenium IDE extension and get the XPATH that way. Here are my results, hope you learn anything from it.\n\n[https://imgur.com/a/TnbCJnb](https://imgur.com/a/TnbCJnb)\n\n[https://pastebin.com/gKhWRSbV](https://pastebin.com/gKhWRSbV)\n\nDon\'t mind the hard-coded waits. Normally, I\'d use implicit waits but this was a quick job. The important bit is of course how we got the attribute using .text and the proper XPATH.']"
Specflow on Docker,https://www.reddit.com/r/selenium/comments/xc80d0/specflow_on_docker/,selenium,"Hey Guys

Anyone out there Running Specflow on Linux Docker Container ?

Looking for an example  tutorial 

&#x200B;

Thanks",['Found this docker image for specflow. You can refer to the dockerfile here.... \nhttps://github.com/rdagumampan/docker-specflow \n\nOr here... \nhttps://github.com/SpecFlowOSS/SpecFlow/blob/master/Dockerfile\n\nFew code snippets here...\nhttps://kandi.openweaver.com/csharp/rdagumampan/docker-specflow#Summary']
I made an RPA library for Selenium and FlaUi,https://www.reddit.com/r/selenium/comments/xc443t/i_made_an_rpa_library_for_selenium_and_flaui/,selenium,"You can find it here https://github.com/ALaurian/Flanium.

It is still in development and you can request features and improve it yourself if you have the time.

I would like to hear what your opinions are on such a project since I could not find anything similar on the web so I spent some time making my own.

I leveraged Polly to make Selenium much more resilient and used FlaUI to leverage the UIA3 power and enable the library to interact with non web elements.

There s also the engine class, which lets you make your own workflows, check out the rpa challenge project for a showcase.","['This submission has been removed because it looks suspicious to automod (c). If this was done in error, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fselenium&subject=about my removed submission&message=I‚Äôm writing to you about my submission that was removed (l). %0D%0DMy issue is...).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/selenium) if you have any questions or concerns.*']"
Selenium Automation,https://www.reddit.com/r/selenium/comments/xc8x3e/selenium_automation/,selenium,"Selenium is the world‚Äôs most popular automating tool that makes it easy to build and maintain automated tests. Selenium introduced the concept of WebDriver, a browser automation framework that supports multiple languages and platforms such as Java, C#, Objective-C, Groovy, Perl, Python and Ruby.

 Selenium is free and an open source tool that provides an API for web developers to provide a language-independent method for writing tests. The underlying framework allows users to write test scripts in various programming languages like Java, Ruby or any other language. You can even use JSON, XML or any other data formats with Selenium without having to learn complex programming languages.

It tests your web applications by performing actions such as clicking buttons and verifying text displays in order to generate an automated functional test. Selenium can also run across different browsers to ensure your application display looks the same across them all.",[]
Web Scraping API Idea,https://www.reddit.com/r/selenium/comments/xb50hx/web_scraping_api_idea/,selenium,"Hey guys,

A while back I created a project called Scrapeium ([website here](https://scrapeium.netlify.app/)), a query language for declaratively and simply extracting data from websites. Right now, it only works in the browser but I was wondering would you guys be willing to use something like this if it was available as a public API?","['My feeling is it is probably too weak to handle the full range of content formats that are found in the wild.  In my experience, it is sometimes necessary to get deep into the DOM and I interact with code to filter and manipulate the elements to get the data properly.  If this project offered raw HTML output as well for those cases, I could see this as being useful though.']"
How can I run my selenium code again and again ?,https://www.reddit.com/r/selenium/comments/xbc0ls/how_can_i_run_my_selenium_code_again_and_again/,selenium,Recently I was automating a website and i wanted to run my code with different data input that was in an excel sheet which was quite long with 2k-3k values . As a beginner I tried using for loop but some error comes after running the code for 20-30 times . Is there a better way to do this task ?,"['Google ""Parameterization in Selenium"".  With Parameterization, you can pass multiple data to your scripts at runtime.', ""Check out DataProvider and use XSSFWorkbook if you're sticking with Excel. You can store the values in a Map object and then add your DataProvider to the Test annotation, if you're using TestNG or something similar. The iteration count will be the number of rows in your Workbook-Worksheet.""]"
ELI5 How do I encrypt/decrypt my password in Selenium ruby,https://www.reddit.com/r/selenium/comments/x9ol2z/eli5_how_do_i_encryptdecrypt_my_password_in/,selenium,"Ok, so I need to submit an automation test case for review. I've built the test case in Selenium ruby using RSpec. The test case has a plain text password in it and I'd like to have it encrypted and then decrypted when it's passed into the password field.   How would I do that, I've been searching all night and the answers I'm coming across are not very clear and I could use some community help. Is there a gem or something that I need that I haven't found?","['This is a ruby function, not Selenium.\n\nhttps://devopsqa.wordpress.com/2018/05/09/encrypt-and-decrypt-passwords-in-ruby/', 'figured it out, thanks to u/aspindler for a push in the right direction.']"
How to grab element in Selenium after a new tab is opened with click(),https://www.reddit.com/r/selenium/comments/x99lrw/how_to_grab_element_in_selenium_after_a_new_tab/,selenium,"A new tab is opened after an [`element.click`](https://element.click)`()` \- however I need to click on an image in the tab that is opened. 

How would I go about doing this? I keep getting `'no such element: Unable to locate element'`","['Refer to the documentation:\nhttps://www.selenium.dev/documentation/webdriver/browser/windows/\nYou can collect the active windows/tabs in a set and switch to the active tab and look for the element. There is demo code in the documentation above so you can try it out :)', 'You need to tell your webdriver instance to switch to the new tab. \n\nShould be some guidance here: https://www.selenium.dev/documentation/webdriver/browser/windows/', 'Use the getWindowHandles() method to capture the window IDs, then use the switchTo() method to switch to that tab\n\nIdeally you can store the window handles in a map and name them based on their titles for easier use in future']"
"TouchActions in selenium, how can I tell how long does the ""longPress"" action execute and if there's an alternative to it?",https://www.reddit.com/r/selenium/comments/x8160j/touchactions_in_selenium_how_can_i_tell_how_long/,selenium,"I'm using Selenium version 3.0.1. I want to use the ""longPress"" action from the org.openqa.selenium.interactions.touch.TouchActions package. However, I also want to long press a web element for a specefic amount of time and the longPress method does not give me this option. So I have two questions here:

1- How long does the longPress method touches on an element? Does it keep pressing on it until I call the release() method?  
2- What alternative do I have to be able to long press on an element for a specific amount of time? Note that by ""long press"" I mean by touching the element and emitting a touch event, not clicking.

In Appium for instance they have a method with these parameters longPress(element, duration), but the Selenium version that I'm using doesn't have this option.

Thank you.","['Not familiar with it but could you do a long press, sleep for X amount of seconds then release?', 'Does this site display a busy indicator of some sort?  If so, you could write a function to test for the presents of this indicator and then continue when finished.\n\n    (in C#)\n            public static void BusyIndicator(this IWebDriver driver, By locator, TimeSpan? timeOut = null)        {            ReadOnlyCollection<IWebElement> busyIndicators;            try            {                busyIndicators = driver.FindElements(locator);            }            catch            {                return;            }            timeOut = (timeOut == null) ? TimeSpan.FromSeconds(30) : timeOut.Value;     // default value for TimeSpan parameter            Task task = Task.Run(() =>            {                foreach (var indicator in busyIndicators)                {                    if (indicator != null)                    {                        while (driver.HasChildren(indicator))                        {            \t\t\tThread.Sleep(300);            \t\t\tThread.Yield();                        }                    }                }            });            task.Wait((TimeSpan)timeOut);        }        public static bool HasChildren(this IWebDriver driver, IWebElement element)        {  // Determine if element has **ANY** children at that moment            bool hasChildren;            if (element == null)                throw new ArgumentNullException(""element"");            //Save implicit timeout to reset it later             TimeSpan tmp = driver.Manage().Timeouts().ImplicitWait;            driver.Manage().Timeouts().ImplicitWait = TimeSpan.Zero;            try            { hasChildren = element.FindElements(By.XPath("".//*"")).Count > 0; }            catch            { hasChildren = false; }            driver.Manage().Timeouts().ImplicitWait = tmp;            return hasChildren;        }']"
Trigger selenium scripts automatically,https://www.reddit.com/r/selenium/comments/x7qpr2/trigger_selenium_scripts_automatically/,selenium,"I made a selenium script in chrome that logs in and downloads a file but I need it to trigger automatically at a specific time.  I wrote a little script in autohotkey that can do the timing part but i don't know how to trigger the automation.  I've tried exporting the selenium script but I'm not sure what to do with it at that point.  Is there some command line arguments i can use that i haven't found yet?

It would be really cool if there was an option to export as a standalone executable.","['Convert it to an executable and use task scheduler in Windows to run said executable at specific intervals', 'In command prompt: pip install pyinstaller \n\nNext: pyinstaller ‚Äîonefile program.py\n\nNow you‚Äôll have a .exe file which you can launch with task scheduler', 'Jenkins', 'I set up a Python file to run via a BAT file, and ran that in the background via VBScript automatically with windows task scheduler. Makes it so you don‚Äôt have to convert the python code or anything. You could try to go that route if you want to look it up. If you do and have trouble let me know and I‚Äôll look up my old files when I‚Äôm around my computer.', 'Use a cronjob. Even better setup a cheap $5 vps and setup the cronjob then just log in and get latest files anytime from anywhere.', 'Upload it to free server like Heroku (Not free anymore) and setup a cron job from there. I did same with NodeJs script', 'Nunit has console runner, you could set up simple cron or win task', ""It's easy enough with Maven and TestNG. You'd create a TestNG XML file, add the Surefire Plugin to your pom.xml, then run mvn clean install in a batch file you put under a Windows scheduled task. An example is here: https://stackoverflow.com/questions/42847732/how-to-run-testng-xml-from-maven-command-line"", ""If you're just trying to run it locally you can create a console application, get the .exe file and trigger it via your autohotkey script. basically just runs the .exe file from the command line.""]"
Can Selenium be used to test UWP applications?,https://www.reddit.com/r/selenium/comments/x7jsud/can_selenium_be_used_to_test_uwp_applications/,selenium,"I have a UWP Point of Sale application made with C# and Xamarin and I want to create automated testing as the application is getting more and more complex.  Is Selenium a possible solution, and if not if someone could point me in the right direction I would appreciate that.

Thank you!","[""Basiacally Selenmium is not a tool for mobile testing, so you have two options, you can use either Xamarin.UiTest Framework or Appium.In both cases you can write in C#, tbh Appium is way way better than Xamarin.UiTest because its basiacally sellenium but adjusted for mobiles, Xamarin relies on XAML files (if you have an app in Xamarin then you know what I mean) and you opperate on them. I work in Xamarin.UITest for over a year and the only advantage I see over Appium is fact of integration with the cloud.I don't know if you do it by yourself or professionally but if an anwser is second option, the advantage with xamrin is followig. Microsoft sells a n access to a cloud platform which lets you to execute tests on real devices, so you can trigger them via the pipeline in Azure to exectue evernight, this platform is called an Appcenter.And the only advantage with Xamarin is fact that Appcenter accepts Xamarin.UiTest code which is by defult written in C#, Appium on the other hand is also supported.... but only with Java.  \nPS: Ofc there are mobile automation frameworks in JS but I think its not the case.""]"
Web Automation on RSA Archer GRC,https://www.reddit.com/r/selenium/comments/x7iiis/web_automation_on_rsa_archer_grc/,selenium,Can I use Selenium for web testing on RSA Archer? Can someone walk me through it?,[]
SOLUTION FOR element not interactable: element has zero size,https://www.reddit.com/r/selenium/comments/x7ca56/solution_for_element_not_interactable_element_has/,selenium,"I had a problem clicking an object that had zero size. The solution I found was to simply click the element location instead of the element itself. I did this by importing ActionChains.

Ac=ActionChains(driver)
Ac.move_to_element(<element name>).move_by_offset(0,0).click().preform()

Like all good code, this was borrowed without permission from someone who had the same question on StackOverflow a few years ago.",['Are you working with a table or a list by any chance?\n\nHave you implemented a screenshot on failure and if so what does that tell you?\n\nI usually get this error when the table or list is empty.']
Error when address is not found,https://www.reddit.com/r/selenium/comments/x7ackf/error_when_address_is_not_found/,selenium,"I'm trying to get Lat and Long coordinates, but when the address is not found the script stops with the following error:

&#x200B;

>*NameError                                 Traceback (most recent call last) /var/folders/tf/lvpv9kg11k14drq\_pv1f6w5m0000gn/T/ipykernel\_34533/135469273.py in <cell line: 31>()      51         lat = latlong\[0\]*       
>  
>*52         long = latlong\[1\] --->*   
>  
>*53 sheet\[i\]\[6\].value = lat.replace(""."","","")*        
>  
>*54     sheet\[i\]\[7\].value = long.replace(""."","","")*        
>  
>*55 # Caso n√£o localize o endere√ßo, ser√£ informado na c√©lula do excel NameError: name 'lat' is not defined*

&#x200B;

script:

`import time  # gerencia o tempo do script`

`import os  # lida com o sistema operacional da m√°quina`

`from selenium import webdriver  # importa o Selenium para manipula√ß√£o Web`

`from tkinter import Tk  # GUI para selecionar arquivo que eu desejo`

`from tkinter.filedialog import askopenfilename`

`from tkinter import messagebox as tkMessageBox`

`from openpyxl import load_workbook, workbook  # biblioteca que lida com o excel`

`import re`

&#x200B;

`# escolha o arquivo em excel que voc√™ deseja trabalhar`

`root = Tk()  # Inicia uma GUI externa`

`excel_file = askopenfilename()  # Abre uma busca do arquivo que voc√™ deseja importar`

`root.destroy()`

&#x200B;

`usuario_os = os.getlogin()`

&#x200B;

`chromedriver = ""/usr/local/bin/chromedriver""  # local onde est√° o seu arquivo chromedriver`

`capabilities = {'chromeOptions': {'useAutomationExtension': False,`

`'args': ['--disable-extensions']}}`

`driver =` [`webdriver.Chrome`](https://webdriver.Chrome)`(chromedriver, desired_capabilities=capabilities)`

`driver.implicitly_wait(30)`

`# iniciando a busca WEB`

`driver.maximize_window()  # maximiza a janela do chrome`

`driver.get(""`[`https://www.google.com/maps`](https://www.google.com/maps)`"")  # acessa o googlemaps`

`time.sleep(5)`

 

`# importando o arquivo excel a ser utilizado`

`book = load_workbook(excel_file)  # abre o arquivo excel que ser√° utilizado para cadastro`

`sheet = book[""Coordenadas""]  # seleciona a sheet chamada ""Coordenadas""`

`i = 2  # aqui indica come√ßar√° da segunda linha do excel, ou seja, pular√° o cabe√ßalho`

`for r in sheet.rows:`

&#x200B;

`endereco = sheet[i][1]`

`munic_UF = sheet[i][3]`

&#x200B;

`if str(type(endereco.value)) == ""<class 'NoneType'>"":`

`break`

&#x200B;

`endereco_completo = endereco.value + "" "" + munic_UF.value`

&#x200B;

`#preenche com o endere√ßo completo e aperta o bot√£o buscar`

`driver.find_element(""id"",""searchboxinput"").send_keys(endereco_completo)`

`driver.find_element(""id"",""searchbox-searchbutton"").click()`

&#x200B;

`# Aguarda carregar a URL e coleta os dados das coordenadas geogr√°ficas`

`time.sleep(5)`

`url = driver.current_url`

`latlong = re.search('@(.+?)17z', url)`

`if latlong:`

`latlong =` [`latlong.group`](https://latlong.group)`(1).rsplit("","", 2)`

`lat = latlong[0]`

`long = latlong[1]`

`sheet[i][6].value = lat.replace(""."","","")`

`sheet[i][7].value = long.replace(""."","","")`

`# Caso n√£o localize o endere√ßo, ser√£ informado na c√©lula do excel`

`if lat == None:`

`sheet[i][6].value= ""Nao foi possivel identificar""`

&#x200B;

`# Limpa os campos para nova busca de coordenadas`

`driver.find_element(""id"",""searchboxinput"").clear()`

`lat=""""`

`long=""""`

&#x200B;

`i += 1`

&#x200B;

`# salva o excel na √°rea de trabalho`

`caminho_arquivo = os.path.join(""Resultado_final.xlsx"")`

[`book.save`](https://book.save)`(caminho_arquivo)`

&#x200B;

`# Avisa sobre a finaliza√ß√£o do rob√¥ e encerra o script`

`window = Tk()`

`window.wm_withdraw()`

`tkMessageBox.showinfo(title=""Aviso"", message=""Script finalizado! Arquivo salvo na pasta Documentos!"")`

`window.destroy()`

`driver.close()`

&#x200B;

Could someone help me to keep running and ignore the error?",[]
How do I correctly add chrome to my conda BASH path,https://www.reddit.com/r/selenium/comments/x793kd/how_do_i_correctly_add_chrome_to_my_conda_bash/,selenium,"I'm trying to run a remote debugging port on Chrome with Selenium.

From what I understand, I need to add chrome to my bash path but am having trouble getting this to work.

I've been following this tutorial [https://apple.stackexchange.com/questions/228512/how-do-i-add-chrome-to-my-path](https://apple.stackexchange.com/questions/228512/how-do-i-add-chrome-to-my-path)

My bash screen currently looks like this:

    if [ $? -eq 0 ]; then eval ""$__conda_setup""""
     else if 
    [ -f
    ""/Users/user/opt/anaconda3/etc/profile.d/conda.sh"" ]; then         . ""/Users/user/opt/anaconda3/etc/profile.d/conda.sh""  
       else      
       export PATH=""/Users/user/opt/anaconda3/bin:$PATH:/Users/user/Applications/Google Chrome.app/Contents/MacOS:$PATH"""" 

When I run try to run Chrome from the terminal it doesn't recognise it.

&#x200B;

If i type echo $PATH i get the following:

/Users/user/opt/anaconda3/bin:/Users/user/opt/anaconda3/condabin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin

What am I doing wrong?",[]
NoSuchElementException: no such element: Unable to locate element,https://www.reddit.com/r/selenium/comments/x6denx/nosuchelementexception_no_such_element_unable_to/,selenium,"Hi,

I'm very new to selenium and writing my first script which is a Google search. Th script has trouble clicking on the search button after typing in what to search for. I get a 'NoSuchElementException: no such element: Unable to locate element'. I also changed the script so it presses enter rather than clicking on the button and got the same error. Can you help? I have placed links to screenshots of my code and the error. 

[https://imgur.com/a/TeNy66B](https://imgur.com/a/TeNy66B)

Thanks","['1. Are sure the locator is correct? If not, fix the locator\n2. If the locator is correct, try to put thread.sleep before pressing enter/clicking on the search button', ""Check if the element you are clicking is inside a frame.\nIf that's the case then you have to switch to that frame."", 'In chrome go to develop mode, inspect and see the code.\n\nCheck if the element is inside\n\n<iframe>\n\n</iframe> \n\nthese tags.\n\nYou can also Google, ""frames in webpage"" and ""handling frames in selenium""', 'The reason is because when you start that search within Google it opens up the suggestion list which covers that original search button. You would need to use a List<WebElement> and do findElementsBy, then do a .get(index).click() -- Either that or just send Keys.ENTER after the search.']"
safaridriver --enable hangs forever,https://www.reddit.com/r/selenium/comments/x6cb78/safaridriver_enable_hangs_forever/,selenium,"I'm trying to enable safaridriver, but it hangs and never return the prompt.

    $ /usr/bin/safaridriver --enable
    Password:
    <hangs forever>

Any other \`sudo program\` works fine. 

I'm just porting some tests to mac. chrome/chromedriver works perfect",['1. Have you made sure the safari driver is in fact in that folder?\n2. Are you in fact sending and not just typing in that password you seem to be hanging at?\n3. Is Safari Browser in fact installed on this device?\n4. Is this a virtual device or a physical device?']
selenium can't run on linux graphical system,https://www.reddit.com/r/selenium/comments/x6b9ik/selenium_cant_run_on_linux_graphical_system/,selenium,"have anyone ever used selenium to launch chromium by chromedriver on linux grapical system?
i failed.
it prints ""syntax error : unterminated qouted string""","['Please post some code, both chromium-browser and chromedriver must be in your path and they must be of almost the versions aka 107.0.5283.X (x can vary)\n\nChromium is exactly google-chrome except for some codecs and extension included, so just change the binary\\_location if you have both installed.\n\n        elif os.environ[\'WEBDRIVER\'] == \'chrome\':\n            options = webdriver.ChromeOptions()\n            options.binary_location = ""/usr/local/chromium-browser-beta/chrome""\n            options.add_argument(\'--profile-directory=Default\')\n            options.add_argument(\'--no-sandbox\')\n            options.add_argument(""--disable-plugins-discovery"")\n            options.add_argument(\'--allow-scripts\')\n            options.add_argument(\'--allow-popups\')\n            options.add_argument(\'--allow-forms\')\n            options.add_argument(\'--disable-gpu\')']"
Error messsage that just doesn't have any solutions,https://www.reddit.com/r/selenium/comments/x5n3gj/error_messsage_that_just_doesnt_have_any_solutions/,selenium,"I have a selenium script running daily to automate certain tasks but for a few days it has stopped midway though and returned this error message when trying to write down a message on a text field:

&#x200B;

\[45128:40772:0904/162815.839:ERROR:interface\_endpoint\_client.cc(665)\] Message 0 rejected by interface blink.mojom.WidgetHost

&#x200B;

What is this related to? I have tried looking through the internet for a solution but no-one seems to have an answer.","['`bool InterfaceEndpointClient::SendMessageWithResponder(`\r  \n`Message* message,`\r  \n`bool is_control_message,`\r  \n`SyncSendMode sync_send_mode,`\r  \n`std::unique_ptr<MessageReceiver> responder) {`\r  \n  `DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);`\r  \n  `DCHECK(message->has_flag(Message::kFlagExpectsResponse));`\r  \n  `DCHECK(!handle_.pending_association());`\r  \n  `// Please see comments in Accept().`\r  \n  `message->SerializeHandles(handle_.group_controller());`\r  \n  `if (encountered_error_)`\r  \n`return false;`\r  \n  `InitControllerIfNecessary();`\r  \n  `// Reserve 0 in case we want it to convey special meaning in the future.`\r  \n  `uint64_t request_id = next_request_id_++;`\r  \n  `if (request_id == 0)`\r  \n`request_id = next_request_id_++;`\r  \n  `message->set_request_id(request_id);`\r  \n  `message->set_heap_profiler_tag(interface_name_);`\r  \n`#if DCHECK_IS_ON()`\r  \n  `// TODO(https://crbug.com/695289): Send |next_call_location_| in a control`\r  \n  `// message before calling |SendMessage()| below.`\r  \n`#endif`\r  \n  `const uint32_t message_name = message->name();`\r  \n  `const bool is_sync = message->has_flag(Message::kFlagIsSync);`\r  \n  `const bool exclusive_wait = message->has_flag(Message::kFlagNoInterrupt);`\r  \n  `if (!controller_->SendMessage(message))`\r  \n`return false;`\r  \n  `if (!is_control_message && idle_handler_)`\r  \n`++num_unacked_messages_;`\r  \n  `if (!is_sync || sync_send_mode == SyncSendMode::kForceAsync) {`\r  \n`if (is_sync) {`\r  \n`// This was forced to send async. Leave a placeholder in the map of`\r  \n`// expected sync responses so HandleValidatedMessage knows what to do.`\r  \n`sync_responses_.emplace(request_id, nullptr);`\r  \n`controller_->RegisterExternalSyncWaiter(request_id);`\r  \n`}`\r  \n`base::AutoLock lock(async_responders_lock_);`\r  \n`async_responders_.emplace(`\r  \n`request_id, PendingAsyncResponse{message_name, std::move(responder)});`\r  \n`return true;`\r  \n  `}`\r  \n  `SyncCallRestrictions::AssertSyncCallAllowed();`\r  \n  `bool response_received = false;`\r  \n  `sync_responses_.insert(std::make_pair(`\r  \n`request_id,`\r  \n`std::make_unique<SyncResponseInfo>(message_name, &response_received)));`\r  \n  `base::WeakPtr<InterfaceEndpointClient> weak_self =`\r  \n`weak_ptr_factory_.GetWeakPtr();`\r  \n  `if (exclusive_wait)`\r  \n`controller_->SyncWatchExclusive(request_id);`\r  \n  `else`\r  \n`controller_->SyncWatch(response_received);`\r  \n  `// Make sure that this instance hasn\'t been destroyed.`\r  \n  `if (weak_self) {`\r  \n`DCHECK(base::Contains(sync_responses_, request_id)); // Specifically Here is where you fail in your code`\r  \n`auto iter = sync_responses_.find(request_id);`\r  \n`DCHECK_EQ(&response_received, iter->second->response_received);`\r  \n`if (response_received) {`\r  \n`std::ignore = responder->Accept(&iter->second->response);`\r  \n`} else {`\r  \n`DVLOG(1) << ""Mojo sync call returns without receiving a response. ""`\r  \n`<< ""Typcially it is because the interface has been ""`\r  \n`<< ""disconnected."";`\r  \n`}`\r  \n`sync_responses_.erase(iter);`\r  \n  `}`\r  \n  `return true;`\r  \n`}`  \n\n\nThis is the section of code in the library that you\'re using that is failing.  \nI found this code at the following URL:  \n[interface\\_endpoint\\_client.cc](https://chromium.googlesource.com/chromium/src/+/refs/heads/main/mojo/public/cpp/bindings/lib/interface_endpoint_client.cc)  \n\n\nMy guess here is that either your message was null or empty, or the request ID was null or empty, or the connection was just lost.   \n\n\nI hope this helps you.']"
How to go onto next page without a url?,https://www.reddit.com/r/selenium/comments/x5gpfo/how_to_go_onto_next_page_without_a_url/,selenium," I'm trying to scrape [booking.com](https://booking.com/) using Selenium/BeautifulSoup but the 'next' button doesn't have a url so I'm not sure how to scrape the other pages.

I'm quite new to this, had a loo online but couldn't find anything about this (maybe I was searching the wrong stuff) pls help.","[""tell browser to press that 'next' button""]"
Security concerns when letting users access a website through selenium,https://www.reddit.com/r/selenium/comments/x4tpeq/security_concerns_when_letting_users_access_a/,selenium,"Hi all, I have a side project which would eventually let users gather the html of a website given the url, and would (sometimes) use selenium if necessary. Now this would mean that arbitrary JS is run on the webdriver, and although this is a side project I was wondering about the security implications of this. Will this JS be a threat to the whole server? Is this talked about in the selenium docs or anywhere else I can look? I haven't found anything","['Who controls the website that selenium is accessing, and how much control do users have over the inputs to selenium?  But yes, there is potential for compromising your system.']"
Not downloading Dependencies,https://www.reddit.com/r/selenium/comments/x4t7f9/not_downloading_dependencies/,selenium,Am trying to create a cucumber project and I have added all the dependcies required in the pom.xml but maven isn't building the nor dependencies are getting installed. Are there any reasons why is that?,['Refresh your project. If you use eclipse it has an option not sure of other IDE. Make sure you added all dependencies properly not missing anything. \n\nOtherwise start over with a new project. \n\nHope it helps']
ElementNotInteractableError: element has zero size,https://www.reddit.com/r/selenium/comments/x4326z/elementnotinteractableerror_element_has_zero_size/,selenium,"I'm trying to open the chat on a website but instead of a regular button, there's a clickable image. The image is just an image, clicking it does nothing. I need to click something behind the image which has a size of 0x0px. Can I tell my mouse to go to Hoover over an element then click? Is there some sort of workaround? Or am Selenium not the tool for the job?

I'm using Python, Selenium and Webdriver","['There\'s a mouseover command on Selenium.\n\nExample\n\n> from selenium import webdriver\nfrom selenium.webdriver.common.action_chains import ActionChains\n\n> firefox = webdriver.Firefox()\n> firefox.get(\'http://foo.bar\')\nelement_to_hover_over = firefox.find_element_by_id(""baz"")\nhover = ActionChains(firefox).move_to_element(element_to_hover_over)\nhover.perform()', 'You can to try set width and height for that element with javascript using ""execute script""', 'You can try clicking the element location']"
Facebook Automation - can it b e done?,https://www.reddit.com/r/selenium/comments/x3qbjt/facebook_automation_can_it_b_e_done/,selenium,"Hi everyone   


I am looking for a way to add friends (who have mutual friends) on Facebook and send them a message seeing if they are interested in our products/services. I am doing this process manually but it takes a long time, and I know an automated way would be a great way to free up time.  


Is this possible to be done with python/selenium?  


Thanks in advance","[""Can it be done? Sure.\n\nSHOULD it be done? No, that's spamming and a violation of the facebook terms of service."", 'If you want to automate it use fbs api.', ""Well, to be fair, it's not like you can make Facebook a worse experience, but at least you're trying."", 'Try it out']"
Is there a way to get the load time of specific page elements using selenium/python?,https://www.reddit.com/r/selenium/comments/x2fry1/is_there_a_way_to_get_the_load_time_of_specific/,selenium,"Hello, I would like to test the duration/load time of each element from the 'Circuit' dropdown.

I would prefer to have selenium do this however if you know a different way or tool do this please let me know. Thanks in advance!

\*\*Link: [https://beta-tools.analog.com/noise/#highSpeedPrecision\*\*](https://beta-tools.analog.com/noise/#highSpeedPrecision**)","[""I don't see a 'Circuit' dropdown anywhere on the linked page?"", 'You could use Chrome devtools and watch the network tab and just do each one one-by-one by hand?']"
Webdriver + Tkinter Singleton-esque pattern,https://www.reddit.com/r/selenium/comments/x1t1fq/webdriver_tkinter_singletonesque_pattern/,selenium,"Hi, so I‚Äôm still pretty new to webdriver and wondering if this design pattern might be a solution to my problem. Generally I try to avoid singleton but it seems to make sense here, unless I‚Äôm misunderstanding how the driver actually works.

I‚Äôve been building out a python tkinter gui to use with webdriver for my company, which helps my coworkers avoid many of the various tedious tasks associated with an ancient front end of an internal web app. Naturally, the buttons and other widgets‚Äô commands need to call on webdriver in some fashion. 

Rather than lump everything into a single script, I‚Äôve been developing this gui with classes and separating components into relevant modules. My selenium code is fashioned in the same way and I‚Äôve been trying to implement it in an OOP manner and observe the POM as best as possible. 

The only solution that comes to mind is to implement the driver as a singleton. Instead of passing it around all my page objects and components and trying to make it work with tkinter‚Äôs event loop at the same time, I just create the single driver instance, and each of my page objects just get this driver as an arg. Due to python‚Äôs annoying import behavior, I can‚Äôt just import the single driver instance into all of my page objects, so I have to settle for passing this driver into page objects in wrapper functions bound to tkinter command attributes.

From google, I‚Äôve been able to come up with this cobbled together code:

```
class WebdriverInstance:
    __driver__ = None


    def __init__(self):

        if WebdriverInstance.__driver__ is None:

            WebdriverInstance.__driver__ = self.__load_driver()
        else:
            raise Exception(""Driver already loaded."")


    @staticmethod
    def driver():
        if not WebdriverInstance.__driver__:
            WebdriverInstance()
        return WebdriverInstance.__driver__


    def __load_driver(self):
        ‚Ä¶
```

It works and I‚Äôm able to import the instance into each of the tkinter widgets that need it.

Anyways, I‚Äôm a noob and just wondering if this approach makes sense or if there‚Äôs a more obvious and logical way I‚Äôm missing? If there‚Äôs another way to further decouple ui from the driver logic, that would be great too. Thanks for any help!",[]
Selenium makes my computer heat up and lose battery,https://www.reddit.com/r/selenium/comments/x1jyfi/selenium_makes_my_computer_heat_up_and_lose/,selenium,"Hello everyone, I created a python script that does automation with selenium. However on my mac pro intel during use there is a sharp drop in the battery as well as overheating. Looking at the monitor I notice that the program is using 45% of the processor. Do you have an idea how to reduce this percentage so that this program runs ""normally""?","['That‚Äôs the cost of chrome‚Ä¶ Firefox might be a little more cpu friendly. \n\nAt one time I had a selenium process that ran 24 browsers at a time. I could have set it in my fireplace and heated the house. ü§£', 'Have you tried to turn off headless to see what the script runs? It seems to me that the web browser would be heavier than the selenium script.']"
Recommendation for open source Selenium Java library for automating tests?,https://www.reddit.com/r/selenium/comments/x1cthu/recommendation_for_open_source_selenium_java/,selenium,"Hi, test automators üòä

Could you please share with me - the best  open source Selenium Java library.

My background is with Ruby üíé where is available an open-source Ruby library for automating web browsers - [Watir](http://watir.com/)

Many thanks in advance!","['https://rubygems.org/gems/selenium-webdriver/versions/4.4.0', '[https://kandi.openweaver.com/search/libraries?q=selenium%20java%20test%20automation](https://kandi.openweaver.com/search/libraries?q=selenium%20java%20test%20automation)\n\nThis may be helpful']"
hey i want to run selenium grid on python file please help,https://www.reddit.com/r/selenium/comments/x1eqqe/hey_i_want_to_run_selenium_grid_on_python_file/,selenium,"Hey @everyone guys, have you been worked on the selenium grid

 I want to run a python file on it to see the response from the browser 

the installation process has been done, 

but most of the video on the internet shows on docker and java related one

 can anyone suggest a video or blog that helps me to run a python file on selenium grid","['Used in the past to run the code on linux servers without xwindow, but nowadays they can all run headless. Far better control with the browser/webdriver/python modules etc\n\nselenium grid afaik need the selenium-server-standalone which is a java runtime.\n\nYou need to set this up on the network and open the firewall so your python clients running selenium can reach it. The only change is using webdrivers remote instance:  \n\\`wd = webdriver.remote(command\\_executor=""[https://selenium-grid-host-ip](https://selenium-grid-host-ip):selenium-grid-host-port/wd/hub""\\`.']"
Creating While Loop in Selenium.,https://www.reddit.com/r/selenium/comments/x0caz7/creating_while_loop_in_selenium/,selenium,"Hi I have the following code.

\# button1=browser.find\_element(By.XPATH,""(//div\[@unselectable='on'\]\[normalize-space()='CS'\])\[1\]"")

\# act.double\_click(button1).perform()

\# time.sleep(1)

\# act.double\_click(button1).perform() 

\# time.sleep(1)

\# act.send\_keys('1').perform()

\# browser.find\_element(By.XPATH,""//div\[@id='ext-gen57'\]"").click()

\# button2=browser.find\_element(By.XPATH,""(//div\[@unselectable='on'\]\[normalize-space()='CS'\])\[1\]"")

\# act.double\_click(button2).perform()

\# time.sleep(1)

\# act.double\_click(button2).perform() 

\# time.sleep(1)

\# act.send\_keys('1').perform()

\# browser.find\_element(By.XPATH,""//div\[@id='ext-gen57'\]"").click()

\# button3=browser.find\_element(By.XPATH,""(//div\[@unselectable='on'\]\[normalize-space()='CS'\])\[1\]"")

\# act.double\_click(button3).perform()

\# time.sleep(1)

\# act.double\_click(button3).perform() 

\# time.sleep(1)

\# act.send\_keys('1').perform()

\# browser.find\_element(By.XPATH,""//div\[@id='ext-gen57'\]"").click()

This code is successfully with what I want Which is replace CS with ""1"". for each element that have text ""CS"".

I tried to create a while loop with this code. This is what I come up :

while (browser.find\_element(By.XPATH,""(//div\[@unselectable='on'\]\[normalize-space()='CS'\])\[1\]"").text) == ""CS"" or ""UNKNOWN"":

act.double\_click().perform()

time.sleep(1)

act.double\_click().perform() 

time.sleep(1)

act.send\_keys('1').perform()

browser.find\_element(By.XPATH,""//div\[@id='ext-gen57'\]"").click()

time.sleep(1)

However, when I do the while loop, it is not doing the same thing with the code that I tried in the beginning. Anyone have any input?  Thanks in advance for the help.",['This is not how to use while loop. You need an if statement inside']
SeleniumBasic v2.0.9.0 ‚Äì Excel Macro Doesn‚Äôt Trigger Onchange Event,https://www.reddit.com/r/selenium/comments/x07ufs/seleniumbasic_v2090_excel_macro_doesnt_trigger/,selenium,"Hello, I‚Äôm using SeleniumBasic v2.0.9.0 in an Excel macro to upload data from a spreadsheet into a web form. One of the dropdown values is supposed to update with the value of a previous entry, but the onchange event isn‚Äôt registering. Is there a way I can force the event to occur with my macro?

I don‚Äôt know much about writing code. Honestly, I‚Äôm just throwing stuff at the wall to see what will stick. I have tried the following:

>Waiting for the script to activate - obj.Wait 60000

&#x200B;

>Hitting tab in the field with the script - obj.FindElementByName(""variable"").SendKeys (""{TAB}"")

&#x200B;

>Clicking on the field with the script - obj.FindElementByName(""variable"").Click

&#x200B;

>Running this chunk I found on a forum - Set Refresh = obj.FindElementByName(""variable2"")  
>  
>obj.ExecuteScript ""arguments\[0\].click();"", Refresh

Nothing is registering as a change or running the event. I can‚Äôt share the page it is on, but I can share the element for it.

><select id=""variable2"" name=""variable2IPackage"" onchange=""jsf.ajax.request('variable2',event,{execute:'@this ','javax.faces.behavior.event':'valueChange'})"" size=""1"" style=""width: 150px;"" title=""variable2""> <option selected=""selected"" value=""""></option></select>

Any advice would be appreciated, as I am literally clueless. My Google-Fu has left me empty-handed.",[]
I'm stuck. Is it possible to launch selenium in docker container?,https://www.reddit.com/r/selenium/comments/wzbn3z/im_stuck_is_it_possible_to_launch_selenium_in/,selenium,"Hi, i was trying to launch selenium in docker using Java. I was able to launch it in headless mode, but i need to see the browser window to interact with it and run a few scripts. How can I accomplish this in a docker container?

I'm using the selenium-standalone image from docker hub and running a jar file of my spring boot application inside the container.

EDIT: I want to run everything in a docker container, My Java scripts, the selenium browser, a react UI","['Check out Selenium Grid in Docker. Connect your script to the Remote webdriver. You can then view grid nodes live with VNC and even record video of test sessions with an ffmpeg container.', ""Aerokube's Selenoid uses containers,  https://aerokube.com/selenoid/latest/\nGreat product for testing browsers"", 'There used to be an image that you could vnc into and get a preview of what was happening there.\nIt basically boiled down to exposing correct port and then connecting to it.', 'Selenium grid / vnc in to device is best way unless you want to pay for it.  \n\nhttps://www.browserstack.com/\nOr similar tools can also be used but there is a cost attached.']"
Cannot see option to run JUnit tests,https://www.reddit.com/r/selenium/comments/wz0hgw/cannot_see_option_to_run_junit_tests/,selenium,"Hi,

I'm very new to Selenium and trying to run some tests for the first time. I am following a book which as has asked me to clone a repo which contains the tests. I have it set up in Eclipse but when I right click on the test and and select 'Run as', I do not see the option 'JUnit test', but just the 'Run Configurations'. What is it that I need to run the JUnit tests? 

Here is a link to a screenshot of my setup in eclipse:

[Setup  in Eclipse](https://ibb.co/rH0H1W5)

Thanks you.",['OP check you Pom.xml if it mention anywhere j unit. In the dependencies section. Also the repo your using will have a read me file.']
Is VS Code a good IDE/text editor to create a selenium framework in?,https://www.reddit.com/r/selenium/comments/wxxehn/is_vs_code_a_good_idetext_editor_to_create_a/,selenium,"To give you all some context of why i am asking this:

I'm currently trying to create the same POM-based selenium webdriver framework for multiple programming languages (Java, C#, Python, and JavaScript). I am doing this so that when it comes to interviews based around Selenium, i will well-prepped and language agnostic since i've already got a Selenium template good to go for whatever language that company uses. I figured that JS, Java, C#, & Python were the most commonly-used languages when it comes to Selenium.

Anyhow... I've noticed so far that VS code is a pain in the ass with Selenium in Java. I installed the Extension Pack for Java, and that seemed to have given me everything i need to run Java Code. I then was able to get selenium to run with the main method. And then soon after a bit of tinkering with TestNG, i was able to get my example test to run from my test method within my testNG test class.

&#x200B;

Heres the problem. When i run my TestNG tests from the test section of VS code (with the java extenion pack installed), the only meaningful failure output i see is that the test method failed. But it doesn't show me which specific assertion in my test method failed. This leads me to think that maybe VS Code isn't the best IDE/editor to run selenium code for Java? What do you all think? 

&#x200B;

I would PREFER to use VS code because that's the same IDE/text Editor that I use with [Cypress.io](https://Cypress.io) automation. But it seems like TestNG is either NOT meant to be run with VS Code at all or i need to use a completely different assertion library or something with VS Code to have an easier ""automation experience"". 

Does anyone have any suggestions on 1) Which IDE/text editor i should be using with Java Selenium. and 2) Any unit test libraries or extensions i could use that i'm not using here?

&#x200B;

NOTE: I'm fully aware of Eclipse and jUnit, but i feel like that IDE is super old and crappy and JUnit doesn't have the same capabilities as TestNG.  Anyhow thanks for the feedback in advance!","['yes', ""I use VS Code for my Selenium/Cypress work and IntelliJ for Java API testing, it's just personal preference really."", 'I log my errors in a file, and make a screenshot.\n\nI use the try/catch approach, and on the catch, I write a log file and a screenshot.']"
Solution for unmaintaned ExpectedCondition class?,https://www.reddit.com/r/selenium/comments/wxqphj/solution_for_unmaintaned_expectedcondition_class/,selenium,"Was wanting to use wait.until instead of using a Thread.sleep() when waiting for a page/element to be found. What is currently the solution without using the unmaintained package seen here: [https://www.nuget.org/packages/DotNetSeleniumExtras.WaitHelpers/](https://www.nuget.org/packages/DotNetSeleniumExtras.WaitHelpers/)

I'm just worried about a future update wiping out that solution\^

Any suggestions are welcomed - thanks.",[]
I'm stuck. Is there a recommended way to deal with cookie settings pop-up windows when running selenium webdriver?,https://www.reddit.com/r/selenium/comments/wxkmfg/im_stuck_is_there_a_recommended_way_to_deal_with/,selenium,"When I run driver.get('website') in Chrome, I always have the problem of the cookie settings window. Since extensions like SelectorsHub are disabled in the window I can't get an xpath in order to click accept/decline. So I don't know either how to activate extensions or to prevent the pop-up window to come up every time. Do you have any suggested solutions?","[""There is a flag for browsers which disables those notifications. Also, selenium offers functions to deal with alerts. For chromium based browser, use option \n\n    option.add_argument('--disable-notifications')\n\nHow to deal with alerts, there is a simple guide\n\nhttps://www.tutorialspoint.com/how-will-you-handle-alerts-in-selenium-with-python#"", 'For the consent cookie pop up in our website i test i found out that the user preference are set in a cookie after he accepts , so try to see if thats the case in ur website as well. I load the homepage create that cookie with the format they look for and reload the homepage.']"
How to allocate selenium particular amount of RAM? It uses all of the RAM any PC it is running on...,https://www.reddit.com/r/selenium/comments/wxs90a/how_to_allocate_selenium_particular_amount_of_ram/,selenium,"If I have 8 GB RAM PC, it takes all of it in time and performs just fine. But, if I have 16 GB RAM, it uses all of it too and still performs fine. So it means it only needs 8 GB RAM. So I have a program that I just can't driver.quit() or something else, because I resize the windows to my liking, so I just can't driver quit then resize them back again too much effort. How to allocate a particular amount of RAM to the selenium web driver? I don't want to fill my RAM.","[""Are you running a single instance of the driver and it takes ALL 16 of ram?\n\nAre you running a non stop script? Are there memory leaks?\n\nIt shouldn't be like that. \n\nAnd I don't think you can set up how much ram does it use. Setting up as headless will lower ram usage.""]"
headless for certain tests ?>,https://www.reddit.com/r/selenium/comments/wwzg4g/headless_for_certain_tests/,selenium,"I have a TestNG XML that has 20 tests.   
I want the first 15 to run on headless more and the last 5 to run normally through the UI since it has some document upload, download, and signing testing that fails when running headless. Is this possible?","['It all depends on how you initialize your browser, since you cannot turn headless on after the browser instance exist.', 'You could add a parameter to your XML file under the methods that need to be in headless mode.  Then use the ITestContext Class to get the current XML test and that parameter value when loading your driver in a @BeforeMethod.']"
Webdriver loads altered version of the webpage,https://www.reddit.com/r/selenium/comments/wwr35c/webdriver_loads_altered_version_of_the_webpage/,selenium,"  Hi! I am learning selenium but I ran into a very unusual case for the first time. I want to find an element and send keys to it but webdriver loads altered version of the webpage. When I inspect the page in webdriver instance the structure of the code as well as class names are different from the code given in Chrome Browser when inspected therefore my xpath won't work. Any idea what could be causing this? I am using python(+pytest, webdriver manager)","[""It's weird, without more informations I really don't know. If the structure of html changed, it means the page has to look different, so maybe you are using different resolution, sice many pages are build to adapt to desktop, tablets and mobiles resolutions. Try to set height and width for the driver you are using."", ""Is it possible that the site is serving you different versions based on agents or other factors?\n\nWhen I encountered this I set my agent to something very specific. May not work for you as don't really know the sites code base or setup.""]"
How to click on a word open to a hidden item?,https://www.reddit.com/r/selenium/comments/wwnxhs/how_to_click_on_a_word_open_to_a_hidden_item/,selenium,"Hi guys, I tried to do an automation for a webpage. Forgive me because I am really new to this. There is a word 'Spend' that click in open a hidden item call 'My Dashboard'. Here is the code below.

<li id=""menu1"">

<a class="""" name="""" title=""Spend"" style=""font-size: xx-small; color: white; font-family: Arial,Helvetica,sans-serif; font-size: 9pt; font-weight: bolder;"" id="""" target="""" disabledby="""" disabledvalue="""" href=""#null"" enabledhref=""#null"" disabledhref=""""><span datasrc="""" datafld="""" dataformatas=""HTML"">

Spend</span></a>

<ul style=""display: block;"">

<li>

<a class="""" name="""" title=""My Dashboard"" style=""color: black; font-family: Arial, Helvetica, sans-serif; font-size: 8pt; background: rgb(255, 255, 204);"" id="""" target=""mainFrame"" disabledby="""" disabledvalue="""" href=""../common/submenu\_tabs.cfm?parentid=12663\&amp;\_Key=6482AB90889D539090861088310A837FC6E35781FC48AB069D9BCE06F9CA94"" enabledhref=""../common/submenu\_tabs.cfm?parentid=12663\&amp;\_Key=6482AB90889D539090861088310A837FC6E35781FC48AB069D9BCE06F9CA94"" disabledhref="""" onclick=""SelectItem(this, '12663', false);""><span datasrc="""" datafld="""" dataformatas=""HTML"">

My Dashboard</span></a>

</li>

</ul>

</li>

here is my code which login in.

button = browser.find\_element([By.ID](https://By.ID),""UserName"").send\_keys(username)

button1 = browser.find\_element([By.ID](https://By.ID),""Password"").send\_keys(password)

time.sleep(1)

button2 = browser.find\_element([By.ID](https://By.ID),""ext-gen36"")

[button2.click](https://button2.click)()

time.sleep(5)

link = browser.find\_element(By.XPATH,""//form\[@id='main'\]"")

[link.click](https://link.click)()

I am really appreciate for the help.","[""On what exactly do you want to click? I don't really understand it ü§î..\n\nAlso don't use the time.sleep(), it makes a lots of problems. Way better is to use WebdriverWait functions which selenium offers. https://selenium-python.readthedocs.io/waits.html#:~:text=Selenium%20Webdriver%20provides%20two%20types,trying%20to%20locate%20an%20element.""]"
See and interact with the page ?,https://www.reddit.com/r/selenium/comments/wvz8mm/see_and_interact_with_the_page/,selenium,"Hello, is there a way to interact with a page manually say for example if one finds a captcha on it, then return to script for further processing ?","[""If you mean to pause the code for manual interaction then yes, you can. Just look for the captcha element and if selenium finds it, just use some kind of breakpoint depending on your language. \n\nIn Python it would be something like\n\n    def is_shown(locator):\n        try:\n            driver.find_element(By.you_choose, locator) #import by and choose what you need\n            return True\n        except NoSuchElementException: #import the exception first\n            return False\n\n    if is_shown('captcha locator'):\n        breakpoint()""]"
Issue with pop-up window,https://www.reddit.com/r/selenium/comments/wvlqiw/issue_with_popup_window/,selenium,"Hi all...  


I've got an issue I'm struggling to solve using the Selenium Python library.  
I have one test setup and working fine, however my second test (which should be the same as the first) is failing due to an element, in fact any element, not being found on a pop-up window.

Here's what works in my working test (`main_page` is defined earlier in the script, and the `print(signin_window)` shows a different window handle to the main window):

`for handle in driver.window_handles:`  
 `if handle != main_page:`  
 `signin_window = handle`  
 `print(signin_window)`  
`try:`  
 `driver.switch_to.window(signin_window)`  
`except:`  
 `print(""Cannot switch to popup"")`  
`try:`

 `driver.find_element(By.ID, ""username"").send_keys(""myusername"")`  
 `driver.find_element(By.ID, ""password"").send_keys(""mypassword"")`  
 `driver.find_element(By.ID, ""login-submit"").click()`  
`except:`  
 `print(""Cannot Login"")`

The exception ""Cannot switch to popup"" does not print, so I guess that the new window is being selected, however the next stage fails.   
I've tried just adding `driver.find_element(By.ID, ""myMainDiv"")`instead, which is the popups main container, and it triggers the exception, so it seems it's not finding any elements on the popup at all.  
My first thought would be that it's not selected the popup as it can't find any of the elements, but as the popup exception doesn't trigger, I'm not sure thats the case  


Is there any other way of debugging this to make sure I'm on the correct window or am I missing something else?  


Thanks!","['Solved it.   \nJust added `time.sleep(5)` before entering the login info, and boom!']"
How do I find an element that is in a drop-down menu that appears after text is inputted.,https://www.reddit.com/r/selenium/comments/wv3euj/how_do_i_find_an_element_that_is_in_a_dropdown/,selenium,"When you go to [instagram.com](https://instagram.com) and type something into the search bar, a menu pops up right below the text box. I want to build a bot that displays some of the contents of that menu.

How do I do this?","['Think about exactly what happens on the page: you type something in and it takes a couple of seconds till the Dropdown is displayed.\n\nIn your code, you go to this webpage, wait till the search bar is loaded, start typing, wait till the dropdown is loaded, select the Dropdown, print out the text. \n\nYou can use implicit or explicit waits for this.']"
Is there an iPhone app that works like Selenium?,https://www.reddit.com/r/selenium/comments/wvajvj/is_there_an_iphone_app_that_works_like_selenium/,selenium,,"['You want to code on your iPhone?', 'Take a look at Appium https://appium.io/']"
Web Data Extraction Summit 2022,https://www.reddit.com/r/selenium/comments/wtyznp/web_data_extraction_summit_2022/,selenium,"Hey folks!

Zyte has recently announced Web Data Extraction Summit will take place in London this year. Are you planning to attend this conference? It‚Äôll be nice to meet some of you folks. Event Website: [https://www.extractsummit.io/](https://www.extractsummit.io/)","['Ooh nice, I wish I could, I am a online data acquisition expert and I use Zyte proxies alot.']"
How to get an element with selenium in VBA,https://www.reddit.com/r/selenium/comments/wtjb4p/how_to_get_an_element_with_selenium_in_vba/,selenium,"I'm trying to do a macro and i cant find a solution for a problem, i'm using Selenium to get data from a website, here is the website:

    <ul class=""milestones""> 
    <li> 
    <img src=""="" title=""Red"" data-pagespeed-url-hash=""820105347"" onload=""pagespeed.CriticalImages.checkImageForCriticality(this);"">
    <span class=""out"">**time** 20/08/2022 12:46</span> <strong>**status** Objeto aguardando retirada no endere√ßo indicado</strong> 
    <br> 
    **where it's** Ag√™ncia dos Correios - CONTAGEM/MG 
    <br> 
    <small>6&nbsp;horas, 2&nbsp;minutos atr√°s</small> </li> 

I want to get the time, status and where it is. the first too I was able to get using:

    Cells(linha, 12).value = navegadorChrome.FindElementsByClass(""milestones"")(1).FindElementsByTag(""span"")(1).Text     
    Cells(linha, 13).value = navegadorChrome.FindElementsByClass(""milestones"")(1).FindElementsByTag(""strong"")(1).Text 

The last I cant do, it's the part after the first <br>.

Any help will be appreciated","['Selenium needs to know which web element you are working with. The <br> tags are not surrounding the target text as they stand alone in HTML. There is no beginning BR tag and ending BR tag.\n\nSelenium needs to have the identity of a web element because it looks for the beginning and ending of the web element in order to access its contents.\n\nThe text that you seek is not within any tags of its own. It lives in the li web element.\n\nAnother problem you have is that all of the text in the li web element will be shown as part of the li web element. This includes the text in the SPAN, STRONG and SMALL tags.\n\nIf it were me, I\'d grab the whole li element using ***FindElementByTag(""li"")*** and then use VBA\'s InStr (instring) function  to find the first and second instances of ""<br>"" and extract the text between them from ***YourWebElement.Text*** .\n\nLet me know if I have not been clear enough.\n\nHere is an excellent tutorial on selectors in VBA and SeleniumBasic - [https://www.youtube.com/watch?v=lr7CFZEI2YA](https://www.youtube.com/watch?v=lr7CFZEI2YA)']"
Can Selenium give me CSS properties based on some text I find in the htmlsource?,https://www.reddit.com/r/selenium/comments/wsg32w/can_selenium_give_me_css_properties_based_on_some/,selenium,"Using Python and Selenium, I am struggling to get CSS properties quickly and easily based on matched text from a search.

In this instance, I want to search a webpage for all instances of $ occurrences, then, from whatever element they're found in, get the font-weight for those instances.

I cannot seem to do this without it being a very long and slow process.

Using beautiful soup doesn't help, as that can find the elements, and give me the class name, but then the ""computed"" css value for the element may differ from what the class name gives me.

I can search the html source and find instances of the $ character, I can then get each match and put this into a find\_elements method, the problem is this is very, very slow and resource intensive, particular if there are many (like 50 or more) instances of $ characters in the source.

Is there something simple I'm missing here? I've also tried Reg Ex search within XPATH, but apparently XPATH1.0 does not properly support this.

Any help is much appreciated.","['You can use driver.find_elements_by_xpath(""//*[contains(text(),\'ABC\')"")\n\nand then iterate over the result and use value_of_css_property(property_name)', 'It would be much faster if you did all the css value getting within the browser JavaScript. Every time you get a value from python it has to make a wire call. Create a JavaScript script that will do the same thing and just execute it and return the values']"
Need help,https://www.reddit.com/r/selenium/comments/wrx6ln/need_help/,selenium,"Hey everyone,

Recently I have wanted to learn how to code and automate web applications through selenium. Would anyone be able to provide me with a tutorial that shows me how to install the proper python library and web driver for google chrome?","['Let me know if [this](https://youtu.be/Xjv1sY630Uc) is of any assistance. 2 years old but still holds same steps\n\nAny trouble shoot me a PM', 'I was having trouble with managing different versions on my computer, so I used conda to install selenium and switched to the conda environment within vs code. The selenium website does a great job explaining the install, but if you have trouble, just use conda.']"
timeout exception issue,https://www.reddit.com/r/selenium/comments/wrj0gh/timeout_exception_issue/,selenium,"Ho guys I am trying to automate the download of edx course and I want to iterate all video clicking on next button and download each video but the click get me no such element so i use webdriverwait but throw me timeout exception how can i resolve this?

Edit. I manager ti solve the problem using execute_script webdriver metod and selcting the button with JavaScript code","[""It's probably in an iframe."", ""It's either in a iframe or you are using wrong locator (or bad syntax, some general error is XPath attribute without @, or missing parentheses, but it's hard to say without the code.""]"
Handling failures,https://www.reddit.com/r/selenium/comments/wrhjoq/handling_failures/,selenium,"Hi all...  
I'll caveat this with the fact I only started with Selenium yesterday, and I'm not really all that great with Python either, so please forgive me if I don't make a whole lot of sense.

I've written a simple test with Python that is checking the things I need to check (opens Chrome, loads page, handles marketing popup, checks a login, signs out, closes Chrome) and it seems to work fine, but I'm wondering what I can do to handle failures.  
Long term, I want to run a browser test, take a screenshot if there is a failure, and also send an email if any part of the test fails.  


I'm trying to understand if something exists in the Selenium library that would do this, or if this needs to be part of the Python script, if that makes sense?

Do I need to run each section of the test as a function of some sort, that will only continue if it passes?  
Do I need to look specifically at Python exception handling?  
Have I just answered my own question?  


Literally any pointers would be useful, apologies for my n00bness.","['https://www.w3schools.com/python/python_try_except.asp\n\nTake a look on this.\n\nYou will write the screenshot/e-mail in the except part, basically']"
Can someone explain this find elements behaviour?,https://www.reddit.com/r/selenium/comments/wrg9pc/can_someone_explain_this_find_elements_behaviour/,selenium,"My code will search through the html source for any instances of a particular reg ex pattern.

When it finds one, it will loop through all elements based on an xpath search for the matched reg ex pattern.

    What I'm finding weird, is one such pattern appears once on the page, yet the the xpath loop finds 9 instances of elements. Code below:
    
    for regexmatches in regexpattern.finditer(htmlsource):
    
    	expr = (""¬£"" + regexmatches.group())
                    
    
        	for i in driver.find_elements(""xpath"", '//*[contains(normalize-space(), ""' + expr + '"")]'):

If i put counters below both the for statements, the first counter may be 80 or so, the second in the high hundreds.

Why would this be?",[]
Taking screenshot of only relevant content of webpage | Selenium | Python,https://www.reddit.com/r/selenium/comments/wqssa7/taking_screenshot_of_only_relevant_content_of/,selenium,"How can I take screenshot of only relevant content of any webpage using Selenium and Python?

[I want to take the screenshot of the marked content (specifications) in this photo instead of whole page](https://i.stack.imgur.com/vlH7p.png)

[Example webpage link](https://www.startech.com.bd/benq-gw2480-fhd-monitor)

Currently I'm taking screenshot of the whole page. Also I want to avoid referencing any class or id while taking the screenshot. Please let me know if I can achieve this (if yes, HOW?) or have to change my requirements. If there is any workaround such as cropping the relevant content, please do share too. Thanks.","['Webdriver 4 has support for taking screenshots of elements', 'https://www.qed42.com/insights/coe/quality-assurance/how-take-screenshots-selenium-4']"
Where to store locators (Python),https://www.reddit.com/r/selenium/comments/wqb7sh/where_to_store_locators_python/,selenium,"Hi there, so I‚Äôm pretty new to Selenium but am hoping to avoid some bad practices, or as their team would say: following ‚Äúencouraged behaviors‚Äù.

For my company, I pretty much have to rely on a bunch of xpaths. I‚Äôve been organizing page sections into component objects just because each page has distinct sections that follow the same respective markup. I‚Äôm not using PageFactory (or pythons equivalent if it exists) because I don‚Äôt know enough yet. Unfortunately I cannot post my code here because of confidentiality issues, but I‚Äôm including a generalized example that follows what I‚Äôm doing. I was hoping somebody could tell me if this is a poor way to handle the necessary locators

```

class SomeClass:
  
    xpath_dict = {
        key1: ‚Äú//blahblah‚Äù,
        key2: ‚Äú//blahblah‚Äù
        ‚Ä¶
    }


    def __init__(self, driver):
        self.driver = driver


    def get_element_dict(self)
        return {key: self.driver.find_element(By.XPATH, xpath) for key, xpath in SomeClass.xpath_dict.items()}

```",[]
staying logged in on each new session,https://www.reddit.com/r/selenium/comments/wq00aq/staying_logged_in_on_each_new_session/,selenium,"Every time I try using salesforce or any site with authentication, I'm met with either a login page, or a login AND an email verification. What's the best way to stay logged in? I've been scrolling through Google and stackoverflow with no success. 

I'm using a macbook and my chrome profile( in case that matters)",['cookies is what you need\n\n\nhttps://www.guru99.com/handling-cookies-selenium-webdriver.html']
dynamically change folder for download.,https://www.reddit.com/r/selenium/comments/wpb0ql/dynamically_change_folder_for_download/,selenium,"Is there a way to dynamically change the download path? The idea is to go through a series of titles, creat the folders for such titles and then download items belonging to that title in its folder. Is there a way to do that using selenium?","['You can just do it for whatever language you are using.\n\nI do it for creating a screenshot folder.\n\nI also create a custom folder for Chrome for any downloads that occur.\n\nhttps://stackoverflow.com/questions/36327240/create-directory-if-not-exists', 'you could get the file URL using selenium and then make an HTTP request  to download the file to a specific location. \n\nIf you need to click to trigger download then you could download to the default download location set in the browser and then copy the file to the other location and delete initial download.']"
Automatic update of chromedriver,https://www.reddit.com/r/selenium/comments/woswjj/automatic_update_of_chromedriver/,selenium,"Hi, I use selenium to download latest chrome and firefox and then I auto update them. Unfortunately, when the Chrome is updated to a new version, a new chromedriver is requied for selenium to work with chrome. How do you guys solve this? My current idea is to download the newest version and store it, then have a check for what version my chrome is so when the stored chromedriver and chrome have the same versions I replace my current chromedriver with the stored one. Anyone who handles this in a smoother way?

Edit: This solves this problem: https://pypi.org/project/webdriver-manager/","['You can try using WebDriverManager Library. Just google it', 'I also use web driver manager - works great  \nAlthough they did have some issues at the start with Edge', ""Was wondering if you have a lot of drivers to update when it happens? I just update my chromedriver manually which only takes a minute, so wondering why a library was created to auto update driver(s) if it isn't a hassle in the first place.""]"
Open source web scraping tools - some of them are actually not bad!,https://www.reddit.com/r/selenium/comments/woyu3t/open_source_web_scraping_tools_some_of_them_are/,selenium,"Personally I feel 5 out of 10 being Python-based is a reach but I assume you guys would disagree

[https://medium.com/codex/the-top-10-open-source-web-scraping-tools-in-2022-ede0134a00d](https://medium.com/codex/the-top-10-open-source-web-scraping-tools-in-2022-ede0134a00d)",[]
Help!!! + Cannot find Password Input Field using Python Selenium Webdriver,https://www.reddit.com/r/selenium/comments/wogile/help_cannot_find_password_input_field_using/,selenium,"Background: I have about two years of fiddling around with selenium. However, do not have much experience with css or html (web design) outside selenium.

Issue: I want to generate an email account. However, when I get to the password page, I am unable to find the Password input box to select the password for the account.

Here is the html code I found with inspect element. I want to find the password box and send it my password.

<input class=""form-control email-input-max-width"" type=""password"" id=""PasswordInput"" name=""Password"" aria-describedby=""PasswordDesc PasswordError"" data-bind=""css:

{

'has-error': showError(password)

},



textInput: password,



hasFocus: password.focused() \&amp;\&amp; !showPassword(),

moveOffScreen: showPassword(),

event: { keyup: onPasswordKeyUp },

ariaLabel: strings.ariaLblPassword,

attr:

{

'placeholder': strings.ariaLblPassword

}"" tabindex=""0"" aria-label=""Create password"" placeholder=""Create password"">

&#x200B;

Stuck and dont know what todo. Thanks in advance, I appreciate the help.","[""That element has an ID of PasswordInput. You should be able to use selenium's FindElementById function to use that ID and locate the element.\n\nIs it possible for you to link to the webpage in question? That'll help a lot to determine if there's anything weird going on that would prevent Selenium from locating that element normally."", ""Can you send the error message?\n\nThere are three common errors when it comes to locating an element.\n\nFirst: wrong locator. As someone already said, this element has an ID attribute so use the find_element_by_id('Passwordinput') function.\n\nSecond: If that doesn't help and selenium shows NoSuchElementException, try to use the WebdriverWait functions since the element isn't fully loaded while selenium is searching for it.\n\nThird: if WebdriverWait functions doesn't work and you are getting the Timeout Exception, it means the input is either in iframe, so you'll have to switch to the iframe first or it has some kind of overlay and selenium doesn't see it. There are few workarounds and using action chain to move you mouse to the coordinates of the input is one of them.\n\nBut my advice is to use breakpoint and try to debug it yourself. This is not the last problem that you'll face and breakpoint is you best friend."", '""How to type in textbox using Selenium WebDriver with Java?"" https://www.tutorialspoint.com/how-to-type-in-textbox-using-selenium-webdriver-with-java']"
How To Scrap Network Type 'XHR' / 'Fetch' Data In Selenium 4?,https://www.reddit.com/r/selenium/comments/wocp4z/how_to_scrap_network_type_xhr_fetch_data_in/,selenium,"## My goal ##

Im trying to scrap raw video stream data (.ts files) from [twitch.tv][1] using Selenium 4.
All live streams are fed in chunks of video,
I can access them manually by:

1. opening a chrome tab with a running [twitch.tv][1] livestream
2. open DevTools (F12)
3. go to Network tab > XHR
4. The stream of .ts (transport stream) files being fetched are my desired files.
5. I can just doubleclick on them and chrome downloads this small video chunk file.


I want to reproduce this using Selenium 4 but I have no experience with Web Programming (POST, Flow etc). My current programm 
is able to scrap image files. But once the response received is of .ts file (XHR/Fetch) it returns.

>DevToolsException: {""id"":11,""error"":{""code"":-32000,""message"":""No data found for resource with given identifier""},""sessionId"":""79BA2C212FABA878DB3524D7D0F49BDC""}


## I have tried ##
Calling [Network.getResponseBody][2] when the [Network.loadingFinished][3] event has fired but this also doesn't work. There is never the same requestID on either event.

Remarks: Im aware there is a Twitch API.

    public static void main(String[] args) {
        
        InitializeSeleniumDrivers();
        driver.get(""https://www.twitch.tv/thebausffs"");
        
        
        DevTools devTools = ((ChromeDriver) driver).getDevTools();
        devTools.createSession();
        devTools.send(Network.clearBrowserCache());
        devTools.send(Network.setCacheDisabled(true));
        devTools.send(Network.enable(Optional.empty(), Optional.empty(), Optional.of(100000000)));


        
        devTools.addListener(Network.responseReceived(), responseReceived -> {

            RequestId requestId = responseReceived.getRequestId();
            
            try {
                Command<Network.GetResponseBodyResponse> getBody = Network.getResponseBody(requestId);
                Network.GetResponseBodyResponse response = devTools.send(getBody);
            } catch (DevToolsException e) {
                e.printStackTrace();
            }

        });
    }



## Headers Example ##

**GENERAL**
```
Request URL: https://video-edge-c55dd0.ams02.abs.hls.ttvnw.net/v1/segment/CrEFZRTkEBMVDg5w4Ygn2pwqXKLGK5NAUAQ7ZWHeCORCjjFxfh9McgTBm_DTCvfP1MrZIg1jb2-oo2769tLAjFKjUd4AQaKtV3LeTEpPJyB_7ZAgolK-dSlLAqnC1xaI7z6iJCC4W1fb5RkkJmLk2D5nYEpyA17gSqe1eoB5zYsrDnal6Sm__B5LhxzOwTPOKI66jxXeIThm8tpaFGabccyd8AcT7RIfqCRv9Jas-IMQCqnBLLpIjk5rC-n4USQzLI6R4xGeTyTwMgX3BQ7EcxB-X62kUvsJm2O7Q2iJEI-ongDyyFRCapzo8iBtGgN2ruxvp8SeCKHO8j9NbS4jymG276ZigtnDXEQbxa6f5i9dHEcf9g1ump4RZtd48eOv6bPsGCDhFfULRd8adcM369ew90NrzyYbImQZnhFcnyqvfYIlCg-FFyjqJHVz37MZGc7TLbSh1YqmrkAClamXb8fFPGCXpsIrY-IDmKgTxh8tEmjbdacBWsKxxwJAOv-H6MUZB67MP1KMeT94YMjGXBcIjJo4JKeFCKoITCLJI4jjzqNmFa_efdlaJ89mUodxQRHJARV3qwdp04TSvZALBbOua6m-0T-01lOEYlr6w408mr5araj7c7gjpvrj_83jb0wqJG7ala1DBUg0U0Vx2rQxzumokyz66MxfMJy3ZSY92L-JdS47RjcOpilnpTI9bI8RPRyY4grds2SHDudWxgp-jJWgHdtbbFpuDCZENwOuU_-Agsf0lA_g59KnXnAuz59yovCO2C_O8ptkyoImgZ47qBPBIn-DDD-rzJloGD-GTQn4zGlmAFcg6GunjeW3PbHjKjMz8vA_K8NOF7ofO94YOtj_1khbCFGfH2_dF8zDwMSieR5Mvg7upQdzwgl_GAmf7OIAbHXwA1DqamnbAeWundcaDEM8dWDJF-pfTicm0CABKglldS13ZXN0LTIwtwQ.ts
Request Method: GET
Status Code: 200 OK
Remote Address: 185.42.204.31:443
Referrer Policy: strict-origin-when-cross-origin
```

**RESPONSE HEADER**
```
Accept-Ranges: bytes
Access-Control-Allow-Origin: *
Cache-Control: no-cache, no-store, private
Content-Length: 1589164
Content-Type: application/octet-stream
Date: Sun, 14 Aug 2022 16:56:31 GMT
```



**REQUEST HEADER**
```
Provisional headers are shown
Learn more
Referer
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36
```


  [1]: https://www.twitch.tv/
  [2]: https://chromedevtools.github.io/devtools-protocol/tot/Network/#event-responseReceived
  [3]: https://chromedevtools.github.io/devtools-protocol/tot/Network/#event-loadingFinished",[]
Selenium stops detecting element on second iteration of for loop in python,https://www.reddit.com/r/selenium/comments/wo7jid/selenium_stops_detecting_element_on_second/,selenium,"I wrote a python code that uses selenium to input values from a list into google search, click on the first result and extract information from the website. The first iteration of the loop runs perfectly and extracts exactly what I need but the second iteration runs into trouble. The code is unable to locate the first result (let alone click it) even though the XPATH does not change. I have tried to use both time.sleep and WebDriverWait but none of these work (I just get a timeout exception). Is there something obvious that I‚Äôm missing? As I mentioned there are no changes in the websites structure in terms of classes, xpaths or ID and I‚Äôm really clueless as to why it happens.","['Post code snippet and screenshot of dom structure?', ""Might help if you can post the code. I know I had something similar where I was using a loop to click on the elements and after the first one the second would fail. I rechecked everything still couldn't find the issue. Later someone told me if I initialize the loop and say while your test is getting executed the page refreshes, the second instances of the loop would need an initialization again. That was it and the loop started working for me."", 'If it can‚Äôt locate the element then it‚Äôs not there. When you reload the page are you clearing the input before inputting your new x value? Hard to say for sure if the google input field is being prefilled which would give you different search results than you expect.']"
why is selenium chrome webdriver running with high data usage ?,https://www.reddit.com/r/selenium/comments/wnorw9/why_is_selenium_chrome_webdriver_running_with/,selenium,"It's for Gmail auto login after running for 2 days it consumed **130GB of my quota!**, any suggestions for adding a line or disabling something to reduce that enormous **data usage**?","['It shouldn‚Äôt, it might be the type of emails you are loading', 'Google headless mode']"
Can I use Selendroid to automate a test in the official Instagram app?,https://www.reddit.com/r/selenium/comments/wn9qtn/can_i_use_selendroid_to_automate_a_test_in_the/,selenium,"Hi community. I¬¥m starting to read about Selendroid and I don¬¥t get if it can be used for automate a test in, for example, the official Instagram app. (As long as I understand it can only be used with apps you are developing... or am I wrong ?)

Thank you !",['You need the Android to be in developer mode \nYou. Need to know the hooks name (good luck without debug builds).  \n\nI would say no no you can‚Äôt (you can but you will struggle. Struggle a lot.']
Get HAR using dev tools,https://www.reddit.com/r/selenium/comments/wmqpho/get_har_using_dev_tools/,selenium,"Hi there!
I'm wondering if it's possibile to get the har out of chrome (and Firefox?) in a clean way (without proxi).
Currently I'm using a js injection that recreates it but got some csp blocks (so needed to find a way to disable csp with an external extension).

Is that possibile?
Thanks","['Hi, its possible with undetected-chromedriver, i use it everyday to get XHR network responses.\n\nBut its in Python. [https://pypi.org/project/undetected-chromedriver/](https://pypi.org/project/undetected-chromedriver/)\n\nI can help with python if you need me.\n\nthank you', 'playwright can do this, I webdriverio can harness devtools for something similar \n\nhttps://playwright.dev/docs/network#record-and-replay-requests']"
"Do I need to add the ""Mobile Emulation"" capability to be able to emit touch events?",https://www.reddit.com/r/selenium/comments/wlazax/do_i_need_to_add_the_mobile_emulation_capability/,selenium,"Just asking to see if the step of adding the capability to a customRemoteWebDriver which extends RemoteWebDriver.

My question why do we need to add this capability to the driver in order to emulate touch events? Can't I just add it completely on me own?",[]
Using Selenium within a webpage,https://www.reddit.com/r/selenium/comments/wle9lo/using_selenium_within_a_webpage/,selenium,"I need to make a webpage that has a username and password field. After the user enters their information I want to go to multiple websites (headless) to see if that username and password work on any of those sites. 

I'm able to do this with tkinter and python, but the client wants it as a webpage. From my findings Selenium won't work for that situation, is that true? Is there something like Selenium that I can use for this situation?",['You can trigger a selenium job running on Jenkins or something similar from your web page.']
How to deploy selenium with firefox on Heroku?,https://www.reddit.com/r/selenium/comments/wl8brr/how_to_deploy_selenium_with_firefox_on_heroku/,selenium,"Putting my head through a wall because whenever I attempt to deploy firefox or chrome w/ selenium on a heroku app (using Ruby) I get an error saying binaries are missing. This is even after I deploy the geckdriver and firefox buildpacks from buitron and it doesn't resolve the errors. For example, when I run the following command:

    browser = Watir::Browser.new :firefox, headless: true

I get an error like the following (and something similar happens when I try with chrome). Any thoughts?

    Selenium::WebDriver::Error::SessionNotCreatedError (Expected browser binary location, but unable to find binary in default location, no 'moz:firefoxOptions.binary' capability provided, and no binary flag set on the command line)","['Have you tried installing chrome as well? Not only chromedriver', 'How are you installing the browsers on the Heroku servers?']"
Is Selenium or Puppeteer good for Browser Automation?,https://www.reddit.com/r/selenium/comments/wkv6vp/is_selenium_or_puppeteer_good_for_browser/,selenium,"Hello!

I'm new into programming and I had a question regarding the use of Selenium. As part of something fun to do, I was thinking of automating a Google search based on user input and then listing the results.

Basically, if a user searches typical book, it searches Google for it (automation linked maybe) and returns the result. I know it sounds they can do it themselves but I'll start off from here and add multiple other functions to it.


Is Selenium good for this or Puppeteer? I've heard both are automation tools but I don't know which one would be better in this case, as after searching much, I've seen many places say Selenium is good for 'testing'.

I don't have any issue with multiple browser or just a single browser atm since Puppeteer only works on Chrome.

Any help would be appreciated.

Thanks a ton! :)","[""There's hardly a case where selenium is better. Selenium had been on the market for so long that it has a bigger community and many adapters. That's for a plus. It's also fully cross browser cross platform. \nI also started test automation from selenium but more than year ago I moved first to pptr then to playwright and they're simpler, easier to learn, bootstrap and they make you finally  focus on writing automation code rather than seeking workarounds for many  things your just can't do with selenium (like check a response for status after click or test different locales, time zones, download files etc )\n\nIf you're new to this, check out playwright. It's newer and better solution than puppeteer, it also provides everything you need for automation as well as testing"", 'I used Selenium and now using Playwright (So no Puppeteer from me). Both are ok for Browser Automation. \n\nI found Playwright a little bit easier (my codes are shorter with it) but Selenium has a much bigger user number which means you would get better support for Selenium.', 'Searching Google is the first example in the Selenium doc, so, yeah ü§£']"
"Python - Can't find element by class name: ""NoSuchElementException""",https://www.reddit.com/r/selenium/comments/wl3way/python_cant_find_element_by_class_name/,selenium,"I'm trying to scrape the Webpage [https://www.instagram.com/buckwild/](https://www.instagram.com/buckwild/) and have identified the class name of the element I would like to target.

However, when I call

`driver.find_element(By.CLASS_NAME, ""_ac2a"")`

I get:

`selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element`

I can prove I've written the class name down correctly, so now I'm wondering why Selenium can't find the element.","['url?', 'I found a solution:\n\nSimply wait until the element exists in the DOM before attempting to access it:\n\nfrom selenium.webdriver.support.wait import WebDriverWait\n\n```\nwait = WebDriverWait(driver, 30).until(\n  \nEC.presence\\_of\\_element\\_located((By.CLASS\\_NAME, ""\\_ac2a"")))\n```', 'Facebook doesn‚Äôt want you to use bots on Instagram.']"
I‚Äôm willing to pay to a selenium tutor please help,https://www.reddit.com/r/selenium/comments/wkvdoq/im_willing_to_pay_to_a_selenium_tutor_please_help/,selenium,"I want a code that finds the best Aliexpress product in a specific niche(for example home sport training), based on different variables, like likes count.  the reason is i want to send links of those products to my audience(affiliate marketing), and it takes me too much time to do it myself.  can someone teach me to make a program that would actually save my time?","[""Start piecing together a script that does what you want using Google and look for the green checkmark on stackoverflow results and tweek them for your use case. Pretty soon you'll have something that does mostly what you want and might have learned something along the way too!"", 'If you utilize Katalon Recorder or Selenium IDE you can generate the basis of a script (i.e. using the Export feature).', 'Best and AliExpress... Seems like an oxymoron to me. üòÖ', 'Get all the products and sort them based on your criteria.']"
Getting text in span class,https://www.reddit.com/r/selenium/comments/wkrcp1/getting_text_in_span_class/,selenium,"Hello, I want to get the text inside of a span class. When I right-click and copied the CSS Selector or XPath and trying to get the text with

 driver.findElement(By.cssSelector(""#comp-kvi6khho > p:nth-child(1) > span:nth-child(1) > span:nth-child(1)"")).getText()

this, I get error unable to locate element. I also tried to do it with xpath instead of cssSelector with using .getAttribute(""InnerHTML""); but didn't work. Same error. The HTML code are as follows: 

    div id=""comp-kvi6khho"" class=""select_wrapper""> <p class=""select_display hovered"" style=""line-height:normal; font-size:18px;""> <span style=""letter-spacing:normal;""> <span class=""selectLabel"">UPS Overnight - Free</span> 

How can I get the text inside of most inner span class? All helps are welcomed. Thanks in advance.","['If By.cssSelector(""span.selectLabel"") is not working, the element might not be present at the exact moment that you\'re trying to locate it. Try using explicit wait.', 'you can use className locator why to use css slector or xpath.\n\ndriver.findElement(By.className(""selectLabel"").getText()', 'Is it solved yet ? If yes please do share the solution']"
Selenium can't find element with ID/Name,https://www.reddit.com/r/selenium/comments/wjv59o/selenium_cant_find_element_with_idname/,selenium,"Im trying to challenge myself by making selenium redeem 1 gamepass code on microsoft issue is I Found the ID but it doesn't work as in Selenium can't find it, [This](https://account.microsoft.com/billing/redeem) is the website I need selenium to recognize and type in it 

&#x200B;

this is the error

Traceback (most recent call last):

  File ""c:\\Users\\jeans\\Downloads\\New folder\\Microsoft\\[redeem.py](https://redeem.py)"", line 30, in <module>

gamepass = driver.find\_element([By.ID](https://By.ID), value=""tokenString"")

  File ""C:\\Users\\jeans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\[webdriver.py](https://webdriver.py)"", line 857, in find\_element

return self.execute(Command.FIND\_ELEMENT, {

  File ""C:\\Users\\jeans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\[webdriver.py](https://webdriver.py)"", line 435, in execute     

self.error\_handler.check\_response(response)

  File ""C:\\Users\\jeans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\[errorhandler.py](https://errorhandler.py)"", line 247, in check\_response

raise exception\_class(message, screen, stacktrace)

selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""css selector"",""selector"":""\[id=""tokenString""\]""}

  (Session info: chrome=104.0.5112.81)

Stacktrace:

Backtrace:

Ordinal0 \[0x00FA78B3+2193587\]

Ordinal0 \[0x00F40681+1771137\]

Ordinal0 \[0x00E541A8+803240\]

Ordinal0 \[0x00E824A0+992416\]

Ordinal0 \[0x00E8273B+993083\]

Ordinal0 \[0x00EAF7C2+1177538\]

Ordinal0 \[0x00E9D7F4+1103860\]

Ordinal0 \[0x00EADAE2+1170146\]

Ordinal0 \[0x00E9D5C6+1103302\]

Ordinal0 \[0x00E777E0+948192\]

Ordinal0 \[0x00E786E6+952038\]

GetHandleVerifier \[0x01250CB2+2738370\]

GetHandleVerifier \[0x012421B8+2678216\]

GetHandleVerifier \[0x010317AA+512954\]

GetHandleVerifier \[0x01030856+509030\]

Ordinal0 \[0x00F4743B+1799227\]

Ordinal0 \[0x00F4BB68+1817448\]

Ordinal0 \[0x00F4BC55+1817685\]

Ordinal0 \[0x00F55230+1856048\]

BaseThreadInitThunk \[0x76CEFA29+25\]

RtlGetAppContainerNamedObjectPath \[0x779B7A9E+286\]

RtlGetAppContainerNamedObjectPath \[0x779B7A6E+238\]","[""You either use wrong locator or element isn't loaded before you try to locate it. Use WedriverWait functions.\n\nhttps://www.selenium.dev/documentation/webdriver/waits/#tabs-5-1""]"
Script that works in Firefox visible browser but fails when headless,https://www.reddit.com/r/selenium/comments/wj6biz/script_that_works_in_firefox_visible_browser_but/,selenium,"I'm using selenium in python, and the title summarizes my problem. What is likely to cause this, and is there some way to overcome it? My script must run in a docker container, so it must be headless.","[""Certain aspects of web pages behave differently when run in a headless browser, generally visually-interactive components like media players, but it can apply to a range of things.\n\nUnfortunately there isn't a straightforward solution to getting the page to run successfully in headless. However, running in docker does not necessitate running selenium headless. I would look into solutions for running non-headless selenium within a docker container."", 'I have seen this kind if issues when the authentication fails in headless mode']"
Selenium Grid Node question from Cloud Virtual Machine to Home PC,https://www.reddit.com/r/selenium/comments/wiy40s/selenium_grid_node_question_from_cloud_virtual/,selenium,"I am trying to setup Selenium Grid using my PC as the hub and a cloud VM as a node. I cannot connect to my PC from my cloud VM. I search for an answer but I keep on getting entries for connecting to my cloud virtual machine FROM my PC, not the other way around? Suggestions would be great on getting this configuration to work.","['Check your router for the correct ports being open.', 'It is difficult because they are two different networks.\nI just tried to do this but was unsuccessful.\n \n\nIn reality, it is not the best way to build a hub grid as you ask.']"
Selenium opening blank tab with 429 error when only making on request,https://www.reddit.com/r/selenium/comments/wifw8h/selenium_opening_blank_tab_with_429_error_when/,selenium,"When I open this URL with webdriver in selenium, I get a blank page with a 429 request. I haven't sent too many request as I only do one and it doesn't work. I've tried multiple solutions but can't manage to do it. Any input would be appreciated. Here is my code:

`from selenium import webdriver  options = webdriver.ChromeOptions()  options.add_argument(""start-maximized"") # to supress the error messages/logs options.add_experimental_option('excludeSwitches', ['enable-logging']) options.add_experimental_option(""excludeSwitches"", [""enable-automation""]) options.add_argument(""disable-blink-features=AutomationControlled"") options.add_experimental_option('useAutomationExtension', False)  driver = webdriver.Chrome(options=options, executable_path=r""C:\\Users\\pople\\OneDrive\\Desktop\\chromedriver.exe"") driver.get('https://www.bluenile.com/diamond-search')`",[]
click() doesnt work for me,https://www.reddit.com/r/selenium/comments/wicfqg/click_doesnt_work_for_me/,selenium,"this is my code:

`from selenium import webdriver`  
`from selenium.webdriver.common.by import By`  
`from selenium.webdriver.chrome.service import Service`  
`import time`  


`#set location the location of the webdriver`  
`s = Service('C:/browserdrivers/chromedriver')`  


`driver = webdriver.Chrome(service=s)`  


`driver.get(""https://he.aliexpress.com/"")`  


`driver.refresh()`  
`search_bar = driver.find_element(by=By.CLASS_NAME, value= ""search-key"")`  
`search_bar.send_keys(""hi"")`  
`time.sleep(5)`  


`enter = driver.find_element(by=By.CLASS_NAME, value=""search-button"")`  
`enter.click()`  


now instead of clicking the element it writes this:

^(""C:\\Program Files\\Python310\\python.exe"" ""C:/◊ú◊ô◊û◊ï◊ì ◊°◊ú◊†◊ô◊ï◊ù/s.py"")

^(Traceback (most recent call last):)

  ^(File ""C:\\◊ú◊ô◊û◊ï◊ì ◊°◊ú◊†◊ô◊ï◊ù\\)[^(s.py)](https://s.py)^("", line 21, in <module>)

[^(enter.click)](https://enter.click)^(())

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(webelement.py)](https://webelement.py)^("", line 88, in click)

^(self.\_execute(Command.CLICK\_ELEMENT))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(webelement.py)](https://webelement.py)^("", line 396, in \_execute)

^(return self.\_parent.execute(command, params))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(webdriver.py)](https://webdriver.py)^("", line 435, in execute)

^(self.error\_handler.check\_response(response))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(errorhandler.py)](https://errorhandler.py)^("", line 247, in check\_response)

^(raise exception\_class(message, screen, stacktrace))

^(selenium.common.exceptions.ElementClickInterceptedException: Message: element click intercepted: Element <input type=""submit"" class=""search-button"" value=""""> is not clickable at point (307, 161). Other element would receive the click: <div class=""\_3KrBP  \_3XMV3"">...</div>)

  ^((Session info: chrome=103.0.5060.134))

^(Stacktrace:)

^(Backtrace:)

	^(Ordinal0 \[0x00CB6463+2188387\])

	^(Ordinal0 \[0x00C4E461+1762401\])

	^(Ordinal0 \[0x00B63D78+802168\])

	^(Ordinal0 \[0x00B97F9B+1015707\])

	^(Ordinal0 \[0x00B95F68+1007464\])

	^(Ordinal0 \[0x00B93C6B+998507\])

	^(Ordinal0 \[0x00B929D9+993753\])

	^(Ordinal0 \[0x00B88613+951827\])

	^(Ordinal0 \[0x00BAC7DC+1099740\])

	^(Ordinal0 \[0x00B87FF4+950260\])

	^(Ordinal0 \[0x00BAC9F4+1100276\])

	^(Ordinal0 \[0x00BBCC22+1166370\])

	^(Ordinal0 \[0x00BAC5F6+1099254\])

	^(Ordinal0 \[0x00B86BE0+945120\])

	^(Ordinal0 \[0x00B87AD6+948950\])

	^(GetHandleVerifier \[0x00F571F2+2712546\])

	^(GetHandleVerifier \[0x00F4886D+2652765\])

	^(GetHandleVerifier \[0x00D4002A+520730\])

	^(GetHandleVerifier \[0x00D3EE06+516086\])

	^(Ordinal0 \[0x00C5468B+1787531\])

	^(Ordinal0 \[0x00C58E88+1805960\])

	^(Ordinal0 \[0x00C58F75+1806197\])

	^(Ordinal0 \[0x00C61DF1+1842673\])

	^(BaseThreadInitThunk \[0x753DFA29+25\])

	^(RtlGetAppContainerNamedObjectPath \[0x77127A9E+286\])

	^(RtlGetAppContainerNamedObjectPath \[0x77127A6E+238\])

&#x200B;

&#x200B;

^(Process finished with exit code 1)

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

also, sometimes the code apparently ends after this line: `driver.get(""`[`https://he.aliexpress.com/`](https://he.aliexpress.com/)`"")`, writing this:

^(""C:\\Program Files\\Python310\\python.exe"" ""C:/◊ú◊ô◊û◊ï◊ì ◊°◊ú◊†◊ô◊ï◊ù/s.py"")

^(Traceback (most recent call last):)

  ^(File ""C:\\◊ú◊ô◊û◊ï◊ì ◊°◊ú◊†◊ô◊ï◊ù\\)[^(s.py)](https://s.py)^("", line 12, in <module>)

^(driver.get("")[^(https://he.aliexpress.com/)](https://he.aliexpress.com/)^(""))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(webdriver.py)](https://webdriver.py)^("", line 447, in get)

^(self.execute(Command.GET, {'url': url}))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(webdriver.py)](https://webdriver.py)^("", line 435, in execute)

^(self.error\_handler.check\_response(response))

  ^(File ""C:\\Users\\Pninia\\AppData\\Roaming\\Python\\Python310\\site-packages\\selenium\\webdriver\\remote\\)[^(errorhandler.py)](https://errorhandler.py)^("", line 247, in check\_response)

^(raise exception\_class(message, screen, stacktrace))

^(selenium.common.exceptions.WebDriverException: Message: unknown error: cannot determine loading status)

^(from unknown error: unexpected command response)

  ^((Session info: chrome=103.0.5060.134))

^(Stacktrace:)

^(Backtrace:)

	^(Ordinal0 \[0x00CB6463+2188387\])

	^(Ordinal0 \[0x00C4E461+1762401\])

	^(Ordinal0 \[0x00B63D78+802168\])

	^(Ordinal0 \[0x00B57210+750096\])

	^(Ordinal0 \[0x00B5675A+747354\])

	^(Ordinal0 \[0x00B55D3F+744767\])

	^(Ordinal0 \[0x00B54C28+740392\])

	^(Ordinal0 \[0x00B55228+741928\])

	^(Ordinal0 \[0x00B5EF2F+782127\])

	^(Ordinal0 \[0x00B69FBB+827323\])

	^(Ordinal0 \[0x00B6D310+840464\])

	^(Ordinal0 \[0x00B554F6+742646\])

	^(Ordinal0 \[0x00B69BF3+826355\])

	^(Ordinal0 \[0x00BBCF6D+1167213\])

	^(Ordinal0 \[0x00BAC5F6+1099254\])

	^(Ordinal0 \[0x00B86BE0+945120\])

	^(Ordinal0 \[0x00B87AD6+948950\])

	^(GetHandleVerifier \[0x00F571F2+2712546\])

	^(GetHandleVerifier \[0x00F4886D+2652765\])

	^(GetHandleVerifier \[0x00D4002A+520730\])

	^(GetHandleVerifier \[0x00D3EE06+516086\])

	^(Ordinal0 \[0x00C5468B+1787531\])

	^(Ordinal0 \[0x00C58E88+1805960\])

	^(Ordinal0 \[0x00C58F75+1806197\])

	^(Ordinal0 \[0x00C61DF1+1842673\])

	^(BaseThreadInitThunk \[0x753DFA29+25\])

	^(RtlGetAppContainerNamedObjectPath \[0x77127A9E+286\])

	^(RtlGetAppContainerNamedObjectPath \[0x77127A6E+238\])

&#x200B;

&#x200B;

^(Process finished with exit code 1)",[]
Proxy Rotating with Amazon API Gateway & LAMBDA,https://www.reddit.com/r/selenium/comments/wib6f7/proxy_rotating_with_amazon_api_gateway_lambda/,selenium,"I want to scrape google with selenium.   
I successfully run the program with lambda but lambda the giving only 1 IP   
so I want to integrate  Amazon API Gateway rotating proxy with my python script  


Any guide would be appreciated. :) 

Thank you",[]
"No such element exception, yet clearly visible in HTML",https://www.reddit.com/r/selenium/comments/whto1l/no_such_element_exception_yet_clearly_visible_in/,selenium,"I am trying to scrape a table, so my first step is to create a list of elements for each entry:

    entries = driver.find_elements(By.CLASS_NAME, ""gq-element"")

This works fine, and I get a list of WebElements. However, when I try and loop through this and extract content, I get an exception: 

    for entry in entries: 
    title = entry.find_element(By.CLASS_NAME, ""col-md-8 filter-content"")

`NoSuchElementException: no such element: Unable to locate element: {""method"":""css selector"",""selector"":"".col-md-8 filter-content""}`

Here is an example of what the HTML looks like (end goal is to extract blue text):

[https://imgur.com/h1oDuCb](https://imgur.com/h1oDuCb)

Any help would be greatly appreciated! Thanks.","['I suspect that your first issue here is that you\'re passing two classes into the method (col-md-8 and filter-content). The class col-md-8 is not going to help you here but filter-content might be unique enough. Also you aren\'t actually grabbing the text of this element even if you find it so I would add .text to the end of your statement. I would try something along the lines of:   \n\n\n`for entry in entries:`  \n`title = entry.find_element(By.CLASS_NAME, ""filter-content"").text`']"
How to navigate a dropdown triggered by javascript using Python Selenium,https://www.reddit.com/r/selenium/comments/whbih2/how_to_navigate_a_dropdown_triggered_by/,selenium,"[Stackoverflow link](https://stackoverflow.com/posts/73255453/edit)

I am trying to navigate a dropdown button that operates via javascript. However, no matter what I try, the HTML list items it should have never seem to show up in selenium.

[An image of the dropdown button](https://i.stack.imgur.com/aiaDx.png)

Inspector page source:
    
    <div class=""dropdown"" style=""border-bottom: 1px solid #ebebeb; padding-bottom: 2px;"">
         <a class=""btn btn-default btn-sm"" onclick=""$(this).parent().toggleClass('open')"" title=""Select an Event"" style=""width: 220px"">
             <span id=""event-selection-span-id"">No event selected.</span>
                  <span class=""fa fa-caret-down routing-toolbar-menu""></span>
         </a>
         <ul class=""dropdown-menu user-dropdown dropdown-menu-left"" id=""call-events-list-id"">
         <li title=""ADRC Archived Call"" event_definition_id=""2f617fc5-c0b0-492a-92e2-561c39c239fc"" form_code=""AACOG_ADRC_CCC_ARCH"" onclick=""CallCenter.SetEvent(this)"" class=""list-group-item event-group-list-item"">ADRC Archived Call</li>
         <li title=""ADRC Information  Call"" event_definition_id=""0a22deba-4788-4647-bee6-47305e182eca"" form_code=""AACOG_ADRC_CCC"" onclick=""CallCenter.SetEvent(this)"" class=""list-group-item event-group-list-item"">ADRC Information  Call</li>
         </ul>
    </div>


Selenium page source:
    
    <div class=""dropdown open"" style=""border-bottom: 1px solid #ebebeb; padding-bottom: 2px;"">
         <a class=""btn btn-default btn-sm"" onclick=""$(this).parent().toggleClass('open')"" title=""Select an Event"" style=""width: 220px"">
               <span id=""event-selection-span-id"">No event selected.</span>
                     <span class=""fa fa-caret-down routing-toolbar-menu""></span>
         </a>
         <ul class=""dropdown-menu user-dropdown dropdown-menu-left"" id=""call-events-list-id""></ul>
    </div>


Here is what I've tried so far, all unsuccessful:

* Originally just tried finding the elements and using the .click() method to click in them. (e.g. `driver.find_element_by_xpath('//*[@id=""step-3""]/div[2]/a').click()` then `driver.find_element_by_xpath('//*[@id=""call-events-list-id""]/li[2]').click()`. Selenium could not find the list element in the second line.

* Then I tried a method that's worked for me before when the previous one didn't: finding the elements then using `driver.execute_script(""arguments[0].click()"", btn)` for each one. Like before, it worked for the main dropdown button but not the list item that should show up afterwords.

* So I figured I could just execute the javascript manually with the elements' JS paths using `driver.execute_script(""$(document.querySelector('#step-3 > div.dropdown > a')).parent().toggleClass('open');"")` then `driver.execute_script(""CallCenter.SetEvent(document.querySelector('#call-events-list-id > li:nth-child(2)'));"")`. This still didn't work and the list elements still did not show up in selenium's page source.

The strangest thing is the list elements show up in the inspector before you even click the main dropdown button. Therefore I should just be able to execute the second JS line manually with no problems, and that works fine when I do it in the browser. I have also tried just waiting for the list elements to show up when the source is loaded but they never show up no matter how long I set the delay for. 

So I am at my wit's end and could really use some help with this one.",['I think that you could try anything with keyboard...\nArrow down ...']
Anybody here using any tools / Chrome extensions to speed up the process of gathering page object information?,https://www.reddit.com/r/selenium/comments/wh5qj3/anybody_here_using_any_tools_chrome_extensions_to/,selenium,,"['I like the ranorex selocity plug-in for quickly grabbing xpath and css selectors. It‚Äôs perhaps not as effective at xpath as writing your own relational xpaths but it works in a pinch', 'Dev tools.', 'Selectorshub chrome extension', 'Chropath plugin to write my own']"
selenium firefox extensions,https://www.reddit.com/r/selenium/comments/wgp2zd/selenium_firefox_extensions/,selenium,"hello,

 i have recently started using selenium it's great for automation.

what are the recommended extensions for firefox for a better selenium experience ,

also for vscode

and mainly is there an extension to find the unique tags(id,name,...) of  a certain object because it's taking too much time and sometimes i find it hard to find  the uniqueness in an object.","[""I'd definitely recommend to try SelectorsHub from extension store of your browser! and for vscode, I guess normal java extensions should be good! like java extension pack, maven, debugger, project manager! \n\nyou will discover more extensions on the way so don't worry about that!""]"
Is it possible to emulate touch events in Selenium?,https://www.reddit.com/r/selenium/comments/wga9px/is_it_possible_to_emulate_touch_events_in_selenium/,selenium,"I'm using Selenium 3.0.1 Java Client. What I want to know is if it's possible to ""touch"" an element instead of clicking it, so by touching the element there should be a touch event emitted.

I tired using org.openqa.selenium.interactions.touch.TouchActions like so:

TouchActions touchActions = new TouchActions(driver);
touchActions.singleTap(element);

Nothing happened as if the element was never clicked. No exceptions were even logged.

I read on the javadoc that the SingleTap action and the HasTouchScreen interface (the driver implements it) are deprecated. Could this be the issue why they aren't functioning?","[""Is there a reason that you're looking to use a touch event instead of a normal click? Surely it achieves the same thing?"", 'I think the [Actions Class](https://www.toolsqa.com/selenium-webdriver/actions-class-in-selenium/) may help you.', 'Also, you may want to look at using .dispatchEvent() to trigger event listeners on the server-side.  I wrote a custom function in C# to handle a custom drop-down-list in written in Angular.  The event listener was waiting for the ""input"" event and not the usual ""change"" event.\n\n    public static void AngularSelectionByText(this IWebDriver driver, By locator, string sOption, bool partialMatch = false) \n    { \n      IWebElement webElement = driver.FindElement(locator); \n      IJavaScriptExecutor js = (IJavaScriptExecutor)driver;\n    \n      if (webElement == null) throw new ArgumentNullException(""webElement"");\n    \n      SelectElement selectElement = new SelectElement(webElement.FindElement(By.XPath("".//select"")));\n    \n      if (sOption.Length > 0) \n      {\n        foreach (IWebElement opt in selectElement.Options) \n        { \n          if (partialMatch ? opt.Text.Contains(sOption) : opt.Text.Trim() == sOption) \n          { \n            js.ExecuteScript(""arguments[0].selected=true"", opt); \n            js.ExecuteScript(""const evt=new Event(\'input\',{\'bubbles\':true,\'cancelable\':true});arguments[0].dispatchEvent(evt);"", selectElement); \n            break; \n          } \n        } \n      } \n    }\n\nSo, you could fire one of the touch events instead of the ""input"" event I have in this example.']"
