Title,Description,Views,Votes,Answers
WebDriverWait not working as expected,"
I am working with selenium to scrape some data.
There is button on the page that I am clicking say ""custom_cols"". This button opens up a window for me where I can select my columns. 
This new window sometimes takes some time to open (around 5 seconds). So to handle this I have used 
WebDriverWait 

with delay as 20 seconds. But some times it fails to select find elements on new window, even if the element is visible. This happens only once in ten times for rest of time it works properly.
I have used same function(WebDriverWait) on other places also and it is works as expected. I mean it waits till the elements gets visible and then clicks it at the moment it finds it.
My question is why elements on new window is not visible even though I am waiting for element to get visible. To add here I have tried to increase delay time but still I get that error once in a while.
My code is here 
def wait_for_elem_xpath(self, delay = None, xpath = """"):
    if delay is None:
        delay = self.delay

    try:
        myElem = WebDriverWait(self.browser, delay).until(EC.presence_of_element_located((By.XPATH , xpath)))
    except TimeoutException:
        print (""xpath: Loading took too much time!"")
    return myElem
select_all_performance = '//*[@id=""mks""]/body/div[7]/div[2]/div/div/div/div/div[2]/div/div[2]/div[2]/div/div[1]/div[1]/section/header/div'
self.wait_for_elem_xpath(xpath = select_all_performance).click()

",14k,"
            15
        ","['\nOnce you wait for the element and moving forward as you are trying to invoke click() method instead of using presence_of_element_located() method you need to use element_to_be_clickable() as follows :\ntry:\n    myElem = WebDriverWait(self.browser, delay).until(EC.element_to_be_clickable((By.XPATH , xpath)))\n\n\nUpdate\nAs per your counter question in the comments here are the details of the three methods :\npresence_of_element_located\npresence_of_element_located(locator) is defined as follows :\nclass selenium.webdriver.support.expected_conditions.presence_of_element_located(locator)\n\nParameter : locator - used to find the element returns the WebElement once it is located\n\nDescription : An expectation for checking that an element is present on the DOM of a page. This does not necessarily mean that the element is visible or interactable (i.e. clickable). \n\nvisibility_of_element_located\nvisibility_of_element_located(locator) is defined as follows :\nclass selenium.webdriver.support.expected_conditions.visibility_of_element_located(locator)\n\nParameter : locator -  used to find the element returns the WebElement once it is located and visible\n\nDescription : An expectation for checking that an element is present on the DOM of a page and visible. Visibility means that the element is not only displayed but also has a height and width that is greater than 0.\n\nelement_to_be_clickable\nelement_to_be_clickable(locator) is defined as follows :\nclass selenium.webdriver.support.expected_conditions.element_to_be_clickable(locator)\n\nParameter : locator - used to find the element returns the WebElement once it is visible, enabled and interactable (i.e. clickable).\n\nDescription : An Expectation for checking an element is visible, enabled and interactable such that you can click it. \n\n']"
Web-scraping JavaScript page with Python,"
I'm trying to develop a simple web scraper. I want to extract text without the HTML code. It works on plain HTML, but not in some pages where JavaScript code adds text.
For example, if some JavaScript code adds some text, I can't see it, because when I call:
response = urllib2.urlopen(request)

I get the original text without the added one (because JavaScript is executed in the client).
So, I'm looking for some ideas to solve this problem.
",436k,"
            264
        ","['\nEDIT Sept 2021: phantomjs isn\'t maintained any more, either\nEDIT 30/Dec/2017: This answer appears in top results of Google searches, so I decided to update it. The old answer is still at the end.\ndryscape isn\'t maintained anymore and the library dryscape developers recommend is Python 2 only. I have found using Selenium\'s python library with Phantom JS as a web driver fast enough and easy to get the work done.\nOnce you have installed Phantom JS, make sure the phantomjs binary is available in the current path:\nphantomjs --version\n# result:\n2.1.1\n\n#Example\nTo give an example, I created a sample page with following HTML code. (link):\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=""utf-8"">\n  <title>Javascript scraping test</title>\n</head>\n<body>\n  <p id=\'intro-text\'>No javascript support</p>\n  <script>\n     document.getElementById(\'intro-text\').innerHTML = \'Yay! Supports javascript\';\n  </script> \n</body>\n</html>\n\nwithout javascript it says: No javascript support and with javascript: Yay! Supports javascript\n#Scraping without JS support:\nimport requests\nfrom bs4 import BeautifulSoup\nresponse = requests.get(my_url)\nsoup = BeautifulSoup(response.text)\nsoup.find(id=""intro-text"")\n# Result:\n<p id=""intro-text"">No javascript support</p>\n\n#Scraping with JS support:\nfrom selenium import webdriver\ndriver = webdriver.PhantomJS()\ndriver.get(my_url)\np_element = driver.find_element_by_id(id_=\'intro-text\')\nprint(p_element.text)\n# result:\n\'Yay! Supports javascript\'\n\n\nYou can also use Python library dryscrape to scrape javascript driven websites.\n#Scraping with JS support:\nimport dryscrape\nfrom bs4 import BeautifulSoup\nsession = dryscrape.Session()\nsession.visit(my_url)\nresponse = session.body()\nsoup = BeautifulSoup(response)\nsoup.find(id=""intro-text"")\n# Result:\n<p id=""intro-text"">Yay! Supports javascript</p>\n\n', '\nWe are not getting the correct results because any javascript generated content needs to be rendered on the DOM. When we fetch an HTML page, we fetch the initial, unmodified by javascript, DOM.\nTherefore we need to render the javascript content before we crawl the page.\nAs selenium is already mentioned many times in this thread (and how slow it gets sometimes was mentioned also), I will list two other possible solutions.\n\nSolution 1: This is a very nice tutorial on how to use Scrapy to crawl javascript generated content and we are going to follow just that.\nWhat we will need:\n\nDocker installed in our machine. This is a plus over other solutions until this point, as it utilizes an OS-independent platform.\nInstall Splash following the instruction listed for our corresponding OS.Quoting from splash documentation:\n\nSplash is a javascript rendering service. It’s a lightweight web browser with an HTTP API, implemented in Python 3 using Twisted and QT5. \n\nEssentially we are going to use Splash to render Javascript generated content.\nRun the splash server: sudo docker run -p 8050:8050 scrapinghub/splash.\nInstall the scrapy-splash plugin: pip install scrapy-splash\nAssuming that we already have a Scrapy project created (if not, let\'s make one), we will follow the guide and update the settings.py:\n\nThen go to your scrapy project’s settings.py and set these middlewares:\nDOWNLOADER_MIDDLEWARES = {\n      \'scrapy_splash.SplashCookiesMiddleware\': 723,\n      \'scrapy_splash.SplashMiddleware\': 725,\n      \'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\': 810,\n}\n\nThe URL of the Splash server(if you’re using Win or OSX this should be the URL of the docker machine: How to get a Docker container\'s IP address from the host?):\nSPLASH_URL = \'http://localhost:8050\'\n\nAnd finally you need to set these values too:\nDUPEFILTER_CLASS = \'scrapy_splash.SplashAwareDupeFilter\'\nHTTPCACHE_STORAGE = \'scrapy_splash.SplashAwareFSCacheStorage\'\n\n\nFinally, we can use a SplashRequest:\n\nIn a normal spider you have Request objects which you can use to open URLs. If the page you want to open contains JS generated data you have to use SplashRequest(or SplashFormRequest) to render the page. Here’s a simple example:\nclass MySpider(scrapy.Spider):\n    name = ""jsscraper""\n    start_urls = [""http://quotes.toscrape.com/js/""]\n\n    def start_requests(self):\n        for url in self.start_urls:\n        yield SplashRequest(\n            url=url, callback=self.parse, endpoint=\'render.html\'\n        )\n\n    def parse(self, response):\n        for q in response.css(""div.quote""):\n        quote = QuoteItem()\n        quote[""author""] = q.css("".author::text"").extract_first()\n        quote[""quote""] = q.css("".text::text"").extract_first()\n        yield quote\n\nSplashRequest renders the URL as html and returns the response which you can use in the callback(parse) method.\n\n\n\nSolution 2: Let\'s call this experimental at the moment (May 2018)...\nThis solution is for Python\'s version 3.6 only (at the moment).\nDo you know the requests module (well who doesn\'t)?\nNow it has a web crawling little sibling: requests-HTML:\n\nThis library intends to make parsing HTML (e.g. scraping the web) as simple and intuitive as possible.\n\n\nInstall requests-html: pipenv install requests-html\nMake a request to the page\'s url:\nfrom requests_html import HTMLSession\n\nsession = HTMLSession()\nr = session.get(a_page_url)\n\nRender the response to get the Javascript generated bits:\nr.html.render()\n\n\nFinally, the module seems to offer scraping capabilities.\nAlternatively, we can try the well-documented way of using BeautifulSoup with the r.html object we just rendered.\n', '\nMaybe selenium can do it.\nfrom selenium import webdriver\nimport time\n\ndriver = webdriver.Firefox()\ndriver.get(url)\ntime.sleep(5)\nhtmlSource = driver.page_source\n\n', ""\nIf you have ever used the Requests module for python before, I recently found out that the developer created a new module called Requests-HTML which now also has the ability to render JavaScript.\nYou can also visit https://html.python-requests.org/ to learn more about this module, or if your only interested about rendering JavaScript then you can visit https://html.python-requests.org/?#javascript-support to directly learn how to use the module to render JavaScript using Python.\nEssentially, Once you correctly install the Requests-HTML module, the following example, which is shown on the above link, shows how you can use this module to scrape a website and render JavaScript contained within the website:\nfrom requests_html import HTMLSession\nsession = HTMLSession()\n\nr = session.get('http://python-requests.org/')\n\nr.html.render()\n\nr.html.search('Python 2 will retire in only {months} months!')['months']\n\n'<time>25</time>' #This is the result.\n\nI recently learnt about this from a YouTube video. Click Here! to watch the YouTube video, which demonstrates how the module works.\n"", ""\nIt sounds like the data you're really looking for can be accessed via secondary URL called by some javascript on the primary page.\nWhile you could try running javascript on the server to handle this, a simpler approach  to might be to load up the page using Firefox and use a tool like Charles or Firebug to identify exactly what that secondary URL is. Then you can just query that URL directly for the data you are interested in.\n"", '\nThis seems to be a good solution also, taken from a great blog post\nimport sys  \nfrom PyQt4.QtGui import *  \nfrom PyQt4.QtCore import *  \nfrom PyQt4.QtWebKit import *  \nfrom lxml import html \n\n#Take this class for granted.Just use result of rendering.\nclass Render(QWebPage):  \n  def __init__(self, url):  \n    self.app = QApplication(sys.argv)  \n    QWebPage.__init__(self)  \n    self.loadFinished.connect(self._loadFinished)  \n    self.mainFrame().load(QUrl(url))  \n    self.app.exec_()  \n\n  def _loadFinished(self, result):  \n    self.frame = self.mainFrame()  \n    self.app.quit()  \n\nurl = \'http://pycoders.com/archive/\'  \nr = Render(url)  \nresult = r.frame.toHtml()\n# This step is important.Converting QString to Ascii for lxml to process\n\n# The following returns an lxml element tree\narchive_links = html.fromstring(str(result.toAscii()))\nprint archive_links\n\n# The following returns an array containing the URLs\nraw_links = archive_links.xpath(\'//div[@class=""campaign""]/a/@href\')\nprint raw_links\n\n', '\nSelenium is the best for scraping JS and Ajax content.\nCheck this article for extracting data from the web using Python\n$ pip install selenium\n\nThen download Chrome webdriver.\nfrom selenium import webdriver\n\nbrowser = webdriver.Chrome()\n\nbrowser.get(""https://www.python.org/"")\n\nnav = browser.find_element_by_id(""mainnav"")\n\nprint(nav.text)\n\nEasy, right?\n', ""\nYou can also execute javascript using webdriver.\nfrom selenium import webdriver\n\ndriver = webdriver.Firefox()\ndriver.get(url)\ndriver.execute_script('document.title')\n\nor store the value in a variable\nresult = driver.execute_script('var text = document.title ; return text')\n\n"", '\nI personally prefer using scrapy and selenium and dockerizing both in separate containers. This way you can install both with minimal hassle and crawl modern websites that almost all contain javascript in one form or another. Here\'s an example:\nUse the scrapy startproject to create your scraper and write your spider, the skeleton can be as simple as this:\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = \'my_spider\'\n    start_urls = [\'https://somewhere.com\']\n\n    def start_requests(self):\n        yield scrapy.Request(url=self.start_urls[0])\n\n\n    def parse(self, response):\n\n        # do stuff with results, scrape items etc.\n        # now were just checking everything worked\n\n        print(response.body)\n\nThe real magic happens in the middlewares.py. Overwrite two methods in the downloader middleware,  __init__ and  process_request, in the following way:\n# import some additional modules that we need\nimport os\nfrom copy import deepcopy\nfrom time import sleep\n\nfrom scrapy import signals\nfrom scrapy.http import HtmlResponse\nfrom selenium import webdriver\n\nclass SampleProjectDownloaderMiddleware(object):\n\ndef __init__(self):\n    SELENIUM_LOCATION = os.environ.get(\'SELENIUM_LOCATION\', \'NOT_HERE\')\n    SELENIUM_URL = f\'http://{SELENIUM_LOCATION}:4444/wd/hub\'\n    chrome_options = webdriver.ChromeOptions()\n\n    # chrome_options.add_experimental_option(""mobileEmulation"", mobile_emulation)\n    self.driver = webdriver.Remote(command_executor=SELENIUM_URL,\n                                   desired_capabilities=chrome_options.to_capabilities())\n\n\ndef process_request(self, request, spider):\n\n    self.driver.get(request.url)\n\n    # sleep a bit so the page has time to load\n    # or monitor items on page to continue as soon as page ready\n    sleep(4)\n\n    # if you need to manipulate the page content like clicking and scrolling, you do it here\n    # self.driver.find_element_by_css_selector(\'.my-class\').click()\n\n    # you only need the now properly and completely rendered html from your page to get results\n    body = deepcopy(self.driver.page_source)\n\n    # copy the current url in case of redirects\n    url = deepcopy(self.driver.current_url)\n\n    return HtmlResponse(url, body=body, encoding=\'utf-8\', request=request)\n\nDont forget to enable this middlware by uncommenting the next lines in the settings.py file:\nDOWNLOADER_MIDDLEWARES = {\n\'sample_project.middlewares.SampleProjectDownloaderMiddleware\': 543,}\n\nNext for dockerization. Create your Dockerfile from a lightweight image (I\'m using python Alpine here), copy your project directory to it, install requirements:\n# Use an official Python runtime as a parent image\nFROM python:3.6-alpine\n\n# install some packages necessary to scrapy and then curl because it\'s  handy for debugging\nRUN apk --update add linux-headers libffi-dev openssl-dev build-base libxslt-dev libxml2-dev curl python-dev\n\nWORKDIR /my_scraper\n\nADD requirements.txt /my_scraper/\n\nRUN pip install -r requirements.txt\n\nADD . /scrapers\n\nAnd finally bring it all together in docker-compose.yaml:\nversion: \'2\'\nservices:\n  selenium:\n    image: selenium/standalone-chrome\n    ports:\n      - ""4444:4444""\n    shm_size: 1G\n\n  my_scraper:\n    build: .\n    depends_on:\n      - ""selenium""\n    environment:\n      - SELENIUM_LOCATION=samplecrawler_selenium_1\n    volumes:\n      - .:/my_scraper\n    # use this command to keep the container running\n    command: tail -f /dev/null\n\nRun docker-compose up -d. If you\'re doing this the first time it will take a while for it to fetch the latest selenium/standalone-chrome and the build your scraper image as well. \nOnce it\'s done, you can check that your containers are running with docker ps and also check that the name of the selenium container matches that of the environment variable that we passed to our scraper container (here, it was SELENIUM_LOCATION=samplecrawler_selenium_1). \nEnter your scraper container with docker exec -ti YOUR_CONTAINER_NAME sh , the command for me was docker exec -ti samplecrawler_my_scraper_1 sh, cd into the right directory and run your scraper with scrapy crawl my_spider.\nThe entire thing is on my github page and you can get it from here\n', '\nA mix of BeautifulSoup and Selenium works very well for me.\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup as bs\n\ndriver = webdriver.Firefox()\ndriver.get(""http://somedomain/url_that_delays_loading"")\n    try:\n        element = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.ID, ""myDynamicElement""))) #waits 10 seconds until element is located. Can have other wait conditions  such as visibility_of_element_located or text_to_be_present_in_element\n\n        html = driver.page_source\n        soup = bs(html, ""lxml"")\n        dynamic_text = soup.find_all(""p"", {""class"":""class_name""}) #or other attributes, optional\n    else:\n        print(""Couldnt locate element"")\n\nP.S. You can find more wait conditions here\n', '\nUsing PyQt5\nfrom PyQt5.QtWidgets import QApplication\nfrom PyQt5.QtCore import QUrl\nfrom PyQt5.QtWebEngineWidgets import QWebEnginePage\nimport sys\nimport bs4 as bs\nimport urllib.request\n\n\nclass Client(QWebEnginePage):\n    def __init__(self,url):\n        global app\n        self.app = QApplication(sys.argv)\n        QWebEnginePage.__init__(self)\n        self.html = """"\n        self.loadFinished.connect(self.on_load_finished)\n        self.load(QUrl(url))\n        self.app.exec_()\n\n    def on_load_finished(self):\n        self.html = self.toHtml(self.Callable)\n        print(""Load Finished"")\n\n    def Callable(self,data):\n        self.html = data\n        self.app.quit()\n\n# url = """"\n# client_response = Client(url)\n# print(client_response.html)\n\n', ""\nYou'll want to use urllib, requests, beautifulSoup and selenium web driver in your script for different parts of the page, (to name a few).\nSometimes you'll get what you need with just one of these modules.\nSometimes you'll need two, three, or all of these modules.\nSometimes you'll need to switch off the js on your browser.\nSometimes you'll need header info in your script.\nNo websites can be scraped the same way and no website can be scraped in the same way forever without having to modify your crawler, usually after a few months. But they can all be scraped! Where there's a will there's a way for sure.\nIf you need scraped data continuously into the future just scrape everything you need and store it in .dat files with pickle.\nJust keep searching how to try what with these modules and copying and pasting your errors into the Google.\n"", '\nPyppeteer\nYou might consider Pyppeteer, a Python port of the Chrome/Chromium driver front-end Puppeteer.\nHere\'s a simple example to show how you can use Pyppeteer to access data that was injected into the page dynamically:\nimport asyncio\nfrom pyppeteer import launch\n\nasync def main():\n    browser = await launch({""headless"": True})\n    [page] = await browser.pages()\n\n    # normally, you go to a live site...\n    #await page.goto(""http://www.example.com"")\n    # but for this example, just set the HTML directly:\n    await page.setContent(""""""\n    <body>\n    <script>\n    // inject content dynamically with JS, not part of the static HTML!\n    document.body.innerHTML = `<p>hello world</p>`; \n    </script>\n    </body>\n    """""")\n    print(await page.content()) # shows that the `<p>` was inserted\n\n    # evaluate a JS expression in browser context and scrape the data\n    expr = ""document.querySelector(\'p\').textContent""\n    print(await page.evaluate(expr, force_expr=True)) # => hello world\n\n    await browser.close()\n\nasyncio.get_event_loop().run_until_complete(main())\n\nSee Pyppeteer\'s reference docs.\n', '\nTry accessing the API directly\nA common scenario you\'ll see in scraping is that the data is being requested asynchronously from an API endpoint by the webpage. A minimal example of this would be the following site:\n\n\n<body>\n<script>\nfetch(""https://jsonplaceholder.typicode.com/posts/1"")\n  .then(res => {\n    if (!res.ok) throw Error(res.status);\n    \n    return res.json();\n  })\n  .then(data => {\n    // inject data dynamically via JS after page load\n    document.body.innerText = data.title;\n  })\n  .catch(err => console.error(err))\n;\n</script>\n</body>\n\n\nIn many cases, the API will be protected by CORS or an access token or prohibitively rate limited, but in other cases it\'s publicly-accessible and you can bypass the website entirely. For CORS issues, you might try cors-anywhere.\nThe general procedure is to use your browser\'s developer tools\' network tab to search the requests made by the page for keywords/substrings of the data you want to scrape. Often, you\'ll see an unprotected API request endpoint with a JSON payload that you can access directly with urllib or requests modules. That\'s the case with the above runnable snippet which you can use to practice. After clicking ""run snippet"", here\'s how I found the endpoint in my network tab:\n\nThis example is contrived; the endpoint URL will likely be non-obvious from looking at the static markup because it could be dynamically assembled, minified and buried under dozens of other requests and endpoints. The network request will also show any relevant request payload details like access token you may need.\nAfter obtaining the endpoint URL and relevant details, build a request in Python using a standard HTTP library and request the data:\n>>> import requests\n>>> res = requests.get(""https://jsonplaceholder.typicode.com/posts/1"")\n>>> data = res.json()\n>>> data[""title""]\n\'sunt aut facere repellat provident occaecati excepturi optio reprehenderit\'\n\nWhen you can get away with it, this tends to be much easier, faster and more reliable than scraping the page with Selenium, Pyppeteer, Scrapy or whatever the popular scraping libraries are at the time you\'re reading this post.\nIf you\'re unlucky and the data hasn\'t arrived via an API request that returns the data in a nice format, it could be part of the original browser\'s payload in a <script> tag, either as a JSON string or (more likely) a JS object. For example:\n\n\n<body>\n<script>\n  var someHardcodedData = {\n    userId: 1,\n    id: 1,\n    title: \'sunt aut facere repellat provident occaecati excepturi optio reprehenderit\', \n    body: \'quia et suscipit\\nsuscipit recusandae con sequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\'\n  };\n  document.body.textContent = someHardcodedData.title;\n</script>\n</body>\n\n\nThere\'s no one-size-fits-all way to obtain this data. The basic technique is to use BeautifulSoup to access the <script> tag text, then apply a regex or a parse to extract the object structure, JSON string, or whatever format the data might be in. Here\'s a proof-of-concept on the sample structure shown above:\nimport json\nimport re\nfrom bs4 import BeautifulSoup\n\n# pretend we\'ve already used requests to retrieve the data, \n# so we hardcode it for the purposes of this example\ntext = """"""\n<body>\n<script>\n  var someHardcodedData = {\n    userId: 1,\n    id: 1,\n    title: \'sunt aut facere repellat provident occaecati excepturi optio reprehenderit\', \n    body: \'quia et suscipit\\nsuscipit recusandae con sequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\'\n  };\n  document.body.textContent = someHardcodedData.title;\n</script>\n</body>\n""""""\nsoup = BeautifulSoup(text, ""lxml"")\nscript_text = str(soup.select_one(""script""))\npattern = r""title: \'(.*?)\'""\nprint(re.search(pattern, script_text, re.S).group(1))\n\nCheck out these resources for parsing JS objects that aren\'t quite valid JSON:\n\nHow to convert raw javascript object to python dictionary?\nHow to Fix JSON Key Values without double-quotes?\n\nHere are some additional case studies/proofs-of-concept where scraping was bypassed using an API:\n\nHow can I scrape yelp reviews and star ratings into CSV using Python beautifulsoup\nBeautiful Soup returns None on existing element\nExtract data from  BeautifulSoup Python\nScraping Bandcamp fan collections via POST (uses a hybrid approach where an initial request was made to the website to extract a token from the markup using BeautifulSoup which was then used in a second request to a JSON endpoint)\n\nIf all else fails, try one of the many dynamic scraping libraries listed in this thread.\n', '\nPlaywright-Python\nYet another option is playwright-python, a port of Microsoft\'s Playwright (itself a Puppeteer-influenced browser automation library) to Python.\nHere\'s the minimal example of selecting an element and grabbing its text:\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch()\n    page = browser.new_page()\n    page.goto(""http://whatsmyuseragent.org/"")\n    ua = page.query_selector("".user-agent"");\n    print(ua.text_content())\n    browser.close()\n\n', '\nAs mentioned, Selenium is a good choice for rendering the results of the JavaScript:\nfrom selenium.webdriver import Firefox\nfrom selenium.webdriver.firefox.options import Options\n\noptions = Options()\noptions.headless = True\nbrowser = Firefox(executable_path=""/usr/local/bin/geckodriver"", options=options)\n\nurl = ""https://www.example.com""\nbrowser.get(url)\n\nAnd gazpacho is a really easy library to parse over the rendered html:\nfrom gazpacho import Soup\n\nsoup = Soup(browser.page_source)\nsoup.find(""a"").attrs[\'href\']\n\n', '\nI recently used requests_html library to solve this problem.\nTheir expanded documentation at readthedocs.io is pretty good (skip the annotated version at pypi.org). If your use case is basic, you are likely to have some success.\nfrom requests_html import HTMLSession\nsession = HTMLSession()\nresponse = session.request(method=""get"",url=""www.google.com/"")\nresponse.html.render()\n\nIf you are having trouble rendering the data you need with response.html.render(), you can pass some javascript to the render function to render the particular js object you need. This is copied from their docs, but it might be just what you need:\n\nIf script is specified, it will execute the provided JavaScript at\nruntime. Example:\n\nscript = """"""\n    () => {\n        return {\n            width: document.documentElement.clientWidth,\n            height: document.documentElement.clientHeight,\n            deviceScaleFactor: window.devicePixelRatio,\n        }\n    } \n""""""\n\n\nReturns the return value of the executed script, if any is provided:\n\n>>> response.html.render(script=script)\n{\'width\': 800, \'height\': 600, \'deviceScaleFactor\': 1}\n\nIn my case, the data I wanted were the arrays that populated a javascript plot but the data wasn\'t getting rendered as text anywhere in the html. Sometimes its not clear at all what the object names are of the data you want if the data is populated dynamically. If you can\'t track down the js objects directly from view source or inspect, you can type in ""window"" followed by ENTER in the debugger console in the browser (Chrome) to pull up a full list of objects rendered by the browser. If you make a few educated guesses about where the data is stored, you might have some luck finding it there. My graph data was under window.view.data in the console, so in the ""script"" variable passed to the .render() method quoted above, I used:\nreturn {\n    data: window.view.data\n}\n\n', '\nEasy and Quick Solution:\nI was dealing with same problem. I want to scrape some data which is build with JavaScript. If I scrape only text from this site with BeautifulSoup then I ended with  tags in text.\nI want to render this  tag and wills to grab information from this.\nAlso, I dont want to use heavy frameworks  like Scrapy and selenium.\nSo, I found that get method of requests module takes urls, and it actually renders the script tag.\nExample:\nimport requests\ncustom_User_agent = ""Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0""\nurl = ""https://www.abc.xyz/your/url""\nresponse = requests.get(url, headers={""User-Agent"": custom_User_agent})\nhtml_text = response.text\n\nThis will renders load site and renders  tags.\nHope this will help as quick and easy solution to render site which is loaded with script tags.\n']"
Scraping data to Google Sheets from a website that uses JavaScript,"
I am trying to import data from the following website to Google Sheets. I want to import all the matches for the day.
https://www.tournamentsoftware.com/tournament/b731fdcd-a0c8-4558-9344-2a14c267ee8b/Matches
I have tried importxml and importhtml, but it seems this does not work as the website uses JavaScript. I have also tried to use Aphipheny with no success.
When using Apipheny, the error message is

'Failed to fetch data - please verify your API Request: {DNS error'

",748,"
            1
        ","['\nTl;Dr\nAdapted from my answer to How to know if Google Sheets IMPORTDATA, IMPORTFEED, IMPORTHTML or IMPORTXML functions are able to get data from a resource hosted on a website? (also posted by me)\nPlease spend some time learning how to use the browsers developers tools so you will be able to identify\n\nif the data is already included in source code of the webpage as JSON / literal JavaScript object or in another form\nif the webpage is doing a GET or POST requests to retrieve the data and when those requests are done (i.e. as some point of the page parsing, or on event)\nif the requests require data from cookies\n\n\nBrief guide about how to use the web browser to find useful details about the webpage / data to import\n\nOpen the source code and look if the required data is included. Sometimes the data is included as JSON and added to the DOM using JavaScript. In this case it might be possible to retrieve the data by using the Google Sheets functions or URL Fetch Service from Google Apps Script.\nLet say that you use Chrome. Open the Dev Tools, then look at the Elements tab. There you will see the DOM. It might be helpful to identify if the data that you want to import besides being on visible elements is included in hidden / not visible elements like <script> tags.\nLook at Source, there you might be able to see the JavaScript code. It might include the data that you want to import as JavaScript object (commonly referred as JSON).\n\n\nThere are a lot of questions about google-sheets +web-scraping that mentions problems using importhtml and/or importxml that already have answers and even many include code (JavaScript snippets, Google Apps Script functions, etc.) that might save you to have to use an specialized web-scraping tool that has a more stepped learning curve. At the bottom of this answer there is a list of questions about using Google Sheets built-in functions, including annotations of the workaround proposed.\nOn Is there a way to get a single response from a text/event-stream without using event listeners? ask about using EventSource. While this can\'t be used on server side code, the answer show how to use the HtmlService to use it on client-side code and retrieve the result to Google Sheets.\n\nAs you already realized, the Google Sheets built-in functions importhtml(), importxml(), importdata() and importfeed() only work with static pages that do not require signing in or other forms of authentication.\nWhen the content of a public page is created dynamically by using JavaScript, it cannot be accessed with those functions, by the other hand the website\'s webmaster may also purposefully have prevented web scraping.\n\nHow to identify if content is added dynamically\nTo check if the content is added dynamically, using Chrome,\n\nOpen the URL of the source data.\nPress F12 to open Chrome Developer Tools\nPress Control+Shift+P to open the Command Menu.\nStart typing javascript, select Disable JavaScript, and then press Enter to run the command. JavaScript is now disabled.\n\nJavaScript will remain disabled in this tab so long as you have DevTools open.\nReload the page to see if the content that you want to import is shown, if it\'s shown it could be imported by using Google Sheets built-in functions, otherwise it\'s not possible but might be possible by using other means for doing web scraping.\n\n According to Wikipedia,\n\n Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.\n\nUse of robots.txt to block Web crawlers\nThe webmasters could use robots.txt file to block access to website. In such case the result will be #N/A Could not fetch URL.\nUse of User agent\nThe webpage could be designed to return a special a custom message instead of the data.\n\nBelow there are more details about how Google Sheets built-in ""web-scraping"" functions works\nIMPORTDATA, IMPORTFEED, IMPORTHTML and IMPORTXML are able to get content from resources hosted on websites that are:\n\nPublicly available. This means that the resource doesn\'t require authorization / to be logged in into any service to access it.\nThe content is ""static"". This mean that if you open the resource using the view source code option of modern web browsers it will be displayed as plain text.\n\nNOTE: The Chrome\'s Inspect tool shows the parsed DOM; in other works the actual structure/content of the web page which could be dynamically modified by JavaScript code or browser extensions/plugins.\n\n\nThe content has the appropriated structure.\n\nIMPORTDATA works with structured content as csv or tsv doesn\'t matter of the file extension of the resource.\nIMPORTFEED works with marked up content as ATOM/RSS\nIMPORTHTML works with marked up content as HTML that includes properly markedup list or tables.\nIMPORTXML works with marked up content as XML or any of its variants like XHTML.\n\n\nThe content doesn\'t exceeds the maximum size. Google haven\'t disclosed this limit but the below error will be shown when the content exceeds the maximum size:\n\nResource at url contents exceeded maximum size.\n\n\nGoogle servers are not blocked by means of robots.txt or the user agent.\n\nOn W3C Markup Validator there are several tools to checkout is the resources had been properly marked up.\nRegarding CSV check out Are there known services to validate CSV files\nIt\'s worth to note that the spreadsheet\n\nshould have enough room for the imported content; Google Sheets has a 10 million cell limit by spreadsheet, according to this post a columns limit of 18278, and a 50 thousand characters as cell content even as a value or formula.\nit doesn\'t handle well large in-cell content; the ""limit"" depends on the user screen size and resolution as now it\'s possible to zoom in/out.\n\n\nReferences\n\nhttps://developers.google.com/web/tools/chrome-devtools/javascript/disable\nhttps://en.wikipedia.org/wiki/Web_scraping\n\nRelated\n\nUsing Google Apps Script to scrape Dynamic Web Pages\nScraping data from website using vba\nBlock Website Scraping by Google Docs\nIs there a way to get a single response from a text/event-stream without using event listeners?\n\nSoftware Recommendations\n\nWeb scraping tool/software available for free?\nRecommendations for web scraping tools that require minimal installation\n\nWeb Applications\nThe following question is about a different result, #N/A Could not fetch URL\n\nInability to use IMPORTHTML in Google sheets\n\n\nSimilar questions\nSome of this questions might be closed as duplicate of this one\n\nImporting javascript table into Google Docs spreadsheet\nImportxml Imported Content Empty\nscrape table using google app scripts\n\nOne answer includes Google Apps Script code using the URL Fetch Service\n\n\nCapture element using ImportXML with XPath\nHow to import Javascript tables into Google spreadsheet?\nScrape the current share price data from the ASX\n\nOne of the answers includes Google Apps Script code to get data from a JSON source\n\n\nGuidance on webscraping using Google Sheets\nHow to Scrape data from Indiegogo.com in google sheets via IMPORTXML formula\nWhy importxml and importhtml not working here?\nGoogle Sheet use Importxml error could not fetch url\n\n\n\nOne answer includes Google Apps Script code using the URL Fetch Service\n\n\n\n\nGoogle Sheets - Pull Data for investment portfolio\nExtracting value from API/Webpage\nIMPORTXML shows an error while scraping data from website\n\nOne answer shows the xhr request found using browser developer tools\n\n\nReplacing =ImportHTML with URLFetchApp\n\nOne answer includes Google Apps Script code using the URL Fetch Service\n\n\nHow to use IMPORTXML to import hidden div tag?\nGoogle Sheet Web-scraping ImportXml Xpath on Yahoo Finance doesn\'t works with french stock\n\nOne of the answers includes Google Apps Script code to get data from a JSON source. As of January 4th 2023, it\'s not longer working, very likely because Yahoo! Finance is now encrying the JSON. See the Tainake\'s answer to How to pull Yahoo Finance Historical Price Data from its Object with Google Apps Script? for script using Crypto.js to handle this.\n\n\nHow to fetch data which is loaded by the ajax (asynchronous) method after the web page has already been loaded using apps script?\n\nOne answer suggest to read the data from the server instead of scraping from a webpage.\n\n\nUsing ImportXML to pull data\nExtracting data from web page using Cheerio Library\n\nOne answer suggest the use of an API and Google Apps Script\n\n\n\n', '\nImportXML is good for basic tasks, but won\'t get you too far if you are serious in scraping:\n\nThe approach only works with most basic websites (no SPAs rendered in browsers can be scraped this way, any basic web scraping protection or connectivity issue breaks the process, no control over HTTP request geo location, or number of retries) - and yahoo finance is not a simple website\n\nIf the target website data requires some cleanup post-processing, it\'s getting very complicated since you are now ""programming with excel formulas"", rather painful process compared to regular code writing in conventional programming languages\n\nThere is no proper launch & cache control so the function can be triggered occasionally and if the HTTP request fails, cells will be populated with ERR! values\n\n\nI recommend using proper tools (automation framework + scraping engine which can render JS powered websites) and use Google Sheets just for basic storage purposes:\nhttps://youtu.be/uBC752CWTew (Pipedream for automation + ScrapeNinja engine for scraping)\n']"
retrieve links from web page using python and BeautifulSoup [closed],"






Closed. This question needs details or clarity. It is not currently accepting answers.
                        
                    










Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



How can I retrieve the links of a webpage and copy the url address of the links using Python?
",317k,"
            177
        ","[""\nHere's a short snippet using the SoupStrainer class in BeautifulSoup:\nimport httplib2\nfrom bs4 import BeautifulSoup, SoupStrainer\n\nhttp = httplib2.Http()\nstatus, response = http.request('http://www.nytimes.com')\n\nfor link in BeautifulSoup(response, parse_only=SoupStrainer('a')):\n    if link.has_attr('href'):\n        print(link['href'])\n\nThe BeautifulSoup documentation is actually quite good, and covers a number of typical scenarios:\nhttps://www.crummy.com/software/BeautifulSoup/bs4/doc/\nEdit: Note that I used the SoupStrainer class because it's a bit more efficient (memory and speed wise), if you know what you're parsing in advance.\n"", '\nFor completeness sake, the BeautifulSoup 4 version, making use of the encoding supplied by the server as well:\nfrom bs4 import BeautifulSoup\nimport urllib.request\n\nparser = \'html.parser\'  # or \'lxml\' (preferred) or \'html5lib\', if installed\nresp = urllib.request.urlopen(""http://www.gpsbasecamp.com/national-parks"")\nsoup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param(\'charset\'))\n\nfor link in soup.find_all(\'a\', href=True):\n    print(link[\'href\'])\n\nor the Python 2 version:\nfrom bs4 import BeautifulSoup\nimport urllib2\n\nparser = \'html.parser\'  # or \'lxml\' (preferred) or \'html5lib\', if installed\nresp = urllib2.urlopen(""http://www.gpsbasecamp.com/national-parks"")\nsoup = BeautifulSoup(resp, parser, from_encoding=resp.info().getparam(\'charset\'))\n\nfor link in soup.find_all(\'a\', href=True):\n    print link[\'href\']\n\nand a version using the requests library, which as written will work in both Python 2 and 3:\nfrom bs4 import BeautifulSoup\nfrom bs4.dammit import EncodingDetector\nimport requests\n\nparser = \'html.parser\'  # or \'lxml\' (preferred) or \'html5lib\', if installed\nresp = requests.get(""http://www.gpsbasecamp.com/national-parks"")\nhttp_encoding = resp.encoding if \'charset\' in resp.headers.get(\'content-type\', \'\').lower() else None\nhtml_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\nencoding = html_encoding or http_encoding\nsoup = BeautifulSoup(resp.content, parser, from_encoding=encoding)\n\nfor link in soup.find_all(\'a\', href=True):\n    print(link[\'href\'])\n\nThe soup.find_all(\'a\', href=True) call finds all <a> elements that have an href attribute; elements without the attribute are skipped.\nBeautifulSoup 3 stopped development in March 2012; new projects really should use BeautifulSoup 4, always.\nNote that you should leave decoding the HTML from bytes to BeautifulSoup. You can inform BeautifulSoup of the characterset found in the HTTP response headers to assist in decoding, but this can be wrong and conflicting with a <meta> header info found in the HTML itself, which is why the above uses the BeautifulSoup internal class method EncodingDetector.find_declared_encoding() to make sure that such embedded encoding hints win over a misconfigured server.\nWith requests, the response.encoding attribute defaults to Latin-1 if the response has a text/* mimetype, even if no characterset was returned. This is consistent with the HTTP RFCs but painful when used with HTML parsing, so you should ignore that attribute when no charset is set in the Content-Type header.\n', '\nOthers have recommended BeautifulSoup, but it\'s much better to use lxml. Despite its name, it is also for parsing and scraping HTML. It\'s much, much faster than BeautifulSoup, and it even handles ""broken"" HTML better than BeautifulSoup (their claim to fame). It has a compatibility API for BeautifulSoup too if you don\'t want to learn the lxml API.\nIan Blicking agrees.\nThere\'s no reason to use BeautifulSoup anymore, unless you\'re on Google App Engine or something where anything not purely Python isn\'t allowed.\nlxml.html also supports CSS3 selectors so this sort of thing is trivial.\nAn example with lxml and xpath would look like this:\nimport urllib\nimport lxml.html\nconnection = urllib.urlopen(\'http://www.nytimes.com\')\n\ndom =  lxml.html.fromstring(connection.read())\n\nfor link in dom.xpath(\'//a/@href\'): # select the url in href for all a tags(links)\n    print link\n\n', '\nimport urllib2\nimport BeautifulSoup\n\nrequest = urllib2.Request(""http://www.gpsbasecamp.com/national-parks"")\nresponse = urllib2.urlopen(request)\nsoup = BeautifulSoup.BeautifulSoup(response)\nfor a in soup.findAll(\'a\'):\n  if \'national-park\' in a[\'href\']:\n    print \'found a url with national-park in the link\'\n\n', '\nThe following code is to retrieve all the links available in a webpage using urllib2 and BeautifulSoup4:\nimport urllib2\nfrom bs4 import BeautifulSoup\n\nurl = urllib2.urlopen(""http://www.espncricinfo.com/"").read()\nsoup = BeautifulSoup(url)\n\nfor line in soup.find_all(\'a\'):\n    print(line.get(\'href\'))\n\n', '\nLinks can be within a variety of attributes so you could pass a list of those attributes to select.\nFor example, with src and href attributes (here I am using the starts with ^ operator to specify that either of these attributes values starts with http):\nfrom bs4 import BeautifulSoup as bs\nimport requests\nr = requests.get(\'https://stackoverflow.com/\')\nsoup = bs(r.content, \'lxml\')\nlinks = [item[\'href\'] if item.get(\'href\') is not None else item[\'src\'] for item in soup.select(\'[href^=""http""], [src^=""http""]\') ]\nprint(links)\n\nAttribute = value selectors\n\n[attr^=value]\nRepresents elements with an attribute name of attr whose value is prefixed (preceded) by value.\n\nThere are also the commonly used $ (ends with) and * (contains) operators. For a full syntax list see the link above.\n', '\nUnder the hood BeautifulSoup now uses lxml. Requests, lxml & list comprehensions makes a killer combo.\nimport requests\nimport lxml.html\n\ndom = lxml.html.fromstring(requests.get(\'http://www.nytimes.com\').content)\n\n[x for x in dom.xpath(\'//a/@href\') if \'//\' in x and \'nytimes.com\' not in x]\n\nIn the list comp, the ""if \'//\' and \'url.com\' not in x"" is a simple method to scrub the url list of the sites \'internal\' navigation urls, etc.\n', '\njust for getting the links, without B.soup and regex:\nimport urllib2\nurl=""http://www.somewhere.com""\npage=urllib2.urlopen(url)\ndata=page.read().split(""</a>"")\ntag=""<a href=\\""""\nendtag=""\\"">""\nfor item in data:\n    if ""<a href"" in item:\n        try:\n            ind = item.index(tag)\n            item=item[ind+len(tag):]\n            end=item.index(endtag)\n        except: pass\n        else:\n            print item[:end]\n\nfor more complex operations, of course BSoup is still preferred.\n', ""\nThis script does what your looking for, But also resolves the relative links to absolute links.\nimport urllib\nimport lxml.html\nimport urlparse\n\ndef get_dom(url):\n    connection = urllib.urlopen(url)\n    return lxml.html.fromstring(connection.read())\n\ndef get_links(url):\n    return resolve_links((link for link in get_dom(url).xpath('//a/@href')))\n\ndef guess_root(links):\n    for link in links:\n        if link.startswith('http'):\n            parsed_link = urlparse.urlparse(link)\n            scheme = parsed_link.scheme + '://'\n            netloc = parsed_link.netloc\n            return scheme + netloc\n\ndef resolve_links(links):\n    root = guess_root(links)\n    for link in links:\n        if not link.startswith('http'):\n            link = urlparse.urljoin(root, link)\n        yield link  \n\nfor link in get_links('http://www.google.com'):\n    print link\n\n"", '\nTo find all the links, we will in this example use the urllib2 module together\nwith the re.module\n*One of the most powerful function in the re module is ""re.findall()"".\nWhile re.search() is used to find the first match for a pattern, re.findall() finds all\nthe matches and returns them as a list of strings, with each string representing one match*\nimport urllib2\n\nimport re\n#connect to a URL\nwebsite = urllib2.urlopen(url)\n\n#read html code\nhtml = website.read()\n\n#use re.findall to get all the links\nlinks = re.findall(\'""((http|ftp)s?://.*?)""\', html)\n\nprint links\n\n', '\nWhy not use regular expressions:\nimport urllib2\nimport re\nurl = ""http://www.somewhere.com""\npage = urllib2.urlopen(url)\npage = page.read()\nlinks = re.findall(r""<a.*?\\s*href=\\""(.*?)\\"".*?>(.*?)</a>"", page)\nfor link in links:\n    print(\'href: %s, HTML text: %s\' % (link[0], link[1]))\n\n', ""\nHere's an example using @ars accepted answer and the BeautifulSoup4, requests, and wget modules to handle the downloads.\nimport requests\nimport wget\nimport os\n\nfrom bs4 import BeautifulSoup, SoupStrainer\n\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/eeg_full/'\nfile_type = '.tar.gz'\n\nresponse = requests.get(url)\n\nfor link in BeautifulSoup(response.content, 'html.parser', parse_only=SoupStrainer('a')):\n    if link.has_attr('href'):\n        if file_type in link['href']:\n            full_path = url + link['href']\n            wget.download(full_path)\n\n"", ""\nI found the answer by @Blairg23 working , after the following correction (covering the scenario where it failed to work correctly):\nfor link in BeautifulSoup(response.content, 'html.parser', parse_only=SoupStrainer('a')):\n    if link.has_attr('href'):\n        if file_type in link['href']:\n            full_path =urlparse.urljoin(url , link['href']) #module urlparse need to be imported\n            wget.download(full_path)\n\nFor Python 3:\nurllib.parse.urljoin has to be used in order to obtain the full URL instead.\n"", '\nBeatifulSoup\'s own parser can be slow. It might be more feasible to use lxml which is capable of parsing directly from a URL (with some limitations mentioned below).\nimport lxml.html\n\ndoc = lxml.html.parse(url)\n\nlinks = doc.xpath(\'//a[@href]\')\n\nfor link in links:\n    print link.attrib[\'href\']\n\nThe code above will return the links as is, and in most cases they would be relative links or absolute from the site root. Since my use case was to only extract a certain type of links, below is a version that converts the links to full URLs and which optionally accepts a glob pattern like *.mp3. It won\'t handle single and double dots in the relative paths though, but so far I didn\'t have the need for it. If you need to parse URL fragments containing ../ or ./ then urlparse.urljoin might come in handy.\nNOTE: Direct lxml url parsing doesn\'t handle loading from https and doesn\'t do redirects, so for this reason the version below is using urllib2 + lxml.\n#!/usr/bin/env python\nimport sys\nimport urllib2\nimport urlparse\nimport lxml.html\nimport fnmatch\n\ntry:\n    import urltools as urltools\nexcept ImportError:\n    sys.stderr.write(\'To normalize URLs run: `pip install urltools --user`\')\n    urltools = None\n\n\ndef get_host(url):\n    p = urlparse.urlparse(url)\n    return ""{}://{}"".format(p.scheme, p.netloc)\n\n\nif __name__ == \'__main__\':\n    url = sys.argv[1]\n    host = get_host(url)\n    glob_patt = len(sys.argv) > 2 and sys.argv[2] or \'*\'\n\n    doc = lxml.html.parse(urllib2.urlopen(url))\n    links = doc.xpath(\'//a[@href]\')\n\n    for link in links:\n        href = link.attrib[\'href\']\n\n        if fnmatch.fnmatch(href, glob_patt):\n\n            if not href.startswith((\'http://\', \'https://\' \'ftp://\')):\n\n                if href.startswith(\'/\'):\n                    href = host + href\n                else:\n                    parent_url = url.rsplit(\'/\', 1)[0]\n                    href = urlparse.urljoin(parent_url, href)\n\n                    if urltools:\n                        href = urltools.normalize(href)\n\n            print href\n\nThe usage is as follows:\ngetlinks.py http://stackoverflow.com/a/37758066/191246\ngetlinks.py http://stackoverflow.com/a/37758066/191246 ""*users*""\ngetlinks.py http://fakedomain.mu/somepage.html ""*.mp3""\n\n', '\nThere can be many duplicate links together with both external and internal links.  To differentiate between the two and just get unique links using sets:\n# Python 3.\nimport urllib    \nfrom bs4 import BeautifulSoup\n\nurl = ""http://www.espncricinfo.com/""\nresp = urllib.request.urlopen(url)\n# Get server encoding per recommendation of Martijn Pieters.\nsoup = BeautifulSoup(resp, from_encoding=resp.info().get_param(\'charset\'))  \nexternal_links = set()\ninternal_links = set()\nfor line in soup.find_all(\'a\'):\n    link = line.get(\'href\')\n    if not link:\n        continue\n    if link.startswith(\'http\'):\n        external_links.add(link)\n    else:\n        internal_links.add(link)\n\n# Depending on usage, full internal links may be preferred.\nfull_internal_links = {\n    urllib.parse.urljoin(url, internal_link) \n    for internal_link in internal_links\n}\n\n# Print all unique external and full internal links.\nfor link in external_links.union(full_internal_links):\n    print(link)\n\n', '\nimport urllib2\nfrom bs4 import BeautifulSoup\na=urllib2.urlopen(\'http://dir.yahoo.com\')\ncode=a.read()\nsoup=BeautifulSoup(code)\nlinks=soup.findAll(""a"")\n#To get href part alone\nprint links[0].attrs[\'href\']\n\n']"
Difference between text and innerHTML using Selenium,"
What’s the difference between getting text and innerHTML when using Selenium?
Even though we have text under a particular element, when we perform .text we get empty values. But doing .get_attribute(""innerHTML"") works fine.
What is the difference between two? When should someone use '.get_attribute(""innerHTML"")' over .text?
",16k,"
            13
        ","['\nTo start with, text is a property where as innerHTML is an attribute. Fundamentally there are some differences between a property and an attribute.\n\nget_attribute(""innerHTML"")\nget_attribute(innerHTML) gets the innerHTML of the element.\nThis method will first try to return the value of a property with the given name. If a property with that name doesn’t exist, it returns the value of the attribute with the same name. If there’s no attribute with that name, None is returned.\nValues which are considered truthy, that is equals true or false, are returned as booleans. All other non-None values are returned as strings. For attributes or properties which do not exist, None is returned.\n\nArguments:\ninnerHTML - Name of the attribute/property to retrieve.\n\n\nExample:\n# Extract the text of an element.\nmy_text = target_element.get_attribute(""innerHTML"")\n\n\n\n\ntext\ntext gets the text of the element.\n\nDefinition:\ndef text(self):\n    """"""The text of the element.""""""\n    return self._execute(Command.GET_ELEMENT_TEXT)[\'value\']\n\n\nExample:\n# Extract the text of an element.\nmy_text = target_element.text\n\n\n\nDoes it still sound similar? Read below...\n\nAttributes and properties\nWhen the browser loads the page, it parses the HTML and generates DOM objects from it. For element nodes, most standard HTML attributes automatically become properties of DOM objects.\nFor instance, if the tag is:\n<body id=""page"">\n\nthen the DOM object has body.id=""page"".\n\nNote: The attribute-property mapping is not one-to-one!\n\n\nHTML attributes\nIn HTML, tags may have attributes. When the browser parses the HTML to create DOM objects for tags, it recognizes standard attributes and creates DOM properties from them.\nSo when an element has id or another standard attribute, the corresponding property gets created. But that doesn’t happen if the attribute is non-standard.\n\nNote: A standard attribute for one element can be unknown for another one. For instance, type is standard attribute for <input> tag, but not for <body> tag. Standard attributes are described in the specification for the corresponding element class.\n\nSo, if an attribute is non-standard, there won’t be a DOM-property for it. In that case all attributes are accessible by using the following methods:\n\nelem.hasAttribute(name): checks for existence.\nelem.getAttribute(name): gets the value.\nelem.setAttribute(name, value): sets the value.\nelem.removeAttribute(name): removes the attribute.\n\nAn example of reading a non-standard property:\n<body something=""non-standard"">\n  <script>\n    alert(document.body.getAttribute(\'something\')); // non-standard\n  </script>\n</body>\n\n\nProperty-attribute synchronization\nWhen a standard attribute changes, the corresponding property is auto-updated, and (with some exceptions) vice versa. But there are exclusions, for instance input.value synchronizes only from attribute -> to property, but not back. This feature actually comes in handy, because the user may modify value, and then after it, if we want to recover the ""original"" value from HTML, it’s in the attribute.\n\nAs per Attributes and Properties in Python when we reference an attribute of an object with something like someObject.someAttr, Python uses several special methods to get the someAttr attribute of the object. In the simplest case, attributes are simply instance variables.\nPython Attributes\nIn a broader perspective:\n\nAn attribute is a name that appears after an object name. This is the syntactic construct. For example, someObj.name.\nAn instance variable is an item in the internal __dict__ of an object.\nThe default semantics of an attribute reference is to provide access to the instance variable. When we mention someObj.name, the default behavior is effectively someObj.__dict__[\'name\']\n\nPython Properties\nIn Python we can bind getter, setter (and deleter) functions with an attribute name, using the built-in property() function or @property decorator. When we do this, each reference to an attribute has the syntax of direct access to an instance variable, but it invokes the given method function.\n', '\n.text will retrieve an empty string of the text in not present in the view port, so you can scroll the object into the viewport and try .text. It should retrieve the value.\nOn the contrary, innerhtml can get the value, even if it is present outside the view port.\n', '\nFor instance, <div><span>Example Text</span></div>.\n.get_attribute(""innerHTML"") gives you the actual HTML inside the current element. So theDivElement.get_attribute(""innerHTML"") returns ""<span>Example Text</span>"".\n.text gives you only text, not including the HTML node. So theDivElement.text returns ""Example Text"".\nPlease note that the algorithm for .text depends on webdriver of each browser. In some cases, such as element is hidden, you might get different text when you use a different webdriver.\nI usually get text from .get_attribute(""innerText"") instead of .text, so I can handle the all the cases.\n', '\nChrome (I\'m not sure about other browsers) ignores the extra spaces within the HTML code and displays them as a single space.\n<div><span>Example  Text</span></div> <!-- Notice the two spaces -->\n\n.get_attribute(\'innerHTML\') will return the double-spaced text, which is what you would see when you inspect element), while .text will return the string with only 1 space.\n>>> print(element.get_attribute(\'innerHTML\'))\n\'Example  Text\'\n>>> print(element.text)\n\'Example Text\'\n\nThis difference is not trivial as the following will result in a NoSuchElementException.\n>>> arg = \'//div[contains(text(),""Example Text"")]\'\n>>> driver.find_element_by_xpath(arg)\n\nSimilarly, .get_attribute(\'innerHTML\') for the following returns Example&nbsp;Text, while .text returns Example Text.\n<div><span>Example&nbsp;Text</span></div>\n\n', '\nI have just selected the CSS selector and used the below code:\nfrom selenium import webdriver\n\ndriver = webdriver.Chrome()\ndriver.maximize_window()\ndriver.get(""http://www.costco.com/Weatherproof%C2%AE-Men\'s-Ultra-Tech-Jacket.product.100106552.html"")\nprint driver.find_element_by_css_selector("".product-h1-container.visible-xl-block>h1"").text\n\nand it prints:\nWeatherproof® Men\'s Ultra Tech Jacket\n\nThe problem is h1[itemprop=\'name\'] selector on Google Chrome or Chrome are returning two matching nodes while .product-h1-container.visible-xl-block>h1 is returning only one matching node. That’s why it\'s printing what is expected.\nTo prove my point, run the below code:\nfrom selenium import webdriver\n\ndriver = webdriver.Chrome()\ndriver.maximize_window()\ndriver.get(""http://www.costco.com/Weatherproof%C2%AE-Men\'s-Ultra-Tech-Jacket.product.100106552.html"")\nx= driver.find_elements_by_css_selector(""h1[itemprop=\'name\'] "")\n\nfor i in x:\n    print ""This is line "" , i.text\n\nIt will print\nThis is line\nThis is line  Weatherproof® Men\'s Ultra Tech Jacket\n\nBecause select_element_by_css_selector selects the first element with matching selector and that does not contain any text so it does not print. Hope you understand now\n']"
How can I pass variable into an evaluate function?,"
I'm trying to pass a variable into a page.evaluate() function in Puppeteer, but when I use the following very simplified example, the variable evalVar is undefined.
I can't find any examples to build on, so I need help passing that variable into the page.evaluate() function so I can use it inside.
const puppeteer = require('puppeteer');

(async() => {

  const browser = await puppeteer.launch({headless: false});
  const page = await browser.newPage();

  const evalVar = 'WHUT??';

  try {

    await page.goto('https://www.google.com.au');
    await page.waitForSelector('#fbar');
    const links = await page.evaluate((evalVar) => {

      console.log('evalVar:', evalVar); // appears undefined

      const urls = [];
      hrefs = document.querySelectorAll('#fbar #fsl a');
      hrefs.forEach(function(el) {
        urls.push(el.href);
      });
      return urls;
    })
    console.log('links:', links);

  } catch (err) {

    console.log('ERR:', err.message);

  } finally {

    // browser.close();

  }

})();

",133k,"
            247
        ","['\nYou have to pass the variable as an argument to the pageFunction like this:\nconst links = await page.evaluate((evalVar) => {\n\n  console.log(evalVar); // 2. should be defined now\n  …\n\n}, evalVar); // 1. pass variable as an argument\n\nYou can pass in multiple variables by passing more arguments to  page.evaluate():\nawait page.evaluate((a, b c) => { console.log(a, b, c) }, a, b, c)\n\nThe arguments must either be serializable as JSON or JSHandles of in-browser objects: https://pptr.dev/#?show=api-pageevaluatepagefunction-args\n', ""\nI encourage you to stick on this style, because it's more convenient and readable.\nlet name = 'jack';\nlet age  = 33;\nlet location = 'Berlin/Germany';\n\nawait page.evaluate(({name, age, location}) => {\n\n    console.log(name);\n    console.log(age);\n    console.log(location);\n\n},{name, age, location});\n\n"", '\nSingle Variable:\nYou can pass one variable to page.evaluate() using the following syntax:\nawait page.evaluate(example => { /* ... */ }, example);\n\n\nNote: You do not need to enclose the variable in (), unless you are going to be passing multiple variables.\n\nMultiple Variables:\nYou can pass multiple variables to page.evaluate() using the following syntax:\nawait page.evaluate((example_1, example_2) => { /* ... */ }, example_1, example_2);\n\n\nNote: Enclosing your variables within {} is not necessary.\n\n', ""\nIt took me quite a while to figure out that console.log() in evaluate() can't show in node console. \nRef: https://github.com/GoogleChrome/puppeteer/issues/1944\n\neverything that is run inside the page.evaluate function is done in the context of the browser page. The script is running in the browser not in node.js so if you log it will show in the browsers console which if you are running headless you will not see. You also can't set a node breakpoint inside the function.\n\nHope this can help.\n"", ""\nFor pass a function, there are two ways you can do it.\n// 1. Defined in evaluationContext\nawait page.evaluate(() => {\n  window.yourFunc = function() {...};\n});\nconst links = await page.evaluate(() => {\n  const func = window.yourFunc;\n  func();\n});\n\n\n// 2. Transform function to serializable(string). (Function can not be serialized)\nconst yourFunc = function() {...};\nconst obj = {\n  func: yourFunc.toString()\n};\nconst otherObj = {\n  foo: 'bar'\n};\nconst links = await page.evaluate((obj, aObj) => {\n   const funStr = obj.func;\n   const func = new Function(`return ${funStr}.apply(null, arguments)`)\n   func();\n\n   const foo = aObj.foo; // bar, for object\n   window.foo = foo;\n   debugger;\n}, obj, otherObj);\n\nYou can add devtools: true to the launch options for test\n"", '\nI have a typescript example that could help someone new in typescript.\nconst hyperlinks: string [] = await page.evaluate((url: string, regex: RegExp, querySelect: string) => {\n.........\n}, url, regex, querySelect);\n\n', ""\nSlightly different version from @wolf answer above. Make code much more reusable between different context.\n// util functions\nexport const pipe = (...fns) => initialVal => fns.reduce((acc, fn) => fn(acc), initialVal)\nexport const pluck = key => obj => obj[key] || null\nexport const map = fn => item => fn(item)\n// these variables will be cast to string, look below at fn.toString()\n\nconst updatedAt = await page.evaluate(\n  ([selector, util]) => {\n    let { pipe, map, pluck } = util\n    pipe = new Function(`return ${pipe}`)()\n    map = new Function(`return ${map}`)()\n    pluck = new Function(`return ${pluck}`)()\n\n    return pipe(\n      s => document.querySelector(s),\n      pluck('textContent'),\n      map(text => text.trim()),\n      map(date => Date.parse(date)),\n      map(timeStamp => Promise.resolve(timeStamp))\n    )(selector)\n  },\n  [\n    '#table-announcements tbody td:nth-child(2) .d-none',\n    { pipe: pipe.toString(), map: map.toString(), pluck: pluck.toString() },\n  ]\n)\n\nAlso not that functions inside pipe cant used something like this\n// incorrect, which is i don't know why\npipe(document.querySelector) \n\n// should be \npipe(s => document.querySelector(s))\n\n""]"
How to find elements by class,"
I'm having trouble parsing HTML elements with ""class"" attribute using Beautifulsoup. The code looks like this
soup = BeautifulSoup(sdata)
mydivs = soup.findAll('div')
for div in mydivs: 
    if (div[""class""] == ""stylelistrow""):
        print div

I get an error on the same line ""after"" the script finishes. 
File ""./beautifulcoding.py"", line 130, in getlanguage
  if (div[""class""] == ""stylelistrow""):
File ""/usr/local/lib/python2.6/dist-packages/BeautifulSoup.py"", line 599, in __getitem__
   return self._getAttrMap()[key]
KeyError: 'class'

How do I get rid of this error?
",992k,"
            607
        ","['\nYou can refine your search to only find those divs with a given class using BS3:\nmydivs = soup.find_all(""div"", {""class"": ""stylelistrow""})\n\n', '\nFrom the documentation:\nAs of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument class_:\nsoup.find_all(""a"", class_=""sister"")\n\nWhich in this case would be:\nsoup.find_all(""div"", class_=""stylelistrow"")\n\nIt would also work for:\nsoup.find_all(""div"", class_=""stylelistrowone stylelistrowtwo"")\n\n', '\nUpdate: 2016\nIn the latest version of beautifulsoup, the method \'findAll\' has been renamed to \n\'find_all\'. Link to official documentation\n\nHence the answer will be \nsoup.find_all(""html_element"", class_=""your_class_name"")\n\n', '\nCSS selectors\nsingle class first match\nsoup.select_one(\'.stylelistrow\')\n\nlist of matches\nsoup.select(\'.stylelistrow\')\n\ncompound class (i.e. AND another class)\nsoup.select_one(\'.stylelistrow.otherclassname\')\nsoup.select(\'.stylelistrow.otherclassname\')\n\nSpaces in compound class names e.g. class = stylelistrow otherclassname are replaced with ""."". You can continue to add classes.\nlist of classes (OR - match whichever present)\nsoup.select_one(\'.stylelistrow, .otherclassname\')\nsoup.select(\'.stylelistrow, .otherclassname\')\n\nClass attribute whose values contains a string e.g. with ""stylelistrow"":\nstarts with ""style"":\n[class^=style]\n\nends with ""row""\n[class$=row]\n\ncontains ""list"":\n[class*=list]\n\nThe ^, $ and * are operators. Read more here: https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors\nIf you wanted to exclude this class then, with anchor tag as an example, selecting anchor tags without this class:\na:not(.stylelistrow)\n\nYou can pass simple, compound and complex css selectors lists inside of :not() pseudo class. See https://facelessuser.github.io/soupsieve/selectors/pseudo-classes/#:not\n\nbs4 4.7.1 +\nSpecific class whose innerText contains a string\nsoup.select_one(\'.stylelistrow:contains(""some string"")\')\nsoup.select(\'.stylelistrow:contains(""some string"")\')\n\nN.B.\nsoupsieve 2.1.0 + Dec\'2020 onwards\n\nNEW: In order to avoid conflicts with future CSS specification\nchanges, non-standard pseudo classes will now start with the :-soup-\nprefix. As a consequence, :contains() will now be known as\n:-soup-contains(), though for a time the deprecated form of\n:contains() will still be allowed with a warning that users should\nmigrate over to :-soup-contains().\nNEW: Added new non-standard pseudo class :-soup-contains-own() which\noperates similar to :-soup-contains() except that it only looks at\ntext nodes directly associated with the currently scoped element and\nnot its descendants.\n\nSpecific class which has a certain child element e.g. a tag\nsoup.select_one(\'.stylelistrow:has(a)\')\nsoup.select(\'.stylelistrow:has(a)\')\n\n', '\nSpecific to BeautifulSoup 3:\nsoup.findAll(\'div\',\n             {\'class\': lambda x: x \n                       and \'stylelistrow\' in x.split()\n             }\n            )\n\nWill find all of these:\n<div class=""stylelistrow"">\n<div class=""stylelistrow button"">\n<div class=""button stylelistrow"">\n\n', ""\nA straight forward way would be :\nsoup = BeautifulSoup(sdata)\nfor each_div in soup.findAll('div',{'class':'stylelist'}):\n    print each_div\n\nMake sure you take of the casing of findAll, its not findall\n"", '\n\nHow to find elements by class\nI\'m having trouble parsing html elements with ""class"" attribute using Beautifulsoup.\n\nYou can easily find by one class, but if you want to find by the intersection of two classes, it\'s a little more difficult,\nFrom the documentation (emphasis added):\n\nIf you want to search for tags that match two or more CSS classes, you should use a CSS selector:\ncss_soup.select(""p.strikeout.body"")\n# [<p class=""body strikeout""></p>]\n\n\nTo be clear, this selects only the p tags that are both strikeout and body class.\nTo find for the intersection of any in a set of classes (not the intersection, but the union), you can give a list to the class_ keyword argument (as of 4.1.2):\nsoup = BeautifulSoup(sdata)\nclass_list = [""stylelistrow""] # can add any other classes to this list.\n# will find any divs with any names in class_list:\nmydivs = soup.find_all(\'div\', class_=class_list) \n\nAlso note that findAll has been renamed from the camelCase to the more Pythonic find_all.\n', ""\nUse class_= If you want to find element(s) without stating the HTML tag.\nFor single element:\nsoup.find(class_='my-class-name')\n\nFor multiple elements:\nsoup.find_all(class_='my-class-name')\n\n"", ""\nAs of BeautifulSoup 4+ ,\nIf you have a single class name , you can just pass the class name as parameter like :\nmydivs = soup.find_all('div', 'class_name')\n\nOr if you have more than one class names , just pass the list of class names as parameter like :\nmydivs = soup.find_all('div', ['class1', 'class2'])\n\n"", '\nthe following worked for me\na_tag = soup.find_all(""div"",class_=\'full tabpublist\')\n\n', ""\nThis works for me to access the class attribute (on beautifulsoup 4, contrary to what the documentation says). The KeyError comes a list being returned not a dictionary.\nfor hit in soup.findAll(name='span'):\n    print hit.contents[1]['class']\n\n"", '\nOther answers did not work for me.\nIn other answers the findAll is being used on the soup object itself, but I needed a way to do a find by class name on objects inside a specific element extracted from the object I obtained after doing findAll.\nIf you are trying to do a search inside nested HTML elements to get objects by class name, try below -\n# parse html\npage_soup = soup(web_page.read(), ""html.parser"")\n\n# filter out items matching class name\nall_songs = page_soup.findAll(""li"", ""song_item"")\n\n# traverse through all_songs\nfor song in all_songs:\n\n    # get text out of span element matching class \'song_name\'\n    # doing a \'find\' by class name within a specific song element taken out of \'all_songs\' collection\n    song.find(""span"", ""song_name"").text\n\nPoints to note:\n\nI\'m not explicitly defining the search to be on \'class\' attribute findAll(""li"", {""class"": ""song_item""}), since it\'s the only attribute I\'m searching on and it will by default search for class attribute if you don\'t exclusively tell which attribute you want to find on. \nWhen you do a findAll or find, the resulting object is of class bs4.element.ResultSet which is a subclass of list. You can utilize all methods of ResultSet, inside any number of nested elements (as long as they are of type ResultSet) to do a find or find all.\nMy BS4 version - 4.9.1, Python version - 3.8.1\n\n', '\nConcerning @Wernight\'s comment on the top answer about partial matching...\nYou can partially match:\n\n<div class=""stylelistrow""> and\n<div class=""stylelistrow button"">\n\nwith gazpacho:\nfrom gazpacho import Soup\n\nmy_divs = soup.find(""div"", {""class"": ""stylelistrow""}, partial=True)\n\nBoth will be captured and returned as a list of Soup objects.\n', '\nAlternatively we can use lxml, it support xpath and very fast!\nfrom lxml import html, etree \n\nattr = html.fromstring(html_text)#passing the raw html\nhandles = attr.xpath(\'//div[@class=""stylelistrow""]\')#xpath exresssion to find that specific class\n\nfor each in handles:\n    print(etree.tostring(each))#printing the html as string\n\n', '\nsingle\nsoup.find(""form"",{""class"":""c-login__form""})\n\nmultiple\nres=soup.find_all(""input"")\nfor each in res:\n    print(each)\n\n', '\nTry to check if the div has a class attribute first, like this:\nsoup = BeautifulSoup(sdata)\nmydivs = soup.findAll(\'div\')\nfor div in mydivs:\n    if ""class"" in div:\n        if (div[""class""]==""stylelistrow""):\n            print div\n\n', '\nThis worked for me:\nfor div in mydivs:\n    try:\n        clazz = div[""class""]\n    except KeyError:\n        clazz = """"\n    if (clazz == ""stylelistrow""):\n        print div\n\n', '\nThis should work:\nsoup = BeautifulSoup(sdata)\nmydivs = soup.findAll(\'div\')\nfor div in mydivs: \n    if (div.find(class_ == ""stylelistrow""):\n        print div\n\n', ""\nThe following should work\nsoup.find('span', attrs={'class':'totalcount'})\n\nreplace 'totalcount' with your class name and 'span' with tag you are looking for. Also, if your class contains multiple names with space, just choose one and use.\nP.S. This finds the first element with given criteria. If you want to find all elements then replace 'find' with 'find_all'.\n""]"
How can I efficiently parse HTML with Java?,"
I do a lot of HTML parsing in my line of work. Up until now, I was using the HtmlUnit headless browser for parsing and browser automation.
Now, I want to separate both the tasks.
I want to use a light HTML parser because it takes much time in HtmlUnit to first load a page, then get the source and then parse it.
I want to know which HTML parser can parse HTML efficiently. I need

Speed
Ease to locate any HtmlElement by its ""id"" or ""name"" or ""tag type"".

It would be ok for me if it doesn't clean the dirty HTML code. I don't need to clean any HTML source. I just need an easiest way to move across HtmlElements and harvest data from them.
",204k,"
            206
        ","['\nSelf plug: I have just released a new Java HTML parser: jsoup. I mention it here because I think it will do what you are after.\nIts party trick is a CSS selector syntax to find elements, e.g.:\nString html = ""<html><head><title>First parse</title></head>""\n  + ""<body><p>Parsed HTML into a doc.</p></body></html>"";\nDocument doc = Jsoup.parse(html);\nElements links = doc.select(""a"");\nElement head = doc.select(""head"").first();\n\nSee the Selector javadoc for more info.\nThis is a new project, so any ideas for improvement are very welcome!\n', ""\nThe best I've seen so far is HtmlCleaner:\n\nHtmlCleaner is open-source HTML parser written in Java. HTML found on Web is usually dirty, ill-formed and unsuitable for further processing. For any serious consumption of such documents, it is necessary to first clean up the mess and bring the order to tags, attributes and ordinary text. For the given HTML document, HtmlCleaner reorders individual elements and produces well-formed XML. By default, it follows similar rules that the most of web browsers use in order to create Document Object Model. However, user may provide custom tag and rule set for tag filtering and balancing.\n\nWith HtmlCleaner you can locate any element using XPath.\nFor other html parsers see this SO question.\n"", ""\nI suggest Validator.nu's parser, based on the HTML5 parsing algorithm. It is the parser used in Mozilla from 2010-05-03\n""]"
selenium with scrapy for dynamic page,"
I'm trying to scrape product information from a webpage, using scrapy. My to-be-scraped webpage looks like this:

starts with a product_list page with 10 products
a click on ""next""  button loads the next 10 products (url doesn't change between the two pages)
i use LinkExtractor to follow each product link into the product page, and get all the information I need

I tried to replicate the next-button-ajax-call but can't get working, so I'm giving selenium a try. I can run selenium's webdriver in a separate script, but I don't know how to integrate with scrapy. Where shall I put the selenium part in my scrapy spider? 
My spider is pretty standard, like the following:
class ProductSpider(CrawlSpider):
    name = ""product_spider""
    allowed_domains = ['example.com']
    start_urls = ['http://example.com/shanghai']
    rules = [
        Rule(SgmlLinkExtractor(restrict_xpaths='//div[@id=""productList""]//dl[@class=""t2""]//dt'), callback='parse_product'),
        ]

    def parse_product(self, response):
        self.log(""parsing product %s"" %response.url, level=INFO)
        hxs = HtmlXPathSelector(response)
        # actual data follows

Any idea is appreciated. Thank you!
",109k,"
            100
        ","['\nIt really depends on how do you need to scrape the site and how and what data do you want to get. \nHere\'s an example how you can follow pagination on ebay using Scrapy+Selenium:\nimport scrapy\nfrom selenium import webdriver\n\nclass ProductSpider(scrapy.Spider):\n    name = ""product_spider""\n    allowed_domains = [\'ebay.com\']\n    start_urls = [\'http://www.ebay.com/sch/i.html?_odkw=books&_osacat=0&_trksid=p2045573.m570.l1313.TR0.TRC0.Xpython&_nkw=python&_sacat=0&_from=R40\']\n\n    def __init__(self):\n        self.driver = webdriver.Firefox()\n\n    def parse(self, response):\n        self.driver.get(response.url)\n\n        while True:\n            next = self.driver.find_element_by_xpath(\'//td[@class=""pagn-next""]/a\')\n\n            try:\n                next.click()\n\n                # get the data and write it to scrapy items\n            except:\n                break\n\n        self.driver.close()\n\nHere are some examples of ""selenium spiders"":\n\nExecuting Javascript Submit form functions using scrapy in python\nhttps://gist.github.com/cheekybastard/4944914\nhttps://gist.github.com/irfani/1045108\nhttp://snipplr.com/view/66998/\n\n\nThere is also an alternative to having to use Selenium with Scrapy. In some cases, using ScrapyJS middleware is enough to handle the dynamic parts of a page. Sample real-world usage:\n\nScraping dynamic content using python-Scrapy\n\n', '\nIf (url doesn\'t change between the two pages) then you should add dont_filter=True with your scrapy.Request() or scrapy will find this url as a duplicate after processing first page. \nIf you need to render pages with javascript you should use scrapy-splash, you can also check this scrapy middleware which can handle javascript pages using selenium or you can do that by launching any headless browser\nBut more effective and faster solution is inspect your browser and see what requests are made during submitting a form or triggering a certain event. Try to simulate the same requests as your browser sends. If you can replicate the request(s) correctly you will get the data you need.\nHere is an example :\nclass ScrollScraper(Spider):\n    name = ""scrollingscraper""\n\n    quote_url = ""http://quotes.toscrape.com/api/quotes?page=""\n    start_urls = [quote_url + ""1""]\n\n    def parse(self, response):\n        quote_item = QuoteItem()\n        print response.body\n        data = json.loads(response.body)\n        for item in data.get(\'quotes\', []):\n            quote_item[""author""] = item.get(\'author\', {}).get(\'name\')\n            quote_item[\'quote\'] = item.get(\'text\')\n            quote_item[\'tags\'] = item.get(\'tags\')\n            yield quote_item\n\n        if data[\'has_next\']:\n            next_page = data[\'page\'] + 1\n            yield Request(self.quote_url + str(next_page))\n\nWhen pagination url is same for every pages & uses POST request then you can use scrapy.FormRequest() instead of scrapy.Request(), both are same but FormRequest adds a new argument (formdata=) to the constructor. \nHere is another spider example form this post:\nclass SpiderClass(scrapy.Spider):\n    # spider name and all\n    name = \'ajax\'\n    page_incr = 1\n    start_urls = [\'http://www.pcguia.pt/category/reviews/#paginated=1\']\n    pagination_url = \'http://www.pcguia.pt/wp-content/themes/flavor/functions/ajax.php\'\n\n    def parse(self, response):\n\n        sel = Selector(response)\n\n        if self.page_incr > 1:\n            json_data = json.loads(response.body)\n            sel = Selector(text=json_data.get(\'content\', \'\'))\n\n        # your code here\n\n        # pagination code starts here\n        if sel.xpath(\'//div[@class=""panel-wrapper""]\'):\n            self.page_incr += 1\n            formdata = {\n                \'sorter\': \'recent\',\n                \'location\': \'main loop\',\n                \'loop\': \'main loop\',\n                \'action\': \'sort\',\n                \'view\': \'grid\',\n                \'columns\': \'3\',\n                \'paginated\': str(self.page_incr),\n                \'currentquery[category_name]\': \'reviews\'\n            }\n            yield FormRequest(url=self.pagination_url, formdata=formdata, callback=self.parse)\n        else:\n            return\n\n']"
How to scrape only visible webpage text with BeautifulSoup?,"
Basically, I want to use BeautifulSoup to grab strictly the visible text on a webpage. For instance, this webpage is my test case. And I mainly want to just get the body text (article) and maybe even a few tab names here and there. I have tried the suggestion in this SO question that returns lots of <script> tags and html comments which I don't want. I can't figure out the arguments I need for the function findAll() in order to just get the visible texts on a webpage.
So, how should I find all visible text excluding scripts, comments, css etc.?
",155k,"
            144
        ","['\nTry this:\nfrom bs4 import BeautifulSoup\nfrom bs4.element import Comment\nimport urllib.request\n\n\ndef tag_visible(element):\n    if element.parent.name in [\'style\', \'script\', \'head\', \'title\', \'meta\', \'[document]\']:\n        return False\n    if isinstance(element, Comment):\n        return False\n    return True\n\n\ndef text_from_html(body):\n    soup = BeautifulSoup(body, \'html.parser\')\n    texts = soup.findAll(text=True)\n    visible_texts = filter(tag_visible, texts)  \n    return u"" "".join(t.strip() for t in visible_texts)\n\nhtml = urllib.request.urlopen(\'http://www.nytimes.com/2009/12/21/us/21storm.html\').read()\nprint(text_from_html(html))\n\n', ""\nThe approved answer from @jbochi does not work for me.  The str() function call raises an exception because it cannot encode the non-ascii characters in the BeautifulSoup element.  Here is a more succinct way to filter the example web page to visible text.\nhtml = open('21storm.html').read()\nsoup = BeautifulSoup(html)\n[s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\nvisible_text = soup.getText()\n\n"", '\nimport urllib\nfrom bs4 import BeautifulSoup\n\nurl = ""https://www.yahoo.com""\nhtml = urllib.urlopen(url).read()\nsoup = BeautifulSoup(html)\n\n# kill all script and style elements\nfor script in soup([""script"", ""style""]):\n    script.extract()    # rip it out\n\n# get text\ntext = soup.get_text()\n\n# break into lines and remove leading and trailing space on each\nlines = (line.strip() for line in text.splitlines())\n# break multi-headlines into a line each\nchunks = (phrase.strip() for line in lines for phrase in line.split(""  ""))\n# drop blank lines\ntext = \'\\n\'.join(chunk for chunk in chunks if chunk)\n\nprint(text.encode(\'utf-8\'))\n\n', '\nI completely respect using Beautiful Soup to get rendered content, but it may not be the ideal package for acquiring the rendered content on a page.\nI had a similar problem to get rendered content, or the visible content in a typical browser.  In particular I had many perhaps atypical cases to work with such a simple example below.  In this case the non displayable tag is nested in a style tag, and is not visible in many browsers that I have checked.  Other variations exist such as defining a class tag setting display to none.  Then using this class for the div. \n<html>\n  <title>  Title here</title>\n\n  <body>\n\n    lots of text here <p> <br>\n    <h1> even headings </h1>\n\n    <style type=""text/css""> \n        <div > this will not be visible </div> \n    </style>\n\n\n  </body>\n\n</html>\n\nOne solution posted above is: \nhtml = Utilities.ReadFile(\'simple.html\')\nsoup = BeautifulSoup.BeautifulSoup(html)\ntexts = soup.findAll(text=True)\nvisible_texts = filter(visible, texts)\nprint(visible_texts)\n\n\n[u\'\\n\', u\'\\n\', u\'\\n\\n        lots of text here \', u\' \', u\'\\n\', u\' even headings \', u\'\\n\', u\' this will not be visible \', u\'\\n\', u\'\\n\']\n\nThis solution certainly has applications in many cases and does the job quite well generally but in the html posted above it retains the text that is not rendered.  After searching SO a couple solutions came up here BeautifulSoup get_text does not strip all tags and JavaScript  and here Rendered HTML to plain text using Python\nI tried both these solutions: html2text and nltk.clean_html and was surprised by the timing results so thought they warranted an answer for posterity.  Of course, the speeds highly depend on the contents of the data...\nOne answer here from @Helge was about using nltk of all things.  \nimport nltk\n\n%timeit nltk.clean_html(html)\nwas returning 153 us per loop\n\nIt worked really well to return a string with rendered html.  This nltk module was faster than even html2text, though perhaps html2text is more robust. \nbetterHTML = html.decode(errors=\'ignore\')\n%timeit html2text.html2text(betterHTML)\n%3.09 ms per loop\n\n', ""\nUsing BeautifulSoup the easiest way with less code to just get the strings, without empty lines and crap.\ntag = <Parent_Tag_that_contains_the_data>\nsoup = BeautifulSoup(tag, 'html.parser')\n\nfor i in soup.stripped_strings:\n    print repr(i)\n\n"", '\nIf you care about performance, here\'s another more efficient way:\nimport re\n\nINVISIBLE_ELEMS = (\'style\', \'script\', \'head\', \'title\')\nRE_SPACES = re.compile(r\'\\s{3,}\')\n\ndef visible_texts(soup):\n    """""" get visible text from a document """"""\n    text = \' \'.join([\n        s for s in soup.strings\n        if s.parent.name not in INVISIBLE_ELEMS\n    ])\n    # collapse multiple spaces to two spaces.\n    return RE_SPACES.sub(\'  \', text)\n\nsoup.strings is an iterator, and it returns NavigableString so that you can check the parent\'s tag name directly, without going through multiple loops.\n', '\nWhile, i would completely suggest using beautiful-soup in general, if anyone is looking to display the visible parts of a malformed html (e.g. where you have just a segment or line of a web-page) for whatever-reason, the the following will remove content between < and > tags:\nimport re   ## only use with malformed html - this is not efficient\ndef display_visible_html_using_re(text):             \n    return(re.sub(""(\\<.*?\\>)"", """",text))\n\n', '\nThe title is inside an <nyt_headline> tag, which is nested inside an <h1> tag and a <div> tag with id ""article"".  \nsoup.findAll(\'nyt_headline\', limit=1)\n\nShould work.\nThe article body is inside an <nyt_text> tag, which is nested inside a <div> tag with id ""articleBody"".  Inside the <nyt_text>  element, the text itself is contained within <p>  tags.  Images are not within those <p> tags.  It\'s difficult for me to experiment with the syntax, but I expect a working scrape to look something like this.\ntext = soup.findAll(\'nyt_text\', limit=1)[0]\ntext.findAll(\'p\')\n\n', '\nfrom bs4 import BeautifulSoup\nfrom bs4.element import Comment\nimport urllib.request\nimport re\nimport ssl\n\ndef tag_visible(element):\n    if element.parent.name in [\'style\', \'script\', \'head\', \'title\', \'meta\', \'[document]\']:\n        return False\n    if isinstance(element, Comment):\n        return False\n    if re.match(r""[\\n]+"",str(element)): return False\n    return True\ndef text_from_html(url):\n    body = urllib.request.urlopen(url,context=ssl._create_unverified_context()).read()\n    soup = BeautifulSoup(body ,""lxml"")\n    texts = soup.findAll(text=True)\n    visible_texts = filter(tag_visible, texts)  \n    text = u"","".join(t.strip() for t in visible_texts)\n    text = text.lstrip().rstrip()\n    text = text.split(\',\')\n    clean_text = \'\'\n    for sen in text:\n        if sen:\n            sen = sen.rstrip().lstrip()\n            clean_text += sen+\',\'\n    return clean_text\nurl = \'http://www.nytimes.com/2009/12/21/us/21storm.html\'\nprint(text_from_html(url))\n\n', '\nThe simplest way to handle this case is by using getattr().  You can adapt this example to your needs:\nfrom bs4 import BeautifulSoup\n\nsource_html = """"""\n<span class=""ratingsDisplay"">\n    <a class=""ratingNumber"" href=""https://www.youtube.com/watch?v=oHg5SJYRHA0"" target=""_blank"" rel=""noopener"">\n        <span class=""ratingsContent"">3.7</span>\n    </a>\n</span>\n""""""\n\nsoup = BeautifulSoup(source_html, ""lxml"")\nmy_ratings = getattr(soup.find(\'span\', {""class"": ""ratingsContent""}), ""text"", None)\nprint(my_ratings)\n\nThis will find the text element,""3.7"", within the tag object <span class=""ratingsContent"">3.7</span> when it exists, however, default to NoneType when it does not.\n\ngetattr(object, name[, default])\nReturn the value of the named attribute of object. name must be a string. If the string is the name of one of the object’s attributes, the result is the value of that attribute. For example, getattr(x, \'foobar\') is equivalent to x.foobar. If the named attribute does not exist, default is returned if provided, otherwise, AttributeError is raised.\n\n', ""\nUPDATE\nFrom the docs: As of Beautiful Soup version 4.9.0, when lxml or html.parser are in use, the contents of <script>, <style>, and <template> tags are generally not considered to be ‘text’, since those tags are not part of the human-visible content of the page.\nTo get all the human readable text of the HTML <body> you can use .get_text(), to get rid of redundant whitespaces, etc. set strip parameter and join/separate all by a single whitespace:\nimport bs4, requests\n\nresponse = requests.get('https://www.nytimes.com/interactive/2022/09/13/us/politics/congress-stock-trading-investigation.html',headers={'User-Agent': 'Mozilla/5.0','cache-control': 'max-age=0'}, cookies={'cookies':''})\nsoup = bs4.BeautifulSoup(response.text)\n\nsoup.article.get_text(' ', strip=True)\n\nIn newer code avoid old syntax findAll() instead use find_all() or select() with css selectors - For more take a minute to check docs\n""]"
Scraping html tables into R data frames using the XML package,"
How do I scrape html tables using the XML package?
Take, for example, this wikipedia page on the Brazilian soccer team. I would like to read it in R and get the ""list of all matches Brazil have played against FIFA recognised teams"" table as a data.frame. How can I do this?
",126k,"
            161
        ","['\n…or a shorter try:\nlibrary(XML)\nlibrary(RCurl)\nlibrary(rlist)\ntheurl <- getURL(""https://en.wikipedia.org/wiki/Brazil_national_football_team"",.opts = list(ssl.verifypeer = FALSE) )\ntables <- readHTMLTable(theurl)\ntables <- list.clean(tables, fun = is.null, recursive = FALSE)\nn.rows <- unlist(lapply(tables, function(t) dim(t)[1]))\n\nthe picked table is the longest one on the page\ntables[[which.max(n.rows)]]\n\n', '\nlibrary(RCurl)\nlibrary(XML)\n\n# Download page using RCurl\n# You may need to set proxy details, etc.,  in the call to getURL\ntheurl <- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""\nwebpage <- getURL(theurl)\n# Process escape characters\nwebpage <- readLines(tc <- textConnection(webpage)); close(tc)\n\n# Parse the html tree, ignoring errors on the page\npagetree <- htmlTreeParse(webpage, error=function(...){})\n\n# Navigate your way through the tree. It may be possible to do this more efficiently using getNodeSet\nbody <- pagetree$children$html$children$body \ndivbodyContent <- body$children$div$children[[1]]$children$div$children[[4]]\ntables <- divbodyContent$children[names(divbodyContent)==""table""]\n\n#In this case, the required table is the only one with class ""wikitable sortable""  \ntableclasses <- sapply(tables, function(x) x$attributes[""class""])\nthetable  <- tables[which(tableclasses==""wikitable sortable"")]$table\n\n#Get columns headers\nheaders <- thetable$children[[1]]$children\ncolumnnames <- unname(sapply(headers, function(x) x$children$text$value))\n\n# Get rows from table\ncontent <- c()\nfor(i in 2:length(thetable$children))\n{\n   tablerow <- thetable$children[[i]]$children\n   opponent <- tablerow[[1]]$children[[2]]$children$text$value\n   others <- unname(sapply(tablerow[-1], function(x) x$children$text$value)) \n   content <- rbind(content, c(opponent, others))\n}\n\n# Convert to data frame\ncolnames(content) <- columnnames\nas.data.frame(content)\n\nEdited to add:\nSample output\n                     Opponent Played Won Drawn Lost Goals for Goals against \xa0% Won\n    1               Argentina     94  36    24   34       148           150  38.3%\n    2                Paraguay     72  44    17   11       160            61  61.1%\n    3                 Uruguay     72  33    19   20       127            93  45.8%\n    ...\n\n', '\nThe rvest along with xml2 is another popular package for parsing html web pages.\nlibrary(rvest)\ntheurl <- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""\nfile<-read_html(theurl)\ntables<-html_nodes(file, ""table"")\ntable1 <- html_table(tables[4], fill = TRUE)\n\nThe syntax is easier to use than the xml package and for most web pages the package provides all of the options ones needs.\n', '\nAnother option using Xpath.\nlibrary(RCurl)\nlibrary(XML)\n\ntheurl <- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""\nwebpage <- getURL(theurl)\nwebpage <- readLines(tc <- textConnection(webpage)); close(tc)\n\npagetree <- htmlTreeParse(webpage, error=function(...){}, useInternalNodes = TRUE)\n\n# Extract table header and contents\ntablehead <- xpathSApply(pagetree, ""//*/table[@class=\'wikitable sortable\']/tr/th"", xmlValue)\nresults <- xpathSApply(pagetree, ""//*/table[@class=\'wikitable sortable\']/tr/td"", xmlValue)\n\n# Convert character vector to dataframe\ncontent <- as.data.frame(matrix(results, ncol = 8, byrow = TRUE))\n\n# Clean up the results\ncontent[,1] <- gsub(""Â\xa0"", """", content[,1])\ntablehead <- gsub(""Â\xa0"", """", tablehead)\nnames(content) <- tablehead\n\nProduces this result\n> head(content)\n   Opponent Played Won Drawn Lost Goals for Goals against % Won\n1 Argentina     94  36    24   34       148           150 38.3%\n2  Paraguay     72  44    17   11       160            61 61.1%\n3   Uruguay     72  33    19   20       127            93 45.8%\n4     Chile     64  45    12    7       147            53 70.3%\n5      Peru     39  27     9    3        83            27 69.2%\n6    Mexico     36  21     6    9        69            34 58.3%\n\n']"
How to use Python requests to fake a browser visit a.k.a and generate User Agent?,"
I want to get the content from this website.
If I use a browser like Firefox or Chrome I could get the real website page I want, but if I use the Python requests package (or wget command) to get it, it returns a totally different HTML page.
I thought the developer of the website had made some blocks for this.
Question
How do I fake a browser visit by using python requests or command wget?
",318k,"
            178
        ","[""\nProvide a User-Agent header:\nimport requests\n\nurl = 'http://www.ichangtou.com/#company:data_000008.html'\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n\nresponse = requests.get(url, headers=headers)\nprint(response.content)\n\nFYI, here is a list of User-Agent strings for different browsers:\n\nList of all Browsers\n\n\nAs a side note, there is a pretty useful third-party package called fake-useragent that provides a nice abstraction layer over user agents:\n\nfake-useragent\nUp to date simple useragent faker with real world database\n\nDemo:\n>>> from fake_useragent import UserAgent\n>>> ua = UserAgent()\n>>> ua.chrome\nu'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36'\n>>> ua.random\nu'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36'\n\n"", '\nI used fake UserAgent.\nHow to use:\nfrom fake_useragent import UserAgent\nimport requests\n   \n\nua = UserAgent()\nprint(ua.chrome)\nheader = {\'User-Agent\':str(ua.chrome)}\nprint(header)\nurl = ""https://www.hybrid-analysis.com/recent-submissions?filter=file&sort=^timestamp""\nhtmlContent = requests.get(url, headers=header)\nprint(htmlContent)\n\nOutput:\nMozilla/5.0 (Macintosh; Intel Mac OS X 10_8_2) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1309.0 Safari/537.17\n{\'User-Agent\': \'Mozilla/5.0 (X11; OpenBSD i386) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\'}\n<Response [200]>\n\n', '\nTry doing this, using firefox as fake user agent (moreover, it\'s a good startup script for web scraping with the use of cookies):\n#!/usr/bin/env python2\n# -*- coding: utf8 -*-\n# vim:ts=4:sw=4\n\n\nimport cookielib, urllib2, sys\n\ndef doIt(uri):\n    cj = cookielib.CookieJar()\n    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n    page = opener.open(uri)\n    page.addheaders = [(\'User-agent\', \'Mozilla/5.0\')]\n    print page.read()\n\nfor i in sys.argv[1:]:\n    doIt(i)\n\nUSAGE:\npython script.py ""http://www.ichangtou.com/#company:data_000008.html""\n\n', '\nThe root of the answer is that the person asking the question needs to have a JavaScript interpreter to get what they are after. What I have found is I am able to get all of the information I wanted on a website in json before it was interpreted by JavaScript. This has saved me a ton of time in what would be parsing html hoping each webpage is in the same format.\nSo when you get a response from a website using requests really look at the html/text because you might find the javascripts JSON in the footer ready to be parsed. \n', '\nI use pyuser_agent. this package use get user agnet\nimport pyuser_agent\nimport requests\n\nua = pyuser_agent.UA()\n\nheaders = {\n      ""User-Agent"" : ua.random\n}\nprint(headers)\n\nuri = ""https://github.com/THAVASIGTI/""\nres = requests.request(""GET"",uri,headers=headers)\nprint(res)\n\nconsole out\n{\'User-Agent\': \'Mozilla/5.0 (Windows; U; Windows NT 6.1; zh-CN) AppleWebKit/533+ (KHTML, like Gecko)\'}\n<Response [200]>\n\n', ""\nAnswer\nYou need to create a header with a proper formatted User agent String, it server to communicate client-server.\nYou can check your own user agent Here.\nExample\nMozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0\nMozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0\n\nThird party Package user_agent 0.1.9 \nI found this module very simple to use, in one line of code it randomly generates a User agent string.\nfrom user_agent import generate_user_agent, generate_navigator\nfrom pprint import pprint\n\nprint(generate_user_agent())\n# 'Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.3; Win64; x64)'\n\nprint(generate_user_agent(os=('mac', 'linux')))\n# 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:36.0) Gecko/20100101 Firefox/36.0'\n\npprint(generate_navigator())\n\n# {'app_code_name': 'Mozilla',\n#  'app_name': 'Netscape',\n#  'appversion': '5.0',\n#  'name': 'firefox',\n#  'os': 'linux',\n#  'oscpu': 'Linux i686 on x86_64',\n#  'platform': 'Linux i686 on x86_64',\n#  'user_agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686 on x86_64; rv:41.0) Gecko/20100101 Firefox/41.0',\n#  'version': '41.0'}\n\npprint(generate_navigator_js())\n\n# {'appCodeName': 'Mozilla',\n#  'appName': 'Netscape',\n#  'appVersion': '38.0',\n#  'platform': 'MacIntel',\n#  'userAgent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:38.0) Gecko/20100101 Firefox/38.0'}\n\n"", ""\nUser agent is ok but he wants to fetch a JavaScript site.we can use selenium but it is annoying to setup and maintain so the best way to fetch a JavaScript rendered page is requests_html module. Which is a superset of the well known request module. To install use pip\npip install requests-html\n\nAnd to fetch a JavaScript rendered page use\nfrom requests_html import HTMLSession\nsession = HTMLSession()\nr = session.get('https://python.org/')\n\nHope it will help. It uses puppter to render javascript and also it downloads chromium but you don't have to worry everything is happening under the hood.you will get the end result.\n"", ""\nI had a similar issue but I was unable to use the UserAgent class inside the fake_useragent module. I was running the code inside a docker container\nimport requests\nimport ujson\nimport random\n\nresponse = requests.get('https://fake-useragent.herokuapp.com/browsers/0.1.11')\nagents_dictionary = ujson.loads(response.text)\nrandom_browser_number = str(random.randint(0, len(agents_dictionary['randomize'])))\nrandom_browser = agents_dictionary['randomize'][random_browser_number]\nuser_agents_list = agents_dictionary['browsers'][random_browser]\nuser_agent = user_agents_list[random.randint(0, len(user_agents_list)-1)]\n\nI targeted the endpoint used in the module. This solution still gave me a random user agent however there is the possibility that the data structure at the endpoint could change.\n"", '\nThis is how, I have been using a random user agent from a list of nearlly 1000 fake user agents\nfrom random_user_agent.user_agent import UserAgent\nfrom random_user_agent.params import SoftwareName, OperatingSystem\nsoftware_names = [SoftwareName.ANDROID.value]\noperating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value, OperatingSystem.MAC.value]   \n\nuser_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=1000)\n\n# Get list of user agents.\nuser_agents = user_agent_rotator.get_user_agents()\n\nuser_agent_random = user_agent_rotator.get_random_user_agent()\n\nExample\nprint(user_agent_random)\n\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\nFor more details visit this link\n']"
Is it ok to scrape data from Google results? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 5 years ago.







                        Improve this question
                    



I'd like to fetch results from Google using curl to detect potential duplicate content.
Is there a high risk of being banned by Google?
",134k,"
            76
        ","['\nGoogle disallows automated access in their TOS, so if you accept their terms you would break them.\nThat said, I know of no lawsuit from Google against a scraper.\nEven Microsoft scraped Google, they powered their search engine Bing with it. They got caught in 2011 red handed :)\nThere are two options to scrape Google results:\n1) Use their API\n\nUPDATE 2020: Google has reprecated previous APIs (again) and has new\nprices and new limits. Now\n(https://developers.google.com/custom-search/v1/overview) you can\nquery up to 10k results per day at 1,500 USD per month, more than that\nis not permitted and the results are not what they display in normal\nsearches.\n\n\nYou can issue around 40 requests per hour You are limited to what\nthey give you, it\'s not really useful if you want to track ranking\npositions or what a real user would see. That\'s something you are not\nallowed to gather.\n\nIf you want a higher amount of API requests you need to pay.\n\n60 requests per hour cost 2000 USD per year, more queries require a\ncustom deal.\n\n\n2) Scrape the normal result pages\n\nHere comes the tricky part. It is possible to scrape the normal result pages.\nGoogle does not allow it.\nIf you scrape at a rate higher than 8 (updated from 15) keyword requests per hour you risk detection, higher than 10/h (updated from 20) will get you blocked from my experience.\nBy using multiple IPs you can up the rate, so with 100 IP addresses you can scrape up to 1000 requests per hour. (24k a day) (updated)\nThere is an open source search engine scraper written in PHP at http://scraping.compunect.com\nIt allows to reliable scrape Google, parses the results properly and manages IP addresses, delays, etc.\nSo if you can use PHP it\'s a nice kickstart, otherwise the code will still be useful to learn how it is done.\n\n3) Alternatively use a scraping service (updated)\n\nRecently a customer of mine had a huge search engine scraping requirement but it was not \'ongoing\', it\'s more like one huge refresh per month.\nIn this case I could not find a self-made solution that\'s \'economic\'.\nI used the service at http://scraping.services instead.\nThey also provide open source code and so far it\'s running well (several thousand resultpages per hour during the refreshes)\nThe downside is that such a service means that your solution is ""bound"" to one professional supplier, the upside is that it was a lot cheaper than the other options I evaluated (and faster in our case)\nOne option to reduce the dependency on one company is to make two approaches at the same time. Using the scraping service as primary source of data and falling back to a proxy based solution like described at 2) when required.\n\n', '\nGoogle will eventually block your IP when you exceed a certain amount of requests. \n', '\nGoogle thrives on scraping websites of the world...so if it was ""so illegal"" then even Google won\'t survive ..of course other answers mention ways of mitigating IP blocks by Google. One more way to explore avoiding captcha could be scraping at random times (dint try) ..Moreover, I have a feeling, that if we provide novelty or some significant processing of data then it sounds fine at least to me...if we are simply copying a website.. or hampering its business/brand in some way...then it is bad and should be avoided..on top of it all...if you are a startup then no one will fight you as there is no benefit.. but if your entire premise is on scraping even when you are funded then you should think of more sophisticated ways...alternative APIs..eventually..Also Google keeps releasing (or depricating)  fields for its API so what you want to scrap now may be in roadmap of new Google API releases..\n']"
can we use XPath with BeautifulSoup?,"
I am using BeautifulSoup to scrape an URL and I had the following code, to find the td tag whose class is 'empformbody':
import urllib
import urllib2
from BeautifulSoup import BeautifulSoup

url =  ""http://www.example.com/servlet/av/ResultTemplate=AVResult.html""
req = urllib2.Request(url)
response = urllib2.urlopen(req)
the_page = response.read()
soup = BeautifulSoup(the_page)

soup.findAll('td',attrs={'class':'empformbody'})

Now in the above code we can use findAll to get tags and information related to them, but I want to use XPath. Is it possible to use XPath with BeautifulSoup? If possible, please provide me example code.
",275k,"
            152
        ","['\nNope, BeautifulSoup, by itself, does not support XPath expressions.\nAn alternative library, lxml, does support XPath 1.0. It has a BeautifulSoup compatible mode where it\'ll try and parse broken HTML the way Soup does. However, the default lxml HTML parser does just as good a job of parsing broken HTML, and I believe is faster.\nOnce you\'ve parsed your document into an lxml tree, you can use the .xpath() method to search for elements.\ntry:\n    # Python 2\n    from urllib2 import urlopen\nexcept ImportError:\n    from urllib.request import urlopen\nfrom lxml import etree\n\nurl =  ""http://www.example.com/servlet/av/ResultTemplate=AVResult.html""\nresponse = urlopen(url)\nhtmlparser = etree.HTMLParser()\ntree = etree.parse(response, htmlparser)\ntree.xpath(xpathselector)\n\nThere is also a dedicated lxml.html() module with additional functionality.\nNote that in the above example I passed the response object directly to lxml, as having the parser read directly from the stream is more efficient than reading the response into a large string first. To do the same with the requests library, you want to set stream=True and pass in the response.raw object after enabling transparent transport decompression:\nimport lxml.html\nimport requests\n\nurl =  ""http://www.example.com/servlet/av/ResultTemplate=AVResult.html""\nresponse = requests.get(url, stream=True)\nresponse.raw.decode_content = True\ntree = lxml.html.parse(response.raw)\n\nOf possible interest to you is the CSS Selector support; the CSSSelector class translates CSS statements into XPath expressions, making your search for td.empformbody that much easier:\nfrom lxml.cssselect import CSSSelector\n\ntd_empformbody = CSSSelector(\'td.empformbody\')\nfor elem in td_empformbody(tree):\n    # Do something with these table cells.\n\nComing full circle: BeautifulSoup itself does have very complete CSS selector support:\nfor cell in soup.select(\'table#foobar td.empformbody\'):\n    # Do something with these table cells.\n\n', '\nI can confirm that there is no XPath support within Beautiful Soup.\n', '\nAs others have said, BeautifulSoup doesn\'t have xpath support.  There are probably a number of ways to get something from an xpath, including using Selenium.  However, here\'s a solution that works in either Python 2 or 3:\nfrom lxml import html\nimport requests\n\npage = requests.get(\'http://econpy.pythonanywhere.com/ex/001.html\')\ntree = html.fromstring(page.content)\n#This will create a list of buyers:\nbuyers = tree.xpath(\'//div[@title=""buyer-name""]/text()\')\n#This will create a list of prices\nprices = tree.xpath(\'//span[@class=""item-price""]/text()\')\n\nprint(\'Buyers: \', buyers)\nprint(\'Prices: \', prices)\n\nI used this as a reference.\n', ""\nBeautifulSoup has a function named findNext from current element directed childern,so:\nfather.findNext('div',{'class':'class_value'}).findNext('div',{'id':'id_value'}).findAll('a') \n\nAbove code can imitate the following xpath:\ndiv[class=class_value]/div[id=id_value]\n\n"", '\nfrom lxml import etree\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(open(\'path of your localfile.html\'),\'html.parser\')\ndom = etree.HTML(str(soup))\nprint dom.xpath(\'//*[@id=""BGINP01_S1""]/section/div/font/text()\')\n\nAbove used the combination of Soup object with lxml and one can extract the value using xpath\n', '\nwhen you use lxml all simple:\ntree = lxml.html.fromstring(html)\ni_need_element = tree.xpath(\'//a[@class=""shared-components""]/@href\')\n\nbut when use BeautifulSoup BS4 all simple too:\n\nfirst remove ""//"" and ""@""  \nsecond - add star before ""=""\n\ntry this magic:\nsoup = BeautifulSoup(html, ""lxml"")\ni_need_element = soup.select (\'a[class*=""shared-components""]\')\n\nas you see, this does not support sub-tag, so i remove ""/@href"" part\n', ""\nI've searched through their docs and it seems there is no XPath option.\nAlso, as you can see here on a similar question on SO, the OP is asking for a translation from XPath to BeautifulSoup, so my conclusion would be - no, there is no XPath parsing available.\n"", '\nMaybe you can try the following without XPath\nfrom simplified_scrapy.simplified_doc import SimplifiedDoc \nhtml = \'\'\'\n<html>\n<body>\n<div>\n    <h1>Example Domain</h1>\n    <p>This domain is for use in illustrative examples in documents. You may use this\n    domain in literature without prior coordination or asking for permission.</p>\n    <p><a href=""https://www.iana.org/domains/example"">More information...</a></p>\n</div>\n</body>\n</html>\n\'\'\'\n# What XPath can do, so can it\ndoc = SimplifiedDoc(html)\n# The result is the same as doc.getElementByTag(\'body\').getElementByTag(\'div\').getElementByTag(\'h1\').text\nprint (doc.body.div.h1.text)\nprint (doc.div.h1.text)\nprint (doc.h1.text) # Shorter paths will be faster\nprint (doc.div.getChildren())\nprint (doc.div.getChildren(\'p\'))\n\n', '\nThis is a pretty old thread, but there is a work-around solution now, which may not have been in BeautifulSoup at the time. \nHere is an example of what I did. I use the ""requests"" module to read an RSS feed and get its text content in a variable called ""rss_text"". With that, I run it thru BeautifulSoup, search for the xpath /rss/channel/title, and retrieve its contents. It\'s not exactly XPath in all its glory (wildcards, multiple paths, etc.), but if you just have a basic path you want to locate, this works. \nfrom bs4 import BeautifulSoup\nrss_obj = BeautifulSoup(rss_text, \'xml\')\ncls.title = rss_obj.rss.channel.title.get_text()\n\n', ""\nuse   soup.find(class_='myclass')\n""]"
Using python Requests with javascript pages,"
I am trying to use the Requests framework with python (http://docs.python-requests.org/en/latest/) but the page I am trying to get to uses javascript to fetch the info that I want. 
I have tried to search on the web for a solution but the fact that I am searching with the keyword javascript most of the stuff I am getting is how to scrape with the javascript language.
Is there anyway to use the requests framework with pages that use javascript?
",167k,"
            83
        ","[""\nGood news: there is now a requests module that supports javascript:  https://pypi.org/project/requests-html/\nfrom requests_html import HTMLSession\n\nsession = HTMLSession()\n\nr = session.get('http://www.yourjspage.com')\n\nr.html.render()  # this call executes the js in the page\n\nAs a bonus this wraps BeautifulSoup, I think, so you can do things like\nr.html.find('#myElementID').text\n\nwhich returns the content of the HTML element as you'd expect.\n"", '\nYou are going to have to make the same request (using the Requests library) that the javascript is making.  You can use any number of tools (including those built into Chrome and Firefox) to inspect the http request that is coming from javascript and simply make this request yourself from Python.\n', '\nWhile Selenium might seem tempting and useful, it has one main problem that can\'t be fixed: performance. By calculating every single thing a browser does, you will need a lot more power. Even PhantomJS does not compete with a simple request. I recommend that you will only use Selenium when you really need to click buttons. If you only need javascript, I recommend PyQt (check https://www.youtube.com/watch?v=FSH77vnOGqU to learn it).\nHowever, if you want to use Selenium, I recommend Chrome over PhantomJS. Many users have problems with PhantomJS where a website simply does not work in Phantom. Chrome can be headless (non-graphical) too!\nFirst, make sure you have installed ChromeDriver, which Selenium depends on for using Google Chrome.\nThen, make sure you have Google Chrome of version 60 or higher by checking it in the URL chrome://settings/help\nNow, all you need to do is the following code:\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium import webdriver\n\nchrome_options = Options()\nchrome_options.add_argument(""--headless"")\n\ndriver = webdriver.Chrome(chrome_options=chrome_options)\n\nIf you do not know how to use Selenium, here is a quick overview:\ndriver.get(""https://www.google.com"") #Browser goes to google.com\n\nFinding elements:\nUse either the ELEMENTS or ELEMENT method. Examples:\ndriver.find_element_by_css_selector(""div.logo-subtext"") #Find your country in Google. (singular)\n\n\ndriver.find_element(s)_by_css_selector(css_selector) # Every element that matches this CSS selector\ndriver.find_element(s)_by_class_name(class_name) # Every element with the following class\ndriver.find_element(s)_by_id(id) # Every element with the following ID\ndriver.find_element(s)_by_link_text(link_text) # Every  with the full link text\ndriver.find_element(s)_by_partial_link_text(partial_link_text) # Every  with partial link text.\ndriver.find_element(s)_by_name(name) # Every element where name=argument\ndriver.find_element(s)_by_tag_name(tag_name) # Every element with the tag name argument\n\nOk! I found an element (or elements list). But what do I do now?\nHere are the methods you can do on an element elem:\n\nelem.tag_name # Could return button in a .\nelem.get_attribute(""id"") # Returns the ID of an element.\nelem.text # The inner text of an element.\nelem.clear() # Clears a text input.\nelem.is_displayed() # True for visible elements, False for invisible elements.\nelem.is_enabled() # True for an enabled input, False otherwise.\nelem.is_selected() # Is this radio button or checkbox element selected?\nelem.location # A dictionary representing the X and Y location of an element on the screen.\nelem.click() # Click elem.\nelem.send_keys(""thelegend27"") # Type thelegend27 into elem (useful for text inputs)\nelem.submit() # Submit the form in which elem takes part.\n\nSpecial commands:\n\ndriver.back() # Click the Back button.\ndriver.forward() # Click the Forward button.\ndriver.refresh() # Refresh the page.\ndriver.quit() # Close the browser including all the tabs.\nfoo = driver.execute_script(""return \'hello\';"") # Execute javascript (COULD TAKE RETURN VALUES!)\n\n', '\nUsing Selenium or jQuery enabled requests are slow. It is more efficient to find out which cookie is generated after website checking for JavaScript on the browser and get that cookie and use it for each of your requests.\nIn one example it worked through following cookies:\nthe cookie generated after checking for javascript for this example is ""cf_clearance"".\nso  simply create a session.\nupdate cookie and headers as such:\ns = requests.Session()\ns.cookies[""cf_clearance""] = ""cb4c883efc59d0e990caf7508902591f4569e7bf-1617321078-0-150""\ns.headers.update({\n            ""user-agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) \n               AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36""\n        })\ns.get(url)\n\nand you are good to go no need for JavaScript solution such as Selenium. This is way faster and efficient. you just have to get cookie once after opening up the browser.\n', '\nSome way to do that is to invoke your request by using selenium.\nLet\'s install dependecies by using pip or pip3:\npip install selenium\netc.\nIf you run script by using python3\nuse instead:\npip3 install selenium\n(...)\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ndriver = webdriver.Chrome(ChromeDriverManager().install())\nurl = \'http://myurl.com\'\n\n# Please wait until the page will be ready:\nelement = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, ""div.some_placeholder"")))\nelement.text = \'Some text on the page :)\' # <-- Here it is! I got what I wanted :)\n\n', '\nits a wrapper around pyppeteer or smth? :( i thought its something different\n    @property\n    async def browser(self):\n        if not hasattr(self, ""_browser""):\n            self._browser = await pyppeteer.launch(ignoreHTTPSErrors=not(self.verify), headless=True, args=self.__browser_args)\n\n        return self._browser\n\n']"
How does reCAPTCHA 3 know I'm using Selenium/chromedriver?,"
I'm curious how reCAPTCHA v3 works. Specifically the browser fingerprinting.
When I launch an instance of Chrome through Selenium/chromedriver and test against reCAPTCHA 3 (https://recaptcha-demo.appspot.com/recaptcha-v3-request-scores.php) I always get a score of 0.1 when using Selenium/chromedriver.
When using incognito with a normal instance, I get 0.3.
I've beaten other detection systems by injecting JavaScript and modifying the web driver object and recompiling webdriver from source and modifying the $cdc_ variables.
I can see what looks like some obfuscated POST back to the server, so I'm going to start digging there.
What might it be looking for to determine if I'm running Selenium/chromedriver?
",80k,"
            42
        ","['\nreCaptcha\nWebsites can easily detect the network traffic and identify your program as a BOT. Google have already released 5(five) reCAPTCHA to choose from when creating a new site. While four of them are active and reCAPTCHA v1 being shutdown.\n\nreCAPTCHA versions and types\n\nreCAPTCHA v3 (verify requests with a score): reCAPTCHA v3 allows you to verify if an interaction is legitimate without any user interaction. It is a pure JavaScript API returning a score, giving you the ability to take action in the context of your site: for instance requiring additional factors of authentication, sending a post to moderation, or throttling bots that may be scraping content.\nreCAPTCHA v2 - ""I\'m not a robot"" Checkbox: The ""I\'m not a robot"" Checkbox requires the user to click a checkbox indicating the user is not a robot. This will either pass the user immediately (with No CAPTCHA) or challenge them to validate whether or not they are human. This is the simplest option to integrate with and only requires two lines of HTML to render the checkbox.\n\n\n\nreCAPTCHA v2 - Invisible reCAPTCHA badge: The invisible reCAPTCHA badge does not require the user to click on a checkbox, instead it is invoked directly when the user clicks on an existing button on your site or can be invoked via a JavaScript API call. The integration requires a JavaScript callback when reCAPTCHA verification is complete. By default only the most suspicious traffic will be prompted to solve a captcha. To alter this behavior edit your site security preference under advanced settings.\n\n\n\nreCAPTCHA v2 - Android: The reCAPTCHA Android library is part of the Google Play services SafetyNet APIs. This library provides native Android APIs that you can integrate directly into an app. You should set up Google Play services in your app and connect to the GoogleApiClient before invoking the reCAPTCHA API. This will either pass the user through immediately (without a CAPTCHA prompt) or challenge them to validate whether they are human. \nreCAPTCHA v1: reCAPTCHA v1 has been shut down since March 2018.\n\n\nSolution\nHowever there are some generic approaches to avoid getting detected while web-scraping:\n\nThe first and foremost attribute a website can determine your script/program is through your monitor size. So it is recommended not to use the conventional Viewport.\nIf you need to send multiple requests to a website keep on changing the User Agent on each request. Here you can find a detailed discussion on Way to change Google Chrome user agent in Selenium?\nTo simulate human like behavior you may require to slow down the script execution even beyond WebDriverWait and expected_conditions inducing time.sleep(secs). Here you can find a detailed discussion on How to sleep webdriver in python for milliseconds\n\n\nOutro\nSome food for thought:\n\nSelenium webdriver: Modifying navigator.webdriver flag to prevent selenium detection\nUnable to use Selenium to automate Chase site login\nConfidence Score of the request using reCAPTCHA v3 API\n\n', '\nSelenium and Puppeteer have some browser configurations that is different from a non-automated browser. Also, since some JavaScript functions are injected into browser to manipulate elements, you need to create some override to avoid detections.\nThere are some good articles explaining some points about Selenium and Puppeteer detection while it runs on a site with detection mechanisms:\nDetecting Chrome headless, new techniques - You can use it to write defensive code for your bot.\nIt is not possible to detect and block Google Chrome headless - it explains in a clear and sound way the differences that JavaScript code can detect between a browser launched by automated software and a real one, and also how to fake it.\nGitHub - headless-cat-n-mouse - Example using Puppeteer + Python to avoid detection\n']"
CasperJS/PhantomJS doesn't load https page,"
I know there are certain web pages PhantomJS/CasperJS can't open, and I was wondering if this one was one of them: https://maizepages.umich.edu. CasperJS gives an error: PhantomJS failed to open page status=fail.
I tried ignoring-ssl-errors and changing my user agent but I'm not sure how to determine which ones to use.
All I'm doing right now is the basic casper setup with casper.start(url, function () { ... }) where url=https://maizepages.umich.edu;
",19k,"
            24
        ","['\nThe problem may be related to the recent discovery of a SSLv3 vulnerability (POODLE). Website owners were forced to remove SSLv3 support from their websites. Since PhantomJS < v1.9.8 uses SSLv3 by default, you should use TLSv1:\ncasperjs --ssl-protocol=tlsv1 yourScript.js\n\nThe catchall solution would be to use any for when newer PhantomJS versions come along with other SSL protocols. But this would make the POODLE vulnerability exploitable on sites which haven\'t yet disabled SSLv3.\ncasperjs --ssl-protocol=any yourScript.js\n\nAlternative method: Update to PhantomJS 1.9.8 or higher. Note that updating to PhantomJS 1.9.8 leads to a new bug, which is especially annoying for CasperJS.\nHow to verify: Add a resource.error event handler like this at the beginning of your script:\ncasper.on(""resource.error"", function(resourceError){\n    console.log(\'Unable to load resource (#\' + resourceError.id + \'URL:\' + resourceError.url + \')\');\n    console.log(\'Error code: \' + resourceError.errorCode + \'. Description: \' + resourceError.errorString);\n});\n\nIf it is indeed a problem with SSLv3 the error will be something like:\n\nError code: 6. Description: SSL handshake failed\n\n\nAs an aside, you also might want to run with the --ignore-ssl-errors=true commandline option, when there is something wrong with the certificate.\n']"
Scrape multiple urls using QWebPage,"
I'm using Qt's QWebPage to render a page that uses javascript to update its content dynamically - so a library that just downloads a static version of the page (such as urllib2) won't work.
My problem is, when I render a second page, about 99% of the time the program just crashes. At other times, it will work three times before crashing. I've also gotten a few segfaults, but it is all very random.
My guess is the object I'm using to render isn't getting deleted properly, so trying to reuse it is possibly causing some problems for myself. I've looked all over and no one really seems to be having this same issue.
Here's the code I'm using. The program downloads web pages from steam's community market so I can create a database of all the items. I need to call the getItemsFromPage function multiple times to get all of the items, as they are broken up into pages (showing results 1-10 out of X amount).
import csv
import re
import sys
from string import replace
from bs4 import BeautifulSoup
from PyQt4.QtGui import *
from PyQt4.QtCore import *
from PyQt4.QtWebKit import *

class Item:
    __slots__ = (""name"", ""count"", ""price"", ""game"")

    def __repr__(self):
        return self.name + ""("" + str(self.count) + "")""

    def __str__(self):
        return self.name + "", "" + str(self.count) + "", $"" + str(self.price)

class Render(QWebPage):  
    def __init__(self, url):
        self.app = QApplication(sys.argv)
        QWebPage.__init__(self)
        self.loadFinished.connect(self._loadFinished)
        self.mainFrame().load(QUrl(url))
        self.app.exec_()

    def _loadFinished(self, result):
        self.frame = self.mainFrame()
        self.app.quit()
        self.deleteLater()

def getItemsFromPage(appid, page=1):

    r = Render(""http://steamcommunity.com/market/search?q=appid:"" + str(appid) + ""#p"" + str(page))

    soup = BeautifulSoup(str(r.frame.toHtml().toUtf8()))

    itemLst = soup.find_all(""div"", ""market_listing_row market_recent_listing_row"")

    items = []

    for k in itemLst:
        i = Item()

        i.name = k.find(""span"", ""market_listing_item_name"").string
        i.count = int(replace(k.find(""span"", ""market_listing_num_listings_qty"").string, "","", """"))
        i.price = float(re.search(r'\$([0-9]+\.[0-9]+)', str(k)).group(1))
        i.game = appid

        items.append(i)

    return items

if __name__ == ""__main__"":

    print ""Updating market items to dota2.csv ...""

    i = 1

    with open(""dota2.csv"", ""w"") as f:
        writer = csv.writer(f)

        r = None

        while True:
            print ""Page "" + str(i)

            items = getItemsFromPage(570)

            if len(items) == 0:
                print ""No items found, stopping...""
                break

            for k in items:
                writer.writerow((k.name, k.count, k.price, k.game))

            i += 1

    print ""Done.""

Calling getItemsFromPage once works fine. Subsequent calls give me my problem. The output of the program is typically
Updating market items to dota2.csv ...
Page 1
Page 2

and then it crashes. It should go on for over 700 pages. 
",3k,"
            6
        ","[""\nThe problem with your program is that you are attempting to create a new QApplication with every url you fetch.\nInstead, only one QApplication and one WebPage should be created. The WebPage can use its loadFinished signal to create an internal loop by fetching a new url after each one has been processed. Custom html processing can be added by connecting a user-defined slot to a signal which emits the html text and the url when they become available. The scripts below (for PyQt5 and PyQt4) show how to implement this.\nHere are some examples which show how to use the WebPage class:\nUsage:\ndef my_html_processor(html, url):\n    print('loaded: [%d chars] %s' % (len(html), url))\n\nimport sys\napp = QApplication(sys.argv)\nwebpage = WebPage(verbose=False)\nwebpage.htmlReady.connect(my_html_processor)\n\n# example 1: process list of urls\n\nurls = ['https://en.wikipedia.org/wiki/Special:Random'] * 3\nprint('Processing list of urls...')\nwebpage.process(urls)\n\n# example 2: process one url continuously\n#\n# import signal, itertools\n# signal.signal(signal.SIGINT, signal.SIG_DFL)\n#\n# print('Processing url continuously...')\n# print('Press Ctrl+C to quit')\n#\n# url = 'https://en.wikipedia.org/wiki/Special:Random'\n# webpage.process(itertools.repeat(url))\n\nsys.exit(app.exec_())\n\nPyQt5 WebPage:\nfrom PyQt5.QtCore import pyqtSignal, QUrl\nfrom PyQt5.QtWidgets import QApplication\nfrom PyQt5.QtWebEngineWidgets import QWebEnginePage\n\nclass WebPage(QWebEnginePage):\n    htmlReady = pyqtSignal(str, str)\n\n    def __init__(self, verbose=False):\n        super().__init__()\n        self._verbose = verbose\n        self.loadFinished.connect(self.handleLoadFinished)\n\n    def process(self, urls):\n        self._urls = iter(urls)\n        self.fetchNext()\n\n    def fetchNext(self):\n        try:\n            url = next(self._urls)\n        except StopIteration:\n            return False\n        else:\n            self.load(QUrl(url))\n        return True\n\n    def processCurrentPage(self, html):\n        self.htmlReady.emit(html, self.url().toString())\n        if not self.fetchNext():\n            QApplication.instance().quit()\n\n    def handleLoadFinished(self):\n        self.toHtml(self.processCurrentPage)\n\n    def javaScriptConsoleMessage(self, *args, **kwargs):\n        if self._verbose:\n            super().javaScriptConsoleMessage(*args, **kwargs)\n\nPyQt4 WebPage:\nfrom PyQt4.QtCore import pyqtSignal, QUrl\nfrom PyQt4.QtGui import QApplication\nfrom PyQt4.QtWebKit import QWebPage\n\nclass WebPage(QWebPage):\n    htmlReady = pyqtSignal(str, str)\n\n    def __init__(self, verbose=False):\n        super(WebPage, self).__init__()\n        self._verbose = verbose\n        self.mainFrame().loadFinished.connect(self.handleLoadFinished)\n\n    def start(self, urls):\n        self._urls = iter(urls)\n        self.fetchNext()\n\n    def fetchNext(self):\n        try:\n            url = next(self._urls)\n        except StopIteration:\n            return False\n        else:\n            self.mainFrame().load(QUrl(url))\n        return True\n\n    def processCurrentPage(self):\n        self.htmlReady.emit(\n            self.mainFrame().toHtml(), self.mainFrame().url().toString())\n        print('loaded: [%d bytes] %s' % (self.bytesReceived(), url))\n\n    def handleLoadFinished(self):\n        self.processCurrentPage()\n        if not self.fetchNext():\n            QApplication.instance().quit()\n\n    def javaScriptConsoleMessage(self, *args, **kwargs):\n        if self._verbose:\n            super(WebPage, self).javaScriptConsoleMessage(*args, **kwargs)\n\n""]"
Web scraping with Python [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I'd like to grab daily sunrise/sunset times from a web site. Is it possible to scrape web content with Python? what are the modules used? Is there any tutorial available?
",213k,"
            196
        ","[""\nUse urllib2 in combination with the brilliant BeautifulSoup library:\nimport urllib2\nfrom BeautifulSoup import BeautifulSoup\n# or if you're using BeautifulSoup4:\n# from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(urllib2.urlopen('http://example.com').read())\n\nfor row in soup('table', {'class': 'spad'})[0].tbody('tr'):\n    tds = row('td')\n    print tds[0].string, tds[1].string\n    # will print date and sunrise\n\n"", ""\nI'd really recommend Scrapy.\nQuote from a deleted answer:\n\n\nScrapy crawling is fastest than mechanize because uses asynchronous operations (on top of Twisted).\nScrapy has better and fastest support for parsing (x)html on top of libxml2.\nScrapy is a mature framework with full unicode, handles redirections, gzipped responses, odd encodings, integrated http cache, etc.\nOnce you are into Scrapy, you can write a spider in less than 5 minutes that download images, creates thumbnails and export the extracted data directly to csv or json.\n\n\n"", '\nI collected together scripts from my web scraping work into this bit-bucket library.\nExample script for your case:\nfrom webscraping import download, xpath\nD = download.Download()\n\nhtml = D.get(\'http://example.com\')\nfor row in xpath.search(html, \'//table[@class=""spad""]/tbody/tr\'):\n    cols = xpath.search(row, \'/td\')\n    print \'Sunrise: %s, Sunset: %s\' % (cols[1], cols[2])\n\nOutput:\nSunrise: 08:39, Sunset: 16:08\nSunrise: 08:39, Sunset: 16:09\nSunrise: 08:39, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:11\nSunrise: 08:40, Sunset: 16:12\nSunrise: 08:40, Sunset: 16:13\n\n', ""\nI would strongly suggest checking out pyquery. It uses jquery-like (aka css-like) syntax which makes things really easy for those coming from that background.\nFor your case, it would be something like:\nfrom pyquery import *\n\nhtml = PyQuery(url='http://www.example.com/')\ntrs = html('table.spad tbody tr')\n\nfor tr in trs:\n  tds = tr.getchildren()\n  print tds[1].text, tds[2].text\n\nOutput:\n5:16 AM 9:28 PM\n5:15 AM 9:30 PM\n5:13 AM 9:31 PM\n5:12 AM 9:33 PM\n5:11 AM 9:34 PM\n5:10 AM 9:35 PM\n5:09 AM 9:37 PM\n\n"", ""\nYou can use urllib2 to make the HTTP requests, and then you'll have web content.\nYou can get it like this:\nimport urllib2\nresponse = urllib2.urlopen('http://example.com')\nhtml = response.read()\n\nBeautiful Soup is a python HTML parser that is supposed to be good for screen scraping.\nIn particular, here is their tutorial on parsing an HTML document.\nGood luck!\n"", '\nI use a combination of Scrapemark (finding urls - py2) and httlib2 (downloading images - py2+3). The scrapemark.py has 500 lines of code, but uses regular expressions, so it may be not so fast, did not test.\nExample for scraping your website:\n\nimport sys\nfrom pprint import pprint\nfrom scrapemark import scrape\n\npprint(scrape(""""""\n    <table class=""spad"">\n        <tbody>\n            {*\n                <tr>\n                    <td>{{[].day}}</td>\n                    <td>{{[].sunrise}}</td>\n                    <td>{{[].sunset}}</td>\n                    {# ... #}\n                </tr>\n            *}\n        </tbody>\n    </table>\n"""""", url=sys.argv[1] ))\n\nUsage:\npython2 sunscraper.py http://www.example.com/\n\nResult:\n[{\'day\': u\'1. Dez 2012\', \'sunrise\': u\'08:18\', \'sunset\': u\'16:10\'},\n {\'day\': u\'2. Dez 2012\', \'sunrise\': u\'08:19\', \'sunset\': u\'16:10\'},\n {\'day\': u\'3. Dez 2012\', \'sunrise\': u\'08:21\', \'sunset\': u\'16:09\'},\n {\'day\': u\'4. Dez 2012\', \'sunrise\': u\'08:22\', \'sunset\': u\'16:09\'},\n {\'day\': u\'5. Dez 2012\', \'sunrise\': u\'08:23\', \'sunset\': u\'16:08\'},\n {\'day\': u\'6. Dez 2012\', \'sunrise\': u\'08:25\', \'sunset\': u\'16:08\'},\n {\'day\': u\'7. Dez 2012\', \'sunrise\': u\'08:26\', \'sunset\': u\'16:07\'}]\n\n', '\nMake your life easier by using CSS Selectors\nI know I have come late to party but I have a nice suggestion for you.\nUsing BeautifulSoup is already been suggested I would rather prefer using CSS Selectors to scrape data inside HTML\nimport urllib2\nfrom bs4 import BeautifulSoup\n\nmain_url = ""http://www.example.com""\n\nmain_page_html  = tryAgain(main_url)\nmain_page_soup = BeautifulSoup(main_page_html)\n\n# Scrape all TDs from TRs inside Table\nfor tr in main_page_soup.select(""table.class_of_table""):\n   for td in tr.select(""td#id""):\n       print(td.text)\n       # For acnhors inside TD\n       print(td.select(""a"")[0].text)\n       # Value of Href attribute\n       print(td.select(""a"")[0][""href""])\n\n# This is method that scrape URL and if it doesnt get scraped, waits for 20 seconds and then tries again. (I use it because my internet connection sometimes get disconnects)\ndef tryAgain(passed_url):\n    try:\n        page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n        return page\n    except Exception:\n        while 1:\n            print(""Trying again the URL:"")\n            print(passed_url)\n            try:\n                page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n                print(""-------------------------------------"")\n                print(""---- URL was successfully scraped ---"")\n                print(""-------------------------------------"")\n                return page\n            except Exception:\n                time.sleep(20)\n                continue \n\n', '\nIf we think of getting name of items from any specific category then we can do that by specifying the class name of that category using css selector:\nimport requests ; from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(requests.get(\'https://www.flipkart.com/\').text, ""lxml"")\nfor link in soup.select(\'div._2kSfQ4\'):\n    print(link.text)\n\nThis is the partial search results:\nPuma, USPA, Adidas & moreUp to 70% OffMen\'s Shoes\nShirts, T-Shirts...Under ₹599For Men\nNike, UCB, Adidas & moreUnder ₹999Men\'s Sandals, Slippers\nPhilips & moreStarting ₹99LED Bulbs & Emergency Lights\n\n', '\nHere is a simple web crawler, i used BeautifulSoup and we will search for all the links(anchors) who\'s class name is _3NFO0d. I used Flipkar.com, it is an online retailing store.\nimport requests\nfrom bs4 import BeautifulSoup\ndef crawl_flipkart():\n    url = \'https://www.flipkart.com/\'\n    source_code = requests.get(url)\n    plain_text = source_code.text\n    soup = BeautifulSoup(plain_text, ""lxml"")\n    for link in soup.findAll(\'a\', {\'class\': \'_3NFO0d\'}):\n        href = link.get(\'href\')\n        print(href)\n\ncrawl_flipkart()\n\n', '\nPython has good options to scrape the web. The best one with a framework is scrapy. It can be a little tricky for beginners, so here is a little help. \n1. Install python above 3.5 (lower ones till 2.7 will work). \n2. Create a environment in conda ( I did this). \n3. Install scrapy at a location and run in from there. \n4. Scrapy shell will give you an interactive interface to test you code. \n5. Scrapy startproject projectname will create a framework.\n6. Scrapy genspider spidername will create a spider. You can create as many spiders as you want. While doing this make sure you are inside the project directory. \n\n\nThe easier one is to use requests and beautiful soup. Before starting give one hour of time to go through the documentation, it will solve most of your doubts. BS4 offer wide range of parsers that you can opt for. Use user-agent and sleep to make scraping easier. BS4 returns a bs.tag so use variable[0]. If there is js running, you wont be able to scrape using requests and bs4 directly. You  could get the api link then parse the JSON to get the information you need or try selenium.  \n']"
Scraping: SSL: CERTIFICATE_VERIFY_FAILED error for http://en.wikipedia.org,"
I'm practicing the code from 'Web Scraping with Python', and I keep having this certificate problem:
from urllib.request import urlopen 
from bs4 import BeautifulSoup 
import re

pages = set()
def getLinks(pageUrl):
    global pages
    html = urlopen(""http://en.wikipedia.org""+pageUrl)
    bsObj = BeautifulSoup(html)
    for link in bsObj.findAll(""a"", href=re.compile(""^(/wiki/)"")):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                #We have encountered a new page
                newPage = link.attrs['href'] 
                print(newPage) 
                pages.add(newPage) 
                getLinks(newPage)
getLinks("""")

The error is:
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 1319, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1049)>

Btw,I was also practicing scrapy, but kept getting the problem: command not found: scrapy (I tried all sorts of solutions online but none works... really frustrating)
",332k,"
            251
        ","['\nOnce upon a time I stumbled  with this issue. If you\'re using macOS go to Macintosh HD > Applications > Python3.6 folder (or whatever version of python you\'re using) > double click on ""Install Certificates.command"" file. :D\n', '\nto use unverified ssl you can add this to your code:\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n', '\nThis terminal command:\nopen /Applications/Python\\ 3.7/Install\\ Certificates.command\nFound here:\nhttps://stackoverflow.com/a/57614113/6207266\nResolved it for me. \nWith my config\npip install --upgrade certifi\nhad no impact.\n', '\nTo solve this: \nAll you need to do is to install Python certificates! A common issue on macOS.  \nOpen these files: \nInstall Certificates.command\nUpdate Shell Profile.command\n\nSimply Run these two scripts and you wont have this issue any more.  \nHope this helps!\n', '\nFor novice users, you can go in the Applications folder and expand the Python 3.7 folder. Now first run (or double click) the Install Certificates.command and then Update Shell Profile.command\n\n', '\nFor anyone who is using anaconda, you would install the certifi package, see more at: \nhttps://anaconda.org/anaconda/certifi\nTo install, type this line in your terminal:\nconda install -c anaconda certifi\n\n', '\nopen /Applications/Python\\ 3.7/Install\\ Certificates.command\n\nTry this command in terminal\n', '\nTwo steps worked for me :\n- going Macintosh HD > Applications > Python3.7 folder \n- click on ""Install Certificates.command""\n', ""\nIf you're running on a Mac you could just search for Install Certificates.command on the spotlight and hit enter.\n"", '\nI could find this solution and is working fine:\ncd /Applications/Python\\ 3.7/\n./Install\\ Certificates.command\n\n', '\nTake a look at this post, it seems like for later versions of Python, certificates are not pre installed which seems to cause this error. You should be able to run the following command to install the certifi package: /Applications/Python\\ 3.6/Install\\ Certificates.command\nPost 1: urllib and ""SSL: CERTIFICATE_VERIFY_FAILED"" Error\nPost 2: Airbrake error: urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate\n', '\nI had the same error and solved the problem by running the program code below:\n# install_certifi.py\n#\n# sample script to install or update a set of default Root Certificates\n# for the ssl module.  Uses the certificates provided by the certifi package:\n#       https://pypi.python.org/pypi/certifi\n\nimport os\nimport os.path\nimport ssl\nimport stat\nimport subprocess\nimport sys\n\nSTAT_0o775 = ( stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR\n             | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP\n             | stat.S_IROTH |                stat.S_IXOTH )\n\n\ndef main():\n    openssl_dir, openssl_cafile = os.path.split(\n        ssl.get_default_verify_paths().openssl_cafile)\n\n    print("" -- pip install --upgrade certifi"")\n    subprocess.check_call([sys.executable,\n        ""-E"", ""-s"", ""-m"", ""pip"", ""install"", ""--upgrade"", ""certifi""])\n\n    import certifi\n\n    # change working directory to the default SSL directory\n    os.chdir(openssl_dir)\n    relpath_to_certifi_cafile = os.path.relpath(certifi.where())\n    print("" -- removing any existing file or link"")\n    try:\n        os.remove(openssl_cafile)\n    except FileNotFoundError:\n        pass\n    print("" -- creating symlink to certifi certificate bundle"")\n    os.symlink(relpath_to_certifi_cafile, openssl_cafile)\n    print("" -- setting permissions"")\n    os.chmod(openssl_cafile, STAT_0o775)\n    print("" -- update complete"")\n\nif __name__ == \'__main__\':\n    main()\n\n', '\ni didn\'t solve the problem, sadly.\nbut managed to make to codes work (almost all of my codes have this probelm btw)\nthe local issuer certificate problem happens under python3.7\nso i changed back to python2.7 QAQ\nand all that needed to change including ""from urllib2 import urlopen"" instead of ""from urllib.request import urlopen""\nso sad...\n', ""\nI'm a relative novice compared to all the experts on Stack Overflow.\nI have 2 versions of jupyter notebook running (one through a fresh Anaconda Navigator installation and one through ????). I think this is because Anaconda was installed as a local installation on my Mac (per Anaconda instructions). \nI already had python 3.7 installed. After that, I used my terminal to open jupyter notebook and I think that it put another version globally onto my Mac. \nHowever, I'm not sure because I'm just learning through trial and error!\nI did the terminal command: \nconda install -c anaconda certifi \n\n(as directed above, but it didn't work.) \nMy python 3.7 is installed on OS Catalina10.15.3 in:\n\n/Library/Python/3.7/site-packages AND\n~/Library/Python/3.7/lib/python/site-packages\n\nThe certificate is at:\n\n~/Library/Python/3.7/lib/python/site-packages/certifi-2019.11.28.dist-info\n\nI tried to find the Install Certificate.command ... but couldn't find it through looking through the file structures...not in Applications...not in links above.\nI finally installed it by finding it through Spotlight (as someone suggested above). And it double clicked automatically and installed ANOTHER certificate in the same folder as:\n\n~/Library/Python/3.7/lib/python/site-packages/\n\nNONE of the above solved anything for me...I still got the same error.  \nSo, I solved the problem by:\n\nclosing my jupyter notebook.\nopening Anaconda Navigator.\nopening jupyter notebook through the Navigator GUI (instead of\nthrough Terminal). \nopening my notebook and running the code.\n\nI can't tell you why this worked.  But it solved the problem for me.\nI just want to save someone the hassle next time. If someone can tell my why it worked, that would be terrific.\nI didn't try the other terminal commands because of the 2 versions of jupyter notebook that I knew were a problem. I just don't know how to fix that.\n"", '\nUse requests library.\nTry this solution, or just add https:// before the URL:\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\n\npages = set()\ndef getLinks(pageUrl):\n    global pages\n    html = requests.get(""http://en.wikipedia.org""+pageUrl, verify=False).text\n    bsObj = BeautifulSoup(html)\n    for link in bsObj.findAll(""a"", href=re.compile(""^(/wiki/)"")):\n        if \'href\' in link.attrs:\n            if link.attrs[\'href\'] not in pages:\n                #We have encountered a new page\n                newPage = link.attrs[\'href\']\n                print(newPage)\n                pages.add(newPage)\n                getLinks(newPage)\ngetLinks("""")\n\nCheck if this works for you\n', '\nFor me the problem was that I was setting REQUESTS_CA_BUNDLE in my .bash_profile\n/Users/westonagreene/.bash_profile:\n...\nexport REQUESTS_CA_BUNDLE=/usr/local/etc/openssl/cert.pem\n...\n\nOnce I set REQUESTS_CA_BUNDLE to blank (i.e. removed from .bash_profile), requests worked again.\nexport REQUESTS_CA_BUNDLE=""""\n\nThe problem only exhibited when executing python requests via a CLI (Command Line Interface). If I ran requests.get(URL, CERT) it resolved just fine.\nMac OS Catalina (10.15.6).\nPyenv of 3.6.11.\nError message I was getting: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)\nMy answer elsewhere: https://stackoverflow.com/a/64151964/4420657\n', ""\nI am using Debian 10 buster and try download a file with youtube-dl and get this error:\nsudo youtube-dl -k https://youtu.be/uscis0CnDjk\n\n[youtube] uscis0CnDjk: Downloading webpage\nERROR: Unable to download webpage: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)> (caused by URLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)')))\n\nCertificates with python2 and python3.8 are installed correctly, but i persistent receive the same error.\nfinally (which is not the best solution, but works for me was to eliminate the certificate check as it is given as an option in youtube-dl) whith this command\nsudo youtube-dl -k --no-check-certificate https://youtu.be/uscis0CnDjk \n"", '\nI am seeing this issue on a Ubuntu 20.04 system and none of the ""real fixes"" (like this one) helped.\nWhile Firefox was willing to open the site just fine neither GNOME Web (i.e. Epiphany) nor Python3 or wget were accepting the certificate. After some searching, I came across this answer on ServerFault which lists two common reasons:\n\n\nThe certificate is really signed by an unknown CA (for instance an internal CA).\nThe certificate is signed with an intermediate CA certificate from one of the well known CA\'s and the remote server is misconfigured in the regard that it doesn\'t include that intermediate CA certificate as a CA chain it\'s response.\n\n\nYou can use the Qualys SSL Labs website to check the site\'s certificates and if there are issues, contact the site\'s administrator to have it fixed.\nIf you really need to work around the issue right now, I\'d recommend a temporary solution like Rambod\'s confined to the site(s) you\'re trying to access.\n', '\nMake sure your websockets is >=10.0\nAdditional to:\nInstall Certificates.command\nUpdate Shell Profile.command\npip3 install websockets==10.0\n', '\nThis will work. Set the environment variable PYTHONHTTPSVERIFY to 0.\n\nBy typing linux command:\n\nexport PYTHONHTTPSVERIFY = 0\n\nOR\n\nUsing in python code:\n\nimport os\nos.environ[""PYTHONHTTPSVERIFY""] = ""0""\n\n', '\nBTW guys if you are getting the same error using aiohttp just put verify_ssl=False argument into your TCPConnector:\nimport aiohttp\n...\n\nasync with aiohttp.ClientSession(\n    connector=aiohttp.TCPConnector(verify_ssl=False)\n) as session:\n    async with session.get(url) as response:\n        body = await response.text()\n\n', ""\nI am using anaconda on windows. Was getting the same error until I tried the following;\nimport urllib.request\nlink = 'http://docs.python.org'\nwith urllib.request.urlopen(link) as response:\n    htmlSource = response.read()\n\nwhich I got from the stackoverflow thread on using urlopen:\nPython urllib urlopen not working\n""]"
Scraping data from website using vba,"
Im trying to scrape data from website: http://uk.investing.com/rates-bonds/financial-futures via vba, like real-time price, i.e. German 5 YR Bobl, US 30Y T-Bond, i have tried excel web query but it only scrapes the whole website, but I would like to scrape the rate only, is there a way of doing this?
",156k,"
            17
        ","['\nThere are several ways of doing this. This is an answer that I write hoping that all the basics of Internet Explorer automation will be found when browsing for the keywords ""scraping data from website"", but remember that nothing\'s worth as your own research (if you don\'t want to stick to pre-written codes that you\'re not able to customize).\nPlease note that this is one way, that I don\'t prefer in terms of performance (since it depends on the browser speed) but that is good to understand the rationale behind Internet automation.\n1) If I need to browse the web, I need a browser! So I create an Internet Explorer browser:\nDim appIE As Object\nSet appIE = CreateObject(""internetexplorer.application"")\n\n2) I ask the browser to browse the target webpage. Through the use of the property "".Visible"", I decide if I want to see the browser doing its job or not. When building the code is nice to have Visible = True, but when the code is working for scraping data is nice not to see it everytime so Visible = False. \nWith appIE\n    .Navigate ""http://uk.investing.com/rates-bonds/financial-futures""\n    .Visible = True\nEnd With\n\n3) The webpage will need some time to load. So, I will wait meanwhile it\'s busy...\nDo While appIE.Busy\n    DoEvents\nLoop\n\n4) Well, now the page is loaded. Let\'s say that I want to scrape the change of the US30Y T-Bond:\nWhat I will do is just clicking F12 on Internet Explorer to see the webpage\'s code, and hence using the pointer (in red circle) I will click on the element that I want to scrape to see how can I reach my purpose. \n\n5) What I should do is straight-forward. First of all, I will get by the ID property the tr element which is containing the value:\nSet allRowOfData = appIE.document.getElementById(""pair_8907"")\n\nHere I will get a collection of td elements (specifically, tr is a row of data, and the td are its cells. We are looking for the 8th, so I will write:\nDim myValue As String: myValue = allRowOfData.Cells(7).innerHTML\n\nWhy did I write 7 instead of 8? Because the collections of cells starts from 0, so the index of the 8th element is 7 (8-1). Shortly analysing this line of code:\n\n.Cells() makes me access the td elements;\ninnerHTML is the property of the cell containing the value we look for. \n\nOnce we have our value, which is now stored into the myValue variable, we can just close the IE browser and releasing the memory by setting it to Nothing:\nappIE.Quit\nSet appIE = Nothing\n\nWell, now you have your value and you can do whatever you want with it: put it into a cell (Range(""A1"").Value = myValue), or into a label of a form (Me.label1.Text = myValue).\nI\'d just like to point you out that this is not how StackOverflow works: here you post questions about specific coding problems, but you should make your own search first. The reason why I\'m answering a question which is not showing too much research effort is just that I see it asked several times and, back to the time when I learned how to do this, I remember that I would have liked having some better support to get started with. So I hope that this answer, which is just a ""study input"" and not at all the best/most complete solution, can be a support for next user having your same problem. Because I have learned how to program thanks to this community, and I like to think that you and other beginners might use my input to discover the beautiful world of programming. \nEnjoy your practice ;) \n', '\nOther methods were mentioned so let us please acknowledge that, at the time of writing, we are in the 21st century. Let\'s park the local bus browser opening, and fly with an XMLHTTP GET request (XHR GET for short).\nWiki moment:\n\nXHR is an API in the form of an object whose methods transfer data\nbetween a web browser and a web server. The object is provided by the\nbrowser\'s JavaScript environment\n\nIt\'s a fast method for retrieving data that doesn\'t require opening a browser. The server response can be read into an HTMLDocument and the process of grabbing the table continued from there.\nNote that javascript rendered/dynamically added content will not be retrieved as there is no javascript engine running (which there is in a browser).\nIn the below code, the table is grabbed by its id cr1.\n\nIn the helper sub, WriteTable,  we loop the columns (td tags) and then the table rows (tr tags), and finally traverse the length of each table row, table cell by table cell. As we only want data from columns 1 and 8, a Select Case statement is used specify what is written out to the sheet.\n\nSample webpage view:\n\n\nSample code output:\n\n\nVBA:\nOption Explicit\nPublic Sub GetRates()\n    Dim html As HTMLDocument, hTable As HTMLTable \'<== Tools > References > Microsoft HTML Object Library\n    \n    Set html = New HTMLDocument\n      \n    With CreateObject(""MSXML2.XMLHTTP"")\n        .Open ""GET"", ""https://uk.investing.com/rates-bonds/financial-futures"", False\n        .setRequestHeader ""If-Modified-Since"", ""Sat, 1 Jan 2000 00:00:00 GMT"" \'to deal with potential caching\n        .send\n        html.body.innerHTML = .responseText\n    End With\n    \n    Application.ScreenUpdating = False\n    \n    Set hTable = html.getElementById(""cr1"")\n    WriteTable hTable, 1, ThisWorkbook.Worksheets(""Sheet1"")\n    \n    Application.ScreenUpdating = True\nEnd Sub\n\nPublic Sub WriteTable(ByVal hTable As HTMLTable, Optional ByVal startRow As Long = 1, Optional ByVal ws As Worksheet)\n    Dim tSection As Object, tRow As Object, tCell As Object, tr As Object, td As Object, r As Long, C As Long, tBody As Object\n    r = startRow: If ws Is Nothing Then Set ws = ActiveSheet\n    With ws\n        Dim headers As Object, header As Object, columnCounter As Long\n        Set headers = hTable.getElementsByTagName(""th"")\n        For Each header In headers\n            columnCounter = columnCounter + 1\n            Select Case columnCounter\n            Case 2\n                .Cells(startRow, 1) = header.innerText\n            Case 8\n                .Cells(startRow, 2) = header.innerText\n            End Select\n        Next header\n        startRow = startRow + 1\n        Set tBody = hTable.getElementsByTagName(""tbody"")\n        For Each tSection In tBody\n            Set tRow = tSection.getElementsByTagName(""tr"")\n            For Each tr In tRow\n                r = r + 1\n                Set tCell = tr.getElementsByTagName(""td"")\n                C = 1\n                For Each td In tCell\n                    Select Case C\n                    Case 2\n                        .Cells(r, 1).Value = td.innerText\n                    Case 8\n                        .Cells(r, 2).Value = td.innerText\n                    End Select\n                    C = C + 1\n                Next td\n            Next tr\n        Next tSection\n    End With\nEnd Sub\n\n', ""\nyou can use winhttprequest object instead of internet explorer as it's good to load data excluding pictures n advertisement instead of downloading full webpage including advertisement n pictures those make internet explorer object heavy compare to winhttpRequest object. \n"", '\nThis question asked long before. But I thought following information will useful for newbies. Actually you can easily get the values from class name like this.\nSub ExtractLastValue()\n\nSet objIE = CreateObject(""InternetExplorer.Application"")\n\nobjIE.Top = 0\nobjIE.Left = 0\nobjIE.Width = 800\nobjIE.Height = 600\n\nobjIE.Visible = True\n\nobjIE.Navigate (""https://uk.investing.com/rates-bonds/financial-futures/"")\n\nDo\nDoEvents\nLoop Until objIE.readystate = 4\n\nMsgBox objIE.document.getElementsByClassName(""pid-8907-last"")(0).innerText\n\nEnd Sub\n\nAnd if you are new to web scraping please read this blog post.\nWeb Scraping - Basics\nAnd also there are various techniques to extract data from web pages. This article explain few of them with examples.\nWeb Scraping - Collecting Data From a Webpage\n', '\nI modified some thing that were poping up error for me and end up with this which worked great to extract the data as I needed:\nSub get_data_web()\n\nDim appIE As Object\nSet appIE = CreateObject(""internetexplorer.application"")\n\nWith appIE\n    .navigate ""https://finance.yahoo.com/quote/NQ%3DF/futures?p=NQ%3DF""\n    .Visible = True\nEnd With\n\nDo While appIE.Busy\n    DoEvents\nLoop\n\nSet allRowofData = appIE.document.getElementsByClassName(""Ta(end) BdT Bdc($c-fuji-grey-c) H(36px)"")\n\nDim i As Long\nDim myValue As String\n\nCount = 1\n\n    For Each itm In allRowofData\n\n        For i = 0 To 4\n\n        myValue = itm.Cells(i).innerText\n        ActiveSheet.Cells(Count, i + 1).Value = myValue\n\n        Next\n\n        Count = Count + 1\n\n    Next\n\nappIE.Quit\nSet appIE = Nothing\n\n\nEnd Sub\n\n']"
"How to ""scan"" a website (or page) for info, and bring it into my program?","
Well, I'm pretty much trying to figure out how to pull information from a webpage, and bring it into my program (in Java). 
For example, if I know the exact page I want info from, for the sake of simplicity a Best Buy item page, how would I get the appropriate info I need off of that page? Like the title, price, description? 
What would this process even be called? I have no idea were to even begin researching this.
Edit:
Okay, I'm running a test for the JSoup(the one posted by BalusC), but I keep getting this error:
Exception in thread ""main"" java.lang.NoSuchMethodError: java.util.LinkedList.peekFirst()Ljava/lang/Object;
at org.jsoup.parser.TokenQueue.consumeWord(TokenQueue.java:209)
at org.jsoup.parser.Parser.parseStartTag(Parser.java:117)
at org.jsoup.parser.Parser.parse(Parser.java:76)
at org.jsoup.parser.Parser.parse(Parser.java:51)
at org.jsoup.Jsoup.parse(Jsoup.java:28)
at org.jsoup.Jsoup.parse(Jsoup.java:56)
at test.main(test.java:12)

I do have Apache Commons
",111k,"
            58
        ","['\nUse a HTML parser like Jsoup. This has my preference above the other HTML parsers available in Java since it supports jQuery like CSS selectors. Also, its class representing a list of nodes, Elements, implements Iterable so that you can iterate over it in an enhanced for loop (so there\'s no need to hassle with verbose Node and NodeList like classes in the average Java DOM parser).\nHere\'s a basic kickoff example (just put the latest Jsoup JAR file in classpath):\npackage com.stackoverflow.q2835505;\n\nimport org.jsoup.Jsoup;\nimport org.jsoup.nodes.Document;\nimport org.jsoup.nodes.Element;\nimport org.jsoup.select.Elements;\n\npublic class Test {\n\n    public static void main(String[] args) throws Exception {\n        String url = ""https://stackoverflow.com/questions/2835505"";\n        Document document = Jsoup.connect(url).get();\n\n        String question = document.select(""#question .post-text"").text();\n        System.out.println(""Question: "" + question);\n\n        Elements answerers = document.select(""#answers .user-details a"");\n        for (Element answerer : answerers) {\n            System.out.println(""Answerer: "" + answerer.text());\n        }\n    }\n\n}\n\nAs you might have guessed, this prints your own question and the names of all answerers.\n', ""\nThis is referred to as screen scraping, wikipedia has this article on the more specific web scraping. It can be a major challenge because there's some ugly, mess-up, broken-if-not-for-browser-cleverness HTML out there, so good luck. \n"", ""\nI would use JTidy - it is simlar to JSoup, but I don't know JSoup well. JTidy handles broken HTML and returns a w3c Document, so you can use this as a source to XSLT to extract the content you are really interested in. If you don't know XSLT, then you might as well go with JSoup, as the Document model is nicer to work with than w3c.\nEDIT: A quick look on the JSoup website shows that JSoup may indeed be the better choice. It seems to support CSS selectors out the box for extracting stuff from the document. This may be a lot easier to work with than getting into XSLT.\n"", ""\nYou may use an html parser (many useful links here: java html parser).\nThe process is called 'grabbing website content'. Search 'grab website content java' for further invertigation.\n"", '\njsoup supports java 1.5\nhttps://github.com/tburch/jsoup/commit/d8ea84f46e009a7f144ee414a9fa73ea187019a3\nlooks like that stack was a bug, and has been fixed\n', ""\nYou'd probably want to look at the HTML to see if you can find strings that are unique and near your text, then you can use line/char-offsets to get to the data.\nCould be awkward in Java, if there aren't any XML classes similar to the ones found in System.XML.Linq in C#.\n"", '\nYou could also try jARVEST.\nIt is based on a JRuby DSL over a pure-Java engine to spider-scrape-transform web sites.\nExample:\nFind all links inside a web page (wget and xpath are constructs of the jARVEST\'s language):\nwget | xpath(\'//a/@href\')\n\nInside a Java program:\nJarvest jarvest = new Jarvest();\n  String[] results = jarvest.exec(\n    ""wget | xpath(\'//a/@href\')"", //robot! \n    ""http://www.google.com"" //inputs\n  );\n  for (String s : results){\n    System.out.println(s);\n  }\n\n', '\nMy answer won\'t probably be useful to the writer of this question (I am 8 months late so not the right timing I guess) but I think it will probably be useful for many other developers that might come across this answer.\nToday, I just released (in the name of my company) an HTML to POJO complete framework that you can use to map HTML to any POJO class with simply some annotations. The library itself is quite handy and features many other things all the while being very pluggable. You can have a look to it right here : https://github.com/whimtrip/jwht-htmltopojo\nHow to use : Basics\nImagine we need to parse the following html page :\n<html>\n    <head>\n        <title>A Simple HTML Document</title>\n    </head>\n    <body>\n        <div class=""restaurant"">\n            <h1>A la bonne Franquette</h1>\n            <p>French cuisine restaurant for gourmet of fellow french people</p>\n            <div class=""location"">\n                <p>in <span>London</span></p>\n            </div>\n            <p>Restaurant n*18,190. Ranked 113 out of 1,550 restaurants</p>  \n            <div class=""meals"">\n                <div class=""meal"">\n                    <p>Veal Cutlet</p>\n                    <p rating-color=""green"">4.5/5 stars</p>\n                    <p>Chef Mr. Frenchie</p>\n                </div>\n\n                <div class=""meal"">\n                    <p>Ratatouille</p>\n                    <p rating-color=""orange"">3.6/5 stars</p>\n                    <p>Chef Mr. Frenchie and Mme. French-Cuisine</p>\n                </div>\n\n            </div> \n        </div>    \n    </body>\n</html>\n\nLet\'s create the POJOs we want to map it to :\npublic class Restaurant {\n\n    @Selector( value = ""div.restaurant > h1"")\n    private String name;\n\n    @Selector( value = ""div.restaurant > p:nth-child(2)"")\n    private String description;\n\n    @Selector( value = ""div.restaurant > div:nth-child(3) > p > span"")    \n    private String location;    \n\n    @Selector( \n        value = ""div.restaurant > p:nth-child(4)""\n        format = ""^Restaurant n\\*([0-9,]+). Ranked ([0-9,]+) out of ([0-9,]+) restaurants$"",\n        indexForRegexPattern = 1,\n        useDeserializer = true,\n        deserializer = ReplacerDeserializer.class,\n        preConvert = true,\n        postConvert = false\n    )\n    // so that the number becomes a valid number as they are shown in this format : 18,190\n    @ReplaceWith(value = "","", with = """")\n    private Long id;\n\n    @Selector( \n        value = ""div.restaurant > p:nth-child(4)""\n        format = ""^Restaurant n\\*([0-9,]+). Ranked ([0-9,]+) out of ([0-9,]+) restaurants$"",\n        // This time, we want the second regex group and not the first one anymore\n        indexForRegexPattern = 2,\n        useDeserializer = true,\n        deserializer = ReplacerDeserializer.class,\n        preConvert = true,\n        postConvert = false\n    )\n    // so that the number becomes a valid number as they are shown in this format : 18,190\n    @ReplaceWith(value = "","", with = """")\n    private Integer rank;\n\n    @Selector(value = "".meal"")    \n    private List<Meal> meals;\n\n    // getters and setters\n\n}\n\nAnd now the Meal class as well :\npublic class Meal {\n\n    @Selector(value = ""p:nth-child(1)"")\n    private String name;\n\n    @Selector(\n        value = ""p:nth-child(2)"",\n        format = ""^([0-9.]+)\\/5 stars$"",\n        indexForRegexPattern = 1\n    )\n    private Float stars;\n\n    @Selector(\n        value = ""p:nth-child(2)"",\n        // rating-color custom attribute can be used as well\n        attr = ""rating-color""\n    )\n    private String ratingColor;\n\n    @Selector(\n        value = ""p:nth-child(3)""\n    )\n    private String chefs;\n\n    // getters and setters.\n}\n\nWe provided some more explanations on the above code on our github page.\nFor the moment, let\'s see how to scrap this.\nprivate static final String MY_HTML_FILE = ""my-html-file.html"";\n\npublic static void main(String[] args) {\n\n\n    HtmlToPojoEngine htmlToPojoEngine = HtmlToPojoEngine.create();\n\n    HtmlAdapter<Restaurant> adapter = htmlToPojoEngine.adapter(Restaurant.class);\n\n    // If they were several restaurants in the same page, \n    // you would need to create a parent POJO containing\n    // a list of Restaurants as shown with the meals here\n    Restaurant restaurant = adapter.fromHtml(getHtmlBody());\n\n    // That\'s it, do some magic now!\n\n}\n\n\nprivate static String getHtmlBody() throws IOException {\n    byte[] encoded = Files.readAllBytes(Paths.get(MY_HTML_FILE));\n    return new String(encoded, Charset.forName(""UTF-8""));\n\n}\n\nAnother short example can be found here\nHope this will help someone out there!\n', '\nJSoup solution is great, but if you need to extract just something really simple it may be easier to use regex or String.indexOf\nAs others have already mentioned the process is called scraping\n', ""\nLook into the cURL library.  I've never used it in Java, but I'm sure there must be bindings for it.  Basically, what you'll do is send a cURL request to whatever page you want to 'scrape'.  The request will return a string with the source code to the page.  From there, you will use regex to parse whatever data you want from the source code.  That's generally how you are going to do it.\n""]"
Java HTML Parsing [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



I'm working on an app which scrapes data from a website and I was wondering how I should go about getting the data.  Specifically I need data contained in a number of div tags which use a specific CSS class - Currently (for testing purposes) I'm just checking for 
div class = ""classname""

in each line of HTML - This works, but I can't help but feel there is a better solution out there.  
Is there any nice way where I could give a class a line of HTML and have some nice methods like:
boolean usesClass(String CSSClassname);
String getText();
String getLink();

",110k,"
            52
        ","['\nAnother library that might be useful for HTML processing is jsoup.\nJsoup tries to clean malformed HTML and allows html parsing in Java using jQuery like tag selector syntax.\nhttp://jsoup.org/ \n', '\nThe main problem as stated by preceding coments is malformed HTML, so an html cleaner or HTML-XML converter is a must. Once you get the XML code (XHTML) there are plenty of tools to handle it. You could get it with a simple SAX handler that extracts only the data you need or any tree-based method (DOM, JDOM, etc.) that let you even modify original code.\nHere is a sample code that uses HTML cleaner to get all DIVs that use a certain class and print out all Text content inside it.\nimport java.io.IOException;\nimport java.net.URL;\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\n\nimport org.htmlcleaner.HtmlCleaner;\nimport org.htmlcleaner.TagNode;\n\n/**\n * @author Fernando Miguélez Palomo <fernandoDOTmiguelezATgmailDOTcom>\n */\npublic class TestHtmlParse\n{\n    static final String className = ""tags"";\n    static final String url = ""http://www.stackoverflow.com"";\n\n    TagNode rootNode;\n\n    public TestHtmlParse(URL htmlPage) throws IOException\n    {\n        HtmlCleaner cleaner = new HtmlCleaner();\n        rootNode = cleaner.clean(htmlPage);\n    }\n\n    List getDivsByClass(String CSSClassname)\n    {\n        List divList = new ArrayList();\n\n        TagNode divElements[] = rootNode.getElementsByName(""div"", true);\n        for (int i = 0; divElements != null && i < divElements.length; i++)\n        {\n            String classType = divElements[i].getAttributeByName(""class"");\n            if (classType != null && classType.equals(CSSClassname))\n            {\n                divList.add(divElements[i]);\n            }\n        }\n\n        return divList;\n    }\n\n    public static void main(String[] args)\n    {\n        try\n        {\n            TestHtmlParse thp = new TestHtmlParse(new URL(url));\n\n            List divs = thp.getDivsByClass(className);\n            System.out.println(""*** Text of DIVs with class \'""+className+""\' at \'""+url+""\' ***"");\n            for (Iterator iterator = divs.iterator(); iterator.hasNext();)\n            {\n                TagNode divElement = (TagNode) iterator.next();\n                System.out.println(""Text child nodes of DIV: "" + divElement.getText().toString());\n            }\n        }\n        catch(Exception e)\n        {\n            e.printStackTrace();\n        }\n    }\n}\n\n', '\nSeveral years ago I used JTidy for the same purpose:\nhttp://jtidy.sourceforge.net/\n""JTidy is a Java port of HTML Tidy, a HTML syntax checker and pretty printer. Like its non-Java cousin, JTidy can be used as a tool for cleaning up malformed and faulty HTML. In addition, JTidy provides a DOM interface to the document that is being processed, which effectively makes you able to use JTidy as a DOM parser for real-world HTML.\nJTidy was written by Andy Quick, who later stepped down from the maintainer position. Now JTidy is maintained by a group of volunteers.\nMore information on JTidy can be found on the JTidy SourceForge project page .""\n', '\nYou might be interested by TagSoup, a Java HTML parser able to handle malformed HTML. XML parsers would work only on well formed XHTML.\n', '\nThe HTMLParser project (http://htmlparser.sourceforge.net/) might be a possibility.  It seems to be pretty decent at handling malformed HTML.  The following snippet should do what you need:\nParser parser = new Parser(htmlInput);\nCssSelectorNodeFilter cssFilter = \n    new CssSelectorNodeFilter(""DIV.targetClassName"");\nNodeList nodes = parser.parse(cssFilter);\n\n', '\nJericho: http://jericho.htmlparser.net/docs/index.html\nEasy to use, supports not well formed HTML, a lot of examples.\n', '\nHTMLUnit might be of help. It does a lot more stuff too.\nhttp://htmlunit.sourceforge.net/1\n', '\nLet\'s not forget Jerry, its jQuery in java: a fast and concise Java Library that simplifies HTML document parsing, traversing and manipulating; includes usage of css3 selectors.\nExample:\nJerry doc = jerry(html);\ndoc.$(""div#jodd p.neat"").css(""color"", ""red"").addClass(""ohmy"");\n\nExample:\ndoc.form(""#myform"", new JerryFormHandler() {\n    public void onForm(Jerry form, Map<String, String[]> parameters) {\n        // process form and parameters\n    }\n});\n\nOf course, these are just some quick examples to get the feeling how it all looks like.\n', ""\nThe nu.validator project is an excellent, high performance HTML parser that doesn't cut corners correctness-wise.\n\nThe Validator.nu HTML Parser is an implementation of the HTML5 parsing algorithm in Java. The parser is designed to work as a drop-in replacement for the XML parser in applications that already support XHTML 1.x content with an XML parser and use SAX, DOM or XOM to interface with the parser. Low-level functionality is provided for applications that wish to perform their own IO and support document.write() with scripting. The parser core compiles on Google Web Toolkit and can be automatically translated into C++. (The C++ translation capability is currently used for porting the parser for use in Gecko.)\n\n"", '\nYou can also use XWiki HTML Cleaner:\nIt uses HTMLCleaner and extends it to generate valid XHTML 1.1 content.\n', ""\nIf your HTML is well-formed, you can easily employ an XML parser to do the job for you... If you're only reading, SAX would be ideal.\n""]"
Scraping dynamic content using python-Scrapy,"
Disclaimer: I've seen numerous other similar posts on StackOverflow and tried to do it the same way but was they don't seem to work on this website.
I'm using Python-Scrapy for getting data from koovs.com. 
However, I'm not able to get the product size, which is dynamically generated. Specifically, if someone could guide me a little on getting the 'Not available' size tag from the drop-down menu on this link, I'd be grateful. 
I am able to get the size list statically, but doing that I only get the list of sizes but not which of them are available.
",55k,"
            47
        ","['\nYou can also solve it with ScrapyJS (no need for selenium and a real browser):\n\nThis library provides Scrapy+JavaScript integration using Splash. \n\nFollow the installation instructions for Splash and ScrapyJS, start the splash docker container:\n$ docker run -p 8050:8050 scrapinghub/splash\n\nPut the following settings into settings.py:\nSPLASH_URL = \'http://192.168.59.103:8050\' \n\nDOWNLOADER_MIDDLEWARES = {\n    \'scrapyjs.SplashMiddleware\': 725,\n}\n\nDUPEFILTER_CLASS = \'scrapyjs.SplashAwareDupeFilter\'\n\nAnd here is your sample spider that is able to see the size availability information:\n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = ""example""\n    allowed_domains = [""koovs.com""]\n    start_urls = (\n        \'http://www.koovs.com/only-onlall-stripe-ls-shirt-59554.html?from=category-651&skuid=236376\',\n    )\n\n    def start_requests(self):\n        for url in self.start_urls:\n            yield scrapy.Request(url, self.parse, meta={\n                \'splash\': {\n                    \'endpoint\': \'render.html\',\n                    \'args\': {\'wait\': 0.5}\n                }\n            })\n\n    def parse(self, response):\n        for option in response.css(""div.select-size select.sizeOptions option"")[1:]:\n            print option.xpath(""text()"").extract()\n\nHere is what is printed on the console:\n[u\'S / 34 -- Not Available\']\n[u\'L / 40 -- Not Available\']\n[u\'L / 42\']\n\n', '\nFrom what I understand, the size availability is determined dynamically in javascript being executed in the browser. Scrapy is not a browser and cannot execute javascript.\nIf you are okay with switching to selenium browser automation tool, here is a sample code:\nfrom selenium import webdriver\nfrom selenium.webdriver.support.select import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\n\nbrowser = webdriver.Firefox()  # can be webdriver.PhantomJS()\nbrowser.get(\'http://www.koovs.com/only-onlall-stripe-ls-shirt-59554.html?from=category-651&skuid=236376\')\n\n# wait for the select element to become visible\nselect_element = WebDriverWait(browser, 10).until(EC.visibility_of_element_located((By.CSS_SELECTOR, ""div.select-size select.sizeOptions"")))\n\nselect = Select(select_element)\nfor option in select.options[1:]:\n    print option.text\n\nbrowser.quit()\n\nIt prints:\nS / 34 -- Not Available\nL / 40 -- Not Available\nL / 42\n\nNote that in place of Firefox you can use other webdrivers like Chrome or Safari. There is also an option to use a headless PhantomJS browser.\nYou can also combine Scrapy with Selenium if needed, see:\n\nselenium with scrapy for dynamic page\nscrapy-webdriver\nseleniumcrawler\n\n', ""\nI faced that problem and solved easily by following these steps\npip install splash \npip install scrapy-splash \npip install scrapyjs\ndownload and install docker-toolbox\nopen docker-quickterminal and enter \n$ docker run -p 8050:8050 scrapinghub/splash\n\nTo set the SPLASH_URL check the default ip configured in the docker machine by entering  $ docker-machine ip default (My IP was 192.168.99.100)\nSPLASH_URL = 'http://192.168.99.100:8050'\nDOWNLOADER_MIDDLEWARES = {\n    'scrapyjs.SplashMiddleware': 725,\n}\n\nDUPEFILTER_CLASS = 'scrapyjs.SplashAwareDupeFilter'\n\nThat's it!\n"", '\nYou have to interpret the json of the website, examples\nscrapy.readthedocs and \ntestingcan.github.io\nimport scrapy\nimport json\nclass QuoteSpider(scrapy.Spider):\n   name = \'quote\'\n   allowed_domains = [\'quotes.toscrape.com\']\n   page = 1\n   start_urls = [\'http://quotes.toscrape.com/api/quotes?page=1\']\n\n   def parse(self, response):\n      data = json.loads(response.text)\n      for quote in data[""quotes""]:\n        yield {""quote"": quote[""text""]}\n      if data[""has_next""]:\n          self.page += 1\n          url = ""http://quotes.toscrape.com/api/quotes?page={}"".format(self.page)\n          yield scrapy.Request(url=url, callback=self.parse)\n\n']"
Headless Browser and scraping - solutions [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



I'm trying to put list of possible solutions for browser automatic tests suits and headless browser platforms capable of scraping.

BROWSER TESTING / SCRAPING:

Selenium - polyglot flagship in browser automation, bindings for Python, Ruby,  JavaScript, C#, Haskell and more, IDE for Firefox (as an extension) for faster test deployment. Can act as a Server and has tons of features.

JAVASCRIPT

PhantomJS - JavaScript, headless testing with screen capture and automation, uses Webkit. As of version 1.8 Selenium's WebDriver API is implemented, so you can use any WebDriver binding and tests will be compatible with Selenium
SlimerJS - similar to PhantomJS, uses Gecko (Firefox) instead of WebKit
CasperJS - JavaScript, build on both PhantomJS and SlimerJS, has extra features
Ghost Driver - JavaScript implementation of the WebDriver Wire Protocol for PhantomJS.
new PhantomCSS - CSS regression testing. A CasperJS module for automating visual regression testing with PhantomJS and Resemble.js.
new WebdriverCSS - plugin for Webdriver.io for automating visual regression testing
new PhantomFlow - Describe and visualize user flows through tests. An experimental approach to Web user interface testing.
new trifleJS - ports the PhantomJS API to use the Internet Explorer engine.
new CasperJS IDE (commercial)

NODE.JS

Node-phantom - bridges the gap between PhantomJS and node.js
WebDriverJs - Selenium WebDriver bindings for node.js by Selenium Team
WD.js - node module for WebDriver/Selenium 2
yiewd - WD.js wrapper using latest Harmony generators! Get rid of the callback pyramid with yield
ZombieJs - Insanely fast, headless full-stack testing using node.js
NightwatchJs - Node JS based testing solution using Selenium Webdriver
Chimera - Chimera: can do everything what phantomJS does, but in a full JS environment
Dalek.js - Automated cross browser testing with JavaScript through Selenium Webdriver
Webdriver.io - better implementation of WebDriver bindings with predefined 50+ actions
Nightmare - Electron bridge with a high-level API.
jsdom - Tailored towards web scraping. A very lightweight DOM implemented in Node.js, it supports pages with javascript.
new Puppeteer - Node library which provides a high-level API to control Chrome or Chromium. Puppeteer runs headless by default.

WEB SCRAPING / MINING

Scrapy - Python, mainly a scraper/miner - fast, well documented and, can be linked with Django Dynamic Scraper for nice mining deployments, or Scrapy Cloud for PaaS (server-less) deployment, works in terminal or an server stand-alone proces, can be used with Celery, built on top of Twisted
Snailer - node.js module, untested yet.
Node-Crawler - node.js module, untested yet.

ONLINE TOOLS

new Web Scraping Language - Simple syntax to crawl the web

new Online HTTP client - Dedicated SO answer

dead CasperBox - Run CasperJS scripts online


Android TOOLS for Automation

new Mechanica Browser App


RELATED LINKS & RESOURCES

Comparsion of Webscraping software
new Resemble.js : Image analysis and comparison

Questions:

Any pure Node.js solution or Nodejs to PhanthomJS/CasperJS module that actually works and is documented?

Answer: Chimera seems to go in that direction, checkout Chimera

Other solutions capable of easier JavaScript injection than Selenium?

Do you know any pure ruby solutions?


Answer: Checkout the list created by rjk with ruby based solutions

Do you know any related tech or solution?

Feel free to edit this question and add content as you wish! Thank you for your contributions!
",83k,"
            377
        ","['\nIf Ruby is your thing, you may also try:\n\nhttps://github.com/chriskite/anemone (dev stopped)\nhttps://github.com/sparklemotion/mechanize\nhttps://github.com/postmodern/spidr\nhttps://github.com/stewartmckee/cobweb\nhttp://watirwebdriver.com/ (Selenium)\n\nalso, Nokogiri gem can be used for scraping:\n\nhttp://nokogiri.org/\n\nthere is a dedicated book about how to utilise nokogiri for scraping by packt publishing\n', '\nhttp://triflejs.org/ is like phantomjs but based on IE\n', '\nA kind of JS-based Selenium is Dalek.js. It not only aims for automated frontend-tests, you can also do screenshots with it. It has webdrivers for all important browsers. Unfortunately those webdrivers seem to be worth improving (just not to say ""buggy"" to Firefox).\n']"
Problem HTTP error 403 in Python 3 Web Scraping,"
I was trying to scrape a website for practice, but I kept on getting the HTTP Error 403 (does it think I'm a bot)?
Here is my code:
#import requests
import urllib.request
from bs4 import BeautifulSoup
#from urllib import urlopen
import re

webpage = urllib.request.urlopen('http://www.cmegroup.com/trading/products/#sortField=oi&sortAsc=false&venues=3&page=1&cleared=1&group=1').read
findrows = re.compile('<tr class=""- banding(?:On|Off)>(.*?)</tr>')
findlink = re.compile('<a href ="">(.*)</a>')

row_array = re.findall(findrows, webpage)
links = re.finall(findlink, webpate)

print(len(row_array))

iterator = []

The error I get is:
 File ""C:\Python33\lib\urllib\request.py"", line 160, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Python33\lib\urllib\request.py"", line 479, in open
    response = meth(req, response)
  File ""C:\Python33\lib\urllib\request.py"", line 591, in http_response
    'http', request, response, code, msg, hdrs)
  File ""C:\Python33\lib\urllib\request.py"", line 517, in error
    return self._call_chain(*args)
  File ""C:\Python33\lib\urllib\request.py"", line 451, in _call_chain
    result = func(*args)
  File ""C:\Python33\lib\urllib\request.py"", line 599, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden

",276k,"
            168
        ","[""\nThis is probably because of mod_security or some similar server security feature which blocks known spider/bot user agents (urllib uses something like python urllib/3.3.0, it's easily detected). Try setting a known browser user agent with:\nfrom urllib.request import Request, urlopen\n\nreq = Request(\n    url='http://www.cmegroup.com/trading/products/#sortField=oi&sortAsc=false&venues=3&page=1&cleared=1&group=1', \n    headers={'User-Agent': 'Mozilla/5.0'}\n)\nwebpage = urlopen(req).read()\n\nThis works for me.\nBy the way, in your code you are missing the () after .read in the urlopen line, but I think that it's a typo.\nTIP: since this is exercise, choose a different, non restrictive site. Maybe they are blocking urllib for some reason...\n"", '\nDefinitely it\'s blocking because of your use of urllib based on the user agent. This same thing is happening to me with OfferUp. You can create a new class called AppURLopener which overrides the user-agent with Mozilla. \nimport urllib.request\n\nclass AppURLopener(urllib.request.FancyURLopener):\n    version = ""Mozilla/5.0""\n\nopener = AppURLopener()\nresponse = opener.open(\'http://httpbin.org/user-agent\')\n\nSource\n', '\n""This is probably because of mod_security or some similar server security feature which blocks known\n\nspider/bot\n\nuser agents (urllib uses something like python urllib/3.3.0, it\'s easily detected)"" - as already mentioned by Stefano Sanfilippo\nfrom urllib.request import Request, urlopen\nurl=""https://stackoverflow.com/search?q=html+error+403""\nreq = Request(url, headers={\'User-Agent\': \'Mozilla/5.0\'})\n\nweb_byte = urlopen(req).read()\n\nwebpage = web_byte.decode(\'utf-8\')\n\nThe web_byte is a byte object returned by the server and the content type present in webpage is mostly utf-8.\nTherefore you need to decode web_byte using decode method.\nThis solves complete problem while I was having trying to scrape from a website using PyCharm\nP.S -> I use python 3.4\n', ""\nBased on previous answers this has worked for me with Python 3.7 by increasing the timeout to 10.\nfrom urllib.request import Request, urlopen\n\nreq = Request('Url_Link', headers={'User-Agent': 'XYZ/3.0'})\nwebpage = urlopen(req, timeout=10).read()\n\nprint(webpage)\n\n"", '\nAdding cookie to the request headers worked for me\nfrom urllib.request import Request, urlopen\n\n# Function to get the page content\ndef get_page_content(url, head):\n  """"""\n  Function to get the page content\n  """"""\n  req = Request(url, headers=head)\n  return urlopen(req)\n\nurl = \'https://example.com\'\nhead = {\n  \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36\',\n  \'Accept\': \'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\',\n  \'Accept-Charset\': \'ISO-8859-1,utf-8;q=0.7,*;q=0.3\',\n  \'Accept-Encoding\': \'none\',\n  \'Accept-Language\': \'en-US,en;q=0.8\',\n  \'Connection\': \'keep-alive\',\n  \'refere\': \'https://example.com\',\n  \'cookie\': """"""your cookie value ( you can get that from your web page) """"""\n}\n\ndata = get_page_content(url, head).read()\nprint(data)\n\n', ""\nSince the page works in browser and not when calling within python program, it seems that the web app that serves that url recognizes that you request the content not by the browser.\nDemonstration:\ncurl --dump-header r.txt http://www.cmegroup.com/trading/products/#sortField=oi&sortAsc=false&venues=3&page=1&cleared=1&group=1\n\n...\n<HTML><HEAD>\n<TITLE>Access Denied</TITLE>\n</HEAD><BODY>\n<H1>Access Denied</H1>\nYou don't have permission to access ...\n</HTML>\n\nand the content in r.txt has status line:\nHTTP/1.1 403 Forbidden\n\nTry posting header 'User-Agent' which fakes web client.\nNOTE: The page contains Ajax call that creates the table you probably want to parse. You'll need to check the javascript logic of the page or simply using browser debugger (like Firebug / Net tab) to see which url you need to call to get the table's content.\n"", ""\nIf you feel guilty about faking the user-agent as Mozilla (comment in the top answer from Stefano), it could work with a non-urllib User-Agent as well. This worked for the sites I reference:\n    req = urlrequest.Request(link, headers={'User-Agent': 'XYZ/3.0'})\n    urlrequest.urlopen(req, timeout=10).read()\n\nMy application is to test validity by scraping specific links that I refer to, in my articles. Not a generic scraper.\n"", ""\nYou can try in two ways. The detail is in this link. \n1) Via pip\n\npip install --upgrade certifi\n\n2) If it doesn't work, try to run a Cerificates.command that comes bundled with Python 3.* for Mac:(Go to your python installation location and double click the file)\n\nopen /Applications/Python\\ 3.*/Install\\ Certificates.command\n\n"", '\nI ran into this same problem and was not able to solve it using the answers above. I ended up getting around the issue by using requests.get() and then using the .text of the result instead of using read():\nfrom requests import get\n\nreq = get(link)\nresult = req.text\n\n', '\nyou can use urllib\'s build_opener like this:\nopener = urllib.request.build_opener()\nopener.addheaders = [(\'User-Agent\', \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\'), (\'Accept\',\'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\'), (\'Accept-Encoding\',\'gzip, deflate, br\'),\\\n    (\'Accept-Language\',\'en-US,en;q=0.5\' ), (""Connection"", ""keep-alive""), (""Upgrade-Insecure-Requests"",\'1\')]\nurllib.request.install_opener(opener)\nurllib.request.urlretrieve(url, ""test.xlsx"")\n\n', '\nI pulled my hair out with this for a while and the answer ended up being pretty simple. I checked the response text and I was getting ""URL signature expired"" which is a message you wouldn\'t normally see unless you checked the response text.\nThis means some URLs just expire, usually for security purposes. Try to get the URL again and update the URL in your script. If there isn\'t a new URL for the content you\'re trying to scrape, then unfortunately you can\'t scrape for it.\n']"
How can I download a file on a click event using selenium?,"
I am working on python and selenium. I want to download file from clicking event using selenium. I wrote following code.  
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.common.keys import Keys

browser = webdriver.Firefox()
browser.get(""http://www.drugcite.com/?q=ACTIMMUNE"")

browser.close()

I want to download both files from links with name ""Export Data"" from given url. How can I achieve it as it works with click event only?
",122k,"
            58
        ","['\nFind the link using find_element(s)_by_*, then call click method.\nfrom selenium import webdriver\n\n# To prevent download dialog\nprofile = webdriver.FirefoxProfile()\nprofile.set_preference(\'browser.download.folderList\', 2) # custom location\nprofile.set_preference(\'browser.download.manager.showWhenStarting\', False)\nprofile.set_preference(\'browser.download.dir\', \'/tmp\')\nprofile.set_preference(\'browser.helperApps.neverAsk.saveToDisk\', \'text/csv\')\n\nbrowser = webdriver.Firefox(profile)\nbrowser.get(""http://www.drugcite.com/?q=ACTIMMUNE"")\n\nbrowser.find_element_by_id(\'exportpt\').click()\nbrowser.find_element_by_id(\'exporthlgt\').click()\n\nAdded profile manipulation code to prevent download dialog.\n', '\nI\'ll admit this solution is a little more ""hacky"" than the Firefox Profile saveToDisk alternative, but it works across both Chrome and Firefox, and doesn\'t rely on a browser-specific feature which could change at any time. And if nothing else, maybe this will give someone a little different perspective on how to solve future challenges.\nPrerequisites: Ensure you have selenium and pyvirtualdisplay installed...\n\nPython 2: sudo pip install selenium pyvirtualdisplay\nPython 3: sudo pip3 install selenium pyvirtualdisplay\n\nThe Magic\nimport pyvirtualdisplay\nimport selenium\nimport selenium.webdriver\nimport time\nimport base64\nimport json\n\nroot_url = \'https://www.google.com\'\ndownload_url = \'https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png\'\n\nprint(\'Opening virtual display\')\ndisplay = pyvirtualdisplay.Display(visible=0, size=(1280, 1024,))\ndisplay.start()\nprint(\'\\tDone\')\n\nprint(\'Opening web browser\')\ndriver = selenium.webdriver.Firefox()\n#driver = selenium.webdriver.Chrome() # Alternately, give Chrome a try\nprint(\'\\tDone\')\n\nprint(\'Retrieving initial web page\')\ndriver.get(root_url)\nprint(\'\\tDone\')\n\nprint(\'Injecting retrieval code into web page\')\ndriver.execute_script(""""""\n    window.file_contents = null;\n    var xhr = new XMLHttpRequest();\n    xhr.responseType = \'blob\';\n    xhr.onload = function() {\n        var reader  = new FileReader();\n        reader.onloadend = function() {\n            window.file_contents = reader.result;\n        };\n        reader.readAsDataURL(xhr.response);\n    };\n    xhr.open(\'GET\', %(download_url)s);\n    xhr.send();\n"""""".replace(\'\\r\\n\', \' \').replace(\'\\r\', \' \').replace(\'\\n\', \' \') % {\n    \'download_url\': json.dumps(download_url),\n})\n\nprint(\'Looping until file is retrieved\')\ndownloaded_file = None\nwhile downloaded_file is None:\n    # Returns the file retrieved base64 encoded (perfect for downloading binary)\n    downloaded_file = driver.execute_script(\'return (window.file_contents !== null ? window.file_contents.split(\\\',\\\')[1] : null);\')\n    print(downloaded_file)\n    if not downloaded_file:\n        print(\'\\tNot downloaded, waiting...\')\n        time.sleep(0.5)\nprint(\'\\tDone\')\n\nprint(\'Writing file to disk\')\nfp = open(\'google-logo.png\', \'wb\')\nfp.write(base64.b64decode(downloaded_file))\nfp.close()\nprint(\'\\tDone\')\ndriver.close() # close web browser, or it\'ll persist after python exits.\ndisplay.popen.kill() # close virtual display, or it\'ll persist after python exits.\n\nExplaination\nWe first load a URL on the domain we\'re targeting a file download from. This allows us to perform an AJAX request on that domain, without running into cross site scripting issues.\nNext, we\'re injecting some javascript into the DOM which fires off an AJAX request. Once the AJAX request returns a response, we take the response and load it into a FileReader object. From there we can extract the base64 encoded content of the file by calling readAsDataUrl(). We\'re then taking the base64 encoded content and appending it to window, a gobally accessible variable.\nFinally, because the AJAX request is asynchronous, we enter  a Python while loop waiting for the content to be appended to the window. Once it\'s appended, we decode the base64 content retrieved from the window and save it to a file.\nThis solution should work across all modern browsers supported by Selenium, and works whether text or binary, and across all mime types.\nAlternate Approach\nWhile I haven\'t tested this, Selenium does afford you the ability to wait until an element is present in the DOM. Rather than looping until a globally accessible variable is populated, you could create an element with a particular ID in the DOM and use the binding of that element as the trigger to retrieve the downloaded file.\n', ""\nIn chrome what I do is downloading the files by clicking on the links, then I open chrome://downloads page and then retrieve the downloaded files list from shadow DOM like this:\ndocs = document\n  .querySelector('downloads-manager')\n  .shadowRoot.querySelector('#downloads-list')\n  .getElementsByTagName('downloads-item')\n\nThis solution is restrained to chrome, the data also contains information like file path and download date. (note this code is from JS, may not be the correct python syntax)\n"", '\nHere is the full working code. You can use web scraping to enter the username password and other field. For getting the field names appearing on the webpage, use inspect element. Element name(Username,Password or Click Button) can be entered through class or name.\nfrom selenium import webdriver\n# Using Chrome to access web\noptions = webdriver.ChromeOptions() \noptions.add_argument(""download.default_directory=C:/Test"") # Set the download Path\ndriver = webdriver.Chrome(options=options)\n# Open the website\ntry:\n    driver.get(\'xxxx\') # Your Website Address\n    password_box = driver.find_element_by_name(\'password\')\n    password_box.send_keys(\'xxxx\') #Password\n    download_button = driver.find_element_by_class_name(\'link_w_pass\')\n    download_button.click()\n    driver.quit()\nexcept:\n    driver.quit()\n    print(""Faulty URL"")\n\n']"
Using BeautifulSoup to extract text without tags,"
My webpage looks like this:
<p>
  <strong class=""offender"">YOB:</strong> 1987<br/>
  <strong class=""offender"">RACE:</strong> WHITE<br/>
  <strong class=""offender"">GENDER:</strong> FEMALE<br/>
  <strong class=""offender"">HEIGHT:</strong> 5'05''<br/>
  <strong class=""offender"">WEIGHT:</strong> 118<br/>
  <strong class=""offender"">EYE COLOR:</strong> GREEN<br/>
  <strong class=""offender"">HAIR COLOR:</strong> BROWN<br/>
</p>

I want to extract the info for each individual and get YOB:1987, RACE:WHITE, etc...
What I tried is:
subc = soup.find_all('p')
subc1 = subc[1]
subc2 = subc1.find_all('strong')

But this gives me only the values of YOB:, RACE:, etc...
Is there a way that I can get the data in YOB:1987, RACE:WHITE format?
",178k,"
            64
        ","['\nJust loop through all the <strong> tags and use next_sibling to get what you want. Like this:\nfor strong_tag in soup.find_all(\'strong\'):\n    print(strong_tag.text, strong_tag.next_sibling)\n\nDemo:\nfrom bs4 import BeautifulSoup\n\nhtml = \'\'\'\n<p>\n  <strong class=""offender"">YOB:</strong> 1987<br />\n  <strong class=""offender"">RACE:</strong> WHITE<br />\n  <strong class=""offender"">GENDER:</strong> FEMALE<br />\n  <strong class=""offender"">HEIGHT:</strong> 5\'05\'\'<br />\n  <strong class=""offender"">WEIGHT:</strong> 118<br />\n  <strong class=""offender"">EYE COLOR:</strong> GREEN<br />\n  <strong class=""offender"">HAIR COLOR:</strong> BROWN<br />\n</p>\n\'\'\'\n\nsoup = BeautifulSoup(html)\n\nfor strong_tag in soup.find_all(\'strong\'):\n    print(strong_tag.text, strong_tag.next_sibling)\n\nThis gives you:\nYOB:  1987\nRACE:  WHITE\nGENDER:  FEMALE\nHEIGHT:  5\'05\'\'\nWEIGHT:  118\nEYE COLOR:  GREEN\nHAIR COLOR:  BROWN\n\n', '\nI think you can get it using subc1.text.\n>>> html = """"""\n<p>\n    <strong class=""offender"">YOB:</strong> 1987<br />\n    <strong class=""offender"">RACE:</strong> WHITE<br />\n    <strong class=""offender"">GENDER:</strong> FEMALE<br />\n    <strong class=""offender"">HEIGHT:</strong> 5\'05\'\'<br />\n    <strong class=""offender"">WEIGHT:</strong> 118<br />\n    <strong class=""offender"">EYE COLOR:</strong> GREEN<br />\n    <strong class=""offender"">HAIR COLOR:</strong> BROWN<br />\n</p>\n""""""\n>>> from bs4 import BeautifulSoup\n>>> soup = BeautifulSoup(html)\n>>> print soup.text\n\n\nYOB: 1987\nRACE: WHITE\nGENDER: FEMALE\nHEIGHT: 5\'05\'\'\nWEIGHT: 118\nEYE COLOR: GREEN\nHAIR COLOR: BROWN\n\nOr if you want to explore it, you can use .contents :\n>>> p = soup.find(\'p\')\n>>> from pprint import pprint\n>>> pprint(p.contents)\n[u\'\\n\',\n <strong class=""offender"">YOB:</strong>,\n u\' 1987\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">RACE:</strong>,\n u\' WHITE\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">GENDER:</strong>,\n u\' FEMALE\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">HEIGHT:</strong>,\n u"" 5\'05\'\'"",\n <br/>,\n u\'\\n\',\n <strong class=""offender"">WEIGHT:</strong>,\n u\' 118\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">EYE COLOR:</strong>,\n u\' GREEN\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">HAIR COLOR:</strong>,\n u\' BROWN\',\n <br/>,\n u\'\\n\']\n\nand filter out the necessary items from the list:\n>>> data = dict(zip([x.text for x in p.contents[1::4]], [x.strip() for x in p.contents[2::4]]))\n>>> pprint(data)\n{u\'EYE COLOR:\': u\'GREEN\',\n u\'GENDER:\': u\'FEMALE\',\n u\'HAIR COLOR:\': u\'BROWN\',\n u\'HEIGHT:\': u""5\'05\'\'"",\n u\'RACE:\': u\'WHITE\',\n u\'WEIGHT:\': u\'118\',\n u\'YOB:\': u\'1987\'}\n\n', '\nyou can try this indside findall for loop:\nitem_price = item.find(\'span\', attrs={\'class\':\'s-item__price\'}).text\n\nit extracts only text and assigs it to ""item_pice""\n', '\nI think you could solve this with .strip() in gazpacho:\nInput:\nhtml = """"""\\\n<p>\n  <strong class=""offender"">YOB:</strong> 1987<br />\n  <strong class=""offender"">RACE:</strong> WHITE<br />\n  <strong class=""offender"">GENDER:</strong> FEMALE<br />\n  <strong class=""offender"">HEIGHT:</strong> 5\'05\'\'<br />\n  <strong class=""offender"">WEIGHT:</strong> 118<br />\n  <strong class=""offender"">EYE COLOR:</strong> GREEN<br />\n  <strong class=""offender"">HAIR COLOR:</strong> BROWN<br />\n</p>\n""""""\n\nCode:\nsoup = Soup(html)\ntext = soup.find(""p"").strip(whitespace=False) # to keep \\n characters intact\nlines = [\n    line.strip()\n    for line in text.split(""\\n"")\n    if line != """"\n]\ndata = dict([line.split("": "") for line in lines])\n\nOutput:\nprint(data)\n# {\'YOB\': \'1987\',\n#  \'RACE\': \'WHITE\',\n#  \'GENDER\': \'FEMALE\',\n#  \'HEIGHT\': ""5\'05\'\'"",\n#  \'WEIGHT\': \'118\',\n#  \'EYE COLOR\': \'GREEN\',\n#  \'HAIR COLOR\': \'BROWN\'}\n\n']"
Headless browser for C# (.NET)? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



I am (was) a Python developer who is building a GUI web scraping application. Recently I've decided to migrate to .NET framework and write the same application in C# (this decision wasn't mine).
In Python, I've used the Mechanize library. However, I can't seem to find anything similar in .NET. What I need is a browser that will run in a headless mode, which has the ability to fill out forms, submit them, etc. JavaScript parser is not a must, but it would be quite useful. 
",57k,"
            40
        ","['\nThere are some options:\n\nWebKit.Net (free)\n\nAwesomium\nIt is based on Chrome/WebKit and works like a charm.\nThere is a free license available but also a commercial one and if need be you can buy the source code :-)\n\nHTML Agility Pack (free) (An HTML Parser library, NOT a headless browser)\nThis helps with extracting information from HTML etc. and might be useful in your case (possibly in combination with HttpWebRequest)\n\n\n', ""\nMore solutions:\n\nPhantomJS - full featured headless web\nbrowser. Often used in pair with Selenium which allows you to\naccess the browser from .NET application.\nOptimus (nuget package)- lightweight headless web browser. It's in beta but it is sufficient for some cases.\n\nI used to use both for web testing. But they are also suitable for web scraping.\n"", ""\nYou may be after TrifleJS (currently in beta), or something similar using the .NET WebBrowser class which communicates with IE via a windowless ActiveX/COM API.\nYou'll essentially be running a fully fledged browser (not a http request wrapper) using Internet Explorer's Trident engine, if you are not interested in the JavaScript API (a port of phantomjs) you may still be able to use some of the C# codebase to get around key concepts (custom headers, cookies, script execution, screenshot rendering etc). \nNote that this can also emulate different versions of IE depending on what you have installed.\n\n""]"
How to convert raw javascript object to a dictionary?,"
When screen-scraping some website, I extract data from <script> tags.
The data I get is not in standard JSON format. I cannot use json.loads().
# from
js_obj = '{x:1, y:2, z:3}'

# to
py_obj = {'x':1, 'y':2, 'z':3}

Currently, I use regex to transform the raw data to JSON format.
But I feel pretty bad when I encounter complicated data structure.
Do you have some better solutions?
",29k,"
            32
        ","['\ndemjson.decode()\nimport demjson\n\n# from\njs_obj = \'{x:1, y:2, z:3}\'\n\n# to\npy_obj = demjson.decode(js_obj)\n\njsonnet.evaluate_snippet()\nimport json, _jsonnet\n\n# from\njs_obj = \'{x:1, y:2, z:3}\'\n\n# to\npy_obj = json.loads(_jsonnet.evaluate_snippet(\'snippet\', js_obj))\n\nast.literal_eval()\nimport ast\n\n# from\njs_obj = ""{\'x\':1, \'y\':2, \'z\':3}""\n\n# to\npy_obj = ast.literal_eval(js_obj)\n\n', ""\nUse json5\nimport json5\n\njs_obj = '{x:1, y:2, z:3}'\n\npy_obj = json5.loads(js_obj)\n\nprint(py_obj)\n\n# output\n# {'x': 1, 'y': 2, 'z': 3}\n\n"", ""\nI'm facing the same problem this afternoon, and I finally found a quite good solution. That is JSON5.\nThe syntax of JSON5 is more similar to native JavaScript, so it can help you parse non-standard JSON objects.\nYou might want to check pyjson5 out.\n"", '\nThis will likely not work everywhere, but as a start, here\'s a simple regex that should convert the keys into quoted strings so you can pass into json.loads.  Or is this what you\'re already doing?\nIn[70] : quote_keys_regex = r\'([\\{\\s,])(\\w+)(:)\'\n\nIn[71] : re.sub(quote_keys_regex, r\'\\1""\\2""\\3\', js_obj)\nOut[71]: \'{""x"":1, ""y"":2, ""z"":3}\'\n\nIn[72] : js_obj_2 = \'{x:1, y:2, z:{k:3,j:2}}\'\n\nInt[73]: re.sub(quote_keys_regex, r\'\\1""\\2""\\3\', js_obj_2)\nOut[73]: \'{""x"":1, ""y"":2, ""z"":{""k"":3,""j"":2}}\'\n\n', '\nIf you have node available on the system, you can ask it to evaluate the javascript expression for you, and print the stringified result. The resulting JSON can then be fed to json.loads:\ndef evaluate_javascript(s):\n    """"""Evaluate and stringify a javascript expression in node.js, and convert the\n    resulting JSON to a Python object""""""\n    node = Popen([\'node\', \'-\'], stdin=PIPE, stdout=PIPE)\n    stdout, _ = node.communicate(f\'console.log(JSON.stringify({s}))\'.encode(\'utf8\'))\n    return json.loads(stdout.decode(\'utf8\'))\n\n', '\nNot including objects \njson.loads()\n\njson.loads() doesn\'t accept undefined, you have to change to null\njson.loads() only accept double quotes\n\n\n{""foo"": 1, ""bar"": null}\n\n\nUse this if you are sure that your javascript code only have double quotes on key names.  \nimport json\n\njson_text = """"""{""foo"": 1, ""bar"": undefined}""""""\njson_text = re.sub(r\'(""\\s*:\\s*)undefined(\\s*[,}])\', \'\\\\1null\\\\2\', json_text)\n\npy_obj = json.loads(json_text)\n\nast.literal_eval()\n\nast.literal_eval() doesn\'t accept undefined, you have to change to None\nast.literal_eval() doesn\'t accept null, you have to change to None\nast.literal_eval() doesn\'t accept true, you have to change to True\nast.literal_eval() doesn\'t accept false, you have to change to False\nast.literal_eval() accept single and double quotes\n\n\n{""foo"": 1, ""bar"": None} or {\'foo\': 1, \'bar\': None}\n\n\nimport ast\n\njs_obj = """"""{\'foo\': 1, \'bar\': undefined}""""""\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)undefined(\\s*[,}])\', \'\\\\1None\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)null(\\s*[,}])\', \'\\\\1None\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)NaN(\\s*[,}])\', \'\\\\1None\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)true(\\s*[,}])\', \'\\\\1True\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)false(\\s*[,}])\', \'\\\\1False\\\\2\', js_obj)\n\npy_obj = ast.literal_eval(js_obj) \n\n']"
How to save an image locally using Python whose URL address I already know?,"
I know the URL of an image on Internet.
e.g. http://www.digimouth.com/news/media/2011/09/google-logo.jpg, which contains the logo of Google.
Now, how can I download this image using Python without actually opening the URL in a browser and saving the file manually.
",334k,"
            198
        ","['\nPython 2\nHere is a more straightforward way if all you want to do is save it as a file:\nimport urllib\n\nurllib.urlretrieve(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"", ""local-filename.jpg"")\n\nThe second argument is the local path where the file should be saved.\nPython 3\nAs SergO suggested the  code below should work with Python 3.\nimport urllib.request\n\nurllib.request.urlretrieve(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"", ""local-filename.jpg"")\n\n', '\nimport urllib\nresource = urllib.urlopen(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"")\noutput = open(""file01.jpg"",""wb"")\noutput.write(resource.read())\noutput.close()\n\nfile01.jpg will contain your image. \n', '\nI wrote a script that does just this, and it is available on my github for your use. \nI utilized BeautifulSoup to allow me to parse any website for images. If you will be doing much web scraping (or intend to use my tool) I suggest you sudo pip install BeautifulSoup. Information on BeautifulSoup is available here.\nFor convenience here is my code:\nfrom bs4 import BeautifulSoup\nfrom urllib2 import urlopen\nimport urllib\n\n# use this image scraper from the location that \n#you want to save scraped images to\n\ndef make_soup(url):\n    html = urlopen(url).read()\n    return BeautifulSoup(html)\n\ndef get_images(url):\n    soup = make_soup(url)\n    #this makes a list of bs4 element tags\n    images = [img for img in soup.findAll(\'img\')]\n    print (str(len(images)) + ""images found."")\n    print \'Downloading images to current working directory.\'\n    #compile our unicode list of image links\n    image_links = [each.get(\'src\') for each in images]\n    for each in image_links:\n        filename=each.split(\'/\')[-1]\n        urllib.urlretrieve(each, filename)\n    return image_links\n\n#a standard call looks like this\n#get_images(\'http://www.wookmark.com\')\n\n', ""\nThis can be done with requests. Load the page and dump the binary content to a file.\nimport os\nimport requests\n\nurl = 'https://apod.nasa.gov/apod/image/1701/potw1636aN159_HST_2048.jpg'\npage = requests.get(url)\n\nf_ext = os.path.splitext(url)[-1]\nf_name = 'img{}'.format(f_ext)\nwith open(f_name, 'wb') as f:\n    f.write(page.content)\n\n"", '\nPython 3\nurllib.request — Extensible library for opening URLs\nfrom urllib.error import HTTPError\nfrom urllib.request import urlretrieve\n\ntry:\n    urlretrieve(image_url, image_local_path)\nexcept FileNotFoundError as err:\n    print(err)   # something wrong with local path\nexcept HTTPError as err:\n    print(err)  # something wrong with url\n\n', '\nI made a script expanding on Yup.\'s script. I fixed some things. It will now bypass 403:Forbidden problems. It wont crash when an image fails to be retrieved. It tries to avoid corrupted previews. It gets the right absolute urls. It gives out more information. It can be run with an argument from the command line. \n# getem.py\n# python2 script to download all images in a given url\n# use: python getem.py http://url.where.images.are\n\nfrom bs4 import BeautifulSoup\nimport urllib2\nimport shutil\nimport requests\nfrom urlparse import urljoin\nimport sys\nimport time\n\ndef make_soup(url):\n    req = urllib2.Request(url, headers={\'User-Agent\' : ""Magic Browser""}) \n    html = urllib2.urlopen(req)\n    return BeautifulSoup(html, \'html.parser\')\n\ndef get_images(url):\n    soup = make_soup(url)\n    images = [img for img in soup.findAll(\'img\')]\n    print (str(len(images)) + "" images found."")\n    print \'Downloading images to current working directory.\'\n    image_links = [each.get(\'src\') for each in images]\n    for each in image_links:\n        try:\n            filename = each.strip().split(\'/\')[-1].strip()\n            src = urljoin(url, each)\n            print \'Getting: \' + filename\n            response = requests.get(src, stream=True)\n            # delay to avoid corrupted previews\n            time.sleep(1)\n            with open(filename, \'wb\') as out_file:\n                shutil.copyfileobj(response.raw, out_file)\n        except:\n            print \'  An error occured. Continuing.\'\n    print \'Done.\'\n\nif __name__ == \'__main__\':\n    url = sys.argv[1]\n    get_images(url)\n\n', '\nA solution which works with Python 2 and Python 3:\ntry:\n    from urllib.request import urlretrieve  # Python 3\nexcept ImportError:\n    from urllib import urlretrieve  # Python 2\n\nurl = ""http://www.digimouth.com/news/media/2011/09/google-logo.jpg""\nurlretrieve(url, ""local-filename.jpg"")\n\nor, if the additional requirement of requests is acceptable and if it is a http(s) URL:\ndef load_requests(source_url, sink_path):\n    """"""\n    Load a file from an URL (e.g. http).\n\n    Parameters\n    ----------\n    source_url : str\n        Where to load the file from.\n    sink_path : str\n        Where the loaded file is stored.\n    """"""\n    import requests\n    r = requests.get(source_url, stream=True)\n    if r.status_code == 200:\n        with open(sink_path, \'wb\') as f:\n            for chunk in r:\n                f.write(chunk)\n\n', ""\nUsing requests library\nimport requests\nimport shutil,os\n\nheaders = {\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'\n}\ncurrentDir = os.getcwd()\npath = os.path.join(currentDir,'Images')#saving images to Images folder\n\ndef ImageDl(url):\n    attempts = 0\n    while attempts < 5:#retry 5 times\n        try:\n            filename = url.split('/')[-1]\n            r = requests.get(url,headers=headers,stream=True,timeout=5)\n            if r.status_code == 200:\n                with open(os.path.join(path,filename),'wb') as f:\n                    r.raw.decode_content = True\n                    shutil.copyfileobj(r.raw,f)\n            print(filename)\n            break\n        except Exception as e:\n            attempts+=1\n            print(e)\n\n\nImageDl(url)\n\n"", ""\nUse a simple python wget module to download the link. Usage below:\nimport wget\nwget.download('http://www.digimouth.com/news/media/2011/09/google-logo.jpg')\n\n"", '\nThis is very short answer.\nimport urllib\nurllib.urlretrieve(""http://photogallery.sandesh.com/Picture.aspx?AlubumId=422040"", ""Abc.jpg"")\n\n', '\nVersion for Python 3\nI adjusted the code of @madprops for Python 3\n# getem.py\n# python2 script to download all images in a given url\n# use: python getem.py http://url.where.images.are\n\nfrom bs4 import BeautifulSoup\nimport urllib.request\nimport shutil\nimport requests\nfrom urllib.parse import urljoin\nimport sys\nimport time\n\ndef make_soup(url):\n    req = urllib.request.Request(url, headers={\'User-Agent\' : ""Magic Browser""}) \n    html = urllib.request.urlopen(req)\n    return BeautifulSoup(html, \'html.parser\')\n\ndef get_images(url):\n    soup = make_soup(url)\n    images = [img for img in soup.findAll(\'img\')]\n    print (str(len(images)) + "" images found."")\n    print(\'Downloading images to current working directory.\')\n    image_links = [each.get(\'src\') for each in images]\n    for each in image_links:\n        try:\n            filename = each.strip().split(\'/\')[-1].strip()\n            src = urljoin(url, each)\n            print(\'Getting: \' + filename)\n            response = requests.get(src, stream=True)\n            # delay to avoid corrupted previews\n            time.sleep(1)\n            with open(filename, \'wb\') as out_file:\n                shutil.copyfileobj(response.raw, out_file)\n        except:\n            print(\'  An error occured. Continuing.\')\n    print(\'Done.\')\n\nif __name__ == \'__main__\':\n    get_images(\'http://www.wookmark.com\')\n\n', '\nLate answer, but for python>=3.6 you can use dload, i.e.:\nimport dload\ndload.save(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"")\n\nif you need the image as bytes, use:\nimg_bytes = dload.bytes(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"")\n\n\ninstall using pip3 install dload\n', '\nSomething fresh for Python 3 using Requests:\nComments in the code. Ready to use function.\n\nimport requests\nfrom os import path\n\ndef get_image(image_url):\n    """"""\n    Get image based on url.\n    :return: Image name if everything OK, False otherwise\n    """"""\n    image_name = path.split(image_url)[1]\n    try:\n        image = requests.get(image_url)\n    except OSError:  # Little too wide, but work OK, no additional imports needed. Catch all conection problems\n        return False\n    if image.status_code == 200:  # we could have retrieved error page\n        base_dir = path.join(path.dirname(path.realpath(__file__)), ""images"") # Use your own path or """" to use current working directory. Folder must exist.\n        with open(path.join(base_dir, image_name), ""wb"") as f:\n            f.write(image.content)\n        return image_name\n\nget_image(""https://apod.nasddfda.gov/apod/image/2003/S106_Mishra_1947.jpg"")\n\n\n', ""\nthis is the easiest method to download images.\nimport requests\nfrom slugify import slugify\n\nimg_url = 'https://apod.nasa.gov/apod/image/1701/potw1636aN159_HST_2048.jpg'\nimg = requests.get(img_url).content\nimg_file = open(slugify(img_url) + '.' + str(img_url).split('.')[-1], 'wb')\nimg_file.write(img)\nimg_file.close()\n\n"", '\nIf you don\'t already have the url for the image, you could scrape it with gazpacho:\nfrom gazpacho import Soup\nbase_url = ""http://books.toscrape.com""\n\nsoup = Soup.get(base_url)\nlinks = [img.attrs[""src""] for img in soup.find(""img"")]\n\nAnd then download the asset with urllib as mentioned:\nfrom pathlib import Path\nfrom urllib.request import urlretrieve as download\n\ndirectory = ""images""\nPath(directory).mkdir(exist_ok=True)\n\nlink = links[0]\nname = link.split(""/"")[-1]\n\ndownload(f""{base_url}/{link}"", f""{directory}/{name}"")\n\n', '\n# import the required libraries from Python\nimport pathlib,urllib.request \n\n# Using pathlib, specify where the image is to be saved\ndownloads_path = str(pathlib.Path.home() / ""Downloads"")\n\n# Form a full image path by joining the path to the \n# images\' new name\n\npicture_path  = os.path.join(downloads_path, ""new-image.png"")\n\n# ""/home/User/Downloads/new-image.png""\n\n# Using ""urlretrieve()"" from urllib.request save the image \nurllib.request.urlretrieve(""//example.com/image.png"", picture_path)\n\n# urlretrieve() takes in 2 arguments\n# 1. The URL of the image to be downloaded\n# 2. The image new name after download. By default, the image is saved\n#    inside your current working directory\n\n', '\nOk, so, this is my rudimentary attempt, and probably total overkill.\nUpdate if needed, as this doesn\'t handle any timeouts, but, I got this working for fun.\nCode listed here: https://github.com/JayRizzo/JayRizzoTools/blob/master/pyImageDownloader.py\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# =============================================================================\n# Created Syst: MAC OSX High Sierra 21.5.0 (17G65)\n# Created Plat: Python 3.9.5 (\'v3.9.5:0a7dcbdb13\', \'May  3 2021 13:17:02\')\n# Created By  : Jeromie Kirchoff\n# Created Date: Thu Jun 15 23:31:01 2022 CDT\n# Last ModDate: Thu Jun 16 01:41:01 2022 CDT\n# =============================================================================\n# NOTE: Doesn\'t work on SVG images at this time.\n# I will look into this further: https://stackoverflow.com/a/6599172/1896134\n# =============================================================================\nimport requests                                 # to get image from the web\nimport shutil                                   # to save it locally\nimport os                                       # needed\nfrom os.path import exists as filepathexist     # check if file paths exist\nfrom os.path import join                        # joins path for different os\nfrom os.path import expanduser                  # expands current home\nfrom pyuser_agent import UA                     # generates random UserAgent\n\nclass ImageDownloader(object):\n    """"""URL ImageDownloader.\n    Input : Full Image URL\n    Output: Image saved to your ~/Pictures/JayRizzoDL folder.\n    """"""\n    def __init__(self, URL: str):\n        self.url = URL\n        self.headers = {""User-Agent"" : UA().random}\n        self.currentHome = expanduser(\'~\')\n        self.desktop = join(self.currentHome + ""/Desktop/"")\n        self.download = join(self.currentHome + ""/Downloads/"")\n        self.pictures = join(self.currentHome + ""/Pictures/JayRizzoDL/"")\n        self.outfile = """"\n        self.filename = """"\n        self.response = """"\n        self.rawstream = """"\n        self.createdfilepath = """"\n        self.imgFileName = """"\n        # Check if the JayRizzoDL exists in the pictures folder.\n        # if it doesn\'t exist create it.\n        if not filepathexist(self.pictures):\n            os.mkdir(self.pictures)\n        self.main()\n\n    def getFileNameFromURL(self, URL: str):\n        """"""Parse the URL for the name after the last forward slash.""""""\n        NewFileName = self.url.strip().split(\'/\')[-1].strip()\n        return NewFileName\n\n    def getResponse(self, URL: str):\n        """"""Try streaming the URL for the raw data.""""""\n        self.response = requests.get(self.url, headers=self.headers, stream=True)\n        return self.response\n\n    def gocreateFile(self, name: str, response):\n        """"""Try creating the file with the raw data in a custom folder.""""""\n        self.outfile = join(self.pictures, name)\n        with open(self.outfile, \'wb\') as outFilePath:\n            shutil.copyfileobj(response.raw, outFilePath)\n        return self.outfile\n\n    def main(self):\n        """"""Combine Everything and use in for loops.""""""\n        self.filename = self.getFileNameFromURL(self.url)\n        self.rawstream = self.getResponse(self.url)\n        self.createdfilepath = self.gocreateFile(self.filename, self.rawstream)\n        print(f""File was created: {self.createdfilepath}"")\n        return\n\nif __name__ == \'__main__\':\n    # Example when calling the file directly.\n    ImageDownloader(""https://stackoverflow.design/assets/img/logos/so/logo-stackoverflow.png"")\n\n\n', '\nDownload Image file, with avoiding all possible error:\nimport requests\nimport validators\nfrom urllib.request import Request, urlopen\nfrom urllib.error import URLError, HTTPError\n\n\ndef is_downloadable(url):\n  valid=validators. url(url)\n  if valid==False:\n    return False\n  req = Request(url)\n  try:\n    response = urlopen(req)\n  except HTTPError as e:\n    return False\n  except URLError as e:\n    return False\n  else:\n    return True\n\n\n\nfor i in range(len(File_data)):   #File data Contain list of address for image \n                                                      #file\n  url = File_data[i][1]\n  try:\n    if (is_downloadable(url)):\n      try:\n        r = requests.get(url, allow_redirects=True)\n        if url.find(\'/\'):\n          fname = url.rsplit(\'/\', 1)[1]\n          fname = pth+File_data[i][0]+""$""+fname #Destination to save \n                                                   #image file\n          open(fname, \'wb\').write(r.content)\n      except Exception as e:\n        print(e)\n  except Exception as e:\n    print(e)\n\n']"
How to scrape a website which requires login using python and beautifulsoup?,"
If I want to scrape a website that requires login with password first, how can I start scraping it with python using beautifulsoup4 library? Below is what I do for websites that do not require login. 
from bs4 import BeautifulSoup    
import urllib2 
url = urllib2.urlopen(""http://www.python.org"")    
content = url.read()    
soup = BeautifulSoup(content)

How should the code be changed to accommodate login? Assume that the website I want to scrape is a forum that requires login. An example is http://forum.arduino.cc/index.php
",135k,"
            93
        ","['\nYou can use mechanize:\nimport mechanize\nfrom bs4 import BeautifulSoup\nimport urllib2 \nimport cookielib ## http.cookiejar in python3\n\ncj = cookielib.CookieJar()\nbr = mechanize.Browser()\nbr.set_cookiejar(cj)\nbr.open(""https://id.arduino.cc/auth/login/"")\n\nbr.select_form(nr=0)\nbr.form[\'username\'] = \'username\'\nbr.form[\'password\'] = \'password.\'\nbr.submit()\n\nprint br.response().read()\n\nOr urllib - Login to website using urllib2\n', ""\nThere is a simpler way, from my pov, that gets you there without selenium or mechanize, or other 3rd party tools, albeit it is semi-automated.\nBasically, when you login into a site in a normal way, you identify yourself in a unique way using your credentials, and the same identity is used  thereafter for every other interaction, which is stored in cookies and headers, for a brief period of time.\nWhat you need to do is use the same cookies and headers when you make your http requests, and you'll be in.\nTo replicate that, follow these steps:\n\nIn your browser, open the developer tools\nGo to the site, and login\nAfter the login, go to the network tab, and then refresh the page\nAt this point, you should see a list of requests, the top one being the actual site - and that will be our focus, because it contains the data with the identity we can use for Python and BeautifulSoup to scrape it\nRight click the site request (the top one), hover over copy, and then copy as \ncURL\nLike this:\n\n\n\nThen go to this site which converts cURL into python requests: https://curl.trillworks.com/\nTake the python code and use the generated cookies and headers to proceed with the scraping\n\n"", '\nIf you go for selenium, then you can do something like below:\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\n\n# If you want to open Chrome\ndriver = webdriver.Chrome()\n# If you want to open Firefox\ndriver = webdriver.Firefox()\n\nusername = driver.find_element_by_id(""username"")\npassword = driver.find_element_by_id(""password"")\nusername.send_keys(""YourUsername"")\npassword.send_keys(""YourPassword"")\ndriver.find_element_by_id(""submit_btn"").click()\n\nHowever, if you\'re adamant that you\'re only going to use BeautifulSoup, you can do that with a library like requests or urllib. Basically all you have to do is POST the data as a payload with the URL.\nimport requests\nfrom bs4 import BeautifulSoup\n\nlogin_url = \'http://example.com/login\'\ndata = {\n    \'username\': \'your_username\',\n    \'password\': \'your_password\'\n}\n\nwith requests.Session() as s:\n    response = s.post(login_url , data)\n    print(response.text)\n    index_page= s.get(\'http://example.com\')\n    soup = BeautifulSoup(index_page.text, \'html.parser\')\n    print(soup.title)\n\n', '\nYou can use selenium to log in and retrieve the page source, which you can then pass to Beautiful Soup to extract the data you want.\n', '\nSince Python version wasn\'t specified, here is my take on it for Python 3, done without any external libraries (StackOverflow). After login use BeautifulSoup as usual, or any other kind of scraping.\nLikewise, script on my GitHub here\nWhole script replicated below as to StackOverflow guidelines:\n# Login to website using just Python 3 Standard Library\nimport urllib.parse\nimport urllib.request\nimport http.cookiejar\n\ndef scraper_login():\n    ####### change variables here, like URL, action URL, user, pass\n    # your base URL here, will be used for headers and such, with and without https://\n    base_url = \'www.example.com\'\n    https_base_url = \'https://\' + base_url\n\n    # here goes URL that\'s found inside form action=\'.....\'\n    #   adjust as needed, can be all kinds of weird stuff\n    authentication_url = https_base_url + \'/login\'\n\n    # username and password for login\n    username = \'yourusername\'\n    password = \'SoMePassw0rd!\'\n\n    # we will use this string to confirm a login at end\n    check_string = \'Logout\'\n\n    ####### rest of the script is logic\n    # but you will need to tweak couple things maybe regarding ""token"" logic\n    #   (can be _token or token or _token_ or secret ... etc)\n\n    # big thing! you need a referer for most pages! and correct headers are the key\n    headers={""Content-Type"":""application/x-www-form-urlencoded"",\n    ""User-agent"":""Mozilla/5.0 Chrome/81.0.4044.92"",    # Chrome 80+ as per web search\n    ""Host"":base_url,\n    ""Origin"":https_base_url,\n    ""Referer"":https_base_url}\n\n    # initiate the cookie jar (using : http.cookiejar and urllib.request)\n    cookie_jar = http.cookiejar.CookieJar()\n    opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar))\n    urllib.request.install_opener(opener)\n\n    # first a simple request, just to get login page and parse out the token\n    #       (using : urllib.request)\n    request = urllib.request.Request(https_base_url)\n    response = urllib.request.urlopen(request)\n    contents = response.read()\n\n    # parse the page, we look for token eg. on my page it was something like this:\n    #    <input type=""hidden"" name=""_token"" value=""random1234567890qwertzstring"">\n    #       this can probably be done better with regex and similar\n    #       but I\'m newb, so bear with me\n    html = contents.decode(""utf-8"")\n    # text just before start and just after end of your token string\n    mark_start = \'<input type=""hidden"" name=""_token"" value=""\'\n    mark_end = \'"">\'\n    # index of those two points\n    start_index = html.find(mark_start) + len(mark_start)\n    end_index = html.find(mark_end, start_index)\n    # and text between them is our token, store it for second step of actual login\n    token = html[start_index:end_index]\n\n    # here we craft our payload, it\'s all the form fields, including HIDDEN fields!\n    #   that includes token we scraped earler, as that\'s usually in hidden fields\n    #   make sure left side is from ""name"" attributes of the form,\n    #       and right side is what you want to post as ""value""\n    #   and for hidden fields make sure you replicate the expected answer,\n    #       eg. ""token"" or ""yes I agree"" checkboxes and such\n    payload = {\n        \'_token\':token,\n    #    \'name\':\'value\',    # make sure this is the format of all additional fields !\n        \'login\':username,\n        \'password\':password\n    }\n\n    # now we prepare all we need for login\n    #   data - with our payload (user/pass/token) urlencoded and encoded as bytes\n    data = urllib.parse.urlencode(payload)\n    binary_data = data.encode(\'UTF-8\')\n    # and put the URL + encoded data + correct headers into our POST request\n    #   btw, despite what I thought it is automatically treated as POST\n    #   I guess because of byte encoded data field you don\'t need to say it like this:\n    #       urllib.request.Request(authentication_url, binary_data, headers, method=\'POST\')\n    request = urllib.request.Request(authentication_url, binary_data, headers)\n    response = urllib.request.urlopen(request)\n    contents = response.read()\n\n    # just for kicks, we confirm some element in the page that\'s secure behind the login\n    #   we use a particular string we know only occurs after login,\n    #   like ""logout"" or ""welcome"" or ""member"", etc. I found ""Logout"" is pretty safe so far\n    contents = contents.decode(""utf-8"")\n    index = contents.find(check_string)\n    # if we find it\n    if index != -1:\n        print(f""We found \'{check_string}\' at index position : {index}"")\n    else:\n        print(f""String \'{check_string}\' was not found! Maybe we did not login ?!"")\n\nscraper_login()\n\n']"
Python: find_element_by_css_selector,"
I am trying to click the login button with webdriver
<a class=""login-btn"" href=""javascript:;"" data-bind=""click:loginSection.loginClick"">
    <span class=""btn-text"">Login</span>
</a>

My code:
submit=driver.find_element_by_css_selector('a.login-btn').click()

or try this code:
submit=driver.find_element_by_class_name('login-btn').click()

Neither of these is working, need some advice. Thanks in advance
Error:
NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""css selector"",""selector"":""a.login-btn""}

",23k,"
            5
        ","['\nTo click on the Login button you can use either of the the following line of code :\n\nLinkText :\ndriver.find_element_by_link_text(""Login"").click()\n\nCssSelector :\ndriver.find_element_by_css_selector(""a.login-btn > span.btn-text"").click()\n\nGetting more granular with the CssSelector you can also use the following line of code :\ndriver.find_element_by_css_selector(""a.login-btn[data-bind=\'click:loginSection.loginClick\'] > span.btn-text"").click()\n\n\nUpdate :\nAs you are seeing NoSuchElementException you can check this discussion\n']"
How do you scrape AJAX pages?,"
Please advise how to scrape AJAX pages.
",74k,"
            57
        ","['\nOverview:\nAll screen scraping first requires manual review of the page you want to extract resources from.  When dealing with AJAX you usually just need to analyze a bit more than just simply the HTML. \nWhen dealing with AJAX this just means that the value you want is not in the initial HTML document that you requested, but that javascript will be exectued which asks the server for the extra information you want. \nYou can therefore usually simply analyze the javascript and see which request the javascript makes and just call this URL instead from the start. \n\nExample:\nTake this as an example, assume the page you want to scrape from has the following script:\n<script type=""text/javascript"">\nfunction ajaxFunction()\n{\nvar xmlHttp;\ntry\n  {\n  // Firefox, Opera 8.0+, Safari\n  xmlHttp=new XMLHttpRequest();\n  }\ncatch (e)\n  {\n  // Internet Explorer\n  try\n    {\n    xmlHttp=new ActiveXObject(""Msxml2.XMLHTTP"");\n    }\n  catch (e)\n    {\n    try\n      {\n      xmlHttp=new ActiveXObject(""Microsoft.XMLHTTP"");\n      }\n    catch (e)\n      {\n      alert(""Your browser does not support AJAX!"");\n      return false;\n      }\n    }\n  }\n  xmlHttp.onreadystatechange=function()\n    {\n    if(xmlHttp.readyState==4)\n      {\n      document.myForm.time.value=xmlHttp.responseText;\n      }\n    }\n  xmlHttp.open(""GET"",""time.asp"",true);\n  xmlHttp.send(null);\n  }\n</script>\n\nThen all you need to do is instead do an HTTP request to time.asp of the same server instead.   Example from w3schools.\n\nAdvanced scraping with C++: \nFor complex usage, and if you\'re using C++ you could also consider using the firefox javascript engine SpiderMonkey to execute the javascript on a page. \nAdvanced scraping with Java:\nFor complex usage, and if you\'re using Java you could also consider using the firefox javascript engine for Java Rhino\nAdvanced scraping with .NET:\nFor complex usage, and if you\'re using .Net you could also consider using the Microsoft.vsa assembly.  Recently replaced with ICodeCompiler/CodeDOM.\n', ""\nIn my opinion the simpliest solution is to use Casperjs, a framework based on the WebKit headless browser phantomjs.\nThe whole page is loaded, and it's very easy to scrape any ajax-related data.\nYou can check this basic tutorial to learn Automating & Scraping with PhantomJS and CasperJS\nYou can also give a look at this example code, on how to scrape google suggests keywords :\n/*global casper:true*/\nvar casper = require('casper').create();\nvar suggestions = [];\nvar word = casper.cli.get(0);\n\nif (!word) {\n    casper.echo('please provide a word').exit(1);\n}\n\ncasper.start('http://www.google.com/', function() {\n    this.sendKeys('input[name=q]', word);\n});\n\ncasper.waitFor(function() {\n  return this.fetchText('.gsq_a table span').indexOf(word) === 0\n}, function() {\n  suggestions = this.evaluate(function() {\n      var nodes = document.querySelectorAll('.gsq_a table span');\n      return [].map.call(nodes, function(node){\n          return node.textContent;\n      });\n  });\n});\n\ncasper.run(function() {\n  this.echo(suggestions.join('\\n')).exit();\n});\n\n"", '\nIf you can get at it, try examining the DOM tree. Selenium does this as a part of testing a page. It also has functions to click buttons and follow links, which may be useful.\n', '\nThe best way to scrape web pages using Ajax or in general pages using Javascript is with a browser itself or a headless browser (a browser without GUI). Currently phantomjs is a well promoted headless browser using WebKit. An alternative that I used with success is HtmlUnit (in Java or .NET via IKVM, which is a simulated browser. Another known alternative is using a web automation tool like Selenium.\nI wrote many articles about this subject like web scraping Ajax and Javascript sites and automated browserless OAuth authentication for Twitter. At the end of the first article there are a lot of extra resources that I have been compiling since 2011.\n', ""\nI like PhearJS, but that might be partially because I built it.\nThat said, it's a service you run in the background that speaks HTTP(S) and renders pages as JSON for you, including any metadata you might need.\n"", ""\nDepends on the ajax page.  The first part of screen scraping is determining how the page works.  Is there some sort of variable you can iterate through to request all the data from the page?  Personally I've used Web Scraper Plus for a lot of screen scraping related tasks because it is cheap, not difficult to get started, non-programmers can get it working relatively quickly.\nSide Note: Terms of Use is probably somewhere you might want to check before doing this.  Depending on the site iterating through everything may raise some flags.  \n"", '\nI think Brian R. Bondy\'s answer is useful when the source code is easy to read. I prefer an easy way using tools like Wireshark or HttpAnalyzer to capture the packet and get the url from  the ""Host"" field and the ""GET"" field.\nFor example,I capture a packet like the following:\nGET /hqzx/quote.aspx?type=3&market=1&sorttype=3&updown=up&page=1&count=8&time=164330 \n HTTP/1.1\nAccept: */*\nReferer: http://quote.hexun.com/stock/default.aspx\nAccept-Language: zh-cn\nAccept-Encoding: gzip, deflate\nUser-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)\nHost: quote.tool.hexun.com\nConnection: Keep-Alive\n\nThen the URL is : \nhttp://quote.tool.hexun.com/hqzx/quote.aspx?type=3&market=1&sorttype=3&updown=up&page=1&count=8&time=164330\n\n', '\nAs a low cost solution you can also try SWExplorerAutomation (SWEA).  The program creates an automation API for any Web application developed with HTML, DHTML or AJAX. \n', '\nSelenium WebDriver is a good solution: you program a browser and you automate what needs to be done in the browser. Browsers (Chrome, Firefox, etc) provide their own drivers that work with Selenium. Since it works as an automated REAL browser, the pages (including javascript and Ajax) get loaded as they do with a human using that browser.\nThe downside is that it is slow (since you would most probably like to wait for all images and scripts to load before you do your scraping on that single page).\n', ""\nI have previously linked to MIT's solvent and EnvJS as my answers to scrape off Ajax pages. These projects seem no longer accessible.\nOut of sheer necessity, I have invented another way to actually scrape off Ajax pages, and it has worked for tough sites like findthecompany which have methods to find headless javascript engines and show no data.\nThe technique is to use chrome extensions to do scraping. Chrome extensions are the best place to scrape off Ajax pages because they actually allow us access to javascript modified DOM. The technique is as follows, I will certainly open source the code in sometime. Create a chrome extension ( assuming you know how to create one, and its architecture and capabilities. This is easy to learn and practice as there are lots of samples),\n\nUse content scripts to access the DOM, by using xpath. Pretty much get the entire list or table or dynamically rendered content using xpath into a variable as string HTML Nodes. ( Only content scripts can access DOM but they can't contact a URL using XMLHTTP )\nFrom content script, using message passing, message the entire stripped DOM as string, to a background script. ( Background scripts can talk to URLs but can't touch the DOM ). We use message passing to get these to talk.\nYou can use various events to loop through web pages and pass each stripped HTML Node content to the background script.\nNow use the background script, to talk to an external server (on localhost), a simple one created using Nodejs/python. Just send the entire HTML Nodes as string, to the server, where the server would just persist the content posted to it, into files, with appropriate variables to identify page numbers or URLs.\nNow you have scraped AJAX content ( HTML Nodes as string ), but these are partial html nodes. Now you can use your favorite XPATH library to load these into memory and use XPATH to scrape information into Tables or text.\n\nPlease comment if you cant understand and I can write it better. ( first attempt ). Also, I am trying to release sample code as soon as possible.\n""]"
Scraping Google Finance (BeautifulSoup),"
I'm trying to scrape Google Finance, and get the ""Related Stocks"" table, which has id ""cc-table"" and class ""gf-table"" based on the webpage inspector in Chrome. (Sample Link: https://www.google.com/finance?q=tsla)
But when I run .find(""table"") or .findAll(""table""), this table does not come up. I can find JSON-looking objects with the table's contents in the HTML content in Python, but do not know how to get it. Any ideas?
",7k,"
            1
        ","['\nThe page is rendered with JavaScript. There are several ways to render and scrape it.\nI can scrape it with Selenium.\nFirst install Selenium:\nsudo pip3 install selenium\n\nThen get a driver https://sites.google.com/a/chromium.org/chromedriver/downloads\nimport bs4 as bs\nfrom selenium import webdriver  \nbrowser = webdriver.Chrome()\nurl = (""https://www.google.com/finance?q=tsla"")\nbrowser.get(url)\nhtml_source = browser.page_source\nbrowser.quit()\nsoup = bs.BeautifulSoup(html_source, ""lxml"")\nfor el in soup.find_all(""table"", {""id"": ""cc-table""}):\n    print(el.get_text())\n\nAlternatively  PyQt5\nfrom PyQt5.QtGui import *  \nfrom PyQt5.QtCore import *  \nfrom PyQt5.QtWebKit import *  \nfrom PyQt5.QtWebKitWidgets import QWebPage\nfrom PyQt5.QtWidgets import QApplication\nimport bs4 as bs\nimport sys\n\nclass Render(QWebPage):  \n    def __init__(self, url):  \n        self.app = QApplication(sys.argv)  \n        QWebPage.__init__(self)  \n        self.loadFinished.connect(self._loadFinished)  \n        self.mainFrame().load(QUrl(url))  \n        self.app.exec_()  \n\n    def _loadFinished(self, result):  \n        self.frame = self.mainFrame()  \n        self.app.quit()  \n\nurl = ""https://www.google.com/finance?q=tsla""\nr = Render(url)  \nresult = r.frame.toHtml()\nsoup = bs.BeautifulSoup(result,\'lxml\')\nfor el in soup.find_all(""table"", {""id"": ""cc-table""}):\n    print(el.get_text())\n\nAlternatively Dryscrape \nimport bs4 as bs\nimport dryscrape\n\nurl = ""https://www.google.com/finance?q=tsla""\nsession = dryscrape.Session()\nsession.visit(url)\ndsire_get = session.body()\nsoup = bs.BeautifulSoup(dsire_get,\'lxml\')\nfor el in soup.find_all(""table"", {""id"": ""cc-table""}):\n    print(el.get_text())\n\nall output:\nValuation▲▼Company name▲▼Price▲▼Change▲▼Chg %▲▼d | m | y▲▼Mkt Cap▲▼TSLATesla Inc328.40-1.52-0.46%53.69BDDAIFDaimler AG72.94-1.50-2.01%76.29BFFord Motor Company11.53-0.17-1.45%45.25BGMGeneral Motors Co...36.07-0.34-0.93%53.93BRNSDFRENAULT SA EUR3.8197.000.000.00%28.69BHMCHonda Motor Co Lt...27.52-0.18-0.65%49.47BAUDVFAUDI AG NPV840.400.000.00%36.14BTMToyota Motor Corp...109.31-0.53-0.48%177.79BBAMXFBAYER MOTOREN WER...94.57-2.41-2.48%56.93BNSANYNissan Motor Co L...20.400.000.00%42.85BMMTOFMITSUBISHI MOTOR ...6.86+0.091.26%10.22B\n\nEDIT\nQtWebKit got deprecated upstream in Qt 5.5 and removed in 5.6.\nYou can switch to PyQt5.QtWebEngineWidgets\n', '\nYou can scrape Google Finance using BeautifulSoup web scraping library without the need to use selenium as the data you want to extract doesn\'t render via Javascript. Plus it will be much faster than launching the whole browser.\nCheck code in online IDE.\n\nfrom bs4 import BeautifulSoup\nimport requests, lxml, json\n   \nparams = {\n        ""hl"": ""en"" \n        }\n\nheaders = {\n        ""User-Agent"": ""Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36"",\n        }\n\nhtml = requests.get(f""https://www.google.com/finance?q=tsla)"", params=params, headers=headers, timeout=30)\nsoup = BeautifulSoup(html.text, ""lxml"")\n\nticker_data = []\n\nfor ticker in soup.select(\'.tOzDHb\'):\n  title = ticker.select_one(\'.RwFyvf\').text\n  price = ticker.select_one(\'.YMlKec\').text\n  index = ticker.select_one(\'.COaKTb\').text\n  price_change = ticker.select_one(""[jsname=Fe7oBc]"")[""aria-label""]\n\n  ticker_data.append({\n    ""index"": index,\n  ""title"" : title,\n  ""price"" : price,\n  ""price_change"" : price_change\n  })  \nprint(json.dumps(ticker_data, indent=2))\n\nExample output\n[\n  {\n    ""index"": ""Index"",\n    ""title"": ""Dow Jones Industrial Average"",\n    ""price"": ""32,774.41"",\n    ""price_change"": ""Down by 0.18%""\n  },\n  {\n    ""index"": ""Index"",\n    ""title"": ""S&P 500"",\n    ""price"": ""4,122.47"",\n    ""price_change"": ""Down by 0.42%""\n  },\n  {\n    ""index"": ""TSLA"",\n    ""title"": ""Tesla Inc"",\n    ""price"": ""$850.00"",\n    ""price_change"": ""Down by 2.44%""\n  },\n  # ...\n]\n\n\nThere\'s a scrape Google Finance Ticker Quote Data in Python blog post if you need to scrape more data from Google Finance.\n', ""\nMost website owners don't like scrapers because they take data the company values, use up a whole bunch of their server time and bandwidth, and give nothing in return. Big companies like Google may have entire teams employing a whole host of methods to detect and block bots trying to scrape their data.\nThere are several ways around this:\n\nScrape from another less secured website.\nSee if Google or another company has an API for public use.\nUse a more advanced scraper like Selenium (and probably still be blocked by google).\n\n""]"
How to run Scrapy from within a Python script,"
I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:
http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/
http://snipplr.com/view/67006/using-scrapy-from-a-script/
I can't figure out where I should put my spider code and how to call it from the main function. Please help. This is the example code:
# This snippet can be used to run scrapy spiders independent of scrapyd or the scrapy command line tool and use it from a script. 
# 
# The multiprocessing library is used in order to work around a bug in Twisted, in which you cannot restart an already running reactor or in this case a scrapy instance.
# 
# [Here](http://groups.google.com/group/scrapy-users/browse_thread/thread/f332fc5b749d401a) is the mailing-list discussion for this snippet. 

#!/usr/bin/python
import os
os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'project.settings') #Must be at the top before other imports

from scrapy import log, signals, project
from scrapy.xlib.pydispatch import dispatcher
from scrapy.conf import settings
from scrapy.crawler import CrawlerProcess
from multiprocessing import Process, Queue

class CrawlerScript():

    def __init__(self):
        self.crawler = CrawlerProcess(settings)
        if not hasattr(project, 'crawler'):
            self.crawler.install()
        self.crawler.configure()
        self.items = []
        dispatcher.connect(self._item_passed, signals.item_passed)

    def _item_passed(self, item):
        self.items.append(item)

    def _crawl(self, queue, spider_name):
        spider = self.crawler.spiders.create(spider_name)
        if spider:
            self.crawler.queue.append_spider(spider)
        self.crawler.start()
        self.crawler.stop()
        queue.put(self.items)

    def crawl(self, spider):
        queue = Queue()
        p = Process(target=self._crawl, args=(queue, spider,))
        p.start()
        p.join()
        return queue.get(True)

# Usage
if __name__ == ""__main__"":
    log.start()

    """"""
    This example runs spider1 and then spider2 three times. 
    """"""
    items = list()
    crawler = CrawlerScript()
    items.append(crawler.crawl('spider1'))
    for i in range(3):
        items.append(crawler.crawl('spider2'))
    print items

# Snippet imported from snippets.scrapy.org (which no longer works)
# author: joehillen
# date  : Oct 24, 2010

Thank you.
",82k,"
            85
        ","[""\nAll other answers reference Scrapy v0.x. According to the updated docs, Scrapy 1.0 demands:\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider(scrapy.Spider):\n    # Your spider definition\n    ...\n\nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\n\nprocess.crawl(MySpider)\nprocess.start() # the script will block here until the crawling is finished\n\n"", '\nSimply we can use\nfrom scrapy.crawler import CrawlerProcess\nfrom project.spiders.test_spider import SpiderName\n\nprocess = CrawlerProcess()\nprocess.crawl(SpiderName, arg1=val1,arg2=val2)\nprocess.start()\n\nUse these arguments inside spider __init__ function with the global scope.\n', ""\nThough I haven't tried it I think the answer can be found within the scrapy documentation. To quote directly from it:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import Settings\nfrom scrapy import log\nfrom testspiders.spiders.followall import FollowAllSpider\n\nspider = FollowAllSpider(domain='scrapinghub.com')\ncrawler = Crawler(Settings())\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here\n\nFrom what I gather this is a new development in the library which renders some of the earlier approaches online (such as that in the question) obsolete.\n"", '\nIn scrapy 0.19.x you should do this:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy import log, signals\nfrom testspiders.spiders.followall import FollowAllSpider\nfrom scrapy.utils.project import get_project_settings\n\nspider = FollowAllSpider(domain=\'scrapinghub.com\')\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.signals.connect(reactor.stop, signal=signals.spider_closed)\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here until the spider_closed signal was sent\n\nNote these lines     \nsettings = get_project_settings()\ncrawler = Crawler(settings)\n\nWithout it your spider won\'t use your settings and will not save the items.\nTook me a while to figure out why the example in documentation wasn\'t saving my items. I sent a pull request to fix the doc example.\nOne more to do so is just call command directly from you script\nfrom scrapy import cmdline\ncmdline.execute(""scrapy crawl followall"".split())  #followall is the spider\'s name\n\nCopied this answer from my first answer in here:\nhttps://stackoverflow.com/a/19060485/1402286\n', '\nWhen there are multiple crawlers need to be run inside one python script, the reactor stop needs to be handled with caution as the reactor can only be stopped once and cannot be restarted. \nHowever, I found while doing my project that using \nos.system(""scrapy crawl yourspider"")\n\nis the easiest. This will save me from handling all sorts of signals especially when I have multiple spiders.\nIf Performance is a concern, you can use multiprocessing to run your spiders in parallel, something like:\ndef _crawl(spider_name=None):\n    if spider_name:\n        os.system(\'scrapy crawl %s\' % spider_name)\n    return None\n\ndef run_crawler():\n\n    spider_names = [\'spider1\', \'spider2\', \'spider2\']\n\n    pool = Pool(processes=len(spider_names))\n    pool.map(_crawl, spider_names)\n\n', '\nit  is an improvement of\nScrapy throws an error when run using crawlerprocess\nand https://github.com/scrapy/scrapy/issues/1904#issuecomment-205331087\nFirst create your usual spider for successful command line running. it is very very important that it should run and export data or image or file\nOnce it is over, do just like pasted in my program above spider class definition and below __name __ to invoke settings.\nit will get necessary settings which ""from scrapy.utils.project import get_project_settings"" failed to do which is recommended by many\nboth above and below portions should be there together. only one don\'t run.\nSpider will run in scrapy.cfg folder not any other folder\ntree  diagram may be displayed by the moderators for reference\n#Tree\n[enter image description here][1]\n\n#spider.py\nimport sys\nsys.path.append(r\'D:\\ivana\\flow\') #folder where scrapy.cfg is located\n\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.settings import Settings\nfrom flow import settings as my_settings\n\n#----------------Typical Spider Program starts here-----------------------------\n\n          spider class definition here\n\n#----------------Typical Spider Program ends here-------------------------------\n\nif __name__ == ""__main__"":\n\n    crawler_settings = Settings()\n    crawler_settings.setmodule(my_settings)\n\n    process = CrawlerProcess(settings=crawler_settings)\n    process.crawl(FlowSpider) # it is for class FlowSpider(scrapy.Spider):\n    process.start(stop_after_crawl=True)\n\n', ""\n# -*- coding: utf-8 -*-\nimport sys\nfrom scrapy.cmdline import execute\n\n\ndef gen_argv(s):\n    sys.argv = s.split()\n\n\nif __name__ == '__main__':\n    gen_argv('scrapy crawl abc_spider')\n    execute()\n\nPut this code to the path you can run scrapy crawl abc_spider from command line. (Tested with Scrapy==0.24.6)\n"", ""\nIf you want to run a simple crawling, It's easy by just running command: \nscrapy crawl . \nThere is another options to export your results to store in some formats like: \nJson, xml, csv. \nscrapy crawl  -o result.csv or result.json or result.xml. \nyou may want to try it\n""]"
How to connect via HTTPS using Jsoup?,"
It's working fine over HTTP, but when I try and use an HTTPS source it throws the following exception:
10-12 13:22:11.169: WARN/System.err(332): javax.net.ssl.SSLHandshakeException: java.security.cert.CertPathValidatorException: Trust anchor for certification path not found.
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.xnet.provider.jsse.OpenSSLSocketImpl.startHandshake(OpenSSLSocketImpl.java:477)
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.xnet.provider.jsse.OpenSSLSocketImpl.startHandshake(OpenSSLSocketImpl.java:328)
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.http.HttpConnection.setupSecureSocket(HttpConnection.java:185)
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.https.HttpsURLConnectionImpl$HttpsEngine.makeSslConnection(HttpsURLConnectionImpl.java:433)
10-12 13:22:11.189: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.https.HttpsURLConnectionImpl$HttpsEngine.makeConnection(HttpsURLConnectionImpl.java:378)
10-12 13:22:11.189: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.http.HttpURLConnectionImpl.connect(HttpURLConnectionImpl.java:205)
10-12 13:22:11.189: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:152)
10-12 13:22:11.189: WARN/System.err(332):     at org.jsoup.helper.HttpConnection$Response.execute(HttpConnection.java:377)
10-12 13:22:11.189: WARN/System.err(332):     at org.jsoup.helper.HttpConnection$Response.execute(HttpConnection.java:364)
10-12 13:22:11.189: WARN/System.err(332):     at org.jsoup.helper.HttpConnection.execute(HttpConnection.java:143)

Here's the relevant code:
try {
    doc = Jsoup.connect(""https url here"").get();
} catch (IOException e) {
    Log.e(""sys"",""coudnt get the html"");
    e.printStackTrace();
}

",53k,"
            28
        ","['\nIf you want to do it the right way, and/or you need to deal with only one site, then you basically need to grab the SSL certificate of the website in question and import it in your Java key store. This will result in a JKS file which you in turn set as SSL trust store before using Jsoup (or java.net.URLConnection). \nYou can grab the certificate from your webbrowser\'s store. Let\'s assume that you\'re using Firefox.\n\nGo to the website in question using Firefox, which is in your case https://web2.uconn.edu/driver/old/timepoints.php?stopid=10\nLeft in the address bar you\'ll see ""uconn.edu"" in blue (this indicates a valid SSL certificate)\nClick on it for details and then click on the More information button.\nIn the security dialogue which appears, click the View Certificate button.\nIn the certificate panel which appears, go to the Details tab.\nClick the deepest item of the certificate hierarchy, which is in this case ""web2.uconn.edu"" and finally click the Export button.\n\nNow you\'ve a web2.uconn.edu.crt file.\nNext, open the command prompt and import it in the Java key store using the keytool command (it\'s part of the JRE):\nkeytool -import -v -file /path/to/web2.uconn.edu.crt -keystore /path/to/web2.uconn.edu.jks -storepass drowssap\n\nThe -file must point to the location of the .crt file which you just downloaded. The -keystore must point to the location of the generated .jks file (which you in turn want to set as SSL trust store). The -storepass is required, you can just enter whatever password you want as long as it\'s at least 6 characters.\nNow, you\'ve a web2.uconn.edu.jks file. You can finally set it as SSL trust store before connecting as follows:\nSystem.setProperty(""javax.net.ssl.trustStore"", ""/path/to/web2.uconn.edu.jks"");\nDocument document = Jsoup.connect(""https://web2.uconn.edu/driver/old/timepoints.php?stopid=10"").get();\n// ...\n\n\nAs a completely different alternative, particularly when you need to deal with multiple sites (i.e. you\'re creating a world wide web crawler), then you can also instruct Jsoup (basically, java.net.URLConnection) to blindly trust all SSL certificates. See also section ""Dealing with untrusted or misconfigured HTTPS sites"" at the very bottom of this answer: Using java.net.URLConnection to fire and handle HTTP requests\n', '\nIn my case, all I needed to do was to add the .validateTLSCertificates(false) in my connection\nDocument doc  = Jsoup.connect(httpsURLAsString)\n            .timeout(60000).validateTLSCertificates(false).get();\n\nI also had to increase the read timeout but I think this is irrelevant\n', '\nI stumbled over the answers here and in the linked question in my search and want to add two pieces of information, as the accepted answer doesn\'t fit my quite similar scenario, but there is an additional solution that fits even in that case (cert and hostname don\'t match for test systems).\n\nThere is a github request to add such a functionality. So perhaps soon the problem will be solved: https://github.com/jhy/jsoup/pull/343 \nedit: Github request was resolved and the method to disable certificate validation is: validateTLSCertificates(boolean validate)\nBased on http://www.nakov.com/blog/2009/07/16/disable-certificate-validation-in-java-ssl-connections/ I found a solution which seems to work (at least in my scenario where jsoup 1.7.3 is called as part of a maven task). I wrapped it in a method disableSSLCertCheck() that I call before the very first Jsoup.connect().\n\nBefore you use this method, you should be really sure that you understand what you do there - not checking SSL certificates is a really stupid thing. Always use correct SSL certificates for your servers which are signed by a commonly accepted CA. If you can\'t afford a commonly accepted CA use correct SSL certificates nevertheless with @BalusC accepted answer above. If you can\'t configure correct SSL certificates (which should never be the case in production environments) the following method could work:\n    private void disableSSLCertCheck() throws NoSuchAlgorithmException, KeyManagementException {\n    // Create a trust manager that does not validate certificate chains\n    TrustManager[] trustAllCerts = new TrustManager[] {new X509TrustManager() {\n            public java.security.cert.X509Certificate[] getAcceptedIssuers() {\n                return null;\n            }\n            public void checkClientTrusted(X509Certificate[] certs, String authType) {\n            }\n            public void checkServerTrusted(X509Certificate[] certs, String authType) {\n            }\n        }\n    };\n\n    // Install the all-trusting trust manager\n    SSLContext sc = SSLContext.getInstance(""SSL"");\n    sc.init(null, trustAllCerts, new java.security.SecureRandom());\n    HttpsURLConnection.setDefaultSSLSocketFactory(sc.getSocketFactory());\n\n    // Create all-trusting host name verifier\n    HostnameVerifier allHostsValid = new HostnameVerifier() {\n        public boolean verify(String hostname, SSLSession session) {\n            return true;\n        }\n    };\n\n    // Install the all-trusting host verifier\n    HttpsURLConnection.setDefaultHostnameVerifier(allHostsValid);\n    }\n\n', '\nTo suppress certificate warnings for specific JSoup connection can use following approach:\nKotlin\n\nval document = Jsoup.connect(""url"")\n        .sslSocketFactory(socketFactory())\n        .get()\n\n\nprivate fun socketFactory(): SSLSocketFactory {\n    val trustAllCerts = arrayOf<TrustManager>(object : X509TrustManager {\n        @Throws(CertificateException::class)\n        override fun checkClientTrusted(chain: Array<X509Certificate>, authType: String) {\n        }\n\n        @Throws(CertificateException::class)\n        override fun checkServerTrusted(chain: Array<X509Certificate>, authType: String) {\n        }\n\n        override fun getAcceptedIssuers(): Array<X509Certificate> {\n            return arrayOf()\n        }\n    })\n\n    try {\n        val sslContext = SSLContext.getInstance(""TLS"")\n        sslContext.init(null, trustAllCerts, java.security.SecureRandom())\n        return sslContext.socketFactory\n    } catch (e: Exception) {\n        when (e) {\n            is RuntimeException, is KeyManagementException -> {\n                throw RuntimeException(""Failed to create a SSL socket factory"", e)\n            }\n            else -> throw e\n        }\n    }\n}\n\n\nJava\n\n\n Document document = Jsoup.connect(""url"")\n        .sslSocketFactory(socketFactory())\n        .get();\n\n\n  private SSLSocketFactory socketFactory() {\n    TrustManager[] trustAllCerts = new TrustManager[]{new X509TrustManager() {\n      public java.security.cert.X509Certificate[] getAcceptedIssuers() {\n        return null;\n      }\n\n      public void checkClientTrusted(X509Certificate[] certs, String authType) {\n      }\n\n      public void checkServerTrusted(X509Certificate[] certs, String authType) {\n      }\n    }};\n\n    try {\n      SSLContext sslContext = SSLContext.getInstance(""TLS"");\n      sslContext.init(null, trustAllCerts, new java.security.SecureRandom());\n      return sslContext.getSocketFactory();\n    } catch (NoSuchAlgorithmException | KeyManagementException e) {\n      throw new RuntimeException(""Failed to create a SSL socket factory"", e);\n    }\n  }\n\n\nNB. As mentioned before ignoring certificates is not a good idea. \n', ""\nI've had the same problem but took the lazy route - tell your app to ignore the cert and carry on anyway.\nI got the code from here:  How do I use a local HTTPS URL in java?\nYou'll have to import these classes for it to work:\nimport javax.net.ssl.HostnameVerifier;\nimport javax.net.ssl.HttpsURLConnection;\nimport javax.net.ssl.SSLContext;\nimport javax.net.ssl.SSLSession;\nimport javax.net.ssl.TrustManager;\nimport javax.net.ssl.X509TrustManager;\n\nJust run that method somewhere before you try to make the connection and voila, it just trusts the cert no matter what.  Of course this isn't any help if you actually want to make sure the cert is real, but good for monitoring your own internal websites etc.\n"", ""\nI'm no expert in this field but I ran into a similar exception when trying to connect to a website over HTTPS using java.net APIs.  The browser does a lot of work for you regarding SSL certificates when you visit a site using HTTPS.  However, when you are manually connecting to sites (using HTTP requests manually), all that work still needs to be done.  Now I don't know what all this work is exactly, but it has to do with downloading certificates and putting them where Java can find them.  Here's a link that will hopefully point you in the right direction.\nhttp://confluence.atlassian.com/display/JIRA/Connecting+to+SSL+services\n"", '\nI was facing the same issue with Jsoup, I was not able to connect and get the document for https urls but when I changed my JDK version from 1.7 to 1.8, the issue got resolved.\nIt may help you :) \n', ""\nI've had that problem only in dev environment. The solution to solve it was just to add a few flags to ignore SSL to VM:\n-Ddeployment.security.TLSv1.1=false \n-Ddeployment.security.TLSv1.2=false\n\n"", '\nAfter testing the solutions here. It is strange that sslSocketFactory setting in Jsoup is completely useless and it never works. So there is no need to get and set SSLSocketFactory.\nActually the second half of Mori solution works. Just need the following before using Jsoup:\n// Create all-trusting host name verifier\nHostnameVerifier allHostsValid = new HostnameVerifier() {\n    public boolean verify(String hostname, SSLSession session) {\n        return true;\n    }\n};\n\n// Install the all-trusting host verifier\nHttpsURLConnection.setDefaultHostnameVerifier(allHostsValid);\n\nThis is tested with Jsoup 1.13.1.\n', '\nTry following (just put it before Jsoup.connect(""https://example.com""):\n    Authenticator.setDefault(new Authenticator() {\n        @Override\n        protected PasswordAuthentication getPasswordAuthentication() {\n            return new PasswordAuthentication(username, password.toCharArray());\n        }\n    });\n\n']"
How to run Puppeteer code in any web browser?,"
I'm trying to do some web scraping with Puppeteer and I need to retrieve the value into a Website I'm building.
I have tried to load the Puppeteer file in the html file as if it was a JavaScript file but I keep getting an error. However, if I run it in a cmd window it works well.

Scraper.js:

getPrice();
function getPrice() {
    const puppeteer = require('puppeteer');
    void (async () => {
        try {
            const browser = await puppeteer.launch()
            const page = await browser.newPage()              
            await page.goto('http://example.com') 
            await page.setViewport({ width: 1920, height: 938 })        
            await page.waitForSelector('.m-hotel-info > .l-container > .l-header-section > .l-m-col-2 > .m-button')
            await page.click('.m-hotel-info > .l-container > .l-header-section > .l-m-col-2 > .m-button')
            await page.waitForSelector('.modal-content')
            await page.click('.tile-hsearch-hws > .m-search-tabs > #edit-search-panel > .l-em-reset > .m-field-wrap > .l-xs-col-4 > .analytics-click')
            await page.waitForNavigation();
            await page.waitForSelector('.tile-search-filter > .l-display-none')
            const innerText = await page.evaluate(() => document.querySelector('.tile-search-filter > .l-display-none').innerText);
            console.log(innerText)
        } catch (error) {
            console.log(error)
        }

    })()
}


index.html:

<html>
  <head></head>
  <body>
    <script src=""../js/scraper.js"" type=""text/javascript""></script>
  </body>
</html>

The expected result should be this one in the console of Chrome:

But I'm getting this error instead:


What am I doing wrong?
",19k,"
            11
        ","['\nEDIT: Since puppeteer removed support for puppeteer-web, I moved it out of the repo and tried to patch it a bit.\nIt does work with browser. The package is called puppeteer-web, specifically made for such cases.\nBut the main point is, there must be some instance of chrome running on some server. Only then you can connect to it.\nYou can use it later on in your web page to drive another browser instance through its WS Endpoint:\n<script src=""https://unpkg.com/puppeteer-web"">\n</script>\n\n<script>\n  const browser = await puppeteer.connect({\n    browserWSEndpoint: `ws://0.0.0.0:8080`, // <-- connect to a server running somewhere\n    ignoreHTTPSErrors: true\n  });\n\n  const pagesCount = (await browser.pages()).length;\n  const browserWSEndpoint = await browser.wsEndpoint();\n  console.log({ browserWSEndpoint, pagesCount });\n</script>\n\nI had some fun with puppeteer and webpack,\n\nplayground-react-puppeteer\nplayground-electron-react-puppeteer-example\n\nSee these answers for full understanding of creating the server and more,\n\nOfficial link to puppeteer-web\nPuppeteer with docker\nPuppeteer with chrome extension\nPuppeteer with local wsEndpoint\n\n', '\nInstead, use Puppeteer in the backend and make an API to interface your frontend with it if your main goal is to web scrape and get the data in the frontend.\n', ""\nPuppeteer runs on the server in Node.js. For the common case, rather than using puppeteer-web to allow the client to write Puppeteer code to control the browser, it's better to create an HTTP or websocket API that lets clients indirectly trigger Puppeteer code.\nReasons to prefer a REST API over puppeteer-connect:\n\nbetter support for arbitrary client codebases--clients that aren't written in JS (desktop, command line and mobile apps, for example) can use the API just as easily as the browser can\nno dependency on puppeteer-connect\nlower client-side complexity; for many use cases JS won't be required at all if HTML forms suffice\nbetter control of client behavior--running a browser on the server is a heavy load and has powerful capabilities that are easy to exploit\neasier to integrate with other backend code and resources like the file system\nprovides seamless integration with an existing API as just another set of routes\nhiding Puppeteer as an implementation detail lets you switch to, say, Playwright in the future without the client code being affected.\n\nSimilarly, rather than exposing a mock fs object to read and write files on the server, we expose REST API endpoints to accomplish these tasks. This is a useful layer of abstraction.\nSince there are many use cases for Puppeteer in the context of an API (usually Express), it's hard to offer a general example, but here are a few case studies you can use as starting points:\n\nPuppeteer unable to run on Heroku\nPuppeteer doesn't close browser\nParallelism of Puppeteer with Express Router Node JS. How to pass page between routes while maintaining concurrency\n\n""]"
Accessing object in iframe using VBA,"
To the point:
I have successfully used VBA to do the following:

Login to a website using getElementsByName
Select parameters for the report that will be generated (using getelementsby...)
generating the report after selecting parameters which renders the resulting dataset into an iframe on the same page

Important to note - The website is client-side
The above was the simple part, the difficult part is as below:

clicking on a gif image within the iframe that exports the dataset to a csv

I have tried the following:
Dim idoc As HTMLDocument
Dim iframe As HTMLFrameElement
Dim iframe2 As HTMLDocument

Set idoc = objIE.document
Set iframe = idoc.all(""iframename"")
Set iframe2 = iframe.contentDocument

    Do Until InStr(1, objIE.document.all(""iframename"").contentDocument.innerHTML, ""img.gif"", vbTextCompare) = 0
        DoEvents
    Loop

To give some context to the logic above -

I accessed the main frame
i accessed the iframe by its name element
i accessed the content within the iframe
I attempted to find the gif image that needs to be clicked to export to csv

It is at this line that it trips up saying ""Object doesn't support this property or method""
Also tried accessing the iframe gif by the a element and href attribute but this totally failed. I also tried grabbing the image from its source URL but all this does it take me to the page the image is from.
note: the iframe does not have an ID and strangely the gif image does not have an ""onclick"" element/event

Final consideration - attempted scraping the iframe using R

accessing the HTML node of the iframe was simple, however trying to access the attributes of the iframe and subsequently the nodes of the table proved unsuccessful. All it returned was ""Character(0)""
library(rvest)
library(magrittr)

Blah <-read_html(""web address redacted"") %>%
  html_nodes(""#iframe"")%>%
  html_nodes(""#img"")%>%
  html_attr(""#src"")%>%
  #read_html()%>%
  head()
Blah

As soon as a i include read_html the following error returns on the script:
Error in if (grepl(""<|>"", x)) { : argument is of length zero
I suspect this is referring to the Character(0) 
Appreciate any guidance here!
Many Thanks,

HTML

<div align=""center""> 
    <table id=""table1"" style=""border-collapse: collapse"" width=""700"" cellspacing=""0"" cellpadding=""0"" border=""0""> 
        <tbody>
            <tr>
                <td colspan=""6""> &nbsp;</td>
            </tr> 
            <tr> 
                <td colspan=""6""> 
                    <a href=""href redacted"">
                        <img src=""img.gif"" width=""38"" height=""38"" border=""0"" align=""right"">
                    </a>
                    <strong>x - </strong>
                </td>
            </tr> 
        </tbody>
    </table>
</div>

",19k,"
            10
        ","['\nIt is sometimes tricky with iframes. Based on html you provided I have created this example. Which works locally, but would it work for you as well?\nTo get to the IFrame the frames collection can be used. Hope you know the name of the IFrame?\nDim iframeDoc As MSHTML.HTMLDocument\nSet iframeDoc = doc.frames(""iframename"").document\n\nThen to go the the image we can use querySelector method e.g. like this:\nDim img As MSHTML.HTMLImg\nSet img = iframeDoc.querySelector(""div table[id=\'table1\'] tbody tr td a[href^=\'https://stackoverflow.com\'] img"")\n\nThe selector a[href^=\'https://stackoverflow.com\'] selects anchor which has an href attribute which starts with given text. The ^ denotes the beginning.\nThen when we have the image just a simple call to click on its parent which is the desired anchor. HTH\n\nComplete example:\nOption Explicit\n\n\' Add reference to Microsoft Internet Controls (SHDocVw)\n\' Add reference to Microsoft HTML Object Library\n\nSub Demo()\n\n    Dim ie As SHDocVw.InternetExplorer\n    Dim doc As MSHTML.HTMLDocument\n    Dim url As String\n    \n    url = ""file:///C:/Users/dusek/Documents/My Web Sites/mainpage.html""\n    Set ie = New SHDocVw.InternetExplorer\n    ie.Visible = True\n    ie.navigate url\n\n    While ie.Busy Or ie.readyState <> READYSTATE_COMPLETE\n        DoEvents\n    Wend\n    \n    Set doc = ie.document\n    \n    Dim iframeDoc As MSHTML.HTMLDocument\n    Set iframeDoc = doc.frames(""iframename"").document\n    If iframeDoc Is Nothing Then\n        MsgBox ""IFrame with name \'iframename\' was not found.""\n        ie.Quit\n        Exit Sub\n    End If\n    \n    Dim img As MSHTML.HTMLImg\n    Set img = iframeDoc.querySelector(""div table[id=\'table1\'] tbody tr td a[href^=\'https://stackoverflow.com\'] img"")\n    If img Is Nothing Then\n        MsgBox ""Image element within iframe was not found.""\n        ie.Quit\n        Exit Sub\n    Else\n        img.parentElement.Click\n    End If\n    \n    ie.Quit\nEnd Sub\n\n\nMain page HTML used\n\n<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">\n<html xmlns=""http://www.w3.org/1999/xhtml"">\n\n<head>\n<!-- saved from url=(0016)http://localhost -->\n<meta content=""text/html; charset=utf-8"" http-equiv=""Content-Type"" />\n<title>x -</title>\n</head>\n\n<body>\n<iframe name=""iframename"" src=""iframe1.html"">\n</iframe>\n</body>\n\n</html>\n\n\nIFrame HTML used (saved as file iframe1.html\n\n<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">\n<html xmlns=""http://www.w3.org/1999/xhtml"">\n\n<head>\n<!-- saved from url=(0016)http://localhost -->\n<meta content=""text/html; charset=utf-8"" http-equiv=""Content-Type"" />\n<title>Untitled 2</title>\n</head>\n\n<body>\n<div align=""center""> \n    <table id=""table1"" style=""border-collapse: collapse"" width=""700"" cellspacing=""0"" cellpadding=""0"" border=""0""> \n        <tbody>\n            <tr>\n                <td colspan=""6""> &nbsp;</td>\n            </tr> \n            <tr> \n                <td colspan=""6""> \n                    <a href=""https://stackoverflow.com/questions/44902558/accessing-object-in-iframe-using-vba"">\n                        <img src=""img.gif"" width=""38"" height=""38"" border=""0"" align=""right"">\n                    </a>\n                    <strong>x - </strong>\n                </td>\n            </tr> \n        </tbody>\n    </table>\n</div>\n\n</body>\n\n</html>\n\n\nBTW, The frame may be referenced by it\'s index also doc.frames(0).document. Thanks to Paulo Bueno.\n\n', '\nI thought I would expand on the answer already given.\nIn the case of Internet Explorer you may have one of two common situations to handle regarding iframes.\n\nsrc of iframe is subject to same origin policy restrictions:\n\n\nThe iframe src has a different origin to the landing page in which case, due to same origin policy, attempts to access it will yield access denied.\nResolution:\nConsider using selenium basic to automate a different browser such as Chrome where CORS is allowed/you can switch to the iframe and continue working with the iframe document\nExample:\nOption Explicit\n\'download selenium https://github.com/florentbr/SeleniumBasic/releases/tag/v2.0.9.0\n\'Ensure latest applicable driver e.g. ChromeDriver.exe in Selenium folder\n\'VBE > Tools > References > Add reference to selenium type library\nPublic Sub Example()\n    Dim d As WebDriver\n    Const URL As String = ""https://www.rosterresource.com/mlb-roster-grid/""\n    Set d = New ChromeDriver\n    With d\n        .Start ""Chrome""\n        .get URL\n        .SwitchToFrame .FindElementByCss(""iframe"") \'< pass the iframe element as the identifier argument\n        \' .SwitchToDefaultContent \'\'to go back to parent document.\n        Stop \'<== delete me later\n        .Quit\n    End With\nEnd Sub\n\n\n\nsrc of iframe is not subject to same origin policy restrictions:\n\n\nResolution:\nThe methods as detailed in answer already given. Additionally, you can extract the src of the iframe and .Navigate2 that to access\n.Navigate2 .document.querySelector(""iframe"").src\n\nIf you only want to work with the contents of the iframe then simply do your initial .Navigate2 the iframe src and don\'t even visit the initial landing page\nExample:\nOption Explicit\nPublic Sub NavigateUsingSrcOfIframe()\n    Dim IE As New InternetExplorer\n    With IE\n        .Visible = True\n        .Navigate2 ""http://www.bursamalaysia.com/market/listed-companies/company-announcements/5978065""\n\n        While .Busy Or .readyState < 4: DoEvents: Wend\n        \n        .Navigate2 .document.querySelector(""iframe"").src\n        \n        While .Busy Or .readyState < 4: DoEvents: Wend\n\n        Stop \'<== delete me later\n        .Quit\n    End With\nEnd Sub\n\n\n\niframe in ShadowRoot\n\n\nAn unlikely case might be an iframe in shadowroot. You should really have one or the other and not one within the other.\n\nResolution:\nIn that case you need an additional accessor of\nElement.shadowRoot.querySelector(""iframe"").contentDocument\n\nwhere Element is your parent element with shadowRoot attached. This method will only work if the shadowRoot mode is set to Open.\nSide note:\nA nice selenium based example, using ExecuteScript to return shadowRoot is given here: How Do I Access Elements in the Shadow DOM using Selenium in VBA?\n', ""\nAdding to the answers given:\nIf you're ok with using a DLL and rewrite your code, you can run Microsoft's Edge browser (a Chrome-based browser) with VBA. With that you can do almost anything you want. Note however, that access to the DOM is performed by javascript, not by an object like Dim IE As New InternetExplorer. Look at the VBA sample and you'll get the grasp.\nhttps://github.com/peakpeak-github/libEdge\nSidenote: Samples for C# and C++ are also included.\n""]"
Scrapy Very Basic Example,"
Hi I have Python Scrapy installed on my mac and I was trying to follow the very first example on their web. 
They were trying to run the command:
scrapy crawl mininova.org -o scraped_data.json -t json

I don't quite understand what does this mean? looks like scrapy turns out to be a separate program. And I don't think they have a command called crawl. In the example, they have a paragraph of code, which is the definition of the class MininovaSpider and the TorrentItem. I don't know where these two classes should go to, go to the same file and what is the name of this python file? 
",24k,"
            26
        ","['\nTL;DR: see Self-contained minimum example script to run scrapy.\nFirst of all, having a normal Scrapy project with a separate .cfg, settings.py, pipelines.py, items.py, spiders package etc is a recommended way to keep and handle your web-scraping logic. It provides a modularity, separation of concerns that keeps things organized, clear and testable. \nIf you are following the official Scrapy tutorial to create a project, you are running web-scraping via a special scrapy command-line tool:\nscrapy crawl myspider\n\n\nBut, Scrapy also provides an API to run crawling from a script.\nThere are several key concepts that should be mentioned:\n\nSettings class - basically a key-value ""container"" which is initialized with default built-in values\nCrawler class - the main class that acts like a glue for all the different components involved in web-scraping with Scrapy\nTwisted reactor - since Scrapy is built-in on top of twisted asynchronous networking library - to start a crawler, we need to put it inside the Twisted Reactor, which is in simple words, an event loop:\n\n\nThe reactor is the core of the event loop within Twisted – the loop which drives applications using Twisted. The event loop is a programming construct that waits for and\n  dispatches events or messages in a program. It works by calling some\n  internal or external “event provider”, which generally blocks until an\n  event has arrived, and then calls the relevant event handler\n  (“dispatches the event”). The reactor provides basic interfaces to a\n  number of services, including network communications, threading, and\n  event dispatching.\n\nHere is a basic and simplified process of running Scrapy from script:\n\ncreate a Settings instance (or use get_project_settings() to use existing settings):\nsettings = Settings()  # or settings = get_project_settings()\n\ninstantiate Crawler with settings instance passed in:\ncrawler = Crawler(settings)\n\ninstantiate a spider (this is what it is all about eventually, right?):\nspider = MySpider()\n\nconfigure signals. This is an important step if you want to have a post-processing logic, collect stats or, at least, to ever finish crawling since the twisted reactor needs to be stopped manually. Scrapy docs suggest to stop the reactor in the spider_closed signal handler:\n\n\nNote that you will also have to shutdown the Twisted reactor yourself\n  after the spider is finished. This can be achieved by connecting a\n  handler to the signals.spider_closed signal.\n\ndef callback(spider, reason):\n    stats = spider.crawler.stats.get_stats()\n    # stats here is a dictionary of crawling stats that you usually see on the console        \n\n    # here we need to stop the reactor\n    reactor.stop()\n\ncrawler.signals.connect(callback, signal=signals.spider_closed)\n\n\nconfigure and start crawler instance with a spider passed in:\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\n\noptionally start logging:\nlog.start()\n\nstart the reactor - this would block the script execution:\nreactor.run()\n\n\nHere is an example self-contained script that is using DmozSpider spider and involves item loaders with input and output processors and item pipelines:\nimport json\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.contrib.loader import ItemLoader\nfrom scrapy.contrib.loader.processor import Join, MapCompose, TakeFirst\nfrom scrapy import log, signals, Spider, Item, Field\nfrom scrapy.settings import Settings\nfrom twisted.internet import reactor\n\n\n# define an item class\nclass DmozItem(Item):\n    title = Field()\n    link = Field()\n    desc = Field()\n\n\n# define an item loader with input and output processors\nclass DmozItemLoader(ItemLoader):\n    default_input_processor = MapCompose(unicode.strip)\n    default_output_processor = TakeFirst()\n\n    desc_out = Join()\n\n\n# define a pipeline\nclass JsonWriterPipeline(object):\n    def __init__(self):\n        self.file = open(\'items.jl\', \'wb\')\n\n    def process_item(self, item, spider):\n        line = json.dumps(dict(item)) + ""\\n""\n        self.file.write(line)\n        return item\n\n\n# define a spider\nclass DmozSpider(Spider):\n    name = ""dmoz""\n    allowed_domains = [""dmoz.org""]\n    start_urls = [\n        ""http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"",\n        ""http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/""\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath(\'//ul/li\'):\n            loader = DmozItemLoader(DmozItem(), selector=sel, response=response)\n            loader.add_xpath(\'title\', \'a/text()\')\n            loader.add_xpath(\'link\', \'a/@href\')\n            loader.add_xpath(\'desc\', \'text()\')\n            yield loader.load_item()\n\n\n# callback fired when the spider is closed\ndef callback(spider, reason):\n    stats = spider.crawler.stats.get_stats()  # collect/log stats?\n\n    # stop the reactor\n    reactor.stop()\n\n\n# instantiate settings and provide a custom configuration\nsettings = Settings()\nsettings.set(\'ITEM_PIPELINES\', {\n    \'__main__.JsonWriterPipeline\': 100\n})\n\n# instantiate a crawler passing in settings\ncrawler = Crawler(settings)\n\n# instantiate a spider\nspider = DmozSpider()\n\n# configure signals\ncrawler.signals.connect(callback, signal=signals.spider_closed)\n\n# configure and start the crawler\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\n\n# start logging\nlog.start()\n\n# start the reactor (blocks execution)\nreactor.run()\n\nRun it in a usual way:\npython runner.py\n\nand observe items exported to items.jl with the help of the pipeline:\n{""desc"": """", ""link"": ""/"", ""title"": ""Top""}\n{""link"": ""/Computers/"", ""title"": ""Computers""}\n{""link"": ""/Computers/Programming/"", ""title"": ""Programming""}\n{""link"": ""/Computers/Programming/Languages/"", ""title"": ""Languages""}\n{""link"": ""/Computers/Programming/Languages/Python/"", ""title"": ""Python""}\n...\n\nGist is available here (feel free to improve): \n\nSelf-contained minimum example script to run scrapy\n\n\nNotes:\nIf you define settings by instantiating a Settings() object - you\'ll get all the defaults Scrapy settings. But, if you want to, for example, configure an existing pipeline, or configure a DEPTH_LIMIT or tweak any other setting, you need to either set it in the script via settings.set() (as demonstrated in the example):\npipelines = {\n    \'mypackage.pipelines.FilterPipeline\': 100,\n    \'mypackage.pipelines.MySQLPipeline\': 200\n}\nsettings.set(\'ITEM_PIPELINES\', pipelines, priority=\'cmdline\')\n\nor, use an existing settings.py with all the custom settings preconfigured:\nfrom scrapy.utils.project import get_project_settings\n\nsettings = get_project_settings()\n\n\nOther useful links on the subject:\n\nHow to run Scrapy from within a Python script\nConfused about running Scrapy from within a Python script\nscrapy run spider from script\n\n', '\nYou may have better luck looking through the tutorial first, as opposed to the ""Scrapy at a glance"" webpage.\nThe tutorial implies that Scrapy is, in fact, a separate program. \nRunning the command scrapy startproject tutorial will create a folder called tutorial several files already set up for you. \nFor example, in my case, the modules/packages items, pipelines, settings and spiders have been added to the root package tutorial .\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\nThe TorrentItem class would be placed inside items.py, and the MininovaSpider class would go inside the spiders folder. \nOnce the project is set up, the command-line parameters for Scrapy appear to be fairly straightforward. They take the form:\nscrapy crawl <website-name> -o <output-file> -t <output-type>\n\nAlternatively, if you want to run scrapy without the overhead of creating a project directory, you can use the runspider command:\nscrapy runspider my_spider.py\n\n']"
Scrape web page contents,"
I am developing a project, for which I want to scrape the contents of a website in the background and get some limited content from that scraped website. For example, in my page I have ""userid"" and ""password"" fields, by using those I will access my mail and scrape my inbox contents and display it in my page.
I done the above by using javascript alone. But when I click the sign in button the URL of my page (http://localhost/web/Login.html) is changed to the URL (http://mail.in.com/mails/inbox.php?nomail=....) which I am scraped. But I scrap the details without changing my url.
",73k,"
            20
        ","[""\nDefinitely go with PHP Simple HTML DOM Parser. It's fast, easy and super flexible. It basically sticks an entire HTML page in an object then you can access any element from that object.\nLike the example of the official site, to get all links on the main Google page:\n// Create DOM from URL or file\n$html = file_get_html('http://www.google.com/');\n\n// Find all images \nforeach($html->find('img') as $element) \n       echo $element->src . '<br>';\n\n// Find all links \nforeach($html->find('a') as $element) \n       echo $element->href . '<br>';\n\n"", '\nThe HTTP Request\nFirst, you make an HTTP request to get the content of the page. There are several ways to do that.\nfopen\nThe most basic way to send an HTTP request, is to use fopen. A main advantage is that you can set how many characters are read at a time, which can be useful when reading very large files. It\'s not the easiest thing to do correctly, though, and it\'s not recommended to do this unless you\'re reading very large files and fear running into memory issues.\n$fp = fopen(""http://www.4wtech.com/csp/web/Employee/Login.csp"", ""rb"");\nif (FALSE === $fp) {\n    exit(""Failed to open stream to URL"");\n}\n\n$result = \'\';\n\nwhile (!feof($fp)) {\n    $result .= fread($fp, 8192);\n}\nfclose($fp);\necho $result;\n\nfile_get_contents\nThe easiest way, is just using file_get_contents. If does more or less the same as fopen, but you have less options to choose from. A main advantage here is that it requires but one line of code.\n$result = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\');\necho $result;\n\nsockets\nIf you need more control of what headers are sent to the server, you can use sockets, in combination with fopen.\n$fp = fsockopen(""www.4wtech.com/csp/web/Employee/Login.csp"", 80, $errno, $errstr, 30);\nif (!$fp) {\n    $result = ""$errstr ($errno)<br />\\n"";\n} else {\n    $result = \'\';\n    $out = ""GET / HTTP/1.1\\r\\n"";\n    $out .= ""Host: www.4wtech.com/csp/web/Employee/Login.csp\\r\\n"";\n    $out .= ""Connection: Close\\r\\n\\r\\n"";\n    fwrite($fp, $out);\n    while (!feof($fp)) {\n        $result .= fgets($fp, 128);\n    }\n    fclose($fp);\n}\necho $result;\n\nstreams\nAlternatively, you can also use streams. Streams are similar to sockets and can be used in combination with both fopen and file_get_contents.\n$opts = array(\n  \'http\'=>array(\n    \'method\'=>""GET"",\n    \'header\'=>""Accept-language: en\\r\\n"" .\n              ""Cookie: foo=bar\\r\\n""\n  )\n);\n\n$context = stream_context_create($opts);\n\n$result = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\', false, $context);\necho result;\n\ncURL\nIf your server supports cURL (it usually does), it is recommended to use cURL. A key advantage of using cURL, is that it relies on a popular C library commonly used in other programming languages. It also provides a convenient way for creating request headers, and auto-parses response headers, with a simple interface in case of errors.\n$defaults = array( \n    CURLOPT_URL, ""http://www.4wtech.com/csp/web/Employee/Login.csp""\n    CURLOPT_HEADER=> 0\n);\n\n$ch = curl_init(); \ncurl_setopt_array($ch, ($options + $defaults)); \nif( ! $result = curl_exec($ch)) { \n    trigger_error(curl_error($ch)); \n} \ncurl_close($ch); \necho $result; \n\nLibraries\nAlternatively, you can use one of many PHP libraries. I wouldn\'t recommend using a library, though, as it\'s likely to be overkill. In most cases, you\'re better off writing your own HTTP class using cURL under the hood.\n\nThe HTML parsing\nPHP has a convenient way to load any HTML into a DOMDocument.\n$pagecontent = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\');\n$doc = new DOMDocument();\n$doc->loadHTML($pagecontent);\necho $doc->saveHTML();\n\nUnfortunately, PHP support for HTML5 is limited. If you run into errors trying to parse your page content, consider using a third party library. For that, I can recommend Masterminds/html5-php. Parsing an HTML file with this library is very similar to parsing an HTML file with DOMDocument.\nuse Masterminds\\HTML5;\n\n$pagecontent = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\');\n$html5 = new HTML5();\n$dom = $html5->loadHTML($html);\necho $html5->saveHTML($dom);\n\nAlternatively, you can use eg. my library PHPPowertools/DOM-Query. It uses customized version of Masterminds/html5-php under the hood parsing an HTML5 string into a DomDocument and symfony/DomCrawler for conversion of CSS selectors to XPath selectors. It always uses the same DomDocument, even when passing one object to another, to ensure decent performance.\nnamespace PowerTools;\n\n// Get file content\n$pagecontent = file_get_contents( \'http://www.4wtech.com/csp/web/Employee/Login.csp\' );\n\n// Define your DOMCrawler based on file string\n$H = new DOM_Query( $pagecontent );\n\n// Define your DOMCrawler based on an existing DOM_Query instance\n$H = new DOM_Query( $H->select(\'body\') );\n\n// Passing a string (CSS selector)\n$s = $H->select( \'div.foo\' );\n\n// Passing an element object (DOM Element)\n$s = $H->select( $documentBody );\n\n// Passing a DOM Query object\n$s = $H->select( $H->select(\'p + p\') );\n\n// Select the body tag\n$body = $H->select(\'body\');\n\n// Combine different classes as one selector to get all site blocks\n$siteblocks = $body->select(\'.site-header, .masthead, .site-body, .site-footer\');\n\n// Nest your methods just like you would with jQuery\n$siteblocks->select(\'button\')->add(\'span\')->addClass(\'icon icon-printer\');\n\n// Use a lambda function to set the text of all site blocks\n$siteblocks->text(function( $i, $val) {\n    return $i . "" - "" . $val->attr(\'class\');\n});\n\n// Append the following HTML to all site blocks\n$siteblocks->append(\'<div class=""site-center""></div>\');\n\n// Use a descendant selector to select the site\'s footer\n$sitefooter = $body->select(\'.site-footer > .site-center\');\n\n// Set some attributes for the site\'s footer\n$sitefooter->attr(array(\'id\' => \'aweeesome\', \'data-val\' => \'see\'));\n\n// Use a lambda function to set the attributes of all site blocks\n$siteblocks->attr(\'data-val\', function( $i, $val) {\n    return $i . "" - "" . $val->attr(\'class\') . "" - photo by Kelly Clark"";\n});\n\n// Select the parent of the site\'s footer\n$sitefooterparent = $sitefooter->parent();\n\n// Remove the class of all i-tags within the site\'s footer\'s parent\n$sitefooterparent->select(\'i\')->removeAttr(\'class\');\n\n// Wrap the site\'s footer within two nex selectors\n$sitefooter->wrap(\'<section><div class=""footer-wrapper""></div></section>\');\n\n', '\nYou can use the cURL extension of PHP to do HTTP requests to another web site from within your PHP page script. See the documentation here.\nOf course the downside here is that your site will respond slowly because you will have to scrape the external web site before you can present the full page/output to your user.\n', ""\nHave you tried OutWit Hub? It's a whole scraping environment. You can let it try to guess the structure or develop your own scrapers. I really suggest you have a look at it. It made my life much simpler.\nZR\n""]"
How can I catch and process the data from the XHR responses using casperjs?,"
The data on the webpage is displayed dynamically and it seems that checking for every change in the html and extracting the data is a very daunting task and also needs me to use very unreliable XPaths. So I would want to be able to extract the data from the XHR packets. 
I hope to be able to extract information from XHR packets as well as generate 'XHR' packets to be sent to the server. 
The extracting information part is more important for me because the sending of information can be handled easily by automatically triggering html elements using casperjs.
I'm attaching a screenshot of what I mean.
The text in the response tab is the data I need to process afterwards. (This XHR response has been received from the server.)
",15k,"
            12
        ","['\nThis is not easily possible, because the resource.received event handler only provides meta data like url, headers or status, but not the actual data. The underlying phantomjs event handler acts the same way.\n\nStateless AJAX Request\nIf the ajax call is stateless, you may repeat the request\ncasper.on(""resource.received"", function(resource){\n    // somehow identify this request, here: if it contains "".json""\n    // it also also only does something when the stage is ""end"" otherwise this would be executed two times\n    if (resource.url.indexOf("".json"") != -1 && resource.stage == ""end"") {\n        var data = casper.evaluate(function(url){\n            // synchronous GET request\n            return __utils__.sendAJAX(url, ""GET"");\n        }, resource.url);\n        // do something with data, you might need to JSON.parse(data)\n    }\n});\ncasper.start(url); // your script\n\nYou may want to add the event listener to resource.requested. That way you don\'t need to way for the call to complete.\nYou can also do this right inside of the control flow like this (source: A: CasperJS waitForResource: how to get the resource i\'ve waited for):\ncasper.start(url);\n\nvar res, resData;\ncasper.waitForResource(function check(resource){\n    res = resource;\n    return resource.url.indexOf("".json"") != -1;\n}, function then(){\n    resData = casper.evaluate(function(url){\n        // synchronous GET request\n        return __utils__.sendAJAX(url, ""GET"");\n    }, res.url);\n    // do something with the data here or in a later step\n});\n\ncasper.run();\n\n\nStateful AJAX Request\nIf it is not stateless, you would need to replace the implementation of XMLHttpRequest. You will need to inject your own implementation of the onreadystatechange handler, collect the information in the page window object and later collect it in another evaluate call.\nYou may want to look at the XHR faker in sinon.js or use the following complete proxy for XMLHttpRequest (I modeled it after method 3 from How can I create a XMLHttpRequest wrapper/proxy?):\nfunction replaceXHR(){\n    (function(window, debug){\n        function args(a){\n            var s = """";\n            for(var i = 0; i < a.length; i++) {\n                s += ""\\t\\n["" + i + ""] => "" + a[i];\n            }\n            return s;\n        }\n        var _XMLHttpRequest = window.XMLHttpRequest;\n\n        window.XMLHttpRequest = function() {\n            this.xhr = new _XMLHttpRequest();\n        }\n\n        // proxy ALL methods/properties\n        var methods = [ \n            ""open"", \n            ""abort"", \n            ""setRequestHeader"", \n            ""send"", \n            ""addEventListener"", \n            ""removeEventListener"", \n            ""getResponseHeader"", \n            ""getAllResponseHeaders"", \n            ""dispatchEvent"", \n            ""overrideMimeType""\n        ];\n        methods.forEach(function(method){\n            window.XMLHttpRequest.prototype[method] = function() {\n                if (debug) console.log(""ARGUMENTS"", method, args(arguments));\n                if (method == ""open"") {\n                    this._url = arguments[1];\n                }\n                return this.xhr[method].apply(this.xhr, arguments);\n            }\n        });\n\n        // proxy change event handler\n        Object.defineProperty(window.XMLHttpRequest.prototype, ""onreadystatechange"", {\n            get: function(){\n                // this will probably never called\n                return this.xhr.onreadystatechange;\n            },\n            set: function(onreadystatechange){\n                var that = this.xhr;\n                var realThis = this;\n                that.onreadystatechange = function(){\n                    // request is fully loaded\n                    if (that.readyState == 4) {\n                        if (debug) console.log(""RESPONSE RECEIVED:"", typeof that.responseText == ""string"" ? that.responseText.length : ""none"");\n                        // there is a response and filter execution based on url\n                        if (that.responseText && realThis._url.indexOf(""whatever"") != -1) {\n                            window.myAwesomeResponse = that.responseText;\n                        }\n                    }\n                    onreadystatechange.call(that);\n                };\n            }\n        });\n\n        var otherscalars = [\n            ""onabort"",\n            ""onerror"",\n            ""onload"",\n            ""onloadstart"",\n            ""onloadend"",\n            ""onprogress"",\n            ""readyState"",\n            ""responseText"",\n            ""responseType"",\n            ""responseXML"",\n            ""status"",\n            ""statusText"",\n            ""upload"",\n            ""withCredentials"",\n            ""DONE"",\n            ""UNSENT"",\n            ""HEADERS_RECEIVED"",\n            ""LOADING"",\n            ""OPENED""\n        ];\n        otherscalars.forEach(function(scalar){\n            Object.defineProperty(window.XMLHttpRequest.prototype, scalar, {\n                get: function(){\n                    return this.xhr[scalar];\n                },\n                set: function(obj){\n                    this.xhr[scalar] = obj;\n                }\n            });\n        });\n    })(window, false);\n}\n\nIf you want to capture the AJAX calls from the very beginning, you need to add this to one of the first event handlers\ncasper.on(""page.initialized"", function(resource){\n    this.evaluate(replaceXHR);\n});\n\nor evaluate(replaceXHR) when you need it.\nThe control flow would look like this:\nfunction replaceXHR(){ /* from above*/ }\n\ncasper.start(yourUrl, function(){\n    this.evaluate(replaceXHR);\n});\n\nfunction getAwesomeResponse(){\n    return this.evaluate(function(){\n        return window.myAwesomeResponse;\n    });\n}\n\n// stops waiting if window.myAwesomeResponse is something that evaluates to true\ncasper.waitFor(getAwesomeResponse, function then(){\n    var data = JSON.parse(getAwesomeResponse());\n    // Do something with data\n});\n\ncasper.run();\n\nAs described above, I create a proxy for XMLHttpRequest so that every time it is used on the page, I can do something with it. The page that you scrape uses the xhr.onreadystatechange callback to receive data. The proxying is done by defining a specific setter function which writes the received data to window.myAwesomeResponse in the page context. The only thing you need to do is retrieving this text.\n\nJSONP Request\nWriting a proxy for JSONP is even easier, if you know the prefix (the function to call with the loaded JSON e.g. insert({""data"":[""Some"", ""JSON"", ""here""],""id"":""asdasda"")). You can overwrite insert in the page context\n\nafter the page is loaded\ncasper.start(url).then(function(){\n    this.evaluate(function(){\n        var oldInsert = insert;\n        insert = function(json){\n            window.myAwesomeResponse = json;\n            oldInsert.apply(window, arguments);\n        };\n    });\n}).waitFor(getAwesomeResponse, function then(){\n    var data = JSON.parse(getAwesomeResponse());\n    // Do something with data\n}).run();\n\nor before the request is received (if the function is registered just before the request is invoked)\ncasper.on(""resource.requested"", function(resource){\n    // filter on the correct call\n    if (resource.url.indexOf("".jsonp"") != -1) {\n        this.evaluate(function(){\n            var oldInsert = insert;\n            insert = function(json){\n                window.myAwesomeResponse = json;\n                oldInsert.apply(window, arguments);\n            };\n        });\n    }\n}).run();\n\ncasper.start(url).waitFor(getAwesomeResponse, function then(){\n    var data = JSON.parse(getAwesomeResponse());\n    // Do something with data\n}).run();\n\n\n', '\nI may be late into the party, but the answer may help someone like me who would fall into this problem later in future.\nI had to start with PhantomJS, then moved to CasperJS but finally settled with SlimerJS. Slimer is based on Phantom, is compatible with Casper, and can send you back the response body using the same onResponseReceived method, in ""response.body"" part.\nReference: https://docs.slimerjs.org/current/api/webpage.html#webpage-onresourcereceived\n', '\n@Artjom\'s answer\'s doesn\'t work for me in the recent Chrome and CasperJS versions.\nBased on @Artjom\'s answer and based on gilly3\'s answer on how to replace XMLHttpRequest, I have composed a new solution that should work in most/all versions of the different browsers. Works for me.\nSlimerJS cannot work on newer version of FireFox, therefore no good for me.\nHere is the the generic code to add a listner to load of XHR (not dependent on CasperJS):\nvar addXHRListener = function (XHROnStateChange) {\n\n    var XHROnLoad = function () {\n        if (this.readyState == 4) {\n            XHROnStateChange(this)\n        }\n    }\n\n    var open_original = XMLHttpRequest.prototype.open;\n\n    XMLHttpRequest.prototype.open = function (method, url, async, unk1, unk2) {\n        this.requestUrl = url\n        open_original.apply(this, arguments);\n    };\n\n    var xhrSend = XMLHttpRequest.prototype.send;\n    XMLHttpRequest.prototype.send = function () {\n\n        var xhr = this;\n        if (xhr.addEventListener) {\n            xhr.removeEventListener(""readystatechange"", XHROnLoad);\n            xhr.addEventListener(""readystatechange"", XHROnLoad, false);\n        } else {\n            function readyStateChange() {\n                if (handler) {\n                    if (handler.handleEvent) {\n                        handler.handleEvent.apply(xhr, arguments);\n                    } else {\n                        handler.apply(xhr, arguments);\n                    }\n                }\n                XHROnLoad.apply(xhr, arguments);\n                setReadyStateChange();\n            }\n\n            function setReadyStateChange() {\n                setTimeout(function () {\n                    if (xhr.onreadystatechange != readyStateChange) {\n                        handler = xhr.onreadystatechange;\n                        xhr.onreadystatechange = readyStateChange;\n                    }\n                }, 1);\n            }\n\n            var handler;\n            setReadyStateChange();\n        }\n        xhrSend.apply(xhr, arguments);\n    };\n\n}\n\nHere is CasperJS code to emit a custom event on load of XHR:\ncasper.on(""page.initialized"", function (resource) {\n    var emitXHRLoad = function (xhr) {\n        window.callPhantom({eventName: \'xhr.load\', eventData: xhr})\n    }\n    this.evaluate(addXHRListener, emitXHRLoad);\n});\n\ncasper.on(\'remote.callback\', function (data) {\n    casper.emit(data.eventName, data.eventData)\n});\n\nHere is a code to listen to ""xhr.load"" event and get the XHR response body:\ncasper.on(\'xhr.load\', function (xhr) {\n    console.log(\'xhr load\', xhr.requestUrl)\n    console.log(\'xhr load\', xhr.responseText)\n});\n\n', '\nAdditionally, you can also directly download the content and manipulate it later. \nHere is the example of the script I am using to retrieve a JSON and save it locally :\n\n\nvar casper = require(\'casper\').create({\r\n    pageSettings: {\r\n        webSecurityEnabled: false\r\n    }\r\n});\r\n\r\nvar url = \'https://twitter.com/users/username_available?username=whatever\';\r\n\r\ncasper.start(\'about:blank\', function() {\r\n   this.download(url, ""hop.json"");\r\n});\r\n\r\ncasper.run(function() {\r\n    this.echo(\'Done.\').exit();\r\n});\n\n\n']"
Scrape website with dynamic mouseover event,"
I am trying to scrape data which is generated dynamically from mouseover events. 
I want to capture the information from the Hash Rate Distribution chart from
https://slushpool.com/stats/?c=btc which is generated when you scroll over each circle.  
The code below gets the html data from the website, and returns the table which is filled once the mouse passes over a circle. However, I have not been able to figure out how to trigger the mouseover event for each circle to fill the table.
from lxml import etree
from xml.etree import ElementTree
from selenium import webdriver

driver_path = ""#Firefox web driver""
browser = webdriver.Firefox(executable_path=driver_path)
browser.get(""https://slushpool.com/stats/?c=btc"") 


page = browser.page_source #Get page html 
tree = etree.HTML(page) #create etree

table_Xpath = '/html/body/div[1]/div/div/div/div/div[5]/div[1]/div/div/div[2]/div[2]/div[2]/div/table'

table =tree.xpath(table_Xpath) #get table using Xpath

print(ElementTree.tostring(table[0])) #Returns empty table. 
#Should return data from each mouseover event

Is there a way to trigger the mouseover event for each circle, then extract the generated data.
Thank you in advance for the help!
",3k,"
            3
        ","['\nTo trigger the mouseover event for each circle you have to induce WebDriverWait for the visibility_of_all_elements_located() and you can use the following Locator Strategies:\n\nCode Block:\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.action_chains import ActionChains\n\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument(""start-maximized"")\nchrome_options.add_experimental_option(""excludeSwitches"", [""enable-automation""])\nchrome_options.add_experimental_option(\'useAutomationExtension\', False)\ndriver = webdriver.Chrome(options=chrome_options, executable_path=r\'C:\\Utility\\BrowserDrivers\\chromedriver.exe\')\ndriver.get(""https://slushpool.com/stats/?c=btc"")\ndriver.execute_script(""return arguments[0].scrollIntoView(true);"", WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, ""//h1//span[text()=\'Distribution\']""))))\nelements = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.XPATH, ""//h1//span[text()=\'Distribution\']//following::div[1]/*[name()=\'svg\']//*[name()=\'g\']//*[name()=\'g\' and @class=\'paper\']//*[name()=\'circle\']"")))\nfor element in elements:\n    ActionChains(driver).move_to_element(element).perform()\n\nBrowser Snapshot:\n\n\n', '\nThis is the circle locator you mean:\n.find_element_by_css_selector(\'._1p0PmxVw._3GzjmWLG\')\n\nBut it will change because mouseover effect, to be:\n.find_element_by_css_selector(\'._1p0PmxVw._3GzjmWLG._1suU9Mx1\')\n\n\nSo you need wait until the element to changed for each move.\nAnd the most important is how to inspect a hover element, then you can get the bellow:\n\nAnd causes the element for get data you mean to be appear:\nxpath: //div[@class=""_3jGHi0co _1zbokARu"" and contains(@style,""display: block"")]\n\nYou can use ActionChains to perform move the element.\nFinally you can try the bellow code:\nbrowser.get(\'https://slushpool.com/stats/?c=btc\')\nbrowser.maximize_window()\n\n#wait all circle\nelements = WebDriverWait(browser, 20).until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, \'._1p0PmxVw._3GzjmWLG\')))\ntable = browser.find_element_by_class_name(\'paper\')\n\n#move perform -> to table\nbrowser.execute_script(""arguments[0].scrollIntoView(true);"", table)\n\ndata = []\nfor circle in elements:\n    #move perform -> to each circle\n    ActionChains(browser).move_to_element(circle).perform()\n    # wait change mouseover effect\n    mouseover = WebDriverWait(browser, 5).until(EC.visibility_of_element_located((By.XPATH, \'//div[@class=""_3jGHi0co _1zbokARu"" and contains(@style,""display: block"")]\')))\n    data.append(mouseover.text)\n\nprint(data[0])\nprint(data)\n\nFollowing import:\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver import ActionChains\n\nConsole output:\n\nFirst data > data[0]\n536.9 Ph/s - 1.074 Eh/s\nUser Count 2\nAverage Hash Rate 546.1 Ph/s\nGroup Hash Rate 1.092 Eh/s\nAll data > data\n\n[u\'536.9 Ph/s - 1.074 Eh/s\\nUser Count 2\\nAverage Hash Rate 546.9 Ph/s\\nGroup Hash Rate 1.094 Eh/s\', u\'67.11 Ph/s - 134.2 Ph/s\\nUser Count 14\\nAverage Hash Rate 91.27 Ph/s\\nGroup Hash Rate 1.278 Eh/s\', u\'67.11 Ph/s - 134.2 Ph/s\\nUser Count 14\\nAverage Hash Rate 91.27 Ph/s\\nGroup Hash Rate 1.278 Eh/s\', u\'16.78 Ph/s - 33.55 Ph/s\\nUser Count 23\\nAverage Hash Rate 23.36 Ph/s\\nGroup Hash Rate 537.2 Ph/s\', u\'8.389 Ph/s - 16.78 Ph/s\\nUser Count 33\\nAverage Hash Rate 11.80 Ph/s\\nGroup Hash Rate 389.4 Ph/s\', u\'4.194 Ph/s - 8.389 Ph/s\\nUser Count 67\\nAverage Hash Rate 5.704 Ph/s\\nGroup Hash Rate 382.2 Ph/s\', u\'2.097 Ph/s - 4.194 Ph/s\\nUser Count 137\\nAverage Hash Rate 2.959 Ph/s\\nGroup Hash Rate 405.3 Ph/s\', u\'1.049 Ph/s - 2.097 Ph/s\\nUser Count 233\\nAverage Hash Rate 1.475 Ph/s\\nGroup Hash Rate 343.7 Ph/s\', u\'1.049 Ph/s - 2.097 Ph/s\\nUser Count 233\\nAverage Hash Rate 1.475 Ph/s\\nGroup Hash Rate 343.7 Ph/s\', u\'524.3 Th/s - 1.049 Ph/s\\nUser Count 397\\nAverage Hash Rate 731.4 Th/s\\nGroup Hash Rate 290.4 Ph/s\', u\'262.1 Th/s - 524.3 Th/s\\nUser Count 745\\nAverage Hash Rate 360.3 Th/s\\nGroup Hash Rate 268.4 Ph/s\', u\'131.1 Th/s - 262.1 Th/s\\nUser Count 1479\\nAverage Hash Rate 182.7 Th/s\\nGroup Hash Rate 270.1 Ph/s\', u\'65.54 Th/s - 131.1 Th/s\\nUser Count 2351\\nAverage Hash Rate 92.47 Th/s\\nGroup Hash Rate 217.4 Ph/s\', u\'32.77 Th/s - 65.54 Th/s\\nUser Count 3107\\nAverage Hash Rate 47.23 Th/s\\nGroup Hash Rate 146.8 Ph/s\', u\'16.38 Th/s - 32.77 Th/s\\nUser Count 3380\\nAverage Hash Rate 25.24 Th/s\\nGroup Hash Rate 85.30 Ph/s\', u\'8.192 Th/s - 16.38 Th/s\\nUser Count 4276\\nAverage Hash Rate 13.00 Th/s\\nGroup Hash Rate 55.57 Ph/s\', u\'4.096 Th/s - 8.192 Th/s\\nUser Count 540\\nAverage Hash Rate 5.953 Th/s\\nGroup Hash Rate 3.215 Ph/s\', u\'2.048 Th/s - 4.096 Th/s\\nUser Count 284\\nAverage Hash Rate 3.193 Th/s\\nGroup Hash Rate 906.8 Th/s\', u\'1.024 Th/s - 2.048 Th/s\\nUser Count 226\\nAverage Hash Rate 1.368 Th/s\\nGroup Hash Rate 309.1 Th/s\', u\'512.0 Gh/s - 1.024 Th/s\\nUser Count 136\\nAverage Hash Rate 774.4 Gh/s\\nGroup Hash Rate 105.3 Th/s\', u\'256.0 Gh/s - 512.0 Gh/s\\nUser Count 116\\nAverage Hash Rate 401.5 Gh/s\\nGroup Hash Rate 46.57 Th/s\', u\'128.0 Gh/s - 256.0 Gh/s\\nUser Count 75\\nAverage Hash Rate 186.4 Gh/s\\nGroup Hash Rate 13.98 Th/s\', u\'64.00 Gh/s - 128.0 Gh/s\\nUser Count 78\\nAverage Hash Rate 96.39 Gh/s\\nGroup Hash Rate 7.518 Th/s\', u\'32.00 Gh/s - 64.00 Gh/s\\nUser Count 70\\nAverage Hash Rate 45.68 Gh/s\\nGroup Hash Rate 3.198 Th/s\', u\'16.00 Gh/s - 32.00 Gh/s\\nUser Count 48\\nAverage Hash Rate 23.37 Gh/s\\nGroup Hash Rate 1.122 Th/s\', u\'8.000 Gh/s - 16.00 Gh/s\\nUser Count 62\\nAverage Hash Rate 11.91 Gh/s\\nGroup Hash Rate 738.5 Gh/s\', u\'4.000 Gh/s - 8.000 Gh/s\\nUser Count 153\\nAverage Hash Rate 3.078 Gh/s\\nGroup Hash Rate 471.0 Gh/s\']\n\n']"
Why does headless need to be false for Puppeteer to work?,"
I'm creating a web api that scrapes a given url and sends that back. I am using Puppeteer to do this. I asked this question: Puppeteer not behaving like in Developer Console
and recieved an answer that suggested it would only work if headless was set to be false. I don't want to be constantly opening up a browser UI i don't need (I just the need the data!) so I'm looking for why headless has to be false and can I get a fix that lets headless = true.
Here's my code:
express()
  .get(""/*"", (req, res) => {
    global.notBaseURL = req.params[0];
    (async () => {
      const browser = await puppet.launch({ headless: false }); // Line of Interest
      const page = await browser.newPage();
      console.log(req.params[0]);
      await page.goto(req.params[0], { waitUntil: ""networkidle2"" }); //this is the url
      title = await page.$eval(""title"", (el) => el.innerText);

      browser.close();

      res.send({
        title: title,
      });
    })();
  })
  .listen(PORT, () => console.log(`Listening on ${PORT}`));

This is the page I'm trying to scrape: https://www.nordstrom.com/s/zella-high-waist-studio-pocket-7-8-leggings/5460106?origin=coordinating-5460106-0-1-FTR-recbot-recently_viewed_snowplow_mvp&recs_placement=FTR&recs_strategy=recently_viewed_snowplow_mvp&recs_source=recbot&recs_page_type=category&recs_seed=0&color=BLACK
",3k,"
            2
        ","['\nThe reason it might work in UI mode but not headless is that sites who aggressively fight scraping will detect that you are running in a headless browser.\nSome possible workarounds:\nUse puppeteer-extra\nFound here: https://github.com/berstend/puppeteer-extra\nCheck out their docs for how to use it. It has a couple plugins that might help in getting past headless-mode detection:\n\npuppeteer-extra-plugin-anonymize-ua -- anonymizes your User Agent. Note that this might help with getting past headless mode detection, but as you\'ll see if you visit https://amiunique.org/ it is unlikely to be enough to keep you from being identified as a repeat visitor.\npuppeteer-extra-plugin-stealth -- this might help win the cat-and-mouse game of not being detected as headless. There are many tricks that are employed to detect headless mode, and as many tricks to evade them.\n\nRun a ""real"" Chromium instance/UI\nIt\'s possible to run a single browser UI in a manner that let\'s you attach puppeteer to that running instance. Here\'s an article that explains it: https://medium.com/@jaredpotter1/connecting-puppeteer-to-existing-chrome-window-8a10828149e0\nEssentially you\'re starting Chrome or Chromium (or Edge?) from the command line with --remote-debugging-port=9222 (or any old port?) plus other command line switches depending on what environment you\'re running it in. Then you use puppeteer to connect to that running instance instead of having it do the default behavior of launching a headless Chromium instance: const browser = await puppeteer.connect({ browserURL: ENDPOINT_URL });.  Read the puppeteer docs here for more info: https://pptr.dev/#?product=Puppeteer&version=v5.2.1&show=api-puppeteerlaunchoptions\nThe ENDPOINT_URL is displayed in the terminal when you launch the browser from the command line with the --remote-debugging-port=9222 option.\nThis option is going to require some server/ops mojo, so be prepared to do a lot more Stack Overflow searches. :-)\nThere are other strategies I\'m sure but those are the two I\'m most familiar with. Good luck!\n', '\nTodd\'s answer is thorough, but worth trying before resorting to some of the recommendations there is to slap on the following user agent line pulled from the relevant Puppeteer GitHub issue Different behavior between { headless: false } and { headless: true }:\nawait page.setUserAgent(""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36"");\nawait page.goto(yourURL);\n\nNow, the Nordstorm site provided by OP seems to be able to detect robots even with headless: false, at least at the present moment. But other sites are less strict and I\'ve found the above line to be useful on some of them as shown in Puppeteer can\'t find elements when Headless TRUE and Puppeteer , bringing back blank array.\nVisit the GH issue thread above for other ideas and see useragents.me for a rotating list of current user agents.\n']"
"How to import a table from web page (with ""div class"") to excel?","
I'm trying to import to Excel a list of exhibitors and countries from this webpage and I'm not getting it.
Can Someone help me?
I have tried the methods listed in this forum and doesn't work.
Sub test()

    Dim objIE As Object
    Dim hmtl As HTMLDocument

    Dim elements As IHTMLElementCollection

    Set objIE = New InternetExplorer
    objIE.Visible = True

    objIE.navigate ""https://sps.mesago.com/events/en/exhibitors_products/exhibitor-list.html""

    Application.StatusBar = ""Loading, Please wait...""

    While objIE.Busy
        DoEvents
    Wend
    Do
    Loop Until objIE.readyState = READYSTATE_COMPLETE

    Application.StatusBar = ""Importing data...""

    Set html = objIE.document

    'I try differents types and name - ByClassName(""...""), ByTagName(""...""), ...
    Set elements = html.getElementsByClassName(""list"") 

    For i = 0 To elements.Length - 1
         Sheet1.Range(""A"" & (i + 1)) = elements(i).innerText
    Next i

    objIE.Quit
    Set objIE = Nothing

    Application.StatusBar = """"

End Sub

Sorry about my English.
",3k,"
            0
        ","['\nYou don\'t need a browser to be opened. You can do this with XHR. The url I am using can be found in the network tab via F12 (Dev tools)\nIf you search that tab after making your request you will find that url and the response has a layout such as:\n\nimage link: https://i.stack.imgur.com/C8oLj.png\nI loop the rows and the columns to populate a 2d array (table like format) which I write out to the sheet in one go at end.\n\nVBA:\nOption Explicit\nPublic Sub GetExhibitorsInfo()\n    Dim ws As Worksheet, results(), i As Long, html As HTMLDocument\n\n    Set ws = ThisWorkbook.Worksheets(""Sheet1"")\n    Set html = New HTMLDocument\n\n    With CreateObject(""MSXML2.XMLHTTP"")\n        .Open ""GET"", ""https://sps.mesago.com/events/en/exhibitors_products/exhibitor-list.html"", False\n        .setRequestHeader ""User-Agent"", ""Mozilla/5.0""\n        .send\n        html.body.innerHTML = .responseText\n    End With\n\n    Dim rows As Object, html2 As HTMLDocument, columnsInfo As Object\n    Dim r As Long, c As Long, j As Long, headers(), columnCount As Long\n\n    headers = Array(""name2_kat"", ""art"", ""std_nr_sort"", ""kfzkz_kat"", ""halle"", _\n    ""sortierung_katalog"", ""std_nr"", ""ort_info_kat"", ""name3_kat"", ""webseite"", _\n    ""land_kat"", ""standbez1"", ""name1_kat"")\n    Set rows = html.querySelectorAll(""[data-entry]"")\n    Set html2 = New HTMLDocument\n    html2.body.innerHTML = rows.item(0).innerHTML\n    columnCount = html2.querySelectorAll(""[data-entry-key]"").length\n\n    ReDim results(1 To rows.length, 1 To columnCount)\n\n    For i = 0 To rows.length - 1\n        r = r + 1: c = 1\n        html2.body.innerHTML = rows.item(i).innerHTML\n        Set columnsInfo = html2.querySelectorAll(""[data-entry-key]"")\n        For j = 0 To columnsInfo.length - 1\n            results(r, c) = columnsInfo.item(j).innerText \'columnsInfo.item(j).getAttribute(""data-entry-key"")\n            c = c + 1\n        Next\n    Next\n    With ws\n        .Cells(1, 1).Resize(1, columnCount) = headers\n        .Cells(2, 1).Resize(UBound(results, 1), UBound(results, 2)) = results\n    End With\nEnd Sub\n\n']"
How do I prevent site scraping? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



I have a fairly large music website with a large artist database.  I've been noticing other music sites scraping our site's data (I enter dummy Artist names here and there and then do google searches for them).  
How can I prevent screen scraping?  Is it even possible?
",130k,"
            328
        ","['\nNote: Since the complete version of this answer exceeds Stack Overflow\'s length limit, you\'ll need to head  to GitHub to read the extended version, with more tips and details.\n\nIn order to hinder scraping (also known as Webscraping, Screenscraping, Web data mining, Web harvesting, or Web data extraction), it helps to know how these scrapers work, and \n, by extension, what prevents them from working well.\nThere\'s various types of scraper, and each works differently:\n\nSpiders, such as Google\'s bot or website copiers like HTtrack, which recursively follow links to other pages in order to get data. These are sometimes used for targeted scraping to get specific data, often in combination with a HTML parser to extract the desired data from each page.\nShell scripts: Sometimes, common Unix tools are used for scraping: Wget or Curl to download pages, and Grep (Regex) to extract the data.\nHTML parsers, such as ones based on Jsoup, Scrapy, and others. Similar to shell-script regex based ones, these work by extracting data from pages based on patterns in HTML, usually ignoring everything else. \nFor example: If your website has a search feature, such a scraper might submit a request for a search, and then get all the result links and their titles from the results page HTML, in order to specifically get only search result links and their titles. These are the most common.\nScreenscrapers, based on eg. Selenium or PhantomJS, which open your website in a real browser, run JavaScript, AJAX, and so on, and then get the desired text from the webpage, usually by:\n\nGetting the HTML from the browser after your page has been loaded and JavaScript has run, and then using a HTML parser to extract the desired data. These are the most common, and so many of the methods for breaking HTML parsers / scrapers also work here.\nTaking a screenshot of the rendered pages, and then using OCR to extract the desired text from the screenshot. These are rare, and only dedicated scrapers who really want your data will set this up.\n\nWebscraping services such as ScrapingHub or Kimono. In fact, there\'s people whose job is to figure out how to scrape your site and pull out the content for others to use.\nUnsurprisingly, professional scraping services are the hardest to deter, but if you make it hard and time-consuming to figure out how to scrape your site, these (and people who pay them to do so) may not be bothered to scrape your website.\nEmbedding your website in other site\'s pages with frames, and embedding your site in mobile apps. \nWhile not technically scraping, mobile apps (Android and iOS) can embed websites, and inject custom CSS and JavaScript, thus completely changing the appearance of your pages.\nHuman copy - paste: People will copy and paste your content in order to use it elsewhere.\n\nThere is a lot overlap between these different kinds of scraper, and many scrapers will behave similarly, even if they use different technologies and methods.\nThese tips mostly my own ideas, various difficulties that I\'ve encountered while writing scrapers, as well as bits of information and ideas from around the interwebs. \nHow to stop scraping\nYou can\'t completely prevent it, since whatever you do, determined scrapers can still figure out how to scrape. However, you can stop a lot of scraping by doing a few things:\nMonitor your logs & traffic patterns; limit access if you see unusual activity:\nCheck your logs regularly, and in case of unusual activity indicative of automated access (scrapers), such as many similar actions from the same IP address, you can block or limit access.\nSpecifically, some ideas:\n\nRate limiting:\nOnly allow users (and scrapers) to perform a limited number of actions in a certain time - for example, only allow a few searches per second from any specific IP address or user. This will slow down scrapers, and make them ineffective. You could also show a captcha if actions are completed too fast or faster than a real user would.\nDetect unusual activity:\nIf you see unusual activity, such as many similar requests from a specific IP address, someone looking at an excessive number of pages or performing an unusual number of searches, you can prevent access, or show a captcha for subsequent requests.\nDon\'t just monitor & rate limit by IP address - use other indicators too:\nIf you do block or rate limit, don\'t just do it on a per-IP address basis; you can use other indicators and methods to identify specific users or scrapers. Some indicators which can help you identify specific users / scrapers include:\n\nHow fast users fill out forms, and where on a button they click;\nYou can gather a lot of information with JavaScript, such as screen size / resolution, timezone, installed fonts, etc; you can use this to identify users.\nHTTP headers and their order, especially User-Agent.\n\nAs an example, if you get many request from a single IP address, all using the same User Agent, screen size (determined with JavaScript), and the user (scraper in this case) always clicks on the button in the same way and at regular intervals, it\'s probably a screen scraper; and you can temporarily block similar requests (eg. block all requests with that user agent and screen size coming from that particular IP address), and this way you won\'t inconvenience real users on that IP address, eg. in case of a shared internet connection.\nYou can also take this further, as you can identify similar requests, even if they come from different IP addresses, indicative of distributed scraping (a scraper using a botnet or a network of proxies). If you get a lot of otherwise identical requests, but they come from different IP addresses, you can block. Again, be aware of not inadvertently blocking real users.\nThis can be effective against screenscrapers which run JavaScript, as you can get a lot of information from them.\nRelated questions on Security Stack Exchange:\n\nHow to uniquely identify users with the same external IP address? for more details, and \nWhy do people use IP address bans when IP addresses often change? for info on the limits of these methods.\n\nInstead of temporarily blocking access, use a Captcha:\nThe simple way to implement rate-limiting would be to temporarily block access for a certain amount of time, however using a Captcha may be better, see the section on Captchas further down.\n\nRequire registration & login\nRequire account creation in order to view your content, if this is feasible for your site. This is a good deterrent for scrapers, but is also a good deterrent for real users.\n\nIf you require account creation and login, you can accurately track user and scraper actions. This way, you can easily detect when a specific account is being used for scraping, and ban it. Things like rate limiting or detecting abuse (such as a huge number of searches in a short time) become easier, as you can identify specific scrapers instead of just IP addresses.\n\nIn order to avoid scripts creating many accounts, you should:\n\nRequire an email address for registration, and verify that email address by sending a link that must be opened in order to activate the account. Allow only one account per email address.\nRequire a captcha to be solved during registration / account creation.\n\nRequiring account creation to view content will drive users and search engines away; if you require account creation in order to view an article, users will go elsewhere.\nBlock access from cloud hosting and scraping service IP addresses\nSometimes, scrapers will be run from web hosting services, such as Amazon Web Services or GAE, or VPSes.  Limit access to your website (or show a captcha) for requests originating from the IP addresses used by such cloud hosting services.\nSimilarly, you can also limit access from IP addresses used by proxy or VPN providers, as scrapers may use such proxy servers to avoid many requests being detected.\nBeware that by blocking access from proxy servers and VPNs, you will negatively affect real users.\nMake your error message nondescript if you do block\nIf you do block / limit access, you should ensure that you don\'t tell the scraper what caused the block, thereby giving them clues as to how to fix their scraper. So a bad idea would be to show error pages with text like:\n\nToo many requests from your IP address, please try again later.\nError, User Agent header not present !\n\nInstead, show a friendly error message that doesn\'t tell the scraper what caused it. Something like this is much better:\n\nSorry, something went wrong. You can contact support via helpdesk@example.com, should the problem persist.\n\nThis is also a lot more user friendly for real users, should they ever see such an error page. You should also consider showing a captcha for subsequent requests instead of a hard block, in case a real user sees the error message, so that you don\'t block and thus cause legitimate users to contact you.\nUse Captchas if you suspect that your website is being accessed by a scraper.\nCaptchas (""Completely Automated Test to Tell Computers and Humans apart"") are very effective against stopping scrapers. Unfortunately, they are also very effective at irritating users. \nAs such, they are useful when you suspect a possible scraper, and want to stop the scraping, without also blocking access in case it isn\'t a scraper but a real user. You might want to consider showing a captcha before allowing access to the content if you suspect a scraper.\nThings to be aware of when using Captchas:\n\nDon\'t roll your own, use something like Google\'s reCaptcha : It\'s a lot easier than implementing a captcha yourself, it\'s more user-friendly than some blurry and warped text solution you might come up with yourself (users often only need to tick a box), and it\'s also a lot harder for a scripter to solve than a simple image served from your site\nDon\'t include the solution to the captcha in the HTML markup: I\'ve actually seen one website which had the solution for the captcha in the page itself, (although quite well hidden) thus making it pretty useless. Don\'t do something like this. Again, use a service like reCaptcha, and you won\'t have this kind of problem (if you use it properly).\nCaptchas can be solved in bulk: There are captcha-solving services where actual, low-paid, humans solve captchas in bulk. Again, using reCaptcha is a good idea here, as they have protections (such as the relatively short time the user has in order to solve the captcha). This kind of service is unlikely to be used unless your data is really valuable.\n\nServe your text content as an image\nYou can render text into an image server-side, and serve that to be displayed, which will hinder simple scrapers extracting text.\nHowever, this is bad for screen readers, search engines, performance, and pretty much everything else. It\'s also illegal in some places (due to accessibility, eg. the Americans with Disabilities Act), and it\'s also easy to circumvent with some OCR, so don\'t do it. \nYou can do something similar with CSS sprites, but that suffers from the same problems.\nDon\'t expose your complete dataset:\nIf feasible, don\'t provide a way for a script / bot to get all of your dataset. As an example: You have a news site, with lots of individual articles. You could make those articles be only accessible by searching for them via the on site search, and, if you don\'t have a list of all the articles on the site and their URLs anywhere, those articles will be only accessible by using the search feature. This means that a script wanting to get all the articles off your site will have to do searches for all possible phrases which may appear in your articles in order to find them all, which will be time-consuming, horribly inefficient, and will hopefully make the scraper give up.\nThis will be ineffective if:\n\nThe bot / script does not want / need the full dataset anyway.\nYour articles are served from a URL which looks something like example.com/article.php?articleId=12345. This (and similar things) which will allow scrapers to simply iterate over all the articleIds and request all the articles that way.\nThere are other ways to eventually find all the articles, such as by writing a script to follow links within articles which lead to other articles.\nSearching for something like ""and"" or ""the"" can reveal almost everything, so that is something to be aware of. (You can avoid this by only returning the top 10 or 20 results).\nYou need search engines to find your content.\n\nDon\'t expose your APIs, endpoints, and similar things:\nMake sure you don\'t expose any APIs, even unintentionally. For example, if you are using AJAX or network requests from within Adobe Flash or Java Applets (God forbid!) to load your data it is trivial to look at the network requests from the page and figure out where those requests are going to, and then reverse engineer and use those endpoints in a scraper program. Make sure you obfuscate your endpoints and make them hard for others to use, as described.\nTo deter HTML parsers and scrapers:\nSince HTML parsers work by extracting content from pages based on identifiable patterns in the HTML, we can intentionally change those patterns in oder to break these scrapers, or even screw with them. Most of these tips also apply to other scrapers like spiders and screenscrapers too.\nFrequently change your HTML\nScrapers which process HTML directly do so by extracting contents from specific, identifiable parts of your HTML page. For example: If all pages on your website have a div with an id of article-content, which contains the text of the article, then it is trivial to write a script to visit all the article pages on your site, and extract the content text of the article-content div on each article page, and voilà, the scraper has all the articles from your site in a format that can be reused elsewhere.\nIf you change the HTML and the structure of your pages frequently, such scrapers will no longer work.\n\nYou can frequently change the id\'s and classes of elements in your HTML, perhaps even automatically. So, if your div.article-content becomes something like div.a4c36dda13eaf0, and changes every week, the scraper will work fine initially, but will break after a week. Make sure to change the length of your ids / classes too, otherwise the scraper will use div.[any-14-characters] to find the desired div instead. Beware of other similar holes too..\nIf there is no way to find the desired content from the markup, the scraper will do so from the way the HTML is structured. So, if all your article pages are similar in that every div inside a div which comes after a h1 is the article content, scrapers will get the article content based on that. Again, to break this, you can add / remove extra markup to your HTML, periodically and randomly, eg. adding extra divs or spans. With modern server side HTML processing, this should not be too hard.\n\nThings to be aware of:\n\nIt will be tedious and difficult to implement, maintain, and debug.\nYou will hinder caching. Especially if you change ids or classes of your HTML elements, this will require corresponding changes in your CSS and JavaScript files, which means that every time you change them, they will have to be re-downloaded by the browser. This will result in longer page load times for repeat visitors, and increased server load. If you only change it once a week, it will not be a big problem.\nClever scrapers will still be able to get your content by inferring where the actual content is, eg. by knowing that a large single block of text on the page is likely to be the actual article. This makes it possible to still find & extract the desired data from the page. Boilerpipe does exactly this.\n\nEssentially, make sure that it is not easy for a script to find the actual, desired content for every similar page.\nSee also How to prevent crawlers depending on XPath from getting page contents for details on how this can be implemented in PHP.\nChange your HTML based on the user\'s location\nThis is sort of similar to the previous tip. If you serve different HTML based on your user\'s location / country (determined by IP address), this may break scrapers which are delivered to users. For example, if someone is writing a mobile app which scrapes data from your site, it will work fine initially, but break when it\'s actually distributed to users, as those users may be in a different country, and thus get different HTML, which the embedded scraper was not designed to consume.\nFrequently change your HTML, actively screw with the scrapers by doing so !\nAn example: You have a search feature on your website, located at example.com/search?query=somesearchquery, which returns the following HTML:\n<div class=""search-result"">\n  <h3 class=""search-result-title"">Stack Overflow has become the world\'s most popular programming Q & A website</h3>\n  <p class=""search-result-excerpt"">The website Stack Overflow has now become the most popular programming Q & A website, with 10 million questions and many users, which...</p>\n  <a class""search-result-link"" href=""/stories/story-link"">Read more</a>\n</div>\n(And so on, lots more identically structured divs with search results)\n\nAs you may have guessed this is easy to scrape: all a scraper needs to do is hit the search URL with a query, and extract the desired data from the returned HTML. In addition to periodically changing the HTML as described above, you could also leave the old markup with the old ids and classes in, hide it with CSS, and fill it with fake data, thereby poisoning the scraper. Here\'s how the search results page could be changed:\n<div class=""the-real-search-result"">\n  <h3 class=""the-real-search-result-title"">Stack Overflow has become the world\'s most popular programming Q & A website</h3>\n  <p class=""the-real-search-result-excerpt"">The website Stack Overflow has now become the most popular programming Q & A website, with 10 million questions and many users, which...</p>\n  <a class""the-real-search-result-link"" href=""/stories/story-link"">Read more</a>\n</div>\n\n<div class=""search-result"" style=""display:none"">\n  <h3 class=""search-result-title"">Visit Example.com now, for all the latest Stack Overflow related news !</h3>\n  <p class=""search-result-excerpt"">Example.com is so awesome, visit now !</p>\n  <a class""search-result-link"" href=""http://example.com/"">Visit Now !</a>\n</div>\n(More real search results follow)\n\nThis will mean that scrapers written to extract data from the HTML based on classes or IDs will continue to seemingly work, but they will get fake data or even ads, data which real users will never see, as they\'re hidden with CSS.\nScrew with the scraper: Insert fake, invisible honeypot data into your page\nAdding on to the previous example, you can add invisible honeypot items to your HTML to catch scrapers. An example which could be added to the previously described search results page:\n<div class=""search-result"" style=""display:none"">\n  <h3 class=""search-result-title"">This search result is here to prevent scraping</h3>\n  <p class=""search-result-excerpt"">If you\'re a human and see this, please ignore it. If you\'re a scraper, please click the link below :-)\n  Note that clicking the link below will block access to this site for 24 hours.</p>\n  <a class""search-result-link"" href=""/scrapertrap/scrapertrap.php"">I\'m a scraper !</a>\n</div>\n(The actual, real, search results follow.)\n\nA scraper written to get all the search results will pick this up, just like any of the other, real search results on the page, and visit the link, looking for the desired content. A real human will never even see it in the first place (due to it being hidden with CSS), and won\'t visit the link. A genuine and desirable spider such as Google\'s will not visit the link either because you disallowed /scrapertrap/ in your robots.txt.\nYou can make your scrapertrap.php do something like block access for the IP address that visited it or force a captcha for all subsequent requests from that IP.\n\nDon\'t forget to disallow your honeypot (/scrapertrap/) in your robots.txt file so that search engine bots don\'t fall into it.\nYou can / should combine this with the previous tip of changing your HTML frequently.\nChange this frequently too, as scrapers will eventually learn to avoid it. Change the honeypot URL and text. Also want to consider changing the inline CSS used for hiding, and use an ID attribute and external CSS instead, as scrapers will learn to avoid anything which has a style attribute with CSS used to hide the content. Also try only enabling it sometimes, so the scraper works initially, but breaks after a while. This also applies to the previous tip.\nMalicious people can prevent access for real users by sharing a link to your honeypot, or even embedding that link somewhere as an image (eg. on a forum). Change the URL frequently, and make any ban times relatively short.\n\nServe fake and useless data if you detect a scraper\nIf you detect what is obviously a scraper, you can serve up fake and useless data; this will corrupt the data the scraper gets from your website. You should also make it impossible to distinguish such fake data from real data, so that scrapers don\'t know that they\'re being screwed with.\nAs an example: you have a news website; if you detect a scraper, instead of blocking access,  serve up fake, randomly generated articles, and this will poison the data the scraper gets. If you make your fake data indistinguishable from the real thing, you\'ll make it hard for scrapers to get what they want, namely the actual, real data. \nDon\'t accept requests if the User Agent is empty / missing\nOften, lazily written scrapers will not send a User Agent header with their request, whereas all  browsers as well as search engine spiders will. \nIf you get a request where the User Agent header is not present, you can show a captcha, or simply block or limit access. (Or serve fake data as described above, or something else..)\nIt\'s trivial to spoof, but as a measure against poorly written scrapers it is worth implementing.\nDon\'t accept requests if the User Agent is a common scraper one; blacklist ones used by scrapers\nIn some cases, scrapers will use a User Agent which no real browser or search engine spider uses, such as:\n\n""Mozilla"" (Just that, nothing else. I\'ve seen a few questions about scraping here, using that. A real browser will never use only that)\n""Java 1.7.43_u43"" (By default, Java\'s HttpUrlConnection uses something like this.)\n""BIZCO EasyScraping Studio 2.0""\n""wget"", ""curl"", ""libcurl"",.. (Wget and cURL are sometimes used for basic scraping)\n\nIf you find that a specific User Agent string is used by scrapers on your site, and it is not used by real browsers or legitimate spiders, you can also add it to your blacklist.\nIf it doesn\'t request assets (CSS, images), it\'s not a real browser.\nA real browser will (almost always) request and download assets such as images and CSS. HTML parsers and scrapers won\'t as they are only interested in the actual pages and their content.\nYou could log requests to your assets, and if you see lots of requests for only the HTML, it may be a scraper.\nBeware that search engine bots, ancient mobile devices, screen readers and misconfigured devices may not request assets either.\nUse and require cookies; use them to track user and scraper actions.\nYou can require cookies to be enabled in order to view your website. This will deter inexperienced and newbie scraper writers, however it is easy to for a scraper to send cookies. If you do use and require them, you can track user and scraper actions with them, and thus implement rate-limiting, blocking, or showing captchas on a per-user instead of a per-IP basis.\nFor example: when the user performs search, set a unique identifying cookie. When the results pages are viewed, verify that cookie. If the user opens all the search results (you can tell from the cookie), then it\'s probably a scraper.\nUsing cookies may be ineffective, as scrapers can send the cookies with their requests too, and discard them as needed. You will also prevent access for real users who have cookies disabled, if your site only works with cookies.\nNote that if you use JavaScript to set and retrieve the cookie, you\'ll block scrapers which don\'t run JavaScript, since they can\'t retrieve and send the cookie with their request.\nUse JavaScript + Ajax to load your content\nYou could use JavaScript + AJAX to load your content after the page itself loads. This will make the content inaccessible to HTML parsers which do not run JavaScript. This is often an effective deterrent to newbie and inexperienced programmers writing scrapers.\nBe aware of:\n\nUsing JavaScript to load the actual content will degrade user experience and performance\nSearch engines may not run JavaScript either, thus preventing them from indexing your content. This may not be a problem for search results pages, but may be for other things, such as article pages.\n\nObfuscate your markup, network requests from scripts, and everything else.\nIf you use Ajax and JavaScript to load your data, obfuscate the data which is transferred. As an example, you could encode your data on the server (with something as simple as base64 or more complex), and then decode and display it on the client, after fetching via Ajax. This will mean that someone inspecting network traffic will not immediately see how your page works and loads data, and it will be tougher for someone to directly request request data from your endpoints, as they will have to reverse-engineer your descrambling algorithm.\n\nIf you do use Ajax for loading the data, you should make it hard to use the endpoints without loading the page first, eg by requiring some session key as a parameter, which you can embed in your JavaScript or your HTML.\nYou can also embed your obfuscated data directly in the initial HTML page and use JavaScript to deobfuscate and display it, which would avoid the extra network requests. Doing this will make it significantly harder to extract the data using a HTML-only parser which does not run JavaScript, as the one writing the scraper will have to reverse engineer your JavaScript (which you should obfuscate too).\nYou might want to change your obfuscation methods regularly, to break scrapers who have figured it out.\n\nThere are several disadvantages to doing something like this, though:\n\nIt will be tedious and difficult to implement, maintain, and debug.\nIt will be ineffective against scrapers and screenscrapers which actually run JavaScript and then extract the data. (Most simple HTML parsers don\'t run JavaScript though)\nIt will make your site nonfunctional for real users if they have JavaScript disabled.\nPerformance and page-load times will suffer.\n\nNon-Technical:\n\nTell people not to scrape, and some will respect it\nFind a lawyer\nMake your data available, provide an API:\nYou could make your data easily available and require attribution and a link back to your site. Perhaps charge $$$ for it.\n\nMiscellaneous:\n\nThere are also commercial scraping protection services, such as the anti-scraping by Cloudflare or Distill Networks (Details on how it works here), which do these things, and more for you.\nFind a balance between usability for real users and scraper-proofness: Everything you do will impact user experience negatively in one way or another,  find compromises.\nDon\'t forget your mobile site and apps. If you have a mobile app, that can be screenscraped too, and network traffic can be inspected to determine the REST endpoints it uses.\nScrapers can scrape other scrapers: If there\'s one website which has content scraped from yours, other scrapers can scrape from that scraper\'s website.\n\nFurther reading:\n\nWikipedia\'s article on Web scraping. Many details on the technologies involved and the different types of web scraper.\nStopping scripters from slamming your website hundreds of times a second. Q & A on a very similar problem - bots checking a website and buying things as soon as they go on sale. A lot of relevant info, esp. on Captchas and rate-limiting.\n\n', '\nI will presume that you have set up robots.txt.\nAs others have mentioned, scrapers can fake nearly every aspect of their activities, and it is probably very difficult to identify the requests that are coming from the bad guys.\nI would consider:\n\nSet up a page, /jail.html.\nDisallow access to the page in robots.txt (so the respectful spiders will never visit).\nPlace a link on one of your pages, hiding it with CSS (display: none).\nRecord IP addresses of visitors to /jail.html.\n\nThis might help you to quickly identify requests from scrapers that are flagrantly disregarding your robots.txt.\nYou might also want to make your /jail.html a whole entire website that has the same, exact markup as normal pages, but with fake data (/jail/album/63ajdka, /jail/track/3aads8, etc.). This way, the bad scrapers won\'t be alerted to ""unusual input"" until you have the chance to block them entirely.\n', ""\nSue 'em. \nSeriously: If you have some money, talk to a good, nice, young lawyer who knows their way around the Internets. You could really be able to do something here. Depending on where the sites are based, you could have a lawyer write up a cease & desist or its equivalent in your country. You may be able to at least scare the bastards.\nDocument the insertion of your dummy values. Insert dummy values that clearly (but obscurely) point to you. I think this is common practice with phone book companies, and here in Germany, I think there have been several instances when copycats got busted through fake entries they copied 1:1.\nIt would be a shame if this would drive you into messing up your HTML code, dragging down SEO, validity and other things (even though a templating system that uses a slightly different HTML structure on each request for identical pages might already help a lot against scrapers that always rely on HTML structures and class/ID names to get the content out.)  \nCases like this are what copyright laws are good for. Ripping off other people's honest work to make money with is something that you should be able to fight against.\n"", ""\nProvide an XML API to access your data; in a manner that is simple to use. If people want your data, they'll get it, you might as well go all out.\nThis way you can provide a subset of functionality in an effective manner, ensuring that, at the very least, the scrapers won't guzzle up HTTP requests and massive amounts of bandwidth.\nThen all you have to do is convince the people who want your data to use the API. ;)\n"", ""\nThere is really nothing you can do to completely prevent this. Scrapers can fake their user agent, use multiple IP addresses, etc. and appear as a normal user. The only thing you can do is make the text not available at the time the page is loaded - make it with image, flash, or load it with JavaScript. However, the first two are bad ideas, and the last one would be an accessibility issue if JavaScript is not enabled for some of your regular users.\nIf they are absolutely slamming your site and rifling through all of your pages, you could do some kind of rate limiting.\nThere is some hope though. Scrapers rely on your site's data being in a consistent format. If you could randomize it somehow it could break their scraper. Things like changing the ID or class names of page elements on each load, etc. But that is a lot of work to do and I'm not sure if it's worth it. And even then, they could probably get around it with enough dedication.\n"", ""\nSorry, it's really quite hard to do this...\nI would suggest that you politely ask them to not use your content (if your content is copyrighted).\nIf it is and they don't take it down, then you can take furthur action and send them a cease and desist letter.\nGenerally, whatever you do to prevent scraping will probably end up with a more negative effect, e.g. accessibility, bots/spiders, etc.\n"", '\nOkay, as all posts say, if you want to make it search engine-friendly then bots can scrape for sure.\nBut you can still do a few things, and it may be affective for 60-70 % scraping bots.\nMake a checker script like below.\nIf a particular IP address is visiting very fast then after a few visits (5-10) put its IP address + browser information in a file or database.\nThe next step\n(This would be a background process and running all time or scheduled after a few minutes.) Make one another script that will keep on checking those suspicious IP addresses.\nCase 1. If the user Agent is of a known search engine like Google, Bing, Yahoo (you can find more information on user agents by googling it). Then you must see http://www.iplists.com/. This list and try to match patterns. And if it seems like a faked user-agent then ask to fill in a CAPTCHA on the next visit. (You need to research a bit more on bots IP addresses. I know this is achievable and also try whois of the IP address. It can be helpful.)\nCase 2. No user agent of a search bot: Simply ask to fill in a CAPTCHA on the next visit.\n', '\nLate answer - and also this answer probably isn\'t the one you want to hear...\nMyself already wrote many (many tens) of different specialized data-mining scrapers. (just because I like the ""open data"" philosophy).\nHere are already many advices in other answers - now i will play the devil\'s advocate role and will extend and/or correct their effectiveness.\nFirst:\n\nif someone really wants your data\nyou can\'t effectively (technically) hide your data\nif the data should be publicly accessible to your ""regular users""\n\nTrying to use some technical barriers aren\'t worth the troubles, caused:\n\nto your regular users by worsening their user-experience\nto regular and welcomed bots (search engines)\netc...\n\nPlain HMTL - the easiest way is parse the plain HTML pages, with well defined structure and css classes. E.g. it is enough to inspect element with Firebug, and use the right Xpaths, and/or CSS path in my scraper.\nYou could generate the HTML structure dynamically and also, you can generate dynamically the CSS class-names (and the CSS itself too) (e.g. by using some random class names) - but\n\nyou want to present the informations to your regular users in consistent way\ne.g. again - it is enough to analyze the page structure once more to setup the scraper.\nand it can be done automatically by analyzing some ""already known content""\n\n\nonce someone already knows (by earlier scrape), e.g.:\nwhat contains the informations about ""phil collins""\nenough display the ""phil collins"" page and (automatically) analyze how the page is structured ""today"" :)\n\n\nYou can\'t change the structure for every response, because your regular users will hate you. Also, this will cause more troubles for you (maintenance) not for the scraper. The XPath or CSS path is determinable by the scraping script automatically from the known content.\nAjax - little bit harder in the start, but many times speeds up the scraping process :) - why?\nWhen analyzing the requests and responses, i just setup my own proxy server (written in perl) and my firefox is using it. Of course, because it is my own proxy - it is completely hidden - the target server see it as regular browser. (So, no X-Forwarded-for and such headers).\nBased on the proxy logs, mostly is possible to determine the ""logic"" of the ajax requests, e.g. i could skip most of the html scraping, and just use the well-structured ajax responses (mostly in JSON format).\nSo, the ajax doesn\'t helps much...\nSome more complicated are pages which uses much packed javascript functions.\nHere is possible to use two basic methods:\n\nunpack and understand the JS and create a scraper which follows the Javascript logic (the hard way)\nor (preferably using by myself) - just using Mozilla with Mozrepl for scrape. E.g. the real scraping is done in full featured javascript enabled browser, which is programmed to clicking to the right elements and just grabbing the ""decoded"" responses directly from the browser window.\n\nSuch scraping is slow (the scraping is done as in regular browser), but it is\n\nvery easy to setup and use\nand it is nearly impossible to counter it :)\nand the ""slowness"" is needed anyway to counter the ""blocking the rapid same IP based requests""\n\nThe User-Agent based filtering doesn\'t helps at all. Any serious data-miner will set it to some correct one in his scraper.\nRequire Login - doesn\'t helps. The simplest way beat it (without any analyze and/or scripting the login-protocol) is just logging into the site as regular user, using Mozilla and after just run the Mozrepl based scraper...\nRemember, the require login helps for anonymous bots, but doesn\'t helps against someone who want scrape your data. He just register himself to your site as regular user.\nUsing frames isn\'t very effective also. This is used by many live movie services and it not very hard to beat. The frames are simply another one HTML/Javascript pages what are needed to analyze... If the data worth the troubles - the data-miner will do the required analyze.\nIP-based limiting isn\'t effective at all - here are too many public proxy servers and also here is the TOR... :) It doesn\'t slows down the scraping (for someone who really wants your data).\nVery hard is scrape data hidden in images. (e.g. simply converting the data into images server-side). Employing ""tesseract"" (OCR) helps many times - but honestly - the data must worth the troubles for the scraper. (which many times doesn\'t worth).\nOn the other side, your users will hate you for this. Myself, (even when not scraping) hate websites which doesn\'t allows copy the page content into the clipboard (because the information are in the images, or (the silly ones) trying to bond to the right click some custom Javascript event. :)\nThe hardest are the sites which using java applets or flash, and the applet uses secure https requests itself internally. But think twice - how happy will be your iPhone users... ;). Therefore, currently very few sites using them. Myself, blocking all flash content in my browser (in regular browsing sessions) - and never using sites which depends on Flash.\nYour milestones could be..., so you can try this method - just remember - you will probably loose some of your users. Also remember, some SWF files are decompilable. ;)\nCaptcha (the good ones - like reCaptcha) helps a lot - but your users will hate you... - just imagine, how your users will love you when they need solve some captchas in all pages showing informations about the music artists.\nProbably don\'t need to continue - you already got into the picture.\nNow what you should do:\nRemember: It is nearly impossible to hide your data, if you on the other side want publish them (in friendly way) to your regular users.\nSo,\n\nmake your data easily accessible - by some API\n\n\nthis allows the easy data access\ne.g. offload your server from scraping - good for you\n\nsetup the right usage rights (e.g. for example must cite the source)\nremember, many data isn\'t copyright-able - and hard to protect them\nadd some fake data (as you already done) and use legal tools\n\n\nas others already said, send an ""cease and desist letter""\nother legal actions (sue and like) probably is too costly and hard to win (especially against non US sites)\n\n\nThink twice before you will try to use some technical barriers.\nRather as trying block the data-miners, just add more efforts to your website usability. Your user will love you. The time (&energy) invested into technical barriers usually aren\'t worth - better to spend the time to make even better website...\nAlso, data-thieves aren\'t like normal thieves.\nIf you buy an inexpensive home alarm and add an warning ""this house is connected to the police"" - many thieves will not even try to break into. Because one wrong move by him - and he going to jail...\nSo, you investing only few bucks, but the thief investing and risk much.\nBut the data-thief hasn\'t such risks. just the opposite - ff you make one wrong move (e.g. if you introduce some BUG as a result of technical barriers), you will loose your users. If the the scraping bot will not work for the first time, nothing happens - the data-miner just will try another approach and/or will debug the script.\nIn this case, you need invest much more - and the scraper investing much less.\nJust think where you want invest your time & energy...\nPs: english isn\'t my native - so forgive my broken english...\n', '\nThings that might work against beginner scrapers:\n\nIP blocking\nuse lots of ajax\ncheck referer request header\nrequire login\n\nThings that will help in general:\n\nchange your layout every week\nrobots.txt\n\nThings that will help but will make your users hate you:\n\ncaptcha\n\n', ""\nI have done a lot of web scraping and summarized some techniques to stop web scrapers  on my blog based on what I find annoying.\nIt is a tradeoff between your users and scrapers. If you limit IP's, use CAPTCHA's, require login, etc, you make like difficult for the scrapers. But this may also drive away your genuine users.\n"", ""\nFrom a tech perspective: \nJust model what Google does when you hit them with too many queries at once. That should put a halt to a lot of it.\nFrom a legal perspective:\nIt sounds like the data you're publishing is not proprietary. Meaning you're publishing names and stats and other information that cannot be copyrighted. \nIf this is the case, the scrapers are not violating copyright by redistributing your information about artist name etc. However, they may be violating copyright when they load your site into memory because your site contains elements that are copyrightable (like layout etc).\nI recommend reading about Facebook v. Power.com and seeing the arguments Facebook used to stop screen scraping. There are many legal ways you can go about trying to stop someone from scraping your website. They can be far reaching and imaginative. Sometimes the courts buy the arguments. Sometimes they don't. \nBut, assuming you're publishing public domain information that's not copyrightable like names and basic stats... you should just let it go in the name of free speech and open data. That is, what the web's all about.\n"", ""\nYour best option is unfortunately fairly manual: Look for traffic patterns that you believe are indicative of scraping and ban their IP addresses.\nSince you're talking about a public site then making the site search-engine friendly will also make the site scraping-friendly. If a search-engine can crawl and scrape your site then an malicious scraper can as well. It's a fine-line to walk.\n"", ""\nSure it's possible. For 100% success, take your site offline.\nIn reality you can do some things that make scraping a little more difficult. Google does browser checks to make sure you're not a robot scraping search results (although this, like most everything else, can be spoofed).\nYou can do things like require several seconds between the first connection to your site, and subsequent clicks. I'm not sure what the ideal time would be or exactly how to do it, but that's another idea.\nI'm sure there are several other people who have a lot more experience, but I hope those ideas are at least somewhat helpful.\n"", ""\n\nNo, it's not possible to stop (in any way)\nEmbrace it. Why not publish as RDFa and become super search engine friendly and encourage the re-use of data? People will thank you and provide credit where due (see musicbrainz as an example).\n\nIt is not the answer you probably want, but why hide what you're trying to make public?\n"", ""\nThere are a few things you can do to try and prevent screen scraping.  Some are not very effective, while others (a CAPTCHA) are, but hinder usability.  You have to keep in mind too that it may hinder legitimate site scrapers, such as search engine indexes.\nHowever, I assume that if you don't want it scraped that means you don't want search engines to index it either.\nHere are some things you can try:\n\nShow the text in an image.  This is quite reliable, and is less of a pain on the user than a CAPTCHA, but means they won't be able to cut and paste and it won't scale prettily or be accessible.\nUse a CAPTCHA and require it to be completed before returning the page.  This is a reliable method, but also the biggest pain to impose on a user.\nRequire the user to sign up for an account before viewing the pages, and confirm their email address.  This will be pretty effective, but not totally - a screen-scraper might set up an account and might cleverly program their script to log in for them.\nIf the client's user-agent string is empty, block access.  A site-scraping script will often be lazily programmed and won't set a user-agent string, whereas all web browsers will.\nYou can set up a black list of known screen scraper user-agent strings as you discover them.  Again, this will only help the lazily-coded ones; a programmer who knows what he's doing can set a user-agent string to impersonate a web browser.\nChange the URL path often.  When you change it, make sure the old one keeps working, but only for as long as one user is likely to have their browser open.  Make it hard to predict what the new URL path will be.  This will make it difficult for scripts to grab it if their URL is hard-coded.  It'd be best to do this with some kind of script.\n\nIf I had to do this, I'd probably use a combination of the last three, because they minimise the inconvenience to legitimate users.  However, you'd have to accept that you won't be able to block everyone this way and once someone figures out how to get around it, they'll be able to scrape it forever.  You could then just try to block their IP addresses as you discover them I guess.\n"", '\nMethod One (Small Sites Only):\nServe encrypted / encoded data.I Scape the web using python (urllib, requests, beautifulSoup etc...) and found many websites that serve encrypted / encoded data that is not decrypt-able in any programming language simply because the encryption method does not exist.\nI achieved this in a PHP website by encrypting and minimizing the output (WARNING: this is not a good idea for large sites) the response was always jumbled content.\nExample of minimizing output in PHP (How to minify php page html output?):\n<?php\n  function sanitize_output($buffer) {\n    $search = array(\n      \'/\\>[^\\S ]+/s\', // strip whitespaces after tags, except space\n      \'/[^\\S ]+\\</s\', // strip whitespaces before tags, except space\n      \'/(\\s)+/s\'      // shorten multiple whitespace sequences\n    );\n    $replace = array(\'>\', \'<\', \'\\\\1\');\n    $buffer = preg_replace($search, $replace, $buffer);\n    return $buffer;\n  }\n  ob_start(""sanitize_output"");\n?>\n\nMethod Two:\nIf you can\'t stop them screw them over serve fake / useless data as a response.\nMethod Three:\nblock common scraping user agents, you\'ll see this in major / large websites as it is impossible to scrape them with ""python3.4"" as you User-Agent.\nMethod Four:\nMake sure all the user headers are valid, I sometimes provide as many headers as possible to make my scraper seem like an authentic user, some of them are not even true or valid like en-FU :).\nHere is a list of some of the headers I commonly provide.\nheaders = {\n  ""Requested-URI"": ""/example"",\n  ""Request-Method"": ""GET"",\n  ""Remote-IP-Address"": ""656.787.909.121"",\n  ""Remote-IP-Port"": ""69696"",\n  ""Protocol-version"": ""HTTP/1.1"",\n  ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"",\n  ""Accept-Encoding"": ""gzip,deflate"",\n  ""Accept-Language"": ""en-FU,en;q=0.8"",\n  ""Cache-Control"": ""max-age=0"",\n  ""Connection"": ""keep-alive"",\n  ""Dnt"": ""1"",  \n  ""Host"": ""http://example.com"",\n  ""Referer"": ""http://example.com"",\n  ""Upgrade-Insecure-Requests"": ""1"",\n  ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36""\n}\n\n', '\nQuick approach to this would be to set a booby/bot trap.\n\nMake a page that if it\'s opened a certain amount of times or even opened at all, will collect certain information like the IP and whatnot (you can also consider irregularities or patterns but this page shouldn\'t have to be opened at all). \nMake a link to this in your page that is hidden with CSS display:none; or left:-9999px; positon:absolute; try to place it in places that are less unlikely to be ignored like where your content falls under and not your footer as sometimes bots can choose to forget about certain parts of a page. \nIn your robots.txt file set a whole bunch of disallow rules to pages you don\'t want friendly bots (LOL, like they have happy faces!) to gather information on and set this page as one of them. \nNow, If a friendly bot comes through it should ignore that page. Right but that still isn\'t good enough. Make a couple more of these pages or somehow re-route a page to accept differnt names. and then place more disallow rules to these trap pages in your robots.txt file alongside pages you want ignored.\nCollect the IP of these bots or anyone that enters into these pages, don\'t ban them but make a function to display noodled text in your content like random numbers, copyright notices, specific text strings, display scary pictures, basically anything to hinder your good content. You can also set links that point to a page which will take forever to load ie. in php you can use the sleep() function. This will fight the crawler back if it has some sort of detection to bypass pages that take way too long to load as some well written bots are set to process X amount of links at a time.  \nIf you have made specific text strings/sentences why not go to your favorite search engine and search for them, it might show you where your content is ending up.\n\nAnyway, if you think tactically and creatively this could be a good starting point. The best thing to do would be to learn how a bot works.\nI\'d also think about scambling some ID\'s or the way attributes on the page element are displayed: \n<a class=""someclass"" href=""../xyz/abc"" rel=""nofollow"" title=""sometitle""> \n\nthat changes its form every time as some bots might be set to be looking for specific patterns in your pages or targeted elements. \n<a title=""sometitle"" href=""../xyz/abc"" rel=""nofollow"" class=""someclass""> \n\nid=""p-12802"" > id=""p-00392""\n\n', ""\nYou can't stop normal screen scraping. For better or worse, it's the nature of the web.\nYou can make it so no one can access certain things (including music files) unless they're logged in as a registered user. It's not too difficult to do in Apache. I assume it wouldn't be too difficult to do in IIS as well.\n"", ""\nRather than blacklisting bots, maybe you should whitelist them.  If you don't want to kill your search results for the top few engines, you can whitelist their user-agent strings, which are generally well-publicized.  The less ethical bots tend to forge user-agent strings of popular web browsers.  The top few search engines should be driving upwards of 95% of your traffic.\nIdentifying the bots themselves should be fairly straightforward, using the techniques other posters have suggested.\n"", ""\nMost have been already said, but have you considered the CloudFlare protection? I mean this:\n\nOther companies probably do this too, CloudFlare is the only one I know.\nI'm pretty sure that would complicate their work. I also once got IP banned automatically for 4 months when I tried to scrap data of a site protected by CloudFlare due to rate limit (I used simple AJAX request loop).\n"", '\nOne way would be to serve the content as XML attributes, URL encoded strings, preformatted text with HTML encoded JSON, or data URIs, then transform it to HTML on the client. Here are a few sites which do this:\n\nSkechers: XML\n<document \n filename="""" \n height="""" \n width="""" \n title=""SKECHERS"" \n linkType="""" \n linkUrl="""" \n imageMap="""" \n href=&quot;http://www.bobsfromskechers.com&quot; \n alt=&quot;BOBS from Skechers&quot; \n title=&quot;BOBS from Skechers&quot; \n/>\n\nChrome Web Store: JSON\n<script type=""text/javascript"" src=""https://apis.google.com/js/plusone.js"">{""lang"": ""en"", ""parsetags"": ""explicit""}</script>\n\nBing News: data URL\n<script type=""text/javascript"">\n  //<![CDATA[\n  (function()\n    {\n    var x;x=_ge(\'emb7\');\n    if(x)\n      {\n      x.src=\'data:image/jpeg;base64,/*...*/\';\n      } \n    }() )\n\nProtopage: URL Encoded Strings\nunescape(\'Rolling%20Stone%20%3a%20Rock%20and%20Roll%20Daily\')\n\nTiddlyWiki : HTML Entities + preformatted JSON\n   <pre>\n   {&quot;tiddlers&quot;: \n    {\n    &quot;GettingStarted&quot;: \n      {\n      &quot;title&quot;: &quot;GettingStarted&quot;,\n      &quot;text&quot;: &quot;Welcome to TiddlyWiki,\n      }\n    }\n   }\n   </pre>\n\nAmazon: Lazy Loading\namzn.copilot.jQuery=i;amzn.copilot.jQuery(document).ready(function(){d(b);f(c,function() {amzn.copilot.setup({serviceEndPoint:h.vipUrl,isContinuedSession:true})})})},f=function(i,h){var j=document.createElement(""script"");j.type=""text/javascript"";j.src=i;j.async=true;j.onload=h;a.appendChild(j)},d=function(h){var i=document.createElement(""link"");i.type=""text/css"";i.rel=""stylesheet"";i.href=h;a.appendChild(i)}})();\namzn.copilot.checkCoPilotSession({jsUrl : \'http://z-ecx.images-amazon.com/images/G/01/browser-scripts/cs-copilot-customer-js/cs-copilot-customer-js-min-1875890922._V1_.js\', cssUrl : \'http://z-ecx.images-amazon.com/images/G/01/browser-scripts/cs-copilot-customer-css/cs-copilot-customer-css-min-2367001420._V1_.css\', vipUrl : \'https://copilot.amazon.com\'\n\nXMLCalabash: Namespaced XML + Custom MIME type + Custom File extension\n   <p:declare-step type=""pxp:zip"">\n        <p:input port=""source"" sequence=""true"" primary=""true""/>\n        <p:input port=""manifest""/>\n        <p:output port=""result""/>\n        <p:option name=""href"" required=""true"" cx:type=""xsd:anyURI""/>\n        <p:option name=""compression-method"" cx:type=""stored|deflated""/>\n        <p:option name=""compression-level"" cx:type=""smallest|fastest|default|huffman|none""/>\n        <p:option name=""command"" select=""\'update\'"" cx:type=""update|freshen|create|delete""/>\n   </p:declare-step>\n\n\nIf you view source on any of the above, you see that scraping will simply return metadata and navigation.\n', ""\nI agree with most of the posts above, and I'd like to add that the more search engine friendly your site is, the more scrape-able it would be. You could try do a couple of things that are very out there that make it harder for scrapers, but it might also affect your search-ability... It depends on how well you want your site to rank on search engines of course.\n"", '\nPutting your content behind a captcha would mean that robots would find it difficult to access your content.  However, humans would be inconvenienced so that may be undesirable.\n', '\nIf you want to see a great example, check out http://www.bkstr.com/.  They use a j/s algorithm to set a cookie, then reloads the page so it can use the cookie to validate that the request is being run within a browser.  A desktop app built to scrape could definitely get by this, but it would stop most cURL type scraping.\n', ""\nScreen scrapers work by processing HTML. And if they are determined to get your data there is not much you can do technically because the human eyeball processes anything. Legally it's already been pointed out you may have some recourse though and that would be my recommendation.\nHowever, you can hide the critical part of your data by using non-HTML-based presentation logic\n\nGenerate a Flash file for each artist/album, etc.\nGenerate an image for each artist content. Maybe just an image for the artist name, etc. would be enough. Do this by rendering the text onto a JPEG/PNG file on the server and linking to that image.\n\nBear in mind that this would probably affect your search rankings.\n"", '\nGenerate the HTML, CSS and JavaScript. It is easier to write generators than parsers, so you could generate each served page differently. You can no longer use a cache or static content then.\n']"
BeautifulSoup webscraping find_all( ): finding exact match,"
I'm using Python and BeautifulSoup for web scraping.
Lets say I have the following html code to scrape:
<body>
    <div class=""product"">Product 1</div>
    <div class=""product"">Product 2</div>
    <div class=""product special"">Product 3</div>
    <div class=""product special"">Product 4</div>
</body>

Using BeautifulSoup, I want to find ONLY the products with the attribute class=""product""
(only Product 1 and 2), not the 'special' products
If I do the following:
result = soup.find_all('div', {'class': 'product'})

the result includes ALL the products (1,2,3, and 4).
What should I do to find products whose class EXACTLY matches 'product'??

The Code I ran:
from bs4 import BeautifulSoup
import re

text = """"""
<body>
    <div class=""product"">Product 1</div>
    <div class=""product"">Product 2</div>
    <div class=""product special"">Product 3</div>
    <div class=""product special"">Product 4</div>
</body>""""""

soup = BeautifulSoup(text)
result = soup.findAll(attrs={'class': re.compile(r""^product$"")})
print result

Output:
[<div class=""product"">Product 1</div>, <div class=""product"">Product 2</div>, <div class=""product special"">Product 3</div>, <div class=""product special"">Product 4</div>]

",109k,"
            40
        ","['\nIn BeautifulSoup 4, the class attribute (and several other attributes, such as accesskey and the headers attribute on table cell elements) is treated as a set; you match against individual elements listed in the attribute. This follows the HTML standard.\nAs such, you cannot limit the search to just one class.\nYou\'ll have to use a custom function here to match against the class instead:\nresult = soup.find_all(lambda tag: tag.name == \'div\' and \n                                   tag.get(\'class\') == [\'product\'])\n\nI used a lambda to create an anonymous function; each tag is matched on name (must be \'div\'), and the class attribute must be exactly equal to the list [\'product\']; e.g. have just the one value.\nDemo:\n>>> from bs4 import BeautifulSoup\n>>> text = """"""\n... <body>\n...     <div class=""product"">Product 1</div>\n...     <div class=""product"">Product 2</div>\n...     <div class=""product special"">Product 3</div>\n...     <div class=""product special"">Product 4</div>\n... </body>""""""\n>>> soup = BeautifulSoup(text)\n>>> soup.find_all(lambda tag: tag.name == \'div\' and tag.get(\'class\') == [\'product\'])\n[<div class=""product"">Product 1</div>, <div class=""product"">Product 2</div>]\n\nFor completeness sake, here are all such set attributes, from the BeautifulSoup source code:\n# The HTML standard defines these attributes as containing a\n# space-separated list of values, not a single value. That is,\n# class=""foo bar"" means that the \'class\' attribute has two values,\n# \'foo\' and \'bar\', not the single value \'foo bar\'.  When we\n# encounter one of these attributes, we will parse its value into\n# a list of values if possible. Upon output, the list will be\n# converted back into a string.\ncdata_list_attributes = {\n    ""*"" : [\'class\', \'accesskey\', \'dropzone\'],\n    ""a"" : [\'rel\', \'rev\'],\n    ""link"" :  [\'rel\', \'rev\'],\n    ""td"" : [""headers""],\n    ""th"" : [""headers""],\n    ""td"" : [""headers""],\n    ""form"" : [""accept-charset""],\n    ""object"" : [""archive""],\n\n    # These are HTML5 specific, as are *.accesskey and *.dropzone above.\n    ""area"" : [""rel""],\n    ""icon"" : [""sizes""],\n    ""iframe"" : [""sandbox""],\n    ""output"" : [""for""],\n    }\n\n', ""\nYou can use CSS selectors like so:\nresult = soup.select('div.product.special')\n\ncss-selectors\n"", '\nsoup.findAll(attrs={\'class\': re.compile(r""^product$"")})\n\nThis code matches anything that doesn\'t have the product at the end of its class.\n', '\nYou could solve this problem and capture just Product 1 and Product 2 with gazpacho by enforcing exact matching:\nfrom gazpacho import Soup\n\nhtml = """"""\\\n<body>\n    <div class=""product"">Product 1</div>\n    <div class=""product"">Product 2</div>\n    <div class=""product special"">Product 3</div>\n    <div class=""product special"">Product 4</div>\n</body>\n""""""\n\nsoup = Soup(html)\ndivs = soup.find(""div"", {""class"": ""product""}, partial=False)\n[div.text for div in divs]\n\nOutputs exactly:\n[\'Product 1\', \'Product 2\']\n\n', '\nchange your code from\nresult = soup.findAll(attrs={\'class\': re.compile(r""^product$"")})\n\nto\nresult = soup.find_all(attrs={\'class\': \'product\'})\n\nand the result is a list and access through index\n']"
Periodically refresh IMPORTXML() spreadsheet function,"
I have a large sheet with around 30 importxml functions that obtain data from a website that updates usually twice a day.
I would like to run the importxml function on a timely basis (every 8 hours) for my Google Spreadsheet to save the data in another sheet. The saving already works, however the updating does not!
I read in Google Spreadsheet row update that it might run every 2 hours, however I do not believe that this is true, because since I added it to my sheet nothing has changed or updated, when the spreadsheet is NOT opened.
How can I ""trigger"" the importxml function in my Google Spreadsheet in an easy way, as I have a lot of importxml functions in it?
",56k,"
            17
        ","['\nI made a couple of adjustments to Mogsdad\'s answer:\n\nFixed the releaseLock() call placement\nUpdates (or adds) a querystring parameter to the url in the import function (as opposed to storing, removing, waiting 5 seconds, and then restoring all relevant formulas)\nWorks on a specific sheet in your spreadsheet\nShows time of last update\n\n...\nfunction RefreshImports() {\n  var lock = LockService.getScriptLock();\n  if (!lock.tryLock(5000)) return;             // Wait up to 5s for previous refresh to end.\n\n  var id = ""[YOUR SPREADSHEET ID]"";\n  var ss = SpreadsheetApp.openById(id);\n  var sheet = ss.getSheetByName(""[SHEET NAME]"");\n  var dataRange = sheet.getDataRange();\n  var formulas = dataRange.getFormulas();\n  var content = """";\n  var now = new Date();\n  var time = now.getTime();\n  var re = /.*[^a-z0-9]import(?:xml|data|feed|html|range)\\(.*/gi;\n  var re2 = /((\\?|&)(update=[0-9]*))/gi;\n  var re3 = /("",)/gi;\n\n  for (var row=0; row<formulas.length; row++) {\n    for (var col=0; col<formulas[0].length; col++) {\n      content = formulas[row][col];\n      if (content != """") {\n        var match = content.search(re);\n        if (match !== -1 ) {\n          // import function is used in this cell\n          var updatedContent = content.toString().replace(re2,""$2update="" + time);\n          if (updatedContent == content) {\n            // No querystring exists yet in url\n            updatedContent = content.toString().replace(re3,""?update="" + time + ""$1"");\n          }\n          // Update url in formula with querystring param\n          sheet.getRange(row+1, col+1).setFormula(updatedContent);\n        }\n      }\n    }\n  }\n\n  // Done refresh; release the lock.\n  lock.releaseLock();\n\n  // Show last updated time on sheet somewhere\n  sheet.getRange(7,2).setValue(""Rates were last updated at "" + now.toLocaleTimeString())\n}\n\n', '\nThe Google Spreadsheet row update question and its answers refer to the ""Old Sheets"", which had different behaviour than the 2015 version of Google Sheets does. There is no automatic refresh of content with ""New Sheets""; changes are only evaluated now in response to edits.\nWhile Sheets no longer provides this capability natively, we can use a script to refresh the ""import"" formulas (IMPORTXML, IMPORTDATA, IMPORTHTML and IMPORTANGE).\nUtility script\nFor periodic refresh of IMPORT formulas, set this function up as a time-driven trigger.\nCaveats:\n\nImport function Formula changes made to the spreadsheet by other scripts or users  during the refresh period COULD BE OVERWRITTEN.\nOverlapping refreshes might make your spreadsheet unstable. To mitigate that, the utility script uses a ScriptLock. This may conflict with other uses of that lock in your script.\n\n\xa0\n/**\n * Go through all sheets in a spreadsheet, identify and remove all spreadsheet\n * import functions, then replace them a while later. This causes a ""refresh""\n * of the ""import"" functions. For periodic refresh of these formulas, set this\n * function up as a time-based trigger.\n *\n * Caution: Formula changes made to the spreadsheet by other scripts or users\n * during the refresh period COULD BE OVERWRITTEN.\n *\n * From: https://stackoverflow.com/a/33875957/1677912\n */\nfunction RefreshImports() {\n  var lock = LockService.getScriptLock();\n  if (!lock.tryLock(5000)) return;             // Wait up to 5s for previous refresh to end.\n  // At this point, we are holding the lock.\n\n  var id = ""YOUR-SHEET-ID"";\n  var ss = SpreadsheetApp.openById(id);\n  var sheets = ss.getSheets();\n\n  for (var sheetNum=0; sheetNum<sheets.length; sheetNum++) {\n    var sheet = sheets[sheetNum];\n    var dataRange = sheet.getDataRange();\n    var formulas = dataRange.getFormulas();\n    var tempFormulas = [];\n    for (var row=0; row<formulas.length; row++) {\n      for (col=0; col<formulas[0].length; col++) {\n        // Blank all formulas containing any ""import"" function\n        // See https://regex101.com/r/bE7fJ6/2\n        var re = /.*[^a-z0-9]import(?:xml|data|feed|html|range)\\(.*/gi;\n        if (formulas[row][col].search(re) !== -1 ) {\n          tempFormulas.push({row:row+1,\n                             col:col+1,\n                             formula:formulas[row][col]});\n          sheet.getRange(row+1, col+1).setFormula("""");\n        }\n      }\n    }\n\n    // After a pause, replace the import functions\n    Utilities.sleep(5000);\n    for (var i=0; i<tempFormulas.length; i++) {\n      var cell = tempFormulas[i];\n      sheet.getRange( cell.row, cell.col ).setFormula(cell.formula)\n    }\n\n    // Done refresh; release the lock.\n    lock.releaseLock();\n  }\n}\n\n', '\nTo answer your question for an easy ""trigger"" to force the function to reload:\nadd an additional not used parameter to the url you are loading, while referencing a cell for the value of that parameter.\nOnce you alter the content of that cell, the function reloads.\nexample:\nimportxml(""http://www.example.com/?noop="" & $A$1,""..."")\n\nunfortunately you cannot put a date calculating function into the referenced cell, that throws an error that this is not allowed.\n', '\nYou can also put each XML formula as a comment in the respective cells and record a macro to copy and paste it in the same cell. Later use the Scripts and then the Trigger functionality to schedule this macro.\n\n\n\n']"
'list' object has no attribute 'get_attribute' while iterating through WebElements,"
I'm trying to use Python and Selenium to scrape multiple links on a web page. I'm using find_elements_by_xpath and I'm able to locate a list of elements but I'm having trouble changing the list that is returned to the actual href links. I know find_element_by_xpath works, but that only works for one element.
Here is my code:
path_to_chromedriver = 'path to chromedriver location'
browser = webdriver.Chrome(executable_path = path_to_chromedriver)

browser.get(""file:///path to html file"")

all_trails = []

#finds all elements with the class 'text-truncate trail-name' then 
#retrieve the a element
#this seems to be just giving us the element location but not the 
#actual location

find_href = browser.find_elements_by_xpath('//div[@class=""text truncate trail-name""]/a[1]')
all_trails.append(find_href)

print all_trails

This code is returning:
<selenium.webdriver.remote.webelement.WebElement 
(session=""dd178d79c66b747696c5d3750ea8cb17"", 
element=""0.5700549730549636-1663"")>, 
<selenium.webdriver.remote.webelement.WebElement 
(session=""dd178d79c66b747696c5d3750ea8cb17"", 
element=""0.5700549730549636-1664"")>,

I expect the all_trails array to be a list of links like: www.google.com, www.yahoo.com, www.bing.com.
I've tried looping through the all_trails list and running the get_attribute('href') method on the list but I get the error:

Does anyone have any idea how to convert the selenium WebElement's to href links?
Any help would be greatly appreciated :)
",16k,"
            4
        ","['\nLet us see what\'s happening in your code :\nWithout any visibility to the concerned HTML it seems the following line returns two WebElements in to the List find_href which are inturn are appended to the all_trails List :\nfind_href = browser.find_elements_by_xpath(\'//div[@class=""text truncate trail-name""]/a[1]\')\n\nHence when we print the List all_trails both the WebElements are printed. Hence No Error.\nAs per the error snap shot you have provided, you are trying to invoke get_attribute(""href"") method over a List which is Not Supported. Hence you see the error :\n\'List\' Object has no attribute \'get_attribute\'\n\nSolution :\nTo get the href attribute, we have to iterate over the List as follows :\nfind_href = browser.find_elements_by_xpath(\'//your_xpath\')\nfor my_href in find_href:\n    print(my_href.get_attribute(""href""))\n\n', '\nIf you have the following HTML:\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 1</a>\n</div>\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 2</a>\n</div>\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 3</a>\n</div>\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 4</a>\n</div>\n\nYour code should look like:\nall_trails = []\n\nall_links = browser.find_elements_by_css_selector("".text-truncate.trail-name>a"")\n\nfor link in all_links:\n\n    all_trails.append(link.get_attribute(""href""))\n\nWhere all_trails -- is a list of links (Link 1, Link 2 and so on).\nHope it helps you!\n', '\nUse it in Singular form as find_element_by_css_selector instead of using find_elements_by_css_selector as it returns many webElements in List. So you need to loop through each webElement to use Attribute.\n', '\nfind_href = browser.find_elements_by_xpath(\'//div[@class=""text truncate trail-name""]/a[1]\')\nfor i in find_href:\n      all_trails.append(i.get_attribute(\'href\'))\n\nget_attribute works on elements of that list, not list itself.\n', '\nget_attribute works on elements of that list only, not list itself. For eg :-\ndef fetch_img_urls(search_query: str):\n    driver.get(\'https://images.google.com/\')\n    search = driver.find_element(By.CLASS_NAME, ""gLFyf.gsfi"")\n    search.send_keys(search_query)\n    search.send_keys(Keys.RETURN)\n    links=[]\n    try:\n        time.sleep(5)\n        urls = driver.find_elements(By.CSS_SELECTOR,\'a.VFACy.kGQAp.sMi44c.lNHeqe.WGvvNb\')\n        for url in urls:\n            #print(url.get_attribute(""href""))\n            links.append(url.get_attribute(""href""))\n            print(links)\n\n    except Exception as e:\n        print(f\'error{e}\')\n        driver.quit()\n\n']"
How would I import YouTube Likes and Dislikes and a ratio from YouTube onto Google Sheets? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 3 years ago.


This post was edited and submitted for review 4 months ago and failed to reopen the post:

Original close reason(s) were not resolved






                        Improve this question
                    



What would be the correct xpath to get  YouTube likes and dislikes from a video?
",3k,"
            -3
        ","['\nTITLE:\n=IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""//*[@id=\'eow-title\']"")\n\nor:\n=REGEXEXTRACT(QUERY(ARRAY_CONSTRAIN(IMPORTDATA(A12), 500, 1),\n ""where Col1 contains \'/title\'"", 0), "">(.+)<"")\n\n\nVIEWS:\n=VALUE(REGEXREPLACE(TEXT(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",\n ""//*[contains(@class, \'watch-view-count\')]""),0),"" view(s)?"",""""))\n\nDURATION:\n=SUBSTITUTE(REGEXREPLACE(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""//*[@itemprop=\'duration\']/@content""),""PT|S"",""""),""M"","":"")\n\nLIKES:\n=IF(ISNA(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-like-button\')])[1]""))=TRUE,0,\n         IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-like-button\')])[1]""))\n\nDISLIKES:\n=IF(ISNA(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-dislike-button\')])[1]""))=TRUE,0,\n         IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-dislike-button\')])[1]""))\n\nUPLOADED:\n=REGEXREPLACE(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",\n ""//*[contains(@class, \'watch-time-text\')]""),""((Uploaded)|(Published)|(Streamed live)) on "","""")\n\nSUBSCRIPTIONS:\n=IFERROR(MID(QUERY(IMPORTXML(""https://www.youtube.com/channel/""&A1,\n ""//div[@class=\'primary-header-actions\']""), ""select Col1""), 31, 20), )\n\n\nCHANNEL NAME:\n=INDEX(IMPORTHTML(""https://www.youtube.com/channel/UC7_gcs09iThXybpVgjHZ_7g"",""list"",1),1,1)\n\nCHANNEL ID:\n=ARRAYFORMULA(REGEXREPLACE(QUERY(SUBSTITUTE(ARRAY_CONSTRAIN(\n IMPORTDATA(https://www.youtube.com/watch?v=rckrnYw5sOA), 3000, 1), """""""", """"),\n ""where Col1 contains \'<meta itemprop=channelId content=\'""),\n ""<meta itemprop=channelId content=|>"", """"))\n\n\n\n\nUPDATE:\nchannel name (07/07/2021):\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(A4)), \n ""where Col1 contains \'\\x22channelName\\x22:\\x22\'"", 0), "":\\\\x22(.+)\\\\x22$"")\n\nvideo title (08/08/2021)\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(A1)), \n ""where Col1 starts with \'title:""""\'"", 0), """"""(.*)"""""")\n\n\nduration (21/04/2022)\n=TEXT(1*REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(B1)), \n ""where Col1 contains \'approxDurationMs\' limit 1"", ), \n ""\\d+"")/3600000/24, ""mm:ss"")\n\n\nchannel views (07/06/2022)\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTXML(A1, ""//*"")), \n ""where Col1 contains \'""&CHAR(10)&""Creators\'"", ), \n "".x22text.x22:.x22(.+).x22,.x22bold.x22"")\n\n\nvideo views (01/09/2022)\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(A1)); \n ""where Col1 starts with \'viewCount\'""; ); ""\\d+"")*1\n\n\n']"
