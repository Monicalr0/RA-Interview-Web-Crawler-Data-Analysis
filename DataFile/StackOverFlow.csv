Title,Description,Views,Votes,Answers,URL,Tag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to switch to new window in Selenium for Python?,"
I am working on selenium automation project using Python.
I am facing an issue, which is handling multiple browser windows.
Scenario is as follows. When I click a link on the home page, a new window opens. In the newly opened window I cannot perform any actions, because the focus is still on the home page web driver.
Can anybody show me how to change focus from the background window to the newly opened window?
A possible solution is driver.switch_to.window(), but it requires the window's name. How to find out the window's name? If this is a wrong way to do this, can anybody give some code examples to perform this action?
",187k,"
            79
        ","['\nYou can do it by using window_handles and switch_to.window method.\nBefore clicking the link first store the window handle as\nwindow_before = driver.window_handles[0]\n\nafter clicking the link store the window handle of newly opened window as\nwindow_after = driver.window_handles[1]\n\nthen execute the switch to window method to move to newly opened window\ndriver.switch_to.window(window_after)\n\nand similarly you can switch between old and new window. Following is the code example\nimport unittest\nfrom selenium import webdriver\n\nclass GoogleOrgSearch(unittest.TestCase):\n\n    def setUp(self):\n        self.driver = webdriver.Firefox()\n\n    def test_google_search_page(self):\n        driver = self.driver\n        driver.get(""http://www.cdot.in"")\n        window_before = driver.window_handles[0]\n        print window_before\n        driver.find_element_by_xpath(""//a[@href=\'http://www.cdot.in/home.htm\']"").click()\n        window_after = driver.window_handles[1]\n        driver.switch_to.window(window_after)\n        print window_after\n        driver.find_element_by_link_text(""ATM"").click()\n        driver.switch_to.window(window_before)\n\n    def tearDown(self):\n        self.driver.close()\n\nif __name__ == ""__main__"":\n    unittest.main()\n\n', '\nOn top of the answers already given, to open a new tab the javascript command window.open() can be used.\nFor example:\n# Opens a new tab\nself.driver.execute_script(""window.open()"")\n\n# Switch to the newly opened tab\nself.driver.switch_to.window(self.driver.window_handles[1])\n\n# Navigate to new URL in new tab\nself.driver.get(""https://google.com"")\n# Run other commands in the new tab here\n\nYou\'re then able to close the original tab as follows\n# Switch to original tab\nself.driver.switch_to.window(self.driver.window_handles[0])\n\n# Close original tab\nself.driver.close()\n\n# Switch back to newly opened tab, which is now in position 0\nself.driver.switch_to.window(self.driver.window_handles[0])\n\nOr close the newly opened tab\n# Close current tab\nself.driver.close()\n\n# Switch back to original tab\nself.driver.switch_to.window(self.driver.window_handles[0])\n\nHope this helps.\n', '\nwindow_handles should give you the references to all open windows.\nthis is what the documentation has to say about switching windows.\n', ""\nfor eg. you may take\ndriver.get('https://www.naukri.com/')\n\nsince, it is a current window ,we can name it\nmain_page = driver.current_window_handle\n\nif there are atleast 1 window popup except the current window,you may try this method and put if condition in break statement by hit n trial for the index\nfor handle in driver.window_handles:\n    if handle != main_page:\n        print(handle)\n        login_page = handle\n        break\n\ndriver.switch_to.window(login_page)\n\nNow ,whatever the credentials you have to apply,provide\nafter it is loggen in. Window will disappear, but you have to come to main page window and you are done\ndriver.switch_to.window(main_page)\nsleep(10)\n\n"", '\nWe can handle the different windows by moving between named windows using the 鈥渟witchTo鈥?method:\ndriver.switch_to.window(""windowName"")\n\n<a href=""somewhere.html"" target=""windowName"">Click here to open a new window</a>\n\nAlternatively, you can pass a 鈥渨indow handle鈥?to the 鈥渟witchTo().window()鈥?method. Knowing this, it鈥檚 possible to iterate over every open window like so:\nfor handle in driver.window_handles:\n    driver.switch_to.window(handle)\n\n']",https://stackoverflow.com/questions/10629815/how-to-switch-to-new-window-in-selenium-for-python,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I use EnumWindows to find windows with a specific caption/title?,"
I am working on an application that will eventually be an api for driving UI Tests for a WPF application.
At one point of the initial test we are working on, we get 2 Windows security popups.
We have some code that loops 10 times, it gets the handle of one of the popups using the FindWindowByCaption method and enters the information and clicks ok.
9 times out of 10 this works just fine, however we are occasionally seeing what looks to be a race condition. My suspicion is that the loop starts when only one of the windows is open and while its entering the information the second one opens and steals focus; after this it just hangs indefinitely.
What I'm wondering is if there is any method to get all of the window handles for a given caption, so that we can wait until there are 2 before starting the loop.
",48k,"
            28
        ","['\nOriginal Answer\nUse EnumWindows and enumerate through all the windows, using GetWindowText to get each window\'s text, then filter it however you want.\n[DllImport(""user32.dll"", CharSet = CharSet.Unicode)]\nprivate static extern int GetWindowText(IntPtr hWnd, StringBuilder strText, int maxCount);\n\n[DllImport(""user32.dll"", CharSet = CharSet.Unicode)]\nprivate static extern int GetWindowTextLength(IntPtr hWnd);\n\n[DllImport(""user32.dll"")]\nprivate static extern bool EnumWindows(EnumWindowsProc enumProc, IntPtr lParam);\n\n// Delegate to filter which windows to include \npublic delegate bool EnumWindowsProc(IntPtr hWnd, IntPtr lParam);\n\n/// <summary> Get the text for the window pointed to by hWnd </summary>\npublic static string GetWindowText(IntPtr hWnd)\n{\n    int size = GetWindowTextLength(hWnd);\n    if (size > 0)\n    {\n        var builder = new StringBuilder(size + 1);\n        GetWindowText(hWnd, builder, builder.Capacity);\n        return builder.ToString();\n    }\n\n    return String.Empty;\n}\n\n/// <summary> Find all windows that match the given filter </summary>\n/// <param name=""filter""> A delegate that returns true for windows\n///    that should be returned and false for windows that should\n///    not be returned </param>\npublic static IEnumerable<IntPtr> FindWindows(EnumWindowsProc filter)\n{\n  IntPtr found = IntPtr.Zero;\n  List<IntPtr> windows = new List<IntPtr>();\n\n  EnumWindows(delegate(IntPtr wnd, IntPtr param)\n  {\n      if (filter(wnd, param))\n      {\n          // only add the windows that pass the filter\n          windows.Add(wnd);\n      }\n\n      // but return true here so that we iterate all windows\n      return true;\n  }, IntPtr.Zero);\n\n  return windows;\n}\n\n/// <summary> Find all windows that contain the given title text </summary>\n/// <param name=""titleText""> The text that the window title must contain. </param>\npublic static IEnumerable<IntPtr> FindWindowsWithText(string titleText)\n{\n    return FindWindows(delegate(IntPtr wnd, IntPtr param)\n    {\n        return GetWindowText(wnd).Contains(titleText);\n    });\n} \n\nFor example, to get all of the windows with ""Notepad"" in the title:\nvar windows = FindWindowsWithText(""Notepad"");\n\nWin32Interop.WinHandles\nThis answer proved popular enough that I created an OSS project, Win32Interop.WinHandles to provide an abstraction over IntPtrs for win32 windows.  Using the library, to get all of the windows that contains ""Notepad"" in the title:\nvar allNotepadWindows\n   = TopLevelWindowUtils.FindWindows(wh => wh.GetWindowText().Contains(""Notepad""));\n\n', '\nI know this is an old question but it is one that answer will change over time as Visual Studio moves into the future.\nI would like to share my solution which allows you to search for a partial Window Title which is often needed when the Title Caption contains unpredictable text. For example if you wanted to find the handle to the Windows Mail Application the Title will contain the text ""Inbox - youremailaccountname"". Obviously you don\'t want to hard code the account name. Here is my code although it is in Visual Basic .NET you can convert it to C#.  Type in a partial title (i.e. ""Inbox - ""), click the button and you will get the hwnd and full title back.  I tried using Process.GetProcesses() but it was way to slow compared to the Win API. \nThis Example will return the window handle of your search in lparm of the EnumWindows call (2nd parameter passed byref) and will bring the application to the front even if it is minimized.\nImports System.Runtime.InteropServices\nImports System.Text\nPublic Class Form1\n    <DllImport(""user32.dll"", SetLastError:=True, CharSet:=CharSet.Auto)> Private Shared Function EnumWindows(ByVal lpEnumFunc As EnumWindowsProcDelegate, ByRef lParam As IntPtr) As Boolean\n    End Function\n    Private Delegate Function EnumWindowsProcDelegate(ByVal hWnd As IntPtr, ByRef lParam As IntPtr) As Integer\n\n    <DllImport(""user32.dll"")>\n    Private Shared Function GetWindowTextLength(ByVal hWnd As IntPtr) As Integer\n    End Function\n\n    <DllImport(""user32.dll"")>\n    Private Shared Function GetWindowText(ByVal hWnd As IntPtr, ByVal lpString As StringBuilder, ByVal nMaxCount As Integer) As Integer\n    End Function\n\n    <DllImport(""user32"", EntryPoint:=""SendMessageA"", CharSet:=CharSet.Ansi, SetLastError:=True, ExactSpelling:=True)> Public Shared Function SendMessage(ByVal hwnd As Integer, ByVal wMsg As Integer, ByVal wParam As Integer, ByRef lParam As Integer) As Integer\n    End Function\n\n    <DllImport(""user32.dll"")>\n    Private Shared Function SetForegroundWindow(ByVal hWnd As IntPtr) As Boolean\n    End Function\n\n    <DllImport(""user32.dll"", SetLastError:=True)>\n    Private Shared Function SetActiveWindow(ByVal hWnd As IntPtr) As Integer\n    End Function\n\n    <DllImport(""user32.dll"", SetLastError:=True)>\n    Private Shared Function SetWindowPos(ByVal hWnd As IntPtr, hWndInsertAfter As IntPtr, x As Integer, y As Integer, cx As Integer, cy As Integer, uFlags As UInt32) As Boolean\n    End Function\n\n    <DllImport(""user32.dll"", SetLastError:=True)>\n    Private Shared Function RedrawWindow(ByVal hWnd As IntPtr, lprcUpdate As Integer, hrgnUpdate As Integer, uFlags As UInt32) As Boolean\n    End Function\n\n    Public Const WM_SYSCOMMAND As Integer = &H112\n    Public Const SC_RESTORE = &HF120\n    Public Const SWP_SHOWWINDOW As Integer = &H40\n    Public Const SWP_NOSIZE As Integer = &H1\n    Public Const SWP_NOMOVE As Integer = &H2\n    Public Const RDW_FRAME As Int32 = 1024 \'Updates the nonclient area if included in the redraw area. RDW_INVALIDATE must also be specified.\n    Public Const RDW_INVALIDATE As Int32 = 1 \'Invalidates the redraw area.\n    Public Const RDW_ALLCHILDREN As Int32 = 128 \'Redraw operation includes child windows if present in the redraw area.\n\n    Private Sub Button1_Click(sender As Object, e As EventArgs) Handles Button1.Click\n        Dim strPartialTitle As String = TextBox1.Text\n        Dim intptrByRefFoundHwnd As IntPtr = Marshal.StringToHGlobalAnsi(strPartialTitle)\n        Dim delegateEnumWindowsProcDelegate As EnumWindowsProcDelegate\n        delegateEnumWindowsProcDelegate = New EnumWindowsProcDelegate(AddressOf EnumWindowsProc)\n        EnumWindows(delegateEnumWindowsProcDelegate, intptrByRefFoundHwnd)\n        LabelHwndAndWindowTitle.Text = intptrByRefFoundHwnd\n        BringWindowToFront(intptrByRefFoundHwnd)\n    End Sub\n\n    Function EnumWindowsProc(ByVal hWnd As IntPtr, ByRef lParam As IntPtr) As Integer\n        Dim strPartialTitle As String = Marshal.PtrToStringAnsi(lParam)\n        Dim length As Integer = GetWindowTextLength(hWnd)\n        Dim stringBuilder As New StringBuilder(length)\n        GetWindowText(hWnd, stringBuilder, (length + 1))\n        If stringBuilder.ToString.Trim.Length > 2 Then\n            If stringBuilder.ToString.ToLower.Contains(strPartialTitle.ToLower) Then\n                Debug.WriteLine(hWnd.ToString & "": "" & stringBuilder.ToString)\n                lParam = hWnd \' Pop hwnd to top, returns in lParm of EnumWindows Call (2nd parameter)\n                Return False\n            End If\n        End If\n        Return True\n    End Function\n\n    Private Sub BringWindowToFront(hwnd As IntPtr)\n        SendMessage(hwnd, WM_SYSCOMMAND, SC_RESTORE, 0) \' restore the minimize window\n        SetForegroundWindow(hwnd)\n        SetActiveWindow(hwnd)\n        SetWindowPos(hwnd, IntPtr.Zero, 0, 0, 0, 0, SWP_SHOWWINDOW Or SWP_NOMOVE Or SWP_NOSIZE)\n        \'redraw to prevent the window blank.\n        RedrawWindow(hwnd, IntPtr.Zero, 0, RDW_FRAME Or RDW_INVALIDATE Or RDW_ALLCHILDREN)\n    End Sub\n\nEnd Class\n\n', '\nusing HWND = IntPtr;\n\n/// <summary>Contains functionality to get all the open windows.</summary>\npublic static class OpenWindowGetter\n{\n/// <summary>Returns a dictionary that contains the handle and title of all the open windows.</summary>\n/// <returns>A dictionary that contains the handle and title of all the open windows.</returns>\npublic static IDictionary<HWND, string> GetOpenWindows()\n{\nHWND shellWindow = GetShellWindow();\nDictionary<HWND, string> windows = new Dictionary<HWND, string>();\n\nEnumWindows(delegate(HWND hWnd, int lParam)\n{\n  if (hWnd == shellWindow) return true;\n  if (!IsWindowVisible(hWnd)) return true;\n\n  int length = GetWindowTextLength(hWnd);\n  if (length == 0) return true;\n\n  StringBuilder builder = new StringBuilder(length);\n  GetWindowText(hWnd, builder, length + 1);\n\n  windows[hWnd] = builder.ToString();\n  return true;\n\n}, 0);\n\nreturn windows;\n}\n\nprivate delegate bool EnumWindowsProc(HWND hWnd, int lParam);\n\n[DllImport(""USER32.DLL"")]\nprivate static extern bool EnumWindows(EnumWindowsProc enumFunc, int lParam);\n\n[DllImport(""USER32.DLL"")]\nprivate static extern int GetWindowText(HWND hWnd, StringBuilder lpString, int nMaxCount);\n\n[DllImport(""USER32.DLL"")]\nprivate static extern int GetWindowTextLength(HWND hWnd);\n\n[DllImport(""USER32.DLL"")]\nprivate static extern bool IsWindowVisible(HWND hWnd);\n\n[DllImport(""USER32.DLL"")]\nprivate static extern IntPtr GetShellWindow();\n}\n\nAnd here鈥檚 some code that uses it:\nforeach(KeyValuePair<IntPtr, string> window in OpenWindowGetter.GetOpenWindows())\n{\nIntPtr handle = window.Key;\nstring title = window.Value;\n\nConsole.WriteLine(""{0}: {1}"", handle, title);\n}\n\nI got this code from http://www.tcx.be/blog/2006/list-open-windows/\nIf you need help on how to use this, let me know, I figured it out\n']",https://stackoverflow.com/questions/19867402/how-can-i-use-enumwindows-to-find-windows-with-a-specific-caption-title,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
programmatically press an enter key after starting .exe file in Matlab,"
In Matlab I can start external .exe files that sometime have a pop up that requires an enter key pressed. For example:
system('C:\Program Files (x86)\WinZip\WINZIP32.EXE')

will start Winzip, and then in order to use it you need to pass the ""buy now"" pop up window by pressing enter. 
Now my problem is not with winzip, I only gave it as an example (i use winrar anyway :).
How can I programmatically press an enter key in Matlab in such cases ? (I use win 7)
Can an event listener be used to solve that?
EDIT: The java.awt.Robot class indeed works on explorer, but not on any software that has a pop up window with an OK button that needs to be pressed. I don't know why it doesn't work for that. I gave the winzip example because I assume everybody has winzip/winrar installed in their machine. The actual software I have is different and irrelevant for the question. 
",9k,"
            26
        ","['\nThere is a way using Java from Matlab, specifically the java.awt.Robot class. See here.\nApparently there are two types of programs, regarding the way they work when called from Matlab with system(\'...\'):\n\nFor some programs, Matlab waits until the program has finished before running the next statement. This happens for example with WinRAR (at least in my Windows 7 machine).\nFor other programs this doesn\'t happen, and Matlab proceeds with the next statement right after the external program has been started. An example of this type is explorer (the standard Windows file explorer).\n\nNow, it is possible to return execution to Matlab immediately even for type 1 programs: just add & at the end of the string passed to system. This is standard in Linux Bash shell, and it also works in Windows, as discussed here.\nSo, you would proceed as follows:\nrobot = java.awt.Robot;\ncommand = \'""C:\\Program Files (x86)\\WinRAR\\WinRAR""\'; %// external program; full path\nsystem([command \' &\']); %// note: \' &\' at the end\npause(5) %// allow some time for the external program to start\nrobot.keyPress (java.awt.event.KeyEvent.VK_ENTER); %// press ""enter"" key\nrobot.keyRelease (java.awt.event.KeyEvent.VK_ENTER); %// release ""enter"" key\n\n', '\nIf your applications are only on Windows platform, you can try using .net objects.\nThe SendWait method of the  SendKeys objects allows to send virtually any key, or key combination, to the application which has the focus, including the ""modifier"" keys like Alt, Shift, Ctrl etc ...\nThe first thing to do is to import the .net library, then the full syntax to send the ENTER key would be:\nNET.addAssembly(\'System.Windows.Forms\');\nSystem.Windows.Forms.SendKeys.SendWait(\'{ENTER}\'); %// send the key ""ENTER""\n\nIf you only do it once the full syntax is OK. If you plan to make extensive use of the command, you can help yourself with an anonymous helper function.\nA little example with notepad\n%% // import the .NET assembly and define helper function\nNET.addAssembly(\'System.Windows.Forms\');\nsendkey = @(strkey) System.Windows.Forms.SendKeys.SendWait(strkey) ;\n\n%% // prepare a few things to send to the notepad\nstr1 = \'Hello World\' ;\nstr2 = \'OMG ... my notepad is alive\' ;\nfile2save = [pwd \'\\SelfSaveTest.txt\'] ;\nif exist(file2save,\'file\')==2 ; delete(file2save) ; end %// this is just in case you run the test multiple times.\n\n%% // go for it\n%// write a few things, save the file then close it.\nsystem(\'notepad &\') ;   %// Start notepad, without matlab waiting for the return value\nsendkey(str1)           %// send a full string to the notepad\nsendkey(\'{ENTER}\');     %// send the {ENTER} key\nsendkey(str2)           %// send another full string to the notepad\nsendkey(\'{! 3}\');       %// note how you can REPEAT a key send instruction\nsendkey(\'%(FA)\');       %// Send key combination to open the ""save as..."" dialog\npause(1)                %// little pause to make sure your hard drive is ready before continuing\nsendkey(file2save);     %// Send the name (full path) of the file to save to the dialog\nsendkey(\'{ENTER}\');     %// validate\npause(3)                %// just wait a bit so you can see you file is now saved (check the titlebar of the notepad)\nsendkey(\'%(FX)\');       %// Bye bye ... close the Notepad\n\n\nAs explained in the Microsoft documentation the SendKeys class may have some timing issues sometimes so if you want to do complex manipulations (like Tab multiple times to change the button you actually want to press), you may have to introduce a pause in your Matlab calls to SendKeys.\nTry without first, but don\'t forget you are managing a process from another without any synchronization between them, so timing all that can require a bit of trial and error before you get it right, at least for complex sequences (simple one should be straightforward).\nIn my case above for example I am running all my data from an external hard drive with an ECO function which puts it into standby, so when I called the ""save as..."" dialog, it takes time for it to display because the HDD has to wake up. If I didn\'t introduce the pause(1), sometimes the file path would be imcomplete (the first part of the path was send before the dialog had the focus).\n\nAlso, do not forget the & character when you execute the external program. All credit to Luis Mendo for highlighting it. (I tend to forget how important it is because I use it by default. I only omit it if I have to specifically wait for a return value from the program, otherwise I let it run on its own)\n\nThe special characters have a special code. Here are a few:\nShift          +\nControl (Ctrl)  ^\nAlt            %\n\nTab            {TAB}\nBackspace      {BACKSPACE}, {BS}, or {BKSP}\nValidation     {ENTER} or ~ (a tilde)\nIns Or Insert  {INSERT} or {INS}\nDelete         {DELETE} or {DEL}\n\nText Navigation {HOME} {END} {PGDN} {PGUP}\nArrow Keys      {UP} {RIGHT} {DOWN} {LEFT}\n\nEscape          {ESC}\nFunction Keys   {F1} ... {F16}\nPrint Screen    {PRTSC}\nBreak           {BREAK}\n\nThe full list from Microsoft can be found here\n', '\nThere is a small javascript utility that simulates keystrokes like this on the Windows javascript interpreter.\nJust create a js file with following code:\nvar WshShell = WScript.CreateObject(""WScript.Shell"");\nWshShell.SendKeys(WScript.Arguments(0));\n\nthen call it from Matlab after the necessary timeout like this:\nsystem(\'c:\\my\\js\\file\\script.js {Enter}\');\n\nCan\'t test here now, but I think this should work...\n', ""\nIf you need to run a console-only program in a context that permits full DOS redirection, you can create a file called, say, CR.txt containing a carriage return and use the '<' notation to pipe the value into the program.\nThis only works if you can provide all the keyboard input can be recorded in the file. It fails dismally if the input has to vary based on responses.\nAn alternative is to duplicate the input (and possibly output) stream(s) for the program and then pipe data into and out of the program. This is more robust and can permit dynamic responses to the data, but will also likely require substantial effort to implement a robot user to the application.\nRog-O-Matic is an example of a large application completely controlled by a program that monitors screen output and simulates keyboard input to play an early (1980s) ASCII graphic adventure game.\nThe other responses will be required for GUI-based applications.\n"", '\nPython package pywinauto can wait any dialog and click buttons automatically. But it\'s capable for native and some .NET applications only. You may have problems with pressing WPF button (maybe QT button is clickable - not checked), but in such case code like app.DialogTitle.wait(\'ready\').set_focus(); app.DialogTitle.type_keys(\'{ENTER}\') may help. Your case is quite simple and probably some tricks with pywinauto are enough. Is your ""app with popup"" 64-bit or 32-bit?\nwait and wait_not functions have timeout parameter. But if you need precisely listener with potentially infinite loop awaiting popups, good direction is global Windows hooks (pyHook can listen mouse and keybd events, but cannot listen dialog opening). I\'ll try to find my prototype that can detect new windows. It uses UI Automation API event handlers... and... ops... it requires IronPython. I still don\'t know how to set UI Automation handler with COM interface from standard CPython.\n\nEDIT (2019, January): new module win32hooks was implemented in pywinauto a while ago. Example of usage is here: examples/hook_and_listen.py.\n']",https://stackoverflow.com/questions/27933270/programmatically-press-an-enter-key-after-starting-exe-file-in-matlab,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an event to all Forms in a Project,"
If I want to display the size of every Form in my Project in the Form's Title what will be the best approach?
I don't want to manually put a event handler in every Form.
I want the process to be automatic.Something like a overloaded Load() event that adds a handler on the resize event.
",2k,"
            5
        ","['\nHere is an attempt to implement an Automation solution to the problem.\nThe problem:\nAttach one or more Event Handlers to each existing Form in a Project (or a subset of them), without editing/modifying these classes existing code.\nA possible solution comes from UIAutomation, which provides means to detect when a new Window is opened and reports the event to the subscribers of its own  Automation.AddAutomationEventHandler, when the EventId of its AutomationEvent is set to a WindowPattern pattern.\nThe AutomationElement member must be set to AutomationElement.RootElement and the Scope member to TreeScope.SubTree.\nAutomation, for each AutomationElement that raises the AutomationEvent, reports:\n\nthe Element.Name (corresponding to the Windows Title)\nthe Process ID\nthe Window Handle (as an Integer value)\n\nThese values are quite enough to identify a Window that belongs to the current process; the Window handle allows to identify the opened Form instance,  testing the Application.OpenForms() collection.\nWhen the Form is singled out, a new Event Handler can be attached to an Event of choice.\nBy expanding this concept, it\'s possible to create a predefined List of Events and a List of Forms to attach these events to.\nPossibly, with a class file to include in a Project when required.\nAs a note, some events will not be meaningful in this scenario, because the Automation reports the opening of a Window when it is already shown, thus the Load() and Shown() events belong to the past.\n\nI\'ve tested this with a couple of events (Form.Resize() and Form.Activate()), but in the code here I\'m using just .Resize() for simplicity.\nThis is a graphics representation of the process.\nStarting the application, the Event Handler is not attached to the .Resize() event.\nIt\'s just because a Boolean fields is set to False.\nClicking a Button, the Boolean field is set to True, enabling the registration of the Event Handler.\nWhen the .Resize() event is registered, all Forms\' Title will report the current size of the Window.\n\nTest environment:\nVisual Studio 2017 pro 15.7.5\n.Net FrameWork 4.7.1\nImported Namespaces:\nSystem.Windows.Automation\nReference Assemblies:\nUIAutomationClient\nUIAutomationTypes\nMainForm code:\nImports System.Diagnostics\nImports System.Windows\nImports System.Windows.Automation\n\nPublic Class MainForm\n\n    Friend GlobalHandlerEnabled As Boolean = False\n    Protected Friend FormsHandler As List(Of Form) = New List(Of Form)\n    Protected Friend ResizeHandler As EventHandler\n\n    Public Sub New()\n\n        InitializeComponent()\n\n        ResizeHandler =\n                Sub(obj, args)\n                    Dim CurrentForm As Form = TryCast(obj, Form)\n                    CurrentForm.Text = CurrentForm.Text.Split({"" (""}, StringSplitOptions.None)(0) &\n                                                               $"" ({CurrentForm.Width}, {CurrentForm.Height})""\n                End Sub\n\n        Automation.AddAutomationEventHandler(WindowPattern.WindowOpenedEvent,\n            AutomationElement.RootElement,\n                TreeScope.Subtree,\n                    Sub(UIElm, evt)\n                        If Not GlobalHandlerEnabled Then Return\n                        Dim element As AutomationElement = TryCast(UIElm, AutomationElement)\n                        If element Is Nothing Then Return\n\n                        Dim NativeHandle As IntPtr = CType(element.Current.NativeWindowHandle, IntPtr)\n                        Dim ProcessId As Integer = element.Current.ProcessId\n                        If ProcessId = Process.GetCurrentProcess().Id Then\n                            Dim CurrentForm As Form = Nothing\n                            Invoke(New MethodInvoker(\n                                Sub()\n                                    CurrentForm = Application.OpenForms.\n                                           OfType(Of Form)().\n                                           FirstOrDefault(Function(f) f.Handle = NativeHandle)\n                                End Sub))\n\n                            If CurrentForm IsNot Nothing Then\n                                Dim FormName As String = FormsHandler.FirstOrDefault(Function(f) f?.Name = CurrentForm.Name)?.Name\n                                If Not String.IsNullOrEmpty(FormName) Then\n                                    RemoveHandler CurrentForm.Resize, ResizeHandler\n                                    FormsHandler.Remove(FormsHandler.Where(Function(fn) fn.Name = FormName).First())\n                                End If\n                                Invoke(New MethodInvoker(\n                                Sub()\n                                    CurrentForm.Text = CurrentForm.Text & $"" ({CurrentForm.Width}, {CurrentForm.Height})""\n                                End Sub))\n\n                                AddHandler CurrentForm.Resize, ResizeHandler\n                                FormsHandler.Add(CurrentForm)\n                            End If\n                        End If\n                    End Sub)\n    End Sub\n\n\n    Private Sub btnOpenForm_Click(sender As Object, e As EventArgs) Handles btnOpenForm.Click\n        Form2.Show(Me)\n    End Sub\n\n    Private Sub btnEnableHandlers_Click(sender As Object, e As EventArgs) Handles btnEnableHandlers.Click\n        GlobalHandlerEnabled = True\n        Me.Hide()\n        Me.Show()\n    End Sub\n\n    Private Sub btnDisableHandlers_Click(sender As Object, e As EventArgs) Handles btnDisableHandlers.Click\n        GlobalHandlerEnabled = False\n        If FormsHandler IsNot Nothing Then\n            For Each Item As Form In FormsHandler\n                RemoveHandler Item.Resize, ResizeHandler\n                Item = Nothing\n            Next\n        End If\n        FormsHandler = New List(Of Form)\n        Me.Text = Me.Text.Split({"" (""}, StringSplitOptions.RemoveEmptyEntries)(0)\n    End Sub\nEnd Class\n\nNote:\nThis previous code is  placed inside the app Starting Form (for testing), but it might be preferable to have a Module to include in the Project when needed, without touching the current code.\nTo get this to work, add a new Module (named Program) which contains a Public Sub Main(), and change the Project properties to start the application from Sub Main() instead of a Form.\nRemove the check mark on Use Application Framework and choose Sub Main from the Startup object Combo.\nAll the code can be transferred to the Sub Main proc with a couple of modifications:\nImports System\nImports System.Diagnostics\nImports System.Windows\nImports System.Windows.Forms\nImports System.Windows.Automation\n\nModule Program\n\n    Friend GlobalHandlerEnabled As Boolean = True\n    Friend FormsHandler As List(Of Form) = New List(Of Form)\n    Friend ResizeHandler As EventHandler\n\n    Public Sub Main()\n\n        Application.EnableVisualStyles()\n        Application.SetCompatibleTextRenderingDefault(False)\n\n        Dim MyMainForm As MainForm = New MainForm()\n\n        ResizeHandler =\n                Sub(obj, args)\n                    Dim CurrentForm As Form = TryCast(obj, Form)\n                    CurrentForm.Text = CurrentForm.Text.Split({"" (""}, StringSplitOptions.None)(0) &\n                                                               $"" ({CurrentForm.Width}, {CurrentForm.Height})""\n                End Sub\n\n        Automation.AddAutomationEventHandler(WindowPattern.WindowOpenedEvent,\n            AutomationElement.RootElement,\n                TreeScope.Subtree,\n                    Sub(UIElm, evt)\n                        If Not GlobalHandlerEnabled Then Return\n                        Dim element As AutomationElement = TryCast(UIElm, AutomationElement)\n                        If element Is Nothing Then Return\n\n                        Dim NativeHandle As IntPtr = CType(element.Current.NativeWindowHandle, IntPtr)\n                        Dim ProcessId As Integer = element.Current.ProcessId\n                        If ProcessId = Process.GetCurrentProcess().Id Then\n                            Dim CurrentForm As Form = Nothing\n                            If Not MyMainForm.IsHandleCreated Then Return\n                            MyMainForm.Invoke(New MethodInvoker(\n                                Sub()\n                                    CurrentForm = Application.OpenForms.\n                                           OfType(Of Form)().\n                                           FirstOrDefault(Function(f) f.Handle = NativeHandle)\n                                End Sub))\n                            If CurrentForm IsNot Nothing Then\n                                Dim FormName As String = FormsHandler.FirstOrDefault(Function(f) f?.Name = CurrentForm.Name)?.Name\n                                If Not String.IsNullOrEmpty(FormName) Then\n                                    RemoveHandler CurrentForm.Resize, ResizeHandler\n                                    FormsHandler.Remove(FormsHandler.Where(Function(fn) fn.Name = FormName).First())\n                                End If\n\n                                AddHandler CurrentForm.Resize, ResizeHandler\n                                FormsHandler.Add(CurrentForm)\n\n                                CurrentForm.Invoke(New MethodInvoker(\n                                Sub()\n                                    CurrentForm.Text = CurrentForm.Text & $"" ({CurrentForm.Width}, {CurrentForm.Height})""\n                                End Sub))\n                            End If\n                        End If\n                    End Sub)\n\n        Application.Run(MyMainForm)\n\n    End Sub\n\nEnd Module\n\n', '\nYou can use Automation as @Jimi suggested.\nYou can use My.Application.OpenForms to iterate throught all opened forms, but it will not help when new form is opened.\nYou can create some ReportSizeForm class that inherits System.Forms.Form. And change inheritance of your forms from regular System.Windows.Forms.Form to your ReportSizeForm. \n']",https://stackoverflow.com/questions/51491566/add-an-event-to-all-forms-in-a-project,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python - Control window with pywinauto while the window is minimized or hidden,"
What I'm trying to do:
I'm trying to create a script in python with pywinauto to automatically install notepad++ in the background (hidden or minimized), notepad++ is just an example since I will edit it to work with other software.
Problem:
The problem is that I want to do it while the installer is hidden or minimized, but if I move my mouse the script will stop working.
Question:
How can I execute this script and make it work, while the notepad++ installer is hidden or minimized.
This is my code so far:
import sys, os, pywinauto

pwa_app = pywinauto.application.Application()

app = pywinauto.Application().Start(r'npp.6.8.3.Installer.exe')

Wizard = app['Installer Language']

Wizard.NextButton.Click()

Wizard = app['Notepad++ v6.8.3 Setup']

Wizard.Wait('visible')

Wizard['Welcome to the Notepad++ v6.8.3 Setup'].Wait('ready')
Wizard.NextButton.Click()

Wizard['License Agreement'].Wait('ready')
Wizard['I &Agree'].Click()

Wizard['Choose Install Location'].Wait('ready')
Wizard.Button2.Click()

Wizard['Choose Components'].Wait('ready')
Wizard.Button2.Click()

Wizard['Create Shortcut on Desktop'].Wait('enabled').CheckByClick()
Wizard.Install.Click()

Wizard['Completing the Notepad++ v6.8.3 Setup'].Wait('ready', timeout=30)
Wizard['CheckBox'].Wait('enabled').Click()
Wizard.Finish.Click()
Wizard.WaitNot('visible')

",22k,"
            9
        ","['\nThe problem is here:\nWizard[\'Create Shortcut on Desktop\'].wait(\'enabled\').check_by_click()\n\ncheck_by_click() uses click_input() method that moves real mouse cursor and performs a realistic click.\nUse check() method instead.\n[EDIT] If the installer doesn\'t handle BM_SETCHECK properly the workaround may look so:\ncheckbox = Wizard[\'Create Shortcut on Desktop\'].wait(\'enabled\')\nif checkbox.get_check_state() != pywinauto.win32defines.BST_CHECKED:\n    checkbox.click()\n\nI will fix it in the next pywinauto release by creating methods check_by_click and check_by_click_input respectively.\n\n[EDIT 2]\nI tried your script with my fix and it works perfectly (and very fast) with and without mouse moves. Win7 x64, 32-bit Python 2.7, pywinauto 0.6.x, run as administrator.\nimport sys\nimport os\nfrom pywinauto import Application\n\napp = Application(backend=""win32"").start(r\'npp.6.8.3.Installer.exe\')\n\nWizard = app[\'Installer Language\']\n\nWizard.minimize()\nWizard.NextButton.click()\n\nWizard = app[\'Notepad++ v6.8.3 Setup\']\n\nWizard.wait(\'visible\')\nWizard.minimize()\n\nWizard[\'Welcome to the Notepad++ v6.8.3 Setup\'].wait(\'ready\')\nWizard.NextButton.click()\n\nWizard.minimize()\nWizard[\'License Agreement\'].wait(\'ready\')\nWizard[\'I &Agree\'].click()\n\nWizard.minimize()\nWizard[\'Choose Install Location\'].wait(\'ready\')\nWizard.Button2.click()\n\nWizard.minimize()\nWizard[\'Choose Components\'].wait(\'ready\')\nWizard.Button2.click()\n\nWizard.minimize()\ncheckbox = Wizard[\'Create Shortcut on Desktop\'].wait(\'enabled\')\nif checkbox.get_check_state() != pywinauto.win32defines.BST_CHECKED:\n    checkbox.click()\nWizard.Install.click()\n\nWizard[\'Completing the Notepad++ v6.8.3 Setup\'].wait(\'ready\', timeout=30)\nWizard.minimize()\nWizard[\'CheckBox\'].wait(\'enabled\').click()\nWizard.Finish.click()\nWizard.wait_not(\'visible\')\n\n']",https://stackoverflow.com/questions/32846550/python-control-window-with-pywinauto-while-the-window-is-minimized-or-hidden,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Access a new window - cypress.io,"
The question is as simple as that. In Cypress, how can I access a new window that opens up when running the test.
Steps to recreate :


Run the test. After some action, new window pops up (the url is dynamic in nature).
Fill in the fields in the new window, and click a few buttons.
After required actions are completed in the new Window, close the new window and move back to the main window.
Continue execution with the main window.


Point of interest: the focus should be
main window -> new window -> main window

I have read few things that relate to use of iframe and confirmation box, but here its none of those. Relates to accessing a whole new window. Something like Window Handlers in Selenium. Unfortunately could not find anything related to it.
",82k,"
            63
        ","[""\nAccessing new windows via Cypress is intentionally not supported.\nHowever, there are many ways this functionality can be tested in Cypress now. You can split up your tests into separate pieces and still have confidence that your application is covered.\n\n\nWrite a test to check that when performing the action in your app, the window.open event is called by using cy.spy() to listen for a window.open event.\n\n\ncy.visit('http://localhost:3000', {\n  onBeforeLoad(win) {\n    cy.stub(win, 'open')\n  }\n})\n\n// Do the action in your app like cy.get('.open-window-btn').click()\n\ncy.window().its('open').should('be.called')\n\n\n\nIn a new test, use cy.visit() to go to the url that would have opened in the new window, fill in the fields and click the buttons like you would in a Cypress test.\n\n\ncy.visit('http://localhost:3000/new-window')\n\n// Do the actions you want to test in the new window\n\nFully working test example can be found here.\n"", '\nI am not cypress expert, just started using it few days ago, but I figured out this kind solution for stateful application with dynamic link:\n// Get window object\ncy.window().then((win) => {\n  // Replace window.open(url, target)-function with our own arrow function\n  cy.stub(win, \'open\', url => \n  {\n    // change window location to be same as the popup url\n    win.location.href = Cypress.config().baseUrl + url;\n  }).as(""popup"") // alias it with popup, so we can wait refer it with @popup\n})\n\n// Click button which triggers javascript\'s window.open() call\ncy.get(""#buttonWhichOpensPopupWithDynamicUrl"").click()\n\n// Make sure that it triggered window.open function call\ncy.get(""@popup"").should(""be.called"")\n\n// Now we can continue integration testing for the new ""popup tab"" inside the same tab\n\nIs there any better way to do this?\n', ""\n// We can remove the offending attribute - target='_blank'\n      // that would normally open content in a new tab.\n      cy.get('#users').invoke('removeAttr', 'target').click()\n\n      // after clicking the <a> we are now navigated to the\n      // new page and we can assert that the url is correct\n      cy.url().should('include', 'users.html')\n\nCypress - tab handling anchor links\n"", ""\nI was able to achieve the same requirement via the following:\nlet newUrl = '';\ncy.window().then((win) => {\n  cy.stub(win, 'open').as('windowOpen').callsFake(url => {\n    newUrl = url;\n  });\n})\n\ncy.get('.open-window-btn').click()\ncy.get('@windowOpen').should('be.called');\ncy.visit(newUrl)\n\n"", '\nHere\'s a solution i\'m using on my project based on ""Cypress using child window""\nCypress Window Helpers (aka. Cypress Tab Helpers)\nThey\'re really popup-windows or child-windows, but i call them tabs for api brevity\ncy.openTab(url, opts)\ncy.tabVisit(url, window_name)\ncy.switchToTab(tab_name)\ncy.closeTab(index_or_name) - pass nothing to close active tab\ncy.closeAllTabs() - except main root window\n\n', ""\nI was recently faced with this issue as well - url for the new tab is dynamic, so I don't know what it is. After much searching, some trial and error, and input from co-workers, resolved by doing the following:\n// AFTER cy.visit()\ncy.window().then((win) => {\n  cy.spy(win, 'open').as('windowOpen'); // 'spy' vs 'stub' lets the new tab still open if you are visually watching it\n});\n// perform action here [for me it was a button being clicked that eventually ended in a window.open]\n// verify the window opened\n// verify the first parameter is a string (this is the dynamic url) and the second is _blank (opens a new window)\ncy.get('@windowOpen').should('be.calledWith', Cypress.sinon.match.string, '_blank');\n\n"", '\nthis is how you can handle tabs in same window..\nuse this code snippet\ncy.xpath(""//a[@href=\'http://www.selenium.dev\']"").invoke(\'removeAttr\',\'target\').click();\n\n']",https://stackoverflow.com/questions/47749956/access-a-new-window-cypress-io,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selenium: Drag and Drop from file system to WebDriver?,"
I have to test a web-application which contains a drag and drop area for uploading files from the local file system. My test environment is based on C#.
For the automation testing I have used Selenium, but it is not possible to drag files from the file system. The upload area is a div tag (no input tag). So what's the best way to do it? AutoIt (is it possible to drop in a web browser)? Sikuli?
",19k,"
            22
        ","['\nIt\'s possible with Selenium alone, but it\'s not simple. It requires to inject a new INPUT element in the page to receive the file through SendKeys. Then, the script needs to simulate the drop by sending the dragenter, dragover, drop events to the targeted area.\nstatic void Main(string[] args)\n{\n    var driver = new ChromeDriver();\n    driver.Url = ""https://react-dropzone.js.org/"";\n\n    IWebElement droparea = driver.FindElementByCssSelector(""[data-preview=\'Basic example\'] [style]"");\n    DropFile(droparea, @""C:\\Users\\florent\\Desktop\\capture.png"");\n\n    driver.Quit();\n}\n\nconst string JS_DROP_FILE = ""for(var b=arguments[0],k=arguments[1],l=arguments[2],c=b.ownerDocument,m=0;;){var e=b.getBoundingClientRect(),g=e.left+(k||e.width/2),h=e.top+(l||e.height/2),f=c.elementFromPoint(g,h);if(f&&b.contains(f))break;if(1<++m)throw b=Error(\'Element not interractable\'),b.code=15,b;b.scrollIntoView({behavior:\'instant\',block:\'center\',inline:\'center\'})}var a=c.createElement(\'INPUT\');a.setAttribute(\'type\',\'file\');a.setAttribute(\'style\',\'position:fixed;z-index:2147483647;left:0;top:0;\');a.onchange=function(){var b={effectAllowed:\'all\',dropEffect:\'none\',types:[\'Files\'],files:this.files,setData:function(){},getData:function(){},clearData:function(){},setDragImage:function(){}};window.DataTransferItemList&&(b.items=Object.setPrototypeOf([Object.setPrototypeOf({kind:\'file\',type:this.files[0].type,file:this.files[0],getAsFile:function(){return this.file},getAsString:function(b){var a=new FileReader;a.onload=function(a){b(a.target.result)};a.readAsText(this.file)}},DataTransferItem.prototype)],DataTransferItemList.prototype));Object.setPrototypeOf(b,DataTransfer.prototype);[\'dragenter\',\'dragover\',\'drop\'].forEach(function(a){var d=c.createEvent(\'DragEvent\');d.initMouseEvent(a,!0,!0,c.defaultView,0,0,0,g,h,!1,!1,!1,!1,0,null);Object.setPrototypeOf(d,null);d.dataTransfer=b;Object.setPrototypeOf(d,DragEvent.prototype);f.dispatchEvent(d)});a.parentElement.removeChild(a)};c.documentElement.appendChild(a);a.getBoundingClientRect();return a;"";\n\nstatic void DropFile(IWebElement target, string filePath, double offsetX = 0, double offsetY = 0)\n{\n    if (!File.Exists(filePath))\n        throw new FileNotFoundException(filePath);\n\n    IWebDriver driver = ((RemoteWebElement)target).WrappedDriver;\n    IJavaScriptExecutor jse = (IJavaScriptExecutor)driver;\n\n    IWebElement input = (IWebElement)jse.ExecuteScript(JS_DROP_FILE, target, offsetX, offsetY);\n    input.SendKeys(filePath);\n}\n\nSource: https://gist.github.com/florentbr/349b1ab024ca9f3de56e6bf8af2ac69e\n', ""\nThe previous answer is correct and works perfectly with the Chrome driver, however might have problems with Mozilla Gecko driver, which throws org.openqa.selenium.ElementNotVisibleException\nIn order to avoid that, remove input.style.display = 'none';\nYou can use input.style.opacity = 0; if you need to make it disappear.\n"", '\nYou can do this with JSExecutor:\npublic void dropFile(File filePath, WebElement target, int offsetX, int offsetY) {\n        if (!filePath.exists())\n            throw new WebDriverException(""File not found: "" + filePath.toString());\n\n        JavascriptExecutor jse = (JavascriptExecutor) driver;\n\n        String JS_DROP_FILE =\n                ""var target = arguments[0],"" +\n                        ""    offsetX = arguments[1],"" +\n                        ""    offsetY = arguments[2],"" +\n                        ""    document = target.ownerDocument || document,"" +\n                        ""    window = document.defaultView || window;"" +\n                        """" +\n                        ""var input = document.createElement(\'INPUT\');"" +\n                        ""input.type = \'file\';"" +\n                        ""input.style.display = \'none\';"" +\n                        ""input.onchange = function () {"" +\n                        ""  var rect = target.getBoundingClientRect(),"" +\n                        ""      x = rect.left + (offsetX || (rect.width >> 1)),"" +\n                        ""      y = rect.top + (offsetY || (rect.height >> 1)),"" +\n                        ""      dataTransfer = { files: this.files };"" +\n                        """" +\n                        ""  [\'dragenter\', \'dragover\', \'drop\'].forEach(function (name) {"" +\n                        ""    var evt = document.createEvent(\'MouseEvent\');"" +\n                        ""    evt.initMouseEvent(name, !0, !0, window, 0, 0, 0, x, y, !1, !1, !1, !1, 0, null);"" +\n                        ""    evt.dataTransfer = dataTransfer;"" +\n                        ""    target.dispatchEvent(evt);"" +\n                        ""  });"" +\n                        """" +\n                        ""  setTimeout(function () { document.body.removeChild(input); }, 25);"" +\n                        ""};"" +\n                        ""document.body.appendChild(input);"" +\n                        ""return input;"";\n\n        WebElement input = (WebElement) jse.executeScript(JS_DROP_FILE, target, offsetX, offsetY);\n        input.sendKeys(filePath.getAbsoluteFile().toString());\n        wait.until(ExpectedConditions.stalenessOf(input));\n    }\n\n', '\nIf you\'re using Selenide:\n    public static void dragAndDropFileUpload(File file, SelenideElement target) throws IOException {\n\n    String inputId = ""seleniumDragAndDropInput"";\n\n    // Create the FileList\n    executeJavaScript(inputId + ""_files = [];"");\n        executeJavaScript(inputId + ""_files.push(new File([new Blob([\'"" + file.getAbsolutePath() + ""\'], {type: \'"" + Files.probeContentType(file.toPath()) + ""\'})], \'"" + file.getName() + ""\'));"");\n\n\n    String targetId = target.getAttribute(""id"");\n\n    // Add an id if the target doesn\'t have one\n    if (targetId == null || targetId.isEmpty()) {\n        targetId = ""seleniumDragAndDropInput_target"";\n        executeJavaScript(""sId=function(e, i){e.id = i;};sId(arguments[0], arguments[1]);"", target, targetId);\n    }\n\n    // Add the item function the the FileList\n    // Create the drop event and dispatch it on the target\n    String initEventJS = inputId + ""_files.item = function (i) {return this[i];};""\n            + ""var eve=document.createEvent(\\""HTMLEvents\\"");""\n            + ""eve.initEvent(\\""drop\\"", true, true);""\n            + ""eve.dataTransfer = {files:seleniumDragAndDropInput_files};""\n            + ""eve.preventDefault = function () {};""\n            + ""eve.type = \\""drop\\"";""\n            + ""document.getElementById(\'"" + targetId + ""\').dispatchEvent(eve);"";\n\n    executeJavaScript(initEventJS);\n\n    if (targetId == ""seleniumDragAndDropInput_target"") {\n        executeJavaScript(""document.getElementById(\'seleniumDragAndDropInput_target\').id = null"");\n    }\n}\n\n']",https://stackoverflow.com/questions/38829153/selenium-drag-and-drop-from-file-system-to-webdriver,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run the current application as Single Instance and show the previous instance,"
I just implemented this code that is guarding the Single Instance of the Application, in order to not run the application twice.
Now I am wondering how I can show the original Application process that is already running.
Here is my code in the program class:
static class Program
{
    [STAThread]
    static void Main()
    {
        const string appName = ""MyappName"";
        bool createdNew;
        mutex = new Mutex(true, appName, out createdNew);

        Application.EnableVisualStyles();
        Application.SetCompatibleTextRenderingDefault(false);
        Form form = new Form1();

        if (!createdNew)
        {
            form.Show();  <<=========================== NOT WORKING
            form.Visible = true; <<===================== None
            form.TopMost = true; <<===================== of
            form.BringToFront(); <<===================== these working!
            form.WindowState = FormWindowState.Maximized;
            return;
        }
        Application.Run(form);
    }        private static Mutex mutex = null;
}

",5k,"
            5
        ","['\nI propose you a different method, using a combination of the System.Threading.Mutex class and UIAutomation AutomationElement class.  \nA Mutex can be, as you already know, a simple string. You can assign an application a Mutex in the form of a GUID, but it can be anything else.\nLet\'s assume this is the current Application Mutex:  \nstring ApplicationMutex = ""BcFFcd23-3456-6543-Fc44abcd1234"";\n//Or\nstring ApplicationMutex = ""Global\\BcFFcd23-3456-6543-Fc44abcd1234"";\n\nNote:\nUse the ""Global\\"" Prefix to define the scope of the Mutex. If no prefix is specified, the ""Local\\"" prefix is assumed and used instead. This will prevent a single instance of the process when multiple desktops are active or Terminal Services is running on the server.  \nIf we want to verify whether another running Process has already registered the same Mutex, we try to register our Mutex and if it fails, another instance of our Application is already running.\nWe let the user know that the Application supports only a single instance, then switch to the running process, showing its interface and finally exit the duplicate Application, disposing of the Mutex.  \nThe method to activate a previous instance of the Application may vary based on the type of the Application, but only some details change.\nWe can use Process..GetProcesses() to retrieve a list of the running processes and verify if one of them has the same details as ours.  \nHere, you have a windowed Application (it has an UI), so it\'s already possible to filter the list, excluding those processes that do not have a MainWindowHandle.  \nProcess[] windowedProcesses = \n    Process.GetProcesses().Where(p => p.MainWindowHandle != IntPtr.Zero).ToArray();\n\nTo identify the right one, we could test if the Process.ProcessName is the same.\nBut this name is tied to the executable name. If the file name changes (someone changes it for some reason), we will never identify the Process this way.  \nOne possible way to identify the right Process is to test the Process.MainModule.FileVersionInfo.ProductName and check whether it\'s the same.  \nWhen found, it\'s possible to bring the original Application to front with an AutomationElement created using the MainWindowHandle of the identified Process.\nThe AutomationElement can automate different Patterns (sort of controls that provide automation functionalities for UI elements).\nA WindowPattern allows to control a window-base control (the Platform is irrelevant, could be a WinForms\' Form or a WPF\'s Window).  \nAutomationElement element = AutomationElement.FromHandle(process.MainWindowHandle);\nWindowPattern wPattern = element.GetCurrentPattern(WindowPattern.Pattern) as WindowPattern;\nwPattern.SetWindowVisualState(WindowVisualState.Normal);\n\n\nTo use the UIAutomation functionalities, you have to add these refereneces in your Project:\n  - UIAutomationClient\n  - UIAutomationTypes\n\nUPDATE:\nSince the Application\'s Form might be hidden, Process.GetProcesses() will not find it\'s Window handle, thus AutomationElement.FromHandle() cannot be used to identify the Form Window.  \nA possible workaround, without dismissing the UIAutomation ""pattern"", is to register an Automation event, using Automation.AddAutomationEventHandler, which allows to receive a notification when an UI Automation events occurs, such as a new Window is about to be shown (a Program is run).  \nThe event is registerd only if the Application needs to run as Single Instance.  When the event is raised, the new Process AutomationElement Name (the Windows Title Text) is compared to the current and, if it\'s the same, the hidden Form will un-hide and show itself in Normal state.\nAs a fail-safe measure, we present an information MessageBox. The MessageBox caption has the same caption as the Application MainForm.\n(Tested with a Form with its WindowsState set to Minimized and its Visible property set to false).  \n\nAfter the orginal Process has been brought to front, we just neeed to close the current thread and release the resources we created (mainly the Mutex, in this case).  \nusing System;\nusing System.Diagnostics;\nusing System.Linq;\nusing System.Threading;\nusing System.Windows.Automation;\nusing System.Windows.Forms;\n\nstatic class Program\n{\n    static Mutex mutex = null;\n\n    [STAThread]\n    static void Main()\n    {\n        Application.ThreadExit += ThreadOnExit;\n        string applicationMutex = @""Global\\BcFFcd23-3456-6543-Fc44abcd1234"";\n        mutex = new Mutex(true, applicationMutex);\n        bool singleInstance = mutex.WaitOne(0, false);\n        if (!singleInstance)\n        {\n            string appProductName = Process.GetCurrentProcess().MainModule.FileVersionInfo.ProductName;\n            Process[] windowedProcesses = \n                Process.GetProcesses().Where(p => p.MainWindowHandle != IntPtr.Zero).ToArray();\n\n            foreach (Process process in windowedProcesses.Where(p => p.MainModule.FileVersionInfo.ProductName == appProductName))\n            {\n                if (process.Id != Process.GetCurrentProcess().Id)\n                {\n                    AutomationElement wElement = AutomationElement.FromHandle(process.MainWindowHandle);\n                    if (wElement.Current.IsOffscreen)\n                    {\n                        WindowPattern wPattern = wElement.GetCurrentPattern(WindowPattern.Pattern) as WindowPattern;\n                        #if DEBUG\n                        WindowInteractionState state = wPattern.Current.WindowInteractionState;\n                        Debug.Assert(!(state == WindowInteractionState.NotResponding), ""The application is not responding"");\n                        Debug.Assert(!(state == WindowInteractionState.BlockedByModalWindow), ""Main Window blocked by a Modal Window"");\n                        #endif\n                        wPattern.SetWindowVisualState(WindowVisualState.Normal);\n                        break;\n                    }\n                }\n            }\n            Thread.Sleep(200);\n            MessageBox.Show(""Application already running"", ""MyApplicationName"",\n                            MessageBoxButtons.OK, MessageBoxIcon.Information, \n                            MessageBoxDefaultButton.Button1, MessageBoxOptions.ServiceNotification);\n        }\n\n        if (SingleInstance) {\n            Application.EnableVisualStyles();\n            Application.SetCompatibleTextRenderingDefault(false);\n            Application.Run(new MyAppMainForm());\n        }\n        else {\n            Application.ExitThread();\n        }\n    }\n    private static void ThreadOnExit(object s, EventArgs e)\n    {\n        mutex.Dispose();\n        Application.ThreadExit -= ThreadOnExit;\n        Application.Exit();\n    }\n}\n\nIn the Application MainForm constructor:\n(this is used in case the Application\'s Main Window is hidden when a new instance is run, hence the procedure in Program.cs cannot find its handle)  \npublic partial class MyAppMainForm : Form\n{\n    public MyAppMainForm()\n    {\n        InitializeComponent();\n        Automation.AddAutomationEventHandler(WindowPattern.WindowOpenedEvent, \n                                             AutomationElement.RootElement, \n                                             TreeScope.Subtree, (uiElm, evt) =>\n        {\n            AutomationElement element = uiElm as AutomationElement;\n            string windowText = element.Current.Name;\n            if (element.Current.ProcessId != Process.GetCurrentProcess().Id && windowText == this.Text)\n            {\n                this.BeginInvoke(new MethodInvoker(() =>\n                {\n                    this.WindowState = FormWindowState.Normal;\n                    this.Show();\n                }));\n            }\n        });\n    }    \n}\n\n', '\nRun Only One time :\nstatic class Program\n{    \n    [STAThread]\n    static void Main()\n    {\n        bool createdNew = true;\n        using (Mutex mutex = new Mutex(true, ""samplename"", out createdNew))\n        {\n            if (createdNew)\n            {\n                Application.EnableVisualStyles();\n                Application.SetCompatibleTextRenderingDefault(false);\n                Application.ThreadException += new ThreadExceptionEventHandler(Application_ThreadException);\n                AppDomain.CurrentDomain.UnhandledException += new UnhandledExceptionEventHandler(CurrentDomain_UnhandledException);\n                Application.Run(new Form1());\n            }\n            else\n            {\n                ProcessUtils.SetFocusToPreviousInstance(""samplename"");\n            }\n        }\n    }\n\n    private static void CurrentDomain_UnhandledException(object sender, UnhandledExceptionEventArgs e)\n    {\n    }\n\n    private static void Application_ThreadException(object sender, ThreadExceptionEventArgs e)\n    {\n    }\n}\n\nProcessUtils :\n   public static class ProcessUtils\n    {\n        [DllImport(""user32.dll"", SetLastError = true)]\n        static extern IntPtr FindWindow(string lpClassName, string lpWindowName);\n\n        [DllImport(""user32.dll"")]\n        [return: MarshalAs(UnmanagedType.Bool)]\n        static extern bool SetForegroundWindow(IntPtr hWnd);\n\n        [DllImport(""user32.dll"")]\n        static extern bool IsIconic(IntPtr hWnd);\n\n        [DllImport(""user32.dll"")]\n        static extern bool ShowWindow(IntPtr hWnd, int nCmdShow);\n\n        const int SW_RESTORE = 9;\n\n        [DllImport(""user32.dll"")]\n        static extern IntPtr GetLastActivePopup(IntPtr hWnd);\n\n        [DllImport(""user32.dll"")]\n        static extern bool IsWindowEnabled(IntPtr hWnd);\n\n\n        public static void SetFocusToPreviousInstance(string windowCaption)\n        {\n\n            IntPtr hWnd = FindWindow(null, windowCaption);\n\n\n            if (hWnd != null)\n            {\n\n                IntPtr hPopupWnd = GetLastActivePopup(hWnd);\n\n\n\n                if (hPopupWnd != null && IsWindowEnabled(hPopupWnd))\n                {\n                    hWnd = hPopupWnd;\n                }\n\n                SetForegroundWindow(hWnd);\n\n\n                if (IsIconic(hWnd))\n                {\n                    ShowWindow(hWnd, SW_RESTORE);\n                }\n            }\n        }\n    }\n\n\nNormal Run :\nstatic class Program\n{\n    [STAThread]\n    static void Main()\n    {\n        Application.EnableVisualStyles();\n        Application.SetCompatibleTextRenderingDefault(false);\n        Application.Run(new Form1());\n    }\n}\n\n', '\nIf you are still looking for an answer. There is a good example here that\nuses windows messages to restore the previous instance. It work even if the first instance is minimized contrary to FindWindow witch does not work in that case. \n']",https://stackoverflow.com/questions/50552592/run-the-current-application-as-single-instance-and-show-the-previous-instance,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python code to automate desktop activities in windows,"
I want to automate desktop activities in Windows environment using Python. How it can be done? Some examples will also be helpful.
By desktop activities, I mean actions such as taking control over mouse and keyboard, access active windows properties, double-click on an icon on the desktop, minimize and maximize windows, enter data to an input popup window through keyboard, etc.
",85k,"
            27
        ","[""\nHave a look at SIKULI.\n\nSikuli is a visual technology to automate and test graphical user\ninterfaces (GUI) using images (screenshots).\n\nSIKULI uses a very clever combination of taking screenshots, and embedding them into your python (it's jython, actually) script.\n\nTake screenshots:\n\nand use them in your code:\n\n"", '\nYou can try Automa.\nIt\'s a Windows GUI automation tool written in Python which is very simple to use. For example, you can do the following:\n# to double click on an icon on the desktop\ndoubleclick(""Recycle Bin"")\n\n# to maximize\nclick(""Maximize"")\n\n# to input some text and press ENTER\nwrite(""Some text"", into=""Label of the text field"")\npress(ENTER)\n\nThe full list of available commands can be found here.\nDisclaimer: I\'m one of Automa\'s developers.\n', '\nThere are different ways of automating user interfaces in Windows that can be accessed via Python (using ctypes or some of the Python windows bindings):\n\nRaw windows APIs -- Get/SetCursorPos for the mouse, HWND APIs like GetFocus and GetForegroundWindow\nAutoIt -- an automation scripting language: Calling AutoIt Functions in Python\nMicrosoft Active Accessibility (MSAA) / WinEvent -- an API for interrogating a UI through the accessibility APIs in Win95.\nUI/Automation (UIA) -- a replacement for MSAA introduced in Vista (available for XP SP3 IIRC).\n\nAutomating a user interface to test it is a non-trivial task. There are a lot of gotchas that can trip you up.\nI would suggest testing your automation framework in an automated way so you can verify that it works on the platforms you are testing (to identify failures in the automation API vs failures in the application).\nAnother consideration is how to deal with localization. Note also that the names for Minimize/Maximize/... are localized as well, and can be in a different language to the application (system vs. user locale)!\nIn pseudo-code, an MSAA program to minimize an application would look something like:\nwindow = AccessibleObjectFromWindow(FindWindow(""My Window""))\ntitlebar = [x for x in window.AccessibleChildren if x.accRole == TitleBar]\nminimize = [x for x in titlebar[0].AccessibleChildren if x.Name == ""Minimize""]\nif len(minimize) != 0: # may already be minimized\n    mimimize[0].accDoDefaultAction()\n\nMSAA accessible items are stored as (object: IAccessible, childId: int) pairs. Care is needed here to get the calls correct (e.g. get_accChildCount only uses the IAccessible, so when childId is not 0 you must return 0 instead of calling get_accChildCount)!\nIAccessible calls can return different error codes to indicate ""this object does not support this property"" -- e.g. DISP_E_MEMBERNOTFOUND or E_NOTIMPL.\nBe aware of the state of the window. If the window is maximized then minimized, restore will restore the window to its maximized state, so you need to restore it again to get it back to the normal/windowed state.\nThe MSAA and UIA APIs don\'t support right mouse button clicks, so you need to use a Win32 API to trigger it.\nThe MSAA model does not support treeview heirarchy information -- it displays it as a flat list. On the other hand, UIA will only enumerate elements that are visible so you will not be able to access elements in the UIA tree that are collapsed.\n', '\nYou can use PyAutoGUI which provide a cross-platform Python way to perform GUI automation.\nMouse Control\nHere is a simple code to move the mouse to the middle of the screen:\nimport pyautogui\nscreenWidth, screenHeight = pyautogui.size()\npyautogui.moveTo(screenWidth / 2, screenHeight / 2)\n\nRelated question: Controlling mouse with Python.\nKeyboard Control\nExample:\npyautogui.typewrite(\'Hello world!\')                 # prints out ""Hello world!"" instantly\npyautogui.typewrite(\'Hello world!\', interval=0.25)  # prints out ""Hello world!"" with a quarter second delay after each character\n\nMessage Box Functions\nIt provides JavaScript-style message boxes.\nAnd other.\n\nFor other suggestions, check: Python GUI automation library for simulating user interaction in apps.\n', '\nTake a look at BotCity Framework, an open-source RPA framework.\xa0\nIt\'s just python (no intermediary code, no jython, etc).\nThe example below executes SAP and logs in:\nfrom botcity.core import DesktopBot\nfrom botcity.maestro import AlertType, AutomationTaskFinishStatus, Column\n\nclass Bot(DesktopBot):\n    def action(self, execution):\n        self.execute(""saplogon.exe"")\n        \n        # #{image:""login""}\n    \n        if not self.find( ""user"", matching=0.97, waiting_time=10000):\n            self.not_found(""user"")\n        self.click_relative(172, 5)\n        \n        self.paste(user)\n        self.tab()\n        self.paste(pass)\n        self.enter()\n        \nif __name__ == \'__main__\':\n    Bot.main()\n\nAs Sikuli, you have a tool to crop elements and have visual clues about the interface and UI elements. But in this case, it\'s a tool for editing .py files (not intermediary code) so you can use any python lib in your automation.\n\n', ""\nYou can try ClointFusion\nIt's again a Python based RPA platform which internally makes use of PyAutoGUI among other packages.\nIt has a friendly Browser based Drag & Drop BOT Builder: DOST\nYou can find more than 100 easy to use functions:\n\n6 gui functions to take any input from user\n4 functions on Mouse Operations\n6 functions on Window Operations (works only in Windows OS)\n5 functions on Window Objects (works only in Windows OS)\n8 functions on Folder Operations\n28 functions on Excel Operations\n3 functions on Keyboard Operations\n5 functions on Screenscraping Operations\n11 functions on Browser Operations\n4 functions on Alert Messages\n3 functions on String Operations\nLoads of miscellaneous functions related to emoji, capture photo, flash (pop-up) messages etc\n\nDisclaimer: I'm one of developers of ClointFusion\n""]",https://stackoverflow.com/questions/11825322/python-code-to-automate-desktop-activities-in-windows,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System.Windows.Automation is extremely slow,"
System.Windows.Automation is EXTREMELY slow. 
I execute:
element.FindAll(TreeScope.Children, Condition.TrueCondition);

Obtaining only 30 child elements may take 1000ms on a very fast computer. 
I have even seen it hanging forever while getting the child elements of a Tree in a QT application.
Is this a known problem?
I cannot find any usefull answer after googling a lot.
",6k,"
            4
        ","['\nSystem.Windows.Automation is EXTREMELY slow. \nSystem.Windows.Automation is full of bugs. It may not return all children of an AutomationElement, which is a very severe bug.\nApart from that the implementation is not thread safe.\nSystem.Windows.Automation is deprecated. Do not use it!\nIn the MSDN you find the following note:\n\nUI Automation was first available in Windows XP as part of the\n  Microsoft .NET Framework. Although an unmanaged C++ API was also\n  published at that time, the usefulness of client functions was limited\n  because of interoperability issues. For Windows 7, the API has been\n  rewritten in the Component Object Model (COM).\n   Although the library functions introduced in the earlier version of\n  UI Automation are still documented, they should not be used in new\n  applications.\n\nThe solution to slow performance is to use the new IUIAutomationElement COM interface instead of the old System.Windows.Automation C# interface. After that the code will be running lightning fast! \nApart from that the new interface offers much more patterns and Microsoft is extending it continously. In the Windows 10 SDK (UIAutomationClient.h and UIAutomationCore.h) several patterns and properties have been added which are not available in the .NET Automation framework.\nThe following patterns are available in the COM version of UIAutomation which do not exist in System.Windows.Automation:\n\nIUIAutomationLegacyIAccessiblePattern\nIUIAutomationObjectModelPattern\nIUIAutomationAnnotationPattern\nIUIAutomationTextPattern2\nIUIAutomationStylesPattern\nIUIAutomationSpreadsheetPattern\nIUIAutomationSpreadsheetItemPattern\nIUIAutomationTransformPattern2\nIUIAutomationTextChildPattern\nIUIAutomationDragPattern\nIUIAutomationDropTargetPattern\nIUIAutomationTextEditPattern\nIUIAutomationCustomNavigationPattern\n\nAdditionally the following Control types have been added:\n\nAppBar\nSemanticZoom\n\nAdditionally the following Element\'s have been added:\n\nIUIAutomationElement2\nIUIAutomationElement3\nIUIAutomationElement4\n\nAnd what concerns the bugs: The new COM UIAutomation Framework is very well designed and I could not find bugs on the client side of the framework which is a great progress compared to System.Windows.Automation. But several missing features and even bugs on the server side of the framework. On the server side each GUI framework must implement an UIAutomation provider (see MSDN: Interfaces for Providers). So these problems differ depending on what type of application you are automating because each GUI framework has it\'s own problems:\nIn the Native Windows GUI features are missing: Lots of controls do not implement the patterns that they should implement. For example a SplitButton in a native Toolbar should implement the Invoke pattern to click the button and the ExpandCollapse pattern to open the drop-down menu. But the ExpandCollapse pattern is missing which makes it difficult to use SplitButtons. If you obtain a Toolbar SplitButton by IUIAutomation->ElementFromPoint() and then ask for it\'s parent you will get a crippled element. And the Pager control cannot be automated at all.\nAlso in WPF applications there are controls that are implemented buggy by Microsoft: For example if you have a Calendar control you see two buttons at the top to switch to the next/previous month. If you execute the Invoke pattern on these buttons you will get an UIA_E_NOTSUPPORTED error. But this is not a bug on the client side of the framework, because for other buttons the Invoke pattern works correctly. This is a bug in the WPF Automation server. And if you test IUIAutomationTextRange with a WPF RichTextBox, you will find that several commands are not implemented: Select() and ScrollIntoView() do simply nothing. \nFor .NET Forms applications Microsoft did not make much effort to support them. The .NET Calendar control cannot be automated at all. The entire control is not even recognized as Calendar. It has the ControlType ""Pane"" with no child elements in it. The same applies to the DateTimePicker. And for complex controls like DataGrid and PropertyGrid the only implemented pattern is LegacyIAccessible which is a poor support. These controls should implement at least the Table and the Grid and the ScrollItem pattern.\nAlso Internet Explorer cannot be automated because elements outside the visible area cannot be scrolled automatically into view due to missing coordinates. (The Bounds are returned as an empty rectangle) And the ScrollItem pattern is not implemented. (Yes, I know that Internet Explorer has been replaced with Edge in Windows 10, but the UIAutomation framework exists since Windows 7 and Microsoft did not implement a usefull automation support in Internet Explorer in all these years)\nI saw even complete crashes of the automated application. For example Visual Studio and TotalCommander will crash if you execute certain automation commands on a certain control. Here - once again - the bug lies in the server side implementation of the framework.\nSummary: We have a great framework with limited usefullness. The Microsoft team that developed the new UIAutomation framework did a great job, but the other areas in Microsoft (the native GUI, WPF, .NET and Internet Explorer team) do not support this framework. This is very sad because only a small effort would have to be made to offer a better functionality. But it seems that the users who use UIAutomation in the first place (handicapped people) are not a profitable market.\n']",https://stackoverflow.com/questions/41768046/system-windows-automation-is-extremely-slow,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"What's a good, if any, .NET Windows automation library?","
I'm looking for a library that can be used in native .NET code, just like any .NET assembly. The purpose of the library must be to automate Windows (push a button, select a window, send keys, record & playback, that sort of thing).
So: the library is supposed to be used natively in .NET, but the automation itself must be able to target any native or .NET Windows application that can receive user input.

Suggestions so far:

benPearce suggested AutoIt. It has a DLL, which is native Win32 but not native .NET and cannot be used without use of .NET Interop.
Chris Dunaway suggested Global Mouse Keyboard Lib. This came closest, but is not an automation lib. It just helps setting up keyboard and mouse hooks.
pm100 suggested Microsoft's WPF UI Automation. This one is pretty good, albeit that it's not available if you develop in .NET 2.0 and it requires the WPF to be installed on the system. It can, however, automate everything from Win32 apps to HTML in a browser.
JasonTrue suggested WebAI from ArtOfTest. This is a testing framework mainly geared towards browsers and web applications. It is unfortunately not well suitable for use for Windows automation.

If nothing else appears available, I'll probably choose Microsoft's UI Automation and upgrade any projects that require it that are still in .NET 2.0 to .NET 3.5, if possible. But I hope for a more widely applicable automation framework (.NET prior to 2.0 does not need to be supported).
",23k,"
            39
        ","['\nHave you looked at the White framework?\n', '\nI have used AutoIt in the past with success.\n', ""\nmicrosoft's own built in one is fine\nhttp://msdn.microsoft.com/en-us/library/ms747327.aspx\nnot restricted to wpf as some seem to think.\n"", ""\nIf you haven't seen it yet, and a commercial library is acceptable, you might check out Ranorex:\nhttp://www.ranorex.com/\nI used Ranorex 1.5 quite a bit to write small C# UI automation utilities.  It was pretty effective!  Development seemed faster compared to using the MS UI Automation API directly, since Ranorex has a lot of useful convenience methods already available.\nI haven't used Ranorex 2 very much yet, though.\nIn Ranorex 1.5, there was also support for traditional Win32 development in C++, but I didn't use it.  As far as I know, that's still available in Ranorex 2.\nI can't speak to the quality of the record/playback support in Ranorex since I never used that feature.\nOne final plus: Their support team was really responsive and helpful anytime I emailed them.\n"", '\nThis library is pretty interesting and is fairly simple.  Perhaps it will help you.\n', '\nCheck out Tools for automated GUI testing on windows\n', '\nI would still suggest FlaUI for autoamating .Net Desktop,Mobile apps. Its based on Microsoft UIA libraries and have support for external controls like the DevExpress Grid too\nMoreover, it is built on top of TestStack.White so indeed a very good library and has a github page also\n', '\nI have used WebAii from ArtOfTest with a fair degree of success in automating integration testing for a Silverlight app. It also supports WinForms and Web applications.\nMicrosoft UI Automation, the successor to Active Accessibility, can do almost all of the Windows UI automation you would need.\n', ""\nHow about CSharpScript, here's an article about it on Codeproject, and here's the link to the main website. Furthermore, it is familiar C#, scripted which can be used to automate anything.\nHope this helps,\nBest regards,\nTom.\n""]",https://stackoverflow.com/questions/2052915/whats-a-good-if-any-net-windows-automation-library,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Is Selenium slow, or is my code wrong?","
So I'm trying to login to Quora using Python and then scrape some stuff.
I'm using Selenium to login to the site. Here's my code:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys

driver = webdriver.Firefox()
driver.get('http://www.quora.com/')

username = driver.find_element_by_name('email')
password = driver.find_element_by_name('password')

username.send_keys('email')
password.send_keys('password')
password.send_keys(Keys.RETURN)

driver.close()

Now the questions:

It took ~4 minutes to find and fill the login form, which painfully slow. Is there something I can do to speed up the process?
When it did login, how do I make sure there were no errors? In other words, how do I check the response code?
How do I save cookies with selenium so I can continue scraping once I login?
If there is no way to make selenium faster, is there any other alternative for logging in? (Quora doesn't have an API)

",45k,"
            18
        ","['\nI had a similar problem with very slow find_elements_xxx calls in Python selenium using the ChromeDriver. I eventually tracked down the trouble to a driver.implicitly_wait() call I made prior to my find_element_xxx() calls; when I took it out, my find_element_xxx() calls ran quickly.\nNow, I know those elements were there when I did the find_elements_xxx() calls. So I cannot imagine why the implicit_wait should have affected the speed of those operations, but it did.\n', '\n\nI have been there, selenium is slow. It may not be as slow as 4 min to fill a form. I then started using phantomjs, which is much faster than firefox, since it is headless. You can simply replace Firefox() with PhantomJS() in the webdriver line after installing latest phantomjs.\nTo check that you have login you can assert for some element which is displayed after login.\nAs long as you do not quit your driver, cookies will be available to follow links\nYou can try using urllib and post directly to the login link. You can use cookiejar to save cookies. You can even simply save cookie, after all, a cookie is simply a string in http header\n\n', '\nYou can fasten your form filling by using your own setAttribute method, here is code for java for it\npublic void setAttribute(By locator, String attribute, String value) {\n    ((JavascriptExecutor) getDriver()).executeScript(""arguments[0].setAttribute(\'"" + attribute\n            + ""\',arguments[1]);"",\n            getElement(locator),\n            value);\n}\n\n', ""\nRunning the web driver headlessly should improve its execution speed to some degree.\nfrom selenium.webdriver import Firefox\nfrom selenium.webdriver.firefox.options import Options\n\noptions = Options()\noptions.add_argument('-headless')\nbrowser = webdriver.Firefox(firefox_options=options)\n\nbrowser.get('https://google.com/')\nbrowser.close()\n\n"", '\nFor Windows 7 and IEDRIVER with Python Selenium, Ending the Windows Command Line and restarting it cured my issue.  \nI was having trouble with find_element..clicks.  They were taking 30 seconds plus a little bit.  Here\'s the type of code I have including capturing how long to run.\ntimeStamp = time.time()\nelem = driver.find_element_by_css_selector(clickDown).click()\nprint(""1 took:"",time.time() - timeStamp)\n\ntimeStamp = time.time()\nelem = driver.find_element_by_id(""cSelect32"").click()\nprint(""2 took:"",time.time() - timeStamp)\n\nThat was recording about 31 seconds for each click.  After ending the command line and restarting it (which does end any IEDRIVERSERVER.exe processes), it was 1 second per click. \n', '\nI have changed locators and this works fast. Also, I have added working with cookies. Check the code below: \nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.keys import Keys\nimport pickle\n\n\ndriver = webdriver.Firefox()\ndriver.get(\'http://www.quora.com/\')\nwait = WebDriverWait(driver, 5)\nusername = wait.until(EC.presence_of_element_located((By.XPATH, \'//div[@class=""login""]//input[@name=""email""]\')))\npassword = wait.until(EC.presence_of_element_located((By.XPATH, \'//div[@class=""login""]//input[@name=""password""]\')))\n\nusername.send_keys(\'email\')\npassword.send_keys(\'password\')\npassword.send_keys(Keys.RETURN)\n\nwait.until(EC.presence_of_element_located((By.XPATH, \'//span[text()=""Add Question""]\'))) # checking that user logged in\npickle.dump( driver.get_cookies() , open(""cookies.pkl"",""wb"")) # saving cookies\ndriver.close()\n\nWe have saved cookies and now we will apply them in a new browser:\ndriver = webdriver.Firefox()\ndriver.get(\'http://www.quora.com/\')\ncookies = pickle.load(open(""cookies.pkl"", ""rb""))\nfor cookie in cookies:\n    driver.add_cookie(cookie)\ndriver.get(\'http://www.quora.com/\')\n\nHope, this will help.\n']",https://stackoverflow.com/questions/17462884/is-selenium-slow-or-is-my-code-wrong,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get Current Activity in Espresso android,"
In case of a test that crosses multiple activities, is there a way to get current activity? 
getActivtiy() method just gives one activity that was used to start the test.
I tried something like below,
public Activity getCurrentActivity() {
    Activity activity = null;
    ActivityManager am = (ActivityManager) this.getActivity().getSystemService(Context.ACTIVITY_SERVICE);
    List<ActivityManager.RunningTaskInfo> taskInfo = am.getRunningTasks(1);
    try {
        Class<?> myClass = taskInfo.get(0).topActivity.getClass();
        activity = (Activity) myClass.newInstance();
    }
    catch (Exception e) {

    }
    return activity;
}

but I get null object.
",46k,"
            55
        ","['\nIn Espresso, you can use ActivityLifecycleMonitorRegistry but it is not officially supported, so it may not work in future versions.\nHere is how it works:\nActivity getCurrentActivity() throws Throwable {\n  getInstrumentation().waitForIdleSync();\n  final Activity[] activity = new Activity[1];\n  runTestOnUiThread(new Runnable() {\n    @Override\n    public void run() {\n      java.util.Collection<Activity> activities = ActivityLifecycleMonitorRegistry.getInstance().getActivitiesInStage(Stage.RESUMED);\n      activity[0] = Iterables.getOnlyElement(activities);\n  }});\n  return activity[0];\n}\n\n', ""\nIf all you need is to make the check against current Activity, use may get along with native Espresso one-liner to check that expected intent was launched:\nintended(hasComponent(new ComponentName(getTargetContext(), ExpectedActivity.class)));\n\nEspresso will also show you the intents fired in the meanwhile if not matching yours.\nThe only setup you need is to replace ActivityTestRule with IntentsTestRule in the test to let it keep track of the intents launching. And make sure this library is in your build.gradle dependencies:\nandroidTestCompile 'com.android.support.test.espresso:espresso-intents:2.2.1'\n\n"", ""\nI like @Ryan's version as it doesn't use undocumented internals, but you can write this even shorter:\nprivate Activity getCurrentActivity() {\n    final Activity[] activity = new Activity[1];\n    onView(isRoot()).check(new ViewAssertion() {\n        @Override\n        public void check(View view, NoMatchingViewException noViewFoundException) {\n            activity[0] = (Activity) view.getContext();\n        }\n    });\n    return activity[0];\n}\n\nPlease be aware, though that this will not work when running your tests in Firebase Test Lab. That fails with\njava.lang.ClassCastException: com.android.internal.policy.DecorContext cannot be cast to android.app.Activity\n\n"", '\nThe Android team has replaced ActivityTestRule with ActivityScenario. We could do activityTestRule.getActivity() with ActivityTestRule but not with ActivityScenario. Here is my work around solution for getting an Activity from ActivityScenario (inspired by @Ryan and @Fabian solutions) \n@get:Rule\nvar activityRule = ActivityScenarioRule(MainActivity::class.java)\n...\nprivate fun getActivity(): Activity? {\n  var activity: Activity? = null\n  activityRule.scenario.onActivity {\n    activity = it\n  }\n  return activity\n}\n\n', ""\nI couldn't get any of the other solutions to work, so I ended up having to do this:\nDeclare your ActivityTestRule:\n@Rule\npublic ActivityTestRule<MainActivity> mainActivityTestRule =\n        new ActivityTestRule<>(MainActivity.class);\n\nDeclare a final Activity array to store your activities:\nprivate final Activity[] currentActivity = new Activity[1];\n\nAdd a helper method to register with the application context to get lifecycle updates:\nprivate void monitorCurrentActivity() {\n    mainActivityTestRule.getActivity().getApplication()\n            .registerActivityLifecycleCallbacks(new Application.ActivityLifecycleCallbacks() {\n                @Override\n                public void onActivityCreated(final Activity activity, final Bundle savedInstanceState) { }\n\n                @Override\n                public void onActivityStarted(final Activity activity) { }\n\n                @Override\n                public void onActivityResumed(final Activity activity) {\n                    currentActivity[0] = activity;\n                }\n\n                @Override\n                public void onActivityPaused(final Activity activity) { }\n\n                @Override\n                public void onActivityStopped(final Activity activity) { }\n\n                @Override\n                public void onActivitySaveInstanceState(final Activity activity, final Bundle outState) { }\n\n                @Override\n                public void onActivityDestroyed(final Activity activity) { }\n            });\n}\n\nAdd a helper method to get the current activity\nprivate Activity getCurrentActivity() {\n    return currentActivity[0];\n}\n\nSo, once you've launched your first activity, just call monitorCurrentActivity() and then whenever you need a reference to the current activity you just call getCurrentActivity()\n"", '\npublic static Activity getActivity() {\n    final Activity[] currentActivity = new Activity[1];\n    Espresso.onView(AllOf.allOf(ViewMatchers.withId(android.R.id.content), isDisplayed())).perform(new ViewAction() {\n        @Override\n        public Matcher<View> getConstraints() {\n            return isAssignableFrom(View.class);\n        }\n\n        @Override\n        public String getDescription() {\n            return ""getting text from a TextView"";\n        }\n\n        @Override\n        public void perform(UiController uiController, View view) {\n            if (view.getContext() instanceof Activity) {\n                Activity activity1 = ((Activity)view.getContext());\n                currentActivity[0] = activity1;\n            }\n        }\n    });\n    return currentActivity[0];\n}\n\n', '\nI improved @Fabian Streitel answer so you can use this method without ClassCastException\npublic static Activity getCurrentActivity() {\n    final Activity[] activity = new Activity[1];\n\n    onView(isRoot()).check((view, noViewFoundException) -> {\n\n        View checkedView = view;\n\n        while (checkedView instanceof ViewGroup && ((ViewGroup) checkedView).getChildCount() > 0) {\n\n            checkedView = ((ViewGroup) checkedView).getChildAt(0);\n\n            if (checkedView.getContext() instanceof Activity) {\n                activity[0] = (Activity) checkedView.getContext();\n                return;\n            }\n        }\n    });\n    return activity[0];\n}\n\n', '\nBased on https://stackoverflow.com/a/50762439/6007104 here\'s a Kotlin version of a generic util for accessing current Activity:\nclass CurrentActivityDelegate(application: Application) {\n    private var cachedActivity: Activity? = null\n\n    init {\n        monitorCurrentActivity(application)\n    }\n\n    fun getCurrentActivity() = cachedActivity\n\n    private fun monitorCurrentActivity(application: Application) {\n        application.registerActivityLifecycleCallbacks(\n            object : Application.ActivityLifecycleCallbacks {\n                override fun onActivityResumed(activity: Activity) {\n                    cachedActivity = activity\n                    Log.i(TAG, ""Current activity updated: ${activity::class.simpleName}"")\n                }\n\n                override fun onActivityCreated(activity: Activity?, savedInstanceState: Bundle?) {}\n                override fun onActivityStarted(activity: Activity?) {}\n                override fun onActivityPaused(activity: Activity?) {}\n                override fun onActivityStopped(activity: Activity?) {}\n                override fun onActivitySaveInstanceState(activity: Activity?, outState: Bundle?) {}\n                override fun onActivityDestroyed(activity: Activity?) {}\n            })\n    }\n}\n\nAnd then simply use like so:\n@Before\nfun setup() {\n    currentActivityDelegate = CurrentActivityDelegate(activityTestRule.activity.application)\n}\n\n', ""\nIf you have the only Activity in your test case, you can do:\n1. declare you test Rule\n@Rule\npublic ActivityTestRule<TestActivity> mActivityTestRule = new ActivityTestRule<>(TestActivity.class);\n\n2. get you Activity:\nmActivityTestRule.getActivity()\n\nThat's a piece of pie!\n"", '\nSolution proposed by @lacton didn\'t work for me, probably because activity was not in a state that was reported by ActivityLifecycleMonitorRegistry.\nI even tried Stage.PRE_ON_CREATE still didn\'t get any activity.\nNote: I could not use the ActivityTestRule or IntentTestRule because I was starting my activity using activitiy-alias and didn\'t make any sense to use the actual class in the tests when I want to test to see if the alias works.\nMy solution to this was subscribing to lifecycle changes through ActivityLifecycleMonitorRegistry and blocking the test thread until activity is launched:\n// NOTE: make sure this is a strong reference (move up as a class field) otherwise will be GCed and you will not stably receive updates.\nActivityLifecycleCallback lifeCycleCallback = new ActivityLifecycleCallback() {\n            @Override\n            public void onActivityLifecycleChanged(Activity activity, Stage stage) {\n                classHolder.setValue(((MyActivity) activity).getClass());\n\n                // release the test thread\n                lock.countDown();\n            }\n         };\n\n// used to block the test thread until activity is launched\nfinal CountDownLatch lock = new CountDownLatch(1);\nfinal Holder<Class<? extends MyActivity>> classHolder = new Holder<>();\ninstrumentation.runOnMainSync(new Runnable() {\n   @Override\n    public void run() {\n        ActivityLifecycleMonitorRegistry.getInstance().addLifecycleCallback(lifeCycleCallback);\n     }\n});\n\n// start the Activity\nintent.setClassName(context, MyApp.class.getPackage().getName() + "".MyActivityAlias"");\ncontext.startActivity(intent);\n// wait for activity to start\nlock.await();\n\n// continue with the tests\nassertTrue(classHolder.hasValue());\nassertTrue(classHolder.getValue().isAssignableFrom(MyActivity.class));\n\nHolder is basically a wrapper object. You can use an array or anything else to capture a value inside the anonymous class.\n', '\nThe accepted answer may not work in many espresso tests. The following works with espresso version 2.2.2 and Android compile/target SDK 27 running on API 25 devices:\n@Nullable\nprivate Activity getActivity() {\n    Activity currentActivity = null;\n\n    Collection resumedActivities = ActivityLifecycleMonitorRegistry.getInstance().getActivitiesInStage(RESUMED);\n    if (resumedActivities.iterator().hasNext()){\n        currentActivity = (Activity) resumedActivities.iterator().next();\n    }\n    return currentActivity;\n}\n\n']",https://stackoverflow.com/questions/24517291/get-current-activity-in-espresso-android,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get webDriver to wait for page to load (C# Selenium project),"
I've started a Selenium project in C#. Trying to wait for page to finish loading up and only afterwards proceed to next action.
My code looks like this:
 loginPage.GoToLoginPage();
        loginPage.LoginAs(TestCase.Username, TestCase.Password);
        loginPage.SelectRole(TestCase.Orgunit);
        loginPage.AcceptRole();

inside loginPage.SelectRole(TestCase.Orgunit):
 RoleHierachyLabel = CommonsBasePage.Driver.FindElement(By.XPath(""//span[contains(text(), "" + role + "")]""));
 RoleHierachyLabel.Click();
 RoleLoginButton.Click();

I search for element RoleHierachyLabel. I've been trying to use multiple ways to wait for page to load or search for an element property allowing for some timeout:
1. _browserInstance.Manage().Timeouts().ImplicitlyWait(TimeSpan.FromSeconds(5));


2. public static bool WaitUntilElementIsPresent(RemoteWebDriver driver, By by, int timeout = 5)
    {
        for (var i = 0; i < timeout; i++)
        {
            if (driver.ElementExists(by)) return true;
        }
        return false;
    }

How would you tackle this obstacle?
",105k,"
            31
        ","['\nI\'ve been searching for alternatives and I\'ve settled for the following versions. All use explicit wait with a defined timeout and are based on element properties in the first case and on element staleness in the second case.\nFirst choice would be checking element properties until a timeout is reached. I\'ve arrived to the following properties that confirm it is available on the page:\nExistence - An expectation for checking that an element is present on the DOM of a page. This does not necessarily mean that the element is visible.\n//this will not wait for page to load\nAssert.True(Driver.FindElement(By elementLocator).Enabled)\n\n//this will search for the element until a timeout is reached\npublic static IWebElement WaitUntilElementExists(By elementLocator, int timeout = 10)\n    {\n        try\n        {\n            var wait = new WebDriverWait(Driver, TimeSpan.FromSeconds(timeout));\n            return wait.Until(ExpectedConditions.ElementExists(elementLocator));\n        }\n        catch (NoSuchElementException)\n        {\n            Console.WriteLine(""Element with locator: \'"" + elementLocator + ""\' was not found in current context page."");\n            throw;\n        }\n    }\n\nVisibility - An expectation for checking that an element is present on the DOM of a page and visible. Visibility means that the element is not only displayed but also has a height and width that is greater than 0.\n//this will not wait for page to load\nAssert.True(Driver.FindElement(By elementLocator).Displayed)\n\n//this will search for the element until a timeout is reached\npublic static IWebElement WaitUntilElementVisible(By elementLocator, int timeout = 10)\n    {\n        try\n        {\n            var wait = new WebDriverWait(Driver, TimeSpan.FromSeconds(timeout));\n            return wait.Until(ExpectedConditions.ElementIsVisible(elementLocator));\n        }\n        catch (NoSuchElementException)\n        {\n            Console.WriteLine(""Element with locator: \'"" + elementLocator + ""\' was not found."");\n            throw;\n        }\n    }\n\nClickable - An expectation for checking an element is visible and enabled such that you can click it.\n//this will not wait for page to load\n//both properties need to be true in order for element to be clickable\nAssert.True(Driver.FindElement(By elementLocator).Enabled)\nAssert.True(Driver.FindElement(By elementLocator).Displayed)\n\n//this will search for the element until a timeout is reached\npublic static IWebElement WaitUntilElementClickable(By elementLocator, int timeout = 10)\n    {\n        try\n        {\n            var wait = new WebDriverWait(Driver, TimeSpan.FromSeconds(timeout));\n            return wait.Until(ExpectedConditions.ElementToBeClickable(elementLocator));\n        }\n        catch (NoSuchElementException)\n        {\n            Console.WriteLine(""Element with locator: \'"" + elementLocator + ""\' was not found in current context page."");\n            throw;\n        }\n    }\n\nSecond choice applies when the trigger object, for example a menu item, is no longer attached to the DOM after it is clicked. This is ususally the case when click action on the element will trigger a redirect to another page. In this case it\'s usefull to check StalenessOf(element) where element is the item that was clicked to trigger the redirect to the new page.\npublic static void ClickAndWaitForPageToLoad(By elementLocator, int timeout = 10)\n    {\n        try\n        {\n            var wait = new WebDriverWait(Driver, TimeSpan.FromSeconds(timeout));\n            var element = Driver.FindElement(elementLocator);\n            element.Click();\n            wait.Until(ExpectedConditions.StalenessOf(element));\n        }\n        catch (NoSuchElementException)\n        {\n            Console.WriteLine(""Element with locator: \'"" + elementLocator + ""\' was not found in current context page."");\n            throw;\n        }\n    }\n\n', '\ndriver.Manage().Timeouts().PageLoad = TimeSpan.FromSeconds(5);\nAlso, see this answer\n', '\nI usually use an explicit wait for this, and wait until an elements is visible, then proceed to the next action. This should look like this:\nWebDriverWait waitForElement = new WebDriverWait(driver, TimeSpan.FromSeconds(5));\nwaitForElement.Until(ExpectedConditions.ElementIsVisible(By.Id(""yourIDHere"")));\n\nMore on Explicit waits here: Explicit waits Selenium C# and here WebDriver Explicit waits\n', '\nJust had the same problem.\nWith the folowing Method I wait for the page to load fully, not relying on causing the page load by JavaScript, a click or an action on an input element.\nprivate void WaitForPageToLoad(Action doing)\n{\n    IWebElement oldPage = _driver.FindElement(By.TagName(""html""));\n    doing();\n    WebDriverWait wait = new WebDriverWait(_driver, new TimeSpan(0, 0, Timeout));\n    try\n    {\n        wait.Until(driver => ExpectedConditions.StalenessOf(oldPage)(_driver) &&\n            ((IJavaScriptExecutor)driver).ExecuteScript(""return document.readyState"").Equals(""complete""));\n    }\n    catch (Exception pageLoadWaitError)\n    {\n        throw new TimeoutException(""Timeout during page load"", pageLoadWaitError);\n    }\n}\n\ncalled like following\nWaitForPageToLoad(() => _driver.FindElement(By.Id(""button1"")).Click());\n\n', ""\nI did this to address this type of issue.  It's a combination of timers and loops that are looking for a specific element until it timesout after a certain number of milliseconds.\nprivate IWebElement FindElementById(string id, int timeout = 1000)\n{\n    IWebElement element = null;\n\n    var s = new Stopwatch();\n    s.Start();\n\n    while (s.Elapsed < TimeSpan.FromMilliseconds(timeout))\n    {\n        try\n        {\n            element = _driver.FindElementById(id);\n            break;\n        }\n        catch (NoSuchElementException)\n        {\n        }\n    }\n\n    s.Stop();\n    return element;\n}\n\nI also made one for element enabled\nprivate IWebElement ElementEnabled(IWebElement element, int timeout = 1000)\n{\n    var s = new Stopwatch();\n    s.Start();\n\n    while (s.Elapsed < TimeSpan.FromMilliseconds(timeout))\n    {\n        if (element.Enabled)\n        {\n            return element;\n        }\n    }\n\n    s.Stop();\n    return null;\n}\n\n"", '\nAs said in Wait for page load in Selenium:\n\nIn general, with Selenium 2.0 the web driver should only return\n  control to the calling code once it has determined that the page has\n  loaded. If it does not, you can call waitforelemement, which cycles\n  round calling findelement until it is found or times out (time out\n  can be set).\n\n', '\nBecause of its simplicity I like this solution. Also it has the benefit of avoiding excessive waits and taking the guess work out of what might be the upper wait limit:\n    public bool WaitToLoad(By by)\n    {\n        int i = 0;\n        while (i < 600)\n        {\n            i++;\n            Thread.Sleep(100); // sleep 100 ms\n            try\n            {\n                driver.FindElement(by);\n                break;\n            }\n            catch { }\n        }\n        if (i == 600) return false; // page load failed in 1 min\n        else return true;\n    }\n\nWhich can be modified to also include a ""timer"" if monitoring page load latency is needed:\n    public int WaitToLoad(By by)\n    {\n        int i = 0;\n        while (i < 600)\n        {\n            i++;\n            Thread.Sleep(100); // sleep 100 ms\n            try\n            {\n                driver.FindElement(by);\n                break;\n            }\n            catch { }\n        }\n        return i; // page load latency in 1/10 secs\n    }\n\n', '\nYou just need to import SeleniumExtras package via NuGet, then using it like the following :\nvar e = new WebDriverWait(driver, new TimeSpan(0, 0, 60)).Until(SeleniumExtras.WaitHelpers.ExpectedConditions.ElementIsVisible(By.Id(""--id"")));\n\n', ""\nAll anwsers without any sence here.\nPage could contain all controls. But you change something and data in controls changing and page goes in reload state. So if you continue working with this page you get a bunch of errors.\nReally I don't see better solution then Thread.Sleep()\n""]",https://stackoverflow.com/questions/43203243/how-to-get-webdriver-to-wait-for-page-to-load-c-selenium-project,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check if element exists python selenium,"
I'm trying to locate element by       
element=driver.find_element_by_partial_link_text(""text"")

in Python selenium and the element does not always exist. Is there a quick line to check if it exists and get NULL or FALSE in place of the error message when it doesn't exist?          
",54k,"
            16
        ","['\nYou can implement try/except block as below to check whether element present or not:\nfrom selenium.common.exceptions import NoSuchElementException\n\ntry:\n    element=driver.find_element_by_partial_link_text(""text"")\nexcept NoSuchElementException:\n    print(""No element found"")\n\nor check the same with one of find_elements_...() methods. It should return you empty list or list of elements matched by passed selector, but no exception in case no elements found:\nelements=driver.find_elements_by_partial_link_text(""text"")\nif not elements:\n    print(""No element found"")  \nelse:\n    element = elements[0]  \n\n', '\nSometimes the element does not appear at once, for this case we need to use explicit wait:\nbrowser = webdriver.Chrome()\nwait = WebDriverWait(browser, 5)\n\ndef is_element_exist(text):\n    try:\n        wait.until(EC.presence_of_element_located((By.PARTIAL_LINK_TEXT, text)))\n    except TimeoutException:\n        return False\n\nSolution without try/ except:\ndef is_element_exist(text):\n    elements = wait.until(EC.presence_of_all_elements_located((By.PARTIAL_LINK_TEXT, text)))\n    return None if elements else False\n\nHow explicit wait works you can read here.\nImports:\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\n\n']",https://stackoverflow.com/questions/45695874/check-if-element-exists-python-selenium,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UI Automation events stop being received after a while monitoring an application and then restart after some time,"
We are using Microsoft's UIAutomation framework to develop a client that monitors events of a specific application and responds to them in different ways. We've started with the managed version of the framework, but due to delay issues, moved to the native version wrapped in UIACOMWrapper. After more issues with performance inside our (massive) WPF application, we decided to move it to a separate terminal application (transfer the events to our WPF app through UDP) which seemed to fix all the performance issues. The only problem is that it seems that every several minutes, the events for TabSelection, StructureChanged, WindowOpened and WindowClosed stop being captured for a few minutes. Surprisingly PropertyChanged events are still received and handled while this happens. I will post the relevant code of our event monitor, but this is probably irrelevant as we have seen similar behavior when using Microsoft's own AccEvent utility. I can't post the code of the monitored application as it is proprietary and confidential as well, I can say that it is a WinForms application that hosts WPF windows and also quite massive.
Has anyone seen this sort of behavior while working with the UI Automation framework?
Thank you for your time.
Here's the monitor code (I know the event handling is on the UI Automation threads here but moving it to a dedicated thread did not change anything):
        public void registerHandlers()
    {
        //Register on structure changed and window opened events 
        System.Windows.Automation.Automation.AddStructureChangedEventHandler(
            this.getMsAutomationElement(), System.Windows.Automation.TreeScope.Subtree, this.handleStructureChanged);
        System.Windows.Automation.Automation.AddAutomationEventHandler(
            System.Windows.Automation.WindowPattern.WindowOpenedEvent,
            this.getMsAutomationElement(),
            System.Windows.Automation.TreeScope.Subtree,
            this.handleWindowOpened);
        System.Windows.Automation.Automation.AddAutomationEventHandler(
            System.Windows.Automation.WindowPattern.WindowClosedEvent,
            System.Windows.Automation.AutomationElement.RootElement,
            System.Windows.Automation.TreeScope.Subtree,
            this.handleWindowClosed);

        this.registerValueChanged();
        this.registerTextNameChange();
        this.registerTabSelected();
        this.registerRangeValueChanged();
    }

    private void registerRangeValueChanged()
    {
        if (this.getMsAutomationElement() != null)
        {
            System.Windows.Automation.Automation.AddAutomationPropertyChangedEventHandler(
                    this.getMsAutomationElement(),
                    System.Windows.Automation.TreeScope.Subtree, this.handlePropertyChange,
                    System.Windows.Automation.RangeValuePattern.ValueProperty);
        }
    }

    private void unregisterRangeValueChanged()
    {
        System.Windows.Automation.Automation.RemoveAutomationPropertyChangedEventHandler(
                this.getMsAutomationElement(),
                this.handlePropertyChange);
    }

    private void registerValueChanged()
    {
        if (this.getMsAutomationElement() != null)
        {
            System.Windows.Automation.Automation.AddAutomationPropertyChangedEventHandler(
                this.getMsAutomationElement(),
                System.Windows.Automation.TreeScope.Subtree, this.handlePropertyChange,
                System.Windows.Automation.ValuePattern.ValueProperty);
        }
    }

    private void unregisterValueChanged()
    {
        System.Windows.Automation.Automation.RemoveAutomationPropertyChangedEventHandler(
                            this.getMsAutomationElement(),
                            this.handlePropertyChange);
    }

    private void registerTextNameChange()
    {
        if (this.getMsAutomationElement() != null)
        {
            System.Windows.Automation.Automation.AddAutomationPropertyChangedEventHandler(
            this.getMsAutomationElement(),
            System.Windows.Automation.TreeScope.Subtree, this.handlePropertyChange,
                System.Windows.Automation.AutomationElement.NameProperty);
        }
    }

    private void unregisterTextNameChange()
    {
        System.Windows.Automation.Automation.RemoveAutomationPropertyChangedEventHandler(
        this.getMsAutomationElement(),
        this.handlePropertyChange);
    }
    private void handleWindowOpened(object src, System.Windows.Automation.AutomationEventArgs e)
    {
        Console.ForegroundColor = ConsoleColor.Magenta;
        Console.WriteLine(DateTime.Now.ToShortTimeString() + "" "" + ""Window opened:"" + "" "" + 
            (src as System.Windows.Automation.AutomationElement).Current.Name);

        System.Windows.Automation.AutomationElement element = src as System.Windows.Automation.AutomationElement;
        //this.sendEventToPluginQueue(src, e, element.GetRuntimeId(), this.getAutomationParent(element).GetRuntimeId());
        //Fill out the fields of the control added message
        int[] parentId = this.getAutomationParent(element).GetRuntimeId();
        this.copyToIcdArray(parentId,
            this.protocol.getMessageSet().outgoing.ControlAddedMessage.Data.controlAdded.parentRuntimeId);
        this.copyToIcdArray(element.GetRuntimeId(),
            this.protocol.getMessageSet().outgoing.ControlAddedMessage.Data.controlAdded.runtimeId);
        //Send the message using the protocol
        this.protocol.send(this.protocol.getMessageSet().outgoing.ControlAddedMessage);
    }

    private void copyToIcdArray(int[] runtimeId, ICD.UI_AUTOMATION.RuntimeId icdRuntimeId)
    {
        icdRuntimeId.runtimeIdNumberOfItems.setVal((byte)runtimeId.Count());
        for (int i = 0; i < runtimeId.Count(); i++)
        {
            icdRuntimeId.runtimeIdArray.getElement(i).setVal(runtimeId[i]);
        }
    }

    private void handleWindowClosed(object src, System.Windows.Automation.AutomationEventArgs e)
    {
        if (src != null)
        {
            Console.ForegroundColor = ConsoleColor.Cyan;
            Console.WriteLine(DateTime.Now.ToShortTimeString() + "" "" + ""Window closed:"" + "" "" +
                (src as System.Windows.Automation.AutomationElement).GetRuntimeId().ToString());

            System.Windows.Automation.AutomationElement element = src as System.Windows.Automation.AutomationElement;
            this.copyToIcdArray(element.GetRuntimeId(),
                this.protocol.getMessageSet().outgoing.ControlRemovedMessage.Data.controlRemoved.runtimeId);
            //Send the message using the protocol
            this.protocol.send(this.protocol.getMessageSet().outgoing.ControlRemovedMessage);

            //this.sendEventToPluginQueue(src, e, element.GetRuntimeId());
        }
    }

EDIT: 
I forgot to mention that I strongly suspect that the issue is that one of the UI-Automation event handler threads gets stuck somehow. The reason I believe this, is that when the problem occurred in my monitor, I started an instance of AccEvent and it received all the missing events that my monitor was not getting. This means that the events are being fired but not passed to my monitor.
EDIT2:
I forgot to mention that this happens running in Windows 8 with the specific target application, I have not seen this phenomenon on my own Windows 7 machine with other applications. Another interesting thing is that it seems to happen periodically more or less, but regardless of when I subscribe to events, i.e. it can happen almost immediately after subscribing but then it takes several minutes to reoccur. 
",5k,"
            12
        ","['\nI\'m afraid I don\'t know the cause of the delays that you\'re seeing, but here are some thoughts on this...\nEverything I say below relates to the native UIA API in Windows, not the managed .NET UIA API. All improvements to UIA in recent years have been made to the Windows UIA API. So whenever I write UIA client C# code, I call UIA through a managed wrapper that I generate with the tlbimp.exe SDK tool.\nThat is, I first generate the wrapper with a command like...\n""C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v8.1A\\bin\\NETFX 4.5.1 Tools\\x64\\tlbimp.exe"" c:\\windows\\system32\\uiautomationcore.dll /out:Interop.UIAutomationCore.dll\nThen I include a reference to the Interop.UIAutomationCore.dll in my C# project, add ""using Interop.UIAutomationCore;"" to my C# file, and then I can do things like...\nIUIAutomation uiAutomation = new CUIAutomation8();\n\nIUIAutomationElement rootElement = uiAutomation.GetRootElement();\n\nuiAutomation.AddAutomationEventHandler(\n    20016, // UIA_Window_WindowOpenedEventId\n    rootElement,\n    TreeScope.TreeScope_Descendants,\n    null,\n    this);\n\n...\npublic void HandleAutomationEvent(IUIAutomationElement sender, int eventId)\n{\n    // Got a window opened event...\n}\n\nIn Windows 7, there were some important constraints around UIA event handlers. It was easy to write event handlers which didn\'t account for those constraints, and that could lead to long delays when interacting with UIA. For example, it was important to not add or remove a UIA event handler from inside an event handler. So at the time, I intentionally made no UIA calls at all from inside my event handlers. Instead, I\'d post myself a message or add some action to a queue, allow my event handler to return, and take whatever action I wanted to in response to the event shortly afterwards on another thread. This required some more work on my part, but I didn\'t want to risk hitting delays. And any threads I created would be running in an MTA.\nAn example of the action described above is in my old focus tracking sample up at https://code.msdn.microsoft.com/windowsapps/Windows-7-UI-Automation-6390614a/sourcecode?fileId=21469&pathId=715901329. The file FocusEventHandler.cs creates the MTA thread and queues messages to avoid making UIA calls inside the event hander.\nSince Window 7, I know the constraints in UIA relating to threading and delays have been relaxed, and the likelihood of encountering delays has been reduced. More recently, there were some improvements between Windows 8.1 and Windows 10 in this area, so if it\'d be practical to run your code on Windows 10, it would be interesting to see if the delays still repro there.\nI know this is time consuming, but you might be interested in removing the interaction with UIA inside your event handlers and seeing if the delays go away. If they do, it\'d be a case of determining which action seems to trigger the problem, and seeing if there\'s an alternative way of achieving your goals without performing the UIA interaction in the event handlers.\nFor example, in your event handler, you call...\nthis.getAutomationParent(element).GetRuntimeId();\nI expect this will lead to two calls back into the provider app which generated the event. The first call is to get the parent of the source element, and the second call is to get the RuntimeId of that parent. So while UIA is waiting for your event handler to return, you\'ve called twice back into UIA. While I don\'t know that that\'s a problem, I\'d avoid it. \nSometimes you can avoid a cross-proc call back to the provider process by having some data of interest cached with the event itself. For example, say I know I\'m going to want the RuntimeId of an element that raised a WindowOpened event. I can ask UIA to cache that data with the events I receive, when I register for the events.\nint propertyRuntimeId = 30000; // UIA_RuntimeIdPropertyId\n\n...\nIUIAutomationCacheRequest cacheRequestRuntimeId = uiAutomation.CreateCacheRequest();\ncacheRequestRuntimeId.AddProperty(propertyRuntimeId);\n\nuiAutomation.AddAutomationEventHandler(\n    20016, // UIA_Window_WindowOpenedEventId\n    rootElement,\n    TreeScope.TreeScope_Descendants,\n    cacheRequestRuntimeId,\n    this);\n\n...\npublic void HandleAutomationEvent(IUIAutomationElement sender, int eventId)\n{\n    // Got a window opened event...\n\n    // Get the RuntimeId from the source element. Because that data is cached with the\n    // event, we don\'t have to call back through UIA into the provider process here.\n    int[] runtimeId = sender.GetCachedPropertyValue(propertyRuntimeId);\n}\n\nOn a side note, when practical, I always cache data when dealing with events or accessing elements through UIA, (by using calls such as FindFirstBuildCache(),) as I want to avoid as many cross-proc calls as possible.\nSo my advice would be:\n\nUse the native Windows UIA API with a managed wrapper generated by tlbimp.exe.\nCache as much data as possible with the events, to avoid having to call back into the provider process unnecessarily later.\nAvoid calls back into UIA from inside a UIA event handler.\n\nThanks,\nGuy\n', '\nI have seen this behavior in my project. The solution was unsubscribes and resubscribe to the events using a timer.\nIn addition, I set off any action following the events in a new task (running in an STA thread pool).\n']",https://stackoverflow.com/questions/32347734/ui-automation-events-stop-being-received-after-a-while-monitoring-an-application,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get selected text from ANY window (using UI Automation) - C#,"
I have a small tray application which registers a system-wide hotkey. When the user selects a text anywhere in any application and presses this hotkey I want to be able to capture the selected text. I'm currently doing this using AutomationElements:
//Using FocusedElement (since the focused element should be the control with the selected text?)
AutomationElement ae = AutomationElement.FocusedElement;        
AutomationElement txtElement = ae.FindFirst(TreeScope.Subtree,Condition.TrueCondition);
if(txtElement == null)
    return;

TextPattern tp;

try
{
    tp = txtElement.GetCurrentPattern(TextPattern.Pattern) as TextPattern;
}
catch(Exception ex)
{
    return;
}

TextPatternRange[] trs;

if (tp.SupportedTextSelection == SupportedTextSelection.None)
{
    return;
            }
else
{
    trs = tp.GetSelection();
    string selectedText = trs[0].GetText(-1);
    MessageBox.Show(selectedText );

}

This works for some apps (such as notepad, visual studios edit boxes and such) but not for all (such as Word, FireFox, Chrome, and so on.)
Anyone here with any ideas of how to be able to retreive the selected text in ANY application?
",8k,"
            7
        ","['\nUnfortunately, there\'s no way to get the selected text from any arbitrary application.  UI Automation works if the application supports UIA TextPattern; unfortunately, most do not.  I wrote an application that tried to do this, and had a bunch of fallbacks.  \nI tried (pretty much in order):  \n\nUIA.TextPattern\nInternet Explorer-specific (this had different implementations for IE 6,7,8,9)\nAdobe Reader-specific\nClipboard\n\nThis covered 80-90% of the applications out there, but there were quite a few that still failed.\nNote that restoring the clipboard has problems of its own; some applications (Office, etc.) put vendor-specific information into the clipboard that can have pointers into internal data; when you put your own info on the clipboard, the internal data gets released, and when you put the old data back, the clipboard now points to freed data, resulting in crashes.  You could work around this somewhat by only saving/restoring known clipboard formats, but again, that results in odd behavior in that apps behave ""wrong"" instead of crashing.\n', '\nUIA technology does not supported by all applications, you can try to use MSAA in some cases (like FF, Chrome, etc.) but you still will get many problems. \nThe best way is to save current clipboard text, send ""CTRL + C"" keypress message via SendMessage WinAPI function, get clipboard text, and restore initial clipboard text as Rick said.\n', ""\nIs it possible to look at the clipboard and make your hotkey: CTRL+C ?\nYou won't be able to read selected text from any application. For example some PDF files have protected content that disallows copies.\n""]",https://stackoverflow.com/questions/4243944/how-to-get-selected-text-from-any-window-using-ui-automation-c-sharp,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to reuse yielded value later in the test,"
Context: I have an element with innerText in Page1 and another element with innerText on Page 2. And I need to compare these two values in Page 3. So I need to find a way to save these values in a variable globally so that they can be used later.
Things I tried:
Trial 1: Didn't work as page1value scope is limited to the cy.get()
  cy.get('#selector').invoke('text').then(text => {
      const page1value = text
  })

Trial 2: Didn't work as whenever I try to print the value outside, it comes as undefined or the value with which it was initialized it with.
it('TC Name', () => {

    let page1value,
        cy.get('#selector').invoke('text').then(text => {
            page1value = text
        })

    cy.log(page1value) //comes as undefined

})

Trial 3: Using .as() as well its coming as undefined.
let page1value;
cy.get('#selector').invoke('text').as('page1value');
cy.log(page1value) //comes as undefined

It would be great if someone could tell me where I am doing wrong.
",4k,"
            6
        ","[""\nCypress commands are pushed (enqueued) into a queue (called the Command queue --- which is basically an array), and then executed serially (one after another), and asynchronously.\nWhile your cy.log() will be executed asynchronously, too, after the previous command, the value you pass to it (page1value) is passed/evaluated synchronously, at the time you push the command to the queue (which is evaluated at the time the callback passed to it() is called --- at the beginning of the test).\nThis is just regular JavaScript behavior, and has nothing to do with Cypress. All the commands cy.* are just methods (functions) on the cy object, and they're called immediately. What is not called (executed) immediately, is the logic that each command does (e.g. query the DOM for the selector you supply to cy.get(), log to Command log when you call cy.log('string'), etc.).\nThus, in your 2nd example:\n\nYou declare page1value.\nYou then immediately enqueue commands cy.get(), cy.invoke, cy.then().\nAnd you also immediately enqueue cy.log, to which you pass page1value (which at this time is still undefined).\nAfter all commands are enqueued, they start to execute, from top to bottom. When the cy.then command takes turn to execute, the page1value variable is assigned, but it's no longer used (read) anywhere for the rest of the test (recall that you've already read it when you passed it to the cy.log command in the previous step).\n\nThus, what you want to do instead, is:\ncy.get('#selector').invoke('text').then(text => {\n  cy.log(text);\n});\n\nIn your 3rd example, if you alias something, you need to access that value using another command (remember, everything is asynchronous, so you can't access values that are set asynchronously, in a synchronous manner as you're doing), in this case cy.get('@aliasName'):\ncy.get('#selector').invoke('text').as('page1value');\ncy.get('@page1value').then( value => {\n  cy.log(value);\n});\n\nNote that the above explanations are slightly inaccurate and inexhaustive (there are more things going on behind the scenes), for the sake of simplicity. But as an intro to how things work, they should do.\nAnyway, you should definitely read Introduction to Cypress.\nYou may also take a look at my older answers that touch on related concepts:\n\nHow to use a variable as a parameter in an API call in Cypress\nIn Cypress when to use Custom Command vs Task?\n\n""]",https://stackoverflow.com/questions/59341731/how-to-reuse-yielded-value-later-in-the-test,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Capture Button Click event inside a MessageBox in another application,"
I want to capture the OK Button's Click event on a MessageBox shown by another WinForms application.
I want to achieve this using UI Automation. After some research, I have found that IUIAutomation::AddAutomationEventHandler will do the work for me.  
Though, I can capture the Click event of any other button, I'm unable to capture a Click event of the MessageBox.  
My code is as follows:  
var FindDialogButton = appElement.FindFirst(TreeScope.Descendants, new PropertyCondition(AutomationElement.NameProperty, ""OK""));

if (FindDialogButton != null)
{
    if (FindDialogButton.GetSupportedPatterns().Any(p => p.Equals(InvokePattern.Pattern)))
    {
        Automation.AddAutomationEventHandler(InvokePattern.InvokedEvent, FindDialogButton, TreeScope.Element, new AutomationEventHandler(DialogHandler));
    }
}

private void DialogHandler(object sender, AutomationEventArgs e)
{
    MessageBox.Show(""Dialog Button clicked at : "" + DateTime.Now);
}


EDIT: 
My Complete code is as follows:  
private void DialogButtonHandle()
{
    AutomationElement rootElement = AutomationElement.RootElement;
    if (rootElement != null)
    {
        System.Windows.Automation.Condition condition = new PropertyCondition
     (AutomationElement.NameProperty, ""Windows Application""); //This part gets the handle of the Windows application that has the MessageBox

        AutomationElement appElement = rootElement.FindFirst(TreeScope.Children, condition);

        var FindDialogButton = appElement.FindFirst(TreeScope.Descendants, new PropertyCondition(AutomationElement.NameProperty, ""OK"")); // This part gets the handle of the button inside the messagebox
        if (FindDialogButton != null)
        {
            if (FindDialogButton.GetSupportedPatterns().Any(p => p.Equals(InvokePattern.Pattern)))
            {
                Automation.AddAutomationEventHandler(InvokePattern.InvokedEvent, FindDialogButton, TreeScope.Element, new AutomationEventHandler(DialogHandler)); //Here I am trying to catch the click of ""OK"" button inside the MessageBox
            }
        }
    }
}

private void DialogHandler(object sender, AutomationEventArgs e)
{
    //On Button click I am trying to display a message that the button has been clicked
    MessageBox.Show(""MessageBox Button Clicked"");
}

",2k,"
            4
        ","['\nI tried to keep this procedure as generic as possible, so that it will work whether the application you\'re watching is already running when your app is started or not.  \nYou just need to provide the watched Application\'s Process Name or its Main Window Title to let the procedure identify this application.\nUse one of these Fields and the corresponding Enumerator:  \nprivate string appProcessName = ""theAppProcessName""; and \nFindWindowMethod.ProcessName\n// Or\nprivate string appWindowTitle = ""theAppMainWindowTitle""; and \nFindWindowMethod.Caption\n\npassing these values to the procedure that starts the watcher, e.g., :  \nStartAppWatcher(appProcessName, FindWindowMethod.ProcessName); \n\nAs you can see - since you tagged your question as winforms - this is a complete Form (named frmWindowWatcher) that contains all the logic required to perform this task.  \nHow does it work:  \n\nWhen you start frmWindowWatcher, the procedure verifies whether the watched application (here, identified using its Process name, but you can change the method, as already described), is already running.\nIf it is, it initializes a support class, ElementWindow, which will contain some informations about the watched application.\nI added this support class in case you need to perform some actions if the watched application is already running (in this case, the ElementWindow windowElement Field won\'t be null when the StartAppWatcher() method is called). These informations may also be useful in other cases.  \nWhen a new Windows is opened in the System, the procedure verifies whether this Window belongs to the watched application. If it does, the Process ID will be the same. If the Windows is a MessageBox (identified using its standard ClassName: #32770) and it belongs to the watched Application, an AutomationEventHandler is attached to the child OK Button.\nHere, I\'m using a Delegate: AutomationEventHandler DialogButtonHandler for the handler and an instance Field (AutomationElement msgBoxButton) for the Button Element, because these references are needed to remove the Button Click Handler when the MessageBox is closed.  \nWhen the MessageBox\'s OK Button is clicked, the MessageBoxButtonHandler method is called. Here, you can determine which action to take at this point.  \nWhen the frmWindowWatcher Form is closed, all Automation Handlers are removed, calling the Automation.RemoveAllEventHandlers() method, to provide a final clean up and prevent your app from leaking resources.  \n\n\n\nusing System.Diagnostics;\nusing System.Linq;\nusing System.Windows.Automation;\nusing System.Windows.Forms;\n\npublic partial class frmWindowWatcher : Form\n{\n    AutomationEventHandler DialogButtonHandler = null;\n    AutomationElement msgBoxButton = null;\n    ElementWindow windowElement = null;\n    int currentProcessId = 0;\n    private string appProcessName = ""theAppProcessName"";\n    //private string appWindowTitle = ""theAppMainWindowTitle"";\n\n    public enum FindWindowMethod\n    {\n        ProcessName,\n        Caption\n    }\n\n    public frmWindowWatcher()\n    {\n        InitializeComponent();\n        using (var proc = Process.GetCurrentProcess()) {\n            currentProcessId = proc.Id;\n        }\n        // Identify the application by its Process name...\n        StartAppWatcher(appProcessName, FindWindowMethod.ProcessName);\n        // ... or by its main Window Title\n        //StartAppWatcher(appWindowTitle, FindWindowMethod.Caption);\n    }\n\n    protected override void OnFormClosed(FormClosedEventArgs e)\n    {\n        Automation.RemoveAllEventHandlers();\n        base.OnFormClosed(e);\n    }\n\n    private void StartAppWatcher(string elementName, FindWindowMethod method)\n    {\n        windowElement = GetAppElement(elementName, method);\n        // (...)\n        // You may want to perform some actions if the watched application is already running when you start your app\n\n        Automation.AddAutomationEventHandler(WindowPattern.WindowOpenedEvent, AutomationElement.RootElement,\n            TreeScope.Subtree, (elm, e) => {\n                AutomationElement element = elm as AutomationElement;\n\n                try\n                {\n                    if (element == null || element.Current.ProcessId == currentProcessId) return;\n                    if (windowElement == null) windowElement = GetAppElement(elementName, method);\n                    if (windowElement == null || windowElement.ProcessId != element.Current.ProcessId) return;\n\n                    // If the Window is a MessageBox generated by the watched app, attach the handler\n                    if (element.Current.ClassName == ""#32770"")\n                    {\n                        msgBoxButton = element.FindFirst(TreeScope.Descendants, \n                            new PropertyCondition(AutomationElement.NameProperty, ""OK""));\n                        if (msgBoxButton != null && msgBoxButton.GetSupportedPatterns().Any(p => p.Equals(InvokePattern.Pattern)))\n                        {\n                            Automation.AddAutomationEventHandler(\n                                InvokePattern.InvokedEvent, msgBoxButton, TreeScope.Element,\n                                    DialogButtonHandler = new AutomationEventHandler(MessageBoxButtonHandler));\n                        }\n                    }\n                }\n                catch (ElementNotAvailableException) {\n                    // Ignore: this exception may be raised if you show a modal dialog, \n                    // in your own app, that blocks the execution. When the dialog is closed, \n                    // AutomationElement element is no longer available\n                }\n            });\n\n        Automation.AddAutomationEventHandler(WindowPattern.WindowClosedEvent, AutomationElement.RootElement,\n            TreeScope.Subtree, (elm, e) => {\n                AutomationElement element = elm as AutomationElement;\n\n                if (element == null || element.Current.ProcessId == currentProcessId || windowElement == null) return;\n                if (windowElement.ProcessId == element.Current.ProcessId) {\n                    if (windowElement.MainWindowTitle == element.Current.Name) {\n                        windowElement = null;\n                    }\n                }\n            });\n    }\n\n    private void MessageBoxButtonHandler(object sender, AutomationEventArgs e)\n    {\n        Console.WriteLine(""Dialog Button clicked at : "" + DateTime.Now.ToString());\n        // (...)\n        // Remove the handler after, since the next MessageBox needs a new handler.\n        Automation.RemoveAutomationEventHandler(e.EventId, msgBoxButton, DialogButtonHandler);\n    }\n\n    private ElementWindow GetAppElement(string elementName, FindWindowMethod method)\n    {\n        Process proc = null;\n\n        try {\n            switch (method) {\n                case FindWindowMethod.ProcessName:\n                    proc = Process.GetProcessesByName(elementName).FirstOrDefault();\n                    break;\n                case FindWindowMethod.Caption:\n                    proc = Process.GetProcesses().FirstOrDefault(p => p.MainWindowTitle == elementName);\n                    break;\n            }\n            return CreateElementWindow(proc);\n        }\n        finally {\n            proc?.Dispose();\n        }\n    }\n\n    private ElementWindow CreateElementWindow(Process process) => \n        process == null ? null : new ElementWindow(process.ProcessName) {\n            MainWindowTitle = process.MainWindowTitle,\n            MainWindowHandle = process.MainWindowHandle,\n            ProcessId = process.Id\n        };\n}\n\nSupport class, used to store informations on the watched application:\nIt\'s initialized using the App\'s Process Name:  \npublic ElementWindow(string processName)\n\nbut of course you can change it as required, using the Window Title as described before, or even remove the initialization\'s argument if you prefer (the class just need to not be null when the watched Application has been detected and identified).  \nusing System.Collections.Generic;\n\npublic class ElementWindow\n{\n    public ElementWindow(string processName) => this.ProcessName = processName;\n\n    public string ProcessName { get; set; }\n    public string MainWindowTitle { get; set; }\n    public int ProcessId { get; set; }\n    public IntPtr MainWindowHandle { get; set; }\n}\n\n']",https://stackoverflow.com/questions/58184953/capture-button-click-event-inside-a-messagebox-in-another-application,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing Android Device orientation with ADB,"
I'm using Android 4.4 on a real device and I want to set the device orientation via adb.  I don't want it done with uiautomator since it won't last after the termination of the uiautomator code.
How can I do this?
",78k,"
            40
        ","['\nInstead of using ""adb shell content"", there\'s a more clean way by using ""adb shell settings"". They are doing the same thing, put value to settings provider.\nadb shell settings put system accelerometer_rotation 0  #disable auto-rotate\nadb shell settings put system user_rotation 3  #270掳 clockwise\n\n\naccelerometer_rotation: auto-rotation, 0 disable, 1 enable\nuser_rotation: actual rotation, clockwise, 0 0掳, 1 90掳, 2 180掳, 3 270掳\n\n', '\nYou may first need to turn off the automatic rotation:\nadb shell content insert --uri content://settings/system --bind name:s:accelerometer_rotation --bind value:i:0\n\nRotate to landscape:\nadb shell content insert --uri content://settings/system --bind name:s:user_rotation --bind value:i:1\n\nRotate portrait:\nadb shell content insert --uri content://settings/system --bind name:s:user_rotation --bind value:i:0\n\n', '\nDisable accelerometer_rotation and set the user_rotation\n\nuser_rotation Values:\n0           # Protrait \n1           # Landscape\n2           # Protrait Reversed\n3           # Landscape Reversed\n\naccelerometer_rotation Values:\n0           # Stay in the current rotation\n1           # Rotate the content of the screen\n\n\nExample using adb:\nadb shell settings put system accelerometer_rotation 0\nadb shell settings put system user_rotation 3\n\nExample programmatically:\nimport android.provider.Settings;\n\n// You can get ContentResolver from the Context\nSettings.System.putInt(getContentResolver(), Settings.System.ACCELEROMETER_ROTATION, 0);\nSettings.System.putInt(getContentResolver(), Settings.System.USER_ROTATION, 3);\n\n', '\nwm cmd can be used to set the user rotation on adb shell\nwm help\nset-user-rotation [free|lock] [-d DISPLAY_ID] [rotation]\nSet user rotation mode and user rotation.\n\nExample:\nwm set-user-rotation lock 0\nwm set-user-rotation lock 1\n\n']",https://stackoverflow.com/questions/25864385/changing-android-device-orientation-with-adb,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Access element whose parent is hidden - cypress.io,"
The question is as given in the title, ie, to access element whose parent is hidden. The problem is that, as per the cypress.io docs  :

An element is considered hidden if:

Its width or height is 0.
Its CSS property (or ancestors) is visibility: hidden.
Its CSS property (or ancestors) is display: none.
Its CSS property is position: fixed and it芒鈧劉s offscreen or covered up.


But the code that I am working with requires me to click on an element whose parent is hidden, while the element itself is visible.
So each time I try to click on the element, it throws up an error reading :

CypressError: Timed out retrying: expected
  '< mdc-select-item#mdc-select-item-4.mdc-list-item>' to be 'visible'
This element '< mdc-select-item#mdc-select-item-4.mdc-list-item>' is
  not visible because its parent
  '< mdc-select-menu.mdc-simple-menu.mdc-select__menu>' has CSS property:
  'display: none'


The element I am working with is a dropdown item, which is written in pug. The element is a component defined in angular-mdc-web, which uses the mdc-select for the dropdown menu and mdc-select-item for its elements (items) which is what I have to access.
A sample code of similar structure :
//pug
mdc-select(placeholder=""installation type""
            '[closeOnScroll]'=""true"")
    mdc-select-item(value=""false"") ITEM1
    mdc-select-item(value=""true"") ITEM2

In the above, ITEM1 is the element I have to access. This I do in cypress.io as follows :
//cypress.io
// click on the dropdown menu to show the dropdown (items)
cy.get(""mdc-select"").contains(""installation type"").click();
// try to access ITEM1
cy.get('mdc-select-item').contains(""ITEM1"").should('be.visible').click();

Have tried with {force:true} to force the item click, but no luck. Have tried to select the items using {enter} keypress on the parent mdc-select, but again no luck as it throws : 

CypressError: cy.type() can only be called on textarea or :text. Your
  subject is a: < mdc-select-label
  class=""mdc-select__selected-text"">Select ...< /mdc-select-label>

Also tried using the select command, but its not possible because the Cypress engine is not able to identify the element as a select element (because its not, inner workings are different). It throws :

CypressError: cy.select() can only be called on a . Your
  subject is a: < mdc-select-label
  class=""mdc-select__selected-text"">Select ...< /mdc-select-label>

The problem is that the mdc-select-menu that is the parent for the mdc-select-item has a property of display:none by some internal computations upon opening of the drop-down items.

This property is overwritten to display:flex, but this does not help.

All out of ideas. This works in Selenium, but does not with cypress.io. Any clue what might be a possible hack for the situation other than shifting to other frameworks, or changing the UI code?
",39k,"
            21
        ","['\nAfter much nashing-of-teeth, I think I have an answer.\nI think the root cause is that mdc-select-item has display:flex, which allows it to exceed the bounds of it\'s parents (strictly speaking, this feels like the wrong application of display flex, if I remember the tutorial correctly, however...).\nCypress does a lot of parent checking when determining visibilty, see visibility.coffee,\n## WARNING:\n## developer beware. visibility is a sink hole\n## that leads to sheer madness. you should\n## avoid this file before its too late.\n...\nwhen $parent = parentHasDisplayNone($el.parent())\n  parentNode = $elements.stringify($parent, ""short"")\n\n  ""This element \'#{node}\' is not visible because its parent \'#{parentNode}\' has CSS property: \'display: none\'""\n...\nwhen $parent = parentHasNoOffsetWidthOrHeightAndOverflowHidden($el.parent())\n  parentNode  = $elements.stringify($parent, ""short"")\n  width       = elOffsetWidth($parent)\n  height      = elOffsetHeight($parent)\n\n  ""This element \'#{node}\' is not visible because its parent \'#{parentNode}\' has CSS property: \'overflow: hidden\' and an effective width and height of: \'#{width} x #{height}\' pixels.""\n\nBut, when using .should(\'be.visible\'), we are stuck with parent properties failing child visibility check, even though we can actually see the child.\nWe need an alternate test.\nThe work-around\nRef jquery.js, this is one definition for visibility of the element itself (ignoring parent properties).\njQuery.expr.pseudos.visible = function( elem ) {\n  return !!( elem.offsetWidth || elem.offsetHeight || elem.getClientRects().length );\n}\n\nso we might use that as the basis for an alternative.\ndescribe(\'Testing select options\', function() {\n\n  // Change this function if other criteria are required.\n  const isVisible = (elem) => !!( \n    elem.offsetWidth || \n    elem.offsetHeight || \n    elem.getClientRects().length \n  )\n\n  it(\'checks select option is visible\', function() {\n\n    const doc = cy.visit(\'http://localhost:4200\')\n    cy.get(""mdc-select"").contains(""installation type"").click()\n\n    //cy.get(\'mdc-select-item\').contains(""ITEM1"").should(\'be.visible\') //this will fail\n    cy.get(\'mdc-select-item\').contains(""ITEM1"").then (item1 => {\n      expect(isVisible(item1[0])).to.be.true\n    });\n  });\n\n  it(\'checks select option is not visible\', function() {\n\n    const doc = cy.visit(\'http://localhost:4200\')\n    cy.get(""mdc-select"").contains(""installation type"").click()\n\n    cy.document().then(function(document) {\n\n      const item1 = document.querySelectorAll(\'mdc-select-item\')[0]\n      item1.style.display = \'none\'\n\n      cy.get(\'mdc-select-item\').contains(""ITEM1"").then (item => {\n        expect(isVisible(item[0])).to.be.false\n      })\n    })\n  });\n\n  it(\'checks select option is clickable\', function() {\n\n    const doc = cy.visit(\'http://localhost:4200\')\n    cy.get(""mdc-select"").contains(""installation type"").click()\n    \n    //cy.get(\'mdc-select-item\').contains(""ITEM1"").click()    // this will fail\n    cy.get(\'mdc-select-item\').contains(""ITEM1"").then (item1 => {\n    \n      cy.get(\'mdc-select-item\').contains(""ITEM2"").then (item2 => {\n        expect(isVisible(item2[0])).to.be.true  //visible when list is first dropped\n      });\n          \n      item1.click();\n      cy.wait(500)\n          \n      cy.get(\'mdc-select-item\').contains(""ITEM2"").then (item2 => {\n        expect(isVisible(item2[0])).to.be.false  // not visible after item1 selected\n      });\n    });\n    \n  })\n\n\nFootnote - Use of \'then\' (or \'each\')\nThe way you normally use assertion in cypress is via command chains, which basically wraps the elements being tested and handles things like retry and waiting for DOM changes.\nHowever, in this case we have a contradiction between the standard visibility assertion .should(\'be.visible\') and the framework used to build the page, so we use then(fn) (ref) to get access to the unwrapped DOM. We can then apply our own version of the visibility test using stand jasmine expect syntax.\nIt turns out you can also use a function with .should(fn), this works as well\nit(\'checks select option is visible - 2\', function() {\n  const doc = cy.visit(\'http://localhost:4200\')\n  cy.get(""mdc-select"").contains(""installation type"").click()\n\n  cy.get(\'mdc-select-item\').contains(""ITEM1"").should(item1 => {\n    expect(isVisible(item1[0])).to.be.true\n  });\n});\n\nUsing should instead of then makes no difference in the visibility test, but note the should version can retry the function multiple times, so it can\'t be used with click test (for example).\nFrom the docs,\n\nWhat芒鈧劉s the difference between .then() and .should()/.and()?\nUsing .then() simply allows you to use the yielded subject in a callback function and should be used when you need to manipulate some values or do some actions.\nWhen using a callback function with .should() or .and(), on the other hand, there is special logic to rerun the callback function until no assertions throw within it. You should be careful of side affects in a .should() or .and() callback function that you would not want performed multiple times.\n\nYou can also solve the problem by extending chai assertions, but the documentation for this isn\'t extensive, so potentially it\'s more work.\n', '\nFor convenience and reusability I had to mix the answer of Richard Matsen and Josef Biehler.\nDefine the command\n// Access element whose parent is hidden\nCypress.Commands.add(\'isVisible\', {\n  prevSubject: true\n}, (subject) => {\n  const isVisible = (elem) => !!(\n    elem.offsetWidth ||\n    elem.offsetHeight ||\n    elem.getClientRects().length\n  )\n  expect(isVisible(subject[0])).to.be.true\n})\n\nYou can now chain it from contains\ndescribe(\'Testing select options\', function() {\n  it(\'checks select option is visible\', function() {\n\n    const doc = cy.visit(\'http://localhost:4200\')\n    cy.get(""mdc-select"").contains(""installation type"").click()\n\n    //cy.get(\'mdc-select-item\').contains(""ITEM1"").should(\'be.visible\') // this will fail\n    cy.get(\'mdc-select-item\').contains(""ITEM1"").isVisible()\n  });\n});\n\n', '\nI came across this topic but was not able to run your example. So I tried a bit and my final solution is this. maybe someone other also needs this. Please note that I use typescript.\nFirst: Define a custom command\nCypress.Commands.add(""isVisible"", { prevSubject: true}, (p1: string) => {\n      cy.get(p1).should((jq: JQuery<HTMLElement>) => {\n        if (!jq || jq.length === 0) {\n            //assert.fail(); seems that we must not assetr.fail() otherwise cypress will exit immediately\n            return;\n        }\n\n        const elem: HTMLElement = jq[0];\n        const doc: HTMLElement = document.documentElement;\n        const pageLeft: number = (window.pageXOffset || doc.scrollLeft) - (doc.clientLeft || 0);\n        const pageTop: number = (window.pageYOffset || doc.scrollTop)  - (doc.clientTop || 0);\n        let elementLeft: number;\n        let elementTop: number;\n        let elementHeight: number;\n        let elementWidth: number;\n\n        const length: number = elem.getClientRects().length;\n\n        if (length > 0) {\n            // TODO: select correct border box!!\n            elementLeft = elem.getClientRects()[length - 1].left;\n            elementTop = elem.getClientRects()[length - 1].top;\n            elementWidth = elem.getClientRects()[length - 1].width;\n            elementHeight = elem.getClientRects()[length - 1].height;\n        }\n\n        const val: boolean = !!( \n            elementHeight > 0 && \n            elementWidth > 0 && \n            elem.getClientRects().length > 0 &&\n            elementLeft >= pageLeft &&\n            elementLeft <= window.outerWidth &&\n            elementTop >= pageTop &&\n            elementTop <= window.outerHeight\n        );\n\n        assert.isTrue(val);\n      });\n});\n\nPlease note the TODO. In my case I was targeting a button which has two border boxes. The first with height and width 0. So i must select the second one. Please adjust this to your needs.\nSecond: Use it\ncy.wrap(""#some_id_or_other_locator"").isVisible();\n\n', '\nI could solve it by calling scrollIntoView after getting an element. See this answer.\n']",https://stackoverflow.com/questions/47551639/access-element-whose-parent-is-hidden-cypress-io,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Get Screen Pixel Value in OS X,"
I'm in the process of building an automated game bot in Python on OS X 10.8.2 and in the process of researching Python GUI automation I discovered autopy. The mouse manipulation API is great, but it seems that the screen capture methods rely on deprecated OpenGL methods...
Are there any efficient ways of getting the color value of a pixel in OS X? The only way I can think of now is to use os.system(""screencapture foo.png"") but the process seems to have unneeded overhead as I'll be polling very quickly.
",8k,"
            10
        ","['\nA small improvement, but using the TIFF compression option for screencapture is a bit quicker:\n$ time screencapture -t png /tmp/test.png\nreal        0m0.235s\nuser        0m0.191s\nsys         0m0.016s\n$ time screencapture -t tiff /tmp/test.tiff\nreal        0m0.079s\nuser        0m0.028s\nsys         0m0.026s\n\nThis does have a lot of overhead, as you say (the subprocess creation, writing/reading from disc, compressing/decompressing).\nInstead, you could use PyObjC to capture the screen using CGWindowListCreateImage. I found it took about 70ms (~14fps) to capture a 1680x1050 pixel screen, and have the values accessible in memory\nA few random notes:\n\nImporting the Quartz.CoreGraphics module is the slowest part, about 1 second. Same is true for importing most of the PyObjC modules. Unlikely to matter in this case, but for short-lived processes you might be better writing the tool in ObjC\nSpecifying a smaller region is a bit quicker, but not hugely (~40ms for a 100x100px block, ~70ms for 1680x1050). Most of the time seems to be spent in just the CGDataProviderCopyData call - I wonder if there\'s a way to access the data directly, since we dont need to modify it?\nThe ScreenPixel.pixel function is pretty quick, but accessing large numbers of pixels is still slow (since 0.01ms * 1650*1050 is about 17 seconds) - if you need to access lots of pixels, probably quicker to struct.unpack_from them all in one go.\n\nHere\'s the code:\nimport time\nimport struct\n\nimport Quartz.CoreGraphics as CG\n\n\nclass ScreenPixel(object):\n    """"""Captures the screen using CoreGraphics, and provides access to\n    the pixel values.\n    """"""\n\n    def capture(self, region = None):\n        """"""region should be a CGRect, something like:\n\n        >>> import Quartz.CoreGraphics as CG\n        >>> region = CG.CGRectMake(0, 0, 100, 100)\n        >>> sp = ScreenPixel()\n        >>> sp.capture(region=region)\n\n        The default region is CG.CGRectInfinite (captures the full screen)\n        """"""\n\n        if region is None:\n            region = CG.CGRectInfinite\n        else:\n            # TODO: Odd widths cause the image to warp. This is likely\n            # caused by offset calculation in ScreenPixel.pixel, and\n            # could could modified to allow odd-widths\n            if region.size.width % 2 > 0:\n                emsg = ""Capture region width should be even (was %s)"" % (\n                    region.size.width)\n                raise ValueError(emsg)\n\n        # Create screenshot as CGImage\n        image = CG.CGWindowListCreateImage(\n            region,\n            CG.kCGWindowListOptionOnScreenOnly,\n            CG.kCGNullWindowID,\n            CG.kCGWindowImageDefault)\n\n        # Intermediate step, get pixel data as CGDataProvider\n        prov = CG.CGImageGetDataProvider(image)\n\n        # Copy data out of CGDataProvider, becomes string of bytes\n        self._data = CG.CGDataProviderCopyData(prov)\n\n        # Get width/height of image\n        self.width = CG.CGImageGetWidth(image)\n        self.height = CG.CGImageGetHeight(image)\n\n    def pixel(self, x, y):\n        """"""Get pixel value at given (x,y) screen coordinates\n\n        Must call capture first.\n        """"""\n\n        # Pixel data is unsigned char (8bit unsigned integer),\n        # and there are for (blue,green,red,alpha)\n        data_format = ""BBBB""\n\n        # Calculate offset, based on\n        # http://www.markj.net/iphone-uiimage-pixel-color/\n        offset = 4 * ((self.width*int(round(y))) + int(round(x)))\n\n        # Unpack data from string into Python\'y integers\n        b, g, r, a = struct.unpack_from(data_format, self._data, offset=offset)\n\n        # Return BGRA as RGBA\n        return (r, g, b, a)\n\n\nif __name__ == \'__main__\':\n    # Timer helper-function\n    import contextlib\n\n    @contextlib.contextmanager\n    def timer(msg):\n        start = time.time()\n        yield\n        end = time.time()\n        print ""%s: %.02fms"" % (msg, (end-start)*1000)\n\n\n    # Example usage\n    sp = ScreenPixel()\n\n    with timer(""Capture""):\n        # Take screenshot (takes about 70ms for me)\n        sp.capture()\n\n    with timer(""Query""):\n        # Get pixel value (takes about 0.01ms)\n        print sp.width, sp.height\n        print sp.pixel(0, 0)\n\n\n    # To verify screen-cap code is correct, save all pixels to PNG,\n    # using http://the.taoofmac.com/space/projects/PNGCanvas\n\n    from pngcanvas import PNGCanvas\n    c = PNGCanvas(sp.width, sp.height)\n    for x in range(sp.width):\n        for y in range(sp.height):\n            c.point(x, y, color = sp.pixel(x, y))\n\n    with open(""test.png"", ""wb"") as f:\n        f.write(c.dump())\n\n', ""\nI came across this post while searching for a solution to get screenshot in Mac OS X used for real-time processing. I have tried using ImageGrab from PIL as suggested in some other posts but couldn't get the data fast enough (with only about 0.5 fps).\nThe answer https://stackoverflow.com/a/13024603/3322123 in this post to use PyObjC saved my day! Thanks @dbr! \nHowever, my task requires to get all pixel values rather than just a single pixel, and also to comment on the third note by @dbr, I added a new method in this class to get a full image, in case anyone else might need it.\nThe image data are returned as a numpy array with dimension of (height, width, 3), which can be directly used for post-processing in numpy or opencv etc鈥?getting individual pixel values from it also becomes pretty trivial using numpy indexing. \nI tested the code with a 1600 x 1000 screenshot - getting the data using capture() took ~30 ms and converting it to a np array getimage() takes only ~50 ms on my Macbook. So now I have >10 fps and even faster for smaller regions.\nimport numpy as np\n\ndef getimage(self):\n    imgdata=np.fromstring(self._data,dtype=np.uint8).reshape(len(self._data)/4,4)\n    return imgdata[:self.width*self.height,:-1].reshape(self.height,self.width,3)\n\nnote I throw away the 鈥渁lpha鈥?channel from the BGRA 4 channel.\n"", ""\nThis was all so very helpful I had to come back to comment / however I don't have the reputation.. I do, however, have a sample code of a combination of the answers above for a lightning quick screen capture / save thanks to @dbr and @qqg!\nimport time\nimport numpy as np\nfrom scipy.misc import imsave\nimport Quartz.CoreGraphics as CG\n\nimage = CG.CGWindowListCreateImage(CG.CGRectInfinite, CG.kCGWindowListOptionOnScreenOnly, CG.kCGNullWindowID, CG.kCGWindowImageDefault)\n\nprov = CG.CGImageGetDataProvider(image)\n_data = CG.CGDataProviderCopyData(prov)\n\nwidth = CG.CGImageGetWidth(image)\nheight = CG.CGImageGetHeight(image)\n\nimgdata=np.fromstring(_data,dtype=np.uint8).reshape(len(_data)/4,4)\nnumpy_img = imgdata[:width*height,:-1].reshape(height,width,3)\nimsave('test_fast.png', numpy_img)\n\n""]",https://stackoverflow.com/questions/12978846/python-get-screen-pixel-value-in-os-x,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UIAutomation won't retrieve children of an element,"
I can see that an element with specific Automation ID has children in the Inspect tool:

But when I try to retrieve them like this:
AutomationElement aPane = mainWindow.FindFirst(TreeScope.Subtree, new PropertyCondition(AutomationElement.AutomationIdProperty, ""8264""));
AutomationElementCollection theChildren = aPane.FindAll(TreeScope.Subtree, Condition.TrueCondition);

The aPane element is retrieved correctly, but theChildren element is empty. Any ideas what went wrong?
",12k,"
            10
        ","[""\nOn rare occasions I've found that the Find* calls don't find all automation objects. The only consistent case I've seen with this that WPF TextBlock controls, when in a data template, won't be found by those calls. In these cases, you can try the RawViewWalker which is probably closer to what Inspect is doing internally.\npublic static IEnumerable<AutomationElement> FindInRawView(this AutomationElement root)\n{\n    TreeWalker rawViewWalker = TreeWalker.RawViewWalker;\n    Queue<AutomationElement> queue = new Queue<AutomationElement>();\n    queue.Enqueue(root);\n    while (queue.Count > 0)\n    {\n       var element = queue.Dequeue();\n       yield return element;\n\n       var sibling = rawViewWalker.GetNextSibling(element);\n       if (sibling != null)\n       {\n          queue.Enqueue(sibling);\n       }\n\n       var child = rawViewWalker.GetFirstChild(element);\n       if (child != null)\n       {\n          queue.Enqueue(child);\n       }\n    }\n}\n\n"", '\nActually the problem is that Inspect.exe is written in unmanaged code while I was trying to achieve the same results in managed code. Unmanaged code returns slightly different results than the managed version (e. g. manged code would return control type document where the unmanaged code would return edit in my application).\nWhile it took me some time to understand it, unmanaged code is much faster, more accurate and therefore more reliable. \nSome examples of unmanaged UI automation code for C# can be found in the Microsoft Windows UI Automation Blog e. g. here, \n', '\nA bit of a late answer, but I wanted to correct the answer chosen here. Yes, it\'s true that the VS provided COM wrapper may use a different UIAutomationClient.dll, and that using native code will be different to managed code while calling UIAutomation methods, but nonetheless the question asked here is a different issue. (By the way, you can use a COM wrapper from managed code to call the correct version of the UIAutomation dll\'s, which will solve issues like ""inspect.exe finds it but my managed code cannot"").\nI also ran into the problem asked here (mine was: FindAll(TreeScope.Children, TrueCondition) not returning anything although FindFirst() was successfully returning children on the same control). \nI tried mike-z\'s approach using RawViewWalker to find children and it worked fine for this case. I\'m writing this separate answer to say that it wasn\'t Find* methods being the problem, but a difference between FindAll & FindFirst methods that caused August\'s problem.\nUpdate\nInconsistent behavior seems to be the norm when it comes to MS tools. The reason for this update is, I\'ve bumped into a similar issue with my tlbimp.exe\'d RCW for uia using C#, and this time I wrote a direct equivalent C code and to my surprise it was working perfectly while the C# code refused working in any way while trying to find a simple OpenFileDialog\'s controls, then another control on the main form. The only difference between the two worlds is the mysterious MS RCW magic. I\'m not sure if it\'s the way the marshaling is handled with the automatically created COM wrappers (by tlbimp) or something else. And the [ComConversionLoss] attribute that appears for the created interface doesn\'t sound right to me. Anyways I\'m now considering manually crafting the COM interface or converting my whole project to native environment.\n', '\nThe difference between managed and unmanaged UI Automation is because the managed use old implementation but Inspect uses COM directly and this is newer version 3.0\n', ""\nMy original example is simplified. I tried to access children using 3 techniques:\n\nThe RawViewWalker in .Net managed code.\nThe equivalent walker in COM, that is, the COM wrappers available in .Net managed code.\nNon-.Net code (i.e. unmanaged code) in a completely separate VB6 application I wrote.\n\nOnly the VB6 (unmanaged) code gave the same results as Microsoft's Inspect tool.  I believe this confirms what others have said above: There are severe problems with Microsoft's UI Automation implementation in .Net. It may be that the only solution to this is to write a custom UI Automation client in .Net, but this assumes that the UI Automation servers in the target applications behave correctly.  And those are beyond my control because the target applications are written by other companies, not mine.\n"", '\nFor FindFirst to find a descendant the target element must have IsControlElement set.\nIn the screenshot from the original poster, IsControlElement is being reported as [Not supported].\nIn other words, FindFirst searches the control tree, not the raw tree.\n']",https://stackoverflow.com/questions/14187110/uiautomation-wont-retrieve-children-of-an-element,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run event when any Form loads,"
I'm trying to create a Popularity Contest for Forms in our primary front end. There are many items that are no longer used, but getting details on which are used and which are no longer used is proving to be difficult.
So I came up with the idea of logging a form when it is loaded and then in a year or so I'll run a group by and get an idea of which forms are used, how often, and by who. Now the issue is that I don't want to add a line to every forms InitializeComponent block. Instead I would like to put this in the Program.cs file and some how intercept all Form loads so I can log them.
Is this possible?
Edit
Using @Jimi's comment I was able to come up with the following.
using CrashReporterDotNET;
using System;
using System.Diagnostics;
using System.Linq;
using System.Threading;
using System.Windows.Automation;
using System.Windows.Forms;

namespace Linnabary
{
    static class Program
    {
        /// <summary>
        /// The main entry point for the application.
        /// </summary>
        [STAThread]
        static void Main()
        {
            //This keeps the user from opening multiple copies of the program
            string[] clArgs = Environment.GetCommandLineArgs();
            if (PriorProcess() != null && clArgs.Count() == 1)
            {
                MessageBox.Show(""Another instance of the WOTC-FE application is already running."");
                return;
            }

            //Error Reporting Engine Setup
            Application.ThreadException += ApplicationThreadException;
            AppDomain.CurrentDomain.UnhandledException += CurrentDomainOnUnhandledException;


            Application.EnableVisualStyles();
            Application.SetCompatibleTextRenderingDefault(false);

            //This is the SyncFusion License Key.
            Syncfusion.Licensing.SyncfusionLicenseProvider.RegisterLicense(""<Removed>"");

            //Popularity Contest
            Automation.AddAutomationEventHandler(WindowPattern.WindowOpenedEvent,
                         AutomationElement.RootElement, TreeScope.Subtree, (UIElm, evt) =>
                          {
                              try
                              {
                                  AutomationElement element = UIElm as AutomationElement;
                                  string AppText = element.Current.Name;
                                  if (element.Current.ProcessId == Process.GetCurrentProcess().Id)
                                  {
                                      Classes.Common.PopularityContest(AppText);
                                  }
                              }
                              catch (Exception)
                              {
                                  //throw;
                              }
                          });


            Application.Run(new Forms.frmMain());
        }

        private static void CurrentDomainOnUnhandledException(object sender, UnhandledExceptionEventArgs unhandledExceptionEventArgs)
        {
            ReportCrash((Exception)unhandledExceptionEventArgs.ExceptionObject);
            Environment.Exit(0);
        }

        private static void ApplicationThreadException(object sender, ThreadExceptionEventArgs e)
        {
            ReportCrash(e.Exception);
        }

        public static void ReportCrash(Exception exception, string developerMessage = """")
        {
            var reportCrash = new ReportCrash(""<Removed>"")
            {
                CaptureScreen = true,
                DeveloperMessage = Environment.UserName,
                ToEmail = ""<Removed>""
            };
            reportCrash.Send(exception);
        }

        public static Process PriorProcess()
        {
            Process curr = Process.GetCurrentProcess();
            Process[] procs = Process.GetProcessesByName(curr.ProcessName);
            foreach (Process p in procs)
            {
                if ((p.Id != curr.Id) && (p.MainModule.FileName == curr.MainModule.FileName))
                {
                    return p;
                }
            }
            return null;
        }
    }
}

However, I wonder if there is a way to get the name of the form instead of it's Text. Since this is accessing ALL windows and is therefor outside of the managed space, I doubt it. Still, it works and I'll post this as an answer tomorrow if no one else does so.
",465,"
            2
        ","['\nI\'m posting the code that is required to detect and log Forms activity, for testing or for comparison reasons.\nAs shown, this code only needs to be inserted in the Program.cs file, inside the Main method.  \nThis procedure logs each new opened Form\'s Title/Caption and the Form\'s Name.\nOther elements can be added to the log, possibly using a dedicated method.  \nWhen a new WindowPattern.WindowOpenedEvent event detects that a new Window is created, the AutomationElement.ProcessId is compared with the Application\'s ProcessId to determine whether the new Window belongs to the Application.  \nThe Application.OpenForms() collection is then parsed, using the Form.AccessibleObject cast to Control.ControlAccessibleObject to compare the AutomationElelement.NativeWindowHandle with a Form.Handle property, to avoid Invoking the UI Thread to get the handle of a Form (which can generate exceptions or thread locks, since the Forms are just loading at that time).  \nusing System.Diagnostics;\nusing System.IO;\nusing System.Security.Permissions;\nusing System.Windows.Automation;\n\nstatic class Program\n{\n    [STAThread]\n    [SecurityPermission(SecurityAction.Demand, Flags = SecurityPermissionFlag.ControlAppDomain)]\n    static void Main(string[] args)\n    {\n        Automation.AddAutomationEventHandler(\n            WindowPattern.WindowOpenedEvent, AutomationElement.RootElement,\n            TreeScope.Subtree, (uiElm, evt) => {\n                AutomationElement element = uiElm as AutomationElement;\n                if (element == null) return;\n                try \n                {\n                    if (element.Current.ProcessId == Process.GetCurrentProcess().Id)\n                    {\n                        IntPtr elmHandle = (IntPtr)element.Current.NativeWindowHandle;\n                        Control form = Application.OpenForms.OfType<Control>()\n                            .FirstOrDefault(f => (f.AccessibilityObject as Control.ControlAccessibleObject).Handle == elmHandle);\n\n                        string log = $""Name: {form?.Name ?? element.Current.AutomationId} "" +\n                                     $""Form title: {element.Current.Name}{Environment.NewLine}"";\n                        File.AppendAllText(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, ""formLogger.txt""), log);\n                    }\n                }\n                catch (ElementNotAvailableException) { /* May happen when Debugging => ignore or log */ }\n            });\n    }\n}\n\n', '\nYeah, this should be easy. There are event hooks like OnLoad, OnShow, OnClose() for all forms and most user controls. If you wanted to see, at a more granule level what controls are being used by your users, you can hook up OnClick(), OnMouseOver() and about a hundred other events. \n... and you can create your own custom events. \nSo, hook up the events by selecting the form, then properties (right click or F4 key). In the properties window at the top, you\'ve got a ""show events"" button that looks like a lightning bolt. Click that and then pick, from the list, the event you want to use for this logging. \n\n', '\nA not so expensive (maybe) solution can be this:\nCreate a new class MyBaseForm, which inherits from System.Windows.Forms.Form, and handle its load event in the way you need.\nNow the hard part: modify all of the existing forms classes so they inherit from MyBaseForm and not from the default System.Windows.Forms.Form; and be sure you do the same for every future Form you will add to your solution.\nNot bullet proof at all, it can be easy to forget to modify the base class for a new form and/or to miss the modification for an existing form class\nBut you can give it a try\n', '\nApplying an IMessageFilter to the application to detect the WM_Create message and then determining if the target handle belonged to a Form would be ideal solution with a minimal performance hit.  Unfortunately, that message does not get passed to the filter.  As an alternative, I have selected the WM_Paint message to reduce the performance impact.  The following filter code creates a dictionary of form type names and a count of Form\'s with that name ultimate disposal.  The Form.Closed Event is not reliable under all closure conditions, but the Disposed event appears reliable.\ninternal class FormCreationFilter : IMessageFilter\n{\n    private List<Form> trackedForms = new List<Form>();\n    internal Dictionary<string, Int32> formCounter = new Dictionary<string, Int32>(); // FormName, CloseCount\n\n    public bool PreFilterMessage(ref Message m)\n    {\n        // Ideally we would trap the WM_Create, butthe message is not routed through\n        // the message filter mechanism.  It is sent directly to the window.\n        // Therefore use WM_Paint as a surrgogate filter to prevent the lookup logic \n        // from running on each message.\n        const Int32 WM_Paint = 0xF;\n        if (m.Msg == WM_Paint)\n        {\n            Form f = Control.FromChildHandle(m.HWnd) as Form;\n            if (f != null && !(trackedForms.Contains(f)))\n            {\n                trackedForms.Add(f);\n                f.Disposed += IncrementFormDisposed;\n            }\n        }\n        return false;\n    }\n\n    private void IncrementFormDisposed(object sender, EventArgs e)\n    {\n        Form f = sender as Form;\n        if (f != null)\n        {\n            string name = f.GetType().Name;\n            if (formCounter.ContainsKey(name))\n            {\n                formCounter[name] += 1;\n            }\n            else\n            {\n                formCounter[name] = 1;\n            }\n            f.Disposed -= IncrementFormDisposed;\n            trackedForms.Remove(f);\n        }\n    }\n}\n\nCreate an instance and install the filter similar to the following example.  The foreach loop is just shown to demonstrate accessing the count. \n    static void Main()\n    {\n        Application.EnableVisualStyles();\n        Application.SetCompatibleTextRenderingDefault(false);\n\n        FormCreationFilter mf = new FormCreationFilter();\n        Application.AddMessageFilter(mf);\n\n        Application.Run(new Form1());\n        Application.RemoveMessageFilter(mf);\n\n        foreach (KeyValuePair<string, Int32> kvp in mf.formCounter)\n        {\n            Debug.Print($""{kvp.Key} opened {kvp.Value} times. "");\n        }\n    }\n\n']",https://stackoverflow.com/questions/55955331/run-event-when-any-form-loads,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UI automation with excel,"
I am new to UI Automation. In my current organisation I was tasked with making an automated tool using GUI(Graphics User Interface) screen reading, but it is not working perfectly with other my colleague's machine because of a difference in screen resolution. 
I watched this link on you-tube to try and understand UI Automation with excel, but I can't find much on this topic anywhere else.
Can anyone direct me toward resources on UI Automation? I Would like to know where I can learn it, read about it, and how to implement it with Excel.
Thanks in advance I really appreciate if anyone could help me. 
",11k,"
            2
        ","['\nUIAutomation from Microsoft is very powerfull and works well with windows 7, 8, 10 also from visual basic for applications (32 and 64 bits) and can be handy used to do some nice GUI Automation without expensive tools.\nMake sure in VBA reference you have UIAutomationCore.Dll references (and weird enough sometimes on some computers you have to copy this to your documents folder)\nBelow you can see 2 base examples but as MS Automation is a huge library for all routines you can read a lot on MSDN for full documentation.\nI use the MS UIA routines in AutoIt and in VBA\n\nFor AutoIt its shared over here\n\nhttps://www.autoitscript.com/forum/topic/153520-iuiautomation-ms-framework-automate-chrome-ff-ie/\n\nFor VBA I do not have a standard library but someone did a try with\nthis\nhttps://github.com/mhumpher/UIAutomation_VBA\n\nOption Explicit\nSub test()\n    Dim c As New CUIAutomation\n    Dim oDesktop As IUIAutomationElement\n\n    Set oDesktop = c.GetRootElement\n\n    Debug.Print oDesktop.CurrentClassName & vbTab & oDesktop.CurrentName & vbTab & oDesktop.CurrentControlType\n\nEnd Sub\n\n\'Test uia just dumps all windows of the desktop to the debug window\nSub testUIA()\n    Dim allEl As IUIAutomationElementArray                  \'Returns an element array with all found results\n    Dim oElement As IUIAutomationElement                    \'Reference to an element\n    Dim ocondition As IUIAutomationCondition\n\n    Dim i As Long\n    Dim x As New clsUIA\n\n\n    \'Just reference the three mainly used properties. many more are available when needed\n    Debug.Print x.oDesktop.CurrentName & x.oDesktop.CurrentClassName & x.oDesktop.CurrentControlType\n\n    Set ocondition = x.oCUIAutomation.CreateTrueCondition             \'Filter on true which means get them all\n    Set allEl = x.oDesktop.FindAll(TreeScope_Children, ocondition)    \'Find them in the direct children, unfortunately hierarchies are strange sometimes\n\n    \'Just reference the three mainly used properties. many more are available when needed\n    For i = 0 To allEl.Length - 1\n        Set oElement = allEl.GetElement(i)\n        \' If oElement.CurrentClassName = ""PuTTY"" Then\n          Debug.Print oElement.CurrentClassName & oElement.CurrentName & oElement.CurrentControlType\n           \' Debug.Print oElement.CurrentBoundingRectangle\n\n            oElement.SetFocus\n            DoEvents\n            Sleep 2000\n       \' End If\n\n    Next\nEnd Sub    \n\n', '\nSeems like you are doing the automation using the coordinates, which changes when you switch to other resolution. If this is the case, please automate your application using ID, Class, Xpath, CSS etc. This link will help you in that: http://www.guru99.com/locators-in-selenium-ide.html\nFor automation using Excel, please look into the following link:\nhttp://www.guru99.com/all-about-excel-in-selenium-poi-jxl.html\n', ""\ncreate clsUIA class  then insert this code\n'clsUIA with some logic like\n'start by add the following code\nDim  c As New CUIAutomation\nPublic oCUIAutomation As New CUIAutomation\nFunction oDesktop() As IUIAutomationElement\nSet oDesktop = c.GetRootElement\nEnd Function\n\n""]",https://stackoverflow.com/questions/44756042/ui-automation-with-excel,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run tests with Karate-Chrome (Connection refused exception),"
Karate UI-based tests run successfully locally with karate-chrome (steps here) which starts a container with an exposed port.
Now, I am trying to run the tests within the karate-chrome container in CI. I have started the karate-chrome container with KARATE_SOCAT_START=trueand then executed the java -jar (standalone jar) command to run the tests. Non-UI based tests pass but the UI tests are throwing the following exception:
ERROR com.intuit.karate - http request failed: 
15:26:09 DOCKER: org.apache.http.conn.HttpHostConnectException: Connect to localhost:9222 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused (Connection refused)

Note that driverTarget with docker has not been configured. Only the following driver's configuration:
  * configure driver = { type: 'chrome', start: false, showDriverLog: true, port:9222 ,pollAttempts: 5}

Is it possible to make it work this way or should a custom docker image be set up?
",659,"
            1
        ","[""\nI was not able to run UI-based tests directly on karate-chrome. Therefore, I ended up creating a new docker image (docker-openjdk8) to run the tests with driverTarget:\n* configure driverTarget = { docker: 'ptrthomas/karate-chrome', showDriverLog: true}\n\nThis basically runs the tests with Docker-in-Docker (DIND) process. Would be happy to know if anybody else has got a better solution.\n"", ""\nI know this question is old, but today I stumbled upon this very same issue where I had to configure Karate UI tests for automation. The solution I found was to configure the chrome driver as follows:\n* configure driver = { type: 'chrome', showDriverLog: true, port: 9222, pollAttempts: 5, addOptions:['--no-sandbox'], headless: true }\n\nThen the karate standalone jar was able to launch the browser and all tests passed.\nHope this helps!\n"", ""\nSorry, your setup is not clear. But to use configure driverTarget = { docker: 'ptrthomas/karate-chrome' } means Docker should be installed on the machine where you are running the test.\nI suggest you start looking at the 1.0 release, and we have a reference test that uses the Docker container a little differently, without using driverTarget. You can find details here: https://stackoverflow.com/a/66005331/143475\nIt would be great if after you get this working, you post your solution as another answer here so that it helps others and helps us improve the documentation if needed.\n""]",https://stackoverflow.com/questions/66273843/run-tests-with-karate-chrome-connection-refused-exception,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Chrome opens with ""Data;"" with selenium","
I am a newbie to Selenium and trying to open localhost:3000 page from Chrome via selenium driver. 
The code is : 
import com.google.common.base.Function;
import org.openqa.selenium.By;
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.WebDriverException;
import org.openqa.selenium.chrome.ChromeDriver;
import org.openqa.selenium.firefox.FirefoxDriver;
import org.openqa.selenium.firefox.FirefoxDriver;
public class SeleniumTests {

    public static void main(String[] args) {


        System.setProperty(""webdriver.chrome.driver"", ""C://chromedriver_win32//chromedriver.exe"");
        WebDriver driver = new ChromeDriver();              
        driver.get(""localhost:3000"");
    }

}

However, this opens my chrome window with a ""data;"" . 
The chrome version is 50.0.2661.94
Any idea what is the exact issue?
",60k,"
            35
        ","[""\nSpecify the protocol you are using, so instead of localhost:3000, use http://localhost:3000. If that doesn't help, see the comment here on the Chromium issue tracker.\n"", '\nI was also getting the same issue. I updated ChromeDriver to the latest version and that fixed it.\n', '\nYes it will start with data. After data just try to give the URL.The \'data:,\' URL is just the default address that chromedriver navigates to when launching chrome. So this by itself doesn\'t necessarily mean that anything is going wrong.\nimport com.google.common.base.Function;\nimport org.openqa.selenium.By;\nimport org.openqa.selenium.WebDriver;\nimport org.openqa.selenium.WebDriverException;\nimport org.openqa.selenium.chrome.ChromeDriver;\nimport org.openqa.selenium.firefox.FirefoxDriver;\nimport org.openqa.selenium.firefox.FirefoxDriver;\npublic class SeleniumTests {\n\npublic static void main(String[] args) {\n\n\n    System.setProperty(""webdriver.chrome.driver"", ""C://chromedriver_win32//chromedriver.exe"");\n    WebDriver driver = new ChromeDriver();              \n    driver.get(""https://www.google.co.in/?gfe_rd=cr&ei=KxAzV8-KEJPT8gfT0IWYAw"");\n}\n\n}\n\nIt will open successfully. Reply if you have any query. Happy Learning.. :-)\n', ""\nMake sure you are using latest release of ChromeDriver (as for now it's 2.28). I had same problem with data:,. By mistake I've downloaded old version and got the issue with specified URL not being opened, just data:,\n"", '\nI\'ve been running in a similar situation, the fix in my case was simply to upgrade chrome webdriver to its latest version (in my case V2.27).\nThe cause of showing Data; instead of the real application URL was that:\nWebDriver driver = new RemoteWebDriver(new URL(""http://<host>:<port>/wd/hub""), desiredCapabilities);\n\nfailed to get created. Instead, driver object was holding a null value.\nSo after chrome driver upgrade , it had been created correctly and problem solved.\nHope this helps who\'s still stuck!\n', ""\nIf you're using Codeception, start the test with :\n$I->amOnPage('/');\n"", '\nYou need to add two things to run :\nFirst - you should use http://localhost:3000\nSecond - You must use debug port before creating webDriver as : options.addArguments(""--remote-debugging-port=9225"");\nWhole Code:\n    WebDriverManager.chromedriver().setup();\n    ChromeOptions options = new ChromeOptions();\n    options.setExperimentalOption(""useAutomationExtension"", false);\n    options.addArguments(""--remote-debugging-port=9225"");\n    WebDriver driver = new ChromeDriver(options);\n\nDrop comments if you have any query\n', '\nThis just happened to me when using selenium grid with python and was caused by something different than the other answers suggest (in my case at least).\nIt turns out there was a runtime exception being raised after the driver object was being created (and connecting to chrome) but before it was being instructed to navigate to a URL. This all runs on a celery task queue so it was easy for me to miss. So if updating the chrome driver doesn\'t work, double check that you\'re navigating to a URL correctly and there\'s no errors etc.\nFor example:\ndriver = webdriver.Remote(\n        command_executor=""http://<ip>:4444/wd/hub"",\n    )\n\n# a function here raised a runtime exception, causing chrome to launch\n# but sit there with the default URL ""data;/""\n\ndriver.get(""www.google.com"")\n\n', '\nI had this problem too. The advice I got here did not help. Eventually I figured out that putting this:\nSystem.setProperty(""webdriver.chrome.driver"", ""/usr/bin/chromedriver""); \nat start of my test-script, solved the problem.\n', '\njust replace the ""chromedriver.exe"" with latest release of ChromeDriver. \n']",https://stackoverflow.com/questions/37159684/chrome-opens-with-data-with-selenium,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to setup Appium on Mac OS to run automated tests from JAVA classes on Android and iOS devices,"
Problem I was facing with Appium that I decided myself below. I could not find a way to install required components for Appium without using SUDO. After installing it all with SUDO, then trying to run Appium, I was getting error that Appium and NODE should have been installed without SUDO. Trying to search online for solution took me quite a while since there are almost no tutorials exist online for Appium to run on MAC while developing tests using JAVA. Below is the step by step instruction on how to set up Appium on Mac OS and run a first test from within a Java Class. Just copy and paste commands into Terminal on your Mac and you will set it up. I wish there were more clear step by step tutorials online for Appium. Tutorials written by developers of Appium are so vague, I don't even want to recommend to look for answers on their website.
",97k,"
            20
        ","['\nAnswered by Igor Vishnevskiy\nI have been looking for the answer everywhere on the internet and could not find anything. It took me some time to make this work. I hope this quick guide will help the next engineer to save some time on setting up Appium to run automation on Android devices. Appium will not run if NODE or Appium itself is installed using SUDO and MAC won\'t let you install neither without using SUDO. There is a workaround though. My steps make it possible to install and setup Appium the right way without need to use SUDO for installation. Everything is tested and it works. Below are the steps. Enjoy!\nThere could be one problem while setting up Appium using bellow steps. If you face some errors while creating or saving data into certain directories, that is caused by the luck of write permissions set to those directories. What you will need to do is to set CHMOD to 777 to the directories where components of Appium are trying to write while installing and then rerun all steps again.\nStep 1:\nInstall JAVA 6. You will need JAVA 6 with Appium. JAVA 6 for Mac OS has to be downloaded from Apple\'s support page:\nhttp://support.apple.com/kb/DL1572\nStep 2:\nIn your bash add the following path using following format:\nexport PATH=$HOME/local/bin:$PATH\n\nStep 3:\nSetup Maven (Download and set Bash profile PATH for Maven):\nhttp://maven.apache.org/download.cgi\n\n------>\nThis is what your Bash Profile should look like:\nexport PATH=""/Users/your_username/Desktop/adt-bundle-mac-x86_64-20140702/sdk/platform-tools"":$PATH\nexport PATH=""/Users/your_username/Desktop/adt-bundle-mac-x86_64-20140702/sdk/tools"":$PATH\nexport PATH=$HOME/local/bin:$PATH\nexport ANDROID_HOME=/Users/your_username/Desktop/adt-bundle-mac-x86_64-20140321/sdk\nexport PATH=""/Users/your_username/Desktop/apache-maven-3.2.2/bin"":$PATH\nexport JAVA_HOME=$(/usr/libexec/java_home -v 1.6)\n\nObviously to run tests on Android device, you will need to download Android SDK and add it to your Bash Profile as well. To run tests on iOS devices, you will only need to install XCode, no need to add that to your Bash profile. But Android SDK has to be added.\nStep 4:\nCopy and paste following sequence of commands into your Terminal window and press ENTER. Copy and pasting it all together will work. It will take some time to install NODE, so be patient.\necho \'export PATH=$HOME/local/bin:$PATH\' >> ~/.bashrc\n. ~/.bashrc\nmkdir ~/local\nmkdir ~/node-latest-install\ncd ~/node-latest-install\ncurl http://nodejs.org/dist/node-latest.tar.gz | tar xz --strip-components=1\n./configure --prefix=~/local\nmake install\n\nStep 5:\nAfter installation from Step 4 is complete, run following command in your Terminal window:\ncurl https://www.npmjs.com/install.sh | sh\n\nStep 6:\nThen in your Terminal window execute following command:\nnpm install -g grunt-cli\n\nStep 7:\nThen in your Terminal window execute following command:\nnpm install -g appium\n\nStep 8:\nThen in your Terminal window execute following command:\nnpm install wd\n\nStep 9:\nThen in your Terminal window execute following command to start the Appium server:\nappium &\n\n(step 9 will start the server).\nStep 10:\nFrom the separate terminal Window\ncd to root directory of your JAVA project in your workspace.\n(example: cd /Users/ivishnevskiy/Documents/workspace/ApiumJUnit)\nStep 11:\nAttach your Android device to USB and to your MAC computer.\nStep 12:\nIn the same Terminal window from Step 10, run following command to launch the Appium test:\nmvn -Dtest=test.java.com.saucelabs.appium.AndroidContactsTest test\n\nwhere test.java.com.saucelabs.appium is a package name\nand\nAndroidContactsTest is a class name.\nIf you still need help setting it up. Let me know. I can help. My LinkedIn:\nhttp://www.linkedin.com/pub/igor-vishnevskiy/86/51a/b65/\n\nAFTER SETTING APPIUM UP ON YOUR DEVICE, FOLLOW MY NEXT TUTORIAL TO CREATE IN ECLIPSE AND RUN YOUR FIRST TEST ON THE ACTUAL iOS DEVICE (NOT EMULATOR):\nhttps://stackoverflow.com/questions/24919159/\n\n', '\nI used this post to help me set up Appium on my Mac. I also used other sources to do my installation completely. Here are step by step instructions to upgrade to appium 1.7.x seamlessly on your Mac OS X.\nPlease make a note of the following details BEFORE you start the upgrade process\n\nIf Appium is not installed on your system previously, please use ONLY the commands related to ""Install"" below\nIf you face any problem of deleting folder/directories using command line, please go to Finder and delete it\nOnce you upgrade to new OS on your Mac machine, App Store and iTunes may open late and work slow for the first time\n\nStep by Step Instructions\n\nNeed to install OS 10.12.x or higher version.\nNeed to install Xcode 9.x. Sign in with your developer account (https://developer.apple.com/download/more/) and download it OR Download it free from the Mac App store\n\nNote - If you face problems while installing the new version of Xcode then please uninstall the old versions.\n\nNeed to install the Command line tools for Xcode 9.x.\n\nLaunch Terminal and enter the below command\nxcode-select --install\n\n\nUninstall HomeBrew\n\nUninstall Command:\n/usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/uninstall)""\n\n\nInstall HomeBrew\n\nInstall Command:\n/usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""\n\n\nUninstall all instances of Node\n\ngo to /usr/local/lib and delete any node and node_modules\ngo to /usr/local/include and delete any node and node_modules directory\nif you installed with brew install node, then run brew uninstall node in your terminal\ncheck your Home directory for any local or lib or include folders, and delete any node or node_modules from there. (To reach home directory open Terminal and enter cd)\ngo to /usr/local/bin and delete any node executable\n\nInstall Node\n\nCommand:\nbrew install node\n\n\nInstall ideviceinstaller:\n\nCommand:\nbrew install ideviceinstaller\n\n\nUninstall Appium from terminal\n\nCommand:\nnpm uninstall -g appium\nnpm cache clean --force\n\n\nInstall Appium\n\nCommand:\nnpm install -g appium@1.7.2** (Or we can just do npm install -g appium)\n\n\nNeed to Install supporting tools for Appium 1.7.2\n\nCommand:\nbrew install carthage\nnpm install -g ios-deploy\nnpm install -g deviceconsole\n\n\nFor downloading simulators go to Xcode --> Preferences --> Components, and download necessary simulators.\n\n', '\nSteps that need to follow:\n\ninstall xcode\ninstall xcode command line tool\ninstall Appium GUI *.dmg file Appium\nInstall homebrew (assuming you have ruby installed on your mac, if not install ruby first)\nInstall Java (it should come with mac OS)\nInstall node and Maven using brew command from terminal\nInstall Appium server using node \n\n\nnpm install 鈥揼 appium\nappium &\n\nAuthorize your iOS simulator and device to access by Appium by typing the command from terminal: sudo authorize_ios\n\n:)\nI have made a video about how to configure appium on a Mac computer which can be viewed here.\nAnd slides can be viewed here.\n', '\nFollow these steps.\nPre-requisites to download.\n1. Appium\n2. Android SDK\n3. Java JDK\n4. Android .apk file\n5. Xcode and command line tools\nProcess:\n\nInstall Xcode with command line tools and appium.\nDownload all the Android SDK necessary tools, that includes mandatorily platform-tools and build-tools\nDownload and install Java JDK \n\nSetting $Path and Configuring\n\nOpen bash_profile with the command open .bash_profile\nCopy the contents to your .bash_profile\nexport ANDROID_HOME=/Users/username/Library/Android/sdk .  (copy it from the sdk manager in android studio)\nexport PATH=$ANDROID_HOME/platform-tools:$PATH\nexport PATH=$ANDROID_HOME/tools:$PATH\nexport JAVA_HOME=$(/usr/libexec/java_home)export       PATH=""/usr/local/opt/openssl/bin:$PATH""\n\n\nCopy the above, and save the .bash_profile\n\nGo to Appium, and click on Android symbol. Select and choose the .apk(place the apk in the project folder)\nTick on the Device name and choose the applicable Android version in the capabilities.\nIn the Advance settings under Android, choose the sdk path(Copy from the android sdk manager)\nClick on the settings symbol, and add value to the environment variables\n       ANDROID_HOME          /Users/username/Library/Android/sdk (Copy the path from sdk manager)\n\nConnect the device or launch the emulator, and click on Launch in appium, then click on Inspector, this should create a session and launch the app in your mobile and grab the current screenshot.\n\n', '\nSorry its a little messy take it from my notes  ;\nconsider \nyou have a system enviroment its located in .bash_profile\nyou have to add jre jdk files there \nalso android sdk if you want to run appium for android and ios from mac \nthere is a ui automator you have to install it \nthere is a setup dr in appium you can check to see if you install appium correctly \nits the steps i take to run appium on ios for [android device and ios device]  ;\ni note every step \nsome step might be un necessary\ni wish it help you   \ngo to terminal  :\necho \'export PATH=$HOME/local/bin:$PATH\' >> ~/.bashrc\n. ~/.bashrc\nmkdir ~/local\nmkdir ~/node-latest-install\ncd ~/node-latest-install\ncurl http://nodejs.org/dist/node-latest.tar.gz | tar xz --strip-components=1\n./configure --prefix=~/local \nmake install\n\nsudo ln -s /path_to_maven_folder/bin/mvn /usr/bin/mvm\n\nnpm ln -s /Users/[your username]/Desktop/Appium/Tools/apache-maven-3.2.5/bin/mvn /usr/bin/mvn\n\n$ mvn 鈥搗ersion\n\ndownload android bundle for iOS then run command \ntools/android update sdk --no-ui\n\ncurl https://www.npmjs.org/install.sh | sh\nnpm install -g grunt-cli\nnpm install -g appium\nnpm install wd\n\n====================================\nOpen and edit .bash_profile file\nopen -e .bash_profile\nIf you don鈥檛 have .bash_profile file in your computer path, then create one. Enter below command to create a new file. Once created follow Step-2.\ntouch .bash_profile\nStep-3\nadd and save \nand again run in terminal\n\nexport ANDROID_HOME=/Applications/Appium/Tools/android-sdk-macosx/\n  export PATH=${PATH}:$ANDROID_HOME/tools:$ANDROID_HOME/platform-tools\n  step 4  set java home  export\n  JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/CurrentJDK/Home\n\nand add it to bash profile \ninstall Eclipse :\ninstall test ng in eclipse\nadd selenium library \nadd maven \nNew java project \nudid >>find udi in xcode \ninstall app in simulator\ndefaults write \ncom.apple.Finder AppleShowAllFiles TRUE\ngo to finder \\\nenable ui automator in XCODE Device\nsetting >>developer \n\ninstall brew  ruby -e ""$(curl -fsSL\n  https://raw.githubusercontent.com/Homebrew/install/master/install)鈥漒n\ninstall idevice\nbrew install ideviceinstaller\nadd maven jar files\nupdate java to 1.8 \ninstall java then run this in terminal \nexport JAVA_HOME=""/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home鈥漒n\npreference  eclipse java select search add newest version\n', '\nHere is the step by step installation of appium on mac via terminal. After where you can run your java class file with the appium server start from script.\nInstallation of Appium in MAC\nStep 1 : Install java JDK:\n-- > Download Link here : http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\nStep 2 : Install Android Studio:\n--> Download Link here: https://developer.android.com/studio/index.html\nScroll till last and find the software as below: \nandroid-studio-ide-173.4819257-mac.dmg\nStep 3 : Install Home brew\n--> In Terminal install brew:\nruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""\n\nStep 4 : Install Node.js\n--> install node.js:\ndownload installer: https://nodejs.org/en/download/ and run\nStep 5 : Install npm\n--> In Terminal install npm:\nbrew update\nbrew install node\nnpm install -g npm\n\nStep 6 : To setup Environment \nTo open bash_profile in terminal, type the following command:\nopen -e .bash_profile\n\nIf bash profile not available, create a bash_profile by following command\nStart up Terminal\nType ""cd ~/"" to go to your home folder\nType ""touch .bash_profile"" to create your new file.\nTo Edit .bash_profile where you can just type ""open -e .bash_profile"" to open it in TextEdit.\npaste following:\nexport ANDROID_HOME=/Users/user/Library/Android/sdk\nexport PATH=$ANDROID_HOME/platform-tools:$PATH\nexport PATH=$ANDROID_HOME/tools:$PATH\n\nexport JAVA_HOME=$(/Library/Java/JavaVirtualMachines/jdk1.8.0_141.jdk)\nexport PATH=${JAVA_HOME}/Contents/Home/bin:$PATH\nexport PATH=/usr/local/bin:/usr/local/sbin:~/bin:$PATH\nexport PATH=$PATH:/opt/bin:$PATH\n\nsave (Ctrl+S) and exit\nNOTE : As default Android studio and Java takes the above path, if you have changed the path then do change it here with correct version\nStep 7. Install Appium\nIn Terminal install Appium command line:\nnpm install -g appium@1.6.0-beta1\nnpm install wd\n\nStep 8. Install Carthage \nNow type the following command to get into WebDriverAgent and Install carthage:\ncd /usr/local/lib/node_modules/appium/node_modules/appium-xcuitest-driver/WebDriverAgent\nbrew install carthage\nnpm i -g webpack\n\nStep 9. Run Appium in command line\nFrom above steps do \'cd\' to get out of all subfolders and then type \'appium\'\ncd\nappium\n\nIf everything works correct you should get the following line \n[Appium] Welcome to Appium v1.6.0-beta1\n[Appium] Appium REST http interface listener started on 0.0.0.0:4723\n\nNow you have to run java class file from the script where have to start the appium server from script, here are the steps to follow\npublic AppiumDriver<WebElement> setUp() throws Exception {\n\n    builder = new AppiumServiceBuilder();\n    //builder.usingAnyFreePort();\n    //builder.withIPAddress(""0.0.0.0"");\n    //builder.usingPort(4723);\n    builder.withCapabilities(cap);\n    builder.withArgument(GeneralServerFlag.SESSION_OVERRIDE);\n    builder.withArgument(GeneralServerFlag.LOG_LEVEL,""error"");\n    builder.withAppiumJS(newFile(""/usr/local/lib/node_modules/appium/build/lib/main.js""));\n    builder.usingDriverExecutable(new File(""/usr/local/bin/node""));\n    service = AppiumDriverLocalService.buildService(builder);\nservice.start();\n\n\nDesiredCapabilities capabilities = new DesiredCapabilities();\n    capabilities.setCapability(""app"", ""/Users/user/Documents/yourapp.app"");\n    capabilities.setCapability(""noReset"", ""false"");\n    capabilities.setCapability(MobileCapabilityType.VERSION, ""12.0"");\n    capabilities.setCapability(MobileCapabilityType.PLATFORM, ""iOS"");\n    capabilities.setCapability(MobileCapabilityType.UDID,""abcddjkfg..."");  \n    capabilities.setCapability(""bundleId"", ""com..."");\n    capabilities.setCapability(MobileCapabilityType.AUTOMATION_NAME, ""XCUITest"");\n    capabilities.setCapability(MobileCapabilityType.DEVICE_NAME, ""user\'s iPhone"");\n\n    driver = new IOSDriver<>(new URL(""http://0.0.0.0:4723/wd/hub""), capabilities);\n\n    driver.manage().timeouts().implicitlyWait(10, TimeUnit.SECONDS);\n    return driver;\n}\n\nIn terminal get the path\necho $PATH\n\nCopy the path and note down\n\nNow open your Run configuration (Eclipse)\nSelect the class file you are going to run\nClick on Environment Tab\nClick New\nGive the variable name as \'PATH\'\nNow paste the copied path in value and save it\n\nHere is the image link for reference\nSet Path in eclipse\nNow you ran the script and you could see the appium server start in editor console and app will get launch in the device and your script will run\nNOTE : For real device iOS automation, you can use appium desktop GUI and some stuff are additionally yet to install for real device iOS automation do the following link for configuration \nhttps://techsouljours.blogspot.com/2018/08/install-appium-on-mac.html\nPost author and executed by https://www.linkedin.com/in/shiv-shankar-siddarth/\n']",https://stackoverflow.com/questions/24813589/how-to-setup-appium-on-mac-os-to-run-automated-tests-from-java-classes-on-androi,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running UIAutomation scripts from Xcode,"
Did anyone succeed in setting up automated UIAutomation tests in Xcode?
I'm trying to set up a target in my Xcode project that should run all the UIAutomation scripts I prepared. Currently, the only Build Phase of this target is this Run Script block:
TEMPLATE=""/Applications/Xcode.app/Contents/Applications/Instruments.app/Contents/PlugIns/AutomationInstrument.bundle/Contents/Resources/Automation.tracetemplate""
MY_APP=""/Users/Me/Library/Application Support/iPhone Simulator/6.0/Applications/564ED15A-A435-422B-82C4-5AE7DBBC27DD/MyApp.app""
RESULTS=""/Users/Me/Projects/MyApp/Tests/UI/Traces/Automation.trace""
SCRIPT=""/Users/Me/Projects/MyApp/Tests/UI/SomeTest.js""
instruments -t $TEMPLATE $MY_APP -e UIASCRIPT $SCRIPT -e UIARESULTSPATH $RESULTS

When I build this target it succeeds after a few seconds, but the script didn't actually run. In the build log I get these errors:
instruments[7222:707] Failed to load Mobile Device Locator plugin
instruments[7222:707] Failed to load Simulator Local Device Locator plugin
instruments[7222:707] Automation Instrument ran into an exception while trying to run the script.  UIATargetHasGoneAWOLException
+0000 Fail: An error occurred while trying to run the script.
Instruments Trace Complete (Duration : 1.077379s; Output : /Users/Me/Projects/MyApp/Tests/UI/Traces/Automation.trace)

I am pretty sure, that my javascript and my run script are both correct, because if I run the exact same instruments command in bash it works as expected. 
Could this be a bug in Xcode?
",9k,"
            20
        ","['\nI finally found a solution for this problem. It seems like Xcode is running the Run Scripts with limited rights. I\'m not entirely sure, what causes the instruments command to fail, but using su to change to your user will fix it.\nsu $USER -l -c <instruments command>\n\nObviously, this will ask you for your password, but you can\'t enter it when running as a script. I didn\'t find a way to specify the password for su, however if you run it as root, you don\'t have to specify one. Luckily sudo can accept a password via the pipe:\necho <password> | sudo -S su $USER -l -c <instruments command>\n\nIf you don\'t want to hardcode your password (always a bad idea), you could use some AppleScript to ask for the password.\nI posted the resulting script below. Copy that to a *.sh file in your project and run that script from a Run Script.\n#!/bin/bash\n\n# This script should run all (currently only one) tests, independently from\n# where it is called from (terminal, or Xcode Run Script).\n\n# REQUIREMENTS: This script has to be located in the same folder as all the\n# UIAutomation tests. Additionally, a *.tracetemplate file has to be present\n# in the same folder. This can be created with Instruments (Save as template...)\n\n# The following variables have to be configured:\nEXECUTABLE=""TestApp.app""\n\n# Optional. If not set, you will be prompted for the password.\n#PASSWORD=""password""\n\n# Find the test folder (this script has to be located in the same folder).\nROOT=""$( cd -P ""$( dirname ""${BASH_SOURCE[0]}"" )"" && pwd )""\n\n# Prepare all the required args for instruments.\nTEMPLATE=`find $ROOT -name \'*.tracetemplate\'`\nEXECUTABLE=`find ~/Library/Application\\ Support/iPhone\\ Simulator | grep ""${EXECUTABLE}$""`\nSCRIPTS=`find $ROOT -name \'*.js\'`\n\n# Prepare traces folder\nTRACES=""${ROOT}/Traces/`date +%Y-%m-%d_%H-%M-%S`""\nmkdir -p ""$TRACES""\n\n# Get the name of the user we should use to run Instruments.\n# Currently this is done, by getting the owner of the folder containing this script.\nUSERNAME=`ls -l ""${ROOT}/.."" | grep \\`basename ""$ROOT""\\` | awk \'{print $3}\'`\n\n# Bring simulator window to front. Depending on the localization, the name is different.\nosascript -e \'try\n    tell application ""iOS Simulator"" to activate\non error\n    tell application ""iOS-Simulator"" to activate\nend try\'\n\n# Prepare an Apple Script that promts for the password.\nPASS_SCRIPT=""tell application \\""System Events\\""\nactivate\ndisplay dialog \\""Password for user $USER:\\"" default answer \\""\\"" with hidden answer\ntext returned of the result\nend tell""\n\n# If the password is not set directly in this script, show the password prompt window.\nif [ -z ""$PASSWORD"" ]; then\n    PASSWORD=`osascript -e ""$PASS_SCRIPT""`\nfi\n\n# Run all the tests.\nfor SCRIPT in $SCRIPTS; do\n    echo -e ""\\nRunning test script $SCRIPT""\n    COMMAND=""instruments -t \\""$TEMPLATE\\"" \\""$EXECUTABLE\\"" -e UIASCRIPT \\""$SCRIPT\\""""\n    COMMAND=""echo \'$PASSWORD\' | sudo -S su $USER -l -c \'$COMMAND\'""\n    echo ""$COMMAND""\n    eval $COMMAND > results.log\n\n    SCRIPTNAME=`basename ""$SCRIPT""`\n    TRACENAME=`echo ""$SCRIPTNAME"" | sed \'s_\\.js$_.trace_g\'`\n    mv *.trace ""${TRACES}/${TRACENAME}""\n\n    if [ `grep "" Fail: "" results.log | wc -l` -gt 0 ]; then\n        echo ""Test ${SCRIPTNAME} failed. See trace for details.""\n        open ""${TRACES}/${TRACENAME}""\n        exit 1\n        break\n    fi\n\ndone\n\nrm results.log\n\n', ""\nIt seems as though this really might be an Xcode problem; at any rate, at least one person has filed a Radar report on it.  Someone in this other thread claims you can work around this exception by disconnecting any iDevices that are currently connected to the computer, but I suspect that does not apply when you're trying to run the script as an Xcode target.\nI would suggest filing a Radar report as well; you may get further details on the issue from Apple, or at least convince them that many people are having the problem and they ought to figure out what's going on.\nSorry for a not-terribly-helpful answer (should have been a comment, but comments and links/formatting do not mix very well).  Please update this question with anything you find out on the issue.\n"", ""\nNote: this is not a direct answer to the question, but it is an alternative solution to the underlying problem.\nWhile searching for in-depth information about UIAutomation, I stumbled across a framework by Square called KIF (Keep it functional). It is a integration testing framework that allows for many of the same features as UIAutomation, but the great thing about is is that you can just write your integration tests in Objective-C.\nIt is very easy to setup (via CocoaPods), they have good examples too, and the best thing is that it's a breeze to set up with your CI system like Jenkins.\nHave a look at: http://github.com/square/KIF\n"", '\nLate to the game but I have a solution that works for Xcode 5.1. Don\'t know if that\'s what broke the above solution or not. With the old solution I was still getting:\nFailed to load Mobile Device Locator plugin, etc.\n\nHowever, this works for the release version of Xcode 5.1.\necho <password> | sudo -S -u username xcrun instruments\n\nNotice I removed the unneeded su command and added the xcrun command. The xcrun was the magic that was needed.\nHere is my complete command:\necho <password> | sudo -S -u username xcrun instruments\\ \n  -w ""iPhone Retina (3.5-inch) - Simulator - iOS 7.1""\\\n  -D ""${PROJECT_DIR}/TestResults/Traces/Traces.trace""\\\n  -t ""${DEVELOPER_DIR}/Instruments.app/Contents/PlugIns/AutomationInstrument.bundle/Contents/Resources/Automation.tracetemplate""\\\n  ""${BUILT_PRODUCTS_DIR}/MyApp.app""\\\n  -e UIARESULTSPATH ""${PROJECT_DIR}/TestResults""\\\n  -e UIASCRIPT ""${PROJECT_DIR}/UITests/main.js""\n\nBy the way if you type:\ninstruments -s devices\n\nyou will get a list of all the supported devices you can use for the -w option.\nEdit: To make this work for different people checking out the project replace the following:\necho <password> | sudo -S -u username xcrun instruments\n\nwith\nsudo -u ${USER} xcrun instruments\n\nSince you are just doing an sudo to the same user no password is required.\n', '\nTake a look at this tutorial that explains how to have Automated UI testing with Jenkins. It also uses Jasmine in the tutorial though. http://shaune.com.au/automated-ui-testing-for-ios-apps-uiautomation-jasmine-jenkins/ hope this helps. It has an example project file so you can download that as a template. Hope this helps.\n', ""\nIn XCode - if you load up organizer (XCode->Window->Organizer)\nThen select your machine under devices -> 'Enable Developer Mode'\nThis should remove the need for prompts with instruments.\n""]",https://stackoverflow.com/questions/13923272/running-uiautomation-scripts-from-xcode,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AutomationProperties.Name VS x:Name,"
There is no difference for the ""CodedUI test builder"" between the AutomationProperties.Name and x:Name. But the first one can override the second one.
Also the AtomationProperties.Name supports data binding, x:Name of course doesn't.
As we know if you are using the MVVM pattern it is best to only use x:Name when needed.
So should AutomationProperties.Name be preferred to x:Name?
",10k,"
            17
        ","['\nSummary\nx:Name and AutomationProperties.Name are two totally different things, so the question ""should I use one or the other"" is based on a false premise: in general, you cannot use one or the other.\nThe purpose of x:Name is to identify a WPF control in code-behind so that the developer can access it. It is not meaningful (or unique) outside the scope of the class that models a specific WPF element.\nOn the other hand, the purpose of AutomationProperties.Name is to identify a user interface element in the context of a dialog or other type of window that is presented to the user for interaction. Specifically, its value should match what a user would perceive as the ""label"" of that user interface element (so that e.g. an accessibility tool can inform the user of the purpose of the element).\nWhile any tool (such as a XAML compiler) can choose to use the value of x:Name for AutomationProperties.Name as well doesn\'t mean that it\'s something you should do; IMHO this is exactly the type of ""convenience"" that results in problems because the difference between the two is hidden from the developer, so invariably one or the other property would end up having a semantically wrong value.\nInformation on the semantic and technical aspects of each of the property follows in the next sections.\nx:Name\nThe MSDN documentation page explains that\n\nAfter x:Name is applied to a framework\'s backing programming model,\n  the name is equivalent to the variable that holds an object reference\n  or an instance as returned by a constructor.\nThe value of an x:Name directive usage must be unique within a XAML\n  namescope.\n[...]\nUnder the standard build configuration for a WPF application that uses\n  XAML, partial classes, and code-behind, the specified x:Name becomes\n  the name of a field that is created in the underlying code when XAML\n  is processed by a markup compilation build task, and that field holds\n  a reference to the object.\n\nFrom the above we can tell that x:Name:\n\nis used to access the element in code (not XAML), since it controls the name of the field that holds the element\nmust be unique within a XAML namescope (since you cannot have two fields with the same name in code)\n\nAutomationProperties.Name\nThe WPF accessibility documentation explains that\n\nThe Name for an automation element is assigned by the developer. The\n  Name property should always be consistent with the label text on\n  screen. For example, the Name must be 鈥淏rowse鈥︹€?for the button element\n  with 鈥淏rowse鈥︹€?as the label.\n\n']",https://stackoverflow.com/questions/4605777/automationproperties-name-vs-xname,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
set text on textfield / textbox with the automation framework and get the change event,"
I want to set a text on a textfield / textbox element with the Mircosoft UI Automation framework, that means on a AutomationElement from the ControlType.Edit or ControlType.Document.
At the moment i'm using the TextPattern to get the text from one of these AutomationElements:
TextPattern tp = (TextPattern)element.GetCurrentPattern(TextPattern.Pattern);
string text = tp.DocumentRange.GetText(-1).Trim();

But now I want to set a new text in the AutomationElement. I can't find a method for this in the TextPattern class. So I'm trying to use the ValuePattern but I'm not sure if that's the right way to do it:
ValuePattern value = element.GetCurrentPattern(ValuePattern.Pattern) as ValuePattern;
value.SetValue(insertText);

Is there an other way to set the text value?
An other question is how can I get an event when the text was changed on a Edit / Document element? I tried to use the TextChangedEvent but i don't get any events fired when changing the text:
AutomationEventHandler ehTextChanged = new AutomationEventHandler(text_event);
Automation.AddAutomationEventHandler(TextPattern.TextChangedEvent, element, TreeScope.Element, ehTextChanged);

private void text_event(object sender, AutomationEventArgs e)
{
    Console.WriteLine(""Text changed"");
}

",13k,"
            12
        ","['\nYou can use the ValuePatern, it\'s the way to do it. From my own code :\nValuePattern etb = EditableTextBox.GetCurrentPattern(ValuePattern.Pattern) as ValuePattern;\netb.SetValue(""test"");\n\nYou can register to Event using:\nvar myEventHandler= \n            new AutomationEventHandler(handler);\n\nAutomation.AddAutomationEventHandler(\n    SelectionItemPattern.ElementSelectedEvent, // In your case you might want to use another pattern\n    targetApp, \n    TreeScope.Descendants, \n    myEventHandler);\n\nAnd the handler method:\nprivate void handler(object src, AutomationEventArgs e) {...}\n\nThere is also an AutomationPropertyChangedEventHandler (use Automation.AddAutomationPropertyChangedEventHandler(...) in this case) that can be useful.\nBased on this sample from MSDN.\n']",https://stackoverflow.com/questions/10720162/set-text-on-textfield-textbox-with-the-automation-framework-and-get-the-change,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UIAutomation Memory Issue,"
I have a simple WPF program that just has a single button with no event handling logic.  I then use the UIAutomation framework to click that button many times in a row.  Finally, I look at the memory used by the WPF program and it seems to grow and grow.
Anyone know why this is the case and how I can prevent this from happening?
Here is the simple WPF program (nothing in the code behind):
<Window x:Class=""SimpleApplication.MainWindow""
        xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation""
        xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml""
        Title=""Simple Application""
        AutomationProperties.AutomationId=""Simple Application""
        Height=""350"" Width=""525"">
    <Grid>
        <Button AutomationProperties.AutomationId=""button"" Height=""50"" Width=""100"">Click Me</Button>
    </Grid>
</Window>

Here is the UIAutomation test program:
class Program
{
    static void Main(string[] args)
    {
        string appPath = @""..\..\..\SimpleApplication\bin\Debug\SimpleApplication.exe"";
        string winAutoId = ""Simple Application"";
        string buttonAutoId = ""button"";

        using (Process process = Process.Start(new ProcessStartInfo(appPath)))
        {
            Thread.Sleep(TimeSpan.FromSeconds(1));

            AutomationElement winElement = AutomationElement.RootElement.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.AutomationIdProperty, winAutoId));

            for (int i = 0; i < 1001; i++)
            {
                AutomationElement buttonElement = winElement.FindFirst(TreeScope.Descendants, new PropertyCondition(AutomationElement.AutomationIdProperty, buttonAutoId));

                InvokePattern invokePattern = (InvokePattern)buttonElement.GetCurrentPattern(InvokePattern.Pattern);
                invokePattern.Invoke();

                process.Refresh();
                long totalMemory = process.WorkingSet64 + process.PagedMemorySize64;

                if (i % 100 == 0)
                {
                    Console.WriteLine(""Memory = {0} MB"", ((double)totalMemory) / (1024 * 1024));
                }
            }

            WindowPattern windowPattern = (WindowPattern)winElement.GetCurrentPattern(WindowPattern.Pattern);
            windowPattern.Close();
        }

        Console.WriteLine();
        Console.WriteLine(""Press Enter to Continue..."");
        Console.ReadLine();
    }
}

Here are the results from the program on my machine:
Memory = 38.20703125 MB
Memory = 42.9296875 MB
Memory = 45.00390625 MB
Memory = 47.04296875 MB
Memory = 51.9296875 MB
Memory = 52.2890625 MB
Memory = 52.41015625 MB
Memory = 55.70703125 MB
Memory = 55.70703125 MB
Memory = 57.21484375 MB
Memory = 59.09375 MB

Looking at it with the .NET Memory Profiler, the new objects that are appearing in the WPF application are from the System.Threading namespace.  When I run the WPF program by itself and click the button with the mouse these objects do no appear.
UPDATE:
I tried doing a similar test using Visual Studio's CodedUI, and the same 8 objects appeared to leak in that situation as well.  The objects that appear to leak are:
System.Threading.CancellationTokenSource
System.Threading.TimerQueueTimer
System.Threading.SparselyPopulatedArray<CancellationCallbackInfo>[]
System.Threading.Timer
System.Threading.TimerHolder
System.Threading.SparselyPopulatedArray<CancellationCallbackInfo>
System.Threading.SparselyPopulatedArrayFragment<CancellationCallbackInfo>
System.Threading.CancellationCallbackInfo[]

I have also submitted a bug to Microsoft:
http://connect.microsoft.com/VisualStudio/feedback/details/801209/uiautomation-memory-issue
",2k,"
            11
        ","['\nAfter talking to Microsoft customer support, we found the answer to the problem.  Internally, WPF gives itself three minutes to respond to a UI Automation event.  To do this, it starts off a timer.  It appears that even if the event is responded to immediately, the timer does not go away until after the three minutes are up.\nSo, the workaround to the problem is to wait until the timer expires and then do a GC.Collect.  Then the memory issue will go away.  Not a great fix, but it works for our situation.\n', '\nTry to declare the objects such as buttonElement, and invokePattern outside of the for loop. \n']",https://stackoverflow.com/questions/18832122/uiautomation-memory-issue,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automation of Android APK with Espresso,"
I am trying to automate some UI of my Android application(I do not have source code so I am using the APK file) .
I have gone through tutorial provided here and also some tutorial available at Google but all of them require source code.
If anyone have some idea how to automate the UI with Espresso without source code, please help.
I am using IntelliJ as IDE and app android version 5.0.2.
",8k,"
            11
        ","[""\nThe answer is yes, you can run automation test using Espresso without app source code.\nEspresso is based on Android instrumentation framework, which means the automation test is built into a single test apk. This test apk is different from normal application apk:\n\nThere is an instrumentation registered in AndroidManifest.xml, which will be registered to Android system once test apk is installed\nThe test apk must be signed using the same signature with the application apk, in order to run automation test\nThe test apk runs in the same process as application apk\n\nAbove are the only requirements of any instrument based test framework has. So there is no dependency of source code.\nBut why we find most of the Espresso tutorials are mixed with source code? Because it will make the test simpler锛歕n\nYou can easily control the activity lifecycle using class ActivityTestRule.\nYou can test application defined classes easily.\nYou can test UI widgets using widget id\n\nOn the contrary, you have to write lots of reflection code to get the classes you need if you don't compile with source code. For example:\n\nYou have to use Class.forName to load the entrance activity and launch it\nYou have to use Java reflection to test application defined classes\nYou have to use literal information to find UI widgets, because you don't have the id of the UI widgets\n\nI think it's due to the above disadvantages, which makes Google prefer to building Espresso test together with source code.\nTo sum up, it is OK to run Espresso automation test without application source code, but it's much harder and make test codes ugly.\nYou can refer the example project from AndroidTestWithoutSource.\n"", '\n\nI have gone through tutorial provided here and also some tutorial available at Google but all of them require source code.\n\nThat is because Espresso is part of instrumentation testing, and it requires source code.\nOther tools 鈥?UI Automator and monkeyrunner, for example 鈥?do not require source code.\n\nAs Espresso is more backward compatible with previous version of Android and also have performance advantage over UIAutomator that why I want to use Espresso\n\nThen talk to the developer of the app and arrange with that person to test the app, with full source code access.\n', '\nTo use espresso you need to know something about the UI elements themselves (like id and type).  When you don\'t have the source code you can use the ""uiautomatorviewer"" tool which is part of the Android SDK.\nhttp://developer.android.com/tools/testing-support-library/index.html#uia-viewer\n']",https://stackoverflow.com/questions/32393159/automation-of-android-apk-with-espresso,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is there a way to set the AutomationID of an object without using XAML?,"
I need to automate a Winform application.  How do I set the AutomationID (or AutomationName) like the the XAML in this article  does?
From this stack overflow article the answer seems to be no, unless I switch the application to a WPF application (so I can use XAML to define the controls).
I have tried this na茂ve approach:
  AutomationElement formAutomation = AutomationElement.FromHandle(this.Handle);
  formAutomation.Current.Name = ""SandboxResponseDialogName"";
  formAutomation.Current.ClassName = ""SandboxResponseDialogClassName"";
  formAutomation.Current.AutomationId = ""SandboxResponseDialogID;

But at this point in the constructor for the control, these Automation properties have getters only; no setters.
",15k,"
            10
        ","['\nIf you want to set anything in relation to UI Automation in code, you need to use this:\nusing System.Windows.Automation;\n\nAnd in your code:\nYourObjectClass element = // just get your element.\nelement.SetValue(AutomationProperties.AutomationIdProperty, ""elementAutomationID"");\n\nYou can also use AutomationProperties.NameProperty for the UIAutomation Name. AutomationProperties contains all the properties for UIAutomation elements (setter and getter) as the name suggest.\n']",https://stackoverflow.com/questions/14617061/is-there-a-way-to-set-the-automationid-of-an-object-without-using-xaml,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C# System.Windows.Automation get element text,"
I am trying to get text/labels from application controls with Automation in C#.
So far I am able to obtain AutomationElement tree of application (for example Notepad) with this function:
    private void WalkControlElements(AutomationElement rootElement, TreeNode treeNode)
    {
        AutomationElement elementNode = TreeWalker.ContentViewWalker.GetFirstChild(rootElement);;

        while (elementNode != null)
        {
            TreeNode childTreeNode = treeNode.Nodes.Add(elementNode.Current.ControlType.LocalizedControlType);

            // here I want to get text from 'elementNode'

            WalkControlElements(elementNode, childTreeNode);
            elementNode = TreeWalker.ControlViewWalker.GetNextSibling(elementNode);
        }
    }

I tried to follow this article http://msdn.microsoft.com/en-us/library/ms788751(v=vs.110).aspx but it only can get text attributes as font name, font weight and so on.
Could anybody point me to the right procedure how to get element text with Automation?
",17k,"
            10
        ","[""\nThat sample is showing you how to get text attributes, i.e. information about the display of the text in the UI, not the actual displayed text. Getting all the actual displayed text for a general application is more difficult that it might first appear.\nIt is made difficult by the fact that there are several ways get text and there is inconsistent support by applications and controls. There are two patterns that are of some use, ValuePattern and TextPattern. By convention the Name property contains text displayed to the user however adherence to this is inconsistent. Below is a helper method that I've used in UI automation for testing. It basically goes through those patterns checking the control for support and falls back to the Name.\npublic static class AutomationExtensions\n{\n    public static string GetText(this AutomationElement element)\n    {\n        object patternObj;\n        if (element.TryGetCurrentPattern(ValuePattern.Pattern, out patternObj))\n        {\n            var valuePattern = (ValuePattern)patternObj;\n            return valuePattern.Current.Value;\n        }\n        else if (element.TryGetCurrentPattern(TextPattern.Pattern, out patternObj))\n        {\n            var textPattern = (TextPattern)patternObj;\n            return textPattern.DocumentRange.GetText(-1).TrimEnd('\\r'); // often there is an extra '\\r' hanging off the end.\n        }\n        else\n        {\n            return element.Current.Name;\n        }\n    }\n}\n\nThis takes care of getting the text out of simple controls like labels, textboxes (both vanilla textbox and richtextbox), and buttons. Controls like listboxes and comboboxes (esp. in WPF) can be tricker because their items can be virtualized so they may not exist in the automation tree until the user interacts with them. You may want to filter and call this method only on certain UI Automation control types like Edit, Text, and Document which you know contain text.\n"", ""\nMike Zboray answer works fine. In case you have access to pattern-Matching, here is the same (condensed) code :\npublic static class AutomationExtensions\n{\n    public static string GetText(this AutomationElement element)\n    => element.TryGetCurrentPattern(ValuePattern.Pattern, out object patternValue) ? ((ValuePattern)patternValue).Current.Value\n        : element.TryGetCurrentPattern(TextPattern.Pattern, out object patternText) ? ((TextPattern)patternText).DocumentRange.GetText(-1).TrimEnd('\\r') // often there is an extra '\\r' hanging off the end.\n        : element.Current.Name;\n}\n\n""]",https://stackoverflow.com/questions/23850176/c-sharp-system-windows-automation-get-element-text,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Instances of Firefox during Selenium Webdriver Testing not handling focus correctly.,"
I have noticed that while running multiple selenium firefox tests in parallel on a grid that the focus event handling is not working correctly. I have confirmed that when each of my tests is run individually and given focus of the OS the tests pass 100% of the time. I have also run the tests in parallel on the grid with Chrome and not seen the issue present. 
I have found the following thread on google groups which suggests launching each browser in a separate instance of xvfb may be a viable solution. 
https://groups.google.com/forum/?fromgroups#!topic/selenium-developers/1cAmsYCp2ho%5B1-25%5D
The portion of the test is failing is due to a jquery date picker which is used in the project. The date picker launches on a focus event and since there are multiple selenium tests executing at the same time the webdriver test executes the .click() command but focus does not remain long enough for the date picker widget to appear. 
.focus(function(){ $input.trigger(""focus""); });

jQuery timepicker addon
By: Trent Richardson [http://trentrichardson.com]

My question is if anyone has seen this before and solved it through some firefox profile settings. I have tried loading the following property which had no affect on the issue. 
profile.setAlwaysLoadNoFocusLib(true);

The test fails in the same way as it did before with that property enabled and loaded in the Remote Driver Firefox Profile. 
I need a way ensure the focus event is triggered 100% of the time or to solve the issue of multiple firefox browsers competing for focus. Considering Chrome displays none of these issues I wonder if it may also be considered a bug in firefox.
Thanks! 
",5k,"
            8
        ","['\n@djangofan: Wrong. You cannot lock the focus. After you requested focus in one window and before you trigger an action, another window requests focus, and your action (like sending keys to input field) just doesn\'t work. This happened in our tests several times daily. It was hard to reproduce, because with each test run it failed on different places. A solution is to execute each browser in a separate display. E.g. you can use Xvfb:\n  Xvfb ... -screen 1 1200x800x24 -screen 2 1200x800x24 ...\n\nThen when you start a browser, assign a separate screen to it:\n  browser.setEnvironmentProperty(""DISPLAY"", "":N.1"");\n  browser.setEnvironmentProperty(""DISPLAY"", "":N.2"");\n  ...\n\n', ""\nI've had the same issue in my continuous integration environment with Jenkins.\nAfter a long research i found an old bug in firefox that led to a new config flag to avoid those problems.\nThe solution is to enable this flag on the firefox profile that the tests use. The flag is focusmanager.testmode, set it to true.\n\nThe explanation is that the focus events are triggered only when firefox window is active. If you run multiple test you have multiple windows so only the active one triggers the focus events. With this param the events are trigered even for non active windows.\n"", '\nYou can wrangle this and get it under your control with no problem.  First write a method to identify the popup window by its window handle id.   Then, use a JavaScriptExecutor  to execute ""window.focus()"" in javascript to force the window to be focused just before you perform another action.  Then, you can close the popup by its window handle name if necessary.\n']",https://stackoverflow.com/questions/11974538/multiple-instances-of-firefox-during-selenium-webdriver-testing-not-handling-foc,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C# : How to detect if screen reader is running?,"
How to detect if screen reader is running (JAWS)?
As I understand in .NET 4 we can use AutomationInteropProvider.ClientsAreListening from System.Windows.Automation.Provider namespace, but what if I have to do it for .NET 2.0?
I tried to inspect ClientsAreListening source code, it calls external RawUiaClientsAreListening method from UIAutomationCore.dll library.
Do you have any ideas how to implement JAWS detection in .NET 2.0?
",4k,"
            5
        ","['\nUse the SystemParametersInfo function passing a uiAction of SPI_GETSCREENREADER.\nYou will need to use P/Invoke for this, for example:\ninternal class UnsafeNativeMethods\n{\n    public const uint SPI_GETSCREENREADER = 0x0046;\n\n    [DllImport(""user32.dll"", SetLastError = true)]\n    [return: MarshalAs(UnmanagedType.Bool)]\n    public static extern bool SystemParametersInfo(uint uiAction, uint uiParam, ref bool pvParam, uint fWinIni);\n}\n\npublic static class ScreenReader\n{\n    public static bool IsRunning\n    {\n        get\n        {\n            bool returnValue = false;\n            if (!UnsafeNativeMethods.SystemParametersInfo(UnsafeNativeMethods.SPI_GETSCREENREADER, 0, ref returnValue, 0))\n            {\n                throw new Win32Exception(Marshal.GetLastWin32Error(), ""error calling SystemParametersInfo"");\n            }\n            return returnValue;\n        }\n    }\n}\n\nThis is possibly better than using the ClientsAreListening property as this property appears to return true for any automation client, not just screen readers.\nAlso see:\n\nUsing SystemParametersInfo from C# (SPI_GETSCREENREADER SPI_SETSCREENREADER) (Stack Overflow)\nINFO: How Clients and Servers Should Use SPI_SETSCREENREADER and SPI_GETSCREENREADER (Microsoft KB)\n\nYou should also listen for the WM_SETTINGCHANGE message to detect if a screen reader starts / stops running.\n\nUpdate (in response to BrendanMcK\'s comments):\nAlthough this is never explicitly documented in as many words, looking at the description of the flag I think the purpose of this flag is relatively clear:\n\nDetermines whether a screen reviewer utility is running. A screen reviewer utility directs textual information to an output device, such as a speech synthesizer or Braille display. When this flag is set, an application should provide textual information in situations where it would otherwise present the information graphically.\n\nWhat this is saying is that applications set this flag whenever an application wishes the UI to behave as if a screen reader is running, regardless of whether or not that application is actually a screen reader or not. \nSuitable things to do in response to this flag is to add text in order to ""read"" otherwise intuitive UI state to the user.  If radical changes are needed to make your UI screen reader accessible then the chances are that your UI also isn\'t that intuitive to sigted users and could probably do with a re-think.\n']",https://stackoverflow.com/questions/8079716/c-sharp-how-to-detect-if-screen-reader-is-running,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get TitleBar Caption of any application using Microsoft UI Automation?,"
In C# or else VB.Net, how I could use Microsoft UI Automation to retrieve the text of any control that contains text?.
I've been researching in the MSDN Docs, but I don't get it.
Obtain Text Attributes Using UI Automation
Then, for example, with the code below I'm trying to retrieve the text of the Window titlebar by giving the hwnd of that window, but I don't know exactlly how to follow the titlebar to find the child control (label?) that really contains the text.
Imports System.Windows.Automation
Imports System.Windows.Automation.Text

.
Dim hwnd As IntPtr = Process.GetProcessesByName(""notepad"").First.MainWindowHandle

Dim targetApp As AutomationElement = AutomationElement.FromHandle(hwnd)

' The control type we're looking for; in this case 'TitleBar' 
Dim cond1 As New PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.TitleBar)

Dim targetTextElement As AutomationElement =
    targetApp.FindFirst(TreeScope.Descendants, cond1)

Debug.WriteLine(targetTextElement Is Nothing)

In the example above I'm trying with the titlebar, but just I would like to do it with any other control that contains text ...like a titlebar.
PS: I'm aware of P/Invoking GetWindowText API.
",4k,"
            5
        ","['\nWith UI Automation, in general, you have to analyze the target application using the SDK tools (UISpy or Inspect - make sure it\'s Inspect 7.2.0.0, the one with a tree view).\nSo here for example, when I run notepad, I run inspect and see this:\n\nI see the titlebar is a direct child of the main window, so I can just query the window tree for direct children and use the TitleBar control type as a discriminant because there\'s no other child of that type beneath the main window. \nHere is a sample console app C# code that demonstrate how to get that \'Untitled - Notepad\' title. Note the TitleBar also supports the Value pattern but we don\'t need here because the titlebar\'s name is also the value.\nclass Program\n{\n    static void Main(string[] args)    \n    {\n        // start our own notepad from scratch\n        Process process = Process.Start(""notepad.exe"");\n        // wait for main window to appear\n        while(process.MainWindowHandle == IntPtr.Zero)\n        {\n            Thread.Sleep(100);\n            process.Refresh();\n        }\n        var window = AutomationElement.FromHandle(process.MainWindowHandle);\n        Console.WriteLine(""window: "" + window.Current.Name);\n\n        // note: carefully choose the tree scope for perf reasons\n        // try to avoid SubTree although it seems easier...\n        var titleBar = window.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.TitleBar));\n        Console.WriteLine(""titleBar: "" + titleBar.Current.Name);\n    }\n}\n\n']",https://stackoverflow.com/questions/30875408/get-titlebar-caption-of-any-application-using-microsoft-ui-automation,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Click a button element on page load,"
I'm trying to auto-click a button to initiate a function when the html page loads. I've tried document.getElementById('watchButton').click and it doesn't seem to work, the button is not clicked. Any suggestions? 
<div class=""content"">
        <div class=""action-area ch30"">
            <button class=""button dh"" id=""watchButton"">Start Geolocation Watch</button>
            <button class=""button dh"" id=""refreshButton"" >Refresh Geolocation</button>
        </div>

The javascript:
    run:function() {
    var that = this;
    document.getElementById(""watchButton"").addEventListener(""click"", function() {
        that._handleWatch.apply(that, arguments);
    }, false); 
    document.getElementById(""refreshButton"").addEventListener(""click"", function() {
        that._handleRefresh.apply(that, arguments);
    }, false);
},

Thanks!
",71k,"
            5
        ","[""\nI'd put it inside document.ready (so it doesn't fire until the DOM loads) and use jQuery syntax:\n$(function() {\n    $('#watchButton').click();\n});\n\nhttp://jsfiddle.net/isherwood/kVJVe/\nHere's the same fiddle using jQuery syntax: http://jsfiddle.net/isherwood/kVJVe/4\nThat said, why not just name your function and call it directly? \n"", '\nIt would be click() not click\ndocument.getElementById(""watchButton"").click();\n\nYou would need to call it onload or after the function has run\n', '\nwindow.onload = function () {\ndocument.getElementById(""watchButton"").click(); };\n\nTry this ^^\n', '\ntry trigger\n<script>\n$(document).ready(function() {\n   $(""#watchButton"").trigger(\'click\');\n});\n</script>\n\n', '\ndocument.getElementById(""studyOne"").click();\n$(""#studyOne"").trigger(\'click\');\n\nPut this in onload function. It worked for me. \n']",https://stackoverflow.com/questions/21418915/click-a-button-element-on-page-load,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using Instruments to test an iOS app without having source code to the application,"
I would like to use UIAutomation via Instruments in Xcode to test an app on my iOS device. Is it possible to do so without having to build the source code? The reason for this is that our team will have testers who will be writing automation scripts to test the apps on our devices, but we don't want them to all go through syncing to latest builds and compiling it through Xcode. Does anyone know if this is possible through UIAutomation or possibly through a 3rd party application?
Thanks.
",2k,"
            4
        ","['\nYou could make UIAutomation tests without the source code, but you will not see your symbols and you cant see where in your code your app is hanging.\nYou still can see if there is a routine that is taking to long to run, but you wont e able to see witch one or the call stack\nto make your tests without the source code just open the automator and bind your test with an application installed on the device\n']",https://stackoverflow.com/questions/12045621/using-instruments-to-test-an-ios-app-without-having-source-code-to-the-applicati,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WPF UI Automation issue,"
This Thread belong to this
I am asking where do I need to insert the workaround from this
I have a WPF application which has performance issue on some clients with Windows 7. On Windows XP all is working fast. The application has a MainShell and some Child-Windows. The MainShell hangs sometimes on some machines, and so do the child windows. Now, do I have to insert the workaround from the thread from the above link in all windows?
Are there still other workaround about this?
",6k,"
            4
        ","['\nI have been working on an application that has been fine on virtually everything but the WPF Controls were slow on certain laptops (Lenovo). It was lagging and freezing and generally inhibiting use.\nI did the following:\n\nImplemented the code above: Improved it, but did not fix.\nDownloaded Hotfix - -    http://archive.msdn.microsoft.com/KB978520 (may not be required)\nDownloaded Hotfix - -    http://archive.msdn.microsoft.com/KB2484841 (definitely required even if you have Windows 7 / .NET 4)\nImproved the code further (the validation was causing an excess of objects) - Why does WPF Style to show validation errors in ToolTip work for a TextBox but fails for a ComboBox?\n\nIt may be that only Number 3 was required, but it worked. Just posting here so people dont lose the days I lost in memory profilers etc.\n', ""\nIn my case it worked by adding that code to the main window. However, I simplified it a bit:\npublic partial class MyMainWindow : Window\n{\n    public MyMainWindow() {\n          GotFocus += WindowGotFocus;\n    }\n\n    private void WindowGotFocus(object sender, RoutedEventArgs e)\n    {\n        WindowInteropHelper helper = new WindowInteropHelper(this);\n        var mainWindowAutomationElement = AutomationElement.FromHandle(helper.Handle);\n        Automation.AddStructureChangedEventHandler(mainWindowAutomationElement, TreeScope.Element,\n                                                   delegate {});\n        GotFocus -= WindowGotFocus;\n    }\n}\n\nThe only problem with this approach, in my machine, is that the debugger window gets cluttered with messages like:\n\nA first chance exception of type System.Windows.Automation.ElementNotAvailableException' occurred in PresentationCore.dll\nA first chance exception of type 'System.ArgumentException' occurred in UIAutomationClientsideProviders.dll\nA first chance exception of type 'System.NotSupportedException' occurred in mscorlib.dll\nA first chance exception of type 'System.ComponentModel.Win32Exception' occurred in UIAutomationClient.dll\n\nAll happening many many times. I couldn't fix these messages, but my application is running faster now.\n""]",https://stackoverflow.com/questions/6362367/wpf-ui-automation-issue,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Does Microsoft UI Automation Framework work with Chrome, Python and Java Apps?","
I am working on an automation project, in which I need to capture the activities [ application launched, data entered, input type etc.] user performs on a desktop. I came across Microsoft UI Automation framework which so far works well for native windows based applications like MS Office, .NET apps etc. However I did not find any useful information / samples of capturing the information from different web browsers [Chrome is a must], Python apps, Java Apps etc. Can someone please confirm whether MS UI Automation Framework supports such apps. Any working example to extract user activities from these apps would be highly appreciated. Thanks.
",5k,"
            4
        ","[""\nChrome only supports UI Automation for toolbars, tabs, menu, buttons around the web page. Everything that's rendered as a web page is not seen by UIA.\nFor the web page content, the easiest way is to use Selenium (driven by the ChromeDriver), which is kind of a de facto standard for browsers, and has nothing to do with UIA.\nTo test if an app supports UIA, and how far it does, it's very easy, just run UIA's Inspect tool and check the UI tree over that application.\n"", ""\nSome additions to Simon's answer...\nChrome page content can be seen by UIA if you run chrome --force-renderer-accessibility. Only for existing Chrome process it won't work. Though user can create a new tab chrome://accessibility manually and enable UIA for all or some chosen pages. This method also works for AT-SPI accessibility technology on Linux. Of course, Selenium WebDriver is an industry standard here. But another way exists. Both Mozilla and IE support UIA by default.\nInspect.exe can be simply downloaded from this GitHub repo.\nRegarding Java apps it depends on the app type. Your chances is about 50/50.\nWxPython or PyQt5 are good for UIA. TkInter or Kivy apps are not.\nP.S. There is an example how to drag a file from explorer.exe and drop to Google Drive in Chrome using Python library pywinauto.\n"", '\nI\'m a bit late to the party..\nBut Chromes accessibility features are only activated once something tries to access it\'s accessibility.\nIf you call AccessibleObjectFromWindow ([DllImport(""oleacc.dll"")]) with the window handle an existing chrome window will have its accessibility activated (and you\'ll see the actual web page content in UIA!).\nIf the chrome window is opened after your app is running - Chrome pings open processes for any open accessibility apps... for that you use AccessibleObjectFromEvent and the event you\'re responding to comes from the windows pipeline: EVENT_SYSTEM_ALERT = 0x0002 .\nThe bottom line is - you have to tell chrome that there\'s something installed that wants to access it\'s web page content.\nOh! and your application has to be signed!! Unsigned apps won\'t be able to access web content - I think that\'s the same in firefox too.\nI hope this helps someone in the future.\nSee:\nhttps://www.chromium.org/developers/design-documents/accessibility\n']",https://stackoverflow.com/questions/47216824/does-microsoft-ui-automation-framework-work-with-chrome-python-and-java-apps,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using javascript to set text for a web element - Selenium Web driver,"
How can text of a web element be set using java script executor? Or is there any other way to do this?
<a class=""selectBox selectBox-dropdown selectBox-menuShowing selectBox-active"" style=""width: 52px; display: inline-block; -moz-user-select: none;"" title="""" tabindex=""0"">
<span class=""selectBox-label"" style=""width: 13px;"">10</span>
<span class=""selectBox-arrow""/>
</a>

There are two span elements under the  tag - which is a drop down. User clicks on span[2] and a list is shown which contains data like 10, 20, 30, 40, etc.. User clicks on the number(element) and that is set as the text of span[1] (In this case, 10 is selected). How should I go about solving this?
I tried Action builder and it is not working. Any others suggestions?
",10k,"
            3
        ","['\nIf you want to change the text of span[1] directly, you may use following code:\nString jScript = ""var myList = document.getElementsByClassName(\\""selectBox-label\\"");""\n    +""myList[0].innerHTML=\\""YourNumber\\"";""; \nJavascriptExecutor executor = (JavascriptExecutor)driver;\nexecutor.executeScript(jScript);\n\nHowever, you may also click the number using java script which as you say will set the text of span[1]. Example below:\nWebElement element = driver.findElement(By.xpath(""YourNumbersXpath""));\nJavascriptExecutor executor = (JavascriptExecutor)driver;\nexecutor.executeScript(""arguments[0].click();"", element);\n\n']",https://stackoverflow.com/questions/21695714/using-javascript-to-set-text-for-a-web-element-selenium-web-driver,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to pass POINT structure to ElementFromPoint method in Python?,"
I'm trying to use method IUIAutomation::ElementFromPoint in Python using comtypes package. There are many examples how to use it in C++, but not in Python. This simple code reproduces the problem on 64-bit Windows 10 (Python 2.7 32-bit):
import comtypes.client

UIA_dll = comtypes.client.GetModule('UIAutomationCore.dll')
UIA_dll.IUIAutomation().ElementFromPoint(10, 10)

I get the following error:
TypeError: Expected a COM this pointer as first argument

Creating the POINT structure this way doesn't help as well:
from ctypes import Structure, c_long

class POINT(Structure):
    _pack_ = 4
    _fields_ = [
        ('x', c_long),
        ('y', c_long),
    ]

point = POINT(10, 10)
UIA_dll.IUIAutomation().ElementFromPoint(point) # raises the same exception

",1k,"
            3
        ","['\nYou can reuse existing POINT structure definition directly, like this:\nimport comtypes\nfrom comtypes import *\nfrom comtypes.client import *\n\ncomtypes.client.GetModule(\'UIAutomationCore.dll\')\nfrom comtypes.gen.UIAutomationClient import *\n\n# get IUIAutomation interface\nuia = CreateObject(CUIAutomation._reg_clsid_, interface=IUIAutomation)\n\n# import tagPOINT from wintypes\nfrom ctypes.wintypes import tagPOINT\npoint = tagPOINT(10, 10)\nelement = uia.ElementFromPoint(point)\n\nrc = element.currentBoundingRectangle # of type ctypes.wintypes.RECT\nprint(""Element bounds left:"", rc.left, ""right:"", rc.right, ""top:"", rc.top, ""bottom:"", rc.bottom)\n\nTo determine what\'s the expected type for ElementFromPoint, you can just go to your python setup directory (for me it was C:\\Users\\<user>\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\site-packages\\comtypes\\gen) and check the files in there. It should contains files automatically generated by comtypes, including the one for UIAutomationCore.dll. The interesting file name starts with _944DE083_8FB8_45CF_BCB7_C477ACB2F897 (the COM type lib\'s GUID).\nThe file contains this:\nCOMMETHOD([], HRESULT, \'ElementFromPoint\',\n          ( [\'in\'], tagPOINT, \'pt\' ),\n\nThis tells you that it expects a tagPOINT type. And this type is defined a the beginning of the file like this:\nfrom ctypes.wintypes import tagPOINT\n\nIt\'s named tagPOINT because that\'s how it\'s defined in original Windows header.\n']",https://stackoverflow.com/questions/44826285/how-to-pass-point-structure-to-elementfrompoint-method-in-python,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AutomationProperties.LiveSetting not working in WPF in .NET Framework 4.7.1,"
I have a TextBlock and I want to track that control from Screen reader and whenever a new value is set to the control in code, the screen reader should readout the new text. This is available in WPF from .NET framework 4.7.1 which is mentioned in the MSDN LINK. 
But I am always getting null for the AutomationPeer value. What am I missing in the code? Am I doing it in the right way? Please help.
XMAL 
      <Window x:Class=""WPFAccessibility.MainWindow""
                xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation""
                xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml""
                xmlns:d=""http://schemas.microsoft.com/expression/blend/2008""
                xmlns:mc=""http://schemas.openxmlformats.org/markup-compatibility/2006""
                xmlns:local=""clr-namespace:WPFAccessibility""
                mc:Ignorable=""d""
                Title=""WPFAccessibility"" Height=""450"" Width=""800"">
            <Grid>

                <TextBlock Name=""MyTextBlock"" AutomationProperties.LiveSetting=""Assertive"">My initial text</TextBlock>

                <Button Name=""Save"" Content=""Save"" HorizontalAlignment=""Left"" VerticalAlignment=""Top"" Width=""75"" Margin=""50,321,0,0"" Height=""49"" Click=""Save_Click""/>   

            </Grid>
        </Window>

Code
 private void Save_Click(object sender, RoutedEventArgs e)
        {
            // Setting the MyTextBlock text to some other value and screen 
            // reader should notify to the user
            MyTextBlock.Text = ""My changed text"";
            var peer = UIElementAutomationPeer.FromElement(MyTextBlock); 
           // I am always getting peer value null 
            peer.RaiseAutomationEvent(AutomationEvents.LiveRegionChanged);
        }

",778,"
            3
        ","['\nUse the CreatePeerForElement method to create a UIElementAutomationPeer for the TextBlock:\nprivate void Save_Click(object sender, RoutedEventArgs e)\n{\n    MyTextBlock.Text = ""My changed text"";\n    var peer = UIElementAutomationPeer.FromElement(MyTextBlock);\n    if (peer == null)\n        peer = UIElementAutomationPeer.CreatePeerForElement(MyTextBlock);\n    peer.RaiseAutomationEvent(AutomationEvents.LiveRegionChanged);\n}\n\n']",https://stackoverflow.com/questions/52699145/automationproperties-livesetting-not-working-in-wpf-in-net-framework-4-7-1,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading out Edge Browser Title & Url with System.Windows.Automation,"
I'm trying to read out the TITLE & URL from the Microsoft EDGE Browser.
Doing this with System.Windows.Automation most preferably since the code base already uses this for other problems.

Is it possible with System.Windows.Automation?
How to access the URL?

I'm currently this far:
AutomationId ""TitleBar""
ClassName ""ApplicationFrameWindow""
Name = [string]
=> Reading out this element gives me the TITLE

=> Walking it's children, I find the item ""addressEditBox"":
   AutomationId ""addressEditBox""
   ClassName ""RichEditBox""
   Name ""Search or enter web address""
   => I always get back the string ""Search or enter web address""
   => This is the control where the url is in, though it isn't updated as the user goes to a website, it always returns a fixed string.

In code:
   var digger1 = AutomationElement.FromHandle(process.MainWindowHandle).RootElement.FindAll(TreeScope.Children, Condition.TrueCondition);

       foreach(AutomationElement d1 in digger1 {
          if(d1.Current.ClassName.Equals(""ApplicationFrameWindow"")) {
             var digger2 = d1.FindAll(TreeScope.Children, Condition.TrueCondition);
             foreach(AutomationElement d2 in digger2) {
                if(d2.Current.ClassName.Equals(""Windows.Ui.Core.CoreWindow"")) {
                   var digger3 = d2.FindAll(TreeScope.Children, Condition.TrueCondition);
                   foreach(AutomationElement d3 in digger3) {
                      if(d3.Current.AutomationId.Equals(""addressEditBox"")) {
                          var url = d3.Current.Name;
                          return url;
                      }
                   }
                }
             }
          }
       }

",4k,"
            2
        ","['\nYou\'re almost there. You just need to get the TextPattern from the addressEditBox element. Here is a full sample Console app that dumps out all currently running Edge\'s windows on the desktop:\nclass Program\n{\n    static void Main(string[] args)\n    {\n        AutomationElement main = AutomationElement.FromHandle(GetDesktopWindow());\n        foreach(AutomationElement child in main.FindAll(TreeScope.Children, PropertyCondition.TrueCondition))\n        {\n            AutomationElement window = GetEdgeCommandsWindow(child);\n            if (window == null) // not edge\n                continue;\n\n            Console.WriteLine(""title:"" + GetEdgeTitle(child));\n            Console.WriteLine(""url:"" + GetEdgeUrl(window));\n            Console.WriteLine();\n        }\n    }\n\n    public static AutomationElement GetEdgeCommandsWindow(AutomationElement edgeWindow)\n    {\n        return edgeWindow.FindFirst(TreeScope.Children, new AndCondition(\n            new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Window),\n            new PropertyCondition(AutomationElement.NameProperty, ""Microsoft Edge"")));\n    }\n\n    public static string GetEdgeUrl(AutomationElement edgeCommandsWindow)\n    {\n        var adressEditBox = edgeCommandsWindow.FindFirst(TreeScope.Children,\n            new PropertyCondition(AutomationElement.AutomationIdProperty, ""addressEditBox""));\n\n        return ((TextPattern)adressEditBox.GetCurrentPattern(TextPattern.Pattern)).DocumentRange.GetText(int.MaxValue);\n    }\n\n    public static string GetEdgeTitle(AutomationElement edgeWindow)\n    {\n        var adressEditBox = edgeWindow.FindFirst(TreeScope.Children,\n            new PropertyCondition(AutomationElement.AutomationIdProperty, ""TitleBar""));\n\n        return adressEditBox.Current.Name;\n    }\n\n    [DllImport(""user32"")]\n    public static extern IntPtr GetDesktopWindow();\n}\n\n']",https://stackoverflow.com/questions/32204961/reading-out-edge-browser-title-url-with-system-windows-automation,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interacting with multiple instances of an application in Coded UI,"
The scenario that I am facing is that I am trying to write a single test which will use Coded UI to interact with multiple instances of the same application, in this case Microsoft Excel. In other words, there will be multiple Excel workbooks open in multiple windows, and I need to be able to direct Coded UI to interact with a specific instance programatically. I initially thought this type of instance management would be a function of the ApplicationUnderTest class, but it is not obvious how this class would achieve this.
The interactions will involve the same UIMap for all instances (in fact, each instance will probably need multiple UIMaps, but for the sake of simplicity that can be ignored for this question unless it is significant to the answer).
A couple of solution approaches I'm already aware of:

Minimize and maximize the instances so only the one currently being used is visible at any given time. Ideally I'd like to avoid this. For one thing, it may eventually become a requirement that two windows are visible simultaneously during the tests.
Dynamically modify the search properties to always include some unique identifier every time the UI Map is accessed. I'm not sure what the best candidate for a search property would be here. 

Ideally I would like something more integrated into Coded UI than either of these options, though the latter would probably suffice if necessary. I would appreciate any direction on whether there are any other possible approaches.
",3k,"
            2
        ","['\nYou could try creating multiple instances of the ui control (the class generated in the UIMap) and set an Instance search property for them (if you have any other unique search properties you can use those too). You only need to set these at the start of the test.\nI used a calculator for this example. It should be Namespace.UIControl and not UIMap.UIControl. You need the class not the property.\nvar a1 = new UICalculatorWindow();\na1.SearchProperties[""Instance""] = ""1"";\nvar a2 = new UICalculatorWindow();\na2.SearchProperties[""Instance""] = ""2"";\na1.Find();\na2.Find();\n\nAfter finding these windows their window handle will be associated with the control object so you don\'t have to worry about their order anymore.\nAnother solution would be to get all current window handles via a pinvoke function, filter these to get the windows you want, then use the UITestControlFactory to create your controls.\n\nEdit: or you can use the FindMatchingControls method.\nvar a = new UICalculatorWindow().FindMatchingControls();\n\nThen you can get the live controls from the returned list. These solutions are a bit workaround-ish but I don\'t think this can be solved on the UIMap level unless all instances of the control are recorded as unique ui controls.\n\nEdit: CUIT searches in the list of window handles it gets from a WinApi call (EnumWindows) and by default it returns the first window from the list that matches the given search properties. If the Instance property is set then it skips the first n-1 windows (that matches the search criteria) and gets you the n-th window.\nWhen you call Find() on a UITestControl it will search for a window with the given search properties and if a window is found the UITestControl keeps a reference to that window\'s window handle or the AccessibleObject it got from that window. \nThe order of window handles can change pretty often, for example if you set focus to a window it will be closer to beginning of the list. So when you have all your windows open you should create the UITestControls, set the Instance property and call Find() on all of them so they don\'t mix up during the test run.\nIf you find a window with Instance set to 1 and then mix up the order of the windows then when you search for a window with Instance set to 2 you might find the window you already found, ending up with two UITestControls set to the same window.\nI have no idea how OrderOfInvocation works, I couldn\'t get it to work yet.\n']",https://stackoverflow.com/questions/23522114/interacting-with-multiple-instances-of-an-application-in-coded-ui,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is it possible to activate a tab in another program using an IntPtr?,"
Thanks in advance.
Is it possible to activate a tab in another program using an IntPtr? If so, how? 
SendKeys is not an option.
Perhaps what I need is a fishing lesson. I have exhausted Google and my lead developer. 
I would appreciate an outright solution OR a recommendation to continue my Google efforts.
basic process is:
I drag a shortcut icon to the launcher

This opens the target application (Notepad++) and grabs IntPtr, etc.
I would like to programmatically select various items in Notepad++ such as Edit, menu items under Edit, or a doc tab.

The basic code I am running is:

the 'blob'

item 1: IntPtr of item
item 2: IntPtr of itemsChild
item 3: control text of item 1
item 4: is rectangle parameters of item 1


root contains similar info:

",2k,"
            2
        ","['\nAs others pointed out, the standard way of doing this is to use UI Automation. Notepad++ does support UI Automation (to some extent, as it\'s somehow automatically provided by the UI Automation Windows layers).\nHere is a sample C# console app that demonstrates the following sceanrio (you need to reference UIAutomationClient.dll, UIAutomationProvider.dll and UIAutomationTypes.dll):\n1) get the first running notepad++ process (you must start at least one)\n2) open two files (note there may be already other opened tabs in notepad++)\n3) selects all tabs in an infinite loop\nclass Program\n{\n    static void Main(string[] args)\n    {\n        // this presumes notepad++ has been started somehow\n        Process process = Process.GetProcessesByName(""notepad++"").FirstOrDefault();\n        if (process == null)\n        {\n            Console.WriteLine(""Cannot find any notepad++ process."");\n            return;\n        }\n        AutomateNpp(process.MainWindowHandle);\n    }\n\n    static void AutomateNpp(IntPtr handle)\n    {\n        // get main window handle\n        AutomationElement window = AutomationElement.FromHandle(handle);\n\n        // display the title\n        Console.WriteLine(""Title: "" + window.Current.Name);\n\n        // open two arbitrary files (change this!)\n        OpenFile(window, @""d:\\my path\\file1.txt"");\n        OpenFile(window, @""d:\\my path\\file2.txt"");\n\n        // selects all tabs in sequence for demo purposes\n        // note the user can interact with n++ (for example close tabs) while all this is working\n        while (true)\n        {\n            var tabs = GetTabsNames(window);\n            if (tabs.Count == 0)\n            {\n                Console.WriteLine(""notepad++ process seems to have gone."");\n                return;\n            }\n\n            for (int i = 0; i < tabs.Count; i++)\n            {\n                Console.WriteLine(""Selecting tab:"" + tabs[i]);\n                SelectTab(window, tabs[i]);\n                Thread.Sleep(1000);\n            }\n        }\n    }\n\n    static IList<string> GetTabsNames(AutomationElement window)\n    {\n        List<string> list = new List<string>();\n\n        // get tab bar\n        var tab = window.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Tab));\n        if (tab != null)\n        {\n            foreach (var item in tab.FindAll(TreeScope.Children, PropertyCondition.TrueCondition).OfType<AutomationElement>())\n            {\n                list.Add(item.Current.Name);\n            }\n        }\n        return list;\n    }\n\n    static void SelectTab(AutomationElement window, string name)\n    {\n        // get tab bar\n        var tab = window.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Tab));\n\n        // get tab\n        var item = tab.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.NameProperty, name));\n        if (item == null)\n        {\n            Console.WriteLine(""Tab item \'"" + name + ""\' has been closed."");\n            return;\n        }\n\n        // select it\n        ((SelectionItemPattern)item.GetCurrentPattern(SelectionItemPattern.Pattern)).Select();\n    }\n\n    static void OpenFile(AutomationElement window, string filePath)\n    {\n        // get menu bar\n        var menu = window.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.MenuBar));\n\n        // get the ""file"" menu\n        var fileMenu = menu.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.NameProperty, ""File""));\n\n        // open it\n        SafeExpand(fileMenu);\n\n        // get the new File menu that appears (this is quite specific to n++)\n        var subFileMenu = fileMenu.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Menu));\n\n        // get the ""open"" menu\n        var openMenu = subFileMenu.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.NameProperty, ""Open...""));\n\n        // click it\n        ((InvokePattern)openMenu.GetCurrentPattern(InvokePattern.Pattern)).Invoke();\n\n        // get the new Open dialog (from root)\n        var openDialog = WaitForDialog(window);\n\n        // get the combobox\n        var cb = openDialog.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.ComboBox));\n\n        // fill the filename\n        ((ValuePattern)cb.GetCurrentPattern(ValuePattern.Pattern)).SetValue(filePath);\n\n        // get the open button\n        var openButton = openDialog.FindFirst(TreeScope.Children, new AndCondition(\n            new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Button),\n            new PropertyCondition(AutomationElement.NameProperty, ""Open"")));\n\n        // press it\n        ((InvokePattern)openButton.GetCurrentPattern(InvokePattern.Pattern)).Invoke();\n    }\n\n    static AutomationElement WaitForDialog(AutomationElement element)\n    {\n        // note: this should be improved for error checking (timeouts, etc.)\n        while(true)\n        {\n            var openDialog = element.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Window));\n            if (openDialog != null)\n                return openDialog;\n        }\n    }\n\n    static void SafeExpand(AutomationElement element)\n    {\n        // for some reason, menus in np++ behave badly\n        while (true)\n        {\n            try\n            {\n                ((ExpandCollapsePattern)element.GetCurrentPattern(ExpandCollapsePattern.Pattern)).Expand();\n                return;\n            }\n            catch\n            {\n            }\n        }\n    }\n}\n\nIf you wonder how this has been made, then you must read about UI Automation. The mother of all tools is called Inspect: https://msdn.microsoft.com/library/windows/desktop/dd318521.aspx\nMake sure you get version at least 7.2.0.0.  Note there is another one called UISpy but inspect is better.\nNote, unfortunately, notepad++ tab text content - because it\'s based on the custom scintilla editor control - does not properly supports automation (we can\'t read from it easily, I suppose we\'d have to use scintilla Windows messages for this), but it could be added to it (hey, scintilla guys, if you read this ... :).\n', ""\nIn addition to the answer from Garath, you might also want to investigate the Windows automation API's i.e. the technology used to implement coded UI tests for GUI applications. As part of regular functional testing, I routinely control an external application from a set of NUnit tests using these API's.\nTools like UIAVerify will give you an indication of what controls are available in the application and you can use the Invoke Pattern (and many others) to interact with the controls at run-time.\nIf you want a detailed example of how to use the automation API's, the open source TestStack White project is pretty handy.\n"", '\nIt is almost not possible if SendKeys is not an option but read more\nNow more important part of the question- why:\nWe have to look how win32 application works: it has a WndProc/WindowProc method which is resposible for processing ""events"" form the UI.\nSo every event in the windows application must go through above method. SendKeys method is a special of SendMessage (MSDN), so you can use SendMessage to control other exe than your.\nSimple code could look like:\nIntPtr hwnd = FindWindow(""Notepad++"", null);\nSendMessageA(hwnd, WM_COMMAND, SOMETHING1, SOMETHING2);\n\nThere is already on StackOverflow example how to do that with chrome: C# - Sending messages to Google Chrome from C# application , but this is only a start. You will have to find out what exactly message you want to send. \nIn exactly situation which you described I will try to send WM_MOUSE and WM_KEYBORD events to Notepad++ events, but it is only an idea :)\n']",https://stackoverflow.com/questions/29951432/is-it-possible-to-activate-a-tab-in-another-program-using-an-intptr,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to refresh UIMap object in CodedUI,"
Can i refresh UIMap object ? 
Problem is I change the location of UI element on and I again try to get the AutomationElement at that time I get AutomationELment but its BoundingRectanle is infinity. 
So i am assuming that it is not refreshing the UIMap object.
Can anyone please help me on this ?
",2k,"
            1
        ","['\nIn the UIMaps section of your test, you have probably something like:\nprivate MyTestUIMap uiMap;\npublic MyTestUImap UIMap\n{\n    get\n    {\n        if (this.uiMap == null)\n        {\n            this.uiMap = new MyTestUIMap();\n        }\n        return this.uiMap;\n    }\n}\n\nThis creates a singleton for the UIMap object the first time it is used. To refresh it, you can make set the uiMap object to null, so it gets reinitialized again when it is used the next time:\npublic void RefreshUIMap() \n{\n    this.uiMap = null;\n}\n\nWhenever you want to refresh the UIMap (get a new instance to it) call this RefreshUIMap method.\nEDIT:\nAfter reading your question again, I think you want to refresh a single Object in the UIMap and not the UIMap object instance. Select the object in the MyTestUIMap.uitest (assuming you have FeaturePack 2 installed), select Search Configuration from the object properties and add the configuration AlwaysSearch. Whenever the object is used in your script, the testrunner will search for it again on the screen instead of trying to get it from the buffer.\n', '\nA call of the Find() method on any UITestControl should perform, or repeat, the search.\nCommonly the Find() method is not called explicitly, it is called implicitly by using the control. Calling Find() on a control should re-evaluate the search and also have the effect of clearing any child controls.\n', '\nSetting the SearchConfiguration to Always Search fixed my issue where a single control was not being refreshed with updated values causes a test to fail.\nthis.UIMap.UIItemWindow2.UIItemWindow11.SearchConfigurations.Add(SearchConfiguration.AlwaysSearch);\n\n']",https://stackoverflow.com/questions/10848757/how-to-refresh-uimap-object-in-codedui,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can we fetch multiple data stored in <div> table in karate?,"
I am trying to fetch set of data from a web table, stored/made using the <div> tags, not like the traditional html data tables. For example:
    <div class=""tabulator-cell"" role=""gridcell"" tabulator-field=""program_name"" title="""" style=""width: 135px; text-align: left; height: 30px;"">
        <span style=""color: #00def; font-weight: 500;"">Consumer xyz</span>
    </div>
    <div class=""tabulator-cell"" role=""gridcell"" tabulator-field=""business_val"" title="""" style=""width: 119px; text-align: center; height: 00px;"">
        11898
        <div class=""tabulator-col-resize-handle""></div>
        <div class=""tabulator-col-resize-handle prev""></div>
    </div>

So, I am using scriptAll() method for this and need data just for 'Consumer xyz' here but unable to do so.
    * def list = scriptAll('div div', '_.textContent', function(x){ return x.contains('Consumer xyz') })
    * delay(3000)
    * print list 

Any help on this would be appreciated. Thanks in advance.
",473,"
            1
        ","['\nI\'ll just give you one hint. This is how you can get a reference to the parent div of the row:\n* def temp = locate(\'{}Consumer Banking\').parent.parent\n\nNow it is up to you:\n* def businessValue = temp.locate(""[tabulator-field=\'business_value\']"")\n* match businessValue.text.trim() == \'11898\'\n\n']",https://stackoverflow.com/questions/65844733/how-can-we-fetch-multiple-data-stored-in-div-table-in-karate,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can we fetch multiple data stored in <div> table in karate?,"
I am trying to fetch set of data from a web table, stored/made using the <div> tags, not like the traditional html data tables. For example:
    <div class=""tabulator-cell"" role=""gridcell"" tabulator-field=""program_name"" title="""" style=""width: 135px; text-align: left; height: 30px;"">
        <span style=""color: #00def; font-weight: 500;"">Consumer xyz</span>
    </div>
    <div class=""tabulator-cell"" role=""gridcell"" tabulator-field=""business_val"" title="""" style=""width: 119px; text-align: center; height: 00px;"">
        11898
        <div class=""tabulator-col-resize-handle""></div>
        <div class=""tabulator-col-resize-handle prev""></div>
    </div>

So, I am using scriptAll() method for this and need data just for 'Consumer xyz' here but unable to do so.
    * def list = scriptAll('div div', '_.textContent', function(x){ return x.contains('Consumer xyz') })
    * delay(3000)
    * print list 

Any help on this would be appreciated. Thanks in advance.
",473,"
            1
        ","['\nI\'ll just give you one hint. This is how you can get a reference to the parent div of the row:\n* def temp = locate(\'{}Consumer Banking\').parent.parent\n\nNow it is up to you:\n* def businessValue = temp.locate(""[tabulator-field=\'business_value\']"")\n* match businessValue.text.trim() == \'11898\'\n\n']",https://stackoverflow.com/questions/65844733/how-can-we-fetch-multiple-data-stored-in-div-table-in-karate,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AutomationElement shows up using Inspect.exe but does show not up when using UIAutomationCore.dll or System.Windows.Automation,"
TL;DR: What am I doing wrong that is causing the workspace pane to show up in Inspect Objects but not show up in my custom code?

I am trying to write some UI automation to a 3rd party program. I am using Inspect.exe that came with the Windows SDK, and I have tried both System.Windows.Automation and direct COM Calls (using the wrapper library from UIA Verify).
Process[] processes = Process.GetProcessesByName(""Redacted Client"");
if (processes.Length == 0) throw new Exception(""Could not find \""Redacted Client\"" process"");

PropertyCondition parentFileCond = new PropertyCondition(AutomationElement.ProcessIdProperty, processes[0].Id);
PropertyCondition workspaceCond = new PropertyCondition(AutomationElement.NameProperty, ""Workspace"", PropertyConditionFlags.IgnoreCase);
PropertyCondition documentCond = new PropertyCondition(AutomationElement.NameProperty, ""Untitled3"", PropertyConditionFlags.IgnoreCase);

var parentElement = AutomationElement.RootElement.FindFirst(TreeScope.Children, parentFileCond);
var workspaceElement = parentElement.FindFirst(TreeScope.Children, workspaceCond); //Also does not work with TreeScope.Descendants
var documentElement = workspaceElement.FindFirst(TreeScope.Children, documentCond);

When I try the above code, parentElement does have the correct reference to the main program window, but workspaceElement is null.

A temporary workaround:
If I change my documentElement code to:
var documentElement = parentElement.FindFirst(TreeScope.Descendants, documentCond);

I will get the correct element returned. I can use this as a workaround as the document window is the one I really wanted anyway, but I would like to know why the Workspace pane would not show up so I can improve my skills in case I run into this in the future with a situation I cannot work around.

UPDATE: I tried MrGomez's suggestions
PropertyCondition parentFileCond = new PropertyCondition(AutomationElement.ProcessIdProperty, 5872);
PropertyCondition panelCond = new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Pane);

var parentElement = AutomationElement.RootElement.FindFirst(TreeScope.Children, parentFileCond);
var panels = parentElement.FindAll(TreeScope.Children, panelCond);


I get 3 results, unfortunately, I have 4 panels, and the one that did not show up was the panel named Workspace.
I also tried to use a TreeWalker
PropertyCondition parentFileCond = new PropertyCondition(AutomationElement.ProcessIdProperty, 5872);
PropertyCondition workspaceCond= new PropertyCondition(AutomationElement.NameProperty, ""Workspace"");

var walker = new TreeWalker(workspaceCond);
var parentElement = AutomationElement.RootElement.FindFirst(TreeScope.Children, parentFileCond);
var workspaceElement = walker.Normalize(parentElement);

but that also returns null for workspaceElement
Finally, in desperation, I tried the current value of ""NativeWindowHandle"" from Inspect and started the walking from the root node.
PropertyCondition workspaceCond = new PropertyCondition(AutomationElement.NativeWindowHandleProperty, 0x110906);
var walker = new TreeWalker(workspaceCond);
var workspaceElement = walker.Normalize(AutomationElement.RootElement);

Workspace element is STILL null.

Result Found
I finally did get Workspace to show up, but I had to perform 
PropertyCondition workspaceCond = new PropertyCondition(AutomationElement.NativeWindowHandleProperty, 0x110906);
var test = AutomationElement.RootElement.FindFirst(TreeScope.Subtree, workspaceCond); 

and it took quite a while to run.
Old Screen Captures
Here is screenshots from Inspect.exe showing the tree view.

Here are the properties of the main window of the program.
How found:  Selected from tree...
RuntimeId:  ""[42.2557552]""
BoundingRectangle:  {l:75 t:1 r:1311 b:1003}
ProcessId:  8160
ControlType:    UIA_WindowControlTypeId (0xC370)
LocalizedControlType:   ""window""
Name:   ""Redacted""
AccessKey:  """"
HasKeyboardFocus:   false
IsKeyboardFocusable:    true
IsEnabled:  true
ClassName:  ""C:\Program Files (x86)\RedactedProgramFiles7\RedactedClientFolder""
HelpText:   """"
IsPassword: false
NativeWindowHandle: 0x270670
IsOffscreen:    false
FrameworkId:    ""Win32""
ProviderDescription:    ""[pid:4000,hwnd:0x270670 Main:Nested [pid:8160,hwnd:0x270670 Annotation(parent link):Microsoft: Annotation Proxy (unmanaged:uiautomationcore.dll); Main:Microsoft: MSAA Proxy (unmanaged:uiautomationcore.dll)]; Nonclient:Microsoft: Non-Client Proxy (unmanaged:uiautomationcore.dll); Hwnd(parent link):Microsoft: HWND Proxy (unmanaged:uiautomationcore.dll)]""
Window.CanMaximize: true
Window.CanMinimize: true
Window.WindowVisualState:   Normal (0)
Window.WindowInteractionState:  ReadyForUserInteraction (2)
Window.IsModal: false
Window.IsTopmost:   false
Transform.CanMove:  true
Transform.CanResize:    true
Transform.CanRotate:    false
LegacyIAccessible.ChildId:  0
LegacyIAccessible.DefaultAction:    """"
LegacyIAccessible.Description:  """"
LegacyIAccessible.Help: """"
LegacyIAccessible.KeyboardShortcut: """"
LegacyIAccessible.Name: ""Redacted""
LegacyIAccessible.Role: client (0xA)
LegacyIAccessible.State:    focusable (0x100000)
LegacyIAccessible.Value:    """"
IsDockPatternAvailable: false
IsExpandCollapsePatternAvailable:   false
IsGridItemPatternAvailable: false
IsGridPatternAvailable: false
IsInvokePatternAvailable:   false
IsLegacyIAccessiblePatternAvailable:    true
IsMultipleViewPatternAvailable: false
IsRangeValuePatternAvailable:   false
IsScrollPatternAvailable:   false
IsScrollItemPatternAvailable:   false
IsSelectionItemPatternAvailable:    false
IsSelectionPatternAvailable:    false
IsTablePatternAvailable:    false
IsTableItemPatternAvailable:    false
IsTextPatternAvailable: false
IsTogglePatternAvailable:   false
IsTransformPatternAvailable:    true
IsValuePatternAvailable:    false
IsWindowPatternAvailable:   true
IsItemContainerPatternAvailable:    false
IsVirtualizedItemPatternAvailable:  false
IsSynchronizedInputPatternAvailable:    false
FirstChild: ""Workspace"" pane
LastChild:  ""Application"" menu bar
Next:   ""Inspect  (HWND: 0x01700F06)"" window
Previous:   ""Sandbox Console (Debugging) - Microsoft Visual Studio (Administrator)"" window
Other Props:    Object has no additional properties
Children:   ""Workspace"" pane
    (null) title bar
    ""Application"" menu bar
Ancestors:  ""Desktop"" pane
    [ No Parent ]

Here are the properties of the problem ""Workspace"" pane.
How found:  Selected from tree...
RuntimeId:  ""[42.34146524]""
BoundingRectangle:  {l:83 t:51 r:1303 b:995}
ProcessId:  8160
ControlType:    UIA_PaneControlTypeId (0xC371)
LocalizedControlType:   ""pane""
Name:   ""Workspace""
AccessKey:  """"
HasKeyboardFocus:   false
IsKeyboardFocusable:    true
IsEnabled:  true
ClassName:  ""MDIClient""
HelpText:   """"
IsPassword: false
NativeWindowHandle: 0x20908DC
IsOffscreen:    false
FrameworkId:    ""Win32""
ProviderDescription:    ""[pid:4000,hwnd:0x20908DC Main:Nested [pid:8160,hwnd:0x20908DC Annotation(parent link):Microsoft: Annotation Proxy (unmanaged:uiautomationcore.dll); Main:Microsoft: MSAA Proxy (unmanaged:uiautomationcore.dll)]; Hwnd(parent link):Microsoft: HWND Proxy (unmanaged:uiautomationcore.dll)]""
LegacyIAccessible.ChildId:  0
LegacyIAccessible.DefaultAction:    """"
LegacyIAccessible.Description:  """"
LegacyIAccessible.Help: """"
LegacyIAccessible.KeyboardShortcut: """"
LegacyIAccessible.Name: ""Workspace""
LegacyIAccessible.Role: client (0xA)
LegacyIAccessible.State:    focusable (0x100000)
LegacyIAccessible.Value:    """"
IsDockPatternAvailable: false
IsExpandCollapsePatternAvailable:   false
IsGridItemPatternAvailable: false
IsGridPatternAvailable: false
IsInvokePatternAvailable:   false
IsLegacyIAccessiblePatternAvailable:    true
IsMultipleViewPatternAvailable: false
IsRangeValuePatternAvailable:   false
IsScrollPatternAvailable:   false
IsScrollItemPatternAvailable:   false
IsSelectionItemPatternAvailable:    false
IsSelectionPatternAvailable:    false
IsTablePatternAvailable:    false
IsTableItemPatternAvailable:    false
IsTextPatternAvailable: false
IsTogglePatternAvailable:   false
IsTransformPatternAvailable:    false
IsValuePatternAvailable:    false
IsWindowPatternAvailable:   false
IsItemContainerPatternAvailable:    false
IsVirtualizedItemPatternAvailable:  false
IsSynchronizedInputPatternAvailable:    false
FirstChild: ""Untitled3"" window
LastChild:  ""Letters (32638 of 32638):"" window
Next:   (null) title bar
Previous:   [null]
Other Props:    Object has no additional properties
Children:   ""Untitled3"" window
    ""Letters (32638 of 32638):"" window
Ancestors:  ""Redacted"" window
    ""Desktop"" pane
    [ No Parent ]

Here are the properties of the ""Working"" document window.
How found:  Selected from tree...
RuntimeId:  ""[42.9505096]""
BoundingRectangle:  {l:85 t:53 r:651 b:491}
ProcessId:  8160
ControlType:    UIA_WindowControlTypeId (0xC370)
LocalizedControlType:   ""window""
Name:   ""Untitled3""
AccessKey:  """"
HasKeyboardFocus:   false
IsKeyboardFocusable:    true
IsEnabled:  true
AutomationId:   ""10""
ClassName:  ""ProToolsSubMDIWndClass""
HelpText:   """"
IsPassword: false
NativeWindowHandle: 0x910948
IsOffscreen:    false
FrameworkId:    ""Win32""
ProviderDescription:    ""[pid:4000,hwnd:0x910948 Main:Nested [pid:8160,hwnd:0x910948 Annotation(parent link):Microsoft: Annotation Proxy (unmanaged:uiautomationcore.dll); Main:Microsoft: MSAA Proxy (unmanaged:uiautomationcore.dll)]; Nonclient:Microsoft: Non-Client Proxy (unmanaged:uiautomationcore.dll); Hwnd(parent link):Microsoft: HWND Proxy (unmanaged:uiautomationcore.dll)]""
Window.CanMaximize: true
Window.CanMinimize: true
Window.WindowVisualState:   Normal (0)
Window.WindowInteractionState:  ReadyForUserInteraction (2)
Window.IsModal: false
Window.IsTopmost:   false
Transform.CanMove:  true
Transform.CanResize:    true
Transform.CanRotate:    false
LegacyIAccessible.ChildId:  0
LegacyIAccessible.DefaultAction:    """"
LegacyIAccessible.Description:  """"
LegacyIAccessible.Help: """"
LegacyIAccessible.KeyboardShortcut: """"
LegacyIAccessible.Name: ""Untitled3""
LegacyIAccessible.Role: client (0xA)
LegacyIAccessible.State:    focusable (0x100000)
LegacyIAccessible.Value:    """"
IsDockPatternAvailable: false
IsExpandCollapsePatternAvailable:   false
IsGridItemPatternAvailable: false
IsGridPatternAvailable: false
IsInvokePatternAvailable:   false
IsLegacyIAccessiblePatternAvailable:    true
IsMultipleViewPatternAvailable: false
IsRangeValuePatternAvailable:   false
IsScrollPatternAvailable:   false
IsScrollItemPatternAvailable:   false
IsSelectionItemPatternAvailable:    false
IsSelectionPatternAvailable:    false
IsTablePatternAvailable:    false
IsTableItemPatternAvailable:    false
IsTextPatternAvailable: false
IsTogglePatternAvailable:   false
IsTransformPatternAvailable:    true
IsValuePatternAvailable:    false
IsWindowPatternAvailable:   true
IsItemContainerPatternAvailable:    false
IsVirtualizedItemPatternAvailable:  false
IsSynchronizedInputPatternAvailable:    false
FirstChild: """" thumb
LastChild:  (null) title bar
Next:   ""Letters (32638 of 32638):"" window
Previous:   [null]
Other Props:    Object has no additional properties
Children:   """" thumb
    (null) title bar
Ancestors:  ""Workspace"" pane
    ""Redacted"" window
    ""Desktop"" pane
    [ No Parent ]

",24k,"
            28
        ","['\nVery nice question. Based upon the problem you\'ve documented, it\'s clear that your conditional:\nPropertyCondition workspaceCond = new PropertyCondition(\n AutomationElement.NameProperty, ""Workspace"", PropertyConditionFlags.IgnoreCase);\n\n... fails evaluation. Why?\nThe answer is how your conditional is evaluated. Looking at your element tree, we notice this property for Workspace:\nIsWindowPatternAvailable:   false\n\nAnd for the main window and Untitled3:\nIsWindowPatternAvailable:   true\n\nFrom MSDN:\n\nUIA_IsWindowPatternAvailablePropertyId 30044\n\nIdentifies the IsWindowPatternAvailable property, which indicates whether the Window control pattern is available for the automation element. If TRUE, a client can retrieve an IUIAutomationWindowPattern interface from the element.\n\nWe find a repro in this thread, which implies the same failure pattern as the one you are currently experiencing. We also note the lack of Window properties present for this element because IUIAutomationWindowPattern is inaccessible.\nA workaround is available from the aforelinked thread. Instead of PropertyCondition, one might use:\npublic class ConditionMatcher : IMatchConditions\n{\n    public bool Matches(AutomationElement element, Condition condition)\n    {\n        return new TreeWalker(condition).Normalize(element) != null;\n    }\n}\n\nOr, alternately, one might use the workaround you\'ve given, provided your tree structure is guaranteed to be shallow (and thus, appropriate to the name of this site, will not trigger a stack overflow).\nAdmittedly, this wasn\'t the most obvious issue. In the perfect world, MSDN should have better documentation on this topic.\n']",https://stackoverflow.com/questions/9282275/automationelement-shows-up-using-inspect-exe-but-does-show-not-up-when-using-uia,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What's the difference of UISpy.exe and Inspect.exe? (From Microsoft Windows SDK),"
I really want to know, how Inspect.exe gets it's UI-Elements, because it gets by far more elements than UISpy (Both available in Microsoft Windows SDK 7).
1) I think UISpy gets it's elements with UIAutomation library, right?
(Tried it with UIAutomation and got exactly the same elements, that UISpy displayed).
2) Which library does Inspect.exe use?
Because it shows some UI-Elements of a Application with MacromediaFlashPlayerActiveX for example, which I need to get in my own UI-Automation-Application, hope somebody knows something about it.
EDIT: Inspect also have a ""UI Automation"" Mode, does it also use UIAutomation library? The strange thing about it is, that in Inspect it also shows many more elements than UISpy.
",20k,"
            19
        ","[""\nUISpy is a .NET program that uses .NET's UIAutomation assemblies. These assemblies were introduced with .NET Framework 2.\nInspect is a native program that uses UIAutomationCore.dll (available in the System directory). UIAutomationCore is a native COM Windows DLL that implements the UI Automation Windows API. This API has been upgraded with Windows, for example here is a link to Windows 8+ improvements: What's New in UI Automation?\n.NET's UIAutomation assemblies use UIAutomationCore.dll internally. However, Microsoft never upgraded them to new features of the native UI Automation API. A lot of properties (for example all ARIA properties which are very useful), patterns, events, etc, are therefore missing if you use the original .NET's UIAutomation.\nBut, there is still hope in the .NET world, because there is a project here: https://uiacomwrapper.codeplex.com/ that is a source-compatible newer version of .NET UIAutomation (It's been in fact written by a Microsoft guy, I don't understand why they don't publish this in a more official way and upgrade .NET's UIAutomation...). It defines most new Windows 8 properties, patterns and interfaces.\n""]",https://stackoverflow.com/questions/40496048/whats-the-difference-of-uispy-exe-and-inspect-exe-from-microsoft-windows-sdk,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use AppleScript to list the names of all UI elements in a window (GUI scripting),"
One line summary - I'm looking for a way to get AppleScript itself to reveal the name it expects a specific piece of window content (UI element) to be referred to as in a ""tell"" statement.
How do I get AppleScript to list the name it wants me to use to refer to a window's contents?
for example I can say tell current application to tell its front window's list 1 to ... 
I'm trying to find out the term like ""list 1"" for all of the window's contents so I can cross-reference it with the list from Accessibility Inspector.. 
I tried this code but the first line generates an error saying ""error ""Can鈥檛 make names of 芦class ects禄 of window 1 of 芦class prcs禄 \""iTunes\"" of application \""System Events\"" into type string."" number -1700 from names of 芦class ects禄 of window 1 of 芦class prcs禄 ""iTunes"" to string""
tell application ""System Events"" to tell process ""iTunes"" to set elementNames to the names of the entire contents of its front window as string
tell application ""TextEdit""
    activate
    make new document at the front
    set the text of the front document to elementNames
    set WrapToWindow to text 2 thru -1 of (localized string ""&Wrap to Window"")
end tell

",16k,"
            16
        ","['\nIn GUI scripting, using entire contents [of] on a process\'s window in the context of the System Events application returns a list of all the UI elements (GUI controls) of the frontmost window in the active application (the entire hierarchy flattened to a list):\ntell application ""System Events""\n  tell front window of (first application process whose frontmost is true)\n    set uiElems to entire contents\n  end tell\nend tell\n\nIf you run the above in Script Editor, the Result pane will show a list of object specifiers - e.g., button 1 of window ""Main"" of application process ""Music"" of application ""System Events"" - which do not directly reveal specific information, but from which you can glean least the type (class) of UI element (button, in this example) and the position among its siblings of the same type (1)\nTo target Music, for instance, substitute application process ""Music"" for (first application process whose frontmost is true) .\nNote that if you only want the immediate child elements, you can use UI elements [of] instead of entire contents [of], and you can apply it not just to a window (to get the top-level UI elements), but to any of the UI elements it contains to get their children.\n\nYou cannot extract properties of these elements by converting the enumeration to a string with as string as you\'ve tried, but you can extract properties in a repeat loop:\nAssuming that by names you mean the class names (such as table and button), try this:\nset classNames to {}\ntell application ""System Events""\n    tell front window of (first application process whose frontmost is true)\n        repeat with uiElem in entire contents as list\n            set classNames to classNames & (class of uiElem as string)\n        end repeat\n    end tell\nend tell\n\nNote that as list is inexplicably needed (as of macOS 10.12.3) to make this work (not necessary with UI elements [of]).\nThis will return a list of class names such as {""splitter group"", ""text field"", ""button"", ""scroll area"", ... }, which in itself is not enough to target a specific element, however, because the hierarchy has been flattened.\n']",https://stackoverflow.com/questions/42231133/use-applescript-to-list-the-names-of-all-ui-elements-in-a-window-gui-scripting,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In UI automator viewer Error Obtaining Device screenshot, Reason : Error Unable to connect to adb. Check if adb is installed correctly","
When I click on UI Automator viewer --> Device screenshot throws Error Unable to connect to adb. Check if adb is installed correctly.
I am trying to run it Appium. I am able to load the apk in the emulator, stuck on the UI Automator viewer due to the adb connection error.
",23k,"
            13
        ","['\nOpen with text editor (notepad/vim) the uiautomatorviewer.bat.\nFind that line: \ncall ""%java_exe%"" ""-Djava.ext.dirs=%javaextdirs%"" ""-Dcom.android.uiautomator.bindir=%prog_dir%"" -jar %jarpath% %*\n\nand change it to call ""%java_exe%"" ""-Djava.ext.dirs=%javaextdirs%"" ""-Dcom.android.uiautomator.bindir=C:\\DEV\\androidSDK\\tools"" -jar %jarpath% %*\nPlease notice that you should put your Tools\' path after the bindir.\n', '\nAnother fix is to just copy adb.exe next to the uiautomatorviewer.bat. \nIf I remember correctly, modified files can confuse Android Studio when updating SDK, so you have to manually delete in order to successfully update, meaning more future proof solution. Example: link\n', '\nIn my case the problem was that the uiautomatorviewer location was user/library/android/sdk/tools/bin and I was using it via $ uiautomatorviewer. The app was working correctly, but when I tried to get the view hierarchy adb error occured. Solution? Open it via $ open uiautomatorviewer xD\n', '\nIt worked for me after I stopped Appium server and retried.\n', '\nJust copy adb.exe into uiautomatorviewer.bat folder.\n', '\nFind the following code in UIautomatorviewer\ncall \n\n""%java_exe%"" ""-Djava.ext.dirs=%javaextdirs%""\n  ""-Dcom.android.uiautomator.bindir=%prog_dir%"" -jar %jarpath% %*\n\nSet the bindir path to adb exe. path.\nOr simply paste adb.exe with uiautomatorViewer\n']",https://stackoverflow.com/questions/42696158/in-ui-automator-viewer-error-obtaining-device-screenshot-reason-error-unable,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is there Anyone who Successfully Implement UI Test Automation Regime using Microsoft UI Automation?,"
I am looking for an Automated UI test framework/ software tool. In the past I have been using TestComplete, and although it's a good piece of software, but the concept of GUI test automation was deemed to be sufficiently difficult that I wrote a few posts to complain about it. 
One of the problems with third party test automation tool is that you have to learn new language in order to be productive on it, not to mention that the tooling support is poor. I am now planning to look into Microsoft UI Automation that comes with .Net 3.0 and the White Framework. But before I do that, I want to know what's the outcome there.
Anyone has any experience to share on this? Have you create a sustainable and successful test suite using UI automation on your application?
Edit: This seems like a very hard question. I would setup bounty for this if I don't receive any answers within these few days. 
",5k,"
            11
        ","[""\nThis is an extremely late response (you probably already shipped), but I tested a WPF application with it.\nI had a lot of flakiness while I was using the beta1 version of VS2010, and encountered some bugs.  Using the release version, I had fewer problems.\nIt was challenging because I was learning WPF, VS UI automation, and WPF/Windows accessibility all at the same time.  The fact that it is hard to create bullet proof automated UI tests added to this difficulty.\nAfter the VS2010 RTM, I still had intermittent issues with a native/custom page that we wrapped.  I think this was partially due to a parenting issue, and partially due to a threading/process issue.\nThe biggest headache I encountered was when I was trying to test against some control we used from the WPF toolkit, in particular the DataGrid control.  It had some known UI automation bugs, and I had a devil of a time working around them.\nI also encountered some challenges where the UI automation framework was caching data behind the scenes.  I would have to re-instantiate controls in order to get updated data on them (particularly visibility, when a control was poorly parented).\nIf I had a pure WPF or pure Win32 app, that didn't inherit UI from other applications, wrap things in WebBrowser controls, use custom controls, etc, then I'd probably use it again.  If you haven't made a decision yet, and your app uses any of those, then I'd see if I could use something else.  Maybe a simpler set of scripts to do more limited integration testing, and try to cover the rest with unit tests, using mock objects.\nI haven't used it within the last 6 months, though, so your mileage might vary.\n"", ""\nWe use the White test framework here with great results.  \nThe framework uses win32 messaging to find the controls and interact with them.  It's fairly slow on large forms with a lot of controls however, that's the only drawback I've encountered.  We automate the test running using buildbot and nunit-console too.\n"", ""\nI wrote an ironruby-based gem called Bewildr that wraps the MS UI Automation framework. It has been successfully used to automate MS WPF apps at various companies including the BBC. Here's a step-by-step guide to using it: http://www.natontesting.com/2011/08/27/step-by-step-example-of-bdding-a-wpf-app-with-cucumber-rspec-ironruby-and-bewildr/\n"", '\nI have been using MS UI Automation framework using C#.net and i found this extremely simplem and helpful.No issues were observed as such but it lacks report generating support so you need to write your own logic for that based on your specific test case.\nOverall satisfaction : 8 / 10.\n', ""\nI have no experience with Microsoft UI Automation, but I'm using AutoIt (http://www.autoitscript.com/autoit3/) to do a little bit of GUI automation. I'm using it to test my Visual Studio add-in. Not really an answer to your question, but it could be interesting for you.\nRegards,\nSebastiaan\n"", '\nCold Response... Better I answer a dummy answer, just in case there is no good answer to this. \n', '\nIve used a lightweight version of UI automated testing for the .net framework using a similar approach to this:\nhttp://msdn.microsoft.com/en-us/magazine/cc163864.aspx\nI did encounter issues using tabs and deciding which panels were going to be used, but that eventually led to a redesign of the form (so it pretty much found issues in design too!)\n']",https://stackoverflow.com/questions/1249041/is-there-anyone-who-successfully-implement-ui-test-automation-regime-using-micro,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python with Selenium: Drag and Drop from file system to webdriver?,"
I have to automate a web-application, which contains a drag and drop area for uploading files from the local file system. My test environment is developed using Python. For the automation tests I have used Selenium, but it is not possible to drag files from the file system, once the upload area is a div tag (No input tag - this way I know it would be easy).
I read a lot of different articles, but by the moment none worked for me. It's important to highlight that I'm not interested in using AutoIT, only native python with selenium.
I found this Selenium: Drag and Drop from file system to webdriver? what looks really promising, however I do not know to adapt to Python.
Thank you a lot in advance!
",11k,"
            11
        ","['\nHere\'s the python version of the trick with input injection via script.\nJS_DROP_FILE = """"""\n    var target = arguments[0],\n        offsetX = arguments[1],\n        offsetY = arguments[2],\n        document = target.ownerDocument || document,\n        window = document.defaultView || window;\n\n    var input = document.createElement(\'INPUT\');\n    input.type = \'file\';\n    input.onchange = function () {\n      var rect = target.getBoundingClientRect(),\n          x = rect.left + (offsetX || (rect.width >> 1)),\n          y = rect.top + (offsetY || (rect.height >> 1)),\n          dataTransfer = { files: this.files };\n\n      [\'dragenter\', \'dragover\', \'drop\'].forEach(function (name) {\n        var evt = document.createEvent(\'MouseEvent\');\n        evt.initMouseEvent(name, !0, !0, window, 0, 0, 0, x, y, !1, !1, !1, !1, 0, null);\n        evt.dataTransfer = dataTransfer;\n        target.dispatchEvent(evt);\n      });\n\n      setTimeout(function () { document.body.removeChild(input); }, 25);\n    };\n    document.body.appendChild(input);\n    return input;\n""""""\n\ndef drag_and_drop_file(drop_target, path):\n    driver = drop_target.parent\n    file_input = driver.execute_script(JS_DROP_FILE, drop_target, 0, 0)\n    file_input.send_keys(path)\n\nAs drop_target pass it some element visible on the page which you can get using any function from the family of driver.get_element_by_....\nThe approach is to invoke a javascript using selenium\'s execute_script function to emulate drag and drop events. The code works as following:\n\nselenium invokes javascript code\njavascript creates input element and attaches it to DOM\njavascript attaches a handler to the input which emulates mouse events that happens when user actually drops a file, namely dragenter, dragover, drop.\nselenium updates the input with the path to the file. At this point the handler from step 2 is invoked and it emulates drag and drop events.\n\n', '\nI know this may be a late answer but just in case for people who find the answer!\nIf you are using Chrome please go to this site to download the Chrome driver.\n(Try to find your chrome version thru this and choose the suitable one)\nThere\'s still another thing you will need to do\nI\'ll show it right now\n\nFirst: Download chrome driver and copy the Xpath\nStep 1: Go to the site you want and copy fullXPath of your ""drag and drop"", by right click on the drag and drop area then hit the inspect.\nPlz do this twice just in case it inspects the right place\nStep 2:\nYou will see the highlight color, again right-click on them\nthen you will find ""copy"" -> ""copy fullXpath""\nFinally, let\'s code\nWait!!!\nJust one more suggestion plz :\nIf you see something goes wrong with pasting the ""Xpath"" or ""link to the folder"" for example\nyou might use \' \' instead of "" ""\nfrom selenium import webdriver\ndriver = webdriver.Chrome(\'D:\\Folder\\chromedriver\')\ndriver.get(\'https://exmaple.com\')\ndrag_&_drop = driver.find_element_by_xpath(\'paste-the-full-xpath-here\')\ndrag_&_drop.send_keys(\'D:\\Folder\\picture.png\')\n#python 3.9\n\n']",https://stackoverflow.com/questions/43382447/python-with-selenium-drag-and-drop-from-file-system-to-webdriver,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I tell if a process has a graphical interface?,"
I'm using automation to test an application, but sometimes I want to start the application via a batch file. When I run ""process.WaitForInputIdle(100)"" I get an error:
""WaitForInputIdle failed.  This could be because the process does not have a graphical interface.""
How can I tell if the process has a graphical interface or not?
",12k,"
            11
        ","['\nSee Environment.UserInteractive.  That will identify whether the process has an interface at all, e.g. services are not user interactive.\nYou could also look at Process.MainWindowHandle which will tell you whether there is a graphical interface.\nA combination of these two checks should cover all the possibilities.\n', '\nYou can simply try and catch the exception:\nProcess process = ...\ntry\n{\n    process.WaitForInputIdle(100);\n}\ncatch (InvalidOperationException ex)\n{\n    // no graphical interface\n}\n\n', '\nI was think along the lines of this, Still ugly but trys to avoid exceptions. \nProcess process = ...\n\nbool hasUI = false;\n\nif (!process.HasExited)\n{\n    try\n    {\n        hasUI = process.MainWindowHandle != IntPtr.Zero;\n    }\n    catch (InvalidOperationException)\n    {\n        if (!process.HasExited)\n            throw;\n    }\n}\n\nif (!process.HasExited && hasUI)\n{\n\n    try\n    {\n        process.WaitForInputIdle(100);\n    }\n    catch (InvalidOperationException)\n    {\n        if (!process.HasExited)\n            throw;\n    }\n}\n\n', '\nAs well as a MainWindowHandle check, one can enumerate the process threads and check if any of them reference a visible window via P/Invokes. This seems to do a good job catching any windows that the first check misses.\nprivate Boolean isProcessWindowed(Process externalProcess)\n{\n    if (externalProcess.MainWindowHandle != IntPtr.Zero)\n    {\n        return true;\n    }\n\n    foreach (ProcessThread threadInfo in externalProcess.Threads)\n    {\n        IntPtr[] windows = GetWindowHandlesForThread(threadInfo.Id);\n\n        if (windows != null)\n        {\n            foreach (IntPtr handle in windows)\n            {\n                if (IsWindowVisible(handle))\n                {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n\nprivate IntPtr[] GetWindowHandlesForThread(int threadHandle)\n{\n    results.Clear();\n    EnumWindows(WindowEnum, threadHandle);\n\n    return results.ToArray();\n}\n\nprivate delegate int EnumWindowsProc(IntPtr hwnd, int lParam);\n\nprivate List<IntPtr> results = new List<IntPtr>();\n\nprivate int WindowEnum(IntPtr hWnd, int lParam)\n{\n    int processID = 0;\n    int threadID = GetWindowThreadProcessId(hWnd, out processID);\n    if (threadID == lParam)\n    {\n        results.Add(hWnd);\n    }\n\n    return 1;\n}\n\n[DllImport(""user32.Dll"")]\nprivate static extern int EnumWindows(EnumWindowsProc x, int y);\n[DllImport(""user32.dll"")]\npublic static extern int GetWindowThreadProcessId(IntPtr handle, out int processId);\n[DllImport(""user32.dll"")]\nstatic extern bool IsWindowVisible(IntPtr hWnd);\n\n', '\nBesides the Process.MainWindowHandle, you can improve your solution by reading the PE header of the process main module to detect the process module subsystem. If the subsystem is IMAGE_SUBSYSTEM_WINDOWS_GUI, then the process can have a graphical interface.\nUse this class to read the PE header of the process main module: https://gist.github.com/ahmedosama007/bfdb8198fe6690d17e7c3db398f6d725\nUse the following code to detect the process module subsystem:\nDim peReader = New PEHeaderReader(""C:\\Windows\\notepad.exe"")\n\nDim subsystem As PEHeaderReader.ImageSubSystem\n\nIf peReader.Is32BitHeader Then \'32-bit\n    subsystem = peReader.OptionalHeader32.Subsystem\nElse \'64-bit\n    subsystem = peReader.OptionalHeader64.Subsystem\nEnd If\n\n\'https://learn.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_optional_header32\n\nIf subsystem = PEHeaderReader.ImageSubSystem.IMAGE_SUBSYSTEM_WINDOWS_GUI Then\n    Console.WriteLine(""GUI"")\nElseIf subsystem = PEHeaderReader.ImageSubSystem.IMAGE_SUBSYSTEM_WINDOWS_CUI Then\n    Console.WriteLine(""Console"")\nElse\n    Console.WriteLine(""Other Subsystem"")\nEnd If\n\nConsole.ReadLine()\n\n']",https://stackoverflow.com/questions/3785698/how-can-i-tell-if-a-process-has-a-graphical-interface,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Open file from windows file dialog with python automatically,"
I do automated testing and get a file dialog. I want to choose a file from the windows open file dialog with python or selenium.
NOTE: The dialog is given by an other program. I don't want to create it with Tkinter.
The Window looks like:
.
How to do this?
",9k,"
            10
        ","[""\nConsider using the pywinauto package. It has a very natural syntax to automate any GUI programs.\n\nCode example, opening a file in notepad. Note that the syntax is locale dependent (it uses the visible window titles / control labels in your GUI program):\nfrom pywinauto import application\napp = application.Application().start_('notepad.exe')\napp.Notepad.MenuSelect('File->Open')\n# app.[window title].[control name]...\napp.Open.Edit.SetText('filename.txt')\napp.Open.Open.Click()\n\n"", '\nYou can use ctypes library.\nConsider this code:\nimport ctypes\n\nEnumWindows = ctypes.windll.user32.EnumWindows\nEnumWindowsProc = ctypes.WINFUNCTYPE(ctypes.c_bool, ctypes.POINTER(ctypes.c_int), ctypes.POINTER(ctypes.c_int))\nGetWindowText = ctypes.windll.user32.GetWindowTextW\nGetWindowTextLength = ctypes.windll.user32.GetWindowTextLengthW\nSendMessage = ctypes.windll.user32.SendMessageW\nIsWindowVisible = ctypes.windll.user32.IsWindowVisible\n\ndef foreach_window(hwnd, lParam):\n    if IsWindowVisible(hwnd):\n        length = GetWindowTextLength(hwnd)\n        buff = ctypes.create_unicode_buffer(length + 1)\n        GetWindowText(hwnd, buff, length + 1)\n\n        if(buff.value == ""Choose File to Upload""): #This is the window label\n            SendMessage(hwnd, 0x0100, 0x09, 0x00000001 )\n    return True\n\nEnumWindows(EnumWindowsProc(foreach_window), 0)\n\nYou loop on every open window, and you send a key stroke to the one you choose.\nThe SendMessage function gets 4 params: the window hendler (hwnd), The phisical key to send - WM_KEYDOWN (0x0100), The virtual-key code of tab (0x09) and the repeat count, scan code, extended-key flag, context code, previous key-state flag, and transition-state flag in the 4th argument.\nYou can also send key up, key down, chars, returns and etc...\nUse the documentation for help.\nI used this as a reference: Win32 Python: Getting all window titles\nGood luck!\n']",https://stackoverflow.com/questions/37027644/open-file-from-windows-file-dialog-with-python-automatically,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do I use UI Automation on a WPF ItemsControl that groups items?,"
I am using Microsoft UI Automation (i.e. AutomationElement) to run automated acceptance tests against my application. This has gone well, but I've hit a situation that doesn't appear to be exposed to the automation framework.
I have an ItemsControl (although I could be using one of its derived controls, e.g. ListBox) and I am using CollectionViewSource to group items. Here is a complete window to demonstrate:
<Window x:Class=""GroupAutomation.Window1"" xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation"" xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml"" Title=""Orchestra"">
    <Window.Resources>

        <!-- Take some simple data -->
        <XmlDataProvider x:Key=""SampleData"" XPath=""Orchestra/Instrument"">
            <x:XData>
                <Orchestra xmlns="""">
                    <Instrument Name=""Flute"" Category=""Woodwind"" />
                    <Instrument Name=""Trombone"" Category=""Brass"" />
                    <Instrument Name=""French horn"" Category=""Brass"" />
                </Orchestra>
            </x:XData>
        </XmlDataProvider>

        <!-- Add grouping -->
        <CollectionViewSource Source=""{Binding Source={StaticResource SampleData}}"" x:Key=""GroupedView"">
            <CollectionViewSource.GroupDescriptions>
                <PropertyGroupDescription PropertyName=""@Category"" />
            </CollectionViewSource.GroupDescriptions>
        </CollectionViewSource>
    </Window.Resources>

    <!-- Show it in an ItemsControl -->
    <ItemsControl ItemsSource=""{Binding Source={StaticResource GroupedView}}"" HorizontalAlignment=""Left"" Margin=""4"">
        <ItemsControl.GroupStyle>
            <GroupStyle>
                <GroupStyle.HeaderTemplate>
                    <DataTemplate>
                        <TextBlock Text=""{Binding Path=Name}"" FontWeight=""Bold"" />
                    </DataTemplate>
                </GroupStyle.HeaderTemplate>
            </GroupStyle>
        </ItemsControl.GroupStyle>
        <ItemsControl.ItemTemplate>
            <DataTemplate>
                <Border Padding=""4"" Margin=""4"" Background=""#FFDEDEDE"">
                    <StackPanel>
                        <Label Content=""{Binding XPath=@Name}"" />
                        <Button Content=""Play"" />
                    </StackPanel>
                </Border>
            </DataTemplate>
        </ItemsControl.ItemTemplate>
    </ItemsControl>
</Window>

This produces a window containing the items grouped into their categories, and each item has a button that I'd like to click with UI Automation:

(source: brizzly.com) 
However, if I look in UISpy.exe (or navigate with AutomationElement) I only see the groups (even in the Raw view):

(source: brizzly.com) 
As you can see, the groups are there but they contain no items, so there is nowhere to look for the buttons. I have tried this in both WPF 3.5 SP1 and WPF 4.0 and get the same result.
Is it possible to use UI Automation on items that are grouped, and if so, how?
",9k,"
            8
        ","[""\nI came across this problem and managed to solve it by implementing a 'GenericAutomationPeer' from\n\nHelping the Coded UI Framework Find Your Custom Controls\n\nand adding a special case for GroupItems.\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\nusing System.Windows;\nusing System.Windows.Controls;\nusing System.Windows.Automation;\nusing System.Windows.Automation.Peers;\nusing System.Windows.Media;\nusing System.Xml;\n\nnamespace ClassLibrary1\n{\n    public class MyItemsControl : ItemsControl\n    {\n        protected override AutomationPeer OnCreateAutomationPeer()\n        {\n            return new GenericAutomationPeer(this);\n        }\n    }\n\n    public class GenericAutomationPeer : UIElementAutomationPeer\n    {\n        public GenericAutomationPeer(UIElement owner) : base(owner)\n        {\n        }\n        \n        protected override List<AutomationPeer> GetChildrenCore()\n        {\n            var list = base.GetChildrenCore();\n            list.AddRange(GetChildPeers(Owner));\n            return list;\n        }\n\n        private List<AutomationPeer> GetChildPeers(UIElement element)\n        {\n            var list = new List<AutomationPeer>();\n            for (int i = 0; i < VisualTreeHelper.GetChildrenCount(element); i++)\n            {\n                var child = VisualTreeHelper.GetChild(element, i) as UIElement;\n                if (child != null)\n                {\n                    AutomationPeer childPeer;\n                    if (child is GroupItem)\n                    {\n                        childPeer = new GenericAutomationPeer(child);\n                    }\n                    else\n                    {\n                        childPeer = UIElementAutomationPeer.CreatePeerForElement(child);\n                    }\n                    if (childPeer != null)\n                    {\n                        list.Add(childPeer);\n                    }\n                    else\n                    {\n                        list.AddRange(GetChildPeers(child));\n                    }\n                }\n            }\n            return list;\n        }\n    }\n\n}\n\nI hope this helps anyone still searching for an answer!\n"", ""\nI'm not 100% sure about buttons, but TextBlock controls that are inside DataTemplates do not get put into the UI Automation tree. Apparently this is an optimization to avoid 1000's of unneccessary textblocks.\nYou can work around it by SubClassing TextBlock. Here's mine:\npublic class AutomatableTextBlock : TextBlock\n{\n    protected override AutomationPeer OnCreateAutomationPeer()\n    {\n        return new AutomatableTextBlockAutomationPeer(this);\n    }\n\n    class AutomatableTextBlockAutomationPeer : TextBlockAutomationPeer\n    {\n        public AutomatableTextBlockAutomationPeer(TextBlock owner)\n            : base(owner)\n        { }\n\n        protected override bool IsControlElementCore()\n        { return true; }\n    }\n}\n\nNote: UI Automation also doesn't expose various other controls like Canvas, Panel, you can get them to show up with a similar subclass.\nIn saying that, I'm not sure why the Button isn't appearing.... Hrmmm\n"", ""\nWhat tools are you using to write the automated scripts? I would have thought there'd be an option to drill into WPF's logical/visual trees rather than relying on the Win32 tree (as surfaced by UISpy).\nIf you have a look at the same application using Snoop, you'll see the full visual and logical trees.\n"", '\nI ended up solving this in my application by using TreeWalker.RawViewWalker to manually navigate the tree after using AutomationElement.FindFirst to find the template. FindFirst seems to reliably exclude all the information you want when automating somebody else\'s application. RawViewWalker seems to work when the elements show up in ""Inspect Objects"", but not in UISpy or your application.\n']",https://stackoverflow.com/questions/2772071/how-do-i-use-ui-automation-on-a-wpf-itemscontrol-that-groups-items,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When I try to use UI Automation for PowerPoint 2013, I can only get the first character/word when I use RangeFromPoint","
The code works for Word and Outlook but fails with PowerPoint in that only the first character or first word of the textbox ever gets selected. Is this a bug? Is there any workaround? Try this on a simple PowerPoint slide in PowerPoint 2013.
private static async Task<string> getText(double x, double y)
{
    string result = null;

    try
    {
        var location = new System.Windows.Point(x, y);
        AutomationElement element = AutomationElement.FromPoint(location);

        object patternObj;
        if (element.TryGetCurrentPattern(TextPattern.Pattern, out patternObj))
        {
            var textPattern = (TextPattern)patternObj;

            var range = textPattern.RangeFromPoint(location);
            range.ExpandToEnclosingUnit(TextUnit.Word);
            range.Select();

            var text = range.GetText(-1).TrimEnd('\r');
            return text.Trim();
        }
        else
        {
            return ""no text found"";
        }
    }
    catch (Exception ex)
    {
        return ex.Message;
    }
}

You cannot see it from the screenshot, but the mouse is on ""first"" not ""stuck"", but regardless of where the mouse is placed, it always is stuck. Maybe this is fixed in PowerPoint 2016?

When I look at the bounding box for the range it is always the whole element, rather than the selected word. That could be part of the problem of why RangeToPoint is not working.
Original posted in MSDN but no response...
Update. If I use 
text = printRange(range, text);
while (range.Move(TextUnit.Word, 1) > 0)
{
    text += Environment.NewLine;
    text = printRange(range, text);
}

I get

",908,"
            7
        ","['\nThis behavior is probably due to a limitation in PowerPoint 2013, and I expect you can\'t work around it using UIA. When you call RangeFromPoint(), the UIA provider hit beneath the mouse, (ie the one that\'s implementing IUIAutomationTextPattern::RangeFromPoint(),) is meant to return a degenerative (ie empty) range where the mouse cursor is. Then the UIA client can expand the returned range to get the surrounding character, word, line or paragraph.\nHowever, as you point out, PowerPoint 2013 isn\'t doing that. I\'ve just written the test code below, (using a managed wrapper for the native Windows UIA API generated by tlbimp.exe,) and found that PowerPoint apparently returns a TextRange for the entire text box beneath the cursor. When I ran the code, I found that I did get the expected word beneath the cursor in WordPad, Word 2013 and PowerPoint OnLine, but not PowerPoint 2013. I got the same results when I ran the Text Explorer tool that\'s part of the Inspect SDK tool. The image below shows Text Explorer reporting that the text returned from PowerPoint 2013 is the entire text in the a text box, when the mouse is hovering over one of those words.\n(I should add that for the test code below to work at all, I think the current display scaling setting needs to be at 100%. I\'ve not added code to account for some other scaling being active.)\nI don\'t know if this is fixed in PowerPoint 2016, I\'ll try to look into that and let you know.\nThanks,\nGuy\n\nprivate void buttonGetTheText_Click(object sender, EventArgs e)\n{\n    labelText.Text = ""No text found."";\n\n    IUIAutomation uiAutomation = new CUIAutomation8();\n\n    Point ptCursor = Cursor.Position;\n\n    tagPOINT pt;\n    pt.x = ptCursor.X;\n    pt.y = ptCursor.Y;\n\n    // Cache the Text pattern that\'s available through the element beneath\n    // the mouse cursor, (if the Text pattern\'s supported by the element,) in\n    // order to avoid another cross-process call to get the pattern later.\n    int patternIdText = 10014; // UIA_TextPatternId\n    IUIAutomationCacheRequest cacheRequestTextPattern =\n        uiAutomation.CreateCacheRequest();\n    cacheRequestTextPattern.AddPattern(patternIdText);\n\n    // Now get the element beneath the mouse.\n    IUIAutomationElement element = \n        uiAutomation.ElementFromPointBuildCache(pt, cacheRequestTextPattern);\n\n    // Does the element support the Text pattern?\n    IUIAutomationTextPattern textPattern =\n        element.GetCachedPattern(patternIdText);\n    if (textPattern != null)\n    {\n        // Now get the degenerative TextRange where the mouse is.\n        IUIAutomationTextRange range = textPattern.RangeFromPoint(pt);\n        if (range != null)\n        {\n            // Expand the range to include the word surrounding \n            // the point where the mouse is.\n            range.ExpandToEnclosingUnit(TextUnit.TextUnit_Word);\n\n            // Show the word in the test app.\n            labelText.Text = ""Text is: \\"""" + range.GetText(256) + ""\\"""";\n        }\n    }\n}\n\n', ""\nI can suggest only Python code getting caption text of the slide (for example). Sorry, I have no time to re-write it on C#. You can play with the PowerPoint.Application COM object and MSDN example of Power Point automation.\nfrom __future__ import print_function\nimport win32com.client as com\npp = com.Dispatch('PowerPoint.Application')\nprint(pp.Presentations[0].Slides[8].Shapes[0].TextFrame.TextRange.Text)\n\n""]",https://stackoverflow.com/questions/32540442/when-i-try-to-use-ui-automation-for-powerpoint-2013-i-can-only-get-the-first-ch,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Programmatically turn on/off wifi on real iOS device with UI Automation,"
I have already read once or twice that turn on/off wifi on a real iOS device (iPad in my case) with a UI Automation script seems not possible.
I've also read that you can create a script with the target ""Settings"" but it seems that it's only for simulators, am I right ?
Do you have any ideas or solutions for me ?
Regards, 
",3k,"
            6
        ","['\ni was able to do so (just make sure you target application is on foremost state when doing so):\nvar target = UIATarget.localTarget();\n\ntarget.dragFromToForDuration({x:0.1, y:557.00}, {x:211.00, y:206.00},\n0.5); //to expose the actions panel on iOS\n\ntarget.frontMostApp().mainWindow().elements()[""Wifi""].tapWithOptions({tapOffset:{x:0.44, y:0.47}});\n\n', ""\nTo do this, like @Larme states, the user's iOS device must be jailbroken due to the locked-down nature of iOS. \n""]",https://stackoverflow.com/questions/21828552/programmatically-turn-on-off-wifi-on-real-ios-device-with-ui-automation,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to inject click event with Android UiAutomation.injectInputEvent,"
I'm automating the testing of a flow in my app where I install a device administrator.  To activate a device administrator on most devices (let's assume here I don't have some enterprise API that lets me do this like what Samsung offers) the system displays a popup to the user who then has to click the ""Activate"" button.
I'm using Robotium and Android JUnit to drive my tests.  In a normal testing case one can only interact with the app and process under test and not any system activities that come up.
The UiAutomation claims to allow you to interact with other applications by leveraging the Accessibility Framework, and then allowing one to inject arbitrary input events.
So - here's what I'm trying to do:
public class AbcTests extends ActivityInstrumentationTestCase2<AbcActivity> {

    private Solo mSolo

    @Override
    public void setUp() {
        mSolo = new Solo(getInstrumentation(), getActivity());

    }

    ...

    public void testAbc(){
    
        final UiAutomation automation = getInstrumentation().getUiAutomation();         
        
        MotionEvent motionDown = MotionEvent.obtain(SystemClock.uptimeMillis(), SystemClock.uptimeMillis(), KeyEvent.ACTION_DOWN,
                100,  100, 0);

        automation.injectInputEvent(motionDown, true)
        MotionEvent motionUp = MotionEvent.obtain(SystemClock.uptimeMillis(), SystemClock.uptimeMillis(), KeyEvent.ACTION_UP,
                100, 100, 0);

        automation.injectInputEvent(motionUp, true)
        motionUp.recycle();
        motionDown.recycle();
     }
    
 }

When this test is run the System popup to ""Activate"" the device administrator is active, and I want to just click on the screen.  I've hardcoded in 100,100 as the position for clicks for the purposes of this question but realistically I'll click in the bottom right corner of the screen so I can hit the button.
I do not get any click events occurring on the screen.  Does anyone have experience with this?   Are there any alternatives to do what I want to do?  From my understanding there are very few tools that do this.
Thanks.
Update
Added setSource for right answer
",5k,"
            6
        ","[""\nFinally figured this out.  I compared my MotionEvents to the two events that get dispatched when I clicked on a button and the only difference was the source.  So, I set the source on the two motionEvents and it worked.\n....\nmotionDown.setSource(InputDevice.SOURCE_TOUCHSCREEN);\n....\nmotionUp.setSource(InputDevice.SOURCE_TOUCHSCREEN);\n\nAnd here's a full version of the method   \n//=========================================================================\n//==                        Utility Methods                             ===\n//=========================================================================\n/**\n * Helper method injects a click event at a point on the active screen via the UiAutomation object.\n * @param x the x position on the screen to inject the click event\n * @param y the y position on the screen to inject the click event\n * @param automation a UiAutomation object rtreived through the current Instrumentation\n */\nstatic void injectClickEvent(float x, float y, UiAutomation automation){\n    //A MotionEvent is a type of InputEvent.  \n    //The event time must be the current uptime.\n    final long eventTime = SystemClock.uptimeMillis();\n\n    //A typical click event triggered by a user click on the touchscreen creates two MotionEvents,\n    //first one with the action KeyEvent.ACTION_DOWN and the 2nd with the action KeyEvent.ACTION_UP\n    MotionEvent motionDown = MotionEvent.obtain(eventTime, eventTime, KeyEvent.ACTION_DOWN,\n            x,  y, 0); \n    //We must set the source of the MotionEvent or the click doesn't work.\n    motionDown.setSource(InputDevice.SOURCE_TOUCHSCREEN);\n    automation.injectInputEvent(motionDown, true);\n    MotionEvent motionUp = MotionEvent.obtain(eventTime, eventTime, KeyEvent.ACTION_UP,\n            x, y, 0);\n    motionUp.setSource(InputDevice.SOURCE_TOUCHSCREEN);\n    automation.injectInputEvent(motionUp, true);\n    //Recycle our events back to the system pool.\n    motionUp.recycle();\n    motionDown.recycle();\n}\n\n""]",https://stackoverflow.com/questions/23159265/how-to-inject-click-event-with-android-uiautomation-injectinputevent,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to check element properties in iOS gui automation?,"
All UI Automation examples I've seen uses standard components whose state can be inspected with the JavaScript API using the value() method. This is a bit limiting. Lets say you want to check the color or alpha value and whatnot.
How can I inspect the properties of a view?
An example: a tap on a certain element should make it ""selected"". I'd like to perform a tap on it and then verify that isSelected is TRUE.
Update:
I found the withPredicate() method which should do it in theory, except it seems to only trigger on name properties:
element.withPredicate(""isSelected == YES"")          // always fails
element.withPredicate(""name matches 'my element'"")  // works

",3k,"
            5
        ","['\nI ended up with this approach which works for my purposes:\nLet UIView.accessibilityValue return a JSON string with relevant properties:\n- (NSString *)accessibilityValue\n{\n    return [NSString stringWithFormat:\n            @""{\'alpha\':%f, \'isSelected\':%@}"", \n            self.alpha, self.isSelected ? @""true"" : @""false""];\n}\n\nThen use eval() in the test code and check those properties. value() is shorthand for calling accessibilityValue:\nvar props = eval(""("" + element.value() + "")"");\n\nif (props.isSelected) {\n    UIALogger.logFail(""Should not be selected"");\n}\n\nUIATarget.localTarget().tap({""x"":471, ""y"":337});\n\nvar props = eval(""("" + element.value() + "")"");\n\nif (!props.isSelected) {\n    UIALogger.logFail(""Should be selected"");\n}\n\n']",https://stackoverflow.com/questions/6504358/how-to-check-element-properties-in-ios-gui-automation,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows UI Automation not showing all child elements?,"
I have a TreeView control on my form, and I'm recursively going through the elements of another window starting with the window itself. I'm  using this to find the elements:
getRecursiveElements(AutomationElement parent)
{
  children = parent.FindAll(TreeScope.Children, Condition.TrueCondition);

  foreach (AutomationElement child in children)
  {
    addToTreeView(child);
    getRecursiveElements(child);
  }
}

Generally speaking, the code works quite well in most cases. The tree is populated and I have a bit of other supporting code allowing me to double click, for example, an element in the tree-view and it will highlight that element on the target form.
The issue I'm having is that, while it generates an awesome tree, there are still some elements missing for certain target programs.
What possible reason could there be for this, and is there any way to get around it?
If I call EnumChildWindows() from user32.dll will that have the same problem?
",7k,"
            5
        ","[""\nNot all programs use separate windowed controls for all their logical children. Mostly this depends on the GUI framework used.\nAs an extreme example, Qt uses a single window for each top-level window. It then paints all the widgets on the form from the form's WM_PAINT message handler.\nPrograms that take this approach are typically impossible to automate through generic methods.\nIt sounds like you have encountered an application that uses some windowed controls but also uses custom controls with a single window for what appears to be multiple widgets. Again this is quite common.\n"", ""\nCould you give a better example of what fails? Thinking about the problem, it may be that the 'element' in the other form is being drawn manually, and so doesn't have distinct registered handles for everything.\n""]",https://stackoverflow.com/questions/7238883/windows-ui-automation-not-showing-all-child-elements,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to add UIAutomationClient.dll and UIAutomationTypes.dll to .Net Core 5.0 project?,"
How to use UIAutomationClient.dll and UIAutomationTypes.dll in .NET 5.0 project since there is no nuget package available!
I'm trying to convert a .NET Framework 4.8 project to .NET 5.0
<Project Sdk=""Microsoft.NET.Sdk.WindowsDesktop"">
    <PropertyGroup>
        <OutputType>WinExe</OutputType>
        <TargetFrameworks>net48;net5.0-windows</TargetFrameworks>
        <UseWindowsForms>true</UseWindowsForms>
        <LangVersion>9.0</LangVersion>
    </PropertyGroup>
</Project>

",2k,"
            5
        ","['\nThis .csproj is enough to use System.Windows.Automation (UIAutomation) in net5\n<Project Sdk=""Microsoft.NET.Sdk"">\n  <PropertyGroup>\n    <OutputType>Exe</OutputType>\n    <TargetFramework>net5.0-windows</TargetFramework>\n  </PropertyGroup>\n  <ItemGroup>\n    <FrameworkReference Include=""Microsoft.WindowsDesktop.App.WPF""/>\n  </ItemGroup>\n</Project>\n\n']",https://stackoverflow.com/questions/67355538/how-to-add-uiautomationclient-dll-and-uiautomationtypes-dll-to-net-core-5-0-pro,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Android Espresso: Make assertion while button is kept pressed,"
I'm quite new to Espresso on Android and I am running into the following problem:
I want Espresso to perform a longclick(or something..) on a button, and while the button is kept pressed down, I want to check the state of a different View.
In (more or less) pseudocode:
onView(withId(button_id)).perform(pressButtonDown());
onView(withId(textBox_id)).check(matches(withText(""Button is pressed"")));
onView(withId(button_id)).perform(releaseButton());

I tried writing 2 custom Taps, PRESS and RELEASE, with MotionEvents.sendDown() and .sendUp(), but did not get it to work.
If this is the right path, I can post the code I've got so far.
Edit:
public class PressViewActions
{
    public static ViewAction pressDown(){
        return new GeneralClickAction(HoldTap.DOWN, GeneralLocation.CENTER, Press.THUMB);
    }

    public static ViewAction release() {
        return new GeneralClickAction(HoldTap.UP, GeneralLocation.CENTER, Press.THUMB);
    }
}

And the code for the Tappers:
public enum HoldTap implements Tapper {

    DOWN {
        @Override
        public Tapper.Status sendTap(UiController uiController, float[] coordinates, float[] precision)
        {
            checkNotNull(uiController);
            checkNotNull(coordinates);
            checkNotNull(precision);

            DownResultHolder res = MotionEvents.sendDown(uiController, coordinates, precision);

            ResultHolder.setController(uiController);
            ResultHolder.setResult(res);

            return Status.SUCCESS;
        }
    },

    UP{
        @Override
        public Tapper.Status sendTap(UiController uiController, float[] coordinates, float[] precision)
        {
            DownResultHolder res = ResultHolder.getResult();
            UiController controller = ResultHolder.getController();

            try {
                if(!MotionEvents.sendUp(controller, res.down))
                {
                    MotionEvents.sendCancel(controller, res.down);
                    return Status.FAILURE;
                }

            }
            finally {
                //res.down.recycle();
            }

            return Status.SUCCESS;
        }
    }
}

The error I get is the following:
android.support.test.espresso.PerformException: Error performing 'up click' on view 'with id: de.test.app:id/key_ptt'.
...
...
Caused by: java.lang.RuntimeException: Couldn't click at: 281.5,1117.5 precision: 25.0, 25.0 . Tapper: UP coordinate provider: CENTER precision describer: THUMB. Tried 3 times. With Rollback? false

I hope, this helps.
Any help and ideas will be appreciated!
Tanks a lot in advance!
",3k,"
            5
        ","['\nA tap in Android is made up of two events, a down event and a up/cancel event. If you want to have these two as separate you cannot make ""taps"" as they already encompass both.\nThat said, your idea works, you just need to use a lower-level api, namely UiController along with the helper methods in MotionEvents. Be warned though: since ""releasing"" a view requires first holding on it, your tests will be dependent on each other if you don\'t do proper clean up. \nExample: in a test you press a View, your test fails, then in another test you release on a view you didn\'t click: your second test would pass while it shouldn\'t have.\nI uploaded on my github a complete sample. Here the keypoints. First the test code:\n@Before\npublic void setUp() throws Exception {\n    super.setUp();\n    injectInstrumentation(InstrumentationRegistry.getInstrumentation());\n    getActivity();\n}\n\n@After\npublic void tearDown() throws Exception {\n    super.tearDown();\n    LowLevelActions.tearDown();\n}\n\n@Test\npublic void testAssertWhilePressed() {\n    onView(withId(R.id.button)).perform(pressAndHold());\n    onView(withId(R.id.text)).check(matches(withText(""Button is held down"")));\n    onView(withId(R.id.button)).perform(release());\n}\n\nThen LowLevelActions:\npublic class LowLevelActions {\n    static MotionEvent sMotionEventDownHeldView = null;\n\n    public static PressAndHoldAction pressAndHold() {\n        return new PressAndHoldAction();\n    }\n\n    public static ReleaseAction release() {\n        return new ReleaseAction();\n    }\n\n    public static void tearDown() {\n        sMotionEventDownHeldView = null;\n    }\n\n    static class PressAndHoldAction implements ViewAction {\n        @Override\n        public Matcher<View> getConstraints() {\n            return isDisplayingAtLeast(90); // Like GeneralClickAction\n        }\n\n        @Override\n        public String getDescription() {\n            return ""Press and hold action"";\n        }\n\n        @Override\n        public void perform(final UiController uiController, final View view) {\n            if (sMotionEventDownHeldView != null) {\n                throw new AssertionError(""Only one view can be held at a time"");\n            }\n\n            float[] precision = Press.FINGER.describePrecision();\n            float[] coords = GeneralLocation.CENTER.calculateCoordinates(view);\n            sMotionEventDownHeldView = MotionEvents.sendDown(uiController, coords, precision).down;\n            // TODO: save view information and make sure release() is on same view\n        }\n    }\n\n    static class ReleaseAction implements ViewAction {\n        @Override\n        public Matcher<View> getConstraints() {\n            return isDisplayingAtLeast(90);  // Like GeneralClickAction\n        }\n\n        @Override\n        public String getDescription() {\n            return ""Release action"";\n        }\n\n        @Override\n        public void perform(final UiController uiController, final View view) {\n            if (sMotionEventDownHeldView == null) {\n                throw new AssertionError(""Before calling release(), you must call pressAndHold() on a view"");\n            }\n\n            float[] coords = GeneralLocation.CENTER.calculateCoordinates(view);\n            MotionEvents.sendUp(uiController, sMotionEventDownHeldView, coords);\n        }\n    }\n}\n\n']",https://stackoverflow.com/questions/32010927/android-espresso-make-assertion-while-button-is-kept-pressed,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filling web form via PowerShell does not recognize the values entered,"
Working as a QA I need to fill in a lot of applications through a web form.
Idea is to have the personal data in some xls/txt/whatever file, read the file and use Powershell to feed data to the browser.
When I use the code below to fill in the form in IE, even though it seems to work fine, I get an error when submitting the form that no data was entered.
Any ideas or suggestions how to get past this would be much appreciated
Sadly my resources are limited to Powershell 2.0. Selenium or any other ""more sophisticated"" tools are out of question at least for now.
validation error here
$ie = New-Object -com InternetExplorer.Application
$ie.Navigate(""MyURL"")
$ie.visible = $true

while ($ie.ReadyState -ne 4){sleep -m 100}

Function ClickById($id) {
    $ie.document.getElementById($id).Click()
}

### Z谩kladn铆 煤daje
$FnId = 'personalData.firstName'
$LnId = 'personalData.lastName'
$PhoneId = 'personalData.mobilePhone'
$EmailId = 'personalData.email'
$DataAgreementCheckBox = 'application.personalDataAgreement'
$SubmitfwdId = 'forward'


$Values = ""Ublala"", ""Pung"", ""222333444"", ""ublala@pung.com""
$Ds1Elements = $FnId, $LnId, $PhoneId, $EmailId

$j = 0
foreach ($El in $Ds1Elements) {
    $ie.document.getElementById($El).value = $values[$j]
    $j++
}

ClickById $DataAgreementCheckBox
ClickById $SubmitfwdId

",7k,"
            4
        ","['\nThanks for the suggestion but it did not work as the form seemes to be stupid in many ways.\nAnyway I used your advice for the focus when going with SendKeys method and it  did the trick.\nAt the beginning I needed to load this assembly\n[void] [System.Reflection.Assembly]::LoadWithPartialName(""\'System.Windows.Forms"")\n\nand then changed the loop accordingly to use the SendKeys method\n$j = 0\nforeach ($El in $Ds1Elements) {\n    $ie.document.getElementById($El).focus()\n    [System.Windows.Forms.SendKeys]::Sendwait($values[$j]);   \n    $j++\n}\n\nAnd tradaaaa the form is filled and nobody is complaining :)\n', ""\nSometimes website forms will wait until the input box has lost focus to pre-validate the value before submitting the form.  The text boxes may never get focus if you are manually setting the values in the background so the form believes that you haven't actually entered any values.  \nYou may be able to get around this by manually focusing each text box in turn, and then focusing the submit button before clicking it.  Each element in the $ie.Document should have a focus() method you can use.\n""]",https://stackoverflow.com/questions/33216366/filling-web-form-via-powershell-does-not-recognize-the-values-entered,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XCUITest interact with a Notification Banner.,"
Is it possible in XCUITest to validate that a notification banner is sent to the screen? 
I can add an accessibility identifier to the notification, but I am having a problem getting XCUITest to interact with it when the banner is ent to the screen.  I know that XCUITest runs in a separate process from the app, but was wondeering if it was still possible to interact with the notification or is it beyond the scope of XCUITest?
Thanks,
",4k,"
            4
        ","['\nWith Xcode 9 this is now possible. You can get access to other apps. That includes the springboard which contains the Notification banner.\nXCUIApplication now has a new initializer that takes a bundle identifier as parameter. It gives you a reference to the app that uses that bundle identifier. You can then use the normal queries to access UI elements. Just like you did before with your own app.\nThis is a test that checks if a Local Notification is being displayed when the app is closed:\nimport XCTest\n\nclass UserNotificationUITests: XCTestCase {\n\n    override func setUp() {\n        super.setUp()\n        continueAfterFailure = false\n    }\n\n    func testIfLocalNotificationIsDisplayed() {\n        let app = XCUIApplication()\n        let springboard = XCUIApplication(bundleIdentifier: ""com.apple.springboard"")\n\n        app.launch()\n\n        // close app to trigger local notification\n        XCUIDevice.shared.press(XCUIDevice.Button.home)\n\n        // wait for the notification\n        let localNotification = springboard.otherElements[""USERNOTIFICATION, now, Buy milk!, Remember to buy milk from store!""]\n        XCTAssertEqual(waiterResultWithExpectation(localNotification), XCTWaiter.Result.completed)\n    }\n}\n\nextension UserNotificationUITests {\n    func waiterResultWithExpectation(_ element: XCUIElement) -> XCTWaiter.Result {\n        let myPredicate = NSPredicate(format: ""exists == true"")\n        let myExpectation = XCTNSPredicateExpectation(predicate: myPredicate,\n                                                      object: element)\n        let result = XCTWaiter().wait(for: [myExpectation], timeout: 6)\n        return result\n    }\n}\n\nYou can checkout the demo app including this test here\nYou can also test Remote Notifications with UITests. That\'s needs a bit more work, because you cannot directly schedule Remote Notifications from your code. You can use a service called NWPusher for that. I wrote a blogpost about how to test Remote Notifications with Xcode UITests and there also is a demo project on github.\n', '\nXcode 12 iOS 14*\nAlso accessing/asserting inside the notification is possible:\nlet notification = springBoard.otherElements[""Notification""].descendants(matching: .any)[""APPNAME, now, TITLE, BODY""]\n    if notification.waitForExistence(timeout: 10) {\n        notification.tap()\n    }\n\n', '\nNot yet, unfortunately. XCUITest does not provide access to the notification bar. For almost every test case scenario you might come up with, Appium is a good alternative, however Apple still does not provide means to test push notifications. One way to work around this is to force your app to send the notification, take a screenshot when the notification popup is being displayed and then use an image recognition technique to verify if the notification is correct (such as OpenCV). This is waaaaay more ""workaround"" then I would like to use, but is the only available method I know so far, hope this helps.\n']",https://stackoverflow.com/questions/42537678/xcuitest-interact-with-a-notification-banner,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Programatically interact with the IE browser to fill in forms and navigate etc,"
I'd like to use C# to interact with the IE browser. 
I have a feeling that shdocvw.dll will be involved, but there are so many classes in there that I don't know where to start, and maybe it's not even necessary to use it.
The goal here is to interact with a website, visiting it's pages and ""warming it up,"" not unlike as described here by Kenneth Scott.  The thing is, javascript is getting executed as you interact with a website, so it would be nice just to be able to login / submit forms exactly as you would on the website itself.
Plus it would be nice to be able to create a program that records my actions in IE, and then be able to slightly automate and slightly modify them.
Additionally, it would be nice if it could do all this in the background, without having to display the webpage at all.
I'm not looking for third party solutions, I want to do this myself (with your advice of course.)  
Thanks.
",6k,"
            4
        ","[""\nYou said you're not looking for a third party solution, however, we have used WatiN in work with great success for automated UI testing.\nIt's open source, so if you want to see how they do it, you can.\n"", '\nThings like selenium and watin are very mature frameworks for doing exactly what you ask.  Unless the point is to learn for yourself how to do this I would use one of them.  \nWatin is also a great way to learn how to do this in c# as it is an open source c# project.\n']",https://stackoverflow.com/questions/8438782/programatically-interact-with-the-ie-browser-to-fill-in-forms-and-navigate-etc,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to mark individual parameterized tests with a marker?,"
I have been trying to parameterize my tests using @pytest.mark.parametrize, and I have a marketer @pytest.mark.test(""1234""), I use the value from the test marker to do post the results to JIRA. Note the value given for the marker changes for every test_data. Essentially the code looks something like below.
@pytest.mark.foo
@pytest.mark.parametrize((""n"", ""expected""),[
    (1, 2),
    (2, 3)])
def test_increment(n, expected):
     assert n + 1 == expected

I want to do something like
@pytest.mark.foo
@pytest.mark.parametrize((""n"", ""expected""), [
    (1, 2,@pytest.mark.test(""T1"")),
    (2, 3,@pytest.mark.test(""T2""))
])

How to add the marker when using parameterized tests given that the value of the marker is expected to change with each test?
",1k,"
            4
        ","['\nIt\'s explained here in the documentation: https://docs.pytest.org/en/stable/example/markers.html#marking-individual-tests-when-using-parametrize\nTo show it here as well, it\'d be:\n@pytest.mark.foo\n@pytest.mark.parametrize((""n"", ""expected""), [\n    pytest.param(1, 2, marks=pytest.mark.T1),\n    pytest.param(2, 3, marks=pytest.mark.T2),\n    (4, 5)\n])\n\n']",https://stackoverflow.com/questions/63561537/how-to-mark-individual-parameterized-tests-with-a-marker,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can we use UI Automation tools with the iPhone Simulator?,"
I鈥檝e been using the new UI automation tools with Instruments and the iPhone SDK 4.0, but so far I haven鈥檛 been able to get it to run under the iPhone Simulator. I鈥檝e tried setting the target to every location possible鈥攎y build folder, the app folder in ~/Library/Application Support/iPhone Simulator, etc.鈥攂ut I get an error message when I try to run it:

Unexpected error in -[UIATarget_0x5a1e3b0 frontMostApp], /SourceCache/UIAutomation_Sim/UIAutomation-37/Framework/UIATargetElements.m line 437,

Has anyone gotten this to work?
",3k,"
            3
        ","['\nYes in theory you can. In practice there are a lot of problems. But did you check if simulator has the accessibity.plist?\nI found this on https://devforums.apple.com/message/261883#261883\nthey recommend: ""Copy ~/Library/Application Support/iPhone Simulator/4.0/Library/Preferences/com.apple.Accessibility.plist to ~/Library/Application Support/iPhone Simulator/4.0.1/Library/Preferences"" and it worked for me.\n', '\nIf you have trouble getting the correct target, running your project with a different instrument (like Leaks) should put your executable into the ""choose target"" list. That is at least one potential error source you can exclude that way. \n']",https://stackoverflow.com/questions/3397733/can-we-use-ui-automation-tools-with-the-iphone-simulator,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Getting full contents of a Datagrid using UIAutomation,"
I have need to retrieve all of the items in a Datagrid from an external application using UIAutomation. Currently, I can only retrieve (and view in UISpy) the visible items. Is there a way to cache all of the items in the Datagrid and then pull them? Here's the code:
static public ObservableCollection<Login> GetLogins()
    {

        ObservableCollection<Login> returnLogins = new ObservableCollection<Login>();

        var id = System.Diagnostics.Process.GetProcessesByName(""<Name here>"")[0].Id;
        var desktop = AutomationElement.RootElement;

        var bw = AutomationElement.RootElement.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ProcessIdProperty, id));

        var datagrid = bw.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.AutomationIdProperty, ""lv""));

        var loginLines = datagrid.FindAll(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.DataItem));

        foreach (AutomationElement loginLine in loginLines)
        {
            var loginInstance = new Login { IP = new IP() };

            var loginLinesDetails = loginLine.FindAll(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Custom));

            for (var i = 0; i < loginLinesDetails.Count; i++)
            {
                var cacheRequest = new CacheRequest 
                { 
                    AutomationElementMode = AutomationElementMode.None,
                    TreeFilter = Automation.RawViewCondition
                };

                cacheRequest.Add(AutomationElement.NameProperty);
                cacheRequest.Add(AutomationElement.AutomationIdProperty);

                cacheRequest.Push();

                var targetText = loginLinesDetails[i].FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ClassNameProperty, ""TextBlock""));

                cacheRequest.Pop();

                var myString = targetText.Cached.Name;

                #region Determine data and write to return object
                //Removed private information
                #endregion
                }

            }

            returnLogins.Add(loginInstance);
        }

        return returnLogins;
    }

",4k,"
            3
        ","['\nYou can only retrieve the visible cells because you have table virtualization on.\nTry disabling the virtualization (not always possible in all application but perhaps you want to move it into configuration and change it before testing)\n', ""\nI am 99% sure that this is not possible.  UI Automation doesn't know about the data structures which are represented by the currently visible portion of a grid.  It only sees what is visible.  I think that you will have to page through the grid to get all the data (that is what I do).\n""]",https://stackoverflow.com/questions/12129592/getting-full-contents-of-a-datagrid-using-uiautomation,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to trigger a click on a chrome extension button?,"
I'm building an automated test suite using Selenium Web Driver. At a certain point I must test how the page works by having a Chrome extension turn on or off. Think of it as you would want to click on the Adblock extension and then click disable for this site. Then, turn it on again. 
I searched all over the Internet and there is no way to implement this using just Selenium. Do you know how could I perform such an action? (from Java ideally)
",12k,"
            3
        ","['\nOne possible solution is to go with Chrome options and manage the extensions set to the WebDriver. Quick example:\nChromeOptions options = new ChromeOptions();\noptions.addExtensions(new File(""/path/to/extension.crx""));\nDesiredCapabilities capabilities = new DesiredCapabilities();\ncapabilities.setCapability(ChromeOptions.CAPABILITY, options);\nChromeDriver driver = new ChromeDriver(capabilities);\n\nIf you want to turn those On and OFF in a single test, you can spawn two separate drivers and compare the results, since I\'m not sure that session reuse will do the job in this case.\n', '\nBelow is the solution is in Python with pyautogui (I believe it\'s similar to autoit in java - so you can extend the same solution for java also).\nPre-Condition:\nsave the extension image in the project folder (I saved it under ""autogui_ref_snaps"" folder in my example with ""capture_full_screenshot.png"" name\nPython:\nImports needed\nfrom selenium import webdriver\nfrom selenium.webdriver import ChromeOptions\nimport pyautogui  #<== need this to click on extension\n\nScript:\noptions = ChromeOptions()\noptions.add_argument(""--load-extension="" + r""C:\\Users\\supputuri\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Extensions\\fdpohaocaechififmbbbbbknoalclacl\\5.1_0"") #<== loading unpacked extension\n\ndriver = webdriver.Chrome(\nexecutable_path=os.path.join(chrome_options=options)\nurl = ""https://google.com/""\ndriver.get(url)\n\n# get the extension box\nextn = pyautogui.locateOnScreen(os.path.join(GenericMethods.get_full_path_to_folder(\'autogui_ref_snaps\') + ""/capture_full_screenshot.png""))\n# click on extension \npyautogui.click(x=extn[0],y=extn[1],clicks=1,interval=0.0,button=""left"")\n\nIf you are loading an extension and it\'s not available in incognito mode then follow my answer in here to enable it.\n', '\nCan use sikuli(GUI Automation tool) to click on browser addon.\nImports needed:\n    import org.sikuli.script.Pattern;\n    import org.sikuli.script.Screen;\n\nScript:\n    Pattern addon=new Pattern(""D:\\\\My Files\\\\Addon.jpg"");  //image of the addon must be given as a pattern for identifying that on the browser/webpage\n    Screen s=new Screen();\n    s.hover(addon);\n    s.click(addon);\n\n', '\nIf you want to click the extension icon on the right side of the chrome and it is the extension that will be opened during the opening of the page or after clicking action\n\nyou can use this\npublic void openBrowserExtension(){\n        JavascriptExecutor js = (JavascriptExecutor) driver;\n        js.executeScript(""window.postMessage(\'clicked_browser_action\', \'*\')"");\n    }\n\n']",https://stackoverflow.com/questions/47042409/how-to-trigger-a-click-on-a-chrome-extension-button,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to keep chrome browser window open to be re-used after selenium script finishes on python [duplicate],"






This question already has answers here:
                        
                    



How can I reconnect to the browser opened by webdriver with selenium?

                                (4 answers)
                            

Closed 3 years ago.



I am trying to keep the chrome browser open after selenium finishes executing my test script. I want to re-use the same window for my second script to run.
",10k,"
            3
        ","[""\nBrowser window closes when your Chrome webdriver instance variable is garbage collected. If you want to avoid this even when your script finishes executing, you can make it global. I.e.:\ndef test():\n    global driver # this will prevent the driver variable from being garbage collected\n    driver = webdriver.Chrome()\n    ...\n\nExplanation:\nA selenium.webdriver.Chrome class instance contains an instance of a Service class. The latter has a __del__ method which is called when the instance is being destructed during garbage collection process. The method in turn stops the service and causes Chrome browser window to close.\nThis also explains why some users don't observe this behavior. I suspect that this is because they have Chrome webdriver instance variable at file scope, not inside a function.\n"", ""\nI know what to do in WATIR(Ruby language), I am writing the code below, So it might give you the clue what to do with your language\nrequire 'watir'\ncaps = Selenium::WebDriver::Remote::Capabilities.chrome(chrome_options: {detach: true})\nb = Watir::Browser.new :chrome, desired_capabilities: caps\nb.goto('www.google.co.uk')\n\nThis given below line is important, If you can re-write this line your language(python),then you may prevent from closing chrome browser\ncaps = Selenium::WebDriver::Remote::Capabilities.chrome(chrome_options: {detach: true})\n\n"", '\nThis should be as simple as not calling driver.quit() at the end of your test case. You should be left with the chrome window in an opened state.\n']",https://stackoverflow.com/questions/42044315/how-to-keep-chrome-browser-window-open-to-be-re-used-after-selenium-script-finis,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to store attribute values of a table for assertion in Karate,"
I have a case where locator doesn't have a text value but it's attribute named title has a text value that I need to assert. While writing custom locator for it I can only get the text value which is """" and not specific attribute value say title = ""#abcdd"".
Example:
<div class=""table-cell"" role=""cell"" table-field= ""risk"" title=""high"">high</div>

Has high as value which I can get
Whereas,
<div class=""table-cell"" role=""cell"" table-field= ""colour"" title=""#abcdd""></div>

Doesn't have any text value but need to get title attribute value #abcdd in this case.
Need a generic code to get all such title attribute values present inside this table.
Where are the things going wrong? Any way I can handle this? Or that text value needs to be included in html?
Using karate as Automation test tool.
",627,"
            3
        ","[""\nTo get an attribute: https://github.com/intuit/karate/tree/master/karate-core#attribute\n* def temp = attribute('.table-cell', 'title')\n\nAnd if you have an Element reference, you can call .attribute('title') on it: https://github.com/intuit/karate/tree/master/karate-core#chaining\nBut always keep in mind you can call the DOM JS API on any element, any time. So I leave it as a homework for you to figure out how to get the results of Element.attributes, ask a new question with specifically what you tried if needed.\nMake sure you read about locateAll() with filter, for example: https://stackoverflow.com/a/63894989/143475\nAnd also see this: https://stackoverflow.com/a/66900081/143475\n""]",https://stackoverflow.com/questions/66718167/need-to-store-attribute-values-of-a-table-for-assertion-in-karate,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"powershell: how to click a ""submit type"" input","
used powershell to do web ui automation.  came up an exception: invoke method failed, because [System.__ComObject] does not contain 鈥渃lick鈥?method.
can submit type input be clicked?
i used getElementsByTagName getElementsByClassName getElementsByName , does not work.
anyone can help me on this?
powershell code is below:
# open the specified web site and commit the key
$ie = new-object -com ""InternetExplorer.Application""
$ie.navigate(""http://gitlab.alibaba-inc.com/keys/new"")
$ie.visible = $true
while($ie.busy) {sleep 1}

$doc = $ie.document

# commit the button
$commit = $doc.getElementsByTagName(""commit"")

if($commit) 
{$commit.click()}

the html source is as below:
<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'>
<title>
Profile | 
GitLab
</title>
<link href=""/assets/favicon-4b751da746de7855d7eb8123072388ed.ico"" rel=""shortcut icon""    type=""image/vnd.microsoft.icon"" />
<link href=""/assets/application-a9eac7f5b0c3b922de8997ae9ad74ab0.css"" media=""screen"" rel=""stylesheet"" type=""text/css"" />
<script src=""/assets/application-61398d184a36e6ae900134f123d5d649.js"" type=""text/javascript""></script>
<meta content=""authenticity_token"" name=""csrf-param"" />
<meta content=""9SLFk6AwlsN2FoyO8xPY+M1hEbKfqlLTQ4CSDVc4efE="" name=""csrf-token"" />
<script type=""text/javascript"">
//<![CDATA[
window.gon =   {};gon.default_issues_tracker=""gitlab"";gon.api_version=""v3"";gon.api_token=""xkMg31Ssva322SDF cgxY"";gon.gravatar_url=""http://www.gravatar.com/avatar/%{hash}?s=% {size}&d=mm"";gon.relative_url_root="""";
//]]>
</script>

</head>

<body class='ui_basic profile' data-page='keys:new'>
<header class='navbar navbar-static-top navbar-gitlab'>
<div class='navbar-inner'>
<div class='container'>
<div class='app_logo'>
<span class='separator'></span>
<a href=""/"" class=""home has_bottom_tooltip"" title=""Dashboard""><h1>GITLAB</h1>
</a><span class='separator'></span>
</div>
<h1 class='project_name'>Profile</h1>
<ul class='nav'>
<li>
<a>
<div class='hide turbolink-spinner'>
<i class='icon-refresh icon-spin'></i>
Loading...
</div>
</a>
</li>
<li>
<div class='search'>
<form accept-charset=""UTF-8"" action=""/search"" class=""navbar-form pull-left""  method=""get""><div style=""margin:0;padding:0;display:inline""><input name=""utf8""  type=""hidden"" value=""&#x2713;"" /></div>
<input class=""search-input"" id=""search"" name=""search"" placeholder=""Search"" type=""text""   />
<input id=""group_id"" name=""group_id"" type=""hidden"" />
<input id=""repository_ref"" name=""repository_ref"" type=""hidden"" />

<div class='search-autocomplete-json hide' data-autocomplete-opts='[{""label"":""project:  kelude2"",""url"":""/kelude2""},{""label"":""My Profile"",""url"":""/profile""},{""label"":""My SSH  Keys"",""url"":""/keys""},{""label"":""My Dashboard"",""url"":""/""},{""label"":""Admin  Section"",""url"":""/admin""},{""label"":""help: API Help"",""url"":""/help/api""},{""label"":""help:  Markdown Help"",""url"":""/help/markdown""},{""label"":""help: Permissions  Help"",""url"":""/help/permissions""},{""label"":""help: Public Access  Help"",""url"":""/help/public_access""},{""label"":""help: Rake Tasks  Help"",""url"":""/help/raketasks""},{""label"":""help: SSH Keys Help"",""url"":""/help/ssh""}, {""label"":""help: System Hooks Help"",""url"":""/help/system_hooks""},{""label"":""help: Web Hooks  Help"",""url"":""/help/web_hooks""},{""label"":""help: Workflow Help"",""url"":""/help/workflow""}]'>   </div>
</form>

</div>

</li>
<li>
<a href=""/public"" class=""has_bottom_tooltip"" data-original-title=""Public area""    title=""Public area""><i class='icon-globe'></i>
</a></li>
<li>
<a href=""/s/heyun"" class=""has_bottom_tooltip"" data-original-title=""Public area""    title=""My snippets""><i class='icon-paste'></i>
</a></li>
<li>
<a href=""/projects/new"" class=""has_bottom_tooltip"" data-original-title=""New project""    title=""Create New Project""><i class='icon-plus'></i>
</a></li>
<li>
<a href=""/profile"" class=""has_bottom_tooltip"" data-original-title=""Your profile""    title=""My Profile""><i class='icon-user'></i>
</a></li>
<li>
<a href=""/users/sign_out"" class=""has_bottom_tooltip"" data-method=""delete"" data-original-title=""Logout"" rel=""nofollow"" title=""Logout""><i class='icon-signout'></i>  
</a></li>
<li>
<a href=""/u/heyun"" class=""profile-pic""><img alt=""F3ea5164088694b48e4980e52d831927? s=26&amp;d=mm"" src=""http://www.gravatar.com/avatar/f3ea5164088694b48e4980e52d831927? s=26&amp;d=mm"" />
</a></li>
</ul>
</div>
</div>
</header>

<div class='flash-container'>
</div>

<nav class='main-nav'>
<div class='container'><ul>
<li class=""home""><a href=""/profile"" title=""Profile""><i class='icon-home'></i>
</a></li><li class=""""><a href=""/profile/account"">Account</a>
</li><li class=""""><a href=""/profile/notifications"">Notifications</a>
</li><li class=""active""><a href=""/keys"">SSH Keys
<span class='count'>1</span>
</a></li><li class=""""><a href=""/profile/design"">Design</a>
</li><li class=""""><a href=""/profile/history"">History</a>
</li></ul>
</div>
</nav>
<div class='container'>
<div class='content'><h3 class='page_title'>Add an SSH Key</h3>
<hr>
<div>
<form accept-charset=""UTF-8"" action=""/keys"" class=""new_key"" id=""new_key"" method=""post"">   <div style=""margin:0;padding:0;display:inline""><input name=""utf8"" type=""hidden""   value=""&#x2713;"" /><input name=""authenticity_token"" type=""hidden""   value=""9SLFk6AwlsN2FoyO8xPY+M1hEbKfqlLTQ4CSDVc4efE="" /></div><div class='clearfix'>
<label for=""key_title"">Title</label>
<div class='input'><input id=""key_title"" name=""key[title]"" size=""30"" type=""text"" />   </div>
</div>
<div class='clearfix'>
<label for=""key_key"">Key</label>
<div class='input'>
<textarea class=""xxlarge thin_area"" cols=""40"" id=""key_key"" name=""key[key]"" rows=""20"">
</textarea>
<p class='hint'>
Paste your public key here. Read more about how generate it
<a href=""/help/ssh"">here</a>
</p>
</div>
</div>
<div class='actions'>
<input class=""btn btn-save"" name=""commit"" type=""submit"" value=""Save"" />
<a href=""/keys"" class=""btn btn-cancel"">Cancel</a>
</div>
</form>

</div>

<script>
  $('#key_key').on('keyup', function(){
    var title = $('#key_title'),
        val      = $('#key_key').val(),
        key_mail = val.match(/([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+|\.[a-zA-Z0-9._-]+)/gi);

    if( key_mail && key_mail.length > 0 && title.val() == '' ){
      $('#key_title').val( key_mail );
    }
  });
</script>
</div>
</div>
</body>
</html>

",23k,"
            3
        ","['\nYou\'re looking for the wrong element. getElementsByTagName() is looking for the actual tag name (input), not the value of the tag\'s name-attribute (commit). Also, getElementsByTagName() returns a collection of COM objects. Even if no matching tag is found, the method will still return a collection (with 0 elements). You need to either check the Length property and then access the first element of the collection:\n$commit = $doc.getElementsByTagName(""input"")\nif ($commit.Length -gt 0) {\n  $commit.item(0).click()\n}\n\nor filter the element with the name you\'re looking for from the collection:\n$commit = $doc.getElementsByTagName(""input"") | ? { $_.name -eq ""commit"" }\nif ($commit) { $commit.click() }\n\n', '\nI could not access the url you had listed above so I used the MIT website to show you an example of how can this be done.\n# setup\n$ie = New-Object -com InternetExplorer.Application \n$ie.visible=$true\n\n$ie.navigate(""http://web.mit.edu/"") \nwhile($ie.ReadyState -ne 4) {start-sleep -m 100} \n\n$termsField = $ie.document.getElementsByName(""terms"")\n@($termsField)[0].value =""powershell""\n\n\n$submitButton = $ie.document.getElementsByTagName(""input"") \nForeach($element in $submitButton )\n{\n    #look for this field by value this is the field(look for screenshot below) \n    if($element.value -eq ""Search""){\n    Write-Host $element.click()\n    }\n}\n\n    Start-Sleep 10\n\n\n']",https://stackoverflow.com/questions/17721295/powershell-how-to-click-a-submit-type-input,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to convert Katalon Script in Selenium Java?,"
As we know Katalon has now become a paid tool so my Katalon scripts need to be converted into Selenium and Java script.
Katalon scripts are in Groovy, and it's written using Katalon Built-in libraries, objects are saved in .rs(.xml) fie on Object repository and user-defined Keywords are also in Groovy.
So please suggest the best way(time-saving) to convert scripts into selenium.
",2k,"
            3
        ","[""\nI don't think there is a simple way to convert all of your scripts to Selenium. \nKatalon keywords are a wrapper around various Selenium commands (or code snippets) so a one-to-one Katalon-Selenium relationship is not always present. Therefore, one simple way of translating one to another does not exist.\n"", '\nFinally, able to convert Katalon script into Selenium. Refer below to make your own Katalon Studio:\nStep 1. Create an interface and store Global variable\npublic interface RunnerConstants {\nreadByExcel rd=  new readByExcel(""Login.xls"",""LoginData"");\npublic static final String url= rd.getexcelCellData(2, 0);\npublic static final  String userName= rd.getexcelCellData(2, 1);\npublic static final  String password = rd.getexcelCellData(2, 2);\npublic static final  String subscriberid = rd.getexcelCellData(2, 3);\npublic static final  String browserName = ""Chrome-Headless"";\n\n}\n\nStep 2: Make an element class and store WebElement( use page Factory concept)\npublic class takeElement {\n\nstatic WebDriver driver= webD.getInstance();\n\n@FindBy\npublic static WebElement inputLogin = \n driver.findElement(By.xpath(""//input[@id=\'loginID\']""));\n@FindBy\npublic static WebElement inputSubscriberId  = \ndriver.findElement(By.xpath(""//input[@id=\'subscriberID\']""));\n\n\n@FindBy\npublic static WebElement submitbtn= \ndriver.findElement(By.xpath(""//input[@id=\'submitLogin\']""));\n}\n\nStep 3: Create a web driver singleton class\nHow to get webdriver instance to use same instance across all class files\nStep 4: Implement Katalon methods as static in WebUI class.\n public  class  WebUI {\n\n static WebDriver driver = webD.getInstance();\n public static void setDriver(WebDriver driver) {\n    WebUI.driver = driver;\n }  \n public static void openBrowser(String url) {\n    driver.get(url);\n }\npublic static void navigateToUrl(String url) {\n    driver.navigate().to(url);\n}\n}\n\nStep 5: Write your script using TestNG annotations\n public class test {\n\n\n @Test\n public void testA() {\n WebUI.openBrowser(RunnerConstants.url);\n  WebUI.setText(takeElement.inputLogin, RunnerConstants.userName);\n WebUI.setText(takeElement.inputPassword, RunnerConstants.password);\n WebUI.setText(takeElement.inputSubscriberId, RunnerConstants.subscriberid);\n WebUI.click(takeElement.submitbtn);\n WebUI.closeBrowser();\n  }\n}\n\nUsing the above ways, you can reuse your Katalon script. I hope it helps!!\n']",https://stackoverflow.com/questions/58836655/how-to-convert-katalon-script-in-selenium-java,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read data from a database or text file in instruments javascript script,"
I have a script like so:
var target = UIATarget.localTarget();
var mainWindow = target.frontMostApp().mainWindow();

var element = mainWindow.textFields()[""UserID""];
element.setValue(""Hello World"");

UIALogger.logStart(""Logging element tree ..."");
target.logElementTree();
UIALogger.logPass();

What I want to do is read a text file or database connection, so I can replace the ""Hello World"" with either a value from a text file or a database query. Is this possible in the Instruments application with using javascript to control UI Automation for the iphone simulator?
",1k,"
            3
        ","['\nYes it is possible. You can acquire every data that you are able to acquire in a bash script.\nWrite a script file that prints the desired information to the standard output. For example\n#!/bin/bash\ncat myfile\n\nYou can run this bash-script from UIAutomation and get the output of it with this command\nvar result = target.host().performTaskWithPathArgumentsTimeout(full_path_to_your_script, [""""], 10);\n\nNow your can use the output of your bash script:\nelement.setValue(result.stdout);\n\n']",https://stackoverflow.com/questions/19008544/read-data-from-a-database-or-text-file-in-instruments-javascript-script,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross platform solution for automating ncurses-type telnet sessions,"
Background
Part of my work in networking and telco involves automating telnet sessions when legacy hardware doesn't offer easy solutions in other interfaces. Many older pieces of equipment can only be accessed via craft ports (RS-232 serial ports), SNMP, or telnet. Sometimes telnet is the only way to access specific information, however telnet is designed as a human interface and thus requires screen scraping. In addition, there is also the issue of scraping screens where only portions are updated in order to save bandwidth (see ncurses). In my work I have used ActiveState Expect and the Python telnet library.
Question
Which languages and libraries are able to automate telnet sessions and have the following requirements:

Suitable for large projects (e.g. Tcl
doesn't seem to scale as well as
Python in my experience and seems outdated)
Cross Platform (e.g. Pexpect does not work on Windows and Activestate
Expect behaves differently on
Windows plus requires DEP on newer
machines to be turned off)
Able to screen scrape sessions that repaint portions of the screen
(similar to the behavior of ncurses in command-line programs)
Free as in beer!

A preferable solution would also include the following:

Easily redistributable (e.g. Does not
require some huge runtime to be installed on a machine.)
Also works for SSH, serial connections, and other command-line interfaces.

",2k,"
            3
        ",['\nTake a look at demos/Expect/term_expect in the ActiveTcl distribution.  It emulates a cursor-addressable terminal and allows you to test output at specific screen locations.  Check out the example screen-scraping code at the end of the file.\n'],https://stackoverflow.com/questions/2060420/cross-platform-solution-for-automating-ncurses-type-telnet-sessions,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
arguments[0].click() not working for select option in selenium,"
I am using selenium for the web application automation.
 I stuck in one point,I am using .ExecuteScript() to perform some action like to click on a link and for that am using :-
((IJavaScriptExecutor)driver).ExecuteScript(""arguments[0].click()"", driver.FindElement(By.XPath(""//a[contains(text(),'Login to the Demo')]"")));

[Note : for every click-able element am using ,this approach because click-able element may be hidden or not visible in web page]
But this approach is not working for  <select> <option>item<option> .. </select>
I am using below code clicking on one of the select option :
((IJavaScriptExecutor)driver).ExecuteScript(""arguments[0].click()"", driver.FindElement(By.XPath(""//select[@id='form_switcher']/option[5]"")));

but nothing is happening nor giving any error/exception.
--Edit start--
But if I use without ExecuteScript() then its work fine:
driver.FindElement(By.XPath(""//select[@id='form_switcher']/option[5]"")).Click();

--Edit end--
[Note : I am using click to select options so that it fire the change event.]
So can anyone please explain me how to click on the select option using ((IJavaScriptExecutor)driver).ExecuteScript


Thanks in advance.
",14k,"
            2
        ","['\nFor dropdowns you need to select and not click.  You should return the element and then perform a element.SelectedIndex = 5; \nIf you need to modify your javascript to get the element via javascript instead of selenium you can utilize the document.evaluate located https://developer.mozilla.org/en-US/docs/Web/API/document.evaluate?redirectlocale=en-US&redirectslug=DOM%2Fdocument.evaluate\nso then you return an element that represents your select element and then set the SelectedIndex value.\nI believe this is correct...\n((IJavaScriptExecutor)driver).ExecuteScript(""var element = document.evaluate(\\""//select[@id=\'form_switcher\']\\"", document.documentElement, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null); element.SelectedIndex = 5;  return element.fireEvent(\'event specifics go here\')"");\n\nhttp://www.java2s.com/Code/JavaScript/HTML/UsingthefireEventMethod.htm\n']",https://stackoverflow.com/questions/25290100/arguments0-click-not-working-for-select-option-in-selenium,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Powershell script strange behaviour when invoked from CMD/CLI,"
This scripts works fine when executed from Powershell console...
but does not work when executed with Powershell.exe from CMD.exe...
(powershell.exe -file script.ps1, using Powershell 5.1.17763.771)
# display Windows Shell Folder propertes
$App  = New-Object -ComObject Shell.Application;
$AppNS = $App.NameSpace( ""c:\windows"" );
$AppNS.Self.InvokeVerb( ""Properties"" );

I tested other GUI objects (Winforms & WPF)
and they work fine...
?any ideas...
",419,"
            2
        ","[""\nThe problem is that the in-process COM object you're creating goes out of scope when the calling process exits, which in your case, when called from cmd.exe via PowerShell's CLI, means that the window typically never even gets a chance to display or is automatically closed after a very brief appearance.\n\nIn an interactive PowerShell session, the process lives on after exiting the script - that's why your code works there.\nWhen you invoke a script via via PowerShell's CLI (powershell.exe for Windows PowerShell, pwsh for PowerShell Core, without the -NoExit switch to keep the process alive indefinitely), the PowerShell process exits when the script terminates.\n\n\nUse of -NoExit would be a stopgap at best, because it would keep the PowerShell process around indefinitely, even though you presumably want it to live only for as long as the Properties dialog window is open - whenever the user chooses to close it.\nTherefore, you need to synchronously wait for (a) the Properties dialog window to open and then (b) wait for it close before exiting the script.\nYou can do this with the help of the .NET UI Automation library as follows; note that the code uses PowerShell v5+ syntax:\nusing namespace System.Windows.Automation\n\n# Load the UI Automation client assemblies.\n# Requires Windows PowerShell or PowerShell Core v7+ (on Windows only).\nAdd-Type -AssemblyName UIAutomationClient; Add-Type -AssemblyName UIAutomationTypes\n\n# Initiate display of the Windows folder's Properties dialog.\n$App = New-Object -ComObject Shell.Application\n$AppNS = $App.NameSpace('c:\\windows')\n$AppNS.Self.InvokeVerb('Properties')\n\n# Comment out this line to suppress the verbose messages.\n$VerbosePreference = 'Continue'\n\nWrite-Verbose 'Wating for the window''s creation...'\ndo {\n  # Search among the current process' top-level windows for a winow\n  # with class name '#32770', which is what the Properties dialog windows\n  # use (don't know why, but it has been stable over time).\n  $w = [AutomationElement]::RootElement.FindFirst([TreeScope]::Children, \n    [AndCondition]::new(\n      [PropertyCondition]::new([AutomationElement]::ClassNameProperty, '#32770'),\n      [PropertyCondition]::new([AutomationElement]::ProcessIdProperty, $PID)\n    )\n  )\n  Start-Sleep -Milliseconds 100\n} while (-not $w)\n\nWrite-Verbose 'Window has appeared, waiting for it to close...'\n\nwhile ($w.Current.ProcessId) {\n  Start-Sleep -Milliseconds 100\n}\n\nWrite-Verbose 'Window is now closed, moving on.'\n\n# At this point, if the script was invoked via PowerShell's CLI (powershell.exe -file ...)\n# the PowerShell process terminates.\n\n\nNow, invoking your PowerShell script as follows from your batch file will pop up the Properties dialog and wait for it to close before continuing:\n@echo off\n\n::  # ... your batch file\n\n::  # Pop up the Properties dialog and *wait for it to close*.\npowershell.exe -file script.ps1\n\n::  # ...\n\nIf, by contrast, you simply want to launch the Properties dialog while continuing to run your batch file (be sure to disable the verbose messages first):\n:: # Only *initiate* display of the Properties dialog and *continue execution*.\nstart /B powershell.exe -file script.ps1\n\n"", '\nSeems like it has to wait for the graphics to finish.  ""get-childitem | out-gridview"" does a similar thing.  Or add ""sleep 120"" to the end of the script, or find some other way to wait.  Killing the script kills the window.\npowershell -noexit .\\explorer.ps1\n\n']",https://stackoverflow.com/questions/58293133/powershell-script-strange-behaviour-when-invoked-from-cmd-cli,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do I get access to a MessageBox through WPF Automation API?,"
How do I get access to MessageBox using the low level WPF Automation API?
I have searched all over but there seems to be very little documentation for this. I would rather not use White as I need more control than it gives.
Thanks
",2k,"
            2
        ","['\nLets suppose you have that simple WPF application:\nXaml:\n<Window x:Class=""WpfApplication1.Window1""\n        xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation""\n        xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml""\n        Title=""Window1"" Height=""300"" Width=""300"">\n    <Grid>\n        <Button Name=""Button1"" Content=""Click Me"" Click=""Button1_Click"" />\n    </Grid>\n</Window>\n\nCode:\npublic partial class Window1 : Window\n{\n    public Window1()\n    {\n        InitializeComponent();\n    }\n\n    private void Button1_Click(object sender, RoutedEventArgs e)\n    {\n        MessageBox.Show(this, ""hello"");\n    }\n}\n\nYou can automate this application with a console app sample like this (run this once you have started the first project):\nclass Program\n{\n    static void Main(string[] args)\n    {\n        // get the WPF app\'s process (must be named ""WpfApplication1"")\n        Process process = Process.GetProcessesByName(""WpfApplication1"")[0];\n\n        // get main window\n        AutomationElement mainWindow = AutomationElement.FromHandle(process.MainWindowHandle);\n\n        // get first button (WPF\'s ""Button1"")\n        AutomationElement button = mainWindow.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Button));\n\n        // click it\n        InvokePattern invoke = (InvokePattern)button.GetCurrentPattern(InvokePattern.Pattern);\n        invoke.Invoke();\n\n        // get the first dialog (in this case the message box that has been opened by the previous button invoke)\n        AutomationElement dlg = mainWindow.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.LocalizedControlTypeProperty, ""Dialog""));\n        AutomationElement dlgText = dlg.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Text));\n\n        Console.WriteLine(""Message Box text:"" + dlgText.Current.Name);\n\n        // get the dialog\'s first button (in this case, \'OK\')\n        AutomationElement dlgButton = dlg.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Button));\n\n        // click it\n        invoke = (InvokePattern)dlgButton.GetCurrentPattern(InvokePattern.Pattern);\n        invoke.Invoke();\n    }\n\n']",https://stackoverflow.com/questions/24480596/how-do-i-get-access-to-a-messagebox-through-wpf-automation-api,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
iOS/UI Automation: UIAActionSheet does not have possibilities to manipulate with buttons,"
My question is related to UI Automation template from XCode's Instruments tool. How does UI Automation support UIActionSheet testing? I know that there is a UIAActionSheet element and I was able to obtain it in my application. But I do not know how to get and manipulate with buttons from the action sheet. UI Automation does not provide any elements for these buttons. The UI Automation documentation does not have any info on the matter either. See the link below. It looks like this control does not use UIButton class for the buttons and renders them in some specific way. Could you give me some clue how to reach the buttons from UIAActionSheet? Thank you.
http://developer.apple.com/library/ios/#documentation/ToolsLanguages/Reference/UIAActionSheetClassReference/UIAActionSheet/UIAActionSheet.html#//apple_ref/doc/uid/TP40009895
",5k,"
            2
        ","['\nI have found a solution for the problem using direct tapping simulation. It is not an excellent solution but at least it works. I have asked the same question in the Apple Developer Forum but did not get any response. \nSo, the function below is my solution. I have tested it and it performs well. I will still continue to search for better approach.\nfunction tapActionSheetButton(target, actionSheet, buttonIndex)\n{\n    var headerHeight = 28;\n    var buttonHeight = 50;\n\n    if (buttonIndex >= 0) \n    {\n        var actionSheetRect = actionSheet.rect();\n        UIALogger.logMessage(""actionSheet:{rect:{origin:{x:"" + actionSheetRect.origin.x.toString() \n                             + "", y:"" + actionSheetRect.origin.y.toString()\n                             + ""}, size:{width:"" + actionSheetRect.size.width.toString()\n                             + "", height:"" + actionSheetRect.size.height.toString()\n                             + ""}}}"");\n        var xOffset = actionSheetRect.size.width / 2;\n        var yOffset = headerHeight + buttonIndex * buttonHeight + buttonHeight / 2;\n        if (yOffset < actionSheetRect.size.height) {\n            var tap_x = actionSheetRect.origin.x + xOffset;\n            var tap_y = actionSheetRect.origin.y + yOffset;\n            target.tap({x:tap_x, y:tap_y});\n        }\n    }\n    else\n    {\n        var message = ""Cannot tap button "" + buttonIndex.toString() + "". It does not exist"";\n        throw message;\n    }\n}\n\n', ""\nAs you can see there is method to return default button for each action sheet, which is Cancel button. As there is no other 'default' buttons for action sheet you need to implement those on your own.\nOne thing to notice woorth apple reference is that it skips methods for given class which are inherited from parent. UIAActionSheet (like most UIA elements) inherits all methods from UIAElement class. Check this reference. You should be able to get array of all buttons from actionsheet by calling\nvar arrButtons = objActionSheet.buttons();\n\nThen you can check by name property which one you need (again method from UIAElement name() ).  \nAlliteratively, if you check which element in array you are interested in you could call  that button directly (assuming, that you won't change ActionSheet) by using UIAElement method elements().\nFor example if you want to tap second button in ActionSheet tree, you just call\nUIATarget.localTarget().frontMostApp().actionSheet().elements()[1].tap();\n\nMaybe you can skip localTarget(), not sure, but code above should work.\n"", ""\nSince my action sheet was in a popover this returns a nil element:\nUIATarget.localTarget().frontMostApp().actionSheet();\n\nThis does return the action sheet:\nUIATarget.localTarget().frontMostApp().mainWindow().popover().actionSheet();\n\nBut it has nil elements() and buttons().\nSo I used MikhailV's function to tap the buttons.\nvar target = UIATarget.localTarget();\nvar actionSheet = UIATarget.localTarget().frontMostApp().mainWindow().popover().actionSheet();\nvar buttonIndex = 1;\n\ntapActionSheetButton(target, actionSheet, buttonIndex);\n\n"", '\nIf you like to keep it organized like me:\nfunction getTarget() {\n    return UIATarget.localTarget();\n}\n\nfunction getApp() {\n    return getTarget().frontMostApp();\n}\n\nfunction getWindow() {\n    return getApp().mainWindow();\n}\n\nfunction tapActionSheetButtonInIndex(index) {\n    var actionSheet = getApp().actionSheet() || getWindow().popover().actionSheet();\n    actionSheet.buttons()[index].tap(); \n}\n\n']",https://stackoverflow.com/questions/5250201/ios-ui-automation-uiaactionsheet-does-not-have-possibilities-to-manipulate-with,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Powershell commands to find and close Windows File Explorer dialogs,"
All windows file explorer windows show up in the Task manager in type of sub-heading all under explorer.exe, however, I would like to close individual file explorer windows by the name on the content of the window.  I am trying to close windows that show connectivity errors after connecting to a VPN such as the one below:

When I use the following command I get info about it but PS gives no indication as to the name of the window (e.g. ""Restoring Network Connections"").  How do I get and use Window info?

Get-Process -name explorer

I also tried the following command, however, it only shows 1 of the window objects when I have over 10 open that match the criteria ""*HFS*""

(New-Object -ComObject Shell.Application).Windows() | Where-Object{$_.LocationName -like ""*HFS*"" }


",275,"
            2
        ","['\nI got this working using FlaUI, a .NET wrapper for UIA2 and UIA3 automation API. Although the .NET framework already includes a UI automation API, it uses UIA2 only, which in my experiments appeared to be unreliable for automating Explorer. Only UIA3 seems to work reliably.\nFirst step is to get the FlaUI assemblies. I mostly followed the steps from this answer for using dotnet CLI to download the FlaUI assemblies from NuGet including all dependencies.\nA prerequisite for using the dotnet CLI is an installation of a .NET SDK. I choose .NET 6.0 x64, but it should work with .NET 4.x SDK as well.\nOnce you have the .NET SDK installed, open a PowerShell console in the directory where your PowerShell script is located and then run the following commands:\nSet-Location (New-Item -Type Directory assemblies)\n# Create a dummy project (""-f netstandard2.0"" to create a compatible .cs file)\ndotnet new classlib -f netstandard2.0\n# Target .NET 4.8 for compatibility with both PowerShell 5.1 and PowerShell (Core) 7+\n(Get-Content assemblies.csproj).Replace(\'netstandard2.0\', \'net48\') | Set-Content assemblies.csproj\n# Download the assemblies from NuGet\ndotnet add package FlaUI.UIA3 -v 3.2.0\n# Copy all assemblies including dependencies into Release directory\ndotnet publish -c Release\n\nYou should now have the following assemblies in the ""\\assemblies\\bin\\Release\\net48\\publish"" folder:\nFlaUI.Core.dll\nFlaUI.UIA3.dll\nInterop.UIAutomationClient.dll\n\nThese are the only files you need, you can move these wherever your script can reach them and delete the remaining files from the ""assemblies"" folder.\nActual code to automate Explorer:\n# Load the FlaUI assembly\nAdd-Type -Path $PSScriptRoot\\assemblies\\bin\\Release\\net48\\publish\\FlaUI.UIA3.dll\n\n# Create UIA3 automation instance\n$automation = [FlaUI.UIA3.UIA3Automation]::new()\n\n# Attach to running Explorer process\n$app = [FlaUI.Core.Application]::Attach( (Get-Process explorer).Id )\n\n# For each top-level window of Explorer\nforeach( $wnd in $app.GetAllTopLevelWindows( $automation ) ) {\n\n    # For each Explorer dialog window (message box)\n    # You may also conditionalize on the window title: $_.Name -eq \'Restoring Network Connections\'\n    foreach( $dlg in $wnd.FindAllChildren().Where{ $_.ClassName -eq \'#32770\' } ) {\n        ""----- Found explorer dialog: -----""\n        $dlg  # debug output\n\n        # Find text control that contains the text \'\\\\HFS\'\n        $child = $dlg.FindAllDescendants().Where{ $_.ControlType -eq \'Text\' -and $_.Name -like \'*\\\\HFS*\' }\n        if( $child ) {\n            ""----- Dialog has required child: -----""\n            $child  # debug output\n\n            ""`n>>> Closing dialog now <<<""\n            $dlg.Patterns.Window.Pattern.Close()\n        }\n    }\n}\n\nI have tested it with a ""path not found"" error message but it should work with your error message as well. Let me know how it works out for you.\nA very useful tool for exploring the UI automation hierarchy is Inspect.exe. If you see something in Inspect, you should be able to automate it.\n']",https://stackoverflow.com/questions/73534710/powershell-commands-to-find-and-close-windows-file-explorer-dialogs,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How get current url address on mains browsers using UIAutomation?,"
I have a source that promises to get the active url from any browser using UIAutomation, but I have difficulty about how to call the main function and show result in a ListBox for example. Then, how would it? 
Here is my code:
uses
UIAutomationClient_TLB, activeX;

var
Firefox_quebrou: boolean;

function GetURL(hTargetWnd: HWND): string;
  function Enumerar(pParent: IUIAutomationElement; Scope: TreeScope; pCondition: IUIAutomationCondition): String;
  var
    found    : IUIAutomationElementArray;
    ALen     : Integer;
    i        : Integer;
    iElement : IUIAutomationElement;

    retorno: integer;
    value : WideString;
    iInter: IInterface;
    ValPattern  : IUIAutomationValuePattern;
  begin
    Result := '';
    Firefox_quebrou := false;
    if pParent = nil then
      Exit;
    pParent.FindAll(Scope, pCondition, found);
    found.Get_Length(ALen);
    for i := 1 to ALen - 1 do
    begin
      found.GetElement(i, iElement);
      iElement.Get_CurrentControlType(retorno);
      if (
          (retorno = UIA_EditControlTypeId) or
          (retorno = UIA_GroupControlTypeId)
         ) then //UIA_DocumentControlTypeId
      begin
        iElement.GetCurrentPattern(UIA_ValuePatternId, iInter);
        if Assigned(iInter) then
        begin
          if iInter.QueryInterface(IID_IUIAutomationValuePattern, ValPattern) = S_OK then
          begin
            ValPattern.Get_CurrentValue(value);
            Result := trim(value);
            Firefox_quebrou := true;
            Break;
          end;
        end;
      end;
      if not Firefox_quebrou then
      begin
        Result := Enumerar(iElement, Scope, pCondition);
      end;
    end;

  end;
var
  UIAuto      : IUIAutomation;
  Ret         : Integer;
  RootElement : IUIAutomationElement;
  Scope       : TreeScope;
  varProp     : OleVariant;
  pCondition  : IUIAutomationCondition;
begin
  Result := '';
  try
    UIAuto := CoCUIAutomation.Create;
    if Succeeded(UIAuto.ElementFromHandle(hTargetWnd, RootElement)) then
    begin
      TVariantArg(varProp).vt    := VT_BOOL;
      TVariantArg(varProp).vbool := True;
      UIAuto.CreatePropertyCondition(UIA_IsControlElementPropertyId,
                                     varProp,
                                     pCondition);
      Scope := TreeScope_Element or TreeScope_Children;
      Result := Enumerar(RootElement, Scope, pCondition);
    end;
  except
    Result := '';
  end;
end;

",2k,"
            2
        ",[],https://stackoverflow.com/questions/25437652/how-get-current-url-address-on-mains-browsers-using-uiautomation,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read cell Items from data grid in SysListView32 of another application using C#,"
I am trying to read data grid items in SysListView32 of another process using C# .net ui-automation and winapi
C# code using ui-automation
http://pastebin.com/6x7rXMiW
C# code using winapi
http://pastebin.com/61RjXZuK
using this code you just have to place your Mouse pointer on SysListView32 on screen and press Enter.
now both code returns empty on the cell item which have following properties
pastebin.com/Rw9FGkYC

but both code works on following properties
pastebin.com/L51T4PLu

the only difference i noted that the name property contains the same data as in cell but problem occurs when name property is empty.
Is there any other way to read the cell ? or any changes I can make, Please elaborate.
",4k,"
            2
        ","['\nI would also suggest the Inspect tool.  If you see this:\nIsLegacyIAccessiblePatternAvailable:    true\n\nyou can use the LegacyIAccessiblePattern.  Other posts seem to indicate that it is not yet in the Client UI Automation Api, but it is in the core.  You can use the core in .NET by wrapping it.  I added this into my build to begin using it:\n""%PROGRAMFILES%\\Microsoft SDKs\\Windows\\v7.0A\\bin\\tlbimp.exe"" %windir%\\system32\\UIAutomationCore.dll /out:..\\interop.UIAutomationCore.dll""\n\nI can add more details if this pattern is indeed supported.\n\nWell, then you are probably good.\nHere is some sample code:\n    // C:\\Program Files\\Microsoft SDKs\\Windows\\v7.1\\Include\\UIAutomationClient.h\n    public const int UIA_LegacyIAccessibleNamePropertyId = 30092;\n    public const int UIA_LegacyIAccessibleValuePropertyId = 30093;\n    public const int UIA_IsTextPatternAvailablePropertyId = 30040;\n    public const int UIA_IsItemContainerPatternAvailablePropertyId = 30108;\n    public const int UIA_AutomationIdPropertyId = 30011;\n    public const int UIA_NamePropertyId = 30005;\n    public const int UIA_IsInvokePatternAvailablePropertyId = 30031;\n\n    public const int UIA_ItemContainerPatternId = 10019;\n    public const int UIA_TextPatternId = 10014;\n    public const int UIA_LegacyIAccessiblePatternId = 10018;\n    public const int UIA_ValuePatternId = 10002;\n    public const int UIA_InvokePatternId = 10000;\n\n    public const int UIA_ButtonControlTypeId = 50000;\n\n        uiAutomationCore = new UiAutomationCore();\n        cacheRequest = UiAuto.CreateCacheRequest();\n        cacheRequest.AddPattern(WindowsConstants.UIA_LegacyIAccessiblePatternId);\n        cacheRequest.AddProperty(WindowsConstants.UIA_LegacyIAccessibleNamePropertyId);\n\n       cacheRequest.AddProperty(WindowsConstants.UIA_LegacyIAccessibleValuePropertyId);\n        cacheRequest.TreeFilter = UiAuto.ContentViewCondition;\n        trueCondition = UiAuto.CreateTrueCondition();\n\n// A Pinvoke GetChildWindows call because it is \n// the fastest way to traverse down to a handle\nforeach (var child in GetChildWindows(someIUIAutomationElement.GetMainWindowHandle()))\n        {\n            var sb = new StringBuilder(100);\n            // get the name of each window & see if it is an ultragrid\n            // (get the name because the getchildwindows call only gets the handles\n            User32.GetClassName(child, sb, sb.Capacity);\n            var foundProperGrid = false;\n            if (Win32Utils.GetText(child) != ""UltraGrid1"")\n                continue;\n            // if this is an ultragrid, create a core automation object\n            var iuiae = UiCore.AutoElementFromHandle(child);\n\n            // get the children of the grid\n            var outerArayOfStuff =\n                iuiae.FindAllBuildCache(interop.UIAutomationCore.TreeScope.TreeScope_Children,\n                                        trueCondition,\n                                        cacheRequest.Clone());\n\n            var countOuter = outerArayOfStuff.Length;\n            // loop through the grid children \n            for (var counterOuter = 0; counterOuter < countOuter; counterOuter++)\n            {\n                // make a core automation object from each\n                var uiAutomationElement = outerArayOfStuff.GetElement(counterOuter);\n\n                // hacky - see if this grid has a GroupBy Box as first \'row\'\n                //       - if so, this is the proper grid\n                //       - ignore other grids\n                if (!foundProperGrid && uiAutomationElement.CurrentName.Equals(""GroupBy Box""))\n                {\n                    foundProperGrid = true;\n                }\n                else if (foundProperGrid)\n                {\n                    // \'cast\' the object to a core \'legacy msaa\' object\n                    IUIAutomationLegacyIAccessiblePattern outerLegacyPattern =\n                        uiAutomationElement.GetCachedPattern(WindowsConstants.UIA_LegacyIAccessiblePatternId);\n                    Log.Info(""OUTER, CachedName = "" + outerLegacyPattern.CachedName);\n\n                    try\n                    {\n                        // select the \'row\' to give visual feedback\n                        outerLegacyPattern.Select(3);\n                    }\n                    catch (Exception exc)\n                    {\n                        Log.Info(exc.Message);\n                    }\n\n                    // get the cells in a row\n                    var arrayOfStuff =\n                        uiAutomationElement.FindAllBuildCache(TreeScope.TreeScope_Children,\n                                                                trueCondition,\n                                                                cacheRequest.Clone());\n                    // loop over the cells in a row\n                    var count = arrayOfStuff.Length;\n                    for (var counter = 0; counter < count; counter++)\n                    {\n                        // get a cell\n                        var currIUIA = arrayOfStuff.GetElement(counter);\n\n                        // \'cast\' cell to a core \'legacy msaa\' object\n                        IUIAutomationLegacyIAccessiblePattern legacyPattern =\n                            currIUIA.GetCachedPattern(WindowsConstants.UIA_LegacyIAccessiblePatternId);\n\n                        // dump cell name & value for reference\n                        var name = legacyPattern.CachedName;\n                        Log.Info(counter + "") CachedName = "" + name);\n                        var value = legacyPattern.CachedValue;\n                        Log.Info(""CachedValue = "" + value);\n                        // check if cell name corresponds to what is being checked\n                        if (name.Equals(""Date""))\n                        {\n                            //if (!value.StartsWith(""5/23/2012""))\n                            if (!value.StartsWith(""5/25/2012""))\n                                errorList.AppendLine(""Bad Date = "" + value);\n                        }\n                        if (name.Equals(""XXX""))\n                        {\n                            if (!(value.Equals(""1"") || value.Equals(""2"")))\n                                errorList.AppendLine(""Bad XXX= "" + value);\n                        }\n                        if (name.Equals(""YYY""))\n                        {\n                            if (!value.Equals(""ZZZ""))\n                                errorList.AppendLine(""Bad YYY = "" + value);\n                        }\n                    }\n                }\n            }\n            foundProperGrid = false;\n        }\n        var stopTime = DateTime.Now;\n        var duration = stopTime - startTime;\n        Log.Info(""duration = "" + duration);\n\n        if (!"""".Equals(errorList.ToString()))\n        {\n            Log.Info(""errorList = "" + errorList);\n            Assert.Fail(""Test errors"");\n        }\n    }\n\n']",https://stackoverflow.com/questions/10799757/read-cell-items-from-data-grid-in-syslistview32-of-another-application-using-c-s,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get url from all open tabs in Google Chrome using VB .Net and UI Automation,"
Hello I have this code working to get current url on Chrome, but only get active tab url. I need to get url from all open tabs using UI Automation.
My working code:
Function GetChromeUrl(ByVal proc As Process) As String
    If proc.MainWindowHandle = IntPtr.Zero Then
    Return Nothing
End If

Dim element As System.Windows.Automation.AutomationElement = AutomationElement.FromHandle(proc.MainWindowHandle)
If element Is Nothing Then
    Return Nothing
End If

Dim edit As System.Windows.Automation.AutomationElement = element.FindFirst(TreeScope.Children, New PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Edit))
Return (edit.GetCurrentPattern(ValuePattern.Pattern)).Current.Value.ToString
End Function

and call it using this code in Form Load event:
For Each proc As Process In Process.GetProcessesByName(""chrome"")
    MsgBox(proc.MainWindowTitle + "" "" + GetChromeUrl(proc))
Next

",11k,"
            2
        ","['\nyou better try this way \nImports NDde.Client \'import the NDde library for firefox\nImports System.Runtime.InteropServices\n\n\'For Chrome\nPrivate Const WM_GETTEXTLENGTH As Integer = &He\nPrivate Const WM_GETTEXT As Integer = &Hd\n\n<DllImport(""user32.dll"")> _\nPrivate Shared Function SendMessage(hWnd As IntPtr, Msg As UInteger, wParam As Integer, lParam As Integer) As Integer\nEnd Function\n<DllImport(""user32.dll"")> _\nPrivate Shared Function SendMessage(hWnd As IntPtr, Msg As UInteger, wParam As Integer, lParam As StringBuilder) As Integer\nEnd Function\n<DllImport(""user32.dll"", SetLastError := True)> _\nPrivate Shared Function FindWindowEx(parentHandle As IntPtr, childAfter As IntPtr, className As String, windowTitle As String) As IntPtr\nEnd Function\n\nPublic Shared Function getChromeUrl(winHandle As IntPtr) As String\n    Dim browserUrl As String = Nothing\n    Dim urlHandle As IntPtr = FindWindowEx(winHandle, IntPtr.Zero, ""Chrome_AutocompleteEditView"", Nothing)\n    Const nChars As Integer = 256\n    Dim Buff As New StringBuilder(nChars)\n    Dim length As Integer = SendMessage(urlHandle, WM_GETTEXTLENGTH, 0, 0)\n    If length > 0 Then\n        SendMessage(urlHandle, WM_GETTEXT, nChars, Buff)\n        browserUrl = Buff.ToString()\n\n        Return browserUrl\n    Else\n        Return browserUrl\n    End If\n\nEnd Function\n\nPublic shared Function GetChromeHandle() As Intptr\n Dim ChromeHandle As IntPtr = Nothing\n Dim Allpro() As Process = Process.GetProcesses();\n For Each pro As Process in Allpro\n  if pro.ProcessName = ""chrome""\n  ChromeHandle = pro.MainWindowHandle\n  Exit For\n  End if\n Next     \nReturn ChromeHandle\nEnd Function\n\n\'USAGE FOR CHROME\n Dim CHandle As IntPtr = GetChromeHandle()\n If Not CHandle,Equals(Intptr.Zero)\n Dim url As String = getChromeUrl(CHandle)\n End If\n\nSource and read more \nEDIT :\ni found my own way and it worked for me \nDim appAs String = ""chrome""\nDim proc As System.Diagnostics.Process = GetBrowser(app)\n...\nPrivate Function GetBrowser(ByVal appName) As System.Diagnostics.Process\n    Dim pList() As System.Diagnostics.Process =  \n System.Diagnostics.Process.GetProcessesByName(app)\n    For Each proc As System.Diagnostics.Process In pList\n        If proc.ProcessName = appThen\n            Return proc\n        End If\n    Next\n    Return Nothing\nEnd Function\n\nusage :\nIf proc IsNot Nothing Then\n    Dim browserName as string = ""Google Chrome""\n    Dim className as String = ""Edit"" \n    Dim s As String = \nGetCurrentUrl(proc.MainWindowHandle, browserName, className, ComboBox1)\n    If s <> """" Then\n        Msgbox.show(s)\n        ComboBox1.SelectedIndex = 0 \'Window list\n    Else\n\n    End If\nElse\n    Label1.Text = browserName & "" is not available""\nend If\n\nhope it helps :))))\n']",https://stackoverflow.com/questions/16305238/get-url-from-all-open-tabs-in-google-chrome-using-vb-net-and-ui-automation,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is it possible to set the value of Style property of an element by nightwatch.js ? if yes then how?,"
I am working with nightwatch.js and i am quite new into this automation testing, i want to set the value into the style property of an element by nightwatch.js, so i am asking, is it possible ? if it is possible then how can we implement it.
I can access the style property values and can check by following nightwatch api command but i couldn't find any way to set the style's value to an element using nightwatch.js
browser.expect.element('#main').to.have.css('display').which.equals('block');

",3k,"
            2
        ","['\nYou can use the Nightwatch Selenium execute protocol to change the style property of an element. With the Selenium execute protocol you can execute arbitrary javascript on the site to be tested. \nFor example you can use it like this: \nbrowser\n.execute(""document.getElementById(\'main\').style.display = \'block\';"")\n.expect.element(\'#main\').to.have.css(\'display\').which.equals(\'block\');\n\n']",https://stackoverflow.com/questions/34648278/is-it-possible-to-set-the-value-of-style-property-of-an-element-by-nightwatch-js,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performing wildcard operations on table containing duplicate elements in karate? [duplicate],"






This question already has an answer here:
                        
                    



Karate UI: How to click a specific checkbox with same class name

                                (1 answer)
                            

Closed 1 year ago.



I am stuck with a case where the need is to click on an icon after asserting inputs from the user. In case there were some unique identifiers, the thing was pretty simple like the use of:  rightOf('{}UniqueIdentifier').find('i').click() served the purpose.
Also working fine with: scroll('{}UniqueIdentifier').parent.children[4].click()
But in case the table contains repeated values nothing could be found unique to search for and click. For which the thought was to match entire row text where the last element is that icon which needs to be clicked OR any other method which suits this?
Table looks like this:-

Need to click on triple dot icon for- A2,P2,2,resolved. How can this be achieved using wildcard locators? I tried creating a list of elements and match it with user input list but failed doing so.
Any help would be appreciated. Thanks!
",280,"
            1
        ","['\nFirst you should get comfortable with locateAll(). It will return an array of Element objects. And after that there are many possible ways of looping over and finding what you want.\nAlso note that there is a ""locateAll() with filter"": https://github.com/intuit/karate/tree/master/karate-core#locateall-with-filter\nSince you haven\'t provided any HTML I will have to guess. And note that x below is an Element and you can even call locate() on it.\n* def filter = function(x){ x.text.contains(\'Unique Identifier\') }\n* def list = locateAll(\'.grand-parent-class\', filter)\n* list[0].parent.children[4].click()\n\n']",https://stackoverflow.com/questions/66898591/performing-wildcard-operations-on-table-containing-duplicate-elements-in-karate,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Convert array of elements received as a result of LocateAll() into text in Karate?,"
I am trying to get the list of project names from a table. Where by using locateAll() method I am able to get the list of elements but when I try to convert them into text value the result is null.
* def ProjectNames = locateAll(""//div[@id='Projects']/@somePath"")
* print ProjectNames 

Above code displays
[DriverElement@aef32g2
DriverElement@ahf38g2
DriverElement@ayf12gj
DriverElement@ae032f2]

But expectation is to get result as below:
[Project1
Project2
Project3
Project4]

For which I tried - * print ProjectNames.text.trim() but this displays nothing and step is passed. Instead when I execute it for particular index value it displays the text for that * print ProjectNames[0].text.trim(). How can I do it for complete list received?
Thanks in advance!
",754,"
            1
        ","[""\nGiven the following HTML:\n  <body>\n    <div>first</div>\n    <div>second</div>\n  </body>  \n\nIf you have an array of anything, you can map over the array to transform it. Note that I'm using the new JS engine in Karate 1.0 :)\n* def temp = locateAll('div')\n* def vals1 = temp.map(x => x.text)\n* match vals1 == ['first', 'second']\n\nAnd a second way to do what you need is scriptAll(), refer the docs: https://github.com/intuit/karate/tree/master/karate-core#scriptall\n* def vals2 = scriptAll('div', '_.textContent')\n* match vals2 == ['first', 'second']\n\n""]",https://stackoverflow.com/questions/66639590/convert-array-of-elements-received-as-a-result-of-locateall-into-text-in-karat,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get playwrightUrl of Docker container for Playwright (needed to integrate Karate scripts)?,"
I am trying to execute the Karate script in mcr.microsoft.com/playwright:bionic Docker container.
I have exposed the port 5900 as shown below but not sure how to get the playwrightUrl for the container. Do I need to execute the node server.js inside it to get websocket endpoint?
docker run --name playwright -it --rm --ipc=host --cap-add=SYS_ADMIN -u root -p 5900:5900  -v $(pwd):/src -v /home/Automation/:/root/.m2 mcr.microsoft.com/playwright:bionic &

",583,"
            1
        ","[""\nSo, first of all, you need to actually provide a working example of what you're trying to achieve.\nWhat I understand from your question, is that you're trying to give the url for the tests from docker command in the terminal.\nSince Playwright v1.13.0, there is a baseURL option available. You can utilise that in this way probably\nIn your config.js file, you can have this\nimport { PlaywrightTestConfig } from '@playwright/test';\n\nconst config: PlaywrightTestConfig = {\n  use: {\n    baseURL: process.env.URL,\n  },\n};\nexport default config;\n\nIn your command for running docker, you can then pass as\ndocker run --name playwright -it --rm --ipc=host --cap-add=SYS_ADMIN -u root -p 5900:5900 -URL=whateverURLyouwant -v $(pwd):/src -v /home/Automation/:/root/.m2 mcr.microsoft.com/playwright:bionic &\n\nAlso, one point I would like to mention is that, you should not use root user for your executions. Using root as user in docker is not a recommended practice. Instead use the pwuser that is already created by the given image.\n""]",https://stackoverflow.com/questions/68978017/how-to-get-playwrighturl-of-docker-container-for-playwright-needed-to-integrate,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chrome Executable getting overriden while running the Karate test on a docker container,"
Below you can see that the Karate Driver is being configured as below.After that it is trying to use the user-data-dir and then the location.How do I disable the process using --user-data-dir for Chrome executable path:
build-env_1  | 12:10:42.702 [ForkJoinPool-1-worker-1] INFO  com.intuit.karate - Karate Driver config:
build-env_1  | {
build-env_1  |   ""type"": ""chrome"",
build-env_1  |   ""executable"": ""/usr/bin/karate_chrome_driver"",
build-env_1  |   ""port"": 9515,
build-env_1  |   ""httpConfig"": {
build-env_1  |     ""readTimeout"": 120000
build-env_1  |   }
build-env_1  | }
build-env_1  | 12:10:42.727 [ForkJoinPool-1-worker-1] WARN  com.intuit.karate - type was null, defaulting to 'chrome'
build-env_1  | 12:10:42.754 [ForkJoinPool-1-worker-1] DEBUG com.intuit.karate.shell.Command - found / verified free local port: 9222
build-env_1  | 12:10:42.759 [chrome_1603973442746] DEBUG c.i.k.driver.chrome_1603973442746 - command: [/usr/bin/google-chrome, --remote-debugging-port=9222, --no-first-run, --user-data-dir=/usr/regression/target/chrome_1603973442746, --disable-popup-blocking]
build-env_1  | 12:10:42.762 [ForkJoinPool-1-worker-1] DEBUG c.i.k.driver.chrome_1603973442746 - poll attempt #0 for port to be ready - localhost:9222
build-env_1  | 12:10:42.762 [chrome_1603973442746] ERROR com.intuit.karate.shell.Command - command error: [/usr/bin/google-chrome, --remote-debugging-port=9222, --no-first-run, --user-data-dir=/usr/regression/target/chrome_1603973442746, --disable-popup-blocking] - Cannot run program ""/usr/bin/google-chrome"" (in directory ""target/chrome_1603973442746""): error=2, No such file or directory.

",473,"
            1
        ",['\nSorry you seem to be trying to mix Karate chrome and expect it to use a chromedriver. This is not supported. The executable should be the chrome executable itself or the default (recommended).\nAlso read the docs. If using chrome you can add userDataDir: null to the config so it will be not used on the command-line.\nhttps://github.com/intuit/karate/tree/master/karate-core#configure-driver\n'],https://stackoverflow.com/questions/64591040/chrome-executable-getting-overriden-while-running-the-karate-test-on-a-docker-co,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c# Getting Chrome URL's from all tab,"
hi i want to get URL from browsers and for chrome i used these and the is not working getting null exception  i think chrome has changed something.. getting error on   elm4 == null.
using UIAutomation i searched more and all the example are not working ...
refrences:- https://stackoverflow.com/a/21799588/5096993
https://social.msdn.microsoft.com/Forums/vstudio/en-US/39bf60a8-2bdc-4aa0-96fb-08dca49cdb06/c-get-all-chrome-urls-opened?forum=csharpgeneral
else if (browser == BrowserType.Chrome)
            {
                //""Chrome_WidgetWin_1""

                Process[] procsChrome = Process.GetProcessesByName(""chrome"");
                foreach (Process chrome in procsChrome)
                {
                    // the chrome process must have a window
                    if (chrome.MainWindowHandle == IntPtr.Zero)
                    {
                        continue;
                    }
                    //AutomationElement elm = AutomationElement.RootElement.FindFirst(TreeScope.Children,
                    //         new PropertyCondition(AutomationElement.ClassNameProperty, ""Chrome_WidgetWin_1""));
                    // find the automation element
                    AutomationElement elm = AutomationElement.FromHandle(chrome.MainWindowHandle);

                    // manually walk through the tree, searching using TreeScope.Descendants is too slow (even if it's more reliable)
                    AutomationElement elmUrlBar = null;
                    try
                    {
                        // walking path found using inspect.exe (Windows SDK) for Chrome 29.0.1547.76 m (currently the latest stable)
                        var elm1 = elm.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.NameProperty, ""Google Chrome""));
                        var elm2 = TreeWalker.ControlViewWalker.GetLastChild(elm1); // I don't know a Condition for this for finding :(
                        var elm3 = elm2.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.NameProperty, """"));
                        var elm4 = elm3.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.ToolBar));
                        elmUrlBar = elm4.FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.NameProperty, ""Address and search bar""));
                    }
                    catch
                    {
                        // Chrome has probably changed something, and above walking needs to be modified. :(
                        // put an assertion here or something to make sure you don't miss it
                        continue;
                    }

                    // make sure it's valid
                    if (elmUrlBar == null)
                    {
                        // it's not..
                        continue;
                    }

                    // elmUrlBar is now the URL bar element. we have to make sure that it's out of keyboard focus if we want to get a valid URL
                    if ((bool)elmUrlBar.GetCurrentPropertyValue(AutomationElement.HasKeyboardFocusProperty))
                    {
                        continue;
                    }

                    // there might not be a valid pattern to use, so we have to make sure we have one
                    AutomationPattern[] patterns = elmUrlBar.GetSupportedPatterns();
                    if (patterns.Length == 1)
                    {
                        string ret = """";
                        try
                        {
                            ret = ((ValuePattern)elmUrlBar.GetCurrentPattern(patterns[0])).Current.Value;
                        }
                        catch { }
                        if (ret != """")
                        {
                            // must match a domain name (and possibly ""https://"" in front)
                            if (Regex.IsMatch(ret, @""^(https:\/\/)?[a-zA-Z0-9\-\.]+(\.[a-zA-Z]{2,4}).*$""))
                            {
                                // prepend http:// to the url, because Chrome hides it if it's not SSL
                                if (!ret.StartsWith(""http""))
                                {
                                    ret = ""http://"" + ret;
                                }
                                return ret;
                            }
                        }
                        continue;
                    }
                }

            }

",6k,"
            1
        ","['\nthis code is working for me and get URL of active tab of chrome\n Process[] procsChrome = Process.GetProcessesByName(""chrome"");\n            foreach (Process chrome in procsChrome)\n            {\n                // the chrome process must have a window\n                if (chrome.MainWindowHandle == IntPtr.Zero)\n                {\n                    continue;\n                }\n\n                // find the automation element\n                AutomationElement elm = AutomationElement.FromHandle(chrome.MainWindowHandle);\n                AutomationElement elmUrlBar = elm.FindFirst(TreeScope.Descendants,\n                  new PropertyCondition(AutomationElement.NameProperty, ""Address and search bar""));\n\n                // if it can be found, get the value from the URL bar\n                if (elmUrlBar != null)\n                {\n                    AutomationPattern[] patterns = elmUrlBar.GetSupportedPatterns();\n                    if (patterns.Length > 0)\n                    {\n                        ValuePattern val = (ValuePattern)elmUrlBar.GetCurrentPattern(patterns[0]);\n                        Console.WriteLine(""Chrome URL found: "" + val.Current.Value);\n                        listbox.Items.Add(val.Current.Value);\n                    }\n                }\n            }\n\n', '\nI solved the same problem recently with System.Windows.Forms.SendKeys\nCompared to your result - it\'s 4 times faster, but doesn\'t work with websites which use ctrl+l hotkey  (for example StackOwerflow in edit mode). \nDepends on your needs :)\n\n public void WithSendkeys()\n    {\n        AutomationElement.RootElement\n            .FindFirst(TreeScope.Children, new PropertyCondition(AutomationElement.ClassNameProperty, ""Chrome_WidgetWin_1""))\n            .SetFocus();\n        SendKeys.SendWait(""^l"");\n        var elmUrlBar = AutomationElement.FocusedElement;\n        var valuePattern = (ValuePattern) elmUrlBar.GetCurrentPattern(ValuePattern.Pattern);\n        Console.WriteLine(valuePattern.Current.Value);\n    }\n\n', '\nHere is the code:\nAutomationElement root = AutomationElement.FromHandle(proc.MainWindowHandle);\nCondition condNewTab = new PropertyCondition(AutomationElement.NameProperty, ""Novo separador"");\nAutomationElement elmNewTab = root.FindFirst(TreeScope.Descendants, condNewTab);\nTreeWalker treewalker = TreeWalker.ControlViewWalker;\n\n// IF THROWS A ERROR HERE, LOOK AT THE AutomationElement.NameProperty (""Novo separador"") - PUT THIS TEXT IN APROPRIATE LANGUAGE\nAutomationElement elmTabStrip = treewalker.GetParent(elmNewTab);\n                                                                            \nCondition condTabItem = new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.TabItem);\nforeach (AutomationElement tabitem in elmTabStrip.FindAll(TreeScope.Children, condTabItem))\n{\n    // HERE IS YOUR TABS!!!!\n    ret.Add(tabitem.Current.Name);\n}\n\n']",https://stackoverflow.com/questions/36516260/c-sharp-getting-chrome-urls-from-all-tab,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
opening URL from a file and taking screenshots in UiPath,"
It's a UI path sequence.
I am trying to read some URLs from an Excel file.
Launch browser
Go to URL from file
Take Screenshot
Save screenshot
I am unable to use the ""take Screenshot"" activity. Unsure, how can I take the screenshot and save it as a file.
",4k,"
            1
        ","['\nThe Take Screenshot activity requires a variable (of type Image). Select the activity first, and in the properties, click on Output. Then, hit CTRL+K to add a new variable (or enter an existing one).\nUse the Save Imageactivity to save any variable of type Imageto disk.\n\n']",https://stackoverflow.com/questions/51420426/opening-url-from-a-file-and-taking-screenshots-in-uipath,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"How to locate the x, y coordinates of text on the screen?","
I am trying to find the x, y coordinates of a web element (part of the web page that's open on screen) and some automated tests using the robotframework are being run on it.
I'd like to provide the function with the text string, and get (x, y) coordinates returned.
I am not sure if I can do this in pyautogui.
Environment: Chrome / OS X
EDIT:
I am wondering if I can use locateOnScreen() function in this library to locate text, (but it seems it's only for images according to the documentation)?
",6k,"
            1
        ","[""\nWeb elements has property .location which return dictionary of x and y coordinates of the element.\ndriver = webdriver.Chrome('/path/to/chromedriver')\nelem = driver.find_element_by_xpath('//xpath_to_the_element')\nloc = elem.location\nprint(loc)\n\nAbove code will return something like :\n{'x' : 123, 'y' : 234}\n\n""]",https://stackoverflow.com/questions/42859908/how-to-locate-the-x-y-coordinates-of-text-on-the-screen,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Getting error while generating test cases for espresso android,"
I am following these two links 1 and 2 for espresso test report but getting error while running ./gradlew createDebugCoverageReport. The error will be shown in image given below. Please help me, I am not able to generated report for espresso and ui automation test cases. Now trying to use jacoco but not able to find any solution. 
Build.Gradle
apply plugin: 'com.android.application'
android {
    compileSdkVersion 24
    buildToolsVersion '25.0.0'
    defaultConfig {
        applicationId ""com.example.project""
        minSdkVersion 18
        targetSdkVersion 24
        versionCode 27
        versionName ""1.5""
        testInstrumentationRunner ""android.support.test.runner.AndroidJUnitRunner""
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
            testCoverageEnabled true
        }
        debug{
            testCoverageEnabled true
        }
    }
}
dependencies {
    compile fileTree(dir: 'libs', include: ['*.jar'])
    androidTestCompile('com.android.support.test.espresso:espresso-core:2.2.2', {
        exclude group: 'com.android.support', module: 'support-annotations'
    })
    compile 'com.github.barteksc:android-pdf-viewer:2.0.3'
    compile 'com.google.code.gson:gson:2.8.0'
    compile 'com.android.support:appcompat-v7:24.2.1'
    compile 'com.android.support:design:24.2.1'
    compile 'com.android.support:recyclerview-v7:24.2.1'
    compile 'com.android.support:cardview-v7:24.2.1'
    compile 'com.android.volley:volley:1.0.0'
    compile 'com.android.support:support-v4:24.2.1'
    compile 'com.google.firebase:firebase-messaging:10.0.1'
    compile 'com.google.firebase:firebase-core:10.0.1'
    compile 'com.google.firebase:firebase-crash:10.0.1'
    testCompile 'junit:junit:4.12'
    compile 'com.android.support:support-annotations:24.2.0'
    androidTestCompile 'com.android.support.test:runner:0.2'
    androidTestCompile 'com.android.support.test:rules:0.2'
    androidTestCompile 'com.android.support.test.uiautomator:uiautomator-v18:2.1.0'
    androidTestCompile 'com.android.support:support-annotations:24.2.1'
    androidTestCompile('com.android.support.test.espresso:espresso-contrib:2.2') {
        exclude group: 'com.android.support', module: 'appcompat'
        exclude group: 'com.android.support', module: 'support-v4'
        exclude group: 'com.android.support', module: 'support-v7'
        exclude group: 'com.android.support', module: 'design'
        exclude module: 'support-annotations'
        exclude module: 'recyclerview-v7'



    }
    androidTestCompile 'com.android.support.test.espresso:espresso-core:2.2.1'
    androidTestCompile ""com.android.support.test.espresso:espresso-intents:2.2.2""
}

apply plugin: 'com.google.gms.google-services'
apply plugin: 'jacoco'

task jacocoTestReport(type: JacocoReport, dependsOn: ['testDebugUnitTest', 'createDebugCoverageReport']) {

    reports {
        xml.enabled = true
        html.enabled = true
    }

    def fileFilter = ['**/R.class', '**/R$*.class', '**/BuildConfig.*', '**/Manifest*.*', '**/*Test*.*', 'android/**/*.*']
    def debugTree = fileTree(dir: ""${buildDir}/intermediates/classes/debug"", excludes: fileFilter)
    def mainSrc = ""${project.projectDir}/src/main/java""

    sourceDirectories = files([mainSrc])
    classDirectories = files([debugTree])
    executionData = fileTree(dir: ""$buildDir"", includes: [
            ""jacoco/testDebugUnitTest.exec"",
            ""outputs/code-coverage/connected/*coverage.ec""
    ])
}

",697,"
            0
        ","[""\nA few things that got this running:\n\nMake sure that all the requisites are checked, per this other thread: https://stackoverflow.com/a/25525390/3195307\nMake sure there is a connected device ready to execute your tests. Emulator, physical device, or cloud device. It should show in 'Select Deployment Target' under 'connected devices' if you play your tests in IDE.\nRun Gradlew (project gradle wrapper) from the project directory like so: ./gradlew :app:createDebugCoverageReport\n\n""]",https://stackoverflow.com/questions/43094105/getting-error-while-generating-test-cases-for-espresso-android,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Why doesn't UI Automation condition find element by UIA_IsScrollPatternAvailablePropertyId?,"
I wanted to find the element within a main window handle that allows scrolling.  So instead of finding scrollbars and then the owner of the scrollbars I wanted to just return the items that allow scrolling via a ScrollPattern so I setup the condition on that but nothing is found. if I search for scrollbar owner window then get the ScrollPattern it works.  Why can't I just find the elements that have a scroll pattern available?
Here's the common code:
BOOL CUIAutomateScroller::FindWindow(HWND hwnd, IUIAutomationElement **windowelement)
{
  BOOL result=FALSE;
  // make sure init completed
  if (m_pClientUIA) {
    // get window element
    HRESULT hr=m_pClientUIA->ElementFromHandle(hwnd, windowelement);
    // check result
    result=SUCCEEDED(hr);
    // output debug info
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""ElementFromHandle error: %d\n""), hr);
    }
    else {
      _ASSERT(*windowelement!=NULL);
    }
  }
  return result;
}

BOOL CUIAutomateScroller::FindContainerWindowElement(const long controltype, IUIAutomationElement **pelement)
{
  // Create search condition
  VARIANT varprop;
  varprop.vt=VT_I4;
  varprop.uintVal=controltype;

  CComPtr<IUIAutomationCondition> pcondition;
  HRESULT hr=m_pClientUIA->CreatePropertyCondition(UIA_ControlTypePropertyId, varprop, &pcondition);
  if (FAILED(hr)) {
    CDebugPrint::DebugPrint(_T(""CreatePropertyCondition error: %d\n""), hr);
    return NULL;
  }

  // find the control based on condition
  CComPtr<IUIAutomationElementArray> pcontrolelementarr;
  hr=m_pWindowElement->FindAll(TreeScope_Subtree, pcondition, &pcontrolelementarr);
  if (FAILED(hr)) {
    CDebugPrint::DebugPrint(_T(""CreatePropertyCondition error: %d\n""), hr);
    return NULL;
  }

  // get number of controls found
  int numfound;
  pcontrolelementarr->get_Length(&numfound);
  CDebugPrint::DebugPrint(_T(""Controls Found: %d\n""), numfound);

  // process controls found, but really we exit earily if container window found
  for (int i=0; i < numfound; i++) {
    // get individual control element
    CComPtr<IUIAutomationElement> pcontrolelement;
    hr=pcontrolelementarr->GetElement(i, &pcontrolelement);
    if (FAILED(hr)) {
      // skip element unable to be retreived
      CDebugPrint::DebugPrint(_T(""GetElement error: %d\n""), hr);
      continue;
    }

    // output debug information
    CComBSTR name;
    hr=pcontrolelement->get_CurrentName(&name);
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""GetCurrentName error: %d\n""), hr);
    }
    CDebugPrint::DebugPrint(_T(""Control Name: %s\n""), name);
    name.Empty();

    hr=pcontrolelement->get_CurrentClassName(&name);
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""GetCurrentClass error: %d\n""), hr);
    }
    CDebugPrint::DebugPrint(_T(""Class Name: %s\n""), name);
    name.Empty();

    CComPtr<IUIAutomationTreeWalker> pcontentwalker=NULL;
    hr=m_pClientUIA->get_ContentViewWalker(&pcontentwalker);
    if (pcontentwalker == NULL) {
      return NULL;
    }

    // Get ancestor element nearest to the scrollbar UI Automation element in the tree view
    hr=pcontentwalker->NormalizeElement(pcontrolelement, pelement);
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""NormalizeElement error: %d\n""), hr);
      return NULL;
    }

    // output debug information
    hr=(*pelement)->get_CurrentName(&name);
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""get_CurrentName error: %d\n""), hr);
    }
    CDebugPrint::DebugPrint(_T(""Ancestor Name: %s\n""), name);
    name.Empty();

    return TRUE;
  }

  return FALSE;
}

This does NOT work (It doesn't find anything):
  // get main window
  if (FindWindow(hwnd, &m_pWindowElement)) {
    HRESULT hr;
    VARIANT varprop;
      
    // create condition for elements that have UIA_IsScrollPatternAvailablePropertyId available
    CComPtr<IUIAutomationCondition> pscrollpatterncondition;
    varprop.vt=VT_BOOL;
    varprop.boolVal=TRUE;
    hr=m_pClientUIA->CreatePropertyCondition(UIA_IsScrollPatternAvailablePropertyId, varprop, &pscrollpatterncondition);
    // check result
    if (FAILED(hr)) {
      CDebugPrint::DebugPrint(_T(""CreatePropertyCondition for ScrollPattern Error: %d\n""), hr);
    }
    else {
      // find the matching element
      CComPtr<IUIAutomationElementArray> pscrollpatternarr;
      hr=m_pWindowElement->FindAll(TreeScope_Subtree, pscrollpatterncondition, &pscrollpatternarr);
      // check result (normal is success with empty array if not found)
      if (FAILED(hr)) {
        CDebugPrint::DebugPrint(_T(""FindAll Error: %d\n""), hr);
      }
      else {
        // get number of elements in array
        int numfound=0;
        pscrollpatternarr->get_Length(&numfound);
        // make sure we only get one scrollable area - in the future we could figure out the rect
        // **numfound is 0**

This DOES work:
  // get main window
  if (FindWindow(hwnd, &m_pWindowElement)) {
    // get scrollable window element based on scrollbar
    if (FindContainerWindowElement(UIA_ScrollBarControlTypeId, &m_pScrollableElement)) {
      HRESULT hr;
      // get the scroll pattern
      hr=m_pScrollableElement->GetCurrentPattern(UIA_ScrollPatternId, (IUnknown**) &m_pScrollPattern);
      if (FAILED(hr)) {
        CDebugPrint::DebugPrint(_T(""GetCurrentPattern for Scroll Pattern Error %d:\n""), hr);
      }
      else if (m_pScrollPattern!=NULL) {
        // **we're good!!**

",629,"
            0
        ","[""\n\nWhy can't I just find the elements that have a scroll pattern\navailable?\n\nAs @HansPassant pointed out, use VARIANT_TRUE (-1) instead of TRUE (1).\nAfter correct above error the following code work for me for finding the elements that have a scroll pattern (IUIAutomationScrollPattern) available and scrolling (vertical scrollbar).\n    VARIANT varprop;\n\n    // create condition for elements that have UIA_IsScrollPatternAvailablePropertyId available\n    CComPtr<IUIAutomationCondition> pscrollpatterncondition;\n    varprop.vt = VT_BOOL;\n    varprop.boolVal = VARIANT_TRUE;\n    hr = m_pClientUIA->CreatePropertyCondition(UIA_IsScrollPatternAvailablePropertyId, varprop, &pscrollpatterncondition);\n    // check result\n    if (FAILED(hr)) {\n    }\n    else {\n        // find the matching element\n        CComPtr<IUIAutomationElementArray> pscrollpatternarr;\n        hr = m_pWindowElement->FindAll(TreeScope_Subtree, pscrollpatterncondition, &pscrollpatternarr);\n        // check result (normal is success with empty array if not found)\n        if (FAILED(hr)) {\n        }\n        else {\n            // get number of elements in array\n            numfound = 0;\n            pscrollpatternarr->get_Length(&numfound);\n            for (int i = 0; i < numfound; i++)\n            {\n                IUIAutomationElement *element = NULL;\n                pscrollpatternarr->GetElement(i, &element);\n\n                IUIAutomationScrollPattern  *m_pScrollPattern = NULL;\n                hr = element->GetCurrentPattern(UIA_ScrollPatternId, (IUnknown**)&m_pScrollPattern);\n                if (FAILED(hr)) {\n                }\n                else if (m_pScrollPattern != NULL) {\n                    // Scroll vertical scrollbar\n                    m_pScrollPattern->Scroll(ScrollAmount_NoAmount, ScrollAmount_LargeIncrement);\n                }\n            }\n        }\n    }\n\n""]",https://stackoverflow.com/questions/63063225/why-doesnt-ui-automation-condition-find-element-by-uia-isscrollpatternavailable,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XCUITest Class teardown isnt deleting the app. But works if its instance teardown. What am I doing wrong?,"
I have a class teardown which is trying to remove the app, but it doesn't recognize app.terminate().
class DeviceSettingsUtilities : UITestUtilities {
func removeApp(productName:String){
        print(""in teardown"")
        let springboard = XCUIApplication(bundleIdentifier: ""com.apple.springboard"")
        XCUIApplication().terminate() // this does nothing
        XCUIApplication(bundleIdentifier: ""com.xxx.xxxx"").terminate()//this does nothing too, but this works when called as an instance teardown
        sleep(5)
        springboard.activate()
        let icon = springboard.icons.matching(identifier: productName).firstMatch
// icon.exists is false when called as a class teardown
// icon.exists is true when called as an instance teardown
        if icon.exists {
            let iconFrame = icon.frame
            let springboardFrame = springboard.frame
            icon.press(forDuration:1.3)
            springboard.coordinate(withNormalizedOffset: CGVector(dx: ((iconFrame.minX + 3) / springboardFrame.maxX), dy:((iconFrame.minY + 3) / springboardFrame.maxY))).tap()
            sleep(5)
            springboard.buttons[""Delete""].firstMatch.tap()
            sleep(5)
        }
        XCUIApplication().terminate()
    }

}
This is being called in the test case class teardown method as shown below
override class func tearDown() {
    super.tearDown()
    let deviceSettings = DeviceSettingsUtilities()
    deviceSettings.removeApp(productName: ProductName.rawValue)
}

This just doesnt delete the app, But if i change class func tearDown() to func tearDown() , it deletes the app with no problem. Not sure what i am missing. Any suggestions ?
",4k,"
            0
        ","[""\nThis seems like a bug in latest Xcode 10. \nXCUIApplication.terminate() doesn't seem to work in tearDown() when declared as class. \nThis can be solved in two ways: \nThe first option is to use:\noverride func tearDown() {\n    XCUIApplication().terminate()\n    super.tearDown()\n}\n\ninstead of: \noverride class func tearDown() {鈥 \n\nOr, terminate the app differently (press home button, open different app...). However, I would use the first way. \nAlso consider reporting this to Apple, so they can fix it. \nEdit: This has nothing to do with app state (XCUIApplication().state.rawValue), since it is same in test and in tearDown() (4 = running foreground). Also - official documentation says that .terminate() will terminate app, which has a debug session with Xcode, but the debug session is active in tearDown() as well. So it is really probably a bug in Xcode. \n"", ""\nThe app is not being reset when you put the code in the class tearDown method because that method only runs once all the tests in the class are complete. The instance tearDown is the best place to put code that you want to run after every test.\nFrom Apple's documentation:\n\nFor each class, testing starts by running the class setup method. For each test method, a new instance of the class is allocated and its instance setup method executed. After that it runs the test method, and after that the instance teardown method. This sequence repeats for all the test methods in the class. After the last test method teardown in the class has been run, Xcode executes the class teardown method and moves on to the next class. This sequence repeats until all the test methods in all test classes have been run.\n\n"", '\nI\'m using the below workaround to terminate the app after last test case of a class.\nclass BaseClass: XCTestCase {\n\n        static var LastTestCaseName: String = """"\n\n        override class func setUp() {\n            LastTestCaseName = defaultTestSuite.tests.last!.name\n            super.setUp()\n        }\n\n        override func tearDown() {\n            let app = XCUIApplication()\n            if BaseClass.LastTestCaseName == testRun?.test.name {\n                app.terminate()\n            }\n        }\n    }\n\n']",https://stackoverflow.com/questions/53181823/xcuitest-class-teardown-isnt-deleting-the-app-but-works-if-its-instance-teardow,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Execute Coded UI Tests in multiple environments,"
Right now my Coded UI Tests use their app.config to determine the domain they execute in, which has a 1-1 relationship with environment. To simplify it:

www.test.com
www.UAT.com
www.prod.com

and in App.config I have something like:
<configuration>
    <appSettings>
        <add key=""EnvironmentURLMod"" value =""test""/>

and to run the test in a different environment, I manually change the value between runs. For instance the I open the browser like this:
browserWindow.NavigateToUrl(new Uri(""http://www.""
                + ConfigurationManager.AppSettings.Get(""EnvironmentURLMod"")
                + "".com""));

Clearly this is inelegant. I suppose I had a vision where we'd drop in a new app.config for each run, but as a spoiler this test will be run in ~10 environments, not 3, and which environments it may run may change.
I know I could decouple these environment URL modifications to yet another XML file, and make the tests access them sequentially in a data-driven scenario. But even this seems like it's not quite what I need, since if one environment fails then the whole test collapses. I've seen Environment Variables as a suggestion, but this would require creating a test agent for each environment, modifying their registries, and running the tests on each of them. If that's what it takes then sure, but it seems like an enormous amount of VM bandwidth to be used for what's a collection of strings.
In an ideal world, I would like to tie these URL mods to something like Test Settings, MTM environments, or builds. I want to execute the suite of tests for each domain and report separately.
In short, what's the best way to parameterize these tests? Is there a way that doesn't involve queuing new builds, or dropping config files? Is Data Driven Testing the answer? Have I structured my solution incorrectly? This seems like it should be such a common scenario, yet my googling doesn't quite get me there.
Any and all help appreciated.
",733,"
            0
        ","['\nThe answer here is data driven testing, and unfortunately there\'s no total silver bullet even if there\'s a ""Better than most"" option.\nUsing any data source lets you iterate through a test in multiple environments (or any other variable you can think of) and essentially return 3 different test results - one for each permutation or data row. However you\'ll have to update your assertions to show which environment you\'re currently executing in, as the test results only show ""Data Row 0"" or something similar by default. If the test passes, you\'ll get no clue as to what\'s actually in the data row for the successful run, unless you embed this information in the action log! I\'m lucky that my use case does this automatically since I\'m just using a URL mod, but other people may need to do that on their own.\nTo allow on-the-fly changing of what environments we\'re testing in, we chose to use a TestCase data source. This has a lot of flexibility - potentially more than using a database or XML for instance - but it comes with its own downsides. Like all data driven scenarios, you have to essentially ""Hard Code"" the test case ID into the decorator above your test method (Because it\'s considered a property). I was hoping we could drop an app.config into the build drop location when we wanted to change which test case we used, at least, but it looks like instead we\'re going to have to do a find + replace across a solution instead. \nIf anyone knows of a better way to decouple the test ID or any other part of the connection string from the code, I\'ll give you an answer here. For anyone else, you can find more information on MSDN.\n']",https://stackoverflow.com/questions/33239523/execute-coded-ui-tests-in-multiple-environments,ui-automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
headless internet browser? [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



I would like to do the following. Log into a website, click a couple of specific links, then click a download link. I'd like to run this as either a scheduled task on windows or cron job on Linux. I'm not picky about the language I use, but I'd like this to run with out putting a browser window up on the screen if possible.
",70k,"
            71
        ","['\nHere are a list of headless browsers that I know about:\n\nHtmlUnit - Java. Custom browser engine. Limited JavaScript support/DOM emulated. Open source.\nGhost - Python only. WebKit-based. Full JavaScript support. Open source.\nTwill - Python/command line. Custom browser engine. No JavaScript. Open source.\nPhantomJS - Command line/all platforms. WebKit-based. Full JavaScript support. Open source.\nAwesomium - C++/.NET/all platforms. Chromium-based. Full JavaScript support. Commercial/free.\nSimpleBrowser - .NET 4/C#. Custom browser engine. No JavaScript support. Open source.\nZombieJS - Node.js. Custom browser engine. JavaScript support/emulated DOM. Open source. Based on jsdom.\nEnvJS - JavaScript via Java/Rhino. Custom browser engine. JavaScript support/emulated DOM. Open source.\nWatir-webdriver with headless gem - Ruby via WebDriver.  Full JS Support via Browsers (Firefox/Chrome/Safari/IE). \nSpynner - Python only.  PyQT and WebKit. \njsdom - Node.js. Custom browser engine. Supports JS via emulated DOM. Open source.\nTrifleJS - port of PhantomJS using MSIE (Trident) and V8. Open source.\nui4j - Pure Java 8 solution. A wrapper library around the JavaFx WebKit Engine incl. headless modes.\nChromium Embedded Framework - Full up-to-date embedded version of Chromium with off-screen rendering as needed. C/C++, with .NET wrappers (and other languages). As it is Chromium, it has support for everything. BSD licensed.\nSelenium WebDriver - Full support for JavaScript via browsers (Firefox, IE, Chrome, Safari, Opera). Officially supported bindings are C#, Java, JavaScript, Haskell, Perl, Ruby, PHP, Python, Objective-C, and R. Unofficial bindings are available for Qt and Go. Open source.\n\nHeadless browsers that have JavaScript support via an emulated DOM generally have issues with some sites that use more advanced/obscure browser features, or have functionality that has visual dependencies (e.g. via CSS positions and so forth), so whilst the pure JavaScript support in these browsers is generally complete, the actual supported browser functionality should be considered as partial only.\n(Note: Original version of this post only mentioned HtmlUnit, hence the comments. If you know of other headless browser implementations and have edit rights, feel free to edit this post and add them.)\n', ""\nCheck out twill, a very convenient scripting language for precisely what you're looking for. From the examples:\nsetlocal username <your username>\nsetlocal password <your password>\n\ngo http://www.slashdot.org/\nformvalue 1 unickname $username\nformvalue 1 upasswd $password\nsubmit\n\ncode 200     # make sure form submission is correct!\n\nThere's also a Python API if you're looking for more flexibility.\n"", '\nHave a look at PhantomJS, a JavaScript based automation framework available for Windows, Mac OS X, Linux, other *ix systems.\nUsing PhantomJS, you can do things like this:\nconsole.log(\'Loading a web page\');\n\nvar page = new WebPage();\nvar url = ""http://www.phantomjs.org/"";\n\npage.open(url, function (status) {\n    // perform your task once the page is ready ...\n    phantom.exit();\n});\n\nOr evaluate a page\'s title:\nvar page = require(\'webpage\').create();\npage.open(url, function (status) {\n    var title = page.evaluate(function () {\n        return document.title;\n    });\n    console.log(\'Page title is \' + title);\n});\n\nExamples from PhantomJS\' Quickstart page. You can even render a page to a PNG, JPEG or PDF using the render() method.\n', '\nI once did that using the Internet Explorer ActiveX control (WebBrowser, MSHTML). You can instantiate it without making it visible.\nThis can be done with any language which supports COM (Delphi, VB6, VB.net, C#, C++, ...)\nOf course this is a quick-and-dirty solution and might not be appropriate in your situation.\n', '\nPhantomJS is a headless WebKit-based browser that you can script with JavaScript.\n', '\nExcept for the auto-download of the file (as that is a dialog box) a win form with the embedded webcontrol will do this.\nYou could look at Watin and Watin Recorder. They may help with C# code that can login to your website, navigate to a URL and possibly even help automate the file download.\nYMMV though.\n', ""\nIf the links are known (e.g, you don't have to search the page for them), then you can probably use wget. I believe that it will do the state management across multiple fetches.\nIf you are a little more enterprising, then I would delve into the new goodies in Python 3.0. They redid the interface to their HTTP stack and, IMHO, have a very nice interface that is susceptible to this type of scripting.\n"", '\nNode.js with YUI on the server. Check out this video: http://www.yuiblog.com/blog/2010/09/29/video-glass-node/\nThe guy in this video Dav Glass shows an example of how he uses node to fetch a page from Digg. He then attached YUI to the DOM he grabbed and can completely manipulate it.\n', '\nIf you use PHP - try http://mink.behat.org/\n', '\nYou can use Watir with Ruby or Watin with mono.\n', ""\nAlso you can use Live Http Headers (Firefox extension) to record headers which are sent to site (Login -> Links -> Download Link) and then replicate them with php using fsockopen. Only thing which you'll probably need to variate is the cookie's value which you receive from login page. \n"", '\nlibCURL could be used to create something like this.\n', '\nCan you not just use a download manager?\nThere\'s better ones, but FlashGet has browser-integration, and supports authentication. You can login, click a bunch of links and queue them up and schedule the download.\nYou could write something that, say, acts as a proxy which catches specific links and queues them for later download, or a Javascript bookmarklet that modifies links to go to ""http://localhost:1234/download_queuer?url="" + $link.href and have that queue the downloads - but you\'d be reinventing the download-manager-wheel, and with authentication it can be more complicated..\nOr, if you want the ""login, click links"" bit to be automated also - look into screen-scraping.. Basically you load the page via a HTTP library, find the download links and download them..\nSlightly simplified example, using Python:\nimport urllib\nfrom BeautifulSoup import BeautifulSoup\nsrc = urllib.urlopen(""http://%s:%s@example.com"" % (""username"", ""password""))\nsoup = BeautifulSoup(src)\n\nfor link_tag in soup.findAll(""a""):\n    link = link_tag[""href""]\n    filename = link.split(""/"")[-1] # get everything after last /\n    urllib.urlretrieve(link, filename)\n\nThat would download every link on example.com, after authenticating with the username/password of ""username"" and ""password"". You could, of course, find more specific links using BeautifulSoup\'s HTML selector\'s (for example, you could find all links with the class ""download"", or URL\'s that start with http://cdn.example.com).\nYou could do the same in pretty much any language..\n', ""\n.NET contains System.Windows.Forms.WebBrowser.  You can create an instance of this, send it to a URL, and then easily parse the html on that page.  You could then follow any links you found, etc.  \nI have worked with this object only minimally, so I'm no expert, but if you're already familiar with .NET then it would probably be worth looking into.\n""]",https://stackoverflow.com/questions/814757/headless-internet-browser,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I login to a website with Python?,"
How can I do it? 
I was trying to enter some specified link (with urllib), but to do it, I need to log in.
I have this source from the site:
<form id=""login-form"" action=""auth/login"" method=""post"">
    <div>
    <!--label for=""rememberme"">Remember me</label><input type=""checkbox"" class=""remember"" checked=""checked"" name=""remember me"" /-->
    <label for=""email"" id=""email-label"" class=""no-js"">Email</label>
    <input id=""email-email"" type=""text"" name=""handle"" value="""" autocomplete=""off"" />
    <label for=""combination"" id=""combo-label"" class=""no-js"">Combination</label>
    <input id=""password-clear"" type=""text"" value=""Combination"" autocomplete=""off"" />
    <input id=""password-password"" type=""password"" name=""password"" value="""" autocomplete=""off"" />
    <input id=""sumbitLogin"" class=""signin"" type=""submit"" value=""Sign In"" />

Is this possible?
",374k,"
            105
        ","['\nMaybe you want to use twill. It\'s quite easy to use and should be able to do what you want.\nIt will look like the following:\nfrom twill.commands import *\ngo(\'http://example.org\')\n\nfv(""1"", ""email-email"", ""blabla.com"")\nfv(""1"", ""password-clear"", ""testpass"")\n\nsubmit(\'0\')\n\nYou can use showforms() to list all forms once you used go鈥?to browse to the site you want to login. Just try it from the python interpreter.\n', '\nLet me try to make it simple, suppose URL of the site is www.example.com and you need to sign up by filling username and password, so we go to the login page say http://www.example.com/login.php now and view it\'s source code and search for the action URL it will be in form tag something like \n <form name=""loginform"" method=""post"" action=""userinfo.php"">\n\nnow take userinfo.php to make absolute URL which will be \'http://example.com/userinfo.php\', now run a simple python script \nimport requests\nurl = \'http://example.com/userinfo.php\'\nvalues = {\'username\': \'user\',\n          \'password\': \'pass\'}\n\nr = requests.post(url, data=values)\nprint r.content\n\nI Hope that this helps someone somewhere someday.\n', '\nTypically you\'ll need cookies to log into a site, which means cookielib, urllib and urllib2. Here\'s a class which I wrote back when I was playing Facebook web games:\nimport cookielib\nimport urllib\nimport urllib2\n\n# set these to whatever your fb account is\nfb_username = ""your@facebook.login""\nfb_password = ""secretpassword""\n\nclass WebGamePlayer(object):\n\n    def __init__(self, login, password):\n        """""" Start up... """"""\n        self.login = login\n        self.password = password\n\n        self.cj = cookielib.CookieJar()\n        self.opener = urllib2.build_opener(\n            urllib2.HTTPRedirectHandler(),\n            urllib2.HTTPHandler(debuglevel=0),\n            urllib2.HTTPSHandler(debuglevel=0),\n            urllib2.HTTPCookieProcessor(self.cj)\n        )\n        self.opener.addheaders = [\n            (\'User-agent\', (\'Mozilla/4.0 (compatible; MSIE 6.0; \'\n                           \'Windows NT 5.2; .NET CLR 1.1.4322)\'))\n        ]\n\n        # need this twice - once to set cookies, once to log in...\n        self.loginToFacebook()\n        self.loginToFacebook()\n\n    def loginToFacebook(self):\n        """"""\n        Handle login. This should populate our cookie jar.\n        """"""\n        login_data = urllib.urlencode({\n            \'email\' : self.login,\n            \'pass\' : self.password,\n        })\n        response = self.opener.open(""https://login.facebook.com/login.php"", login_data)\n        return \'\'.join(response.readlines())\n\nYou won\'t necessarily need the HTTPS or Redirect handlers, but they don\'t hurt, and it makes the opener much more robust. You also might not need cookies, but it\'s hard to tell just from the form that you\'ve posted. I suspect that you might, purely from the \'Remember me\' input that\'s been commented out.\n', '\nWeb page automation ? Definitely ""webbot""\nwebbot even works web pages which have dynamically changing id and classnames and has more methods and features than selenium or mechanize.\n\nHere\'s a snippet :)\n\nfrom webbot import Browser \nweb = Browser()\nweb.go_to(\'google.com\') \nweb.click(\'Sign in\')\nweb.type(\'mymail@gmail.com\' , into=\'Email\')\nweb.click(\'NEXT\' , tag=\'span\')\nweb.type(\'mypassword\' , into=\'Password\' , id=\'passwordFieldId\') # specific selection\nweb.click(\'NEXT\' , tag=\'span\') # you are logged in ^_^\n\nThe docs are also pretty straight forward and simple to  use : https://webbot.readthedocs.io\n', ""\nimport cookielib\nimport urllib\nimport urllib2\n\nurl = 'http://www.someserver.com/auth/login'\nvalues = {'email-email' : 'john@example.com',\n          'password-clear' : 'Combination',\n          'password-password' : 'mypassword' }\n\ndata = urllib.urlencode(values)\ncookies = cookielib.CookieJar()\n\nopener = urllib2.build_opener(\n    urllib2.HTTPRedirectHandler(),\n    urllib2.HTTPHandler(debuglevel=0),\n    urllib2.HTTPSHandler(debuglevel=0),\n    urllib2.HTTPCookieProcessor(cookies))\n\nresponse = opener.open(url, data)\nthe_page = response.read()\nhttp_headers = response.info()\n# The login cookies should be contained in the cookies variable\n\nFor more information visit: https://docs.python.org/2/library/urllib2.html\n"", '\nWebsites in general can check authorization in many different ways, but the one you\'re targeting seems to make it reasonably easy for you.\nAll you need is to POST to the auth/login URL a form-encoded blob with the various fields you see there (forget the labels for, they\'re decoration for human visitors).  handle=whatever&password-clear=pwd and so on, as long as you know the values for the handle (AKA email) and password you should be fine.\nPresumably that POST will redirect you to some ""you\'ve successfully logged in"" page with a Set-Cookie header validating your session (be sure to save that cookie and send it back on further interaction along the session!).\n', '\nFor HTTP things, the current choice should be: Requests- HTTP for Humans\n']",https://stackoverflow.com/questions/2910221/how-can-i-login-to-a-website-with-python,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Textbox events?,"
I am using Kantu to automate filling out some forms. There is a textbox that when a persons id number is entered and you click into another box or tab out of the textbox it will load that persons vcard. I can try to expound if you need more clarity. 
I don't know much but i'm guessing me clicking into another box is activiating some kind of event to load this vcard. I can't seem to simulate this. Does anyone know of a way to do so?
",345,"
            1
        ","[""\nWelcome to SO. There are different option to get the associated events.\nLet's take the stackoverflow search box (the one which is on the top with s-input js-search-field class)\n1) Using getEventListeners\nGo to chrome console in the dev tools and then use getEventListeners(element). \n\ngetEventListeners(document.querySelector('.s-input.js-search-field '))\n\n\n2) Using Dev Tools Event Listner\nGo to chrome dev tools and select the element for which you want to know the events, click on the Event Listeners tab on the right hand side pane.\n   \n3) Using the firefox event \nGoto dev tools in firefox and click on events bubble at the end of html element tag. \n    \n"", '\nSample of code\nI consulted with a friend who showed me the problem was an onblur event triggered by clicking away. \nThe solution to my problem was to call the event using the following line, \n{\n  ""Command"": ""storeEval"",\n  ""Target"": ""lawformTextBlur(document.getElementById(\'_f4\'))"",\n  ""Value"": """"\n},\n\n']",https://stackoverflow.com/questions/55977388/textbox-events,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
element not interactable exception in selenium web automation,"
In the below code i cannot send password keys in the password field, i tried clicking the field, clearing the field and sending the keys. But now working in any of the method. But its working if i debug and test
  public class TestMail {
   protected static WebDriver driver;

   protected static String result;

   @BeforeClass

   public static void setup()  {
              System.setProperty(""webdriver.gecko.driver"",""D:\\geckodriver.exe"");

   driver = new FirefoxDriver();

   driver.manage().timeouts().implicitlyWait(60, TimeUnit.SECONDS);

  }

   @Test

 void Testcase1() {

   driver.get(""http://mail.google.com"");

   WebElement loginfield = driver.findElement(By.name(""Email""));
   if(loginfield.isDisplayed()){
       loginfield.sendKeys(""ragesh@gmail.in"");
   }
   else{
  WebElement newloginfield = driver.findElemnt(By.cssSelector(""#identifierId""));                                      
       newloginfield.sendKeys(""ragesh@gmail.in"");
      // System.out.println(""This is new login"");
   }


    driver.findElement(By.name(""signIn"")).click();

  // driver.findElement(By.cssSelector("".RveJvd"")).click();

   driver.manage().timeouts().implicitlyWait(15, TimeUnit.SECONDS);
 // WebElement pwd = driver.findElement(By.name(""Passwd""));
  WebElement pwd = driver.findElement(By.cssSelector(""#Passwd""));

  pwd.click();
  pwd.clear();
 // pwd.sendKeys(""123"");
 if(pwd.isEnabled()){
     pwd.sendKeys(""123"");
 }
 else{
     System.out.println(""Not Enabled"");
 }

",318k,"
            33
        ","['\nTry setting an implicit wait of maybe 10 seconds.\ngmail.manage().timeouts().implicitlyWait(10, TimeUnit.SECONDS);\n\nOr set an explicit wait. An explicit waits is code you define to wait for a certain condition to occur before proceeding further in the code. In your case, it is the visibility of the password input field. (Thanks to ainlolcat\'s comment)\nWebDriver gmail= new ChromeDriver();\ngmail.get(""https://www.gmail.co.in""); \ngmail.findElement(By.id(""Email"")).sendKeys(""abcd"");\ngmail.findElement(By.id(""next"")).click();\nWebDriverWait wait = new WebDriverWait(gmail, 10);\nWebElement element = wait.until(\nExpectedConditions.visibilityOfElementLocated(By.id(""Passwd"")));\ngmail.findElement(By.id(""Passwd"")).sendKeys(""xyz"");\n\nExplanation: The reason selenium can\'t find the element is because the id of the password input field is initially Passwd-hidden. After you click on the ""Next"" button, Google first verifies the email address entered and then shows the password input field (by changing the id from Passwd-hidden to Passwd). So, when the password field is still hidden (i.e. Google is still verifying the email id), your webdriver starts searching for the password input field with id Passwd which is still hidden. And hence, an exception is thrown.\n', '\n""element not interactable"" error can mean two things :\na.  Element has not properly rendered:\nSolution for this is just to use implicit /explicit wait\n\nImplicit wait :\ndriver.manage().timeouts().implicitlyWait(50, TimeUnit.SECONDS);\n\nExplicit wait :\nWebDriverWait wait=new WebDriverWait(driver, 20);\nelement1 = wait.until(ExpectedConditions.elementToBeClickable(By.className(""fa-stack-1x"")));\n\n\nb. Element has rendered but it is not in the visible part of the screen:\nSolution is just to scroll till the element. Based on the version of Selenium it can be handled in different ways but I will provide a solution that works in all versions :\n    JavascriptExecutor executor = (JavascriptExecutor) driver;\n    executor.executeScript(""arguments[0].scrollIntoView(true);"", element1);\n\n\nSuppose all this fails then another way is to again make use of Javascript executor as following :\nexecutor.executeScript(""arguments[0].click();"", element1);\n\nIf you still can\'t click , then it could again mean two things :\n\n\n1. Iframe\nCheck the DOM to see if the element you are inspecting lives in any frame. If that is true then you would need to switch to this frame before attempting any operation.\n    driver.switchTo().frame(""a077aa5e""); //switching the frame by ID\n    System.out.println(""********We are switching to the iframe*******"");\n    driver.findElement(By.xpath(""html/body/a/img"")).click();\n\n2. New tab\nIf a new tab has opened up and the element exists on it then you again need to code something like below to switch to it before attempting operation.\nString parent = driver.getWindowHandle();\ndriver.findElement(By.partialLinkText(""Continue"")).click();\nSet<String> s = driver.getWindowHandles();\n// Now iterate using Iterator\nIterator<String> I1 = s.iterator();\nwhile (I1.hasNext()) {\nString child_window = I1.next();\nif (!parent.equals(child_window)) {\n    driver.switchTo().window(child_window);\n    element1.click() \n}\n\n', '\nPlease try selecting the password field like this.\n    WebDriverWait wait = new WebDriverWait(driver, 10);\n    WebElement passwordElement = wait.until(ExpectedConditions.elementToBeClickable(By.cssSelector(""#Passwd"")));\n    passwordElement.click();\n  passwordElement.clear();\n     passwordElement.sendKeys(""123"");\n\n', '\nyou may also try full xpath, I had a similar issue where I had to click on an element which has a property javascript onclick function. the full xpath method worked and no interactable exception was thrown.\n', '\nIn my case the element that generated the Exception was a button belonging to a form. I replaced\nWebElement btnLogin = driver.findElement(By.cssSelector(""button""));\nbtnLogin.click();\n\nwith\nbtnLogin.submit();\n\nMy environment was chromedriver windows 10\n', ""\nIn my case, I'm using python-selenium.\nI have two instructions. The second instruction wasn't able to execute.\nI put a time.sleep(1) between two instructions and I'm done.\nIf you want you can change the sleep amount according to your need.\n"", '\nI had the same problem and then figured out the cause. I was trying to type in a span tag instead of an input tag. My XPath was written with a span tag, which was a wrong thing to do. I reviewed the Html for the element and found the problem. All I then did was to find the input tag which happens to be a child element. You can only type in an input field if your XPath is created with an input tagname\n', ""\nI'm going to hedge this answer with this: I know it's crap.. and there's got to be a better way. (See above answers) But I tried all the suggestions here and still got nill. Ended up chasing errors, ripping the code to bits. Then I tried this:\nimport keyboard    \nkeyboard.press_and_release('tab')\nkeyboard.press_and_release('tab')\nkeyboard.press_and_release('tab') #repeat as needed\nkeyboard.press_and_release('space') \n\nIt's pretty insufferable and you've got to make sure that you don't lose focus otherwise you'll just be tabbing and spacing on the wrong thing.\nMy assumption on why the other methods didn't work for me is that I'm trying to click on something the developers didn't want a bot clicking on. So I'm not clicking on it!\n"", '\nI got this error because I was using a wrong CSS selector with the Selenium WebDriver Node.js function By.css().\nYou can check if your selector is correct by using it in the web console of your web browser (Ctrl+Shift+K shortcut), with the JavaScript function document.querySelectorAll().\n', '\nIf it\'s working in the debug, then wait must be the proper solution.\nI will suggest to use the explicit wait, as given below:\nWebDriverWait wait = new WebDriverWait(new ChromeDriver(), 5);\nwait.until(ExpectedConditions.presenceOfElementLocated(By.cssSelector(""#Passwd"")));\n\n', '\nI came across this error too.  I thought it might have been because the field was not visible.  I tried the scroll solution above and although the field became visible in the controlled browser session I still got the exception.  The solution I am committing looks similar to below.  It looks like the event can bubble to the contained input field and the end result is the Selected property becomes true.\nThe field appears in my page something like this.\n<label>\n  <input name=""generic"" type=""checkbox"" ... >\n<label>\n\nThe generic working code looks more or less like this:\nvar checkbox = driver.FindElement(By.Name(""generic""), mustBeVisible: false);\ncheckbox.Selected.Should().BeFalse();\nvar label = checkbox.FindElement(By.XPath(""..""));\nlabel.Click();\ncheckbox.Selected.Should().BeTrue();\n\nYou\'ll need to translate this to your specific language.  I\'m using C# and FluentAssertions.  This solution worked for me with Chrome 94 and Selenium 3.141.0.\n', ""\nI had to hover over the element first for the sub-elements to appear. I didn't take that into account at first.\n    WebElement boardMenu = this.driver.findElement(By.linkText(boardTitle));\n    Actions action = new Actions(this.driver);\n\n    action.moveToElement(boardMenu).perform();\n\nAnother tip is to check that you are having one element of that DOM. Try using Ctrl+F when inspecting the web page and check your xpath there; it should return one element if you are going with the findElement method.\n""]",https://stackoverflow.com/questions/45183797/element-not-interactable-exception-in-selenium-web-automation,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to select elements within an iframe element in Puppeteer,"
Since ESPN does not provide an API, I am trying to use Puppeteer to scrape data about my fantasy football league. However, I am having a hard time trying to login using puppeteer due to the login form being nested with an iframe element.
I have gone to http://www.espn.com/login and selected the iframe. I can't seem to select any of the elements within the iframe except for the main section by doing
    frame.$('.main')

This is the code that seems to get the iframe with the login form.
    const browser = await puppeteer.launch({headless:false});
    const page = await browser.newPage();

    await page.goto('http://www.espn.com/login')
    await page.waitForSelector(""iframe"");

    const elementHandle = await page.$('div#disneyid-wrapper iframe');
    const frame = await elementHandle.contentFrame();
    await browser.close()

I want to be able to access the username field, password field, and the login button within the iframe element. Whenever I try to access these fields, I get a return of null.
",26k,"
            18
        ","['\nYou can get the iframe using contentFrame as you are doing now, and then call $.\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto(\'http://www.espn.com/login\')\n\nconst elementHandle = await page.waitForSelector(\'div#disneyid-wrapper iframe\');\nconst frame = await elementHandle.contentFrame();\nawait frame.waitForSelector(\'[ng-model=""vm.username""]\');\nconst username = await frame.$(\'[ng-model=""vm.username""]\');\nawait username.type(\'foo\');\nawait browser.close()\n\n\n', ""\nI had an issue with finding stripe elements. \nThe reason for that is the following: \n\nYou can't access an  with different origin using JavaScript, it would be a huge security flaw if you could do it. For the same-origin policy browsers block scripts trying to access a frame with a different origin. See more detailed answer here\n\nTherefore when I tried to use puppeteer's methods:Page.frames() and Page.mainFrame(). ElementHandle.contentFrame() I did not return any iframe to me. The problem is that it was happening silently and I couldn't figure out why it couldn't find anything.\nAdding these arguments to launch options solved the issue:\n\n '--disable-web-security',\n'--disable-features=IsolateOrigins,site-per-process'\n\n""]",https://stackoverflow.com/questions/56420047/how-to-select-elements-within-an-iframe-element-in-puppeteer,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web automation from C++,"
We need to do some fairly complex web automation from C++ application (log into application, do some actions, logout), but performance is really important so we are looking at options.

Is there a way to drive WebKit or other headless engine directly from C++, without the need for few more layers in between (like selenium+webdriver+network communication+...)? Chromedriver perhaps?
If option 1 is not possible, what is the most optimal way to run WebDriver (with real browser) from C++?

",38k,"
            18
        ","['\nYou can use selenium server and JsonWireProtocol. In C++ you can implement CURL requests to selenium server and do web automation with C++.\nUse this link first: My fork of Webdriver++.\nThere are also some C++ libraries that do this work. \nThe first is Webdriver++ By sekogan but last commit was 3 years ago, and it seems not all things works for now.\nThe second is my fork of Webdriver++, i\'ve fixed some bugs and make this project as shared library, so you can use it in any C++ project.\nThis is an example of how you can use my My fork of Webdriver++.\n#include <webdriverxx/webdriverxx.h>\nusing namespace webdriverxx;\n\nint main() {\n   WebDriver firefox = Start(Firefox());\n   firefox\n       .Navigate(""http://google.com"")\n       .FindElement(ByClass(""class_name""))\n       .SendKeys(""Hello, world!"")\n       .Submit();\n   return 0;    \n}\n\n', ""\nYou should look into PhantomJS (a headless WebKit browser), which comes with GhostDriver, which is the WebDriver protocol implementation for PhantomJS.\nYou will still need to use one of the WebDriver language bindings, which I'm not aware of any of the language bindings that are in C++, but perhaps one of the available languages could be used by your team for automation purposes.\nWorst case, you could always create your WebDriver script in Python, and call the Python script from your C++ application.\n""]",https://stackoverflow.com/questions/17345551/web-automation-from-c,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
puppeteer wait for page/DOM updates - respond to new items that are added after initial loading,"
I want to use Puppeteer to respond to page updates.
The page shows items and when I leave the page open new items can appear over time.
E.g. every 10 seconds a new item is added.
I can use the following to wait for an item on the initial load of the page:
await page.waitFor("".item"");
console.log(""the initial items have been loaded"")

How can I wait for / catch future items?
I would like to achieve something like this (pseudo code):
await page.goto('http://mysite');
await page.waitFor("".item"");
// check items (=these initial items)

// event when receiving new items:
// check item(s) (= the additional [or all] items)

",7k,"
            5
        ","['\nYou can use exposeFunction to expose a local function:\nawait page.exposeFunction(\'getItem\', function(a) {\n    console.log(a);\n});\n\nThen you can use page.evaluate to create an observer and listen to new nodes created inside a parent node.\nThis example scrapes (it\'s just an idea, not a final work) the python chat in Stack Overflow, and prints new items being created in that chat.\nvar baseurl =  \'https://chat.stackoverflow.com/rooms/6/python\';\nconst browser = await puppeteer.launch({headless: false});\nconst page = await browser.newPage();\nawait page.goto(baseurl);\n\nawait page.exposeFunction(\'getItem\', function(a) {\n    console.log(a);\n});\n\nawait page.evaluate(() => {\n    var observer = new MutationObserver((mutations) => { \n        for(var mutation of mutations) {\n            if(mutation.addedNodes.length) {\n                getItem(mutation.addedNodes[0].innerText);\n            }\n        }\n    });\n    observer.observe(document.getElementById(""chat""), { attributes: false, childList: true, subtree: true });\n});\n\n', '\nAs an alternative to the excellent current answer which injects a MutationObserver using evaluate which forwards the data to an exposed Node function, Puppeteer offers a higher-level function called page.waitForFunction that blocks on an arbitrary predicate and uses either a MutationObserver or requestAnimationFrame under the hood to determine when to re-evaluate the predicate.\nCalling page.waitForFunction in a loop might add overhead since each new call involves registering a fresh observer or RAF. You\'d have to profile for your use case -- this isn\'t something I\'d worry much about prematurely, though.\nThat said, the RAF option may provide tighter latency than MO for the cost of some extra CPU cycles to poll constantly.\nHere\'s a minimal example on the following site that offers a periodically updating feed:\n\n\nconst wait = ms => new Promise(r => setTimeout(r, ms));\nconst r = (lo, hi) => ~~(Math.random() * (hi - lo) + lo);\n\nconst randomString = n =>\n  [...Array(n)].map(() => String.fromCharCode(r(97, 123))).join("""")\n;\n\n(async () => {\n  for (let i = 0; i < 500; i++) {\n    const el = document.createElement(""div"");\n    document.body.appendChild(el);\n    el.innerText = randomString(r(5, 15));\n    await wait(r(1000, 5000));\n  }\n})();\n\n\nconst puppeteer = require(""puppeteer"");\n\nconst html = `\n<html><body><div class=""container""></div><script>\nconst wait = ms => new Promise(r => setTimeout(r, ms));\nconst r = (lo, hi) => ~~(Math.random() * (hi - lo) + lo);\nconst randomString = n =>\n  [...Array(n)].map(() => String.fromCharCode(r(97, 123))).join("""")\n;\n(async () => {\n  for (;;) {\n    const el = document.createElement(""div"");\n    document.querySelector("".container"").appendChild(el);\n    el.innerText = randomString(r(5, 15));\n    await wait(r(1000, 5000));\n  }\n})();\n</script></body></html>\n`;\nlet browser;\n(async () => {\n  browser = await puppeteer.launch({headless: false});\n  const [page] = await browser.pages();\n  await page.setContent(html);\n  \n  for (;;) {\n    await page.waitForFunction((el, oldLength) =>\n      el.children.length > oldLength,                           // predicate\n      {polling: ""mutation"" /* or: ""raf"" */, timeout: 0},        // wFF options\n      await page.$("".container""),                               // elem to watch\n      await page.$eval("".container"", el => el.children.length), // oldLength\n    );\n    const selMostRecent = "".container div:last-child"";\n    console.log(await page.$eval(selMostRecent, el => el.textContent));\n  }\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close())\n;\n\nSee also:\n\nPass a function inside page.waitForFunction() with puppeteer which shows a generic waitForTextChange helper function that wraps page.waitForFunction.\nRealtime scrape a chat using Nodejs which aptly suggests the alternative approach of intercepting API responses as they populate the feed, when possible.\n\n']",https://stackoverflow.com/questions/54109078/puppeteer-wait-for-page-dom-updates-respond-to-new-items-that-are-added-after,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accessing an element with no attributes in Watir,"
Using Watir, is there a way to access an element without attributes?
For example:
<span>Text</span>

I'd like to avoid using xpath, but if that's the only way it's cool.
",7k,"
            6
        ","['\nDisregarding the non-WATIR issues of having tags in the first place, or requesting unique attributes from your developers (or yourself), you can always access an element via its parent elements, or by index.\nFor example:\n    \n       \n          Text\n       \n    \n@browser.div(:name => ""content"").span(:index => 1)\n#this is the first span element inside this div\n\nYou can work through however many unique elements you need to before reaching the child span element, without using Xpath.  Of course, you only need one unique parent element to reach that specific child element, and you work down from that to the child.\ndiv(:how => what).table(:how => what).td(:how => what).span(:how => what).text\n\nAnother example, assuming it is the nth span on the page:\n       @browser.span(:index => n)\nThe by-index approach is very brittle and prone to breaking when any update is made to the page, however.\n', '\nIf it has text:\nbrowser.span(:text => ""Text"")\n\nIf you know only part of the text you can use regular expression:\nbrowser.span(:text => /Text/)\n\n', '\nThere are basically three ways to address this particular challenge.  Zeljko has addressed the first which is based on what is inside the element such as known text.  Adam addresses the most common way, what is enclosing or containing the element  I\'ll address the third way, which is what is enclosed-by or beside the element.  \nIf you have a known element that is inside the one you want, then you can start with that and use the .parent method to get the \'container\' element.  This can also be used to find a \'sibling\' element by using .parent to get to the one you want via a common container such as a table row.  The first use is fairly obvious, but the second is probably more common and very useful when working with tables.\nFor example Lets say you have a table with multiple rows of data where one column is unique part numbers, and another column has ""add to cart"" links.  Now, if you want to add a specific part to your cart, you could use Index combined with the text \'add to cart\' using code like this based on it being the 5th link with that specific text\nbrowser.link(:text => \'add to cart\', :index => 4).click\n\nBut this is brittle because as soon as the results change, (which can happen a lot with live data) your part is no longer the 5th one in that table, and your test would break. You would need some verification you\'ve found the correct part and not something else on that row. However, in watir you can do something like this:\nbrowser.cell(:text => \'Part no. 123-45\').parent.link(:text => \'add to cart\').click\n\nIn the case of a table cell, the parent of the cell will usually be a table row, and thus in plain english this translates to \'find the cell with \'part no 123-45\' in it, and then in that same row find and click on the \'add to cart\' link.  (although I\'m guessing you figured that out just by reading the code.) \nYou can use this to get any \'sibling\' or even just the \'parent\' itself where there\'s some unique element next to or within the object you need to interact with.\nYou can probably do something similar to that with Xpath, but good luck making any sense out of it when reading the code five weeks later.  This is one reason I vastly prefer Watir and Watir-Webdriver vs Selenium.\n']",https://stackoverflow.com/questions/8330926/accessing-an-element-with-no-attributes-in-watir,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Puppeteer: Get innerHTML,"
Does anybody know how to get the innerHTML or text of an element? Or even better; how to click an element with a specific innerHTML? This is how it would work with normal JavaScript:
var found = false
$(selector).each(function() {
    if (found) return;
    else if ($(this).text().replace(/[^0-9]/g, '') === '5' {
        $(this).trigger('click');
        found = true
    }
});

Thanks in advance for any help!
",59k,"
            39
        ","['\nThis is how i get innerHTML:\npage.$eval(selector, (element) => {\n  return element.innerHTML\n})\n\n', ""\nReturning innerHTML of an Element\nYou can use the following methods to return the innerHTML of an element:\npage.$eval()\nconst inner_html = await page.$eval('#example', element => element.innerHTML);\n\npage.evaluate()\nconst inner_html = await page.evaluate(() => document.querySelector('#example').innerHTML);\n\npage.$() / elementHandle.getProperty() / jsHandle.jsonValue()\nconst element = await page.$('#example');\nconst element_property = await element.getProperty('innerHTML');\nconst inner_html = await element_property.jsonValue();\n\n\nClicking an Element with Specific innerHTML\nYou can use the following methods to click on an element based on the innerHTML that is contained within the element:\npage.$$eval()\nawait page.$$eval('.example', elements => {\n  const element = elements.find(element => element.innerHTML === '<h1>Hello, world!</h1>');\n  element.click();\n});\n\npage.evaluate()\nawait page.evaluate(() => {\n  const elements = [...document.querySelectorAll('.example')];\n  const element = elements.find(element => element.innerHTML === '<h1>Hello, world!</h1>');\n  element.click();\n});\n\npage.evaluateHandle() / elementHandle.click()\nconst element = await page.evaluateHandle(() => {\n  const elements = [...document.querySelectorAll('.example')];\n  const element = elements.find(element => element.innerHTML === '<h1>Hello, world!</h1>');\n  return element;\n});\n\nawait element.click();\n\n"", ""\nThis should work with puppeteer:)\nconst page = await browser.newPage();\nconst title = await page.evaluate(el => el.innerHTML, await page.$('h1'));\n\n"", ""\nYou can leverage the page.$$(selector) to get all your target elments and then use page.evaluate() to get the content(innerHTML), then apply your criteria. It should look something like: \nconst targetEls = await page.$$('yourFancySelector');\nfor(let target of targetEls){\n  const iHtml = await page.evaluate(el => el.innerHTML, target); \n  if (iHtml.replace(/[^0-9]/g, '') === '5') {\n    await target.click();\n    break;\n  }\n}\n\n"", '\nWith regard to this part of your question...\n\n""Or even better; how to click an element with a specific innerHTML.""\n\nThere are some particulars around innerHTML, innerText, and textContent that might give you grief. Which you can work-around using a sufficiently loose XPath query with Puppeteer v1.1.1.  \nSomething like this:\nconst el = await page.$x(\'//*[text()[contains(., ""search-text-here"")]]\');\nawait el[0].click({     \n                button: \'left\',\n                clickCount: 1,\n                delay: 50\n            });\n\nJust keep in mind that you will get an array of ElementHandles back from that query. So... the particular item you are looking for might not be at [0] if your text isn\'t unique. \nOptions passed to .click() aren\'t necessary if all you need is a single left-click.\n', ""\nI can never get the .innerHtml to work reliable. I always do the following:\nlet els = page.$$('selector');\nfor (let el of els) {\n  let content = await (await el.getProperty('textContent')).jsonValue();\n}\n\nThen you have your text in the 'content' variable.\n"", ""\nYou can simply write as below. (no need await sentence in the last part)\nconst center = await page.$eval('h2.font-34.uppercase > strong', e => e.innerHTML);\n\n"", '\n<div id=""innerHTML"">Hello</div>\n\n\nvar myInnerHtml = document.getElementById(""innerHTML"").innerHTML;\nconsole.log(myInnerHtml);\n\n']",https://stackoverflow.com/questions/46431288/puppeteer-get-innerhtml,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automatic login script for a website on windows machine?,"
I saw some guy had a file (I guess a batch file). On clicking of the batch file he was able to log in to multiple sites. (Perhaps it was done using VB.)
I looked for such a script on Google but didn't find anything useful.
I know a bit of C++ and UNIX (also some HTML and JavaScript). I don't know if it can be done on a windows machine using these languages, but even if it could be done I think it would be difficult compared to VB or C## or some other high level languages.
I learned how to open multiple sites using basic windows batch commands enclosed in a batch file like:
start http://www.gmail.com
start http://stackoverflow.com

But still I can't figure out how actually clicking on the batch file would help me to log in to the sites without even typing the username and password.
Do I need to start learning Visual Basic, .NET, or windows batch programming to do this?
One more thing: can I also use it to log in to remote desktops?
",168k,"
            23
        ","['\nFrom the term ""automatic login"" I suppose security (password protection) is not of key importance here.\nThe guidelines for solution could be to use a JavaScript bookmark (idea borrowed form a nice game published on M&M\'s DK site).\nThe idea is to create a javascript file and store it locally. It should do the login data entering depending on current site address. Just an example using jQuery:\n// dont forget to include jQuery code\n// preferably with .noConflict() in order not to break the site scripts\nif (window.location.indexOf(""mail.google.com"") > -1) {\n    // Lets login to Gmail\n    jQuery(""#Email"").val(""youremail@gmail.com"");\n    jQuery(""#Passwd"").val(""superSecretPassowrd"");\n    jQuery(""#gaia_loginform"").submit();\n}\n\nNow save this as say login.js\nThen create a bookmark (in any browser) with this (as an) url:\njavascript:document.write(""<script type=\'text/javascript\' src=\'file:///path/to/login.js\'></script>"");\n\nNow when you go to Gmail and click this bookmark you will get automatically logged in by your script.\nMultiply the code blocks in your script, to add more sites in the similar manner. You could even combine it with window.open(...) functionality to open more sites, but that may get the script inclusion more complicated.\nNote: This only illustrates an idea and needs lots of further work, it\'s not a complete solution.\n', '\nThe code below does just that.  The below is a working example to log into a game.  I made a similar file to log in into Yahoo and a kurzweilai.net forum.\nJust copy the login form from any webpage\'s source code. Add value= ""your user name"" and value = ""your password"".  Normally the -input- elements in the source code do not have the value attribute, and sometime, you will see something like that:  value="""" \nSave the file as a html on a local machine double click it, or make a bat/cmd file to  launch and close them as required.\n    <!doctype html>\n    <!-- saved from url=(0014)about:internet -->\n\n    <html>\n    <title>Ikariam Autologin</title>\n    </head>\n    <body>\n    <form id=""loginForm"" name=""loginForm"" method=""post""    action=""http://s666.en.ikariam.com/index.php?action=loginAvatar&function=login"">\n    <select name=""uni_url"" id=""logServer"" class=""validate[required]"">\n    <option  class=""""  value=""s666.en.ikariam.com"" fbUrl=""""  cookieName=""""  >\n            Test_en\n    </option>\n    </select>\n    <input id=""loginName"" name=""name"" type=""text"" value=""PlayersName"" class="""" />\n    <input id=""loginPassword"" name=""password"" type=""password"" value=""examplepassword"" class="""" />\n    <input type=""hidden"" id=""loginKid"" name=""kid"" value=""""/>\n                        </form>\n  <script>document.loginForm.submit();</script>       \n  </body></html>\n\nNote that -script- is just -script-. I found there is no need to specify that is is JavaScript. It works anyway. I also found out that a bare-bones version that contains just two input filds: userName and password also work. But I left a hidded input field etc. just in case.  Yahoo mail has a lot of hidden fields. Some are to do with password encryption, and it counts login attempts.\nSecurity warnings and other staff, like Mark of the Web to make it work smoothly in IE are explained here:\nhttp://happy-snail.webs.com/autologinintogames.htm\n', '\nI used @qwertyjones\'s answer to automate logging into Oracle Agile with a public password.\nI saved the login page as index.html, edited all the href= and action= fields to have the full URL to the Agile server.\nThe key <form> line needed to change from\n<form autocomplete=""off"" name=""MainForm"" method=""POST""\n action=""j_security_check"" \n onsubmit=""return false;"" target=""_top"">\n\nto\n<form autocomplete=""off"" name=""MainForm"" method=""POST""\n action=""http://my.company.com:7001/Agile/default/j_security_check""   \n onsubmit=""return false;"" target=""_top"">\n\nI also added this snippet to the end of the <body>\n<script>\nfunction checkCookiesEnabled(){ return true; }\ndocument.MainForm.j_username.value = ""joeuser"";\ndocument.MainForm.j_password.value = ""abcdef"";\nsubmitLoginForm();\n</script> \n\nI had to disable the cookie check by redefining the function that did the check, because I was hosting this from XAMPP and I didn\'t want to deal with it. The submitLoginForm() call was inspired by inspecting the keyPressEvent() function.\n', '\nYou can use Autohotkey, download it from: http://ahkscript.org/download/\nAfter the installation, if you want to open Gmail website when you press Alt+g, you can do something like this:\n!g::\nRun www.gmail.com \nreturn\n\nFurther reference: Hotkeys (Mouse, Joystick and Keyboard Shortcuts)\n', '\nWell, its true that we can use Vb Script for what you intended to do.\nWe can open an application through the code like Internet Explorer. We can navigate to site you intend for. Later we can check the element names of Text Boxes which require username and password; can set then and then Login. It works fine all of using code.\nNo manual interaction with the website. And eventually you will end up signing in by just double clicking the file.\nTo get you started :\nSet objIE = CreateObject(""InternetExplorer.Application"")\n\nCall objIE.Navigate(""https://gmail.com"")\n\nThis will open an instance of internet explore and navigate to gmail.\nRest you can learn and apply.\n']",https://stackoverflow.com/questions/6248679/automatic-login-script-for-a-website-on-windows-machine,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Using Playwright for Python, how do I select (or find) an element?","
I'm trying to learn the Python version of Playwright. See here
I would like to learn how to locate an element, so that I can do
things with it. Like printing the inner HTML, clicking on it and such.
The example below loads a page and prints the HTML
from playwright import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(headless=False)
    page = browser.newPage()
    page.goto('http://whatsmyuseragent.org/')
    print(page.innerHTML(""*""))
    browser.close()

This page contains an element
<div class=""user-agent"">
    <p class=""intro-text"">Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4238.0 Safari/537.36</p>
</div>

Using Selenium, I could locate the element and print it's content like this
elem = driver.find_element_by_class_name(""user-agent"")
print(elem)
print(elem.get_attribute(""innerHTML""))

How can I do the same in Playwright?
#UPDATE# - Note if you want to run this in 2021+ that current versions of playwright have changed the syntax from CamelCase to snake_case.
",19k,"
            8
        ","['\nThe accepted answer does not work with the newer versions of Playwright. (Thanks @576i for pointing this out)\nHere is the Python code that works with the newer versions (tested with version 1.5):\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch()\n    page = browser.new_page()\n    page.goto(\'http://whatsmyuseragent.org/\')\n    ua = page.query_selector("".user-agent"");\n    print(ua.inner_html())\n    browser.close()\n\nTo get only the text, use the inner_text() function.\nprint(ua.inner_text())\n\n', '\nYou can use the querySelector function, and then call the innerHTML function:\nhandle = page.querySelector("".user-agent"")\nprint(handle.innerHTML())\n\n', '\naccording to Latest official python version Playwright, you should use:\n\npage\'s query_selector\nand element\'s inner_html\n\n-> the code:\n# userAgentSelector = "".user-agent""\nuserAgentSelector = ""div.user-agent""\nelementHandle = page.query_selector(userAgentSelector)\nuaHtml = elementHandle.inner_html()\nprint(""uaHtml=%s"" % uaHtml)\n\n', '\nI think you can find the solutions in the following article.\nPlaywright >> Find, Locate, Select Elements/Tags using Playwright\n\nPlaywright find all elements/tags containing specified text\nPlaywright find elements/tags containing specified child element/tag\nPlaywright loop through all elements/tags in locator() result\nPlaywright find/get first element Playwright find/get last element\nPlaywright get the parent element Playwright get the child element\nPlaywright get nth child element Playwright find elements/tags by css\nclass Playwright find elements near the specified text Playwright\nfind elements/tags by attribute Playwright find elements/tags by id\n\n']",https://stackoverflow.com/questions/64303326/using-playwright-for-python-how-do-i-select-or-find-an-element,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Python, mechanize, proper syntax for setting multiple headers?","
I can't seem to find how to do this anywere, I am trying to set multiple headers with python's mechanize module, such as:
br.addheaders = [('user-agent', '   Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.2.3) Gecko/20100423 Ubuntu/10.04 (lucid) Firefox/3.6.3')]
br.addheaders = [('accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')]

But it seems that it only takes the last br.addheaders.. so it only shows the 'accept' header, not the 'user-agent' header, which leads me to believe that each call to 'br.addheaders' overwrites any previous calls to this.. I can't figure the syntax to include 2 or more headers so I would greatly appreciate any help..
I am using this website to test headers output:
http://www.ericgiguere.com/tools/http-header-viewer.html
",8k,"
            8
        ","[""\nAccording to http://wwwsearch.sourceforge.net/mechanize/doc.html#adding-headers, the syntax would be \nbr.addheaders = [('user-agent', '   Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.2.3) Gecko/20100423 Ubuntu/10.04 (lucid) Firefox/3.6.3'),\n('accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')]\n\nThat is, make a list of header tuples.\n""]",https://stackoverflow.com/questions/3564509/python-mechanize-proper-syntax-for-setting-multiple-headers,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Execute javascript trough Internet Explorer's com interface using PowerShell,"
I am writing some Internet Explorer automation scripts using PowerShell. Here is how I start the IE com object:
$ie = New-Object -com ""InternetExplorer.Application""
$ie.Navigate(""about:blank"")
$ie.visible = $true

$doc = $ie.Document

So, what I would like to do is to execute some javascript on the $doc object. For example, I have an item on the page that has an onclick event which executes submitCommand('lookup'), so I'd like to run that directly on the $doc instead of having to find the object on the page and then calling the Click() method on it.
It would be easier as the object has no name nor id, making it very sensible to change as I can only rely on it's position on the page (eg: the 11th span item on the page).
Alternatively, how would you select elements based on their class? That would help a lot as the ""button"" has it's own class.
Thanks
",19k,"
            7
        ","['\n$spans=@($ie.document.getElementsByTagName(""SPAN""))\nPipe to where-object to filter the one you need (based on its attributes) and then call the click method, for example:\n$span11 = $spans | where {$_.innerText -eq \'something\'}\n$span11.click()\n\n']",https://stackoverflow.com/questions/1444330/execute-javascript-trough-internet-explorers-com-interface-using-powershell,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to code vba to open internet explorer in new session?,"
I am struggling to get this done since months, how to code VBA to open internet explorer in new session i have an application with many logins  i need to open them simultaneously using automation , i have used 
  set ie=new InternetExplorer  

but it opens the ie within the old session, i want to open new session for each and every login please help me, i googled a lot for it but ended up with out any solution.
 this is my code
 Function GetIE() As InternetExplorer

  Dim WScript
Dim objShellWindows

 Set objShell = CreateObject(""Shell.Application"")
 Set objShellWindows = objShell.Windows
 Set WScript = CreateObject(""WScript.Shell"")


 Dim ieStarted
 ieStarted = False

  Dim ieError
  ieError = False

    Dim seconds
      seconds = 0

  While (Not ieStarted) And (Not ieError) And (seconds < 30)

If (Not objShellWindows Is Nothing) Then
    Dim objIE As InternetExplorer
    Dim IE


    For Each objIE In objShellWindows

        If (Not objIE Is Nothing) Then

            If IsObject(objIE.Document) Then
                Set IE = objIE.Document

                If VarType(IE) = 8 Then

                    If IE.Title = EmptyTitle Then
                        If Err.Number = 0 Then
                            IE.Write LoadingMessage

                            objIE.navigate Sheet1.Login.Text
                        ieStarted = True
                        Set GetIE = objIE


                      Else

                       MsgBox ErrorMessage
                            Err.Clear
                            ieError = True

                            Exit For
                        End If
                    End If
                End If
            End If
        End If

        Set IE = Nothing
        Set objIE = Nothing
    Next
End If

Application.Wait Now + TimeValue(""00:00:1"")
seconds = seconds + 1
Wend

 Set objShellWindows = Nothing
 Set objShell = Nothing



   End Function

with this code im able to open the browser but sadly my webpage is opening in outlook which is already opened pls help
",48k,"
            3
        ","['\nApparently the -nomerge argument will prevent session merging.\nShell(""iexplore.exe -nomerge http://www.yoursite.com"")\n\nUPDATE\nAs per your comment, you need to get the IE object. You may be able to work with this:\nDim wshShell\nSet wshShell = WScript.CreateObject(""WScript.Shell"")\n\nwshShell.Run ""iexplore -nomerge http://www.google.com""\n\nDim objShell\nSet objShell = CreateObject(""Shell.Application"")\n\nDim objShellWindows\nSet objShellWindows = objShell.Windows\n\nDim i\nDim ieObject\nFor i = 0 To objShellWindows.Count - 1\n    If InStr(objShellWindows.Item(i).FullName, ""iexplore.exe"") <> 0 Then\n        Set ieObject = objShellWindows.Item(i)\n        If VarType(ieObject.Document) = 8 Then\n            MsgBox ""Loaded "" & ieObject.Document.Title\n            Exit For\n        End If\n    End If\nNext\n\nSet ieObject = Nothing\nSet objShellWindows = Nothing\nSet objShell = Nothing\nSet wshShell = Nothing\n\n', '\nUsing Excel 2010 - This is what I use with a command button. Replace google.com with the website you want to open in another browser.\nPrivate Sub commandname_Click()\n\n\'Opens an Explorer with a web site \n\nDim IE As InternetExplorer\n\n  Set IE = CreateObject(""InternetExplorer.Application"")\n\n  IE.navigate (""http://WWW.GOOGLE.COM"")\n\n  IE.Visible = True\n\nEnd Sub\n\n', '\nThis answer works for me after changing:\nDim IE As InternetExplorer\n\nto\nDim IE As Object\n\n']",https://stackoverflow.com/questions/14184340/how-to-code-vba-to-open-internet-explorer-in-new-session,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Breaking out of a Protractor .filter() or .map() loop,"
I'm using Protractor and cucumber framework; how do I break out of a .filter or .map loop? I do not want to continue to iterate further if I found a match!
Page.prototype.getElementByKey = function (key) {
      var foundElement = null;
      return someElement.all(by.css('.someClass')).map(function (rawItem, index) {
        var itemObject = new ItemObjectClass(rawItem);
        return itemObject.getItemKey().then(function (foundItemKey) {
          var matched = String(foundItemKey).trim() === String(key).trim();

         console.log(' Matched: { ' + matched + ' }  index {'+index+'}');
          //if we have a match break out of the .filter function
          if (matched) {
            foundElement = itemObject;
            throw new Error(""Just our way of breaking out of .filter() above"");
          }
        });
      }).then(function () {
        //callback
        throw new Error('\n!!!!!Callback should not be called; 
       this means that we could not find an element that matched the passed in key above');
      }, function (error) {
        //error
        console.log('\n*******************errorCallback was called; '+error);
        return foundElement;
      });
    };

The above code finds the element but continues to iterate until the end instead of stopping when there's a match and breaking out by calling the errorCallback function. 
Given that .map function returns ""a promise that resolves to an array of values returned by the map function"" http://www.protractortest.org/#/api?view=ElementArrayFinder.prototype.map, I'm taking advantage of the fact that a promise will call its errCallback if the promise cannot be resolved. 
By throwing an a fake error, the errorCallback should be called and thereby break out of the .map loop.
Unfortunately, it successfully throws the error but continues with the loop instead of breaking out. I know that because when I

console.log(""boolean ""+matched+"" and index ""+index);

I get this: 
matched: false index: 0
matched: false index: 1
matched: true index 2 //it should have stopped here since matched = true
matched false index 3 // this should NOT have printed

so breaking out isn't working any ideas?
",4k,"
            2
        ","['\nYou are returning a single element, so .reduce would be preferable.\nHere is a usage example to return the first link where the text is ""mylink"":\nvar link = element.all(by.css(\'a\')).reduce(function (result, elem, index) {\n    if(result) return result;\n\n    return elem.getText().then(function(text){\n        if(text === ""mylink"") return elem;\n    });\n\n}).then(function(result){\n    if(!result) throw new Error(""Element not found"");\n    return result;\n});\n\n', ""\nWhat i understood from your post is, you would like to exit loop(iterate) when find a match element. \nIf yes, then better go with .filter() method. As it iterates on all available list of element finders and returns when a match finds.\nCode Snippet:\n\nelement.all(by.css('.items li')).filter(function(elem, index) {\n          return elem.getText().then(function(text) {\n                                if(text === 'RequiredElementFind'){\n                                      return ele;//return matched element\n                                 };\n    });\n}).click();//only matched element comes from the loop do what would you like    \n  to do\n\n""]",https://stackoverflow.com/questions/38777577/breaking-out-of-a-protractor-filter-or-map-loop,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to login to a website with python and mechanize,"
i'm trying to log in to the website http://www.magickartenmarkt.de and do some analyzing in the member-area (https://www.magickartenmarkt.de/?mainPage=showWants). I saw other examples for this, but i don't get why my approaches didn't work. I identified the right forms for the first approach, but it's not clear if it worked.
In the second approach the returing webpage shows me that i don't have access to the member area. 
I would by glad for any help.
import urllib2
import cookielib
import urllib
import requests
import mechanize
from mechanize._opener import urlopen
from mechanize._form import ParseResponse

USERNAME = 'Test'
PASSWORD = 'bla123'
URL      = ""http://www.magickartenmarkt.de""

# first approach
request = mechanize.Request(URL)
response = mechanize.urlopen(request)
forms = mechanize.ParseResponse(response, backwards_compat=False)
# I don't want to close?!
#response.close()

# Username and Password are stored in this form
form = forms[1]

form[""username""] = USERNAME
form[""userPassword""] = PASSWORD

#proof entering data has worked
user = form[""username""]  # a string, NOT a Control instance
print user
pw = form[""userPassword""]  # a string, NOT a Control instance
print pw
#is this the page where I will redirected after login?
print urlopen(form.click()).read () 

#second approach
cj = cookielib.CookieJar()
opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
login_data = urllib.urlencode({'username' : USERNAME, 'userPassword': PASSWORD})

#login
response_web = opener.open(URL, login_data)

#did it work? for me not....
resp = opener.open('https://www.magickartenmarkt.de/?mainPage=showWants')
print resp.read()

",36k,"
            22
        ","[""\nWhy not use a browser instance to facilitate navigation?  Mechanize also has the ability to select particular forms (e.g. nr = 0 will select the first form on the page)\nbrowser = mechanize.Browser()\nbrowser.open(YOUR URL)\nbrowser.select_form(nr = 0)\nbrowser.form['username'] = USERNAME\nbrowser.form['password'] = PASSWORD\nbrowser.submit()\n\n"", '\nWeb automation ? Definitely ""WEBBOT""\nwebbot works even for webpages with dynamically changing id and classnames and has more methods and features than selenium.\n\nHere\'s a snippet :)\n\nfrom webbot import Browser \nweb = Browser()\nweb.go_to(\'google.com\') \nweb.click(\'Sign in\')\nweb.type(\'mymail@gmail.com\' , into=\'Email\')\nweb.click(\'NEXT\' , tag=\'span\')\nweb.type(\'mypassword\' , into=\'Password\' , id=\'passwordFieldId\') # specific selection\nweb.click(\'NEXT\' , tag=\'span\') # you are logged in ^_^\n\n']",https://stackoverflow.com/questions/16598145/how-to-login-to-a-website-with-python-and-mechanize,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selenium Webdriver vs Mechanize,"
I am interested in automating repetitive data entry in some forms for a website I frequent. So far the tools I've looked up that would provide support for this in a headless fashion could be Selenium WebDriver and Mechanize. 
My question is, is there a fundamental technical difference in using once versus the other? Selenium is mostly used for testing. I've also noticed some folks use it for doing exactly what I'm looking for, and that's automating data entry. Testing becomes a second benefit in that case. 
Is there reasons to not use Selenium for what I want to do over Mechanize? Does it not matter and both of these tools will work? 
I'm not asking which is better, I'm asking which is the right tool for the job. Perhaps I'm not understanding the premise behind the purpose of each tool.
",15k,"
            18
        ","['\nThese are completely different tools that somewhat ""cross"" in the web-scraping, web automation, automated data extraction scope.\nmechanize is a mature and widely-used tool for programmatic web-browsing with a lot of built-in features, like cookie handing, browser history, form submissions. The key thing to understand here is that mechanize.Browser is not a real browser, it cannot execute and understand javascript, it cannot send asynchronous requests often needed to form a web page.\nThis is where selenium comes into play - it is a browser automation tool which is also widely used in web-scraping. selenium usually becomes a ""fall-back"" tool - when someone cannot web-scrape a site with mechanize or RoboBrowser or MechanicalSoup (note - another alternatives) because of, for instance, it\'s javascript ""heaviness"", the choice is usually selenium. With selenium you can also go headless, automating PhantomJS browser, or having a virtual display. As a commonly mentioned drawback, performance is often mentioned - with selenium you are working with a target site as a real user in a web browser, which is loading additional files needed to form a page, making XHR requests, rendering etc.\nAnd this itself does not mean you should use selenium everywhere - choose the tool wisely, choose it because it fits the problem better, not because you are more familiar with an instrument.\n\nAlso note that you should, first, consider using an API (if provided by the target website) instead of going down to web-scraping. And, if it comes to it, be a good web-scraping citizen:\n\nHow to be a good citizen when crawling web sites?\nWeb scraping etiquette\n\n']",https://stackoverflow.com/questions/31530335/selenium-webdriver-vs-mechanize,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Click a checkbox with selenium-webdriver,"
I'm testing my app with tumblr and I have to log in and out as I go through procedures. While doing so, I'm having trouble clicking a checkbox that keeps popping up. How can I use selenium-webriver in python to click it?
I've tried selecting xpaths, ...by_ids, and by_classes, they won't work, so now I'm trying to use the mouse's coordinates to physically click the item. (This is on the tumblr login page, fyi)
 
Above is the html of the item I'm trying to select.
(EDIT:)
I've the following selectors:
#checkbox = driver.find_element_by_id(""recaptcha-anchor"")
#checkbox = driver.find_element_by_id(""g-recaptcha"") 
#driver.find_element_by_xpath(""//*[@id='recaptcha-token']"")
#driver.find_element_by_css_selector(""#recaptcha-anchor"")
#driver.find_element_by_xpath(""//*[@id='recaptcha-anchor']"")
#driver.find_element_by_id(""recaptcha-token"").click()
#driver.find_element_by_class_name('rc-anchor-center-container')
#checkbox = driver.find_element_by_id(""recaptcha-anchor"")

",14k,"
            5
        ","['\nI realise this is an old thread, but I couldn\'t find the answer anywhere else.  In the end I figured it out as follows.\nNote 1: this will tick the recaptcha box, but it won\'t solve it, you\'ll still need to do that manually.\nNote 2: this is on macOS, so you might need a different format for chrome_path on Windows\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n#modify line below to location of your chromedriver executable\nchrome_path = r""/Users/Username/chromedriver""\ndriver = webdriver.Chrome(chrome_path)\ndriver.get(""https://www.btcmarkets.net/login"")\n\nusername = driver.find_element_by_id(""userIdText"")\nusername.send_keys(""Us3rn4me"")\n\npassword = driver.find_element_by_id(""userPasswordText"")\npassword.send_keys(""Pa55w0rD"")\n\n#the line below tabs to the recaptcha tickbox and ticks it with the space bar\npassword.send_keys(Keys.TAB + Keys.TAB + "" "")\n\n', '\nSeems like this is not an input tag. So, probably manipulating the aria-checked attribute and set it to true would do it. The only way to change attribute value is JavaScriptExecutor. Try the following:\ndriver.execute_script(""$(\'#recaptcha-anchor\').setAttribute(\'aria-checked\',\'true\');"")\n\n', '\nUse code below can find the checkbox with id ""recaptcha-anchor"" and click it, but unable to bypass it. The following pictures will pop up. \nList<WebElement> frames = driver.findElements(By.tagName(""iframe""));\n    String winHanaleBefore = driver.getWindowHandle();\n    driver.switchTo().frame(0);\ndriver.findElement(By.id(""recaptcha-anchor"")).click();\ndriver.switchTo().window(winHanaleBefore);\n\n', '\nHere is a simple example that works for me in Java:\ndriver.findElement(By.id(""checkbox_id"")).click();\n\nIn Python, it seems to be:\ndriver.find_element_by_id(""checkbox_id"").click()\n\n']",https://stackoverflow.com/questions/32446151/click-a-checkbox-with-selenium-webdriver,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"powershell: how to click a ""submit type"" input","
used powershell to do web ui automation.  came up an exception: invoke method failed, because [System.__ComObject] does not contain 鈥渃lick鈥?method.
can submit type input be clicked?
i used getElementsByTagName getElementsByClassName getElementsByName , does not work.
anyone can help me on this?
powershell code is below:
# open the specified web site and commit the key
$ie = new-object -com ""InternetExplorer.Application""
$ie.navigate(""http://gitlab.alibaba-inc.com/keys/new"")
$ie.visible = $true
while($ie.busy) {sleep 1}

$doc = $ie.document

# commit the button
$commit = $doc.getElementsByTagName(""commit"")

if($commit) 
{$commit.click()}

the html source is as below:
<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'>
<title>
Profile | 
GitLab
</title>
<link href=""/assets/favicon-4b751da746de7855d7eb8123072388ed.ico"" rel=""shortcut icon""    type=""image/vnd.microsoft.icon"" />
<link href=""/assets/application-a9eac7f5b0c3b922de8997ae9ad74ab0.css"" media=""screen"" rel=""stylesheet"" type=""text/css"" />
<script src=""/assets/application-61398d184a36e6ae900134f123d5d649.js"" type=""text/javascript""></script>
<meta content=""authenticity_token"" name=""csrf-param"" />
<meta content=""9SLFk6AwlsN2FoyO8xPY+M1hEbKfqlLTQ4CSDVc4efE="" name=""csrf-token"" />
<script type=""text/javascript"">
//<![CDATA[
window.gon =   {};gon.default_issues_tracker=""gitlab"";gon.api_version=""v3"";gon.api_token=""xkMg31Ssva322SDF cgxY"";gon.gravatar_url=""http://www.gravatar.com/avatar/%{hash}?s=% {size}&d=mm"";gon.relative_url_root="""";
//]]>
</script>

</head>

<body class='ui_basic profile' data-page='keys:new'>
<header class='navbar navbar-static-top navbar-gitlab'>
<div class='navbar-inner'>
<div class='container'>
<div class='app_logo'>
<span class='separator'></span>
<a href=""/"" class=""home has_bottom_tooltip"" title=""Dashboard""><h1>GITLAB</h1>
</a><span class='separator'></span>
</div>
<h1 class='project_name'>Profile</h1>
<ul class='nav'>
<li>
<a>
<div class='hide turbolink-spinner'>
<i class='icon-refresh icon-spin'></i>
Loading...
</div>
</a>
</li>
<li>
<div class='search'>
<form accept-charset=""UTF-8"" action=""/search"" class=""navbar-form pull-left""  method=""get""><div style=""margin:0;padding:0;display:inline""><input name=""utf8""  type=""hidden"" value=""&#x2713;"" /></div>
<input class=""search-input"" id=""search"" name=""search"" placeholder=""Search"" type=""text""   />
<input id=""group_id"" name=""group_id"" type=""hidden"" />
<input id=""repository_ref"" name=""repository_ref"" type=""hidden"" />

<div class='search-autocomplete-json hide' data-autocomplete-opts='[{""label"":""project:  kelude2"",""url"":""/kelude2""},{""label"":""My Profile"",""url"":""/profile""},{""label"":""My SSH  Keys"",""url"":""/keys""},{""label"":""My Dashboard"",""url"":""/""},{""label"":""Admin  Section"",""url"":""/admin""},{""label"":""help: API Help"",""url"":""/help/api""},{""label"":""help:  Markdown Help"",""url"":""/help/markdown""},{""label"":""help: Permissions  Help"",""url"":""/help/permissions""},{""label"":""help: Public Access  Help"",""url"":""/help/public_access""},{""label"":""help: Rake Tasks  Help"",""url"":""/help/raketasks""},{""label"":""help: SSH Keys Help"",""url"":""/help/ssh""}, {""label"":""help: System Hooks Help"",""url"":""/help/system_hooks""},{""label"":""help: Web Hooks  Help"",""url"":""/help/web_hooks""},{""label"":""help: Workflow Help"",""url"":""/help/workflow""}]'>   </div>
</form>

</div>

</li>
<li>
<a href=""/public"" class=""has_bottom_tooltip"" data-original-title=""Public area""    title=""Public area""><i class='icon-globe'></i>
</a></li>
<li>
<a href=""/s/heyun"" class=""has_bottom_tooltip"" data-original-title=""Public area""    title=""My snippets""><i class='icon-paste'></i>
</a></li>
<li>
<a href=""/projects/new"" class=""has_bottom_tooltip"" data-original-title=""New project""    title=""Create New Project""><i class='icon-plus'></i>
</a></li>
<li>
<a href=""/profile"" class=""has_bottom_tooltip"" data-original-title=""Your profile""    title=""My Profile""><i class='icon-user'></i>
</a></li>
<li>
<a href=""/users/sign_out"" class=""has_bottom_tooltip"" data-method=""delete"" data-original-title=""Logout"" rel=""nofollow"" title=""Logout""><i class='icon-signout'></i>  
</a></li>
<li>
<a href=""/u/heyun"" class=""profile-pic""><img alt=""F3ea5164088694b48e4980e52d831927? s=26&amp;d=mm"" src=""http://www.gravatar.com/avatar/f3ea5164088694b48e4980e52d831927? s=26&amp;d=mm"" />
</a></li>
</ul>
</div>
</div>
</header>

<div class='flash-container'>
</div>

<nav class='main-nav'>
<div class='container'><ul>
<li class=""home""><a href=""/profile"" title=""Profile""><i class='icon-home'></i>
</a></li><li class=""""><a href=""/profile/account"">Account</a>
</li><li class=""""><a href=""/profile/notifications"">Notifications</a>
</li><li class=""active""><a href=""/keys"">SSH Keys
<span class='count'>1</span>
</a></li><li class=""""><a href=""/profile/design"">Design</a>
</li><li class=""""><a href=""/profile/history"">History</a>
</li></ul>
</div>
</nav>
<div class='container'>
<div class='content'><h3 class='page_title'>Add an SSH Key</h3>
<hr>
<div>
<form accept-charset=""UTF-8"" action=""/keys"" class=""new_key"" id=""new_key"" method=""post"">   <div style=""margin:0;padding:0;display:inline""><input name=""utf8"" type=""hidden""   value=""&#x2713;"" /><input name=""authenticity_token"" type=""hidden""   value=""9SLFk6AwlsN2FoyO8xPY+M1hEbKfqlLTQ4CSDVc4efE="" /></div><div class='clearfix'>
<label for=""key_title"">Title</label>
<div class='input'><input id=""key_title"" name=""key[title]"" size=""30"" type=""text"" />   </div>
</div>
<div class='clearfix'>
<label for=""key_key"">Key</label>
<div class='input'>
<textarea class=""xxlarge thin_area"" cols=""40"" id=""key_key"" name=""key[key]"" rows=""20"">
</textarea>
<p class='hint'>
Paste your public key here. Read more about how generate it
<a href=""/help/ssh"">here</a>
</p>
</div>
</div>
<div class='actions'>
<input class=""btn btn-save"" name=""commit"" type=""submit"" value=""Save"" />
<a href=""/keys"" class=""btn btn-cancel"">Cancel</a>
</div>
</form>

</div>

<script>
  $('#key_key').on('keyup', function(){
    var title = $('#key_title'),
        val      = $('#key_key').val(),
        key_mail = val.match(/([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+|\.[a-zA-Z0-9._-]+)/gi);

    if( key_mail && key_mail.length > 0 && title.val() == '' ){
      $('#key_title').val( key_mail );
    }
  });
</script>
</div>
</div>
</body>
</html>

",23k,"
            3
        ","['\nYou\'re looking for the wrong element. getElementsByTagName() is looking for the actual tag name (input), not the value of the tag\'s name-attribute (commit). Also, getElementsByTagName() returns a collection of COM objects. Even if no matching tag is found, the method will still return a collection (with 0 elements). You need to either check the Length property and then access the first element of the collection:\n$commit = $doc.getElementsByTagName(""input"")\nif ($commit.Length -gt 0) {\n  $commit.item(0).click()\n}\n\nor filter the element with the name you\'re looking for from the collection:\n$commit = $doc.getElementsByTagName(""input"") | ? { $_.name -eq ""commit"" }\nif ($commit) { $commit.click() }\n\n', '\nI could not access the url you had listed above so I used the MIT website to show you an example of how can this be done.\n# setup\n$ie = New-Object -com InternetExplorer.Application \n$ie.visible=$true\n\n$ie.navigate(""http://web.mit.edu/"") \nwhile($ie.ReadyState -ne 4) {start-sleep -m 100} \n\n$termsField = $ie.document.getElementsByName(""terms"")\n@($termsField)[0].value =""powershell""\n\n\n$submitButton = $ie.document.getElementsByTagName(""input"") \nForeach($element in $submitButton )\n{\n    #look for this field by value this is the field(look for screenshot below) \n    if($element.value -eq ""Search""){\n    Write-Host $element.click()\n    }\n}\n\n    Start-Sleep 10\n\n\n']",https://stackoverflow.com/questions/17721295/powershell-how-to-click-a-submit-type-input,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to find_element_by_link_text while having: NoSuchElement Exception?,"
This question has been asked over and over again - and in-spite of trying all the hacks I still can't seem to figure out what's wrong.
I tried increasing the implicitly_wait to 30 (and even increased it upto 100) - yet it did not work. 
Use case -: I am trying to create a list that wil populate all the items in the page here, as a base case - and I intend to bind this to a mini-module that I already have with scrapy which has all (pages with similar web elements) crawled links - so essentially will be building the whole pipeline, post I am done with this.
###My source code - generated via Selenium IDE, exported to a Python webdriver and manipulated a little later ###

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.support.wait import WebDriverWait
import unittest, time, re

class Einstein(unittest.TestCase):
    def setUp(self):
        self.driver = webdriver.Firefox()
        self.driver.implicitly_wait(30)
        self.base_url = ""http://shopap.lenovo.com/in/en/laptops/""
        self.verificationErrors = []
        self.accept_next_alert = True

    def test_einstein(self):
        driver = self.driver
        driver.get(self.base_url)
        print driver.title
        driver.find_element_by_link_text(""T430"").click()
        print driver.title
#       driver.find_element_by_xpath(""id('facetedBrowseWrapper')/div/div/div[1]/div[2]/ul[1]/li[1]/a"").click()
        driver.find_element_by_xpath(""//div[@id='subseries']/div[2]/div/p[3]/a"").click()
        print driver.title
       # driver.find_element_by_xpath(""//div[@id='subseries']/div[2]/div/p[3]/a"").click()
        try: self.assertEqual(""Thinkpad Edge E530 (Black)"", driver.find_element_by_link_text(""Thinkpad Edge E530 (Black)"").text)
        except AssertionError as e: self.verificationErrors.append(str(e))
       # Everything ok till here


        #**THE CODE FAILS HERE**#
        laptop1 = driver.find_element_by_link_text(""Thinkpad Edge E530 (Black)"").text
        print laptop1
        price1 = driver.find_element_by_css_selector(""span.price"").text
        print price1
        detail1 = self.is_element_present(By.CSS_SELECTOR, ""div.desc.std"")
        print detail1

            def is_element_present(self, how, what):
        try: self.driver.find_element(by=how, value=what)
        except NoSuchElementException, e: return False
        return True

    def is_alert_present(self):
        try: self.driver.switch_to_alert()
        except NoAlertPresentException, e: return False
        return True

    def close_alert_and_get_its_text(self):
        try:
            alert = self.driver.switch_to_alert()
            alert_text = alert.text
            if self.accept_next_alert:
                alert.accept()
            else:
                alert.dismiss()
            return alert_text
        finally: self.accept_next_alert = True

    def tearDown(self):
        self.driver.quit()
        self.assertEqual([], self.verificationErrors)

if __name__ == ""__main__"":
    unittest.main()


Errors & output :
ekta@ekta-VirtualBox:~$ python einstein.py
Laptops & Ultrabooks | Lenovo (IN)
ThinkPad T430 Laptop PC for Business Computing | Lenovo (IN)
Buy Lenovo Thinkpad Laptops | Lenovo Thinkpad Laptops Price India
E
======================================================================
ERROR: test_einstein (__main__.Einstein)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""einstein.py"", line 27, in test_einstein
    try: self.assertEqual(""Thinkpad Edge E530 (Black)"", driver.find_element_by_link_text(""Thinkpad Edge E530 (Black)"").text)
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/webdriver.py"", line 246, in find_element_by_link_text
    return self.find_element(by=By.LINK_TEXT, value=link_text)
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/webdriver.py"", line 680, in find_element
    {'using': by, 'value': value})['value']
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/webdriver.py"", line 165, in execute
    self.error_handler.check_response(response)
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/errorhandler.py"", line 158, in check_response
    raise exception_class(message, screen, stacktrace)
NoSuchElementException: Message: u'Unable to locate element: {""method"":""link text"",""selector"":""Thinkpad Edge E530 (Black)""}' ; Stacktrace: 
    at FirefoxDriver.prototype.findElementInternal_ (file:///tmp/tmphli5Jg/extensions/fxdriver@googlecode.com/components/driver_component.js:8444)
    at fxdriver.Timer.prototype.setTimeout/<.notify (file:///tmp/tmphli5Jg/extensions/fxdriver@googlecode.com/components/driver_component.js:386) 

----------------------------------------------------------------------
Ran 1 test in 79.348s

FAILED (errors=1)

Questions & comments: 

If you are answering this question - please mention why this specific ""find_element_by_link_text"" does not work. 
(Very Basic) In the GUI of my selenium IDE -> Show all available commands - why dont I see the css (find_element_by_css_selector) for all the web elements - is there a way to force feed an element to be read as a CSS selector ?
In case you suggest using some other locator - please mention if that will be consistent way to fetch elements, given (1) 
Does assert work to capture the exceptions and ""move on"" - since even after trying ""verify"" , ""assert"" loops, I still cant fetch this  ""find_element_by_link_text""
I tried using Xpath to build this ""element"" , but in the view Xpath (in firefox) - I see nothing, to clue why that happens (Of course I removed the namespace "":x"" )

Other things I tried apart from implicity_wait(30):
find_element_by_partial_link(鈥淭hinkpad鈥? and appending Unicode to this (wasn鈥檛 sure if it was reading the brackets ( , driver.find_element_by_link_text(u""Thinkpad Edge E530 (Black)"").text, still did not work.


Related questions:

How to use find_element_by_link_text() properly to not raise NoSuchElementException?
NoSuchElement Exception using find_element_by_link_text when implicitly_wait doesn't work?

",13k,"
            2
        ","['\nIt happened to me before that the find_element_by_link_text method sometimes works and sometimes doesn\'t work; even in a single case. I think it\'s not a reliable way to access elements; the best way is to use find_element_by_id. \nBut in your case, as I visit the page, there is no id to help you. Still you can try find_elements_by_xpath in 3 ways:\n1- Accessing title: find_element_by_xpath[""//a[contains(@title = \'T430\')]""] \n2- Accessing text: find_element_by_xpath[""//a[contains(text(), \'T430\')]""]\n3- Accessing href: find_element_by_xpath[""//a[contains(@href = \'http://www.thedostore.com/laptops/thinkpad-laptops/thinkpad-t430-u-black-627326q.html\')]""].\nHope it helps.\n', '\nNoSuchElementException is thrown when the element could not be found.\nIf you encounter this exception, please check the followings:\n\nCheck your selector used in your find_by...\nElement may not yet be on the screen at the time of the find operation.\n\nIf webpage is still loading, check for selenium.webdriver.support.wait.WebDriverWait() and write a wait wrapper  to wait for an element to appear.\nTroubleshooting and code samples\nYou can add breakpoint just before your failing line pdb.set_trace() (don\'t forget to import pdb), then run your test and once your debugger stops, then do the following tests.\n\nYou could try:\ndriver.find_element_by_xpath(u\'//a[text()=""Foo text""]\')\n\ninstead. This is more reliable test, so if this would work, use it instead.\nIf above won\'t help, please check if your page has been loaded properly via:\n(Pdb) driver.execute_script(""return document.readyState"")\n\'complete\'\n\nSometimes when the page is not loaded, you\'re actually fetching the elements from the old page. But even though, readyState could still indicate the state of the old page (especially when using click()). Here is how this is explained in this blog:\n\nSince Selenium webdriver has become more advanced, clicks are much more like ""real"" clicks, which has the benefit of making our tests more realistic, but it also means it\'s hard for Selenium to be able to track the impact that a click has on the browsers\' internals -- it might try to poll the browser for its page-loaded status immediately after clicking, but that\'s open to a race condition where the browser was multitasking, hasn\'t quite got round to dealing with the click yet, and it gives you the .readyState of the old page.\n\nIf you think this is happening because the page wasn\'t loaded properly, the ""recommended"" (however still ugly) solution is an explicit wait:\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait \nfrom selenium.webdriver.support import expected_conditions\n\nold_value = browser.find_element_by_id(\'thing-on-old-page\').text\nbrowser.find_element_by_link_text(\'my link\').click()\nWebDriverWait(browser, 3).until(\n    expected_conditions.text_to_be_present_in_element(\n        (By.ID, \'thing-on-new-page\'),\n        \'expected new text\'\n    )\n)\n\nThe naive attempt would be something like this:\ndef wait_for(condition_function):\n    start_time = time.time()\n    while time.time() < start_time + 3:\n        if condition_function():\n            return True\n        else:\n            time.sleep(0.1)\n    raise Exception(\n        \'Timeout waiting for {}\'.format(condition_function.__name__)\n    )\n\n\ndef click_through_to_new_page(link_text):\n    browser.find_element_by_link_text(\'my link\').click()\n\n    def page_has_loaded():\n        page_state = browser.execute_script(\n            \'return document.readyState;\'\n        ) \n        return page_state == \'complete\'\n\n    wait_for(page_has_loaded)\n\nAnother, better one would be (credits to @ThomasMarks):\ndef click_through_to_new_page(link_text):\n    link = browser.find_element_by_link_text(\'my link\')\n    link.click()\n\n    def link_has_gone_stale():\n        try:\n            # poll the link with an arbitrary call\n            link.find_elements_by_id(\'doesnt-matter\') \n            return False\n        except StaleElementReferenceException:\n            return True\n\n    wait_for(link_has_gone_stale)\n\nAnd the final example includes comparing page ids as below (which could be bulletproof):\nclass wait_for_page_load(object):\n\n    def __init__(self, browser):\n        self.browser = browser\n\n    def __enter__(self):\n        self.old_page = self.browser.find_element_by_tag_name(\'html\')\n\n    def page_has_loaded(self):\n        new_page = self.browser.find_element_by_tag_name(\'html\')\n        return new_page.id != self.old_page.id\n\n    def __exit__(self, *_):\n        wait_for(self.page_has_loaded)\n\nAnd now we can do:\nwith wait_for_page_load(browser):\n    browser.find_element_by_link_text(\'my link\').click()\n\nAbove code samples are from Harry\'s blog.\nHere is the version proposed by Tommy Beadle (by using staleness approach):\nimport contextlib\nfrom selenium.webdriver import Remote\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support.expected_conditions import staleness_of\n\nclass MyRemote(Remote):\n    @contextlib.contextmanager\n    def wait_for_page_load(self, timeout=30):\n        old_page = self.find_element_by_tag_name(\'html\')\n        yield\n        WebDriverWait(self, timeout).until(staleness_of(old_page))\n\nIf you think it isn\'t about page load, double check if your element isn\'t in iframe or different window. If so, you\'ve to switch to it first. To check list of available windows, run: driver.window_handles.\n\n', '\nFrom viewing the source of the page that you provided a link to, it seems you are using an incorrect selector.\nYou should use instead find_elements_by_link_text(u\'text here\')[0] to select the first occurrence instead as there seems to be the potential for multiple links with the same link text.\nSo instead of:\nself.assertEqual(""Thinkpad Edge E530 (Black)"", driver.find_element_by_link_text(""Thinkpad Edge E530 (Black)"").text)\n\nYou should use:\nself.assertEqual(""Thinkpad Edge E530 (Black)"", driver.find_elements_by_link_text(""Thinkpad Edge E530 (Black)"")[0].text)\n\n', '\nSolution posted by OP:\nHack 1: Instead of identifying the element as a text-link, I identified the ""bigger frame"" in which this element was present. \nitemlist_1 = driver.find_element_by_css_selector(""li.item.first"").text\nThis will give the whole item along with the name, price and detail (and the unwanted add to cart and compare"" \nSee the attached image for more .\n\nHack 2: I found that the ""Buy Now"" which was an image element with xPath (driver.find_element_by_xpath(""//div[@id=\'subseries\']/div[2]/div/p[3]/a"").click()\n, in the code above) , could be made to click/identified faster if I added the following line, before finding this by xpath. I think this sort of narrows down where the Webdriver is looking for an element.  This is what I added "" driver.find_element_by_css_selector(""#subseries"").text""\nThis must have decreased my wait by at least 20 seconds, on that page .Hope that helps.\n']",https://stackoverflow.com/questions/18023678/how-to-find-element-by-link-text-while-having-nosuchelement-exception,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Puppeteer does not change selector,"
I'm trying to automate the task of querying for data on this site using Puppeteer. So I need to select the dataset (Daily Summaries, 1st option), then select location type (State, 3rd option), then select state (Alaska, 2nd option). The problem is my code does not change to the next table. So instead of selecting the 3rd option (State) after selecting the 1st option in dataset (Daily Summaries), it just selects the 3rd option but in dataset table again! I am new to Puppeteer so I don't really know what to do with this. Any help is appreciated.
Below is my code:


const puppeteer = require('puppeteer');
(async () => {
  const browser = await puppeteer.launch({headless:false})
  const page = await browser.newPage()

  const navigationPromise = page.waitForNavigation()

  await page.goto('https://www.ncdc.noaa.gov/cdo-web/datatools/selectlocation')

  await page.waitForSelector('.selectLocationFilters > .datasetContainer > .slideElement > #datasetSelect > option:nth-child(1)')
  await page.click('.selectLocationFilters > .datasetContainer > .slideElement > #datasetSelect > option:nth-child(1)')

  await page.select('.inset #locationCategorySelect', '')

  await page.waitForSelector('.selectLocationFilters > .locationCategoryContainer > .locationCategoryFilter > #locationCategorySelect > option:nth-child(3)')
  await page.click('.selectLocationFilters > .locationCategoryContainer > .locationCategoryFilter > #locationCategorySelect > option:nth-child(3)')

  await page.select('.inset #selectedState', '')

  await page.waitForSelector('.selectLocationFilters > .locationContainer > .stateFilter > #selectedState > option:nth-child(2)')
  await page.click('.selectLocationFilters > .locationContainer > .stateFilter > #selectedState > option:nth-child(2)')

  await browser.close()
})()


This is what I want. Dataset -> Location type -> State Alaska. Instead the code keeps selecting only in the Dataset table.

",869,"
            1
        ","['\nThe problem you have there is that CSS transitions are preventing you from clicking those elements. One possible solution would be disabling all CSS animations on the page.\nYou can add that after the goto call: \n\nawait page.addStyleTag({ content : `\n    *,\n    *::after,\n    *::before {\n        transition-delay: 0s !important;\n        transition-duration: 0s !important;\n        animation-delay: -0.0001s !important;\n        animation-duration: 0s !important;\n        animation-play-state: paused !important;\n        caret-color: transparent !important;\n    }`})\n\n\n']",https://stackoverflow.com/questions/61647401/puppeteer-does-not-change-selector,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using internetexplorer object what is the correct way to wait for an ajax response?,"
I tried to upload a file to a sharepoint library, my code fails to properly detect  if ie is still waiting for an ajax response or not. What is the proper way to do this ?
[void] [System.Reflection.Assembly]::LoadWithPartialName(""'Microsoft.VisualBasic"")
[void] [System.Reflection.Assembly]::LoadWithPartialName(""'System.Windows.Forms"")

function wait4IE($ie=$global:ie){
    while ($ie.busy -or $ie.readystate -lt 4){start-sleep -milliseconds 200}
}

$global:ie=new-object -com ""internetexplorer.application""
$ie.visible=$true
[Microsoft.VisualBasic.Interaction]::AppActivate(""internet explorer"")

# open EDM
$ie.navigate(""https://xxx.sharepoint.com/sites/site1/Forms/AllItems.aspx"")
wait4IE

# click on  the button to display the form
$ie.Document.getElementById(""QCB1_Button2"").click()

wait4IE

the rest of the code is executed, but the uploading form is not shown yet.
How to wait for the display of the form  ?
I also tried this (should wait untill a button of the upload form is not find), but it never ends ...
while( $ie.document.getElementById(""ctl00_PlaceHolderMain_UploadDocumentSection_ctl05_InputFile"") -eq $null){
        echo ""waiting ...""
        wait4IE
}


Update :
I think I've found the problem : the form is open in an iframe :
<iframe id=""DlgFrame0be35d71-22cb-47bd-bbf0-44c97db61fd6"" class=""ms-dlgFrame"" src=""https://.../Upload.aspx?List={45085FA0-3AE3-4410-88AD-3E80A218FC0C}&amp;RootFolder=&amp;IsDlg=1"" frameborder=""0"" style=""width: 592px; height: 335px;""></iframe>

But now, How to get the good frame number ?
PS>($ie.Document.frames.Item(4).document.body.getElementsbytagname(""input"") |?{$_.type -eq 'file'}).id
ctl00_PlaceHolderMain_UploadDocumentSection_ctl05_InputFile

moreover it seems i can access the frame content with getElementsByTagName, but not with getElementById ....?I still don't understand why .:
PS>$ie.Document.frames.Item(4).document.body.getElementById('ctl00_PlaceHolderMain_UploadDocumentSection_ctl05_InputFile
    ')
    脡chec lors de l'appel de la m茅thode, car [System.__ComObject] ne contient pas de m茅thode nomm茅e 芦聽getElementById聽禄.
    Au caract猫re Ligne:1 : 1
    + $ie.Document.frames.Item(4).document.body.getElementById('ctl00_PlaceHolderMain_ ...
    + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        + CategoryInfo          : InvalidOperation : (getElementById:String) [], RuntimeException
        + FullyQualifiedErrorId : MethodNotFound

",550,"
            0
        ","['\nok here is how I\'ve done :\nthe trick was to look each iframes, select the one with the correct location\nfor($i=0;$i -lt $ie.Document.frames.length;$i++){\n            if( $ie.Document.frames.item($i).location.href -match \'upload.aspx\' ){ $frm=$ie.Document.frames.item($i)}\n    }\n\nthen wait for my input to show \nwhile( ($frm.document.body.getElementsbytagname(""input"") |?{$_.type -eq \'file\'}) -eq $null){\n    echo ""waiting ...""\n    start-sleep -milliseconds 100\n}\n\n']",https://stackoverflow.com/questions/29209227/using-internetexplorer-object-what-is-the-correct-way-to-wait-for-an-ajax-respon,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do I install Geckodriver?,"
I am attempting to work with Selenium in Python. However, I do not know what to do given the below from https://pypi.python.org/pypi/selenium
Selenium requires a driver to interface with the chosen browser. Firefox, for example, requires geckodriver, which needs to be installed before the below examples can be run. Make sure it鈥檚 in your PATH, e.g., place it in /usr/bin or /usr/local/bin.
I am running windows 7 32bit. I found geckodriver here: https://github.com/mozilla/geckodriver/releases
I have mostly used the Anaconda distribution of Python to work with excel so I do not know what is a ""PATH""
Thanks,  
UPDATE:
I updated the PATH as shown in the comments. Here is the full error traceback. 

Microsoft Windows [Version 6.1.7601]
  Copyright (c) 2009 Microsoft Corporation.  All rights reserved.
C:\Users\user1>python
Python 3.5.2 |Anaconda 4.2.0 (32-bit)| (default, Jul  5 2016, 11:45:57) [MSC v.1
  900 32 bit (Intel)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
from selenium import webdriver
driver = webdriver.Firefox()

Traceback (most recent call last):
File """", line 1, in 
File ""C:\Users\user1\AppData\Local\Continuum\Anaconda3\lib\site
  -packages\selenium-2.53.6-py3.5.egg\selenium\webdriver\firefox\webdriver.py"", li
  ne 80, in init
      self.binary, timeout)
File ""C:\Users\user1\AppData\Local\Continuum\Anaconda3\lib\site
  -packages\selenium-2.53.6-py3.5.egg\selenium\webdriver\firefox\extension_connect
  ion.py"", line 52, in init
      self.binary.launch_browser(self.profile, timeout=timeout)
File ""C:\Users\user1\AppData\Local\Continuum\Anaconda3\lib\site
  -packages\selenium-2.53.6-py3.5.egg\selenium\webdriver\firefox\firefox_binary.py
  "", line 67, in launch_browser
      self._start_from_profile_path(self.profile.path)
File ""C:\Users\user1\AppData\Local\Continuum\Anaconda3\lib\site
  -packages\selenium-2.53.6-py3.5.egg\selenium\webdriver\firefox\firefox_binary.py
  "", line 90, in _start_from_profile_path
      env=self._firefox_env)
File ""C:\Users\user1\AppData\Local\Continuum\Anaconda3\lib\subp
  rocess.py"", line 947, in init
      restore_signals, start_new_session)
File ""C:\Users\user1\AppData\Local\Continuum\Anaconda3\lib\subp
  rocess.py"", line 1224, in _execute_child
      startupinfo)
FileNotFoundError: [WinError 2] The system cannot find the file specified

",115k,"
            34
        ","['\n\nYou can download the geckodriver\nunzip it\nCopy that .exe file and put your into python parent folder (e.g., C:\\Python34)\nwrite your scripts.\n\nIt will execute successfully.\n', '\nThere is an easy way to install Geckodriver:\n\nInstall webdrivermanager with pip\npip install webdrivermanager\nInstall the driver for Firefox and Chrome\nwebdrivermanager firefox chrome --linkpath /usr/local/bin\nOr install the driver only for Firefox\nwebdrivermanager firefox --linkpath /usr/local/bin\nOr install the driver only for Chrome\nwebdrivermanager chrome --linkpath /usr/local/bin\n\n', ""\nThe easiest way if you are on windows:\ndriver = webdriver.Firefox(executable_path=r'[Your path]\\geckodriver.exe')\n\nExample:\ndriver = webdriver.Firefox(executable_path=r'D:\\geckodriver.exe')\n\n"", '\nFor Linux/Ubuntu:\nThe following simple installation worked for me:\nsudo apt install firefox-geckodriver\n\nNo additional driver installation was required.\nReference: https://github.com/timgrossmann/InstaPy/issues/5282#issuecomment-666283451\nFor windows:\nFollow the instructions here: http://www.learningaboutelectronics.com/Articles/How-to-install-geckodriver-Python-windows.php\n', ""\nIf you're on macOS/Apple, you can use Homebrew:\nbrew install geckodriver\nSee this related question\n"", '\nSome options, choose 1:\n\nMove the exe file to a folder in your PATH environment variable.\nUpdate PATH to have the directory that contains the exe.\nExplicitly override os.environ[""webdriver.gecko.driver""]\n\nbasically drag and drop the geckodriver someplace where you have your executables, you should then be able to open the command line and use it.\n/bin on linux, and C:\\Program Files\nsee:\n\nhttps://github.com/SeleniumHQ/selenium/issues/2672\nhttps://superuser.com/questions/124239/what-is-the-default-path-environment-variable-setting-on-fresh-install-of-window\nhttps://askubuntu.com/questions/27213/what-is-the-equivalent-to-the-windows-program-files-folder-where-do-things-g\n\nspecifically the explanations on how the driver is seen, \nwhere it can be put ,and how to modify the way selenium finds it. \n', ""\nFor me this worked (Windows 10, Firefox browser):\nfrom selenium import webdriver\ndriver = webdriver.Firefox(executable_path=r'C:\\......YOUR_PATH.......\\geckodriver.exe')\ndriver.get('http://EXAMPLE_URL.com')\n\n"", '\nMeanwhile for Win10 you can simply use\nfrom selenium import webdriver\nfrom webdriver_manager.firefox import GeckoDriverManager\n\ndriver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\ndriver.get(""https://www.google.com"")\n\nThis will download the geckodrive prior to its first use and store it at the appropriate location. No need to set any paths explicitly.\n', '\nFor Python 3 - Selenium plus webdriver for Firefox;\n\nOpen up Command line\nEnter Pip install -U Selenium   (The -U will upgrade it to the latest Selenium version.)\nThis example selenium is already installed\nGo to https://github.com/mozilla/geckodriver/releases\nAt the time of writing I chose the latest version which was simply the version listed at the top of the page. For me it was v0.24.0.\nScroll down to assets and then click and download the correct driver. For windows it will be a zip file. Most likely 64bit. \nDownload the webdriver by clicking on the link\n5.Right click on the downloaded file and unzip the file. \nCopy and paste the file to somewhere in your python directory. e.g. If I installed Python in C:\\Python\\Python37 I would paste the file in there so gecko would be located in C:\\Python\\Python37\\geckodriver-v0.24.0-win64\n\nCopying the file path of the geckodriver\n\nInside that folder you just copied will be the geckodriver.exe\nIn Windows 10, click the ""windows"" button and search ""environment variables"" \nFind environment variables\nOR find it using these instructions; https://www.computerhope.com/issues/ch000549.htm\nClick on the ""environment variables"" box at the bottom right hand corner. \nIn the bottom box ""System Variables"" highlight the ""Path"" variable like so\nAdding environment variable Path\nPress edit and then add the entry at the bottom of the list. Copy and paste the location where the geckodriver.exe file lives. For me it was C:\\Python\\Python37\\geckodriver-v0.24.0-win64 (or where you copied the file in step 6)\nAdding gecko to the windows PATH\n\n', '\nto avoid links becoming out of date, please refer the soource.\nhttps://github.com/mozilla/geckodriver\nfollow the readme instructions to the ""Downloads"" > ""Releases"" link.\n']",https://stackoverflow.com/questions/41190989/how-do-i-install-geckodriver,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get body / json response from XHR request with Puppeteer [duplicate],"






This question already has answers here:
                        
                    



Puppeteer: How to listen to a specific response?

                                (5 answers)
                            

Closed 4 months ago.



I want to get the JSON data from a website I'm scraping with Puppeteer, but I can't figure how to get the body of the request back. Here's what I've tried:
const puppeteer = require('puppeteer')
const results = [];
(async () => {
    const browser = await puppeteer.launch({
        headless: false
    })
    const page = await browser.newPage()
    await page.goto(""https://capuk.org/i-want-help/courses/cap-money-course/introduction"", {
        waitUntil: 'networkidle2'
    });

    await page.type('#search-form > input[type=""text""]', 'bd14ew')  
    await page.click('#search-form > input[type=""submit""]')

    await page.on('response', response => {    
        if (response.url() == ""https://capuk.org/ajax_search/capmoneycourses""){
            console.log('XHR response received'); 
            console.log(response.json()); 
        } 
    }); 
})()

This just returns a promise pending function. Any help would be great.
",27k,"
            22
        ","['\nAs response.json returns a promise we need to await it.\npage.on(\'response\', async (response) => {    \n    if (response.url() == ""https://capuk.org/ajax_search/capmoneycourses""){\n        console.log(\'XHR response received\'); \n        console.log(await response.json()); \n    } \n}); \n\n']",https://stackoverflow.com/questions/56689420/how-to-get-body-json-response-from-xhr-request-with-puppeteer,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connecting Browsers in Puppeteer,"
Is it possible to connect a browser to puppeteer without instantiating it in puppeteer? For example, running an instance of chromium like a regular user and then connecting that to an instance of puppeteer in code?
",16k,"
            15
        ","['\nThe answer is Yes and No.\nYou can connect to an existing using the connect function:\nconst browserURL = \'http://127.0.0.1:21222\';\nconst browser = await puppeteer.connect({browserURL});\n\nBut, if you want to use those 2 lines you need to launch Chrome with the ""--remote-debugging-port=21222 argument.\n', '\nI believe you need to connect to an address ended with an id:\nws://127.0.0.1:9222/devtools/browser/{id}\n\nWhen you launch Chrome with --remote-debugging-port, you\'ll see something like\n/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222   [17:57:55]\n\n...\n\nDevTools listening on ws://127.0.0.1:9222/devtools/browser/44b3c476-5524-497e-9918-d73fa39e40cf\n\nThe address on the last line is what you need, i.e.\nconst browser = await puppeteer.connect({\n    browserWSEndpoint: ""ws://127.0.0.1:9222/devtools/browser/44b3c476-5524-497e-9918-d73fa39e40cf""\n});\n\n']",https://stackoverflow.com/questions/55096771/connecting-browsers-in-puppeteer,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get children of elements by Puppeteer,"
I understand that puppeteer get its own handles rather than standard DOM elements, but I don't understand why I cannot continue the same query by found elements as
const els = await page.$$('div.parent');

for (let i = 0; i < els.length; i++) {
    const img = await els[i].$('img').getAttribute('src');
    console.log(img);
    const link = await els[i].$('a').getAttribute('href');
    console.log(link);
}

",24k,"
            12
        ","[""\nProblem\nThe element handles are necessary as an abstraction layer between the Node.js and browser runtime. The actual DOM elements are not sent to the Node.js environment.\nThat means when you want to get an attribute from an element, there has to be data transferred to the browser (which DOM element to use) and back (the result).\nSolution\nTherefore, the result from await els[i].$('img') is not really the DOM element, but only a wrapper that links to the element in the browser environment. To get the attribute, you have to use a function like elementHandle.$eval:\nconst imgSrc = await els[i].$eval('img', el => el.getAttribute('src'));\n\nThis runs the querySelector function on the given element and executes the given function to return its attribute.\n"", ""\nYou can use function $eval\nconst els = await page.$$('div.parent');\n\nfor (let i = 0; i < els.length; i++) {\n    const img = await els[i].$eval('img', i => i.getAttribute('src'));\n    console.log(img);\n    const link = await els[i].$eval('a', a => a.getAttribute('href'));\n    console.log(link);\n}\n\n""]",https://stackoverflow.com/questions/55659097/how-to-get-children-of-elements-by-puppeteer,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to use installed version of chrome in Playwright?,"
I want to use chrome instead of chromium. I can achieve the same in puppeteer by providing executable path. In playwright it doesn't work as browser type argument supports only 'chromium, webkit, firefox'


const { chromium } = require('playwright');
(async () => {
    const browser = await chromium.launch({
        headless: false,
        executablePath: '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome',
    });
    const context = await browser.newContext();
    const page = await context.newPage();
    await page.goto('http://whatsmyuseragent.org/');
    await page.screenshot({ path: `example-${browserType}.png` });
})();


",17k,"
            10
        ","['\nYou need to pick one of those flavors. But once you pick the browser type Chromium, you will still be able to pass an executablePath to the launch function.\n', '\nIn 1.19 you can use chrome.\nbrowser = playwright.chromium.launch(channel=""chrome"")\n\nor you can simply put it in your playwright configuration file like:\n////\n    use: {\n        headless: true,\n        viewport: { width: 1600, height: 1000},\n        ignoreHTTPSErrors: true,\n        trace: \'on\',\n        screenshot: \'on\',\n        channel: ""chrome"",\n        video: \'on\'\n    },\n    ////\n\nMore on https://playwright.dev/python/docs/browsers\n', ""\nYou can specify browser path in config option\n////\nuse: {\n    headless: true,\n    viewport: { width: 1600, height: 1000},\n    channel: 'chrome',\n    launchOptions: {\n        executablePath: '/path/to/the/browser',\n    },\n},\n\n""]",https://stackoverflow.com/questions/62281859/how-to-use-installed-version-of-chrome-in-playwright,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to open the new tab using Playwright (ex. click the button to open the new section in a new tab),"
I am looking for a simpler solution to a current situation. For example, you open the google (any another website) and you want BY CLICK on the button (ex. Gmail) - open this page in the new tab using Playwright.
let browser, page, context;
describe('Check the main page view', function () {
    before(async () => {
        for (const browserType of ['chromium']) {
            browser = await playwright[browserType].launch({headless: false});
            context = await browser.newContext();
            page = await context.newPage();
            await page.goto(baseUrl);
        }
    });
    after(async function () {
        browser.close();
    });
    
        await page.click(tax);
        const taxPage = await page.getAttribute(taxAccount, 'href');

        const [newPage] = await Promise.all([
        context.waitForEvent('page'),
        page.evaluate((taxPage) => window.open(taxPage, '_blank'), taxPage)]);

        await newPage.waitForLoadState();
        console.log(await newPage.title());

",15k,"
            8
        ","['\nit(\'Open a new tab\', async function () {\n     await page.click(button, { button: ""middle"" });\n     await page.waitForTimeout(2000); //waitForNavigation and waitForLoadState do not work in this case\n     let pages = await context.pages();\n     expect(await pages[1].title()).equal(\'Title\');\n\n', '\nYou could pass a modifier to the click function. In macos it would be Meta because you\'d open in a new tab with cmd+click. In windows it would be Control.\nconst browser = await playwright[""chromium""].launch({headless : false});\nconst page = await browser.newPage();\nawait page.goto(\'https://www.facebook.com/\');\nvar pagePromise = page.context().waitForEvent(\'page\', p => p.url() ==\'https://www.messenger.com/\');\nawait page.click(\'text=Messenger\', { modifiers: [\'Meta\']});\nconst newPage = await pagePromise;\nawait newPage.bringToFront();\nawait browser.close();\n\n', '\nIn my case i am clicking on link in a pop up like (ctrl + click on link) then it opens new tab and work on that new tab\nawait page.click(\'#open\')\nconst [newTab] = await Promise.all([\n    page.waitForEvent(\'popup\'),\n    await page.keyboard.down(\'Control\'),\n    await page.frameLocator(\'//iframe[@title=""New tab.""]\').locator(\'a\').click(), // in popup\n    await page.keyboard.up(\'Control\'),\n    console.log(""clicked on link"")\n]);\nawait newTab.waitForFunction(()=>document.title === \'new tab title\')\nawait newTab.fill(\'#firstname\')\nawait newTab.close() // close the current tab\nawait page.click(\'#exitbutton\') //back to parent tab and work on it\n....\n....\nawait page.close() // close the parent tab\n\n']",https://stackoverflow.com/questions/64277178/how-to-open-the-new-tab-using-playwright-ex-click-the-button-to-open-the-new-s,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to scroll down in an instagram pop-up frame with Selenium,"
I have a python script using selenium to go to a given Instagram profile and iterate over the user's followers. On the instagram website when one clicks to see the list of followers, a pop-up opens with the accounts listed (here's a screenshot of the site)
However both visually and in the html, only 12 accounts are shown. In order to see more one has to scroll down, so I tried doing this with the Keys.PAGE_DOWN input.
from selenium import webdriver
from selenium.common.exceptions         import TimeoutException
from selenium.webdriver.support.ui      import WebDriverWait 
from selenium.webdriver.support         import expected_conditions as EC
from selenium.webdriver.chrome.options  import Options
from selenium.webdriver.common.keys     import Keys
import time 

...
username = 'Username'
password = 'Password'
message  = 'blahblah'
tryTime  = 2

#create driver and log in
driver = webdriver.Chrome()
logIn(driver, username, password, tryTime)

#gets rid of preference pop-up
a = driver.find_elements_by_class_name(""HoLwm"")
a[0].click()

#go to profile
driver.get(""https://www.instagram.com/{}/"".format(username))

#go to followers list
followers = driver.find_element_by_xpath(""//a[@href='/{}/followers/']"".format(username))
followers.click()
time.sleep(tryTime) 

#find all li elements in list
fBody  = driver.find_element_by_xpath(""//div[@role='dialog']"")
fBody.send_keys(Keys.PAGE_DOWN) 

fList  = fBody.find_elements_by_tag(""li"")
print(""fList len is {}"".format(len(fList)))

time.sleep(tryTime)

print(""ended"")
driver.quit()

When I try to run this I get the following error:
Message: unknown error: cannot focus element

I know this is probably because I'm using the wrong element for fBody, but I don't know which would be the right one. Does anybody know which element I should send the PAGE_DOWN key to, or if there is another way to load  the accounts? 
Any help is much appreciated!
",10k,"
            7
        ","['\nthe element you\'re looking is //div[@class=\'isgrP\'] and Keys.PAGE_DOWN is not work for scrollable div. \nYour variable fList hold old value, you need to find again the elements after scroll.\n#find all li elements in list\nfBody  = driver.find_element_by_xpath(""//div[@class=\'isgrP\']"")\nscroll = 0\nwhile scroll < 5: # scroll 5 times\n    driver.execute_script(\'arguments[0].scrollTop = arguments[0].scrollTop + arguments[0].offsetHeight;\', fBody)\n    time.sleep(tryTime)\n    scroll += 1\n\nfList  = driver.find_elements_by_xpath(""//div[@class=\'isgrP\']//li"")\nprint(""fList len is {}"".format(len(fList)))\n\nprint(""ended"")\n#driver.quit()\n\n', '\nThe above code works fine if you add iteration (for) with range\nfor i in range(1, 4):\n            try:\n            #find all li elements in list\n            fBody  = self.driver.find_element_by_xpath(""//div[@class=\'isgrP\']"")\n            scroll = 0\n            while scroll < 5: # scroll 5 times\n                self.driver.execute_script(\'arguments[0].scrollTop = arguments[0].scrollTop + arguments[0].offsetHeight;\', fBody)\n                time.sleep(2)\n                scroll += 1\n\n            fList  = self.driver.find_elements_by_xpath(""//div[@class=\'isgrP\']//li"")\n            print(""fList len is {}"".format(len(fList)))\n\n        except Exception as e:\n            print(e, ""canot scrol"")\n\n        try:\n            #get tags with a\n            hrefs_in_view = self.driver.find_elements_by_tag_name(\'a\')\n            # finding relevant hrefs\n            hrefs_in_view = [elem.get_attribute(\'title\') for elem in hrefs_in_view]\n\n            [pic_hrefs.append(title) for title in hrefs_in_view if title not in pic_hrefs]\n            print(""Check: pic href length "" + str(len(pic_hrefs)))\n\n        except Exception as tag:\n            print(tag, ""can not find tag"")\n\nSo, the for loop makes it to possible scrol even if the while loop miss\n']",https://stackoverflow.com/questions/54173603/how-to-scroll-down-in-an-instagram-pop-up-frame-with-selenium,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wait for a particular URL in selenium,"
I have the requirement of waiting for a particular URL in website automation using Selenium in Chrome browser. 
The user will be doing online payment on our website. Fro our website user is redirected to the payment gateway. When the user completes the payment, the gateway will redirect to our website. I want to get notified redirection from gateway to our site. 
I got an example which waits for 鈥淧articular Id鈥?in the web page, here is vb.net code
driver.Url = ""http://gmail.com""
   Dim wait As New WebDriverWait(driver, TimeSpan.FromSeconds(10))
                wait.Until(Of IWebElement)(Function(d) d.FindElement(By.Id(""next"")))

This navigates to 鈥済mail.com鈥?and waits for ID 鈥渘ext鈥?on that page. Instead, I want to continue the code only when particular URL loads. 
How can I do this?
Please help me.
",25k,"
            5
        ","['\nI\'m not sure what language you\'re using, but in Java you can do something like this:\nnew WebDriverWait(driver, 20).Until(ExpectedConditions.UrlToBe(""my-url""));\n\nTo wait until your url has loaded.\nIf you cannot use the latest selenium version for some reason, you can implement the method yourself:\npublic static Func<IWebDriver, bool> UrlToBe(string url)\n{\n    return (driver) => { return driver.Url.ToLowerInvariant().Equals(url.ToLowerInvariant()); };\n}\n\n', '\nThey have added more support for expected conditions now. You would have to create a webdriver wait and expect the url to contain a value\nWebDriverWait wait = new WebDriverWait(yourDriver, TimeSpan.FromSeconds(5));\nwait.Until(ExpectedConditions.UrlContains(""/url-fragment""));\n\n']",https://stackoverflow.com/questions/37570322/wait-for-a-particular-url-in-selenium,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Why is switch_to_window() method not working for selenium webdriver in Python?,"
I am trying to switch to a newly opened window using the Python selenium webdriver. The code worked fine before but now it is showing error. Surprisingly, the switch_to_window() method is not being recognized by Python and has no declaration to go to.
def process_ebl_statements(self, account_number):

    current_window = self.driver.current_window_handle
    all_windows = self.driver.window_handles

    print(""Current window: "", current_window)
    print(""All windows: "", all_windows)
    number_of_windows = len(all_windows)
    self.driver.switch_to_window(all_windows[number_of_windows - 1])

Error details:
'WebDriver' object has no attribute 'switch_to_window'


",3k,"
            4
        ","[""\nThis error message...\n'WebDriver' object has no attribute 'switch_to_window'\n\n...implies that the WebDriver object no more supports the attribute switch_to_window()\n\nswitch_to_window\nswitch_to_window was deprecated in Selenium v2.41 :\n\nSelenium 2.41\n\ndeprecating switch_to_* in favour of driver.switch_to.*\n\n\nHence you see the error.\n\nSolution\nInstead of switch_to_window you need to use switch_to.\nExamples:\n\ndriver.switch_to.active_element\ndriver.switch_to.alert\ndriver.switch_to.default_content()\ndriver.switch_to.frame()\ndriver.switch_to.parent_frame()\ndriver.switch_to.window('main')\n\n""]",https://stackoverflow.com/questions/70360072/why-is-switch-to-window-method-not-working-for-selenium-webdriver-in-python,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Puppeteer Get all data attribute values,"
My html doc is
<div class=""inner-column"">
 <div data-thing=""abc1""></div>
 <div data-thing=""abc2""></div>
 <div data-thing=""abc3""></div>
</div>

How can I get all ""data-thing"" value (eg. [""abc1"", ""abc2"", ""abc3""]) inside div with class .inner-column?
const puppeteer = require('puppeteer');
const fs = require('fs');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  page.setViewport({width: 1440, height: 1200})
  await page.goto('https://www.example.com')

  const data = await page.content();

  await browser.close();
})();

",11k,"
            3
        ","[""\nYou could use the page.$$eval function for that like this:\nconst dataValues = await page.$$eval(\n    '.inner-column div',\n    divs => divs.map(div => div.dataset.thing)\n);\n\nExplanation\nWhat the page.$$eval function does (quote from the docs linked above):\n\nThis method runs Array.from(document.querySelectorAll(selector)) within the page and passes it as the first argument to pageFunction.\nIf pageFunction returns a Promise, then page.$$eval would wait for the promise to resolve and return its value.\n\nTherefore, it will first query the targeted divs and then map the divs to their data-* value by using the dataset property.\n"", '\nYou can use the evaluate function\nconst data = await page.evaluate(() => \n  Array.from(document.querySelectorAll("".inner-column DIV"")).map(d => d.getAttribute(""data-thing""))\n)\n\n']",https://stackoverflow.com/questions/55797082/puppeteer-get-all-data-attribute-values,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controlling a web browser using Excel VBA,"
I have been assigned the task of automating a web based task ( for a HTTPS website). The users currently are filling in the Excel sheet with the data, they now want to automate excel in such a way that it directly controls the browser and fills in the data.
I found the iMacros Scripting edition as a possible solution for doing this, I wanted to know if there are any other similar tools which can be used for controlling the browser and filling in data.
I also had a look at the Selenium Client Driver, but I am not sure on how to use it in Excel VBA.
Any help would be appreciated.
Thanks,
",25k,"
            3
        ","['\nYou can use Selenium from Visual Basic Editor by installing the tools provided here :\nhttp://code.google.com/p/selenium-vba/\nThere is a Selenium IDE plugin to automatically record a script in VBA and an installation package to run Selenium command in Visual Basic Editor.\nThe following example starts firefox, opens links in the 1st column, compares the title with the 2nd column and past the result in the 3rd column.\nUsed data are in a sheet, in a range named ""MyValues"".\nPublic Sub TC002()\n   Dim selenium As New SeleniumWrapper.WebDriver, r As Range\n   selenium.Start ""firefox"", ""http://www.google.com"" \n   For Each r In Range(""MyValues"").Rows\n     selenium.open r.Cells(, 1)\n     selenium.waitForNotTitle """"\n     r.Cells(, 3) = selenium.verifyTitle(r.Cells(, 2))\n   Next\n   selenium.stop\nEnd Sub\n\n', '\nThis sample open stackoverflow site an show IE\nSub OpenIE()\n\'officevb.com\nDim ie As Object\nSet ie = CreateObject(""InternetExplorer.Application"")\n\nie.Navigate ""http://www.stackowerflow.com""\n\n \'wait load\n While ie.ReadyState <> READYSTATE_COMPLETE\n  DoEvents\n Wend\n\nie.Visible = True\n\nEnd Sub\n\n[]\'s\n', '\nI use this code for reading data from excel and passin it to selenium for to do task like ""click, select, close etc"" and also you can write data to excel.\nThis is in python i don know VB and i do know perl if u wish i\'ll give same code in perl too.\ni hop this may help.\nfrom xlwt import Workbook\n\nimport xlrd\n\ntestconfigfilename=""testconfig.xls""\n\n    if (len(sys.argv) > 1):\n\n        testconfigfilename=sys.argv[1]       \n\n    wb = xlrd.open_workbook(testconfigfilename);\n\n    wb.sheet_names();\n\n    sh = wb.sheet_by_index(0); \'Sheet 0 - selenium server configuration\'\n\n\n\n    seleniumHost = sh.cell(1,0).value\n\n    seleniumPort = int(sh.cell(1,1).value)\n\n    testBaseURL = sh.cell(1,2).value\n\n    browser = sh.cell(1,3).value\n\n    timeout = int(sh.cell(1,4).value)\n\n    path = sh.cell(1,5).value\n\noutputwb = Workbook()\n\n    outputsheet = outputwb.add_sheet(""result"",cell_overwrite_ok=True) #get the first sheet in the result xls \n\noutputsheet.write(RowNumber,colNumber,""data"")\n\n']",https://stackoverflow.com/questions/7489418/controlling-a-web-browser-using-excel-vba,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selenium 3.0.1 -interactive gives ParameterException: Unknown option: -interactive,"
How to run selenum standanlone jar in interactive mode so that we can trigger commands from terminal. When I run as shown below exception occurs
java -jar selenium-server-standalone-3.0.1.jar -interactive
Exception in thread ""main"" com.beust.jcommander.ParameterException: Unknown option: -interactive
    at com.beust.jcommander.JCommander.parseValues(JCommander.java:742)
    at com.beust.jcommander.JCommander.parse(JCommander.java:282)
    at com.beust.jcommander.JCommander.parse(JCommander.java:265)
    at com.beust.jcommander.JCommander.<init>(JCommander.java:210)
    at org.openqa.grid.selenium.GridLauncherV3$1.setConfiguration(GridLauncherV3.java:219)
    at org.openqa.grid.selenium.GridLauncherV3.buildLauncher(GridLauncherV3.java:147)
    at org.openqa.grid.selenium.GridLauncherV3.main(GridLauncherV3.java:73)

However I am able to run version 2.48.2 in the above manner and it works fine.
",5k,"
            3
        ","['\nstarting version 3, the selenium team changed the available configuration options (see an example configuration), as well as replaced the command line arguments parser to JCommander.\nthe implications are;\n\nsome options may not be available anymore, or their names have changed. looking at the V3 example configuration, it seems the support for -interactive has dropped.\nyou should specify all -D arguments first, i.e. right after the java command.\n\n']",https://stackoverflow.com/questions/41275503/selenium-3-0-1-interactive-gives-parameterexception-unknown-option-interacti,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jbehave + thucydides: how to override 300s timeout,"
I'm using thucydides jbehave plugin to run Selenium tests. However, I can't run tests longer than 5 min in total due to jbehave timeout. I can't figure out how can thucydides/jbehave should be configured to override this limitation. Selenium tests use to be longer that 5 mins, so that should be an actual problem for many people.
",5k,"
            3
        ","['\nTo override the timeout the user should add thucydides.properties file to the main folder of the project (if you use thucidides jbehave archetype, there is no such file by default). \nset, for instance, story.timeout.in.secs=3000 and save the file. the timeout parameter will be overriden\n', '\nTo overide the timeout we can use the following :\n @Override\n        public Embedder configuredEmbedder(){\n\nStoryTimeouts.TimeoutParser t=new StoryTimeouts.TimeoutParser() {\n                @Override\n                public boolean isValid(String timeout) {\n                    return true;\n                }\n\n                @Override\n                public long asSeconds(String timeout) {\n                    return 500; <--- Storytimeout \n                }\n            };\n\n            embedder.useTimeoutParsers(t);\n}\n\n']",https://stackoverflow.com/questions/16238759/jbehave-thucydides-how-to-override-300s-timeout,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get the total number of Rows in a table | Cypress,"

I have a table with N rows. How can I get the total number of rows present in the table?
I search for a name, and that particular name is in row number X, how can I get the value of that particular row.

",9k,"
            3
        ","['\nYou can use .find to solve both of your cases.\nTo get the table row count:\n  cy.get(""#tableID"")\n    .find(""tr"")\n    .then((row) => {\n      //row.length will give you the row count\n      cy.log(row.length);\n    });\n\nTo get the value ( index ) of the particular row, you can do something like this.\n  cy.get(""#Table Id"")\n    .find(""tr"")\n    .then((rows) => {\n      rows.toArray().forEach((element) => {\n        if (element.innerHTML.includes(""Your Value"")) {\n        //rows.index(element) will give you the row index\n          cy.log(rows.index(element));\n        }\n      });\n    });\n\nAdditional tip: If you want to select a specific table cell containing a value, you can do this:\n  cy.get(""#customers"").find(""tr"").find(""td"").contains(""Germany"");\n\nNote: to get the table row index there can be many other alternative ways. Hope you will figure them out on the go.\n']",https://stackoverflow.com/questions/63086695/how-to-get-the-total-number-of-rows-in-a-table-cypress,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
how to use edgeDriver with qaf,"
I use below properties to run edge browser, but it doesn't work.
webdriver.edge.driver=src/main/resources/common/msedgedriver.exe
driver.name=edgeDriver

How can I use edge browser?
",575,"
            2
        ","['\nIn order to set driver executable, you need to set value using appropriate system property for the driver. You can set system property either at the time of execution using -Dpropertyname=value or through code.  If you are using qaf, it provides feature to set system property through property file. For that purpose you need to add system prefix with property name. For example:\nsystem.webdriver.edge.driver=src/main/resources/common/msedgedriver.exe\n', '\nAccording to the doc https://qmetry.github.io/qaf/latest/how_to_use_driver.html and check the UiDriverFactory.java in com.qmetry.qaf.automation.ui of qaf, it looks qaf not support this.\n']",https://stackoverflow.com/questions/62354345/how-to-use-edgedriver-with-qaf,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selenium Chrome WebDriver how to scroll horizontally,"
Chrome web driver has a limitation that only loads webpage content that is in view. I have a website that has long horizontal table. I have Xpath that extracts column headers of a table that stretches more than the screen width. In chrome dev tool console if I run the xpath $x(myxpathgoeshere) I get all the headers including the ones that are not in view(the one that makes you scroll to see all). So I know my xpath is correct. But in code, when I access it by using selenium webdriver it only gives header names that are in current view. I came across various posts on chrome webdriver google group page, users mentioning this limitation and answer to it was to not fix it. So anyways, now I am trying to make it work using javascript to scroll horizontally and then do the findelement by xpath again to see if the elements to the right are loaded. But for some strange reason I cannot seem to get the scrolling horizontally to work. I am using C# Javascript executor.
IJavaScriptExecutor js = (IJavaScriptExecutor) Driver;
js.ExecuteScript(""scrollTo(3000,0);""); // whatever X value I use chrome is not scrolling to the right. 

I have also tried scrollX and no luck. Is there something wrong with my code?
Edited: forgot that I was using X for horizontal not Y
",8k,"
            2
        ","['\nfor this type of issue i had use browser zoom-in and zoom-out functionality but i am using this in java. With java robot class i am doing browser zoom-out so i automatically shows the hidden column. you can try that it may help you. \n', '\nChange the code: \njs.ExecuteScript(""scrollTo(3000,0);"");\n\nto\njs.ExecuteScript(""scroll(3000,0);"");\n\nSimply, it\'s not scrollTo and scroll only.\n']",https://stackoverflow.com/questions/26104952/selenium-chrome-webdriver-how-to-scroll-horizontally,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do I switch to new tab or window in playwright-java?,"
How can we switch to a new window that has opened while running test, and how can I get back to the parent window in playwright-java?
",4k,"
            2
        ","['\nThere is no Switch action like Selenium. You can use the waitForPage or waitForPopup functions. You just need to know what is the action triggering that new page. e.g.\nPage popup = context.waitForPage(() -> page.click(""a""));\n\nThe context class also has a pages() function, which returns all the open pages.\n', ""\nexpanding on @hardkoded's answer, I got an error and am now using this:\ncontext.waitForEvent('page')\n\nworks for my purposes so far\n"", '\nWhat you want to do is continue your test in a new page. The official docs: https://playwright.dev/docs/pages#handling-new-pages\nHere is an example where we first work in the initial ""page"" and then, after clicking a button we want to continue our tests in a new tab we define as ""newPage"":\n        // Here we are working in the initial page\n        await page.locator(""#locator"").type(""This happens in the initial page.."");\n\n        /*  When ""Ok"" is clicked the test waits for a new page event and assigns to new page object to a variable called newPage\n            After this point we want the test to continue in the new tab,\n            so we\'ll have to use the newly defined newPage variable when working on that tab\n        */\n        const [newPage] = await Promise.all([\n            context.waitForEvent(\'page\'),\n            page.locator(""span >> text=Ok"").click()\n            \n        ])\n        await newPage.waitForLoadState();\n\n        console.log(""A new tab opened and the url of the tab is: "" + newPage.url());\n\n        // Here we work with the newPage object and we can perform actions like with page\n        await newPage.locator(""#Description"").type(""This happens in a new tab!"");\n\n']",https://stackoverflow.com/questions/66638076/how-do-i-switch-to-new-tab-or-window-in-playwright-java,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Selenium - What are possible keys in FireFox webdriver profile preferences,"
I couldn't really find this information anywhere, I am looking for a list of possible keys that can be used in the profile.set_preference() API.
Here is some context:
from selenium import webdriver
from pyvirtualdisplay import Display
display = Display(visible=0, size=(1024, 768))
display.start()
profile = webdriver.FirefoxProfile()

Now, if I want to, say specify a client SSL, I need to configure that as a preference of FireFox profile. I am trying to find the list of all the preferences so I can play with this. 
",2k,"
            2
        ","[""\nYou can look at profile.DEFAULT_PREFERENCES which is the json at python2.7/site-packages/selenium/webdriver/firefox/webdriver_prefs.json\n{u'frozen': {u'app.update.auto': False,\n  u'app.update.enabled': False,\n  u'browser.EULA.3.accepted': True,\n  u'browser.EULA.override': True,\n  u'browser.displayedE10SNotice': 4,\n  u'browser.download.manager.showWhenStarting': False,\n  u'browser.link.open_external': 2,\n  u'browser.link.open_newwindow': 2,\n  u'browser.offline': False,\n  u'browser.reader.detectedFirstArticle': True,\n  u'browser.safebrowsing.enabled': False,\n  u'browser.safebrowsing.malware.enabled': False,\n  u'browser.search.update': False,\n  u'browser.selfsupport.url': u'',\n  u'browser.sessionstore.resume_from_crash': False,\n  u'browser.shell.checkDefaultBrowser': False,\n  u'browser.tabs.warnOnClose': False,\n  u'browser.tabs.warnOnOpen': False,\n  u'datareporting.healthreport.logging.consoleEnabled': False,\n  u'datareporting.healthreport.service.enabled': False,\n  u'datareporting.healthreport.service.firstRun': False,\n  u'datareporting.healthreport.uploadEnabled': False,\n  u'datareporting.policy.dataSubmissionEnabled': False,\n  u'datareporting.policy.dataSubmissionPolicyAccepted': False,\n  u'devtools.errorconsole.enabled': True,\n  u'dom.disable_open_during_load': False,\n  u'extensions.autoDisableScopes': 10,\n  u'extensions.blocklist.enabled': False,\n  u'extensions.logging.enabled': True,\n  u'extensions.update.enabled': False,\n  u'extensions.update.notifyUser': False,\n  u'javascript.enabled': True,\n  u'network.http.phishy-userpass-length': 255,\n  u'network.manage-offline-status': False,\n  u'offline-apps.allow_by_default': True,\n  u'prompts.tab_modal.enabled': False,\n  u'security.csp.enable': False,\n  u'security.fileuri.origin_policy': 3,\n  u'security.fileuri.strict_origin_policy': False,\n  u'security.warn_entering_secure': False,\n  u'security.warn_entering_secure.show_once': False,\n  u'security.warn_entering_weak': False,\n  u'security.warn_entering_weak.show_once': False,\n  u'security.warn_leaving_secure': False,\n  u'security.warn_leaving_secure.show_once': False,\n  u'security.warn_submit_insecure': False,\n  u'security.warn_viewing_mixed': False,\n  u'security.warn_viewing_mixed.show_once': False,\n  u'signon.rememberSignons': False,\n  u'toolkit.networkmanager.disable': True,\n  u'toolkit.telemetry.enabled': False,\n  u'toolkit.telemetry.prompted': 2,\n  u'toolkit.telemetry.rejected': True},\n u'mutable': {u'browser.dom.window.dump.enabled': True,\n  u'browser.newtab.url': u'about:blank',\n  u'browser.newtabpage.enabled': False,\n  u'browser.startup.homepage': u'about:blank',\n  u'browser.startup.page': 0,\n  u'dom.max_chrome_script_run_time': 30,\n  u'dom.max_script_run_time': 30,\n  u'dom.report_all_js_exceptions': True,\n  u'javascript.options.showInConsole': True,\n  u'network.http.max-connections-per-server': 10,\n  u'startup.homepage_welcome_url': u'about:blank',\n  u'webdriver_accept_untrusted_certs': True,\n  u'webdriver_assume_untrusted_issuer': True}}\n\n"", '\nI usually just open about:config and search the list of preferences there. There is also this helpful resource which documents part of the preferences.\n']",https://stackoverflow.com/questions/38316910/python-selenium-what-are-possible-keys-in-firefox-webdriver-profile-preference,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to add wait / Delay until web page is fully loaded in Automation Anywhere?,"
I want to know 'How to add wait or Delay until webpage is fully loaded,' in automations anywhere,
I used 

wait for screen change

But it hold the process until some time specified by the developer , but I want to add delay until the web page fully loaded, 
Is there anyone can help me?
sorry for the bad English.
",8k,"
            1
        ","['\nUsually, a website is ""loaded"" or ""ready"" before the actual content is loaded. Some websites even have dummy content which is replaced once the actual content is retrieved from \'somewhere\'. Hence waiting for the screen to change is not a good idea. \nMy approach is to pick an element which you know is loaded after the element you want to interact with. For instance the navigation bar on this website is loaded before the comments are. You can either figure out which element to use by looking at the source of the website by right-clicking anywhere and selecting view source or by simply refreshing the page a couple of times and eye-balling it. The former requires some HTML knowledge, but is a better approach in my opinion.\nOnce you\'ve identified your element, use Object Cloning on said element and use the built-in wait as a delay (usually set to 15 sec, depending on the website/connection). The Action should be some random get property (store whatever you retrieve in some dummy variable as we\'re not going to use it anyway).\nObject Cloning\'s wait function polls every so many milliseconds and once the element is found it will almost instantaneously go to the next line in the code. This is where you interact with your target element. \nThis way you know your target element is loaded and the code is very optimized and robust.\nOn a final note: It\'s usually a good idea to surround this with some exception handling as automating websites is prone to errors.\n', '\nA very simple solution is to run your automation while watching and determine the amount of time it takes for the webpage to load. You can add a Delay rather than a wait if you know the page is generally loaded within 30 seconds or so. \n']",https://stackoverflow.com/questions/46641179/how-to-add-wait-delay-until-web-page-is-fully-loaded-in-automation-anywhere,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to capture response.json() in playwright,"
I am trying to capture json response using playwright. I keep on getting Promise pending. However under headless:false mode i can see the data is being received and populated on the browser. I have just started playing with Playwright and also not very familiar with ""Promise"".
What i have tried is as below:
(async () => {
        let browser = await firefox.launch({headless: true, userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0'});
        let page = await browser.newPage();
        page.waitForResponse(async(response) => {
            if (response.url().includes('/abcd') && response.status() == 200) {
                let resp = await response.json();
                console.log(resp);
            }
        });
        await page.goto('https://myurl.com', {waitUntil: 'networkidle', timeout: 30000});
        await page.waitForTimeout(20000);
        await browser.close();
})

What am i doing wrong? I have tried increasing timeout. Doesnot help.
",5k,"
            1
        ","[""\nThe waitForResponse won't handle your async function. You could do something like this:\n(async () => {\n  let browser = await firefox.launch({headless: true, userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0'});\n  let page = await browser.newPage();\n  const promise page.waitForResponse(/abcd/); // This is a regex to match the url\n  await page.goto('https://myurl.com', {waitUntil: 'networkidle', timeout: 30000});\n  var response = await promise; // here we wait for the promise to be fullfiled. \n  let resp = await response.json();\n  console.log(resp);\n  await browser.close();\n})\n\n""]",https://stackoverflow.com/questions/67019344/unable-to-capture-response-json-in-playwright,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multi-threaded C# Selenium WebDriver automation with Uris not known beforehand,"
I need to perform some simultaneous webdrivers manipulation, but I am uncertain as to how to do this.
What I am asking here is: 

What is the correct way to achieve this ?
What is the reason for the exception I am getting (revealed below)

After some research I ended up with:
1. The way I see people doing this (and the one I ended up using after playing with the API, before searching) is to loop over the window handles my WebDriver has at hand, and perform a switch to and out of the window handle I want to process, closing it when I am finished.
2. Selenium Grid does not seem like an option fore me - am I wrong or it is intended for parallel processing ? Since am running everything in a single computer, it will be of no use for me.

In trying the 1st option, I have the following scenario (a code sample is available below, I skipped stuff that is not relevant/repeat itself (where ever I added 3 dots:
I have a html page, with several submit buttons, stacked.
Clicking each of them will open a new browser/tab (interestingly enough, using ChromeDriver opens tabs, while FirefoxDriver opens separate windows for each.)
As a side note: I can't determine the uris of each submit beforehand (they must be determined by javascript, and at this point, let's just assume I want to handle everything knowing nothing about the client code.
Now, after looping over all the submit buttons, and issuing webElement.Click() on the corresponding elements, the tabs/windows open. The code flows to create a list of tasks to be executed, one for each new tab/window.
The problem is: since all tasks all depend upon the same instance of webdriver to switch to the window handles, seems I will need to add resource sharing locks/control. I am uncertain as whether I am correct, since I saw no mention of locks/resource access control in searching for multi-threaded web driver examples.
On the other hand, if I am able to determine the tabs/windows uris beforehand, I would be able to skip all the automation steps needed to reach this point, and then creating a webDriver instance for each thread, via Navigate().GoToUrl() would be straightforward. But this looks like a deadlock! I don't see webDriver's API providing any access to the newly opened tab/window without performing a switch. And I only want to switch if I do not have to repeat all the automation steps that lead me to the current window !
...
In any case, I keep getting the exception:
Element belongs to a different frame than the current one - switch to its containing frame to use it
at 
IWebElement element = cell.FindElement

inside the ToDictionary() block.
I obviously checked that all my selectors are returning results, in chrome's console. 
foreach (WebElement resultSet in resultSets)
    resultSet.Click();


foreach(string windowHandle in webDriver.WindowHandles.Skip(1))
{
    dataCollectionTasks.Add(Task.Factory.StartNew<List<DataTable>>(obj =>
    {
        List<DataTable> collectedData = new List<DataTable>();
        string window = obj as string;

        if (window != null)
        {
            webDriver.SwitchTo().Window(windowHandle);
            List<WebElement> dataSets = webDriver.FindElements(By.JQuerySelector(utils.GetAppSetting(""selectors.ResultSetData""))).ToList();

            DataTable data = null;

            for (int i = 0; i < dataSets.Count; i += 2)
            {
                data = new DataTable();

                data.Columns.Add(""Col1"", typeof(string));
                data.Columns.Add(""Col2"", typeof(string));
                data.Columns.Add(""Col3"", typeof(string));

                ///...

                //data set header
                if (i % 2 != 0)
                {
                    IWebElement headerElement = dataSets[i].FindElement(OpenQA.Selenium.By.CssSelector(utils.GetAppSetting(""selectors.ResultSetDataHeader"")));
                    data.TableName = string.Join("" "", headerElement.Text.Split().Take(3));
                }
                //data set records
                else
                {
                    Dictionary<string, string> cells = dataSets[i]
                        .FindElements(OpenQA.Selenium.By.CssSelector(utils.GetAppSetting(""selectors.ResultSetDataCell"")))
                        .ToDictionary(
                            cell =>
                            {
                                IWebElement element = cell.FindElement(OpenQA.Selenium.By.CssSelector(utils.GetAppSetting(""selectors.ResultSetDataHeaderColumn"")));
                                return element == null ? string.Empty : element.Text;
                            },
                            cell =>
                            {
                                return cell == null ? string.Empty : cell.Text;
                            });

                    string col1Value, col2Value, col3Value; //...
                    cells.TryGetValue(""Col1"", out col1Value);
                    cells.TryGetValue(""Col2"", out col2Value);
                    cells.TryGetValue(""Col3"", out col3Value);
                    //...

                    data.Rows.Add(col1Value, col2Value, col3Value /*...*/);
                }
            }

            collectedData.Add(data);
        }

        webDriver.SwitchTo().Window(mainWindow);
        webDriver.Close();

        return collectedData;
    }, windowHandle));
} //foreach

Task.WaitAll(dataCollectionTasks.ToArray());
foreach (Task<List<DataTable>> dataCollectionTask in dataCollectionTasks)
{
    results.AddRange(dataCollectionTask.Result);
}

return results;

",3k,"
            1
        ",[],https://stackoverflow.com/questions/31654380/multi-threaded-c-sharp-selenium-webdriver-automation-with-uris-not-known-beforeh,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get a specific frame in a web page and retrieve its content,"
I wanted to access the translation results of the following url 

http://translate.google.com/translate?hl=en&sl=en&tl=ar&u=http%3A%2F%2Fwww.saltycrane.com%2Fblog%2F2008%2F10%2Fhow-escape-percent-encode-url-python%2F

the translation is displayed in the bottom  content frame out of the two frames. I am interested in retrieving only the bottom content frame to get the translations 
selenium for python allows us to fetch page contents via web automation:
browser.get('http://translate.google.com/#en/ar/'+hurl)

The required frame is an iframe :
<div id=""contentframe"" style=""top:160px""><iframe   src=""/translate_p?hl=en&am... name=c frameborder=""0"" style=""height:100%;width:100%;position:absolute;top:0px;bottom:0px;""></div></iframe>

but how to get the bottom content frame element to retrieve the translations using web automation?
Came to know that PyQuery also allows us to browse the contents using the JQuery formalism
Update:
An answer mentioned that Selenium provides a method where you can do that.
frame = browser.find_element_by_tag_name('iframe')
browser.switch_to_frame(frame)
# get page source
browser.page_source

but it does not work in the above example. It returns an empty page .
",11k,"
            1
        ","['\nYou can use driver.switchTo.frame(1); here, the digit 1 inside frame() is the index of frames present in the webpage. as your requirement is to switch to second frame and the index starts with 0, you should use driver.switchTo.frame(1);\nBut the above code is in Java. In Python, you can use the below line.\ndriver.switch_to_frame(1);\n\nUPDATE\n driver.get(""http://translate.google.com/translate?hl=en&sl=en&tl=ar&u=http://www.saltycrane.com/blog/2008/10/how-escape-percent-encode-url-python/"");\n driver.switchTo().frame(0);\n System.out.println(driver.findElement(By.xpath(""/html/body/div/div/div[3]/h1/span/a"")).getText());\n\nOutput: SaltyCrane ???????\nI have just tried to print the title name SaltCrane that is present inside the iframe.\nIt worked for me except for the ? symbols after the SaltCrane. As it was arabic, it was unable to decode the same.\nThe above code is in Java. Same logic should also work in Python.\n', ""\nSelenium provides a method where you can do that.\nframe = browser.find_element_by_tag_name('iframe')\nbrowser.switch_to_frame(frame)\n# get page source\nbrowser.page_source\n\n""]",https://stackoverflow.com/questions/15785920/how-to-get-a-specific-frame-in-a-web-page-and-retrieve-its-content,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selecting value from a dropdown list on a webpage using vba,"
On this site I am able to select the country and language from dropdown menu but when I click on ""Complete new application form"" button. It says fields are empty.
Any help would be appreciated.
Sub Test()

strURL = ""https://visa.kdmid.ru/PetitionChoice.aspx""

  With ie
    .Visible = True
    .navigate strURL

    While .Busy
        DoEvents
    Wend

    Set html = .document

    'Country where you will apply for visa.
    Set ctY = html.getElementById(""ctl00$phBody$Country"")
    For i = 1 To ctY.Options.Length
        If ctY.Options(i).Text = ""NETHERLANDS"" Then
            ctY.selectedIndex = i
            Exit For
        End If
    Next i

    'Select Language
    Set lnG = html.getElementById(""ctl00$phBody$ddlLanguage"")
    For i = 1 To lnG.Options.Length
        If lnG.Options(i).Text = ""ENGLISH"" Then
            lnG.selectedIndex = i
            Exit For
        End If
    Next i

    'Click I have read instructions check box
    html.getElementById(""ctl00$phBody$cbConfirm"").Click


    'Click apply button
    Set btnGo = html.forms(0).all(""ctl00$phBody$btnNewApplication"") 
    btnGo.Click

  End With

  End Sub

",3k,"
            0
        ","['\nSo you are on the right track but if you look at the HTML of the site there are actually two elements with the country selection- you got the first one, \'ctl00_phBody_Country\', but this is actually just the drop down, and the actual selected value is stored in \'ctl00_phBody_cddCountry_ClientState\'... the language section has similar structure. Lastly the accepted value is not just the country name you see in the drop down, it is actually a combination of a country code from the drop down and the country name....\nSee below for sample code:\nPublic Sub Test()\nDim IE As InternetExplorer\nDim HTMLDoc As HTMLDocument\n\nDim countryStr As String\nDim countryObj As HTMLObjectElement\nDim countryCodes As IHTMLElementCollection\nDim codeCounter As Long\nDim languageStr As String\nDim languageObj As HTMLObjectElement\nDim languageCodes As IHTMLElementCollection\n\ncountryStr = ""Netherlands""\nlanguageStr = ""English""\n\nSet IE = New InternetExplorer\n\nWith IE\n    .Visible = False\n    .Navigate ""https://visa.kdmid.ru/PetitionChoice.aspx?AspxAutoDetectCookieSupport=1""\n    While .Busy Or .ReadyState <> READYSTATE_COMPLETE: Wend\n    Set HTMLDoc = IE.document\nEnd With\n\nSet countryObj = HTMLDoc.getElementById(""ctl00_phBody_cddCountry_ClientState"")\nSet countryCodes = HTMLDoc.getElementById(""ctl00_phBody_Country"").getElementsByTagName(""option"")\nFor codeCounter = 0 To countryCodes.Length - 1\n    If countryCodes(codeCounter).innerText = UCase(countryStr) Then\n        countryObj.Value = countryCodes(codeCounter).Value & "":::"" & countryCodes(codeCounter).innerText & "":::""\n        While IE.Busy Or IE.ReadyState <> READYSTATE_COMPLETE: Wend\n        Exit For\n    End If\nNext\n\nSet languageObj = HTMLDoc.getElementById(""ctl00_phBody_cddLanguage_ClientState"")\nSet languageCodes = HTMLDoc.getElementById(""ctl00_phBody_ddlLanguage"").getElementsByTagName(""option"")\nFor codeCounter = 0 To languageCodes.Length - 1\n    If languageCodes(codeCounter).innerText = UCase(languageStr) Then\n        languageObj.Value = languageCodes(codeCounter).Value & "":::"" & languageCodes(codeCounter).innerText & "":::""\n        While IE.Busy Or IE.ReadyState <> READYSTATE_COMPLETE: Wend\n        Exit For\n    End If\nNext\n\nHTMLDoc.getElementById(""ctl00$phBody$cbConfirm"").Click\nWhile IE.Busy Or IE.ReadyState <> READYSTATE_COMPLETE: Wend\nHTMLDoc.getElementById(""ctl00_phBody_btnNewApplication"").Click      \'Launch Form\n\nIE.Quit\nSet IE = Nothing\nEnd Sub\n\n']",https://stackoverflow.com/questions/41688620/selecting-value-from-a-dropdown-list-on-a-webpage-using-vba,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StaleElement exception error when asserting data in a table,"
I am trying to add data to a table and then asserting that data is added by collecting table data in a list but every time it throws me a StaleElement exception error, now I guess it is happening because the list is getting refreshed, so I am not sure how do I handle it.
Here is my implementation
    private static List<WebElement> listOfJobs = 
    driver.findElements(By.xpath((""//*[@id='resultTable']//tbody/tr//a"")));

    public static List<WebElement> getListOfJobs() 
        {
            try 
            {
                return listOfJobs;
            } 
            catch (Exception e) 
            {
                e.printStackTrace();
            }
            return null;
        }

    public static String generateName()
        {
            String AlphaNumericString = ""abcdefghijklmnopqrstuvxyz"";
            StringBuilder sb = new StringBuilder(9);
    
            for (int i = 0; i < 9; i++) 
            {
                int index = (int)(AlphaNumericString.length() * Math.random());
                sb.append(AlphaNumericString.charAt(index));
            }
            return sb.toString()+""digi"";
        }
    
    @SuppressWarnings({ ""null"" })
        public static List<String> listOfJobs()
        {
            List<String> jobs = null;
            for(int i=0; i < OrangeHRMAddJobCategories.getListOfJobs().size(); i++)
            {
                jobs.add(OrangeHRMAddJobCategories.getListOfJobs().get(i).getText());
            }
            return jobs;
        }

    OrangeHRMAddJobCategories jobCategories = new OrangeHRMAddJobCategories();
            jobCategories.clickJobTab().clickJobCategoires().clickAdd().setJobCategoryName(UsefulFunctionUtils.generateName()).saveJobCategory();   

    Assertions.assertThat(UsefulFunctionUtils.listOfJobs().contains(""digi""));

I feel that the listOfJobs should be reinjected somewhere but not sure where exactly do I put it because refreshing the page did not work.
Complete stacktrace

    org.openqa.selenium.StaleElementReferenceException: stale element reference: element is not attached to the page document
      (Session info: chrome=94.0.4606.61)
    For documentation on this error, please visit: https://www.seleniumhq.org/exceptions/stale_element_reference.html
    Build info: version: '3.141.59', revision: 'e82be7d358', time: '2018-11-14T08:17:03'
    System info: host: 'DESKTOP-R3JT7MO', ip: '192.168.0.103', os.name: 'Windows 10', os.arch: 'amd64', os.version: '10.0', java.version: '16.0.1'
    Driver info: org.openqa.selenium.chrome.ChromeDriver
    Capabilities {acceptInsecureCerts: false, browserName: chrome, browserVersion: 94.0.4606.61, chrome: {chromedriverVersion: 94.0.4606.61 (418b78f5838ed..., userDataDir: C:\Users\CHINMA~1\AppData\L...}, goog:chromeOptions: {debuggerAddress: localhost:62489}, javascriptEnabled: true, networkConnectionEnabled: false, pageLoadStrategy: normal, platform: WINDOWS, platformName: WINDOWS, proxy: Proxy(), setWindowRect: true, strictFileInteractability: false, timeouts: {implicit: 0, pageLoad: 300000, script: 30000}, unhandledPromptBehavior: dismiss and notify, webauthn:extension:credBlob: true, webauthn:extension:largeBlob: true, webauthn:virtualAuthenticators: true}
    Session ID: 8b68238ee73ae8190f250fa15fbb41f1
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:78)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
        at org.openqa.selenium.remote.http.W3CHttpResponseCodec.createException(W3CHttpResponseCodec.java:187)
        at org.openqa.selenium.remote.http.W3CHttpResponseCodec.decode(W3CHttpResponseCodec.java:122)
        at org.openqa.selenium.remote.http.W3CHttpResponseCodec.decode(W3CHttpResponseCodec.java:49)
        at org.openqa.selenium.remote.HttpCommandExecutor.execute(HttpCommandExecutor.java:158)
        at org.openqa.selenium.remote.service.DriverCommandExecutor.execute(DriverCommandExecutor.java:83)
        at org.openqa.selenium.remote.RemoteWebDriver.execute(RemoteWebDriver.java:552)
        at org.openqa.selenium.remote.RemoteWebElement.execute(RemoteWebElement.java:285)
        at org.openqa.selenium.remote.RemoteWebElement.getText(RemoteWebElement.java:166)
        at com.digicorp.utils.UsefulFunctionUtils.listOfJobs(UsefulFunctionUtils.java:45)
        at com.digicorp.testcases.TC_AddJobCategory.testAddJobCategory(TC_AddJobCategory.java:26)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:567)
        at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:133)
        at org.testng.internal.TestInvoker.invokeMethod(TestInvoker.java:598)
        at org.testng.internal.TestInvoker.invokeTestMethod(TestInvoker.java:173)
        at org.testng.internal.MethodRunner.runInSequence(MethodRunner.java:46)
        at org.testng.internal.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:824)
        at org.testng.internal.TestInvoker.invokeTestMethods(TestInvoker.java:146)
        at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)
        at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:128)
        at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
        at org.testng.TestRunner.privateRun(TestRunner.java:794)
        at org.testng.TestRunner.run(TestRunner.java:596)
        at org.testng.SuiteRunner.runTest(SuiteRunner.java:377)
        at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:371)
        at org.testng.SuiteRunner.privateRun(SuiteRunner.java:332)
        at org.testng.SuiteRunner.run(SuiteRunner.java:276)
        at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:53)
        at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:96)
        at org.testng.TestNG.runSuitesSequentially(TestNG.java:1212)
        at org.testng.TestNG.runSuitesLocally(TestNG.java:1134)
        at org.testng.TestNG.runSuites(TestNG.java:1063)
        at org.testng.TestNG.run(TestNG.java:1031)
        at org.testng.remote.AbstractRemoteTestNG.run(AbstractRemoteTestNG.java:115)
        at org.testng.remote.RemoteTestNG.initAndRun(RemoteTestNG.java:251)
        at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:77)


",67,"
            0
        ","['\n\norg.openqa.selenium.StaleElementReferenceException: stale element reference: element is not attached to the page document\n\nIndicates that a reference to an element is now ""stale"" --- the element no longer appears on the DOM of the page. The reason for this expectation is may be your DOM got updated or refreshed. For an example, after performing an action like click() your DOM may get updated or refreshed. In this time when you are trying to find an element on DOM you will experience this error.\nYou have to re-find that element in updated or refreshed DOM\ntry{\n    jobs.add(OrangeHRMAddJobCategories.getListOfJobs().get(i).getText());\n   }catch(org.openqa.selenium.StaleElementReferenceException e){\n     new WebDriverWait(driver, 10).until(ExpectedConditions.visibilityOf(OrangeHRMAddJobCategories.getListOfJobs().get(i)));\n     jobs.add(OrangeHRMAddJobCategories.getListOfJobs().get(i).getText());\n   }\n\n\n', '\nAs per the convo with OP, it\'s getting stale at this point\njobs.add(OrangeHRMAddJobCategories.getListOfJobs().get(i).getText());\n\nIt could be because he is calling getListOfJobs which returns a List of WebElement.\nand when you call again, they are no longer available to page dom, cause you might have interacted with web element/elements in first iteration. There are few fixes, such as using Explicit wait staleness or redefining the elements again and again for next iteration.\nFix 1 :\nprivate static List<WebElement> listOfJobs = driver.findElements(By.xpath((""//*[@id=\'resultTable\']//tbody/tr//a"")));\n\npublic static List<WebElement> getListOfJobs(int j) \n {\n     try \n     {\n         List<WebElement> listOfJobs = driver.findElements(By.xpath(""(//*[@id=\'resultTable\']//tbody/tr//a)[\'""+j+""\']""));\n         return listOfJobs;\n     } \n     catch (Exception e) \n     {\n         e.printStackTrace();\n     }\n     return null;\n }\n \n \n    @SuppressWarnings({ ""null"" })\n    public static List<String> listOfJobs()\n    {\n        int j = 1;\n        List<String> jobs = null;\n        for(int i=0; i < listOfJobs.size(); i++)\n        {\n            jobs.add(OrangeHRMAddJobCategories.getListOfJobs(j).get(i).getText());\n            j++;\n        }\n        return jobs;\n    }\n\n', '\nYou are doing the operation before the element is loaded in the webpage, so please add the wait conditions and then do your operation.\nBy divCss = By.xpath((""//*[@id=\'resultTable\']//tbody/tr//a""));\nwait.until(ExpectedConditions.elementToBeClickable(divCss));\nwait.until(ExpectedConditions.presenceOfAllElementsLocatedBy(divCss));\n\n']",https://stackoverflow.com/questions/69431495/staleelement-exception-error-when-asserting-data-in-a-table,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
how to instantiate the webdriver object from the custom library when doing web automation using robot framework,"
while defining user keywords in custom library for web automation,which library should be imported?selenium2library or importing webdriver from selenium.How to use the webdriver to click on some elements.Kindly explain with an example 
",1k,"
            0
        ","['\nIn most scenarios you do not need to instantiate the webdriver object. Usually you use the webdriver instance that Selenium2Library already has. How you access that instance depends on how you plan on interacting with Selenium2Library. See the ""Extending existing test libraries"" section in the user guide for options. Each options have pros and cons.\nIf you inherit Selenium2Library, then you would access the driver via self._current_browser().\nIf you plan on using the Selenium2Library directly instead of inheriting, you would declare both Selenium2Library and your custom libraries. The most convenient way to access the driver is through a private property as demonstrated below. \nfrom robot.libraries.BuiltIn import BuiltIn\n\nclass Selenium2LibraryExt(object):\n\n    @property\n    def _s2l(self):\n        return BuiltIn().get_library_instance(\'Selenium2Library\')\n\n    @property\n    def _driver(self):\n        return self._s2l._current_browser()\n\n    def perform_search(self, criteria):\n        textbox = self._driver.find_element_by_name(\'q\')\n        textbox.send_keys(criteria)\n        textbox.submit()\n\nTest suite file:\n*** Settings ***\nTest Teardown     Close All Browsers\nLibrary           Selenium2Library\nLibrary           c:/ws/Selenium2LibraryExt.py\n\n*** Test Cases ***\nDo a search\n    Open Browser    http://www.google.com/    gc\n    Perform Search    happiness\n\n', '\nI have found that inheriting Selenium2Library is usually enough, like this\nfrom Selenium2Library import Selenium2Library\n\nclass MySelenium2Library(Selenium2Library):\n    def my_keyword(self):\n        my_element = self.get_my_element()\n        self.click_element(my_element)\n\nIn Robot you import this new library\n*** Settings ***\nLibrary    MySelenium2Library.py\n\n*** Test Cases ***\nTest 1\n    My Keyword\n\nI have not needed webdriver to click elements. I do all my clicking with Selenium2Library click methods like click_element.\n']",https://stackoverflow.com/questions/35308330/how-to-instantiate-the-webdriver-object-from-the-custom-library-when-doing-web-a,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can't convert webbot script to an executable,"
I am trying to convert a python script which uses the webbot library for web automation.
As I tried to convert my running Python (3.6.5) script to an .exe file using pyinstaller I was getting an error that the path of the webbot module could not be found.
In order to overcome this problem I tried to specify the path of the module in the spec file, without success. An easier workaround suggests copying the downloaded folder webbot in the same folder where the .exe file is.
Its a very handy tool to use , i don't wanna ditch it .
",230,"
            0
        ","['\nIn order for this to work you should not convert it as one file like this\npyinstaller --onefile file.py\n\nbut like this\npyinstaller file.py\n\nAfter doing this the .exe could be launched without any problems.\nAn other error was coming because of trying to import webbot in my script. Probably this is a noob mistake, but one has to:\nfrom webbot import Browser\n\n']",https://stackoverflow.com/questions/53138328/cant-convert-webbot-script-to-an-executable,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IE Web Automation - How to auto select value from combo box using Excel VBA/XML Macro,"
I'm a beginner in VBA and I've failed to select country name automatically in web Combo box or list box from my Excel spreadsheet. My code is entering country name only, but not selecting it. 
How can I change this code so it can pick country name from my Excel spreadsheet and select the same in web combo box as a loop. Passport number, DOB and Nationality are correct on my code. If you'll use manually then you can find the work permit number which I need to capture in my spreadsheet. Chrome Inspect Element screenshot is attached herewith.

My code is as follows:
Sub MOL()
    Dim IE As New SHDocVw.InternetExplorer
    Dim Doc As MSHTML.HTMLDocument
    Dim Buttons As MSHTML.IHTMLElementCollection
    Dim Button As MSHTML.IHTMLElement
    Dim HTMLInput As MSHTML.IHTMLElement
    Dim Tags As MSHTML.IHTMLElement
    Dim HTMLTables As MSHTML.IHTMLElementCollection
    Dim HTMLTable As MSHTML.IHTMLElement
    Dim HTMLRow As MSHTML.IHTMLElement
    Dim HTMLCell As MSHTML.IHTMLElement
    Dim Alltext As IHTMLElementCollection

Application.ScreenUpdating = False
'Application.Calculation = xlCalculationManual
'Application.EnableEvents = False

On Error Resume Next

    IE.Visible = True
    IE.navigate ""https://eservices.mol.gov.ae/SmartTasheel/Complain/IndexLogin?lang=en-gb""

Do While IE.readyState <> READYSTATE_COMPLETE: Loop

Set Doc = IE.document
Set Buttons = Doc.getElementsByTagName(""Button"")
Buttons(2).Click
Do While IE.readyState <> READYSTATE_INTERACTIVE = 3: Loop
Set HTMLInputs = Doc.getElementsByTagName(""Input"")
    HTMLInputs(46).Value = ""somevalue""
    HTMLInputs(48).Value = ""24/02/1990""
    HTMLInputs(47).Value = ""India""
Buttons(21).Click
End Sub

",2k,"
            0
        ","['\nThe solution you look for is a bit difficult to provide. There are few tricky parts to hurdle to select the NATIONALITY from dropdown. I\'ve used .querySelector() within the script to make it concise. However, it should serve your purpose no matter whatever country you wanna select from dropdown. Give it a shot:\nSub GetInfo()\n    Dim IE As New InternetExplorer, HTML As HTMLDocument, post As Object, URL$\n\n    URL = ""https://eservices.mol.gov.ae/SmartTasheel/Complain/IndexLogin?lang=en-gb""\n\n    With IE\n        .Visible = True\n        .navigate URL\n        While .Busy = True Or .readyState < 4: DoEvents: Wend\n        Set HTML = .document\n\n        HTML.getElementById(""TransactionInfo_WorkPermitNumber"").innerText = ""2659558""\n        HTML.querySelector(""button[ng-click=\'showEmployeeSearch()\']"").Click\n\n        Application.Wait Now + TimeValue(""00:00:03"")  \'\'If for some reason the script fails, make sure to increase the delay\n\n        HTML.getElementById(""txtPassportNumber"").Value = ""J2659558""\n        HTML.getElementById(""Nationality"").Focus\n        For Each post In HTML.getElementsByClassName(""ng-scope"")\n            With post.getElementsByClassName(""ng-binding"")\n                For I = 0 To .Length - 1\n                    If .item(I).innerText = ""INDIA"" Then \'\'you can change the country name here to select from dropdown\n                        .item(I).Click\n                        Exit For\n                    End If\n                Next I\n            End With\n        Next post\n        HTML.getElementById(""txtBirthDate"").Value = ""24/02/1990""\n        HTML.querySelector(""button[onclick=\'SearchEmployee()\']"").Click\n    End With\nEnd Sub\n\nReference to add to the library:\nMicrosoft Internet Controls\nMicrosoft HTML Object library\n\nWhen you execute the above script, it should give you the desired result.\nAnother way would be to go for using xmlhttp request which is way faster than IE. You need to pass the query string parameter arguments as dictionary through ""POST"" request. If you want to change the parameter as in, birth date,passportor nationality just do it in the QueryString. Btw, the Nationality parameter should be filled in with value instead of name as in, 100 for INDIA.  This is how your script should look like:\nSub Get_Data()\n    Dim res As Variant, QueryString$, ID$, Name$\n\n    QueryString = ""{""""PersonPassportNumber"""":""""J2659558"""",""""PersonNationality"""":""""100"""",""""PersonBirthDate"""":""""24/02/1990""""}""\n\n    With New XMLHTTP\n        .Open ""POST"", ""https://eservices.mol.gov.ae/SmartTasheel/Dashboard/GetEmployees"", False\n        .setRequestHeader ""User-Agent"", ""Mozilla/5.0""\n        .setRequestHeader ""Content-Type"", ""application/json""\n        .send QueryString\n        res = .responseText\n    End With\n\n    ID = Split(Split(Split(res, ""Employees"""":"")(1), ""ID"""":"""""")(1), """""","")(0)\n    Name = Split(Split(Split(res, ""Employees"""":"")(1), ""OtherData2"""":"""""")(1), """"""}"")(0)\n\n    [A1] = ID: [B1] = Name\nEnd Sub\n\nReference to add to the library:\nMicrosoft XML, V6.0\n\nRunning the above script, you should get the NAME and ID of your required search.\n']",https://stackoverflow.com/questions/50086005/ie-web-automation-how-to-auto-select-value-from-combo-box-using-excel-vba-xml,webautomation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebDriverWait not working as expected,"
I am working with selenium to scrape some data.
There is button on the page that I am clicking say ""custom_cols"". This button opens up a window for me where I can select my columns. 
This new window sometimes takes some time to open (around 5 seconds). So to handle this I have used 
WebDriverWait 

with delay as 20 seconds. But some times it fails to select find elements on new window, even if the element is visible. This happens only once in ten times for rest of time it works properly.
I have used same function(WebDriverWait) on other places also and it is works as expected. I mean it waits till the elements gets visible and then clicks it at the moment it finds it.
My question is why elements on new window is not visible even though I am waiting for element to get visible. To add here I have tried to increase delay time but still I get that error once in a while.
My code is here 
def wait_for_elem_xpath(self, delay = None, xpath = """"):
    if delay is None:
        delay = self.delay

    try:
        myElem = WebDriverWait(self.browser, delay).until(EC.presence_of_element_located((By.XPATH , xpath)))
    except TimeoutException:
        print (""xpath: Loading took too much time!"")
    return myElem
select_all_performance = '//*[@id=""mks""]/body/div[7]/div[2]/div/div/div/div/div[2]/div/div[2]/div[2]/div/div[1]/div[1]/section/header/div'
self.wait_for_elem_xpath(xpath = select_all_performance).click()

",14k,"
            16
        ","['\nOnce you wait for the element and moving forward as you are trying to invoke click() method instead of using presence_of_element_located() method you need to use element_to_be_clickable() as follows :\ntry:\n    myElem = WebDriverWait(self.browser, delay).until(EC.element_to_be_clickable((By.XPATH , xpath)))\n\n\nUpdate\nAs per your counter question in the comments here are the details of the three methods :\npresence_of_element_located\npresence_of_element_located(locator) is defined as follows :\nclass selenium.webdriver.support.expected_conditions.presence_of_element_located(locator)\n\nParameter : locator - used to find the element returns the WebElement once it is located\n\nDescription : An expectation for checking that an element is present on the DOM of a page. This does not necessarily mean that the element is visible or interactable (i.e. clickable). \n\nvisibility_of_element_located\nvisibility_of_element_located(locator) is defined as follows :\nclass selenium.webdriver.support.expected_conditions.visibility_of_element_located(locator)\n\nParameter : locator -  used to find the element returns the WebElement once it is located and visible\n\nDescription : An expectation for checking that an element is present on the DOM of a page and visible. Visibility means that the element is not only displayed but also has a height and width that is greater than 0.\n\nelement_to_be_clickable\nelement_to_be_clickable(locator) is defined as follows :\nclass selenium.webdriver.support.expected_conditions.element_to_be_clickable(locator)\n\nParameter : locator - used to find the element returns the WebElement once it is visible, enabled and interactable (i.e. clickable).\n\nDescription : An Expectation for checking an element is visible, enabled and interactable such that you can click it. \n\n']",https://stackoverflow.com/questions/49775502/webdriverwait-not-working-as-expected,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web-scraping JavaScript page with Python,"
I'm trying to develop a simple web scraper. I want to extract text without the HTML code. It works on plain HTML, but not in some pages where JavaScript code adds text.
For example, if some JavaScript code adds some text, I can't see it, because when I call:
response = urllib2.urlopen(request)

I get the original text without the added one (because JavaScript is executed in the client).
So, I'm looking for some ideas to solve this problem.
",438k,"
            264
        ","['\nEDIT Sept 2021: phantomjs isn\'t maintained any more, either\nEDIT 30/Dec/2017: This answer appears in top results of Google searches, so I decided to update it. The old answer is still at the end.\ndryscape isn\'t maintained anymore and the library dryscape developers recommend is Python 2 only. I have found using Selenium\'s python library with Phantom JS as a web driver fast enough and easy to get the work done.\nOnce you have installed Phantom JS, make sure the phantomjs binary is available in the current path:\nphantomjs --version\n# result:\n2.1.1\n\n#Example\nTo give an example, I created a sample page with following HTML code. (link):\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=""utf-8"">\n  <title>Javascript scraping test</title>\n</head>\n<body>\n  <p id=\'intro-text\'>No javascript support</p>\n  <script>\n     document.getElementById(\'intro-text\').innerHTML = \'Yay! Supports javascript\';\n  </script> \n</body>\n</html>\n\nwithout javascript it says: No javascript support and with javascript: Yay! Supports javascript\n#Scraping without JS support:\nimport requests\nfrom bs4 import BeautifulSoup\nresponse = requests.get(my_url)\nsoup = BeautifulSoup(response.text)\nsoup.find(id=""intro-text"")\n# Result:\n<p id=""intro-text"">No javascript support</p>\n\n#Scraping with JS support:\nfrom selenium import webdriver\ndriver = webdriver.PhantomJS()\ndriver.get(my_url)\np_element = driver.find_element_by_id(id_=\'intro-text\')\nprint(p_element.text)\n# result:\n\'Yay! Supports javascript\'\n\n\nYou can also use Python library dryscrape to scrape javascript driven websites.\n#Scraping with JS support:\nimport dryscrape\nfrom bs4 import BeautifulSoup\nsession = dryscrape.Session()\nsession.visit(my_url)\nresponse = session.body()\nsoup = BeautifulSoup(response)\nsoup.find(id=""intro-text"")\n# Result:\n<p id=""intro-text"">Yay! Supports javascript</p>\n\n', '\nWe are not getting the correct results because any javascript generated content needs to be rendered on the DOM. When we fetch an HTML page, we fetch the initial, unmodified by javascript, DOM.\nTherefore we need to render the javascript content before we crawl the page.\nAs selenium is already mentioned many times in this thread (and how slow it gets sometimes was mentioned also), I will list two other possible solutions.\n\nSolution 1: This is a very nice tutorial on how to use Scrapy to crawl javascript generated content and we are going to follow just that.\nWhat we will need:\n\nDocker installed in our machine. This is a plus over other solutions until this point, as it utilizes an OS-independent platform.\nInstall Splash following the instruction listed for our corresponding OS.Quoting from splash documentation:\n\nSplash is a javascript rendering service. It鈥檚 a lightweight web browser with an HTTP API, implemented in Python 3 using Twisted and QT5. \n\nEssentially we are going to use Splash to render Javascript generated content.\nRun the splash server: sudo docker run -p 8050:8050 scrapinghub/splash.\nInstall the scrapy-splash plugin: pip install scrapy-splash\nAssuming that we already have a Scrapy project created (if not, let\'s make one), we will follow the guide and update the settings.py:\n\nThen go to your scrapy project鈥檚 settings.py and set these middlewares:\nDOWNLOADER_MIDDLEWARES = {\n      \'scrapy_splash.SplashCookiesMiddleware\': 723,\n      \'scrapy_splash.SplashMiddleware\': 725,\n      \'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\': 810,\n}\n\nThe URL of the Splash server(if you鈥檙e using Win or OSX this should be the URL of the docker machine: How to get a Docker container\'s IP address from the host?):\nSPLASH_URL = \'http://localhost:8050\'\n\nAnd finally you need to set these values too:\nDUPEFILTER_CLASS = \'scrapy_splash.SplashAwareDupeFilter\'\nHTTPCACHE_STORAGE = \'scrapy_splash.SplashAwareFSCacheStorage\'\n\n\nFinally, we can use a SplashRequest:\n\nIn a normal spider you have Request objects which you can use to open URLs. If the page you want to open contains JS generated data you have to use SplashRequest(or SplashFormRequest) to render the page. Here鈥檚 a simple example:\nclass MySpider(scrapy.Spider):\n    name = ""jsscraper""\n    start_urls = [""http://quotes.toscrape.com/js/""]\n\n    def start_requests(self):\n        for url in self.start_urls:\n        yield SplashRequest(\n            url=url, callback=self.parse, endpoint=\'render.html\'\n        )\n\n    def parse(self, response):\n        for q in response.css(""div.quote""):\n        quote = QuoteItem()\n        quote[""author""] = q.css("".author::text"").extract_first()\n        quote[""quote""] = q.css("".text::text"").extract_first()\n        yield quote\n\nSplashRequest renders the URL as html and returns the response which you can use in the callback(parse) method.\n\n\n\nSolution 2: Let\'s call this experimental at the moment (May 2018)...\nThis solution is for Python\'s version 3.6 only (at the moment).\nDo you know the requests module (well who doesn\'t)?\nNow it has a web crawling little sibling: requests-HTML:\n\nThis library intends to make parsing HTML (e.g. scraping the web) as simple and intuitive as possible.\n\n\nInstall requests-html: pipenv install requests-html\nMake a request to the page\'s url:\nfrom requests_html import HTMLSession\n\nsession = HTMLSession()\nr = session.get(a_page_url)\n\nRender the response to get the Javascript generated bits:\nr.html.render()\n\n\nFinally, the module seems to offer scraping capabilities.\nAlternatively, we can try the well-documented way of using BeautifulSoup with the r.html object we just rendered.\n', '\nMaybe selenium can do it.\nfrom selenium import webdriver\nimport time\n\ndriver = webdriver.Firefox()\ndriver.get(url)\ntime.sleep(5)\nhtmlSource = driver.page_source\n\n', ""\nIf you have ever used the Requests module for python before, I recently found out that the developer created a new module called Requests-HTML which now also has the ability to render JavaScript.\nYou can also visit https://html.python-requests.org/ to learn more about this module, or if your only interested about rendering JavaScript then you can visit https://html.python-requests.org/?#javascript-support to directly learn how to use the module to render JavaScript using Python.\nEssentially, Once you correctly install the Requests-HTML module, the following example, which is shown on the above link, shows how you can use this module to scrape a website and render JavaScript contained within the website:\nfrom requests_html import HTMLSession\nsession = HTMLSession()\n\nr = session.get('http://python-requests.org/')\n\nr.html.render()\n\nr.html.search('Python 2 will retire in only {months} months!')['months']\n\n'<time>25</time>' #This is the result.\n\nI recently learnt about this from a YouTube video. Click Here! to watch the YouTube video, which demonstrates how the module works.\n"", ""\nIt sounds like the data you're really looking for can be accessed via secondary URL called by some javascript on the primary page.\nWhile you could try running javascript on the server to handle this, a simpler approach  to might be to load up the page using Firefox and use a tool like Charles or Firebug to identify exactly what that secondary URL is. Then you can just query that URL directly for the data you are interested in.\n"", '\nThis seems to be a good solution also, taken from a great blog post\nimport sys  \nfrom PyQt4.QtGui import *  \nfrom PyQt4.QtCore import *  \nfrom PyQt4.QtWebKit import *  \nfrom lxml import html \n\n#Take this class for granted.Just use result of rendering.\nclass Render(QWebPage):  \n  def __init__(self, url):  \n    self.app = QApplication(sys.argv)  \n    QWebPage.__init__(self)  \n    self.loadFinished.connect(self._loadFinished)  \n    self.mainFrame().load(QUrl(url))  \n    self.app.exec_()  \n\n  def _loadFinished(self, result):  \n    self.frame = self.mainFrame()  \n    self.app.quit()  \n\nurl = \'http://pycoders.com/archive/\'  \nr = Render(url)  \nresult = r.frame.toHtml()\n# This step is important.Converting QString to Ascii for lxml to process\n\n# The following returns an lxml element tree\narchive_links = html.fromstring(str(result.toAscii()))\nprint archive_links\n\n# The following returns an array containing the URLs\nraw_links = archive_links.xpath(\'//div[@class=""campaign""]/a/@href\')\nprint raw_links\n\n', '\nSelenium is the best for scraping JS and Ajax content.\nCheck this article for extracting data from the web using Python\n$ pip install selenium\n\nThen download Chrome webdriver.\nfrom selenium import webdriver\n\nbrowser = webdriver.Chrome()\n\nbrowser.get(""https://www.python.org/"")\n\nnav = browser.find_element_by_id(""mainnav"")\n\nprint(nav.text)\n\nEasy, right?\n', ""\nYou can also execute javascript using webdriver.\nfrom selenium import webdriver\n\ndriver = webdriver.Firefox()\ndriver.get(url)\ndriver.execute_script('document.title')\n\nor store the value in a variable\nresult = driver.execute_script('var text = document.title ; return text')\n\n"", '\nI personally prefer using scrapy and selenium and dockerizing both in separate containers. This way you can install both with minimal hassle and crawl modern websites that almost all contain javascript in one form or another. Here\'s an example:\nUse the scrapy startproject to create your scraper and write your spider, the skeleton can be as simple as this:\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = \'my_spider\'\n    start_urls = [\'https://somewhere.com\']\n\n    def start_requests(self):\n        yield scrapy.Request(url=self.start_urls[0])\n\n\n    def parse(self, response):\n\n        # do stuff with results, scrape items etc.\n        # now were just checking everything worked\n\n        print(response.body)\n\nThe real magic happens in the middlewares.py. Overwrite two methods in the downloader middleware,  __init__ and  process_request, in the following way:\n# import some additional modules that we need\nimport os\nfrom copy import deepcopy\nfrom time import sleep\n\nfrom scrapy import signals\nfrom scrapy.http import HtmlResponse\nfrom selenium import webdriver\n\nclass SampleProjectDownloaderMiddleware(object):\n\ndef __init__(self):\n    SELENIUM_LOCATION = os.environ.get(\'SELENIUM_LOCATION\', \'NOT_HERE\')\n    SELENIUM_URL = f\'http://{SELENIUM_LOCATION}:4444/wd/hub\'\n    chrome_options = webdriver.ChromeOptions()\n\n    # chrome_options.add_experimental_option(""mobileEmulation"", mobile_emulation)\n    self.driver = webdriver.Remote(command_executor=SELENIUM_URL,\n                                   desired_capabilities=chrome_options.to_capabilities())\n\n\ndef process_request(self, request, spider):\n\n    self.driver.get(request.url)\n\n    # sleep a bit so the page has time to load\n    # or monitor items on page to continue as soon as page ready\n    sleep(4)\n\n    # if you need to manipulate the page content like clicking and scrolling, you do it here\n    # self.driver.find_element_by_css_selector(\'.my-class\').click()\n\n    # you only need the now properly and completely rendered html from your page to get results\n    body = deepcopy(self.driver.page_source)\n\n    # copy the current url in case of redirects\n    url = deepcopy(self.driver.current_url)\n\n    return HtmlResponse(url, body=body, encoding=\'utf-8\', request=request)\n\nDont forget to enable this middlware by uncommenting the next lines in the settings.py file:\nDOWNLOADER_MIDDLEWARES = {\n\'sample_project.middlewares.SampleProjectDownloaderMiddleware\': 543,}\n\nNext for dockerization. Create your Dockerfile from a lightweight image (I\'m using python Alpine here), copy your project directory to it, install requirements:\n# Use an official Python runtime as a parent image\nFROM python:3.6-alpine\n\n# install some packages necessary to scrapy and then curl because it\'s  handy for debugging\nRUN apk --update add linux-headers libffi-dev openssl-dev build-base libxslt-dev libxml2-dev curl python-dev\n\nWORKDIR /my_scraper\n\nADD requirements.txt /my_scraper/\n\nRUN pip install -r requirements.txt\n\nADD . /scrapers\n\nAnd finally bring it all together in docker-compose.yaml:\nversion: \'2\'\nservices:\n  selenium:\n    image: selenium/standalone-chrome\n    ports:\n      - ""4444:4444""\n    shm_size: 1G\n\n  my_scraper:\n    build: .\n    depends_on:\n      - ""selenium""\n    environment:\n      - SELENIUM_LOCATION=samplecrawler_selenium_1\n    volumes:\n      - .:/my_scraper\n    # use this command to keep the container running\n    command: tail -f /dev/null\n\nRun docker-compose up -d. If you\'re doing this the first time it will take a while for it to fetch the latest selenium/standalone-chrome and the build your scraper image as well. \nOnce it\'s done, you can check that your containers are running with docker ps and also check that the name of the selenium container matches that of the environment variable that we passed to our scraper container (here, it was SELENIUM_LOCATION=samplecrawler_selenium_1). \nEnter your scraper container with docker exec -ti YOUR_CONTAINER_NAME sh , the command for me was docker exec -ti samplecrawler_my_scraper_1 sh, cd into the right directory and run your scraper with scrapy crawl my_spider.\nThe entire thing is on my github page and you can get it from here\n', '\nA mix of BeautifulSoup and Selenium works very well for me.\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup as bs\n\ndriver = webdriver.Firefox()\ndriver.get(""http://somedomain/url_that_delays_loading"")\n    try:\n        element = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.ID, ""myDynamicElement""))) #waits 10 seconds until element is located. Can have other wait conditions  such as visibility_of_element_located or text_to_be_present_in_element\n\n        html = driver.page_source\n        soup = bs(html, ""lxml"")\n        dynamic_text = soup.find_all(""p"", {""class"":""class_name""}) #or other attributes, optional\n    else:\n        print(""Couldnt locate element"")\n\nP.S. You can find more wait conditions here\n', '\nUsing PyQt5\nfrom PyQt5.QtWidgets import QApplication\nfrom PyQt5.QtCore import QUrl\nfrom PyQt5.QtWebEngineWidgets import QWebEnginePage\nimport sys\nimport bs4 as bs\nimport urllib.request\n\n\nclass Client(QWebEnginePage):\n    def __init__(self,url):\n        global app\n        self.app = QApplication(sys.argv)\n        QWebEnginePage.__init__(self)\n        self.html = """"\n        self.loadFinished.connect(self.on_load_finished)\n        self.load(QUrl(url))\n        self.app.exec_()\n\n    def on_load_finished(self):\n        self.html = self.toHtml(self.Callable)\n        print(""Load Finished"")\n\n    def Callable(self,data):\n        self.html = data\n        self.app.quit()\n\n# url = """"\n# client_response = Client(url)\n# print(client_response.html)\n\n', ""\nYou'll want to use urllib, requests, beautifulSoup and selenium web driver in your script for different parts of the page, (to name a few).\nSometimes you'll get what you need with just one of these modules.\nSometimes you'll need two, three, or all of these modules.\nSometimes you'll need to switch off the js on your browser.\nSometimes you'll need header info in your script.\nNo websites can be scraped the same way and no website can be scraped in the same way forever without having to modify your crawler, usually after a few months. But they can all be scraped! Where there's a will there's a way for sure.\nIf you need scraped data continuously into the future just scrape everything you need and store it in .dat files with pickle.\nJust keep searching how to try what with these modules and copying and pasting your errors into the Google.\n"", '\nPyppeteer\nYou might consider Pyppeteer, a Python port of the Chrome/Chromium driver front-end Puppeteer.\nHere\'s a simple example to show how you can use Pyppeteer to access data that was injected into the page dynamically:\nimport asyncio\nfrom pyppeteer import launch\n\nasync def main():\n    browser = await launch({""headless"": True})\n    [page] = await browser.pages()\n\n    # normally, you go to a live site...\n    #await page.goto(""http://www.example.com"")\n    # but for this example, just set the HTML directly:\n    await page.setContent(""""""\n    <body>\n    <script>\n    // inject content dynamically with JS, not part of the static HTML!\n    document.body.innerHTML = `<p>hello world</p>`; \n    </script>\n    </body>\n    """""")\n    print(await page.content()) # shows that the `<p>` was inserted\n\n    # evaluate a JS expression in browser context and scrape the data\n    expr = ""document.querySelector(\'p\').textContent""\n    print(await page.evaluate(expr, force_expr=True)) # => hello world\n\n    await browser.close()\n\nasyncio.get_event_loop().run_until_complete(main())\n\nSee Pyppeteer\'s reference docs.\n', '\nTry accessing the API directly\nA common scenario you\'ll see in scraping is that the data is being requested asynchronously from an API endpoint by the webpage. A minimal example of this would be the following site:\n\n\n<body>\n<script>\nfetch(""https://jsonplaceholder.typicode.com/posts/1"")\n  .then(res => {\n    if (!res.ok) throw Error(res.status);\n    \n    return res.json();\n  })\n  .then(data => {\n    // inject data dynamically via JS after page load\n    document.body.innerText = data.title;\n  })\n  .catch(err => console.error(err))\n;\n</script>\n</body>\n\n\nIn many cases, the API will be protected by CORS or an access token or prohibitively rate limited, but in other cases it\'s publicly-accessible and you can bypass the website entirely. For CORS issues, you might try cors-anywhere.\nThe general procedure is to use your browser\'s developer tools\' network tab to search the requests made by the page for keywords/substrings of the data you want to scrape. Often, you\'ll see an unprotected API request endpoint with a JSON payload that you can access directly with urllib or requests modules. That\'s the case with the above runnable snippet which you can use to practice. After clicking ""run snippet"", here\'s how I found the endpoint in my network tab:\n\nThis example is contrived; the endpoint URL will likely be non-obvious from looking at the static markup because it could be dynamically assembled, minified and buried under dozens of other requests and endpoints. The network request will also show any relevant request payload details like access token you may need.\nAfter obtaining the endpoint URL and relevant details, build a request in Python using a standard HTTP library and request the data:\n>>> import requests\n>>> res = requests.get(""https://jsonplaceholder.typicode.com/posts/1"")\n>>> data = res.json()\n>>> data[""title""]\n\'sunt aut facere repellat provident occaecati excepturi optio reprehenderit\'\n\nWhen you can get away with it, this tends to be much easier, faster and more reliable than scraping the page with Selenium, Pyppeteer, Scrapy or whatever the popular scraping libraries are at the time you\'re reading this post.\nIf you\'re unlucky and the data hasn\'t arrived via an API request that returns the data in a nice format, it could be part of the original browser\'s payload in a <script> tag, either as a JSON string or (more likely) a JS object. For example:\n\n\n<body>\n<script>\n  var someHardcodedData = {\n    userId: 1,\n    id: 1,\n    title: \'sunt aut facere repellat provident occaecati excepturi optio reprehenderit\', \n    body: \'quia et suscipit\\nsuscipit recusandae con sequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\'\n  };\n  document.body.textContent = someHardcodedData.title;\n</script>\n</body>\n\n\nThere\'s no one-size-fits-all way to obtain this data. The basic technique is to use BeautifulSoup to access the <script> tag text, then apply a regex or a parse to extract the object structure, JSON string, or whatever format the data might be in. Here\'s a proof-of-concept on the sample structure shown above:\nimport json\nimport re\nfrom bs4 import BeautifulSoup\n\n# pretend we\'ve already used requests to retrieve the data, \n# so we hardcode it for the purposes of this example\ntext = """"""\n<body>\n<script>\n  var someHardcodedData = {\n    userId: 1,\n    id: 1,\n    title: \'sunt aut facere repellat provident occaecati excepturi optio reprehenderit\', \n    body: \'quia et suscipit\\nsuscipit recusandae con sequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\'\n  };\n  document.body.textContent = someHardcodedData.title;\n</script>\n</body>\n""""""\nsoup = BeautifulSoup(text, ""lxml"")\nscript_text = str(soup.select_one(""script""))\npattern = r""title: \'(.*?)\'""\nprint(re.search(pattern, script_text, re.S).group(1))\n\nCheck out these resources for parsing JS objects that aren\'t quite valid JSON:\n\nHow to convert raw javascript object to python dictionary?\nHow to Fix JSON Key Values without double-quotes?\n\nHere are some additional case studies/proofs-of-concept where scraping was bypassed using an API:\n\nHow can I scrape yelp reviews and star ratings into CSV using Python beautifulsoup\nBeautiful Soup returns None on existing element\nExtract data from  BeautifulSoup Python\nScraping Bandcamp fan collections via POST (uses a hybrid approach where an initial request was made to the website to extract a token from the markup using BeautifulSoup which was then used in a second request to a JSON endpoint)\n\nIf all else fails, try one of the many dynamic scraping libraries listed in this thread.\n', '\nPlaywright-Python\nYet another option is playwright-python, a port of Microsoft\'s Playwright (itself a Puppeteer-influenced browser automation library) to Python.\nHere\'s the minimal example of selecting an element and grabbing its text:\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch()\n    page = browser.new_page()\n    page.goto(""http://whatsmyuseragent.org/"")\n    ua = page.query_selector("".user-agent"");\n    print(ua.text_content())\n    browser.close()\n\n', '\nAs mentioned, Selenium is a good choice for rendering the results of the JavaScript:\nfrom selenium.webdriver import Firefox\nfrom selenium.webdriver.firefox.options import Options\n\noptions = Options()\noptions.headless = True\nbrowser = Firefox(executable_path=""/usr/local/bin/geckodriver"", options=options)\n\nurl = ""https://www.example.com""\nbrowser.get(url)\n\nAnd gazpacho is a really easy library to parse over the rendered html:\nfrom gazpacho import Soup\n\nsoup = Soup(browser.page_source)\nsoup.find(""a"").attrs[\'href\']\n\n', '\nI recently used requests_html library to solve this problem.\nTheir expanded documentation at readthedocs.io is pretty good (skip the annotated version at pypi.org). If your use case is basic, you are likely to have some success.\nfrom requests_html import HTMLSession\nsession = HTMLSession()\nresponse = session.request(method=""get"",url=""www.google.com/"")\nresponse.html.render()\n\nIf you are having trouble rendering the data you need with response.html.render(), you can pass some javascript to the render function to render the particular js object you need. This is copied from their docs, but it might be just what you need:\n\nIf script is specified, it will execute the provided JavaScript at\nruntime. Example:\n\nscript = """"""\n    () => {\n        return {\n            width: document.documentElement.clientWidth,\n            height: document.documentElement.clientHeight,\n            deviceScaleFactor: window.devicePixelRatio,\n        }\n    } \n""""""\n\n\nReturns the return value of the executed script, if any is provided:\n\n>>> response.html.render(script=script)\n{\'width\': 800, \'height\': 600, \'deviceScaleFactor\': 1}\n\nIn my case, the data I wanted were the arrays that populated a javascript plot but the data wasn\'t getting rendered as text anywhere in the html. Sometimes its not clear at all what the object names are of the data you want if the data is populated dynamically. If you can\'t track down the js objects directly from view source or inspect, you can type in ""window"" followed by ENTER in the debugger console in the browser (Chrome) to pull up a full list of objects rendered by the browser. If you make a few educated guesses about where the data is stored, you might have some luck finding it there. My graph data was under window.view.data in the console, so in the ""script"" variable passed to the .render() method quoted above, I used:\nreturn {\n    data: window.view.data\n}\n\n', '\nEasy and Quick Solution:\nI was dealing with same problem. I want to scrape some data which is build with JavaScript. If I scrape only text from this site with BeautifulSoup then I ended with  tags in text.\nI want to render this  tag and wills to grab information from this.\nAlso, I dont want to use heavy frameworks  like Scrapy and selenium.\nSo, I found that get method of requests module takes urls, and it actually renders the script tag.\nExample:\nimport requests\ncustom_User_agent = ""Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0""\nurl = ""https://www.abc.xyz/your/url""\nresponse = requests.get(url, headers={""User-Agent"": custom_User_agent})\nhtml_text = response.text\n\nThis will renders load site and renders  tags.\nHope this will help as quick and easy solution to render site which is loaded with script tags.\n']",https://stackoverflow.com/questions/8049520/web-scraping-javascript-page-with-python,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping data to Google Sheets from a website that uses JavaScript,"
I am trying to import data from the following website to Google Sheets. I want to import all the matches for the day.
https://www.tournamentsoftware.com/tournament/b731fdcd-a0c8-4558-9344-2a14c267ee8b/Matches
I have tried importxml and importhtml, but it seems this does not work as the website uses JavaScript. I have also tried to use Apipheny without any success.
When using Apipheny, the error message is

'Failed to fetch data - please verify your API Request: {DNS error'

",1k,"
            1
        ","['\nTl;Dr\nAdapted from my answer to How to know if Google Sheets IMPORTDATA, IMPORTFEED, IMPORTHTML or IMPORTXML functions are able to get data from a resource hosted on a website? (also posted by me)\nPlease spend some time learning how to use the browsers developers tools so you will be able to identify\n\nif the data is already included in source code of the webpage as JSON / literal JavaScript object or in another form\nif the webpage is doing a GET or POST requests to retrieve the data and when those requests are done (i.e. as some point of the page parsing, or on event)\nif the requests require data from cookies\n\n\nBrief guide about how to use the web browser to find useful details about the webpage / data to import\n\nOpen the source code and look if the required data is included. Sometimes the data is included as JSON and added to the DOM using JavaScript. In this case it might be possible to retrieve the data by using the Google Sheets functions or URL Fetch Service from Google Apps Script.\nLet say that you use Chrome. Open the Dev Tools, then look at the Elements tab. There you will see the DOM. It might be helpful to identify if the data that you want to import besides being on visible elements is included in hidden / not visible elements like <script> tags.\nLook at Source, there you might be able to see the JavaScript code. It might include the data that you want to import as JavaScript object (commonly referred as JSON).\n\n\nThere are a lot of questions about google-sheets +web-scraping that mentions problems using importhtml and/or importxml that already have answers and even many include code (JavaScript snippets, Google Apps Script functions, etc.) that might save you to have to use an specialized web-scraping tool that has a more stepped learning curve. At the bottom of this answer there is a list of questions about using Google Sheets built-in functions, including annotations of the workaround proposed.\nOn Is there a way to get a single response from a text/event-stream without using event listeners? ask about using EventSource. While this can\'t be used on server side code, the answer show how to use the HtmlService to use it on client-side code and retrieve the result to Google Sheets.\n\nAs you already realized, the Google Sheets built-in functions importhtml(), importxml(), importdata() and importfeed() only work with static pages that do not require signing in or other forms of authentication.\nWhen the content of a public page is created dynamically by using JavaScript, it cannot be accessed with those functions, by the other hand the website\'s webmaster may also purposefully have prevented web scraping.\n\nHow to identify if content is added dynamically\nTo check if the content is added dynamically, using Chrome,\n\nOpen the URL of the source data.\nPress F12 to open Chrome Developer Tools\nPress Control+Shift+P to open the Command Menu.\nStart typing javascript, select Disable JavaScript, and then press Enter to run the command. JavaScript is now disabled.\n\nJavaScript will remain disabled in this tab so long as you have DevTools open.\nReload the page to see if the content that you want to import is shown, if it\'s shown it could be imported by using Google Sheets built-in functions, otherwise it\'s not possible but might be possible by using other means for doing web scraping.\n\n According to Wikipedia,\n\n Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.\n\nUse of robots.txt to block Web crawlers\nThe webmasters could use robots.txt file to block access to website. In such case the result will be #N/A Could not fetch URL.\nUse of User agent\nThe webpage could be designed to return a special a custom message instead of the data.\n\nBelow there are more details about how Google Sheets built-in ""web-scraping"" functions works\nIMPORTDATA, IMPORTFEED, IMPORTHTML and IMPORTXML are able to get content from resources hosted on websites that are:\n\nPublicly available. This means that the resource doesn\'t require authorization / to be logged in into any service to access it.\nThe content is ""static"". This mean that if you open the resource using the view source code option of modern web browsers it will be displayed as plain text.\n\nNOTE: The Chrome\'s Inspect tool shows the parsed DOM; in other works the actual structure/content of the web page which could be dynamically modified by JavaScript code or browser extensions/plugins.\n\n\nThe content has the appropriated structure.\n\nIMPORTDATA works with structured content as csv or tsv doesn\'t matter of the file extension of the resource.\nIMPORTFEED works with marked up content as ATOM/RSS\nIMPORTHTML works with marked up content as HTML that includes properly markedup list or tables.\nIMPORTXML works with marked up content as XML or any of its variants like XHTML.\n\n\nThe content doesn\'t exceeds the maximum size. Google haven\'t disclosed this limit but the below error will be shown when the content exceeds the maximum size:\n\nResource at url contents exceeded maximum size.\n\n\nGoogle servers are not blocked by means of robots.txt or the user agent.\n\nOn W3C Markup Validator there are several tools to checkout is the resources had been properly marked up.\nRegarding CSV check out Are there known services to validate CSV files\nIt\'s worth to note that the spreadsheet\n\nshould have enough room for the imported content; Google Sheets has a 10 million cell limit by spreadsheet, according to this post a columns limit of 18278, and a 50 thousand characters as cell content even as a value or formula.\nit doesn\'t handle well large in-cell content; the ""limit"" depends on the user screen size and resolution as now it\'s possible to zoom in/out.\n\n\nReferences\n\nhttps://developers.google.com/web/tools/chrome-devtools/javascript/disable\nhttps://en.wikipedia.org/wiki/Web_scraping\n\nRelated\n\nUsing Google Apps Script to scrape Dynamic Web Pages\nScraping data from website using vba\nBlock Website Scraping by Google Docs\nIs there a way to get a single response from a text/event-stream without using event listeners?\n\nSoftware Recommendations\n\nWeb scraping tool/software available for free?\nRecommendations for web scraping tools that require minimal installation\n\nWeb Applications\nThe following question is about a different result, #N/A Could not fetch URL\n\nInability to use IMPORTHTML in Google sheets\n\n\nSimilar questions\nSome of this questions might be closed as duplicate of this one\n\nImporting javascript table into Google Docs spreadsheet\nImportxml Imported Content Empty\nscrape table using google app scripts\n\nOne answer includes Google Apps Script code using the URL Fetch Service\n\n\nCapture element using ImportXML with XPath\nHow to import Javascript tables into Google spreadsheet?\nScrape the current share price data from the ASX\n\nOne of the answers includes Google Apps Script code to get data from a JSON source\n\n\nGuidance on webscraping using Google Sheets\nHow to Scrape data from Indiegogo.com in google sheets via IMPORTXML formula\nWhy importxml and importhtml not working here?\nGoogle Sheet use Importxml error could not fetch url\n\n\n\nOne answer includes Google Apps Script code using the URL Fetch Service\n\n\n\n\nGoogle Sheets - Pull Data for investment portfolio\nExtracting value from API/Webpage\nIMPORTXML shows an error while scraping data from website\n\nOne answer shows the xhr request found using browser developer tools\n\n\nReplacing =ImportHTML with URLFetchApp\n\nOne answer includes Google Apps Script code using the URL Fetch Service\n\n\nHow to use IMPORTXML to import hidden div tag?\nGoogle Sheet Web-scraping ImportXml Xpath on Yahoo Finance doesn\'t works with french stock\n\nOne of the answers includes Google Apps Script code to get data from a JSON source. As of January 4th 2023, it\'s not longer working, very likely because Yahoo! Finance is now encrying the JSON. See the Tainake\'s answer to How to pull Yahoo Finance Historical Price Data from its Object with Google Apps Script? for script using Crypto.js to handle this.\n\n\nHow to fetch data which is loaded by the ajax (asynchronous) method after the web page has already been loaded using apps script?\n\nOne answer suggest to read the data from the server instead of scraping from a webpage.\n\n\nUsing ImportXML to pull data\nExtracting data from web page using Cheerio Library\n\nOne answer suggest the use of an API and Google Apps Script\n\n\n\n', '\nImportXML is good for basic tasks, but it won\'t get you too far if you are serious in scraping:\n\nThe approach only works with the most basic websites (no SPAs rendered in browsers can be scraped this way. Any basic web scraping protection or connectivity issue breaks the process, and there isn\'t any control over HTTP request geo location, or number of retries) - and Yahoo Finance is not a simple website\n\nIf the target website data requires some cleanup post-processing, it\'s getting very complicated since you are now ""programming with Excel formulas"", rather a painful process compared to regular code writing in conventional programming languages\n\nThere isn\'t any proper launch and cache control, so the function can be triggered occasionally and if the HTTP request fails, cells will be populated with ERR! values\n\n\nI recommend using proper tools (automation framework and scraping engine which can render JavaScript-powered websites) and use Google Sheets just for basic storage purposes:\nhttps://youtu.be/uBC752CWTew (Pipedream for automation and ScrapeNinja engine for scraping)\n']",https://stackoverflow.com/questions/74237688/scraping-data-to-google-sheets-from-a-website-that-uses-javascript,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
retrieve links from web page using python and BeautifulSoup [closed],"






Closed. This question needs details or clarity. It is not currently accepting answers.
                        
                    










Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



How can I retrieve the links of a webpage and copy the url address of the links using Python?
",319k,"
            178
        ","[""\nHere's a short snippet using the SoupStrainer class in BeautifulSoup:\nimport httplib2\nfrom bs4 import BeautifulSoup, SoupStrainer\n\nhttp = httplib2.Http()\nstatus, response = http.request('http://www.nytimes.com')\n\nfor link in BeautifulSoup(response, parse_only=SoupStrainer('a')):\n    if link.has_attr('href'):\n        print(link['href'])\n\nThe BeautifulSoup documentation is actually quite good, and covers a number of typical scenarios:\nhttps://www.crummy.com/software/BeautifulSoup/bs4/doc/\nEdit: Note that I used the SoupStrainer class because it's a bit more efficient (memory and speed wise), if you know what you're parsing in advance.\n"", '\nFor completeness sake, the BeautifulSoup 4 version, making use of the encoding supplied by the server as well:\nfrom bs4 import BeautifulSoup\nimport urllib.request\n\nparser = \'html.parser\'  # or \'lxml\' (preferred) or \'html5lib\', if installed\nresp = urllib.request.urlopen(""http://www.gpsbasecamp.com/national-parks"")\nsoup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param(\'charset\'))\n\nfor link in soup.find_all(\'a\', href=True):\n    print(link[\'href\'])\n\nor the Python 2 version:\nfrom bs4 import BeautifulSoup\nimport urllib2\n\nparser = \'html.parser\'  # or \'lxml\' (preferred) or \'html5lib\', if installed\nresp = urllib2.urlopen(""http://www.gpsbasecamp.com/national-parks"")\nsoup = BeautifulSoup(resp, parser, from_encoding=resp.info().getparam(\'charset\'))\n\nfor link in soup.find_all(\'a\', href=True):\n    print link[\'href\']\n\nand a version using the requests library, which as written will work in both Python 2 and 3:\nfrom bs4 import BeautifulSoup\nfrom bs4.dammit import EncodingDetector\nimport requests\n\nparser = \'html.parser\'  # or \'lxml\' (preferred) or \'html5lib\', if installed\nresp = requests.get(""http://www.gpsbasecamp.com/national-parks"")\nhttp_encoding = resp.encoding if \'charset\' in resp.headers.get(\'content-type\', \'\').lower() else None\nhtml_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\nencoding = html_encoding or http_encoding\nsoup = BeautifulSoup(resp.content, parser, from_encoding=encoding)\n\nfor link in soup.find_all(\'a\', href=True):\n    print(link[\'href\'])\n\nThe soup.find_all(\'a\', href=True) call finds all <a> elements that have an href attribute; elements without the attribute are skipped.\nBeautifulSoup 3 stopped development in March 2012; new projects really should use BeautifulSoup 4, always.\nNote that you should leave decoding the HTML from bytes to BeautifulSoup. You can inform BeautifulSoup of the characterset found in the HTTP response headers to assist in decoding, but this can be wrong and conflicting with a <meta> header info found in the HTML itself, which is why the above uses the BeautifulSoup internal class method EncodingDetector.find_declared_encoding() to make sure that such embedded encoding hints win over a misconfigured server.\nWith requests, the response.encoding attribute defaults to Latin-1 if the response has a text/* mimetype, even if no characterset was returned. This is consistent with the HTTP RFCs but painful when used with HTML parsing, so you should ignore that attribute when no charset is set in the Content-Type header.\n', '\nOthers have recommended BeautifulSoup, but it\'s much better to use lxml. Despite its name, it is also for parsing and scraping HTML. It\'s much, much faster than BeautifulSoup, and it even handles ""broken"" HTML better than BeautifulSoup (their claim to fame). It has a compatibility API for BeautifulSoup too if you don\'t want to learn the lxml API.\nIan Blicking agrees.\nThere\'s no reason to use BeautifulSoup anymore, unless you\'re on Google App Engine or something where anything not purely Python isn\'t allowed.\nlxml.html also supports CSS3 selectors so this sort of thing is trivial.\nAn example with lxml and xpath would look like this:\nimport urllib\nimport lxml.html\nconnection = urllib.urlopen(\'http://www.nytimes.com\')\n\ndom =  lxml.html.fromstring(connection.read())\n\nfor link in dom.xpath(\'//a/@href\'): # select the url in href for all a tags(links)\n    print link\n\n', '\nimport urllib2\nimport BeautifulSoup\n\nrequest = urllib2.Request(""http://www.gpsbasecamp.com/national-parks"")\nresponse = urllib2.urlopen(request)\nsoup = BeautifulSoup.BeautifulSoup(response)\nfor a in soup.findAll(\'a\'):\n  if \'national-park\' in a[\'href\']:\n    print \'found a url with national-park in the link\'\n\n', '\nThe following code is to retrieve all the links available in a webpage using urllib2 and BeautifulSoup4:\nimport urllib2\nfrom bs4 import BeautifulSoup\n\nurl = urllib2.urlopen(""http://www.espncricinfo.com/"").read()\nsoup = BeautifulSoup(url)\n\nfor line in soup.find_all(\'a\'):\n    print(line.get(\'href\'))\n\n', '\nLinks can be within a variety of attributes so you could pass a list of those attributes to select.\nFor example, with src and href attributes (here I am using the starts with ^ operator to specify that either of these attributes values starts with http):\nfrom bs4 import BeautifulSoup as bs\nimport requests\nr = requests.get(\'https://stackoverflow.com/\')\nsoup = bs(r.content, \'lxml\')\nlinks = [item[\'href\'] if item.get(\'href\') is not None else item[\'src\'] for item in soup.select(\'[href^=""http""], [src^=""http""]\') ]\nprint(links)\n\nAttribute = value selectors\n\n[attr^=value]\nRepresents elements with an attribute name of attr whose value is prefixed (preceded) by value.\n\nThere are also the commonly used $ (ends with) and * (contains) operators. For a full syntax list see the link above.\n', '\nUnder the hood BeautifulSoup now uses lxml. Requests, lxml & list comprehensions makes a killer combo.\nimport requests\nimport lxml.html\n\ndom = lxml.html.fromstring(requests.get(\'http://www.nytimes.com\').content)\n\n[x for x in dom.xpath(\'//a/@href\') if \'//\' in x and \'nytimes.com\' not in x]\n\nIn the list comp, the ""if \'//\' and \'url.com\' not in x"" is a simple method to scrub the url list of the sites \'internal\' navigation urls, etc.\n', '\njust for getting the links, without B.soup and regex:\nimport urllib2\nurl=""http://www.somewhere.com""\npage=urllib2.urlopen(url)\ndata=page.read().split(""</a>"")\ntag=""<a href=\\""""\nendtag=""\\"">""\nfor item in data:\n    if ""<a href"" in item:\n        try:\n            ind = item.index(tag)\n            item=item[ind+len(tag):]\n            end=item.index(endtag)\n        except: pass\n        else:\n            print item[:end]\n\nfor more complex operations, of course BSoup is still preferred.\n', ""\nThis script does what your looking for, But also resolves the relative links to absolute links.\nimport urllib\nimport lxml.html\nimport urlparse\n\ndef get_dom(url):\n    connection = urllib.urlopen(url)\n    return lxml.html.fromstring(connection.read())\n\ndef get_links(url):\n    return resolve_links((link for link in get_dom(url).xpath('//a/@href')))\n\ndef guess_root(links):\n    for link in links:\n        if link.startswith('http'):\n            parsed_link = urlparse.urlparse(link)\n            scheme = parsed_link.scheme + '://'\n            netloc = parsed_link.netloc\n            return scheme + netloc\n\ndef resolve_links(links):\n    root = guess_root(links)\n    for link in links:\n        if not link.startswith('http'):\n            link = urlparse.urljoin(root, link)\n        yield link  \n\nfor link in get_links('http://www.google.com'):\n    print link\n\n"", '\nTo find all the links, we will in this example use the urllib2 module together\nwith the re.module\n*One of the most powerful function in the re module is ""re.findall()"".\nWhile re.search() is used to find the first match for a pattern, re.findall() finds all\nthe matches and returns them as a list of strings, with each string representing one match*\nimport urllib2\n\nimport re\n#connect to a URL\nwebsite = urllib2.urlopen(url)\n\n#read html code\nhtml = website.read()\n\n#use re.findall to get all the links\nlinks = re.findall(\'""((http|ftp)s?://.*?)""\', html)\n\nprint links\n\n', '\nWhy not use regular expressions:\nimport urllib2\nimport re\nurl = ""http://www.somewhere.com""\npage = urllib2.urlopen(url)\npage = page.read()\nlinks = re.findall(r""<a.*?\\s*href=\\""(.*?)\\"".*?>(.*?)</a>"", page)\nfor link in links:\n    print(\'href: %s, HTML text: %s\' % (link[0], link[1]))\n\n', ""\nHere's an example using @ars accepted answer and the BeautifulSoup4, requests, and wget modules to handle the downloads.\nimport requests\nimport wget\nimport os\n\nfrom bs4 import BeautifulSoup, SoupStrainer\n\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/eeg_full/'\nfile_type = '.tar.gz'\n\nresponse = requests.get(url)\n\nfor link in BeautifulSoup(response.content, 'html.parser', parse_only=SoupStrainer('a')):\n    if link.has_attr('href'):\n        if file_type in link['href']:\n            full_path = url + link['href']\n            wget.download(full_path)\n\n"", ""\nI found the answer by @Blairg23 working , after the following correction (covering the scenario where it failed to work correctly):\nfor link in BeautifulSoup(response.content, 'html.parser', parse_only=SoupStrainer('a')):\n    if link.has_attr('href'):\n        if file_type in link['href']:\n            full_path =urlparse.urljoin(url , link['href']) #module urlparse need to be imported\n            wget.download(full_path)\n\nFor Python 3:\nurllib.parse.urljoin has to be used in order to obtain the full URL instead.\n"", '\nBeatifulSoup\'s own parser can be slow. It might be more feasible to use lxml which is capable of parsing directly from a URL (with some limitations mentioned below).\nimport lxml.html\n\ndoc = lxml.html.parse(url)\n\nlinks = doc.xpath(\'//a[@href]\')\n\nfor link in links:\n    print link.attrib[\'href\']\n\nThe code above will return the links as is, and in most cases they would be relative links or absolute from the site root. Since my use case was to only extract a certain type of links, below is a version that converts the links to full URLs and which optionally accepts a glob pattern like *.mp3. It won\'t handle single and double dots in the relative paths though, but so far I didn\'t have the need for it. If you need to parse URL fragments containing ../ or ./ then urlparse.urljoin might come in handy.\nNOTE: Direct lxml url parsing doesn\'t handle loading from https and doesn\'t do redirects, so for this reason the version below is using urllib2 + lxml.\n#!/usr/bin/env python\nimport sys\nimport urllib2\nimport urlparse\nimport lxml.html\nimport fnmatch\n\ntry:\n    import urltools as urltools\nexcept ImportError:\n    sys.stderr.write(\'To normalize URLs run: `pip install urltools --user`\')\n    urltools = None\n\n\ndef get_host(url):\n    p = urlparse.urlparse(url)\n    return ""{}://{}"".format(p.scheme, p.netloc)\n\n\nif __name__ == \'__main__\':\n    url = sys.argv[1]\n    host = get_host(url)\n    glob_patt = len(sys.argv) > 2 and sys.argv[2] or \'*\'\n\n    doc = lxml.html.parse(urllib2.urlopen(url))\n    links = doc.xpath(\'//a[@href]\')\n\n    for link in links:\n        href = link.attrib[\'href\']\n\n        if fnmatch.fnmatch(href, glob_patt):\n\n            if not href.startswith((\'http://\', \'https://\' \'ftp://\')):\n\n                if href.startswith(\'/\'):\n                    href = host + href\n                else:\n                    parent_url = url.rsplit(\'/\', 1)[0]\n                    href = urlparse.urljoin(parent_url, href)\n\n                    if urltools:\n                        href = urltools.normalize(href)\n\n            print href\n\nThe usage is as follows:\ngetlinks.py http://stackoverflow.com/a/37758066/191246\ngetlinks.py http://stackoverflow.com/a/37758066/191246 ""*users*""\ngetlinks.py http://fakedomain.mu/somepage.html ""*.mp3""\n\n', '\nThere can be many duplicate links together with both external and internal links.  To differentiate between the two and just get unique links using sets:\n# Python 3.\nimport urllib    \nfrom bs4 import BeautifulSoup\n\nurl = ""http://www.espncricinfo.com/""\nresp = urllib.request.urlopen(url)\n# Get server encoding per recommendation of Martijn Pieters.\nsoup = BeautifulSoup(resp, from_encoding=resp.info().get_param(\'charset\'))  \nexternal_links = set()\ninternal_links = set()\nfor line in soup.find_all(\'a\'):\n    link = line.get(\'href\')\n    if not link:\n        continue\n    if link.startswith(\'http\'):\n        external_links.add(link)\n    else:\n        internal_links.add(link)\n\n# Depending on usage, full internal links may be preferred.\nfull_internal_links = {\n    urllib.parse.urljoin(url, internal_link) \n    for internal_link in internal_links\n}\n\n# Print all unique external and full internal links.\nfor link in external_links.union(full_internal_links):\n    print(link)\n\n', '\nimport urllib2\nfrom bs4 import BeautifulSoup\na=urllib2.urlopen(\'http://dir.yahoo.com\')\ncode=a.read()\nsoup=BeautifulSoup(code)\nlinks=soup.findAll(""a"")\n#To get href part alone\nprint links[0].attrs[\'href\']\n\n']",https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Difference between text and innerHTML using Selenium,"
What鈥檚 the difference between getting text and innerHTML when using Selenium?
Even though we have text under a particular element, when we perform .text we get empty values. But doing .get_attribute(""innerHTML"") works fine.
What is the difference between two? When should someone use '.get_attribute(""innerHTML"")' over .text?
",16k,"
            14
        ","['\nTo start with, text is a property where as innerHTML is an attribute. Fundamentally there are some differences between a property and an attribute.\n\nget_attribute(""innerHTML"")\nget_attribute(innerHTML) gets the innerHTML of the element.\nThis method will first try to return the value of a property with the given name. If a property with that name doesn鈥檛 exist, it returns the value of the attribute with the same name. If there鈥檚 no attribute with that name, None is returned.\nValues which are considered truthy, that is equals true or false, are returned as booleans. All other non-None values are returned as strings. For attributes or properties which do not exist, None is returned.\n\nArguments:\ninnerHTML - Name of the attribute/property to retrieve.\n\n\nExample:\n# Extract the text of an element.\nmy_text = target_element.get_attribute(""innerHTML"")\n\n\n\n\ntext\ntext gets the text of the element.\n\nDefinition:\ndef text(self):\n    """"""The text of the element.""""""\n    return self._execute(Command.GET_ELEMENT_TEXT)[\'value\']\n\n\nExample:\n# Extract the text of an element.\nmy_text = target_element.text\n\n\n\nDoes it still sound similar? Read below...\n\nAttributes and properties\nWhen the browser loads the page, it parses the HTML and generates DOM objects from it. For element nodes, most standard HTML attributes automatically become properties of DOM objects.\nFor instance, if the tag is:\n<body id=""page"">\n\nthen the DOM object has body.id=""page"".\n\nNote: The attribute-property mapping is not one-to-one!\n\n\nHTML attributes\nIn HTML, tags may have attributes. When the browser parses the HTML to create DOM objects for tags, it recognizes standard attributes and creates DOM properties from them.\nSo when an element has id or another standard attribute, the corresponding property gets created. But that doesn鈥檛 happen if the attribute is non-standard.\n\nNote: A standard attribute for one element can be unknown for another one. For instance, type is standard attribute for <input> tag, but not for <body> tag. Standard attributes are described in the specification for the corresponding element class.\n\nSo, if an attribute is non-standard, there won鈥檛 be a DOM-property for it. In that case all attributes are accessible by using the following methods:\n\nelem.hasAttribute(name): checks for existence.\nelem.getAttribute(name): gets the value.\nelem.setAttribute(name, value): sets the value.\nelem.removeAttribute(name): removes the attribute.\n\nAn example of reading a non-standard property:\n<body something=""non-standard"">\n  <script>\n    alert(document.body.getAttribute(\'something\')); // non-standard\n  </script>\n</body>\n\n\nProperty-attribute synchronization\nWhen a standard attribute changes, the corresponding property is auto-updated, and (with some exceptions) vice versa. But there are exclusions, for instance input.value synchronizes only from attribute -> to property, but not back. This feature actually comes in handy, because the user may modify value, and then after it, if we want to recover the ""original"" value from HTML, it鈥檚 in the attribute.\n\nAs per Attributes and Properties in Python when we reference an attribute of an object with something like someObject.someAttr, Python uses several special methods to get the someAttr attribute of the object. In the simplest case, attributes are simply instance variables.\nPython Attributes\nIn a broader perspective:\n\nAn attribute is a name that appears after an object name. This is the syntactic construct. For example, someObj.name.\nAn instance variable is an item in the internal __dict__ of an object.\nThe default semantics of an attribute reference is to provide access to the instance variable. When we mention someObj.name, the default behavior is effectively someObj.__dict__[\'name\']\n\nPython Properties\nIn Python we can bind getter, setter (and deleter) functions with an attribute name, using the built-in property() function or @property decorator. When we do this, each reference to an attribute has the syntax of direct access to an instance variable, but it invokes the given method function.\n', '\n.text will retrieve an empty string of the text in not present in the view port, so you can scroll the object into the viewport and try .text. It should retrieve the value.\nOn the contrary, innerhtml can get the value, even if it is present outside the view port.\n', '\nFor instance, <div><span>Example Text</span></div>.\n.get_attribute(""innerHTML"") gives you the actual HTML inside the current element. So theDivElement.get_attribute(""innerHTML"") returns ""<span>Example Text</span>"".\n.text gives you only text, not including the HTML node. So theDivElement.text returns ""Example Text"".\nPlease note that the algorithm for .text depends on webdriver of each browser. In some cases, such as element is hidden, you might get different text when you use a different webdriver.\nI usually get text from .get_attribute(""innerText"") instead of .text, so I can handle the all the cases.\n', '\nChrome (I\'m not sure about other browsers) ignores the extra spaces within the HTML code and displays them as a single space.\n<div><span>Example  Text</span></div> <!-- Notice the two spaces -->\n\n.get_attribute(\'innerHTML\') will return the double-spaced text, which is what you would see when you inspect element), while .text will return the string with only 1 space.\n>>> print(element.get_attribute(\'innerHTML\'))\n\'Example  Text\'\n>>> print(element.text)\n\'Example Text\'\n\nThis difference is not trivial as the following will result in a NoSuchElementException.\n>>> arg = \'//div[contains(text(),""Example Text"")]\'\n>>> driver.find_element_by_xpath(arg)\n\nSimilarly, .get_attribute(\'innerHTML\') for the following returns Example&nbsp;Text, while .text returns Example Text.\n<div><span>Example&nbsp;Text</span></div>\n\n', '\nI have just selected the CSS selector and used the below code:\nfrom selenium import webdriver\n\ndriver = webdriver.Chrome()\ndriver.maximize_window()\ndriver.get(""http://www.costco.com/Weatherproof%C2%AE-Men\'s-Ultra-Tech-Jacket.product.100106552.html"")\nprint driver.find_element_by_css_selector("".product-h1-container.visible-xl-block>h1"").text\n\nand it prints:\nWeatherproof庐 Men\'s Ultra Tech Jacket\n\nThe problem is h1[itemprop=\'name\'] selector on Google Chrome or Chrome are returning two matching nodes while .product-h1-container.visible-xl-block>h1 is returning only one matching node. That鈥檚 why it\'s printing what is expected.\nTo prove my point, run the below code:\nfrom selenium import webdriver\n\ndriver = webdriver.Chrome()\ndriver.maximize_window()\ndriver.get(""http://www.costco.com/Weatherproof%C2%AE-Men\'s-Ultra-Tech-Jacket.product.100106552.html"")\nx= driver.find_elements_by_css_selector(""h1[itemprop=\'name\'] "")\n\nfor i in x:\n    print ""This is line "" , i.text\n\nIt will print\nThis is line\nThis is line  Weatherproof庐 Men\'s Ultra Tech Jacket\n\nBecause select_element_by_css_selector selects the first element with matching selector and that does not contain any text so it does not print. Hope you understand now\n']",https://stackoverflow.com/questions/40416048/difference-between-text-and-innerhtml-using-selenium,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I pass variable into an evaluate function?,"
I'm trying to pass a variable into a page.evaluate() function in Puppeteer, but when I use the following very simplified example, the variable evalVar is undefined.
I can't find any examples to build on, so I need help passing that variable into the page.evaluate() function so I can use it inside.
const puppeteer = require('puppeteer');

(async() => {

  const browser = await puppeteer.launch({headless: false});
  const page = await browser.newPage();

  const evalVar = 'WHUT??';

  try {

    await page.goto('https://www.google.com.au');
    await page.waitForSelector('#fbar');
    const links = await page.evaluate((evalVar) => {

      console.log('evalVar:', evalVar); // appears undefined

      const urls = [];
      hrefs = document.querySelectorAll('#fbar #fsl a');
      hrefs.forEach(function(el) {
        urls.push(el.href);
      });
      return urls;
    })
    console.log('links:', links);

  } catch (err) {

    console.log('ERR:', err.message);

  } finally {

    // browser.close();

  }

})();

",134k,"
            248
        ","['\nYou have to pass the variable as an argument to the pageFunction like this:\nconst links = await page.evaluate((evalVar) => {\n\n  console.log(evalVar); // 2. should be defined now\n  鈥n\n}, evalVar); // 1. pass variable as an argument\n\nYou can pass in multiple variables by passing more arguments to  page.evaluate():\nawait page.evaluate((a, b c) => { console.log(a, b, c) }, a, b, c)\n\nThe arguments must either be serializable as JSON or JSHandles of in-browser objects: https://pptr.dev/#?show=api-pageevaluatepagefunction-args\n', ""\nI encourage you to stick on this style, because it's more convenient and readable.\nlet name = 'jack';\nlet age  = 33;\nlet location = 'Berlin/Germany';\n\nawait page.evaluate(({name, age, location}) => {\n\n    console.log(name);\n    console.log(age);\n    console.log(location);\n\n},{name, age, location});\n\n"", '\nSingle Variable:\nYou can pass one variable to page.evaluate() using the following syntax:\nawait page.evaluate(example => { /* ... */ }, example);\n\n\nNote: You do not need to enclose the variable in (), unless you are going to be passing multiple variables.\n\nMultiple Variables:\nYou can pass multiple variables to page.evaluate() using the following syntax:\nawait page.evaluate((example_1, example_2) => { /* ... */ }, example_1, example_2);\n\n\nNote: Enclosing your variables within {} is not necessary.\n\n', ""\nIt took me quite a while to figure out that console.log() in evaluate() can't show in node console. \nRef: https://github.com/GoogleChrome/puppeteer/issues/1944\n\neverything that is run inside the page.evaluate function is done in the context of the browser page. The script is running in the browser not in node.js so if you log it will show in the browsers console which if you are running headless you will not see. You also can't set a node breakpoint inside the function.\n\nHope this can help.\n"", ""\nFor pass a function, there are two ways you can do it.\n// 1. Defined in evaluationContext\nawait page.evaluate(() => {\n  window.yourFunc = function() {...};\n});\nconst links = await page.evaluate(() => {\n  const func = window.yourFunc;\n  func();\n});\n\n\n// 2. Transform function to serializable(string). (Function can not be serialized)\nconst yourFunc = function() {...};\nconst obj = {\n  func: yourFunc.toString()\n};\nconst otherObj = {\n  foo: 'bar'\n};\nconst links = await page.evaluate((obj, aObj) => {\n   const funStr = obj.func;\n   const func = new Function(`return ${funStr}.apply(null, arguments)`)\n   func();\n\n   const foo = aObj.foo; // bar, for object\n   window.foo = foo;\n   debugger;\n}, obj, otherObj);\n\nYou can add devtools: true to the launch options for test\n"", '\nI have a typescript example that could help someone new in typescript.\nconst hyperlinks: string [] = await page.evaluate((url: string, regex: RegExp, querySelect: string) => {\n.........\n}, url, regex, querySelect);\n\n', ""\nSlightly different version from @wolf answer above. Make code much more reusable between different context.\n// util functions\nexport const pipe = (...fns) => initialVal => fns.reduce((acc, fn) => fn(acc), initialVal)\nexport const pluck = key => obj => obj[key] || null\nexport const map = fn => item => fn(item)\n// these variables will be cast to string, look below at fn.toString()\n\nconst updatedAt = await page.evaluate(\n  ([selector, util]) => {\n    let { pipe, map, pluck } = util\n    pipe = new Function(`return ${pipe}`)()\n    map = new Function(`return ${map}`)()\n    pluck = new Function(`return ${pluck}`)()\n\n    return pipe(\n      s => document.querySelector(s),\n      pluck('textContent'),\n      map(text => text.trim()),\n      map(date => Date.parse(date)),\n      map(timeStamp => Promise.resolve(timeStamp))\n    )(selector)\n  },\n  [\n    '#table-announcements tbody td:nth-child(2) .d-none',\n    { pipe: pipe.toString(), map: map.toString(), pluck: pluck.toString() },\n  ]\n)\n\nAlso not that functions inside pipe cant used something like this\n// incorrect, which is i don't know why\npipe(document.querySelector) \n\n// should be \npipe(s => document.querySelector(s))\n\n""]",https://stackoverflow.com/questions/46088351/how-can-i-pass-variable-into-an-evaluate-function,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to find elements by class,"
I'm having trouble parsing HTML elements with ""class"" attribute using Beautifulsoup. The code looks like this
soup = BeautifulSoup(sdata)
mydivs = soup.findAll('div')
for div in mydivs: 
    if (div[""class""] == ""stylelistrow""):
        print div

I get an error on the same line ""after"" the script finishes. 
File ""./beautifulcoding.py"", line 130, in getlanguage
  if (div[""class""] == ""stylelistrow""):
File ""/usr/local/lib/python2.6/dist-packages/BeautifulSoup.py"", line 599, in __getitem__
   return self._getAttrMap()[key]
KeyError: 'class'

How do I get rid of this error?
",1.0m,"
            609
        ","['\nYou can refine your search to only find those divs with a given class using BS3:\nmydivs = soup.find_all(""div"", {""class"": ""stylelistrow""})\n\n', '\nFrom the documentation:\nAs of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument class_:\nsoup.find_all(""a"", class_=""sister"")\n\nWhich in this case would be:\nsoup.find_all(""div"", class_=""stylelistrow"")\n\nIt would also work for:\nsoup.find_all(""div"", class_=""stylelistrowone stylelistrowtwo"")\n\n', '\nUpdate: 2016\nIn the latest version of beautifulsoup, the method \'findAll\' has been renamed to \n\'find_all\'. Link to official documentation\n\nHence the answer will be \nsoup.find_all(""html_element"", class_=""your_class_name"")\n\n', '\nCSS selectors\nsingle class first match\nsoup.select_one(\'.stylelistrow\')\n\nlist of matches\nsoup.select(\'.stylelistrow\')\n\ncompound class (i.e. AND another class)\nsoup.select_one(\'.stylelistrow.otherclassname\')\nsoup.select(\'.stylelistrow.otherclassname\')\n\nSpaces in compound class names e.g. class = stylelistrow otherclassname are replaced with ""."". You can continue to add classes.\nlist of classes (OR - match whichever present)\nsoup.select_one(\'.stylelistrow, .otherclassname\')\nsoup.select(\'.stylelistrow, .otherclassname\')\n\nClass attribute whose values contains a string e.g. with ""stylelistrow"":\nstarts with ""style"":\n[class^=style]\n\nends with ""row""\n[class$=row]\n\ncontains ""list"":\n[class*=list]\n\nThe ^, $ and * are operators. Read more here: https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors\nIf you wanted to exclude this class then, with anchor tag as an example, selecting anchor tags without this class:\na:not(.stylelistrow)\n\nYou can pass simple, compound and complex css selectors lists inside of :not() pseudo class. See https://facelessuser.github.io/soupsieve/selectors/pseudo-classes/#:not\n\nbs4 4.7.1 +\nSpecific class whose innerText contains a string\nsoup.select_one(\'.stylelistrow:contains(""some string"")\')\nsoup.select(\'.stylelistrow:contains(""some string"")\')\n\nN.B.\nsoupsieve 2.1.0 + Dec\'2020 onwards\n\nNEW: In order to avoid conflicts with future CSS specification\nchanges, non-standard pseudo classes will now start with the :-soup-\nprefix. As a consequence, :contains() will now be known as\n:-soup-contains(), though for a time the deprecated form of\n:contains() will still be allowed with a warning that users should\nmigrate over to :-soup-contains().\nNEW: Added new non-standard pseudo class :-soup-contains-own() which\noperates similar to :-soup-contains() except that it only looks at\ntext nodes directly associated with the currently scoped element and\nnot its descendants.\n\nSpecific class which has a certain child element e.g. a tag\nsoup.select_one(\'.stylelistrow:has(a)\')\nsoup.select(\'.stylelistrow:has(a)\')\n\n', '\nSpecific to BeautifulSoup 3:\nsoup.findAll(\'div\',\n             {\'class\': lambda x: x \n                       and \'stylelistrow\' in x.split()\n             }\n            )\n\nWill find all of these:\n<div class=""stylelistrow"">\n<div class=""stylelistrow button"">\n<div class=""button stylelistrow"">\n\n', ""\nA straight forward way would be :\nsoup = BeautifulSoup(sdata)\nfor each_div in soup.findAll('div',{'class':'stylelist'}):\n    print each_div\n\nMake sure you take of the casing of findAll, its not findall\n"", '\n\nHow to find elements by class\nI\'m having trouble parsing html elements with ""class"" attribute using Beautifulsoup.\n\nYou can easily find by one class, but if you want to find by the intersection of two classes, it\'s a little more difficult,\nFrom the documentation (emphasis added):\n\nIf you want to search for tags that match two or more CSS classes, you should use a CSS selector:\ncss_soup.select(""p.strikeout.body"")\n# [<p class=""body strikeout""></p>]\n\n\nTo be clear, this selects only the p tags that are both strikeout and body class.\nTo find for the intersection of any in a set of classes (not the intersection, but the union), you can give a list to the class_ keyword argument (as of 4.1.2):\nsoup = BeautifulSoup(sdata)\nclass_list = [""stylelistrow""] # can add any other classes to this list.\n# will find any divs with any names in class_list:\nmydivs = soup.find_all(\'div\', class_=class_list) \n\nAlso note that findAll has been renamed from the camelCase to the more Pythonic find_all.\n', ""\nUse class_= If you want to find element(s) without stating the HTML tag.\nFor single element:\nsoup.find(class_='my-class-name')\n\nFor multiple elements:\nsoup.find_all(class_='my-class-name')\n\n"", ""\nAs of BeautifulSoup 4+ ,\nIf you have a single class name , you can just pass the class name as parameter like :\nmydivs = soup.find_all('div', 'class_name')\n\nOr if you have more than one class names , just pass the list of class names as parameter like :\nmydivs = soup.find_all('div', ['class1', 'class2'])\n\n"", '\nthe following worked for me\na_tag = soup.find_all(""div"",class_=\'full tabpublist\')\n\n', ""\nThis works for me to access the class attribute (on beautifulsoup 4, contrary to what the documentation says). The KeyError comes a list being returned not a dictionary.\nfor hit in soup.findAll(name='span'):\n    print hit.contents[1]['class']\n\n"", '\nOther answers did not work for me.\nIn other answers the findAll is being used on the soup object itself, but I needed a way to do a find by class name on objects inside a specific element extracted from the object I obtained after doing findAll.\nIf you are trying to do a search inside nested HTML elements to get objects by class name, try below -\n# parse html\npage_soup = soup(web_page.read(), ""html.parser"")\n\n# filter out items matching class name\nall_songs = page_soup.findAll(""li"", ""song_item"")\n\n# traverse through all_songs\nfor song in all_songs:\n\n    # get text out of span element matching class \'song_name\'\n    # doing a \'find\' by class name within a specific song element taken out of \'all_songs\' collection\n    song.find(""span"", ""song_name"").text\n\nPoints to note:\n\nI\'m not explicitly defining the search to be on \'class\' attribute findAll(""li"", {""class"": ""song_item""}), since it\'s the only attribute I\'m searching on and it will by default search for class attribute if you don\'t exclusively tell which attribute you want to find on. \nWhen you do a findAll or find, the resulting object is of class bs4.element.ResultSet which is a subclass of list. You can utilize all methods of ResultSet, inside any number of nested elements (as long as they are of type ResultSet) to do a find or find all.\nMy BS4 version - 4.9.1, Python version - 3.8.1\n\n', '\nConcerning @Wernight\'s comment on the top answer about partial matching...\nYou can partially match:\n\n<div class=""stylelistrow""> and\n<div class=""stylelistrow button"">\n\nwith gazpacho:\nfrom gazpacho import Soup\n\nmy_divs = soup.find(""div"", {""class"": ""stylelistrow""}, partial=True)\n\nBoth will be captured and returned as a list of Soup objects.\n', '\nAlternatively we can use lxml, it support xpath and very fast!\nfrom lxml import html, etree \n\nattr = html.fromstring(html_text)#passing the raw html\nhandles = attr.xpath(\'//div[@class=""stylelistrow""]\')#xpath exresssion to find that specific class\n\nfor each in handles:\n    print(etree.tostring(each))#printing the html as string\n\n', '\nsingle\nsoup.find(""form"",{""class"":""c-login__form""})\n\nmultiple\nres=soup.find_all(""input"")\nfor each in res:\n    print(each)\n\n', '\nTry to check if the div has a class attribute first, like this:\nsoup = BeautifulSoup(sdata)\nmydivs = soup.findAll(\'div\')\nfor div in mydivs:\n    if ""class"" in div:\n        if (div[""class""]==""stylelistrow""):\n            print div\n\n', '\nThis worked for me:\nfor div in mydivs:\n    try:\n        clazz = div[""class""]\n    except KeyError:\n        clazz = """"\n    if (clazz == ""stylelistrow""):\n        print div\n\n', '\nThis should work:\nsoup = BeautifulSoup(sdata)\nmydivs = soup.findAll(\'div\')\nfor div in mydivs: \n    if (div.find(class_ == ""stylelistrow""):\n        print div\n\n', ""\nThe following should work\nsoup.find('span', attrs={'class':'totalcount'})\n\nreplace 'totalcount' with your class name and 'span' with tag you are looking for. Also, if your class contains multiple names with space, just choose one and use.\nP.S. This finds the first element with given criteria. If you want to find all elements then replace 'find' with 'find_all'.\n""]",https://stackoverflow.com/questions/5041008/how-to-find-elements-by-class,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I efficiently parse HTML with Java?,"
I do a lot of HTML parsing in my line of work. Up until now, I was using the HtmlUnit headless browser for parsing and browser automation.
Now, I want to separate both the tasks.
I want to use a light HTML parser because it takes much time in HtmlUnit to first load a page, then get the source and then parse it.
I want to know which HTML parser can parse HTML efficiently. I need

Speed
Ease to locate any HtmlElement by its ""id"" or ""name"" or ""tag type"".

It would be ok for me if it doesn't clean the dirty HTML code. I don't need to clean any HTML source. I just need an easiest way to move across HtmlElements and harvest data from them.
",205k,"
            206
        ","['\nSelf plug: I have just released a new Java HTML parser: jsoup. I mention it here because I think it will do what you are after.\nIts party trick is a CSS selector syntax to find elements, e.g.:\nString html = ""<html><head><title>First parse</title></head>""\n  + ""<body><p>Parsed HTML into a doc.</p></body></html>"";\nDocument doc = Jsoup.parse(html);\nElements links = doc.select(""a"");\nElement head = doc.select(""head"").first();\n\nSee the Selector javadoc for more info.\nThis is a new project, so any ideas for improvement are very welcome!\n', ""\nThe best I've seen so far is HtmlCleaner:\n\nHtmlCleaner is open-source HTML parser written in Java. HTML found on Web is usually dirty, ill-formed and unsuitable for further processing. For any serious consumption of such documents, it is necessary to first clean up the mess and bring the order to tags, attributes and ordinary text. For the given HTML document, HtmlCleaner reorders individual elements and produces well-formed XML. By default, it follows similar rules that the most of web browsers use in order to create Document Object Model. However, user may provide custom tag and rule set for tag filtering and balancing.\n\nWith HtmlCleaner you can locate any element using XPath.\nFor other html parsers see this SO question.\n"", ""\nI suggest Validator.nu's parser, based on the HTML5 parsing algorithm. It is the parser used in Mozilla from 2010-05-03\n""]",https://stackoverflow.com/questions/2168610/how-can-i-efficiently-parse-html-with-java,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
selenium with scrapy for dynamic page,"
I'm trying to scrape product information from a webpage, using scrapy. My to-be-scraped webpage looks like this:

starts with a product_list page with 10 products
a click on ""next""  button loads the next 10 products (url doesn't change between the two pages)
i use LinkExtractor to follow each product link into the product page, and get all the information I need

I tried to replicate the next-button-ajax-call but can't get working, so I'm giving selenium a try. I can run selenium's webdriver in a separate script, but I don't know how to integrate with scrapy. Where shall I put the selenium part in my scrapy spider? 
My spider is pretty standard, like the following:
class ProductSpider(CrawlSpider):
    name = ""product_spider""
    allowed_domains = ['example.com']
    start_urls = ['http://example.com/shanghai']
    rules = [
        Rule(SgmlLinkExtractor(restrict_xpaths='//div[@id=""productList""]//dl[@class=""t2""]//dt'), callback='parse_product'),
        ]

    def parse_product(self, response):
        self.log(""parsing product %s"" %response.url, level=INFO)
        hxs = HtmlXPathSelector(response)
        # actual data follows

Any idea is appreciated. Thank you!
",109k,"
            100
        ","['\nIt really depends on how do you need to scrape the site and how and what data do you want to get. \nHere\'s an example how you can follow pagination on ebay using Scrapy+Selenium:\nimport scrapy\nfrom selenium import webdriver\n\nclass ProductSpider(scrapy.Spider):\n    name = ""product_spider""\n    allowed_domains = [\'ebay.com\']\n    start_urls = [\'http://www.ebay.com/sch/i.html?_odkw=books&_osacat=0&_trksid=p2045573.m570.l1313.TR0.TRC0.Xpython&_nkw=python&_sacat=0&_from=R40\']\n\n    def __init__(self):\n        self.driver = webdriver.Firefox()\n\n    def parse(self, response):\n        self.driver.get(response.url)\n\n        while True:\n            next = self.driver.find_element_by_xpath(\'//td[@class=""pagn-next""]/a\')\n\n            try:\n                next.click()\n\n                # get the data and write it to scrapy items\n            except:\n                break\n\n        self.driver.close()\n\nHere are some examples of ""selenium spiders"":\n\nExecuting Javascript Submit form functions using scrapy in python\nhttps://gist.github.com/cheekybastard/4944914\nhttps://gist.github.com/irfani/1045108\nhttp://snipplr.com/view/66998/\n\n\nThere is also an alternative to having to use Selenium with Scrapy. In some cases, using ScrapyJS middleware is enough to handle the dynamic parts of a page. Sample real-world usage:\n\nScraping dynamic content using python-Scrapy\n\n', '\nIf (url doesn\'t change between the two pages) then you should add dont_filter=True with your scrapy.Request() or scrapy will find this url as a duplicate after processing first page. \nIf you need to render pages with javascript you should use scrapy-splash, you can also check this scrapy middleware which can handle javascript pages using selenium or you can do that by launching any headless browser\nBut more effective and faster solution is inspect your browser and see what requests are made during submitting a form or triggering a certain event. Try to simulate the same requests as your browser sends. If you can replicate the request(s) correctly you will get the data you need.\nHere is an example :\nclass ScrollScraper(Spider):\n    name = ""scrollingscraper""\n\n    quote_url = ""http://quotes.toscrape.com/api/quotes?page=""\n    start_urls = [quote_url + ""1""]\n\n    def parse(self, response):\n        quote_item = QuoteItem()\n        print response.body\n        data = json.loads(response.body)\n        for item in data.get(\'quotes\', []):\n            quote_item[""author""] = item.get(\'author\', {}).get(\'name\')\n            quote_item[\'quote\'] = item.get(\'text\')\n            quote_item[\'tags\'] = item.get(\'tags\')\n            yield quote_item\n\n        if data[\'has_next\']:\n            next_page = data[\'page\'] + 1\n            yield Request(self.quote_url + str(next_page))\n\nWhen pagination url is same for every pages & uses POST request then you can use scrapy.FormRequest() instead of scrapy.Request(), both are same but FormRequest adds a new argument (formdata=) to the constructor. \nHere is another spider example form this post:\nclass SpiderClass(scrapy.Spider):\n    # spider name and all\n    name = \'ajax\'\n    page_incr = 1\n    start_urls = [\'http://www.pcguia.pt/category/reviews/#paginated=1\']\n    pagination_url = \'http://www.pcguia.pt/wp-content/themes/flavor/functions/ajax.php\'\n\n    def parse(self, response):\n\n        sel = Selector(response)\n\n        if self.page_incr > 1:\n            json_data = json.loads(response.body)\n            sel = Selector(text=json_data.get(\'content\', \'\'))\n\n        # your code here\n\n        # pagination code starts here\n        if sel.xpath(\'//div[@class=""panel-wrapper""]\'):\n            self.page_incr += 1\n            formdata = {\n                \'sorter\': \'recent\',\n                \'location\': \'main loop\',\n                \'loop\': \'main loop\',\n                \'action\': \'sort\',\n                \'view\': \'grid\',\n                \'columns\': \'3\',\n                \'paginated\': str(self.page_incr),\n                \'currentquery[category_name]\': \'reviews\'\n            }\n            yield FormRequest(url=self.pagination_url, formdata=formdata, callback=self.parse)\n        else:\n            return\n\n']",https://stackoverflow.com/questions/17975471/selenium-with-scrapy-for-dynamic-page,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to scrape only visible webpage text with BeautifulSoup?,"
Basically, I want to use BeautifulSoup to grab strictly the visible text on a webpage. For instance, this webpage is my test case. And I mainly want to just get the body text (article) and maybe even a few tab names here and there. I have tried the suggestion in this SO question that returns lots of <script> tags and html comments which I don't want. I can't figure out the arguments I need for the function findAll() in order to just get the visible texts on a webpage.
So, how should I find all visible text excluding scripts, comments, css etc.?
",156k,"
            145
        ","['\nTry this:\nfrom bs4 import BeautifulSoup\nfrom bs4.element import Comment\nimport urllib.request\n\n\ndef tag_visible(element):\n    if element.parent.name in [\'style\', \'script\', \'head\', \'title\', \'meta\', \'[document]\']:\n        return False\n    if isinstance(element, Comment):\n        return False\n    return True\n\n\ndef text_from_html(body):\n    soup = BeautifulSoup(body, \'html.parser\')\n    texts = soup.findAll(text=True)\n    visible_texts = filter(tag_visible, texts)  \n    return u"" "".join(t.strip() for t in visible_texts)\n\nhtml = urllib.request.urlopen(\'http://www.nytimes.com/2009/12/21/us/21storm.html\').read()\nprint(text_from_html(html))\n\n', ""\nThe approved answer from @jbochi does not work for me.  The str() function call raises an exception because it cannot encode the non-ascii characters in the BeautifulSoup element.  Here is a more succinct way to filter the example web page to visible text.\nhtml = open('21storm.html').read()\nsoup = BeautifulSoup(html)\n[s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\nvisible_text = soup.getText()\n\n"", '\nimport urllib\nfrom bs4 import BeautifulSoup\n\nurl = ""https://www.yahoo.com""\nhtml = urllib.urlopen(url).read()\nsoup = BeautifulSoup(html)\n\n# kill all script and style elements\nfor script in soup([""script"", ""style""]):\n    script.extract()    # rip it out\n\n# get text\ntext = soup.get_text()\n\n# break into lines and remove leading and trailing space on each\nlines = (line.strip() for line in text.splitlines())\n# break multi-headlines into a line each\nchunks = (phrase.strip() for line in lines for phrase in line.split(""  ""))\n# drop blank lines\ntext = \'\\n\'.join(chunk for chunk in chunks if chunk)\n\nprint(text.encode(\'utf-8\'))\n\n', '\nI completely respect using Beautiful Soup to get rendered content, but it may not be the ideal package for acquiring the rendered content on a page.\nI had a similar problem to get rendered content, or the visible content in a typical browser.  In particular I had many perhaps atypical cases to work with such a simple example below.  In this case the non displayable tag is nested in a style tag, and is not visible in many browsers that I have checked.  Other variations exist such as defining a class tag setting display to none.  Then using this class for the div. \n<html>\n  <title>  Title here</title>\n\n  <body>\n\n    lots of text here <p> <br>\n    <h1> even headings </h1>\n\n    <style type=""text/css""> \n        <div > this will not be visible </div> \n    </style>\n\n\n  </body>\n\n</html>\n\nOne solution posted above is: \nhtml = Utilities.ReadFile(\'simple.html\')\nsoup = BeautifulSoup.BeautifulSoup(html)\ntexts = soup.findAll(text=True)\nvisible_texts = filter(visible, texts)\nprint(visible_texts)\n\n\n[u\'\\n\', u\'\\n\', u\'\\n\\n        lots of text here \', u\' \', u\'\\n\', u\' even headings \', u\'\\n\', u\' this will not be visible \', u\'\\n\', u\'\\n\']\n\nThis solution certainly has applications in many cases and does the job quite well generally but in the html posted above it retains the text that is not rendered.  After searching SO a couple solutions came up here BeautifulSoup get_text does not strip all tags and JavaScript  and here Rendered HTML to plain text using Python\nI tried both these solutions: html2text and nltk.clean_html and was surprised by the timing results so thought they warranted an answer for posterity.  Of course, the speeds highly depend on the contents of the data...\nOne answer here from @Helge was about using nltk of all things.  \nimport nltk\n\n%timeit nltk.clean_html(html)\nwas returning 153 us per loop\n\nIt worked really well to return a string with rendered html.  This nltk module was faster than even html2text, though perhaps html2text is more robust. \nbetterHTML = html.decode(errors=\'ignore\')\n%timeit html2text.html2text(betterHTML)\n%3.09 ms per loop\n\n', ""\nUsing BeautifulSoup the easiest way with less code to just get the strings, without empty lines and crap.\ntag = <Parent_Tag_that_contains_the_data>\nsoup = BeautifulSoup(tag, 'html.parser')\n\nfor i in soup.stripped_strings:\n    print repr(i)\n\n"", '\nIf you care about performance, here\'s another more efficient way:\nimport re\n\nINVISIBLE_ELEMS = (\'style\', \'script\', \'head\', \'title\')\nRE_SPACES = re.compile(r\'\\s{3,}\')\n\ndef visible_texts(soup):\n    """""" get visible text from a document """"""\n    text = \' \'.join([\n        s for s in soup.strings\n        if s.parent.name not in INVISIBLE_ELEMS\n    ])\n    # collapse multiple spaces to two spaces.\n    return RE_SPACES.sub(\'  \', text)\n\nsoup.strings is an iterator, and it returns NavigableString so that you can check the parent\'s tag name directly, without going through multiple loops.\n', '\nWhile, i would completely suggest using beautiful-soup in general, if anyone is looking to display the visible parts of a malformed html (e.g. where you have just a segment or line of a web-page) for whatever-reason, the the following will remove content between < and > tags:\nimport re   ## only use with malformed html - this is not efficient\ndef display_visible_html_using_re(text):             \n    return(re.sub(""(\\<.*?\\>)"", """",text))\n\n', '\nThe title is inside an <nyt_headline> tag, which is nested inside an <h1> tag and a <div> tag with id ""article"".  \nsoup.findAll(\'nyt_headline\', limit=1)\n\nShould work.\nThe article body is inside an <nyt_text> tag, which is nested inside a <div> tag with id ""articleBody"".  Inside the <nyt_text>  element, the text itself is contained within <p>  tags.  Images are not within those <p> tags.  It\'s difficult for me to experiment with the syntax, but I expect a working scrape to look something like this.\ntext = soup.findAll(\'nyt_text\', limit=1)[0]\ntext.findAll(\'p\')\n\n', '\nfrom bs4 import BeautifulSoup\nfrom bs4.element import Comment\nimport urllib.request\nimport re\nimport ssl\n\ndef tag_visible(element):\n    if element.parent.name in [\'style\', \'script\', \'head\', \'title\', \'meta\', \'[document]\']:\n        return False\n    if isinstance(element, Comment):\n        return False\n    if re.match(r""[\\n]+"",str(element)): return False\n    return True\ndef text_from_html(url):\n    body = urllib.request.urlopen(url,context=ssl._create_unverified_context()).read()\n    soup = BeautifulSoup(body ,""lxml"")\n    texts = soup.findAll(text=True)\n    visible_texts = filter(tag_visible, texts)  \n    text = u"","".join(t.strip() for t in visible_texts)\n    text = text.lstrip().rstrip()\n    text = text.split(\',\')\n    clean_text = \'\'\n    for sen in text:\n        if sen:\n            sen = sen.rstrip().lstrip()\n            clean_text += sen+\',\'\n    return clean_text\nurl = \'http://www.nytimes.com/2009/12/21/us/21storm.html\'\nprint(text_from_html(url))\n\n', '\nThe simplest way to handle this case is by using getattr().  You can adapt this example to your needs:\nfrom bs4 import BeautifulSoup\n\nsource_html = """"""\n<span class=""ratingsDisplay"">\n    <a class=""ratingNumber"" href=""https://www.youtube.com/watch?v=oHg5SJYRHA0"" target=""_blank"" rel=""noopener"">\n        <span class=""ratingsContent"">3.7</span>\n    </a>\n</span>\n""""""\n\nsoup = BeautifulSoup(source_html, ""lxml"")\nmy_ratings = getattr(soup.find(\'span\', {""class"": ""ratingsContent""}), ""text"", None)\nprint(my_ratings)\n\nThis will find the text element,""3.7"", within the tag object <span class=""ratingsContent"">3.7</span> when it exists, however, default to NoneType when it does not.\n\ngetattr(object, name[, default])\nReturn the value of the named attribute of object. name must be a string. If the string is the name of one of the object鈥檚 attributes, the result is the value of that attribute. For example, getattr(x, \'foobar\') is equivalent to x.foobar. If the named attribute does not exist, default is returned if provided, otherwise, AttributeError is raised.\n\n', ""\nUPDATE\nFrom the docs: As of Beautiful Soup version 4.9.0, when lxml or html.parser are in use, the contents of <script>, <style>, and <template> tags are generally not considered to be 鈥榯ext鈥? since those tags are not part of the human-visible content of the page.\nTo get all the human readable text of the HTML <body> you can use .get_text(), to get rid of redundant whitespaces, etc. set strip parameter and join/separate all by a single whitespace:\nimport bs4, requests\n\nresponse = requests.get('https://www.nytimes.com/interactive/2022/09/13/us/politics/congress-stock-trading-investigation.html',headers={'User-Agent': 'Mozilla/5.0','cache-control': 'max-age=0'}, cookies={'cookies':''})\nsoup = bs4.BeautifulSoup(response.text)\n\nsoup.article.get_text(' ', strip=True)\n\nIn newer code avoid old syntax findAll() instead use find_all() or select() with css selectors - For more take a minute to check docs\n""]",https://stackoverflow.com/questions/1936466/how-to-scrape-only-visible-webpage-text-with-beautifulsoup,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping html tables into R data frames using the XML package,"
How do I scrape html tables using the XML package?
Take, for example, this wikipedia page on the Brazilian soccer team. I would like to read it in R and get the ""list of all matches Brazil have played against FIFA recognised teams"" table as a data.frame. How can I do this?
",127k,"
            161
        ","['\n鈥r a shorter try:\nlibrary(XML)\nlibrary(RCurl)\nlibrary(rlist)\ntheurl <- getURL(""https://en.wikipedia.org/wiki/Brazil_national_football_team"",.opts = list(ssl.verifypeer = FALSE) )\ntables <- readHTMLTable(theurl)\ntables <- list.clean(tables, fun = is.null, recursive = FALSE)\nn.rows <- unlist(lapply(tables, function(t) dim(t)[1]))\n\nthe picked table is the longest one on the page\ntables[[which.max(n.rows)]]\n\n', '\nlibrary(RCurl)\nlibrary(XML)\n\n# Download page using RCurl\n# You may need to set proxy details, etc.,  in the call to getURL\ntheurl <- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""\nwebpage <- getURL(theurl)\n# Process escape characters\nwebpage <- readLines(tc <- textConnection(webpage)); close(tc)\n\n# Parse the html tree, ignoring errors on the page\npagetree <- htmlTreeParse(webpage, error=function(...){})\n\n# Navigate your way through the tree. It may be possible to do this more efficiently using getNodeSet\nbody <- pagetree$children$html$children$body \ndivbodyContent <- body$children$div$children[[1]]$children$div$children[[4]]\ntables <- divbodyContent$children[names(divbodyContent)==""table""]\n\n#In this case, the required table is the only one with class ""wikitable sortable""  \ntableclasses <- sapply(tables, function(x) x$attributes[""class""])\nthetable  <- tables[which(tableclasses==""wikitable sortable"")]$table\n\n#Get columns headers\nheaders <- thetable$children[[1]]$children\ncolumnnames <- unname(sapply(headers, function(x) x$children$text$value))\n\n# Get rows from table\ncontent <- c()\nfor(i in 2:length(thetable$children))\n{\n   tablerow <- thetable$children[[i]]$children\n   opponent <- tablerow[[1]]$children[[2]]$children$text$value\n   others <- unname(sapply(tablerow[-1], function(x) x$children$text$value)) \n   content <- rbind(content, c(opponent, others))\n}\n\n# Convert to data frame\ncolnames(content) <- columnnames\nas.data.frame(content)\n\nEdited to add:\nSample output\n                     Opponent Played Won Drawn Lost Goals for Goals against \xa0% Won\n    1               Argentina     94  36    24   34       148           150  38.3%\n    2                Paraguay     72  44    17   11       160            61  61.1%\n    3                 Uruguay     72  33    19   20       127            93  45.8%\n    ...\n\n', '\nThe rvest along with xml2 is another popular package for parsing html web pages.\nlibrary(rvest)\ntheurl <- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""\nfile<-read_html(theurl)\ntables<-html_nodes(file, ""table"")\ntable1 <- html_table(tables[4], fill = TRUE)\n\nThe syntax is easier to use than the xml package and for most web pages the package provides all of the options ones needs.\n', '\nAnother option using Xpath.\nlibrary(RCurl)\nlibrary(XML)\n\ntheurl <- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""\nwebpage <- getURL(theurl)\nwebpage <- readLines(tc <- textConnection(webpage)); close(tc)\n\npagetree <- htmlTreeParse(webpage, error=function(...){}, useInternalNodes = TRUE)\n\n# Extract table header and contents\ntablehead <- xpathSApply(pagetree, ""//*/table[@class=\'wikitable sortable\']/tr/th"", xmlValue)\nresults <- xpathSApply(pagetree, ""//*/table[@class=\'wikitable sortable\']/tr/td"", xmlValue)\n\n# Convert character vector to dataframe\ncontent <- as.data.frame(matrix(results, ncol = 8, byrow = TRUE))\n\n# Clean up the results\ncontent[,1] <- gsub(""脗\xa0"", """", content[,1])\ntablehead <- gsub(""脗\xa0"", """", tablehead)\nnames(content) <- tablehead\n\nProduces this result\n> head(content)\n   Opponent Played Won Drawn Lost Goals for Goals against % Won\n1 Argentina     94  36    24   34       148           150 38.3%\n2  Paraguay     72  44    17   11       160            61 61.1%\n3   Uruguay     72  33    19   20       127            93 45.8%\n4     Chile     64  45    12    7       147            53 70.3%\n5      Peru     39  27     9    3        83            27 69.2%\n6    Mexico     36  21     6    9        69            34 58.3%\n\n']",https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to use Python requests to fake a browser visit a.k.a and generate User Agent?,"
I want to get the content from this website.
If I use a browser like Firefox or Chrome I could get the real website page I want, but if I use the Python requests package (or wget command) to get it, it returns a totally different HTML page.
I thought the developer of the website had made some blocks for this.
Question
How do I fake a browser visit by using python requests or command wget?
",321k,"
            179
        ","[""\nProvide a User-Agent header:\nimport requests\n\nurl = 'http://www.ichangtou.com/#company:data_000008.html'\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n\nresponse = requests.get(url, headers=headers)\nprint(response.content)\n\nFYI, here is a list of User-Agent strings for different browsers:\n\nList of all Browsers\n\n\nAs a side note, there is a pretty useful third-party package called fake-useragent that provides a nice abstraction layer over user agents:\n\nfake-useragent\nUp to date simple useragent faker with real world database\n\nDemo:\n>>> from fake_useragent import UserAgent\n>>> ua = UserAgent()\n>>> ua.chrome\nu'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36'\n>>> ua.random\nu'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36'\n\n"", '\nI used fake UserAgent.\nHow to use:\nfrom fake_useragent import UserAgent\nimport requests\n   \n\nua = UserAgent()\nprint(ua.chrome)\nheader = {\'User-Agent\':str(ua.chrome)}\nprint(header)\nurl = ""https://www.hybrid-analysis.com/recent-submissions?filter=file&sort=^timestamp""\nhtmlContent = requests.get(url, headers=header)\nprint(htmlContent)\n\nOutput:\nMozilla/5.0 (Macintosh; Intel Mac OS X 10_8_2) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1309.0 Safari/537.17\n{\'User-Agent\': \'Mozilla/5.0 (X11; OpenBSD i386) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\'}\n<Response [200]>\n\n', '\nTry doing this, using firefox as fake user agent (moreover, it\'s a good startup script for web scraping with the use of cookies):\n#!/usr/bin/env python2\n# -*- coding: utf8 -*-\n# vim:ts=4:sw=4\n\n\nimport cookielib, urllib2, sys\n\ndef doIt(uri):\n    cj = cookielib.CookieJar()\n    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n    page = opener.open(uri)\n    page.addheaders = [(\'User-agent\', \'Mozilla/5.0\')]\n    print page.read()\n\nfor i in sys.argv[1:]:\n    doIt(i)\n\nUSAGE:\npython script.py ""http://www.ichangtou.com/#company:data_000008.html""\n\n', '\nThe root of the answer is that the person asking the question needs to have a JavaScript interpreter to get what they are after. What I have found is I am able to get all of the information I wanted on a website in json before it was interpreted by JavaScript. This has saved me a ton of time in what would be parsing html hoping each webpage is in the same format.\nSo when you get a response from a website using requests really look at the html/text because you might find the javascripts JSON in the footer ready to be parsed. \n', '\nI use pyuser_agent. this package use get user agnet\nimport pyuser_agent\nimport requests\n\nua = pyuser_agent.UA()\n\nheaders = {\n      ""User-Agent"" : ua.random\n}\nprint(headers)\n\nuri = ""https://github.com/THAVASIGTI/""\nres = requests.request(""GET"",uri,headers=headers)\nprint(res)\n\nconsole out\n{\'User-Agent\': \'Mozilla/5.0 (Windows; U; Windows NT 6.1; zh-CN) AppleWebKit/533+ (KHTML, like Gecko)\'}\n<Response [200]>\n\n', ""\nAnswer\nYou need to create a header with a proper formatted User agent String, it server to communicate client-server.\nYou can check your own user agent Here.\nExample\nMozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0\nMozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0\n\nThird party Package user_agent 0.1.9 \nI found this module very simple to use, in one line of code it randomly generates a User agent string.\nfrom user_agent import generate_user_agent, generate_navigator\nfrom pprint import pprint\n\nprint(generate_user_agent())\n# 'Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.3; Win64; x64)'\n\nprint(generate_user_agent(os=('mac', 'linux')))\n# 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:36.0) Gecko/20100101 Firefox/36.0'\n\npprint(generate_navigator())\n\n# {'app_code_name': 'Mozilla',\n#  'app_name': 'Netscape',\n#  'appversion': '5.0',\n#  'name': 'firefox',\n#  'os': 'linux',\n#  'oscpu': 'Linux i686 on x86_64',\n#  'platform': 'Linux i686 on x86_64',\n#  'user_agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686 on x86_64; rv:41.0) Gecko/20100101 Firefox/41.0',\n#  'version': '41.0'}\n\npprint(generate_navigator_js())\n\n# {'appCodeName': 'Mozilla',\n#  'appName': 'Netscape',\n#  'appVersion': '38.0',\n#  'platform': 'MacIntel',\n#  'userAgent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:38.0) Gecko/20100101 Firefox/38.0'}\n\n"", ""\nUser agent is ok but he wants to fetch a JavaScript site.we can use selenium but it is annoying to setup and maintain so the best way to fetch a JavaScript rendered page is requests_html module. Which is a superset of the well known request module. To install use pip\npip install requests-html\n\nAnd to fetch a JavaScript rendered page use\nfrom requests_html import HTMLSession\nsession = HTMLSession()\nr = session.get('https://python.org/')\n\nHope it will help. It uses puppter to render javascript and also it downloads chromium but you don't have to worry everything is happening under the hood.you will get the end result.\n"", ""\nI had a similar issue but I was unable to use the UserAgent class inside the fake_useragent module. I was running the code inside a docker container\nimport requests\nimport ujson\nimport random\n\nresponse = requests.get('https://fake-useragent.herokuapp.com/browsers/0.1.11')\nagents_dictionary = ujson.loads(response.text)\nrandom_browser_number = str(random.randint(0, len(agents_dictionary['randomize'])))\nrandom_browser = agents_dictionary['randomize'][random_browser_number]\nuser_agents_list = agents_dictionary['browsers'][random_browser]\nuser_agent = user_agents_list[random.randint(0, len(user_agents_list)-1)]\n\nI targeted the endpoint used in the module. This solution still gave me a random user agent however there is the possibility that the data structure at the endpoint could change.\n"", '\nThis is how, I have been using a random user agent from a list of nearlly 1000 fake user agents\nfrom random_user_agent.user_agent import UserAgent\nfrom random_user_agent.params import SoftwareName, OperatingSystem\nsoftware_names = [SoftwareName.ANDROID.value]\noperating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value, OperatingSystem.MAC.value]   \n\nuser_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=1000)\n\n# Get list of user agents.\nuser_agents = user_agent_rotator.get_user_agents()\n\nuser_agent_random = user_agent_rotator.get_random_user_agent()\n\nExample\nprint(user_agent_random)\n\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\nFor more details visit this link\n']",https://stackoverflow.com/questions/27652543/how-to-use-python-requests-to-fake-a-browser-visit-a-k-a-and-generate-user-agent,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is it ok to scrape data from Google results? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 5 years ago.







                        Improve this question
                    



I'd like to fetch results from Google using curl to detect potential duplicate content.
Is there a high risk of being banned by Google?
",135k,"
            77
        ","['\nGoogle disallows automated access in their TOS, so if you accept their terms you would break them.\nThat said, I know of no lawsuit from Google against a scraper.\nEven Microsoft scraped Google, they powered their search engine Bing with it. They got caught in 2011 red handed :)\nThere are two options to scrape Google results:\n1) Use their API\n\nUPDATE 2020: Google has reprecated previous APIs (again) and has new\nprices and new limits. Now\n(https://developers.google.com/custom-search/v1/overview) you can\nquery up to 10k results per day at 1,500 USD per month, more than that\nis not permitted and the results are not what they display in normal\nsearches.\n\n\nYou can issue around 40 requests per hour You are limited to what\nthey give you, it\'s not really useful if you want to track ranking\npositions or what a real user would see. That\'s something you are not\nallowed to gather.\n\nIf you want a higher amount of API requests you need to pay.\n\n60 requests per hour cost 2000 USD per year, more queries require a\ncustom deal.\n\n\n2) Scrape the normal result pages\n\nHere comes the tricky part. It is possible to scrape the normal result pages.\nGoogle does not allow it.\nIf you scrape at a rate higher than 8 (updated from 15) keyword requests per hour you risk detection, higher than 10/h (updated from 20) will get you blocked from my experience.\nBy using multiple IPs you can up the rate, so with 100 IP addresses you can scrape up to 1000 requests per hour. (24k a day) (updated)\nThere is an open source search engine scraper written in PHP at http://scraping.compunect.com\nIt allows to reliable scrape Google, parses the results properly and manages IP addresses, delays, etc.\nSo if you can use PHP it\'s a nice kickstart, otherwise the code will still be useful to learn how it is done.\n\n3) Alternatively use a scraping service (updated)\n\nRecently a customer of mine had a huge search engine scraping requirement but it was not \'ongoing\', it\'s more like one huge refresh per month.\nIn this case I could not find a self-made solution that\'s \'economic\'.\nI used the service at http://scraping.services instead.\nThey also provide open source code and so far it\'s running well (several thousand resultpages per hour during the refreshes)\nThe downside is that such a service means that your solution is ""bound"" to one professional supplier, the upside is that it was a lot cheaper than the other options I evaluated (and faster in our case)\nOne option to reduce the dependency on one company is to make two approaches at the same time. Using the scraping service as primary source of data and falling back to a proxy based solution like described at 2) when required.\n\n', '\nGoogle will eventually block your IP when you exceed a certain amount of requests. \n', '\nGoogle thrives on scraping websites of the world...so if it was ""so illegal"" then even Google won\'t survive ..of course other answers mention ways of mitigating IP blocks by Google. One more way to explore avoiding captcha could be scraping at random times (dint try) ..Moreover, I have a feeling, that if we provide novelty or some significant processing of data then it sounds fine at least to me...if we are simply copying a website.. or hampering its business/brand in some way...then it is bad and should be avoided..on top of it all...if you are a startup then no one will fight you as there is no benefit.. but if your entire premise is on scraping even when you are funded then you should think of more sophisticated ways...alternative APIs..eventually..Also Google keeps releasing (or depricating)  fields for its API so what you want to scrap now may be in roadmap of new Google API releases..\n']",https://stackoverflow.com/questions/22657548/is-it-ok-to-scrape-data-from-google-results,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can we use XPath with BeautifulSoup?,"
I am using BeautifulSoup to scrape an URL and I had the following code, to find the td tag whose class is 'empformbody':
import urllib
import urllib2
from BeautifulSoup import BeautifulSoup

url =  ""http://www.example.com/servlet/av/ResultTemplate=AVResult.html""
req = urllib2.Request(url)
response = urllib2.urlopen(req)
the_page = response.read()
soup = BeautifulSoup(the_page)

soup.findAll('td',attrs={'class':'empformbody'})

Now in the above code we can use findAll to get tags and information related to them, but I want to use XPath. Is it possible to use XPath with BeautifulSoup? If possible, please provide me example code.
",277k,"
            153
        ","['\nNope, BeautifulSoup, by itself, does not support XPath expressions.\nAn alternative library, lxml, does support XPath 1.0. It has a BeautifulSoup compatible mode where it\'ll try and parse broken HTML the way Soup does. However, the default lxml HTML parser does just as good a job of parsing broken HTML, and I believe is faster.\nOnce you\'ve parsed your document into an lxml tree, you can use the .xpath() method to search for elements.\ntry:\n    # Python 2\n    from urllib2 import urlopen\nexcept ImportError:\n    from urllib.request import urlopen\nfrom lxml import etree\n\nurl =  ""http://www.example.com/servlet/av/ResultTemplate=AVResult.html""\nresponse = urlopen(url)\nhtmlparser = etree.HTMLParser()\ntree = etree.parse(response, htmlparser)\ntree.xpath(xpathselector)\n\nThere is also a dedicated lxml.html() module with additional functionality.\nNote that in the above example I passed the response object directly to lxml, as having the parser read directly from the stream is more efficient than reading the response into a large string first. To do the same with the requests library, you want to set stream=True and pass in the response.raw object after enabling transparent transport decompression:\nimport lxml.html\nimport requests\n\nurl =  ""http://www.example.com/servlet/av/ResultTemplate=AVResult.html""\nresponse = requests.get(url, stream=True)\nresponse.raw.decode_content = True\ntree = lxml.html.parse(response.raw)\n\nOf possible interest to you is the CSS Selector support; the CSSSelector class translates CSS statements into XPath expressions, making your search for td.empformbody that much easier:\nfrom lxml.cssselect import CSSSelector\n\ntd_empformbody = CSSSelector(\'td.empformbody\')\nfor elem in td_empformbody(tree):\n    # Do something with these table cells.\n\nComing full circle: BeautifulSoup itself does have very complete CSS selector support:\nfor cell in soup.select(\'table#foobar td.empformbody\'):\n    # Do something with these table cells.\n\n', '\nI can confirm that there is no XPath support within Beautiful Soup.\n', '\nAs others have said, BeautifulSoup doesn\'t have xpath support.  There are probably a number of ways to get something from an xpath, including using Selenium.  However, here\'s a solution that works in either Python 2 or 3:\nfrom lxml import html\nimport requests\n\npage = requests.get(\'http://econpy.pythonanywhere.com/ex/001.html\')\ntree = html.fromstring(page.content)\n#This will create a list of buyers:\nbuyers = tree.xpath(\'//div[@title=""buyer-name""]/text()\')\n#This will create a list of prices\nprices = tree.xpath(\'//span[@class=""item-price""]/text()\')\n\nprint(\'Buyers: \', buyers)\nprint(\'Prices: \', prices)\n\nI used this as a reference.\n', ""\nBeautifulSoup has a function named findNext from current element directed childern,so:\nfather.findNext('div',{'class':'class_value'}).findNext('div',{'id':'id_value'}).findAll('a') \n\nAbove code can imitate the following xpath:\ndiv[class=class_value]/div[id=id_value]\n\n"", '\nfrom lxml import etree\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(open(\'path of your localfile.html\'),\'html.parser\')\ndom = etree.HTML(str(soup))\nprint dom.xpath(\'//*[@id=""BGINP01_S1""]/section/div/font/text()\')\n\nAbove used the combination of Soup object with lxml and one can extract the value using xpath\n', '\nwhen you use lxml all simple:\ntree = lxml.html.fromstring(html)\ni_need_element = tree.xpath(\'//a[@class=""shared-components""]/@href\')\n\nbut when use BeautifulSoup BS4 all simple too:\n\nfirst remove ""//"" and ""@""  \nsecond - add star before ""=""\n\ntry this magic:\nsoup = BeautifulSoup(html, ""lxml"")\ni_need_element = soup.select (\'a[class*=""shared-components""]\')\n\nas you see, this does not support sub-tag, so i remove ""/@href"" part\n', ""\nI've searched through their docs and it seems there is no XPath option.\nAlso, as you can see here on a similar question on SO, the OP is asking for a translation from XPath to BeautifulSoup, so my conclusion would be - no, there is no XPath parsing available.\n"", '\nMaybe you can try the following without XPath\nfrom simplified_scrapy.simplified_doc import SimplifiedDoc \nhtml = \'\'\'\n<html>\n<body>\n<div>\n    <h1>Example Domain</h1>\n    <p>This domain is for use in illustrative examples in documents. You may use this\n    domain in literature without prior coordination or asking for permission.</p>\n    <p><a href=""https://www.iana.org/domains/example"">More information...</a></p>\n</div>\n</body>\n</html>\n\'\'\'\n# What XPath can do, so can it\ndoc = SimplifiedDoc(html)\n# The result is the same as doc.getElementByTag(\'body\').getElementByTag(\'div\').getElementByTag(\'h1\').text\nprint (doc.body.div.h1.text)\nprint (doc.div.h1.text)\nprint (doc.h1.text) # Shorter paths will be faster\nprint (doc.div.getChildren())\nprint (doc.div.getChildren(\'p\'))\n\n', '\nThis is a pretty old thread, but there is a work-around solution now, which may not have been in BeautifulSoup at the time. \nHere is an example of what I did. I use the ""requests"" module to read an RSS feed and get its text content in a variable called ""rss_text"". With that, I run it thru BeautifulSoup, search for the xpath /rss/channel/title, and retrieve its contents. It\'s not exactly XPath in all its glory (wildcards, multiple paths, etc.), but if you just have a basic path you want to locate, this works. \nfrom bs4 import BeautifulSoup\nrss_obj = BeautifulSoup(rss_text, \'xml\')\ncls.title = rss_obj.rss.channel.title.get_text()\n\n', ""\nuse   soup.find(class_='myclass')\n""]",https://stackoverflow.com/questions/11465555/can-we-use-xpath-with-beautifulsoup,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using python Requests with javascript pages,"
I am trying to use the Requests framework with python (http://docs.python-requests.org/en/latest/) but the page I am trying to get to uses javascript to fetch the info that I want. 
I have tried to search on the web for a solution but the fact that I am searching with the keyword javascript most of the stuff I am getting is how to scrape with the javascript language.
Is there anyway to use the requests framework with pages that use javascript?
",169k,"
            85
        ","[""\nGood news: there is now a requests module that supports javascript:  https://pypi.org/project/requests-html/\nfrom requests_html import HTMLSession\n\nsession = HTMLSession()\n\nr = session.get('http://www.yourjspage.com')\n\nr.html.render()  # this call executes the js in the page\n\nAs a bonus this wraps BeautifulSoup, I think, so you can do things like\nr.html.find('#myElementID').text\n\nwhich returns the content of the HTML element as you'd expect.\n"", '\nYou are going to have to make the same request (using the Requests library) that the javascript is making.  You can use any number of tools (including those built into Chrome and Firefox) to inspect the http request that is coming from javascript and simply make this request yourself from Python.\n', '\nWhile Selenium might seem tempting and useful, it has one main problem that can\'t be fixed: performance. By calculating every single thing a browser does, you will need a lot more power. Even PhantomJS does not compete with a simple request. I recommend that you will only use Selenium when you really need to click buttons. If you only need javascript, I recommend PyQt (check https://www.youtube.com/watch?v=FSH77vnOGqU to learn it).\nHowever, if you want to use Selenium, I recommend Chrome over PhantomJS. Many users have problems with PhantomJS where a website simply does not work in Phantom. Chrome can be headless (non-graphical) too!\nFirst, make sure you have installed ChromeDriver, which Selenium depends on for using Google Chrome.\nThen, make sure you have Google Chrome of version 60 or higher by checking it in the URL chrome://settings/help\nNow, all you need to do is the following code:\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium import webdriver\n\nchrome_options = Options()\nchrome_options.add_argument(""--headless"")\n\ndriver = webdriver.Chrome(chrome_options=chrome_options)\n\nIf you do not know how to use Selenium, here is a quick overview:\ndriver.get(""https://www.google.com"") #Browser goes to google.com\n\nFinding elements:\nUse either the ELEMENTS or ELEMENT method. Examples:\ndriver.find_element_by_css_selector(""div.logo-subtext"") #Find your country in Google. (singular)\n\n\ndriver.find_element(s)_by_css_selector(css_selector) # Every element that matches this CSS selector\ndriver.find_element(s)_by_class_name(class_name) # Every element with the following class\ndriver.find_element(s)_by_id(id) # Every element with the following ID\ndriver.find_element(s)_by_link_text(link_text) # Every  with the full link text\ndriver.find_element(s)_by_partial_link_text(partial_link_text) # Every  with partial link text.\ndriver.find_element(s)_by_name(name) # Every element where name=argument\ndriver.find_element(s)_by_tag_name(tag_name) # Every element with the tag name argument\n\nOk! I found an element (or elements list). But what do I do now?\nHere are the methods you can do on an element elem:\n\nelem.tag_name # Could return button in a .\nelem.get_attribute(""id"") # Returns the ID of an element.\nelem.text # The inner text of an element.\nelem.clear() # Clears a text input.\nelem.is_displayed() # True for visible elements, False for invisible elements.\nelem.is_enabled() # True for an enabled input, False otherwise.\nelem.is_selected() # Is this radio button or checkbox element selected?\nelem.location # A dictionary representing the X and Y location of an element on the screen.\nelem.click() # Click elem.\nelem.send_keys(""thelegend27"") # Type thelegend27 into elem (useful for text inputs)\nelem.submit() # Submit the form in which elem takes part.\n\nSpecial commands:\n\ndriver.back() # Click the Back button.\ndriver.forward() # Click the Forward button.\ndriver.refresh() # Refresh the page.\ndriver.quit() # Close the browser including all the tabs.\nfoo = driver.execute_script(""return \'hello\';"") # Execute javascript (COULD TAKE RETURN VALUES!)\n\n', '\nUsing Selenium or jQuery enabled requests are slow. It is more efficient to find out which cookie is generated after website checking for JavaScript on the browser and get that cookie and use it for each of your requests.\nIn one example it worked through following cookies:\nthe cookie generated after checking for javascript for this example is ""cf_clearance"".\nso  simply create a session.\nupdate cookie and headers as such:\ns = requests.Session()\ns.cookies[""cf_clearance""] = ""cb4c883efc59d0e990caf7508902591f4569e7bf-1617321078-0-150""\ns.headers.update({\n            ""user-agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) \n               AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36""\n        })\ns.get(url)\n\nand you are good to go no need for JavaScript solution such as Selenium. This is way faster and efficient. you just have to get cookie once after opening up the browser.\n', '\nSome way to do that is to invoke your request by using selenium.\nLet\'s install dependecies by using pip or pip3:\npip install selenium\netc.\nIf you run script by using python3\nuse instead:\npip3 install selenium\n(...)\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ndriver = webdriver.Chrome(ChromeDriverManager().install())\nurl = \'http://myurl.com\'\n\n# Please wait until the page will be ready:\nelement = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, ""div.some_placeholder"")))\nelement.text = \'Some text on the page :)\' # <-- Here it is! I got what I wanted :)\n\n', '\nits a wrapper around pyppeteer or smth? :( i thought its something different\n    @property\n    async def browser(self):\n        if not hasattr(self, ""_browser""):\n            self._browser = await pyppeteer.launch(ignoreHTTPSErrors=not(self.verify), headless=True, args=self.__browser_args)\n\n        return self._browser\n\n']",https://stackoverflow.com/questions/26393231/using-python-requests-with-javascript-pages,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How does reCAPTCHA 3 know I'm using Selenium/chromedriver?,"
I'm curious how reCAPTCHA v3 works. Specifically the browser fingerprinting.
When I launch an instance of Chrome through Selenium/chromedriver and test against reCAPTCHA 3 (https://recaptcha-demo.appspot.com/recaptcha-v3-request-scores.php) I always get a score of 0.1 when using Selenium/chromedriver.
When using incognito with a normal instance, I get 0.3.
I've beaten other detection systems by injecting JavaScript and modifying the web driver object and recompiling webdriver from source and modifying the $cdc_ variables.
I can see what looks like some obfuscated POST back to the server, so I'm going to start digging there.
What might it be looking for to determine if I'm running Selenium/chromedriver?
",80k,"
            42
        ","['\nreCaptcha\nWebsites can easily detect the network traffic and identify your program as a BOT. Google have already released 5(five) reCAPTCHA to choose from when creating a new site. While four of them are active and reCAPTCHA v1 being shutdown.\n\nreCAPTCHA versions and types\n\nreCAPTCHA v3 (verify requests with a score): reCAPTCHA v3 allows you to verify if an interaction is legitimate without any user interaction. It is a pure JavaScript API returning a score, giving you the ability to take action in the context of your site: for instance requiring additional factors of authentication, sending a post to moderation, or throttling bots that may be scraping content.\nreCAPTCHA v2 - ""I\'m not a robot"" Checkbox: The ""I\'m not a robot"" Checkbox requires the user to click a checkbox indicating the user is not a robot. This will either pass the user immediately (with No CAPTCHA) or challenge them to validate whether or not they are human. This is the simplest option to integrate with and only requires two lines of HTML to render the checkbox.\n\n\n\nreCAPTCHA v2 - Invisible reCAPTCHA badge: The invisible reCAPTCHA badge does not require the user to click on a checkbox, instead it is invoked directly when the user clicks on an existing button on your site or can be invoked via a JavaScript API call. The integration requires a JavaScript callback when reCAPTCHA verification is complete. By default only the most suspicious traffic will be prompted to solve a captcha. To alter this behavior edit your site security preference under advanced settings.\n\n\n\nreCAPTCHA v2 - Android: The reCAPTCHA Android library is part of the Google Play services SafetyNet APIs. This library provides native Android APIs that you can integrate directly into an app. You should set up Google Play services in your app and connect to the GoogleApiClient before invoking the reCAPTCHA API. This will either pass the user through immediately (without a CAPTCHA prompt) or challenge them to validate whether they are human. \nreCAPTCHA v1: reCAPTCHA v1 has been shut down since March 2018.\n\n\nSolution\nHowever there are some generic approaches to avoid getting detected while web-scraping:\n\nThe first and foremost attribute a website can determine your script/program is through your monitor size. So it is recommended not to use the conventional Viewport.\nIf you need to send multiple requests to a website keep on changing the User Agent on each request. Here you can find a detailed discussion on Way to change Google Chrome user agent in Selenium?\nTo simulate human like behavior you may require to slow down the script execution even beyond WebDriverWait and expected_conditions inducing time.sleep(secs). Here you can find a detailed discussion on How to sleep webdriver in python for milliseconds\n\n\nOutro\nSome food for thought:\n\nSelenium webdriver: Modifying navigator.webdriver flag to prevent selenium detection\nUnable to use Selenium to automate Chase site login\nConfidence Score of the request using reCAPTCHA v3 API\n\n', '\nSelenium and Puppeteer have some browser configurations that is different from a non-automated browser. Also, since some JavaScript functions are injected into browser to manipulate elements, you need to create some override to avoid detections.\nThere are some good articles explaining some points about Selenium and Puppeteer detection while it runs on a site with detection mechanisms:\nDetecting Chrome headless, new techniques - You can use it to write defensive code for your bot.\nIt is not possible to detect and block Google Chrome headless - it explains in a clear and sound way the differences that JavaScript code can detect between a browser launched by automated software and a real one, and also how to fake it.\nGitHub - headless-cat-n-mouse - Example using Puppeteer + Python to avoid detection\n']",https://stackoverflow.com/questions/55501524/how-does-recaptcha-3-know-im-using-selenium-chromedriver,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CasperJS/PhantomJS doesn't load https page,"
I know there are certain web pages PhantomJS/CasperJS can't open, and I was wondering if this one was one of them: https://maizepages.umich.edu. CasperJS gives an error: PhantomJS failed to open page status=fail.
I tried ignoring-ssl-errors and changing my user agent but I'm not sure how to determine which ones to use.
All I'm doing right now is the basic casper setup with casper.start(url, function () { ... }) where url=https://maizepages.umich.edu;
",19k,"
            24
        ","['\nThe problem may be related to the recent discovery of a SSLv3 vulnerability (POODLE). Website owners were forced to remove SSLv3 support from their websites. Since PhantomJS < v1.9.8 uses SSLv3 by default, you should use TLSv1:\ncasperjs --ssl-protocol=tlsv1 yourScript.js\n\nThe catchall solution would be to use any for when newer PhantomJS versions come along with other SSL protocols. But this would make the POODLE vulnerability exploitable on sites which haven\'t yet disabled SSLv3.\ncasperjs --ssl-protocol=any yourScript.js\n\nAlternative method: Update to PhantomJS 1.9.8 or higher. Note that updating to PhantomJS 1.9.8 leads to a new bug, which is especially annoying for CasperJS.\nHow to verify: Add a resource.error event handler like this at the beginning of your script:\ncasper.on(""resource.error"", function(resourceError){\n    console.log(\'Unable to load resource (#\' + resourceError.id + \'URL:\' + resourceError.url + \')\');\n    console.log(\'Error code: \' + resourceError.errorCode + \'. Description: \' + resourceError.errorString);\n});\n\nIf it is indeed a problem with SSLv3 the error will be something like:\n\nError code: 6. Description: SSL handshake failed\n\n\nAs an aside, you also might want to run with the --ignore-ssl-errors=true commandline option, when there is something wrong with the certificate.\n']",https://stackoverflow.com/questions/26415188/casperjs-phantomjs-doesnt-load-https-page,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrape multiple urls using QWebPage,"
I'm using Qt's QWebPage to render a page that uses javascript to update its content dynamically - so a library that just downloads a static version of the page (such as urllib2) won't work.
My problem is, when I render a second page, about 99% of the time the program just crashes. At other times, it will work three times before crashing. I've also gotten a few segfaults, but it is all very random.
My guess is the object I'm using to render isn't getting deleted properly, so trying to reuse it is possibly causing some problems for myself. I've looked all over and no one really seems to be having this same issue.
Here's the code I'm using. The program downloads web pages from steam's community market so I can create a database of all the items. I need to call the getItemsFromPage function multiple times to get all of the items, as they are broken up into pages (showing results 1-10 out of X amount).
import csv
import re
import sys
from string import replace
from bs4 import BeautifulSoup
from PyQt4.QtGui import *
from PyQt4.QtCore import *
from PyQt4.QtWebKit import *

class Item:
    __slots__ = (""name"", ""count"", ""price"", ""game"")

    def __repr__(self):
        return self.name + ""("" + str(self.count) + "")""

    def __str__(self):
        return self.name + "", "" + str(self.count) + "", $"" + str(self.price)

class Render(QWebPage):  
    def __init__(self, url):
        self.app = QApplication(sys.argv)
        QWebPage.__init__(self)
        self.loadFinished.connect(self._loadFinished)
        self.mainFrame().load(QUrl(url))
        self.app.exec_()

    def _loadFinished(self, result):
        self.frame = self.mainFrame()
        self.app.quit()
        self.deleteLater()

def getItemsFromPage(appid, page=1):

    r = Render(""http://steamcommunity.com/market/search?q=appid:"" + str(appid) + ""#p"" + str(page))

    soup = BeautifulSoup(str(r.frame.toHtml().toUtf8()))

    itemLst = soup.find_all(""div"", ""market_listing_row market_recent_listing_row"")

    items = []

    for k in itemLst:
        i = Item()

        i.name = k.find(""span"", ""market_listing_item_name"").string
        i.count = int(replace(k.find(""span"", ""market_listing_num_listings_qty"").string, "","", """"))
        i.price = float(re.search(r'\$([0-9]+\.[0-9]+)', str(k)).group(1))
        i.game = appid

        items.append(i)

    return items

if __name__ == ""__main__"":

    print ""Updating market items to dota2.csv ...""

    i = 1

    with open(""dota2.csv"", ""w"") as f:
        writer = csv.writer(f)

        r = None

        while True:
            print ""Page "" + str(i)

            items = getItemsFromPage(570)

            if len(items) == 0:
                print ""No items found, stopping...""
                break

            for k in items:
                writer.writerow((k.name, k.count, k.price, k.game))

            i += 1

    print ""Done.""

Calling getItemsFromPage once works fine. Subsequent calls give me my problem. The output of the program is typically
Updating market items to dota2.csv ...
Page 1
Page 2

and then it crashes. It should go on for over 700 pages. 
",3k,"
            6
        ","[""\nThe problem with your program is that you are attempting to create a new QApplication with every url you fetch.\nInstead, only one QApplication and one WebPage should be created. The WebPage can use its loadFinished signal to create an internal loop by fetching a new url after each one has been processed. Custom html processing can be added by connecting a user-defined slot to a signal which emits the html text and the url when they become available. The scripts below (for PyQt5 and PyQt4) show how to implement this.\nHere are some examples which show how to use the WebPage class:\nUsage:\ndef my_html_processor(html, url):\n    print('loaded: [%d chars] %s' % (len(html), url))\n\nimport sys\napp = QApplication(sys.argv)\nwebpage = WebPage(verbose=False)\nwebpage.htmlReady.connect(my_html_processor)\n\n# example 1: process list of urls\n\nurls = ['https://en.wikipedia.org/wiki/Special:Random'] * 3\nprint('Processing list of urls...')\nwebpage.process(urls)\n\n# example 2: process one url continuously\n#\n# import signal, itertools\n# signal.signal(signal.SIGINT, signal.SIG_DFL)\n#\n# print('Processing url continuously...')\n# print('Press Ctrl+C to quit')\n#\n# url = 'https://en.wikipedia.org/wiki/Special:Random'\n# webpage.process(itertools.repeat(url))\n\nsys.exit(app.exec_())\n\nPyQt5 WebPage:\nfrom PyQt5.QtCore import pyqtSignal, QUrl\nfrom PyQt5.QtWidgets import QApplication\nfrom PyQt5.QtWebEngineWidgets import QWebEnginePage\n\nclass WebPage(QWebEnginePage):\n    htmlReady = pyqtSignal(str, str)\n\n    def __init__(self, verbose=False):\n        super().__init__()\n        self._verbose = verbose\n        self.loadFinished.connect(self.handleLoadFinished)\n\n    def process(self, urls):\n        self._urls = iter(urls)\n        self.fetchNext()\n\n    def fetchNext(self):\n        try:\n            url = next(self._urls)\n        except StopIteration:\n            return False\n        else:\n            self.load(QUrl(url))\n        return True\n\n    def processCurrentPage(self, html):\n        self.htmlReady.emit(html, self.url().toString())\n        if not self.fetchNext():\n            QApplication.instance().quit()\n\n    def handleLoadFinished(self):\n        self.toHtml(self.processCurrentPage)\n\n    def javaScriptConsoleMessage(self, *args, **kwargs):\n        if self._verbose:\n            super().javaScriptConsoleMessage(*args, **kwargs)\n\nPyQt4 WebPage:\nfrom PyQt4.QtCore import pyqtSignal, QUrl\nfrom PyQt4.QtGui import QApplication\nfrom PyQt4.QtWebKit import QWebPage\n\nclass WebPage(QWebPage):\n    htmlReady = pyqtSignal(str, str)\n\n    def __init__(self, verbose=False):\n        super(WebPage, self).__init__()\n        self._verbose = verbose\n        self.mainFrame().loadFinished.connect(self.handleLoadFinished)\n\n    def start(self, urls):\n        self._urls = iter(urls)\n        self.fetchNext()\n\n    def fetchNext(self):\n        try:\n            url = next(self._urls)\n        except StopIteration:\n            return False\n        else:\n            self.mainFrame().load(QUrl(url))\n        return True\n\n    def processCurrentPage(self):\n        self.htmlReady.emit(\n            self.mainFrame().toHtml(), self.mainFrame().url().toString())\n        print('loaded: [%d bytes] %s' % (self.bytesReceived(), url))\n\n    def handleLoadFinished(self):\n        self.processCurrentPage()\n        if not self.fetchNext():\n            QApplication.instance().quit()\n\n    def javaScriptConsoleMessage(self, *args, **kwargs):\n        if self._verbose:\n            super(WebPage, self).javaScriptConsoleMessage(*args, **kwargs)\n\n""]",https://stackoverflow.com/questions/21274865/scrape-multiple-urls-using-qwebpage,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web scraping with Python [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I'd like to grab daily sunrise/sunset times from a web site. Is it possible to scrape web content with Python? what are the modules used? Is there any tutorial available?
",214k,"
            196
        ","[""\nUse urllib2 in combination with the brilliant BeautifulSoup library:\nimport urllib2\nfrom BeautifulSoup import BeautifulSoup\n# or if you're using BeautifulSoup4:\n# from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(urllib2.urlopen('http://example.com').read())\n\nfor row in soup('table', {'class': 'spad'})[0].tbody('tr'):\n    tds = row('td')\n    print tds[0].string, tds[1].string\n    # will print date and sunrise\n\n"", ""\nI'd really recommend Scrapy.\nQuote from a deleted answer:\n\n\nScrapy crawling is fastest than mechanize because uses asynchronous operations (on top of Twisted).\nScrapy has better and fastest support for parsing (x)html on top of libxml2.\nScrapy is a mature framework with full unicode, handles redirections, gzipped responses, odd encodings, integrated http cache, etc.\nOnce you are into Scrapy, you can write a spider in less than 5 minutes that download images, creates thumbnails and export the extracted data directly to csv or json.\n\n\n"", '\nI collected together scripts from my web scraping work into this bit-bucket library.\nExample script for your case:\nfrom webscraping import download, xpath\nD = download.Download()\n\nhtml = D.get(\'http://example.com\')\nfor row in xpath.search(html, \'//table[@class=""spad""]/tbody/tr\'):\n    cols = xpath.search(row, \'/td\')\n    print \'Sunrise: %s, Sunset: %s\' % (cols[1], cols[2])\n\nOutput:\nSunrise: 08:39, Sunset: 16:08\nSunrise: 08:39, Sunset: 16:09\nSunrise: 08:39, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:11\nSunrise: 08:40, Sunset: 16:12\nSunrise: 08:40, Sunset: 16:13\n\n', ""\nI would strongly suggest checking out pyquery. It uses jquery-like (aka css-like) syntax which makes things really easy for those coming from that background.\nFor your case, it would be something like:\nfrom pyquery import *\n\nhtml = PyQuery(url='http://www.example.com/')\ntrs = html('table.spad tbody tr')\n\nfor tr in trs:\n  tds = tr.getchildren()\n  print tds[1].text, tds[2].text\n\nOutput:\n5:16 AM 9:28 PM\n5:15 AM 9:30 PM\n5:13 AM 9:31 PM\n5:12 AM 9:33 PM\n5:11 AM 9:34 PM\n5:10 AM 9:35 PM\n5:09 AM 9:37 PM\n\n"", ""\nYou can use urllib2 to make the HTTP requests, and then you'll have web content.\nYou can get it like this:\nimport urllib2\nresponse = urllib2.urlopen('http://example.com')\nhtml = response.read()\n\nBeautiful Soup is a python HTML parser that is supposed to be good for screen scraping.\nIn particular, here is their tutorial on parsing an HTML document.\nGood luck!\n"", '\nI use a combination of Scrapemark (finding urls - py2) and httlib2 (downloading images - py2+3). The scrapemark.py has 500 lines of code, but uses regular expressions, so it may be not so fast, did not test.\nExample for scraping your website:\n\nimport sys\nfrom pprint import pprint\nfrom scrapemark import scrape\n\npprint(scrape(""""""\n    <table class=""spad"">\n        <tbody>\n            {*\n                <tr>\n                    <td>{{[].day}}</td>\n                    <td>{{[].sunrise}}</td>\n                    <td>{{[].sunset}}</td>\n                    {# ... #}\n                </tr>\n            *}\n        </tbody>\n    </table>\n"""""", url=sys.argv[1] ))\n\nUsage:\npython2 sunscraper.py http://www.example.com/\n\nResult:\n[{\'day\': u\'1. Dez 2012\', \'sunrise\': u\'08:18\', \'sunset\': u\'16:10\'},\n {\'day\': u\'2. Dez 2012\', \'sunrise\': u\'08:19\', \'sunset\': u\'16:10\'},\n {\'day\': u\'3. Dez 2012\', \'sunrise\': u\'08:21\', \'sunset\': u\'16:09\'},\n {\'day\': u\'4. Dez 2012\', \'sunrise\': u\'08:22\', \'sunset\': u\'16:09\'},\n {\'day\': u\'5. Dez 2012\', \'sunrise\': u\'08:23\', \'sunset\': u\'16:08\'},\n {\'day\': u\'6. Dez 2012\', \'sunrise\': u\'08:25\', \'sunset\': u\'16:08\'},\n {\'day\': u\'7. Dez 2012\', \'sunrise\': u\'08:26\', \'sunset\': u\'16:07\'}]\n\n', '\nMake your life easier by using CSS Selectors\nI know I have come late to party but I have a nice suggestion for you.\nUsing BeautifulSoup is already been suggested I would rather prefer using CSS Selectors to scrape data inside HTML\nimport urllib2\nfrom bs4 import BeautifulSoup\n\nmain_url = ""http://www.example.com""\n\nmain_page_html  = tryAgain(main_url)\nmain_page_soup = BeautifulSoup(main_page_html)\n\n# Scrape all TDs from TRs inside Table\nfor tr in main_page_soup.select(""table.class_of_table""):\n   for td in tr.select(""td#id""):\n       print(td.text)\n       # For acnhors inside TD\n       print(td.select(""a"")[0].text)\n       # Value of Href attribute\n       print(td.select(""a"")[0][""href""])\n\n# This is method that scrape URL and if it doesnt get scraped, waits for 20 seconds and then tries again. (I use it because my internet connection sometimes get disconnects)\ndef tryAgain(passed_url):\n    try:\n        page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n        return page\n    except Exception:\n        while 1:\n            print(""Trying again the URL:"")\n            print(passed_url)\n            try:\n                page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n                print(""-------------------------------------"")\n                print(""---- URL was successfully scraped ---"")\n                print(""-------------------------------------"")\n                return page\n            except Exception:\n                time.sleep(20)\n                continue \n\n', '\nIf we think of getting name of items from any specific category then we can do that by specifying the class name of that category using css selector:\nimport requests ; from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(requests.get(\'https://www.flipkart.com/\').text, ""lxml"")\nfor link in soup.select(\'div._2kSfQ4\'):\n    print(link.text)\n\nThis is the partial search results:\nPuma, USPA, Adidas & moreUp to 70% OffMen\'s Shoes\nShirts, T-Shirts...Under 鈧?99For Men\nNike, UCB, Adidas & moreUnder 鈧?99Men\'s Sandals, Slippers\nPhilips & moreStarting 鈧?9LED Bulbs & Emergency Lights\n\n', '\nHere is a simple web crawler, i used BeautifulSoup and we will search for all the links(anchors) who\'s class name is _3NFO0d. I used Flipkar.com, it is an online retailing store.\nimport requests\nfrom bs4 import BeautifulSoup\ndef crawl_flipkart():\n    url = \'https://www.flipkart.com/\'\n    source_code = requests.get(url)\n    plain_text = source_code.text\n    soup = BeautifulSoup(plain_text, ""lxml"")\n    for link in soup.findAll(\'a\', {\'class\': \'_3NFO0d\'}):\n        href = link.get(\'href\')\n        print(href)\n\ncrawl_flipkart()\n\n', '\nPython has good options to scrape the web. The best one with a framework is scrapy. It can be a little tricky for beginners, so here is a little help. \n1. Install python above 3.5 (lower ones till 2.7 will work). \n2. Create a environment in conda ( I did this). \n3. Install scrapy at a location and run in from there. \n4. Scrapy shell will give you an interactive interface to test you code. \n5. Scrapy startproject projectname will create a framework.\n6. Scrapy genspider spidername will create a spider. You can create as many spiders as you want. While doing this make sure you are inside the project directory. \n\n\nThe easier one is to use requests and beautiful soup. Before starting give one hour of time to go through the documentation, it will solve most of your doubts. BS4 offer wide range of parsers that you can opt for. Use user-agent and sleep to make scraping easier. BS4 returns a bs.tag so use variable[0]. If there is js running, you wont be able to scrape using requests and bs4 directly. You  could get the api link then parse the JSON to get the information you need or try selenium.  \n']",https://stackoverflow.com/questions/2081586/web-scraping-with-python,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping: SSL: CERTIFICATE_VERIFY_FAILED error for http://en.wikipedia.org,"
I'm practicing the code from 'Web Scraping with Python', and I keep having this certificate problem:
from urllib.request import urlopen 
from bs4 import BeautifulSoup 
import re

pages = set()
def getLinks(pageUrl):
    global pages
    html = urlopen(""http://en.wikipedia.org""+pageUrl)
    bsObj = BeautifulSoup(html)
    for link in bsObj.findAll(""a"", href=re.compile(""^(/wiki/)"")):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                #We have encountered a new page
                newPage = link.attrs['href'] 
                print(newPage) 
                pages.add(newPage) 
                getLinks(newPage)
getLinks("""")

The error is:
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 1319, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1049)>

Btw,I was also practicing scrapy, but kept getting the problem: command not found: scrapy (I tried all sorts of solutions online but none works... really frustrating)
",338k,"
            252
        ","['\nOnce upon a time I stumbled  with this issue. If you\'re using macOS go to Macintosh HD > Applications > Python3.6 folder (or whatever version of python you\'re using) > double click on ""Install Certificates.command"" file. :D\n', '\nto use unverified ssl you can add this to your code:\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n', '\nThis terminal command:\nopen /Applications/Python\\ 3.7/Install\\ Certificates.command\nFound here:\nhttps://stackoverflow.com/a/57614113/6207266\nResolved it for me. \nWith my config\npip install --upgrade certifi\nhad no impact.\n', '\nTo solve this: \nAll you need to do is to install Python certificates! A common issue on macOS.  \nOpen these files: \nInstall Certificates.command\nUpdate Shell Profile.command\n\nSimply Run these two scripts and you wont have this issue any more.  \nHope this helps!\n', '\nFor novice users, you can go in the Applications folder and expand the Python 3.7 folder. Now first run (or double click) the Install Certificates.command and then Update Shell Profile.command\n\n', '\nFor anyone who is using anaconda, you would install the certifi package, see more at: \nhttps://anaconda.org/anaconda/certifi\nTo install, type this line in your terminal:\nconda install -c anaconda certifi\n\n', '\nopen /Applications/Python\\ 3.7/Install\\ Certificates.command\n\nTry this command in terminal\n', '\nTwo steps worked for me :\n- going Macintosh HD > Applications > Python3.7 folder \n- click on ""Install Certificates.command""\n', ""\nIf you're running on a Mac you could just search for Install Certificates.command on the spotlight and hit enter.\n"", '\nI could find this solution and is working fine:\ncd /Applications/Python\\ 3.7/\n./Install\\ Certificates.command\n\n', '\nTake a look at this post, it seems like for later versions of Python, certificates are not pre installed which seems to cause this error. You should be able to run the following command to install the certifi package: /Applications/Python\\ 3.6/Install\\ Certificates.command\nPost 1: urllib and ""SSL: CERTIFICATE_VERIFY_FAILED"" Error\nPost 2: Airbrake error: urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate\n', '\nI had the same error and solved the problem by running the program code below:\n# install_certifi.py\n#\n# sample script to install or update a set of default Root Certificates\n# for the ssl module.  Uses the certificates provided by the certifi package:\n#       https://pypi.python.org/pypi/certifi\n\nimport os\nimport os.path\nimport ssl\nimport stat\nimport subprocess\nimport sys\n\nSTAT_0o775 = ( stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR\n             | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP\n             | stat.S_IROTH |                stat.S_IXOTH )\n\n\ndef main():\n    openssl_dir, openssl_cafile = os.path.split(\n        ssl.get_default_verify_paths().openssl_cafile)\n\n    print("" -- pip install --upgrade certifi"")\n    subprocess.check_call([sys.executable,\n        ""-E"", ""-s"", ""-m"", ""pip"", ""install"", ""--upgrade"", ""certifi""])\n\n    import certifi\n\n    # change working directory to the default SSL directory\n    os.chdir(openssl_dir)\n    relpath_to_certifi_cafile = os.path.relpath(certifi.where())\n    print("" -- removing any existing file or link"")\n    try:\n        os.remove(openssl_cafile)\n    except FileNotFoundError:\n        pass\n    print("" -- creating symlink to certifi certificate bundle"")\n    os.symlink(relpath_to_certifi_cafile, openssl_cafile)\n    print("" -- setting permissions"")\n    os.chmod(openssl_cafile, STAT_0o775)\n    print("" -- update complete"")\n\nif __name__ == \'__main__\':\n    main()\n\n', '\ni didn\'t solve the problem, sadly.\nbut managed to make to codes work (almost all of my codes have this probelm btw)\nthe local issuer certificate problem happens under python3.7\nso i changed back to python2.7 QAQ\nand all that needed to change including ""from urllib2 import urlopen"" instead of ""from urllib.request import urlopen""\nso sad...\n', ""\nI'm a relative novice compared to all the experts on Stack Overflow.\nI have 2 versions of jupyter notebook running (one through a fresh Anaconda Navigator installation and one through ????). I think this is because Anaconda was installed as a local installation on my Mac (per Anaconda instructions). \nI already had python 3.7 installed. After that, I used my terminal to open jupyter notebook and I think that it put another version globally onto my Mac. \nHowever, I'm not sure because I'm just learning through trial and error!\nI did the terminal command: \nconda install -c anaconda certifi \n\n(as directed above, but it didn't work.) \nMy python 3.7 is installed on OS Catalina10.15.3 in:\n\n/Library/Python/3.7/site-packages AND\n~/Library/Python/3.7/lib/python/site-packages\n\nThe certificate is at:\n\n~/Library/Python/3.7/lib/python/site-packages/certifi-2019.11.28.dist-info\n\nI tried to find the Install Certificate.command ... but couldn't find it through looking through the file structures...not in Applications...not in links above.\nI finally installed it by finding it through Spotlight (as someone suggested above). And it double clicked automatically and installed ANOTHER certificate in the same folder as:\n\n~/Library/Python/3.7/lib/python/site-packages/\n\nNONE of the above solved anything for me...I still got the same error.  \nSo, I solved the problem by:\n\nclosing my jupyter notebook.\nopening Anaconda Navigator.\nopening jupyter notebook through the Navigator GUI (instead of\nthrough Terminal). \nopening my notebook and running the code.\n\nI can't tell you why this worked.  But it solved the problem for me.\nI just want to save someone the hassle next time. If someone can tell my why it worked, that would be terrific.\nI didn't try the other terminal commands because of the 2 versions of jupyter notebook that I knew were a problem. I just don't know how to fix that.\n"", '\nUse requests library.\nTry this solution, or just add https:// before the URL:\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\n\npages = set()\ndef getLinks(pageUrl):\n    global pages\n    html = requests.get(""http://en.wikipedia.org""+pageUrl, verify=False).text\n    bsObj = BeautifulSoup(html)\n    for link in bsObj.findAll(""a"", href=re.compile(""^(/wiki/)"")):\n        if \'href\' in link.attrs:\n            if link.attrs[\'href\'] not in pages:\n                #We have encountered a new page\n                newPage = link.attrs[\'href\']\n                print(newPage)\n                pages.add(newPage)\n                getLinks(newPage)\ngetLinks("""")\n\nCheck if this works for you\n', '\nFor me the problem was that I was setting REQUESTS_CA_BUNDLE in my .bash_profile\n/Users/westonagreene/.bash_profile:\n...\nexport REQUESTS_CA_BUNDLE=/usr/local/etc/openssl/cert.pem\n...\n\nOnce I set REQUESTS_CA_BUNDLE to blank (i.e. removed from .bash_profile), requests worked again.\nexport REQUESTS_CA_BUNDLE=""""\n\nThe problem only exhibited when executing python requests via a CLI (Command Line Interface). If I ran requests.get(URL, CERT) it resolved just fine.\nMac OS Catalina (10.15.6).\nPyenv of 3.6.11.\nError message I was getting: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)\nMy answer elsewhere: https://stackoverflow.com/a/64151964/4420657\n', ""\nI am using Debian 10 buster and try download a file with youtube-dl and get this error:\nsudo youtube-dl -k https://youtu.be/uscis0CnDjk\n\n[youtube] uscis0CnDjk: Downloading webpage\nERROR: Unable to download webpage: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)> (caused by URLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)')))\n\nCertificates with python2 and python3.8 are installed correctly, but i persistent receive the same error.\nfinally (which is not the best solution, but works for me was to eliminate the certificate check as it is given as an option in youtube-dl) whith this command\nsudo youtube-dl -k --no-check-certificate https://youtu.be/uscis0CnDjk \n"", '\nI am seeing this issue on a Ubuntu 20.04 system and none of the ""real fixes"" (like this one) helped.\nWhile Firefox was willing to open the site just fine neither GNOME Web (i.e. Epiphany) nor Python3 or wget were accepting the certificate. After some searching, I came across this answer on ServerFault which lists two common reasons:\n\n\nThe certificate is really signed by an unknown CA (for instance an internal CA).\nThe certificate is signed with an intermediate CA certificate from one of the well known CA\'s and the remote server is misconfigured in the regard that it doesn\'t include that intermediate CA certificate as a CA chain it\'s response.\n\n\nYou can use the Qualys SSL Labs website to check the site\'s certificates and if there are issues, contact the site\'s administrator to have it fixed.\nIf you really need to work around the issue right now, I\'d recommend a temporary solution like Rambod\'s confined to the site(s) you\'re trying to access.\n', '\nMake sure your websockets is >=10.0\nAdditional to:\nInstall Certificates.command\nUpdate Shell Profile.command\npip3 install websockets==10.0\n', '\nThis will work. Set the environment variable PYTHONHTTPSVERIFY to 0.\n\nBy typing linux command:\n\nexport PYTHONHTTPSVERIFY = 0\n\nOR\n\nUsing in python code:\n\nimport os\nos.environ[""PYTHONHTTPSVERIFY""] = ""0""\n\n', '\nBTW guys if you are getting the same error using aiohttp just put verify_ssl=False argument into your TCPConnector:\nimport aiohttp\n...\n\nasync with aiohttp.ClientSession(\n    connector=aiohttp.TCPConnector(verify_ssl=False)\n) as session:\n    async with session.get(url) as response:\n        body = await response.text()\n\n', ""\nI am using anaconda on windows. Was getting the same error until I tried the following;\nimport urllib.request\nlink = 'http://docs.python.org'\nwith urllib.request.urlopen(link) as response:\n    htmlSource = response.read()\n\nwhich I got from the stackoverflow thread on using urlopen:\nPython urllib urlopen not working\n""]",https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping data from website using vba,"
Im trying to scrape data from website: http://uk.investing.com/rates-bonds/financial-futures via vba, like real-time price, i.e. German 5 YR Bobl, US 30Y T-Bond, i have tried excel web query but it only scrapes the whole website, but I would like to scrape the rate only, is there a way of doing this?
",157k,"
            17
        ","['\nThere are several ways of doing this. This is an answer that I write hoping that all the basics of Internet Explorer automation will be found when browsing for the keywords ""scraping data from website"", but remember that nothing\'s worth as your own research (if you don\'t want to stick to pre-written codes that you\'re not able to customize).\nPlease note that this is one way, that I don\'t prefer in terms of performance (since it depends on the browser speed) but that is good to understand the rationale behind Internet automation.\n1) If I need to browse the web, I need a browser! So I create an Internet Explorer browser:\nDim appIE As Object\nSet appIE = CreateObject(""internetexplorer.application"")\n\n2) I ask the browser to browse the target webpage. Through the use of the property "".Visible"", I decide if I want to see the browser doing its job or not. When building the code is nice to have Visible = True, but when the code is working for scraping data is nice not to see it everytime so Visible = False. \nWith appIE\n    .Navigate ""http://uk.investing.com/rates-bonds/financial-futures""\n    .Visible = True\nEnd With\n\n3) The webpage will need some time to load. So, I will wait meanwhile it\'s busy...\nDo While appIE.Busy\n    DoEvents\nLoop\n\n4) Well, now the page is loaded. Let\'s say that I want to scrape the change of the US30Y T-Bond:\nWhat I will do is just clicking F12 on Internet Explorer to see the webpage\'s code, and hence using the pointer (in red circle) I will click on the element that I want to scrape to see how can I reach my purpose. \n\n5) What I should do is straight-forward. First of all, I will get by the ID property the tr element which is containing the value:\nSet allRowOfData = appIE.document.getElementById(""pair_8907"")\n\nHere I will get a collection of td elements (specifically, tr is a row of data, and the td are its cells. We are looking for the 8th, so I will write:\nDim myValue As String: myValue = allRowOfData.Cells(7).innerHTML\n\nWhy did I write 7 instead of 8? Because the collections of cells starts from 0, so the index of the 8th element is 7 (8-1). Shortly analysing this line of code:\n\n.Cells() makes me access the td elements;\ninnerHTML is the property of the cell containing the value we look for. \n\nOnce we have our value, which is now stored into the myValue variable, we can just close the IE browser and releasing the memory by setting it to Nothing:\nappIE.Quit\nSet appIE = Nothing\n\nWell, now you have your value and you can do whatever you want with it: put it into a cell (Range(""A1"").Value = myValue), or into a label of a form (Me.label1.Text = myValue).\nI\'d just like to point you out that this is not how StackOverflow works: here you post questions about specific coding problems, but you should make your own search first. The reason why I\'m answering a question which is not showing too much research effort is just that I see it asked several times and, back to the time when I learned how to do this, I remember that I would have liked having some better support to get started with. So I hope that this answer, which is just a ""study input"" and not at all the best/most complete solution, can be a support for next user having your same problem. Because I have learned how to program thanks to this community, and I like to think that you and other beginners might use my input to discover the beautiful world of programming. \nEnjoy your practice ;) \n', '\nOther methods were mentioned so let us please acknowledge that, at the time of writing, we are in the 21st century. Let\'s park the local bus browser opening, and fly with an XMLHTTP GET request (XHR GET for short).\nWiki moment:\n\nXHR is an API in the form of an object whose methods transfer data\nbetween a web browser and a web server. The object is provided by the\nbrowser\'s JavaScript environment\n\nIt\'s a fast method for retrieving data that doesn\'t require opening a browser. The server response can be read into an HTMLDocument and the process of grabbing the table continued from there.\nNote that javascript rendered/dynamically added content will not be retrieved as there is no javascript engine running (which there is in a browser).\nIn the below code, the table is grabbed by its id cr1.\n\nIn the helper sub, WriteTable,  we loop the columns (td tags) and then the table rows (tr tags), and finally traverse the length of each table row, table cell by table cell. As we only want data from columns 1 and 8, a Select Case statement is used specify what is written out to the sheet.\n\nSample webpage view:\n\n\nSample code output:\n\n\nVBA:\nOption Explicit\nPublic Sub GetRates()\n    Dim html As HTMLDocument, hTable As HTMLTable \'<== Tools > References > Microsoft HTML Object Library\n    \n    Set html = New HTMLDocument\n      \n    With CreateObject(""MSXML2.XMLHTTP"")\n        .Open ""GET"", ""https://uk.investing.com/rates-bonds/financial-futures"", False\n        .setRequestHeader ""If-Modified-Since"", ""Sat, 1 Jan 2000 00:00:00 GMT"" \'to deal with potential caching\n        .send\n        html.body.innerHTML = .responseText\n    End With\n    \n    Application.ScreenUpdating = False\n    \n    Set hTable = html.getElementById(""cr1"")\n    WriteTable hTable, 1, ThisWorkbook.Worksheets(""Sheet1"")\n    \n    Application.ScreenUpdating = True\nEnd Sub\n\nPublic Sub WriteTable(ByVal hTable As HTMLTable, Optional ByVal startRow As Long = 1, Optional ByVal ws As Worksheet)\n    Dim tSection As Object, tRow As Object, tCell As Object, tr As Object, td As Object, r As Long, C As Long, tBody As Object\n    r = startRow: If ws Is Nothing Then Set ws = ActiveSheet\n    With ws\n        Dim headers As Object, header As Object, columnCounter As Long\n        Set headers = hTable.getElementsByTagName(""th"")\n        For Each header In headers\n            columnCounter = columnCounter + 1\n            Select Case columnCounter\n            Case 2\n                .Cells(startRow, 1) = header.innerText\n            Case 8\n                .Cells(startRow, 2) = header.innerText\n            End Select\n        Next header\n        startRow = startRow + 1\n        Set tBody = hTable.getElementsByTagName(""tbody"")\n        For Each tSection In tBody\n            Set tRow = tSection.getElementsByTagName(""tr"")\n            For Each tr In tRow\n                r = r + 1\n                Set tCell = tr.getElementsByTagName(""td"")\n                C = 1\n                For Each td In tCell\n                    Select Case C\n                    Case 2\n                        .Cells(r, 1).Value = td.innerText\n                    Case 8\n                        .Cells(r, 2).Value = td.innerText\n                    End Select\n                    C = C + 1\n                Next td\n            Next tr\n        Next tSection\n    End With\nEnd Sub\n\n', ""\nyou can use winhttprequest object instead of internet explorer as it's good to load data excluding pictures n advertisement instead of downloading full webpage including advertisement n pictures those make internet explorer object heavy compare to winhttpRequest object. \n"", '\nThis question asked long before. But I thought following information will useful for newbies. Actually you can easily get the values from class name like this.\nSub ExtractLastValue()\n\nSet objIE = CreateObject(""InternetExplorer.Application"")\n\nobjIE.Top = 0\nobjIE.Left = 0\nobjIE.Width = 800\nobjIE.Height = 600\n\nobjIE.Visible = True\n\nobjIE.Navigate (""https://uk.investing.com/rates-bonds/financial-futures/"")\n\nDo\nDoEvents\nLoop Until objIE.readystate = 4\n\nMsgBox objIE.document.getElementsByClassName(""pid-8907-last"")(0).innerText\n\nEnd Sub\n\nAnd if you are new to web scraping please read this blog post.\nWeb Scraping - Basics\nAnd also there are various techniques to extract data from web pages. This article explain few of them with examples.\nWeb Scraping - Collecting Data From a Webpage\n', '\nI modified some thing that were poping up error for me and end up with this which worked great to extract the data as I needed:\nSub get_data_web()\n\nDim appIE As Object\nSet appIE = CreateObject(""internetexplorer.application"")\n\nWith appIE\n    .navigate ""https://finance.yahoo.com/quote/NQ%3DF/futures?p=NQ%3DF""\n    .Visible = True\nEnd With\n\nDo While appIE.Busy\n    DoEvents\nLoop\n\nSet allRowofData = appIE.document.getElementsByClassName(""Ta(end) BdT Bdc($c-fuji-grey-c) H(36px)"")\n\nDim i As Long\nDim myValue As String\n\nCount = 1\n\n    For Each itm In allRowofData\n\n        For i = 0 To 4\n\n        myValue = itm.Cells(i).innerText\n        ActiveSheet.Cells(Count, i + 1).Value = myValue\n\n        Next\n\n        Count = Count + 1\n\n    Next\n\nappIE.Quit\nSet appIE = Nothing\n\n\nEnd Sub\n\n']",https://stackoverflow.com/questions/27066963/scraping-data-from-website-using-vba,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"How to ""scan"" a website (or page) for info, and bring it into my program?","
Well, I'm pretty much trying to figure out how to pull information from a webpage, and bring it into my program (in Java). 
For example, if I know the exact page I want info from, for the sake of simplicity a Best Buy item page, how would I get the appropriate info I need off of that page? Like the title, price, description? 
What would this process even be called? I have no idea were to even begin researching this.
Edit:
Okay, I'm running a test for the JSoup(the one posted by BalusC), but I keep getting this error:
Exception in thread ""main"" java.lang.NoSuchMethodError: java.util.LinkedList.peekFirst()Ljava/lang/Object;
at org.jsoup.parser.TokenQueue.consumeWord(TokenQueue.java:209)
at org.jsoup.parser.Parser.parseStartTag(Parser.java:117)
at org.jsoup.parser.Parser.parse(Parser.java:76)
at org.jsoup.parser.Parser.parse(Parser.java:51)
at org.jsoup.Jsoup.parse(Jsoup.java:28)
at org.jsoup.Jsoup.parse(Jsoup.java:56)
at test.main(test.java:12)

I do have Apache Commons
",111k,"
            58
        ","['\nUse a HTML parser like Jsoup. This has my preference above the other HTML parsers available in Java since it supports jQuery like CSS selectors. Also, its class representing a list of nodes, Elements, implements Iterable so that you can iterate over it in an enhanced for loop (so there\'s no need to hassle with verbose Node and NodeList like classes in the average Java DOM parser).\nHere\'s a basic kickoff example (just put the latest Jsoup JAR file in classpath):\npackage com.stackoverflow.q2835505;\n\nimport org.jsoup.Jsoup;\nimport org.jsoup.nodes.Document;\nimport org.jsoup.nodes.Element;\nimport org.jsoup.select.Elements;\n\npublic class Test {\n\n    public static void main(String[] args) throws Exception {\n        String url = ""https://stackoverflow.com/questions/2835505"";\n        Document document = Jsoup.connect(url).get();\n\n        String question = document.select(""#question .post-text"").text();\n        System.out.println(""Question: "" + question);\n\n        Elements answerers = document.select(""#answers .user-details a"");\n        for (Element answerer : answerers) {\n            System.out.println(""Answerer: "" + answerer.text());\n        }\n    }\n\n}\n\nAs you might have guessed, this prints your own question and the names of all answerers.\n', ""\nThis is referred to as screen scraping, wikipedia has this article on the more specific web scraping. It can be a major challenge because there's some ugly, mess-up, broken-if-not-for-browser-cleverness HTML out there, so good luck. \n"", ""\nI would use JTidy - it is simlar to JSoup, but I don't know JSoup well. JTidy handles broken HTML and returns a w3c Document, so you can use this as a source to XSLT to extract the content you are really interested in. If you don't know XSLT, then you might as well go with JSoup, as the Document model is nicer to work with than w3c.\nEDIT: A quick look on the JSoup website shows that JSoup may indeed be the better choice. It seems to support CSS selectors out the box for extracting stuff from the document. This may be a lot easier to work with than getting into XSLT.\n"", ""\nYou may use an html parser (many useful links here: java html parser).\nThe process is called 'grabbing website content'. Search 'grab website content java' for further invertigation.\n"", '\njsoup supports java 1.5\nhttps://github.com/tburch/jsoup/commit/d8ea84f46e009a7f144ee414a9fa73ea187019a3\nlooks like that stack was a bug, and has been fixed\n', ""\nYou'd probably want to look at the HTML to see if you can find strings that are unique and near your text, then you can use line/char-offsets to get to the data.\nCould be awkward in Java, if there aren't any XML classes similar to the ones found in System.XML.Linq in C#.\n"", '\nYou could also try jARVEST.\nIt is based on a JRuby DSL over a pure-Java engine to spider-scrape-transform web sites.\nExample:\nFind all links inside a web page (wget and xpath are constructs of the jARVEST\'s language):\nwget | xpath(\'//a/@href\')\n\nInside a Java program:\nJarvest jarvest = new Jarvest();\n  String[] results = jarvest.exec(\n    ""wget | xpath(\'//a/@href\')"", //robot! \n    ""http://www.google.com"" //inputs\n  );\n  for (String s : results){\n    System.out.println(s);\n  }\n\n', '\nMy answer won\'t probably be useful to the writer of this question (I am 8 months late so not the right timing I guess) but I think it will probably be useful for many other developers that might come across this answer.\nToday, I just released (in the name of my company) an HTML to POJO complete framework that you can use to map HTML to any POJO class with simply some annotations. The library itself is quite handy and features many other things all the while being very pluggable. You can have a look to it right here : https://github.com/whimtrip/jwht-htmltopojo\nHow to use : Basics\nImagine we need to parse the following html page :\n<html>\n    <head>\n        <title>A Simple HTML Document</title>\n    </head>\n    <body>\n        <div class=""restaurant"">\n            <h1>A la bonne Franquette</h1>\n            <p>French cuisine restaurant for gourmet of fellow french people</p>\n            <div class=""location"">\n                <p>in <span>London</span></p>\n            </div>\n            <p>Restaurant n*18,190. Ranked 113 out of 1,550 restaurants</p>  \n            <div class=""meals"">\n                <div class=""meal"">\n                    <p>Veal Cutlet</p>\n                    <p rating-color=""green"">4.5/5 stars</p>\n                    <p>Chef Mr. Frenchie</p>\n                </div>\n\n                <div class=""meal"">\n                    <p>Ratatouille</p>\n                    <p rating-color=""orange"">3.6/5 stars</p>\n                    <p>Chef Mr. Frenchie and Mme. French-Cuisine</p>\n                </div>\n\n            </div> \n        </div>    \n    </body>\n</html>\n\nLet\'s create the POJOs we want to map it to :\npublic class Restaurant {\n\n    @Selector( value = ""div.restaurant > h1"")\n    private String name;\n\n    @Selector( value = ""div.restaurant > p:nth-child(2)"")\n    private String description;\n\n    @Selector( value = ""div.restaurant > div:nth-child(3) > p > span"")    \n    private String location;    \n\n    @Selector( \n        value = ""div.restaurant > p:nth-child(4)""\n        format = ""^Restaurant n\\*([0-9,]+). Ranked ([0-9,]+) out of ([0-9,]+) restaurants$"",\n        indexForRegexPattern = 1,\n        useDeserializer = true,\n        deserializer = ReplacerDeserializer.class,\n        preConvert = true,\n        postConvert = false\n    )\n    // so that the number becomes a valid number as they are shown in this format : 18,190\n    @ReplaceWith(value = "","", with = """")\n    private Long id;\n\n    @Selector( \n        value = ""div.restaurant > p:nth-child(4)""\n        format = ""^Restaurant n\\*([0-9,]+). Ranked ([0-9,]+) out of ([0-9,]+) restaurants$"",\n        // This time, we want the second regex group and not the first one anymore\n        indexForRegexPattern = 2,\n        useDeserializer = true,\n        deserializer = ReplacerDeserializer.class,\n        preConvert = true,\n        postConvert = false\n    )\n    // so that the number becomes a valid number as they are shown in this format : 18,190\n    @ReplaceWith(value = "","", with = """")\n    private Integer rank;\n\n    @Selector(value = "".meal"")    \n    private List<Meal> meals;\n\n    // getters and setters\n\n}\n\nAnd now the Meal class as well :\npublic class Meal {\n\n    @Selector(value = ""p:nth-child(1)"")\n    private String name;\n\n    @Selector(\n        value = ""p:nth-child(2)"",\n        format = ""^([0-9.]+)\\/5 stars$"",\n        indexForRegexPattern = 1\n    )\n    private Float stars;\n\n    @Selector(\n        value = ""p:nth-child(2)"",\n        // rating-color custom attribute can be used as well\n        attr = ""rating-color""\n    )\n    private String ratingColor;\n\n    @Selector(\n        value = ""p:nth-child(3)""\n    )\n    private String chefs;\n\n    // getters and setters.\n}\n\nWe provided some more explanations on the above code on our github page.\nFor the moment, let\'s see how to scrap this.\nprivate static final String MY_HTML_FILE = ""my-html-file.html"";\n\npublic static void main(String[] args) {\n\n\n    HtmlToPojoEngine htmlToPojoEngine = HtmlToPojoEngine.create();\n\n    HtmlAdapter<Restaurant> adapter = htmlToPojoEngine.adapter(Restaurant.class);\n\n    // If they were several restaurants in the same page, \n    // you would need to create a parent POJO containing\n    // a list of Restaurants as shown with the meals here\n    Restaurant restaurant = adapter.fromHtml(getHtmlBody());\n\n    // That\'s it, do some magic now!\n\n}\n\n\nprivate static String getHtmlBody() throws IOException {\n    byte[] encoded = Files.readAllBytes(Paths.get(MY_HTML_FILE));\n    return new String(encoded, Charset.forName(""UTF-8""));\n\n}\n\nAnother short example can be found here\nHope this will help someone out there!\n', '\nJSoup solution is great, but if you need to extract just something really simple it may be easier to use regex or String.indexOf\nAs others have already mentioned the process is called scraping\n', ""\nLook into the cURL library.  I've never used it in Java, but I'm sure there must be bindings for it.  Basically, what you'll do is send a cURL request to whatever page you want to 'scrape'.  The request will return a string with the source code to the page.  From there, you will use regex to parse whatever data you want from the source code.  That's generally how you are going to do it.\n""]",https://stackoverflow.com/questions/2835505/how-to-scan-a-website-or-page-for-info-and-bring-it-into-my-program,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java HTML Parsing [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 11 years ago.



I'm working on an app which scrapes data from a website and I was wondering how I should go about getting the data.  Specifically I need data contained in a number of div tags which use a specific CSS class - Currently (for testing purposes) I'm just checking for 
div class = ""classname""

in each line of HTML - This works, but I can't help but feel there is a better solution out there.  
Is there any nice way where I could give a class a line of HTML and have some nice methods like:
boolean usesClass(String CSSClassname);
String getText();
String getLink();

",110k,"
            52
        ","['\nAnother library that might be useful for HTML processing is jsoup.\nJsoup tries to clean malformed HTML and allows html parsing in Java using jQuery like tag selector syntax.\nhttp://jsoup.org/ \n', '\nThe main problem as stated by preceding coments is malformed HTML, so an html cleaner or HTML-XML converter is a must. Once you get the XML code (XHTML) there are plenty of tools to handle it. You could get it with a simple SAX handler that extracts only the data you need or any tree-based method (DOM, JDOM, etc.) that let you even modify original code.\nHere is a sample code that uses HTML cleaner to get all DIVs that use a certain class and print out all Text content inside it.\nimport java.io.IOException;\nimport java.net.URL;\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\n\nimport org.htmlcleaner.HtmlCleaner;\nimport org.htmlcleaner.TagNode;\n\n/**\n * @author Fernando Migu茅lez Palomo <fernandoDOTmiguelezATgmailDOTcom>\n */\npublic class TestHtmlParse\n{\n    static final String className = ""tags"";\n    static final String url = ""http://www.stackoverflow.com"";\n\n    TagNode rootNode;\n\n    public TestHtmlParse(URL htmlPage) throws IOException\n    {\n        HtmlCleaner cleaner = new HtmlCleaner();\n        rootNode = cleaner.clean(htmlPage);\n    }\n\n    List getDivsByClass(String CSSClassname)\n    {\n        List divList = new ArrayList();\n\n        TagNode divElements[] = rootNode.getElementsByName(""div"", true);\n        for (int i = 0; divElements != null && i < divElements.length; i++)\n        {\n            String classType = divElements[i].getAttributeByName(""class"");\n            if (classType != null && classType.equals(CSSClassname))\n            {\n                divList.add(divElements[i]);\n            }\n        }\n\n        return divList;\n    }\n\n    public static void main(String[] args)\n    {\n        try\n        {\n            TestHtmlParse thp = new TestHtmlParse(new URL(url));\n\n            List divs = thp.getDivsByClass(className);\n            System.out.println(""*** Text of DIVs with class \'""+className+""\' at \'""+url+""\' ***"");\n            for (Iterator iterator = divs.iterator(); iterator.hasNext();)\n            {\n                TagNode divElement = (TagNode) iterator.next();\n                System.out.println(""Text child nodes of DIV: "" + divElement.getText().toString());\n            }\n        }\n        catch(Exception e)\n        {\n            e.printStackTrace();\n        }\n    }\n}\n\n', '\nSeveral years ago I used JTidy for the same purpose:\nhttp://jtidy.sourceforge.net/\n""JTidy is a Java port of HTML Tidy, a HTML syntax checker and pretty printer. Like its non-Java cousin, JTidy can be used as a tool for cleaning up malformed and faulty HTML. In addition, JTidy provides a DOM interface to the document that is being processed, which effectively makes you able to use JTidy as a DOM parser for real-world HTML.\nJTidy was written by Andy Quick, who later stepped down from the maintainer position. Now JTidy is maintained by a group of volunteers.\nMore information on JTidy can be found on the JTidy SourceForge project page .""\n', '\nYou might be interested by TagSoup, a Java HTML parser able to handle malformed HTML. XML parsers would work only on well formed XHTML.\n', '\nThe HTMLParser project (http://htmlparser.sourceforge.net/) might be a possibility.  It seems to be pretty decent at handling malformed HTML.  The following snippet should do what you need:\nParser parser = new Parser(htmlInput);\nCssSelectorNodeFilter cssFilter = \n    new CssSelectorNodeFilter(""DIV.targetClassName"");\nNodeList nodes = parser.parse(cssFilter);\n\n', '\nJericho: http://jericho.htmlparser.net/docs/index.html\nEasy to use, supports not well formed HTML, a lot of examples.\n', '\nHTMLUnit might be of help. It does a lot more stuff too.\nhttp://htmlunit.sourceforge.net/1\n', '\nLet\'s not forget Jerry, its jQuery in java: a fast and concise Java Library that simplifies HTML document parsing, traversing and manipulating; includes usage of css3 selectors.\nExample:\nJerry doc = jerry(html);\ndoc.$(""div#jodd p.neat"").css(""color"", ""red"").addClass(""ohmy"");\n\nExample:\ndoc.form(""#myform"", new JerryFormHandler() {\n    public void onForm(Jerry form, Map<String, String[]> parameters) {\n        // process form and parameters\n    }\n});\n\nOf course, these are just some quick examples to get the feeling how it all looks like.\n', ""\nThe nu.validator project is an excellent, high performance HTML parser that doesn't cut corners correctness-wise.\n\nThe Validator.nu HTML Parser is an implementation of the HTML5 parsing algorithm in Java. The parser is designed to work as a drop-in replacement for the XML parser in applications that already support XHTML 1.x content with an XML parser and use SAX, DOM or XOM to interface with the parser. Low-level functionality is provided for applications that wish to perform their own IO and support document.write() with scripting. The parser core compiles on Google Web Toolkit and can be automatically translated into C++. (The C++ translation capability is currently used for porting the parser for use in Gecko.)\n\n"", '\nYou can also use XWiki HTML Cleaner:\nIt uses HTMLCleaner and extends it to generate valid XHTML 1.1 content.\n', ""\nIf your HTML is well-formed, you can easily employ an XML parser to do the job for you... If you're only reading, SAX would be ideal.\n""]",https://stackoverflow.com/questions/238036/java-html-parsing,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping dynamic content using python-Scrapy,"
Disclaimer: I've seen numerous other similar posts on StackOverflow and tried to do it the same way but was they don't seem to work on this website.
I'm using Python-Scrapy for getting data from koovs.com. 
However, I'm not able to get the product size, which is dynamically generated. Specifically, if someone could guide me a little on getting the 'Not available' size tag from the drop-down menu on this link, I'd be grateful. 
I am able to get the size list statically, but doing that I only get the list of sizes but not which of them are available.
",55k,"
            47
        ","['\nYou can also solve it with ScrapyJS (no need for selenium and a real browser):\n\nThis library provides Scrapy+JavaScript integration using Splash. \n\nFollow the installation instructions for Splash and ScrapyJS, start the splash docker container:\n$ docker run -p 8050:8050 scrapinghub/splash\n\nPut the following settings into settings.py:\nSPLASH_URL = \'http://192.168.59.103:8050\' \n\nDOWNLOADER_MIDDLEWARES = {\n    \'scrapyjs.SplashMiddleware\': 725,\n}\n\nDUPEFILTER_CLASS = \'scrapyjs.SplashAwareDupeFilter\'\n\nAnd here is your sample spider that is able to see the size availability information:\n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = ""example""\n    allowed_domains = [""koovs.com""]\n    start_urls = (\n        \'http://www.koovs.com/only-onlall-stripe-ls-shirt-59554.html?from=category-651&skuid=236376\',\n    )\n\n    def start_requests(self):\n        for url in self.start_urls:\n            yield scrapy.Request(url, self.parse, meta={\n                \'splash\': {\n                    \'endpoint\': \'render.html\',\n                    \'args\': {\'wait\': 0.5}\n                }\n            })\n\n    def parse(self, response):\n        for option in response.css(""div.select-size select.sizeOptions option"")[1:]:\n            print option.xpath(""text()"").extract()\n\nHere is what is printed on the console:\n[u\'S / 34 -- Not Available\']\n[u\'L / 40 -- Not Available\']\n[u\'L / 42\']\n\n', '\nFrom what I understand, the size availability is determined dynamically in javascript being executed in the browser. Scrapy is not a browser and cannot execute javascript.\nIf you are okay with switching to selenium browser automation tool, here is a sample code:\nfrom selenium import webdriver\nfrom selenium.webdriver.support.select import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\n\nbrowser = webdriver.Firefox()  # can be webdriver.PhantomJS()\nbrowser.get(\'http://www.koovs.com/only-onlall-stripe-ls-shirt-59554.html?from=category-651&skuid=236376\')\n\n# wait for the select element to become visible\nselect_element = WebDriverWait(browser, 10).until(EC.visibility_of_element_located((By.CSS_SELECTOR, ""div.select-size select.sizeOptions"")))\n\nselect = Select(select_element)\nfor option in select.options[1:]:\n    print option.text\n\nbrowser.quit()\n\nIt prints:\nS / 34 -- Not Available\nL / 40 -- Not Available\nL / 42\n\nNote that in place of Firefox you can use other webdrivers like Chrome or Safari. There is also an option to use a headless PhantomJS browser.\nYou can also combine Scrapy with Selenium if needed, see:\n\nselenium with scrapy for dynamic page\nscrapy-webdriver\nseleniumcrawler\n\n', ""\nI faced that problem and solved easily by following these steps\npip install splash \npip install scrapy-splash \npip install scrapyjs\ndownload and install docker-toolbox\nopen docker-quickterminal and enter \n$ docker run -p 8050:8050 scrapinghub/splash\n\nTo set the SPLASH_URL check the default ip configured in the docker machine by entering  $ docker-machine ip default (My IP was 192.168.99.100)\nSPLASH_URL = 'http://192.168.99.100:8050'\nDOWNLOADER_MIDDLEWARES = {\n    'scrapyjs.SplashMiddleware': 725,\n}\n\nDUPEFILTER_CLASS = 'scrapyjs.SplashAwareDupeFilter'\n\nThat's it!\n"", '\nYou have to interpret the json of the website, examples\nscrapy.readthedocs and \ntestingcan.github.io\nimport scrapy\nimport json\nclass QuoteSpider(scrapy.Spider):\n   name = \'quote\'\n   allowed_domains = [\'quotes.toscrape.com\']\n   page = 1\n   start_urls = [\'http://quotes.toscrape.com/api/quotes?page=1\']\n\n   def parse(self, response):\n      data = json.loads(response.text)\n      for quote in data[""quotes""]:\n        yield {""quote"": quote[""text""]}\n      if data[""has_next""]:\n          self.page += 1\n          url = ""http://quotes.toscrape.com/api/quotes?page={}"".format(self.page)\n          yield scrapy.Request(url=url, callback=self.parse)\n\n']",https://stackoverflow.com/questions/30345623/scraping-dynamic-content-using-python-scrapy,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Headless Browser and scraping - solutions [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



I'm trying to put list of possible solutions for browser automatic tests suits and headless browser platforms capable of scraping.

BROWSER TESTING / SCRAPING:

Selenium - polyglot flagship in browser automation, bindings for Python, Ruby,  JavaScript, C#, Haskell and more, IDE for Firefox (as an extension) for faster test deployment. Can act as a Server and has tons of features.

JAVASCRIPT

PhantomJS - JavaScript, headless testing with screen capture and automation, uses Webkit. As of version 1.8 Selenium's WebDriver API is implemented, so you can use any WebDriver binding and tests will be compatible with Selenium
SlimerJS - similar to PhantomJS, uses Gecko (Firefox) instead of WebKit
CasperJS - JavaScript, build on both PhantomJS and SlimerJS, has extra features
Ghost Driver - JavaScript implementation of the WebDriver Wire Protocol for PhantomJS.
new PhantomCSS - CSS regression testing. A CasperJS module for automating visual regression testing with PhantomJS and Resemble.js.
new WebdriverCSS - plugin for Webdriver.io for automating visual regression testing
new PhantomFlow - Describe and visualize user flows through tests. An experimental approach to Web user interface testing.
new trifleJS - ports the PhantomJS API to use the Internet Explorer engine.
new CasperJS IDE (commercial)

NODE.JS

Node-phantom - bridges the gap between PhantomJS and node.js
WebDriverJs - Selenium WebDriver bindings for node.js by Selenium Team
WD.js - node module for WebDriver/Selenium 2
yiewd - WD.js wrapper using latest Harmony generators! Get rid of the callback pyramid with yield
ZombieJs - Insanely fast, headless full-stack testing using node.js
NightwatchJs - Node JS based testing solution using Selenium Webdriver
Chimera - Chimera: can do everything what phantomJS does, but in a full JS environment
Dalek.js - Automated cross browser testing with JavaScript through Selenium Webdriver
Webdriver.io - better implementation of WebDriver bindings with predefined 50+ actions
Nightmare - Electron bridge with a high-level API.
jsdom - Tailored towards web scraping. A very lightweight DOM implemented in Node.js, it supports pages with javascript.
new Puppeteer - Node library which provides a high-level API to control Chrome or Chromium. Puppeteer runs headless by default.

WEB SCRAPING / MINING

Scrapy - Python, mainly a scraper/miner - fast, well documented and, can be linked with Django Dynamic Scraper for nice mining deployments, or Scrapy Cloud for PaaS (server-less) deployment, works in terminal or an server stand-alone proces, can be used with Celery, built on top of Twisted
Snailer - node.js module, untested yet.
Node-Crawler - node.js module, untested yet.

ONLINE TOOLS

new Web Scraping Language - Simple syntax to crawl the web

new Online HTTP client - Dedicated SO answer

dead CasperBox - Run CasperJS scripts online


Android TOOLS for Automation

new Mechanica Browser App


RELATED LINKS & RESOURCES

Comparsion of Webscraping software
new Resemble.js : Image analysis and comparison

Questions:

Any pure Node.js solution or Nodejs to PhanthomJS/CasperJS module that actually works and is documented?

Answer: Chimera seems to go in that direction, checkout Chimera

Other solutions capable of easier JavaScript injection than Selenium?

Do you know any pure ruby solutions?


Answer: Checkout the list created by rjk with ruby based solutions

Do you know any related tech or solution?

Feel free to edit this question and add content as you wish! Thank you for your contributions!
",83k,"
            377
        ","['\nIf Ruby is your thing, you may also try:\n\nhttps://github.com/chriskite/anemone (dev stopped)\nhttps://github.com/sparklemotion/mechanize\nhttps://github.com/postmodern/spidr\nhttps://github.com/stewartmckee/cobweb\nhttp://watirwebdriver.com/ (Selenium)\n\nalso, Nokogiri gem can be used for scraping:\n\nhttp://nokogiri.org/\n\nthere is a dedicated book about how to utilise nokogiri for scraping by packt publishing\n', '\nhttp://triflejs.org/ is like phantomjs but based on IE\n', '\nA kind of JS-based Selenium is Dalek.js. It not only aims for automated frontend-tests, you can also do screenshots with it. It has webdrivers for all important browsers. Unfortunately those webdrivers seem to be worth improving (just not to say ""buggy"" to Firefox).\n']",https://stackoverflow.com/questions/18539491/headless-browser-and-scraping-solutions,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem HTTP error 403 in Python 3 Web Scraping,"
I was trying to scrape a website for practice, but I kept on getting the HTTP Error 403 (does it think I'm a bot)?
Here is my code:
#import requests
import urllib.request
from bs4 import BeautifulSoup
#from urllib import urlopen
import re

webpage = urllib.request.urlopen('http://www.cmegroup.com/trading/products/#sortField=oi&sortAsc=false&venues=3&page=1&cleared=1&group=1').read
findrows = re.compile('<tr class=""- banding(?:On|Off)>(.*?)</tr>')
findlink = re.compile('<a href ="">(.*)</a>')

row_array = re.findall(findrows, webpage)
links = re.finall(findlink, webpate)

print(len(row_array))

iterator = []

The error I get is:
 File ""C:\Python33\lib\urllib\request.py"", line 160, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Python33\lib\urllib\request.py"", line 479, in open
    response = meth(req, response)
  File ""C:\Python33\lib\urllib\request.py"", line 591, in http_response
    'http', request, response, code, msg, hdrs)
  File ""C:\Python33\lib\urllib\request.py"", line 517, in error
    return self._call_chain(*args)
  File ""C:\Python33\lib\urllib\request.py"", line 451, in _call_chain
    result = func(*args)
  File ""C:\Python33\lib\urllib\request.py"", line 599, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden

",279k,"
            168
        ","[""\nThis is probably because of mod_security or some similar server security feature which blocks known spider/bot user agents (urllib uses something like python urllib/3.3.0, it's easily detected). Try setting a known browser user agent with:\nfrom urllib.request import Request, urlopen\n\nreq = Request(\n    url='http://www.cmegroup.com/trading/products/#sortField=oi&sortAsc=false&venues=3&page=1&cleared=1&group=1', \n    headers={'User-Agent': 'Mozilla/5.0'}\n)\nwebpage = urlopen(req).read()\n\nThis works for me.\nBy the way, in your code you are missing the () after .read in the urlopen line, but I think that it's a typo.\nTIP: since this is exercise, choose a different, non restrictive site. Maybe they are blocking urllib for some reason...\n"", '\nDefinitely it\'s blocking because of your use of urllib based on the user agent. This same thing is happening to me with OfferUp. You can create a new class called AppURLopener which overrides the user-agent with Mozilla. \nimport urllib.request\n\nclass AppURLopener(urllib.request.FancyURLopener):\n    version = ""Mozilla/5.0""\n\nopener = AppURLopener()\nresponse = opener.open(\'http://httpbin.org/user-agent\')\n\nSource\n', '\n""This is probably because of mod_security or some similar server security feature which blocks known\n\nspider/bot\n\nuser agents (urllib uses something like python urllib/3.3.0, it\'s easily detected)"" - as already mentioned by Stefano Sanfilippo\nfrom urllib.request import Request, urlopen\nurl=""https://stackoverflow.com/search?q=html+error+403""\nreq = Request(url, headers={\'User-Agent\': \'Mozilla/5.0\'})\n\nweb_byte = urlopen(req).read()\n\nwebpage = web_byte.decode(\'utf-8\')\n\nThe web_byte is a byte object returned by the server and the content type present in webpage is mostly utf-8.\nTherefore you need to decode web_byte using decode method.\nThis solves complete problem while I was having trying to scrape from a website using PyCharm\nP.S -> I use python 3.4\n', ""\nBased on previous answers this has worked for me with Python 3.7 by increasing the timeout to 10.\nfrom urllib.request import Request, urlopen\n\nreq = Request('Url_Link', headers={'User-Agent': 'XYZ/3.0'})\nwebpage = urlopen(req, timeout=10).read()\n\nprint(webpage)\n\n"", '\nAdding cookie to the request headers worked for me\nfrom urllib.request import Request, urlopen\n\n# Function to get the page content\ndef get_page_content(url, head):\n  """"""\n  Function to get the page content\n  """"""\n  req = Request(url, headers=head)\n  return urlopen(req)\n\nurl = \'https://example.com\'\nhead = {\n  \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36\',\n  \'Accept\': \'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\',\n  \'Accept-Charset\': \'ISO-8859-1,utf-8;q=0.7,*;q=0.3\',\n  \'Accept-Encoding\': \'none\',\n  \'Accept-Language\': \'en-US,en;q=0.8\',\n  \'Connection\': \'keep-alive\',\n  \'refere\': \'https://example.com\',\n  \'cookie\': """"""your cookie value ( you can get that from your web page) """"""\n}\n\ndata = get_page_content(url, head).read()\nprint(data)\n\n', ""\nSince the page works in browser and not when calling within python program, it seems that the web app that serves that url recognizes that you request the content not by the browser.\nDemonstration:\ncurl --dump-header r.txt http://www.cmegroup.com/trading/products/#sortField=oi&sortAsc=false&venues=3&page=1&cleared=1&group=1\n\n...\n<HTML><HEAD>\n<TITLE>Access Denied</TITLE>\n</HEAD><BODY>\n<H1>Access Denied</H1>\nYou don't have permission to access ...\n</HTML>\n\nand the content in r.txt has status line:\nHTTP/1.1 403 Forbidden\n\nTry posting header 'User-Agent' which fakes web client.\nNOTE: The page contains Ajax call that creates the table you probably want to parse. You'll need to check the javascript logic of the page or simply using browser debugger (like Firebug / Net tab) to see which url you need to call to get the table's content.\n"", ""\nIf you feel guilty about faking the user-agent as Mozilla (comment in the top answer from Stefano), it could work with a non-urllib User-Agent as well. This worked for the sites I reference:\n    req = urlrequest.Request(link, headers={'User-Agent': 'XYZ/3.0'})\n    urlrequest.urlopen(req, timeout=10).read()\n\nMy application is to test validity by scraping specific links that I refer to, in my articles. Not a generic scraper.\n"", ""\nYou can try in two ways. The detail is in this link. \n1) Via pip\n\npip install --upgrade certifi\n\n2) If it doesn't work, try to run a Cerificates.command that comes bundled with Python 3.* for Mac:(Go to your python installation location and double click the file)\n\nopen /Applications/Python\\ 3.*/Install\\ Certificates.command\n\n"", '\nI ran into this same problem and was not able to solve it using the answers above. I ended up getting around the issue by using requests.get() and then using the .text of the result instead of using read():\nfrom requests import get\n\nreq = get(link)\nresult = req.text\n\n', '\nyou can use urllib\'s build_opener like this:\nopener = urllib.request.build_opener()\nopener.addheaders = [(\'User-Agent\', \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\'), (\'Accept\',\'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\'), (\'Accept-Encoding\',\'gzip, deflate, br\'),\\\n    (\'Accept-Language\',\'en-US,en;q=0.5\' ), (""Connection"", ""keep-alive""), (""Upgrade-Insecure-Requests"",\'1\')]\nurllib.request.install_opener(opener)\nurllib.request.urlretrieve(url, ""test.xlsx"")\n\n', '\nI pulled my hair out with this for a while and the answer ended up being pretty simple. I checked the response text and I was getting ""URL signature expired"" which is a message you wouldn\'t normally see unless you checked the response text.\nThis means some URLs just expire, usually for security purposes. Try to get the URL again and update the URL in your script. If there isn\'t a new URL for the content you\'re trying to scrape, then unfortunately you can\'t scrape for it.\n', '\nOpen the developer tools and open the network tap. chose among the items u want yo scrap, the expanding details will have the user agent and add it there\n']",https://stackoverflow.com/questions/16627227/problem-http-error-403-in-python-3-web-scraping,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I download a file on a click event using selenium?,"
I am working on python and selenium. I want to download file from clicking event using selenium. I wrote following code.  
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.common.keys import Keys

browser = webdriver.Firefox()
browser.get(""http://www.drugcite.com/?q=ACTIMMUNE"")

browser.close()

I want to download both files from links with name ""Export Data"" from given url. How can I achieve it as it works with click event only?
",123k,"
            58
        ","['\nFind the link using find_element(s)_by_*, then call click method.\nfrom selenium import webdriver\n\n# To prevent download dialog\nprofile = webdriver.FirefoxProfile()\nprofile.set_preference(\'browser.download.folderList\', 2) # custom location\nprofile.set_preference(\'browser.download.manager.showWhenStarting\', False)\nprofile.set_preference(\'browser.download.dir\', \'/tmp\')\nprofile.set_preference(\'browser.helperApps.neverAsk.saveToDisk\', \'text/csv\')\n\nbrowser = webdriver.Firefox(profile)\nbrowser.get(""http://www.drugcite.com/?q=ACTIMMUNE"")\n\nbrowser.find_element_by_id(\'exportpt\').click()\nbrowser.find_element_by_id(\'exporthlgt\').click()\n\nAdded profile manipulation code to prevent download dialog.\n', '\nI\'ll admit this solution is a little more ""hacky"" than the Firefox Profile saveToDisk alternative, but it works across both Chrome and Firefox, and doesn\'t rely on a browser-specific feature which could change at any time. And if nothing else, maybe this will give someone a little different perspective on how to solve future challenges.\nPrerequisites: Ensure you have selenium and pyvirtualdisplay installed...\n\nPython 2: sudo pip install selenium pyvirtualdisplay\nPython 3: sudo pip3 install selenium pyvirtualdisplay\n\nThe Magic\nimport pyvirtualdisplay\nimport selenium\nimport selenium.webdriver\nimport time\nimport base64\nimport json\n\nroot_url = \'https://www.google.com\'\ndownload_url = \'https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png\'\n\nprint(\'Opening virtual display\')\ndisplay = pyvirtualdisplay.Display(visible=0, size=(1280, 1024,))\ndisplay.start()\nprint(\'\\tDone\')\n\nprint(\'Opening web browser\')\ndriver = selenium.webdriver.Firefox()\n#driver = selenium.webdriver.Chrome() # Alternately, give Chrome a try\nprint(\'\\tDone\')\n\nprint(\'Retrieving initial web page\')\ndriver.get(root_url)\nprint(\'\\tDone\')\n\nprint(\'Injecting retrieval code into web page\')\ndriver.execute_script(""""""\n    window.file_contents = null;\n    var xhr = new XMLHttpRequest();\n    xhr.responseType = \'blob\';\n    xhr.onload = function() {\n        var reader  = new FileReader();\n        reader.onloadend = function() {\n            window.file_contents = reader.result;\n        };\n        reader.readAsDataURL(xhr.response);\n    };\n    xhr.open(\'GET\', %(download_url)s);\n    xhr.send();\n"""""".replace(\'\\r\\n\', \' \').replace(\'\\r\', \' \').replace(\'\\n\', \' \') % {\n    \'download_url\': json.dumps(download_url),\n})\n\nprint(\'Looping until file is retrieved\')\ndownloaded_file = None\nwhile downloaded_file is None:\n    # Returns the file retrieved base64 encoded (perfect for downloading binary)\n    downloaded_file = driver.execute_script(\'return (window.file_contents !== null ? window.file_contents.split(\\\',\\\')[1] : null);\')\n    print(downloaded_file)\n    if not downloaded_file:\n        print(\'\\tNot downloaded, waiting...\')\n        time.sleep(0.5)\nprint(\'\\tDone\')\n\nprint(\'Writing file to disk\')\nfp = open(\'google-logo.png\', \'wb\')\nfp.write(base64.b64decode(downloaded_file))\nfp.close()\nprint(\'\\tDone\')\ndriver.close() # close web browser, or it\'ll persist after python exits.\ndisplay.popen.kill() # close virtual display, or it\'ll persist after python exits.\n\nExplaination\nWe first load a URL on the domain we\'re targeting a file download from. This allows us to perform an AJAX request on that domain, without running into cross site scripting issues.\nNext, we\'re injecting some javascript into the DOM which fires off an AJAX request. Once the AJAX request returns a response, we take the response and load it into a FileReader object. From there we can extract the base64 encoded content of the file by calling readAsDataUrl(). We\'re then taking the base64 encoded content and appending it to window, a gobally accessible variable.\nFinally, because the AJAX request is asynchronous, we enter  a Python while loop waiting for the content to be appended to the window. Once it\'s appended, we decode the base64 content retrieved from the window and save it to a file.\nThis solution should work across all modern browsers supported by Selenium, and works whether text or binary, and across all mime types.\nAlternate Approach\nWhile I haven\'t tested this, Selenium does afford you the ability to wait until an element is present in the DOM. Rather than looping until a globally accessible variable is populated, you could create an element with a particular ID in the DOM and use the binding of that element as the trigger to retrieve the downloaded file.\n', ""\nIn chrome what I do is downloading the files by clicking on the links, then I open chrome://downloads page and then retrieve the downloaded files list from shadow DOM like this:\ndocs = document\n  .querySelector('downloads-manager')\n  .shadowRoot.querySelector('#downloads-list')\n  .getElementsByTagName('downloads-item')\n\nThis solution is restrained to chrome, the data also contains information like file path and download date. (note this code is from JS, may not be the correct python syntax)\n"", '\nHere is the full working code. You can use web scraping to enter the username password and other field. For getting the field names appearing on the webpage, use inspect element. Element name(Username,Password or Click Button) can be entered through class or name.\nfrom selenium import webdriver\n# Using Chrome to access web\noptions = webdriver.ChromeOptions() \noptions.add_argument(""download.default_directory=C:/Test"") # Set the download Path\ndriver = webdriver.Chrome(options=options)\n# Open the website\ntry:\n    driver.get(\'xxxx\') # Your Website Address\n    password_box = driver.find_element_by_name(\'password\')\n    password_box.send_keys(\'xxxx\') #Password\n    download_button = driver.find_element_by_class_name(\'link_w_pass\')\n    download_button.click()\n    driver.quit()\nexcept:\n    driver.quit()\n    print(""Faulty URL"")\n\n']",https://stackoverflow.com/questions/18439851/how-can-i-download-a-file-on-a-click-event-using-selenium,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using BeautifulSoup to extract text without tags,"
My webpage looks like this:
<p>
  <strong class=""offender"">YOB:</strong> 1987<br/>
  <strong class=""offender"">RACE:</strong> WHITE<br/>
  <strong class=""offender"">GENDER:</strong> FEMALE<br/>
  <strong class=""offender"">HEIGHT:</strong> 5'05''<br/>
  <strong class=""offender"">WEIGHT:</strong> 118<br/>
  <strong class=""offender"">EYE COLOR:</strong> GREEN<br/>
  <strong class=""offender"">HAIR COLOR:</strong> BROWN<br/>
</p>

I want to extract the info for each individual and get YOB:1987, RACE:WHITE, etc...
What I tried is:
subc = soup.find_all('p')
subc1 = subc[1]
subc2 = subc1.find_all('strong')

But this gives me only the values of YOB:, RACE:, etc...
Is there a way that I can get the data in YOB:1987, RACE:WHITE format?
",179k,"
            64
        ","['\nJust loop through all the <strong> tags and use next_sibling to get what you want. Like this:\nfor strong_tag in soup.find_all(\'strong\'):\n    print(strong_tag.text, strong_tag.next_sibling)\n\nDemo:\nfrom bs4 import BeautifulSoup\n\nhtml = \'\'\'\n<p>\n  <strong class=""offender"">YOB:</strong> 1987<br />\n  <strong class=""offender"">RACE:</strong> WHITE<br />\n  <strong class=""offender"">GENDER:</strong> FEMALE<br />\n  <strong class=""offender"">HEIGHT:</strong> 5\'05\'\'<br />\n  <strong class=""offender"">WEIGHT:</strong> 118<br />\n  <strong class=""offender"">EYE COLOR:</strong> GREEN<br />\n  <strong class=""offender"">HAIR COLOR:</strong> BROWN<br />\n</p>\n\'\'\'\n\nsoup = BeautifulSoup(html)\n\nfor strong_tag in soup.find_all(\'strong\'):\n    print(strong_tag.text, strong_tag.next_sibling)\n\nThis gives you:\nYOB:  1987\nRACE:  WHITE\nGENDER:  FEMALE\nHEIGHT:  5\'05\'\'\nWEIGHT:  118\nEYE COLOR:  GREEN\nHAIR COLOR:  BROWN\n\n', '\nI think you can get it using subc1.text.\n>>> html = """"""\n<p>\n    <strong class=""offender"">YOB:</strong> 1987<br />\n    <strong class=""offender"">RACE:</strong> WHITE<br />\n    <strong class=""offender"">GENDER:</strong> FEMALE<br />\n    <strong class=""offender"">HEIGHT:</strong> 5\'05\'\'<br />\n    <strong class=""offender"">WEIGHT:</strong> 118<br />\n    <strong class=""offender"">EYE COLOR:</strong> GREEN<br />\n    <strong class=""offender"">HAIR COLOR:</strong> BROWN<br />\n</p>\n""""""\n>>> from bs4 import BeautifulSoup\n>>> soup = BeautifulSoup(html)\n>>> print soup.text\n\n\nYOB: 1987\nRACE: WHITE\nGENDER: FEMALE\nHEIGHT: 5\'05\'\'\nWEIGHT: 118\nEYE COLOR: GREEN\nHAIR COLOR: BROWN\n\nOr if you want to explore it, you can use .contents :\n>>> p = soup.find(\'p\')\n>>> from pprint import pprint\n>>> pprint(p.contents)\n[u\'\\n\',\n <strong class=""offender"">YOB:</strong>,\n u\' 1987\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">RACE:</strong>,\n u\' WHITE\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">GENDER:</strong>,\n u\' FEMALE\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">HEIGHT:</strong>,\n u"" 5\'05\'\'"",\n <br/>,\n u\'\\n\',\n <strong class=""offender"">WEIGHT:</strong>,\n u\' 118\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">EYE COLOR:</strong>,\n u\' GREEN\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">HAIR COLOR:</strong>,\n u\' BROWN\',\n <br/>,\n u\'\\n\']\n\nand filter out the necessary items from the list:\n>>> data = dict(zip([x.text for x in p.contents[1::4]], [x.strip() for x in p.contents[2::4]]))\n>>> pprint(data)\n{u\'EYE COLOR:\': u\'GREEN\',\n u\'GENDER:\': u\'FEMALE\',\n u\'HAIR COLOR:\': u\'BROWN\',\n u\'HEIGHT:\': u""5\'05\'\'"",\n u\'RACE:\': u\'WHITE\',\n u\'WEIGHT:\': u\'118\',\n u\'YOB:\': u\'1987\'}\n\n', '\nyou can try this indside findall for loop:\nitem_price = item.find(\'span\', attrs={\'class\':\'s-item__price\'}).text\n\nit extracts only text and assigs it to ""item_pice""\n', '\nI think you could solve this with .strip() in gazpacho:\nInput:\nhtml = """"""\\\n<p>\n  <strong class=""offender"">YOB:</strong> 1987<br />\n  <strong class=""offender"">RACE:</strong> WHITE<br />\n  <strong class=""offender"">GENDER:</strong> FEMALE<br />\n  <strong class=""offender"">HEIGHT:</strong> 5\'05\'\'<br />\n  <strong class=""offender"">WEIGHT:</strong> 118<br />\n  <strong class=""offender"">EYE COLOR:</strong> GREEN<br />\n  <strong class=""offender"">HAIR COLOR:</strong> BROWN<br />\n</p>\n""""""\n\nCode:\nsoup = Soup(html)\ntext = soup.find(""p"").strip(whitespace=False) # to keep \\n characters intact\nlines = [\n    line.strip()\n    for line in text.split(""\\n"")\n    if line != """"\n]\ndata = dict([line.split("": "") for line in lines])\n\nOutput:\nprint(data)\n# {\'YOB\': \'1987\',\n#  \'RACE\': \'WHITE\',\n#  \'GENDER\': \'FEMALE\',\n#  \'HEIGHT\': ""5\'05\'\'"",\n#  \'WEIGHT\': \'118\',\n#  \'EYE COLOR\': \'GREEN\',\n#  \'HAIR COLOR\': \'BROWN\'}\n\n']",https://stackoverflow.com/questions/23380171/using-beautifulsoup-to-extract-text-without-tags,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Headless browser for C# (.NET)? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



I am (was) a Python developer who is building a GUI web scraping application. Recently I've decided to migrate to .NET framework and write the same application in C# (this decision wasn't mine).
In Python, I've used the Mechanize library. However, I can't seem to find anything similar in .NET. What I need is a browser that will run in a headless mode, which has the ability to fill out forms, submit them, etc. JavaScript parser is not a must, but it would be quite useful. 
",57k,"
            40
        ","['\nThere are some options:\n\nWebKit.Net (free)\n\nAwesomium\nIt is based on Chrome/WebKit and works like a charm.\nThere is a free license available but also a commercial one and if need be you can buy the source code :-)\n\nHTML Agility Pack (free) (An HTML Parser library, NOT a headless browser)\nThis helps with extracting information from HTML etc. and might be useful in your case (possibly in combination with HttpWebRequest)\n\n\n', ""\nMore solutions:\n\nPhantomJS - full featured headless web\nbrowser. Often used in pair with Selenium which allows you to\naccess the browser from .NET application.\nOptimus (nuget package)- lightweight headless web browser. It's in beta but it is sufficient for some cases.\n\nI used to use both for web testing. But they are also suitable for web scraping.\n"", ""\nYou may be after TrifleJS (currently in beta), or something similar using the .NET WebBrowser class which communicates with IE via a windowless ActiveX/COM API.\nYou'll essentially be running a fully fledged browser (not a http request wrapper) using Internet Explorer's Trident engine, if you are not interested in the JavaScript API (a port of phantomjs) you may still be able to use some of the C# codebase to get around key concepts (custom headers, cookies, script execution, screenshot rendering etc). \nNote that this can also emulate different versions of IE depending on what you have installed.\n\n""]",https://stackoverflow.com/questions/10161413/headless-browser-for-c-sharp-net,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to convert raw javascript object to a dictionary?,"
When screen-scraping some website, I extract data from <script> tags.
The data I get is not in standard JSON format. I cannot use json.loads().
# from
js_obj = '{x:1, y:2, z:3}'

# to
py_obj = {'x':1, 'y':2, 'z':3}

Currently, I use regex to transform the raw data to JSON format.
But I feel pretty bad when I encounter complicated data structure.
Do you have some better solutions?
",30k,"
            32
        ","['\ndemjson.decode()\nimport demjson\n\n# from\njs_obj = \'{x:1, y:2, z:3}\'\n\n# to\npy_obj = demjson.decode(js_obj)\n\njsonnet.evaluate_snippet()\nimport json, _jsonnet\n\n# from\njs_obj = \'{x:1, y:2, z:3}\'\n\n# to\npy_obj = json.loads(_jsonnet.evaluate_snippet(\'snippet\', js_obj))\n\nast.literal_eval()\nimport ast\n\n# from\njs_obj = ""{\'x\':1, \'y\':2, \'z\':3}""\n\n# to\npy_obj = ast.literal_eval(js_obj)\n\n', ""\nUse json5\nimport json5\n\njs_obj = '{x:1, y:2, z:3}'\n\npy_obj = json5.loads(js_obj)\n\nprint(py_obj)\n\n# output\n# {'x': 1, 'y': 2, 'z': 3}\n\n"", ""\nI'm facing the same problem this afternoon, and I finally found a quite good solution. That is JSON5.\nThe syntax of JSON5 is more similar to native JavaScript, so it can help you parse non-standard JSON objects.\nYou might want to check pyjson5 out.\n"", '\nThis will likely not work everywhere, but as a start, here\'s a simple regex that should convert the keys into quoted strings so you can pass into json.loads.  Or is this what you\'re already doing?\nIn[70] : quote_keys_regex = r\'([\\{\\s,])(\\w+)(:)\'\n\nIn[71] : re.sub(quote_keys_regex, r\'\\1""\\2""\\3\', js_obj)\nOut[71]: \'{""x"":1, ""y"":2, ""z"":3}\'\n\nIn[72] : js_obj_2 = \'{x:1, y:2, z:{k:3,j:2}}\'\n\nInt[73]: re.sub(quote_keys_regex, r\'\\1""\\2""\\3\', js_obj_2)\nOut[73]: \'{""x"":1, ""y"":2, ""z"":{""k"":3,""j"":2}}\'\n\n', '\nIf you have node available on the system, you can ask it to evaluate the javascript expression for you, and print the stringified result. The resulting JSON can then be fed to json.loads:\ndef evaluate_javascript(s):\n    """"""Evaluate and stringify a javascript expression in node.js, and convert the\n    resulting JSON to a Python object""""""\n    node = Popen([\'node\', \'-\'], stdin=PIPE, stdout=PIPE)\n    stdout, _ = node.communicate(f\'console.log(JSON.stringify({s}))\'.encode(\'utf8\'))\n    return json.loads(stdout.decode(\'utf8\'))\n\n', '\nNot including objects \njson.loads()\n\njson.loads() doesn\'t accept undefined, you have to change to null\njson.loads() only accept double quotes\n\n\n{""foo"": 1, ""bar"": null}\n\n\nUse this if you are sure that your javascript code only have double quotes on key names.  \nimport json\n\njson_text = """"""{""foo"": 1, ""bar"": undefined}""""""\njson_text = re.sub(r\'(""\\s*:\\s*)undefined(\\s*[,}])\', \'\\\\1null\\\\2\', json_text)\n\npy_obj = json.loads(json_text)\n\nast.literal_eval()\n\nast.literal_eval() doesn\'t accept undefined, you have to change to None\nast.literal_eval() doesn\'t accept null, you have to change to None\nast.literal_eval() doesn\'t accept true, you have to change to True\nast.literal_eval() doesn\'t accept false, you have to change to False\nast.literal_eval() accept single and double quotes\n\n\n{""foo"": 1, ""bar"": None} or {\'foo\': 1, \'bar\': None}\n\n\nimport ast\n\njs_obj = """"""{\'foo\': 1, \'bar\': undefined}""""""\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)undefined(\\s*[,}])\', \'\\\\1None\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)null(\\s*[,}])\', \'\\\\1None\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)NaN(\\s*[,}])\', \'\\\\1None\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)true(\\s*[,}])\', \'\\\\1True\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)false(\\s*[,}])\', \'\\\\1False\\\\2\', js_obj)\n\npy_obj = ast.literal_eval(js_obj) \n\n']",https://stackoverflow.com/questions/24027589/how-to-convert-raw-javascript-object-to-a-dictionary,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to save an image locally using Python whose URL address I already know?,"
I know the URL of an image on Internet.
e.g. http://www.digimouth.com/news/media/2011/09/google-logo.jpg, which contains the logo of Google.
Now, how can I download this image using Python without actually opening the URL in a browser and saving the file manually.
",336k,"
            199
        ","['\nPython 2\nHere is a more straightforward way if all you want to do is save it as a file:\nimport urllib\n\nurllib.urlretrieve(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"", ""local-filename.jpg"")\n\nThe second argument is the local path where the file should be saved.\nPython 3\nAs SergO suggested the  code below should work with Python 3.\nimport urllib.request\n\nurllib.request.urlretrieve(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"", ""local-filename.jpg"")\n\n', '\nimport urllib\nresource = urllib.urlopen(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"")\noutput = open(""file01.jpg"",""wb"")\noutput.write(resource.read())\noutput.close()\n\nfile01.jpg will contain your image. \n', '\nI wrote a script that does just this, and it is available on my github for your use. \nI utilized BeautifulSoup to allow me to parse any website for images. If you will be doing much web scraping (or intend to use my tool) I suggest you sudo pip install BeautifulSoup. Information on BeautifulSoup is available here.\nFor convenience here is my code:\nfrom bs4 import BeautifulSoup\nfrom urllib2 import urlopen\nimport urllib\n\n# use this image scraper from the location that \n#you want to save scraped images to\n\ndef make_soup(url):\n    html = urlopen(url).read()\n    return BeautifulSoup(html)\n\ndef get_images(url):\n    soup = make_soup(url)\n    #this makes a list of bs4 element tags\n    images = [img for img in soup.findAll(\'img\')]\n    print (str(len(images)) + ""images found."")\n    print \'Downloading images to current working directory.\'\n    #compile our unicode list of image links\n    image_links = [each.get(\'src\') for each in images]\n    for each in image_links:\n        filename=each.split(\'/\')[-1]\n        urllib.urlretrieve(each, filename)\n    return image_links\n\n#a standard call looks like this\n#get_images(\'http://www.wookmark.com\')\n\n', ""\nThis can be done with requests. Load the page and dump the binary content to a file.\nimport os\nimport requests\n\nurl = 'https://apod.nasa.gov/apod/image/1701/potw1636aN159_HST_2048.jpg'\npage = requests.get(url)\n\nf_ext = os.path.splitext(url)[-1]\nf_name = 'img{}'.format(f_ext)\nwith open(f_name, 'wb') as f:\n    f.write(page.content)\n\n"", '\nPython 3\nurllib.request 鈥?Extensible library for opening URLs\nfrom urllib.error import HTTPError\nfrom urllib.request import urlretrieve\n\ntry:\n    urlretrieve(image_url, image_local_path)\nexcept FileNotFoundError as err:\n    print(err)   # something wrong with local path\nexcept HTTPError as err:\n    print(err)  # something wrong with url\n\n', '\nI made a script expanding on Yup.\'s script. I fixed some things. It will now bypass 403:Forbidden problems. It wont crash when an image fails to be retrieved. It tries to avoid corrupted previews. It gets the right absolute urls. It gives out more information. It can be run with an argument from the command line. \n# getem.py\n# python2 script to download all images in a given url\n# use: python getem.py http://url.where.images.are\n\nfrom bs4 import BeautifulSoup\nimport urllib2\nimport shutil\nimport requests\nfrom urlparse import urljoin\nimport sys\nimport time\n\ndef make_soup(url):\n    req = urllib2.Request(url, headers={\'User-Agent\' : ""Magic Browser""}) \n    html = urllib2.urlopen(req)\n    return BeautifulSoup(html, \'html.parser\')\n\ndef get_images(url):\n    soup = make_soup(url)\n    images = [img for img in soup.findAll(\'img\')]\n    print (str(len(images)) + "" images found."")\n    print \'Downloading images to current working directory.\'\n    image_links = [each.get(\'src\') for each in images]\n    for each in image_links:\n        try:\n            filename = each.strip().split(\'/\')[-1].strip()\n            src = urljoin(url, each)\n            print \'Getting: \' + filename\n            response = requests.get(src, stream=True)\n            # delay to avoid corrupted previews\n            time.sleep(1)\n            with open(filename, \'wb\') as out_file:\n                shutil.copyfileobj(response.raw, out_file)\n        except:\n            print \'  An error occured. Continuing.\'\n    print \'Done.\'\n\nif __name__ == \'__main__\':\n    url = sys.argv[1]\n    get_images(url)\n\n', '\nA solution which works with Python 2 and Python 3:\ntry:\n    from urllib.request import urlretrieve  # Python 3\nexcept ImportError:\n    from urllib import urlretrieve  # Python 2\n\nurl = ""http://www.digimouth.com/news/media/2011/09/google-logo.jpg""\nurlretrieve(url, ""local-filename.jpg"")\n\nor, if the additional requirement of requests is acceptable and if it is a http(s) URL:\ndef load_requests(source_url, sink_path):\n    """"""\n    Load a file from an URL (e.g. http).\n\n    Parameters\n    ----------\n    source_url : str\n        Where to load the file from.\n    sink_path : str\n        Where the loaded file is stored.\n    """"""\n    import requests\n    r = requests.get(source_url, stream=True)\n    if r.status_code == 200:\n        with open(sink_path, \'wb\') as f:\n            for chunk in r:\n                f.write(chunk)\n\n', ""\nUsing requests library\nimport requests\nimport shutil,os\n\nheaders = {\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'\n}\ncurrentDir = os.getcwd()\npath = os.path.join(currentDir,'Images')#saving images to Images folder\n\ndef ImageDl(url):\n    attempts = 0\n    while attempts < 5:#retry 5 times\n        try:\n            filename = url.split('/')[-1]\n            r = requests.get(url,headers=headers,stream=True,timeout=5)\n            if r.status_code == 200:\n                with open(os.path.join(path,filename),'wb') as f:\n                    r.raw.decode_content = True\n                    shutil.copyfileobj(r.raw,f)\n            print(filename)\n            break\n        except Exception as e:\n            attempts+=1\n            print(e)\n\n\nImageDl(url)\n\n"", ""\nUse a simple python wget module to download the link. Usage below:\nimport wget\nwget.download('http://www.digimouth.com/news/media/2011/09/google-logo.jpg')\n\n"", '\nThis is very short answer.\nimport urllib\nurllib.urlretrieve(""http://photogallery.sandesh.com/Picture.aspx?AlubumId=422040"", ""Abc.jpg"")\n\n', '\nVersion for Python 3\nI adjusted the code of @madprops for Python 3\n# getem.py\n# python2 script to download all images in a given url\n# use: python getem.py http://url.where.images.are\n\nfrom bs4 import BeautifulSoup\nimport urllib.request\nimport shutil\nimport requests\nfrom urllib.parse import urljoin\nimport sys\nimport time\n\ndef make_soup(url):\n    req = urllib.request.Request(url, headers={\'User-Agent\' : ""Magic Browser""}) \n    html = urllib.request.urlopen(req)\n    return BeautifulSoup(html, \'html.parser\')\n\ndef get_images(url):\n    soup = make_soup(url)\n    images = [img for img in soup.findAll(\'img\')]\n    print (str(len(images)) + "" images found."")\n    print(\'Downloading images to current working directory.\')\n    image_links = [each.get(\'src\') for each in images]\n    for each in image_links:\n        try:\n            filename = each.strip().split(\'/\')[-1].strip()\n            src = urljoin(url, each)\n            print(\'Getting: \' + filename)\n            response = requests.get(src, stream=True)\n            # delay to avoid corrupted previews\n            time.sleep(1)\n            with open(filename, \'wb\') as out_file:\n                shutil.copyfileobj(response.raw, out_file)\n        except:\n            print(\'  An error occured. Continuing.\')\n    print(\'Done.\')\n\nif __name__ == \'__main__\':\n    get_images(\'http://www.wookmark.com\')\n\n', '\nLate answer, but for python>=3.6 you can use dload, i.e.:\nimport dload\ndload.save(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"")\n\nif you need the image as bytes, use:\nimg_bytes = dload.bytes(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"")\n\n\ninstall using pip3 install dload\n', '\nSomething fresh for Python 3 using Requests:\nComments in the code. Ready to use function.\n\nimport requests\nfrom os import path\n\ndef get_image(image_url):\n    """"""\n    Get image based on url.\n    :return: Image name if everything OK, False otherwise\n    """"""\n    image_name = path.split(image_url)[1]\n    try:\n        image = requests.get(image_url)\n    except OSError:  # Little too wide, but work OK, no additional imports needed. Catch all conection problems\n        return False\n    if image.status_code == 200:  # we could have retrieved error page\n        base_dir = path.join(path.dirname(path.realpath(__file__)), ""images"") # Use your own path or """" to use current working directory. Folder must exist.\n        with open(path.join(base_dir, image_name), ""wb"") as f:\n            f.write(image.content)\n        return image_name\n\nget_image(""https://apod.nasddfda.gov/apod/image/2003/S106_Mishra_1947.jpg"")\n\n\n', ""\nthis is the easiest method to download images.\nimport requests\nfrom slugify import slugify\n\nimg_url = 'https://apod.nasa.gov/apod/image/1701/potw1636aN159_HST_2048.jpg'\nimg = requests.get(img_url).content\nimg_file = open(slugify(img_url) + '.' + str(img_url).split('.')[-1], 'wb')\nimg_file.write(img)\nimg_file.close()\n\n"", '\nIf you don\'t already have the url for the image, you could scrape it with gazpacho:\nfrom gazpacho import Soup\nbase_url = ""http://books.toscrape.com""\n\nsoup = Soup.get(base_url)\nlinks = [img.attrs[""src""] for img in soup.find(""img"")]\n\nAnd then download the asset with urllib as mentioned:\nfrom pathlib import Path\nfrom urllib.request import urlretrieve as download\n\ndirectory = ""images""\nPath(directory).mkdir(exist_ok=True)\n\nlink = links[0]\nname = link.split(""/"")[-1]\n\ndownload(f""{base_url}/{link}"", f""{directory}/{name}"")\n\n', '\n# import the required libraries from Python\nimport pathlib,urllib.request \n\n# Using pathlib, specify where the image is to be saved\ndownloads_path = str(pathlib.Path.home() / ""Downloads"")\n\n# Form a full image path by joining the path to the \n# images\' new name\n\npicture_path  = os.path.join(downloads_path, ""new-image.png"")\n\n# ""/home/User/Downloads/new-image.png""\n\n# Using ""urlretrieve()"" from urllib.request save the image \nurllib.request.urlretrieve(""//example.com/image.png"", picture_path)\n\n# urlretrieve() takes in 2 arguments\n# 1. The URL of the image to be downloaded\n# 2. The image new name after download. By default, the image is saved\n#    inside your current working directory\n\n', '\nOk, so, this is my rudimentary attempt, and probably total overkill.\nUpdate if needed, as this doesn\'t handle any timeouts, but, I got this working for fun.\nCode listed here: https://github.com/JayRizzo/JayRizzoTools/blob/master/pyImageDownloader.py\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# =============================================================================\n# Created Syst: MAC OSX High Sierra 21.5.0 (17G65)\n# Created Plat: Python 3.9.5 (\'v3.9.5:0a7dcbdb13\', \'May  3 2021 13:17:02\')\n# Created By  : Jeromie Kirchoff\n# Created Date: Thu Jun 15 23:31:01 2022 CDT\n# Last ModDate: Thu Jun 16 01:41:01 2022 CDT\n# =============================================================================\n# NOTE: Doesn\'t work on SVG images at this time.\n# I will look into this further: https://stackoverflow.com/a/6599172/1896134\n# =============================================================================\nimport requests                                 # to get image from the web\nimport shutil                                   # to save it locally\nimport os                                       # needed\nfrom os.path import exists as filepathexist     # check if file paths exist\nfrom os.path import join                        # joins path for different os\nfrom os.path import expanduser                  # expands current home\nfrom pyuser_agent import UA                     # generates random UserAgent\n\nclass ImageDownloader(object):\n    """"""URL ImageDownloader.\n    Input : Full Image URL\n    Output: Image saved to your ~/Pictures/JayRizzoDL folder.\n    """"""\n    def __init__(self, URL: str):\n        self.url = URL\n        self.headers = {""User-Agent"" : UA().random}\n        self.currentHome = expanduser(\'~\')\n        self.desktop = join(self.currentHome + ""/Desktop/"")\n        self.download = join(self.currentHome + ""/Downloads/"")\n        self.pictures = join(self.currentHome + ""/Pictures/JayRizzoDL/"")\n        self.outfile = """"\n        self.filename = """"\n        self.response = """"\n        self.rawstream = """"\n        self.createdfilepath = """"\n        self.imgFileName = """"\n        # Check if the JayRizzoDL exists in the pictures folder.\n        # if it doesn\'t exist create it.\n        if not filepathexist(self.pictures):\n            os.mkdir(self.pictures)\n        self.main()\n\n    def getFileNameFromURL(self, URL: str):\n        """"""Parse the URL for the name after the last forward slash.""""""\n        NewFileName = self.url.strip().split(\'/\')[-1].strip()\n        return NewFileName\n\n    def getResponse(self, URL: str):\n        """"""Try streaming the URL for the raw data.""""""\n        self.response = requests.get(self.url, headers=self.headers, stream=True)\n        return self.response\n\n    def gocreateFile(self, name: str, response):\n        """"""Try creating the file with the raw data in a custom folder.""""""\n        self.outfile = join(self.pictures, name)\n        with open(self.outfile, \'wb\') as outFilePath:\n            shutil.copyfileobj(response.raw, outFilePath)\n        return self.outfile\n\n    def main(self):\n        """"""Combine Everything and use in for loops.""""""\n        self.filename = self.getFileNameFromURL(self.url)\n        self.rawstream = self.getResponse(self.url)\n        self.createdfilepath = self.gocreateFile(self.filename, self.rawstream)\n        print(f""File was created: {self.createdfilepath}"")\n        return\n\nif __name__ == \'__main__\':\n    # Example when calling the file directly.\n    ImageDownloader(""https://stackoverflow.design/assets/img/logos/so/logo-stackoverflow.png"")\n\n\n', '\nDownload Image file, with avoiding all possible error:\nimport requests\nimport validators\nfrom urllib.request import Request, urlopen\nfrom urllib.error import URLError, HTTPError\n\n\ndef is_downloadable(url):\n  valid=validators. url(url)\n  if valid==False:\n    return False\n  req = Request(url)\n  try:\n    response = urlopen(req)\n  except HTTPError as e:\n    return False\n  except URLError as e:\n    return False\n  else:\n    return True\n\n\n\nfor i in range(len(File_data)):   #File data Contain list of address for image \n                                                      #file\n  url = File_data[i][1]\n  try:\n    if (is_downloadable(url)):\n      try:\n        r = requests.get(url, allow_redirects=True)\n        if url.find(\'/\'):\n          fname = url.rsplit(\'/\', 1)[1]\n          fname = pth+File_data[i][0]+""$""+fname #Destination to save \n                                                   #image file\n          open(fname, \'wb\').write(r.content)\n      except Exception as e:\n        print(e)\n  except Exception as e:\n    print(e)\n\n']",https://stackoverflow.com/questions/8286352/how-to-save-an-image-locally-using-python-whose-url-address-i-already-know,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to scrape a website which requires login using python and beautifulsoup?,"
If I want to scrape a website that requires login with password first, how can I start scraping it with python using beautifulsoup4 library? Below is what I do for websites that do not require login. 
from bs4 import BeautifulSoup    
import urllib2 
url = urllib2.urlopen(""http://www.python.org"")    
content = url.read()    
soup = BeautifulSoup(content)

How should the code be changed to accommodate login? Assume that the website I want to scrape is a forum that requires login. An example is http://forum.arduino.cc/index.php
",137k,"
            94
        ","['\nYou can use mechanize:\nimport mechanize\nfrom bs4 import BeautifulSoup\nimport urllib2 \nimport cookielib ## http.cookiejar in python3\n\ncj = cookielib.CookieJar()\nbr = mechanize.Browser()\nbr.set_cookiejar(cj)\nbr.open(""https://id.arduino.cc/auth/login/"")\n\nbr.select_form(nr=0)\nbr.form[\'username\'] = \'username\'\nbr.form[\'password\'] = \'password.\'\nbr.submit()\n\nprint br.response().read()\n\nOr urllib - Login to website using urllib2\n', ""\nThere is a simpler way, from my pov, that gets you there without selenium or mechanize, or other 3rd party tools, albeit it is semi-automated.\nBasically, when you login into a site in a normal way, you identify yourself in a unique way using your credentials, and the same identity is used  thereafter for every other interaction, which is stored in cookies and headers, for a brief period of time.\nWhat you need to do is use the same cookies and headers when you make your http requests, and you'll be in.\nTo replicate that, follow these steps:\n\nIn your browser, open the developer tools\nGo to the site, and login\nAfter the login, go to the network tab, and then refresh the page\nAt this point, you should see a list of requests, the top one being the actual site - and that will be our focus, because it contains the data with the identity we can use for Python and BeautifulSoup to scrape it\nRight click the site request (the top one), hover over copy, and then copy as \ncURL\nLike this:\n\n\n\nThen go to this site which converts cURL into python requests: https://curl.trillworks.com/\nTake the python code and use the generated cookies and headers to proceed with the scraping\n\n"", '\nIf you go for selenium, then you can do something like below:\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\n\n# If you want to open Chrome\ndriver = webdriver.Chrome()\n# If you want to open Firefox\ndriver = webdriver.Firefox()\n\nusername = driver.find_element_by_id(""username"")\npassword = driver.find_element_by_id(""password"")\nusername.send_keys(""YourUsername"")\npassword.send_keys(""YourPassword"")\ndriver.find_element_by_id(""submit_btn"").click()\n\nHowever, if you\'re adamant that you\'re only going to use BeautifulSoup, you can do that with a library like requests or urllib. Basically all you have to do is POST the data as a payload with the URL.\nimport requests\nfrom bs4 import BeautifulSoup\n\nlogin_url = \'http://example.com/login\'\ndata = {\n    \'username\': \'your_username\',\n    \'password\': \'your_password\'\n}\n\nwith requests.Session() as s:\n    response = s.post(login_url , data)\n    print(response.text)\n    index_page= s.get(\'http://example.com\')\n    soup = BeautifulSoup(index_page.text, \'html.parser\')\n    print(soup.title)\n\n', '\nYou can use selenium to log in and retrieve the page source, which you can then pass to Beautiful Soup to extract the data you want.\n', '\nSince Python version wasn\'t specified, here is my take on it for Python 3, done without any external libraries (StackOverflow). After login use BeautifulSoup as usual, or any other kind of scraping.\nLikewise, script on my GitHub here\nWhole script replicated below as to StackOverflow guidelines:\n# Login to website using just Python 3 Standard Library\nimport urllib.parse\nimport urllib.request\nimport http.cookiejar\n\ndef scraper_login():\n    ####### change variables here, like URL, action URL, user, pass\n    # your base URL here, will be used for headers and such, with and without https://\n    base_url = \'www.example.com\'\n    https_base_url = \'https://\' + base_url\n\n    # here goes URL that\'s found inside form action=\'.....\'\n    #   adjust as needed, can be all kinds of weird stuff\n    authentication_url = https_base_url + \'/login\'\n\n    # username and password for login\n    username = \'yourusername\'\n    password = \'SoMePassw0rd!\'\n\n    # we will use this string to confirm a login at end\n    check_string = \'Logout\'\n\n    ####### rest of the script is logic\n    # but you will need to tweak couple things maybe regarding ""token"" logic\n    #   (can be _token or token or _token_ or secret ... etc)\n\n    # big thing! you need a referer for most pages! and correct headers are the key\n    headers={""Content-Type"":""application/x-www-form-urlencoded"",\n    ""User-agent"":""Mozilla/5.0 Chrome/81.0.4044.92"",    # Chrome 80+ as per web search\n    ""Host"":base_url,\n    ""Origin"":https_base_url,\n    ""Referer"":https_base_url}\n\n    # initiate the cookie jar (using : http.cookiejar and urllib.request)\n    cookie_jar = http.cookiejar.CookieJar()\n    opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar))\n    urllib.request.install_opener(opener)\n\n    # first a simple request, just to get login page and parse out the token\n    #       (using : urllib.request)\n    request = urllib.request.Request(https_base_url)\n    response = urllib.request.urlopen(request)\n    contents = response.read()\n\n    # parse the page, we look for token eg. on my page it was something like this:\n    #    <input type=""hidden"" name=""_token"" value=""random1234567890qwertzstring"">\n    #       this can probably be done better with regex and similar\n    #       but I\'m newb, so bear with me\n    html = contents.decode(""utf-8"")\n    # text just before start and just after end of your token string\n    mark_start = \'<input type=""hidden"" name=""_token"" value=""\'\n    mark_end = \'"">\'\n    # index of those two points\n    start_index = html.find(mark_start) + len(mark_start)\n    end_index = html.find(mark_end, start_index)\n    # and text between them is our token, store it for second step of actual login\n    token = html[start_index:end_index]\n\n    # here we craft our payload, it\'s all the form fields, including HIDDEN fields!\n    #   that includes token we scraped earler, as that\'s usually in hidden fields\n    #   make sure left side is from ""name"" attributes of the form,\n    #       and right side is what you want to post as ""value""\n    #   and for hidden fields make sure you replicate the expected answer,\n    #       eg. ""token"" or ""yes I agree"" checkboxes and such\n    payload = {\n        \'_token\':token,\n    #    \'name\':\'value\',    # make sure this is the format of all additional fields !\n        \'login\':username,\n        \'password\':password\n    }\n\n    # now we prepare all we need for login\n    #   data - with our payload (user/pass/token) urlencoded and encoded as bytes\n    data = urllib.parse.urlencode(payload)\n    binary_data = data.encode(\'UTF-8\')\n    # and put the URL + encoded data + correct headers into our POST request\n    #   btw, despite what I thought it is automatically treated as POST\n    #   I guess because of byte encoded data field you don\'t need to say it like this:\n    #       urllib.request.Request(authentication_url, binary_data, headers, method=\'POST\')\n    request = urllib.request.Request(authentication_url, binary_data, headers)\n    response = urllib.request.urlopen(request)\n    contents = response.read()\n\n    # just for kicks, we confirm some element in the page that\'s secure behind the login\n    #   we use a particular string we know only occurs after login,\n    #   like ""logout"" or ""welcome"" or ""member"", etc. I found ""Logout"" is pretty safe so far\n    contents = contents.decode(""utf-8"")\n    index = contents.find(check_string)\n    # if we find it\n    if index != -1:\n        print(f""We found \'{check_string}\' at index position : {index}"")\n    else:\n        print(f""String \'{check_string}\' was not found! Maybe we did not login ?!"")\n\nscraper_login()\n\n']",https://stackoverflow.com/questions/23102833/how-to-scrape-a-website-which-requires-login-using-python-and-beautifulsoup,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python: find_element_by_css_selector,"
I am trying to click the login button with webdriver
<a class=""login-btn"" href=""javascript:;"" data-bind=""click:loginSection.loginClick"">
    <span class=""btn-text"">Login</span>
</a>

My code:
submit=driver.find_element_by_css_selector('a.login-btn').click()

or try this code:
submit=driver.find_element_by_class_name('login-btn').click()

Neither of these is working, need some advice. Thanks in advance
Error:
NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""css selector"",""selector"":""a.login-btn""}

",23k,"
            5
        ","['\nTo click on the Login button you can use either of the the following line of code :\n\nLinkText :\ndriver.find_element_by_link_text(""Login"").click()\n\nCssSelector :\ndriver.find_element_by_css_selector(""a.login-btn > span.btn-text"").click()\n\nGetting more granular with the CssSelector you can also use the following line of code :\ndriver.find_element_by_css_selector(""a.login-btn[data-bind=\'click:loginSection.loginClick\'] > span.btn-text"").click()\n\n\nUpdate :\nAs you are seeing NoSuchElementException you can check this discussion\n']",https://stackoverflow.com/questions/48578336/python-find-element-by-css-selector,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can bs4 get the dynamic content of a webpage if requests can't?,"
So I've tried Selenium previously and now wanted to test out bs4. I tried running the following code but recieved None as an output.
res_pewdiepie = requests.get(
    'https://www.youtube.com/user/PewDiePie')
soup = bs4.BeautifulSoup(res_pewdiepie.content, ""lxml"")
subs = soup.find(id=""sub-count"")
print(subs)

After researching for a while, I found out that requests doesn't load dynamic content like the subcount on YouTube or Socialblade.  Is there a way to get this information with bs4 or if do I have to switch back to something like Selenium?
Thanks in advance!
",624,"
            2
        ","[""\nBeautifulSoup can only parse a text you give it, in this case the page source. If the information is not there it can't do anything about it. So, I believe you have to switch back to something that supports javascript.\nSome options:\npython-selenium\nrequests-html\n"", '\nI use splash for stuff like this. You can run it in a docker container. You can tweak how long it waits for rendering on a per-request basis. There\'s also a scrapy plugin if you\'re doing any serious crawling. Here\'s a snippet from one of my crawlers, running Splash locally using Docker. Good luck.\ntarget_url = ""https://somewhere.example.com/""\nsplash_url = ""http://localhost:8050/render.json""\nbody = json.dumps({""url"": target_url, ""har"": 0, ""html"": 1, ""wait"": 10,})\nheaders = {""Content-Type"": ""application/json""}\n\nresponse = requests.post(splash_url, data=body, headers=headers)\nresult = json.loads(response.text)\nhtml = result[""html""]\n\n']",https://stackoverflow.com/questions/65265321/can-bs4-get-the-dynamic-content-of-a-webpage-if-requests-cant,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do you scrape AJAX pages?,"
Please advise how to scrape AJAX pages.
",74k,"
            57
        ","['\nOverview:\nAll screen scraping first requires manual review of the page you want to extract resources from.  When dealing with AJAX you usually just need to analyze a bit more than just simply the HTML. \nWhen dealing with AJAX this just means that the value you want is not in the initial HTML document that you requested, but that javascript will be exectued which asks the server for the extra information you want. \nYou can therefore usually simply analyze the javascript and see which request the javascript makes and just call this URL instead from the start. \n\nExample:\nTake this as an example, assume the page you want to scrape from has the following script:\n<script type=""text/javascript"">\nfunction ajaxFunction()\n{\nvar xmlHttp;\ntry\n  {\n  // Firefox, Opera 8.0+, Safari\n  xmlHttp=new XMLHttpRequest();\n  }\ncatch (e)\n  {\n  // Internet Explorer\n  try\n    {\n    xmlHttp=new ActiveXObject(""Msxml2.XMLHTTP"");\n    }\n  catch (e)\n    {\n    try\n      {\n      xmlHttp=new ActiveXObject(""Microsoft.XMLHTTP"");\n      }\n    catch (e)\n      {\n      alert(""Your browser does not support AJAX!"");\n      return false;\n      }\n    }\n  }\n  xmlHttp.onreadystatechange=function()\n    {\n    if(xmlHttp.readyState==4)\n      {\n      document.myForm.time.value=xmlHttp.responseText;\n      }\n    }\n  xmlHttp.open(""GET"",""time.asp"",true);\n  xmlHttp.send(null);\n  }\n</script>\n\nThen all you need to do is instead do an HTTP request to time.asp of the same server instead.   Example from w3schools.\n\nAdvanced scraping with C++: \nFor complex usage, and if you\'re using C++ you could also consider using the firefox javascript engine SpiderMonkey to execute the javascript on a page. \nAdvanced scraping with Java:\nFor complex usage, and if you\'re using Java you could also consider using the firefox javascript engine for Java Rhino\nAdvanced scraping with .NET:\nFor complex usage, and if you\'re using .Net you could also consider using the Microsoft.vsa assembly.  Recently replaced with ICodeCompiler/CodeDOM.\n', ""\nIn my opinion the simpliest solution is to use Casperjs, a framework based on the WebKit headless browser phantomjs.\nThe whole page is loaded, and it's very easy to scrape any ajax-related data.\nYou can check this basic tutorial to learn Automating & Scraping with PhantomJS and CasperJS\nYou can also give a look at this example code, on how to scrape google suggests keywords :\n/*global casper:true*/\nvar casper = require('casper').create();\nvar suggestions = [];\nvar word = casper.cli.get(0);\n\nif (!word) {\n    casper.echo('please provide a word').exit(1);\n}\n\ncasper.start('http://www.google.com/', function() {\n    this.sendKeys('input[name=q]', word);\n});\n\ncasper.waitFor(function() {\n  return this.fetchText('.gsq_a table span').indexOf(word) === 0\n}, function() {\n  suggestions = this.evaluate(function() {\n      var nodes = document.querySelectorAll('.gsq_a table span');\n      return [].map.call(nodes, function(node){\n          return node.textContent;\n      });\n  });\n});\n\ncasper.run(function() {\n  this.echo(suggestions.join('\\n')).exit();\n});\n\n"", '\nIf you can get at it, try examining the DOM tree. Selenium does this as a part of testing a page. It also has functions to click buttons and follow links, which may be useful.\n', '\nThe best way to scrape web pages using Ajax or in general pages using Javascript is with a browser itself or a headless browser (a browser without GUI). Currently phantomjs is a well promoted headless browser using WebKit. An alternative that I used with success is HtmlUnit (in Java or .NET via IKVM, which is a simulated browser. Another known alternative is using a web automation tool like Selenium.\nI wrote many articles about this subject like web scraping Ajax and Javascript sites and automated browserless OAuth authentication for Twitter. At the end of the first article there are a lot of extra resources that I have been compiling since 2011.\n', ""\nI like PhearJS, but that might be partially because I built it.\nThat said, it's a service you run in the background that speaks HTTP(S) and renders pages as JSON for you, including any metadata you might need.\n"", ""\nDepends on the ajax page.  The first part of screen scraping is determining how the page works.  Is there some sort of variable you can iterate through to request all the data from the page?  Personally I've used Web Scraper Plus for a lot of screen scraping related tasks because it is cheap, not difficult to get started, non-programmers can get it working relatively quickly.\nSide Note: Terms of Use is probably somewhere you might want to check before doing this.  Depending on the site iterating through everything may raise some flags.  \n"", '\nI think Brian R. Bondy\'s answer is useful when the source code is easy to read. I prefer an easy way using tools like Wireshark or HttpAnalyzer to capture the packet and get the url from  the ""Host"" field and the ""GET"" field.\nFor example,I capture a packet like the following:\nGET /hqzx/quote.aspx?type=3&market=1&sorttype=3&updown=up&page=1&count=8&time=164330 \n HTTP/1.1\nAccept: */*\nReferer: http://quote.hexun.com/stock/default.aspx\nAccept-Language: zh-cn\nAccept-Encoding: gzip, deflate\nUser-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)\nHost: quote.tool.hexun.com\nConnection: Keep-Alive\n\nThen the URL is : \nhttp://quote.tool.hexun.com/hqzx/quote.aspx?type=3&market=1&sorttype=3&updown=up&page=1&count=8&time=164330\n\n', '\nAs a low cost solution you can also try SWExplorerAutomation (SWEA).  The program creates an automation API for any Web application developed with HTML, DHTML or AJAX. \n', '\nSelenium WebDriver is a good solution: you program a browser and you automate what needs to be done in the browser. Browsers (Chrome, Firefox, etc) provide their own drivers that work with Selenium. Since it works as an automated REAL browser, the pages (including javascript and Ajax) get loaded as they do with a human using that browser.\nThe downside is that it is slow (since you would most probably like to wait for all images and scripts to load before you do your scraping on that single page).\n', ""\nI have previously linked to MIT's solvent and EnvJS as my answers to scrape off Ajax pages. These projects seem no longer accessible.\nOut of sheer necessity, I have invented another way to actually scrape off Ajax pages, and it has worked for tough sites like findthecompany which have methods to find headless javascript engines and show no data.\nThe technique is to use chrome extensions to do scraping. Chrome extensions are the best place to scrape off Ajax pages because they actually allow us access to javascript modified DOM. The technique is as follows, I will certainly open source the code in sometime. Create a chrome extension ( assuming you know how to create one, and its architecture and capabilities. This is easy to learn and practice as there are lots of samples),\n\nUse content scripts to access the DOM, by using xpath. Pretty much get the entire list or table or dynamically rendered content using xpath into a variable as string HTML Nodes. ( Only content scripts can access DOM but they can't contact a URL using XMLHTTP )\nFrom content script, using message passing, message the entire stripped DOM as string, to a background script. ( Background scripts can talk to URLs but can't touch the DOM ). We use message passing to get these to talk.\nYou can use various events to loop through web pages and pass each stripped HTML Node content to the background script.\nNow use the background script, to talk to an external server (on localhost), a simple one created using Nodejs/python. Just send the entire HTML Nodes as string, to the server, where the server would just persist the content posted to it, into files, with appropriate variables to identify page numbers or URLs.\nNow you have scraped AJAX content ( HTML Nodes as string ), but these are partial html nodes. Now you can use your favorite XPATH library to load these into memory and use XPATH to scrape information into Tables or text.\n\nPlease comment if you cant understand and I can write it better. ( first attempt ). Also, I am trying to release sample code as soon as possible.\n""]",https://stackoverflow.com/questions/260540/how-do-you-scrape-ajax-pages,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping Google Finance (BeautifulSoup),"
I'm trying to scrape Google Finance, and get the ""Related Stocks"" table, which has id ""cc-table"" and class ""gf-table"" based on the webpage inspector in Chrome. (Sample Link: https://www.google.com/finance?q=tsla)
But when I run .find(""table"") or .findAll(""table""), this table does not come up. I can find JSON-looking objects with the table's contents in the HTML content in Python, but do not know how to get it. Any ideas?
",8k,"
            1
        ","['\nThe page is rendered with JavaScript. There are several ways to render and scrape it.\nI can scrape it with Selenium.\nFirst install Selenium:\nsudo pip3 install selenium\n\nThen get a driver https://sites.google.com/a/chromium.org/chromedriver/downloads\nimport bs4 as bs\nfrom selenium import webdriver  \nbrowser = webdriver.Chrome()\nurl = (""https://www.google.com/finance?q=tsla"")\nbrowser.get(url)\nhtml_source = browser.page_source\nbrowser.quit()\nsoup = bs.BeautifulSoup(html_source, ""lxml"")\nfor el in soup.find_all(""table"", {""id"": ""cc-table""}):\n    print(el.get_text())\n\nAlternatively  PyQt5\nfrom PyQt5.QtGui import *  \nfrom PyQt5.QtCore import *  \nfrom PyQt5.QtWebKit import *  \nfrom PyQt5.QtWebKitWidgets import QWebPage\nfrom PyQt5.QtWidgets import QApplication\nimport bs4 as bs\nimport sys\n\nclass Render(QWebPage):  \n    def __init__(self, url):  \n        self.app = QApplication(sys.argv)  \n        QWebPage.__init__(self)  \n        self.loadFinished.connect(self._loadFinished)  \n        self.mainFrame().load(QUrl(url))  \n        self.app.exec_()  \n\n    def _loadFinished(self, result):  \n        self.frame = self.mainFrame()  \n        self.app.quit()  \n\nurl = ""https://www.google.com/finance?q=tsla""\nr = Render(url)  \nresult = r.frame.toHtml()\nsoup = bs.BeautifulSoup(result,\'lxml\')\nfor el in soup.find_all(""table"", {""id"": ""cc-table""}):\n    print(el.get_text())\n\nAlternatively Dryscrape \nimport bs4 as bs\nimport dryscrape\n\nurl = ""https://www.google.com/finance?q=tsla""\nsession = dryscrape.Session()\nsession.visit(url)\ndsire_get = session.body()\nsoup = bs.BeautifulSoup(dsire_get,\'lxml\')\nfor el in soup.find_all(""table"", {""id"": ""cc-table""}):\n    print(el.get_text())\n\nall output:\nValuation鈻测柤Company name鈻测柤Price鈻测柤Change鈻测柤Chg %鈻测柤d | m | y鈻测柤Mkt Cap鈻测柤TSLATesla Inc328.40-1.52-0.46%53.69BDDAIFDaimler AG72.94-1.50-2.01%76.29BFFord Motor Company11.53-0.17-1.45%45.25BGMGeneral Motors Co...36.07-0.34-0.93%53.93BRNSDFRENAULT SA EUR3.8197.000.000.00%28.69BHMCHonda Motor Co Lt...27.52-0.18-0.65%49.47BAUDVFAUDI AG NPV840.400.000.00%36.14BTMToyota Motor Corp...109.31-0.53-0.48%177.79BBAMXFBAYER MOTOREN WER...94.57-2.41-2.48%56.93BNSANYNissan Motor Co L...20.400.000.00%42.85BMMTOFMITSUBISHI MOTOR ...6.86+0.091.26%10.22B\n\nEDIT\nQtWebKit got deprecated upstream in Qt 5.5 and removed in 5.6.\nYou can switch to PyQt5.QtWebEngineWidgets\n', '\nYou can scrape Google Finance using BeautifulSoup web scraping library without the need to use selenium as the data you want to extract doesn\'t render via Javascript. Plus it will be much faster than launching the whole browser.\nCheck code in online IDE.\n\nfrom bs4 import BeautifulSoup\nimport requests, lxml, json\n   \nparams = {\n        ""hl"": ""en"" \n        }\n\nheaders = {\n        ""User-Agent"": ""Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36"",\n        }\n\nhtml = requests.get(f""https://www.google.com/finance?q=tsla)"", params=params, headers=headers, timeout=30)\nsoup = BeautifulSoup(html.text, ""lxml"")\n\nticker_data = []\n\nfor ticker in soup.select(\'.tOzDHb\'):\n  title = ticker.select_one(\'.RwFyvf\').text\n  price = ticker.select_one(\'.YMlKec\').text\n  index = ticker.select_one(\'.COaKTb\').text\n  price_change = ticker.select_one(""[jsname=Fe7oBc]"")[""aria-label""]\n\n  ticker_data.append({\n    ""index"": index,\n  ""title"" : title,\n  ""price"" : price,\n  ""price_change"" : price_change\n  })  \nprint(json.dumps(ticker_data, indent=2))\n\nExample output\n[\n  {\n    ""index"": ""Index"",\n    ""title"": ""Dow Jones Industrial Average"",\n    ""price"": ""32,774.41"",\n    ""price_change"": ""Down by 0.18%""\n  },\n  {\n    ""index"": ""Index"",\n    ""title"": ""S&P 500"",\n    ""price"": ""4,122.47"",\n    ""price_change"": ""Down by 0.42%""\n  },\n  {\n    ""index"": ""TSLA"",\n    ""title"": ""Tesla Inc"",\n    ""price"": ""$850.00"",\n    ""price_change"": ""Down by 2.44%""\n  },\n  # ...\n]\n\n\nThere\'s a scrape Google Finance Ticker Quote Data in Python blog post if you need to scrape more data from Google Finance.\n', ""\nMost website owners don't like scrapers because they take data the company values, use up a whole bunch of their server time and bandwidth, and give nothing in return. Big companies like Google may have entire teams employing a whole host of methods to detect and block bots trying to scrape their data.\nThere are several ways around this:\n\nScrape from another less secured website.\nSee if Google or another company has an API for public use.\nUse a more advanced scraper like Selenium (and probably still be blocked by google).\n\n""]",https://stackoverflow.com/questions/45259232/scraping-google-finance-beautifulsoup,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to run Scrapy from within a Python script,"
I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:
http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/
http://snipplr.com/view/67006/using-scrapy-from-a-script/
I can't figure out where I should put my spider code and how to call it from the main function. Please help. This is the example code:
# This snippet can be used to run scrapy spiders independent of scrapyd or the scrapy command line tool and use it from a script. 
# 
# The multiprocessing library is used in order to work around a bug in Twisted, in which you cannot restart an already running reactor or in this case a scrapy instance.
# 
# [Here](http://groups.google.com/group/scrapy-users/browse_thread/thread/f332fc5b749d401a) is the mailing-list discussion for this snippet. 

#!/usr/bin/python
import os
os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'project.settings') #Must be at the top before other imports

from scrapy import log, signals, project
from scrapy.xlib.pydispatch import dispatcher
from scrapy.conf import settings
from scrapy.crawler import CrawlerProcess
from multiprocessing import Process, Queue

class CrawlerScript():

    def __init__(self):
        self.crawler = CrawlerProcess(settings)
        if not hasattr(project, 'crawler'):
            self.crawler.install()
        self.crawler.configure()
        self.items = []
        dispatcher.connect(self._item_passed, signals.item_passed)

    def _item_passed(self, item):
        self.items.append(item)

    def _crawl(self, queue, spider_name):
        spider = self.crawler.spiders.create(spider_name)
        if spider:
            self.crawler.queue.append_spider(spider)
        self.crawler.start()
        self.crawler.stop()
        queue.put(self.items)

    def crawl(self, spider):
        queue = Queue()
        p = Process(target=self._crawl, args=(queue, spider,))
        p.start()
        p.join()
        return queue.get(True)

# Usage
if __name__ == ""__main__"":
    log.start()

    """"""
    This example runs spider1 and then spider2 three times. 
    """"""
    items = list()
    crawler = CrawlerScript()
    items.append(crawler.crawl('spider1'))
    for i in range(3):
        items.append(crawler.crawl('spider2'))
    print items

# Snippet imported from snippets.scrapy.org (which no longer works)
# author: joehillen
# date  : Oct 24, 2010

Thank you.
",83k,"
            85
        ","[""\nAll other answers reference Scrapy v0.x. According to the updated docs, Scrapy 1.0 demands:\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider(scrapy.Spider):\n    # Your spider definition\n    ...\n\nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\n\nprocess.crawl(MySpider)\nprocess.start() # the script will block here until the crawling is finished\n\n"", '\nSimply we can use\nfrom scrapy.crawler import CrawlerProcess\nfrom project.spiders.test_spider import SpiderName\n\nprocess = CrawlerProcess()\nprocess.crawl(SpiderName, arg1=val1,arg2=val2)\nprocess.start()\n\nUse these arguments inside spider __init__ function with the global scope.\n', ""\nThough I haven't tried it I think the answer can be found within the scrapy documentation. To quote directly from it:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import Settings\nfrom scrapy import log\nfrom testspiders.spiders.followall import FollowAllSpider\n\nspider = FollowAllSpider(domain='scrapinghub.com')\ncrawler = Crawler(Settings())\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here\n\nFrom what I gather this is a new development in the library which renders some of the earlier approaches online (such as that in the question) obsolete.\n"", '\nIn scrapy 0.19.x you should do this:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy import log, signals\nfrom testspiders.spiders.followall import FollowAllSpider\nfrom scrapy.utils.project import get_project_settings\n\nspider = FollowAllSpider(domain=\'scrapinghub.com\')\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.signals.connect(reactor.stop, signal=signals.spider_closed)\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here until the spider_closed signal was sent\n\nNote these lines     \nsettings = get_project_settings()\ncrawler = Crawler(settings)\n\nWithout it your spider won\'t use your settings and will not save the items.\nTook me a while to figure out why the example in documentation wasn\'t saving my items. I sent a pull request to fix the doc example.\nOne more to do so is just call command directly from you script\nfrom scrapy import cmdline\ncmdline.execute(""scrapy crawl followall"".split())  #followall is the spider\'s name\n\nCopied this answer from my first answer in here:\nhttps://stackoverflow.com/a/19060485/1402286\n', '\nWhen there are multiple crawlers need to be run inside one python script, the reactor stop needs to be handled with caution as the reactor can only be stopped once and cannot be restarted. \nHowever, I found while doing my project that using \nos.system(""scrapy crawl yourspider"")\n\nis the easiest. This will save me from handling all sorts of signals especially when I have multiple spiders.\nIf Performance is a concern, you can use multiprocessing to run your spiders in parallel, something like:\ndef _crawl(spider_name=None):\n    if spider_name:\n        os.system(\'scrapy crawl %s\' % spider_name)\n    return None\n\ndef run_crawler():\n\n    spider_names = [\'spider1\', \'spider2\', \'spider2\']\n\n    pool = Pool(processes=len(spider_names))\n    pool.map(_crawl, spider_names)\n\n', '\nit  is an improvement of\nScrapy throws an error when run using crawlerprocess\nand https://github.com/scrapy/scrapy/issues/1904#issuecomment-205331087\nFirst create your usual spider for successful command line running. it is very very important that it should run and export data or image or file\nOnce it is over, do just like pasted in my program above spider class definition and below __name __ to invoke settings.\nit will get necessary settings which ""from scrapy.utils.project import get_project_settings"" failed to do which is recommended by many\nboth above and below portions should be there together. only one don\'t run.\nSpider will run in scrapy.cfg folder not any other folder\ntree  diagram may be displayed by the moderators for reference\n#Tree\n[enter image description here][1]\n\n#spider.py\nimport sys\nsys.path.append(r\'D:\\ivana\\flow\') #folder where scrapy.cfg is located\n\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.settings import Settings\nfrom flow import settings as my_settings\n\n#----------------Typical Spider Program starts here-----------------------------\n\n          spider class definition here\n\n#----------------Typical Spider Program ends here-------------------------------\n\nif __name__ == ""__main__"":\n\n    crawler_settings = Settings()\n    crawler_settings.setmodule(my_settings)\n\n    process = CrawlerProcess(settings=crawler_settings)\n    process.crawl(FlowSpider) # it is for class FlowSpider(scrapy.Spider):\n    process.start(stop_after_crawl=True)\n\n', ""\n# -*- coding: utf-8 -*-\nimport sys\nfrom scrapy.cmdline import execute\n\n\ndef gen_argv(s):\n    sys.argv = s.split()\n\n\nif __name__ == '__main__':\n    gen_argv('scrapy crawl abc_spider')\n    execute()\n\nPut this code to the path you can run scrapy crawl abc_spider from command line. (Tested with Scrapy==0.24.6)\n"", ""\nIf you want to run a simple crawling, It's easy by just running command: \nscrapy crawl . \nThere is another options to export your results to store in some formats like: \nJson, xml, csv. \nscrapy crawl  -o result.csv or result.json or result.xml. \nyou may want to try it\n""]",https://stackoverflow.com/questions/13437402/how-to-run-scrapy-from-within-a-python-script,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to connect via HTTPS using Jsoup?,"
It's working fine over HTTP, but when I try and use an HTTPS source it throws the following exception:
10-12 13:22:11.169: WARN/System.err(332): javax.net.ssl.SSLHandshakeException: java.security.cert.CertPathValidatorException: Trust anchor for certification path not found.
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.xnet.provider.jsse.OpenSSLSocketImpl.startHandshake(OpenSSLSocketImpl.java:477)
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.xnet.provider.jsse.OpenSSLSocketImpl.startHandshake(OpenSSLSocketImpl.java:328)
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.http.HttpConnection.setupSecureSocket(HttpConnection.java:185)
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.https.HttpsURLConnectionImpl$HttpsEngine.makeSslConnection(HttpsURLConnectionImpl.java:433)
10-12 13:22:11.189: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.https.HttpsURLConnectionImpl$HttpsEngine.makeConnection(HttpsURLConnectionImpl.java:378)
10-12 13:22:11.189: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.http.HttpURLConnectionImpl.connect(HttpURLConnectionImpl.java:205)
10-12 13:22:11.189: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:152)
10-12 13:22:11.189: WARN/System.err(332):     at org.jsoup.helper.HttpConnection$Response.execute(HttpConnection.java:377)
10-12 13:22:11.189: WARN/System.err(332):     at org.jsoup.helper.HttpConnection$Response.execute(HttpConnection.java:364)
10-12 13:22:11.189: WARN/System.err(332):     at org.jsoup.helper.HttpConnection.execute(HttpConnection.java:143)

Here's the relevant code:
try {
    doc = Jsoup.connect(""https url here"").get();
} catch (IOException e) {
    Log.e(""sys"",""coudnt get the html"");
    e.printStackTrace();
}

",53k,"
            28
        ","['\nIf you want to do it the right way, and/or you need to deal with only one site, then you basically need to grab the SSL certificate of the website in question and import it in your Java key store. This will result in a JKS file which you in turn set as SSL trust store before using Jsoup (or java.net.URLConnection). \nYou can grab the certificate from your webbrowser\'s store. Let\'s assume that you\'re using Firefox.\n\nGo to the website in question using Firefox, which is in your case https://web2.uconn.edu/driver/old/timepoints.php?stopid=10\nLeft in the address bar you\'ll see ""uconn.edu"" in blue (this indicates a valid SSL certificate)\nClick on it for details and then click on the More information button.\nIn the security dialogue which appears, click the View Certificate button.\nIn the certificate panel which appears, go to the Details tab.\nClick the deepest item of the certificate hierarchy, which is in this case ""web2.uconn.edu"" and finally click the Export button.\n\nNow you\'ve a web2.uconn.edu.crt file.\nNext, open the command prompt and import it in the Java key store using the keytool command (it\'s part of the JRE):\nkeytool -import -v -file /path/to/web2.uconn.edu.crt -keystore /path/to/web2.uconn.edu.jks -storepass drowssap\n\nThe -file must point to the location of the .crt file which you just downloaded. The -keystore must point to the location of the generated .jks file (which you in turn want to set as SSL trust store). The -storepass is required, you can just enter whatever password you want as long as it\'s at least 6 characters.\nNow, you\'ve a web2.uconn.edu.jks file. You can finally set it as SSL trust store before connecting as follows:\nSystem.setProperty(""javax.net.ssl.trustStore"", ""/path/to/web2.uconn.edu.jks"");\nDocument document = Jsoup.connect(""https://web2.uconn.edu/driver/old/timepoints.php?stopid=10"").get();\n// ...\n\n\nAs a completely different alternative, particularly when you need to deal with multiple sites (i.e. you\'re creating a world wide web crawler), then you can also instruct Jsoup (basically, java.net.URLConnection) to blindly trust all SSL certificates. See also section ""Dealing with untrusted or misconfigured HTTPS sites"" at the very bottom of this answer: Using java.net.URLConnection to fire and handle HTTP requests\n', '\nIn my case, all I needed to do was to add the .validateTLSCertificates(false) in my connection\nDocument doc  = Jsoup.connect(httpsURLAsString)\n            .timeout(60000).validateTLSCertificates(false).get();\n\nI also had to increase the read timeout but I think this is irrelevant\n', '\nI stumbled over the answers here and in the linked question in my search and want to add two pieces of information, as the accepted answer doesn\'t fit my quite similar scenario, but there is an additional solution that fits even in that case (cert and hostname don\'t match for test systems).\n\nThere is a github request to add such a functionality. So perhaps soon the problem will be solved: https://github.com/jhy/jsoup/pull/343 \nedit: Github request was resolved and the method to disable certificate validation is: validateTLSCertificates(boolean validate)\nBased on http://www.nakov.com/blog/2009/07/16/disable-certificate-validation-in-java-ssl-connections/ I found a solution which seems to work (at least in my scenario where jsoup 1.7.3 is called as part of a maven task). I wrapped it in a method disableSSLCertCheck() that I call before the very first Jsoup.connect().\n\nBefore you use this method, you should be really sure that you understand what you do there - not checking SSL certificates is a really stupid thing. Always use correct SSL certificates for your servers which are signed by a commonly accepted CA. If you can\'t afford a commonly accepted CA use correct SSL certificates nevertheless with @BalusC accepted answer above. If you can\'t configure correct SSL certificates (which should never be the case in production environments) the following method could work:\n    private void disableSSLCertCheck() throws NoSuchAlgorithmException, KeyManagementException {\n    // Create a trust manager that does not validate certificate chains\n    TrustManager[] trustAllCerts = new TrustManager[] {new X509TrustManager() {\n            public java.security.cert.X509Certificate[] getAcceptedIssuers() {\n                return null;\n            }\n            public void checkClientTrusted(X509Certificate[] certs, String authType) {\n            }\n            public void checkServerTrusted(X509Certificate[] certs, String authType) {\n            }\n        }\n    };\n\n    // Install the all-trusting trust manager\n    SSLContext sc = SSLContext.getInstance(""SSL"");\n    sc.init(null, trustAllCerts, new java.security.SecureRandom());\n    HttpsURLConnection.setDefaultSSLSocketFactory(sc.getSocketFactory());\n\n    // Create all-trusting host name verifier\n    HostnameVerifier allHostsValid = new HostnameVerifier() {\n        public boolean verify(String hostname, SSLSession session) {\n            return true;\n        }\n    };\n\n    // Install the all-trusting host verifier\n    HttpsURLConnection.setDefaultHostnameVerifier(allHostsValid);\n    }\n\n', '\nTo suppress certificate warnings for specific JSoup connection can use following approach:\nKotlin\n\nval document = Jsoup.connect(""url"")\n        .sslSocketFactory(socketFactory())\n        .get()\n\n\nprivate fun socketFactory(): SSLSocketFactory {\n    val trustAllCerts = arrayOf<TrustManager>(object : X509TrustManager {\n        @Throws(CertificateException::class)\n        override fun checkClientTrusted(chain: Array<X509Certificate>, authType: String) {\n        }\n\n        @Throws(CertificateException::class)\n        override fun checkServerTrusted(chain: Array<X509Certificate>, authType: String) {\n        }\n\n        override fun getAcceptedIssuers(): Array<X509Certificate> {\n            return arrayOf()\n        }\n    })\n\n    try {\n        val sslContext = SSLContext.getInstance(""TLS"")\n        sslContext.init(null, trustAllCerts, java.security.SecureRandom())\n        return sslContext.socketFactory\n    } catch (e: Exception) {\n        when (e) {\n            is RuntimeException, is KeyManagementException -> {\n                throw RuntimeException(""Failed to create a SSL socket factory"", e)\n            }\n            else -> throw e\n        }\n    }\n}\n\n\nJava\n\n\n Document document = Jsoup.connect(""url"")\n        .sslSocketFactory(socketFactory())\n        .get();\n\n\n  private SSLSocketFactory socketFactory() {\n    TrustManager[] trustAllCerts = new TrustManager[]{new X509TrustManager() {\n      public java.security.cert.X509Certificate[] getAcceptedIssuers() {\n        return null;\n      }\n\n      public void checkClientTrusted(X509Certificate[] certs, String authType) {\n      }\n\n      public void checkServerTrusted(X509Certificate[] certs, String authType) {\n      }\n    }};\n\n    try {\n      SSLContext sslContext = SSLContext.getInstance(""TLS"");\n      sslContext.init(null, trustAllCerts, new java.security.SecureRandom());\n      return sslContext.getSocketFactory();\n    } catch (NoSuchAlgorithmException | KeyManagementException e) {\n      throw new RuntimeException(""Failed to create a SSL socket factory"", e);\n    }\n  }\n\n\nNB. As mentioned before ignoring certificates is not a good idea. \n', ""\nI've had the same problem but took the lazy route - tell your app to ignore the cert and carry on anyway.\nI got the code from here:  How do I use a local HTTPS URL in java?\nYou'll have to import these classes for it to work:\nimport javax.net.ssl.HostnameVerifier;\nimport javax.net.ssl.HttpsURLConnection;\nimport javax.net.ssl.SSLContext;\nimport javax.net.ssl.SSLSession;\nimport javax.net.ssl.TrustManager;\nimport javax.net.ssl.X509TrustManager;\n\nJust run that method somewhere before you try to make the connection and voila, it just trusts the cert no matter what.  Of course this isn't any help if you actually want to make sure the cert is real, but good for monitoring your own internal websites etc.\n"", ""\nI'm no expert in this field but I ran into a similar exception when trying to connect to a website over HTTPS using java.net APIs.  The browser does a lot of work for you regarding SSL certificates when you visit a site using HTTPS.  However, when you are manually connecting to sites (using HTTP requests manually), all that work still needs to be done.  Now I don't know what all this work is exactly, but it has to do with downloading certificates and putting them where Java can find them.  Here's a link that will hopefully point you in the right direction.\nhttp://confluence.atlassian.com/display/JIRA/Connecting+to+SSL+services\n"", '\nI was facing the same issue with Jsoup, I was not able to connect and get the document for https urls but when I changed my JDK version from 1.7 to 1.8, the issue got resolved.\nIt may help you :) \n', ""\nI've had that problem only in dev environment. The solution to solve it was just to add a few flags to ignore SSL to VM:\n-Ddeployment.security.TLSv1.1=false \n-Ddeployment.security.TLSv1.2=false\n\n"", '\nAfter testing the solutions here. It is strange that sslSocketFactory setting in Jsoup is completely useless and it never works. So there is no need to get and set SSLSocketFactory.\nActually the second half of Mori solution works. Just need the following before using Jsoup:\n// Create all-trusting host name verifier\nHostnameVerifier allHostsValid = new HostnameVerifier() {\n    public boolean verify(String hostname, SSLSession session) {\n        return true;\n    }\n};\n\n// Install the all-trusting host verifier\nHttpsURLConnection.setDefaultHostnameVerifier(allHostsValid);\n\nThis is tested with Jsoup 1.13.1.\n', '\nTry following (just put it before Jsoup.connect(""https://example.com""):\n    Authenticator.setDefault(new Authenticator() {\n        @Override\n        protected PasswordAuthentication getPasswordAuthentication() {\n            return new PasswordAuthentication(username, password.toCharArray());\n        }\n    });\n\n']",https://stackoverflow.com/questions/7744075/how-to-connect-via-https-using-jsoup,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to run Puppeteer code in any web browser?,"
I'm trying to do some web scraping with Puppeteer and I need to retrieve the value into a Website I'm building.
I have tried to load the Puppeteer file in the html file as if it was a JavaScript file but I keep getting an error. However, if I run it in a cmd window it works well.

Scraper.js:

getPrice();
function getPrice() {
    const puppeteer = require('puppeteer');
    void (async () => {
        try {
            const browser = await puppeteer.launch()
            const page = await browser.newPage()              
            await page.goto('http://example.com') 
            await page.setViewport({ width: 1920, height: 938 })        
            await page.waitForSelector('.m-hotel-info > .l-container > .l-header-section > .l-m-col-2 > .m-button')
            await page.click('.m-hotel-info > .l-container > .l-header-section > .l-m-col-2 > .m-button')
            await page.waitForSelector('.modal-content')
            await page.click('.tile-hsearch-hws > .m-search-tabs > #edit-search-panel > .l-em-reset > .m-field-wrap > .l-xs-col-4 > .analytics-click')
            await page.waitForNavigation();
            await page.waitForSelector('.tile-search-filter > .l-display-none')
            const innerText = await page.evaluate(() => document.querySelector('.tile-search-filter > .l-display-none').innerText);
            console.log(innerText)
        } catch (error) {
            console.log(error)
        }

    })()
}


index.html:

<html>
  <head></head>
  <body>
    <script src=""../js/scraper.js"" type=""text/javascript""></script>
  </body>
</html>

The expected result should be this one in the console of Chrome:

But I'm getting this error instead:


What am I doing wrong?
",20k,"
            11
        ","['\nEDIT: Since puppeteer removed support for puppeteer-web, I moved it out of the repo and tried to patch it a bit.\nIt does work with browser. The package is called puppeteer-web, specifically made for such cases.\nBut the main point is, there must be some instance of chrome running on some server. Only then you can connect to it.\nYou can use it later on in your web page to drive another browser instance through its WS Endpoint:\n<script src=""https://unpkg.com/puppeteer-web"">\n</script>\n\n<script>\n  const browser = await puppeteer.connect({\n    browserWSEndpoint: `ws://0.0.0.0:8080`, // <-- connect to a server running somewhere\n    ignoreHTTPSErrors: true\n  });\n\n  const pagesCount = (await browser.pages()).length;\n  const browserWSEndpoint = await browser.wsEndpoint();\n  console.log({ browserWSEndpoint, pagesCount });\n</script>\n\nI had some fun with puppeteer and webpack,\n\nplayground-react-puppeteer\nplayground-electron-react-puppeteer-example\n\nSee these answers for full understanding of creating the server and more,\n\nOfficial link to puppeteer-web\nPuppeteer with docker\nPuppeteer with chrome extension\nPuppeteer with local wsEndpoint\n\n', '\nInstead, use Puppeteer in the backend and make an API to interface your frontend with it if your main goal is to web scrape and get the data in the frontend.\n', ""\nPuppeteer runs on the server in Node.js. For the common case, rather than using puppeteer-web to allow the client to write Puppeteer code to control the browser, it's better to create an HTTP or websocket API that lets clients indirectly trigger Puppeteer code.\nReasons to prefer a REST API over puppeteer-connect:\n\nbetter support for arbitrary client codebases--clients that aren't written in JS (desktop, command line and mobile apps, for example) can use the API just as easily as the browser can\nno dependency on puppeteer-connect\nlower client-side complexity; for many use cases JS won't be required at all if HTML forms suffice\nbetter control of client behavior--running a browser on the server is a heavy load and has powerful capabilities that are easy to exploit\neasier to integrate with other backend code and resources like the file system\nprovides seamless integration with an existing API as just another set of routes\nhiding Puppeteer as an implementation detail lets you switch to, say, Playwright in the future without the client code being affected.\n\nSimilarly, rather than exposing a mock fs object to read and write files on the server, we expose REST API endpoints to accomplish these tasks. This is a useful layer of abstraction.\nSince there are many use cases for Puppeteer in the context of an API (usually Express), it's hard to offer a general example, but here are a few case studies you can use as starting points:\n\nPuppeteer unable to run on Heroku\nPuppeteer doesn't close browser\nParallelism of Puppeteer with Express Router Node JS. How to pass page between routes while maintaining concurrency\n\n""]",https://stackoverflow.com/questions/54647694/how-to-run-puppeteer-code-in-any-web-browser,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accessing object in iframe using VBA,"
To the point:
I have successfully used VBA to do the following:

Login to a website using getElementsByName
Select parameters for the report that will be generated (using getelementsby...)
generating the report after selecting parameters which renders the resulting dataset into an iframe on the same page

Important to note - The website is client-side
The above was the simple part, the difficult part is as below:

clicking on a gif image within the iframe that exports the dataset to a csv

I have tried the following:
Dim idoc As HTMLDocument
Dim iframe As HTMLFrameElement
Dim iframe2 As HTMLDocument

Set idoc = objIE.document
Set iframe = idoc.all(""iframename"")
Set iframe2 = iframe.contentDocument

    Do Until InStr(1, objIE.document.all(""iframename"").contentDocument.innerHTML, ""img.gif"", vbTextCompare) = 0
        DoEvents
    Loop

To give some context to the logic above -

I accessed the main frame
i accessed the iframe by its name element
i accessed the content within the iframe
I attempted to find the gif image that needs to be clicked to export to csv

It is at this line that it trips up saying ""Object doesn't support this property or method""
Also tried accessing the iframe gif by the a element and href attribute but this totally failed. I also tried grabbing the image from its source URL but all this does it take me to the page the image is from.
note: the iframe does not have an ID and strangely the gif image does not have an ""onclick"" element/event

Final consideration - attempted scraping the iframe using R

accessing the HTML node of the iframe was simple, however trying to access the attributes of the iframe and subsequently the nodes of the table proved unsuccessful. All it returned was ""Character(0)""
library(rvest)
library(magrittr)

Blah <-read_html(""web address redacted"") %>%
  html_nodes(""#iframe"")%>%
  html_nodes(""#img"")%>%
  html_attr(""#src"")%>%
  #read_html()%>%
  head()
Blah

As soon as a i include read_html the following error returns on the script:
Error in if (grepl(""<|>"", x)) { : argument is of length zero
I suspect this is referring to the Character(0) 
Appreciate any guidance here!
Many Thanks,

HTML

<div align=""center""> 
    <table id=""table1"" style=""border-collapse: collapse"" width=""700"" cellspacing=""0"" cellpadding=""0"" border=""0""> 
        <tbody>
            <tr>
                <td colspan=""6""> &nbsp;</td>
            </tr> 
            <tr> 
                <td colspan=""6""> 
                    <a href=""href redacted"">
                        <img src=""img.gif"" width=""38"" height=""38"" border=""0"" align=""right"">
                    </a>
                    <strong>x - </strong>
                </td>
            </tr> 
        </tbody>
    </table>
</div>

",19k,"
            10
        ","['\nIt is sometimes tricky with iframes. Based on html you provided I have created this example. Which works locally, but would it work for you as well?\nTo get to the IFrame the frames collection can be used. Hope you know the name of the IFrame?\nDim iframeDoc As MSHTML.HTMLDocument\nSet iframeDoc = doc.frames(""iframename"").document\n\nThen to go the the image we can use querySelector method e.g. like this:\nDim img As MSHTML.HTMLImg\nSet img = iframeDoc.querySelector(""div table[id=\'table1\'] tbody tr td a[href^=\'https://stackoverflow.com\'] img"")\n\nThe selector a[href^=\'https://stackoverflow.com\'] selects anchor which has an href attribute which starts with given text. The ^ denotes the beginning.\nThen when we have the image just a simple call to click on its parent which is the desired anchor. HTH\n\nComplete example:\nOption Explicit\n\n\' Add reference to Microsoft Internet Controls (SHDocVw)\n\' Add reference to Microsoft HTML Object Library\n\nSub Demo()\n\n    Dim ie As SHDocVw.InternetExplorer\n    Dim doc As MSHTML.HTMLDocument\n    Dim url As String\n    \n    url = ""file:///C:/Users/dusek/Documents/My Web Sites/mainpage.html""\n    Set ie = New SHDocVw.InternetExplorer\n    ie.Visible = True\n    ie.navigate url\n\n    While ie.Busy Or ie.readyState <> READYSTATE_COMPLETE\n        DoEvents\n    Wend\n    \n    Set doc = ie.document\n    \n    Dim iframeDoc As MSHTML.HTMLDocument\n    Set iframeDoc = doc.frames(""iframename"").document\n    If iframeDoc Is Nothing Then\n        MsgBox ""IFrame with name \'iframename\' was not found.""\n        ie.Quit\n        Exit Sub\n    End If\n    \n    Dim img As MSHTML.HTMLImg\n    Set img = iframeDoc.querySelector(""div table[id=\'table1\'] tbody tr td a[href^=\'https://stackoverflow.com\'] img"")\n    If img Is Nothing Then\n        MsgBox ""Image element within iframe was not found.""\n        ie.Quit\n        Exit Sub\n    Else\n        img.parentElement.Click\n    End If\n    \n    ie.Quit\nEnd Sub\n\n\nMain page HTML used\n\n<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">\n<html xmlns=""http://www.w3.org/1999/xhtml"">\n\n<head>\n<!-- saved from url=(0016)http://localhost -->\n<meta content=""text/html; charset=utf-8"" http-equiv=""Content-Type"" />\n<title>x -</title>\n</head>\n\n<body>\n<iframe name=""iframename"" src=""iframe1.html"">\n</iframe>\n</body>\n\n</html>\n\n\nIFrame HTML used (saved as file iframe1.html\n\n<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">\n<html xmlns=""http://www.w3.org/1999/xhtml"">\n\n<head>\n<!-- saved from url=(0016)http://localhost -->\n<meta content=""text/html; charset=utf-8"" http-equiv=""Content-Type"" />\n<title>Untitled 2</title>\n</head>\n\n<body>\n<div align=""center""> \n    <table id=""table1"" style=""border-collapse: collapse"" width=""700"" cellspacing=""0"" cellpadding=""0"" border=""0""> \n        <tbody>\n            <tr>\n                <td colspan=""6""> &nbsp;</td>\n            </tr> \n            <tr> \n                <td colspan=""6""> \n                    <a href=""https://stackoverflow.com/questions/44902558/accessing-object-in-iframe-using-vba"">\n                        <img src=""img.gif"" width=""38"" height=""38"" border=""0"" align=""right"">\n                    </a>\n                    <strong>x - </strong>\n                </td>\n            </tr> \n        </tbody>\n    </table>\n</div>\n\n</body>\n\n</html>\n\n\nBTW, The frame may be referenced by it\'s index also doc.frames(0).document. Thanks to Paulo Bueno.\n\n', '\nI thought I would expand on the answer already given.\nIn the case of Internet Explorer you may have one of two common situations to handle regarding iframes.\n\nsrc of iframe is subject to same origin policy restrictions:\n\n\nThe iframe src has a different origin to the landing page in which case, due to same origin policy, attempts to access it will yield access denied.\nResolution:\nConsider using selenium basic to automate a different browser such as Chrome where CORS is allowed/you can switch to the iframe and continue working with the iframe document\nExample:\nOption Explicit\n\'download selenium https://github.com/florentbr/SeleniumBasic/releases/tag/v2.0.9.0\n\'Ensure latest applicable driver e.g. ChromeDriver.exe in Selenium folder\n\'VBE > Tools > References > Add reference to selenium type library\nPublic Sub Example()\n    Dim d As WebDriver\n    Const URL As String = ""https://www.rosterresource.com/mlb-roster-grid/""\n    Set d = New ChromeDriver\n    With d\n        .Start ""Chrome""\n        .get URL\n        .SwitchToFrame .FindElementByCss(""iframe"") \'< pass the iframe element as the identifier argument\n        \' .SwitchToDefaultContent \'\'to go back to parent document.\n        Stop \'<== delete me later\n        .Quit\n    End With\nEnd Sub\n\n\n\nsrc of iframe is not subject to same origin policy restrictions:\n\n\nResolution:\nThe methods as detailed in answer already given. Additionally, you can extract the src of the iframe and .Navigate2 that to access\n.Navigate2 .document.querySelector(""iframe"").src\n\nIf you only want to work with the contents of the iframe then simply do your initial .Navigate2 the iframe src and don\'t even visit the initial landing page\nExample:\nOption Explicit\nPublic Sub NavigateUsingSrcOfIframe()\n    Dim IE As New InternetExplorer\n    With IE\n        .Visible = True\n        .Navigate2 ""http://www.bursamalaysia.com/market/listed-companies/company-announcements/5978065""\n\n        While .Busy Or .readyState < 4: DoEvents: Wend\n        \n        .Navigate2 .document.querySelector(""iframe"").src\n        \n        While .Busy Or .readyState < 4: DoEvents: Wend\n\n        Stop \'<== delete me later\n        .Quit\n    End With\nEnd Sub\n\n\n\niframe in ShadowRoot\n\n\nAn unlikely case might be an iframe in shadowroot. You should really have one or the other and not one within the other.\n\nResolution:\nIn that case you need an additional accessor of\nElement.shadowRoot.querySelector(""iframe"").contentDocument\n\nwhere Element is your parent element with shadowRoot attached. This method will only work if the shadowRoot mode is set to Open.\nSide note:\nA nice selenium based example, using ExecuteScript to return shadowRoot is given here: How Do I Access Elements in the Shadow DOM using Selenium in VBA?\n', ""\nAdding to the answers given:\nIf you're ok with using a DLL and rewrite your code, you can run Microsoft's Edge browser (a Chrome-based browser) with VBA. With that you can do almost anything you want. Note however, that access to the DOM is performed by javascript, not by an object like Dim IE As New InternetExplorer. Look at the VBA sample and you'll get the grasp.\nhttps://github.com/peakpeak-github/libEdge\nSidenote: Samples for C# and C++ are also included.\n""]",https://stackoverflow.com/questions/44902558/accessing-object-in-iframe-using-vba,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrapy Very Basic Example,"
Hi I have Python Scrapy installed on my mac and I was trying to follow the very first example on their web. 
They were trying to run the command:
scrapy crawl mininova.org -o scraped_data.json -t json

I don't quite understand what does this mean? looks like scrapy turns out to be a separate program. And I don't think they have a command called crawl. In the example, they have a paragraph of code, which is the definition of the class MininovaSpider and the TorrentItem. I don't know where these two classes should go to, go to the same file and what is the name of this python file? 
",24k,"
            26
        ","['\nTL;DR: see Self-contained minimum example script to run scrapy.\nFirst of all, having a normal Scrapy project with a separate .cfg, settings.py, pipelines.py, items.py, spiders package etc is a recommended way to keep and handle your web-scraping logic. It provides a modularity, separation of concerns that keeps things organized, clear and testable. \nIf you are following the official Scrapy tutorial to create a project, you are running web-scraping via a special scrapy command-line tool:\nscrapy crawl myspider\n\n\nBut, Scrapy also provides an API to run crawling from a script.\nThere are several key concepts that should be mentioned:\n\nSettings class - basically a key-value ""container"" which is initialized with default built-in values\nCrawler class - the main class that acts like a glue for all the different components involved in web-scraping with Scrapy\nTwisted reactor - since Scrapy is built-in on top of twisted asynchronous networking library - to start a crawler, we need to put it inside the Twisted Reactor, which is in simple words, an event loop:\n\n\nThe reactor is the core of the event loop within Twisted 鈥?the loop which drives applications using Twisted. The event loop is a programming construct that waits for and\n  dispatches events or messages in a program. It works by calling some\n  internal or external 鈥渆vent provider鈥? which generally blocks until an\n  event has arrived, and then calls the relevant event handler\n  (鈥渄ispatches the event鈥?. The reactor provides basic interfaces to a\n  number of services, including network communications, threading, and\n  event dispatching.\n\nHere is a basic and simplified process of running Scrapy from script:\n\ncreate a Settings instance (or use get_project_settings() to use existing settings):\nsettings = Settings()  # or settings = get_project_settings()\n\ninstantiate Crawler with settings instance passed in:\ncrawler = Crawler(settings)\n\ninstantiate a spider (this is what it is all about eventually, right?):\nspider = MySpider()\n\nconfigure signals. This is an important step if you want to have a post-processing logic, collect stats or, at least, to ever finish crawling since the twisted reactor needs to be stopped manually. Scrapy docs suggest to stop the reactor in the spider_closed signal handler:\n\n\nNote that you will also have to shutdown the Twisted reactor yourself\n  after the spider is finished. This can be achieved by connecting a\n  handler to the signals.spider_closed signal.\n\ndef callback(spider, reason):\n    stats = spider.crawler.stats.get_stats()\n    # stats here is a dictionary of crawling stats that you usually see on the console        \n\n    # here we need to stop the reactor\n    reactor.stop()\n\ncrawler.signals.connect(callback, signal=signals.spider_closed)\n\n\nconfigure and start crawler instance with a spider passed in:\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\n\noptionally start logging:\nlog.start()\n\nstart the reactor - this would block the script execution:\nreactor.run()\n\n\nHere is an example self-contained script that is using DmozSpider spider and involves item loaders with input and output processors and item pipelines:\nimport json\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.contrib.loader import ItemLoader\nfrom scrapy.contrib.loader.processor import Join, MapCompose, TakeFirst\nfrom scrapy import log, signals, Spider, Item, Field\nfrom scrapy.settings import Settings\nfrom twisted.internet import reactor\n\n\n# define an item class\nclass DmozItem(Item):\n    title = Field()\n    link = Field()\n    desc = Field()\n\n\n# define an item loader with input and output processors\nclass DmozItemLoader(ItemLoader):\n    default_input_processor = MapCompose(unicode.strip)\n    default_output_processor = TakeFirst()\n\n    desc_out = Join()\n\n\n# define a pipeline\nclass JsonWriterPipeline(object):\n    def __init__(self):\n        self.file = open(\'items.jl\', \'wb\')\n\n    def process_item(self, item, spider):\n        line = json.dumps(dict(item)) + ""\\n""\n        self.file.write(line)\n        return item\n\n\n# define a spider\nclass DmozSpider(Spider):\n    name = ""dmoz""\n    allowed_domains = [""dmoz.org""]\n    start_urls = [\n        ""http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"",\n        ""http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/""\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath(\'//ul/li\'):\n            loader = DmozItemLoader(DmozItem(), selector=sel, response=response)\n            loader.add_xpath(\'title\', \'a/text()\')\n            loader.add_xpath(\'link\', \'a/@href\')\n            loader.add_xpath(\'desc\', \'text()\')\n            yield loader.load_item()\n\n\n# callback fired when the spider is closed\ndef callback(spider, reason):\n    stats = spider.crawler.stats.get_stats()  # collect/log stats?\n\n    # stop the reactor\n    reactor.stop()\n\n\n# instantiate settings and provide a custom configuration\nsettings = Settings()\nsettings.set(\'ITEM_PIPELINES\', {\n    \'__main__.JsonWriterPipeline\': 100\n})\n\n# instantiate a crawler passing in settings\ncrawler = Crawler(settings)\n\n# instantiate a spider\nspider = DmozSpider()\n\n# configure signals\ncrawler.signals.connect(callback, signal=signals.spider_closed)\n\n# configure and start the crawler\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\n\n# start logging\nlog.start()\n\n# start the reactor (blocks execution)\nreactor.run()\n\nRun it in a usual way:\npython runner.py\n\nand observe items exported to items.jl with the help of the pipeline:\n{""desc"": """", ""link"": ""/"", ""title"": ""Top""}\n{""link"": ""/Computers/"", ""title"": ""Computers""}\n{""link"": ""/Computers/Programming/"", ""title"": ""Programming""}\n{""link"": ""/Computers/Programming/Languages/"", ""title"": ""Languages""}\n{""link"": ""/Computers/Programming/Languages/Python/"", ""title"": ""Python""}\n...\n\nGist is available here (feel free to improve): \n\nSelf-contained minimum example script to run scrapy\n\n\nNotes:\nIf you define settings by instantiating a Settings() object - you\'ll get all the defaults Scrapy settings. But, if you want to, for example, configure an existing pipeline, or configure a DEPTH_LIMIT or tweak any other setting, you need to either set it in the script via settings.set() (as demonstrated in the example):\npipelines = {\n    \'mypackage.pipelines.FilterPipeline\': 100,\n    \'mypackage.pipelines.MySQLPipeline\': 200\n}\nsettings.set(\'ITEM_PIPELINES\', pipelines, priority=\'cmdline\')\n\nor, use an existing settings.py with all the custom settings preconfigured:\nfrom scrapy.utils.project import get_project_settings\n\nsettings = get_project_settings()\n\n\nOther useful links on the subject:\n\nHow to run Scrapy from within a Python script\nConfused about running Scrapy from within a Python script\nscrapy run spider from script\n\n', '\nYou may have better luck looking through the tutorial first, as opposed to the ""Scrapy at a glance"" webpage.\nThe tutorial implies that Scrapy is, in fact, a separate program. \nRunning the command scrapy startproject tutorial will create a folder called tutorial several files already set up for you. \nFor example, in my case, the modules/packages items, pipelines, settings and spiders have been added to the root package tutorial .\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\nThe TorrentItem class would be placed inside items.py, and the MininovaSpider class would go inside the spiders folder. \nOnce the project is set up, the command-line parameters for Scrapy appear to be fairly straightforward. They take the form:\nscrapy crawl <website-name> -o <output-file> -t <output-type>\n\nAlternatively, if you want to run scrapy without the overhead of creating a project directory, you can use the runspider command:\nscrapy runspider my_spider.py\n\n']",https://stackoverflow.com/questions/18838494/scrapy-very-basic-example,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrape web page contents,"
I am developing a project, for which I want to scrape the contents of a website in the background and get some limited content from that scraped website. For example, in my page I have ""userid"" and ""password"" fields, by using those I will access my mail and scrape my inbox contents and display it in my page.
I done the above by using javascript alone. But when I click the sign in button the URL of my page (http://localhost/web/Login.html) is changed to the URL (http://mail.in.com/mails/inbox.php?nomail=....) which I am scraped. But I scrap the details without changing my url.
",73k,"
            20
        ","[""\nDefinitely go with PHP Simple HTML DOM Parser. It's fast, easy and super flexible. It basically sticks an entire HTML page in an object then you can access any element from that object.\nLike the example of the official site, to get all links on the main Google page:\n// Create DOM from URL or file\n$html = file_get_html('http://www.google.com/');\n\n// Find all images \nforeach($html->find('img') as $element) \n       echo $element->src . '<br>';\n\n// Find all links \nforeach($html->find('a') as $element) \n       echo $element->href . '<br>';\n\n"", '\nThe HTTP Request\nFirst, you make an HTTP request to get the content of the page. There are several ways to do that.\nfopen\nThe most basic way to send an HTTP request, is to use fopen. A main advantage is that you can set how many characters are read at a time, which can be useful when reading very large files. It\'s not the easiest thing to do correctly, though, and it\'s not recommended to do this unless you\'re reading very large files and fear running into memory issues.\n$fp = fopen(""http://www.4wtech.com/csp/web/Employee/Login.csp"", ""rb"");\nif (FALSE === $fp) {\n    exit(""Failed to open stream to URL"");\n}\n\n$result = \'\';\n\nwhile (!feof($fp)) {\n    $result .= fread($fp, 8192);\n}\nfclose($fp);\necho $result;\n\nfile_get_contents\nThe easiest way, is just using file_get_contents. If does more or less the same as fopen, but you have less options to choose from. A main advantage here is that it requires but one line of code.\n$result = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\');\necho $result;\n\nsockets\nIf you need more control of what headers are sent to the server, you can use sockets, in combination with fopen.\n$fp = fsockopen(""www.4wtech.com/csp/web/Employee/Login.csp"", 80, $errno, $errstr, 30);\nif (!$fp) {\n    $result = ""$errstr ($errno)<br />\\n"";\n} else {\n    $result = \'\';\n    $out = ""GET / HTTP/1.1\\r\\n"";\n    $out .= ""Host: www.4wtech.com/csp/web/Employee/Login.csp\\r\\n"";\n    $out .= ""Connection: Close\\r\\n\\r\\n"";\n    fwrite($fp, $out);\n    while (!feof($fp)) {\n        $result .= fgets($fp, 128);\n    }\n    fclose($fp);\n}\necho $result;\n\nstreams\nAlternatively, you can also use streams. Streams are similar to sockets and can be used in combination with both fopen and file_get_contents.\n$opts = array(\n  \'http\'=>array(\n    \'method\'=>""GET"",\n    \'header\'=>""Accept-language: en\\r\\n"" .\n              ""Cookie: foo=bar\\r\\n""\n  )\n);\n\n$context = stream_context_create($opts);\n\n$result = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\', false, $context);\necho result;\n\ncURL\nIf your server supports cURL (it usually does), it is recommended to use cURL. A key advantage of using cURL, is that it relies on a popular C library commonly used in other programming languages. It also provides a convenient way for creating request headers, and auto-parses response headers, with a simple interface in case of errors.\n$defaults = array( \n    CURLOPT_URL, ""http://www.4wtech.com/csp/web/Employee/Login.csp""\n    CURLOPT_HEADER=> 0\n);\n\n$ch = curl_init(); \ncurl_setopt_array($ch, ($options + $defaults)); \nif( ! $result = curl_exec($ch)) { \n    trigger_error(curl_error($ch)); \n} \ncurl_close($ch); \necho $result; \n\nLibraries\nAlternatively, you can use one of many PHP libraries. I wouldn\'t recommend using a library, though, as it\'s likely to be overkill. In most cases, you\'re better off writing your own HTTP class using cURL under the hood.\n\nThe HTML parsing\nPHP has a convenient way to load any HTML into a DOMDocument.\n$pagecontent = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\');\n$doc = new DOMDocument();\n$doc->loadHTML($pagecontent);\necho $doc->saveHTML();\n\nUnfortunately, PHP support for HTML5 is limited. If you run into errors trying to parse your page content, consider using a third party library. For that, I can recommend Masterminds/html5-php. Parsing an HTML file with this library is very similar to parsing an HTML file with DOMDocument.\nuse Masterminds\\HTML5;\n\n$pagecontent = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\');\n$html5 = new HTML5();\n$dom = $html5->loadHTML($html);\necho $html5->saveHTML($dom);\n\nAlternatively, you can use eg. my library PHPPowertools/DOM-Query. It uses customized version of Masterminds/html5-php under the hood parsing an HTML5 string into a DomDocument and symfony/DomCrawler for conversion of CSS selectors to XPath selectors. It always uses the same DomDocument, even when passing one object to another, to ensure decent performance.\nnamespace PowerTools;\n\n// Get file content\n$pagecontent = file_get_contents( \'http://www.4wtech.com/csp/web/Employee/Login.csp\' );\n\n// Define your DOMCrawler based on file string\n$H = new DOM_Query( $pagecontent );\n\n// Define your DOMCrawler based on an existing DOM_Query instance\n$H = new DOM_Query( $H->select(\'body\') );\n\n// Passing a string (CSS selector)\n$s = $H->select( \'div.foo\' );\n\n// Passing an element object (DOM Element)\n$s = $H->select( $documentBody );\n\n// Passing a DOM Query object\n$s = $H->select( $H->select(\'p + p\') );\n\n// Select the body tag\n$body = $H->select(\'body\');\n\n// Combine different classes as one selector to get all site blocks\n$siteblocks = $body->select(\'.site-header, .masthead, .site-body, .site-footer\');\n\n// Nest your methods just like you would with jQuery\n$siteblocks->select(\'button\')->add(\'span\')->addClass(\'icon icon-printer\');\n\n// Use a lambda function to set the text of all site blocks\n$siteblocks->text(function( $i, $val) {\n    return $i . "" - "" . $val->attr(\'class\');\n});\n\n// Append the following HTML to all site blocks\n$siteblocks->append(\'<div class=""site-center""></div>\');\n\n// Use a descendant selector to select the site\'s footer\n$sitefooter = $body->select(\'.site-footer > .site-center\');\n\n// Set some attributes for the site\'s footer\n$sitefooter->attr(array(\'id\' => \'aweeesome\', \'data-val\' => \'see\'));\n\n// Use a lambda function to set the attributes of all site blocks\n$siteblocks->attr(\'data-val\', function( $i, $val) {\n    return $i . "" - "" . $val->attr(\'class\') . "" - photo by Kelly Clark"";\n});\n\n// Select the parent of the site\'s footer\n$sitefooterparent = $sitefooter->parent();\n\n// Remove the class of all i-tags within the site\'s footer\'s parent\n$sitefooterparent->select(\'i\')->removeAttr(\'class\');\n\n// Wrap the site\'s footer within two nex selectors\n$sitefooter->wrap(\'<section><div class=""footer-wrapper""></div></section>\');\n\n', '\nYou can use the cURL extension of PHP to do HTTP requests to another web site from within your PHP page script. See the documentation here.\nOf course the downside here is that your site will respond slowly because you will have to scrape the external web site before you can present the full page/output to your user.\n', ""\nHave you tried OutWit Hub? It's a whole scraping environment. You can let it try to guess the structure or develop your own scrapers. I really suggest you have a look at it. It made my life much simpler.\nZR\n""]",https://stackoverflow.com/questions/584826/scrape-web-page-contents,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I catch and process the data from the XHR responses using casperjs?,"
The data on the webpage is displayed dynamically and it seems that checking for every change in the html and extracting the data is a very daunting task and also needs me to use very unreliable XPaths. So I would want to be able to extract the data from the XHR packets. 
I hope to be able to extract information from XHR packets as well as generate 'XHR' packets to be sent to the server. 
The extracting information part is more important for me because the sending of information can be handled easily by automatically triggering html elements using casperjs.
I'm attaching a screenshot of what I mean.
The text in the response tab is the data I need to process afterwards. (This XHR response has been received from the server.)
",15k,"
            12
        ","['\nThis is not easily possible, because the resource.received event handler only provides meta data like url, headers or status, but not the actual data. The underlying phantomjs event handler acts the same way.\n\nStateless AJAX Request\nIf the ajax call is stateless, you may repeat the request\ncasper.on(""resource.received"", function(resource){\n    // somehow identify this request, here: if it contains "".json""\n    // it also also only does something when the stage is ""end"" otherwise this would be executed two times\n    if (resource.url.indexOf("".json"") != -1 && resource.stage == ""end"") {\n        var data = casper.evaluate(function(url){\n            // synchronous GET request\n            return __utils__.sendAJAX(url, ""GET"");\n        }, resource.url);\n        // do something with data, you might need to JSON.parse(data)\n    }\n});\ncasper.start(url); // your script\n\nYou may want to add the event listener to resource.requested. That way you don\'t need to way for the call to complete.\nYou can also do this right inside of the control flow like this (source: A: CasperJS waitForResource: how to get the resource i\'ve waited for):\ncasper.start(url);\n\nvar res, resData;\ncasper.waitForResource(function check(resource){\n    res = resource;\n    return resource.url.indexOf("".json"") != -1;\n}, function then(){\n    resData = casper.evaluate(function(url){\n        // synchronous GET request\n        return __utils__.sendAJAX(url, ""GET"");\n    }, res.url);\n    // do something with the data here or in a later step\n});\n\ncasper.run();\n\n\nStateful AJAX Request\nIf it is not stateless, you would need to replace the implementation of XMLHttpRequest. You will need to inject your own implementation of the onreadystatechange handler, collect the information in the page window object and later collect it in another evaluate call.\nYou may want to look at the XHR faker in sinon.js or use the following complete proxy for XMLHttpRequest (I modeled it after method 3 from How can I create a XMLHttpRequest wrapper/proxy?):\nfunction replaceXHR(){\n    (function(window, debug){\n        function args(a){\n            var s = """";\n            for(var i = 0; i < a.length; i++) {\n                s += ""\\t\\n["" + i + ""] => "" + a[i];\n            }\n            return s;\n        }\n        var _XMLHttpRequest = window.XMLHttpRequest;\n\n        window.XMLHttpRequest = function() {\n            this.xhr = new _XMLHttpRequest();\n        }\n\n        // proxy ALL methods/properties\n        var methods = [ \n            ""open"", \n            ""abort"", \n            ""setRequestHeader"", \n            ""send"", \n            ""addEventListener"", \n            ""removeEventListener"", \n            ""getResponseHeader"", \n            ""getAllResponseHeaders"", \n            ""dispatchEvent"", \n            ""overrideMimeType""\n        ];\n        methods.forEach(function(method){\n            window.XMLHttpRequest.prototype[method] = function() {\n                if (debug) console.log(""ARGUMENTS"", method, args(arguments));\n                if (method == ""open"") {\n                    this._url = arguments[1];\n                }\n                return this.xhr[method].apply(this.xhr, arguments);\n            }\n        });\n\n        // proxy change event handler\n        Object.defineProperty(window.XMLHttpRequest.prototype, ""onreadystatechange"", {\n            get: function(){\n                // this will probably never called\n                return this.xhr.onreadystatechange;\n            },\n            set: function(onreadystatechange){\n                var that = this.xhr;\n                var realThis = this;\n                that.onreadystatechange = function(){\n                    // request is fully loaded\n                    if (that.readyState == 4) {\n                        if (debug) console.log(""RESPONSE RECEIVED:"", typeof that.responseText == ""string"" ? that.responseText.length : ""none"");\n                        // there is a response and filter execution based on url\n                        if (that.responseText && realThis._url.indexOf(""whatever"") != -1) {\n                            window.myAwesomeResponse = that.responseText;\n                        }\n                    }\n                    onreadystatechange.call(that);\n                };\n            }\n        });\n\n        var otherscalars = [\n            ""onabort"",\n            ""onerror"",\n            ""onload"",\n            ""onloadstart"",\n            ""onloadend"",\n            ""onprogress"",\n            ""readyState"",\n            ""responseText"",\n            ""responseType"",\n            ""responseXML"",\n            ""status"",\n            ""statusText"",\n            ""upload"",\n            ""withCredentials"",\n            ""DONE"",\n            ""UNSENT"",\n            ""HEADERS_RECEIVED"",\n            ""LOADING"",\n            ""OPENED""\n        ];\n        otherscalars.forEach(function(scalar){\n            Object.defineProperty(window.XMLHttpRequest.prototype, scalar, {\n                get: function(){\n                    return this.xhr[scalar];\n                },\n                set: function(obj){\n                    this.xhr[scalar] = obj;\n                }\n            });\n        });\n    })(window, false);\n}\n\nIf you want to capture the AJAX calls from the very beginning, you need to add this to one of the first event handlers\ncasper.on(""page.initialized"", function(resource){\n    this.evaluate(replaceXHR);\n});\n\nor evaluate(replaceXHR) when you need it.\nThe control flow would look like this:\nfunction replaceXHR(){ /* from above*/ }\n\ncasper.start(yourUrl, function(){\n    this.evaluate(replaceXHR);\n});\n\nfunction getAwesomeResponse(){\n    return this.evaluate(function(){\n        return window.myAwesomeResponse;\n    });\n}\n\n// stops waiting if window.myAwesomeResponse is something that evaluates to true\ncasper.waitFor(getAwesomeResponse, function then(){\n    var data = JSON.parse(getAwesomeResponse());\n    // Do something with data\n});\n\ncasper.run();\n\nAs described above, I create a proxy for XMLHttpRequest so that every time it is used on the page, I can do something with it. The page that you scrape uses the xhr.onreadystatechange callback to receive data. The proxying is done by defining a specific setter function which writes the received data to window.myAwesomeResponse in the page context. The only thing you need to do is retrieving this text.\n\nJSONP Request\nWriting a proxy for JSONP is even easier, if you know the prefix (the function to call with the loaded JSON e.g. insert({""data"":[""Some"", ""JSON"", ""here""],""id"":""asdasda"")). You can overwrite insert in the page context\n\nafter the page is loaded\ncasper.start(url).then(function(){\n    this.evaluate(function(){\n        var oldInsert = insert;\n        insert = function(json){\n            window.myAwesomeResponse = json;\n            oldInsert.apply(window, arguments);\n        };\n    });\n}).waitFor(getAwesomeResponse, function then(){\n    var data = JSON.parse(getAwesomeResponse());\n    // Do something with data\n}).run();\n\nor before the request is received (if the function is registered just before the request is invoked)\ncasper.on(""resource.requested"", function(resource){\n    // filter on the correct call\n    if (resource.url.indexOf("".jsonp"") != -1) {\n        this.evaluate(function(){\n            var oldInsert = insert;\n            insert = function(json){\n                window.myAwesomeResponse = json;\n                oldInsert.apply(window, arguments);\n            };\n        });\n    }\n}).run();\n\ncasper.start(url).waitFor(getAwesomeResponse, function then(){\n    var data = JSON.parse(getAwesomeResponse());\n    // Do something with data\n}).run();\n\n\n', '\nI may be late into the party, but the answer may help someone like me who would fall into this problem later in future.\nI had to start with PhantomJS, then moved to CasperJS but finally settled with SlimerJS. Slimer is based on Phantom, is compatible with Casper, and can send you back the response body using the same onResponseReceived method, in ""response.body"" part.\nReference: https://docs.slimerjs.org/current/api/webpage.html#webpage-onresourcereceived\n', '\n@Artjom\'s answer\'s doesn\'t work for me in the recent Chrome and CasperJS versions.\nBased on @Artjom\'s answer and based on gilly3\'s answer on how to replace XMLHttpRequest, I have composed a new solution that should work in most/all versions of the different browsers. Works for me.\nSlimerJS cannot work on newer version of FireFox, therefore no good for me.\nHere is the the generic code to add a listner to load of XHR (not dependent on CasperJS):\nvar addXHRListener = function (XHROnStateChange) {\n\n    var XHROnLoad = function () {\n        if (this.readyState == 4) {\n            XHROnStateChange(this)\n        }\n    }\n\n    var open_original = XMLHttpRequest.prototype.open;\n\n    XMLHttpRequest.prototype.open = function (method, url, async, unk1, unk2) {\n        this.requestUrl = url\n        open_original.apply(this, arguments);\n    };\n\n    var xhrSend = XMLHttpRequest.prototype.send;\n    XMLHttpRequest.prototype.send = function () {\n\n        var xhr = this;\n        if (xhr.addEventListener) {\n            xhr.removeEventListener(""readystatechange"", XHROnLoad);\n            xhr.addEventListener(""readystatechange"", XHROnLoad, false);\n        } else {\n            function readyStateChange() {\n                if (handler) {\n                    if (handler.handleEvent) {\n                        handler.handleEvent.apply(xhr, arguments);\n                    } else {\n                        handler.apply(xhr, arguments);\n                    }\n                }\n                XHROnLoad.apply(xhr, arguments);\n                setReadyStateChange();\n            }\n\n            function setReadyStateChange() {\n                setTimeout(function () {\n                    if (xhr.onreadystatechange != readyStateChange) {\n                        handler = xhr.onreadystatechange;\n                        xhr.onreadystatechange = readyStateChange;\n                    }\n                }, 1);\n            }\n\n            var handler;\n            setReadyStateChange();\n        }\n        xhrSend.apply(xhr, arguments);\n    };\n\n}\n\nHere is CasperJS code to emit a custom event on load of XHR:\ncasper.on(""page.initialized"", function (resource) {\n    var emitXHRLoad = function (xhr) {\n        window.callPhantom({eventName: \'xhr.load\', eventData: xhr})\n    }\n    this.evaluate(addXHRListener, emitXHRLoad);\n});\n\ncasper.on(\'remote.callback\', function (data) {\n    casper.emit(data.eventName, data.eventData)\n});\n\nHere is a code to listen to ""xhr.load"" event and get the XHR response body:\ncasper.on(\'xhr.load\', function (xhr) {\n    console.log(\'xhr load\', xhr.requestUrl)\n    console.log(\'xhr load\', xhr.responseText)\n});\n\n', '\nAdditionally, you can also directly download the content and manipulate it later. \nHere is the example of the script I am using to retrieve a JSON and save it locally :\n\n\nvar casper = require(\'casper\').create({\r\n    pageSettings: {\r\n        webSecurityEnabled: false\r\n    }\r\n});\r\n\r\nvar url = \'https://twitter.com/users/username_available?username=whatever\';\r\n\r\ncasper.start(\'about:blank\', function() {\r\n   this.download(url, ""hop.json"");\r\n});\r\n\r\ncasper.run(function() {\r\n    this.echo(\'Done.\').exit();\r\n});\n\n\n']",https://stackoverflow.com/questions/24555370/how-can-i-catch-and-process-the-data-from-the-xhr-responses-using-casperjs,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrape website with dynamic mouseover event,"
I am trying to scrape data which is generated dynamically from mouseover events. 
I want to capture the information from the Hash Rate Distribution chart from
https://slushpool.com/stats/?c=btc which is generated when you scroll over each circle.  
The code below gets the html data from the website, and returns the table which is filled once the mouse passes over a circle. However, I have not been able to figure out how to trigger the mouseover event for each circle to fill the table.
from lxml import etree
from xml.etree import ElementTree
from selenium import webdriver

driver_path = ""#Firefox web driver""
browser = webdriver.Firefox(executable_path=driver_path)
browser.get(""https://slushpool.com/stats/?c=btc"") 


page = browser.page_source #Get page html 
tree = etree.HTML(page) #create etree

table_Xpath = '/html/body/div[1]/div/div/div/div/div[5]/div[1]/div/div/div[2]/div[2]/div[2]/div/table'

table =tree.xpath(table_Xpath) #get table using Xpath

print(ElementTree.tostring(table[0])) #Returns empty table. 
#Should return data from each mouseover event

Is there a way to trigger the mouseover event for each circle, then extract the generated data.
Thank you in advance for the help!
",3k,"
            3
        ","['\nTo trigger the mouseover event for each circle you have to induce WebDriverWait for the visibility_of_all_elements_located() and you can use the following Locator Strategies:\n\nCode Block:\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.action_chains import ActionChains\n\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument(""start-maximized"")\nchrome_options.add_experimental_option(""excludeSwitches"", [""enable-automation""])\nchrome_options.add_experimental_option(\'useAutomationExtension\', False)\ndriver = webdriver.Chrome(options=chrome_options, executable_path=r\'C:\\Utility\\BrowserDrivers\\chromedriver.exe\')\ndriver.get(""https://slushpool.com/stats/?c=btc"")\ndriver.execute_script(""return arguments[0].scrollIntoView(true);"", WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, ""//h1//span[text()=\'Distribution\']""))))\nelements = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.XPATH, ""//h1//span[text()=\'Distribution\']//following::div[1]/*[name()=\'svg\']//*[name()=\'g\']//*[name()=\'g\' and @class=\'paper\']//*[name()=\'circle\']"")))\nfor element in elements:\n    ActionChains(driver).move_to_element(element).perform()\n\nBrowser Snapshot:\n\n\n', '\nThis is the circle locator you mean:\n.find_element_by_css_selector(\'._1p0PmxVw._3GzjmWLG\')\n\nBut it will change because mouseover effect, to be:\n.find_element_by_css_selector(\'._1p0PmxVw._3GzjmWLG._1suU9Mx1\')\n\n\nSo you need wait until the element to changed for each move.\nAnd the most important is how to inspect a hover element, then you can get the bellow:\n\nAnd causes the element for get data you mean to be appear:\nxpath: //div[@class=""_3jGHi0co _1zbokARu"" and contains(@style,""display: block"")]\n\nYou can use ActionChains to perform move the element.\nFinally you can try the bellow code:\nbrowser.get(\'https://slushpool.com/stats/?c=btc\')\nbrowser.maximize_window()\n\n#wait all circle\nelements = WebDriverWait(browser, 20).until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, \'._1p0PmxVw._3GzjmWLG\')))\ntable = browser.find_element_by_class_name(\'paper\')\n\n#move perform -> to table\nbrowser.execute_script(""arguments[0].scrollIntoView(true);"", table)\n\ndata = []\nfor circle in elements:\n    #move perform -> to each circle\n    ActionChains(browser).move_to_element(circle).perform()\n    # wait change mouseover effect\n    mouseover = WebDriverWait(browser, 5).until(EC.visibility_of_element_located((By.XPATH, \'//div[@class=""_3jGHi0co _1zbokARu"" and contains(@style,""display: block"")]\')))\n    data.append(mouseover.text)\n\nprint(data[0])\nprint(data)\n\nFollowing import:\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver import ActionChains\n\nConsole output:\n\nFirst data > data[0]\n536.9 Ph/s - 1.074 Eh/s\nUser Count 2\nAverage Hash Rate 546.1 Ph/s\nGroup Hash Rate 1.092 Eh/s\nAll data > data\n\n[u\'536.9 Ph/s - 1.074 Eh/s\\nUser Count 2\\nAverage Hash Rate 546.9 Ph/s\\nGroup Hash Rate 1.094 Eh/s\', u\'67.11 Ph/s - 134.2 Ph/s\\nUser Count 14\\nAverage Hash Rate 91.27 Ph/s\\nGroup Hash Rate 1.278 Eh/s\', u\'67.11 Ph/s - 134.2 Ph/s\\nUser Count 14\\nAverage Hash Rate 91.27 Ph/s\\nGroup Hash Rate 1.278 Eh/s\', u\'16.78 Ph/s - 33.55 Ph/s\\nUser Count 23\\nAverage Hash Rate 23.36 Ph/s\\nGroup Hash Rate 537.2 Ph/s\', u\'8.389 Ph/s - 16.78 Ph/s\\nUser Count 33\\nAverage Hash Rate 11.80 Ph/s\\nGroup Hash Rate 389.4 Ph/s\', u\'4.194 Ph/s - 8.389 Ph/s\\nUser Count 67\\nAverage Hash Rate 5.704 Ph/s\\nGroup Hash Rate 382.2 Ph/s\', u\'2.097 Ph/s - 4.194 Ph/s\\nUser Count 137\\nAverage Hash Rate 2.959 Ph/s\\nGroup Hash Rate 405.3 Ph/s\', u\'1.049 Ph/s - 2.097 Ph/s\\nUser Count 233\\nAverage Hash Rate 1.475 Ph/s\\nGroup Hash Rate 343.7 Ph/s\', u\'1.049 Ph/s - 2.097 Ph/s\\nUser Count 233\\nAverage Hash Rate 1.475 Ph/s\\nGroup Hash Rate 343.7 Ph/s\', u\'524.3 Th/s - 1.049 Ph/s\\nUser Count 397\\nAverage Hash Rate 731.4 Th/s\\nGroup Hash Rate 290.4 Ph/s\', u\'262.1 Th/s - 524.3 Th/s\\nUser Count 745\\nAverage Hash Rate 360.3 Th/s\\nGroup Hash Rate 268.4 Ph/s\', u\'131.1 Th/s - 262.1 Th/s\\nUser Count 1479\\nAverage Hash Rate 182.7 Th/s\\nGroup Hash Rate 270.1 Ph/s\', u\'65.54 Th/s - 131.1 Th/s\\nUser Count 2351\\nAverage Hash Rate 92.47 Th/s\\nGroup Hash Rate 217.4 Ph/s\', u\'32.77 Th/s - 65.54 Th/s\\nUser Count 3107\\nAverage Hash Rate 47.23 Th/s\\nGroup Hash Rate 146.8 Ph/s\', u\'16.38 Th/s - 32.77 Th/s\\nUser Count 3380\\nAverage Hash Rate 25.24 Th/s\\nGroup Hash Rate 85.30 Ph/s\', u\'8.192 Th/s - 16.38 Th/s\\nUser Count 4276\\nAverage Hash Rate 13.00 Th/s\\nGroup Hash Rate 55.57 Ph/s\', u\'4.096 Th/s - 8.192 Th/s\\nUser Count 540\\nAverage Hash Rate 5.953 Th/s\\nGroup Hash Rate 3.215 Ph/s\', u\'2.048 Th/s - 4.096 Th/s\\nUser Count 284\\nAverage Hash Rate 3.193 Th/s\\nGroup Hash Rate 906.8 Th/s\', u\'1.024 Th/s - 2.048 Th/s\\nUser Count 226\\nAverage Hash Rate 1.368 Th/s\\nGroup Hash Rate 309.1 Th/s\', u\'512.0 Gh/s - 1.024 Th/s\\nUser Count 136\\nAverage Hash Rate 774.4 Gh/s\\nGroup Hash Rate 105.3 Th/s\', u\'256.0 Gh/s - 512.0 Gh/s\\nUser Count 116\\nAverage Hash Rate 401.5 Gh/s\\nGroup Hash Rate 46.57 Th/s\', u\'128.0 Gh/s - 256.0 Gh/s\\nUser Count 75\\nAverage Hash Rate 186.4 Gh/s\\nGroup Hash Rate 13.98 Th/s\', u\'64.00 Gh/s - 128.0 Gh/s\\nUser Count 78\\nAverage Hash Rate 96.39 Gh/s\\nGroup Hash Rate 7.518 Th/s\', u\'32.00 Gh/s - 64.00 Gh/s\\nUser Count 70\\nAverage Hash Rate 45.68 Gh/s\\nGroup Hash Rate 3.198 Th/s\', u\'16.00 Gh/s - 32.00 Gh/s\\nUser Count 48\\nAverage Hash Rate 23.37 Gh/s\\nGroup Hash Rate 1.122 Th/s\', u\'8.000 Gh/s - 16.00 Gh/s\\nUser Count 62\\nAverage Hash Rate 11.91 Gh/s\\nGroup Hash Rate 738.5 Gh/s\', u\'4.000 Gh/s - 8.000 Gh/s\\nUser Count 153\\nAverage Hash Rate 3.078 Gh/s\\nGroup Hash Rate 471.0 Gh/s\']\n\n']",https://stackoverflow.com/questions/57901045/scrape-website-with-dynamic-mouseover-event,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Why does headless need to be false for Puppeteer to work?,"
I'm creating a web api that scrapes a given url and sends that back. I am using Puppeteer to do this. I asked this question: Puppeteer not behaving like in Developer Console
and recieved an answer that suggested it would only work if headless was set to be false. I don't want to be constantly opening up a browser UI i don't need (I just the need the data!) so I'm looking for why headless has to be false and can I get a fix that lets headless = true.
Here's my code:
express()
  .get(""/*"", (req, res) => {
    global.notBaseURL = req.params[0];
    (async () => {
      const browser = await puppet.launch({ headless: false }); // Line of Interest
      const page = await browser.newPage();
      console.log(req.params[0]);
      await page.goto(req.params[0], { waitUntil: ""networkidle2"" }); //this is the url
      title = await page.$eval(""title"", (el) => el.innerText);

      browser.close();

      res.send({
        title: title,
      });
    })();
  })
  .listen(PORT, () => console.log(`Listening on ${PORT}`));

This is the page I'm trying to scrape: https://www.nordstrom.com/s/zella-high-waist-studio-pocket-7-8-leggings/5460106?origin=coordinating-5460106-0-1-FTR-recbot-recently_viewed_snowplow_mvp&recs_placement=FTR&recs_strategy=recently_viewed_snowplow_mvp&recs_source=recbot&recs_page_type=category&recs_seed=0&color=BLACK
",3k,"
            2
        ","['\nThe reason it might work in UI mode but not headless is that sites who aggressively fight scraping will detect that you are running in a headless browser.\nSome possible workarounds:\nUse puppeteer-extra\nFound here: https://github.com/berstend/puppeteer-extra\nCheck out their docs for how to use it. It has a couple plugins that might help in getting past headless-mode detection:\n\npuppeteer-extra-plugin-anonymize-ua -- anonymizes your User Agent. Note that this might help with getting past headless mode detection, but as you\'ll see if you visit https://amiunique.org/ it is unlikely to be enough to keep you from being identified as a repeat visitor.\npuppeteer-extra-plugin-stealth -- this might help win the cat-and-mouse game of not being detected as headless. There are many tricks that are employed to detect headless mode, and as many tricks to evade them.\n\nRun a ""real"" Chromium instance/UI\nIt\'s possible to run a single browser UI in a manner that let\'s you attach puppeteer to that running instance. Here\'s an article that explains it: https://medium.com/@jaredpotter1/connecting-puppeteer-to-existing-chrome-window-8a10828149e0\nEssentially you\'re starting Chrome or Chromium (or Edge?) from the command line with --remote-debugging-port=9222 (or any old port?) plus other command line switches depending on what environment you\'re running it in. Then you use puppeteer to connect to that running instance instead of having it do the default behavior of launching a headless Chromium instance: const browser = await puppeteer.connect({ browserURL: ENDPOINT_URL });.  Read the puppeteer docs here for more info: https://pptr.dev/#?product=Puppeteer&version=v5.2.1&show=api-puppeteerlaunchoptions\nThe ENDPOINT_URL is displayed in the terminal when you launch the browser from the command line with the --remote-debugging-port=9222 option.\nThis option is going to require some server/ops mojo, so be prepared to do a lot more Stack Overflow searches. :-)\nThere are other strategies I\'m sure but those are the two I\'m most familiar with. Good luck!\n', '\nTodd\'s answer is thorough, but worth trying before resorting to some of the recommendations there is to slap on the following user agent line pulled from the relevant Puppeteer GitHub issue Different behavior between { headless: false } and { headless: true }:\nawait page.setUserAgent(""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36"");\nawait page.goto(yourURL);\n\nNow, the Nordstorm site provided by OP seems to be able to detect robots even with headless: false, at least at the present moment. But other sites are less strict and I\'ve found the above line to be useful on some of them as shown in Puppeteer can\'t find elements when Headless TRUE and Puppeteer , bringing back blank array.\nVisit the GH issue thread above for other ideas and see useragents.me for a rotating list of current user agents.\n']",https://stackoverflow.com/questions/63818869/why-does-headless-need-to-be-false-for-puppeteer-to-work,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"How to import a table from web page (with ""div class"") to excel?","
I'm trying to import to Excel a list of exhibitors and countries from this webpage and I'm not getting it.
Can Someone help me?
I have tried the methods listed in this forum and doesn't work.
Sub test()

    Dim objIE As Object
    Dim hmtl As HTMLDocument

    Dim elements As IHTMLElementCollection

    Set objIE = New InternetExplorer
    objIE.Visible = True

    objIE.navigate ""https://sps.mesago.com/events/en/exhibitors_products/exhibitor-list.html""

    Application.StatusBar = ""Loading, Please wait...""

    While objIE.Busy
        DoEvents
    Wend
    Do
    Loop Until objIE.readyState = READYSTATE_COMPLETE

    Application.StatusBar = ""Importing data...""

    Set html = objIE.document

    'I try differents types and name - ByClassName(""...""), ByTagName(""...""), ...
    Set elements = html.getElementsByClassName(""list"") 

    For i = 0 To elements.Length - 1
         Sheet1.Range(""A"" & (i + 1)) = elements(i).innerText
    Next i

    objIE.Quit
    Set objIE = Nothing

    Application.StatusBar = """"

End Sub

Sorry about my English.
",3k,"
            0
        ","['\nYou don\'t need a browser to be opened. You can do this with XHR. The url I am using can be found in the network tab via F12 (Dev tools)\nIf you search that tab after making your request you will find that url and the response has a layout such as:\n\nimage link: https://i.stack.imgur.com/C8oLj.png\nI loop the rows and the columns to populate a 2d array (table like format) which I write out to the sheet in one go at end.\n\nVBA:\nOption Explicit\nPublic Sub GetExhibitorsInfo()\n    Dim ws As Worksheet, results(), i As Long, html As HTMLDocument\n\n    Set ws = ThisWorkbook.Worksheets(""Sheet1"")\n    Set html = New HTMLDocument\n\n    With CreateObject(""MSXML2.XMLHTTP"")\n        .Open ""GET"", ""https://sps.mesago.com/events/en/exhibitors_products/exhibitor-list.html"", False\n        .setRequestHeader ""User-Agent"", ""Mozilla/5.0""\n        .send\n        html.body.innerHTML = .responseText\n    End With\n\n    Dim rows As Object, html2 As HTMLDocument, columnsInfo As Object\n    Dim r As Long, c As Long, j As Long, headers(), columnCount As Long\n\n    headers = Array(""name2_kat"", ""art"", ""std_nr_sort"", ""kfzkz_kat"", ""halle"", _\n    ""sortierung_katalog"", ""std_nr"", ""ort_info_kat"", ""name3_kat"", ""webseite"", _\n    ""land_kat"", ""standbez1"", ""name1_kat"")\n    Set rows = html.querySelectorAll(""[data-entry]"")\n    Set html2 = New HTMLDocument\n    html2.body.innerHTML = rows.item(0).innerHTML\n    columnCount = html2.querySelectorAll(""[data-entry-key]"").length\n\n    ReDim results(1 To rows.length, 1 To columnCount)\n\n    For i = 0 To rows.length - 1\n        r = r + 1: c = 1\n        html2.body.innerHTML = rows.item(i).innerHTML\n        Set columnsInfo = html2.querySelectorAll(""[data-entry-key]"")\n        For j = 0 To columnsInfo.length - 1\n            results(r, c) = columnsInfo.item(j).innerText \'columnsInfo.item(j).getAttribute(""data-entry-key"")\n            c = c + 1\n        Next\n    Next\n    With ws\n        .Cells(1, 1).Resize(1, columnCount) = headers\n        .Cells(2, 1).Resize(UBound(results, 1), UBound(results, 2)) = results\n    End With\nEnd Sub\n\n']",https://stackoverflow.com/questions/56277464/how-to-import-a-table-from-web-page-with-div-class-to-excel,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How would I import YouTube Likes and Dislikes and a ratio from YouTube onto Google Sheets? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 3 years ago.


This post was edited and submitted for review 5 months ago and failed to reopen the post:

Original close reason(s) were not resolved






                        Improve this question
                    



What would be the correct xpath to get  YouTube likes and dislikes from a video?
",3k,"
            -3
        ","['\nTITLE:\n=IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""//*[@id=\'eow-title\']"")\n\nor:\n=REGEXEXTRACT(QUERY(ARRAY_CONSTRAIN(IMPORTDATA(A12), 500, 1),\n ""where Col1 contains \'/title\'"", 0), "">(.+)<"")\n\n\nVIEWS:\n=VALUE(REGEXREPLACE(TEXT(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",\n ""//*[contains(@class, \'watch-view-count\')]""),0),"" view(s)?"",""""))\n\nDURATION:\n=SUBSTITUTE(REGEXREPLACE(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""//*[@itemprop=\'duration\']/@content""),""PT|S"",""""),""M"","":"")\n\nLIKES:\n=IF(ISNA(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-like-button\')])[1]""))=TRUE,0,\n         IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-like-button\')])[1]""))\n\nDISLIKES:\n=IF(ISNA(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-dislike-button\')])[1]""))=TRUE,0,\n         IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-dislike-button\')])[1]""))\n\nUPLOADED:\n=REGEXREPLACE(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",\n ""//*[contains(@class, \'watch-time-text\')]""),""((Uploaded)|(Published)|(Streamed live)) on "","""")\n\nSUBSCRIPTIONS:\n=IFERROR(MID(QUERY(IMPORTXML(""https://www.youtube.com/channel/""&A1,\n ""//div[@class=\'primary-header-actions\']""), ""select Col1""), 31, 20), )\n\n\nCHANNEL NAME:\n=INDEX(IMPORTHTML(""https://www.youtube.com/channel/UC7_gcs09iThXybpVgjHZ_7g"",""list"",1),1,1)\n\nCHANNEL ID:\n=ARRAYFORMULA(REGEXREPLACE(QUERY(SUBSTITUTE(ARRAY_CONSTRAIN(\n IMPORTDATA(https://www.youtube.com/watch?v=rckrnYw5sOA), 3000, 1), """""""", """"),\n ""where Col1 contains \'<meta itemprop=channelId content=\'""),\n ""<meta itemprop=channelId content=|>"", """"))\n\n\n\n\nUPDATE:\nchannel name (07/07/2021):\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(A4)), \n ""where Col1 contains \'\\x22channelName\\x22:\\x22\'"", 0), "":\\\\x22(.+)\\\\x22$"")\n\nvideo title (08/08/2021)\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(A1)), \n ""where Col1 starts with \'title:""""\'"", 0), """"""(.*)"""""")\n\n\nduration (21/04/2022)\n=TEXT(1*REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(B1)), \n ""where Col1 contains \'approxDurationMs\' limit 1"", ), \n ""\\d+"")/3600000/24, ""mm:ss"")\n\n\nchannel views (07/06/2022)\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTXML(A1, ""//*"")), \n ""where Col1 contains \'""&CHAR(10)&""Creators\'"", ), \n "".x22text.x22:.x22(.+).x22,.x22bold.x22"")\n\n\nvideo views (01/09/2022)\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(A1)); \n ""where Col1 starts with \'viewCount\'""; ); ""\\d+"")*1\n\n\n']",https://stackoverflow.com/questions/55060363/how-would-i-import-youtube-likes-and-dislikes-and-a-ratio-from-youtube-onto-goog,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do I prevent site scraping? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



I have a fairly large music website with a large artist database.  I've been noticing other music sites scraping our site's data (I enter dummy Artist names here and there and then do google searches for them).  
How can I prevent screen scraping?  Is it even possible?
",131k,"
            329
        ","['\nNote: Since the complete version of this answer exceeds Stack Overflow\'s length limit, you\'ll need to head  to GitHub to read the extended version, with more tips and details.\n\nIn order to hinder scraping (also known as Webscraping, Screenscraping, Web data mining, Web harvesting, or Web data extraction), it helps to know how these scrapers work, and \n, by extension, what prevents them from working well.\nThere\'s various types of scraper, and each works differently:\n\nSpiders, such as Google\'s bot or website copiers like HTtrack, which recursively follow links to other pages in order to get data. These are sometimes used for targeted scraping to get specific data, often in combination with a HTML parser to extract the desired data from each page.\nShell scripts: Sometimes, common Unix tools are used for scraping: Wget or Curl to download pages, and Grep (Regex) to extract the data.\nHTML parsers, such as ones based on Jsoup, Scrapy, and others. Similar to shell-script regex based ones, these work by extracting data from pages based on patterns in HTML, usually ignoring everything else. \nFor example: If your website has a search feature, such a scraper might submit a request for a search, and then get all the result links and their titles from the results page HTML, in order to specifically get only search result links and their titles. These are the most common.\nScreenscrapers, based on eg. Selenium or PhantomJS, which open your website in a real browser, run JavaScript, AJAX, and so on, and then get the desired text from the webpage, usually by:\n\nGetting the HTML from the browser after your page has been loaded and JavaScript has run, and then using a HTML parser to extract the desired data. These are the most common, and so many of the methods for breaking HTML parsers / scrapers also work here.\nTaking a screenshot of the rendered pages, and then using OCR to extract the desired text from the screenshot. These are rare, and only dedicated scrapers who really want your data will set this up.\n\nWebscraping services such as ScrapingHub or Kimono. In fact, there\'s people whose job is to figure out how to scrape your site and pull out the content for others to use.\nUnsurprisingly, professional scraping services are the hardest to deter, but if you make it hard and time-consuming to figure out how to scrape your site, these (and people who pay them to do so) may not be bothered to scrape your website.\nEmbedding your website in other site\'s pages with frames, and embedding your site in mobile apps. \nWhile not technically scraping, mobile apps (Android and iOS) can embed websites, and inject custom CSS and JavaScript, thus completely changing the appearance of your pages.\nHuman copy - paste: People will copy and paste your content in order to use it elsewhere.\n\nThere is a lot overlap between these different kinds of scraper, and many scrapers will behave similarly, even if they use different technologies and methods.\nThese tips mostly my own ideas, various difficulties that I\'ve encountered while writing scrapers, as well as bits of information and ideas from around the interwebs. \nHow to stop scraping\nYou can\'t completely prevent it, since whatever you do, determined scrapers can still figure out how to scrape. However, you can stop a lot of scraping by doing a few things:\nMonitor your logs & traffic patterns; limit access if you see unusual activity:\nCheck your logs regularly, and in case of unusual activity indicative of automated access (scrapers), such as many similar actions from the same IP address, you can block or limit access.\nSpecifically, some ideas:\n\nRate limiting:\nOnly allow users (and scrapers) to perform a limited number of actions in a certain time - for example, only allow a few searches per second from any specific IP address or user. This will slow down scrapers, and make them ineffective. You could also show a captcha if actions are completed too fast or faster than a real user would.\nDetect unusual activity:\nIf you see unusual activity, such as many similar requests from a specific IP address, someone looking at an excessive number of pages or performing an unusual number of searches, you can prevent access, or show a captcha for subsequent requests.\nDon\'t just monitor & rate limit by IP address - use other indicators too:\nIf you do block or rate limit, don\'t just do it on a per-IP address basis; you can use other indicators and methods to identify specific users or scrapers. Some indicators which can help you identify specific users / scrapers include:\n\nHow fast users fill out forms, and where on a button they click;\nYou can gather a lot of information with JavaScript, such as screen size / resolution, timezone, installed fonts, etc; you can use this to identify users.\nHTTP headers and their order, especially User-Agent.\n\nAs an example, if you get many request from a single IP address, all using the same User Agent, screen size (determined with JavaScript), and the user (scraper in this case) always clicks on the button in the same way and at regular intervals, it\'s probably a screen scraper; and you can temporarily block similar requests (eg. block all requests with that user agent and screen size coming from that particular IP address), and this way you won\'t inconvenience real users on that IP address, eg. in case of a shared internet connection.\nYou can also take this further, as you can identify similar requests, even if they come from different IP addresses, indicative of distributed scraping (a scraper using a botnet or a network of proxies). If you get a lot of otherwise identical requests, but they come from different IP addresses, you can block. Again, be aware of not inadvertently blocking real users.\nThis can be effective against screenscrapers which run JavaScript, as you can get a lot of information from them.\nRelated questions on Security Stack Exchange:\n\nHow to uniquely identify users with the same external IP address? for more details, and \nWhy do people use IP address bans when IP addresses often change? for info on the limits of these methods.\n\nInstead of temporarily blocking access, use a Captcha:\nThe simple way to implement rate-limiting would be to temporarily block access for a certain amount of time, however using a Captcha may be better, see the section on Captchas further down.\n\nRequire registration & login\nRequire account creation in order to view your content, if this is feasible for your site. This is a good deterrent for scrapers, but is also a good deterrent for real users.\n\nIf you require account creation and login, you can accurately track user and scraper actions. This way, you can easily detect when a specific account is being used for scraping, and ban it. Things like rate limiting or detecting abuse (such as a huge number of searches in a short time) become easier, as you can identify specific scrapers instead of just IP addresses.\n\nIn order to avoid scripts creating many accounts, you should:\n\nRequire an email address for registration, and verify that email address by sending a link that must be opened in order to activate the account. Allow only one account per email address.\nRequire a captcha to be solved during registration / account creation.\n\nRequiring account creation to view content will drive users and search engines away; if you require account creation in order to view an article, users will go elsewhere.\nBlock access from cloud hosting and scraping service IP addresses\nSometimes, scrapers will be run from web hosting services, such as Amazon Web Services or GAE, or VPSes.  Limit access to your website (or show a captcha) for requests originating from the IP addresses used by such cloud hosting services.\nSimilarly, you can also limit access from IP addresses used by proxy or VPN providers, as scrapers may use such proxy servers to avoid many requests being detected.\nBeware that by blocking access from proxy servers and VPNs, you will negatively affect real users.\nMake your error message nondescript if you do block\nIf you do block / limit access, you should ensure that you don\'t tell the scraper what caused the block, thereby giving them clues as to how to fix their scraper. So a bad idea would be to show error pages with text like:\n\nToo many requests from your IP address, please try again later.\nError, User Agent header not present !\n\nInstead, show a friendly error message that doesn\'t tell the scraper what caused it. Something like this is much better:\n\nSorry, something went wrong. You can contact support via helpdesk@example.com, should the problem persist.\n\nThis is also a lot more user friendly for real users, should they ever see such an error page. You should also consider showing a captcha for subsequent requests instead of a hard block, in case a real user sees the error message, so that you don\'t block and thus cause legitimate users to contact you.\nUse Captchas if you suspect that your website is being accessed by a scraper.\nCaptchas (""Completely Automated Test to Tell Computers and Humans apart"") are very effective against stopping scrapers. Unfortunately, they are also very effective at irritating users. \nAs such, they are useful when you suspect a possible scraper, and want to stop the scraping, without also blocking access in case it isn\'t a scraper but a real user. You might want to consider showing a captcha before allowing access to the content if you suspect a scraper.\nThings to be aware of when using Captchas:\n\nDon\'t roll your own, use something like Google\'s reCaptcha : It\'s a lot easier than implementing a captcha yourself, it\'s more user-friendly than some blurry and warped text solution you might come up with yourself (users often only need to tick a box), and it\'s also a lot harder for a scripter to solve than a simple image served from your site\nDon\'t include the solution to the captcha in the HTML markup: I\'ve actually seen one website which had the solution for the captcha in the page itself, (although quite well hidden) thus making it pretty useless. Don\'t do something like this. Again, use a service like reCaptcha, and you won\'t have this kind of problem (if you use it properly).\nCaptchas can be solved in bulk: There are captcha-solving services where actual, low-paid, humans solve captchas in bulk. Again, using reCaptcha is a good idea here, as they have protections (such as the relatively short time the user has in order to solve the captcha). This kind of service is unlikely to be used unless your data is really valuable.\n\nServe your text content as an image\nYou can render text into an image server-side, and serve that to be displayed, which will hinder simple scrapers extracting text.\nHowever, this is bad for screen readers, search engines, performance, and pretty much everything else. It\'s also illegal in some places (due to accessibility, eg. the Americans with Disabilities Act), and it\'s also easy to circumvent with some OCR, so don\'t do it. \nYou can do something similar with CSS sprites, but that suffers from the same problems.\nDon\'t expose your complete dataset:\nIf feasible, don\'t provide a way for a script / bot to get all of your dataset. As an example: You have a news site, with lots of individual articles. You could make those articles be only accessible by searching for them via the on site search, and, if you don\'t have a list of all the articles on the site and their URLs anywhere, those articles will be only accessible by using the search feature. This means that a script wanting to get all the articles off your site will have to do searches for all possible phrases which may appear in your articles in order to find them all, which will be time-consuming, horribly inefficient, and will hopefully make the scraper give up.\nThis will be ineffective if:\n\nThe bot / script does not want / need the full dataset anyway.\nYour articles are served from a URL which looks something like example.com/article.php?articleId=12345. This (and similar things) which will allow scrapers to simply iterate over all the articleIds and request all the articles that way.\nThere are other ways to eventually find all the articles, such as by writing a script to follow links within articles which lead to other articles.\nSearching for something like ""and"" or ""the"" can reveal almost everything, so that is something to be aware of. (You can avoid this by only returning the top 10 or 20 results).\nYou need search engines to find your content.\n\nDon\'t expose your APIs, endpoints, and similar things:\nMake sure you don\'t expose any APIs, even unintentionally. For example, if you are using AJAX or network requests from within Adobe Flash or Java Applets (God forbid!) to load your data it is trivial to look at the network requests from the page and figure out where those requests are going to, and then reverse engineer and use those endpoints in a scraper program. Make sure you obfuscate your endpoints and make them hard for others to use, as described.\nTo deter HTML parsers and scrapers:\nSince HTML parsers work by extracting content from pages based on identifiable patterns in the HTML, we can intentionally change those patterns in oder to break these scrapers, or even screw with them. Most of these tips also apply to other scrapers like spiders and screenscrapers too.\nFrequently change your HTML\nScrapers which process HTML directly do so by extracting contents from specific, identifiable parts of your HTML page. For example: If all pages on your website have a div with an id of article-content, which contains the text of the article, then it is trivial to write a script to visit all the article pages on your site, and extract the content text of the article-content div on each article page, and voil脿, the scraper has all the articles from your site in a format that can be reused elsewhere.\nIf you change the HTML and the structure of your pages frequently, such scrapers will no longer work.\n\nYou can frequently change the id\'s and classes of elements in your HTML, perhaps even automatically. So, if your div.article-content becomes something like div.a4c36dda13eaf0, and changes every week, the scraper will work fine initially, but will break after a week. Make sure to change the length of your ids / classes too, otherwise the scraper will use div.[any-14-characters] to find the desired div instead. Beware of other similar holes too..\nIf there is no way to find the desired content from the markup, the scraper will do so from the way the HTML is structured. So, if all your article pages are similar in that every div inside a div which comes after a h1 is the article content, scrapers will get the article content based on that. Again, to break this, you can add / remove extra markup to your HTML, periodically and randomly, eg. adding extra divs or spans. With modern server side HTML processing, this should not be too hard.\n\nThings to be aware of:\n\nIt will be tedious and difficult to implement, maintain, and debug.\nYou will hinder caching. Especially if you change ids or classes of your HTML elements, this will require corresponding changes in your CSS and JavaScript files, which means that every time you change them, they will have to be re-downloaded by the browser. This will result in longer page load times for repeat visitors, and increased server load. If you only change it once a week, it will not be a big problem.\nClever scrapers will still be able to get your content by inferring where the actual content is, eg. by knowing that a large single block of text on the page is likely to be the actual article. This makes it possible to still find & extract the desired data from the page. Boilerpipe does exactly this.\n\nEssentially, make sure that it is not easy for a script to find the actual, desired content for every similar page.\nSee also How to prevent crawlers depending on XPath from getting page contents for details on how this can be implemented in PHP.\nChange your HTML based on the user\'s location\nThis is sort of similar to the previous tip. If you serve different HTML based on your user\'s location / country (determined by IP address), this may break scrapers which are delivered to users. For example, if someone is writing a mobile app which scrapes data from your site, it will work fine initially, but break when it\'s actually distributed to users, as those users may be in a different country, and thus get different HTML, which the embedded scraper was not designed to consume.\nFrequently change your HTML, actively screw with the scrapers by doing so !\nAn example: You have a search feature on your website, located at example.com/search?query=somesearchquery, which returns the following HTML:\n<div class=""search-result"">\n  <h3 class=""search-result-title"">Stack Overflow has become the world\'s most popular programming Q & A website</h3>\n  <p class=""search-result-excerpt"">The website Stack Overflow has now become the most popular programming Q & A website, with 10 million questions and many users, which...</p>\n  <a class""search-result-link"" href=""/stories/story-link"">Read more</a>\n</div>\n(And so on, lots more identically structured divs with search results)\n\nAs you may have guessed this is easy to scrape: all a scraper needs to do is hit the search URL with a query, and extract the desired data from the returned HTML. In addition to periodically changing the HTML as described above, you could also leave the old markup with the old ids and classes in, hide it with CSS, and fill it with fake data, thereby poisoning the scraper. Here\'s how the search results page could be changed:\n<div class=""the-real-search-result"">\n  <h3 class=""the-real-search-result-title"">Stack Overflow has become the world\'s most popular programming Q & A website</h3>\n  <p class=""the-real-search-result-excerpt"">The website Stack Overflow has now become the most popular programming Q & A website, with 10 million questions and many users, which...</p>\n  <a class""the-real-search-result-link"" href=""/stories/story-link"">Read more</a>\n</div>\n\n<div class=""search-result"" style=""display:none"">\n  <h3 class=""search-result-title"">Visit Example.com now, for all the latest Stack Overflow related news !</h3>\n  <p class=""search-result-excerpt"">Example.com is so awesome, visit now !</p>\n  <a class""search-result-link"" href=""http://example.com/"">Visit Now !</a>\n</div>\n(More real search results follow)\n\nThis will mean that scrapers written to extract data from the HTML based on classes or IDs will continue to seemingly work, but they will get fake data or even ads, data which real users will never see, as they\'re hidden with CSS.\nScrew with the scraper: Insert fake, invisible honeypot data into your page\nAdding on to the previous example, you can add invisible honeypot items to your HTML to catch scrapers. An example which could be added to the previously described search results page:\n<div class=""search-result"" style=""display:none"">\n  <h3 class=""search-result-title"">This search result is here to prevent scraping</h3>\n  <p class=""search-result-excerpt"">If you\'re a human and see this, please ignore it. If you\'re a scraper, please click the link below :-)\n  Note that clicking the link below will block access to this site for 24 hours.</p>\n  <a class""search-result-link"" href=""/scrapertrap/scrapertrap.php"">I\'m a scraper !</a>\n</div>\n(The actual, real, search results follow.)\n\nA scraper written to get all the search results will pick this up, just like any of the other, real search results on the page, and visit the link, looking for the desired content. A real human will never even see it in the first place (due to it being hidden with CSS), and won\'t visit the link. A genuine and desirable spider such as Google\'s will not visit the link either because you disallowed /scrapertrap/ in your robots.txt.\nYou can make your scrapertrap.php do something like block access for the IP address that visited it or force a captcha for all subsequent requests from that IP.\n\nDon\'t forget to disallow your honeypot (/scrapertrap/) in your robots.txt file so that search engine bots don\'t fall into it.\nYou can / should combine this with the previous tip of changing your HTML frequently.\nChange this frequently too, as scrapers will eventually learn to avoid it. Change the honeypot URL and text. Also want to consider changing the inline CSS used for hiding, and use an ID attribute and external CSS instead, as scrapers will learn to avoid anything which has a style attribute with CSS used to hide the content. Also try only enabling it sometimes, so the scraper works initially, but breaks after a while. This also applies to the previous tip.\nMalicious people can prevent access for real users by sharing a link to your honeypot, or even embedding that link somewhere as an image (eg. on a forum). Change the URL frequently, and make any ban times relatively short.\n\nServe fake and useless data if you detect a scraper\nIf you detect what is obviously a scraper, you can serve up fake and useless data; this will corrupt the data the scraper gets from your website. You should also make it impossible to distinguish such fake data from real data, so that scrapers don\'t know that they\'re being screwed with.\nAs an example: you have a news website; if you detect a scraper, instead of blocking access,  serve up fake, randomly generated articles, and this will poison the data the scraper gets. If you make your fake data indistinguishable from the real thing, you\'ll make it hard for scrapers to get what they want, namely the actual, real data. \nDon\'t accept requests if the User Agent is empty / missing\nOften, lazily written scrapers will not send a User Agent header with their request, whereas all  browsers as well as search engine spiders will. \nIf you get a request where the User Agent header is not present, you can show a captcha, or simply block or limit access. (Or serve fake data as described above, or something else..)\nIt\'s trivial to spoof, but as a measure against poorly written scrapers it is worth implementing.\nDon\'t accept requests if the User Agent is a common scraper one; blacklist ones used by scrapers\nIn some cases, scrapers will use a User Agent which no real browser or search engine spider uses, such as:\n\n""Mozilla"" (Just that, nothing else. I\'ve seen a few questions about scraping here, using that. A real browser will never use only that)\n""Java 1.7.43_u43"" (By default, Java\'s HttpUrlConnection uses something like this.)\n""BIZCO EasyScraping Studio 2.0""\n""wget"", ""curl"", ""libcurl"",.. (Wget and cURL are sometimes used for basic scraping)\n\nIf you find that a specific User Agent string is used by scrapers on your site, and it is not used by real browsers or legitimate spiders, you can also add it to your blacklist.\nIf it doesn\'t request assets (CSS, images), it\'s not a real browser.\nA real browser will (almost always) request and download assets such as images and CSS. HTML parsers and scrapers won\'t as they are only interested in the actual pages and their content.\nYou could log requests to your assets, and if you see lots of requests for only the HTML, it may be a scraper.\nBeware that search engine bots, ancient mobile devices, screen readers and misconfigured devices may not request assets either.\nUse and require cookies; use them to track user and scraper actions.\nYou can require cookies to be enabled in order to view your website. This will deter inexperienced and newbie scraper writers, however it is easy to for a scraper to send cookies. If you do use and require them, you can track user and scraper actions with them, and thus implement rate-limiting, blocking, or showing captchas on a per-user instead of a per-IP basis.\nFor example: when the user performs search, set a unique identifying cookie. When the results pages are viewed, verify that cookie. If the user opens all the search results (you can tell from the cookie), then it\'s probably a scraper.\nUsing cookies may be ineffective, as scrapers can send the cookies with their requests too, and discard them as needed. You will also prevent access for real users who have cookies disabled, if your site only works with cookies.\nNote that if you use JavaScript to set and retrieve the cookie, you\'ll block scrapers which don\'t run JavaScript, since they can\'t retrieve and send the cookie with their request.\nUse JavaScript + Ajax to load your content\nYou could use JavaScript + AJAX to load your content after the page itself loads. This will make the content inaccessible to HTML parsers which do not run JavaScript. This is often an effective deterrent to newbie and inexperienced programmers writing scrapers.\nBe aware of:\n\nUsing JavaScript to load the actual content will degrade user experience and performance\nSearch engines may not run JavaScript either, thus preventing them from indexing your content. This may not be a problem for search results pages, but may be for other things, such as article pages.\n\nObfuscate your markup, network requests from scripts, and everything else.\nIf you use Ajax and JavaScript to load your data, obfuscate the data which is transferred. As an example, you could encode your data on the server (with something as simple as base64 or more complex), and then decode and display it on the client, after fetching via Ajax. This will mean that someone inspecting network traffic will not immediately see how your page works and loads data, and it will be tougher for someone to directly request request data from your endpoints, as they will have to reverse-engineer your descrambling algorithm.\n\nIf you do use Ajax for loading the data, you should make it hard to use the endpoints without loading the page first, eg by requiring some session key as a parameter, which you can embed in your JavaScript or your HTML.\nYou can also embed your obfuscated data directly in the initial HTML page and use JavaScript to deobfuscate and display it, which would avoid the extra network requests. Doing this will make it significantly harder to extract the data using a HTML-only parser which does not run JavaScript, as the one writing the scraper will have to reverse engineer your JavaScript (which you should obfuscate too).\nYou might want to change your obfuscation methods regularly, to break scrapers who have figured it out.\n\nThere are several disadvantages to doing something like this, though:\n\nIt will be tedious and difficult to implement, maintain, and debug.\nIt will be ineffective against scrapers and screenscrapers which actually run JavaScript and then extract the data. (Most simple HTML parsers don\'t run JavaScript though)\nIt will make your site nonfunctional for real users if they have JavaScript disabled.\nPerformance and page-load times will suffer.\n\nNon-Technical:\n\nTell people not to scrape, and some will respect it\nFind a lawyer\nMake your data available, provide an API:\nYou could make your data easily available and require attribution and a link back to your site. Perhaps charge $$$ for it.\n\nMiscellaneous:\n\nThere are also commercial scraping protection services, such as the anti-scraping by Cloudflare or Distill Networks (Details on how it works here), which do these things, and more for you.\nFind a balance between usability for real users and scraper-proofness: Everything you do will impact user experience negatively in one way or another,  find compromises.\nDon\'t forget your mobile site and apps. If you have a mobile app, that can be screenscraped too, and network traffic can be inspected to determine the REST endpoints it uses.\nScrapers can scrape other scrapers: If there\'s one website which has content scraped from yours, other scrapers can scrape from that scraper\'s website.\n\nFurther reading:\n\nWikipedia\'s article on Web scraping. Many details on the technologies involved and the different types of web scraper.\nStopping scripters from slamming your website hundreds of times a second. Q & A on a very similar problem - bots checking a website and buying things as soon as they go on sale. A lot of relevant info, esp. on Captchas and rate-limiting.\n\n', '\nI will presume that you have set up robots.txt.\nAs others have mentioned, scrapers can fake nearly every aspect of their activities, and it is probably very difficult to identify the requests that are coming from the bad guys.\nI would consider:\n\nSet up a page, /jail.html.\nDisallow access to the page in robots.txt (so the respectful spiders will never visit).\nPlace a link on one of your pages, hiding it with CSS (display: none).\nRecord IP addresses of visitors to /jail.html.\n\nThis might help you to quickly identify requests from scrapers that are flagrantly disregarding your robots.txt.\nYou might also want to make your /jail.html a whole entire website that has the same, exact markup as normal pages, but with fake data (/jail/album/63ajdka, /jail/track/3aads8, etc.). This way, the bad scrapers won\'t be alerted to ""unusual input"" until you have the chance to block them entirely.\n', ""\nSue 'em. \nSeriously: If you have some money, talk to a good, nice, young lawyer who knows their way around the Internets. You could really be able to do something here. Depending on where the sites are based, you could have a lawyer write up a cease & desist or its equivalent in your country. You may be able to at least scare the bastards.\nDocument the insertion of your dummy values. Insert dummy values that clearly (but obscurely) point to you. I think this is common practice with phone book companies, and here in Germany, I think there have been several instances when copycats got busted through fake entries they copied 1:1.\nIt would be a shame if this would drive you into messing up your HTML code, dragging down SEO, validity and other things (even though a templating system that uses a slightly different HTML structure on each request for identical pages might already help a lot against scrapers that always rely on HTML structures and class/ID names to get the content out.)  \nCases like this are what copyright laws are good for. Ripping off other people's honest work to make money with is something that you should be able to fight against.\n"", ""\nProvide an XML API to access your data; in a manner that is simple to use. If people want your data, they'll get it, you might as well go all out.\nThis way you can provide a subset of functionality in an effective manner, ensuring that, at the very least, the scrapers won't guzzle up HTTP requests and massive amounts of bandwidth.\nThen all you have to do is convince the people who want your data to use the API. ;)\n"", ""\nThere is really nothing you can do to completely prevent this. Scrapers can fake their user agent, use multiple IP addresses, etc. and appear as a normal user. The only thing you can do is make the text not available at the time the page is loaded - make it with image, flash, or load it with JavaScript. However, the first two are bad ideas, and the last one would be an accessibility issue if JavaScript is not enabled for some of your regular users.\nIf they are absolutely slamming your site and rifling through all of your pages, you could do some kind of rate limiting.\nThere is some hope though. Scrapers rely on your site's data being in a consistent format. If you could randomize it somehow it could break their scraper. Things like changing the ID or class names of page elements on each load, etc. But that is a lot of work to do and I'm not sure if it's worth it. And even then, they could probably get around it with enough dedication.\n"", ""\nSorry, it's really quite hard to do this...\nI would suggest that you politely ask them to not use your content (if your content is copyrighted).\nIf it is and they don't take it down, then you can take furthur action and send them a cease and desist letter.\nGenerally, whatever you do to prevent scraping will probably end up with a more negative effect, e.g. accessibility, bots/spiders, etc.\n"", '\nOkay, as all posts say, if you want to make it search engine-friendly then bots can scrape for sure.\nBut you can still do a few things, and it may be affective for 60-70 % scraping bots.\nMake a checker script like below.\nIf a particular IP address is visiting very fast then after a few visits (5-10) put its IP address + browser information in a file or database.\nThe next step\n(This would be a background process and running all time or scheduled after a few minutes.) Make one another script that will keep on checking those suspicious IP addresse",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.\nCase 1. If the user Agent is of a known search engine like Google, Bing, Yahoo (you can find more information on user agents by googling it). Then you must see http://www.iplists.com/. This list and try to match patterns. And if it seems like a faked user-agent then ask to fill in a CAPTCHA on the next visit. (You need to research a bit more on bots IP addresses. I know this is achievable and also try whois of the IP address. It can be helpful.)\nCase 2. No user agent of a search bot: Simply ask to fill in a CAPTCHA on the next visit.\n'," '\nLate answer - and also this answer probably isn\'t the one you want to hear...\nMyself already wrote many (many tens) of different specialized data-mining scrapers. (just because I like the """"open data"""" philosophy).\nHere are already many advices in other answers - now i will play the devil\'s advocate role and will extend and/or correct their effectiveness.\nFirst:\n\nif someone really wants your data\nyou can\'t effectively (technically) hide your data\nif the data should be publicly accessible to your """"regular users""""\n\nTrying to use some technical barriers aren\'t worth the troubles", caused:\n\nto your regular users by worsening their user-experience\nto regular and welcomed bots (search engines)\netc...\n\nPlain HMTL - the easiest way is parse the plain HTML pages, with well defined structure and css classes. E.g. it is enough to inspect element with Firebug, and use the right Xpaths, and/or CSS path in my scraper.\nYou could generate the HTML structure dynamically and also," you can generate dynamically the CSS class-names (and the CSS itself too) (e.g. by using some random class names) - but\n\nyou want to present the informations to your regular users in consistent way\ne.g. again - it is enough to analyze the page structure once more to setup the scraper.\nand it can be done automatically by analyzing some """"already known content""""\n\n\nonce someone already knows (by earlier scrape)"," e.g.:\nwhat contains the informations about """"phil collins""""\nenough display the """"phil collins"""" page and (automatically) analyze how the page is structured """"today"""" :)\n\n\nYou can\'t change the structure for every response", because your regular users will hate you. Also, this will cause more troubles for you (maintenance) not for the scraper. The XPath or CSS path is determinable by the scraping script automatically from the known content.\nAjax - little bit harder in the start, but many times speeds up the scraping process :) - why?\nWhen analyzing the requests and responses, i just setup my own proxy server (written in perl) and my firefox is using it. Of course, because it is my own proxy - it is completely hidden - the target server see it as regular browser. (So, no X-Forwarded-for and such headers).\nBased on the proxy logs," mostly is possible to determine the """"logic"""" of the ajax requests", e.g. i could skip most of the html scraping, and just use the well-structured ajax responses (mostly in JSON format).\nSo, the ajax doesn\'t helps much...\nSome more complicated are pages which uses much packed javascript functions.\nHere is possible to use two basic methods:\n\nunpack and understand the JS and create a scraper which follows the Javascript logic (the hard way)\nor (preferably using by myself) - just using Mozilla with Mozrepl for scrape. E.g. the real scraping is done in full featured javascript enabled browser," which is programmed to clicking to the right elements and just grabbing the """"decoded"""" responses directly from the browser window.\n\nSuch scraping is slow (the scraping is done as in regular browser)"," but it is\n\nvery easy to setup and use\nand it is nearly impossible to counter it :)\nand the """"slowness"""" is needed anyway to counter the """"blocking the rapid same IP based requests""""\n\nThe User-Agent based filtering doesn\'t helps at all. Any serious data-miner will set it to some correct one in his scraper.\nRequire Login - doesn\'t helps. The simplest way beat it (without any analyze and/or scripting the login-protocol) is just logging into the site as regular user", using Mozilla and after just run the Mozrepl based scraper...\nRemember, the require login helps for anonymous bots," but doesn\'t helps against someone who want scrape your data. He just register himself to your site as regular user.\nUsing frames isn\'t very effective also. This is used by many live movie services and it not very hard to beat. The frames are simply another one HTML/Javascript pages what are needed to analyze... If the data worth the troubles - the data-miner will do the required analyze.\nIP-based limiting isn\'t effective at all - here are too many public proxy servers and also here is the TOR... :) It doesn\'t slows down the scraping (for someone who really wants your data).\nVery hard is scrape data hidden in images. (e.g. simply converting the data into images server-side). Employing """"tesseract"""" (OCR) helps many times - but honestly - the data must worth the troubles for the scraper. (which many times doesn\'t worth).\nOn the other side", your users will hate you for this. Myself, (even when not scraping) hate websites which doesn\'t allows copy the page content into the clipboard (because the information are in the images, or (the silly ones) trying to bond to the right click some custom Javascript event. :)\nThe hardest are the sites which using java applets or flash, and the applet uses secure https requests itself internally. But think twice - how happy will be your iPhone users... ;). Therefore, currently very few sites using them. Myself, blocking all flash content in my browser (in regular browsing sessions) - and never using sites which depends on Flash.\nYour milestones could be..., so you can try this method - just remember - you will probably loose some of your users. Also remember, some SWF files are decompilable. ;)\nCaptcha (the good ones - like reCaptcha) helps a lot - but your users will hate you... - just imagine, how your users will love you when they need solve some captchas in all pages showing informations about the music artists.\nProbably don\'t need to continue - you already got into the picture.\nNow what you should do:\nRemember: It is nearly impossible to hide your data, if you on the other side want publish them (in friendly way) to your regular users.\nSo,\n\nmake your data easily accessible - by some API\n\n\nthis allows the easy data access\ne.g. offload your server from scraping - good for you\n\nsetup the right usage rights (e.g. for example must cite the source)\nremember, many data isn\'t copyright-able - and hard to protect them\nadd some fake data (as you already done) and use legal tools\n\n\nas others already said," send an """"cease and desist letter""""\nother legal actions (sue and like) probably is too costly and hard to win (especially against non US sites)\n\n\nThink twice before you will try to use some technical barriers.\nRather as trying block the data-miners", just add more efforts to your website usability. Your user will love you. The time (&energy) invested into technical barriers usually aren\'t worth - better to spend the time to make even better website...\nAlso," data-thieves aren\'t like normal thieves.\nIf you buy an inexpensive home alarm and add an warning """"this house is connected to the police"""" - many thieves will not even try to break into. Because one wrong move by him - and he going to jail...\nSo", you investing only few bucks, but the thief investing and risk much.\nBut the data-thief hasn\'t such risks. just the opposite - ff you make one wrong move (e.g. if you introduce some BUG as a result of technical barriers), you will loose your users. If the the scraping bot will not work for the first time, nothing happens - the data-miner just will try another approach and/or will debug the script.\nIn this case, you need invest much more - and the scraper investing much less.\nJust think where you want invest your time & energy...\nPs: english isn\'t my native - so forgive my broken english...\n', '\nThings that might work against beginner scrapers:\n\nIP blocking\nuse lots of ajax\ncheck referer request header\nrequire login\n\nThings that will help in general:\n\nchange your layout every week\nrobots.txt\n\nThings that will help but will make your users hate you:\n\ncaptcha\n\n'," """"\nI have done a lot of web scraping and summarized some techniques to stop web scrapers  on my blog based on what I find annoying.\nIt is a tradeoff between your users and scrapers. If you limit IP's", use CAPTCHA's, require login, etc," you make like difficult for the scrapers. But this may also drive away your genuine users.\n"""""," """"\nFrom a tech perspective: \nJust model what Google does when you hit them with too many queries at once. That should put a halt to a lot of it.\nFrom a legal perspective:\nIt sounds like the data you're publishing is not proprietary. Meaning you're publishing names and stats and other information that cannot be copyrighted. \nIf this is the case", the scrapers are not violating copyright by redistributing your information about artist name etc. However, they may be violating copyright when they load your site into memory because your site contains elements that are copyrightable (like layout etc).\nI recommend reading about Facebook v. Power.com and seeing the arguments Facebook used to stop screen scraping. There are many legal ways you can go about trying to stop someone from scraping your website. They can be far reaching and imaginative. Sometimes the courts buy the arguments. Sometimes they don't. \nBut, assuming you're publishing public domain information that's not copyrightable like names and basic stats... you should just let it go in the name of free speech and open data. That is," what the web's all about.\n"""""," """"\nYour best option is unfortunately fairly manual: Look for traffic patterns that you believe are indicative of scraping and ban their IP addresses.\nSince you're talking about a public site then making the site search-engine friendly will also make the site scraping-friendly. If a search-engine can crawl and scrape your site then an malicious scraper can as well. It's a fine-line to walk.\n"""""," """"\nSure it's possible. For 100% success", take your site offline.\nIn reality you can do some things that make scraping a little more difficult. Google does browser checks to make sure you're not a robot scraping search results (although this, like most everything else, can be spoofed).\nYou can do things like require several seconds between the first connection to your site, and subsequent clicks. I'm not sure what the ideal time would be or exactly how to do it, but that's another idea.\nI'm sure there are several other people who have a lot more experience," but I hope those ideas are at least somewhat helpful.\n"""""," """"\n\nNo", it's not possible to stop (in any way)\nEmbrace it. Why not publish as RDFa and become super search engine friendly and encourage the re-use of data? People will thank you and provide credit where due (see musicbrainz as an example).\n\nIt is not the answer you probably want," but why hide what you're trying to make public?\n"""""," """"\nThere are a few things you can do to try and prevent screen scraping.  Some are not very effective", while others (a CAPTCHA) are, but hinder usability.  You have to keep in mind too that it may hinder legitimate site scrapers, such as search engine indexes.\nHowever, I assume that if you don't want it scraped that means you don't want search engines to index it either.\nHere are some things you can try:\n\nShow the text in an image.  This is quite reliable, and is less of a pain on the user than a CAPTCHA, but means they won't be able to cut and paste and it won't scale prettily or be accessible.\nUse a CAPTCHA and require it to be completed before returning the page.  This is a reliable method, but also the biggest pain to impose on a user.\nRequire the user to sign up for an account before viewing the pages, and confirm their email address.  This will be pretty effective, but not totally - a screen-scraper might set up an account and might cleverly program their script to log in for them.\nIf the client's user-agent string is empty, block access.  A site-scraping script will often be lazily programmed and won't set a user-agent string, whereas all web browsers will.\nYou can set up a black list of known screen scraper user-agent strings as you discover them.  Again, this will only help the lazily-coded ones; a programmer who knows what he's doing can set a user-agent string to impersonate a web browser.\nChange the URL path often.  When you change it, make sure the old one keeps working, but only for as long as one user is likely to have their browser open.  Make it hard to predict what the new URL path will be.  This will make it difficult for scripts to grab it if their URL is hard-coded.  It'd be best to do this with some kind of script.\n\nIf I had to do this, I'd probably use a combination of the last three, because they minimise the inconvenience to legitimate users.  However, you'd have to accept that you won't be able to block everyone this way and once someone figures out how to get around it," they'll be able to scrape it forever.  You could then just try to block their IP addresses as you discover them I guess.\n""""", '\nMethod One (Small Sites Only):\nServe encrypted / encoded data.I Scape the web using python (urllib, requests, beautifulSoup etc...) and found many websites that serve encrypted / encoded data that is not decrypt-able in any programming language simply because the encryption method does not exist.\nI achieved this in a PHP website by encrypting and minimizing the output (WARNING: this is not a good idea for large sites) the response was always jumbled content.\nExample of minimizing output in PHP (How to minify php page html output?):\n<?php\n  function sanitize_output($buffer) {\n    $search = array(\n      \'/\\>[^\\S ]+/s\', // strip whitespaces after tags, except space\n      \'/[^\\S ]+\\</s\', // strip whitespaces before tags, except space\n      \'/(\\s)+/s\'      // shorten multiple whitespace sequences\n    );\n    $replace = array(\'>\', \'<\', \'\\\\1\');\n    $buffer = preg_replace($search, $replace," $buffer);\n    return $buffer;\n  }\n  ob_start(""""sanitize_output"""");\n?>\n\nMethod Two:\nIf you can\'t stop them screw them over serve fake / useless data as a response.\nMethod Three:\nblock common scraping user agents"," you\'ll see this in major / large websites as it is impossible to scrape them with """"python3.4"""" as you User-Agent.\nMethod Four:\nMake sure all the user headers are valid", I sometimes provide as many headers as possible to make my scraper seem like an authentic user," some of them are not even true or valid like en-FU :).\nHere is a list of some of the headers I commonly provide.\nheaders = {\n  """"Requested-URI"""": """"/example""""","\n  """"Request-Method"""": """"GET""""","\n  """"Remote-IP-Address"""": """"656.787.909.121""""","\n  """"Remote-IP-Port"""": """"69696""""","\n  """"Protocol-version"""": """"HTTP/1.1""""","\n  """"Accept"""": """"text/html",application/xhtml+xml,application/xml;q=0.9,image/webp,"*/*;q=0.8""""","\n  """"Accept-Encoding"""": """"gzip","deflate""""","\n  """"Accept-Language"""": """"en-FU","en;q=0.8""""","\n  """"Cache-Control"""": """"max-age=0""""","\n  """"Connection"""": """"keep-alive""""","\n  """"Dnt"""": """"1""""","  \n  """"Host"""": """"http://example.com""""","\n  """"Referer"""": """"http://example.com""""","\n  """"Upgrade-Insecure-Requests"""": """"1""""","\n  """"User-Agent"""": """"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML"," like Gecko) Chrome/47.0.2526.111 Safari/537.36""""\n}\n\n'", '\nQuick approach to this would be to set a booby/bot trap.\n\nMake a page that if it\'s opened a certain amount of times or even opened at all, will collect certain information like the IP and whatnot (you can also consider irregularities or patterns but this page shouldn\'t have to be opened at all). \nMake a link to this in your page that is hidden with CSS display:none; or left:-9999px; positon:absolute; try to place it in places that are less unlikely to be ignored like where your content falls under and not your footer as sometimes bots can choose to forget about certain parts of a page. \nIn your robots.txt file set a whole bunch of disallow rules to pages you don\'t want friendly bots (LOL, like they have happy faces!) to gather information on and set this page as one of them. \nNow, If a friendly bot comes through it should ignore that page. Right but that still isn\'t good enough. Make a couple more of these pages or somehow re-route a page to accept differnt names. and then place more disallow rules to these trap pages in your robots.txt file alongside pages you want ignored.\nCollect the IP of these bots or anyone that enters into these pages, don\'t ban them but make a function to display noodled text in your content like random numbers, copyright notices, specific text strings, display scary pictures, basically anything to hinder your good content. You can also set links that point to a page which will take forever to load ie. in php you can use the sleep() function. This will fight the crawler back if it has some sort of detection to bypass pages that take way too long to load as some well written bots are set to process X amount of links at a time.  \nIf you have made specific text strings/sentences why not go to your favorite search engine and search for them, it might show you where your content is ending up.\n\nAnyway," if you think tactically and creatively this could be a good starting point. The best thing to do would be to learn how a bot works.\nI\'d also think about scambling some ID\'s or the way attributes on the page element are displayed: \n<a class=""""someclass"""" href=""""../xyz/abc"""" rel=""""nofollow"""" title=""""sometitle""""> \n\nthat changes its form every time as some bots might be set to be looking for specific patterns in your pages or targeted elements. \n<a title=""""sometitle"""" href=""""../xyz/abc"""" rel=""""nofollow"""" class=""""someclass""""> \n\nid=""""p-12802"""" > id=""""p-00392""""\n\n'"," """"\nYou can't stop normal screen scraping. For better or worse"," it's the nature of the web.\nYou can make it so no one can access certain things (including music files) unless they're logged in as a registered user. It's not too difficult to do in Apache. I assume it wouldn't be too difficult to do in IIS as well.\n"""""," """"\nRather than blacklisting bots", maybe you should whitelist them.  If you don't want to kill your search results for the top few engines, you can whitelist their user-agent strings, which are generally well-publicized.  The less ethical bots tend to forge user-agent strings of popular web browsers.  The top few search engines should be driving upwards of 95% of your traffic.\nIdentifying the bots themselves should be fairly straightforward," using the techniques other posters have suggested.\n"""""," """"\nMost have been already said", but have you considered the CloudFlare protection? I mean this:\n\nOther companies probably do this too," CloudFlare is the only one I know.\nI'm pretty sure that would complicate their work. I also once got IP banned automatically for 4 months when I tried to scrap data of a site protected by CloudFlare due to rate limit (I used simple AJAX request loop).\n""""", '\nOne way would be to serve the content as XML attributes, URL encoded strings, preformatted text with HTML encoded JSON, or data URIs," then transform it to HTML on the client. Here are a few sites which do this:\n\nSkechers: XML\n<document \n filename="""""""" \n height="""""""" \n width="""""""" \n title=""""SKECHERS"""" \n linkType="""""""" \n linkUrl="""""""" \n imageMap="""""""" \n href=&quot;http://www.bobsfromskechers.com&quot; \n alt=&quot;BOBS from Skechers&quot; \n title=&quot;BOBS from Skechers&quot; \n/>\n\nChrome Web Store: JSON\n<script type=""""text/javascript"""" src=""""https://apis.google.com/js/plusone.js"""">{""""lang"""": """"en"""""," """"parsetags"""": """"explicit""""}</script>\n\nBing News: data URL\n<script type=""""text/javascript"""">\n  //<![CDATA[\n  (function()\n    {\n    var x;x=_ge(\'emb7\');\n    if(x)\n      {\n      x.src=\'data:image/jpeg;base64",/*...*/\';\n      } \n    }() )\n\nProtopage: URL Encoded Strings\nunescape(\'Rolling%20Stone%20%3a%20Rock%20and%20Roll%20Daily\')\n\nTiddlyWiki : HTML Entities + preformatted JSON\n   <pre>\n   {&quot;tiddlers&quot;: \n    {\n    &quot;GettingStarted&quot;: \n      {\n      &quot;title&quot;: &quot;GettingStarted&quot;,\n      &quot;text&quot;: &quot;Welcome to TiddlyWiki,\n      }\n    }\n   }\n   </pre>\n\nAmazon: Lazy Loading\namzn.copilot.jQuery=i;amzn.copilot.jQuery(document).ready(function(){d(b);f(c,function() {amzn.copilot.setup({serviceEndPoint:h.vipUrl,isContinuedSession:true})})})},f=function(i,"h){var j=document.createElement(""""script"""");j.type=""""text/javascript"""";j.src=i;j.async=true;j.onload=h;a.appendChild(j)}","d=function(h){var i=document.createElement(""""link"""");i.type=""""text/css"""";i.rel=""""stylesheet"""";i.href=h;a.appendChild(i)}})();\namzn.copilot.checkCoPilotSession({jsUrl : \'http://z-ecx.images-amazon.com/images/G/01/browser-scripts/cs-copilot-customer-js/cs-copilot-customer-js-min-1875890922._V1_.js\'", cssUrl : \'http://z-ecx.images-amazon.com/images/G/01/browser-scripts/cs-copilot-customer-css/cs-copilot-customer-css-min-2367001420._V1_.css\'," vipUrl : \'https://copilot.amazon.com\'\n\nXMLCalabash: Namespaced XML + Custom MIME type + Custom File extension\n   <p:declare-step type=""""pxp:zip"""">\n        <p:input port=""""source"""" sequence=""""true"""" primary=""""true""""/>\n        <p:input port=""""manifest""""/>\n        <p:output port=""""result""""/>\n        <p:option name=""""href"""" required=""""true"""" cx:type=""""xsd:anyURI""""/>\n        <p:option name=""""compression-method"""" cx:type=""""stored|deflated""""/>\n        <p:option name=""""compression-level"""" cx:type=""""smallest|fastest|default|huffman|none""""/>\n        <p:option name=""""command"""" select=""""\'update\'"""" cx:type=""""update|freshen|create|delete""""/>\n   </p:declare-step>\n\n\nIf you view source on any of the above", you see that scraping will simply return metadata and navigation.\n'," """"\nI agree with most of the posts above", and I'd like to add that the more search engine friendly your site is, the more scrape-able it would be. You could try do a couple of things that are very out there that make it harder for scrapers," but it might also affect your search-ability... It depends on how well you want your site to rank on search engines of course.\n""""", '\nPutting your content behind a captcha would mean that robots would find it difficult to access your content.  However, humans would be inconvenienced so that may be undesirable.\n', '\nIf you want to see a great example, check out http://www.bkstr.com/.  They use a j/s algorithm to set a cookie, then reloads the page so it can use the cookie to validate that the request is being run within a browser.  A desktop app built to scrape could definitely get by this, but it would stop most cURL type scraping.\n'," """"\nScreen scrapers work by processing HTML. And if they are determined to get your data there is not much you can do technically because the human eyeball processes anything. Legally it's already been pointed out you may have some recourse though and that would be my recommendation.\nHowever", you can hide the critical part of your data by using non-HTML-based presentation logic\n\nGenerate a Flash file for each artist/album, etc.\nGenerate an image for each artist content. Maybe just an image for the artist name," etc. would be enough. Do this by rendering the text onto a JPEG/PNG file on the server and linking to that image.\n\nBear in mind that this would probably affect your search rankings.\n""""", '\nGenerate the HTML, CSS and JavaScript. It is easier to write generators than parsers," so you could generate each served page differently. You can no longer use a cache or static content then.\n']""",https://stackoverflow.com/questions/3161548/how-do-i-prevent-site-scraping,web-scraping
BeautifulSoup webscraping find_all( ): finding exact match,"
I'm using Python and BeautifulSoup for web scraping.
Lets say I have the following html code to scrape:
<body>
    <div class=""product"">Product 1</div>
    <div class=""product"">Product 2</div>
    <div class=""product special"">Product 3</div>
    <div class=""product special"">Product 4</div>
</body>

Using BeautifulSoup, I want to find ONLY the products with the attribute class=""product""
(only Product 1 and 2), not the 'special' products
If I do the following:
result = soup.find_all('div', {'class': 'product'})

the result includes ALL the products (1,2,3, and 4).
What should I do to find products whose class EXACTLY matches 'product'??

The Code I ran:
from bs4 import BeautifulSoup
import re

text = """"""
<body>
    <div class=""product"">Product 1</div>
    <div class=""product"">Product 2</div>
    <div class=""product special"">Product 3</div>
    <div class=""product special"">Product 4</div>
</body>""""""

soup = BeautifulSoup(text)
result = soup.findAll(attrs={'class': re.compile(r""^product$"")})
print result

Output:
[<div class=""product"">Product 1</div>, <div class=""product"">Product 2</div>, <div class=""product special"">Product 3</div>, <div class=""product special"">Product 4</div>]

",111k,"
            40
        ","['\nIn BeautifulSoup 4, the class attribute (and several other attributes, such as accesskey and the headers attribute on table cell elements) is treated as a set; you match against individual elements listed in the attribute. This follows the HTML standard.\nAs such, you cannot limit the search to just one class.\nYou\'ll have to use a custom function here to match against the class instead:\nresult = soup.find_all(lambda tag: tag.name == \'div\' and \n                                   tag.get(\'class\') == [\'product\'])\n\nI used a lambda to create an anonymous function; each tag is matched on name (must be \'div\'), and the class attribute must be exactly equal to the list [\'product\']; e.g. have just the one value.\nDemo:\n>>> from bs4 import BeautifulSoup\n>>> text = """"""\n... <body>\n...     <div class=""product"">Product 1</div>\n...     <div class=""product"">Product 2</div>\n...     <div class=""product special"">Product 3</div>\n...     <div class=""product special"">Product 4</div>\n... </body>""""""\n>>> soup = BeautifulSoup(text)\n>>> soup.find_all(lambda tag: tag.name == \'div\' and tag.get(\'class\') == [\'product\'])\n[<div class=""product"">Product 1</div>, <div class=""product"">Product 2</div>]\n\nFor completeness sake, here are all such set attributes, from the BeautifulSoup source code:\n# The HTML standard defines these attributes as containing a\n# space-separated list of values, not a single value. That is,\n# class=""foo bar"" means that the \'class\' attribute has two values,\n# \'foo\' and \'bar\', not the single value \'foo bar\'.  When we\n# encounter one of these attributes, we will parse its value into\n# a list of values if possible. Upon output, the list will be\n# converted back into a string.\ncdata_list_attributes = {\n    ""*"" : [\'class\', \'accesskey\', \'dropzone\'],\n    ""a"" : [\'rel\', \'rev\'],\n    ""link"" :  [\'rel\', \'rev\'],\n    ""td"" : [""headers""],\n    ""th"" : [""headers""],\n    ""td"" : [""headers""],\n    ""form"" : [""accept-charset""],\n    ""object"" : [""archive""],\n\n    # These are HTML5 specific, as are *.accesskey and *.dropzone above.\n    ""area"" : [""rel""],\n    ""icon"" : [""sizes""],\n    ""iframe"" : [""sandbox""],\n    ""output"" : [""for""],\n    }\n\n', ""\nYou can use CSS selectors like so:\nresult = soup.select('div.product.special')\n\ncss-selectors\n"", '\nsoup.findAll(attrs={\'class\': re.compile(r""^product$"")})\n\nThis code matches anything that doesn\'t have the product at the end of its class.\n', '\nYou could solve this problem and capture just Product 1 and Product 2 with gazpacho by enforcing exact matching:\nfrom gazpacho import Soup\n\nhtml = """"""\\\n<body>\n    <div class=""product"">Product 1</div>\n    <div class=""product"">Product 2</div>\n    <div class=""product special"">Product 3</div>\n    <div class=""product special"">Product 4</div>\n</body>\n""""""\n\nsoup = Soup(html)\ndivs = soup.find(""div"", {""class"": ""product""}, partial=False)\n[div.text for div in divs]\n\nOutputs exactly:\n[\'Product 1\', \'Product 2\']\n\n', '\nchange your code from\nresult = soup.findAll(attrs={\'class\': re.compile(r""^product$"")})\n\nto\nresult = soup.find_all(attrs={\'class\': \'product\'})\n\nand the result is a list and access through index\n']",https://stackoverflow.com/questions/22726860/beautifulsoup-webscraping-find-all-finding-exact-match,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Periodically refresh IMPORTXML() spreadsheet function,"
I have a large sheet with around 30 importxml functions that obtain data from a website that updates usually twice a day.
I would like to run the importxml function on a timely basis (every 8 hours) for my Google Spreadsheet to save the data in another sheet. The saving already works, however the updating does not!
I read in Google Spreadsheet row update that it might run every 2 hours, however I do not believe that this is true, because since I added it to my sheet nothing has changed or updated, when the spreadsheet is NOT opened.
How can I ""trigger"" the importxml function in my Google Spreadsheet in an easy way, as I have a lot of importxml functions in it?
",56k,"
            17
        ","['\nI made a couple of adjustments to Mogsdad\'s answer:\n\nFixed the releaseLock() call placement\nUpdates (or adds) a querystring parameter to the url in the import function (as opposed to storing, removing, waiting 5 seconds, and then restoring all relevant formulas)\nWorks on a specific sheet in your spreadsheet\nShows time of last update\n\n...\nfunction RefreshImports() {\n  var lock = LockService.getScriptLock();\n  if (!lock.tryLock(5000)) return;             // Wait up to 5s for previous refresh to end.\n\n  var id = ""[YOUR SPREADSHEET ID]"";\n  var ss = SpreadsheetApp.openById(id);\n  var sheet = ss.getSheetByName(""[SHEET NAME]"");\n  var dataRange = sheet.getDataRange();\n  var formulas = dataRange.getFormulas();\n  var content = """";\n  var now = new Date();\n  var time = now.getTime();\n  var re = /.*[^a-z0-9]import(?:xml|data|feed|html|range)\\(.*/gi;\n  var re2 = /((\\?|&)(update=[0-9]*))/gi;\n  var re3 = /("",)/gi;\n\n  for (var row=0; row<formulas.length; row++) {\n    for (var col=0; col<formulas[0].length; col++) {\n      content = formulas[row][col];\n      if (content != """") {\n        var match = content.search(re);\n        if (match !== -1 ) {\n          // import function is used in this cell\n          var updatedContent = content.toString().replace(re2,""$2update="" + time);\n          if (updatedContent == content) {\n            // No querystring exists yet in url\n            updatedContent = content.toString().replace(re3,""?update="" + time + ""$1"");\n          }\n          // Update url in formula with querystring param\n          sheet.getRange(row+1, col+1).setFormula(updatedContent);\n        }\n      }\n    }\n  }\n\n  // Done refresh; release the lock.\n  lock.releaseLock();\n\n  // Show last updated time on sheet somewhere\n  sheet.getRange(7,2).setValue(""Rates were last updated at "" + now.toLocaleTimeString())\n}\n\n', '\nThe Google Spreadsheet row update question and its answers refer to the ""Old Sheets"", which had different behaviour than the 2015 version of Google Sheets does. There is no automatic refresh of content with ""New Sheets""; changes are only evaluated now in response to edits.\nWhile Sheets no longer provides this capability natively, we can use a script to refresh the ""import"" formulas (IMPORTXML, IMPORTDATA, IMPORTHTML and IMPORTANGE).\nUtility script\nFor periodic refresh of IMPORT formulas, set this function up as a time-driven trigger.\nCaveats:\n\nImport function Formula changes made to the spreadsheet by other scripts or users  during the refresh period COULD BE OVERWRITTEN.\nOverlapping refreshes might make your spreadsheet unstable. To mitigate that, the utility script uses a ScriptLock. This may conflict with other uses of that lock in your script.\n\n\xa0\n/**\n * Go through all sheets in a spreadsheet, identify and remove all spreadsheet\n * import functions, then replace them a while later. This causes a ""refresh""\n * of the ""import"" functions. For periodic refresh of these formulas, set this\n * function up as a time-based trigger.\n *\n * Caution: Formula changes made to the spreadsheet by other scripts or users\n * during the refresh period COULD BE OVERWRITTEN.\n *\n * From: https://stackoverflow.com/a/33875957/1677912\n */\nfunction RefreshImports() {\n  var lock = LockService.getScriptLock();\n  if (!lock.tryLock(5000)) return;             // Wait up to 5s for previous refresh to end.\n  // At this point, we are holding the lock.\n\n  var id = ""YOUR-SHEET-ID"";\n  var ss = SpreadsheetApp.openById(id);\n  var sheets = ss.getSheets();\n\n  for (var sheetNum=0; sheetNum<sheets.length; sheetNum++) {\n    var sheet = sheets[sheetNum];\n    var dataRange = sheet.getDataRange();\n    var formulas = dataRange.getFormulas();\n    var tempFormulas = [];\n    for (var row=0; row<formulas.length; row++) {\n      for (col=0; col<formulas[0].length; col++) {\n        // Blank all formulas containing any ""import"" function\n        // See https://regex101.com/r/bE7fJ6/2\n        var re = /.*[^a-z0-9]import(?:xml|data|feed|html|range)\\(.*/gi;\n        if (formulas[row][col].search(re) !== -1 ) {\n          tempFormulas.push({row:row+1,\n                             col:col+1,\n                             formula:formulas[row][col]});\n          sheet.getRange(row+1, col+1).setFormula("""");\n        }\n      }\n    }\n\n    // After a pause, replace the import functions\n    Utilities.sleep(5000);\n    for (var i=0; i<tempFormulas.length; i++) {\n      var cell = tempFormulas[i];\n      sheet.getRange( cell.row, cell.col ).setFormula(cell.formula)\n    }\n\n    // Done refresh; release the lock.\n    lock.releaseLock();\n  }\n}\n\n', '\nTo answer your question for an easy ""trigger"" to force the function to reload:\nadd an additional not used parameter to the url you are loading, while referencing a cell for the value of that parameter.\nOnce you alter the content of that cell, the function reloads.\nexample:\nimportxml(""http://www.example.com/?noop="" & $A$1,""..."")\n\nunfortunately you cannot put a date calculating function into the referenced cell, that throws an error that this is not allowed.\n', '\nYou can also put each XML formula as a comment in the respective cells and record a macro to copy and paste it in the same cell. Later use the Scripts and then the Trigger functionality to schedule this macro.\n\n\n\n']",https://stackoverflow.com/questions/33872967/periodically-refresh-importxml-spreadsheet-function,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'list' object has no attribute 'get_attribute' while iterating through WebElements,"
I'm trying to use Python and Selenium to scrape multiple links on a web page. I'm using find_elements_by_xpath and I'm able to locate a list of elements but I'm having trouble changing the list that is returned to the actual href links. I know find_element_by_xpath works, but that only works for one element.
Here is my code:
path_to_chromedriver = 'path to chromedriver location'
browser = webdriver.Chrome(executable_path = path_to_chromedriver)

browser.get(""file:///path to html file"")

all_trails = []

#finds all elements with the class 'text-truncate trail-name' then 
#retrieve the a element
#this seems to be just giving us the element location but not the 
#actual location

find_href = browser.find_elements_by_xpath('//div[@class=""text truncate trail-name""]/a[1]')
all_trails.append(find_href)

print all_trails

This code is returning:
<selenium.webdriver.remote.webelement.WebElement 
(session=""dd178d79c66b747696c5d3750ea8cb17"", 
element=""0.5700549730549636-1663"")>, 
<selenium.webdriver.remote.webelement.WebElement 
(session=""dd178d79c66b747696c5d3750ea8cb17"", 
element=""0.5700549730549636-1664"")>,

I expect the all_trails array to be a list of links like: www.google.com, www.yahoo.com, www.bing.com.
I've tried looping through the all_trails list and running the get_attribute('href') method on the list but I get the error:

Does anyone have any idea how to convert the selenium WebElement's to href links?
Any help would be greatly appreciated :)
",16k,"
            4
        ","['\nLet us see what\'s happening in your code :\nWithout any visibility to the concerned HTML it seems the following line returns two WebElements in to the List find_href which are inturn are appended to the all_trails List :\nfind_href = browser.find_elements_by_xpath(\'//div[@class=""text truncate trail-name""]/a[1]\')\n\nHence when we print the List all_trails both the WebElements are printed. Hence No Error.\nAs per the error snap shot you have provided, you are trying to invoke get_attribute(""href"") method over a List which is Not Supported. Hence you see the error :\n\'List\' Object has no attribute \'get_attribute\'\n\nSolution :\nTo get the href attribute, we have to iterate over the List as follows :\nfind_href = browser.find_elements_by_xpath(\'//your_xpath\')\nfor my_href in find_href:\n    print(my_href.get_attribute(""href""))\n\n', '\nIf you have the following HTML:\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 1</a>\n</div>\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 2</a>\n</div>\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 3</a>\n</div>\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 4</a>\n</div>\n\nYour code should look like:\nall_trails = []\n\nall_links = browser.find_elements_by_css_selector("".text-truncate.trail-name>a"")\n\nfor link in all_links:\n\n    all_trails.append(link.get_attribute(""href""))\n\nWhere all_trails -- is a list of links (Link 1, Link 2 and so on).\nHope it helps you!\n', '\nUse it in Singular form as find_element_by_css_selector instead of using find_elements_by_css_selector as it returns many webElements in List. So you need to loop through each webElement to use Attribute.\n', '\nfind_href = browser.find_elements_by_xpath(\'//div[@class=""text truncate trail-name""]/a[1]\')\nfor i in find_href:\n      all_trails.append(i.get_attribute(\'href\'))\n\nget_attribute works on elements of that list, not list itself.\n', '\nget_attribute works on elements of that list only, not list itself. For eg :-\ndef fetch_img_urls(search_query: str):\n    driver.get(\'https://images.google.com/\')\n    search = driver.find_element(By.CLASS_NAME, ""gLFyf.gsfi"")\n    search.send_keys(search_query)\n    search.send_keys(Keys.RETURN)\n    links=[]\n    try:\n        time.sleep(5)\n        urls = driver.find_elements(By.CSS_SELECTOR,\'a.VFACy.kGQAp.sMi44c.lNHeqe.WGvvNb\')\n        for url in urls:\n            #print(url.get_attribute(""href""))\n            links.append(url.get_attribute(""href""))\n            print(links)\n\n    except Exception as e:\n        print(f\'error{e}\')\n        driver.quit()\n\n']",https://stackoverflow.com/questions/47735375/list-object-has-no-attribute-get-attribute-while-iterating-through-webelemen,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web scraping program cannot find element which I can see in the browser,"
I am trying to get the titles of the streams on https://www.twitch.tv/directory/game/Dota%202, using Requests and BeautifulSoup. I know that my search criteria are correct, yet my program does not find the elements I need.
Here is a screenshot showing the relevant part of the source code in the browser:

The HTML source as text:


<div class=""tw-media-card-meta__title"">
  <div class=""tw-c-text-alt"">
    <a class=""tw-full-width tw-interactive tw-link tw-link--button tw-link--hover-underline-none tw-link--inherit"" data-a-target=""preview-card-title-link"" href=""/weplayesport_en"">
      <div class=""tw-align-items-start tw-flex"">
        <h3 class=""tw-ellipsis tw-font-size-5"" title=""NAVI vs HellRaisers | BO5 | ODPixel &amp; S4 | WeSave! Charity Play"">NAVI vs HellRaisers | BO5 | ODPixel &amp; S4 | WeSave! Charity Play</h3>
      </div>
    </a>
  </div>
</div>


Here is my code:
import requests
from bs4 import BeautifulSoup

req = requests.get(""https://www.twitch.tv/directory/game/Dota%202"")

soup = BeautifulSoup(req.content, ""lxml"")

title_elems = soup.find_all(""h3"", attrs={""title"": True})

print(title_elems)

When I run it, title_elems is just the empty list ([]).
Why is my program not finding the elements?
",1k,"
            -8
        ","[""\nThe element you're interested in is dynamically generated, after the initial page load, which means that your browser executed JavaScript, made other network requests, etc. in order to build the page. Requests is just an HTTP library, and as such will not do those things.\nYou could use a tool like Selenium, or perhaps even analyze the network traffic for the data you need and make the requests directly.\n"", '\nThis is something you can do to grab the title of different games from that site using requests module:\nimport re\nimport requests\n\nurl = ""https://www.twitch.tv/directory/game/Dota%202""\nlink = ""https://gql.twitch.tv/gql""\n\nformdata = [{""operationName"":""DirectoryRoot_Directory"",""variables"":{""name"":""dota 2""},""extensions"":{""persistedQuery"":{""version"":1,""sha256Hash"":""9f4f6ae67f21ee50b454fcf048691107a52bfe7907ead73b9427398e343ca319""}}},{""operationName"":""Directory_DirectoryBanner"",""variables"":{""name"":""Dota 2""},""extensions"":{""persistedQuery"":{""version"":1,""sha256Hash"":""a64b0348103e054cbdb20c58de5fc05160da3f86c37c80263d7e6282f2577f54""}}},{""operationName"":""DirectoryPage_Game"",""variables"":{""name"":""dota 2"",""options"":{""sort"":""RELEVANCE"",""recommendationsContext"":{""platform"":""web""},""requestID"":""JIRA-VXP-2397"",""tags"":[]},""sortTypeIsRecency"":False,""limit"":30},""extensions"":{""persistedQuery"":{""version"":1,""sha256Hash"":""f2ac02ded21558ad8b747a0b63c0bb02b0533b6df8080259be10d82af63d50b3""}}}]\n\nwith requests.Session() as s:\n    s.headers[\'User-Agent\'] = ""Mozilla/5.0 (Windows NT 6.1; ) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36""\n    res = s.get(url)\n    s.headers[\'Client-Id\'] = re.findall(r""Client-ID\\"":\\""(.*?)\\"""",res.text)[0]\n    res = s.post(link,json=formdata)\n    for item in res.json()[2][\'data\'][\'game\'][\'streams\'][\'edges\']:\n        print(item[\'node\'][\'title\'])\n\nThe type of results you may get:\nPUBS 7.27 POG VIBE COOL FUN\nPlaying more 7.27\n7.27 Pubs :) Climb to 9k\n袙 薪芯胁褘泄 褋械蟹芯薪 褋 薪芯胁褘屑懈 锌芯斜械写邪屑懈 Custom Hero Chaos |  !discord \n袘芯斜褉褘泄 胁械褔械褉 !褉芯蟹褘谐褉褘褕 \nRERUN: Adroit vs Execration Game 1 - BTS Pro Series 2: SEA - Group Stage w/ MLP & johnxfire\n(RU) 袩芯胁褌芯褉 | EGB.com Arena of Blood \nNo Matter What\nmid 5000-6000(5600) (!褉芯蟹褘谐褉褘褕 泻邪卸写褘泄 写械薪褜)\nchegou a cam, pena que to parecendo um neanderthal\n\n']",https://stackoverflow.com/questions/60904786/web-scraping-program-cannot-find-element-which-i-can-see-in-the-browser,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web scraping with Java,"
I'm not able to find any good web scraping Java based API. The site which I need to scrape does not provide any API as well; I want to iterate over all web pages using some pageID and extract the HTML titles / other stuff in their DOM trees.
Are there ways other than web scraping?
",142k,"
            75
        ","['\njsoup\nExtracting the title is not difficult, and you have many options, search here on Stack Overflow for ""Java HTML parsers"". One of them is Jsoup.\nYou can navigate the page using DOM if you know the page structure, see\nhttp://jsoup.org/cookbook/extracting-data/dom-navigation\nIt\'s a good library and I\'ve used it in my last projects.\n', ""\nYour best bet is to use Selenium Web Driver since it\n\nProvides visual feedback to the coder (see your scraping in action, see where it stops)\n\nAccurate and Consistent as it directly controls the browser you use.\n\nSlow. Doesn't hit web pages like HtmlUnit does but sometimes you don't want to hit too fast.\nHtmlunit is fast but is horrible at handling Javascript and AJAX.\n\n\n"", '\nHTMLUnit can be used to do web scraping, it supports invoking pages, filling & submitting forms. I have used this in my project. It is good java library for web scraping.\nread here for more\n', '\nmechanize for Java would be a good fit for this, and as Wadjy Essam mentioned it uses JSoup for the HMLT. mechanize is a stageful HTTP/HTML client that supports navigation, form submissions, and page scraping.\nhttp://gistlabs.com/software/mechanize-for-java/ (and the GitHub here https://github.com/GistLabs/mechanize)\n', '\nThere is also Jaunt Java Web Scraping & JSON Querying - http://jaunt-api.com\n', '\nYou might look into jwht-scraper!\nThis is a complete scraping framework that has all the features a developper could expect from a web scraper :\n\nProxy support\nWarning Sign Support to detect captchas and more\nComplex link following features\nMultithreading\nVarious scraping delays when required\nRotating User-Agent\nRequest auto retry and HTTP redirections supports\nHTTP headers, cookies and more support\nGET and POST support\nAnnotation Configuration\nDetailed Scraping Metrics\nAsync handling of the scraper client\njwht-htmltopojo fully featured framework to map HTML to POJO\nCustom Input Format handling and built in JSON -> POJO mapping\nFull Exception Handling Control\nDetailed Logging with log4j\nPOJO injection\nCustom processing hooks\nEasy to use and well documented API\n\nIt works with (jwht-htmltopojo)[https://github.com/whimtrip/jwht-htmltopojo) lib which itsef uses Jsoup mentionned by several other people here.\nTogether they will help you built awesome scrapers mapping directly HTML to POJOs and bypassing any classical scraping problems in only a matter of minutes!\nHope this might help some people here!\nDisclaimer, I am the one who developed it, feel free to let me know your remarks!\n', '\nLook at an HTML parser such as TagSoup, HTMLCleaner or NekoHTML.\n', '\nIf you wish to automate scraping of large amount pages or data, then you could try Gotz ETL. \nIt is completely model driven like a real ETL tool. Data structure, task workflow and pages to scrape are defined with a set of XML definition files and no coding is required. Query can be written either using Selectors with JSoup or XPath with HtmlUnit.\n', '\nFor tasks of this type I usually use Crawller4j + Jsoup.\nWith crawler4j I download the pages from a domain, you can specify which ULR with a regular expression.\nWith jsoup, I ""parsed"" the html data you have searched for and downloaded with crawler4j.\nNormally you can also download data with jsoup, but Crawler4J makes it easier to find links.\nAnother advantage of using crawler4j is that it is multithreaded and you can configure the number of concurrent threads\nhttps://github.com/yasserg/crawler4j/wiki\n', '\nNormally I use selenium, which is software for testing automation.\nYou can control a browser through a webdriver, so you will not have problems with javascripts and it is usually not very detected if you use the full version. Headless browsers can be more identified.\n', '\nI am an engineer at WebScrapingAPI and I recommend your our product since we offer many features such as rendering javascript, CSS extracting, ip rotations, proxies and many others which you can find here , in our docs. Furthermore, we provide support for Java and we have Java examples for our features in order to ease your implementation process.\nOur API is very easy to use and beginner friendly. Take the following example where we want to render the javascript of httpbin.org. It can be done as simple as that:\nHttpResponse<String> response = Unirest.get(""https://api.webscrapingapi.com/v1?api_key=%7B%7Bapi_key%7D%7D&url=https%3A%2F%2Fhttpbin.org&render_js=1"")\n  .asString(); \n\nOr an example with using residential proxy:\nHttpResponse<String> response = Unirest.get(""https://api.webscrapingapi.com/v1?api_key=%7B%7Bapi_key%7D%7D&url=https%3A%2F%2Fhttpbin.org%2Fget&proxy_type=residential"")\n  .asString();\n\nOn top of that, in case you struggle with implementation or you encounter any issue with our services we have a very effective customer support which is ready to jump in and help right away.\n']",https://stackoverflow.com/questions/3202305/web-scraping-with-java,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python - Download Images from google Image search?,"
I want to download all Images of google image search using python . The code I am using seems to have some problem some times .My code is 
import os
import sys
import time
from urllib import FancyURLopener
import urllib2
import simplejson

# Define search term
searchTerm = ""parrot""

# Replace spaces ' ' in search term for '%20' in order to comply with request
searchTerm = searchTerm.replace(' ','%20')


# Start FancyURLopener with defined version 
class MyOpener(FancyURLopener): 
    version = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127     Firefox/2.0.0.11'
    myopener = MyOpener()

    # Set count to 0
    count= 0

    for i in range(0,10):
    # Notice that the start changes for each iteration in order to request a new set of   images for each loop
    url = ('https://ajax.googleapis.com/ajax/services/search/images?' + 'v=1.0& q='+searchTerm+'&start='+str(i*10)+'&userip=MyIP')
    print url
    request = urllib2.Request(url, None, {'Referer': 'testing'})
    response = urllib2.urlopen(request)

# Get results using JSON
    results = simplejson.load(response)
    data = results['responseData']
    dataInfo = data['results']

# Iterate for each result and get unescaped url
    for myUrl in dataInfo:
        count = count + 1
        my_url = myUrl['unescapedUrl']
        myopener.retrieve(myUrl['unescapedUrl'],str(count)+'.jpg')        

After downloading few pages I am getting an error as follows:
Traceback (most recent call last):
  File ""C:\Python27\img_google3.py"", line 37, in <module>
    dataInfo = data['results']
TypeError: 'NoneType' object has no attribute '__getitem__'

What to do ??????   
",130k,"
            43
        ","['\nI have modified my code. Now the code can download 100 images for a given query, and images are full high resolution that is original images are being downloaded.\nI am downloading the images using urllib2 & Beautiful soup\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport urllib2\nimport os\nimport cookielib\nimport json\n\ndef get_soup(url,header):\n    return BeautifulSoup(urllib2.urlopen(urllib2.Request(url,headers=header)),\'html.parser\')\n\n\nquery = raw_input(""query image"")# you can change the query for the image  here\nimage_type=""ActiOn""\nquery= query.split()\nquery=\'+\'.join(query)\nurl=""https://www.google.co.in/search?q=""+query+""&source=lnms&tbm=isch""\nprint url\n#add the directory for your image here\nDIR=""Pictures""\nheader={\'User-Agent\':""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36""\n}\nsoup = get_soup(url,header)\n\n\nActualImages=[]# contains the link for Large original images, type of  image\nfor a in soup.find_all(""div"",{""class"":""rg_meta""}):\n    link , Type =json.loads(a.text)[""ou""]  ,json.loads(a.text)[""ity""]\n    ActualImages.append((link,Type))\n\nprint  ""there are total"" , len(ActualImages),""images""\n\nif not os.path.exists(DIR):\n            os.mkdir(DIR)\nDIR = os.path.join(DIR, query.split()[0])\n\nif not os.path.exists(DIR):\n            os.mkdir(DIR)\n###print images\nfor i , (img , Type) in enumerate( ActualImages):\n    try:\n        req = urllib2.Request(img, headers={\'User-Agent\' : header})\n        raw_img = urllib2.urlopen(req).read()\n\n        cntr = len([i for i in os.listdir(DIR) if image_type in i]) + 1\n        print cntr\n        if len(Type)==0:\n            f = open(os.path.join(DIR , image_type + ""_""+ str(cntr)+"".jpg""), \'wb\')\n        else :\n            f = open(os.path.join(DIR , image_type + ""_""+ str(cntr)+"".""+Type), \'wb\')\n\n\n        f.write(raw_img)\n        f.close()\n    except Exception as e:\n        print ""could not load : ""+img\n        print e\n\ni hope this helps you \n', '\nThe Google Image Search API is deprecated, you need to use the Google Custom Search for what you want to achieve. To fetch the images you need to do this:\nimport urllib2\nimport simplejson\nimport cStringIO\n\nfetcher = urllib2.build_opener()\nsearchTerm = \'parrot\'\nstartIndex = 0\nsearchUrl = ""http://ajax.googleapis.com/ajax/services/search/images?v=1.0&q="" + searchTerm + ""&start="" + startIndex\nf = fetcher.open(searchUrl)\ndeserialized_output = simplejson.load(f)\n\nThis will give you 4 results, as JSON, you need to iteratively get the results by incrementing the startIndex in the API request.\nTo get the images you need to use a library like cStringIO.\nFor example, to access the first image, you need to do this:\nimageUrl = deserialized_output[\'responseData\'][\'results\'][0][\'unescapedUrl\']\nfile = cStringIO.StringIO(urllib.urlopen(imageUrl).read())\nimg = Image.open(file)\n\n', ""\nGoogle deprecated their API, scraping Google is complicated, so I would suggest using Bing API instead to automatically download images. The pip package bing-image-downloader allows you to easily download an arbitrary number of images to a directory with a single line of code.\nfrom bing_image_downloader import downloader\n\ndownloader.download(query_string, limit=100, output_dir='dataset', adult_filter_off=True, force_replace=False, timeout=60, verbose=True)\n\nGoogle is not so good, and Microsoft is not so evil\n"", '\nHere\'s my latest google image snarfer, written in Python, using Selenium and headless Chrome.\nIt requires python-selenium, the chromium-driver, and a module called retry from pip.\nLink: http://sam.aiki.info/b/google-images.py\nExample Usage:\ngoogle-images.py tiger 10 --opts isz:lt,islt:svga,itp:photo > urls.txt\nparallel=5\nuser_agent=""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36""\n(i=0; while read url; do wget -e robots=off -T10 --tries 10 -U""$user_agent"" ""$url"" -O`printf %04d $i`.jpg & i=$(($i+1)) ; [ $(($i % $parallel)) = 0 ] && wait; done < urls.txt; wait)\n\nHelp Usage:\n\n$ google-images.py --help\nusage: google-images.py [-h] [--safe SAFE] [--opts OPTS] query n\n\nFetch image URLs from Google Image Search.\n\npositional arguments:\n  query        image search query\n  n            number of images (approx)\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --safe SAFE  safe search [off|active|images]\n  --opts OPTS  search options, e.g.\n               isz:lt,islt:svga,itp:photo,ic:color,ift:jpg\n\nCode:\n#!/usr/bin/env python3\n\n# requires: selenium, chromium-driver, retry\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nimport selenium.common.exceptions as sel_ex\nimport sys\nimport time\nimport urllib.parse\nfrom retry import retry\nimport argparse\nimport logging\n\nlogging.basicConfig(stream=sys.stderr, level=logging.INFO)\nlogger = logging.getLogger()\nretry_logger = None\n\ncss_thumbnail = ""img.Q4LuWd""\ncss_large = ""img.n3VNCb""\ncss_load_more = "".mye4qd""\nselenium_exceptions = (sel_ex.ElementClickInterceptedException, sel_ex.ElementNotInteractableException, sel_ex.StaleElementReferenceException)\n\ndef scroll_to_end(wd):\n    wd.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")\n\n@retry(exceptions=KeyError, tries=6, delay=0.1, backoff=2, logger=retry_logger)\ndef get_thumbnails(wd, want_more_than=0):\n    wd.execute_script(""document.querySelector(\'{}\').click();"".format(css_load_more))\n    thumbnails = wd.find_elements_by_css_selector(css_thumbnail)\n    n_results = len(thumbnails)\n    if n_results <= want_more_than:\n        raise KeyError(""no new thumbnails"")\n    return thumbnails\n\n@retry(exceptions=KeyError, tries=6, delay=0.1, backoff=2, logger=retry_logger)\ndef get_image_src(wd):\n    actual_images = wd.find_elements_by_css_selector(css_large)\n    sources = []\n    for img in actual_images:\n        src = img.get_attribute(""src"")\n        if src.startswith(""http"") and not src.startswith(""https://encrypted-tbn0.gstatic.com/""):\n            sources.append(src)\n    if not len(sources):\n        raise KeyError(""no large image"")\n    return sources\n\n@retry(exceptions=selenium_exceptions, tries=6, delay=0.1, backoff=2, logger=retry_logger)\ndef retry_click(el):\n    el.click()\n\ndef get_images(wd, start=0, n=20, out=None):\n    thumbnails = []\n    count = len(thumbnails)\n    while count < n:\n        scroll_to_end(wd)\n        try:\n            thumbnails = get_thumbnails(wd, want_more_than=count)\n        except KeyError as e:\n            logger.warning(""cannot load enough thumbnails"")\n            break\n        count = len(thumbnails)\n    sources = []\n    for tn in thumbnails:\n        try:\n            retry_click(tn)\n        except selenium_exceptions as e:\n            logger.warning(""main image click failed"")\n            continue\n        sources1 = []\n        try:\n            sources1 = get_image_src(wd)\n        except KeyError as e:\n            pass\n            # logger.warning(""main image not found"")\n        if not sources1:\n            tn_src = tn.get_attribute(""src"")\n            if not tn_src.startswith(""data""):\n                logger.warning(""no src found for main image, using thumbnail"")          \n                sources1 = [tn_src]\n            else:\n                logger.warning(""no src found for main image, thumbnail is a data URL"")\n        for src in sources1:\n            if not src in sources:\n                sources.append(src)\n                if out:\n                    print(src, file=out)\n                    out.flush()\n        if len(sources) >= n:\n            break\n    return sources\n\ndef google_image_search(wd, query, safe=""off"", n=20, opts=\'\', out=None):\n    search_url_t = ""https://www.google.com/search?safe={safe}&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img&tbs={opts}""\n    search_url = search_url_t.format(q=urllib.parse.quote(query), opts=urllib.parse.quote(opts), safe=safe)\n    wd.get(search_url)\n    sources = get_images(wd, n=n, out=out)\n    return sources\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Fetch image URLs from Google Image Search.\')\n    parser.add_argument(\'--safe\', type=str, default=""off"", help=\'safe search [off|active|images]\')\n    parser.add_argument(\'--opts\', type=str, default="""", help=\'search options, e.g. isz:lt,islt:svga,itp:photo,ic:color,ift:jpg\')\n    parser.add_argument(\'query\', type=str, help=\'image search query\')\n    parser.add_argument(\'n\', type=int, default=20, help=\'number of images (approx)\')\n    args = parser.parse_args()\n\n    opts = Options()\n    opts.add_argument(""--headless"")\n    # opts.add_argument(""--blink-settings=imagesEnabled=false"")\n    with webdriver.Chrome(options=opts) as wd:\n        sources = google_image_search(wd, args.query, safe=args.safe, n=args.n, opts=args.opts, out=sys.stdout)\n\nmain()\n\n', '\nHaven\'t looked into your code but this is an example solution made with selenium to try to get 400 pictures from the search term\n# -*- coding: utf-8 -*-\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nimport json\nimport os\nimport urllib2\n\nsearchterm = \'vannmelon\' # will also be the name of the folder\nurl = ""https://www.google.co.in/search?q=""+searchterm+""&source=lnms&tbm=isch""\nbrowser = webdriver.Firefox()\nbrowser.get(url)\nheader={\'User-Agent\':""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36""}\ncounter = 0\nsuccounter = 0\n\nif not os.path.exists(searchterm):\n    os.mkdir(searchterm)\n\nfor _ in range(500):\n    browser.execute_script(""window.scrollBy(0,10000)"")\n\nfor x in browser.find_elements_by_xpath(""//div[@class=\'rg_meta\']""):\n    counter = counter + 1\n    print ""Total Count:"", counter\n    print ""Succsessful Count:"", succounter\n    print ""URL:"",json.loads(x.get_attribute(\'innerHTML\'))[""ou""]\n\n    img = json.loads(x.get_attribute(\'innerHTML\'))[""ou""]\n    imgtype = json.loads(x.get_attribute(\'innerHTML\'))[""ity""]\n    try:\n        req = urllib2.Request(img, headers={\'User-Agent\': header})\n        raw_img = urllib2.urlopen(req).read()\n        File = open(os.path.join(searchterm , searchterm + ""_"" + str(counter) + ""."" + imgtype), ""wb"")\n        File.write(raw_img)\n        File.close()\n        succounter = succounter + 1\n    except:\n            print ""can\'t get img""\n\nprint succounter, ""pictures succesfully downloaded""\nbrowser.close()\n\n', '\nAdding to Piees\'s answer, for downloading any number of images from the search results, we need to simulate a click on \'Show more results\' button after first 400 results are loaded.\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nimport os\nimport json\nimport urllib2\nimport sys\nimport time\n\n# adding path to geckodriver to the OS environment variable\n# assuming that it is stored at the same path as this script\nos.environ[""PATH""] += os.pathsep + os.getcwd()\ndownload_path = ""dataset/""\n\ndef main():\n    searchtext = sys.argv[1] # the search query\n    num_requested = int(sys.argv[2]) # number of images to download\n    number_of_scrolls = num_requested / 400 + 1 \n    # number_of_scrolls * 400 images will be opened in the browser\n\n    if not os.path.exists(download_path + searchtext.replace("" "", ""_"")):\n        os.makedirs(download_path + searchtext.replace("" "", ""_""))\n\n    url = ""https://www.google.co.in/search?q=""+searchtext+""&source=lnms&tbm=isch""\n    driver = webdriver.Firefox()\n    driver.get(url)\n\n    headers = {}\n    headers[\'User-Agent\'] = ""Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36""\n    extensions = {""jpg"", ""jpeg"", ""png"", ""gif""}\n    img_count = 0\n    downloaded_img_count = 0\n\n    for _ in xrange(number_of_scrolls):\n        for __ in xrange(10):\n            # multiple scrolls needed to show all 400 images\n            driver.execute_script(""window.scrollBy(0, 1000000)"")\n            time.sleep(0.2)\n        # to load next 400 images\n        time.sleep(0.5)\n        try:\n            driver.find_element_by_xpath(""//input[@value=\'Show more results\']"").click()\n        except Exception as e:\n            print ""Less images found:"", e\n            break\n\n    # imges = driver.find_elements_by_xpath(\'//div[@class=""rg_meta""]\') # not working anymore\n    imges = driver.find_elements_by_xpath(\'//div[contains(@class,""rg_meta"")]\')\n    print ""Total images:"", len(imges), ""\\n""\n    for img in imges:\n        img_count += 1\n        img_url = json.loads(img.get_attribute(\'innerHTML\'))[""ou""]\n        img_type = json.loads(img.get_attribute(\'innerHTML\'))[""ity""]\n        print ""Downloading image"", img_count, "": "", img_url\n        try:\n            if img_type not in extensions:\n                img_type = ""jpg""\n            req = urllib2.Request(img_url, headers=headers)\n            raw_img = urllib2.urlopen(req).read()\n            f = open(download_path+searchtext.replace("" "", ""_"")+""/""+str(downloaded_img_count)+"".""+img_type, ""wb"")\n            f.write(raw_img)\n            f.close\n            downloaded_img_count += 1\n        except Exception as e:\n            print ""Download failed:"", e\n        finally:\n            print\n        if downloaded_img_count >= num_requested:\n            break\n\n    print ""Total downloaded: "", downloaded_img_count, ""/"", img_count\n    driver.quit()\n\nif __name__ == ""__main__"":\n    main()\n\nFull code is here.  \n', '\nYou can also use Selenium with Python. Here is how:\nfrom selenium import webdriver\nimport urllib\nfrom selenium.webdriver.common.keys import Keys\ndriver = webdriver.Chrome(\'C:/Python27/Scripts/chromedriver.exe\')\nword=""apple""\nurl=""http://images.google.com/search?q=""+word+""&tbm=isch&sout=1""\ndriver.get(url)\nimageXpathSelector=\'//*[@id=""ires""]/table/tbody/tr[1]/td[1]/a/img\'\nimg=driver.find_element_by_xpath(imageXpathSelector)\nsrc=(img.get_attribute(\'src\'))\nurllib.urlretrieve(src, word+"".jpg"")\ndriver.close()\n\n(This code works on Python 2.7)\nPlease be informed that you should install Selenium package with \'pip install selenium\' and you should download chromedriver.exe from here\nOn the contrary of the other web scraping techniques, Selenium opens the browser and download the items because Selenium\'s mission is testing rather than scraping.\n', ""\nThis worked for me in Windows 10, Python 3.9.7:\n\npip install bing-image-downloader\n\nBelow code downloads 10 images of India from Bing search Engine to desired output folder:\nfrom bing_image_downloader import downloader\ndownloader.download('India', limit=10,  output_dir='dataset', adult_filter_off=True, force_replace=False, timeout=60, verbose=True)\n\nDocumentation: https://pypi.org/project/bing-image-downloader/\n"", '\nThis one as other code snippets have grown old and no longer worked for me. Downloads 100 images for each keyword, inspired from one of the solutions above.\nfrom bs4 import BeautifulSoup\nimport urllib2\nimport os\n\n\nclass GoogleeImageDownloader(object):\n    _URL = ""https://www.google.co.in/search?q={}&source=lnms&tbm=isch""\n    _BASE_DIR = \'GoogleImages\'\n    _HEADERS = {\n        \'User-Agent\':""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36""\n    }\n\n    def __init__(self):\n        query = raw_input(""Enter keyword to search images\\n"")\n        self.dir_name = os.path.join(self._BASE_DIR, query.split()[0])\n        self.url = self._URL.format(urllib2.quote(query)) \n        self.make_dir_for_downloads()\n        self.initiate_downloads()\n\n    def make_dir_for_downloads(self):\n        print ""Creating necessary directories""\n        if not os.path.exists(self._BASE_DIR):\n            os.mkdir(self._BASE_DIR)\n\n        if not os.path.exists(self.dir_name):\n            os.mkdir(self.dir_name)\n\n    def initiate_downloads(self):\n        src_list = []\n        soup = BeautifulSoup(urllib2.urlopen(urllib2.Request(self.url,headers=self._HEADERS)),\'html.parser\')\n        for img in soup.find_all(\'img\'):\n            if img.has_attr(""data-src""):\n                src_list.append(img[\'data-src\'])\n        print ""{} of images collected for downloads"".format(len(src_list))\n        self.save_images(src_list)\n\n    def save_images(self, src_list):\n        print ""Saving Images...""\n        for i , src in enumerate(src_list):\n            try:\n                req = urllib2.Request(src, headers=self._HEADERS)\n                raw_img = urllib2.urlopen(req).read()\n                with open(os.path.join(self.dir_name , str(i)+"".jpg""), \'wb\') as f:\n                    f.write(raw_img)\n            except Exception as e:\n                print (""could not save image"")\n                raise e\n\n\nif __name__ == ""__main__"":\n    GoogleeImageDownloader()\n\n', '\nI know this question is old, but I ran across it recently and none of the previous answers work anymore. So I wrote this script to gather images from google. As of right now it can download as many images as are available.\nhere is a github link to it as well https://github.com/CumminUp07/imengine/blob/master/get_google_images.py\nDISCLAIMER: DUE TO COPYRIGHT ISSUES, IMAGES GATHERED SHOULD ONLY BE USED FOR RESEARCH AND EDUCATION PURPOSES ONLY\nfrom bs4 import BeautifulSoup as Soup\nimport urllib2\nimport json\nimport urllib\n\n#programtically go through google image ajax json return and save links to list#\n#num_images is more of a suggestion                                            #  \n#it will get the ceiling of the nearest 100 if available                       #\ndef get_links(query_string, num_images):\n    #initialize place for links\n    links = []\n    #step by 100 because each return gives up to 100 links\n    for i in range(0,num_images,100):\n        url = \'https://www.google.com/search?ei=1m7NWePfFYaGmQG51q7IBg&hl=en&q=\'+query_string+\'\\\n        &tbm=isch&ved=0ahUKEwjjovnD7sjWAhUGQyYKHTmrC2kQuT0I7gEoAQ&start=\'+str(i)+\'\\\n        &yv=2&vet=10ahUKEwjjovnD7sjWAhUGQyYKHTmrC2kQuT0I7gEoAQ.1m7NWePfFYaGmQG51q7IBg.i&ijn=1&asearch=ichunk&async=_id:rg_s,_pms:s\'\n\n        #set user agent to avoid 403 error\n        request = urllib2.Request(url, None, {\'User-Agent\': \'Mozilla/5.0\'}) \n\n        #returns json formatted string of the html\n        json_string = urllib2.urlopen(request).read() \n\n        #parse as json\n        page = json.loads(json_string) \n\n        #html found here\n        html = page[1][1] \n\n        #use BeautifulSoup to parse as html\n        new_soup = Soup(html,\'lxml\')\n\n        #all img tags, only returns results of search\n        imgs = new_soup.find_all(\'img\')\n\n        #loop through images and put src in links list\n        for j in range(len(imgs)):\n            links.append(imgs[j][""src""])\n\n    return links\n\n#download images                              #\n#takes list of links, directory to save to    # \n#and prefix for file names                    #\n#saves images in directory as a one up number #\n#with prefix added                            #\n#all images will be .jpg                      #\ndef get_images(links,directory,pre):\n    for i in range(len(links)):\n        urllib.urlretrieve(links[i], ""./""+directory+""/""+str(pre)+str(i)+"".jpg"")\n\n#main function to search images                 #\n#takes two lists, base term and secondary terms #\n#also takes number of images to download per    #\n#combination                                    #\n#it runs every combination of search terms      #\n#with base term first then secondary            #\ndef search_images(base,terms,num_images):\n    for y in range(len(base)):\n        for x in range(len(terms)):\n            all_links = get_links(base[y]+\'+\'+terms[x],num_images)\n            get_images(all_links,""images"",x)\n\nif __name__ == \'__main__\':\n    terms = [""cars"",""numbers"",""scenery"",""people"",""dogs"",""cats"",""animals""]\n    base = [""animated""]\n    search_images(base,terms,1000)\n\n', '\nInstead of google image search, try other image searches like ecosia or bing.\nHere is a sample code for retrieving images from ecosia search engine.\nfrom bs4 import BeautifulSoup\nimport requests\nimport urllib\n\nuser_agent = \'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7\'\nheaders = {\'User-Agent\':user_agent} \nurls = [""https://www.ecosia.org/images?q=india%20pan%20card%20example""]\n#The url\'s from which the image is to be extracted.\nindex = 0\n\nfor url in urls:\n    request = urllib.request.Request(url,None,headers) #The assembled request\n    response = urllib.request.urlopen(request)\n    data = response.read() # Read the html result page\n\n    soup = BeautifulSoup(data, \'html.parser\')\n    \n    for link in soup.find_all(\'img\'):   \n        #The images are enclosed in \'img\' tag and the \'src\' contains the url of the image.\n        img_url = link.get(\'src\')\n        dest = str(index) + "".jpg""  #Destination to store the image.\n        try:\n            urllib.request.urlretrieve(img_url)\n            index += 1\n        except:\n            continue\n\nThe code works with google image search but it fails to retrieve images because google stores the images in encrypted format which is difficult to retrieve from the image url.\nThe solutions works as on 1-Feb-2021.\n', '\nOkay, so instead of coding this from you I am going to tell you what you\'re doing wrong and it might lead you in the right direction. Usually most modern websites render html dynamically via javascript and so if you simply send a GET request(with urllib/CURL/fetch/axios) you wont get what you usually see in the browser going to the same URL/web address. What you need is something that renders the javascript code to create the same HTML/webpage you see on your browser, you can use something like selenium gecko driver for firefox to do this and there python modules out there that let you do this.\nI hope this helps, if you still feel lost here\'s a simple script i wrote a while back to extract something similar from your google photos\nfrom selenium import webdriver\nimport re\nurl=""https://photos.app.goo.gl/xxxxxxx""\ndriver = webdriver.Firefox()\ndriver.get(url)\nregPrms=""^background-image\\:url\\(.*\\)$""\nregPrms=""^The.*Spain$""\nhtml = driver.page_source\n\nurls=re.findall(""(?P<url>https?://[^\\s\\""$]+)"", html)\n\nfin=[]\nfor url in urls:\n        if ""video-downloads"" in url:\n            fin.append(url)\nprint(""The Following ZIP contains all your pictures"")\nfor url in fin:\n        print(""-------------------"")\n        print(url)\n\n\n', '\n\nYou can achieve this using selenium as others mentioned it above.\nAlternatively, you can try using Google Images API from SerpApi. Check out the playground.\n\nCode and example. Fuction to download images was taken from this answer:\nimport os, time, shutil, httpx, asyncio\nfrom urllib.parse import urlparse\nfrom serpapi import GoogleSearch\n\n# https://stackoverflow.com/a/39217788/1291371\nasync def download_file(url):\n    print(f\'Downloading {url}\')\n\n    # https://stackoverflow.com/a/18727481/1291371\n    parsed_url = urlparse(url)\n    local_filename = os.path.basename(parsed_url.path)\n\n    os.makedirs(\'images\', exist_ok=True)\n\n    async with httpx.AsyncClient() as client:\n        async with client.stream(\'GET\', url) as response:\n            async with open(f\'images/{local_filename}\', \'wb\') as f:\n                await asyncio.to_thread(shutil.copyfileobj, response.raw, f)\n\n    return local_filename\n\nasync def main():\n    start = time.perf_counter()\n\n    params = {\n        ""engine"": ""google"",\n        ""ijn"": ""0"",\n        ""q"": ""lasagna"",\n        ""tbm"": ""isch"",\n        ""api_key"": os.getenv(""API_KEY""),\n    }\n\n    search = GoogleSearch(params)\n    results = search.get_dict()\n\n    download_files_tasks = [\n        download_file(image[\'original\']) for image in results[\'images_results\']\n    ]\n\n    await asyncio.gather(*download_files_tasks, return_exceptions=True)\n\n    print(\n        f""Downloaded {len(download_files_tasks)} images in {time.perf_counter() - start:0.4f} seconds"")\n\nasyncio.run(main())\n\n\nDisclaimer, I work for SerpApi.\n\n', ""\nThe one I used is :\nhttps://github.com/hellock/icrawler\nThis package is a mini framework of web crawlers. With modularization design, it is easy to use and extend. It supports media data like images and videos very well, and can also be applied to texts and another type of files. Scrapy is heavy and powerful, while icrawler is tiny and flexible.\ndef main():\n    parser = ArgumentParser(description='Test built-in crawlers')\n    parser.add_argument(\n        '--crawler',\n        nargs='+',\n        default=['google', 'bing', 'baidu', 'flickr', 'greedy', 'urllist'],\n        help='which crawlers to test')\n    args = parser.parse_args()\n    for crawler in args.crawler:\n        eval('test_{}()'.format(crawler))\n        print('\\n')\n\n""]",https://stackoverflow.com/questions/20716842/python-download-images-from-google-image-search,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wait page to load before getting data with requests.get in python 3,"
I have a page that i need to get the source to use with BS4, but the middle of the page takes 1 second(maybe less) to load the content, and requests.get catches the source of the page before the section loads, how can I wait a second before getting the data?
r = requests.get(URL + self.search, headers=USER_AGENT, timeout=5 )
    soup = BeautifulSoup(r.content, 'html.parser')
    a = soup.find_all('section', 'wrapper')

The page
<section class=""wrapper"" id=""resultado_busca"">

",94k,"
            38
        ","['\nIt doesn\'t look like a problem of waiting, it looks like the element is being created by JavaScript, requests can\'t handle dynamically generated elements by JavaScript. A suggestion is to use selenium together with PhantomJS to get the page source, then you can use BeautifulSoup for your parsing, the code shown below will do exactly that:\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\n\nurl = ""http://legendas.tv/busca/walking%20dead%20s03e02""\nbrowser = webdriver.PhantomJS()\nbrowser.get(url)\nhtml = browser.page_source\nsoup = BeautifulSoup(html, \'lxml\')\na = soup.find(\'section\', \'wrapper\')\n\nAlso, there\'s no need to use .findAll if you are only looking for one element only.\n', '\nI had the same problem, and none of the submitted answers really worked for me.\nBut after long research, I found a solution:\nfrom requests_html import HTMLSession\ns = HTMLSession()\nresponse = s.get(url)\nresponse.html.render()\n\nprint(response)\n# prints out the content of the fully loaded page\n# response can be parsed with for example bs4\n\nThe requests_html package (docs) is an official package, distributed by the Python Software Foundation. It has some additional JavaScript capabilities, like for example the ability to wait until the JS of a page has finished loading.\nThe package only supports Python Version 3.6 and above at the moment, so it might not work with another version.\n', ""\nI found a way to that !!!\nr = requests.get('https://github.com', timeout=(3.05, 27))\n\nIn this, timeout has two values, first one is to set session timeout and the second one is what you need. The second one decides after how much seconds the response is sent. You can calculate the time it takes to populate and then print the data out. \n"", '\nSelenium is good way to solve that, but accepted answer is quite deprecated. As @Seth mentioned in comments headless mode of Firefox/Chrome (or possibly other browsers) should be used instead of PhantomJS.\nFirst of all you need to download specific driver:\nGeckodriver for Firefox\nChromeDriver for Chrome\nNext you can add path to downloaded driver to system your PATH variable. But that\'s not necessary, you can also specify in code where executable lies.\nFirefox:\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\n\noptions = webdriver.FirefoxOptions()\noptions.add_argument(\'--headless\')\n# executable_path param is not needed if you updated PATH\nbrowser = webdriver.Firefox(options=options, executable_path=\'YOUR_PATH/geckodriver.exe\')\nbrowser.get(""http://legendas.tv/busca/walking%20dead%20s03e02"")\nhtml = browser.page_source\nsoup = BeautifulSoup(html, features=""html.parser"")\nprint(soup)\nbrowser.quit()\n\nSimilarly for Chrome:\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver    \n\noptions = webdriver.ChromeOptions()\noptions.add_argument(\'--headless\')\n# executable_path param is not needed if you updated PATH\nbrowser = webdriver.Chrome(options=options, executable_path=\'YOUR_PATH/chromedriver.exe\')\nbrowser.get(""http://legendas.tv/busca/walking%20dead%20s03e02"")\nhtml = browser.page_source\nsoup = BeautifulSoup(html, features=""html.parser"")\nprint(soup)\nbrowser.quit()\n\nIt\'s good to remember about browser.quit() to avoid hanging processes after code execution. If you worry that your code may fail before browser is disposed you can wrap it in try...except block and put browser.quit() in finally part to ensure it will be called.\nAdditionally, if part of source is still not loaded using that method, you can ask selenium to wait till specific element is present:\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as ec\nfrom selenium.webdriver.common.by import By\nfrom selenium.common.exceptions import TimeoutException\n\noptions = webdriver.FirefoxOptions()\noptions.add_argument(\'--headless\')\nbrowser = webdriver.Firefox(options=options, executable_path=\'YOUR_PATH/geckodriver.exe\')\n\ntry:\n    browser.get(""http://legendas.tv/busca/walking%20dead%20s03e02"")\n    timeout_in_seconds = 10\n    WebDriverWait(browser, timeout_in_seconds).until(ec.presence_of_element_located((By.ID, \'resultado_busca\')))\n    html = browser.page_source\n    soup = BeautifulSoup(html, features=""html.parser"")\n    print(soup)\nexcept TimeoutException:\n    print(""I give up..."")\nfinally:\n    browser.quit()\n\nIf you\'re interested in other drivers than Firefox or Chrome check docs.\n', '\nIn Python 3, Using the module urllib in practice works better when loading dynamic webpages than the requests module. \ni.e\nimport urllib.request\ntry:\n    with urllib.request.urlopen(url) as response:\n\n        html = response.read().decode(\'utf-8\')#use whatever encoding as per the webpage\nexcept urllib.request.HTTPError as e:\n    if e.code==404:\n        print(f""{url} is not found"")\n    elif e.code==503:\n        print(f\'{url} base webservices are not available\')\n        ## can add authentication here \n    else:\n        print(\'http error\',e)\n\n', '\nJust to list my way of doing it, maybe it can be of value for someone:\nmax_retries = # some int\nretry_delay = # some int\nn = 1\nready = 0\nwhile n < max_retries:\n  try:\n     response = requests.get(\'https://github.com\')\n     if response.ok:\n        ready = 1\n        break\n  except requests.exceptions.RequestException:\n     print(""Website not availabe..."")\n  n += 1\n  time.sleep(retry_delay)\n\nif ready != 1:\n  print(""Problem"")\nelse:\n  print(""All good"")\n\n']",https://stackoverflow.com/questions/45448994/wait-page-to-load-before-getting-data-with-requests-get-in-python-3,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""SSL: certificate_verify_failed"" error when scraping https://www.thenewboston.com/","
So I started learning Python recently using ""The New Boston's"" videos on youtube, everything was going great until I got to his tutorial of making a simple web crawler. While I understood it with no problem, when I run the code I get errors all seemingly based around ""SSL: CERTIFICATE_VERIFY_FAILED."" I've been searching for an answer since last night trying to figure out how to fix it, it seems no one else in the comments on the video or on his website are having the same problem as me and even using someone elses code from his website I get the same results. I'll post the code from the one I got from the website as it's giving me the same error and the one I coded is a mess right now.
import requests
from bs4 import BeautifulSoup

def trade_spider(max_pages):
    page = 1
    while page <= max_pages:
        url = ""https://www.thenewboston.com/forum/category.php?id=15&orderby=recent&page="" + str(page) #this is page of popular posts
        source_code = requests.get(url)
        # just get the code, no headers or anything
        plain_text = source_code.text
        # BeautifulSoup objects can be sorted through easy
        for link in soup.findAll('a', {'class': 'index_singleListingTitles'}): #all links, which contains """" class='index_singleListingTitles' """" in it.
            href = ""https://www.thenewboston.com/"" + link.get('href')
            title = link.string # just the text, not the HTML
            print(href)
            print(title)
            # get_single_item_data(href)
    page += 1
trade_spider(1)

The full error is: ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:645)
I apologize if this is  a dumb question, I'm still new to programming but I seriously can't figure this out, I was thinking about just skipping this tutorial but it's bothering me not being able to fix this, thanks!
",95k,"
            28
        ","[""\nThe problem is not in your code but in the web site you are trying to access. When looking at the analysis by SSLLabs you will note:\n\nThis server's certificate chain is incomplete. Grade capped to B.\n\nThis means that the server configuration is wrong and that not only python but several others will have problems with this site. Some desktop browsers work around this configuration problem by trying to load the missing certificates from the internet or fill in with cached certificates. But other browsers or applications will fail too, similar to python.\nTo work around the broken server configuration you might explicitly extract the missing certificates and add them to you trust store. Or you might give the certificate as trust inside the verify argument. From the documentation:\n\nYou can pass verify the path to a CA_BUNDLE file or directory with\ncertificates of trusted CAs:\n>>> requests.get('https://github.com', verify='/path/to/certfile') \n\nThis list of trusted CAs can also be specified through the\nREQUESTS_CA_BUNDLE environment variable.\n\n"", '\nYou can tell requests not to verify the SSL certificate:\n>>> url = ""https://www.thenewboston.com/forum/category.php?id=15&orderby=recent&page=1""\n>>> response = requests.get(url, verify=False)\n>>> response.status_code\n200\n\nSee more in the requests doc\n', '\nYou are probably missing the stock certificates in your system. E.g. if running on Ubuntu, check that ca-certificates package is installed.\n', ""\nif you want to use the Python dmg installer, you also have to read Python 3's ReadMe and run the bash command to get new certificates.\nTry running \n/Applications/Python\\ 3.6/Install\\ Certificates.command\n\n"", '\nIt\'s worth shedding a bit more ""hands-on"" light about what happens here, adding upon @Steffen Ullrich\'s answer here and elsewhere:\n\nurllib and 鈥淪SL: CERTIFICATE_VERIFY_FAILED鈥?Error\nPython Urllib2 SSL error (a very detailed answer)\n\nNotes:\n\nI\'ll use another website than the OP, because the OP\'s website currently has no issues.\nI used Ubunto to run the following commands (curl and openssl). I tried running curl on my Windows 10, but got different, unhelpful output.\n\nThe error experienced by the OP can be ""reproduced"" by using the following curl command:\ncurl -vvI https://www.vimmi.net\n\nWhich outputs (note the last line):\n* TCP_NODELAY set\n* Connected to www.vimmi.net (82.80.192.7) port 443 (#0)\n* ALPN, offering h2\n* ALPN, offering http/1.1\n* successfully set certificate verify locations:\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: /etc/ssl/certs\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\n* TLSv1.2 (IN), TLS handshake, Certificate (11):\n* TLSv1.2 (OUT), TLS alert, Server hello (2):\n* SSL certificate problem: unable to get local issuer certificate\n* stopped the pause stream!\n* Closing connection 0\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\n\nNow let\'s run it with the --insecure flag, which will display the problematic certificate:\ncurl --insecure -vvI https://www.vimmi.net\n\nOutputs (note the last two lines):\n* Rebuilt URL to: https://www.vimmi.net/\n*   Trying 82.80.192.7...\n* TCP_NODELAY set\n* Connected to www.vimmi.net (82.80.192.7) port 443 (#0)\n* ALPN, offering h2\n* ALPN, offering http/1.1\n* successfully set certificate verify locations:\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: /etc/ssl/certs\n* [...]\n* Server certificate:\n*  subject: OU=Domain Control Validated; CN=vimmi.net\n*  start date: Aug  5 15:43:45 2019 GMT\n*  expire date: Oct  4 16:16:12 2020 GMT\n*  issuer: C=US; ST=Arizona; L=Scottsdale; O=GoDaddy.com, Inc.; OU=http://certs.godaddy.com/repository/; CN=Go Daddy Secure Certificate Authority - G2\n*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.\n\nThe same result can be seen using openssl, which is worth mentioning because it\'s used internally by python:\necho | openssl s_client -connect vimmi.net:443\n\nOutputs:\nCONNECTED(00000005)\ndepth=0 OU = Domain Control Validated, CN = vimmi.net\nverify error:num=20:unable to get local issuer certificate\nverify return:1\ndepth=0 OU = Domain Control Validated, CN = vimmi.net\nverify error:num=21:unable to verify the first certificate\nverify return:1\n---\nCertificate chain\n 0 s:OU = Domain Control Validated, CN = vimmi.net\n   i:C = US, ST = Arizona, L = Scottsdale, O = ""GoDaddy.com, Inc."", OU = http://certs.godaddy.com/repository/, CN = Go Daddy Secure Certificate Authority - G2\n---\nServer certificate\n-----BEGIN CERTIFICATE-----\n[...]\n-----END CERTIFICATE-----\n[...]\n---\nDONE\n\nSo why both curl and openssl can\'t verify the certificate Go Daddy issued for that website?\nWell, to ""verify a certificate"" (to use openssl\'s error message terminology) means to verify that the certificate contains a trusted source signature (put differently: the certificate was signed by a trusted source), thus verifying vimmi.net identity (""identity"" here strictly means that ""the public key contained in the certificate belongs to the person, organization, server or other entity noted in the certificate"").\nA source is ""trusted"" if we can establish its ""chain of trust"", with the following properties:\n\n\nThe Issuer of each certificate (except the last one) matches the Subject of the next certificate in the list\nEach certificate (except the last one) is signed by the secret key corresponding to the next certificate in the chain (i.e. the signature\nof one certificate can be verified using the public key contained in\nthe following certificate)\nThe last certificate in the list is a trust anchor: a certificate that you trust because it was delivered to you by some trustworthy\nprocedure\n\n\nIn our case, the issuer is ""Go Daddy Secure Certificate Authority - G2"". That is, the entity named ""Go Daddy Secure Certificate Authority - G2"" signed the certificate, so it\'s supposed to be a trusted source.\nTo establish this entity\'s trustworthiness, we have 2 options:\n\nAssume that ""Go Daddy Secure Certificate Authority - G2"" is a ""trust anchor"" (see listing 3 above). Well, it turns out that curl and openssl try to act upon this assumption: they searched that entity\'s certificate on their default paths (called CA paths), which are:\n\nfor curl, it\'s /etc/ssl/certs.\nfor openssl, it\'s /use/lib/ssl (run openssl version -a to see that).\n\n\n\nBut that certificate wasn\'t found, leaving us with a second option:\n\nFollow steps 1 and 2 listed above; in order to do that, we need to get the certificate issued for that entity.\nThis can be achieved by downloading it from its source, or using the browser.\n\nfor example, go to vimmi.net using Chrome, click the padlock > ""Certificate"" > ""Certification Path"" tab, select the entity > ""View Certificate"", then in the opened window go to ""Details"" tab > ""Copy to File"" > Base-64 encoded > save the file)\n\n\n\nGreat! Now that we have that certificate (which can be in whatever file format: cer, pem, etc.; you can even save it as a txt file), let\'s tell curl to use it:\ncurl --cacert test.cer https://vimmi.net\n\nGoing back to Python\nOnce we have:\n\n""Go Daddy Secure Certificate Authority - G2"" certificate\n""Go Daddy Root Certificate Authority - G2"" certificate (wasn\'t mentioned above, but can be achieved in a similar way).\n\nWe need to copy their contents into a single file, let\'s call it combined.cer, and let\'s put it in the current directory. Then, simply:\nimport requests\n\nres = requests.get(""https://vimmi.net"", verify=""./combined.cer"")\nprint (res.status_code) # 200\n\n\nBTW, ""Go Daddy Root Certificate Authority - G2"" is listed as a trusted authority by browsers and various tools; that\'s why we didn\'t have to specify it for curl.\n\nFurther reading:\n\nhow are ssl certificates verified, especially @ychaouche image.\nThe First Few Milliseconds of an HTTPS Connection\nWikipedia: Public key certificate, Certificate authority\nNice video: Basics of Certificate Chain Validation.\nHelpful SE answers that focus on certificate signature terminology: 1, 2, 3.\nCertificates in relation to Man-In-The-Middle attack: 1, 2.\nThe most dangerous code in the world: validating SSL certificates in non-browser software\n\n', '\nI\'m posting this as an answer because I\'ve gotten past your issue thus far, but there\'s still issues in your code (which when fixed, I can update).\nSo long story short: you could be using an old version of requests or the ssl certificate should be invalid. There\'s more information in this SO question: Python requests ""certificate verify failed""\nI\'ve updated the code into my own bsoup.py file:\n#!/usr/bin/env python3\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef trade_spider(max_pages):\n    page = 1\n    while page <= max_pages:\n        url = ""https://www.thenewboston.com/forum/category.php?id=15&orderby=recent&page="" + str(page) #this is page of popular posts\n        source_code = requests.get(url, timeout=5, verify=False)\n        # just get the code, no headers or anything\n        plain_text = source_code.text\n        # BeautifulSoup objects can be sorted through easy\n        for link in BeautifulSoup.findAll(\'a\', {\'class\': \'index_singleListingTitles\'}): #all links, which contains """" class=\'index_singleListingTitles\' """" in it.\n            href = ""https://www.thenewboston.com/"" + link.get(\'href\')\n            title = link.string # just the text, not the HTML\n            print(href)\n            print(title)\n            # get_single_item_data(href)\n\n        page += 1\n\nif __name__ == ""__main__"":\n    trade_spider(1)\n\nWhen I run the script, it gives me this error:\nhttps://www.thenewboston.com/forum/category.php?id=15&orderby=recent&page=1\nTraceback (most recent call last):\n  File ""./bsoup.py"", line 26, in <module>\n    trade_spider(1)\n  File ""./bsoup.py"", line 16, in trade_spider\n    for link in BeautifulSoup.findAll(\'a\', {\'class\': \'index_singleListingTitles\'}): #all links, which contains """" class=\'index_singleListingTitles\' """" in it.\n  File ""/usr/local/lib/python3.4/dist-packages/bs4/element.py"", line 1256, in find_all\n    generator = self.descendants\nAttributeError: \'str\' object has no attribute \'descendants\'\n\nThere\'s an issue somewhere with your findAll method. I\'ve used both python3 and python2, wherein python2 reports this:\nTypeError: unbound method find_all() must be called with BeautifulSoup instance as first argument (got str instance instead)\n\nSo it looks like you\'ll need to fix up that method before you can continue\n', '\nI spent several hours trying to fix some Python and update certs on a VM.  In my case I was working against a server that someone else had set up.  It turned out that the wrong cert had been uploaded to the server.  I found this command on another SO answer.\nroot@ubuntu:~/cloud-tools# openssl s_client -connect abc.def.com:443\nCONNECTED(00000005)\ndepth=0 OU = Domain Control Validated, CN = abc.def.com\nverify error:num=20:unable to get local issuer certificate\nverify return:1\ndepth=0 OU = Domain Control Validated, CN = abc.def.com\nverify error:num=21:unable to verify the first certificate\nverify return:1\n---\nCertificate chain\n0 s:OU = Domain Control Validated, CN = abc.def.com\n   i:C = US, ST = Arizona, L = Scottsdale, O = ""GoDaddy.com, Inc."", OU = http://certs.godaddy.com/repository/, CN = Go Daddy Secure Certificate Authority - G2\n\n']",https://stackoverflow.com/questions/34503206/ssl-certificate-verify-failed-error-when-scraping-https-www-thenewboston-co,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading dynamically generated web pages using python,"
I am trying to scrape a web site using python and beautiful soup. I encountered that in some sites, the image links although seen on the browser is cannot be seen in the source code. However on using Chrome Inspect or Fiddler, we can see the the corresponding codes. 
What I see in the source code is:
<div id=""cntnt""></div>

But on Chrome Inspect, I can see a whole bunch of HTML\CSS code generated within this div class. Is there a way to load the generated content also within python? I am using the regular urllib in python and I am able to get the source but without the generated part.
I am not a web developer hence I am not able to express the behaviour in better terms. Please feel free to clarify if my question seems vague !
",42k,"
            24
        ","['\nYou need JavaScript Engine to parse and run JavaScript code inside the page.\nThere are a bunch of headless browsers that can help you\nhttp://code.google.com/p/spynner/\nhttp://phantomjs.org/\nhttp://zombie.labnotes.org/\nhttp://github.com/ryanpetrello/python-zombie\nhttp://jeanphix.me/Ghost.py/\nhttp://webscraping.com/blog/Scraping-JavaScript-webpages-with-webkit/\n', '\nThe Content of the website may be generated after load via javascript, In order to obtain the generated script via python refer to this answer\n', '\nA regular scraper gets just the HTML document. To get any content generated by JavaScript logic, you rather need a Headless browser that would also generate the DOM, load and run the scripts like a regular browser would. The Wikipedia article and some other pages on the Net have lists of those and their capabilities.\nKeep in mind when choosing that some previously major products of those are abandoned now.\n', '\nTRY THIS FIRST!\nPerhaps the data technically could be in the javascript itself and all this javascript engine business is needed. (Some GREAT links here!)\nBut from experience, my first guess is that the JS is pulling the data in via an ajax request. If you can get your program simulate that, you\'ll probably get everything you need handed right to you without any tedious parsing/executing/scraping involved!\nIt will take a little detective work though. I suggest turning on your network traffic logger (such as ""Web Developer Toolbar"" in Firefox) and then visiting the site. Focus your attention attention on any/all XmlHTTPRequests.  The data you need should be found somewhere in one of these responses, probably in the middle of some JSON text.\nNow, see if you can re-create that request and get the data directly.  (NOTE: You may have to set the User-Agent of your request so the server thinks you\'re a ""real"" web browser.)\n']",https://stackoverflow.com/questions/13960567/reading-dynamically-generated-web-pages-using-python,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Detect when a web page is loaded without using sleep,"
I am creating a VB script on windows which opens a site in IE. What I want: Detect when the web page is loaded and display a message. I achieved this by using sleep (WScript.Sleep) for approx. seconds when the site gets loaded. However, the site pops up user name, password in the midway. Only when the user enter credentials, it finishes loading the page. So I don't want to use ""sleep"" for approx seconds, instead an exact function or a way to detect that the page got loaded. I checked on line and tried using Do While loop, onload, onclick functions, but nothing works. To simplify, even if I write a script to open a site like yahoo and detect, display a message ""Hi"" when the page is loaded: It doesn't work without using sleep (WScript.Sleep).
",15k,"
            5
        ","['\nTry conventional method:\nSet objIE = CreateObject(""InternetExplorer.Application"")\nobjIE.Visible = True\nobjIE.Navigate ""https://www.yahoo.com/""\nDo While objIE.ReadyState <> 4\n    WScript.Sleep 10\nLoop\n\' your code here\n\' ...\n\nUPD: this one should check for errors:\nSet objIE = CreateObject(""InternetExplorer.Application"")\nobjIE.Visible = True\nobjIE.Navigate ""https://www.yahoo.com/""\nOn Error Resume Next\nDo \n    If objIE.ReadyState = 4 Then\n        If Err = 0 Then\n            Exit Do\n        Else\n            Err.Clear\n        End If\n    End If\n    WScript.Sleep 10\nLoop\nOn Error Goto 0\n\' your code here\n\' ...\n\nUPD2: You wrote that IE gets disconnected as the login pop-up comes in, hypothetically there is a way to catch disconnection, and then get IE instance again. Note this is ""abnormal programming"" :) I hope this helps:\nOption Explicit\nDim objIE, strSignature, strInitType\n\nSet objIE = CreateObject(""InternetExplorer.Application"") \' create IE instance\nobjIE.Visible = True\nstrSignature = Left(CreateObject(""Scriptlet.TypeLib"").GUID, 38) \' generate uid\nobjIE.putproperty ""marker"", strSignature \' tokenize the instance\nstrInitType = TypeName(objIE) \' get typename\nobjIE.Navigate ""https://www.yahoo.com/""\nMsgBox ""Initial type = "" & TypeName(objIE) \' for visualisation\n\nOn Error Resume Next\nDo While TypeName(objIE) = strInitType \' wait until typename changes (ActveX disconnection), may cause error 800A000E if not within OERN\n    WScript.Sleep 10\nLoop\nMsgBox ""Changed type = "" & TypeName(objIE) \' for visualisation\n\nSet objIE = Nothing \' excessive statement, just for clearance\nDo\n    For Each objIE In CreateObject(""Shell.Application"").Windows \' loop through all explorer windows to find tokenized instance\n        If objIE.getproperty(""marker"") = strSignature Then \' our instance found\n            If TypeName(objIE) = strInitType Then Exit Do \' may be excessive type check\n        End If\n    Next\n    WScript.Sleep 10\nLoop\nMsgBox ""Found type = "" & TypeName(objIE) \' for visualisation\nOn Error GoTo 0\n\nDo While objIE.ReadyState <> 4 \' conventional wait if instance not ready\n    WScript.Sleep 10\nLoop\n\nMsgBox ""Title = "" & objIE.Document.Title \' for visualisation\n\nYou can get all text nodes, links etc. from DOM, as follows:\nOption Explicit\nDim objIE, colTags, strResult, objTag, objChild, arrResult\n\nSet objIE = CreateObject(""InternetExplorer.Application"")\nobjIE.Visible = True\nobjIE.Navigate ""https://www.yahoo.com/""\n\nDo While objIE.ReadyState <> 4\n    WScript.Sleep 10\nLoop\n\nSet colTags = objIE.Document.GetElementsByTagName(""a"")\nstrResult = ""Total "" & colTags.Length & "" DOM Anchor Nodes:"" & vbCrLf\nFor Each objTag In colTags\n    strResult = strResult & objTag.GetAttribute(""href"") & vbCrLf\nNext\nShowInNotepad strResult\n\nSet colTags = objIE.Document.GetElementsByTagName(""*"")\narrResult = Array()\nFor Each objTag In colTags\n    For Each objChild In objTag.ChildNodes\n        If objChild.NodeType = 3 Then\n            ReDim Preserve arrResult(UBound(arrResult) + 1)\n            arrResult(UBound(arrResult)) = objChild.NodeValue\n        End If\n    Next\nNext\nstrResult = ""Total "" & colTags.Length & "" DOM object nodes + total "" & UBound(arrResult) + 1 & "" #text nodes:"" & vbCrLf\nstrResult = strResult & Join(arrResult, vbCrLf)\nShowInNotepad strResult\n\nobjIE.Quit\n\nSub ShowInNotepad(strToFile)\n    Dim strTempPath\n    With CreateObject(""Scripting.FileSystemObject"")\n        strTempPath = CreateObject(""WScript.Shell"").ExpandEnvironmentStrings(""%TEMP%"") & ""\\"" & .gettempname\n        With .CreateTextFile(strTempPath, True, True)\n            .WriteLine (strToFile)\n            .Close\n        End With\n        CreateObject(""WScript.Shell"").Run ""notepad.exe "" & strTempPath, 1, True\n        .DeleteFile (strTempPath)\n    End With\nEnd Sub\n\nAlso look get text data\nUPD3: I want to place here additional check if webpage loading and initialization are completed:\n\' ...\n\' Navigating to some url\nobjIE.Navigate strUrl\n\' Wait for IE ready\nDo While objIE.ReadyState <> 4 Or objIE.Busy\n    WScript.Sleep 10\nLoop\n\' Wait for document complete\nDo While objIE.Document.ReadyState <> ""complete""\n    WScript.Sleep 10\nLoop\n\' Processing loaded webpage code\n\' ...\n\nUPD4: There are some cases when you need to track if a target node have been created in the document (usually it\'s necessary if you get Object required error while attempting to access the node by .getElementById, etc.):\nIf the page uses AJAX (loaded page source HTML doesn\'t contain target node, active content like JavaScript creates it dynamically), there is the example in the below snippet of a page, showing how that could look like. The text node 5.99 might be created after the page was completely loaded, and some other requests to a server for extra data to be displayed have taken a place:\n...\n<td class=""price-label"">\n    <span id=""priceblock"" class=""price-big color"">\n        5.99\n    </span>\n</td>\n...\n\nOr if you are loading e. g. Google search result page and waiting for Next button is appeared (especially, if you invoked .click method on the previous page), or loading some page with login web form and waiting for username input field like <input name=""userID"" id=""userID"" type=""text"" maxlength=""24"" required="""" placeholder=""Username"" autofocus="""">.\nThe below code allows to make an additional check if the target node is accessible:\nWith objIE\n    \' Navigating to some url\n    .Navigate strUrl\n    \' Wait for IE ready\n    Do While .ReadyState <> 4 Or .Busy\n        WScript.Sleep 10\n    Loop\n    \' Wait for document complete\n    Do While .Document.ReadyState <> ""complete""\n        WScript.Sleep 10\n    Loop\n    \' Wait for target node created\n    Do While TypeName(.Document.getElementById(""userID"")) = ""Null""\n        WScript.Sleep 10\n    Loop\n    \' Processing target node\n    .Document.getElementById(""userID"").Value = ""myusername""\n    \' ...\n    \'\nEnd With\n\n', '\nThe Following Check by Element Solved for me : \nFunction waitLoadByElement(p_ElementName)\n\nDo While IE.ReadyState <> 4 Or IE.Busy\n        WScript.Sleep 1000\n    Loop\n\n    Do While IE.Document.ReadyState <> ""complete""\n        WScript.Sleep 1000\n    Loop\n\n       \' This is the interesting part\n\n    Do While (instr(IE.document.getElementsByTagName(""body"")(0).InnerHTML,p_ElementName) < 1 )\n    v_counter = v_counter + 1\n\n        WScript.Sleep 1000\n    Loop\n    On Error GoTo 0\n\n    if v_counter > 0 then\n        MyEcho ""[ Waited Object to Load ] : "" & v_counter & "" - Seconds""\n    end if\n\nEnd Function\n\n']",https://stackoverflow.com/questions/23232488/detect-when-a-web-page-is-loaded-without-using-sleep,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problems submitting a login form with Jsoup,"
For some reason this code will not let me into the website when I use the correct login information. The System.out.println posts the code of the login page, indicating my code did not work. Can someone tell me what I'm forgetting or what's wrong with it?
public void connect() {

    try {
        Connection.Response loginForm = Jsoup.connect(""https://www.capitaliq.com/CIQDotNet/Login.aspx/login.php"")
                .method(Connection.Method.GET)
                .execute();

        org.jsoup.nodes.Document document = Jsoup.connect(""https://www.capitaliq.com/CIQDotNet/Login.aspx/authentication.php"")
                .data(""cookieexists"", ""false"")
                .data(""username"", ""myUsername"")
                .data(""password"", ""myPassword"")
                .cookies(loginForm.cookies())
                .post();
        System.out.println(document);
    } catch (IOException ex) {
        Logger.getLogger(WebCrawler.class.getName()).log(Level.SEVERE, null, ex);
    }
}

",2k,"
            1
        ","['\nBesides the username, password and the cookies, the site requeires two additional values for the login - VIEWSTATE and EVENTVALIDATION.\nYou can get them from the response of the first Get request, like this -  \nDocument doc = loginForm.parse();\nElement e = doc.select(""input[id=__VIEWSTATE]"").first();\nString viewState = e.attr(""value"");\ne = doc.select(""input[id=__EVENTVALIDATION]"").first();\nString eventValidation = e.attr(""value"");\n\nAnd add it after the password (the order doesn\'t really matter) -  \norg.jsoup.nodes.Document document = (org.jsoup.nodes.Document) Jsoup.connect(""https://www.capitaliq.com/CIQDotNet/Login.aspx/authentication.php"").userAgent(""Mozilla/5.0"")               \n            .data(""myLogin$myUsername"", ""MyUsername"")\n            .data(""myLogin$myPassword, ""MyPassword"")\n            .data(""myLogin$myLoginButton.x"", ""22"")                   \n            .data(""myLogin$myLoginButton.y"", ""8"")\n            .data(""__VIEWSTATE"", viewState)\n            .data(""__EVENTVALIDATION"", eventValidation)\n            .cookies(loginForm.cookies())\n            .post();\n\nI would also add the userAgent field to both requests - some sites test it and send different pages to different clients, so if you would like to get the same response as you get with your browser, add to the requests .userAgent(""Mozilla/5.0"") (or whatever browser you\'re using).\nEdit\nThe userName\'s field name is myLogin$myUsername, the password is myLogin$myPassword and the Post request also contains data about the login button. Ican\'t test it, because I don\'t have user at that site, but I believe it will work. Hope this solves your problem.  \nEDIT 2\nTo enable the remember me field during login, add this line to the post request:  \n.data(""myLogin$myEnableAutoLogin"", ""on"")\n\n']",https://stackoverflow.com/questions/31871801/problems-submitting-a-login-form-with-jsoup,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I get the CSS Selector in Chrome?,"
I want to be able to select/highlight an element on the page and find its selector like this:

div.firstRow
  div.priceAvail>div>div.PriceCompare>div.BodyS

I know you can see the selection on the bottom after doing an inspect element, but how can I copy this path to the clipboard? In Firebug I think you can do this, but don't see a way to do this using the Chrome Developer Tools and search for an extension did not turn-up anything.
This is what I am trying to do for more reference:
http://asciicasts.com/episodes/173-screen-scraping-with-scrapi
",125k,"
            54
        ","['\n\nIf Chrome Dev tools if you select the element in the source pane and right click, then you will see the ""Copy CSS Path"" option.\nIn newer versions of Chrome, this is (right-click) > Copy > Copy selector.\n You can also get the XPath with (right-click) > Copy > Copy XPath\n', '\nAlthough not an extension, I did find a bookmarklet called Selector Gadget that does exactly what I was looking for.\n', '\nThe workflow I currently follow to get CSS selectors from elements with the latest Chrome version (59) is as follows:\n\nOpen Chrome Dev tools (cmd/ctrl + alt + j):\n\n\n\nClick on the select element tool in page (cmd/ctrl + alt + c):\n\n\n\nClick on the element you want to get the selector from in order to view it in the dev tools panel:\n\n\n\nRight click on the dev tools element:\n\n\n\nClick on Copy -> Copy selector:\n\n\nWhich gives me the following:\n#question > table > tbody > tr:nth-child(1) > td.postcell > div > div.post-text > blockquote > p\n', '\nDo ""Inspect Element"" or Ctrl+Shift+I, it\'s at the VERY bottom of the screen. You can also type in the ""Search Elements"" box at the top-right of the dev tools if not sure about the selector. \n', ""\nIt's sometime necessary to do this if you have a very complex app structure that you inherited and are trying to track down a very tricky multi nested css depth problem. Jquery mobile pre 1.3 would be a good example of this. Bootstrap apps etc..\nI tried the above tool but could not get that to actually select the entire parent and children of a complex inheritance similar to the original posters question.\n""]",https://stackoverflow.com/questions/4500572/how-can-i-get-the-css-selector-in-chrome,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do I call a Javascript function from Python?,"
I am working on a web-scraping project. One of the websites I am working with has the data coming from Javascript.
There was a suggestion on one of my earlier questions that I can directly call the Javascript from Python, but I'm not sure how to accomplish this.
For example: If a JavaScript function is defined as: add_2(var,var2)
How would I call that JavaScript function from Python?
",115k,"
            42
        ","['\nFind a JavaScript interpreter that has Python bindings. (Try Rhino? V8? SeaMonkey?). When you have found one, it should come with examples of how to use it from python.\nPython itself, however, does not include a JavaScript interpreter.\n', '\nTo interact with JavaScript from Python I use webkit, which is the browser renderer behind Chrome and Safari. There are Python bindings to webkit through Qt. In particular there is a function for executing JavaScript called evaluateJavaScript().\nHere is a full example to execute JavaScript and extract the final HTML.\n', ""\nAn interesting alternative I discovered recently is the Python bond module, which can be used to communicate with a NodeJs process (v8 engine).\nUsage would be very similar to the pyv8 bindings, but you can directly use any NodeJs library without modification, which is a major selling point for me.\nYour python code would look like this:\nval = js.call('add2', var1, var2)\n\nor even:\nadd2 = js.callable('add2')\nval = add2(var1, var2)\n\nCalling functions though is definitely slower than pyv8, so it greatly depends on your needs. If you need to use an npm package that does a lot of heavy-lifting, bond is great. You can even have more nodejs processes running in parallel.\nBut if you just need to call a bunch of JS functions (for instance, to have the same validation functions between the browser/backend), pyv8 will definitely be a lot faster.\n"", '\nYou can eventually get the JavaScript from the page and execute it through some interpreter (such as v8 or Rhino). However, you can get a good result in a way easier way by using some functional testing tools, such as Selenium or Splinter. These solutions launch a browser and effectively load the page - it can be slow but assures that the expected browser displayed content will be available.\nFor example, consider the HTML document below:\n<html>\n    <head>\n        <title>Test</title>\n        <script type=""text/javascript"">\n            function addContent(divId) {\n                var div = document.getElementById(divId);\n                div.innerHTML = \'<em>My content!</em>\';\n            }\n        </script>\n    </head>\n    <body>\n        <p>The element below will receive content</p>\n        <div id=""mydiv"" />\n        <script type=""text/javascript"">addContent(\'mydiv\')</script>\n    </body>\n</html>\n\nThe script below will use Splinter. Splinter will launch Firefox and after the complete load of the page it will get the content added to a div by JavaScript:\nfrom splinter.browser import Browser\nimport os.path\n\nbrowser = Browser()\nbrowser.visit(\'file://\' + os.path.realpath(\'test.html\'))\nelements = browser.find_by_css(""#mydiv"")\ndiv = elements[0]\nprint div.value\n\nbrowser.quit()\n\nThe result will be the content printed in the stdout.\n', ""\nYou might call node through Popen.\nMy example how to do it\nprint execute('''function (args) {\n    var result = 0;\n    args.map(function (i) {\n        result += i;\n    });\n    return result;\n}''', args=[[1, 2, 3, 4, 5]])\n\n"", '\nHi so one possible solution would be to use ajax with flask to comunicate between javascript and python. You would run a server with flask and then open the website in a browser. This way you could run javascript functions when the website is created via pythoncode or with a button how it is done in this example.\nHTML code:\n\n\n<html>\n<script src=""//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js""></script>\n\n\n<script>\n    function pycall() {\n      $.getJSON(\'/pycall\', {content: ""content from js""},function(data) {\n          alert(data.result);\n      });\n    }\n</script>\n\n\n<button type=""button"" onclick=""pycall()"">click me</button>\n \n\n</html>\n\nPython Code:\nfrom flask import Flask, jsonify, render_template, request\n\napp = Flask(__name__)\n\n\ndef load_file(file_name):\n    data = None\n    with open(file_name, \'r\') as file:\n        data = file.read()\n    return data\n\n@app.route(\'/pycall\')\ndef pycall():\n    content = request.args.get(\'content\', 0, type=str)\n    \n    print(""call_received"",content)\n    return jsonify(result=""data from python"")\n\n@app.route(\'/\')\ndef index():\n    return load_file(""basic.html"")\n\n\n\nimport webbrowser\nprint(""opening localhost"")\nurl = ""http://127.0.0.1:5000/""\nwebbrowser.open(url)\napp.run()\n\noutput in python:\ncall_received content from js\nalert in browser:\ndata from python\n', '\nThis worked for me for simple js file, source:\nhttps://www.geeksforgeeks.org/how-to-run-javascript-from-python/\npip install js2py\npip install temp\n\nfile.py\nimport js2py\neval_res, tempfile = js2py.run_file(""scripts/dev/test.js"")\ntempfile.wish(""GeeksforGeeks"")\n\nscripts/dev/test.js\nfunction wish(name) {\n    console.log(""Hello, "" + name + ""!"")\n}\n\n', '\nDid a whole run-down of the different methods recently.  \nPyQt4\nnode.js/zombie.js\nphantomjs\nPhantomjs was the winner hands down, very straightforward with lots of examples.\n']",https://stackoverflow.com/questions/8284765/how-do-i-call-a-javascript-function-from-python,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web scraping in PHP,"
I'm looking for a way to make a small preview of another page from a URL given by the user in PHP.
I'd like to retrieve only the title of the page, an image (like the logo of the website) and a bit of text or a description if it's available. Is there any simple way to do this without any external libraries/classes? Thanks
So far I've tried using the DOCDocument class, loading the HTML and displaying it on the screen, but I don't think that's the proper way to do it
",117k,"
            40
        ","['\nI recommend you consider simple_html_dom for this. It will make it very easy. \nHere is a working example of how to pull the title, and first image.\n<?php\nrequire \'simple_html_dom.php\';\n\n$html = file_get_html(\'http://www.google.com/\');\n$title = $html->find(\'title\', 0);\n$image = $html->find(\'img\', 0);\n\necho $title->plaintext.""<br>\\n"";\necho $image->src;\n?>\n\nHere is a second example that will do the same without an external library. I should note that using regex on HTML is NOT a good idea.\n<?php\n$data = file_get_contents(\'http://www.google.com/\');\n\npreg_match(\'/<title>([^<]+)<\\/title>/i\', $data, $matches);\n$title = $matches[1];\n\npreg_match(\'/<img[^>]*src=[\\\'""]([^\\\'""]+)[\\\'""][^>]*>/i\', $data, $matches);\n$img = $matches[1];\n\necho $title.""<br>\\n"";\necho $img;\n?>\n\n', ""\nYou may use either of these libraries. As you know each one has pros & cons, so you may consult notes about each one or take time & try it on your own:\n\nGuzzle: An Independent HTTP client, so no need to depend on cURL, SOAP or REST.\nGoutte: Built on Guzzle & some of Symfony components by Symfony developer.\nhQuery: A fast scraper with caching capabilities. high performance on scraping large docs.\nRequests: Famous for its user-friendly usage.\nBuzz: A lightweight client, ideal for beginners.\nReactPHP: Async scraper, with comprehensive tutorials & examples.\n\nYou'd better check them all & use everyone in its best intended occasion.\n"", '\nThis question is fairly old but still ranks very highly on Google Search results for web scraping tools in PHP.  Web scraping in PHP has advanced considerably in the intervening years since the question was asked.  I actively maintain the Ultimate Web Scraper Toolkit, which hasn\'t been mentioned yet but predates many of the other tools listed here except for Simple HTML DOM.\nThe toolkit includes TagFilter, which I actually prefer over other parsing options because it uses a state engine to process HTML with a continuous streaming tokenizer for precise data extraction.\nTo answer the original question of, ""Is there any simple way to do this without any external libraries/classes?""  The answer is no.  HTML is rather complex and there\'s nothing built into PHP that\'s particularly suitable for the task.  You really need a reusable library to parse generic HTML correctly and consistently.  Plus you\'ll find plenty of uses for such a library.\nAlso, a really good web scraper toolkit will have three major, highly-polished components/capabilities:\n\nData retrieval.  This is making a HTTP(S) request to a server and pulling down data.  A good web scraping library will also allow for large binary data blobs to be written directly to disk as they come down off the network instead of loading the whole thing into RAM.  The ability to do dynamic form extraction and submission is also very handy.  A really good library will let you fine-tune every aspect of each request to each server as well as look at the raw data it sent and received on the wire.  Some web servers are extremely picky about input, so being able to accurately replicate a browser is handy.\nData extraction.  This is finding pieces of content inside retrieved HTML and pulling it out, usually to store it into a database for future lookups.  A good web scraping library will also be able to correctly parse any semi-valid HTML thrown at it, including Microsoft Word HTML and ASP.NET output where odd things show up like a single HTML tag that spans several lines.  The ability to easily extract all the data from poorly designed, complex, classless tags like ASP.NET HTML table elements that some overpaid government employees made is also very nice to have (i.e. the extraction tool has more than just a DOM or CSS3-style selection engine available).  Also, in your case, the ability to early-terminate both the data retrieval and data extraction after reading in 50KB or as soon as you find what you are looking for is a plus, which could be useful if someone submits a URL to a 500MB file.\nData manipulation.  This is the inverse of #2.  A really good library will be able to modify the input HTML document several times without negatively impacting performance.  When would you want to do this?  Sanitizing user-submitted HTML, transforming content for a newsletter or sending other email, downloading content for offline viewing, or preparing content for transport to another service that\'s finicky about input (e.g. sending to Apple News or Amazon Alexa).  The ability to create a custom HTML-style template language is also a nice bonus.\n\nObviously, Ultimate Web Scraper Toolkit does all of the above...and more:\nI also like my toolkit because it comes with a WebSocket client class, which makes scraping WebSocket content easier.  I\'ve had to do that a couple of times.\nIt was also relatively simple to turn the clients on their heads and make WebServer and WebSocketServer classes.  You know you\'ve got a good library when you can turn the client into a server....but then I went and made PHP App Server with those classes.  I think it\'s becoming a monster!\n', '\nYou can use SimpleHtmlDom for this. and then look for the title and img tags or what ever else you need to do.\n', ""\nI like the Dom Crawler library. Very easy to use, has lots of options like:\n$crawler = $crawler\n->filter('body > p')\n->reduce(function (Crawler $node, $i) {\n    // filters every other node\n    return ($i % 2) == 0;\n});\n\n""]",https://stackoverflow.com/questions/9813273/web-scraping-in-php,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python selenium multiprocessing,"
I've written a script in python in combination with selenium to scrape the links of different posts from its landing page and finally get the title of each post by tracking the url leading to its inner page. Although the content I parsed here are static ones, I used selenium to see how it works in multiprocessing. 
However, my intention is to do the scraping using multiprocessing. So far I know that selenium doesn't support multiprocessing but it seems I was wrong.
My question: how can I reduce the execution time using selenium when it is made to run using multiprocessing?
This is my try (it's a working one):
import requests
from urllib.parse import urljoin
from multiprocessing.pool import ThreadPool
from bs4 import BeautifulSoup
from selenium import webdriver

def get_links(link):
  res = requests.get(link)
  soup = BeautifulSoup(res.text,""lxml"")
  titles = [urljoin(url,items.get(""href"")) for items in soup.select("".summary .question-hyperlink"")]
  return titles

def get_title(url):
  chromeOptions = webdriver.ChromeOptions()
  chromeOptions.add_argument(""--headless"")
  driver = webdriver.Chrome(chrome_options=chromeOptions)
  driver.get(url)
  sauce = BeautifulSoup(driver.page_source,""lxml"")
  item = sauce.select_one(""h1 a"").text
  print(item)

if __name__ == '__main__':
  url = ""https://stackoverflow.com/questions/tagged/web-scraping""
  ThreadPool(5).map(get_title,get_links(url))

",22k,"
            31
        ","['\n\nhow can I reduce the execution time using selenium when it is made to run using multiprocessing\n\nA lot of time in your solution is spent on launching the webdriver for each URL. You can reduce this time by launching the driver only once per thread:\n(... skipped for brevity ...)\n\nthreadLocal = threading.local()\n\ndef get_driver():\n  driver = getattr(threadLocal, \'driver\', None)\n  if driver is None:\n    chromeOptions = webdriver.ChromeOptions()\n    chromeOptions.add_argument(""--headless"")\n    driver = webdriver.Chrome(chrome_options=chromeOptions)\n    setattr(threadLocal, \'driver\', driver)\n  return driver\n\n\ndef get_title(url):\n  driver = get_driver()\n  driver.get(url)\n  (...)\n\n(...)\n\nOn my system this reduces the time from 1m7s to just 24.895s, a ~35% improvement. To test yourself, download the full script.\nNote: ThreadPool uses threads, which are constrained by the Python GIL. That\'s ok if for the most part the task is I/O bound. Depending on the post-processing you do with the scraped results, you may want to use a multiprocessing.Pool instead. This launches parallel processes which as a group are not constrained by the GIL. The rest of the code stays the same.\n', '\nThe one potential problem I see with the clever one-driver-per-thread answer is that it omits any mechanism for ""quitting"" the drivers and thus leaving the possibility of processes hanging around. I would make the following changes:\n\nUse instead class Driver that will crate the driver instance and store it on the thread-local storage but also have a destructor that will quit the driver when the thread-local storage is deleted:\n\nclass Driver:\n    def __init__(self):\n        options = webdriver.ChromeOptions()\n        options.add_argument(""--headless"")\n        self.driver = webdriver.Chrome(options=options)\n\n    def __del__(self):\n        self.driver.quit() # clean up driver when we are cleaned up\n        #print(\'The driver has been ""quitted"".\')\n\n\ncreate_driver now becomes:\n\nthreadLocal = threading.local()\n\ndef create_driver():\n    the_driver = getattr(threadLocal, \'the_driver\', None)\n    if the_driver is None:\n        the_driver = Driver()\n        setattr(threadLocal, \'the_driver\', the_driver)\n    return the_driver.driver\n\n\nFinally, after you have no further use for the ThreadPool instance but before it is terminated, add the following lines to delete the thread-local storage and force the Driver instances\' destructors to be called (hopefully):\n\ndel threadLocal\nimport gc\ngc.collect() # a little extra insurance\n\n', ""\n\nMy question: how can I reduce the execution time?\n\nSelenium seems the wrong tool for web scraping - though I appreciate YMMV, in particular if you need to simulate user interaction with the web site or there is some JavaScript limitation/requirement.\nFor scraping tasks without much interaction, I have had good results using the opensource Scrapy Python package for large-scale scrapying tasks. It does multiprocessing out of the box, it is easy to write new scripts and store the data in files or a database -- and it is really fast. \nYour script would look something like this when implemented as a fully parallel Scrapy spider (note I did not test this, see documentation on selectors).\nimport scrapy\nclass BlogSpider(scrapy.Spider):\n    name = 'blogspider'\n    start_urls = ['https://stackoverflow.com/questions/tagged/web-scraping']\n\n    def parse(self, response):\n        for title in response.css('.summary .question-hyperlink'):\n            yield title.get('href')\n\nTo run put this into blogspider.py and run \n$ scrapy runspider blogspider.py\n\nSee the Scrapy website for a complete tutorial.\nNote that Scrapy also supports JavaScript through scrapy-splash, thanks to the pointer by @SIM. I didn't have any exposure with that so far so can't speak to this other than it looks well integrated with how Scrapy works.\n""]",https://stackoverflow.com/questions/53475578/python-selenium-multiprocessing,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Android: Using WebView outside an Activity context,"
I am trying to achieve Web Scraping through a background IntentService that periodically scrape a website without a view displaying on the users phone.  

Since I have to do call some javascript on the loaded page I cannot use any HttpGet's etc.
I therefore have to use a WebView instance which can only run on an UI thread.
Any attempts to start an Activity that use a WebView results in a View coming into the phones foreground (as per Android's design of Activities)
Any attempts to use a WebView outside of an Activity context resulted in error pointing to the fact that you cannot use WebView on a non-UI thread.
For various complexity reasons I cannot consider using libraries such as Rhino for UI-less web scraping.

Is there any way of working around this problem?
",29k,"
            29
        ","['\nYou can display a webview from a service. Code below creates a window which your service has access to. The window isn\'t visible because the size is 0 by 0.\npublic class ServiceWithWebView extends Service {\n\n    @Override\n    public void onCreate() {\n        super.onCreate();\n\n        WindowManager windowManager = (WindowManager) getSystemService(WINDOW_SERVICE);\n        params = new WindowManager.LayoutParams(WindowManager.LayoutParams.WRAP_CONTENT, WindowManager.LayoutParams.WRAP_CONTENT, WindowManager.LayoutParams.TYPE_SYSTEM_OVERLAY, WindowManager.LayoutParams.FLAG_NOT_TOUCHABLE, PixelFormat.TRANSLUCENT);\n        params.gravity = Gravity.TOP | Gravity.LEFT;\n        params.x = 0;\n        params.y = 0;\n        params.width = 0;\n        params.height = 0;\n\n        LinearLayout view = new LinearLayout(this);\n        view.setLayoutParams(new RelativeLayout.LayoutParams(RelativeLayout.LayoutParams.MATCH_PARENT, RelativeLayout.LayoutParams.MATCH_PARENT));\n\n        WebView wv = new WebView(this);\n        wv.setLayoutParams(new LinearLayout.LayoutParams(LinearLayout.LayoutParams.MATCH_PARENT, LinearLayout.LayoutParams.MATCH_PARENT));\n        view.addView(wv);\n        wv.loadUrl(""http://google.com"");\n\n        windowManager.addView(view, params);\n    }\n}\n\nAlso this will require the android.permission.SYSTEM_ALERT_WINDOW permission.\n', '\nCorrect me if I am wrong but the correct answer to this question is that there is NO possible way to use a WebView in the background while the user is doing other things on the phone without interrupting the user by means of an Activity.\nI have applied both Randy and Code_Yoga\'s suggestions:  Using an activity with ""Theme.NoDisplay"" to launch a background service with a WebView to do some work.  However even though no view is visible the switching to that activity for that second to start the services interrupts the user (ex. like pausing a running game that was being played).\nTotally disastrous news for my app so I am still hoping someone will give me a way to use a WebView that does not need an Activity (or a substitute for a WebView that can accomplish the same)\n', '\nYou can use this to hide the Activity \n         <activity android:name=""MyActivity""\n          android:label=""@string/app_name""\n          android:theme=""@android:style/Theme.NoDisplay"">\n\nDoing this will prevent the app from showing any Activity. \nAnd then you can do your stuff in the Activity.\n', '\nthe solution was like this, but with Looper.getMainLooper() :\nhttps://github.com/JonasCz/save-for-offline/blob/master/app/src/main/java/jonas/tool/saveForOffline/ScreenshotService.java\n@Override\npublic void onCreate() {\n    super.onCreate();\n    //HandlerThread thread = new HandlerThread(""ScreenshotService"", Process.THREAD_PRIORITY_BACKGROUND);\n    //thread.start();\n    //mServiceHandler = new ServiceHandler(thread.getLooper()); // not working\n    mServiceHandler = new ServiceHandler(Looper.getMainLooper()); // working\n}\n\nwith help of @JonasCz : https://stackoverflow.com/a/28234761/466363\n', ""\nI used the following code to get round this problem:\nHandler handler = new Handler(Looper.getMainLooper());\ntry\n{\n    handler.post(\n        new Runnable()\n        {\n            @Override\n            public void run()\n            {\n                ProcessRequest(); // Where this method runs the code you're needing\n            }\n        }\n    );\n} catch (Exception e)\n{\n    e.printStackTrace();\n}\n\n"", ""\nA WebView cannot exist outside of an Activity or Fragment due to it being a UI.\nHowever, this means that an Activity is only needed to create the WebView, not handle all its requests.\nIf you create the invisible WebView in your main activity and have it accessible from a static context, you should be able to perform tasks in the view in the background from anywhere, since I believe all of WebView's IO is done asynchronously. \nTo take away the ick of that global access, you could always launch a Service with a reference to the WebView to do the work you need.\n"", '\nor a substitute for a WebView that can accomplish the same <=== if you do not wish to show the loaded info on UI, maybe you can try to use HTTP to call the url directly, and process on the returned response from HTTP\n', ""\nWhy don't you create a Backend Service that does the scraping for you?\nAnd then you just poll results from a RESTful Webservice or even use a messaging middleware (e.g. ZeroMQ).\nMaybe more elegant if it fits your use case: let the Scraping Service send your App Push Messages via GCM :)\n"", ""\nI am not sure if this is a silver bullet to the given problem.\nAs per @Pierre's accepted answer (sounds correct to me)\n\nthere is NO possible way to use a WebView in the background while the\nuser is doing other things on the phone without interrupting the user\nby means of an Activity.\n\nThus, I believe there must be some architectural/flow/strategy changes that must be done in order to solve this problem.\nProposed Solution #1: Instead of getting a push notification from the server and run a background job and followed by running some JS code or WebView. Instead, Whenever user launch the application one should query the backend server to know whether there is any need to perform any scraping or not. And on the basis of backend input android client can run JS code or WebView and pass the result back to the server.\nI haven't tried this solution. But hope it is feasible.\n\nThis will also solve the following problem stated in the comments:\nReason for this is because the backend will get detected as a bot scraping from the same IP and get blocked (in addition to backend resources needed to do a lot of scraping on different pages).\nData might be unavailable for some time (until some user scrape it for you). But surely we can provide a better user experience to the end users using this strategy.\n"", ""\nI know it'a been a year and a half, but I'm now facing the same issue. I solved it eventually by running my Javascript code inside a Node engine that is running inside my Android App. It's called JXCore. You can take a look. Also, take a look at this sample that runs Javascript without a WebView. I really would like to know what did you end up using?\n""]",https://stackoverflow.com/questions/18865035/android-using-webview-outside-an-activity-context,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping ajax pages using python,"
I've already seen this question about scraping ajax, but python isn't mentioned there. I considered using scrapy, i believe they have some docs on that subject, but as you can see the website is down. So i don't know what to do. I want to do the following:
I only have one url, example.com you go from page to page by clicking submit, the url doesn't change since they're using ajax to display the content. I want to scrape the content of each page, how to do it? 
Lets say that i want to scrape only the numbers, is there anything other than scrapy that would do it? If not, would you give me a snippet on how to do it, just because their website is down so i can't reach the docs.
",47k,"
            18
        ","['\nFirst of all, scrapy docs are available at https://scrapy.readthedocs.org/en/latest/.\nSpeaking about handling ajax while web scraping. Basically, the idea is rather simple: \n\nopen browser developer tools, network tab\ngo to the target site\nclick submit button and see what XHR request is going to the server\nsimulate this XHR request in your spider\n\nAlso see:\n\nCan scrapy be used to scrape dynamic content from websites that are using AJAX?\nPagination using scrapy\n\nHope that helps.\n', '\nI found the answer very useful but I would like to make it more simple.\nresponse = requests.post(request_url, data=payload, headers=request_headers)\n\nrequest.post takes three parameters url, data and headers. Values for these three attributes can be found in the XHR request.\nCopy the whole request header and form data to load into the above variables and you are good to go\n']",https://stackoverflow.com/questions/16390257/scraping-ajax-pages-using-python,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Android - Parse JS generated urls with JSOUP,"
im trying to parse url generated by Bootstrap`s Bootpage.js which looks like
https://example.com/#page-2
but JSOUP cant parse it and showing main url.
how to get normal link from Bootpage or how to make JSOUP to parse it.
Parsing code:
Jsoup.connect(""https://example.com/#page-2"").followRedirects(true).get();

",6k,"
            6
        ","['\n(See UPDATE below, first/accepted solution didn\'t met the android requirement, but is left for reference.) \n\nDesktop Solution\nHtmlUnit doesn\'t seem able to handle this site (often the case, lately). So I don\'t have a plain java solution either, but you could use PhantomJS: download the binary for your os, create a script file, start the process from within your java code and parse the output with a dom parser like jsoup.\nScript file (here called simple.js): \nvar page = require(\'webpage\').create();\nvar fs = require(\'fs\');\nvar system = require(\'system\');\n\nvar url = """";\nvar fileName = ""output"";\n// first parameter: url\n// second parameter: filename for output\nconsole.log(""args length: "" + system.args.length);\n\nif (system.args.length > 1) {\n    url=system.args[1];\n}\nif (system.args.length > 2){\n    fileName=system.args[2];\n}\nif(url===""""){\n    phantom.exit();\n}\n\npage.settings.userAgent = \'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.120 Safari/537.36\';\npage.settings.loadImages = false; \n\npage.open(url, function(status) {\n    console.log(""Status: "" + status);\n    if(status === ""success"") {\n        var path = fileName+\'.html\';\n        fs.write(path, page.content, \'w\');\n    }\n    phantom.exit();\n});\n\nJava code (example to get title and cover-url):\ntry {\n    //change path to phantomjs binary and your script file\n    String outputFileName = ""srulad"";\n    String phantomJSPath = ""phantomjs"" + File.separator + ""bin"" + File.separator + ""phantomjs"";\n    String scriptFile = ""simple.js"";\n\n    String urlParameter = ""http://srulad.com/#page-2"";\n\n    new File(outputFileName+"".html"").delete();\n\n    Process process = Runtime.getRuntime().exec(phantomJSPath + "" "" + scriptFile + "" "" + urlParameter + "" "" + outputFileName);\n    process.waitFor();\n\n    Document doc = Jsoup.parse(new File(outputFileName + "".html""),""UTF-8""); // output.html is created by phantom.js, same path as page.js\n    Elements elements = doc.select(""#list_page-2 > div"");\n\n    for (Element element : elements) {\n        System.out.println(element.select(""div.l-description.float-left > div:nth-child(1) > a"").first().attr(""title""));\n        System.out.println(element.select(""div.l-image.float-left > a > img.lazy"").first().attr(""data-original""));\n    }\n} catch (IOException | InterruptedException e) {\n    e.printStackTrace();\n}\n\nOutput:\n醿♂儤醿п儠醿愥儬醿ａ儦醿?醿撫儛 醿涐儩醿儳醿愥儦醿斸儜醿?/ Love & Mercy\nhttp://srulad.com/assets/uploads/42410_Love_and_Mercy.jpg\n醿涐儯醿栣儛 / The Muse\nhttp://srulad.com/assets/uploads/43164_large_qRzsimNz0eDyFLFJcbVLIxlqii.jpg\n...\n\n\nUPDATE\nParsing of websites with javascript based dynamic content in Android is possible using WebView and jsoup. \nThe following example app uses a javascript enabled WebView to render a Javascript dependent website. With a JavascriptInterface the html source is returned, parsed with jsoup and as a proof of concept the titles and the urls to the cover-images are used to populate a ListView. The buttons decrement or increment the page number triggering an update of the ListView. Note: tested on an Android 5.1.1/API 22 device.\nadd internet permission to your AndroidManifest.xml\n<uses-permission android:name=""android.permission.INTERNET"" />\n\nactivity_main.xml\n<?xml version=""1.0"" encoding=""utf-8""?>\n<LinearLayout xmlns:android=""http://schemas.android.com/apk/res/android""\n    android:orientation=""vertical""\n    android:layout_width=""match_parent""\n    android:layout_height=""match_parent"">\n\n    <LinearLayout\n        android:orientation=""horizontal""\n        android:layout_width=""match_parent""\n        android:layout_height=""wrap_content"">\n\n        <Button\n            android:layout_width=""wrap_content""\n            android:layout_height=""wrap_content""\n            android:text=""@string/page_down""\n            android:id=""@+id/buttonDown""\n            android:layout_weight=""0.5"" />\n\n        <Button\n            android:layout_width=""wrap_content""\n            android:layout_height=""wrap_content""\n            android:text=""@string/page_up""\n            android:id=""@+id/buttonUp""\n            android:layout_weight=""0.5"" />\n    </LinearLayout>\n\n    <ListView\n        android:layout_width=""match_parent""\n        android:layout_height=""0dp""\n        android:id=""@+id/listView""\n        android:layout_gravity=""bottom""\n        android:layout_weight=""0.5"" />\n</LinearLayout>\n\nMainActivity.java\npublic class MainActivity extends AppCompatActivity {\n\n    private final Handler uiHandler = new Handler();\n    private ArrayAdapter<String> adapter;\n    private ArrayList<String> entries = new ArrayList<>();\n    private ProgressDialog progressDialog;\n\n    private class JSHtmlInterface {\n        @android.webkit.JavascriptInterface\n        public void showHTML(String html) {\n            final String htmlContent = html;\n\n            uiHandler.post(\n                new Runnable() {\n                    @Override\n                    public void run() {\n                        Document doc = Jsoup.parse(htmlContent);\n                        Elements elements = doc.select(""#online_movies > div > div"");\n                        entries.clear();\n                        for (Element element : elements) {\n                            String title = element.select(""div.l-description.float-left > div:nth-child(1) > a"").first().attr(""title"");\n                            String imgUrl = element.select(""div.l-image.float-left > a > img.lazy"").first().attr(""data-original"");\n                            entries.add(title + ""\\n"" + imgUrl);\n                        }\n                        adapter.notifyDataSetChanged();\n                    }\n                }\n            );\n        }\n    }\n\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        ListView listView = (ListView) findViewById(R.id.listView);\n        adapter = new ArrayAdapter<>(this, android.R.layout.simple_list_item_1, android.R.id.text1, entries);\n        listView.setAdapter(adapter);\n\n        progressDialog = ProgressDialog.show(this, ""Loading"",""Please wait..."", true);\n        progressDialog.setCancelable(false);\n\n        try {\n            final WebView browser = new WebView(this);\n            browser.setVisibility(View.INVISIBLE);\n            browser.setLayerType(View.LAYER_TYPE_NONE,null);\n            browser.getSettings().setJavaScriptEnabled(true);\n            browser.getSettings().setBlockNetworkImage(true);\n            browser.getSettings().setDomStorageEnabled(false);\n            browser.getSettings().setCacheMode(WebSettings.LOAD_NO_CACHE);\n            browser.getSettings().setLoadsImagesAutomatically(false);\n            browser.getSettings().setGeolocationEnabled(false);\n            browser.getSettings().setSupportZoom(false);\n\n            browser.addJavascriptInterface(new JSHtmlInterface(), ""JSBridge"");\n\n            browser.setWebViewClient(\n                new WebViewClient() {\n\n                    @Override\n                    public void onPageStarted(WebView view, String url, Bitmap favicon) {\n                        progressDialog.show();\n                        super.onPageStarted(view, url, favicon);\n                    }\n\n                    @Override\n                    public void onPageFinished(WebView view, String url) {\n                        browser.loadUrl(""javascript:window.JSBridge.showHTML(\'<html>\'+document.getElementsByTagName(\'html\')[0].innerHTML+\'</html>\');"");\n                        progressDialog.dismiss();\n                    }\n                }\n            );\n\n            findViewById(R.id.buttonDown).setOnClickListener(new View.OnClickListener() {\n                @Override\n                public void onClick(View view) {\n                    uiHandler.post(new Runnable() {\n                        @Override\n                        public void run() {\n                            int page = Integer.parseInt(browser.getUrl().split(""-"")[1]);\n                            int newPage = page > 1 ? page-1 : 1;\n                            browser.loadUrl(""http://srulad.com/#page-"" + newPage);\n                            browser.loadUrl(browser.getUrl()); // not sure why this is needed, but doesn\'t update without it on my device\n                            if(getSupportActionBar()!=null) getSupportActionBar().setTitle(browser.getUrl());\n                        }\n                    });\n                }\n            });\n\n            findViewById(R.id.buttonUp).setOnClickListener(new View.OnClickListener() {\n                @Override\n                public void onClick(View view) {\n                    uiHandler.post(new Runnable() {\n                        @Override\n                        public void run() {\n                            int page = Integer.parseInt(browser.getUrl().split(""-"")[1]);\n                            int newPage = page+1;\n                            browser.loadUrl(""http://srulad.com/#page-"" + newPage);\n                            browser.loadUrl(browser.getUrl()); // not sure why this is needed, but doesn\'t update without it on my device\n                            if(getSupportActionBar()!=null) getSupportActionBar().setTitle(browser.getUrl());\n                        }\n                    });\n                }\n            });\n\n            browser.loadUrl(""http://srulad.com/#page-1"");\n            if(getSupportActionBar()!=null) getSupportActionBar().setTitle(browser.getUrl());\n\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n\n']",https://stackoverflow.com/questions/39140121/android-parse-js-generated-urls-with-jsoup,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnicodeEncodeError: 'ascii' codec can't encode character '\xe9' - -when using urlib.request python3,"
I'm writing a script that goes to a list of links and parses the information.
It works for most sites but It's choking on some with 
""UnicodeEncodeError: 'ascii' codec can't encode character '\xe9' in position 13: ordinal not in range(128)""
It stops on client.py which is part of urlib on python3
the exact link is:
http://finance.yahoo.com/news/caf茅s-growing-faster-than-fast-food-peers-144512056.html
There are quite a few similar postings here but none of the answers seems to work for me.
my code is:
from urllib import request

def __request(link,debug=0):      

try:
    html = request.urlopen(link, timeout=35).read() #made this long as I was getting lots of timeouts
    unicode_html = html.decode('utf-8','ignore')

# NOTE the except HTTPError must come first, otherwise except URLError will also catch an HTTPError.
except HTTPError as e:
    if debug:
        print('The server couldn\'t fulfill the request for ' + link)
        print('Error code: ', e.code)
    return ''
except URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('timeout')
        return ''    
else:
    return unicode_html

this calls the request function
link = 'http://finance.yahoo.com/news/caf茅s-growing-faster-than-fast-food-peers-144512056.html'
page = __request(link)
And the traceback is:
Traceback (most recent call last):
  File ""<string>"", line 250, in run_nodebug
  File ""C:\reader\get_news.py"", line 276, in <module>
    main()
  File ""C:\reader\get_news.py"", line 255, in main
    body = get_article_body(item['link'],debug=0)
  File ""C:\reader\get_news.py"", line 155, in get_article_body
    page = __request('na',url)
  File ""C:\reader\get_news.py"", line 50, in __request
    html = request.urlopen(link, timeout=35).read()
  File ""C:\Python33\Lib\urllib\request.py"", line 156, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Python33\Lib\urllib\request.py"", line 469, in open
    response = self._open(req, data)
  File ""C:\Python33\Lib\urllib\request.py"", line 487, in _open
    '_open', req)
  File ""C:\Python33\Lib\urllib\request.py"", line 447, in _call_chain
    result = func(*args)
  File ""C:\Python33\Lib\urllib\request.py"", line 1268, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File ""C:\Python33\Lib\urllib\request.py"", line 1248, in do_open
    h.request(req.get_method(), req.selector, req.data, headers)
  File ""C:\Python33\Lib\http\client.py"", line 1061, in request
    self._send_request(method, url, body, headers)
  File ""C:\Python33\Lib\http\client.py"", line 1089, in _send_request
    self.putrequest(method, url, **skips)
  File ""C:\Python33\Lib\http\client.py"", line 953, in putrequest
    self._output(request.encode('ascii'))
UnicodeEncodeError: 'ascii' codec can't encode character '\xe9' in position 13: ordinal not in range(128)

Any help appreciated It's driving me crazy , I think I've tried all combinations of x.decode    and similar 
(I could ignore the offending characters if that is possible.)
",4k,"
            5
        ","[""\nUse a percent-encoded URL:\nlink = 'http://finance.yahoo.com/news/caf%C3%A9s-growing-faster-than-fast-food-peers-144512056.html'\n\n\nI found the above percent-encoded URL by pointing the browser at \nhttp://finance.yahoo.com/news/caf茅s-growing-faster-than-fast-food-peers-144512056.html\n\ngoing to the page, then copying-and-pasting the \nencoded url supplied by the browser back into the text editor. However, you can generate a percent-encoded URL programmatically using:\nfrom urllib import parse\n\nlink = 'http://finance.yahoo.com/news/caf茅s-growing-faster-than-fast-food-peers-144512056.html'\n\nscheme, netloc, path, query, fragment = parse.urlsplit(link)\npath = parse.quote(path)\nlink = parse.urlunsplit((scheme, netloc, path, query, fragment))\n\nwhich yields\nhttp://finance.yahoo.com/news/caf%C3%A9s-growing-faster-than-fast-food-peers-144512056.html\n\n"", ""\nYour URL contains characters that cannot be represented as ASCII characters.\nYou'll have to ensure that all characters have been properly URL encoded; use urllib.parse.quote_plus for example; it'll use UTF-8 URL-encoded escaping to represent any non-ASCII characters.\n""]",https://stackoverflow.com/questions/22734464/unicodeencodeerror-ascii-codec-cant-encode-character-xe9-when-using-ur,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IMPORTXML and right XPath from Bloomberg price [duplicate],"






This question already has answers here:
                        
                    



Scraping data to Google Sheets from a website that uses JavaScript

                                (2 answers)
                            

Closed last month.



I am trying to get price from mutual-fund from the Bloomberg website.
I have tried to use the ImportXML function in Google sheets, put in the Bloomberg link and copy the Full XPath but it always return with the #N/A.
This is my function:
=IMPORTXML(""https://www.bloomberg.com/quote/KAUGVAA:LX"",""/html/body/div[6]/div/div/section/section[1]/div/div[2]/section[1]/section/section/section/div[1]/span[1]"")

This is the Bloomberg link:
https://www.bloomberg.com/quote/KAUGVAA:LX?leadSource=uverify%20wall
Does anyone know what I am doing wrong?
",194,"
            0
        ","['\nif all you are getting is #N/A error you have 3 options before turning to a script\n\ndisable JavaScript. google sheets\' IMPORT formulae do not support the reading of JS content/elements. after you disable JS on your URL and the element you wish to scrape is not present there is 99.9% certainty you can give up! if the stuff you seek is still there move to point 2...\n\n\nrun an XML debugging formula to test what can be scrapped:\n=IMPORTXML(""URL""; ""//*"")\n\nif the result is #N/A give up and move to point 3...\n\nrun a sourcecode debugging formula to test what else can be scrapped:\n=IMPORTDATA(""URL"")\n\nif the output is #N/A give up and move to the next point. if the output is any other kind of error try:\n=QUERY(FLATTEN(IMPORTDATA(""URL"")); ""where Col1 is not null""; )\n\n\nat this stage open a google and try to find a different website that hosts the same data you want to get. then repeat steps 1-3. still no luck and your requirements are not that high? move to the next point...\n\ngo to google and search the URL. if there is a match try to check if there is a Cache:\n\ntake the URL and repeat steps 2-3. if this is not your thing or if luck left your life for good, move to point 6...\n=IMPORTXML(""https://webcache.googleusercontent.com/search?q=cache:aQET6JV0DywJ:https://www.bloomberg.com/quote/KAUGVAA:LX&cd=1&hl=en&ct=clnk"", \n ""//div[@class=\'overviewRow__66339412a5\']"")\n\n\n\ngive up or use a script\n\n\n']",https://stackoverflow.com/questions/74014518/importxml-and-right-xpath-from-bloomberg-price,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to scroll down with Phantomjs to load dynamic content,"
I am trying to scrape links from a page that generates content dynamically as the user scroll down to the bottom (infinite scrolling). I have tried doing different things with Phantomjs but not able to gather links beyond first page. Let say the element at the bottom which loads content has class .has-more-items. It is available until final content is loaded while scrolling and then becomes unavailable in DOM (display:none). Here are the things I have tried-

Setting viewportSize to a large height right after var page = require('webpage').create();


page.viewportSize = {             width: 1600,            height: 10000,
          };


Using page.scrollPosition = { top: 10000, left: 0 } inside page.open but have no effect like-


page.open('http://example.com/?q=houston', function(status) {
   if (status == ""success"") {
      page.scrollPosition = { top: 10000, left: 0 };  
   }
});



Also tried putting it inside page.evaluate function but that gives 


Reference error: Can't find variable page


Tried using jQuery and JS code inside page.evaluate and page.open but to no avail-


$(""html, body"").animate({ scrollTop: $(document).height() }, 10,
  function() {
          //console.log('check for execution');
      });

as it is and also inside document.ready. Similarly for JS code-
window.scrollBy(0,10000)

as it is and also inside window.onload
I am really struck on it for 2 days now and not able to find a way. Any help or hint would be appreciated.
Update
I have found a helpful piece of code at https://groups.google.com/forum/?fromgroups=#!topic/phantomjs/8LrWRW8ZrA0
var hitRockBottom = false; while (!hitRockBottom) {
    // Scroll the page (not sure if this is the best way to do so...)
    page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };

    // Check if we've hit the bottom
    hitRockBottom = page.evaluate(function() {
        return document.querySelector("".has-more-items"") === null;
    }); }

Where .has-more-items is the element class I want to access which is available at the bottom of the page initially and as we scroll down, it moves further down until all data is loaded and then becomes unavailable.
However, when I tested it is clear that it is running into infinite loops without scrolling down (I render pictures to check). I have tried to replace page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 }; with codes from below as well (one at a time)
window.document.body.scrollTop = '1000';
location.href = "".has-more-items"";
page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };
document.location.href="".has-more-items"";

But nothing seems to work.
",36k,"
            47
        ","['\nFound a way to do it and tried to adapt to your situation. I didn\'t test the best way of finding the bottom of the page because I had a different context, but check the solution below. The thing here is that you have to wait a little for the page to load and javascript works asynchronously so you have to use setInterval or setTimeout (see) to achieve this.\npage.open(\'http://example.com/?q=houston\', function () {\n\n  // Check for the bottom div and scroll down from time to time\n  window.setInterval(function() {\n      // Check if there is a div with class="".has-more-items"" \n      // (not sure if there\'s a better way of doing this)\n      var count = page.content.match(/class="".has-more-items""/g);\n\n      if(count === null) { // Didn\'t find\n        page.evaluate(function() {\n          // Scroll to the bottom of page\n          window.document.body.scrollTop = document.body.scrollHeight;\n        });\n      }\n      else { // Found\n        // Do what you want\n        ...\n        phantom.exit();\n      }\n  }, 500); // Number of milliseconds to wait between scrolls\n\n});\n\n', '\nI know that it has been answered a long time ago, but I also found a solution to my specific scenario. The result is a piece of javascript that scrolls to the bottom of the page. It is optimized to reduce waiting time.\nIt is not written for PhantomJS by default, so that will have to be modified. However, for a beginner or someone who doesn\'t have root access, an Iframe with injected javascript (run Google Chrome with --disable-javascript parameter) is a good alternative method for scraping a smaller set of ajax pages. The main benefit is that it\'s easily debuggable, because you have a visual overview of what\'s going on with your scraper.\nfunction ScrollForAjax () {\n\n    scrollintervals = 50;\n    scrollmaxtime = 1000;\n\n    if(typeof(scrolltime)==""undefined""){\n        scrolltime = 0;\n    }\n\n    scrolldocheight1 = $(iframeselector).contents().find(""body"").height();\n\n    $(""body"").scrollTop(scrolldocheight1);\n    setTimeout(function(){\n\n        scrolldocheight2 = $(""body"").height();\n\n        if(scrolltime===scrollmaxtime || scrolltime>scrollmaxtime){\n            scrolltime = 0;\n            $(""body"").scrollTop(0);\n            ScrapeCurrentPage(iframeselector);\n        }\n\n        else if(scrolldocheight2>scrolldocheight1){\n            scrolltime = 0;\n            ScrollForAjax (iframeselector);\n        }\n\n        else if(scrolldocheight1>=scrolldocheight2){\n            ScrollForAjax (iframeselector);\n        }\n\n    },scrollintervals);\n\n    scrolltime += scrollintervals;\n}\n\nscrollmaxtime is a timeout variable. Hope this is useful to someone :)\n', '\nThe ""correct"" solution didn\'t work for me. And, from what I\'ve read CasperJS doesn\'t use window (but I may be wrong on that), which makes me doubt that window works.\nThe following works for me in the Firefox/Chrome console; but, doesn\'t work in CasperJS (within casper.evaluate function).\n$(document).scrollTop($(document).height());\n\nWhat did work for me in CasperJS was:\ncasper.scrollToBottom();\ncasper.wait(1000, function waitCb() {\n  casper.capture(""loadedContent.png"");\n});\n\nWhich, also worked when moving casper.capture into Casper\'s then function.\nHowever, the above solution won\'t work on some sites like Twitter; jQuery seems to break the casper.scrollToBottom() function, and I had to remove the clientScripts reference to jQuery when working within Twitter.\nvar casper = require(\'casper\').create({\n    clientScripts: [\n       // \'jquery.js\'\n    ]\n});\n\nSome websites (e.g. BoingBoing.net) seem to work fine with jQuery and CasperJS scrollToBottom(). Not sure why some sites work and others don\'t.\n', ""\nThe code snippet below work just fine for pinterest. I researched a lot to scrape pinterest without phantomjs but it is impossible to find the infinite scroll trigger link. I think the code below will help other infinite scroll web page to scrape.\npage.open(pageUrl).then(function (status) {\n    var count = 0;\n    // Scrolls to the bottom of page\n    function scroll2btm() {\n        if (count < 500) {\n            page.evaluate(function(limit) {\n                window.scrollTo(0, document.body.scrollHeight || document.documentElement.scrollHeight);\n                return document.getElementsByClassName('pinWrapper').length; // use desired contents (eg. pin) selector for count presence number\n            }).then(function(c) {\n                count = c;\n                console.log(count); // print no of content found to check\n            });\n            setTimeout(scroll2btm,3000);\n        } else {\n            // required number of item found\n        }\n    }\n    scroll2btm();\n});\n\n""]",https://stackoverflow.com/questions/16561582/how-to-scroll-down-with-phantomjs-to-load-dynamic-content,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping a dynamic ecommerce page with infinite scroll,"
I'm using rvest in R to do some scraping. I know some HTML and CSS.
I want to get the prices of every product of a URI:
http://www.linio.com.co/tecnologia/celulares-telefonia-gps/
The new items load as you go down on the page (as you do some scrolling).
What I've done so far:
Linio_Celulares <- html(""http://www.linio.com.co/celulares-telefonia-gps/"")

Linio_Celulares %>%
  html_nodes("".product-itm-price-new"") %>%
  html_text()

And i get what i need, but just for the 25 first elements (those load for default). 
 [1] ""$ 1.999.900"" ""$ 1.999.900"" ""$ 1.999.900"" ""$ 2.299.900"" ""$ 2.279.900""
 [6] ""$ 2.279.900"" ""$ 1.159.900"" ""$ 1.749.900"" ""$ 1.879.900"" ""$ 189.900""  
[11] ""$ 2.299.900"" ""$ 2.499.900"" ""$ 2.499.900"" ""$ 2.799.000"" ""$ 529.900""  
[16] ""$ 2.699.900"" ""$ 2.149.900"" ""$ 189.900""   ""$ 2.549.900"" ""$ 1.395.900""
[21] ""$ 249.900""   ""$ 41.900""    ""$ 319.900""   ""$ 149.900"" 

Question: How to get all the elements of this dynamic section? 
I guess, I could scroll the page until all elements are loaded and then use html(URL). But this seems like a lot of work (i'm planning of doing this on different sections). There should be a programmatic work around.
",13k,"
            22
        ","['\nAs @nrussell suggested, you can use RSelenium to programatically scroll down the page before getting the source code.\nYou could for example do:\nlibrary(RSelenium)\nlibrary(rvest)\n#start RSelenium\ncheckForServer()\nstartServer()\nremDr <- remoteDriver()\nremDr$open()\n\n#navigate to your page\nremDr$navigate(""http://www.linio.com.co/tecnologia/celulares-telefonia-gps/"")\n\n#scroll down 5 times, waiting for the page to load at each time\nfor(i in 1:5){      \nremDr$executeScript(paste(""scroll(0,"",i*10000,"");""))\nSys.sleep(3)    \n}\n\n#get the page html\npage_source<-remDr$getPageSource()\n\n#parse it\nhtml(page_source[[1]]) %>% html_nodes("".product-itm-price-new"") %>%\n  html_text()\n\n', '\nlibrary(rvest)\nurl<-""https://www.linio.com.co/c/celulares-y-tablets?page=1""\npage<-html_session(url)\n\nhtml_nodes(page,css="".price-secondary"") %>% html_text()\n\nLoop through the website https://www.linio.com.co/c/celulares-y-tablets?page=2 and 3 and so on and it will be easy for you to scrape the data\nEDIT dated 07/05/2019\nThe website elements changed. Hence new code\nlibrary(rvest)\nurl<-""https://www.linio.com.co/c/celulares-y-tablets?page=1""\npage<-html_session(url)\n\nhtml_nodes(page,css="".price-main"") %>% html_text()\n\n']",https://stackoverflow.com/questions/29861117/scraping-a-dynamic-ecommerce-page-with-infinite-scroll,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrape password-protected website in R,"
I'm trying to scrape data from a password-protected website in R.  Reading around, it seems that the httr and RCurl packages are the best options for scraping with password authentication (I've also looked into the XML package).
The website I'm trying to scrape is below (you need a free account in order to access the full page):
http://subscribers.footballguys.com/myfbg/myviewprojections.php?projector=2
Here are my two attempts (replacing ""username"" with my username and ""password"" with my password):
#This returns ""Status: 200"" without the data from the page:
library(httr)
GET(""http://subscribers.footballguys.com/myfbg/myviewprojections.php?projector=2"", authenticate(""username"", ""password""))

#This returns the non-password protected preview (i.e., not the full page):
library(XML)
library(RCurl)
readHTMLTable(getURL(""http://subscribers.footballguys.com/myfbg/myviewprojections.php?projector=2"", userpwd = ""username:password""))

I have looked at other relevant posts (links below), but can't figure out how to apply their answers to my case.
How to use R to download a zipped file from a SSL page that requires cookies
How to webscrape secured pages in R (https links) (using readHTMLTable from XML package)?
Reading information from a password protected site
R - RCurl scrape data from a password-protected site
http://www.inside-r.org/questions/how-scrape-data-password-protected-https-website-using-r-hold
",27k,"
            18
        ","['\nYou can use RSelenium. I have used the dev version as you can run phantomjs without a Selenium Server. \n# Install RSelenium if required. You will need phantomjs in your path or follow instructions\n# in package vignettes\n# devtools::install_github(""ropensci/RSelenium"")\n# login first\nappURL <- \'http://subscribers.footballguys.com/amember/login.php\'\nlibrary(RSelenium)\npJS <- phantom() # start phantomjs\nremDr <- remoteDriver(browserName = ""phantomjs"")\nremDr$open()\nremDr$navigate(appURL)\nremDr$findElement(""id"", ""login"")$sendKeysToElement(list(""myusername""))\nremDr$findElement(""id"", ""pass"")$sendKeysToElement(list(""mypass""))\nremDr$findElement(""css"", "".am-login-form input[type=\'submit\']"")$clickElement()\n\nappURL <- \'http://subscribers.footballguys.com/myfbg/myviewprojections.php?projector=2\'\nremDr$navigate(appURL)\ntableElem<- remDr$findElement(""css"", ""table.datamedium"")\nres <- readHTMLTable(header = TRUE, tableElem$getElementAttribute(""outerHTML"")[[1]])\n> res[[1]][1:5, ]\nRank             Name Tm/Bye Age Exp Cmp Att  Cm%  PYd Y/Att PTD Int Rsh  Yd TD FantPt\n1    1   Peyton Manning  DEN/4  38  17 415 620 66.9 4929  7.95  43  12  24   7  0 407.15\n2    2       Drew Brees   NO/6  35  14 404 615 65.7 4859  7.90  37  16  22  44  1 385.35\n3    3    Aaron Rodgers   GB/9  31  10 364 560 65.0 4446  7.94  33  13  52 224  3 381.70\n4    4      Andrew Luck IND/10  25   3 366 610 60.0 4423  7.25  27  13  62 338  2 361.95\n5    5 Matthew Stafford  DET/9  26   6 377 643 58.6 4668  7.26  32  19  34 102  1 358.60\n\nFinally when you are finished close phantomjs\npJS$stop()\n\nIf you want to use a traditional browser like firefox for example (if you wanted to stick to the version on CRAN) you would use:\nRSelenium::startServer()\nremDr <- remoteDriver()\n........\n........\nremDr$closeServer()\n\nin place of the related phantomjs calls.\n', '\nI don\'t have an account to test with, but maybe this will work:\nlibrary(httr)\nlibrary(XML)\n\nhandle <- handle(""http://subscribers.footballguys.com"") \npath   <- ""amember/login.php""\n\n# fields found in the login form.\nlogin <- list(\n  amember_login = ""username""\n ,amember_pass  = ""password""\n ,amember_redirect_url = \n   ""http://subscribers.footballguys.com/myfbg/myviewprojections.php?projector=2""\n)\n\nresponse <- POST(handle = handle, path = path, body = login)\n\nNow, the response object might hold what you need (or maybe you can directly query the page of interest after the login request; I am not sure the redirect will work, but it is a field in the web form), and handle might be re-used for subsequent requests. Can\'t test it; but this works for me in many situations.\nYou can output the table using XML\n> readHTMLTable(content(response))[[1]][1:5,]\n  Rank             Name Tm/Bye Age Exp Cmp Att  Cm%  PYd Y/Att PTD Int Rsh  Yd TD FantPt\n1    1   Peyton Manning  DEN/4  38  17 415 620 66.9 4929  7.95  43  12  24   7  0 407.15\n2    2       Drew Brees   NO/6  35  14 404 615 65.7 4859  7.90  37  16  22  44  1 385.35\n3    3    Aaron Rodgers   GB/9  31  10 364 560 65.0 4446  7.94  33  13  52 224  3 381.70\n4    4      Andrew Luck IND/10  25   3 366 610 60.0 4423  7.25  27  13  62 338  2 361.95\n5    5 Matthew Stafford  DET/9  26   6 377 643 58.6 4668  7.26  32  19  34 102  1 358.60\n\n']",https://stackoverflow.com/questions/24723606/scrape-password-protected-website-in-r,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pandas read_html ValueError: No tables found,"
I am trying to scrap the historical weather data from the ""https://www.wunderground.com/personal-weather-station/dashboard?ID=KMAHADLE7#history/tdata/s20170201/e20170201/mcustom.html"" weather underground page. I have the following code: 
import pandas as pd 

page_link = 'https://www.wunderground.com/personal-weather-station/dashboard?ID=KMAHADLE7#history/tdata/s20170201/e20170201/mcustom.html'
df = pd.read_html(page_link)
print(df)

I have the following response: 
Traceback (most recent call last):
 File ""weather_station_scrapping.py"", line 11, in <module>
  result = pd.read_html(page_link)
 File ""/anaconda3/lib/python3.6/site-packages/pandas/io/html.py"", line 987, in read_html
  displayed_only=displayed_only)
 File ""/anaconda3/lib/python3.6/site-packages/pandas/io/html.py"", line 815, in _parse raise_with_traceback(retained)
 File ""/anaconda3/lib/python3.6/site-packages/pandas/compat/__init__.py"", line 403, in raise_with_traceback
  raise exc.with_traceback(traceback)
ValueError: No tables found

Although, this page clearly has a table but it is not being picked by the read_html. I have tried using Selenium so that the page can be loaded before I read it. 
from selenium import webdriver
from selenium.webdriver.common.keys import Keys

driver = webdriver.Firefox()
driver.get(""https://www.wunderground.com/personal-weather-station/dashboard?ID=KMAHADLE7#history/tdata/s20170201/e20170201/mcustom.html"")
elem = driver.find_element_by_id(""history_table"")

head = elem.find_element_by_tag_name('thead')
body = elem.find_element_by_tag_name('tbody')

list_rows = []

for items in body.find_element_by_tag_name('tr'):
    list_cells = []
    for item in items.find_elements_by_tag_name('td'):
        list_cells.append(item.text)
    list_rows.append(list_cells)
driver.close()

Now, the problem is that it cannot find ""tr"". I would appreciate any suggestions. 
",42k,"
            12
        ","['\nHere\'s a solution using selenium for browser automation\nfrom selenium import webdriver\nimport pandas as pd\ndriver = webdriver.Chrome(chromedriver)\ndriver.implicitly_wait(30)\n\ndriver.get(\'https://www.wunderground.com/personal-weather-station/dashboard?ID=KMAHADLE7#history/tdata/s20170201/e20170201/mcustom.html\')\n    df=pd.read_html(driver.find_element_by_id(""history_table"").get_attribute(\'outerHTML\'))[0]\n\nTime    Temperature Dew Point   Humidity    Wind    Speed   Gust    Pressure  Precip. Rate. Precip. Accum.  UV  Solar\n0   12:02 AM    25.5 掳C 18.7 掳C 75 %    East    0 kph   0 kph   29.3 hPa    0 mm    0 mm    0   0 w/m虏\n1   12:07 AM    25.5 掳C 19 掳C   76 %    East    0 kph   0 kph   29.31 hPa   0 mm    0 mm    0   0 w/m虏\n2   12:12 AM    25.5 掳C 19 掳C   76 %    East    0 kph   0 kph   29.31 hPa   0 mm    0 mm    0   0 w/m虏\n3   12:17 AM    25.5 掳C 18.7 掳C 75 %    East    0 kph   0 kph   29.3 hPa    0 mm    0 mm    0   0 w/m虏\n4   12:22 AM    25.5 掳C 18.7 掳C 75 %    East    0 kph   0 kph   29.3 hPa    0 mm    0 mm    0   0 w/m虏\n\nEditing with breakdown of exactly what\'s happening, since the above one-liner is actually not very good self-documenting code:\nAfter setting up the driver, we select the table with its ID value (Thankfully this site actually uses reasonable and descriptive IDs)\ntab=driver.find_element_by_id(""history_table"")\n\nThen, from that element, we get the HTML instead of the web driver element object\ntab_html=tab.get_attribute(\'outerHTML\')\n\nWe use pandas to parse the html\ntab_dfs=pd.read_html(tab_html)\n\nFrom the docs:\n\n""read_html returns a list of DataFrame objects, even if there is only\n  a single table contained in the HTML content""\n\nSo we index into that list with the only table we have, at index zero\ndf=tab_dfs[0]\n\n', '\nYou can use requests and avoid opening browser. \nYou can get current conditions by using:\nhttps://stationdata.wunderground.com/cgi-bin/stationlookup?station=KMAHADLE7&units=both&v=2.0&format=json&callback=jQuery1720724027235122559_1542743885014&_=15\nand strip of \'jQuery1720724027235122559_1542743885014(\' from the left and \')\' from the right. Then handle the json string.\nYou can get summary and history by calling the API with the following\nhttps://api-ak.wunderground.com/api/606f3f6977348613/history_20170201null/units:both/v:2.0/q/pws:KMAHADLE7.json?callback=jQuery1720724027235122559_1542743885015&_=1542743886276\nYou then need to strip \'jQuery1720724027235122559_1542743885015(\' from the front and \');\' from the right. You then have a JSON string you can parse. \nSample of JSON:\n\nYou can find these URLs by using F12 dev tools in browser and inspecting the network tab for the traffic created during page load.\nAn example for current, noting there seems to be a problem with nulls in the JSON so I am replacing with ""placeholder"":\nimport requests\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\nfrom bs4 import BeautifulSoup\n\nurl = \'https://stationdata.wunderground.com/cgi-bin/stationlookup?station=KMAHADLE7&units=both&v=2.0&format=json&callback=jQuery1720724027235122559_1542743885014&_=15\'\nres = requests.get(url)\nsoup = BeautifulSoup(res.content, ""lxml"")\ns = soup.select(\'html\')[0].text.strip(\'jQuery1720724027235122559_1542743885014(\').strip(\')\')\ns = s.replace(\'null\',\'""placeholder""\')\ndata= json.loads(s)\ndata = json_normalize(data)\ndf = pd.DataFrame(data)\nprint(df)\n\n']",https://stackoverflow.com/questions/53398785/pandas-read-html-valueerror-no-tables-found,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Beautiful Soup 4 find_all don't find links that Beautiful Soup 3 finds,"
I noticed a really annoying bug: BeautifulSoup4 (package: bs4) often finds less tags than the previous version (package: BeautifulSoup).
Here's a reproductible instance of that issue:
import requests
import bs4
import BeautifulSoup

r = requests.get('http://wordpress.org/download/release-archive/')
s4 = bs4.BeautifulSoup(r.text)
s3 = BeautifulSoup.BeautifulSoup(r.text)

print 'With BeautifulSoup 4 : {}'.format(len(s4.findAll('a')))
print 'With BeautifulSoup 3 : {}'.format(len(s3.findAll('a')))

Output:
With BeautifulSoup 4 : 557
With BeautifulSoup 3 : 1701

The difference is not minor as you can see.
Here are the exact versions of the modules in case someone is wondering:
In [20]: bs4.__version__
Out[20]: '4.2.1'

In [21]: BeautifulSoup.__version__
Out[21]: '3.2.1'

",4k,"
            0
        ","[""\nYou have lxml installed, which means that BeautifulSoup 4 will use that parser over the standard-library html.parser option.\nYou can upgrade lxml to 3.2.1 (which for me returns 1701 results for your test page); lxml itself uses libxml2 and libxslt which may be to blame too here. You may have to upgrade those instead / as well. See the lxml requirements page; currently libxml2 2.7.8 or newer is recommended.\nOr explicitly specify the other parser when parsing the soup:\ns4 = bs4.BeautifulSoup(r.text, 'html.parser')\n\n""]",https://stackoverflow.com/questions/17698836/beautiful-soup-4-find-all-dont-find-links-that-beautiful-soup-3-finds,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Android Web Scraping with a Headless Browser [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I have spent a day on researching a library that can be used to accomplish the following:

Retrieve the full contents of a webpage like in the background without rendering result to a view.
The lib should support pages that fires off ajax requests to load some additional result data after the initial HTML has loaded for example.
From the resulting html I need to grab elements in xpath or css selector form.
In future I also possibly need to navigate to a next page (fire off events, submitting buttons/links etc)

Here is what I have tried without success:

Jsoup: Works great but no support for javascript/ajax (so it does not load full page)
Android built in HttpEntity: same problem with javascript/ajax as jsoup
HtmlUnit: Looks exactly what I need but after hours cannot get it to work on Android (Other users failed by trying to load the 12MB+ worth of jar files.  I myself loaded the full source code and referenced it as a project library only to find that things such as Applets and java.awt (used by HtmlUnit) does not exist in Android).
Rhino - I find this very confusing and don't know how to get it working in Android and even if it is what I am looking for.
Selenium Driver: Looks like it can work but you don't have an straightforward way to implement it in a headless way so that you don't have the actual html displayed to a view.

I really want HtmlUnit to work as it seems the best suited for my solution.  Is there any way or at least another library I have missed that is suitable for my needs?
I am currently using Android Studio 0.1.7 and can move to Ellipse if needed.
Thanks in advance!
",21k,"
            35
        ","['\nOk after 2 weeks I admit defeat and are using a workaround which works great for me at the moment.\nThe problem: \nIt is too difficult to port HTMLUnit to Android (or at least with my level of expertise).  I am sure its a worthwhile project (and not that time consuming for experienced java programmer) . I emailed the guys at HTMLUnit and they commented that they are not looking into a port or what effort will be involved but suggested anyone who wants to start with such a project should send an message to their mailing list to get more developers involved (http://htmlunit.sourceforge.net/mail-lists.html).\nThe workaround: \nI used android\'s built in WebView and overrided the onPageFinished method of Webview class to inject Javascript that grabs all the html after the page has fully loaded. Webview can also be used to called futher javascript actions, clicking buttons, filling in forms etc.\nCode: \nwebView.getSettings().setJavaScriptEnabled(true);\nMyJavaScriptInterface jInterface = new MyJavaScriptInterface();\nwebView.addJavascriptInterface(jInterface, ""HtmlViewer"");\n\nwebView.setWebViewClient(new WebViewClient() {\n\n    @Override\n    public void onPageFinished(WebView view, String url) {\n       //Load HTML\n       webView.loadUrl(""javascript:window.HtmlViewer.showHTML(\'<html>\'+document.getElementsByTagName(\'html\')[0].innerHTML+\'</html>\');"");\n    }\n\n}\n\nwebView.loadUrl(StartURL);\nParseHtml(jInterface.html);   \n\npublic class MyJavaScriptInterface {\n\n    public String html;\n\n    @JavascriptInterface\n    public void showHTML(String _html) {\n        html = _html;\n    }\n}\n\n', ""\nI have taken the implementation mentioned above (injecting JavaScript) and that works for me. All I do is simply set the visibility of the webview to be hidden under other UI elements. I was also thinking of doing the same with selenium. I have used selenium with Chrome in Python and it's great but like you mentioned it is not easy to not show the browser window. But I think it might be possible to just not show the component in Android. I'll have to try.\n""]",https://stackoverflow.com/questions/17399055/android-web-scraping-with-a-headless-browser,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jsoup Cookies for HTTPS scraping,"
I am experimenting with this site to gather my username on the welcome page to learn Jsoup and Android.  Using the following code
Connection.Response res = Jsoup.connect(""http://www.mikeportnoy.com/forum/login.aspx"")
    .data(""ctl00$ContentPlaceHolder1$ctl00$Login1$UserName"", ""username"", ""ctl00$ContentPlaceHolder1$ctl00$Login1$Password"", ""password"")
    .method(Method.POST)
    .execute();
String sessionId = res.cookie("".ASPXAUTH"");

Document doc2 = Jsoup.connect(""http://www.mikeportnoy.com/forum/default.aspx"")
.cookie("".ASPXAUTH"", sessionId)
.get();

My cookie (.ASPXAUTH) always ends up NULL.  If I delete this cookie in a webbrowser, I lose my connection.  So I am sure it is the correct cookie.  In addition, if I change the code
.cookie("".ASPXAUTH"", ""jkaldfjjfasldjf"")  Using the correct values of course

I am able to scrape my login name from this page.  This also makes me think I have the correct cookie.  So, how come my cookie comes up Null?  Are my username and password name fields incorrect?  Something else?  
Thanks.
",36k,"
            28
        ","['\nI know I\'m kinda late by 10 months here. But a good option using Jsoup is to use this easy peasy piece of code:\n//This will get you the response.\nResponse res = Jsoup\n    .connect(""url"")\n    .data(""loginField"", ""login@login.com"", ""passField"", ""pass1234"")\n    .method(Method.POST)\n    .execute();\n\n//This will get you cookies\nMap<String, String> cookies = res.cookies();\n\n//And this is the easieste way I\'ve found to remain in session\nDocumente doc = Jsoup.connect(""url"").cookies(cookies).get();\n\nThough I\'m still having trouble connection to SOME websites, I connect to a whole lot of them with the same basic piece of code. Oh, and before I forget.. What I figured my problem is, is SSL certificates. You have to properly manage them in a way I still haven\'t quite figured out. \n', '\nI always do this in two steps (like normal human),\n\nRead login page (by GET, read cookies)\nSubmit form and cookies (by POST, without cookie manipulation)\n\nExample:\nConnection.Response response = Jsoup.connect(""http://www.mikeportnoy.com/forum/login.aspx"")\n        .method(Connection.Method.GET)\n        .execute();\n\nresponse = Jsoup.connect(""http://www.mikeportnoy.com/forum/login.aspx"")\n        .data(""ctl00$ContentPlaceHolder1$ctl00$Login1$UserName"", ""username"")\n        .data(""ctl00$ContentPlaceHolder1$ctl00$Login1$Password"", ""password"")\n        .cookies(response.cookies())\n        .method(Connection.Method.POST)\n        .execute();\n\nDocument homePage = Jsoup.connect(""http://www.mikeportnoy.com/forum/default.aspx"")\n        .cookies(response.cookies())\n        .get();\n\nAnd always set cookies from previuos request to next using\n         .cookies(response.cookies())\n\nSSL is not important here. If you have problem with certifcates then execute this method for ignore SSL.\npublic static void trustEveryone() {\n    try {\n        HttpsURLConnection.setDefaultHostnameVerifier(new HostnameVerifier() {\n            public boolean verify(String hostname, SSLSession session) {\n                return true;\n            }\n        });\n\n        SSLContext context = SSLContext.getInstance(""TLS"");\n        context.init(null, new X509TrustManager[]{new X509TrustManager() {\n            public void checkClientTrusted(X509Certificate[] chain, String authType) throws CertificateException { }\n\n            public void checkServerTrusted(X509Certificate[] chain, String authType) throws CertificateException { }\n\n            public X509Certificate[] getAcceptedIssuers() {\n                return new X509Certificate[0];\n            }\n        }}, new SecureRandom());\n        HttpsURLConnection.setDefaultSSLSocketFactory(context.getSocketFactory());\n    } catch (Exception e) { // should never happen\n        e.printStackTrace();\n    }\n}\n\n', '\nWhat if you try fetching and passing all cookies without assuming anything like this: Sending POST request with username and password and save session cookie\nIf you still have problems try looking in to this: Issues with passing cookies to GET request (after POST)\n']",https://stackoverflow.com/questions/7139178/jsoup-cookies-for-https-scraping,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping webpage generated by JavaScript with C#,"
I have a web browser, and a label in Visual Studio, and basically what I'm trying to do is grab a section from another webpage.
I tried using WebClient.DownloadString and WebClient.DownloadFile, and both of them give me the source code of the web page before the JavaScript loads the content.  My next idea was to use a web browser tool and just call webBrowser.DocumentText after the page loaded and that did not work, it still gives me the original source of the page.
Is there a way I can grab the page post JavaScript load?
",31k,"
            23
        ","['\nThe problem is the browser usually executes the javascript and it results with an updated DOM. Unless you can analyze the javascript or intercept the data it uses, you will need to execute the code as a browser would. In the past I ran into the same issue, I utilized selenium and PhantomJS to render the page. After it renders the page, I would use the WebDriver client to navigate the DOM and retrieve the content I needed, post AJAX. \nAt a high-level, these are the steps:\n\nInstalled selenium: http://docs.seleniumhq.org/\nStarted the selenium hub as a service\nDownloaded phantomjs (a headless browser, that can execute the javascript): http://phantomjs.org/\nStarted phantomjs in webdriver mode pointing to the selenium hub\nIn my scraping application installed the webdriver client nuget package: Install-Package Selenium.WebDriver\n\nHere is an example usage of the phantomjs webdriver:\nvar options = new PhantomJSOptions();\noptions.AddAdditionalCapability(""IsJavaScriptEnabled"",true);\n\nvar driver = new RemoteWebDriver( new URI(Configuration.SeleniumServerHub),\n                    options.ToCapabilities(),\n                    TimeSpan.FromSeconds(3)\n                  );\ndriver.Url = ""http://www.regulations.gov/#!documentDetail;D=APHIS-2013-0013-0083"";\ndriver.Navigate();\n//the driver can now provide you with what you need (it will execute the script)\n//get the source of the page\nvar source = driver.PageSource;\n//fully navigate the dom\nvar pathElement = driver.FindElementById(""some-id"");\n\nMore info on selenium, phantomjs and webdriver can be found at the following links:\nhttp://docs.seleniumhq.org/\nhttp://docs.seleniumhq.org/projects/webdriver/\nhttp://phantomjs.org/\nEDIT: Easier Method \nIt appears there is a nuget package for the phantomjs, such that you don\'t need the hub (I used a cluster to do massive scrapping in this manner):\nInstall web driver:\nInstall-Package Selenium.WebDriver\n\nInstall embedded exe:\nInstall-Package phantomjs.exe\n\nUpdated code:\nvar driver = new PhantomJSDriver();\ndriver.Url = ""http://www.regulations.gov/#!documentDetail;D=APHIS-2013-0013-0083"";\ndriver.Navigate();\n//the driver can now provide you with what you need (it will execute the script)\n//get the source of the page\nvar source = driver.PageSource;\n//fully navigate the dom\nvar pathElement = driver.FindElementById(""some-id"");\n\n', '\nThanks to wbennet, I discovered PhantomJSCloud.com. Enough free service to scrap pages through web API calls.\npublic static string GetPagePhantomJs(string url)\n{\n    using (var client = new System.Net.Http.HttpClient())\n    {\n        client.DefaultRequestHeaders.ExpectContinue = false;\n        var pageRequestJson = new System.Net.Http.StringContent\n            (@""{\'url\':\'"" + url + ""\',\'renderType\':\'html\',\'outputAsJson\':false }"");\n        var response = client.PostAsync\n            (""https://PhantomJsCloud.com/api/browser/v2/{YOUR_API_KEY}/"",\n            pageRequestJson).Result;\n        return response.Content.ReadAsStringAsync().Result;\n    }\n}\n\nYeah.\n', '\nok i will show you how to enable javascript using phantomjs and selenuim with c# \n\ncreate a new console project name it as you want \ngo to solution explorer in your right hand \na right click  on References click on Manage NuGet packages\na windows will shows click on browse than install Selenium.WebDriver \ndownold phantomjs from here Phantomjs\nin your main function type this code \n    var options = new PhantomJSOptions();\n    options.AddAdditionalCapability(""IsJavaScriptEnabled"", true);\n    IWebDriver driver = new PhantomJSDriver(""phantomjs Folder Path"", options);\n    driver.Navigate().GoToUrl(""https://www.yourwebsite.com/"");\n\n    try\n    {\n        string pagesource = driver.PageSource;\n        driver.FindElement(By.Id(""yourelement""));\n        Console.Write(""yourelement founded"");\n\n    }\n    catch (Exception e)\n    {\n        Console.WriteLine(e.Message);\n\n    }\n\n    Console.Read();\n\n\n\ndon\'t forget to put yourwebsite and the element that you loooking for and the phantomjs.exe path in you machine in this code below\n\nhave great time of coding and thanks wbennett\n']",https://stackoverflow.com/questions/24288726/scraping-webpage-generated-by-javascript-with-c-sharp,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
web scraping dynamic content with python,"
I'd like to use Python to scrape the contents of the ""Were you looking for these authors:"" box on web pages like this one: http://academic.research.microsoft.com/Search?query=lander
Unfortunately the contents of the box get loaded dynamically by JavaScript. Usually in this situation I can read the Javascript to figure out what's going on, or I can use an browser extension like Firebug to figure out where the dynamic content is coming from. No such luck this time...the Javascript is pretty convoluted and Firebug doesn't give many clues about how to get at the content.
Are there any tricks that will make this task easy? 
",21k,"
            6
        ","['\nInstead of trying to reverse engineer it, you can use ghost.py to directly interact with JavaScript on the page.\nIf you run the following query in a chrome console, you\'ll see it returns everything you want.\ndocument.getElementsByClassName(\'inline-text-org\');\n\nReturns\n[<div class=\u200b""inline-text-org"" title=\u200b""University of Manchester"">\u200bUniversity of Manchester\u200b</div>, \n <div class=\u200b""inline-text-org"" title=\u200b""University of California Irvine"">\u200bUniversity of California ...\u200b</div>\u200b\n  etc...\n\nYou can run JavaScript through python in a real life DOM using ghost.py.\nThis is really cool:\nfrom ghost import Ghost\nghost = Ghost()\npage, resources = ghost.open(\'http://academic.research.microsoft.com/Search?query=lander\')\nresult, resources = ghost.evaluate(\n    ""document.getElementsByClassName(\'inline-text-org\');"")\n\n', ""\nA very similar question was asked earlier here.\nQuoted is selenium, originally a testing environment for web-apps.\nI usually use Chrome's Developer Mode, which IMHO already gives even more details than Firefox.\n"", ""\nFor scraping dynamic content, you need not a simple scraper but a full-fledged headless browser.\ndhamaniasad/HeadlessBrowsers: A list of (almost) all headless web browsers in existence is the fullest list of these that I've seen; it lists which languages each has bindings for.\n(Note that more than a few of the listed projects are abandoned!)\n""]",https://stackoverflow.com/questions/17608572/web-scraping-dynamic-content-with-python,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic dropdown doesn't populate with auto suggestions on https://www.nseindia.com/ when values are passed using Selenium and Python,"
driver = webdriver.Chrome('C:/Workspace/Development/chromedriver.exe')
driver.get('https://www.nseindia.com/companies-listing/corporate-filings-actions')
inputbox = driver.find_element_by_xpath('/html/body/div[7]/div[1]/div/section/div/div/div/div/div/div[1]/div[1]/div[1]/div/span/input[2]')
inputbox.send_keys(""Reliance"")

I'm trying to scrape the table from this website that would appear after you key in the company name in the textfield above it. The attached code block works well with such similar drop-downs of a normal google search and wolfram website, but when i run my script on the required website, that essentially just inputs the required text in the textfield - the dropdown shows 'No Records Found', whereas, when done manually it works well.
",2k,"
            3
        ","['\nI executed your test adding a few tweaks and ran the test as follows:\n\nCode Block:\nfrom selenium import webdriver        \nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\n\noptions = webdriver.ChromeOptions() \noptions.add_argument(""start-maximized"")\noptions.add_experimental_option(""excludeSwitches"", [""enable-automation""])\noptions.add_experimental_option(\'useAutomationExtension\', False)\ndriver = webdriver.Chrome(options=options, executable_path=r\'C:\\WebDrivers\\chromedriver.exe\')\ndriver.get(\'https://www.nseindia.com/companies-listing/corporate-filings-actions\')\nWebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, ""//div[@id=\'Corporate_Actions_equity\']//input[@placeholder=\'Company Name or Symbol\']""))).send_keys(""Reliance"")\n\nObservation: Similar to your observation, I have hit the same roadblock with no results as follows:\n\n\n\nDeep Dive\nIt seems the click() on the element with text as Get Data does happens. But while inspecting the DOM Tree of the webpage you will find that some of the <script> tag refers to JavaScripts having keyword akam. As an example:\n\n<script type=""text/javascript"" src=""https://www.nseindia.com/akam/11/3b383b75"" defer=""""></script>\n<noscript><img src=""https://www.nseindia.com/akam/11/pixel_3b383b75?a=dD02ZDMxODU2ODk2YTYwODA4M2JlOTlmOGNkZTY3Njg4ZWRmZjE4YmMwJmpzPW9mZg=="" style=""visibility: hidden; position: absolute; left: -999px; top: -999px;"" /></noscript>\n\nWhich is a clear indication that the website is protected by Bot Manager an advanced bot detection service provided by Akamai and the response gets blocked.\n\nBot Manager\nAs per the article Bot Manager - Foundations:\n\n\nConclusion\nSo it can be concluded that the request for the data is detected as being performed by Selenium driven WebDriver instance and the response is blocked.\n\nReferences\nA couple of documentations:\n\nBot Manager\nBot Manager : Foundations\n\n\ntl; dr\nA couple of relevant discussions:\n\nSelenium webdriver: Modifying navigator.webdriver flag to prevent selenium detection\nUnable to use Selenium to automate Chase site login\n\n']",https://stackoverflow.com/questions/62457093/dynamic-dropdown-doesnt-populate-with-auto-suggestions-on-https-www-nseindia,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to click a link by text with No Text in Python,"
I am trying to scrape a Wine data from vivino.com and using selenium to automate it and scrape as many data as possible. My code looks like this:
import time 
from selenium import webdriver

browser = webdriver.Chrome('C:\Program Files (x86)\chromedriver.exe')

browser.get('https://www.vivino.com/explore?e=eJwFwbEOQDAUBdC_uaNoMN7NZhQLEXmqmiZaUk3x987xkVXRwLtAVcLLy7qE_tiN0Bz6FhcV7M4s0ZkkB86VUZIL9l4kmyjW4ORmbo0nTTPVDxlkGvg%3D&cart_item_source=nav-explore') # Vivino Website with 5 wines for now (simple example). Plan to scrape around 10,000 wines 

lenOfPage = browser.execute_script(""window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;"")

match=False
while(match==False):
    lastCount = lenOfPage
    time.sleep(7)
    lenOfPage = browser.execute_script(""window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;"")
    if lastCount==lenOfPage:
        match=True

That opens a website with 5 wines and scrolls down. Now I want to click to hyperlink of the wine one by one to scrape information about its price, wine grapes sort, etc. So, basically my script will try scroll down which allows to have as many wines displayed on the page and then click to a first hyperlink, get additional information and go back. Then, the process will repeat. I don't think that's an efficient strategy but that's what I came up so far.
The problem I have is with hyperlink in the vivino website. There is no text near the href link which allows me to use find_element_by_link_text function:
<a class=""anchor__anchor--2QZvA"" href=""/weingut-r-a-pfaffl-austrian-cherry-zweigelt/w/1261542?year=2018&amp;price_id=23409078&amp;cart_item_source=direct-explore"" target=""_blank"">

Could you please suggest the way how click for a wine with Selenium that has not text after the hyperlink? I haven't found proper answer during my web search. Thanks in advance
",740,"
            1
        ","['\nYou\'re doing way more work than you have to - with Selenium I mean. When visiting the page, I logged my network traffic with Google Chrome\'s dev tools, and I saw that my browser made an HTTP GET request to a REST API, the response of which is JSON and contains all the wine/price information you could ever want. So, you don\'t need to do any scraping. Just imitate that GET request with the desired query-string parameters and the correct headers. It seems the REST API just cares about the user-agent header, which is trivial.\n\n\nFirst, visit the URL\nin Google Chrome.\nPress the F12 key to open the Google Chrome dev tools menu.\nClick on the Network tab.\nClick on the round Record button. It should turn red, and it will\nstart logging all network traffic in the log below. Click on the\nFilter button next to it, and then click on XHR. This will only\nshow XHR (XmlHttpRequest) requests in the log. We are interested in\nthese requests specifically, because it is via XHR that requests to\nAPIs are typically made. Here\'s what it should look like now:\n\n\n\nWith the Chrome dev tools menu still open, right-click (not\nleft-click) on the page refresh button to reveal a drop-down menu.\nThen, click on Empty Cache and Hard Reload.\n\n\nThis will empty your browser\'s cache for this page, and force your\nbrowser to refresh the page. As the page is being refreshed, you\nshould start to see some entries appearing in the traffic log.\n\nThere should now be some XHR request entries in the log. We don\'t know which one of these is the one we\'re actually interested in, so we just look at all of them until we find one that looks like it could be the right one (or, if it contains the information we\'re looking for, like information for individual wines, etc.). I happen to know that we are interested in the one that starts with explore?..., so let\'s click on that.\n\nOnce you click on the entry, a panel will open on the right of the\nlog. Click on the Headers tab.\n\n\nThis tab contains all the information regarding how this request was made. Under the General area, you can see the Request URL, which is the URL to the REST API endpoint that we made a request to. This URL might be quite long, because it will also typically contain the query-string parameters (those are the key-value pairs that come after explore?, like country_code=DE or currency_code=EUR. They are separated by &). The query-string parameters are important, because they contain information about certain filters that we want to apply to our query. In my code example, I\'ve removed them from the REST API endpoint URL, and instead moved them into the params dictionary. This step isn\'t required - you could also just leave them in the URL, but I find that it is easier to read and modify this way. The query-string parameters are also important because, sometimes, certains APIs will expect certain parameters to be present in the request, or they will expect them to have certain values - in other words, some APIs are very picky about their query-string parameters, and if you remove them or tamper with them in a way that the API doesn\'t expect, the API will say that your request isn\'t formulated correctly.\nIn the General area, you can also see Request Method, which in our case is GET. This tells us, that our browser made an HTTP GET request. Not all API endpoints work the same, some want HTTP POST, etc.\nStatus Code tells us what status code the server sent back. 200 means everything went OK. You can learn more about HTTP status codes here.\nLet\'s take a look at the Response Headers area. This area contains all the response headers that the server sent back after the request was made. These can be useful for a browser for things like setting cookies or knowing how to interpret the data the server has sent back.\nThe Request Headers area contains all the headers that your browser sent to the server when it made the request. Usually, it\'s a good idea to copy all of these key-value pairs and turn them into a Python dictionary headers, because that way you can be sure that your Python script will make the exact same request that your browser made. However, usually, I like to trim this down as much as I can. I know that many APIs desperately care about the user-agent field, so usually I\'ll keep that one, but sometimes they also care about the referer. As you work with different APIs, you\'ll have to just kind of figure out which request headers the API cares about through trial-and-error. This API happens to only care about the user-agent.\nThe last area Query String Parameters is just a cute way of showing the query-string parameters from the Request URL in a human-friendly list of key-value pairs. Sometimes it\'s helpful to copy them from here, rather than from the URL.\n\nNow, click on the Preview tab, next to the Headers tab.\n\n\nThe Preview tab contains a pretty-printed preview of the actual data that was sent back as a result of the browser\'s request. In our case, this contains the JSON data sent back by the server. You can click on the little gray triangles to expand or collapse certain parts of the JSON structure, to reveal different data.\n\nLooking at this, I can tell that the JSON response is one big dictionary, which has a key explore_vintage, whose value is another dictionary, which has a key records whose value is a list of dictionaries, where each dictionary in this list represents one wine object. Expanding the first record (the 0th one) reveals all information regarding the first wine in the list. You can explore these structures as much as you like to see what kinds of information are available to you.\n\n\ndef main():\n\n    import requests\n\n    url = ""https://www.vivino.com/api/explore/explore""\n\n    params = {\n        ""country_code"": ""DE"",\n        ""currency_code"": ""EUR"",\n        ""grape_filter"": ""varietal"",\n        ""min_rating"": ""3.5"",\n        ""order_by"": ""ratings_average"",\n        ""order"": ""desc"",\n        ""page"": ""1"",\n        ""price_range_max"": ""30"",\n        ""price_range_min"": ""7"",\n        ""wine_type_ids[]"": ""1""\n    }\n\n    headers = {\n        ""user-agent"": ""Mozilla/5.0""\n    }\n\n    response = requests.get(url, params=params, headers=headers)\n    response.raise_for_status()\n\n    records = response.json()[""explore_vintage""][""records""]\n\n    for record in records:\n        name = record[""vintage""][""name""]\n        price = record[""price""][""amount""]\n        currency = record[""price""][""currency""][""code""]\n        print(f""\\""{name}\\"" - Price: {price} {currency}"")\n\n    return 0\n\n\nif __name__ == ""__main__"":\n    import sys\n    sys.exit(main())\n\nOutput:\n""Varvaglione Cosimo Varvaglione Collezione Privata Primitivo di Manduria 2015"" - Price: 21.9 EUR\n""Masseria Borgo dei Trulli Mirea Primitivo di Manduria 2019"" - Price: 19.9 EUR\n""Vigneti del Salento Vigne Vecchie Primitivo di Manduria 2016"" - Price: 22.95 EUR\n""Vigneti del Salento Vigne Vecchie Leggenda Primitivo di Manduria 2016"" - Price: 17.87 EUR\n""Varvaglione Papale Linea Oro Primitivo di Manduria 2016"" - Price: 18.85 EUR\n""Caballo Loco Grand Cru Apalta 2014"" - Price: 27.9 EUR\n""Luccarelli Il Bacca Old Vine Primitivo di Manduria 2016"" - Price: 20.9 EUR\n""Mottura Stilio Primitivo di Manduria 2018"" - Price: 12.89 EUR\n""Caballo Loco Grand Cru Maipo 2015"" - Price: 24.81 EUR\n""Lorusso Michele Solone Primitivo 2017"" - Price: 21.39 EUR\n""Ch芒teau Purcari Negru de Purcari 2017"" - Price: 29.8 EUR\n""San Marzano 60 Sessantanni Limited Edition Old Vines Primitivo di Manduria 2016"" - Price: 22.85 EUR\n""San Marzano 60 Sessantanni Old Vines Primitivo di Manduria 2016"" - Price: 20.9 EUR\n""San Marzano 60 Sessantanni Old Vines Primitivo di Manduria 2017"" - Price: 17.775 EUR\n""Lenotti Amarone della Valpolicella Classico 2015"" - Price: 27.95 EUR\n""Zeni Cruino Rosso Veronese 2015"" - Price: 22.9 EUR\n""Masseria Pietrosa Palmenti Primitivo di Manduria Vigne Vecchie 2016"" - Price: 25 EUR\n""Ravazzi Prezioso 2016"" - Price: 29.95 EUR\n""Nino Negri Sfursat Carlo Negri 2017"" - Price: 23.89 EUR\n""Quinta do Paral Reserva Tinto 2017"" - Price: 29.24 EUR\n""Wildekrans Barrel Select Reserve Pinotage 2016"" - Price: 29.9 EUR\n""Caballo Loco Grand Cru Limar铆 2016"" - Price: 27.9 EUR\n""San Marzano F Negroamaro 2018"" - Price: 16.9 EUR\n""Atlan & Artisan 8 Vents Mallorca 2018"" - Price: 19 EUR\n""Schneider Rooi Olifant Red 2017"" - Price: 19.5 EUR\n>>> \n\nIt just seems to grab twenty-five records/wines per page, but changing the page key-value pair in the params query-string parameter dictionary will yield the records from whatever page you desire. I\'m currently located in Germany, that\'s why my country_code and currency_code are ""DE"" and ""EUR"", but you should be able to change those to suit your needs.\n\nEDIT - here are some more key-value pairs you may be interested in, though I would recommend you get familiar with how your browser\'s dev tools work so that you can discover these fields in the JSON yourself:\nrecord[""vintage""][""year""]\nrecord[""vintage""][""wine""][""region""][""name""]\nrecord[""vintage""][""wine""][""region""][""country""][""name""]\nrecord[""vintage""][""wine""][""taste""][""structure""][""acidity""]\nrecord[""vintage""][""wine""][""taste""][""structure""][""intensity""]\nrecord[""vintage""][""wine""][""taste""][""structure""][""sweetness""]\nrecord[""vintage""][""wine""][""taste""][""structure""][""tannin""]\nrecord[""vintage""][""wine""][""style""][""grapes""][0][""name""] # 0th grape information\nrecord[""vintage""][""wine""][""winery""][""name""]\n\nThe JSON unfortunately doesn\'t contain the alcohol content. This information is hardcoded in the HTML of a given wine\'s summary page. You would have to make a request to each wine summary page and pull the alcohol content value out of the page, maybe with a regex.\n', ""\nJust to answer your question, how to select the a tag with selenium to perform the .click():\nlinks = browser.find_elements_by_css_selector('div.cleanWineCard__cleanWineCard--tzKxV.cleanWineCard__row--CBPRR > a')\n\nUsing the css selector you can locate the div tag where the image of the wine is placed in surrounded by an a tag.\nNow you could loop the links perform the .click() and do what ever you wanna do:\nfor a in links:\n    a.click()\n    # grab information\n    # sleep a bit\n    #...\n\n"", '\n1-\nBy beautiful soup you can do the following. I change the code a little but you can still use the beautiful soup part.\nfrom bs4 import BeautifulSoup\nimport requests\nimport time, os\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\nchromedriver = ""C:\\\\Program Files\\Google\\Chrome\\Application\\chromedriver"" # path to the chromedriver executable\nos.environ[""webdriver.chrome.driver""] = chromedriver\ndriver = webdriver.Chrome(chromedriver)\ndriver.get(\'https://www.vivino.com/explore?e=eJwFwbEOQDAUBdC_uaNoMN7NZhQLEXmqmiZaUk3x987xkVXRwLtAVcLLy7qE_tiN0Bz6FhcV7M4s0ZkkB86VUZIL9l4kmyjW4ORmbo0nTTPVDxlkGvg%3D&cart_item_source=nav-explore\')\n\n#browser.get(\'https://www.vivino.com/explore?e=eJwFwbEOQDAUBdC_uaNoMN7NZhQLEXmqmiZaUk3x987xkVXRwLtAVcLLy7qE_tiN0Bz6FhcV7M4s0ZkkB86VUZIL9l4kmyjW4ORmbo0nTTPVDxlkGvg%3D&cart_item_source=nav-explore\') # Vivino Website with 5 wines for now (simple example). Plan to scrape around 10,000 wines \n\nlenOfPage = driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;"")\n\nlist= []\n\nmatch=False\nwhile(match==False):\n    lastCount = lenOfPage\n    time.sleep(7)\n    lenOfPage = driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;"")\n    match=True\n    soup = BeautifulSoup(driver.page_source, \'html.parser\')\n\n    for a in soup.find_all(\'a\', href=True):\n        print(  a[\'href\'] )\n\nand it will list you all the hrefs.\n2- For alternative: selenium also supports getting element by XPath. you can inspect the item and copy the XPath and then you can use it. Please check the guide below. It will help.\nhttps://selenium-python.readthedocs.io/locating-elements.html\nfor example the following works. I got this working code from at link https://stackoverflow.com/a/63828196/3756587. all credit goes to user mamal : https://stackoverflow.com/users/4941102/mamal\n    links = driver.find_elements_by_xpath(\'//*[@href]\')\n    for i in links:\n        print(i.get_attribute(\'href\'))\n\nNow you have all the links.  With a simple for loop, you can proceed.\nEnjoy!\n']",https://stackoverflow.com/questions/65585597/how-to-click-a-link-by-text-with-no-text-in-python,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping dynamic data selenium - Unable to locate element,"
I am very new to scraping and have a question. I am scraping worldometers covid data. As it is dynamic - I am doing it with selenium.
The code is the following:
from selenium import webdriver
import time

URL = ""https://www.worldometers.info/coronavirus/""

# Start the Driver
driver = webdriver.Chrome(executable_path = r""C:\Webdriver\chromedriver.exe"")
# Hit the url and wait for 10 seconds.
driver.get(URL)
time.sleep(10)
#find class element
data= driver.find_elements_by_class_name(""odd"" and ""even"")
#for loop
for d in data:
    country=d.find_element_by_xpath("".//*[@id='main_table_countries_today']"").text
    print(country)

current output:
NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""xpath"",""selector"":"".//*[@id='main_table_countries_today']""}
  (Session info: chrome=96.0.4664.45)

",124,"
            1
        ","['\nTo scrape table within worldometers covid data you need to induce WebDriverWait for the visibility_of_element_located() and using DataFrame from Pandas you can use the following Locator Strategy:\nCode Block:\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pandas as pd\n\noptions = Options()\noptions.add_argument(""start-maximized"")\ns = Service(\'C:\\\\BrowserDrivers\\\\chromedriver.exe\')\ndriver = webdriver.Chrome(service=s, options=options)\ndriver.get(""https://www.worldometers.info/coronavirus/"")\ndata = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.CSS_SELECTOR, ""table#main_table_countries_today""))).get_attribute(""outerHTML"")\ndf  = pd.read_html(data)\nprint(df)\ndriver.quit()\n\nConsole Output:\n[         # Country,Other  TotalCases  NewCases  ...  Deaths/1M pop   TotalTests  Tests/ 1M pop    Population\n0      NaN         World   264359298  632349.0  ...          673.3          NaN            NaN           NaN\n1      1.0           USA    49662381   89259.0  ...         2415.0  756671013.0      2267182.0  3.337495e+08\n2      2.0         India    34609741    3200.0  ...          336.0  643510926.0       459914.0  1.399198e+09\n3      3.0        Brazil    22118782   12910.0  ...         2865.0   63776166.0       297051.0  2.146975e+08\n4      4.0            UK    10329074   53945.0  ...         2124.0  364875273.0      5335159.0  6.839070e+07\n..     ...           ...         ...       ...  ...            ...          ...            ...           ...\n221  221.0         Samoa           3       NaN  ...            NaN          NaN            NaN  2.002800e+05\n222  222.0  Saint Helena           2       NaN  ...            NaN          NaN            NaN  6.103000e+03\n223  223.0    Micronesia           1       NaN  ...            NaN          NaN            NaN  1.167290e+05\n224  224.0         Tonga           1       NaN  ...            NaN          NaN            NaN  1.073890e+05\n225    NaN        Total:   264359298  632349.0  ...          673.3          NaN            NaN           NaN\n\n[226 rows x 15 columns]]\n\n']",https://stackoverflow.com/questions/70206678/scraping-dynamic-data-selenium-unable-to-locate-element,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to find tag with particular text with Beautiful Soup?,"
How to find text I am looking for in the following HTML (line breaks marked with \n)?
...
<tr>
  <td class=""pos"">\n
      ""Some text:""\n
      <br>\n
      <strong>some value</strong>\n
  </td>
</tr>
<tr>
  <td class=""pos"">\n
      ""Fixed text:""\n
      <br>\n
      <strong>text I am looking for</strong>\n
  </td>
</tr>
<tr>
  <td class=""pos"">\n
      ""Some other text:""\n
      <br>\n
      <strong>some other value</strong>\n
  </td>
</tr>
...

The code below returns first found value, so I need to filter by ""Fixed text:"" somehow.
result = soup.find('td', {'class' :'pos'}).find('strong').text

UPDATE: If I use the following code:
title = soup.find('td', text = re.compile(ur'Fixed text:(.*)', re.DOTALL), attrs = {'class': 'pos'})
self.response.out.write(str(title.string).decode('utf8'))

then it returns just Fixed text:, not the <strong>-highlighted text in that same element.
",145k,"
            45
        ","[""\nYou can pass a regular expression to the text parameter of findAll, like so:\nimport BeautifulSoup\nimport re\n\ncolumns = soup.findAll('td', text = re.compile('your regex here'), attrs = {'class' : 'pos'})\n\n"", '\nThis post got me to my answer even though the answer is missing from this post. I felt I should give back.\nThe challenge here is in the inconsistent behavior of BeautifulSoup.find when searching with and without text.\nNote:\nIf you have BeautifulSoup, you can test this locally via:\ncurl https://gist.githubusercontent.com/RichardBronosky/4060082/raw/test.py | python\n\nCode: https://gist.github.com/4060082\n# Taken from https://gist.github.com/4060082\nfrom BeautifulSoup import BeautifulSoup\nfrom urllib2 import urlopen\nfrom pprint import pprint\nimport re\n\nsoup = BeautifulSoup(urlopen(\'https://gist.githubusercontent.com/RichardBronosky/4060082/raw/test.html\').read())\n# I\'m going to assume that Peter knew that re.compile is meant to cache a computation result for a performance benefit. However, I\'m going to do that explicitly here to be very clear.\npattern = re.compile(\'Fixed text\')\n\n# Peter\'s suggestion here returns a list of what appear to be strings\ncolumns = soup.findAll(\'td\', text=pattern, attrs={\'class\' : \'pos\'})\n# ...but it is actually a BeautifulSoup.NavigableString\nprint type(columns[0])\n#>> <class \'BeautifulSoup.NavigableString\'>\n\n# you can reach the tag using one of the convenience attributes seen here\npprint(columns[0].__dict__)\n#>> {\'next\': <br />,\n#>>  \'nextSibling\': <br />,\n#>>  \'parent\': <td class=""pos"">\\n\n#>>       ""Fixed text:""\\n\n#>>       <br />\\n\n#>>       <strong>text I am looking for</strong>\\n\n#>>   </td>,\n#>>  \'previous\': <td class=""pos"">\\n\n#>>       ""Fixed text:""\\n\n#>>       <br />\\n\n#>>       <strong>text I am looking for</strong>\\n\n#>>   </td>,\n#>>  \'previousSibling\': None}\n\n# I feel that \'parent\' is safer to use than \'previous\' based on http://www.crummy.com/software/BeautifulSoup/bs4/doc/#method-names\n# So, if you want to find the \'text\' in the \'strong\' element...\npprint([t.parent.find(\'strong\').text for t in soup.findAll(\'td\', text=pattern, attrs={\'class\' : \'pos\'})])\n#>> [u\'text I am looking for\']\n\n# Here is what we have learned:\nprint soup.find(\'strong\')\n#>> <strong>some value</strong>\nprint soup.find(\'strong\', text=\'some value\')\n#>> u\'some value\'\nprint soup.find(\'strong\', text=\'some value\').parent\n#>> <strong>some value</strong>\nprint soup.find(\'strong\', text=\'some value\') == soup.find(\'strong\')\n#>> False\nprint soup.find(\'strong\', text=\'some value\') == soup.find(\'strong\').text\n#>> True\nprint soup.find(\'strong\', text=\'some value\').parent == soup.find(\'strong\')\n#>> True\n\nThough it is most certainly too late to help the OP, I hope they will make this as the answer since it does satisfy all quandaries around finding by text.\n']",https://stackoverflow.com/questions/9007653/how-to-find-tag-with-particular-text-with-beautiful-soup,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I handle Javascript in a Perl web crawler?,"
I would like to crawl a website, the problem is, that its full of JavaScript things, such as buttons and such that when they are pressed, they do not change the URL, but the data on the page is changed. 
Usually I use LWP / Mechanize etc to crawl sites, but neither support JavaScript.
any idea?
",5k,"
            19
        ","['\nAnother option might be Selenium with WWW::Selenium module\n', ""\nThe WWW::Scripter module has a JavaScript plugin that may be useful. Can't say I've used it myself, however.\n"", '\nWWW::Mechanize::Firefox might be of use.  that way you can have Firefox handle the complex JavaScript issues and then extract the resultant html.\n', '\nI would suggest HtmlUnit and Perl wrapper: WWW::HtmlUnit.\n']",https://stackoverflow.com/questions/3769015/how-can-i-handle-javascript-in-a-perl-web-crawler,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python regular expression for HTML parsing,"
I want to grab the value of a hidden input field in HTML.
<input type=""hidden"" name=""fooId"" value=""12-3456789-1111111111"" />

I want to write a regular expression in Python that will return the value of fooId, given that I know the line in the HTML follows the format
<input type=""hidden"" name=""fooId"" value=""**[id is here]**"" />

Can someone provide an example in Python to parse the HTML for the value?
",27k,"
            11
        ","[""\nFor this particular case, BeautifulSoup is harder to write than a regex, but it is much more robust... I'm just contributing with the BeautifulSoup example, given that you already know which regexp to use :-)\nfrom BeautifulSoup import BeautifulSoup\n\n#Or retrieve it from the web, etc. \nhtml_data = open('/yourwebsite/page.html','r').read()\n\n#Create the soup object from the HTML data\nsoup = BeautifulSoup(html_data)\nfooId = soup.find('input',name='fooId',type='hidden') #Find the proper tag\nvalue = fooId.attrs[2][1] #The value of the third attribute of the desired tag \n                          #or index it directly via fooId['value']\n\n"", ""\nI agree with Vinko BeautifulSoup is the way to go. However I suggest using fooId['value'] to get the attribute rather than relying on value being the third attribute.\nfrom BeautifulSoup import BeautifulSoup\n#Or retrieve it from the web, etc.\nhtml_data = open('/yourwebsite/page.html','r').read()\n#Create the soup object from the HTML data\nsoup = BeautifulSoup(html_data)\nfooId = soup.find('input',name='fooId',type='hidden') #Find the proper tag\nvalue = fooId['value'] #The value attribute\n\n"", '\nimport re\nreg = re.compile(\'<input type=""hidden"" name=""([^""]*)"" value=""<id>"" />\')\nvalue = reg.search(inputHTML).group(1)\nprint \'Value is\', value\n\n', ""\nParsing is one of those areas where you really don't want to roll your own if you can avoid it, as you'll be chasing down the edge-cases and bugs for years go come\nI'd recommend using BeautifulSoup. It has a very good reputation and looks from the docs like it's pretty easy to use.\n"", '\nPyparsing is a good interim step between BeautifulSoup and regex.  It is more robust than just regexes, since its HTML tag parsing comprehends variations in case, whitespace, attribute presence/absence/order, but simpler to do this kind of basic tag extraction than using BS.\nYour example is especially simple, since everything you are looking for is in the attributes of the opening ""input"" tag.  Here is a pyparsing example showing several variations on your input tag that would give regexes fits, and also shows how NOT to match a tag if it is within a comment:\nhtml = """"""<html><body>\n<input type=""hidden"" name=""fooId"" value=""**[id is here]**"" />\n<blah>\n<input name=""fooId"" type=""hidden"" value=""**[id is here too]**"" />\n<input NAME=""fooId"" type=""hidden"" value=""**[id is HERE too]**"" />\n<INPUT NAME=""fooId"" type=""hidden"" value=""**[and id is even here TOO]**"" />\n<!--\n<input type=""hidden"" name=""fooId"" value=""**[don\'t report this id]**"" />\n-->\n<foo>\n</body></html>""""""\n\nfrom pyparsing import makeHTMLTags, withAttribute, htmlComment\n\n# use makeHTMLTags to create tag expression - makeHTMLTags returns expressions for\n# opening and closing tags, we\'re only interested in the opening tag\ninputTag = makeHTMLTags(""input"")[0]\n\n# only want input tags with special attributes\ninputTag.setParseAction(withAttribute(type=""hidden"", name=""fooId""))\n\n# don\'t report tags that are commented out\ninputTag.ignore(htmlComment)\n\n# use searchString to skip through the input \nfoundTags = inputTag.searchString(html)\n\n# dump out first result to show all returned tags and attributes\nprint foundTags[0].dump()\nprint\n\n# print out the value attribute for all matched tags\nfor inpTag in foundTags:\n    print inpTag.value\n\nPrints:\n[\'input\', [\'type\', \'hidden\'], [\'name\', \'fooId\'], [\'value\', \'**[id is here]**\'], True]\n- empty: True\n- name: fooId\n- startInput: [\'input\', [\'type\', \'hidden\'], [\'name\', \'fooId\'], [\'value\', \'**[id is here]**\'], True]\n  - empty: True\n  - name: fooId\n  - type: hidden\n  - value: **[id is here]**\n- type: hidden\n- value: **[id is here]**\n\n**[id is here]**\n**[id is here too]**\n**[id is HERE too]**\n**[and id is even here TOO]**\n\nYou can see that not only does pyparsing match these unpredictable variations, it returns the data in an object that makes it easy to read out the individual tag attributes and their values.\n', '\n/<input type=""hidden"" name=""fooId"" value=""([\\d-]+)"" \\/>/\n\n', '\n/<input\\s+type=""hidden""\\s+name=""([A-Za-z0-9_]+)""\\s+value=""([A-Za-z0-9_\\-]*)""\\s*/>/\n\n>>> import re\n>>> s = \'<input type=""hidden"" name=""fooId"" value=""12-3456789-1111111111"" />\'\n>>> re.match(\'<input\\s+type=""hidden""\\s+name=""([A-Za-z0-9_]+)""\\s+value=""([A-Za-z0-9_\\-]*)""\\s*/>\', s).groups()\n(\'fooId\', \'12-3456789-1111111111\')\n\n']",https://stackoverflow.com/questions/55391/python-regular-expression-for-html-parsing,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failproof Wait for IE to load,"
Is there a foolproof way for the script to wait till the Internet explorer is completely loaded?
Both oIE.Busy and / or oIE.ReadyState are not working the way they should: 
Set oIE = CreateObject(""InternetExplorer.application"")

    oIE.Visible = True
    oIE.navigate (""http://technopedia.com"")

    Do While oIE.Busy Or oIE.ReadyState <> 4: WScript.Sleep 100: Loop  

    ' <<<<< OR >>>>>>

    Do While oIE.ReadyState <> 4: WScript.Sleep 100: Loop

Any other suggestions?
",60k,"
            9
        ","['\nTry this one, it helped me to solve similar problem with IE once:\nSet oIE = CreateObject(""InternetExplorer.application"")\noIE.Visible = True\noIE.navigate (""http://technopedia.com"")\nDo While oIE.ReadyState = 4: WScript.Sleep 100: Loop\nDo While oIE.ReadyState <> 4: WScript.Sleep 100: Loop\n\' example ref to DOM\nMsgBox oIE.Document.GetElementsByTagName(""div"").Length\n\nUPD: Drilling down IE events I found that IE_DocumentComplete is the last event before the page is actually ready. So there is one more method to detect when a web page is loaded (note that you have to specify the exact destination URL which may differ from the target URL eg in case of redirection):\noption explicit\ndim ie, targurl, desturl, completed\n\nset ie = wscript.createobject(""internetexplorer.application"", ""ie_"")\nie.visible = true\n\ntargurl = ""http://technopedia.com/""\ndesturl = ""http://technopedia.com/""\n\n\' targurl = ""http://tumblr.com/""\n\' desturl = ""https://www.tumblr.com/"" \' redirection if you are not login\n\' desturl = ""https://www.tumblr.com/dashboard"" \' redirection if you are login\n\ncompleted = false\nie.navigate targurl\ndo until completed\n    wscript.sleep 100\nloop\n\' your code here\nmsgbox ie.document.getelementsbytagname(""*"").length\nie.quit\n\nsub ie_documentcomplete(byval pdisp, byval url)\n    if url = desturl then completed = true\nend sub\n\n', '\nI have for a very long time been successfully using:\nWhile IE.readyState <> 4 Or IE.Busy: DoEvents: Wend\n\nIt has been working perfectly until today, when I changed my PC and switched to Windows 10 and Office 16. Then it started working on some cases, but there were times when the loop was not completed. Neither one of the conditions in the loop was reached, so the loop was ENDLESS.\nAfter a lot of Googling, I have tried many suggestions until I found the solution in this post: Excel VBA Controlling IE local intranet\nThe solution is to add the URL to the trusted sites list in Internet Explorer Security tab. Finally!\n', '\nA few years later, it also hit me. I looked at the proposed solutions and tested a lot. The following combination of commands has been developed, which I will now use in my application.\nSet oIE = CreateObject(""InternetExplorer.application"")\n\noIE.Visible = True\noIE.navigate (""http://technopedia.com"")\n\nwscript.sleep 100\nDo While oIE.Busy or oIE.ReadyState <> 4: WScript.Sleep 100: Loop  \n\nwscript.sleep 100\nDo While oIE.Busy or oIE.ReadyState <> 4: WScript.Sleep 100: Loop  \n\nmsgbox oIE.ReadyState & "" / "" & oIE.Busy , vbInformation +  vbMsgBoxForeground , ""Information""\n\noIE.Quit\n\nset oIE = Nothing\n\nThe second identical loop I did install after it turned out that oIE.Busy = True was sometimes after the first loop.\nRegards,\nScriptMan\n', '\ntry to put this script on the top, this may solve Your problem.\n{ \n    $myWindowsID = [System.Security.Principal.WindowsIdentity]::GetCurrent();\n    $myWindowsPrincipal = New-Object System.Security.Principal.WindowsPrincipal($myWindowsID);\n    $adminRole = [System.Security.Principal.WindowsBuiltInRole]::Administrator;\n\n    if ($myWindowsPrincipal.IsInRole($adminRole)) {\n        $Host.UI.RawUI.WindowTitle = $myInvocation.MyCommand.Definition + ""(Elevated)"";\n        Clear-Host;\n    }\n    else {\n        $newProcess = New-Object System.Diagnostics.ProcessStartInfo ""PowerShell"";\n        $newProcess.Arguments = ""& \'"" + $script:MyInvocation.MyCommand.Path + ""\'""\n        $newProcess.Verb = ""runas"";\n        [System.Diagnostics.Process]::Start($newProcess);\n        Exit;\n    }\n}\n\nExplanation:\nPowershell is not having some rights when you are running script from the normal mode \nso it is not reading IE status properly \nand that is why DOM is not being loaded\nso, script doesn\'t found any parameter  \n', '\nSo through looking up this answer, I still had trouble with IE waiting for the page to completely load. In the hopes that this solution will help others, here is what I did:\nDim i As Integer\nDim j As Integer\nDim tagnames As Integer\n\nWhile ie.Busy\n    DoEvents\nWend\nWhile ie.ReadyState <> 4\n    DoEvents\nWend\n\ni = 0\nj = 0\nWhile (i <> 5)\n    i = 0\n    tagnames = ie.document.getelementsbytagname(""*"").Length\n    For j = 1 To 5\n        Sleep (50)\n        If tagnames = ie.document.getelementsbytagname(""*"").Length Then\n            b = b + 1\n        End If\n    Next j\nWend\n\nBasically, what this does is wait for 5 50ms intervals for the number of tagnames loaded to stop increasing.\nOf course, this is adjustable depending on what you want, but that worked for my application.\n', '\nIf your working with IE on a form submission, it\'s better to place it in a Sub so you can reference the same Sub repeatedly. \nDim IE\nSet IE = WScript.CreateObject(""InternetExplorer.Application"")\n    IE.Visible = True\n    IE.Navigate ""http://www.google.com""\n    Wait IE, 500\n\nSub Wait(IE, SleepInterval)\n    Do\n        WScript.Sleep SleepInterval\n    Loop While IE.ReadyState < 4 Or IE.Busy\nEnd Sub\n\nThe difference being, that your referencing the IE object AFTER the wscript.sleep. If you check them first and foremost before the object is loaded. It could cause script failure. \n']",https://stackoverflow.com/questions/23299134/failproof-wait-for-ie-to-load,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I scrape website content in PHP from a website that requires a cookie login?,"
My problem is that it doesn't just require a basic cookie, but rather asks for a session cookie, and for randomly generated IDs. I think this means I need to use a web browser emulator with a cookie jar?
I have tried to use Snoopy, Goutte and a couple of other web browser emulators, but as of yet I have not been able to find tutorials on how to receive cookies. I am getting a little desperate!
Can anyone give me an example of how to accept cookies in Snoopy or Goutte?
Thanks in advance!
",16k,"
            6
        ","['\nYou can do that in cURL without needing external \'emulators\'.\nThe code below retrieves a page into a PHP variable to be parsed.\nScenario\nThere is a page (let\'s call it HOME) that opens the session. Server side, if it is in PHP, is the one (any one actually) calling session_start() for the first time. In other languages you need a specific page that will do all the session setup. From the client side it\'s the page supplying the session ID cookie. In PHP, all sessioned pages do; in other languages the landing page will do it, all the others will check if the cookie is there, and if there isn\'t, instead of creating the session, will drop you to HOME.\nThere is a page (LOGIN) that generates the login form and adds a critical information to the session - ""This user is logged in"". In the code below, this is the page asking for the session ID.\nAnd finally there are N pages where the goodies to be scrapes reside.\nSo we want to hit HOME, then LOGIN, then GOODIES one after another. In PHP (and other languages actually), again, HOME and LOGIN might well be the same page. Or all pages might share the same address, for example in Single Page Applications.\nThe Code\n    $url            = ""the url generating the session ID"";\n    $next_url       = ""the url asking for session"";\n\n    $ch             = curl_init();\n    curl_setopt($ch, CURLOPT_URL,    $url);\n    // We do not authenticate, only access page to get a session going.\n    // Change to False if it is not enough (you\'ll see that cookiefile\n    // remains empty).\n    curl_setopt($ch, CURLOPT_NOBODY, True);\n\n    // You may want to change User-Agent here, too\n    curl_setopt($ch, CURLOPT_COOKIEFILE, ""cookiefile"");\n    curl_setopt($ch, CURLOPT_COOKIEJAR,  ""cookiefile"");\n\n    // Just in case\n    curl_setopt($ch, CURLOPT_FOLLOWLOCATION, true);\n\n    $ret    = curl_exec($ch);\n\n    // This page we retrieve, and scrape, with GET method\n    foreach(array(\n            CURLOPT_POST            => False,       // We GET...\n            CURLOPT_NOBODY          => False,       // ...the body...\n            CURLOPT_URL             => $next_url,   // ...of $next_url...\n            CURLOPT_BINARYTRANSFER  => True,        // ...as binary...\n            CURLOPT_RETURNTRANSFER  => True,        // ...into $ret...\n            CURLOPT_FOLLOWLOCATION  => True,        // ...following redirections...\n            CURLOPT_MAXREDIRS       => 5,           // ...reasonably...\n            CURLOPT_REFERER         => $url,        // ...as if we came from $url...\n            //CURLOPT_COOKIEFILE      => \'cookiefile\', // Save these cookies\n            //CURLOPT_COOKIEJAR       => \'cookiefile\', // (already set above)\n            CURLOPT_CONNECTTIMEOUT  => 30,          // Seconds\n            CURLOPT_TIMEOUT         => 300,         // Seconds\n            CURLOPT_LOW_SPEED_LIMIT => 16384,       // 16 Kb/s\n            CURLOPT_LOW_SPEED_TIME  => 15,          // \n            ) as $option => $value)\n            if (!curl_setopt($ch, $option, $value))\n                    die(""could not set $option to "" . serialize($value));\n\n    $ret = curl_exec($ch);\n    // Done; cleanup.\n    curl_close($ch);\n\nImplementation\nFirst of all we have to get the login page.\nWe use a special User-Agent to introduce ourselves, in order both to be recognizable (we don\'t want to antagonize the webmaster) but also to fool the server into sending us a specific version of the site that is browser tailored. Ideally, we use the same User-Agent as any browser we\'re going to use to debug the page, plus a suffix to make it clear to whoever checks that it is an automated tool they\'re looking at (see comment by Halfer).\n    $ua = \'Mozilla/5.0 (Windows NT 5.1; rv:16.0) Gecko/20100101 Firefox/16.0 (ROBOT)\';\n    $cookiefile = ""cookiefile"";\n    $url1 = ""the login url generating the session ID"";\n\n    $ch             = curl_init();\n\n    curl_setopt($ch, CURLOPT_URL,            $url1);\n    curl_setopt($ch, CURLOPT_USERAGENT,      $ua);\n    curl_setopt($ch, CURLOPT_COOKIEFILE,     $cookiefile);\n    curl_setopt($ch, CURLOPT_COOKIEJAR,      $cookiefile);\n    curl_setopt($ch, CURLOPT_FOLLOWLOCATION, True);\n    curl_setopt($ch, CURLOPT_NOBODY,         False);\n    curl_setopt($ch, CURLOPT_RETURNTRANSFER, True);\n    curl_setopt($ch, CURLOPT_BINARYTRANSFER, True);\n    $ret    = curl_exec($ch);\n\nThis will retrieve the page asking for user/password. By inspecting the page, we find the needed fields (including hidden ones) and can populate them. The FORM tag tells us whether we need to go on with POST or GET.\nWe might want to inspect the form code to adjust the following operations, so we ask cURL to return the page content as-is into $ret, and to do return the page body. Sometimes, CURLOPT_NOBODY set to True is still enough to trigger session creation and cookie submission, and if so, it\'s faster. But CURLOPT_NOBODY (""no body"") works by issuing a HEAD request, instead of a GET; and sometimes the HEAD request doesn\'t work because the server will only react to a full GET.\nInstead of retrieving the body this way, it is also possible to login using a real Firefox and sniff the form content being posted with Firebug (or Chrome with Chrome Tools); some sites will try and populate/modify hidden fields with Javascript, so that the form being submitted will not be the one you see in the HTML code.\nA webmaster who wanted his site not scraped might send a hidden field with the timestamp. A human being (not aided by a too-clever browser - there are ways to tell browsers not to be clever; at worst, every time you change the name of user and pass fields) takes at least three seconds to fill a form. A cURL script takes zero. Of course, a delay can be simulated. It\'s all shadowboxing...\nWe may also want to be on the lookout for form appearance. A webmaster could for example build a form asking name, email, and password; and then, through use of CSS, move the ""email"" field where you would expect to find the name, and vice versa. So the real form being submitted will have a ""@"" in a field called username, none in the field called email. The server, that expects this, merely inverts again the two fields. A ""scraper"" built by hand (or a spambot) would do what seems natural, and send an email in the email field. And by so doing, it betrays itself. By working through the form once with a real CSS and JS aware browser, sending meaningful data, and sniffing what actually gets sent, we might be able to overcome this particular obstacle. Might, because there are ways of making life difficult. As I said, shadowboxing.\nBack to the case at hand, in this case the form contains three fields and has no Javascript overlay. We have cPASS, cUSR, and checkLOGIN with a value of \'Check login\'.\nSo we prepare the form with the proper fields. Note that the form is to be sent as application/x-www-form-urlencoded, which in PHP cURL means two things:\n\nwe are to use CURLOPT_POST\nthe option CURLOPT_POSTFIELDS must be a string (an array would signal cURL to submit as multipart/form-data, which might work... or might not).\n\nThe form fields are, as it says, urlencoded; there\'s a function for that.\nWe read the action field of the form; that\'s the URL we are to use to submit our authentication (which we must have).\nSo everything being ready...\n    $fields = array(\n        \'checkLOGIN\' => \'Check Login\',\n        \'cUSR\'       => \'jb007\',\n        \'cPASS\'      => \'astonmartin\',\n    );\n    $coded = array();\n    foreach($fields as $field => $value)\n        $coded[] = $field . \'=\' . urlencode($value);\n    $string = implode(\'&\', $coded);\n\n    curl_setopt($ch, CURLOPT_URL,         $url1); //same URL as before, the login url generating the session ID\n    curl_setopt($ch, CURLOPT_POST,        True);\n    curl_setopt($ch, CURLOPT_POSTFIELDS,  $string);\n    $ret    = curl_exec($ch);\n\nWe expect now a ""Hello, James - how about a nice game of chess?"" page. But more than that, we expect that the session associated with the cookie saved in the $cookiefile has been supplied with the critical information -- ""user is authenticated"".\nSo all following page requests made using $ch and the same cookie jar will be granted access, allowing us to \'scrape\' pages quite easily - just remember to set request mode back to GET:\n    curl_setopt($ch, CURLOPT_POST,        False);\n\n    // Start spidering\n    foreach($urls as $url)\n    {\n        curl_setopt($ch, CURLOPT_URL, $url);\n        $HTML = curl_exec($ch);\n        if (False === $HTML)\n        {\n            // Something went wrong, check curl_error() and curl_errno().\n        }\n    }\n    curl_close($ch);\n\nIn the loop, you have access to $HTML -- the HTML code of every single page.\nGreat the temptation of using regexps is. Resist it you must. To better cope with ever-changing HTML, as well as being sure not to turn up false positives or false negatives when the layout stays the same but the content changes (e.g. you discover that you have the weather forecasts of Nice, Tourrette-Levens, Castagniers, but never Aspr茅mont or Gatti猫res, and isn\'t that c眉rious?), the best option is to use DOM:\nGrabbing the href attribute of an A element\n', '\nObject-Oriented answer\nWe implement as much as possible of the previous answer in one class called Browser that should supply the normal navigation features.\nThen we should be able to put the site-specific code, in very simple form, in a new derived class that we call, say, FooBrowser, that performs scraping of the site Foo.\nThe class deriving Browser must supply some site-specific function such as a path() function allowing to store site-specific information, for example\nfunction path($basename) {\n    return \'/var/tmp/www.foo.bar/\' . $basename;\n}\n\nabstract class Browser\n{\n    private $options = [];\n    private $state   = [];\n    protected $cookies;\n\n    abstract protected function path($basename);\n\n    public function __construct($site, $options = []) {\n        $this->cookies   = $this->path(\'cookies\');\n        $this->options  = array_merge(\n            [\n                \'site\'      => $site,\n                \'userAgent\' => \'Mozilla/5.0 (Windows NT 5.1; rv:16.0) Gecko/20100101 Firefox/16.0 - LeoScraper\',\n                \'waitTime\'  => 250000,\n            ],\n            $options\n        );\n        $this->state = [\n            \'referer\' => \'/\',\n            \'url\'     => \'\',\n            \'curl\'    => \'\',\n        ];\n        $this->__wakeup();\n    }\n\n    /**\n     * Reactivates after sleep (e.g. in session) or creation\n     */\n    public function __wakeup() {\n        $this->state[\'curl\'] = curl_init();\n        $this->config([\n            CURLOPT_USERAGENT       => $this->options[\'userAgent\'],\n            CURLOPT_ENCODING        => \'\',\n            CURLOPT_NOBODY          => false,\n            // ...retrieving the body...\n            CURLOPT_BINARYTRANSFER  => true,\n            // ...as binary...\n            CURLOPT_RETURNTRANSFER  => true,\n            // ...into $ret...\n            CURLOPT_FOLLOWLOCATION  => true,\n            // ...following redirections...\n            CURLOPT_MAXREDIRS       => 5,\n            // ...reasonably...\n            CURLOPT_COOKIEFILE      => $this->cookies,\n            // Save these cookies\n            CURLOPT_COOKIEJAR       => $this->cookies,\n            // (already set above)\n            CURLOPT_CONNECTTIMEOUT  => 30,\n            // Seconds\n            CURLOPT_TIMEOUT         => 300,\n            // Seconds\n            CURLOPT_LOW_SPEED_LIMIT => 16384,\n            // 16 Kb/s\n            CURLOPT_LOW_SPEED_TIME  => 15,\n        ]);\n    }\n\n    /**\n     * Imports an options array.\n     *\n     * @param array $opts\n     * @throws DetailedError\n     */\n    private function config(array $opts = []) {\n        foreach ($opts as $key => $value) {\n            if (true !== curl_setopt($this->state[\'curl\'], $key, $value)) {\n                throw new \\Exception(\'Could not set cURL option\');\n            }\n        }\n    }\n\n    private function perform($url) {\n        $this->state[\'referer\'] = $this->state[\'url\'];\n        $this->state[\'url\'] = $url;\n        $this->config([\n            CURLOPT_URL     => $this->options[\'site\'] . $this->state[\'url\'],\n            CURLOPT_REFERER => $this->options[\'site\'] . $this->state[\'referer\'],\n        ]);\n        $response = curl_exec($this->state[\'curl\']);\n        // Should we ever want to randomize waitTime, do so here.\n        usleep($this->options[\'waitTime\']);\n\n        return $response;\n    }\n\n    /**\n     * Returns a configuration option.\n     * @param string $key       configuration key name\n     * @param string $value     value to set\n     * @return mixed\n     */\n    protected function option($key, $value = \'__DEFAULT__\') {\n        $curr   = $this->options[$key];\n        if (\'__DEFAULT__\' !== $value) {\n            $this->options[$key]    = $value;\n        }\n        return $curr;\n    }\n\n    /**\n     * Performs a POST.\n     *\n     * @param $url\n     * @param $fields\n     * @return mixed\n     */\n    public function post($url, array $fields) {\n        $this->config([\n            CURLOPT_POST       => true,\n            CURLOPT_POSTFIELDS => http_build_query($fields),\n        ]);\n        return $this->perform($url);\n    }\n\n    /**\n     * Performs a GET.\n     *\n     * @param       $url\n     * @param array $fields\n     * @return mixed\n     */\n    public function get($url, array $fields = []) {\n        $this->config([ CURLOPT_POST => false ]);\n        if (empty($fields)) {\n            $query = \'\';\n        } else {\n            $query = \'?\' . http_build_query($fields);\n        }\n        return $this->perform($url . $query);\n    }\n}\n\nNow to scrape FooSite:\n/* WWW_FOO_COM requires username and password to construct */\n\nclass WWW_FOO_COM_Browser extends Browser\n{\n    private $loggedIn   = false;\n\n    public function __construct($username, $password) {\n        parent::__construct(\'http://www.foo.bar.baz\', [\n            \'username\'  => $username,\n            \'password\'  => $password,\n            \'waitTime\'  => 250000,\n            \'userAgent\' => \'FooScraper\',\n            \'cache\'     => true\n        ]);\n        // Open the session\n        $this->get(\'/\');\n        // Navigate to the login page\n        $this->get(\'/login.do\');\n    }\n\n    /**\n     * Perform login.\n     */\n    public function login() {\n        $response = $this->post(\n            \'/ajax/loginPerform\',\n            [\n                \'j_un\'    => $this->option(\'username\'),\n                \'j_pw\'    => $this->option(\'password\'),\n            ]\n        );\n        // TODO: verify that response is OK.\n        // if (!strstr($response, ""Welcome "" . $this->option(\'username\'))\n        //     throw new \\Exception(""Bad username or password"")\n        $this->loggedIn = true;\n        return true;\n    }\n\n    public function scrape($entry) {\n        // We could implement caching to avoid scraping the same entry\n        // too often. Save $data into path(""entry-"" . md5($entry))\n        // and verify the filemtime of said file, is it newer than time()\n        // minus, say, 86400 seconds? If yes, return file_get_content and\n        // leave remote site alone.\n        $data = $this->get(\n            \'/foobars/baz.do\',\n            [\n                \'ticker\' => $entry\n            ]\n        );\n        return $data;\n    }\n\nNow the actual scraping code would be:\n    $scraper = new WWW_FOO_COM_Browser(\'lserni\', \'mypassword\');\n    if (!$scraper->login()) {\n        throw new \\Exception(""bad user or pass"");\n    }\n    // www.foo.com is a ticker site, we need little info for each\n    // Other examples might be much more complex.\n    $entries = [\n        \'APPL\', \'MSFT\', \'XKCD\'\n    ];\n    foreach ($entries as $entry) {\n        $html = $scraper->scrape($entry);\n        // Parse HTML\n    }\n\nMandatory notice: use a suitable parser to get data from raw HTML.\n']",https://stackoverflow.com/questions/13210140/how-can-i-scrape-website-content-in-php-from-a-website-that-requires-a-cookie-lo,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to scrape a public tableau dashboard? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



Every day I need to downlaod the data available on a public Tableau dashboard. After defining the parameters of interest (time series frequency, time series interval, etc) the dashboard allows you to download the series. 
My life would be reasonably easier if I could automate the download of these series to a database using Python or R. I've already tried to analyze the requests made on the page but I couldn't get much further. Is there any way to automate this process?
The dashboard: https://tableau.ons.org.br/t/ONS_Publico/views/DemandaMxima/HistricoDemandaMxima?:embed=y&:showAppBanner=false&:showShareOptions=true&:display_count=no&:showVizHome=no
",7k,"
            4
        ","['\nEdit\nI\'ve made a tableau scraper library to extract the data from Tableau worksheets\nYou can get the data from worksheets in a pandas dataframe directly. Also, the parametered values are supported.\nThe following example get the data from worksheet Simples Demanda M谩xima Ano, then switch to daily mode, shows the worksheet Simples Demanda M谩xima Semana Dia data and then set start date to 01/01/2017 :\nfrom tableauscraper import TableauScraper as TS\n\nurl = ""https://tableau.ons.org.br/t/ONS_Publico/views/DemandaMxima/HistricoDemandaMxima""\n\nts = TS()\nts.loads(url)\nwb = ts.getWorkbook()\n\n# show dataframe with yearly data\nws = wb.getWorksheet(""Simples Demanda M谩xima Ano"")\nprint(ws.data)\n\n# switch to daily\nwb = wb.setParameter(""Escala de Tempo DM Simp 4"", ""Dia"")\n\n# show dataframe with daily data\nws = wb.getWorksheet(""Simples Demanda M谩xima Semana Dia"")\nprint(ws.data)\n\n# switch to daily\nwb = wb.setParameter(\n    ""In铆cio Primeiro Per铆odo DM Simp 4"", ""01/01/2017"")\n\n# show dataframe with daily data from 01/01/2017\nws = wb.getWorksheet(""Simples Demanda M谩xima Semana Dia"")\nprint(ws.data)\n\n\nTry this on repl.it\n\nOriginal post\nThis answer is similar to this one but the initial URL page and tableau base URL differ. The process/algo remains the same essentially but I will details the steps :\nThe graphic is generated in JS from the result of an API :\nPOST https://tableau.ons.org.br/ROOT_PATH/bootstrapSession/sessions/SESSION_ID\n\nThe SESSION_ID parameter is located (among other things) in tsConfigContainer textarea in the URL used to build the iframe.\nStarting from https://tableau.ons.org.br/t/ONS_Publico/views/DemandaMxima/HistricoDemandaMxima?:embed=y&:showAppBanner=false&:showShareOptions=true&:display_count=no&:showVizHome=no :\n\nthere is a textarea with id tsConfigContainer with a bunch of json values\nextract the session_id and root path (vizql_root)\nmake a POST on https://tableau.ons.org.br/ROOT_PATH/bootstrapSession/sessions/SESSION_ID with the sheetId as form data\nextract the json from the result (result is not json)\n\nCode :\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport re\n\nurl = ""https://tableau.ons.org.br/t/ONS_Publico/views/DemandaMxima/HistricoDemandaMxima""\n\nr = requests.get(\n    url,\n    params= {\n        "":embed"":""y"",\n        "":showAppBanner"":""false"",\n        "":showShareOptions"":""true"",\n        "":display_count"":""no"",\n        ""showVizHome"": ""no""\n    }\n)\nsoup = BeautifulSoup(r.text, ""html.parser"")\n\ntableauData = json.loads(soup.find(""textarea"",{""id"": ""tsConfigContainer""}).text)\n\ndataUrl = f\'https://tableau.ons.org.br{tableauData[""vizql_root""]}/bootstrapSession/sessions/{tableauData[""sessionid""]}\'\n\nr = requests.post(dataUrl, data= {\n    ""sheet_id"": tableauData[""sheetId""],\n})\n\ndataReg = re.search(\'\\d+;({.*})\\d+;({.*})\', r.text, re.MULTILINE)\ninfo = json.loads(dataReg.group(1))\ndata = json.loads(dataReg.group(2))\n\nprint(data[""secondaryInfo""][""presModelMap""][""dataDictionary""][""presModelHolder""][""genDataDictionaryPresModel""][""dataSegments""][""0""][""dataColumns""])\n\n']",https://stackoverflow.com/questions/62095206/how-to-scrape-a-public-tableau-dashboard,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selenium Python: How to web scrape the element text,"
I am trying to webscrap data from roulette game.
While trying to
find element by class name (roulette_round_result-position__text)

I am getting this output:
<selenium.webdriver.remote.webelement.WebElement (session=""d4f20fd17bf4037ed8cf50b00e844a7f"", element=""f12cf837-6c77-4c90-9da2-7b5fb9da9e5d"")>

Any idea how to scrap this value? (In this case number 2)
My code:
number_1=0
    while number_1 == 0:
        try:
            number_1 = self.driver.find_element_by_class_name('roulette-round-result-position__text')
        except:
            pass

Screen shot from DevTools:

",581,"
            -1
        ","['\nYou are printing the WebElement. Hence you see the output as:\n<selenium.webdriver.remote.webelement.WebElement (session=""d4f20fd17bf4037ed8cf50b00e844a7f"", element=""f12cf837-6c77-4c90-9da2-7b5fb9da9e5d"")>\n\nInstead you may like to print the text within the element as:\nnumber_1 = self.driver.find_element_by_class_name(\'roulette-round-result-position__text\')\nprint(number_1.text)\n\nor\nprint(self.driver.find_element_by_class_name(\'roulette-round-result-position__text\').text)\n\n']",https://stackoverflow.com/questions/70385960/selenium-python-how-to-web-scrape-the-element-text,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Selenium-Debugging: Element is not clickable at point (X,Y)","
I try to scrape this site by Selenium.
I want to click in ""Next Page"" buttom, for this I do:
 driver.find_element_by_class_name('pagination-r').click()

it works for many pages but not for all, I got this error
WebDriverException: Message: Element is not clickable at point (918, 13). Other element would receive the click: <div class=""linkAuchan""></div>

always for this page 
I read this question 
and I tried this 
driver.implicitly_wait(10)
el = driver.find_element_by_class_name('pagination-r')
action = webdriver.common.action_chains.ActionChains(driver)
action.move_to_element_with_offset(el, 918, 13)
action.click()
action.perform()

but I got the same error
",80k,"
            70
        ","['\nAnother element is covering the element you are trying to click. You could use execute_script() to click on this.\nelement = driver.find_element_by_class_name(\'pagination-r\')\ndriver.execute_script(""arguments[0].click();"", element)\n\n', '\nI had a similar issue where using ActionChains was not solving my error:\nWebDriverException: Message: unknown error: Element is not clickable at point (5\n74, 892)\nI found a nice solution if you dont want to use execute_script:\n    from selenium.webdriver.common.keys import Keys #need to send keystrokes\n\n    inputElement = self.driver.find_element_by_name(\'checkout\')\n\n    inputElement.send_keys(""\\n"") #send enter for links, buttons\n\nor\n    inputElement.send_keys(Keys.SPACE) #for checkbox etc\n\n', '\nBecause element is not visible on the browser, first you need to scroll down to the element\nthis can be performed by executing javascript.\nelement = driver.find_element_by_class_name(\'pagination-r\')\ndriver.execute_script(""arguments[0].scrollIntoView();"", element)\ndriver.execute_script(""arguments[0].click();"", element)\n\n', '\nI have written logic to handle these type of exception . \n   def find_element_click(self, by, expression, search_window=None, timeout=32, ignore_exception=None,\n                       poll_frequency=4):\n    """"""It find the element and click then  handle all type of exception during click\n\n    :param poll_frequency:\n    :param by:\n    :param expression:\n    :param timeout:\n    :param ignore_exception:list It is a list of exception which is need to ignore.\n    :return:\n    """"""\n    if ignore_exception is None:\n        ignore_exception = []\n\n    ignore_exception.append(NoSuchElementException)\n    if search_window is None:\n        search_window = self.driver\n\n    end_time = time.time() + timeout\n    while True:\n        try:\n            web_element = search_window.find_element(by=by, value=expression)\n            web_element.click()\n            return True\n        except tuple(ignore_exception) as e:\n            self.logger.debug(str(e))\n            if time.time() > end_time:\n                self.logger.exception(e)\n                time.sleep(poll_frequency)\n                break\n        except Exception as e:\n            raise\n    return False\n\n', '\nIf you are receiving an element not clickable error, even after using wait on the element, try one of these workarounds:\n\nUse Action to move to the location of element and then run perform on action\n\nWebElement element = driver.findElement(By(""element_path""));\nActions actions = new Actions(driver);\nactions.moveToElement(element).click().perform();`\n\n\nCheck for an overlay or spinner on the element and wait for its invisibility\n\nBy spinnerimg = By.id(""spinner ID"");\nWebDriverWait wait = new WebDriverWait(driver, timeOutInSeconds);\nwait.until(ExpectedConditions.invisibilityOfElementLocated(spinnerimg ));\n\nHope this helps\n', '\nUse explicit wait instead of implicit.\n new WebDriverWait(TestingSession.Browser.WebDriver, TimeSpan.FromSeconds(10)).Until(ExpectedConditions.ElementExists((By.ClassName(""pagination-r\'"")))); \n\n', ""\nI had the similar issue with Chrome driver, changing the PageLoadStrategy of chromeOptions from 'Eager' to Normal fixed my problem.\nchromeOptions.PageLoadStrategy = PageLoadStrategy.Normal;\n\n""]",https://stackoverflow.com/questions/37879010/selenium-debugging-element-is-not-clickable-at-point-x-y,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Click a Button in Scrapy,"
I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking).
I found out that Scrapy can handle forms (like logins) as shown here. But the problem is that there is no form to fill out, so it's not exactly what I need.
How can I simply click a button, which then shows the information I need?
Do I have to use an external library like mechanize or lxml?
",76k,"
            67
        ","[""\nScrapy cannot interpret javascript.\nIf you absolutely must interact with the javascript on the page, you want to be using Selenium.\nIf using Scrapy, the solution to the problem depends on what the button is doing.\nIf it's just showing content that was previously hidden, you can scrape the data without a problem, it doesn't matter that it wouldn't appear in the browser, the HTML is still there.\nIf it's fetching the content dynamically via AJAX when the button is pressed, the best thing to do is to view the HTTP request that goes out when you press the button using a tool like Firebug. You can then just request the data directly from that URL.\n\nDo I have to use an external library like mechanize or lxml?\n\nIf you want to interpret javascript, yes you need to use a different library, although neither of those two fit the bill. Neither of them know anything about javascript. Selenium is the way to go.\nIf you can give the URL of the page you're working on scraping I can take a look.\n"", '\nSelenium browser provide very nice solution. Here is an example (pip install -U selenium):\nfrom selenium import webdriver\n\nclass northshoreSpider(Spider):\n    name = \'xxx\'\n    allowed_domains = [\'www.example.org\']\n    start_urls = [\'https://www.example.org\']\n\n    def __init__(self):\n        self.driver = webdriver.Firefox()\n\n    def parse(self,response):\n            self.driver.get(\'https://www.example.org/abc\')\n\n            while True:\n                try:\n                    next = self.driver.find_element_by_xpath(\'//*[@id=""BTN_NEXT""]\')\n                    url = \'http://www.example.org/abcd\'\n                    yield Request(url,callback=self.parse2)\n                    next.click()\n                except:\n                    break\n\n            self.driver.close()\n\n    def parse2(self,response):\n        print \'you are here!\'\n\n', '\nTo properly and fully use JavaScript you need a full browser engine and this is possible only with Watir/WatiN/Selenium etc.\n', ""\nAlthough it's an old thread I've found quite useful to use Helium (built on top of Selenium) for this purpose and far more easier/simpler than using Selenium. It will be something like the following:\nfrom helium import *\n\nstart_firefox('your_url')\ns = S('path_to_your_button')\nclick(s)\n...\n\n\n""]",https://stackoverflow.com/questions/6682503/click-a-button-in-scrapy,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Crawling multiple URLs in a loop using Puppeteer,"
I have an array of URLs to scrape data from:
urls = ['url','url','url'...]

This is what I'm doing:
urls.map(async (url)=>{
  await page.goto(url);
  await page.waitForNavigation({ waitUntil: 'networkidle' });
})

This seems to not wait for page load and visits all the URLs quite rapidly (I even tried using page.waitFor).
I wanted to know if am I doing something fundamentally wrong or this type of functionality is not advised/supported.
",27k,"
            27
        ","[""\nmap, forEach, reduce, etc, does not wait for the asynchronous operation within them, before they proceed to the next element of the iterator they are iterating over.\nThere are multiple ways of going through each item of an iterator synchronously while performing an asynchronous operation, but the easiest in this case I think would be to simply use a normal for operator, which does wait for the operation to finish.\nconst urls = [...]\n\nfor (let i = 0; i < urls.length; i++) {\n    const url = urls[i];\n    await page.goto(`${url}`);\n    await page.waitForNavigation({ waitUntil: 'networkidle2' });\n}\n\nThis would visit one url after another, as you are expecting. If you are curious about iterating serially using await/async, you can have a peek at this answer: https://stackoverflow.com/a/24586168/791691\n"", '\nThe accepted answer shows how to serially visit each page one at a time. However, you may want to visit multiple pages simultaneously when the task is embarrassingly parallel, that is, scraping a particular page isn\'t dependent on data extracted from other pages.\nA tool that can help achieve this is Promise.allSettled which lets us fire off a bunch of promises at once, determine which were successful and harvest results.\nFor a basic example, let\'s say we want to scrape usernames for Stack Overflow users given a series of ids.\nSerial code:\nconst puppeteer = require(""puppeteer""); // ^14.3.0\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch({dumpio: false});\n  const [page] = await browser.pages();\n  const baseURL = ""https://stackoverflow.com/users"";\n  const startId = 6243352;\n  const qty = 5;\n  const usernames = [];\n\n  for (let i = startId; i < startId + qty; i++) {\n    await page.goto(`${baseURL}/${i}`, {\n      waitUntil: ""domcontentloaded""\n    });\n    const sel = "".flex--item.mb12.fs-headline2.lh-xs"";\n    const el = await page.waitForSelector(sel);\n    usernames.push(await el.evaluate(el => el.textContent.trim()));\n  }\n\n  console.log(usernames);\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser.close())\n;\n\nParallel code:\nconst puppeteer = require(""puppeteer"");\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch();\n  const [page] = await browser.pages();\n  const baseURL = ""https://stackoverflow.com/users"";\n  const startId = 6243352;\n  const qty = 5;\n\n  const usernames = (await Promise.allSettled(\n    [...Array(qty)].map(async (_, i) => {\n      const page = await browser.newPage();\n      await page.goto(`${baseURL}/${i + startId}`, {\n        waitUntil: ""domcontentloaded""\n      });\n      const sel = "".flex--item.mb12.fs-headline2.lh-xs"";\n      const el = await page.waitForSelector(sel);\n      const text = await el.evaluate(el => el.textContent.trim());\n      await page.close();\n      return text;\n    })))\n    .filter(e => e.status === ""fulfilled"")\n    .map(e => e.value)\n  ;\n  console.log(usernames);\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser.close())\n;\n\nRemember that this is a technique, not a silver bullet that guarantees a speed increase on all workloads. It will take some experimentation to find the optimal balance between the cost of creating more pages versus the parallelization of network requests on a given particular task and system.\nThe example here is contrived since it\'s not interacting with the page dynamically, so there\'s not as much room for gain as in a typical Puppeteer use case that involves network requests and blocking waits per page.\nOf course, beware of rate limiting and any other restrictions imposed by sites (running the code above may anger Stack Overflow\'s rate limiter).\nFor tasks where creating a page per task is prohibitively expensive or you\'d like to set a cap on parallel request dispatches, consider using a task queue or combining serial and parallel code shown above to send requests in chunks. This answer shows a generic pattern for this agnostic of Puppeteer.\nThese patterns can be extended to handle the case when certain pages depend on data from other pages, forming a dependency graph.\nSee also Using async/await with a forEach loop which explains why the original attempt in this thread using map fails to wait for each promise.\n', ""\nIf you find that you are waiting on your promise indefinitely, the proposed solution is to use the following:\nconst urls = [...]\n\nfor (let i = 0; i < urls.length; i++) {\n    const url = urls[i];\n    const promise = page.waitForNavigation({ waitUntil: 'networkidle' });\n    await page.goto(`${url}`);\n    await promise;\n}\n\nAs referenced from this github issue\n"", ""\nBest way I found to achieve this. \n const puppeteer = require('puppeteer');\n(async () => {\n    const urls = ['https://www.google.com/', 'https://www.google.com/']\n    for (let i = 0; i < urls.length; i++) {\n\n        const url = urls[i];\n        const browser = await puppeteer.launch({ headless: false });\n        const page = await browser.newPage();\n        await page.goto(`${url}`, { waitUntil: 'networkidle2' });\n        await browser.close();\n\n    }\n})();\n\n"", '\nSomething no one else mentions is that if you are fetching multiple pages using the same page object it is crucial that you set its timeout to 0. Otherwise, once it has fetched the default 30 seconds worth of pages, it will timeout.\n  const browser = await puppeteer.launch();\n  const page = await browser.newPage();\n  page.setDefaultNavigationTimeout(0);\n\n']",https://stackoverflow.com/questions/46293216/crawling-multiple-urls-in-a-loop-using-puppeteer,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get text from span tag in BeautifulSoup,"
I have links looks like this
<div class=""systemRequirementsMainBox"">
<div class=""systemRequirementsRamContent"">
<span title=""000 Plus Minimum RAM Requirement"">1 GB</span> </div>

I'm trying to get 1 GB from there. I tried
tt  = [a['title'] for a in soup.select("".systemRequirementsRamContent span"")]
for ram in tt:
    if ""RAM"" in ram.split():
        print (soup.string)

It outputs None.
I tried a['text'] but it gives me KeyError. How can I fix this and what is my mistake?
",91k,"
            25
        ","['\nYou can use a css selector, pulling the span you want using the title text :\nsoup = BeautifulSoup(""""""<div class=""systemRequirementsMainBox"">\n<div class=""systemRequirementsRamContent"">\n<span title=""000 Plus Minimum RAM Requirement"">1 GB</span> </div>"""""", ""xml"")\n\nprint(soup.select_one(""span[title*=RAM]"").text)\n\nThat finds the span with a title attribute that contains RAM, it is equivalent to saying in python, if ""RAM"" in span[""title""].\nOr using find with re.compile\nimport re\nprint(soup.find(""span"", title=re.compile(""RAM"")).text)\n\nTo get all the data:\nfrom bs4 import BeautifulSoup \nr  = requests.get(""http://www.game-debate.com/games/index.php?g_id=21580&game=000%20Plus"").content\n\nsoup = BeautifulSoup(r,""lxml"")\ncont = soup.select_one(""div.systemRequirementsRamContent"")\nram = cont.select_one(""span"")\nprint(ram[""title""], ram.text)\nfor span in soup.select(""div.systemRequirementsSmallerBox.sysReqGameSmallBox span""):\n        print(span[""title""],span.text)\n\nWhich will give you:\n000 Plus Minimum RAM Requirement 1 GB\n000 Plus Minimum Operating System Requirement Win Xp 32\n000 Plus Minimum Direct X Requirement DX 9\n000 Plus Minimum Hard Disk Drive Space Requirement 500 MB\n000 Plus GD Adjusted Operating System Requirement Win Xp 32\n000 Plus GD Adjusted Direct X Requirement DX 9\n000 Plus GD Adjusted Hard Disk Drive Space Requirement 500 MB\n000 Plus Recommended Operating System Requirement Win Xp 32\n000 Plus Recommended Hard Disk Drive Space Requirement 500 MB\n\n', '\nI tried to extract the text inside all the span tags inside the HTML document using find_all() function from bs4 (BeautifulSoup):\nfrom bs4 import BeautifulSoup\nimport requests\nurl=""YOUR_URL_HERE""\nresponse=requests.get(url)\nsoup=BeautifulSoup(response.content,html5lib)\nspans=soup.find_all(\'span\',""ENTER_Css_CLASS_HERE"")\nfor span in spans:\n  print(span.text)\n\n', '\nYou can simply use span tag in BeautifulSoup or you can include other attributes like class, title along with the span tag.\nfrom BeautifulSoup import BeautifulSoup as BSHTML\n\nhtmlText = """"""<div class=""systemRequirementsMainBox"">\n<div class=""systemRequirementsRamContent"">\n<span title=""000 Plus Minimum RAM Requirement"">1 GB</span> </div>""""""\n\nsoup = BSHTML(htmlText)\nspans = soup.findAll(\'span\')\n# spans = soup.findAll(\'span\', attrs = {\'class\' : \'your-class-name\'}) # or span by class name\n# spans = soup.findAll(\'span\', attrs = {\'title\' : \'000 Plus Minimum RAM Requirement\'}) # or span with a title\nfor span in spans:\n    print span.text\n\n', '\nYou could solve this with just a couple lines of gazpacho:\nfrom gazpacho import Soup\n\nhtml = """"""\\\n<div class=""systemRequirementsMainBox"">\n<div class=""systemRequirementsRamContent"">\n<span title=""000 Plus Minimum RAM Requirement"">1 GB</span> </div>\n""""""\n\nsoup = Soup(html)\nsoup.find(""span"", {""title"": ""Minimum RAM Requirement""}).text\n# \'1 GB\'\n\n']",https://stackoverflow.com/questions/38133759/how-to-get-text-from-span-tag-in-beautifulsoup,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to use Beautiful Soup to extract string in <script> tag?,"
In a given .html page, I have a script tag like so:
     <script>jQuery(window).load(function () {
  setTimeout(function(){
    jQuery(""input[name=Email]"").val(""name@email.com"");
  }, 1000);
});</script>

How can I use Beautiful Soup to extract the email address?
",43k,"
            22
        ","['\nTo add a bit more to the @Bob\'s answer and assuming you need to also locate the script tag in the HTML which may have other script tags.\nThe idea is to define a regular expression that would be used for both locating the element with BeautifulSoup and extracting the email value:\nimport re\n\nfrom bs4 import BeautifulSoup\n\n\ndata = """"""\n<body>\n    <script>jQuery(window).load(function () {\n      setTimeout(function(){\n        jQuery(""input[name=Email]"").val(""name@email.com"");\n      }, 1000);\n    });</script>\n</body>\n""""""\npattern = re.compile(r\'\\.val\\(""([^@]+@[^@]+\\.[^@]+)""\\);\', re.MULTILINE | re.DOTALL)\nsoup = BeautifulSoup(data, ""html.parser"")\n\nscript = soup.find(""script"", text=pattern)\nif script:\n    match = pattern.search(script.text)\n    if match:\n        email = match.group(1)\n        print(email)\n\nPrints: name@email.com.\nHere we are using a simple regular expression for the email address, but we can go further and be more strict about it but I doubt that would be practically necessary for this problem.\n', '\nI ran into a similar problem and the issue seems to be that calling script_tag.text returns an empty string. Instead, you have to call script_tag.string. Maybe this changed in some version of BeautifulSoup?\nAnyway, @alecxe\'s answer didn\'t work for me, so I modified their solution:\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndata = """"""\n<body>\n    <script>jQuery(window).load(function () {\n      setTimeout(function(){\n        jQuery(""input[name=Email]"").val(""name@email.com"");\n      }, 1000);\n    });</script>\n</body>\n""""""\nsoup = BeautifulSoup(data, ""html.parser"")\n\nscript_tag = soup.find(""script"")\nif script_tag:\n  # contains all of the script tag, e.g. ""jQuery(window)...""\n  script_tag_contents = script_tag.string\n\n  # from there you can search the string using a regex, etc.\n  email = re.search(r\'\\.+val\\(""(.+)""\\);\', script_tag_contents).group(1)\n  print(email)\n\nThis prints name@email.com.\n', '\nnot possible using only BeautifulSoup, but you can do it for example with BS + regular expressions\nimport re\nfrom bs4 import BeautifulSoup as BS\n\nhtml = """"""<script> ... </script>""""""\n\nbs = BS(html)\n\ntxt = bs.script.get_text()\n\nemail = re.match(r\'.+val\\(""(.+?)""\\);\', txt).group(1)\n\nor like this:\n...\n\nemail = txt.split(\'.val(""\')[1].split(\'"");\')[0]\n\n', '\nIn order to get the string inside the <script> tag, you can use .contents or .string.\ndata = """"""\n   <body>\n<script>jQuery(window).load(function () {\n  setTimeout(function(){\n    jQuery(""input[name=Email]"").val(""name@email.com"");\n  }, 1000);\n});</script>\n </body>\n    """"""\nsoup = BeautifulSoup(data, ""html.parser"")\n\nscript = soup.find(""script"")\ninner_text_with_string = script.string\ninner_text_with_content = script.contents[0]\n\nprint(\'inner_text_with_string\', inner_text_with_string)\nprint(\'inner_text_with_content\', inner_text_with_content)\n\n', '\nYou could solve this with just a couple of lines of gazpacho and .split, no regex required!\nfrom gazpacho import Soup\n\nhtml = """"""\\\n<script>jQuery(window).load(function () {\n  setTimeout(function(){\n    jQuery(""input[name=Email]"").val(""name@email.com"");\n  }, 1000);\n});</script>\n""""""\n\nsoup = Soup(html)\nstring = soup.find(""script"").text\nstring.split("".val(\\"""")[-1].split(""\\"");"")[0]\n\nWhich would output:\n\'name@email.com\'\n\n']",https://stackoverflow.com/questions/38547569/how-to-use-beautiful-soup-to-extract-string-in-script-tag,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to deal with the captcha when doing Web Scraping in Puppeteer?,"
I'm using Puppeteer for Web Scraping and I have just noticed that sometimes, the website I'm trying to scrape asks for a captcha due to the amount of visits I'm doing from my computer. The captcha form looks like this one:

So, I would need help about how to handle this. I have been thinking about sending the captcha form to the client-side since I use Express and EJS in order to send the values to my index website, but I don't know if Puppeteer can send something like that.
Any ideas?
",42k,"
            20
        ","['\nThis is a reCAPTCHA (version 2, check out demos here), which is shown to you as the owner of the page does not want you to automatically crawl the page.\nYour options are the following:\nOption 1: Stop crawling or try to use an official API\nAs the owner of the page does not want you to crawl that page, you could simply respect that decision and stop crawling. Maybe there is a documented API that you can use.\nOption 2: Automate/Outsource the captcha solving\nThere is an entire industry which has people (often in developing countries) filling out captchas for other people\'s bots. I will not link to any particular site, but you can check out the other answer from Md. Abu Taher for more information on the topic or search for captcha solver.\nOption 3: Solve the captcha yourself\nFor this, let me explain how reCAPTCHA works and what happens when you visit a page using it.\n\nHow reCAPTCHA (v2) works\nEach page has an ID, which you can check by looking at the source code, example:\n<div class=""g-recaptcha form-field"" data-sitekey=""ID_OF_THE_WEBSITE_LONG_RANDOM_STRING""></div>\n\nWhen the reCAPTCHA code is loaded it will add a response textarea to the form with no value. It will look like this:\n<textarea id=""g-recaptcha-response"" name=""g-recaptcha-response"" class=""g-recaptcha-response"" style=""... display: none;""></textarea>\n\nAfter you solved the challenge, reCAPTCHA will add a very long string to this text field (which can then later be checked by the server/reCAPTCHA service in the backend) when the form is submitted.\n\nHow to solve the captcha yourself\nBy copying the value of the textarea field you can transfer the ""solved challenge"" from one browser to another (this is also what the solving services to for you). The full process looks like this:\n\nDetect if the page uses reCAPTCHA (e.g. check for .g-recaptcha) in the ""crawling"" browser\nOpen a second browser in non-headless mode with the same URL\nSolve the captcha yourself\nRead the value from: document.querySelector(\'#g-recaptcha-response\').value\nPut that value into the first browser: document.querySelector(\'#g-recaptcha-response\').value = \'...\'\nSubmit the form\n\nFurther information/reading\nThere is not much public information from Google how exactly reCAPTCHA works as this is a cat-and-mouse game between bot creators and Google detection algorithms, but there are some resources online with more information:\n\nOfficial docs from Google: Obviously, they just explain the basics and not how it works ""in the back""\nInsideReCaptcha: This is a project from 2014 which tries to ""reverse-engineer"" reCAPTCHA. Although this is quite old, there is still a lot of useful information on the page.\nAnother question on stackoverflow: This question contains some useful information about reCAPTCHA, but also many speculative (and very likely) outdated approaches on how to fool a reCAPTCHA.\n\n', ""\nYou should use combination of following:\n\nUse an API if the target website provides that. It's the most legal way.\nIncrease wait time between scraping request, do not send mass request to the server.\nChange/rotate IP frequently.\nChange user agent, browser viewport size and fingerprint.\nUse third party solutions for captcha.\nResolve the captcha by yourself, check the answer by Thomas Dondorf. Basically you need to wait for the captcha to appear on another browser, solve it from there. Third party solutions does this for you. \n\n\nDisclaimer: Do not use anti-captcha plugins/services to misuse resources. Resources are expensive.\n\nBasically the idea is to use anti-captcha services like (2captcha) to deal with persisting recaptcha. \nYou can use this plugin called puppeteer-extra-plugin-recaptcha by berstend. \n// puppeteer-extra is a drop-in replacement for puppeteer,\n// it augments the installed puppeteer with plugin functionality\nconst puppeteer = require('puppeteer-extra')\n\n// add recaptcha plugin and provide it your 2captcha token\n// 2captcha is the builtin solution provider but others work as well.\nconst RecaptchaPlugin = require('puppeteer-extra-plugin-recaptcha')\npuppeteer.use(\n  RecaptchaPlugin({\n    provider: { id: '2captcha', token: 'XXXXXXX' },\n    visualFeedback: true // colorize reCAPTCHAs (violet = detected, green = solved)\n  })\n)\n\nAfterwards you can run the browser as usual. It will pick up any captcha on the page and attempt to resolve it. You have to find the submit button which varies from site to site if it exists.\n// puppeteer usage as normal\npuppeteer.launch({ headless: true }).then(async browser =>""]",https://stackoverflow.com/questions/55493536/how-to-deal-with-the-captcha-when-doing-web-scraping-in-puppeteer,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CasperJS passing data back to PHP,"
CasperJS is being called by PHP using an exec() command. After CasperJS does its work such as retrieving parts of a webpage, how can the retrieved data be returned back to PHP?
",11k,"
            9
        ","['\nI think the best way to transfer data from CasperJS to another language such as PHP is running CasperJS script as a service. Because CasperJS has been written over PhantomJS, CasperJS can use an embedded web server module of PhantomJS called Mongoose.\nFor information about how works the embedded web server see here\nHere an example about how a CasperJS script can start a web server.\n//define ip and port to web service\nvar ip_server = \'127.0.0.1:8585\';\n\n//includes web server modules\nvar server = require(\'webserver\').create();\n\n//start web server\nvar service = server.listen(ip_server, function(request, response) {\n\n    var links = [];\n    var casper = require(\'casper\').create();\n\n    function getLinks() {\n        var links = document.querySelectorAll(\'h3.r a\');\n        return Array.prototype.map.call(links, function(e) {\n            return e.getAttribute(\'href\')\n        });\n    }\n\n    casper.start(\'http://google.fr/\', function() {\n        // search for \'casperjs\' from google form\n        this.fill(\'form[action=""/search""]\', { q: \'casperjs\' }, true);\n    });\n\n    casper.then(function() {\n        // aggregate results for the \'casperjs\' search\n        links = this.evaluate(getLinks);\n        // now search for \'phantomjs\' by filling the form again\n        this.fill(\'form[action=""/search""]\', { q: \'phantomjs\' }, true);\n    });\n\n    casper.then(function() {\n        // aggregate results for the \'phantomjs\' search\n        links = links.concat(this.evaluate(getLinks));\n    });\n\n    //\n    casper.run(function() {\n            response.statusCode = 200;\n            //sends results as JSON object\n            response.write(JSON.stringify(links, null, null));\n            response.close();              \n    });\n\n});\nconsole.log(\'Server running at http://\' + ip_server+\'/\');\n\n', ""\nYou can redirect output from stdout to an array.\nOn this page it says you can do:  \nstring exec ( string $command [, array &$output [, int &$return_var ]] )\n\nIt goes on to say: \n\nIf the output argument is present, then the specified array will be filled with every line of output from the command. \n\nSo basically you can do exec('casperjs command here, $array_here);\n""]",https://stackoverflow.com/questions/15852987/casperjs-passing-data-back-to-php,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R web scraping across multiple pages,"
I am working on a web scraping program to search for specific wines and return a list of local wines of that variety. The problem I am having is multiple page results. The code below is a basic example of what I am working with 
url2 <- ""http://www.winemag.com/?s=washington+merlot&search_type=reviews""
htmlpage2 <- read_html(url2)
names2 <- html_nodes(htmlpage2, "".review-listing .title"")
Wines2 <- html_text(names2)

For this specific search there are 39 pages of results. I know the url changes to http://www.winemag.com/?s=washington%20merlot&drink_type=wine&page=2, but is there an easy way to make the code loop through all the returned pages and compile the results from all 39 pages into a single list? I know I can manually do all the urls, but that seems like overkill. 
",16k,"
            8
        ","['\nYou can do something similar with purrr::map_df() as well if you want all the info as a data.frame:\nlibrary(rvest)\nlibrary(purrr)\n\nurl_base <- ""http://www.winemag.com/?s=washington merlot&drink_type=wine&page=%d""\n\nmap_df(1:39, function(i) {\n\n  # simple but effective progress indicator\n  cat(""."")\n\n  pg <- read_html(sprintf(url_base, i))\n\n  data.frame(wine=html_text(html_nodes(pg, "".review-listing .title"")),\n             excerpt=html_text(html_nodes(pg, ""div.excerpt"")),\n             rating=gsub("" Points"", """", html_text(html_nodes(pg, ""span.rating""))),\n             appellation=html_text(html_nodes(pg, ""span.appellation"")),\n             price=gsub(""\\\\$"", """", html_text(html_nodes(pg, ""span.price""))),\n             stringsAsFactors=FALSE)\n\n}) -> wines\n\ndplyr::glimpse(wines)\n## Observations: 1,170\n## Variables: 5\n## $ wine        (chr) ""Charles Smith 2012 Royal City Syrah (Columbia Valley (WA)...\n## $ excerpt     (chr) ""Green olive, green stem and fresh herb aromas are at the ...\n## $ rating      (chr) ""96"", ""95"", ""94"", ""93"", ""93"", ""93"", ""93"", ""93"", ""93"", ""93""...\n## $ appellation (chr) ""Columbia Valley"", ""Columbia Valley"", ""Columbia Valley"", ""...\n## $ price       (chr) ""140"", ""70"", ""70"", ""20"", ""70"", ""40"", ""135"", ""50"", ""60"", ""3...\n\n', '\nYou can lapply across a vector of the URLs, which you can make by pasting the base URL to a sequence:\nlibrary(rvest)\n\nwines <- lapply(paste0(\'http://www.winemag.com/?s=washington%20merlot&drink_type=wine&page=\', 1:39),\n                function(url){\n                    url %>% read_html() %>% \n                        html_nodes("".review-listing .title"") %>% \n                        html_text()\n                })\n\nThe result will be returned in a list with an element for each page.\n']",https://stackoverflow.com/questions/36683510/r-web-scraping-across-multiple-pages,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Excel VBA ""Method 'Document' of object 'IWebBrowser2' failed""","
I'm trying to automate a form submission in Excel for work, and In have trouble with the basics. I keep getting the error message:

""Method 'Document' of object 'IWebBrowser2' failed""

With the code as is, and if I include the Or part in the waiting check, I get the error

""Automation Error The object invoked has disconnected from its clients.""

I'm not sure what to do here, I've searched all over for solutions. This code is intended to eventually do more than this, but it keeps failing on the first try to getElementsByTagName. 
Sub GoToWebsiteTest()
Dim appIE As Object 'Internet Explorer
Set appIE = Nothing
Dim objElement As Object
Dim objCollection As Object

If appIE Is Nothing Then Set appIE = CreateObject(""InternetExplorer.Application"")
sURL = *link*
With appIE
    .Visible = True
    .Navigate sURL
End With

Do While appIE.Busy ' Or appIE.ReadyState <> 4
    DoEvents
Loop

Set objCollection = appIE.Document.getElementsByTagName(""input"")

Set appIE = Nothing
End Sub

",42k,"
            5
        ","['\nI ran into this same issue a while back. Use internet explorer at a medium integrity level. InternetExplorer defaults to a low integrity level which, if you are doing this over a local intranet at work, sometimes will give the second error message you show above. \nClick here for more reading on this. I\'ve modified your code below. Please let me know if that helps.\nSub GoToWebsiteTest()\nDim appIE As InternetExplorerMedium\n\'Set appIE = Nothing\nDim objElement As Object\nDim objCollection As Object\n\nSet appIE = New InternetExplorerMedium\nsURL = ""http://example.com""\nWith appIE\n    .Navigate sURL\n    .Visible = True\nEnd With\n\nDo While appIE.Busy Or appIE.ReadyState <> 4\n    DoEvents\nLoop\n\nSet objCollection = appIE.Document.getElementsByTagName(""input"")\n\nSet appIE = Nothing\nEnd Sub\n\nRemember references for Microsoft Internet Controls, and depending on what you plan on doing further, Microsoft HTML Object Library\n', '\nNot exactly same as above code but somehow similar , the following code solved my problem:\nDo\nLoop Until ie.readystate = 3\nDo\nLoop Until ie.readystate = 4\n\nJust put it before the line you want to start working with the contents.\nTo get more information about how does it work you can check here\n', ""\nThe below method solved my problem for this error:\nClose all the explorer instances through 'Task manager' and try to run the code it will work.\n""]",https://stackoverflow.com/questions/30086425/excel-vba-method-document-of-object-iwebbrowser2-failed,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping dynamic content in a website [duplicate],"






This question already has answers here:
                        
                    



Scrape a dynamic website

                                (8 answers)
                            

Closed 5 months ago.



I need to scrape news announcements from this website, Link.
The announcements seem to be generated dynamically. They dont appear in the source. I usually use mechanize but I assume it wouldnt work. What can I do for this? I'm ok with python or perl.
",19k,"
            2
        ","[""\nIf the content is generated dynamically, you can use Windmill or Seleninum to drive the browser and get the data once it's been rendered.\nYou can find an example here.\n"", ""\nThe polite option would be to ask the owners of the site if they have an API which allows you access to their news stories.\nThe less polite option would be to trace the HTTP transactions that take place while the page is loading and work out which one is the AJAX call which pulls in the data.\nLooks like it's this one. But it looks like it might contain session data, so I don't know how long it will continue to work for.\n"", '\nThere\'s also WWW::Scripter ""For scripting web sites that have scripts"" . Never used it.\n', '\nIn python you can use urllib and urllib2 to connect to a website and collect data.  For example:\nfrom urllib2 import urlopen\nmyUrl = ""http://www.marketvectorsindices.com/#!News/List""\ninStream = urlopen(myUrl)\ninstream.read(1024) # etc, in a while loop\n# all your fun page parsing code (perhaps: import from xml.dom.minidom import parse)\n\n']",https://stackoverflow.com/questions/8323728/scraping-dynamic-content-in-a-website,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Save and render a webpage with PhantomJS and node.js,"
I'm looking for an example of requesting a webpage, waiting for the JavaScript to render (JavaScript modifies the DOM), and then grabbing the HTML of the page.
This should be a simple example with an obvious use-case for PhantomJS. I can't find a decent example, the documentation seems to be all about command line use.
",63k,"
            62
        ","[""\nFrom your comments, I'd guess you have 2 options\n\nTry to find a phantomjs node module - https://github.com/amir20/phantomjs-node \nRun phantomjs as a child process inside node - http://nodejs.org/api/child_process.html\n\nEdit: \nIt seems the child process is suggested by phantomjs as a way of interacting with node, see faq - http://code.google.com/p/phantomjs/wiki/FAQ\nEdit:\nExample Phantomjs script for getting the pages HTML markup:\nvar page = require('webpage').create();  \npage.open('http://www.google.com', function (status) {\n    if (status !== 'success') {\n        console.log('Unable to access network');\n    } else {\n        var p = page.evaluate(function () {\n            return document.getElementsByTagName('html')[0].innerHTML\n        });\n        console.log(p);\n    }\n    phantom.exit();\n});\n\n"", ""\nWith v2 of phantomjs-node it's pretty easy to print the HTML after it has been processed. \nvar phantom = require('phantom');\n\nphantom.create().then(function(ph) {\n  ph.createPage().then(function(page) {\n    page.open('https://stackoverflow.com/').then(function(status) {\n      console.log(status);\n      page.property('content').then(function(content) {\n        console.log(content);\n        page.close();\n        ph.exit();\n      });\n    });\n  });\n});\n\nThis will show the output as it would have been rendered with the browser. \nEdit 2019: \nYou can use async/await:\nconst phantom = require('phantom');\n\n(async function() {\n  const instance = await phantom.create();\n  const page = await instance.createPage();\n  await page.on('onResourceRequested', function(requestData) {\n    console.info('Requesting', requestData.url);\n  });\n\n  const status = await page.open('https://stackoverflow.com/');\n  const content = await page.property('content');\n  console.log(content);\n\n  await instance.exit();\n})();\n\nOr if you just want to test, you can use npx\nnpx phantom@latest https://stackoverflow.com/\n\n"", ""\nI've used two different ways in the past, including the page.evaluate() method that queries the DOM that Declan mentioned. The other way I've passed info from the web page is to spit it out to console.log() from there, and in the phantomjs script use:\npage.onConsoleMessage = function (msg, line, source) {\n  console.log('console [' +source +':' +line +']> ' +msg);\n}\n\nI might also trap the variable msg in the onConsoleMessage and search for some encapsulate data. Depends on how you want to use the output.\nThen in the Nodejs script, you would have to scan the output of the Phantomjs script:\nvar yourfunc = function(...params...) {\n  var phantom = spawn('phantomjs', [...args]);\n  phantom.stdout.setEncoding('utf8');\n  phantom.stdout.on('data', function(data) {\n    //parse or echo data\n    var str_phantom_output = data.toString();\n    // The above will get triggered one or more times, so you'll need to\n    // add code to parse for whatever info you're expecting from the browser\n  });\n  phantom.stderr.on('data', function(data) {\n    // do something with error data\n  });\n  phantom.on('exit', function(code) {\n    if (code !== 0) {\n      // console.log('phantomjs exited with code ' +code);\n    } else {\n      // clean exit: do something else such as a passed-in callback\n    }\n  });\n}\n\nHope that helps some.\n"", '\nWhy not just use this ? \nvar page = require(\'webpage\').create();\npage.open(""http://example.com"", function (status)\n{\n    if (status !== \'success\') \n    {\n        console.log(\'FAIL to load the address\');            \n    } \n    else \n    {\n        console.log(\'Success in fetching the page\');\n        console.log(page.content);\n    }\n    phantom.exit();\n});\n\n', '\nLate update in case anyone stumbles on this question:\nA project on GitHub developed by a colleague of mine exactly aims at helping you do that: https://github.com/vmeurisse/phantomCrawl.\nIt still a bit young, it certainly is missing some documentation, but the example provided should help doing basic crawling.\n', ""\nHere's an old version that I use running node, express and phantomjs which saves out the page as a .png. You could tweak it fairly quickly to get the html.\nhttps://github.com/wehrhaus/sitescrape.git\n""]",https://stackoverflow.com/questions/9966826/save-and-render-a-webpage-with-phantomjs-and-node-js,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Puppeteer - Protocol error (Page.navigate): Target closed,"
As you can see with the sample code below, I'm using Puppeteer with a cluster of workers in Node to run multiple requests of websites screenshots by a given URL:
const cluster = require('cluster');
const express = require('express');
const bodyParser = require('body-parser');
const puppeteer = require('puppeteer');

async function getScreenshot(domain) {
    let screenshot;
    const browser = await puppeteer.launch({ args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage'] });
    const page = await browser.newPage();

    try {
        await page.goto('http://' + domain + '/', { timeout: 60000, waitUntil: 'networkidle2' });
    } catch (error) {
        try {
            await page.goto('http://' + domain + '/', { timeout: 120000, waitUntil: 'networkidle2' });
            screenshot = await page.screenshot({ type: 'png', encoding: 'base64' });
        } catch (error) {
            console.error('Connecting to: ' + domain + ' failed due to: ' + error);
        }

    await page.close();
    await browser.close();

    return screenshot;
}

if (cluster.isMaster) {
    const numOfWorkers = require('os').cpus().length;
    for (let worker = 0; worker < numOfWorkers; worker++) {
        cluster.fork();
    }

    cluster.on('exit', function (worker, code, signal) {
        console.debug('Worker ' + worker.process.pid + ' died with code: ' + code + ', and signal: ' + signal);
        Cluster.fork();
    });

    cluster.on('message', function (handler, msg) {
        console.debug('Worker: ' + handler.process.pid + ' has finished working on ' + msg.domain + '. Exiting...');
        if (Cluster.workers[handler.id]) {
            Cluster.workers[handler.id].kill('SIGTERM');
        }
    });
} else {
    const app = express();
    app.use(bodyParser.json());
    app.listen(80, function() {
        console.debug('Worker ' + process.pid + ' is listening to incoming messages');
    });

    app.post('/screenshot', (req, res) => {
        const domain = req.body.domain;

        getScreenshot(domain)
            .then((screenshot) =>
                try {
                    process.send({ domain: domain });
                } catch (error) {
                    console.error('Error while exiting worker ' + process.pid + ' due to: ' + error);
                }

                res.status(200).json({ screenshot: screenshot });
            })
            .catch((error) => {
                try {
                    process.send({ domain: domain });
                } catch (error) {
                    console.error('Error while exiting worker ' + process.pid + ' due to: ' + error);
                }

                res.status(500).json({ error: error });
            });
    });
}

Some explanation:

Each time a request arrives a worker will process it and kill itself at the end
Each worker creates a new browser instance with a single page, and if a page took more than 60sec to load, it will retry reloading it (in the same page because maybe some resources has already been loaded) with timeout of 120sec
Once finished both the page and the browser will be closed

My problem is that some legitimate domains get errors that I can't explain:
Error: Protocol error (Page.navigate): Target closed.

Error: Protocol error (Runtime.callFunctionOn): Session closed. Most likely the page has been closed.

I read at some git issue (that I can't find now) that it can happen when the page redirects and adds 'www' at the start, but I'm hoping it's false...
Is there something I'm missing?
",76k,"
            51
        ","['\nWhat ""Target closed"" means\nWhen you launch a browser via puppeteer.launch it will start a browser and connect to it. From there on any function you execute on your opened browser (like page.goto) will be send via the Chrome DevTools Protocol to the browser. A target means a tab in this context.\nThe Target closed exception is thrown when you are trying to run a function, but the target (tab) was already closed.\nSimilar error messages\nThe error message was recently changed to give more meaningful information. It now gives the following message:\n\nError: Protocol error (Target.activateTarget): Session closed. Most likely the page has been closed.\n\n\nWhy does it happen\nThere are multiple reasons why this could happen.\n\nYou used a resource that was already closed\nMost likely, you are seeing this message because you closed the tab/browser and are still trying to use the resource. To give an simple example:\nconst browser = await puppeteer.launch();\nconst page = await browser.newPage();\n\nawait browser.close();\nawait page.goto(\'http://www.google.com\');\n\nIn this case the browser was closed and after that, a page.goto was called resulting in the error message. Most of the time, it will not be that obvious. Maybe an error handler already closed the page during a cleanup task, while your script is still crawling.\nThe browser crashed or was unable to initialize\nI also experience this every few hundred requests. There is an issue about this on the puppeteer repository as well. It seems to be the case, when you are using a lot of memory or CPU power. Maybe you are spawning a lot of browser? In these cases the browser might crash or disconnect.\nI found no ""silver bullet"" solution to this problem. But you might want to check out the library puppeteer-cluster (disclaimer: I\'m the author) which handles these kind of error cases and let\'s you retry the URL when the error happens. It can also manage a pool of browser instances and would also simplify your code.\n\n', ""\nFor me removing '--single-process' from args fixed the issue.\npuppeteerOptions: {\n    headless: true,\n    args: [\n        '--disable-gpu',\n        '--disable-dev-shm-usage',\n        '--disable-setuid-sandbox',\n        '--no-first-run',\n        '--no-sandbox',\n        '--no-zygote',\n        '--deterministic-fetch',\n        '--disable-features=IsolateOrigins',\n        '--disable-site-isolation-trials',\n        // '--single-process',\n    ],\n}\n\n"", ""\nI was just experiencing the same issue every time I tried running my puppeteer script*. The above did not resolve this issue for me.\nI got it to work by removing and reinstalling the puppeteer package:\nnpm remove puppeteer\nnpm i puppeteer\n\n*I only experienced this issue when setting the headless option to 'false`\n"", '\nI\'ve wound up at this thread a few times, and the typical culprit is that I forgot to await a Puppeteer page call that returned a promise, causing a race condition.\nHere\'s a minimal example of what this can look like:\nconst puppeteer = require(""puppeteer"");\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch({headless: true});\n  const [page] = await browser.pages();\n  page.goto(""https://www.stackoverflow.com""); // whoops, forgot await!\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close())\n;\n\nOutput is:\nC:\\Users\\foo\\Desktop\\puppeteer-playground\\node_modules\\puppeteer\\lib\\cjs\\puppeteer\\common\\Connection.js:217\n            this._callbacks.set(id, { resolve, reject, error: new Error(), method });\n                                                              ^\n\nError: Protocol error (Page.navigate): Target closed.\n    at C:\\Users\\foo\\Desktop\\puppeteer-playground\\node_modules\\puppeteer\\lib\\cjs\\puppeteer\\common\\Connection.js:217:63\n\nIn this case, it seems like an unmissable error, but in a larger chunk of code and the promise is nested or in a condition, it\'s easy to overlook.\nYou\'ll get a similar error for forgetting to await a page.click() or other promise call, for example, Error: Protocol error (Runtime.callFunctionOn): Target closed., which can be seen in the question UnhandledPromiseRejectionWarning: Error: Protocol error (Runtime.callFunctionOn): Target closed. (Puppeteer)\nThis is a contribution to the thread as a canonical resource for the error and may not be the solution to OP\'s problem, although the fundamental race condition seems to be a likely cause.\n', ""\nIn 2021 I'm receiving the very similar following error Error: Error pdf creationError: Protocol error (Target.setDiscoverTargets): Target closed., I solved it by playing with different args, so if your production server has a pipe:true flag in puppeteer.launch obj it will produce errors.\nAlso --disable-dev-shm-usage flag do the trick\nThe solution below works for me:\nconst browser = await puppeteer.launch({\n  headless: true,\n  // pipe: true, <-- delete this property\n  args: [\n    '--no-sandbox',\n    '--disable-dev-shm-usage', // <-- add this one\n    ],\n});\n\n"", '\nCheck your jest-puppeteer.config.js file.\nI made the below mistake\nmodule.exports = {\n    launch: {\n        headless: false,\n        browserContext: ""default"",\n    },\n};\n\nand after correcting it as below\nmodule.exports = {\n    launch: {\n        headless: false\n    },\n    browserContext: ""default"",\n};\n\neverything worked just fine!!!\n', '\nAfter hours of frustrations I realized that this happens when it goes to a new page and I need to be using await page.waitForNavigation() before I do anything and after I press a button or do any action that will cause it to redirect.\n']",https://stackoverflow.com/questions/51629151/puppeteer-protocol-error-page-navigate-target-closed,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get the scrapy failure URLs?,"
I'm a newbie of scrapy and it's amazing crawler framework i have known! 
In my project, I sent more than 90, 000 requests, but there are some of them failed. 
I set the log level to be INFO, and i just can see some statistics but no details. 
2012-12-05 21:03:04+0800 [pd_spider] INFO: Dumping spider stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.ConnectionDone': 1,
 'downloader/request_bytes': 46282582,
 'downloader/request_count': 92383,
 'downloader/request_method_count/GET': 92383,
 'downloader/response_bytes': 123766459,
 'downloader/response_count': 92382,
 'downloader/response_status_count/200': 92382,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2012, 12, 5, 13, 3, 4, 836000),
 'item_scraped_count': 46191,
 'request_depth_max': 1,
 'scheduler/memory_enqueued': 92383,
 'start_time': datetime.datetime(2012, 12, 5, 12, 23, 25, 427000)}

Is there any way to get more detail report? For example, show those failed URLs. Thanks!
",39k,"
            51
        ","['\nYes, this is possible. \n\nThe code below adds a failed_urls list to a basic spider class and appends urls to it if the response status of the url is 404 (this would need to be extended to cover other error statuses as required). \nNext I added a handle that joins the list into a single string and adds it to the spider\'s stats when the spider is closed.\nBased on your comments, it\'s possible to track Twisted errors, and some of the answers below give examples on how to handle that particular use case\nThe code has been updated to work with Scrapy 1.8. All thanks to this should go to Juliano Mendieta, since all I did was simply to add his suggested edits and confirm that the spider worked as intended.\n\n\nfrom scrapy import Spider, signals\n\nclass MySpider(Spider):\n    handle_httpstatus_list = [404] \n    name = ""myspider""\n    allowed_domains = [""example.com""]\n    start_urls = [\n        \'http://www.example.com/thisurlexists.html\',\n        \'http://www.example.com/thisurldoesnotexist.html\',\n        \'http://www.example.com/neitherdoesthisone.html\'\n    ]\n\n    def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.failed_urls = []\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super(MySpider, cls).from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.handle_spider_closed, signals.spider_closed)\n        return spider\n\n    def parse(self, response):\n        if response.status == 404:\n            self.crawler.stats.inc_value(\'failed_url_count\')\n            self.failed_urls.append(response.url)\n\n    def handle_spider_closed(self, reason):\n        self.crawler.stats.set_value(\'failed_urls\', \', \'.join(self.failed_urls))\n\n    def process_exception(self, response, exception, spider):\n        ex_class = ""%s.%s"" % (exception.__class__.__module__, exception.__class__.__name__)\n        self.crawler.stats.inc_value(\'downloader/exception_count\', spider=spider)\n        self.crawler.stats.inc_value(\'downloader/exception_type_count/%s\' % ex_class, spider=spider)\n\n\nExample output (note that the downloader/exception_count* stats will only appear if exceptions are actually thrown - I simulated them by trying to run the spider after I\'d turned off my wireless adapter):\n2012-12-10 11:15:26+0000 [myspider] INFO: Dumping Scrapy stats:\n    {\'downloader/exception_count\': 15,\n     \'downloader/exception_type_count/twisted.internet.error.DNSLookupError\': 15,\n     \'downloader/request_bytes\': 717,\n     \'downloader/request_count\': 3,\n     \'downloader/request_method_count/GET\': 3,\n     \'downloader/response_bytes\': 15209,\n     \'downloader/response_count\': 3,\n     \'downloader/response_status_count/200\': 1,\n     \'downloader/response_status_count/404\': 2,\n     \'failed_url_count\': 2,\n     \'failed_urls\': \'http://www.example.com/thisurldoesnotexist.html, http://www.example.com/neitherdoesthisone.html\'\n     \'finish_reason\': \'finished\',\n     \'finish_time\': datetime.datetime(2012, 12, 10, 11, 15, 26, 874000),\n     \'log_count/DEBUG\': 9,\n     \'log_count/ERROR\': 2,\n     \'log_count/INFO\': 4,\n     \'response_received_count\': 3,\n     \'scheduler/dequeued\': 3,\n     \'scheduler/dequeued/memory\': 3,\n     \'scheduler/enqueued\': 3,\n     \'scheduler/enqueued/memory\': 3,\n     \'spider_exceptions/NameError\': 2,\n     \'start_time\': datetime.datetime(2012, 12, 10, 11, 15, 26, 560000)}\n\n', '\nHere\'s another example how to handle and collect 404 errors (checking github help pages):\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.item import Item, Field\n\n\nclass GitHubLinkItem(Item):\n    url = Field()\n    referer = Field()\n    status = Field()\n\n\nclass GithubHelpSpider(CrawlSpider):\n    name = ""github_help""\n    allowed_domains = [""help.github.com""]\n    start_urls = [""https://help.github.com"", ]\n    handle_httpstatus_list = [404]\n    rules = (Rule(SgmlLinkExtractor(), callback=\'parse_item\', follow=True),)\n\n    def parse_item(self, response):\n        if response.status == 404:\n            item = GitHubLinkItem()\n            item[\'url\'] = response.url\n            item[\'referer\'] = response.request.headers.get(\'Referer\')\n            item[\'status\'] = response.status\n\n            return item\n\nJust run scrapy runspider with -o output.json and see list of items in the output.json file.\n', '\nScrapy ignores 404 by default and does not parse it. If you are getting an error code 404 in response, you can handle this with a very easy way.\nIn settings.py, write:\nHTTPERROR_ALLOWED_CODES = [404,403]\n\nAnd then handle the response status code in your parse function:\ndef parse(self,response):\n    if response.status == 404:\n        #your action on error\n\n', ""\nThe answers from @Talvalin and @alecxe helped me a great deal, but they do not seem to capture downloader events that do not generate a response object (for instance, twisted.internet.error.TimeoutError and twisted.web.http.PotentialDataLoss). These errors show up in the stats dump at the end of the run, but without any meta info. \nAs I found out here, the errors are tracked by the stats.py middleware, captured in the DownloaderStats class' process_exception method, and specifically in the ex_class variable, which increments each error type as necessary, and then dumps the counts at the end of the run.  \nTo match such errors with information from the corresponding request object, you can add a unique id to each request (via request.meta), then pull it into the process_exception method of stats.py:\nself.stats.set_value('downloader/my_errs/{0}'.format(request.meta), ex_class)\n\nThat will generate a unique string for each downloader-based error not accompanied by a response. You can then save the altered stats.py as something else (e.g. my_stats.py), add it to the downloadermiddlewares (with the right precedence), and disable the stock stats.py:\nDOWNLOADER_MIDDLEWARES = {\n    'myproject.my_stats.MyDownloaderStats': 850,\n    'scrapy.downloadermiddleware.stats.DownloaderStats': None,\n    }\n\nThe output at the end of the run looks like this (here using meta info where each request url is mapped to a group_id and member_id separated by a slash, like '0/14'):\n{'downloader/exception_count': 3,\n 'downloader/exception_type_count/twisted.web.http.PotentialDataLoss': 3,\n 'downloader/my_errs/0/1': 'twisted.web.http.PotentialDataLoss',\n 'downloader/my_errs/0/38': 'twisted.web.http.PotentialDataLoss',\n 'downloader/my_errs/0/86': 'twisted.web.http.PotentialDataLoss',\n 'downloader/request_bytes': 47583,\n 'downloader/request_count': 133,\n 'downloader/request_method_count/GET': 133,\n 'downloader/response_bytes': 3416996,\n 'downloader/response_count': 130,\n 'downloader/response_status_count/200': 95,\n 'downloader/response_status_count/301': 24,\n 'downloader/response_status_count/302': 8,\n 'downloader/response_status_count/500': 3,\n 'finish_reason': 'finished'....}\n\nThis answer deals with non-downloader-based errors.\n"", '\nAs of scrapy 0.24.6, the method suggested by alecxe won\'t catch errors with the start URLs. To record errors with the start URLs you need to override parse_start_urls. Adapting alexce\'s answer for this purpose, you\'d get:\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.item import Item, Field\n\nclass GitHubLinkItem(Item):\n    url = Field()\n    referer = Field()\n    status = Field()\n\nclass GithubHelpSpider(CrawlSpider):\n    name = ""github_help""\n    allowed_domains = [""help.github.com""]\n    start_urls = [""https://help.github.com"", ]\n    handle_httpstatus_list = [404]\n    rules = (Rule(SgmlLinkExtractor(), callback=\'parse_item\', follow=True),)\n\n    def parse_start_url(self, response):\n        return self.handle_response(response)\n\n    def parse_item(self, response):\n        return self.handle_response(response)\n\n    def handle_response(self, response):\n        if response.status == 404:\n            item = GitHubLinkItem()\n            item[\'url\'] = response.url\n            item[\'referer\'] = response.request.headers.get(\'Referer\')\n            item[\'status\'] = response.status\n\n            return item\n\n', '\nThis is an update on this question. I ran in to a similar problem and needed to use the scrapy signals to call a function in my pipeline. I have edited @Talvalin\'s code, but wanted to make an answer just for some more clarity. \nBasically, you should add in self as an argument for handle_spider_closed.\nYou should also call the dispatcher in init so that you can pass the spider instance (self) to the handleing method.  \nfrom scrapy.spider import Spider\nfrom scrapy.xlib.pydispatch import dispatcher\nfrom scrapy import signals\n\nclass MySpider(Spider):\n    handle_httpstatus_list = [404] \n    name = ""myspider""\n    allowed_domains = [""example.com""]\n    start_urls = [\n        \'http://www.example.com/thisurlexists.html\',\n        \'http://www.example.com/thisurldoesnotexist.html\',\n        \'http://www.example.com/neitherdoesthisone.html\'\n    ]\n\n    def __init__(self, category=None):\n        self.failed_urls = []\n        # the dispatcher is now called in init\n        dispatcher.connect(self.handle_spider_closed,signals.spider_closed) \n\n\n    def parse(self, response):\n        if response.status == 404:\n            self.crawler.stats.inc_value(\'failed_url_count\')\n            self.failed_urls.append(response.url)\n\n    def handle_spider_closed(self, spider, reason): # added self \n        self.crawler.stats.set_value(\'failed_urls\',\',\'.join(spider.failed_urls))\n\n    def process_exception(self, response, exception, spider):\n        ex_class = ""%s.%s"" % (exception.__class__.__module__,  exception.__class__.__name__)\n        self.crawler.stats.inc_value(\'downloader/exception_count\', spider=spider)\n        self.crawler.stats.inc_value(\'downloader/exception_type_count/%s\' % ex_class, spider=spider)\n\nI hope this helps anyone with the same problem in the future.\n', ""\nIn addition to some of these answers, if you want to track Twisted errors, I would take a look at using the Request object's errback parameter, on which you can set a callback function to be called with the Twisted Failure on a request failure.\nIn addition to the url, this method can allow you to track the type of failure.\nYou can then log the urls by using: failure.request.url (where failure is the Twisted Failure object passed into errback).\n# these would be in a Spider\ndef start_requests(self):\n    for url in self.start_urls:\n        yield scrapy.Request(url, callback=self.parse,\n                                  errback=self.handle_error)\n\ndef handle_error(self, failure):\n    url = failure.request.url\n    logging.error('Failure type: %s, URL: %s', failure.type,\n                                               url)\n\nThe Scrapy docs give a full example of how this can be done, except that the calls to the Scrapy logger are now depreciated, so I've adapted my example to use Python's built in logging):\nhttps://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-errbacks\n"", ""\nYou can capture failed urls in two ways.\n\nDefine scrapy request with errback\nclass TestSpider(scrapy.Spider):\n    def start_requests(self):\n        yield scrapy.Request(url, callback=self.parse, errback=self.errback)\n\n    def errback(self, failure):\n        '''handle failed url (failure.request.url)'''\n        pass\n\nUse signals.item_dropped\nclass TestSpider(scrapy.Spider):\n    def __init__(self):\n        crawler.signals.connect(self.request_dropped, signal=signals.request_dropped)\n\n    def request_dropped(self, request, spider):\n        '''handle failed url (request.url)'''\n        pass\n\n\n[!Notice] Scrapy request with errback can not catch some auto retry failure, like connection error, RETRY_HTTP_CODES in settings.\n"", '\nBasically Scrapy Ignores 404 Error by Default, It was defined in httperror middleware.\nSo, Add HTTPERROR_ALLOW_ALL = True to your settings file.\nAfter this you can access response.status through your parse function.\nYou can handle it like this.\ndef parse(self,response):\n    if response.status==404:\n        print(response.status)\n    else:\n        do something\n\n']",https://stackoverflow.com/questions/13724730/how-to-get-the-scrapy-failure-urls,web-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
automatically execute an Excel macro on a cell change,"
How can I automatically execute an Excel macro each time a value in a particular cell changes?
Right now, my working code is:
Private Sub Worksheet_Change(ByVal Target As Range)
    If Not Intersect(Target, Range(""H5"")) Is Nothing Then Macro
End Sub

where ""H5"" is the particular cell being monitored and Macro is the name of the macro.
Is there a better way?
",526k,"
            98
        ","['\nYour code looks pretty good.\nBe careful, however, for your call to Range(""H5"") is a shortcut command to Application.Range(""H5""), which is equivalent to Application.ActiveSheet.Range(""H5""). This could be fine, if the only changes are user-changes -- which is the most typical -- but it is possible for the worksheet\'s cell values to change when it is not the active sheet via programmatic changes, e.g. VBA.\nWith this in mind, I would utilize Target.Worksheet.Range(""H5""):\nPrivate Sub Worksheet_Change(ByVal Target As Range)\n    If Not Intersect(Target, Target.Worksheet.Range(""H5"")) Is Nothing Then Macro\nEnd Sub\n\nOr you can use Me.Range(""H5""), if the event handler is on the code page for the worksheet in question (it usually is):\nPrivate Sub Worksheet_Change(ByVal Target As Range)\n    If Not Intersect(Target, Me.Range(""H5"")) Is Nothing Then Macro\nEnd Sub\n\n', '\nI spent a lot of time researching this and learning how it all works, after really messing up the event triggers. Since there was so much scattered info I decided to share what I have found to work all in one place, step by step as follows:\n1) Open VBA Editor, under VBA Project (YourWorkBookName.xlsm) open Microsoft Excel Object and select the Sheet to which the change event will pertain.\n2) The default code view is ""General."" From the drop-down list at the top middle, select ""Worksheet.""\n3) Private Sub Worksheet_SelectionChange is already there as it should be, leave it alone. Copy/Paste Mike Rosenblum\'s code from above and change the .Range reference to the cell for which you are watching for a change (B3, in my case). Do not place your Macro yet, however (I removed the word ""Macro"" after ""Then""):\nPrivate Sub Worksheet_Change(ByVal Target As Range)\n    If Not Intersect(Target, Me.Range(""H5"")) Is Nothing Then\nEnd Sub\n\nor from the drop-down list at the top left, select ""Change"" and in the space between Private Sub and End Sub, paste If Not Intersect(Target, Me.Range(""H5"")) Is Nothing Then\n4) On the line after ""Then"" turn off events so that when you call your macro, it does not trigger events and try to run this Worksheet_Change again in a never ending cycle that crashes Excel and/or otherwise messes everything up:\nApplication.EnableEvents = False\n\n5) Call your macro\nCall YourMacroName\n\n6) Turn events back on so the next change (and any/all other events) trigger:\nApplication.EnableEvents = True\n\n7) End the If block and the Sub:\n    End If\nEnd Sub\n\nThe entire code:\nPrivate Sub Worksheet_Change(ByVal Target As Range)\n    If Not Intersect(Target, Me.Range(""B3"")) Is Nothing Then\n        Application.EnableEvents = False\n        Call UpdateAndViewOnly\n        Application.EnableEvents = True\n    End If\nEnd Sub\n\nThis takes turning events on/off out of the Modules which creates problems and simply lets the change trigger, turns off events, runs your macro and turns events back on.\n', '\nHandle the Worksheet_Change event or the Workbook_SheetChange event.\nThe event handlers take an argument ""Target As Range"", so you can check if the range that\'s changing includes the cell you\'re interested in.\n', '\nI prefer this way, not using a cell but a range\n    Dim cell_to_test As Range, cells_changed As Range\n\n    Set cells_changed = Target(1, 1)\n    Set cell_to_test = Range( RANGE_OF_CELLS_TO_DETECT )\n\n    If Not Intersect(cells_changed, cell_to_test) Is Nothing Then \n       Macro\n    End If\n\n', '\nI have a cell which is linked to online stock database and updated frequently. I want to trigger a macro whenever the cell value is updated.\nI believe this is similar to cell value change by a program or any external data update but above examples somehow do not work for me. I think the problem is because excel internal events are not triggered, but thats my guess.\nI did the following,\nPrivate Sub Worksheet_Change(ByVal Target As Range) \n  If Not Intersect(Target, Target.Worksheets(""Symbols"").Range(""$C$3"")) Is Nothing Then\n   \'Run Macro\nEnd Sub\n\n']",https://stackoverflow.com/questions/409434/automatically-execute-an-excel-macro-on-a-cell-change,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switch tabs using Selenium WebDriver with Java,"
Using Selenium WebDriver with Java.
I am trying to automate a functionality where I have to open a new tab do some operations there and come back to previous tab (Parent).
I used switch handle but it's not working.
And one strange thing the two tabs are having same window handle due to which I am not able to switch between tabs.
However when I am trying with different Firefox windows it works, but for tab it's not working.
How can I switch tabs?
Or, how can I switch tabs without using window handle as window handle is same of both tabs in my case?
(I have observed that when you open different tabs in same window, window handle remains same)
",385k,"
            75
        ","['\n    psdbComponent.clickDocumentLink();\n    ArrayList<String> tabs2 = new ArrayList<String> (driver.getWindowHandles());\n    driver.switchTo().window(tabs2.get(1));\n    driver.close();\n    driver.switchTo().window(tabs2.get(0));\n\nThis code perfectly worked for me. Try it out. You always need to switch your driver to new tab, before you want to do something on new tab.\n', '\nThis is a simple solution for opening a new tab, changing focus to it, closing the tab and return focus to the old/original tab:\n@Test\npublic void testTabs() {\n    driver.get(""https://business.twitter.com/start-advertising"");\n    assertStartAdvertising();\n\n    // considering that there is only one tab opened in that point.\n    String oldTab = driver.getWindowHandle();\n    driver.findElement(By.linkText(""Twitter Advertising Blog"")).click();\n    ArrayList<String> newTab = new ArrayList<String>(driver.getWindowHandles());\n    newTab.remove(oldTab);\n    // change focus to new tab\n    driver.switchTo().window(newTab.get(0));\n    assertAdvertisingBlog();\n\n    // Do what you want here, you are in the new tab\n\n    driver.close();\n    // change focus back to old tab\n    driver.switchTo().window(oldTab);\n    assertStartAdvertising();\n\n    // Do what you want here, you are in the old tab\n}\n\nprivate void assertStartAdvertising() {\n    assertEquals(""Start Advertising | Twitter for Business"", driver.getTitle());\n}\n\nprivate void assertAdvertisingBlog() {\n    assertEquals(""Twitter Advertising"", driver.getTitle());\n}\n\n', '\nThere is a difference how web driver handles different windows and how it handles different tabs.\nCase 1:\nIn case there are multiple windows, then the following code can help:\n//Get the current window handle\nString windowHandle = driver.getWindowHandle();\n\n//Get the list of window handles\nArrayList tabs = new ArrayList (driver.getWindowHandles());\nSystem.out.println(tabs.size());\n//Use the list of window handles to switch between windows\ndriver.switchTo().window(tabs.get(0));\n\n//Switch back to original window\ndriver.switchTo().window(mainWindowHandle);\n\n\nCase 2:\nIn case there are multiple tabs in the same window, then there is only one window handle. Hence switching between window handles keeps the control in the same tab. In this case using Ctrl + \\t (Ctrl + Tab) to switch between tabs is more useful.\n//Open a new tab using Ctrl + t\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL +""t"");\n//Switch between tabs using Ctrl + \\t\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL +""\\t"");\n\nDetailed sample code can be found here:\nhttp://design-interviews.blogspot.com/2014/11/switching-between-tabs-in-same-browser-window.html\n', '\nWork around\nAssumption : By Clicking something on your web page leads to open a new tab.\nUse below logic to switch to second tab.\nnew Actions(driver).sendKeys(driver.findElement(By.tagName(""html"")), Keys.CONTROL).sendKeys(driver.findElement(By.tagName(""html"")),Keys.NUMPAD2).build().perform();\n\nIn the same manner you can switch back to first tab again.\nnew Actions(driver).sendKeys(driver.findElement(By.tagName(""html"")), Keys.CONTROL).sendKeys(driver.findElement(By.tagName(""html"")),Keys.NUMPAD1).build().perform();\n\n', '\nSince the driver.window_handles is not in order , a better solution is this.\n\nfirst switch to the first tab using the shortcut Control + X to switch to the \'x\' th tab in the browser window .\n\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + ""1"");\n# goes to 1st tab\n\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + ""4"");\n# goes to 4th tab if its exists or goes to last tab.\n\n', '\nString selectLinkOpeninNewTab = Keys.chord(Keys.CONTROL, Keys.RETURN);\n    WebElement e = driver.findElement(By\n            .xpath(""html/body/header/div/div[1]/nav/a""));\ne.sendKeys(selectLinkOpeninNewTab);//to open the link in a current page in to the browsers new tab\n\n    e.sendKeys(Keys.CONTROL + ""\\t"");//to move focus to next tab in same browser\n    try {\n        Thread.sleep(8000);\n    } catch (InterruptedException e1) {\n        // TODO Auto-generated catch block\n        e1.printStackTrace();\n    }\n    //to wait some time in that tab\n    e.sendKeys(Keys.CONTROL + ""\\t"");//to switch the focus to old tab again\n\nHope it helps to you..\n', '\nThe first thing you need to do is opening a new tab and save it\'s handle name. It will be best to do it using javascript and not keys(ctrl+t) since keys aren\'t always available on automation servers. example:\npublic static String openNewTab(String url) {\n    executeJavaScript(""window.parent = window.open(\'parent\');"");\n    ArrayList<String> tabs = new ArrayList<String>(bot.driver.getWindowHandles());\n    String handleName = tabs.get(1);\n    bot.driver.switchTo().window(handleName);\n    System.setProperty(""current.window.handle"", handleName);\n    bot.driver.get(url);\n    return handleName;\n}\n\nThe second thing you need to do is switching between the tabs. Doing it by switch window handles only, will not always work since the tab you\'ll work on, won\'t always be in focus and Selenium will fail from time to time.\nAs I said, it\'s a bit problematic to use keys, and javascript doesn\'t really support switching tabs, so I used alerts to switch tabs and it worked like a charm:\npublic static void switchTab(int tabNumber, String handleName) {\n        driver.switchTo().window(handleName);\n        System.setProperty(""current.window.handle"", handleName);\n        if (tabNumber==1)\n            executeJavaScript(""alert(\\""alert\\"");"");\n        else\n            executeJavaScript(""parent.alert(\\""alert\\"");"");\n        bot.wait(1000);\n        driver.switchTo().alert().accept();\n    }\n\n', '\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL,Keys.SHIFT,Keys.TAB);\n\nThis method helps in switching between multiple windows. The restricting problem with this method is that it can only be used so many times until the required window is reached. Hope it helps.\n', '\nWith Selenium 2.53.1 using firefox 47.0.1 as the WebDriver in Java: no matter how many tabs I opened, ""driver.getWindowHandles()"" would only return one handle so it was impossible to switch between tabs.\nOnce I started using Chrome 51.0, I could get all handles.  The following code show how to access multiple drivers and multiple tabs within each driver.\n// INITIALIZE TWO DRIVERS (THESE REPRESENT SEPARATE CHROME WINDOWS)\ndriver1 = new ChromeDriver();\ndriver2 = new ChromeDriver();\n\n// LOOP TO OPEN AS MANY TABS AS YOU WISH\nfor(int i = 0; i < TAB_NUMBER; i++) {\n   driver1.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + ""t"");\n   // SLEEP FOR SPLIT SECOND TO ALLOW DRIVER TIME TO OPEN TAB\n   Thread.sleep(100);\n\n// STORE TAB HANDLES IN ARRAY LIST FOR EASY ACCESS\nArrayList tabs1 = new ArrayList<String> (driver1.getWindowHandles());\n\n// REPEAT FOR THE SECOND DRIVER (SECOND CHROME BROWSER WINDOW)\n\n// LOOP TO OPEN AS MANY TABS AS YOU WISH\nfor(int i = 0; i < TAB_NUMBER; i++) {\n   driver2.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + ""t"");\n   // SLEEP FOR SPLIT SECOND TO ALLOW DRIVER TIME TO OPEN TAB\n   Thread.sleep(100);\n\n// STORE TAB HANDLES IN ARRAY LIST FOR EASY ACCESS\nArrayList tabs2 = new ArrayList<String> (driver1.getWindowHandles());\n\n// NOW PERFORM DESIRED TASKS WITH FIRST BROWSER IN ANY TAB\nfor(int ii = 0; ii <= TAB_NUMBER; ii++) {\n   driver1.switchTo().window(tabs1.get(ii));\n   // LOGIC FOR THAT DRIVER\'S CURRENT TAB\n}\n\n// PERFORM DESIRED TASKS WITH SECOND BROWSER IN ANY TAB\nfor(int ii = 0; ii <= TAB_NUMBER; ii++) {\n   drvier2.switchTo().window(tabs2.get(ii));\n   // LOGIC FOR THAT DRIVER\'S CURRENT TAB\n}\n\nHopefully that gives you a good idea of how to manipulate multiple tabs in multiple browser windows.\n', '\nSimple Answer which worked for me:\nfor (String handle1 : driver1.getWindowHandles()) {\n        System.out.println(handle1); \n        driver1.switchTo().window(handle1);     \n}\n\n', '\nSet<String> tabs = driver.getWindowHandles();\nIterator<String> it = tabs.iterator();\ntab1 = it.next();\ntab2 = it.next();\ndriver.switchTo().window(tab1);\ndriver.close();\ndriver.switchTo().window(tab2);\n\nTry this. It should work\n', ""\nI had a problem recently, the link was opened in a new tab, but selenium focused still on the initial tab.\nI'm using Chromedriver and the only way to focus on a tab was for me to use switch_to_window().\nHere's the Python code:\ndriver.switch_to_window(driver.window_handles[-1])\n\nSo the tip is to find out the name of the window handle you need, they are stored as list in\ndriver.window_handles\n\n"", '\nPlease see below:\nWebDriver driver = new FirefoxDriver();\n\ndriver.manage().window().maximize();\ndriver.get(""https://www.irctc.co.in/"");\nString oldTab = driver.getWindowHandle();\n\n//For opening window in New Tab\nString selectLinkOpeninNewTab = Keys.chord(Keys.CONTROL,Keys.RETURN); \ndriver.findElement(By.linkText(""Hotels & Lounge"")).sendKeys(selectLinkOpeninNewTab);\n\n// Perform Ctrl + Tab to focus on new Tab window\nnew Actions(driver).sendKeys(Keys.chord(Keys.CONTROL, Keys.TAB)).perform();\n\n// Switch driver control to focused tab window\ndriver.switchTo().window(oldTab);\n\ndriver.findElement(By.id(""textfield"")).sendKeys(""bangalore"");\n\nHope this is helpful!\n', '\nIt is A very simple process: assume you have two tabs so you need to first close the current tab by using client.window(callback) because the switch command ""switches to the first available one"". Then you can easily switch tab using client.switchTab.\n', '\nA brief example of how to switch between tabs in a browser (in case with one window):\n// open the first tab\ndriver.get(""https://www.google.com"");\nThread.sleep(2000);\n\n// open the second tab\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + ""t"");\ndriver.get(""https://www.google.com"");\nThread.sleep(2000);\n\n// switch to the previous tab\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + """" + Keys.SHIFT + """" + Keys.TAB);\nThread.sleep(2000);\n\nI write Thread.sleep(2000) just to have a timeout to see switching between the tabs.\nYou can use CTRL+TAB for switching to the next tab and CTRL+SHIFT+TAB for switching to the previous tab.\n', '\nThis will work for the MacOS for Firefox and Chrome:\n// opens the default browser tab with the first webpage\ndriver.get(""the url 1"");\nthread.sleep(2000);\n\n// opens the second tab\ndriver.findElement(By.cssSelector(""Body"")).sendKeys(Keys.COMMAND + ""t"");\ndriver.get(""the url 2"");\nThread.sleep(2000);\n\n// comes back to the first tab\ndriver.findElement(By.cssSelector(""Body"")).sendKeys(Keys.COMMAND, Keys.SHIFT, ""{"");\n\n', '\nTo get parent window handles.   \nString parentHandle = driverObj.getWindowHandle();\npublic String switchTab(String parentHandle){\n    String currentHandle ="""";\n    Set<String> win  = ts.getDriver().getWindowHandles();   \n\n    Iterator<String> it =  win.iterator();\n    if(win.size() > 1){\n        while(it.hasNext()){\n            String handle = it.next();\n            if (!handle.equalsIgnoreCase(parentHandle)){\n                ts.getDriver().switchTo().window(handle);\n                currentHandle = handle;\n            }\n        }\n    }\n    else{\n        System.out.println(""Unable to switch"");\n    }\n    return currentHandle;\n}\n\n', '\nThe flaw with the selected answer is that it unnecessarily assumes order in webDriver.getWindowHandles().  The getWindowHandles() method returns a Set, which does not guarantee order. \nI used the following code to change tabs, which does not assume any ordering. \nString currentTabHandle = driver.getWindowHandle();\nString newTabHandle = driver.getWindowHandles()\n       .stream()\n       .filter(handle -> !handle.equals(currentTabHandle ))\n       .findFirst()\n       .get();\ndriver.switchTo().window(newTabHandle);\n\n', '\nprotected void switchTabsUsingPartOfUrl(String platform) {\n    String currentHandle = null;\n    try {\n        final Set<String> handles = driver.getWindowHandles();\n        if (handles.size() > 1) {\n            currentHandle = driver.getWindowHandle();\n        }\n        if (currentHandle != null) {\n            for (final String handle : handles) {\n                driver.switchTo().window(handle);\n                if (currentUrl().contains(platform) && !currentHandle.equals(handle)) {\n                    break;\n                }\n            }\n        } else {\n            for (final String handle : handles) {\n                driver.switchTo().window(handle);\n                if (currentUrl().contains(platform)) {\n                    break;\n                }\n            }\n        }\n    } catch (Exception e) {\n        System.out.println(""Switching tabs failed"");\n    }\n}\n\nCall this method and pass parameter a substring of url of the tab you want to switch to\n', '\npublic class TabBrowserDemo {\npublic static void main(String[] args) throws InterruptedException {\n    System.out.println(""Main Started"");\n    System.setProperty(""webdriver.gecko.driver"", ""driver//geckodriver.exe"");\n    WebDriver driver = new FirefoxDriver();\n    driver.get(""https://www.irctc.co.in/eticketing/userSignUp.jsf"");\n    driver.manage().timeouts().implicitlyWait(30, TimeUnit.SECONDS);\n\n    driver.findElement(By.xpath(""//a[text()=\'Flights\']"")).click();\n    waitForLoad(driver);\n    Set<String> ids = driver.getWindowHandles();\n    Iterator<String> iterator = ids.iterator();\n    String parentID = iterator.next();\n    System.out.println(""Parent WIn id "" + parentID);\n    String childID = iterator.next();\n    System.out.println(""child win id "" + childID);\n\n    driver.switchTo().window(childID);\n    List<WebElement> hyperlinks = driver.findElements(By.xpath(""//a""));\n\n    System.out.println(""Total links in tabbed browser "" + hyperlinks.size());\n\n    Thread.sleep(3000);\n//  driver.close();\n    driver.switchTo().window(parentID);\n    List<WebElement> hyperlinksOfParent = driver.findElements(By.xpath(""//a""));\n\n    System.out.println(""Total links "" + hyperlinksOfParent.size());\n\n}\n\npublic static void waitForLoad(WebDriver driver) {\n    ExpectedCondition<Boolean> pageLoadCondition = new\n            ExpectedCondition<Boolean>() {\n                public Boolean apply(WebDriver driver) {\n                    return ((JavascriptExecutor)driver).executeScript(""return document.readyState"").equals(""complete"");\n                }\n            };\n    WebDriverWait wait = new WebDriverWait(driver, 30);\n    wait.until(pageLoadCondition);\n}\n\n', '\n    public void switchToNextTab() {\n        ArrayList<String> tab = new ArrayList<>(driver.getWindowHandles());\n        driver.switchTo().window(tab.get(1));\n    }\n    \n    public void closeAndSwitchToNextTab() {\n        driver.close();\n        ArrayList<String> tab = new ArrayList<>(driver.getWindowHandles());\n        driver.switchTo().window(tab.get(1));\n    }\n\n    public void switchToPreviousTab() {\n        ArrayList<String> tab = new ArrayList<>(driver.getWindowHandles());\n        driver.switchTo().window(tab.get(0));\n    }\n\n    public void closeTabAndReturn() {\n        driver.close();\n        ArrayList<String> tab = new ArrayList<>(driver.getWindowHandles());\n        driver.switchTo().window(tab.get(0));\n    }\n\n    public void switchToPreviousTabAndClose() {\n        ArrayList<String> tab = new ArrayList<>(driver.getWindowHandles());\n        driver.switchTo().window(tab.get(1));\n        driver.close();\n    }\n\n', '\nWebDriver driver = new FirefoxDriver();\n\ndriver.switchTo().window(driver.getWindowHandles().toArray()[numPage].toString());\n\nnumPage - int (0,1..)\n\n', '\nString mainWindow = driver.getWindowHandle();\nseleniumHelper.switchToChildWindow();\n..\n..//your assertion steps\nseleniumHelper.switchToWindow(mainWindow);\n', '\nwith Java I used this for switching the selenium focus to the new tab.\n//Before the action that redirect to the new tab:\nString windHandleCurrent = driver.getWindowHandle();\n// code that click in a btn/link in order to open a new tab goes here\n// now to make selenium move to the new tab \nArrayList<String> windows = new ArrayList<String>(driver.getWindowHandles());\n    for(int i =0;i<windows.size();i++ ) {\n        String aWindow = windows.get(i);\n        if(aWindow != windHandleCurrent) {\n            driver.switchTo().window(aWindow);\n        }\n    }\n// now you can code your AssertJUnit for the new tab.\n\n', '\nSelenium 4 has new features:\n// Opens a new tab and switches to new tab\ndriver.switchTo().newWindow(WindowType.TAB);\n// Opens a new window and switches to new window\ndriver.switchTo().newWindow(WindowType.WINDOW);\n', '\ndriver.getWindowHandles() is a Set.I converted it to array of objects  by\nObject[] a=driver.getWindowHandles().toArray;\n\nSay you want to switch to 2nd tab then (after conversion) use\ndriver.switchTo().windows(a[1].toString());\n\n']",https://stackoverflow.com/questions/12729265/switch-tabs-using-selenium-webdriver-with-java,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
headless internet browser? [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



I would like to do the following. Log into a website, click a couple of specific links, then click a download link. I'd like to run this as either a scheduled task on windows or cron job on Linux. I'm not picky about the language I use, but I'd like this to run with out putting a browser window up on the screen if possible.
",70k,"
            71
        ","['\nHere are a list of headless browsers that I know about:\n\nHtmlUnit - Java. Custom browser engine. Limited JavaScript support/DOM emulated. Open source.\nGhost - Python only. WebKit-based. Full JavaScript support. Open source.\nTwill - Python/command line. Custom browser engine. No JavaScript. Open source.\nPhantomJS - Command line/all platforms. WebKit-based. Full JavaScript support. Open source.\nAwesomium - C++/.NET/all platforms. Chromium-based. Full JavaScript support. Commercial/free.\nSimpleBrowser - .NET 4/C#. Custom browser engine. No JavaScript support. Open source.\nZombieJS - Node.js. Custom browser engine. JavaScript support/emulated DOM. Open source. Based on jsdom.\nEnvJS - JavaScript via Java/Rhino. Custom browser engine. JavaScript support/emulated DOM. Open source.\nWatir-webdriver with headless gem - Ruby via WebDriver.  Full JS Support via Browsers (Firefox/Chrome/Safari/IE). \nSpynner - Python only.  PyQT and WebKit. \njsdom - Node.js. Custom browser engine. Supports JS via emulated DOM. Open source.\nTrifleJS - port of PhantomJS using MSIE (Trident) and V8. Open source.\nui4j - Pure Java 8 solution. A wrapper library around the JavaFx WebKit Engine incl. headless modes.\nChromium Embedded Framework - Full up-to-date embedded version of Chromium with off-screen rendering as needed. C/C++, with .NET wrappers (and other languages). As it is Chromium, it has support for everything. BSD licensed.\nSelenium WebDriver - Full support for JavaScript via browsers (Firefox, IE, Chrome, Safari, Opera). Officially supported bindings are C#, Java, JavaScript, Haskell, Perl, Ruby, PHP, Python, Objective-C, and R. Unofficial bindings are available for Qt and Go. Open source.\n\nHeadless browsers that have JavaScript support via an emulated DOM generally have issues with some sites that use more advanced/obscure browser features, or have functionality that has visual dependencies (e.g. via CSS positions and so forth), so whilst the pure JavaScript support in these browsers is generally complete, the actual supported browser functionality should be considered as partial only.\n(Note: Original version of this post only mentioned HtmlUnit, hence the comments. If you know of other headless browser implementations and have edit rights, feel free to edit this post and add them.)\n', ""\nCheck out twill, a very convenient scripting language for precisely what you're looking for. From the examples:\nsetlocal username <your username>\nsetlocal password <your password>\n\ngo http://www.slashdot.org/\nformvalue 1 unickname $username\nformvalue 1 upasswd $password\nsubmit\n\ncode 200     # make sure form submission is correct!\n\nThere's also a Python API if you're looking for more flexibility.\n"", '\nHave a look at PhantomJS, a JavaScript based automation framework available for Windows, Mac OS X, Linux, other *ix systems.\nUsing PhantomJS, you can do things like this:\nconsole.log(\'Loading a web page\');\n\nvar page = new WebPage();\nvar url = ""http://www.phantomjs.org/"";\n\npage.open(url, function (status) {\n    // perform your task once the page is ready ...\n    phantom.exit();\n});\n\nOr evaluate a page\'s title:\nvar page = require(\'webpage\').create();\npage.open(url, function (status) {\n    var title = page.evaluate(function () {\n        return document.title;\n    });\n    console.log(\'Page title is \' + title);\n});\n\nExamples from PhantomJS\' Quickstart page. You can even render a page to a PNG, JPEG or PDF using the render() method.\n', '\nI once did that using the Internet Explorer ActiveX control (WebBrowser, MSHTML). You can instantiate it without making it visible.\nThis can be done with any language which supports COM (Delphi, VB6, VB.net, C#, C++, ...)\nOf course this is a quick-and-dirty solution and might not be appropriate in your situation.\n', '\nPhantomJS is a headless WebKit-based browser that you can script with JavaScript.\n', '\nExcept for the auto-download of the file (as that is a dialog box) a win form with the embedded webcontrol will do this.\nYou could look at Watin and Watin Recorder. They may help with C# code that can login to your website, navigate to a URL and possibly even help automate the file download.\nYMMV though.\n', ""\nIf the links are known (e.g, you don't have to search the page for them), then you can probably use wget. I believe that it will do the state management across multiple fetches.\nIf you are a little more enterprising, then I would delve into the new goodies in Python 3.0. They redid the interface to their HTTP stack and, IMHO, have a very nice interface that is susceptible to this type of scripting.\n"", '\nNode.js with YUI on the server. Check out this video: http://www.yuiblog.com/blog/2010/09/29/video-glass-node/\nThe guy in this video Dav Glass shows an example of how he uses node to fetch a page from Digg. He then attached YUI to the DOM he grabbed and can completely manipulate it.\n', '\nIf you use PHP - try http://mink.behat.org/\n', '\nYou can use Watir with Ruby or Watin with mono.\n', ""\nAlso you can use Live Http Headers (Firefox extension) to record headers which are sent to site (Login -> Links -> Download Link) and then replicate them with php using fsockopen. Only thing which you'll probably need to variate is the cookie's value which you receive from login page. \n"", '\nlibCURL could be used to create something like this.\n', '\nCan you not just use a download manager?\nThere\'s better ones, but FlashGet has browser-integration, and supports authentication. You can login, click a bunch of links and queue them up and schedule the download.\nYou could write something that, say, acts as a proxy which catches specific links and queues them for later download, or a Javascript bookmarklet that modifies links to go to ""http://localhost:1234/download_queuer?url="" + $link.href and have that queue the downloads - but you\'d be reinventing the download-manager-wheel, and with authentication it can be more complicated..\nOr, if you want the ""login, click links"" bit to be automated also - look into screen-scraping.. Basically you load the page via a HTTP library, find the download links and download them..\nSlightly simplified example, using Python:\nimport urllib\nfrom BeautifulSoup import BeautifulSoup\nsrc = urllib.urlopen(""http://%s:%s@example.com"" % (""username"", ""password""))\nsoup = BeautifulSoup(src)\n\nfor link_tag in soup.findAll(""a""):\n    link = link_tag[""href""]\n    filename = link.split(""/"")[-1] # get everything after last /\n    urllib.urlretrieve(link, filename)\n\nThat would download every link on example.com, after authenticating with the username/password of ""username"" and ""password"". You could, of course, find more specific links using BeautifulSoup\'s HTML selector\'s (for example, you could find all links with the class ""download"", or URL\'s that start with http://cdn.example.com).\nYou could do the same in pretty much any language..\n', ""\n.NET contains System.Windows.Forms.WebBrowser.  You can create an instance of this, send it to a URL, and then easily parse the html on that page.  You could then follow any links you found, etc.  \nI have worked with this object only minimally, so I'm no expert, but if you're already familiar with .NET then it would probably be worth looking into.\n""]",https://stackoverflow.com/questions/814757/headless-internet-browser,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typing the Enter/Return key in Selenium,"
I'm looking for a quick way to type the Enter or Return key in Selenium.
Unfortunately, the form I'm trying to test (not my own code, so I can't modify) doesn't have a Submit button. When working with it manually, I just type Enter or Return. How can I do that with the Selenium type command as there is no button to click?
",913k,"
            330
        ","['\nimport org.openqa.selenium.Keys\n\nWebElement.sendKeys(Keys.RETURN);\n\n\nThe import statement is for Java. For other languages, it is maybe different. For example, in Python it is from selenium.webdriver.common.keys import Keys\n', '\nJava\ndriver.findElement(By.id(""Value"")).sendKeys(Keys.RETURN);\n\nOR,\ndriver.findElement(By.id(""Value"")).sendKeys(Keys.ENTER);\n\n\nPython\nfrom selenium.webdriver.common.keys import Keys\ndriver.find_element_by_name(""Value"").send_keys(Keys.RETURN)\n\nOR,\ndriver.find_element_by_name(""Value"").send_keys(Keys.ENTER)\n\nOR,\nelement = driver.find_element_by_id(""Value"")\nelement.send_keys(""keysToSend"")\nelement.submit()\n\n\nRuby\nelement = @driver.find_element(:name, ""value"")\nelement.send_keys ""keysToSend""\nelement.submit\n\nOR,\nelement = @driver.find_element(:name, ""value"")\nelement.send_keys ""keysToSend""\nelement.send_keys:return\n\nOR,\n@driver.action.send_keys(:enter).perform\n@driver.action.send_keys(:return).perform\n\n\nC#\ndriver.FindElement(By.Id(""Value"")).SendKeys(Keys.Return);\n\nOR,\ndriver.FindElement(By.Id(""Value"")).SendKeys(Keys.Enter);\n\n', '\nYou can use either of  Keys.ENTER or Keys.RETURN. Here are the details:\nUsage:\n\nJava:\n\nUsing Keys.ENTER:\nimport org.openqa.selenium.Keys;\ndriver.findElement(By.id(""element_id"")).sendKeys(Keys.ENTER);\n\n\nUsing Keys.RETURN:\nimport org.openqa.selenium.Keys;\ndriver.findElement(By.id(""element_id"")).sendKeys(Keys.RETURN);\n\n\n\n\nPython:\n\nUsing Keys.ENTER:\nfrom selenium.webdriver.common.keys import Keys\ndriver.find_element_by_id(""element_id"").send_keys(Keys.ENTER)\n\n\nUsing Keys.RETURN:\nfrom selenium.webdriver.common.keys import Keys\ndriver.find_element_by_id(""element_id"").send_keys(Keys.RETURN)\n\n\n\n\n\nKeys.ENTER and Keys.RETURN both are from org.openqa.selenium.Keys, which extends java.lang.Enum<Keys> and implements java.lang.CharSequence.\n\nEnum Keys\nEnum Keys is the representations of pressable keys that aren\'t text. These are stored in the Unicode PUA (Private Use Area) code points, 0xE000-0xF8FF.\nKey Codes:\nThe special keys codes for them are as follows:\n\nRETURN = u\'\\ue006\'\nENTER = u\'\\ue007\'\n\nThe implementation of all the Enum Keys are handled the same way.\nHence these is No Functional or Operational difference while working with either sendKeys(Keys.ENTER); or WebElement.sendKeys(Keys.RETURN); through Selenium.\n\nEnter Key and Return Key\nOn computer keyboards, the Enter (or the Return on Mac\xa0OS\xa0X) in most cases causes a command line, window form, or dialog box to operate its default function. This is typically to finish an ""entry"" and begin the desired process and is usually an alternative to pressing an OK button.\nThe Return is often also referred as the Enter and they usually perform identical functions; however in some particular applications (mainly page layout) Return operates specifically like the Carriage Return key from which it originates. In contrast, the Enter is commonly labelled with its name in plain text on generic PC keyboards.\n\nReferences\n\nEnter Key\nCarriage Return\n\n', '\nNow that Selenium 2 has been released, it\'s a bit easier to send an Enter key, since you can do it with the send_keys method of the selenium.webdriver.remote.webelement.WebElement class (this example code is in Python, but the same method exists in Java):\n>>> from selenium import webdriver\n>>> wd = webdriver.Firefox()\n>>> wd.get(""http://localhost/example/page"")\n>>> textbox = wd.find_element_by_css_selector(""input"")\n>>> textbox.send_keys(""Hello World\\n"")\n\n', '\nIn Python\nStep 1. from selenium.webdriver.common.keys import Keys\nStep 2. driver.find_element_by_name("""").send_keys(Keys.ENTER)\nNote: you have to write Keys.ENTER\n', '\nWhen writing HTML tests, the ENTER key is available as ${KEY_ENTER}.\nYou can use it with sendKeys, here is an example:\nsendKeys | id=search | ${KEY_ENTER}\n\n', '\nselenium.keyPress(""css=input.tagit-input.ui-autocomplete-input"", ""13"");\n\n', '\nYou just do this:\nfinal private WebElement input = driver.findElement(By.id(""myId""));\ninput.clear();\ninput.sendKeys(value); // The value we want to set to input\ninput.sendKeys(Keys.RETURN);\n\n', ""\nFor those folks who are using WebDriverJS Keys.RETURN would be referenced as \nwebdriver.Key.RETURN\n\nA more complete example as a reference might be helpful too:\nvar pressEnterToSend = function () {\n    var deferred = webdriver.promise.defer();\n    webdriver.findElement(webdriver.By.id('id-of-input-element')).then(function (element) {\n        element.sendKeys(webdriver.Key.RETURN);\n        deferred.resolve();\n    });\n\n    return deferred.promise;\n};\n\n"", '\ndriver.findElement(By.id(""Value"")).sendKeys(Keys.RETURN); or driver.findElement(By.id(""Value"")).sendKeys(Keys.ENTER);\n', '\nFor Selenium Remote Control with Java:\nselenium.keyPress(""elementID"", ""\\13"");\n\nFor Selenium WebDriver (a.k.a. Selenium 2) with Java:\ndriver.findElement(By.id(""elementID"")).sendKeys(Keys.ENTER);\n\nOr,\ndriver.findElement(By.id(""elementID"")).sendKeys(Keys.RETURN);\n\nAnother way to press Enter in WebDriver is by using the Actions class:\nActions action = new Actions(driver);\naction.sendKeys(driver.findElement(By.id(""elementID"")), Keys.ENTER).build().perform();\n\n', '\nsearch = browser.find_element_by_xpath(""//*[@type=\'text\']"")\nsearch.send_keys(u\'\\ue007\')\n\n#ENTER = u\'\\ue007\'\nRefer to Selenium\'s documentation \'Special Keys\'.\n', '\nI just like to note that I needed this for my Cucumber tests and found out that if you like to simulate pressing the enter/return key, you need to send the :return value and not the :enter value (see the values described here)\n', '\nTry to use an XPath expression for searching the element and then, the following code works:\ndriver.findElement(By.xpath("".//*[@id=\'txtFilterContentUnit\']"")).sendKeys(Keys.ENTER);\n\n', ""\nYou can call submit() on the element object in which you entered your text.\nAlternatively, you can specifically send the Enter key to it as shown in this Python snippet:\nfrom selenium.webdriver.common.keys import Keys\nelement.send_keys(Keys.ENTER) # 'element' is the WebElement object corresponding to the input field on the page\n\n"", '\nIf you are looking for ""how to press the Enter key from the keyboard in Selenium WebDriver (Java)"",then below code will definitely help you.\n// Assign a keyboard object\nKeyboard keyboard = ((HasInputDevices) driver).getKeyboard();\n\n// Enter a key\nkeyboard.pressKey(Keys.ENTER);\n\n', '\nTo enter keys using Selenium, first you need to import the following library:\nimport org.openqa.selenium.Keys\n\nthen add this code where you want to enter the key\nWebElement.sendKeys(Keys.RETURN);\n\nYou can replace RETURN with any key from the list according to your requirement.\n', '\nThere are the following ways of pressing keys - C#:\nDriver.FindElement(By.Id(""Value"")).SendKeys(Keys.Return);\n\nOR\nOpenQA.Selenium.Interactions.Actions action = new OpenQA.Selenium.Interactions.Actions(Driver);\naction.SendKeys(OpenQA.Selenium.Keys.Escape);\n\nOR\nIWebElement body = GlobalDriver.FindElement(By.TagName(""body""));\nbody.SendKeys(Keys.Escape);\n\n', '\nobject.sendKeys(""your message"", Keys.ENTER);\n\nIt works.\n', ""\nWhen you don't want to search any locator, you can use the Robot class. For example,\nRobot robot = new Robot();\nrobot.keyPress(KeyEvent.VK_ENTER);\nrobot.keyRelease(KeyEvent.VK_ENTER);\n\n"", '\nIf you just want to press the Enter key (python):\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.keys import Keys\n\naction = ActionChains(driver)\naction.send_keys(Keys.ENTER)\naction.perform()\n\n', '\nActions action = new Actions(driver);\naction.sendKeys(Keys.RETURN);\n\n', '\nFor Ruby:\ndriver.find_element(:id, ""XYZ"").send_keys:return\n\n', '\nFor Selenium WebDriver using XPath (if the key is visible):\ndriver.findElement(By.xpath(""xpath of text field"")).sendKeys(Keys.ENTER);\n\nor,\ndriver.findElement(By.xpath(""xpath of text field"")).sendKeys(Keys.RETURN);\n\n', '\nI had to send the Enter key in the middle of a text. So I passed the following text to send keys function to achieve 1\\n2\\n3:\n1\\N{U+E007}2\\N{U+E007}3\n\n', '\nJava/JavaScript:\nYou could probably do it this way also, non-natively:\npublic void triggerButtonOnEnterKeyInTextField(String textFieldId, String clickableButId)\n{\n    ((JavascriptExecutor) driver).executeScript(\n        ""   elementId = arguments[0];\n            buttonId = arguments[1];\n            document.getElementById(elementId)\n                .addEventListener(""keyup"", function(event) {\n                    event.preventDefault();\n                    if (event.keyCode == 13) {\n                        document.getElementById(buttonId).click();\n                    }\n                });"",\n\n        textFieldId,\n        clickableButId);\n}\n\n', '\nIt could be achieved using Action interface as well. In case of WebDriver -\nWebElement username = driver.findElement(By.name(""q""));\nusername.sendKeys(searchKey);\nActions action = new Actions(driver);\naction.sendKeys(Keys.RETURN);\naction.perform();\n\n', '\nYou can try:\nselenium.keyPress(""id="""", ""\\\\13"");\n\n', '\nIf you are in this specific situation:\na) want to just press the key, but you not have a specific webElement to click on\nb) you are using Selenium 2 (WebDriver)\nThen the solution is:\n    Actions builder = new Actions(webDriverInstance);\n    builder.sendKeys(Keys.RETURN).perform();\n\n', ""\nFor everyone using JavaScript / Node.js, this worked for me:\ndriver.findElement(By.xpath('xpath')).sendKeys('ENTER');\n\n""]",https://stackoverflow.com/questions/1629053/typing-the-enter-return-key-in-selenium,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.doc to pdf using python,"
I'am tasked with converting tons of .doc files to .pdf. And the only way my supervisor wants me to do this is through MSWord 2010. I know I should be able to automate this with python COM automation. Only problem is I dont know how and where to start. I tried searching for some tutorials but was not able to find any (May be I might have, but I don't know what I'm looking for). 
Right now I'm reading through this. Dont know how useful this is going to be.
",172k,"
            75
        ","[""\nA simple example using comtypes, converting a single file, input and output filenames given as commandline arguments:\nimport sys\nimport os\nimport comtypes.client\n\nwdFormatPDF = 17\n\nin_file = os.path.abspath(sys.argv[1])\nout_file = os.path.abspath(sys.argv[2])\n\nword = comtypes.client.CreateObject('Word.Application')\ndoc = word.Documents.Open(in_file)\ndoc.SaveAs(out_file, FileFormat=wdFormatPDF)\ndoc.Close()\nword.Quit()\n\nYou could also use pywin32, which would be the same except for:\nimport win32com.client\n\nand then:\nword = win32com.client.Dispatch('Word.Application')\n\n"", '\nYou can use the docx2pdf python package to bulk convert docx to pdf. It can be used as both a CLI and a python library. It requires Microsoft Office to be installed and uses COM on Windows and AppleScript (JXA) on macOS.\nfrom docx2pdf import convert\n\nconvert(""input.docx"")\nconvert(""input.docx"", ""output.pdf"")\nconvert(""my_docx_folder/"")\n\npip install docx2pdf\ndocx2pdf input.docx output.pdf\n\nDisclaimer: I wrote the docx2pdf package. https://github.com/AlJohri/docx2pdf\n', ""\nI have tested many solutions but no one of them works efficiently on Linux distribution.\nI recommend this solution :\nimport sys\nimport subprocess\nimport re\n\n\ndef convert_to(folder, source, timeout=None):\n    args = [libreoffice_exec(), '--headless', '--convert-to', 'pdf', '--outdir', folder, source]\n\n    process = subprocess.run(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n    filename = re.search('-> (.*?) using filter', process.stdout.decode())\n\n    return filename.group(1)\n\n\ndef libreoffice_exec():\n    # TODO: Provide support for more platforms\n    if sys.platform == 'darwin':\n        return '/Applications/LibreOffice.app/Contents/MacOS/soffice'\n    return 'libreoffice'\n\nand you call your function:\nresult = convert_to('TEMP Directory',  'Your File', timeout=15)\n\nAll resources:\n\nhttps://michalzalecki.com/converting-docx-to-pdf-using-python/\n\n"", '\nI have worked on this problem for half a day, so I think I should share some of my experience on this matter. Steven\'s answer is right, but it will fail on my computer. There are two key points to fix it here:\n(1). The first time when I created the \'Word.Application\' object, I should make it (the word app) visible before open any documents. (Actually, even I myself cannot explain why this works. If I do not do this on my computer, the program will crash when I try to open a document in the invisible model, then the \'Word.Application\' object will be deleted by OS. )\n(2). After doing (1), the program will work well sometimes but may fail often. The crash error ""COMError: (-2147418111, \'Call was rejected by callee.\', (None, None, None, 0, None))"" means that the COM Server may not be able to response so quickly. So I add a delay before I tried to open a document.\nAfter doing these two steps, the program will work perfectly with no failure anymore. The demo code is as below. If you have encountered the same problems, try to follow these two steps. Hope it helps.\n    import os\n    import comtypes.client\n    import time\n\n\n    wdFormatPDF = 17\n\n\n    # absolute path is needed\n    # be careful about the slash \'\\\', use \'\\\\\' or \'/\' or raw string r""...""\n    in_file=r\'absolute path of input docx file 1\'\n    out_file=r\'absolute path of output pdf file 1\'\n\n    in_file2=r\'absolute path of input docx file 2\'\n    out_file2=r\'absolute path of outputpdf file 2\'\n\n    # print out filenames\n    print in_file\n    print out_file\n    print in_file2\n    print out_file2\n\n\n    # create COM object\n    word = comtypes.client.CreateObject(\'Word.Application\')\n    # key point 1: make word visible before open a new document\n    word.Visible = True\n    # key point 2: wait for the COM Server to prepare well.\n    time.sleep(3)\n\n    # convert docx file 1 to pdf file 1\n    doc=word.Documents.Open(in_file) # open docx file 1\n    doc.SaveAs(out_file, FileFormat=wdFormatPDF) # conversion\n    doc.Close() # close docx file 1\n    word.Visible = False\n    # convert docx file 2 to pdf file 2\n    doc = word.Documents.Open(in_file2) # open docx file 2\n    doc.SaveAs(out_file2, FileFormat=wdFormatPDF) # conversion\n    doc.Close() # close docx file 2   \n    word.Quit() # close Word Application \n\n', '\nunoconv (writen in Python) and OpenOffice running as a headless daemon.\nhttps://github.com/unoconv/unoconv\nhttp://dag.wiee.rs/home-made/unoconv/\nWorks very nicely for doc, docx, ppt, pptx, xls, xlsx.\nVery useful if you need to convert docs or save/convert to certain formats on a server.\n', ""\nAs an alternative to the SaveAs function, you could also use ExportAsFixedFormat which gives you access to the PDF options dialog you would normally see in Word. With this you can specify bookmarks and other document properties.\ndoc.ExportAsFixedFormat(OutputFileName=pdf_file,\n    ExportFormat=17, #17 = PDF output, 18=XPS output\n    OpenAfterExport=False,\n    OptimizeFor=0,  #0=Print (higher res), 1=Screen (lower res)\n    CreateBookmarks=1, #0=No bookmarks, 1=Heading bookmarks only, 2=bookmarks match word bookmarks\n    DocStructureTags=True\n    );\n\nThe full list of function arguments is: 'OutputFileName', 'ExportFormat', 'OpenAfterExport', 'OptimizeFor', 'Range', 'From', 'To', 'Item', 'IncludeDocProps', 'KeepIRM', 'CreateBookmarks', 'DocStructureTags', 'BitmapMissingFonts', 'UseISO19005_1', 'FixedFormatExtClassPtr'\n"", ""\nIt's worth noting that Stevens answer works, but make sure if using a for loop to export multiple files to place the ClientObject or Dispatch statements before the loop - it only needs to be created once - see my problem: Python win32com.client.Dispatch looping through Word documents and export to PDF; fails when next loop occurs\n"", ""\nIf you don't mind using PowerShell have a look at this Hey, Scripting Guy! article. The code presented could be adopted to use the wdFormatPDF enumeration value of WdSaveFormat (see here).\nThis blog article presents a different implementation of the same idea.\n"", '\nI have modified it for ppt support as well. My solution support all the below-specified extensions.\nword_extensions = ["".doc"", "".odt"", "".rtf"", "".docx"", "".dotm"", "".docm""]\nppt_extensions = ["".ppt"", "".pptx""]\n\nMy Solution: Github Link\nI have modified code from Docx2PDF\n', '\nI tried the accepted answer but wasn\'t particularly keen on the bloated PDFs Word was producing which was usually an order of magnitude bigger than expected. After looking how to disable the dialogs when using a virtual PDF printer I came across Bullzip PDF Printer and I\'ve been rather impressed with its features. It\'s now replaced the other virtual printers I used previously. You\'ll find a ""free community edition"" on their download page.\nThe COM API can be found here and a list of the usable settings can be found here. The settings are written to a ""runonce"" file which is used for one print job only and then removed automatically. When printing multiple PDFs we need to make sure one print job completes before starting another to ensure the settings are used correctly for each file.\nimport os, re, time, datetime, win32com.client\n\ndef print_to_Bullzip(file):\n    util = win32com.client.Dispatch(""Bullzip.PDFUtil"")\n    settings = win32com.client.Dispatch(""Bullzip.PDFSettings"")\n    settings.PrinterName = util.DefaultPrinterName      # make sure we\'re controlling the right PDF printer\n\n    outputFile = re.sub(""\\.[^.]+$"", "".pdf"", file)\n    statusFile = re.sub(""\\.[^.]+$"", "".status"", file)\n\n    settings.SetValue(""Output"", outputFile)\n    settings.SetValue(""ConfirmOverwrite"", ""no"")\n    settings.SetValue(""ShowSaveAS"", ""never"")\n    settings.SetValue(""ShowSettings"", ""never"")\n    settings.SetValue(""ShowPDF"", ""no"")\n    settings.SetValue(""ShowProgress"", ""no"")\n    settings.SetValue(""ShowProgressFinished"", ""no"")     # disable balloon tip\n    settings.SetValue(""StatusFile"", statusFile)         # created after print job\n    settings.WriteSettings(True)                        # write settings to the runonce.ini\n    util.PrintFile(file, util.DefaultPrinterName)       # send to Bullzip virtual printer\n\n    # wait until print job completes before continuing\n    # otherwise settings for the next job may not be used\n    timestamp = datetime.datetime.now()\n    while( (datetime.datetime.now() - timestamp).seconds < 10):\n        if os.path.exists(statusFile) and os.path.isfile(statusFile):\n            error = util.ReadIniString(statusFile, ""Status"", ""Errors"", \'\')\n            if error != ""0"":\n                raise IOError(""PDF was created with errors"")\n            os.remove(statusFile)\n            return\n        time.sleep(0.1)\n    raise IOError(""PDF creation timed out"")\n\n', '\nI was working with this solution but I needed to search all .docx, .dotm, .docm, .odt, .doc or .rtf and then turn them all to .pdf (python 3.7.5). Hope it works...\nimport os\nimport win32com.client\n\nwdFormatPDF = 17\n\nfor root, dirs, files in os.walk(r\'your directory here\'):\n    for f in files:\n\n        if  f.endswith("".doc"")  or f.endswith("".odt"") or f.endswith("".rtf""):\n            try:\n                print(f)\n                in_file=os.path.join(root,f)\n                word = win32com.client.Dispatch(\'Word.Application\')\n                word.Visible = False\n                doc = word.Documents.Open(in_file)\n                doc.SaveAs(os.path.join(root,f[:-4]), FileFormat=wdFormatPDF)\n                doc.Close()\n                word.Quit()\n                word.Visible = True\n                print (\'done\')\n                os.remove(os.path.join(root,f))\n                pass\n            except:\n                print(\'could not open\')\n                # os.remove(os.path.join(root,f))\n        elif f.endswith("".docx"") or f.endswith("".dotm"") or f.endswith("".docm""):\n            try:\n                print(f)\n                in_file=os.path.join(root,f)\n                word = win32com.client.Dispatch(\'Word.Application\')\n                word.Visible = False\n                doc = word.Documents.Open(in_file)\n                doc.SaveAs(os.path.join(root,f[:-5]), FileFormat=wdFormatPDF)\n                doc.Close()\n                word.Quit()\n                word.Visible = True\n                print (\'done\')\n                os.remove(os.path.join(root,f))\n                pass\n            except:\n                print(\'could not open\')\n                # os.remove(os.path.join(root,f))\n        else:\n            pass\n\nThe try and except was for those documents I couldn\'t read and won\'t exit the code until the last document.\n', '\nYou should start from investigating so called virtual PDF print drivers.\nAs soon as you will find one you should be able to write batch file that prints your DOC files into PDF files. You probably can do this in Python too (setup printer driver output and issue document/print command in MSWord, later can be done using command line AFAIR).\n', '\nimport docx2txt\nfrom win32com import client\n\nimport os\n\nfiles_from_folder = r""c:\\\\doc""\n\ndirectory = os.fsencode(files_from_folder)\n\namount = 1\n\nword = client.DispatchEx(""Word.Application"")\nword.Visible = True\n\nfor file in os.listdir(directory):\n    filename = os.fsdecode(file)\n    print(filename)\n\n    if filename.endswith(\'docx\'):\n        text = docx2txt.process(os.path.join(files_from_folder, filename))\n\n        print(f\'{filename} transfered ({amount})\')\n        amount += 1\n        new_filename = filename.split(\'.\')[0] + \'.txt\'\n\n        try:\n            with open(os.path.join(files_from_folder + r\'\\txt_files\', new_filename), \'w\', encoding=\'utf-8\') as t:\n                t.write(text)\n        except:\n            os.mkdir(files_from_folder + r\'\\txt_files\')\n            with open(os.path.join(files_from_folder + r\'\\txt_files\', new_filename), \'w\', encoding=\'utf-8\') as t:\n                t.write(text)\n    elif filename.endswith(\'doc\'):\n        doc = word.Documents.Open(os.path.join(files_from_folder, filename))\n        text = doc.Range().Text\n        doc.Close()\n\n        print(f\'{filename} transfered ({amount})\')\n        amount += 1\n        new_filename = filename.split(\'.\')[0] + \'.txt\'\n\n        try:\n            with open(os.path.join(files_from_folder + r\'\\txt_files\', new_filename), \'w\', encoding=\'utf-8\') as t:\n                t.write(text)\n        except:\n            os.mkdir(files_from_folder + r\'\\txt_files\')\n            with open(os.path.join(files_from_folder + r\'\\txt_files\', new_filename), \'w\', encoding=\'utf-8\') as t:\n                t.write(text)\nword.Quit()\n\nThe Source Code, see here:\nhttps://neculaifantanaru.com/en/python-full-code-how-to-convert-doc-and-docx-files-to-pdf-from-the-folder.html\n', ""\nI would suggest ignoring your supervisor and use OpenOffice which has a Python api.  OpenOffice has built in support for Python and someone created a library specific for this purpose (PyODConverter).\nIf he isn't happy with the output, tell him it could take you weeks to do it with word.\n""]",https://stackoverflow.com/questions/6011115/doc-to-pdf-using-python,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Best practice - Git + Build automation - Keeping configs separate,"
Searching for the best approach to keep my config files separate, yet not introduce extra steps for new developers setting up their environments.
I am guessing a submodule would suffice to do the job, but then how would I switch configs seamlessly depending on the task at hand, aka pull in DEV config regularly, pull PROD branch of config repo during build?
Needs to be:

Easy and painless for new devs.  
PROD config files should only be accessible to select users + build user.

Thank you in advance.
",2k,"
            6
        ","['\nThat is called content filter driver, and it allows you to declare, in a .gitattributes file (and only for your config files type) a smudge script which will automatically on checkout:\n\ncombine a config file template file (config.tpl)\nwith the right config file value (config.dev, config.prod, ...)\nin order to produced a non-versioned config file (private file)\n\n\nSee ""Customizing Git - Git Attributes"":\necho \'*.cfg.tpl filter=config\' >> .gitattributes\ngit config --global filter.config.smudge yourScript\n\nWith that approach, you don\'t need submodules, but you can generate as many config file you need depending on your environment, like for instance your branch:\nA bit like in ""Find Git branch name in post-update hook"", your smudge script can find out in which branch it is currently executing with:\n#!/bin/sh\nbranch=$(git rev-parse --symbolic --abbrev-ref HEAD)\n\n']",https://stackoverflow.com/questions/20822073/best-practice-git-build-automation-keeping-configs-separate,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stored procedure that Automatically delete rows older than 7 days in MYSQL,"
I would like to know if is possible to create a stored procedure that automatically, every day at 00:00, deletes every row of every table that is over 7 days.
I have seen few solutions but not sure if its what I am looking for, and would be nice if someone has any good example. I know this could be done with simple scripts in python and php, but I would like something more automated by MySQL.
Any help would be really appreciate.
Thanks!
",17k,"
            2
        ","[""\nMysql has its EVENT functionality for avoiding complicated cron interactions when much of what you are scheduling is sql related, and less file related. See the Manual page here. Hopefully the below reads as a quick overview of the important steps and things to consider, and verifiable testing too.\nshow variables where variable_name='event_scheduler';\n+-----------------+-------+\n| Variable_name   | Value |\n+-----------------+-------+\n| event_scheduler | OFF   |\n+-----------------+-------+\n\nooops, the event scheduler is not turned on. Nothing will trigger.\nSET GLOBAL event_scheduler = ON;  -- turn her on and confirm below\nshow variables where variable_name='event_scheduler';\n+-----------------+-------+\n| Variable_name   | Value |\n+-----------------+-------+\n| event_scheduler | ON    |\n+-----------------+-------+\n\nSchema for testing\ncreate table theMessages\n(   id int auto_increment primary key,\n    userId int not null,\n    message varchar(255) not null,\n    updateDt datetime not null,\n    key(updateDt)\n    -- FK's not shown\n);\n-- it is currently 2015-09-10 13:12:00\n-- truncate table theMessages;\ninsert theMessages(userId,message,updateDt) values (1,'I need to go now, no followup questions','2015-08-24 11:10:09');\ninsert theMessages(userId,message,updateDt) values (7,'You always say that ... just hiding','2015-08-29');\ninsert theMessages(userId,message,updateDt) values (1,'7 day test1','2015-09-03 12:00:00');\ninsert theMessages(userId,message,updateDt) values (1,'7 day test2','2015-09-03 14:00:00');\n\nCreate 2 events, 1st runs daily, 2nd runs every 10 minutes\nIgnore what they are actually doing (playing against one another). The point is on time difference approaches and scheduling.\nDELIMITER $$\nCREATE EVENT `delete7DayOldMessages`\n  ON SCHEDULE EVERY 1 DAY STARTS '2015-09-01 00:00:00'\n  ON COMPLETION PRESERVE\nDO BEGIN\n   delete from theMessages \n   where datediff(now(),updateDt)>6; -- not terribly exact, yesterday but <24hrs is still 1 day\n   -- etc etc all your stuff in here\nEND;$$\nDELIMITER ;\n\n...\nDELIMITER $$\nCREATE EVENT `Every_10_Minutes_Cleanup`\n  ON SCHEDULE EVERY 10 MINUTE STARTS '2015-09-01 00:00:00'\n  ON COMPLETION PRESERVE\nDO BEGIN\n   delete from theMessages \n   where TIMESTAMPDIFF(HOUR, updateDt, now())>168; -- messages over 1 week old (168 hours)\n   -- etc etc all your stuff in here\nEND;$$\nDELIMITER ;\n\nShow event statuses (different approaches)\nshow events from so_gibberish; -- list all events by schema name (db name)\nshow events; -- <--------- from workbench / sqlyog\nshow events\\G;` -- <--------- I like this one from mysql> prompt\n\n*************************** 1. row ***************************\n                  Db: so_gibberish\n                Name: delete7DayOldMessages\n             Definer: root@localhost\n           Time zone: SYSTEM\n                Type: RECURRING\n          Execute at: NULL\n      Interval value: 1\n      Interval field: DAY\n              Starts: 2015-09-01 00:00:00\n                Ends: NULL\n              Status: ENABLED\n          Originator: 1\ncharacter_set_client: utf8\ncollation_connection: utf8_general_ci\n  Database Collation: utf8_general_ci\n*************************** 2. row ***************************\n                  Db: so_gibberish\n                Name: Every_10_Minutes_Cleanup\n             Definer: root@localhost\n           Time zone: SYSTEM\n                Type: RECURRING\n          Execute at: NULL\n      Interval value: 10\n      Interval field: MINUTE\n              Starts: 2015-09-01 00:00:00\n                Ends: NULL\n              Status: ENABLED\n          Originator: 1\ncharacter_set_client: utf8\ncollation_connection: utf8_general_ci\n  Database Collation: utf8_general_ci\n2 rows in set (0.06 sec)\n\nRandom stuff to consider\ndrop event someEventName; -- <----- a good thing to know about\ncan't alias datediff and use in where clause in 1 line, so\nselect id,DATEDIFF(now(),updateDt) from theMessages where datediff(now(),updateDt)>6;\n\nget more exact, 168 hours for 1 week old\nselect id,TIMESTAMPDIFF(HOUR, updateDt, now()) as `difference` FROM theMessages;\n+----+------------+\n| id | difference |\n+----+------------+\n|  1 |        410 |\n|  2 |        301 |\n|  3 |        169 |\n|  4 |        167 |\n+----+------------+\n\nThe link to the Manual Page shows quite a bit of flexibilty with interval choices, shown below:\n\ninterval:\nquantity {YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE |\n          WEEK | SECOND | YEAR_MONTH | DAY_HOUR | DAY_MINUTE |\n          DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND}\n\n\nConcurrency\nEmbed any concurrency measures necessary that multiple events (or multiple firings of the same event) don't cause data to run amok.\nSet and Forget\nRemember, for now, because you are going to forget it, that these events just keep firing. So build in solid code that will just keep running, even when you forget. Which you most likely will.\nYour particular requirements\nYou need to determine which rows need to be deleted first by table, such that it honors Primary Key constraints. Just lump them all in proper order inside of the obvious area via the CREATE EVENT statement, which can be massive.\n"", '\nYou can use below stored procedure and either schedule it by crontab or through events.\nNote: Just change mydb with your Database, which database tables data you want to delete and test first in testing environment.\nDELIMITER $$\n\nUSE `mydb`$$\n\nDROP PROCEDURE IF EXISTS `sp_delete`$$\n\nCREATE DEFINER=`root`@`localhost` PROCEDURE `sp_delete`()\nBEGIN\nDECLARE done INT(1) DEFAULT 0;\nDECLARE _tbl VARCHAR(100) DEFAULT \'\';\nDECLARE CONTINUE HANDLER FOR SQLSTATE \'02000\' SET done=done;\n\nDECLARE cur1 CURSOR FOR SELECT table_name FROM information_schema.tables WHERE table_schema=\'mydb\' AND table_type=\'base table\';\nDECLARE CONTINUE HANDLER FOR SQLSTATE \'02000\' SET done=done;\nOPEN cur1;\n\nREPEAT\nFETCH cur1 INTO _tbl;\nIF _db = \'\' THEN\n    SET done = 1;\nEND IF;\n            IF (done<>1) THEN\n\n                             SET @str=CONCAT(""delete from "",_tbl,"" where updateon < SUBDATE(CURDATE(),INTERVAL 7 DAY)"");\n                 PREPARE stmt FROM @str;\n                 EXECUTE stmt;\n                 DEALLOCATE PREPARE stmt;\n\n            END IF;\n\nUNTIL done\nEND REPEAT;\n     CLOSE cur1;\n     SELECT \'done\';\n\n\nEND$$\n\nDELIMITER ;\n\n']",https://stackoverflow.com/questions/32507258/stored-procedure-that-automatically-delete-rows-older-than-7-days-in-mysql,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selenium waitForElement,"
How do I write the function for Selenium to wait for a table with just a class identifier in Python? I'm having a devil of a time learning to use Selenium's Python webdriver functions.
",101k,"
            48
        ","[""\nFrom the Selenium Documentation PDF :\nimport contextlib\nimport selenium.webdriver as webdriver\nimport selenium.webdriver.support.ui as ui\n\nwith contextlib.closing(webdriver.Firefox()) as driver:\n    driver.get('http://www.google.com')\n    wait = ui.WebDriverWait(driver,10)\n    # Do not call `implicitly_wait` if using `WebDriverWait`.\n    #     It magnifies the timeout.\n    # driver.implicitly_wait(10)  \n    inputElement=driver.find_element_by_name('q')\n    inputElement.send_keys('Cheese!')\n    inputElement.submit()\n    print(driver.title)\n\n    wait.until(lambda driver: driver.title.lower().startswith('cheese!'))\n    print(driver.title)\n\n    # This raises\n    #     selenium.common.exceptions.TimeoutException: Message: None\n    #     after 10 seconds\n    wait.until(lambda driver: driver.find_element_by_id('someId'))\n    print(driver.title)\n\n"", ""\nSelenium 2's Python bindings have a new support class called expected_conditions.py for doing all sorts of things like testing if an element is visible. It's available here.\nNOTE: the above file is in the trunk as of Oct 12, 2012, but not yet in the latest download which is still 2.25. For the time being until a new Selenium version is released, you can just save this file locally for now and include it in your imports like I've done below.\nTo make life a little simpler, you can combine some of these expected condition methods with the Selenium wait until logic to make some very handy functions similar to what was available in Selenium 1. For example, I put this into my base class called SeleniumTest which all of my Selenium test classes extend:\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nimport selenium.webdriver.support.expected_conditions as EC\nimport selenium.webdriver.support.ui as ui\n\n@classmethod\ndef setUpClass(cls):\n    cls.selenium = WebDriver()\n    super(SeleniumTest, cls).setUpClass()\n\n@classmethod\ndef tearDownClass(cls):\n    cls.selenium.quit()\n    super(SeleniumTest, cls).tearDownClass()\n\n# return True if element is visible within 2 seconds, otherwise False\ndef is_visible(self, locator, timeout=2):\n    try:\n        ui.WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.CSS_SELECTOR, locator)))\n        return True\n    except TimeoutException:\n        return False\n\n# return True if element is not visible within 2 seconds, otherwise False\ndef is_not_visible(self, locator, timeout=2):\n    try:\n        ui.WebDriverWait(driver, timeout).until_not(EC.visibility_of_element_located((By.CSS_SELECTOR, locator)))\n        return True\n    except TimeoutException:\n        return False\n\nYou can then use these easily in your tests like so:\ndef test_search_no_city_entered_then_city_selected(self):\n    sel = self.selenium\n    sel.get('%s%s' % (self.live_server_url, '/'))\n    self.is_not_visible('#search-error')\n\n"", '\nI have made good experiences using:\n\ntime.sleep(seconds)\nwebdriver.Firefox.implicitly_wait(seconds)\n\nThe first one is pretty obvious - just wait a few seconds for some stuff.\nFor all my Selenium Scripts the sleep() with a few seconds (range from 1 to 3) works when I run them on my laptop, but on my Server the time to wait has a wider range, so I use implicitly_wait() too. I usually use implicitly_wait(30), which is really enough.\n\nAn implicit wait is to tell WebDriver to poll the DOM for a certain amount of time when trying to find an element or elements if they are not immediately available. The default setting is 0. Once set, the implicit wait is set for the life of the WebDriver object instance.\n\n', '\nI implemented the following for python for wait_for_condition since the python selenium driver does not support this function.\ndef wait_for_condition(c):\nfor x in range(1,10):\n    print ""Waiting for ajax: "" + c\n    x = browser.execute_script(""return "" + c)\n    if(x):\n        return\n    time.sleep(1)\n\nto be used as\nWait that an ExtJS Ajax call is not pending:\nwait_for_condition(""!Ext.Ajax.isLoading()"")\n\nA Javascript variable is set\nwait_for_condition(""CG.discovery != undefined;"")\n\netc.\n', '\nYou could always use a short sleep in a loop and pass it your element id:\ndef wait_for_element(element):\n     count = 1\n     if(self.is_element_present(element)):\n          if(self.is_visible(element)):\n              return\n          else:\n              time.sleep(.1)\n              count = count + 1\n     else:\n         time.sleep(.1)\n         count = count + 1\n         if(count > 300):\n             print(""Element %s not found"" % element)\n             self.stop\n             #prevents infinite loop\n\n', '\nUse Wait Until Page Contains Element with the proper XPath locator. For example, given the following HTML:\n<body>\n  <div id=""myDiv"">\n    <table class=""myTable"">\n      <!-- implementation -->\n    </table>\n  </div>\n</body>\n\n... you can enter the following keyword:\nWait Until Page Contains Element  //table[@class=\'myTable\']  5 seconds\n\nUnless I missed something, there is no need to create a new function for this.\n', '\nIn case this helps ...\nIn the Selenium IDE, I added ...\n  Command: waitForElementPresent\n  Target: //table[@class=\'pln\']\nThen I did File>Export TestCase As Python2(Web Driver), and it gave me this ...\ndef test_sel(self):\n    driver = self.driver\n    for i in range(60):\n        try:\n            if self.is_element_present(By.XPATH, ""//table[@class=\'pln\']""): break\n        except: pass\n        time.sleep(1)\n    else: self.fail(""time out"")\n\n', ""\neasier solution: \n    from selenium.webdriver.common.by import By    \n    import time\n\n    while len(driver.find_elements(By.ID, 'cs-paginate-next'))==0:\n        time.sleep(100)\n\n"", ""\nHopefully this helps\nfrom selenium import webdriver\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.wait import WebDriverWait\nfrom selenium.webdriver.common.by import By   \n\n\ndriver = webdriver.Firefox()\ndriver.get('www.url.com')\n\ntry:\n    wait = driver.WebDriverWait(driver,10).until(EC.presence_of_element_located(By.CLASS_NAME,'x'))\nexcept:\n    pass\n\n"", ""\nIf I don't know something about selenium command, I use selenium web idea RC with firefox. You can choose and add command in the combobox and when finish your test case after you can export the test code different language. like java, ruby, phyton, C#, etc..\n"", '\nYou can modify this function to all type of elements. The one below is just for the class element:\nWhere ""driver"" is the driver, ""element_name"" is the class name you are looking for, and ""sec"" is the maximum amount of seconds you are willing to wait.\ndef wait_for_class_element(driver,element_name,sec):\n\n    for i in range(sec):        \n        try:\n            driver.find_element_by_class_name(element_name)\n            break\n        except:        \n            print(""not yet"")\n            time.sleep(1)\n\n', ""\nI found an easier way to build this using a custom function, which is recursive in nature\nfrom selenium import webdriver\nimport time\n\ndef wait_element_by_id(id_value):\n    try:\n        elem = driver.find_element_by_id(id_value)\n    except:\n        time.sleep(2)\n        print 'Waiting for id '+id_value\n        wait_element_by_id(id_value)\n\nYou can replace find_element_by_id with find_element_by_name or find_element_by_tag_name based on your requirement\n"", '\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\n\n\n# wait until present\nWebDriverWait(driver, waittime).until(\n    EC.presence_of_element_located((By.CSS_SELECTOR, css_selector))\n)\n\n# wait until visible\nWebDriverWait(driver, waittime).until(\n    EC.visibility_of_element_located((By.CSS_SELECTOR, css_selector))\n)\n\n', ""\nI hope that's might help:\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as ec\n\ndriver = webdriver.Chrome()\ndriver.get(myURL)\nwait = WebDriverWait(driver, 10) \n\nwait.until(ec.presence_of_element_located((By.XPATH, myXPATH)))\n\nI recommend you to read this article to make it more clear.\n""]",https://stackoverflow.com/questions/7781792/selenium-waitforelement,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to Automatically Start a Download in PHP?,"
What code do you need to add in PHP to automatically have the browser download a file to the local machine when a link is visited?
I am specifically thinking of functionality similar to that of download sites that prompt the user to save a file to disk once you click on the name of the software?
",114k,"
            55
        ","['\nSend the following headers before outputting the file:\nheader(""Content-Disposition: attachment; filename=\\"""" . basename($File) . ""\\"""");\nheader(""Content-Type: application/octet-stream"");\nheader(""Content-Length: "" . filesize($File));\nheader(""Connection: close"");\n\n@grom: Interesting about the \'application/octet-stream\' MIME type. I wasn\'t aware of that, have always just used \'application/force-download\' :)\n', '\nHere is an example of sending back a pdf.\nheader(\'Content-type: application/pdf\');\nheader(\'Content-Disposition: attachment; filename=""\' . basename($filename) . \'""\');\nheader(\'Content-Transfer-Encoding: binary\');\nreadfile($filename);\n\n@Swish I didn\'t find application/force-download content type to do anything different (tested in IE and Firefox). Is there a reason for not sending back the actual MIME type?\nAlso in the PHP manual Hayley Watson posted:\n\nIf you wish to force a file to be downloaded and saved, instead of being rendered, remember that there is no such MIME type as ""application/force-download"". The correct type to use in this situation is ""application/octet-stream"", and using anything else is merely relying on the fact that clients are supposed to ignore unrecognised MIME types and use ""application/octet-stream"" instead (reference: Sections 4.1.4 and 4.5.1 of RFC 2046).\n\nAlso according IANA there is no registered application/force-download type.\n', '\nA clean example.\n<?php\n    header(\'Content-Type: application/download\');\n    header(\'Content-Disposition: attachment; filename=""example.txt""\');\n    header(""Content-Length: "" . filesize(""example.txt""));\n\n    $fp = fopen(""example.txt"", ""r"");\n    fpassthru($fp);\n    fclose($fp);\n?>\n\n', '\nNone of above worked for me!\nWorking on 2021 for WordPress and PHP:\n<?php\n$file = ABSPATH . \'pdf.pdf\'; // Where ABSPATH is the absolute server path, not url\n//echo $file; //Be sure you are echoing the absolute path and file name\n$filename = \'Custom file name for the.pdf\'; /* Note: Always use .pdf at the end. */\n\nheader(\'Content-type: application/pdf\');\nheader(\'Content-Disposition: inline; filename=""\' . $filename . \'""\');\nheader(\'Content-Transfer-Encoding: binary\');\nheader(\'Content-Length: \' . filesize($file));\nheader(\'Accept-Ranges: bytes\');\n@readfile($file);\n\nThanks to: https://qastack.mx/programming/4679756/show-a-pdf-files-in-users-browser-via-php-perl\n', '\nmy code works for txt,doc,docx,pdf,ppt,pptx,jpg,png,zip extensions and I think its better to use the actual MIME types explicitly.\n$file_name = ""a.txt"";\n\n// extracting the extension:\n$ext = substr($file_name, strpos($file_name,\'.\')+1);\n\nheader(\'Content-disposition: attachment; filename=\'.$file_name);\n\nif(strtolower($ext) == ""txt"")\n{\n    header(\'Content-type: text/plain\'); // works for txt only\n}\nelse\n{\n    header(\'Content-type: application/\'.$ext); // works for all extensions except txt\n}\nreadfile($decrypted_file_path);\n\n']",https://stackoverflow.com/questions/40943/how-to-automatically-start-a-download-in-php,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Headless browser for C# (.NET)? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



I am (was) a Python developer who is building a GUI web scraping application. Recently I've decided to migrate to .NET framework and write the same application in C# (this decision wasn't mine).
In Python, I've used the Mechanize library. However, I can't seem to find anything similar in .NET. What I need is a browser that will run in a headless mode, which has the ability to fill out forms, submit them, etc. JavaScript parser is not a must, but it would be quite useful. 
",57k,"
            40
        ","['\nThere are some options:\n\nWebKit.Net (free)\n\nAwesomium\nIt is based on Chrome/WebKit and works like a charm.\nThere is a free license available but also a commercial one and if need be you can buy the source code :-)\n\nHTML Agility Pack (free) (An HTML Parser library, NOT a headless browser)\nThis helps with extracting information from HTML etc. and might be useful in your case (possibly in combination with HttpWebRequest)\n\n\n', ""\nMore solutions:\n\nPhantomJS - full featured headless web\nbrowser. Often used in pair with Selenium which allows you to\naccess the browser from .NET application.\nOptimus (nuget package)- lightweight headless web browser. It's in beta but it is sufficient for some cases.\n\nI used to use both for web testing. But they are also suitable for web scraping.\n"", ""\nYou may be after TrifleJS (currently in beta), or something similar using the .NET WebBrowser class which communicates with IE via a windowless ActiveX/COM API.\nYou'll essentially be running a fully fledged browser (not a http request wrapper) using Internet Explorer's Trident engine, if you are not interested in the JavaScript API (a port of phantomjs) you may still be able to use some of the C# codebase to get around key concepts (custom headers, cookies, script execution, screenshot rendering etc). \nNote that this can also emulate different versions of IE depending on what you have installed.\n\n""]",https://stackoverflow.com/questions/10161413/headless-browser-for-c-sharp-net,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Karate UI drag and drop [duplicate],"






This question already has an answer here:
                        
                    



KarateDSL UI Testing - Friendly Locators not working

                                (1 answer)
                            

Closed 2 years ago.



I am studying KarateUI possibilities. And I tried to use drag and drop functionality of framework.
I used a page with draggable elements https://www.seleniumeasy.com/test/drag-and-drop-demo.html and my script does not work on it. What is wrong with my script? Here it is:
mouse().move('{div/span}Draggable 1').down().move('#mydropzone').up()

And i also see in console of IDE next log
16:11:40.196 [ForkJoinPool-1-worker-1] DEBUG c.intuit.karate.driver.DriverOptions - >> {""method"":""Input.dispatchMouseEvent"",""params"":{""type"":""mouseMoved"",""x"":31,""y"":820},""id"":16}
16:11:40.200 [nioEventLoopGroup-2-1] DEBUG c.intuit.karate.driver.DriverOptions - << {""id"":16,""result"":{}}
16:11:40.203 [ForkJoinPool-1-worker-1] DEBUG c.intuit.karate.driver.DriverOptions - >> {""method"":""Input.dispatchMouseEvent"",""params"":{""type"":""mousePressed"",""x"":31,""y"":820,""button"":""left"",""clickCount"":1},""id"":17}
16:11:40.234 [nioEventLoopGroup-2-1] DEBUG c.intuit.karate.driver.DriverOptions - << {""id"":17,""result"":{}}
16:11:40.234 [ForkJoinPool-1-worker-1] DEBUG c.intuit.karate.driver.DriverOptions - >> {""method"":""Input.dispatchMouseEvent"",""params"":{""type"":""mouseMoved"",""x"":231,""y"":827},""id"":18}
16:11:40.242 [nioEventLoopGroup-2-1] DEBUG c.intuit.karate.driver.DriverOptions - << {""id"":18,""result"":{}}
16:11:40.242 [ForkJoinPool-1-worker-1] DEBUG c.intuit.karate.driver.DriverOptions - >> {""method"":""Input.dispatchMouseEvent"",""params"":{""type"":""mouseReleased"",""x"":231,""y"":827,""button"":""left"",""clickCount"":1},""id"":19}
16:11:40.250 [nioEventLoopGroup-2-1] DEBUG c.intuit.karate.driver.DriverOptions - << {""id"":19,""result"":{}}

",4k,"
            1
        ","['\nDrag and drop is actually quite hard to get right, so I recommend doing this via JavaScript. Executing JS is actually quite easy using Karate:\n* driver \'https://www.seleniumeasy.com/test/drag-and-drop-demo.html\'\n* script(""var myDragEvent = new Event(\'dragstart\'); myDragEvent.dataTransfer = new DataTransfer()"")\n* waitFor(\'{}Draggable 1\').script(""_.dispatchEvent(myDragEvent)"")\n* script(""var myDropEvent = new Event(\'drop\'); myDropEvent.dataTransfer = myDragEvent.dataTransfer"")\n* script(\'#mydropzone\', ""_.dispatchEvent(myDropEvent)"")\n* screenshot()\n\nSo with a little bit of awareness of some of the internals - e.g. the HTML5 DataTransfer API - you can do pretty much anything. I think ""bending the rules"" in cases like this is fine when it comes to automating complex E2E user-interactions in a browser.\nYou can of course wrap the drag-and-drop into a re-usable function in Karate, just keep in mind that ""DOM JS"" is sent to the browser as plain-text.\nRefer the docs: https://github.com/intuit/karate/tree/master/karate-core#function-composition\nEDIT: for those looking for other examples of using JS on the DOM:\n\nhttps://stackoverflow.com/a/60618233/143475\nhttps://stackoverflow.com/a/61478834/143475\nhttps://stackoverflow.com/a/66677401/143475\nhttps://stackoverflow.com/a/67701399/143475\nhttps://stackoverflow.com/a/67629911/143475\n\n']",https://stackoverflow.com/questions/60637144/karate-ui-drag-and-drop,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I use powershell to run through an installer?,"
I am trying to install a piece of software that when done manually has configuration options you can choose from when going through the process. I am trying to figure out a way to automate this using powershell but am stuck as to how I can set those configuration options. I believe I would need to run the start-process command on the installer .exe but I don't know where to go from there. Can I use the parameters on the start-process command to pass in the configurations I want? 
",11k,"
            0
        ","['\nUPDATE: Several links towards the bottom with information on how to handle installation, configuration and file extraction for setup.exe files.\nUPDATE: See Windows Installer PowerShell Module on github.com (scroll down for description, use releases tab for download). I haven\'t really tested it much, but it is from Heath Stewart - Microsoft Senior Software Engineer (github).\n\nI had a quick look for that installer, but didn\'t find it easily. Essentially the installer is either a Windows Installer database (MSI) or something else  - generally a setup.exe of some kind. An MSI database can also be wrapped in a setup.exe.\nYou should be aware that for legacy style installers a common practice for large scale deployment is to capture the legacy install with an application repackager tool, and then compile an MSI file to use for installation (effectively converting an installer from an old format to modern MSI format). This is a specialist task requiring good understanding of Windows and setups. It is generally done in large corporations for very large software distributions. If you are in a large company there might be a team dedicated to packaging software like the one you mention. Maybe check with your management. If the setup is an MSI the same team can also modify that for you according to your specifications.\n\nWith regards to your installer EXE. Try to run setup.exe /a from the command line and see if you get an option to extract files to a ""network install point"" (administrative install). Then you are dealing with an MSI file wrapped in a setup.exe. If that doesn\'t work you can try setup.exe /x or setup.exe /extract as well. \nWindows Installer has built-in features to allow you to customize the install via PUBLIC properties (uppercase) set at the command line or applied via a transform (Windows Installer\'s mechanism to apply substantial changes to the vendor file - it is a partial database that gets applied to the installation database from the vendor at runtime).\nNon-MSI, legacy installer technologies generally have fewer reliable ways to customize the installation settings, and they tend to be rather ad hoc when they are there. In particular the silent running and uninstall may be features that are missing or poorly executed. These installs are generally all wrapped in EXE format, and there are many tools used to generate them - each with their own quirks and features.\nIn other words, it all depends on what the installer is implemented as. Give that setup.exe /a a go, and update your answer with new information for us (don\'t add too many comments - we will check back).\n\nWith regards to using PowerShell. I haven\'t used PowerShell for deployment so far to be perfectly honest. Here is a basic description of how to install using PowerShell: https://kevinmarquette.github.io/2016-10-21-powershell-installing-msi-files/\nYou can also invoke automation for MSI files from PowerShell, I don\'t think this is relevant for what you asked, but here is a quick link for modifying a transform file: http://www.itninja.com/question/ps-how-to-edit-a-mst-file.\nThe normal way to install MSI files is via Window\'s built-in msiexec.exe command line. The basic msiexec.exe command line to install software is:\nmsiexec.exe /I ""C:\\Your.msi"" /QN /L*V ""C:\\msilog.log"" TRANSFORMS=""C:\\1031.mst;C:\\My.mst""\n\nQuick Parameter Explanation:\n/I = run install sequence\n/QN = run completely silently\n/L*V ""C:\\msilog.log"" = verbose logging\nTRANSFORMS=""C:\\1031.mst;C:\\My.mst"" = Apply transforms 1031.mst and My.mst (see below).\n\nWhat is a transform? Explained here: How to make better use of MSI files.\nAdvanced Installer has a general page on msiexec.exe command lines. And here is Microsoft\'s msiexec.exe documentation on MSDN.\n\nSome links:\n\nPerhaps see Michael Urman\'s answer here: Programmatically extract contents of InstallShield setup.exe. This is for Installshield packaged EXE files only.\nInstallshield setup.exe commands (general reference with some sample command lines - towards the end of the document it looks like the command lines are not correct, but the first ones look ok. The later ones are pretty obscure anyway - just thought I\'d let you know since I link to it). Here is the official Installshield help documentation.\nWise setup.exe commands - Wise is no longer available, but if the setup is older it can still be packaged with Wise.\nAdvanced Installer standard command line. For this tool setups can apparently be extracted with setup.exe /x or setup.exe /extract. See the link for full list. \nThere was also a ""silent switch finder"" tool used to find hidden switches in exe files (for deployment), but it failed a virustotal.com scan so I won\'t link to it. Maybe it is using something exotic, such as scanning a file\'s header at a bit level or something weird that is flagged as malware by mistake? Either way, not a tool I would use.\nAnd finally: http://unattended.sourceforge.net/installers.php. This link isn\'t bad, it presents some of the tools above and a few others - and the most common switches used. Untested by me, but looks ok.\nAnd there are other deployment tools that have their own way of packaging and delivering EXE files - it can be a jungle. I can provide a list of such tools with more links, but maybe that\'s just confusing. Please try what is provided above first.\nHere is a generic answer that might be helpful as well: Extract MSI from EXE\n\n']",https://stackoverflow.com/questions/46221983/how-can-i-use-powershell-to-run-through-an-installer,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running script upon login in mac OS X [closed],"






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.


Closed 10 years ago.


This post was edited and submitted for review 9 months ago and failed to reopen the post:

Original close reason(s) were not resolved






                        Improve this question
                    



I am wondering if anyone is able to help me out with getting a shell (.sh) program to automatically run whenever I log in to my account on my computer. I am running Mac OS X 10.6.7.
I have a file ""Example.sh"" that I want to run when I log onto my computer. I do not have a problem running it when I am already logged in, but I want this to run automatically.
",330k,"
            338
        ","['\nFollow this:\n\nstart Automator.app\n\nselect Application\n\nclick Show library in the toolbar (if hidden)\n\nadd Run shell script (from the Actions/Utilities)\n\ncopy & paste your script into the window\n\ntest it\n\nsave somewhere (for example you can make an Applications folder in your HOME, you will get an your_name.app)\n\ngo to System Preferences -> Users & Groups -> Login items (or System Preferences -> Accounts -> Login items / depending of your MacOS version)\n\nadd this app\n\ntest & done ;)\n\n\nEDIT:\nI\'ve recently earned a ""Good answer"" badge for this answer. While my solution is simple and working, the cleanest way to run any program or shell script at login time is described in @trisweb\'s answer, unless, you want interactivity.\nWith automator solution you can do things like next:\n\nso, asking to run a script or quit the app, asking passwords, running other automator workflows at login time, conditionally run applications at login time and so on...\n', '\ntl;dr: use OSX\'s native process launcher and manager, launchd.\nTo do so, make a launchctl daemon. You\'ll have full control over all aspects of the script. You can run once or keep alive as a daemon. In most cases, this is the way to go.\n\nCreate a .plist file according to the instructions in the Apple Dev docs here or more detail below.\nPlace in ~/Library/LaunchAgents\nLog in (or run manually via launchctl load [filename.plist])\n\nFor more on launchd, the wikipedia article is quite good and describes the system and its advantages over other older systems.\n\nHere\'s the specific plist file to run a script at login.\n\nUpdated 2017/09/25 for OSX El Capitan and newer (credit to Jos茅 Messias Jr):\n\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<!DOCTYPE plist PUBLIC ""-//Apple Computer//DTD PLIST 1.0//EN"" ""http://www.apple.com/DTDs/PropertyList-1.0.dtd"">\n<plist version=""1.0"">\n<dict>\n   <key>Label</key>\n   <string>com.user.loginscript</string>\n   <key>ProgramArguments</key>\n   <array><string>/path/to/executable/script.sh</string></array>\n   <key>RunAtLoad</key>\n   <true/>\n</dict>\n</plist>\n\nReplace the <string> after the Program key with your desired command (note that any script referenced by that command must be executable: chmod a+x /path/to/executable/script.sh to ensure it is for all users).\nSave as ~/Library/LaunchAgents/com.user.loginscript.plist\nRun launchctl load ~/Library/LaunchAgents/com.user.loginscript.plist and log out/in to test (or to test directly, run launchctl start com.user.loginscript)\nTail /var/log/system.log for error messages.\nThe key is that this is a User-specific launchd entry, so it will be run on login for the given user. System-specific launch daemons (placed in /Library/LaunchDaemons) are run on boot.\nIf you want a script to run on login for all users, I believe LoginHook is your only option, and that\'s probably the reason it exists.\n', '\n\nCreate a shell script named as login.sh in your $HOME folder.\n\nPaste the following one-line script into Script Editor: do shell script ""$HOME/login.sh""\n\nThen save it as an application.\n\nFinally add the application to your login items.\n\n\nIf you want to make the script output visual, you can swap step 2 for this:\ntell application ""Terminal""\n  activate\n  do script ""$HOME/login.sh""\nend tell\n\nIf multiple commands are needed something like this can be used:\ntell application ""Terminal""\n  activate\n  do script ""cd $HOME""\n  do script ""./login.sh"" in window 1\nend tell\n\n']",https://stackoverflow.com/questions/6442364/running-script-upon-login-in-mac-os-x,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to copy a file to a remote server in Python using SCP or SSH?,"
I have a text file on my local machine that is generated by a daily Python script run in cron. 
I would like to add a bit of code to have that file sent securely to my server over SSH.
",359k,"
            134
        ","['\nTo do this in Python (i.e. not wrapping scp through subprocess.Popen or similar) with the Paramiko library, you would do something like this:\nimport os\nimport paramiko\n\nssh = paramiko.SSHClient() \nssh.load_host_keys(os.path.expanduser(os.path.join(""~"", "".ssh"", ""known_hosts"")))\nssh.connect(server, username=username, password=password)\nsftp = ssh.open_sftp()\nsftp.put(localpath, remotepath)\nsftp.close()\nssh.close()\n\n(You would probably want to deal with unknown hosts, errors, creating any directories necessary, and so on).\n', '\nYou can call the scp bash command (it copies files over SSH) with subprocess.run:\nimport subprocess\nsubprocess.run([""scp"", FILE, ""USER@SERVER:PATH""])\n#e.g. subprocess.run([""scp"", ""foo.bar"", ""joe@srvr.net:/path/to/foo.bar""])\n\nIf you\'re creating the file that you want to send in the same Python program, you\'ll want to call subprocess.run command outside the with block you\'re using to open the file (or call .close() on the file first if you\'re not using a with block), so you know it\'s flushed to disk from Python.\nYou need to generate (on the source machine) and install (on the destination machine) an ssh key beforehand so that the scp automatically gets authenticated with your public ssh key (in other words, so your script doesn\'t ask for a password).\n', '\nYou\'d probably use the subprocess module. Something like this:\nimport subprocess\np = subprocess.Popen([""scp"", myfile, destination])\nsts = os.waitpid(p.pid, 0)\n\nWhere destination is probably of the form user@remotehost:remotepath. Thanks to\n@Charles Duffy for pointing out the weakness in my original answer, which used a single string argument to specify the scp operation shell=True - that wouldn\'t handle whitespace in paths.\nThe module documentation has examples of error checking that you may want to perform in conjunction with this operation.\nEnsure that you\'ve set up proper credentials so that you can perform an unattended, passwordless scp between the machines. There is a stackoverflow question for this already.\n', '\nThere are a couple of different ways to approach the problem:\n\nWrap command-line programs\nuse a Python library that provides SSH capabilities (eg - Paramiko or Twisted Conch)\n\nEach approach has its own quirks. You will need to setup SSH keys to enable password-less logins if you are wrapping system commands like ""ssh"", ""scp"" or ""rsync."" You can embed a password in a script using Paramiko or some other library, but you might find the lack of documentation frustrating, especially if you are not familiar with the basics of the SSH connection (eg - key exchanges, agents, etc). It probably goes without saying that SSH keys are almost always a better idea than passwords for this sort of stuff.\nNOTE: its hard to beat rsync if you plan on transferring files via SSH, especially if the alternative is plain old scp.\nI\'ve used Paramiko with an eye towards replacing system calls but found myself drawn back to the wrapped commands due to their ease of use and immediate familiarity. You might be different. I gave Conch the once-over some time ago but it didn\'t appeal to me.\nIf opting for the system-call path, Python offers an array of options such as os.system or the commands/subprocess modules. I\'d go with the subprocess module if using version 2.4+.\n', '\nReached the same problem, but instead of ""hacking"" or emulating command line:\nFound this answer here.\nfrom paramiko import SSHClient\nfrom scp import SCPClient\n\nssh = SSHClient()\nssh.load_system_host_keys()\nssh.connect(\'example.com\')\n\nwith SCPClient(ssh.get_transport()) as scp:\n    scp.put(\'test.txt\', \'test2.txt\')\n    scp.get(\'test2.txt\')\n\n', '\nYou can do something like this, to handle the host key checking as well\nimport os\nos.system(""sshpass -p password scp -o StrictHostKeyChecking=no local_file_path username@hostname:remote_path"")\n\n', '\nfabric could be used to upload files vis ssh:\n#!/usr/bin/env python\nfrom fabric.api import execute, put\nfrom fabric.network import disconnect_all\n\nif __name__==""__main__"":\n    import sys\n    # specify hostname to connect to and the remote/local paths\n    srcdir, remote_dirname, hostname = sys.argv[1:]\n    try:\n        s = execute(put, srcdir, remote_dirname, host=hostname)\n        print(repr(s))\n    finally:\n        disconnect_all()\n\n', '\nYou can use the vassal package, which is exactly designed for this.\nAll you need is to install vassal and do\nfrom vassal.terminal import Terminal\nshell = Terminal([""scp username@host:/home/foo.txt foo_local.txt""])\nshell.run()\n\nAlso, it will save you authenticate credential and don\'t need to type them again and again.\n', '\nUsing the external resource paramiko;\n    from paramiko import SSHClient\n    from scp import SCPClient\n    import os\n\n    ssh = SSHClient() \n    ssh.load_host_keys(os.path.expanduser(os.path.join(""~"", "".ssh"", ""known_hosts"")))\n    ssh.connect(server, username=\'username\', password=\'password\')\n    with SCPClient(ssh.get_transport()) as scp:\n            scp.put(\'test.txt\', \'test2.txt\')\n\n', ""\nI used sshfs to mount the remote directory via ssh, and shutil to copy the files:\n$ mkdir ~/sshmount\n$ sshfs user@remotehost:/path/to/remote/dst ~/sshmount\n\nThen in python:\nimport shutil\nshutil.copy('a.txt', '~/sshmount')\n\nThis method has the advantage that you can stream data over if you are generating data rather than caching locally and sending a single large file.\n"", ""\nTry this if you wan't to use SSL certificates:\nimport subprocess\n\ntry:\n    # Set scp and ssh data.\n    connUser = 'john'\n    connHost = 'my.host.com'\n    connPath = '/home/john/'\n    connPrivateKey = '/home/user/myKey.pem'\n\n    # Use scp to send file from local to host.\n    scp = subprocess.Popen(['scp', '-i', connPrivateKey, 'myFile.txt', '{}@{}:{}'.format(connUser, connHost, connPath)])\n\nexcept CalledProcessError:\n    print('ERROR: Connection to host failed!')\n\n"", '\nA very simple approach is the following: \nimport os\nos.system(\'sshpass -p ""password"" scp user@host:/path/to/file ./\')\n\nNo python library are required (only os), and it works, however using this method relies on another ssh client to be installed. This could result in undesired behavior if ran on another system.\n', '\nCalling scp command via subprocess doesn\'t allow to receive the progress report inside the script. pexpect could be used to extract that info:\nimport pipes\nimport re\nimport pexpect # $ pip install pexpect\n\ndef progress(locals):\n    # extract percents\n    print(int(re.search(br\'(\\d+)%$\', locals[\'child\'].after).group(1)))\n\ncommand = ""scp %s %s"" % tuple(map(pipes.quote, [srcfile, destination]))\npexpect.run(command, events={r\'\\d+%\': progress})\n\nSee python copy file in local network (linux -> linux)\n', '\nKind of hacky, but the following should work :)\nimport os\nfilePath = ""/foo/bar/baz.py""\nserverPath = ""/blah/boo/boom.py""\nos.system(""scp ""+filePath+"" user@myserver.com:""+serverPath)\n\n']",https://stackoverflow.com/questions/68335/how-to-copy-a-file-to-a-remote-server-in-python-using-scp-or-ssh,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Exception in thread ""main"" org.openqa.selenium.NoSuchElementException: Unable to locate element: //*[@id='login-email']","
I had to re-test the xpath, Previously it was working fine, But now it gives me an error. 
I tried with different locators as well, Like id, name. but still get the same error.
package staging;

import org.openqa.selenium.By;
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.firefox.FirefoxDriver;

public class login {

    public static void main (String[]args){
        System.setProperty(""webdriver.gecko.driver"",""C:\\Program Files\\geckodriver.exe"");
        WebDriver driver = new FirefoxDriver();

        //opening the browser
        driver.get(""https://staging.keela.co/login"");

        //logging
        driver.findElement(By.xpath(""//*[@id='login-email']"")).sendKeys(""bandanakeela@yopmail.com"");
        driver.findElement(By.xpath(""//*[@id='login-password']"")).sendKeys(""keela"");
        driver.findElement(By.xpath(""//*[@id='login-form']/div[3]/div/button"")).click();       
 }
}

",12k,"
            5
        ","['\nAs you access the url https://staging.keela.co/login there is a Ajax loader which blocks the UI, so we have to wait for the Ajax loader to complete loading the all the WebElements and the email and password field becomes visible. To achieve that we will introduce ExplicitWait i.e. WebDriverWait with ExpectedConditions set to elementToBeClickable for the email field.Here is the working code block:   \nSystem.setProperty(""webdriver.gecko.driver"",""C:\\\\Utility\\\\BrowserDrivers\\\\geckodriver.exe"");\nWebDriver driver = new FirefoxDriver();\ndriver.get(""https://staging.keela.co/login"");\nWebDriverWait wait = new WebDriverWait (driver, 15);\nWebElement element = wait.until(ExpectedConditions.elementToBeClickable(By.xpath(""//input[@id=\'login-email\']"")));\nelement.sendKeys(""bandanakeela@yopmail.com"");\ndriver.findElement(By.xpath(""//input[@id=\'login-password\']"")).sendKeys(""keela"");\ndriver.findElement(By.xpath(""//button[@class=\'btn btn-sm btn-block btn-primary\']"")).click(); \n\n', '\nTry this below code.\nNote: If id attribute is available then you should use id and for xpath try to use relative xpath.\nI have used explicit wait method, so your driver may able to find the next webelement, after page is fully loaded. \ndriver.get(""https://staging.keela.co/login"");\ndriver.manage().window().maximize();\n\n//Explicit wait for 60 seconds, to find the webelement. You can increase or decrease the time as per your specification.        \nnew WebDriverWait(driver, 60).until(ExpectedConditions.elementToBeClickable(driver.findElement(By.id(""login-email""))));\ndriver.findElement(By.id(""login-email"")).sendKeys(""bandanakeela@yopmail.com"");\ndriver.findElement(By.id(""login-password"")).sendKeys(""keela"");\ndriver.findElement(By.xpath(""//button[@type=\'submit\'][text()=\'Log in\']"")).click();\n\n', '\nThe page is using js to construct elements. So I would suggest you to use phantomjs driver. \nThen you have to wait until element exist. You see the gear icon when page is loading. wait until the element loads. and also you can use id instead of xpath since you know your element id .\nYou can choose which wait type you want to use. Explicit Waits or Implicit Waits.\nHere is the selenium documentation.\nand example code for wait:\nWebElement myDynamicElement = (new WebDriverWait(driver, 10))\n  .until(ExpectedConditions.presenceOfElementLocated(By.id(""login-email"")));\n\nor you can wait until page load:\nnew WebDriverWait(firefoxDriver, pageLoadTimeout).until(\n          webDriver -> ((JavascriptExecutor) webDriver).executeScript(""return document.readyState"").equals(""complete""));\n\n', '\nYou are opening the URL and at the very next moment entering email-id. Before entering email-id, you need to check if the page is fully loaded. In this case, explicit wait will help you out-\n//opening the browser\ndriver.get(""https://staging.keela.co/login"");\n\n//Explicit wait\n\nWebDriverWait wait = new WebDriverWait(WebDriverRefrence,20);\nWebElement email;\nemail = wait.until(ExpectedConditions.visibilityOfElementLocated(By.id(""login-email"")));\n\n//logging\ndriver.findElement(By.xpath(""//*[@id=\'login-email\']"")).sendKeys(""bandanakeela@yopmail.com"");\ndriver.findElement(By.xpath(""//*[@id=\'login-password\']"")).sendKeys(""keela"");\ndriver.findElement(By.xpath(""//*[@id=\'login-form\']/div[3]/div/button"")).click();\n\n']",https://stackoverflow.com/questions/46202283/exception-in-thread-main-org-openqa-selenium-nosuchelementexception-unable-to,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Have bash script answer interactive prompts [duplicate],"






This question already has answers here:
                        
                    



Passing arguments to an interactive program non-interactively

                                (5 answers)
                            

Closed 2 years ago.
The community reviewed whether to reopen this question 1 year ago and left it closed:

Original close reason(s) were not resolved




Is it possible to have a bash script automatically handle prompts that would normally be presented to the user with default actions?  Currently I am using a bash script to call an in-house tool that will display prompts to the user (prompting for Y/N) to complete actions, however the script I'm writing needs to be completely ""hands-off"", so I need a way to send Y|N to the prompt to allow the program to continue execution.  Is this possible?
",216k,"
            157
        ","['\nA simple\necho ""Y Y N N Y N Y Y N"" | ./your_script\n\nThis allow you to pass any sequence of ""Y"" or ""N"" to your script.\n', '\nThis is not ""auto-completion"", this is automation. One common tool for these things is called Expect.\nYou might also get away with just piping input from yes.\n', '\nIf you only have Y to send : \n$> yes Y |./your_script\n\nIf you only have N to send : \n$> yes N |./your_script\n\n', '\nI found the best way to send input is to use cat and a text file to pass along whatever input you need.\ncat ""input.txt"" | ./Script.sh\n\n', ""\nIn my situation I needed to answer some questions without Y or N but with text or blank.  I found the best way to do this in my situation was to create a shellscript file.  In my case I called it autocomplete.sh\nI was needing to answer some questions for a doctrine schema exporter so my file looked like this.\n-- This is an example only --\nphp vendor/bin/mysql-workbench-schema-export mysqlworkbenchfile.mwb ./doctrine << EOF\n`#Export to Doctrine Annotation Format`                                     1\n`#Would you like to change the setup configuration before exporting`        y\n`#Log to console`                                                           y\n`#Log file`                                                                 testing.log\n`#Filename [%entity%.%extension%]`\n`#Indentation [4]`\n`#Use tabs [no]`\n`#Eol delimeter (win, unix) [win]`\n`#Backup existing file [yes]`\n`#Add generator info as comment [yes]`\n`#Skip plural name checking [no]`\n`#Use logged storage [no]`\n`#Sort tables and views [yes]`\n`#Export only table categorized []`\n`#Enhance many to many detection [yes]`\n`#Skip many to many tables [yes]`\n`#Bundle namespace []`\n`#Entity namespace []`\n`#Repository namespace []`\n`#Use automatic repository [yes]`\n`#Skip column with relation [no]`\n`#Related var name format [%name%%related%]`\n`#Nullable attribute (auto, always) [auto]`\n`#Generated value strategy (auto, identity, sequence, table, none) [auto]`\n`#Default cascade (persist, remove, detach, merge, all, refresh, ) [no]`\n`#Use annotation prefix [ORM\\]`\n`#Skip getter and setter [no]`\n`#Generate entity serialization [yes]`\n`#Generate extendable entity [no]`                                          y\n`#Quote identifier strategy (auto, always, none) [auto]`\n`#Extends class []`\n`#Property typehint [no]`\nEOF\n\nThe thing I like about this strategy is you can comment what your answers are and using EOF a blank line is just that (the default answer).  Turns out by the way this exporter tool has its own JSON counterpart for answering these questions, but I figured that out after I did this =).\nto run the script simply be in the directory you want and run 'sh autocomplete.sh' in terminal.\nIn short by using << EOL & EOF in combination with Return Lines you can answer each question of the prompt as necessary.  Each new line is a new answer.\nMy example just shows how this can be done with comments also using the ` character so you remember what each step is.\nNote the other advantage of this method is you can answer with more then just Y or N ... in fact you can answer with blanks!\nHope this helps someone out.\n"", ""\nThere is a special build-in util for this - 'yes'.\nTo answer all questions with the same answer, you can run \nyes [answer] |./your_script\n\nOr you can put it inside your script have specific answer to each question\n""]",https://stackoverflow.com/questions/3804577/have-bash-script-answer-interactive-prompts,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is there a PHP equivalent of Perl's WWW::Mechanize?,"
I'm looking for a library that has functionality similar to Perl's WWW::Mechanize, but for PHP. Basically, it should allow me to submit HTTP GET and POST requests with a simple syntax, and then parse the resulting page and return in a simple format all forms and their fields, along with all links on the page.
I know about CURL, but it's a little too barebones, and the syntax is pretty ugly (tons of curl_foo($curl_handle, ...) statements
Clarification:
I want something more high-level than the answers so far. For example, in Perl, you could do something like:
# navigate to the main page
$mech->get( 'http://www.somesite.com/' ); 

# follow a link that contains the text 'download this'
$mech->follow_link( text_regex => qr/download this/i );

# submit a POST form, to log into the site
$mech->submit_form(
    with_fields      => {
        username    => 'mungo',
        password    => 'lost-and-alone',
    }
);

# save the results as a file
$mech->save_content('somefile.zip');

To do the same thing using HTTP_Client or wget or CURL would be a lot of work, I'd have to manually parse the pages to find the links, find the form URL, extract all the hidden fields, and so on. The reason I'm asking for a PHP solution is that I have no experience with Perl, and I could probably build what I need with a lot of work, but it would be much quicker if I could do the above in PHP.
",15k,"
            26
        ","[""\nSimpleTest's ScriptableBrowser can be used independendly from the testing framework. I've used it for numerous automation-jobs.\n"", '\nI feel compelled to answer this, even though its an old post... I\'ve been working with PHP curl a lot and it is not as good anywhere near comparable to something like WWW:Mechanize, which I am switching to (I think I am going to go with the Ruby language implementation).. Curl is outdated as it requires too much ""grunt work"" to automate anything, the simpletest scriptable browser looked promising to me but in testing it, it won\'t work on most web forms I try it on... honestly, I think PHP is lacking in this category of scraping, web automation so its best to look at a different language, just wanted to post this since I have spent countless hours on this topic and maybe it will save someone else some time in the future.\n', '\nIt\'s 2016 now and there\'s Mink. It even supports different engines from headless pure-PHP ""browser"" (without JavaScript), over Selenium (which needs a browser like Firefox or Chrome) to a headless ""browser.js"" in NPM, which DOES support JavaScript.\n', '\nTry looking in the PEAR library. If all else fails, create an object wrapper for curl.\nYou can so something simple like this:\nclass curl {\n    private $resource;\n\n    public function __construct($url) {\n        $this->resource = curl_init($url);\n    }\n\n    public function __call($function, array $params) {\n        array_unshift($params, $this->resource);\n        return call_user_func_array(""curl_$function"", $params);\n    }\n}\n\n', ""\nTry one of the following:\n\nPEAR's HTTP_Request\nZend_Http_Client\n\n(Yes, it's ZendFramework code, but it doesn't make your class slower using it since it just loads the required libs.)\n"", '\nLook into Snoopy:\nhttp://sourceforge.net/projects/snoopy/\n', '\nCurl is the way to go for simple requests. It runs cross platform, has a PHP extension and is widely adopted and tested.\nI created a nice class that can GET and POST an array of data (INCLUDING FILES!) to a url by just calling CurlHandler::Get($url, $data) || CurlHandler::Post($url, $data). There\'s an optional HTTP User authentication option too :)\n/**\n * CURLHandler handles simple HTTP GETs and POSTs via Curl \n * \n * @package Pork\n * @author SchizoDuckie\n * @copyright SchizoDuckie 2008\n * @version 1.0\n * @access public\n */\nclass CURLHandler\n{\n\n    /**\n     * CURLHandler::Get()\n     * \n     * Executes a standard GET request via Curl.\n     * Static function, so that you can use: CurlHandler::Get(\'http://www.google.com\');\n     * \n     * @param string $url url to get\n     * @return string HTML output\n     */\n    public static function Get($url)\n    {\n       return self::doRequest(\'GET\', $url);\n    }\n\n    /**\n     * CURLHandler::Post()\n     * \n     * Executes a standard POST request via Curl.\n     * Static function, so you can use CurlHandler::Post(\'http://www.google.com\', array(\'q\'=>\'StackOverFlow\'));\n     * If you want to send a File via post (to e.g. PHP\'s $_FILES), prefix the value of an item with an @ ! \n     * @param string $url url to post data to\n     * @param Array $vars Array with key=>value pairs to post.\n     * @return string HTML output\n     */\n    public static function Post($url, $vars, $auth = false) \n    {\n       return self::doRequest(\'POST\', $url, $vars, $auth);\n    }\n\n    /**\n     * CURLHandler::doRequest()\n     * This is what actually does the request\n     * <pre>\n     * - Create Curl handle with curl_init\n     * - Set options like CURLOPT_URL, CURLOPT_RETURNTRANSFER and CURLOPT_HEADER\n     * - Set eventual optional options (like CURLOPT_POST and CURLOPT_POSTFIELDS)\n     * - Call curl_exec on the interface\n     * - Close the connection\n     * - Return the result or throw an exception.\n     * </pre>\n     * @param mixed $method Request Method (Get/ Post)\n     * @param mixed $url URI to get or post to\n     * @param mixed $vars Array of variables (only mandatory in POST requests)\n     * @return string HTML output\n     */\n    public static function doRequest($method, $url, $vars=array(), $auth = false)\n    {\n        $curlInterface = curl_init();\n\n        curl_setopt_array ($curlInterface, array( \n            CURLOPT_URL => $url,\n            CURLOPT_RETURNTRANSFER => 1,\n            CURLOPT_FOLLOWLOCATION =>1,\n            CURLOPT_HEADER => 0));\n        if (strtoupper($method) == \'POST\')\n        {\n            curl_setopt_array($curlInterface, array(\n                CURLOPT_POST => 1,\n                CURLOPT_POSTFIELDS => http_build_query($vars))\n            );  \n        }\n        if($auth !== false)\n        {\n              curl_setopt($curlInterface, CURLOPT_USERPWD, $auth[\'username\'] . "":"" . $auth[\'password\']);\n        }\n        $result = curl_exec ($curlInterface);\n        curl_close ($curlInterface);\n\n        if($result === NULL)\n        {\n            throw new Exception(\'Curl Request Error: \'.curl_errno($curlInterface) . "" - "" . curl_error($curlInterface));\n        }\n        else\n        {\n            return($result);\n        }\n    }\n\n}\n\n?>\n\n[edit] Read the clarification only now... You probably want to go with one of the tools mentioned above that automates stuff. You could also decide to use a clientside firefox extension like ChickenFoot for more flexibility. I\'ll leave the example class above here for future searches.\n', '\nIf you\'re using CakePHP in your project, or if you\'re inclined to extract the relevant library you can use their curl wrapper HttpSocket. It has the simple page-fetching syntax you describe, e.g., \n# This is the sugar for importing the library within CakePHP       \nApp::import(\'Core\', \'HttpSocket\');\n$HttpSocket = new HttpSocket();\n\n$result = $HttpSocket->post($login_url,\narray(\n  ""username"" => ""username"",\n  ""password"" => ""password""\n)\n);\n\n...although it doesn\'t have a way to parse the response page. For that I\'m going to use simplehtmldom: http://net.tutsplus.com/tutorials/php/html-parsing-and-screen-scraping-with-the-simple-html-dom-library/ which describes itself as having a jQuery-like syntax.\nI tend to agree that the bottom line is that PHP doesn\'t have the awesome scraping/automation libraries that Perl/Ruby have.\n', ""\nIf you're on a *nix system you could use shell_exec() with wget, which has a lot of nice options.\n""]",https://stackoverflow.com/questions/199045/is-there-a-php-equivalent-of-perls-wwwmechanize,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CRON job to run on the last day of the month,"
I need to create a CRON job that will run on the last day of every month.
I will create it using cPanel.
Any help is appreciated. 
Thanks
",237k,"
            138
        ","['\nPossibly the easiest way is to simply do three separate jobs:\n55 23 30 4,6,9,11        * myjob.sh\n55 23 31 1,3,5,7,8,10,12 * myjob.sh\n55 23 28 2               * myjob.sh\n\nThat will run on the 28th of February though, even on leap years so, if that\'s a problem, you\'ll need to find another way.\n\nHowever, it\'s usually both substantially easier and correct to run the job as soon as possible on the first day of each month, with something like:\n0 0 1 * * myjob.sh\n\nand modify the script to process the previous month\'s data.\nThis removes any hassles you may encounter with figuring out which day is the last of the month, and also ensures that all data for that month is available, assuming you\'re processing data. Running at five minutes to midnight on the last day of the month may see you missing anything that happens between then and midnight.\nThis is the usual way to do it anyway, for most end-of-month jobs.\n\nIf you still really want to run it on the last day of the month, one option is to simply detect if tomorrow is the first (either as part of your script, or in the crontab itself).\nSo, something like:\n55 23 28-31 * * [[ ""$(date --date=tomorrow +\\%d)"" == ""01"" ]] && myjob.sh\n\nshould be a good start, assuming you have a relatively intelligent date program.\nIf your date program isn\'t quite advanced enough to give you relative dates, you can just put together a very simple program to give you tomorrow\'s day of the month (you don\'t need the full power of date), such as:\n#include <stdio.h>\n#include <time.h>\n\nint main (void) {\n    // Get today, somewhere around midday (no DST issues).\n\n    time_t noonish = time (0);\n    struct tm *localtm = localtime (&noonish);\n    localtm->tm_hour = 12;\n\n    // Add one day (86,400 seconds).\n\n    noonish = mktime (localtm) + 86400;\n    localtm = localtime (&noonish);\n\n    // Output just day of month.\n\n    printf (""%d\\n"", localtm->tm_mday);\n\n    return 0;\n}\n\nand then use (assuming you\'ve called it tomdom for ""tomorrow\'s day of month""):\n55 23 28-31 * * [[ ""$(tomdom)"" == ""1"" ]] && myjob.sh\n\nThough you may want to consider adding error checking since both time() and mktime() can return -1 if something goes wrong. The code above, for reasons of simplicity, does not take that into account.\n', '\nThere\'s a slightly shorter method that can be used similar to one of the ones above. That is:\n[ $(date -d +1day +%d) -eq 1 ] && echo ""last day of month""\n\nAlso, the crontab entry could be update to only check on the 28th to 31st as it\'s pointless running it the other days of the month. Which would give you:\n0 23 28-31 * * [ $(date -d +1day +%d) -eq 1 ] && myscript.sh\n\n', '\nWhat about this one, after Wikipedia?\n55 23 L * * /full/path/to/command\n\n', '\nFor AWS Cloudwatch cron implementation (Scheduling Lambdas, etc..) this works:\n55 23 L * ? *\n\nRunning at 11:55pm on the last day of each month.\n', ""\nAdapting paxdiablo's solution, I run on the 28th and 29th of February.  The data from the 29th overwrites the 28th.\n# min  hr  date     month          dow\n  55   23  31     1,3,5,7,8,10,12   * /path/monthly_copy_data.sh\n  55   23  30     4,6,9,11          * /path/monthly_copy_data.sh\n  55   23  28,29  2                 * /path/monthly_copy_data.sh\n\n"", '\nYou could set up a cron job to run on every day of the month, and have it run a shell script like the following.  This script works out whether tomorrow\'s day number is less than today\'s (i.e. if tomorrow is a new month), and then does whatever you want.\nTODAY=`date +%d`\nTOMORROW=`date +%d -d ""1 day""`\n\n# See if tomorrow\'s day is less than today\'s\nif [ $TOMORROW -lt $TODAY ]; then\necho ""This is the last day of the month""\n# Do stuff...\nfi\n\n', '\nFor a safer method in a crontab based on @Indie solution (use absolute path to date + $() does not works on all crontab systems):\n0 23 28-31 * * [ `/bin/date -d +1day +\\%d` -eq 1 ] && myscript.sh\n\n', '\nSome cron implementations support the ""L"" flag to represent the last day of the month.\nIf you\'re lucky to be using one of those implementations, it\'s as simple as:\n0 55 23 L * ?\n\nThat will run at 11:55 pm on the last day of every month.\nhttp://www.quartz-scheduler.org/documentation/quartz-1.x/tutorials/crontrigger\n', '\n#########################################################\n# Memory Aid \n# environment    HOME=$HOME SHELL=$SHELL LOGNAME=$LOGNAME PATH=$PATH\n#########################################################\n#\n# string         meaning\n# ------         -------\n# @reboot        Run once, at startup.\n# @yearly        Run once a year, ""0 0 1 1 *"".\n# @annually      (same as @yearly)\n# @monthly       Run once a month, ""0 0 1 * *"".\n# @weekly        Run once a week, ""0 0 * * 0"".\n# @daily         Run once a day, ""0 0 * * *"".\n# @midnight      (same as @daily)\n# @hourly        Run once an hour, ""0 * * * *"".\n#mm     hh      Mday    Mon     Dow     CMD # minute, hour, month-day month DayofW CMD\n#........................................Minute of the hour\n#|      .................................Hour in the day (0..23)\n#|      |       .........................Day of month, 1..31 (mon,tue,wed)\n#|      |       |       .................Month (1.12) Jan, Feb.. Dec\n#|      |       |       |        ........day of the week 0-6  7==0\n#|      |       |       |        |      |command to be executed\n#V      V       V       V        V      V\n*       *       28-31   *       *       [ `date -d +\'1 day\' +\\%d` -eq 1 ] && echo ""Tomorrow is the first today now is  `date`"" >> ~/message\n1       0       1       *       *       rm -f ~/message\n*       *       28-31   *       *       [ `date -d +\'1 day\' +\\%d` -eq 1 ] && echo ""HOME=$HOME LOGNAME=$LOGNAME SHELL = $SHELL PATH=$PATH"" \n\n', ""\nSet up a cron job to run on the first day of the month. Then change the system's clock to be one day ahead.\n"", ""\n00 23 * * * [[ $(date +'%d') -eq $(cal | awk '!/^$/{ print $NF }' | tail -1) ]] && job\n\nCheck out a related question on the unix.com forum.\n"", '\nI found out solution (On the last day of the month) like below from this site.\n 0 0 0 L * ? *\n\nCRON details:\nSeconds Minutes Hours   Day Of Month    Month   Day Of Week  Year\n0       0       0       L               *       ?            *\n\nTo cross verify above expression,  click here which gives output like below.\n2021-12-31 Fri 00:00:00\n2022-01-31 Mon 00:00:00\n2022-02-28 Mon 00:00:00\n2022-03-31 Thu 00:00:00\n2022-04-30 Sat 00:00:00\n\n', ""\nYou can just connect all answers in one cron line and use only date command.\nJust check the difference between day of the month which is today and will be tomorrow:\n0 23 * * * root [ $(expr $(date +\\%d -d '1 days') - $(date +\\%d)  ) -le 0 ]  && echo true\n\nIf these difference is below 0 it means that we change the month and there is last day of the month.\n"", '\n55 23 28-31 * * echo ""[ $(date -d +1day +%d) -eq 1 ] && my.sh"" | /bin/bash \n\n', ""\nWhat about this? \nedit user's .bashprofile adding: \nexport LAST_DAY_OF_MONTH=$(cal | awk '!/^$/{ print $NF }' | tail -1)\n\nThen add this entry to crontab: \nmm hh * * 1-7 [[ $(date +'%d') -eq $LAST_DAY_OF_MONTH ]] && /absolutepath/myscript.sh\n\n"", ""\nIn tools like Jenkins, where usually there is no support for L nor tools similar to date, a cool trick might be setting up the timezone correctly. E.g. Pacific/Kiritimati is GMT+14:00, so if you're in Europe or in the US, this might do the trick.\nTZ=Pacific/Kiritimati \\n H 0 1 * * \n\nResult: Would last have run at Saturday, April 30, 2022 10:54:53 AM GMT; would next run at Tuesday, May 31, 2022 10:54:53 AM GMT.\n"", ""\nUse the below code to run cron on the last day of the month in PHP\n$commands = '30 23 '.date('t').' '.date('n').' *';\n\n"", '\nThe last day of month can be 28-31 depending on what month it is (Feb, March etc). However in either of these cases, the next day is always 1st of next month. So we can use that to make sure we run some job always on the last day of a month using the code below:\n0 8 28-31 * * [ ""$(date +%d -d tomorrow)"" = ""01"" ] && /your/script.sh\n\n', '\nNot sure of other languages but in javascript it is possible.\nIf you need your job to be completed before first day of month node-cron will allow you to set timezone - you have to set UTC+12:00 and if job is not too long most of the world will have results before start of their month.\n', '\nIf the day-of-the-month field could accept day zero that would very simply solve this problem. Eg. astronomers use day zero to express the last day of the previous month. So\n00 08 00 * * /usr/local/sbin/backup\n\nwould do the job in simple and easy way.\n', '\nBetter way to schedule cron on every next month of 1st day\nThis will run the command foo at 12:00AM.\n0 0 1 * * /usr/bin/foo\n', '\nBe cautious with ""yesterday"", ""today"", ""1day"" in the \'date\' program if running between midnight and 1am, because often those really mean ""24 hours"" which will be two days when daylight saving time change causes a 23 hour day.  I use ""date -d \'1am -12 hour\' ""\n']",https://stackoverflow.com/questions/6139189/cron-job-to-run-on-the-last-day-of-the-month,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I login to a website with Python?,"
How can I do it? 
I was trying to enter some specified link (with urllib), but to do it, I need to log in.
I have this source from the site:
<form id=""login-form"" action=""auth/login"" method=""post"">
    <div>
    <!--label for=""rememberme"">Remember me</label><input type=""checkbox"" class=""remember"" checked=""checked"" name=""remember me"" /-->
    <label for=""email"" id=""email-label"" class=""no-js"">Email</label>
    <input id=""email-email"" type=""text"" name=""handle"" value="""" autocomplete=""off"" />
    <label for=""combination"" id=""combo-label"" class=""no-js"">Combination</label>
    <input id=""password-clear"" type=""text"" value=""Combination"" autocomplete=""off"" />
    <input id=""password-password"" type=""password"" name=""password"" value="""" autocomplete=""off"" />
    <input id=""sumbitLogin"" class=""signin"" type=""submit"" value=""Sign In"" />

Is this possible?
",374k,"
            105
        ","['\nMaybe you want to use twill. It\'s quite easy to use and should be able to do what you want.\nIt will look like the following:\nfrom twill.commands import *\ngo(\'http://example.org\')\n\nfv(""1"", ""email-email"", ""blabla.com"")\nfv(""1"", ""password-clear"", ""testpass"")\n\nsubmit(\'0\')\n\nYou can use showforms() to list all forms once you used go鈥?to browse to the site you want to login. Just try it from the python interpreter.\n', '\nLet me try to make it simple, suppose URL of the site is www.example.com and you need to sign up by filling username and password, so we go to the login page say http://www.example.com/login.php now and view it\'s source code and search for the action URL it will be in form tag something like \n <form name=""loginform"" method=""post"" action=""userinfo.php"">\n\nnow take userinfo.php to make absolute URL which will be \'http://example.com/userinfo.php\', now run a simple python script \nimport requests\nurl = \'http://example.com/userinfo.php\'\nvalues = {\'username\': \'user\',\n          \'password\': \'pass\'}\n\nr = requests.post(url, data=values)\nprint r.content\n\nI Hope that this helps someone somewhere someday.\n', '\nTypically you\'ll need cookies to log into a site, which means cookielib, urllib and urllib2. Here\'s a class which I wrote back when I was playing Facebook web games:\nimport cookielib\nimport urllib\nimport urllib2\n\n# set these to whatever your fb account is\nfb_username = ""your@facebook.login""\nfb_password = ""secretpassword""\n\nclass WebGamePlayer(object):\n\n    def __init__(self, login, password):\n        """""" Start up... """"""\n        self.login = login\n        self.password = password\n\n        self.cj = cookielib.CookieJar()\n        self.opener = urllib2.build_opener(\n            urllib2.HTTPRedirectHandler(),\n            urllib2.HTTPHandler(debuglevel=0),\n            urllib2.HTTPSHandler(debuglevel=0),\n            urllib2.HTTPCookieProcessor(self.cj)\n        )\n        self.opener.addheaders = [\n            (\'User-agent\', (\'Mozilla/4.0 (compatible; MSIE 6.0; \'\n                           \'Windows NT 5.2; .NET CLR 1.1.4322)\'))\n        ]\n\n        # need this twice - once to set cookies, once to log in...\n        self.loginToFacebook()\n        self.loginToFacebook()\n\n    def loginToFacebook(self):\n        """"""\n        Handle login. This should populate our cookie jar.\n        """"""\n        login_data = urllib.urlencode({\n            \'email\' : self.login,\n            \'pass\' : self.password,\n        })\n        response = self.opener.open(""https://login.facebook.com/login.php"", login_data)\n        return \'\'.join(response.readlines())\n\nYou won\'t necessarily need the HTTPS or Redirect handlers, but they don\'t hurt, and it makes the opener much more robust. You also might not need cookies, but it\'s hard to tell just from the form that you\'ve posted. I suspect that you might, purely from the \'Remember me\' input that\'s been commented out.\n', '\nWeb page automation ? Definitely ""webbot""\nwebbot even works web pages which have dynamically changing id and classnames and has more methods and features than selenium or mechanize.\n\nHere\'s a snippet :)\n\nfrom webbot import Browser \nweb = Browser()\nweb.go_to(\'google.com\') \nweb.click(\'Sign in\')\nweb.type(\'mymail@gmail.com\' , into=\'Email\')\nweb.click(\'NEXT\' , tag=\'span\')\nweb.type(\'mypassword\' , into=\'Password\' , id=\'passwordFieldId\') # specific selection\nweb.click(\'NEXT\' , tag=\'span\') # you are logged in ^_^\n\nThe docs are also pretty straight forward and simple to  use : https://webbot.readthedocs.io\n', ""\nimport cookielib\nimport urllib\nimport urllib2\n\nurl = 'http://www.someserver.com/auth/login'\nvalues = {'email-email' : 'john@example.com',\n          'password-clear' : 'Combination',\n          'password-password' : 'mypassword' }\n\ndata = urllib.urlencode(values)\ncookies = cookielib.CookieJar()\n\nopener = urllib2.build_opener(\n    urllib2.HTTPRedirectHandler(),\n    urllib2.HTTPHandler(debuglevel=0),\n    urllib2.HTTPSHandler(debuglevel=0),\n    urllib2.HTTPCookieProcessor(cookies))\n\nresponse = opener.open(url, data)\nthe_page = response.read()\nhttp_headers = response.info()\n# The login cookies should be contained in the cookies variable\n\nFor more information visit: https://docs.python.org/2/library/urllib2.html\n"", '\nWebsites in general can check authorization in many different ways, but the one you\'re targeting seems to make it reasonably easy for you.\nAll you need is to POST to the auth/login URL a form-encoded blob with the various fields you see there (forget the labels for, they\'re decoration for human visitors).  handle=whatever&password-clear=pwd and so on, as long as you know the values for the handle (AKA email) and password you should be fine.\nPresumably that POST will redirect you to some ""you\'ve successfully logged in"" page with a Set-Cookie header validating your session (be sure to save that cookie and send it back on further interaction along the session!).\n', '\nFor HTTP things, the current choice should be: Requests- HTTP for Humans\n']",https://stackoverflow.com/questions/2910221/how-can-i-login-to-a-website-with-python,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to use Selenium with Python?,"
How do I set up Selenium to work with Python? I just want to write/export scripts in Python, and then run them. Are there any resources for that? I tried googling, but the stuff I found was either referring to an outdated version of Selenium (RC), or an outdated version of Python.
",116k,"
            51
        ","['\nYou mean Selenium WebDriver? \nHuh....\nPrerequisite: Install Python based on your OS\nInstall with following command \npip install -U selenium\n\nAnd use this module in your code \nfrom selenium import webdriver\n\nYou can also use many of the following as required \nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.common.exceptions import NoSuchElementException\n\nHere is an updated answer\nI would recommend you to run script without IDE... Here is my approach\n\nUSE IDE to find xpath of object / element\nAnd use find_element_by_xpath().click() \n\nAn example below shows login page automation \n#ScriptName : Login.py\n#---------------------\nfrom selenium import webdriver\n\n#Following are optional required\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.common.exceptions import NoSuchElementException\n\nbaseurl = ""http://www.mywebsite.com/login.php""\nusername = ""admin""\npassword = ""admin""\n\nxpaths = { \'usernameTxtBox\' : ""//input[@name=\'username\']"",\n           \'passwordTxtBox\' : ""//input[@name=\'password\']"",\n           \'submitButton\' :   ""//input[@name=\'login\']""\n         }\n\nmydriver = webdriver.Firefox()\nmydriver.get(baseurl)\nmydriver.maximize_window()\n\n#Clear Username TextBox if already allowed ""Remember Me"" \nmydriver.find_element_by_xpath(xpaths[\'usernameTxtBox\']).clear()\n\n#Write Username in Username TextBox\nmydriver.find_element_by_xpath(xpaths[\'usernameTxtBox\']).send_keys(username)\n\n#Clear Password TextBox if already allowed ""Remember Me"" \nmydriver.find_element_by_xpath(xpaths[\'passwordTxtBox\']).clear()\n\n#Write Password in password TextBox\nmydriver.find_element_by_xpath(xpaths[\'passwordTxtBox\']).send_keys(password)\n\n#Click Login button\nmydriver.find_element_by_xpath(xpaths[\'submitButton\']).click()\n\nThere is an another way that you can find xpath of any object -\n\nInstall Firebug and Firepath addons in firefox\nOpen URL in Firefox\nPress F12 to open Firepath developer instance \nSelect Firepath in below browser pane and chose select by ""xpath"" \nMove cursor of the mouse to element on webpage\nin the xpath textbox you will get xpath of an object/element.\nCopy Paste xpath to the script.\n\nRun script -\npython Login.py\n\nYou can also use a CSS selector instead of xpath. CSS selectors are slightly faster than xpath in most cases, and are usually preferred over xpath (if there isn\'t an ID attribute on the elements you\'re interacting with).\nFirepath can also capture the object\'s locator as a CSS selector if you move your cursor to the object. You\'ll have to update your code to use the equivalent find by CSS selector method instead -\nfind_element_by_css_selector(css_selector) \n\n', ""\nThere are a lot of sources for selenium - here is good one for simple use Selenium, and here is a example snippet too Selenium Examples\nYou can find a lot of good sources to use selenium, it's not too hard to get it set up and start using it.\n"", '\nYou just need to get selenium package imported, that you can do from command prompt using the command\npip install selenium\n\nWhen you have to use it in any IDE just import this package, no other documentation required to be imported\nFor Eg :\nimport selenium \nprint(selenium.__filepath__)\n\nThis is just a general command you may use in starting to check the filepath of selenium\n']",https://stackoverflow.com/questions/17540971/how-to-use-selenium-with-python,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to put the WebBrowser control into IE9 into standards?,"
i am using automation (i.e. COM automation) to display some HTML in Internet Explorer (9):
ie = CoInternetExplorer.Create;
ie.Navigate2(""about:blank"");
webDocument = ie.Document;
webDocument.Write(szSourceHTML);
webDocument.Close();
ie.Visible = True;

Internet Explorer appears, showing my html, which starts off as:
<!DOCTYPE html>
<HTML>
<HEAD>
   ...


Note: the html5 standards-mode opt-in doctype html

Except that the document is not in ie9 standards mode; it's in ie8 standards mode:


If i save the html to my computer first:

and then view that html document, IE is put into standards mode:

My question is how update my SpawnIEWithSource(String html) function to throw the browser into standards mode?
void SpawnIEWithSource(String html)
{
   Variant ie = CoInternetExplorer.Create();
   ie.Navigate2(""about:blank"");
   webDocument = ie.Document;
   webDocument.Write(html);
   webDocument.Close();
   ie.Visible = true;
}


Edit: A more verbose, less understandable or readable code sample, that doesn't help further the question might be:
IWebBrowser2 ie;
CoCreateInstance(CLASS_InternetExplorer, null, CLSCTX_INPROC_SERVER | CLSCTX_LOCAL_SERVER, IID_WebBrowser2, ie);
ie.AddRef();
ie.Navigate2(""about:blank"");

IHtmlDocument doc;
dispDoc = ie.Document;
dispDoc.AddRef();
dispDoc.QueryInterface(IHTMLDocument2, doc);
dispDoc.Release()
doc.Write(html); 
doc.Close();
doc.Release();
ie.Visible = true;
ie.Release();


Update
Commenter asked on the ieblog entry Testing sites with Browser Mode vs. Doc Mode:

Can we get a description of how the document mode is determined when the HTML content is within an embedded webcontrol? Seems to be that the document mode is choosen differently - maybe for compatibility reasons?

MarkSil [MSFT] responded:

@Thomas: Thanks for raising that question. The WebBrowser Control determines the doc mode the same way that IE does because it contains the same web platform (e.g. there is one shared mshtml.dll across IE and WebBrowser Control hosts). The WebBrowser Control does default to the Compatibility View browser mode, which means that the default doc mode is IE7. Here is a blog post with more detail on this: blogs.msdn.com/.../more-ie8-extensibility-improvements.aspx.

To which Thomas responded:

@MarcSil (re: WebBrowser Control)
The problem with using registry entries to select document mode for WebControl is that it applies to the application as a whole. I write plugins for Google SketchUp where you have WebDialog windows to create UIs - it's just a WebBrowser control in a window. But that leads to problems as I want to force a document mode for my instance of the WebBrowser control, not for all of SU's WebBrowser controls as a whole.
So, my question is: how do you control the document mode per instance for a WebBrowser control?

",29k,"
            27
        ","['\nHave you tried setting in your html the\n<meta http-equiv=""X-UA-Compatible"" content=""IE=9"" />\n\nor\n<meta http-equiv=""X-UA-Compatible"" content=""IE=edge"" />\n\nwhich means latest version\n', '\nThe IE9 ""version"" of the WebBrowser control, like the IE8 version, is actually several browsers in one. Unlike the IE8 version, you do have a little more control over the rendering mode inside the page by changing the doctype. Of course, to change the browser mode you have to set your registry like the earlier answer. Here is the location of FEATURE_BROWSER_EMULATION:\nHKEY_LOCAL_MACHINE (or HKEY_CURRENT_USER)\n     SOFTWARE\n          Microsoft\n               Internet Explorer\n                    Main\n                         FeatureControl\n                              FEATURE_BROWSER_EMULATION\n                                   contoso.exe = (DWORD) 000090000\n\nHere is the complete set of codes:\n\n9999 (0x270F) - Internet Explorer 9.\nWebpages are displayed in IE9\nStandards mode, regardless of the\n!DOCTYPE directive. \n9000 (0x2328) - Internet Explorer 9. Webpages containing standards-based !DOCTYPE\ndirectives are displayed in IE9 mode.   \n8888 (0x22B8) -Webpages are\ndisplayed in IE8 Standards mode,\nregardless of the !DOCTYPE directive.\n8000 (0x1F40) - Webpages containing\nstandards-based !DOCTYPE directives\nare displayed in IE8 mode.\n7000 (0x1B58) - Webpages containing\nstandards-based !DOCTYPE directives\nare displayed in IE7 Standards mode.\n\nThe full docs:\nhttp://msdn.microsoft.com/en-us/library/ee330730%28VS.85%29.aspx#browser_emulation\n', '\nFEATURE_BROWSER_EMULATION does not works with CoInternetSetFeatureEnabled. The documentation of INTERNETFEATURELIST is not updated since IE7.\nSince the feature setting is under HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Internet Explorer\\Main\\FeatureControl you may be able to override the value in your process via a registry API hook. \n']",https://stackoverflow.com/questions/4097593/how-to-put-the-webbrowser-control-into-ie9-into-standards,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"C# WebBrowser Control - Form Submit Not Working using InvokeMember(""Click"")","
I am working on automated testing script and am using the WebBrowser control. I am trying to submit the following HTML and testing when the user accepts the terms of service:
    <form action=""http://post.dev.dealerconnextion/k/6hRbDTwn4xGVl2MHITQsBw/hrshq"" method=""post"">
        <input name=""StepCheck"" value=""U2FsdGVkX18zMTk5MzE5OUgFyFgD3V5yf5Rwbtfhf3gjdH4KSx4hqj4vkrw7K6e-"" type=""hidden"">
        <button type=""submit"" name=""continue"" value=""y"">ACCEPT the terms of use</button>
        <button type=""submit"" name=""continue"" value=""n"">DECLINE the terms of use</button>
    </form>

    // Terms of Use Information

    <form action=""http://post.dev.dealerconnextion/k/6hRbDTwn4xGVl2MHITQsBw/hrshq"" method=""post"">
        <input name=""StepCheck"" value=""U2FsdGVkX18zMTk5MzE5OUgFyFgD3V5yf5Rwbtfhf3gjdH4KSx4hqj4vkrw7K6e-"" type=""hidden"">
        <button type=""submit"" name=""continue"" value=""y"">ACCEPT the terms of use</button>
        <button type=""submit"" name=""continue"" value=""n"">DECLINE the terms of use</button>
    </form>

Here is the code in C#, but does not submit the form.
            HtmlElementCollection el = webBrowser.Document.GetElementsByTagName(""button"");
            foreach (HtmlElement btn in el)
            {
                if (btn.InnerText == ""ACCEPT the terms of use"")
                {
                    btn.InvokeMember(""Click"");
                }
            }

Any help would be much appreciated. Thanks.
",16k,"
            5
        ","['\nThe following code works for me, using the live form action URL from the question comments, tested with IE10. Try it as is. If it works for you as well, feel free to use it as a template for your web automation tasks. A couple of points:\n\nFEATURE_BROWSER_EMULATION is used to make sure the WebBrowser behaves in the same way as standalone IE browser (or as close as possible). This is a must for almost any WebBrowser-based project. I believe that\'s what should help to solve the original problem on your side.  \nAsynchronous code is used to improve the automation logic reliability, add support timeouts and cancellation and promote natural linear code flow (using async/await).\n\nC#:\nusing Microsoft.Win32;\nusing System;\nusing System.Diagnostics;\nusing System.Linq;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing System.Windows.Forms;\n\nnamespace WebAutomation\n{\n    // http://stackoverflow.com/q/19044659/1768303\n\n    public partial class MainForm : Form\n    {\n        WebBrowser webBrowser;\n\n        // non-deterministic delay to let AJAX code run\n        const int AJAX_DELAY = 1000;\n\n        // keep track of the main automation task\n        CancellationTokenSource mainCts;\n        Task mainTask = null;\n\n        public MainForm()\n        {\n            SetBrowserFeatureControl(); // set FEATURE_BROWSER_EMULATION first\n\n            InitializeComponent();\n\n            InitBrowser();\n\n            this.Load += (s, e) =>\n            {\n                // start the automation when form is loaded\n                // timeout the whole automation task in 30s\n                mainCts = new CancellationTokenSource(30000);\n                mainTask = DoAutomationAsync(mainCts.Token).ContinueWith((completedTask) =>\n                {\n                    Trace.WriteLine(String.Format(""Automation task status: {0}"", completedTask.Status.ToString()));\n                }, TaskScheduler.FromCurrentSynchronizationContext());\n            };\n\n            this.FormClosing += (s, e) =>\n            {\n                // cancel the automation if form closes\n                if (this.mainTask != null && !this.mainTask.IsCompleted)\n                    mainCts.Cancel();\n            };\n        }\n\n        // create a WebBrowser instance (could use an existing one)\n        void InitBrowser()\n        {\n            this.webBrowser = new WebBrowser();\n            this.webBrowser.Dock = DockStyle.Fill;\n            this.Controls.Add(this.webBrowser);\n            this.webBrowser.Visible = true;\n        }\n\n        // the main automation logic\n        async Task DoAutomationAsync(CancellationToken ct)\n        {\n            await NavigateAsync(ct, () => this.webBrowser.Navigate(""http://localhost:81/test.html""), 10000); // timeout in 10s\n            // page loaded, log the page\'s HTML\n            Trace.WriteLine(GetBrowserDocumentHtml());\n\n            // do the DOM automation\n            HtmlElementCollection all = webBrowser.Document.GetElementsByTagName(""button"");\n            // throw if none or more than one element found\n            HtmlElement btn = all.Cast<HtmlElement>().Single(\n                el => el.InnerHtml == ""ACCEPT the terms of use"");\n\n            ct.ThrowIfCancellationRequested();\n\n            // simulate a click which causes navigation\n            await NavigateAsync(ct, () => btn.InvokeMember(""click""), 10000); // timeout in 10s\n\n            // form submitted and new page loaded, log the page\'s HTML\n            Trace.WriteLine(GetBrowserDocumentHtml());\n\n            // could continue with another NavigateAsync\n            // othrwise, the automation session completed\n        }\n\n        // Get the full HTML content of the document\n        string GetBrowserDocumentHtml()\n        {\n            return this.webBrowser.Document.GetElementsByTagName(""html"")[0].OuterHtml;\n        }\n\n        // Async navigation\n        async Task NavigateAsync(CancellationToken ct, Action startNavigation, int timeout = Timeout.Infinite)\n        {\n            var onloadTcs = new TaskCompletionSource<bool>();\n            EventHandler onloadEventHandler = null;\n\n            WebBrowserDocumentCompletedEventHandler documentCompletedHandler = delegate\n            {\n                // DocumentCompleted may be called several time for the same page,\n                // beacuse of frames\n                if (onloadEventHandler != null || onloadTcs == null || onloadTcs.Task.IsCompleted)\n                    return;\n\n                // handle DOM onload event to make sure the document is fully loaded\n                onloadEventHandler = (s, e) =>\n                    onloadTcs.TrySetResult(true);\n                this.webBrowser.Document.Window.AttachEventHandler(""onload"", onloadEventHandler);\n            };\n\n            using (var cts = CancellationTokenSource.CreateLinkedTokenSource(ct))\n            {\n                if (timeout != Timeout.Infinite)\n                    cts.CancelAfter(Timeout.Infinite);\n\n                using (cts.Token.Register(() => onloadTcs.TrySetCanceled(), useSynchronizationContext: true)) \n                {\n                    this.webBrowser.DocumentCompleted += documentCompletedHandler;\n                    try \n                    {\n                        startNavigation();\n                        // wait for DOM onload, throw if cancelled\n                        await onloadTcs.Task;\n                        ct.ThrowIfCancellationRequested();\n                        // let AJAX code run, throw if cancelled\n                        await Task.Delay(AJAX_DELAY, ct);\n                    }\n                    finally \n                    {\n                        this.webBrowser.DocumentCompleted -= documentCompletedHandler;\n                        if (onloadEventHandler != null)\n                            this.webBrowser.Document.Window.DetachEventHandler(""onload"", onloadEventHandler);\n                    }\n                }\n            }\n        }\n\n        // Browser feature conntrol\n        void SetBrowserFeatureControl()\n        {\n            // http://msdn.microsoft.com/en-us/library/ee330720(v=vs.85).aspx\n\n            // FeatureControl settings are per-process\n            var fileName = System.IO.Path.GetFileName(Process.GetCurrentProcess().MainModule.FileName);\n\n            // make the control is not running inside Visual Studio Designer\n            if (String.Compare(fileName, ""devenv.exe"", true) == 0 || String.Compare(fileName, ""XDesProc.exe"", true) == 0)\n                return;\n\n            SetBrowserFeatureControlKey(""FEATURE_BROWSER_EMULATION"", fileName, GetBrowserEmulationMode()); // Webpages containing standards-based !DOCTYPE directives are displayed in IE10 Standards mode.\n        }\n\n        void SetBrowserFeatureControlKey(string feature, string appName, uint value)\n        {\n            using (var key = Registry.CurrentUser.CreateSubKey(\n                String.Concat(@""Software\\Microsoft\\Internet Explorer\\Main\\FeatureControl\\"", feature),\n                RegistryKeyPermissionCheck.ReadWriteSubTree))\n            {\n                key.SetValue(appName, (UInt32)value, RegistryValueKind.DWord);\n            }\n        }\n\n        UInt32 GetBrowserEmulationMode()\n        {\n            int browserVersion = 7;\n            using (var ieKey = Registry.LocalMachine.OpenSubKey(@""SOFTWARE\\Microsoft\\Internet Explorer"",\n                RegistryKeyPermissionCheck.ReadSubTree,\n                System.Security.AccessControl.RegistryRights.QueryValues))\n            {\n                var version = ieKey.GetValue(""svcVersion"");\n                if (null == version)\n                {\n                    version = ieKey.GetValue(""Version"");\n                    if (null == version)\n                        throw new ApplicationException(""Microsoft Internet Explorer is required!"");\n                }\n                int.TryParse(version.ToString().Split(\'.\')[0], out browserVersion);\n            }\n\n            UInt32 mode = 10000; // Internet Explorer 10. Webpages containing standards-based !DOCTYPE directives are displayed in IE10 Standards mode. Default value for Internet Explorer 10.\n            switch (browserVersion)\n            {\n                case 7:\n                    mode = 7000; // Webpages containing standards-based !DOCTYPE directives are displayed in IE7 Standards mode. Default value for applications hosting the WebBrowser Control.\n                    break;\n                case 8:\n                    mode = 8000; // Webpages containing standards-based !DOCTYPE directives are displayed in IE8 mode. Default value for Internet Explorer 8\n                    break;\n                case 9:\n                    mode = 9000; // Internet Explorer 9. Webpages containing standards-based !DOCTYPE directives are displayed in IE9 mode. Default value for Internet Explorer 9.\n                    break;\n                default:\n                    // use IE10 mode by default\n                    break;\n            }\n\n            return mode;\n        }\n    }\n}\n\nThe content of http://localhost:81/test.html:\n<!DOCTYPE html>\n<head>\n<meta http-equiv=""X-UA-Compatible"" content=""IE=edge""/>\n</head>\n<body>\n    <form action=""<the URL from OP\'s comments>"" method=""post"">\n        <input name=""StepCheck"" value=""U2FsdGVkX18zMTk5MzE5OUgFyFgD3V5yf5Rwbtfhf3gjdH4KSx4hqj4vkrw7K6e-"" type=""hidden"">\n        <button type=""submit"" name=""continue"" value=""y"">ACCEPT the terms of use</button>\n        <button type=""submit"" name=""continue"" value=""n"">DECLINE the terms of use</button>\n    </form>\n</body>\n\n', '\nThis works for me as follow. may that would be useful for someone.\nFirst I create an event handler for the button element when got focus. Once all the other form element are filled up with the appropriate values, You should give the focus to the button as follow:\nHtmlElement xUsername = xDoc.GetElementById(""username_txt"");\nHtmlElement xPassword = xDoc.GetElementById(""password_txt"");\nHtmlElement btnSubmit = xDoc.GetElementById(""btnSubmit"");\nif (xUsername != null && xPassword != null && btnSubmit != null)\n{\n    xUsername.SetAttribute(""value"", ""testUserName"");\n    xPassword.SetAttribute(""value"", ""123456789"");\n    btnSubmit.GotFocus += BtnSubmit_GotFocus;\n    btnSubmit.Focus();\n}\n\nThen event handler implementation would be like this:\nprivate void BtnSubmit_GotFocus(object sender, HtmlElementEventArgs e)\n{\n    var btnSubmit = sender as HtmlElement;\n    btnSubmit.RaiseEvent(""onclick"");\n    btnSubmit.InvokeMember(""click"");\n}\n\n', '\nIn my case I also couldn\'t get element clicked by simply invoking Click method of the found element.\nWhat worked is similar solution that Ali Tabandeh listed in his answer above:\n\nfind the needed html element\ndefine proper GotFocus event handler\nthen invoke Focus method of the found element.\n\nEvent handler for GotFocus should\n\nRaiseEvent ""onclick""\nInvoke ""click"" method\n\nThe problem was that this worked in my case from 3rd time (3 times needed to call htmlelement.Focus()).\n']",https://stackoverflow.com/questions/19044659/c-sharp-webbrowser-control-form-submit-not-working-using-invokememberclick,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
webdriver.FirefoxProfile(): Is it possible to use a profile without making a copy of it?,"
As the documentation states, you can call webdriver.FirefoxProfile() with the optional argument of profile_directory to point to the directory of a specific profile you want the browser to use. I noticed it was taking a long time to run this command, so when I looked into the code, it looked like it was copying the specified profile Problem is, it takes an extremely long time for the profile to copy (something like >30 minutes, didn't have the patience to wait for it to finish.)
I'm using a hybrid of userscripts and selenium to do some automation for me, so to setup a new profile every single time I want to test out my code would be burdensome.
Is the only way to change this behaviour to edit the firefox_profile.py itself (if so, what would be the best way to go about it?)?
",10k,"
            6
        ","['\nAs per the current implementation of GeckoDriver with Firefox using the FirefoxProfile() works as follows :\n\nIf case of initiating a Browsing Session through a new Firefox Profile as follows :\nfrom selenium import webdriver\n\nmyprofile = webdriver.FirefoxProfile()\ndriver = webdriver.Firefox(firefox_profile=myprofile, executable_path=r\'C:\\Utility\\BrowserDrivers\\geckodriver.exe\')\ndriver.get(\'https://www.google.co.in\')\nprint(""Page Title is : %s"" %driver.title)\ndriver.quit()\n\nA new rust_mozprofile gets created on the run as follows :\n1521446301607   mozrunner::runner   INFO    Running command: ""C:\\\\Program Files\\\\Mozilla Firefox\\\\firefox.exe"" ""-marionette"" ""-profile"" ""C:\\\\Users\\\\ATECHM~1\\\\AppData\\\\Local\\\\Temp\\\\rust_mozprofile.xFayqKkZrOB8""\n\nOf-coarse on a successful closure (i.e. successful invocation of driver.quit()) the temporary rust_mozprofile.xFayqKkZrOB8 gets deleted/destroyed completely.\nAgain in case of initiating a Browsing Session through an existing Firefox Profile() as follows :\nfrom selenium import webdriver\n\nmyprofile = webdriver.FirefoxProfile(r\'C:\\Users\\AtechM_03\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\moskcpdq.SeleniumTest\')\ndriver = webdriver.Firefox(firefox_profile=myprofile, executable_path=r\'C:\\Utility\\BrowserDrivers\\geckodriver.exe\')\ndriver.get(\'https://www.google.co.in\')\nprint(""Page Title is : %s"" %driver.title)\ndriver.quit()\n\nSimilarly a new rust_mozprofile gets created on the run as follows :\n1521447102321   mozrunner::runner   INFO    Running command: ""C:\\\\Program Files\\\\Mozilla Firefox\\\\firefox.exe"" ""-marionette"" ""-profile"" ""C:\\\\Users\\\\ATECHM~1\\\\AppData\\\\Local\\\\Temp\\\\rust_mozprofile.2oSwrQwQoby9""\n\nSimilarly in this case as well on a successful closure (i.e. successful invocation of driver.quit()) the temporary rust_mozprofile.2oSwrQwQoby9 gets deleted/destroyed completely.\nSo the timespan you are observing is the time needed for a FirefoxProfile() to scoop out a new rust_mozprofile.\n\nPerhaps as per your question timespan for profile to copy (something like >30 minutes) is a pure overhead. So it won\'t be possible to use a Firefox Profile without making a copy of rust_mozprofile.\n\nSolution\n\nUpgrade Selenium Client to  current levels Version 3.11.0.\nUpgrade GeckoDriver to  current GeckoDriver v0.20.0 level.\nUpgrade Firefox version to Firefox Quantum v59.0.1 levels.\nClean your Project Workspace through your IDE and Rebuild your project with required dependencies only.\nUse CCleaner tool to wipe off all the OS chores before and after the execution of your test Suite.\nIf your base Firefox base version is too old, then uninstall it through Revo Uninstaller and install a recent GA and released version of Firefox Quantum.\nExecute your @Test.\n\n']",https://stackoverflow.com/questions/49356081/webdriver-firefoxprofile-is-it-possible-to-use-a-profile-without-making-a-cop,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Matlab: Running an m-file from command-line,"
Suppose that;
I have an m-file at location:
C:\M1\M2\M3\mfile.m
And exe file of the matlab is at this location:
C:\E1\E2\E3\matlab.exe
I want to run this m-file with Matlab, from command-line, for example inside a .bat file. How can I do this, is there a way to do it?
",214k,"
            124
        ","['\nA command like this runs the m-file successfully:\n""C:\\<a long path here>\\matlab.exe"" -nodisplay -nosplash -nodesktop -r ""run(\'C:\\<a long path here>\\mfile.m\'); exit;""\n', '\nI think that one important point that was not mentioned in the previous answers is that, if not explicitly indicated, the matlab interpreter will remain open.\nTherefore, to the answer of @hkBattousai I will add the exit command:\n""C:\\<a long path here>\\matlab.exe"" -nodisplay -nosplash -nodesktop -r ""run(\'C:\\<a long path here>\\mfile.m\');exit;""\n', '\nHere is what I would use instead, to gracefully handle errors from the script:\n""C:\\<a long path here>\\matlab.exe"" -nodisplay -nosplash -nodesktop -r ""try, run(\'C:\\<a long path here>\\mfile.m\'), catch, exit, end, exit""\n\nIf you want more verbosity:\n""C:\\<a long path here>\\matlab.exe"" -nodisplay -nosplash -nodesktop -r ""try, run(\'C:\\<a long path here>\\mfile.m\'), catch me, fprintf(\'%s / %s\\n\',me.identifier,me.message), end, exit""\n\nI found the original reference here. Since original link is now gone, here is the link to an alternate newreader still alive today:\n\nexit matlab when running batch m file\n\n', '\nOn Linux you can do the same and you can actually send back to the shell a custom error code, like the following:\n#!/bin/bash\nmatlab -nodisplay -nojvm -nosplash -nodesktop -r \\ \n      ""try, run(\'/foo/bar/my_script.m\'), catch, exit(1), end, exit(0);""\necho ""matlab exit code: $?""\n\nit prints matlab exit code: 1 if the script throws an exception, matlab exit code: 0 otherwise.\n', '\nHere are the steps:\n\nStart the command line.\nEnter the folder containing the .m file with cd C:\\M1\\M2\\M3\nRun the following: C:\\E1\\E2\\E3\\matlab.exe -r mfile\n\nWindows systems will use your current folder as the location for MATLAB to search for .m files, and the -r option tries to start the given .m file as soon as startup occurs.\n', '\nSince R2019b, there is a new command line option, -batch. It replaces -r, which is no longer recommended. It also unifies the syntax across platforms. See for example the documentation for Windows, for the other platforms the description is identical.\nmatlab -batch ""statement to run""\n\nThis starts MATLAB without the desktop or splash screen, logs all output to stdout and stderr, exits automatically when the statement completes, and provides an exit code reporting success or error.\nIt is thus no longer necessary to use try/catch around the code to run, and it is no longer necessary to add an exit statement.\n', '\ncat 1.m | matlab -nodesktop -nosplash\n\nAnd I use Ubuntu\n', ""\nThanks to malat. Your comment helped me. \nBut I want to add my try-catch block, as I found the MExeption method getReport() that returns the whole error message and prints it to the matlab console.\nAdditionally I printed the filename as this compilation is part of a batch script that calls matlab.\ntry\n    some_code\n    ...\ncatch message\n    display(['ERROR in file: ' message.stack.file])\n    display(['ERROR: ' getReport(message)])\nend;\n\nFor a false model name passed to legacy code generation method, the output would look like:\nERROR in file: C:\\..\\..\\..\nERROR: Undefined function or variable 'modelname'.\n\nError in sub-m-file (line 63)\nlegacy_code( 'slblock_generate', specs, modelname);\n\nError in m-file (line 11)\nsub-m-file\n\nError in run (line 63)\nevalin('caller', [script ';']);\n\nFinally, to display the output at the windows command prompt window, just log the matlab console to a file with -logfile logfile.txt (use additionally -wait) and call the batch command type logfile.txt\n"", '\nI run this command within a bash script, in particular to submit SGE jobs and batch process things:\n/Path_to_matlab -nodisplay -nosplash -nodesktop < m_file.m\n\n', ""\nSince none of the answers has information about feeding input argument, it is important to \nadd it here. After some research, I found this link\nFeeding the arguments is very similar to how we run a Matlab function. \nmatlab -r 'try myfunction(argument1,argument2); catch; end; quit'\n\nIf you are somehow getting an argument from bash/terminal, you simply need to insert that into the bash command as: \nmatlab -r 'try myfunction($MY_BASH_ARG,argument2); catch; end; quit'\n\n(This is after a couple of trial and error)\n""]",https://stackoverflow.com/questions/6657005/matlab-running-an-m-file-from-command-line,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handling Browser Authentication using Selenium,"
Does anyone know about handling Browser Authentication using Selenium or any other tool during automation? 
",87k,"
            19
        ","[""\nEDIT in 2015:\nThis answer is outdated. WebDriver nowadays supports authentication! See How to handle authentication popup with Selenium WebDriver using Java\n\nOriginal answer:\nThis is not handled very well by Selenium.\n\nYou can try using http://username:password@example.com/yourpage\ninstead of just http://example.com/yourpage\nHowever, as far as I know, Firefox will still pop up a browser dialog requesting a confirmation.\n\nYou can try Robot if you're using Java (or any similar tool like AutoIt).\n\nYou could use driver.manage().addCookie() if you're using WebDriver.\n\nOr a custom FirefoxProfile that has already passed the authentication once.\n"", ""\nI spent days on this - literally.\nTrying to get past browser level authentication within my company network to hit an application.\nThe solution was to use the 'unsername:password@' component within the URL, BUT to add a forward slash at the end of the login URL.\nSo total login URL looks like this (note the '/' after yourpage):\nhttp://username:password@example.com/yourpage/\nWorks with Watir, Capybara and Selenium Webdriver.\n"", '\nEverything I have read on the Web didn\'t help me. So before making a request, like this:\ndriver.get(url);\n\nyou have to run a new thread like this:\nRunScript runScript = new RunScript();\nrunScript.start();\n\nIn this case you are free to input login and password on another thread of follwing class\npublic class RunScript extends Thread {\n\n@Override\npublic void run() {\n    try {\n        File file = new File(""D:\\\\jacob-1.18-x86.dll"");\n        System.setProperty(LibraryLoader.JACOB_DLL_PATH, file.getAbsolutePath());\n        AutoItX autoIt = new AutoItX();\n        Thread.sleep(2000);\n        autoIt.winActivate(""yourWindowName"", """");\n        autoIt.winWaitActive(""yourWindowName"");\n        if (autoIt.winExists(""yourWindowName"")) {\n            autoIt.send(""username{TAB}"", false);\n            autoIt.send(""password{Enter}"", false);\n            }\n        }\n    } catch (InterruptedException ex) {\n        //\n    }\n}\n}\n\n', '\nAll the hacks via auto-it, sikuli, etc. just wasting your time when you\'ll run it in your CI solution, using several browser types / OS / Version / Resolutions etc.\nThe way to do it correctly is to identify the authentication actual method and perform a login using Rest protocol for instance.\nI used it to get the JSESIONID cookie and insert it to the selenium driver.\nhint on that: go to a non-exiting url of the domian first, then set the cookie, then go to the required url - you are logged-in.\nuse: rest client authentication to get the JSESSION ID \nand With this information:\nbrowser().navigate(foo.getUrl()+""non-exiting-url"");\n\n//the information got from the rest client login:\nCookie cookie = new Cookie(name, value, domain, path, expiry, isSecure, isHttpOnly);\n\ntry {\n    driver.manage().addCookie(cookie);\n} catch (Exception e) {\n    System.out.println(e.toString());\n}\n\nbrowser().navigate(foo.getUrl());\n\n', '\nyou can use auto IT script to handle this problem\nWinWaitActive(""[CLASS:Chrome_WidgetWin_1]"", """", time)\nSend(""user"")\nSend(""{TAB}"")\nSend(""pass"")\nSend(""{ENTER}"")\n\n', '\nwith Chrome 70 and other versions :\nhttp://username:password@example.com/yourpage\n\n', '\nYou can use Java Robot class with Selenium 2 /Selenium WebDriver using Firefox\nWebDriver driver = new FirefoxDriver();\n    driver.get(""http://localhost:9990"");\n\n    WebElement myDynamicElement = driver.findElement(By.id(""app""));\n\n    Alert alert = driver.switchTo().alert();\n\n\n    try {\n        Robot robot = new Robot();\n        alert.sendKeys(""username"");\n\n        robot.keyPress(KeyEvent.VK_TAB);//go to password feild\n\n        robot.keyPress(KeyEvent.VK_P);\n        robot.keyPress(KeyEvent.VK_A);\n        robot.keyPress(KeyEvent.VK_S);\n        robot.keyPress(KeyEvent.VK_S);\n\n        robot.keyPress(KeyEvent.VK_ENTER);\n\n\n        } catch (AWTException e) {\n        e.printStackTrace();\n        }\n\n    }\n\nUsing Selenium with Robot\nhttp://docs.oracle.com/javase/1.5.0/docs/api/java/awt/Robot.html\n']",https://stackoverflow.com/questions/10395462/handling-browser-authentication-using-selenium,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get the range of occupied cells in excel sheet,"
I use C# to automate an excel file. I was able to get the workbook and the sheets it contains.
If for example I have in sheet1 two cols and 5 rows. I wanted o get the range for the occupied cells as A1:B5. I tried the following code but it did not give the correct result.
the columns # and row # were much bigger and the cells were empty as well. 
     Excel.Range xlRange = excelWorksheet.UsedRange;
     int col = xlRange.Columns.Count;
     int row = xlRange.Rows.Count;

Is there another way I can use to get that range?
",117k,"
            35
        ","['\nI had a very similar issue as you had. What actually worked is this:\niTotalColumns = xlWorkSheet.UsedRange.Columns.Count;\niTotalRows = xlWorkSheet.UsedRange.Rows.Count;\n\n//These two lines do the magic.\nxlWorkSheet.Columns.ClearFormats();\nxlWorkSheet.Rows.ClearFormats();\n\niTotalColumns = xlWorkSheet.UsedRange.Columns.Count;\niTotalRows = xlWorkSheet.UsedRange.Rows.Count;\n\nIMHO what happens is that when you delete data from Excel, it keeps on thinking that there is data in those cells, though they are blank. When I cleared the formats, it removes the blank cells and hence returns actual counts.\n', '\nExcel.Range last = sheet.Cells.SpecialCells(Excel.XlCellType.xlCellTypeLastCell, Type.Missing);\nExcel.Range range = sheet.get_Range(""A1"", last);\n\n""range"" will now be the occupied cell range\n', '\nSee the Range.SpecialCells method. For example, to get cells with constant values or formulas use:\n_xlWorksheet.UsedRange.SpecialCells(\n        Microsoft.Office.Interop.Excel.XlCellType.xlCellTypeConstants |\n        Microsoft.Office.Interop.Excel.XlCellType.xlCellTypeFormulas)\n\n', '\nThe only way I could get it to work in ALL scenarios (except Protected sheets) (based on Farham\'s Answer):\nIt supports:\n\nScanning Hidden Row / Columns\nIgnores formatted cells with no data / formula\n\nCode:\n// Unhide All Cells and clear formats\nsheet.Columns.ClearFormats();\nsheet.Rows.ClearFormats();\n\n// Detect Last used Row - Ignore cells that contains formulas that result in blank values\nint lastRowIgnoreFormulas = sheet.Cells.Find(\n                ""*"",\n                System.Reflection.Missing.Value,\n                InteropExcel.XlFindLookIn.xlValues,\n                InteropExcel.XlLookAt.xlWhole,\n                InteropExcel.XlSearchOrder.xlByRows,\n                InteropExcel.XlSearchDirection.xlPrevious,\n                false,\n                System.Reflection.Missing.Value,\n                System.Reflection.Missing.Value).Row;\n// Detect Last Used Column  - Ignore cells that contains formulas that result in blank values\nint lastColIgnoreFormulas = sheet.Cells.Find(\n                ""*"",\n                System.Reflection.Missing.Value,\n                System.Reflection.Missing.Value,\n                System.Reflection.Missing.Value,\n                InteropExcel.XlSearchOrder.xlByColumns,\n                InteropExcel.XlSearchDirection.xlPrevious,\n                false,\n                System.Reflection.Missing.Value,\n                System.Reflection.Missing.Value).Column;\n\n// Detect Last used Row / Column - Including cells that contains formulas that result in blank values\nint lastColIncludeFormulas = sheet.UsedRange.Columns.Count;\nint lastColIncludeFormulas = sheet.UsedRange.Rows.Count;\n\n', '\ndim lastRow as long   \'in VBA it\'s a long \nlastrow = wks.range(""A65000"").end(xlup).row\n\n', '\nBit old question now, but if somebody is looking for solution this works for me.\nusing Excel = Microsoft.Office.Interop.Excel;\n\nExcel.ApplicationClass excel = new Excel.ApplicationClass();\nExcel.Application app = excel.Application;\nExcel.Range all = app.get_Range(""A1:H10"", Type.Missing);\n\n', ""\nThese two lines on their own wasnt working for me:\nxlWorkSheet.Columns.ClearFormats();\nxlWorkSheet.Rows.ClearFormats();\n\nYou can test by hitting ctrl+end in the sheet and seeing which cell is selected. \nI found that adding this line after the first two solved the problem in all instances I've encountered:\nExcel.Range xlActiveRange = WorkSheet.UsedRange;\n\n"", '\nYou should try the currentRegion property, if you know from where you are to find the range. This will give you the boundaries of your used range.\n', '\nThis is tailored to finding formulas but you should be able to expand it to general content by altering how you test the starting cells.  You\'ll have to handle single cell ranges outside of this.\n    public static Range GetUsedPartOfRange(this Range range)\n    {\n        Excel.Range beginCell = range.Cells[1, 1];\n        Excel.Range endCell = range.Cells[range.Rows.Count, range.Columns.Count];\n\n        if (!beginCell.HasFormula)\n        {\n            var beginCellRow = range.Find(\n                ""*"",\n                beginCell,\n                XlFindLookIn.xlFormulas,\n                XlLookAt.xlPart,\n                XlSearchOrder.xlByRows,\n                XlSearchDirection.xlNext,\n                false);\n\n            var beginCellCol = range.Find(\n                ""*"",\n                beginCell,\n                XlFindLookIn.xlFormulas,\n                XlLookAt.xlPart,\n                XlSearchOrder.xlByColumns,\n                XlSearchDirection.xlNext,\n                false);\n\n            if (null == beginCellRow || null == beginCellCol)\n                return null;\n\n            beginCell = range.Worksheet.Cells[beginCellRow.Row, beginCellCol.Column];\n        }\n\n        if (!endCell.HasFormula)\n        {\n            var endCellRow = range.Find(\n            ""*"",\n            endCell,\n            XlFindLookIn.xlFormulas,\n            XlLookAt.xlPart,\n            XlSearchOrder.xlByRows,         \n            XlSearchDirection.xlPrevious,\n            false);\n\n            var endCellCol = range.Find(\n                ""*"",\n                endCell,\n                XlFindLookIn.xlFormulas,\n                XlLookAt.xlPart,\n                XlSearchOrder.xlByColumns,\n                XlSearchDirection.xlPrevious,\n                false);\n\n            if (null == endCellRow || null == endCellCol)\n                return null;\n\n            endCell = range.Worksheet.Cells[endCellRow.Row, endCellCol.Column];\n        }\n\n        if (null == endCell || null == beginCell)\n            return null;\n\n        Excel.Range finalRng = range.Worksheet.Range[beginCell, endCell];\n\n        return finalRng;\n    }\n}\n\n', '\nYou should not delete the data in box by pressing ""delete"", i think thats the problem , because excel will still detected the box as """" <- still have value, u should delete by right click the box and click delete.\n']",https://stackoverflow.com/questions/1284388/how-to-get-the-range-of-occupied-cells-in-excel-sheet,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selenium Webdriver + Java - Eclipse: java.lang.NoClassDefFoundError,"
I installed JDK from here: http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
(This version for windows x64: Java SE Development Kit 8u151)
I downloaded eclipse from here:
http://www.eclipse.org/downloads/packages/eclipse-ide-java-developers/oxygenr
(Windows 64-bit)
I opened a new project in eclipse: File->New->Java Project
Then I downloaded Selenium Java Jars from here:
http://www.seleniumhq.org/download/ ---> java language
Then in eclipse I click on my project -> properties ->Java Build Path -> Libraries tab -> Add External JARs... -> I go to ""SeleniumDrivers\Java"" library (there I saved all the JARS that I downloaded) -> I checked all the files there:
these files
I clicked on ""ok"" and created a new class in eclipse
Then I downloaded chromedriver from here: http://www.seleniumhq.org/download
I unzipped it and saved it here: C:\Selenium\Drivers
This is my script:
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.chrome.ChromeDriver;

public class MainClass {

    public static void main(String[] args) {
        // TODO Auto-generated method stub
        System.out.println(""hi there\n"");

        System.setProperty(""webdriver.chrome.driver"", 
        ""C:/Selenium/Drivers/chromedriver.exe"");
        WebDriver driver = new ChromeDriver();
        driver.get(""https://www.facebook.com"");
    }

}

As you can see, this is a very basic script which opens chrome browser and navigate to facebook.
I run this script and got this error:
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/http/config/RegistryBuilder
    at org.openqa.selenium.remote.internal.HttpClientFactory.getClientConnectionManager(HttpClientFactory.java:69)
    at org.openqa.selenium.remote.internal.HttpClientFactory.<init>(HttpClientFactory.java:57)
    at org.openqa.selenium.remote.internal.HttpClientFactory.<init>(HttpClientFactory.java:60)
    at org.openqa.selenium.remote.internal.ApacheHttpClient$Factory.getDefaultHttpClientFactory(ApacheHttpClient.java:242)
    at org.openqa.selenium.remote.internal.ApacheHttpClient$Factory.<init>(ApacheHttpClient.java:219)
    at org.openqa.selenium.remote.HttpCommandExecutor.getDefaultClientFactory(HttpCommandExecutor.java:93)
    at org.openqa.selenium.remote.HttpCommandExecutor.<init>(HttpCommandExecutor.java:72)
    at org.openqa.selenium.remote.service.DriverCommandExecutor.<init>(DriverCommandExecutor.java:63)
    at org.openqa.selenium.chrome.ChromeDriverCommandExecutor.<init>(ChromeDriverCommandExecutor.java:36)
    at org.openqa.selenium.chrome.ChromeDriver.<init>(ChromeDriver.java:181)
    at org.openqa.selenium.chrome.ChromeDriver.<init>(ChromeDriver.java:168)
    at org.openqa.selenium.chrome.ChromeDriver.<init>(ChromeDriver.java:123)
    at MainClass.main(MainClass.java:11)
Caused by: java.lang.ClassNotFoundException: org.apache.http.config.RegistryBuilder
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    ... 13 more

I don't know how to resolve this issue, can you please help to solve it so that I will be able to run my basic script?
",18k,"
            0
        ","[""\njava.lang.NoClassDefFoundError is observed when the JRE can't find a Class.\nIn simple words the required imports or jar files are not available. From the snapshot you have shared its pretty much evident that you have tried to add the Java Client related jars.\nIn this case you need to follow the following steps:\n\nRemove all the jars referring to previous versions of Selenium standalone server & Selenium Java client\nImport only the selenium-server-standalone-3.7.0.\nIn your IDE within Project menu, select the option Build Automatically and execute the Clean option for all of your Projects.\nExecute your Test.\n\n"", ""\nSeems like the latest (v3.7) Selenium-Java zip file contains lesser jars in the lib folder. v3.6 contained 10 lib jars but v3.7 contains only 7 jars. \nThe missing jar which is causing all the issue for the op is 'httpcore-4.4.6.jar'. I am not sure whether the removal of jar is intentional or not. Maybe chromedriver has catch up with Selenium java 3.7seeing that .\nI the mean time use Selenium Java 3.6. Don't forget to add the /lib folder as well.\nhttp://selenium-release.storage.googleapis.com/3.6/selenium-java-3.6.0.zip\n"", ""\ni've added the three missing jars from version 3.6 and fixed everything.\nhttp://selenium-release.storage.googleapis.com/3.6/selenium-java-3.6.0.zip\n"", '\nupdate the the appium java-client to 7.3.0 and selenium-java to 3.141.59 this resolved my issue hope it helps.\n', ""\nI faced the same issue. For me, it didn't found the WebDriver. It seemed to happen as I imported the libraries the location other than classpath. Then I opened up a new project, went to the Properties>Java Build Path>Libraries. This time I imported the libraries under classpath. Now it works fine.\n"", ""\nFor those who are using Appium java client with Selenium, don't try to import Java client and selenium dependencies together in your pom.xml, you have already selenium dependencies imported with the java client dependency, you only have to import Java client dependency in your pom.xml and it should work.\nReference: https://mvnrepository.com/artifact/io.appium/java-client/7.6.0\n"", ""\nFor me, the issue was solved by adding the external jars (selenium-java jar files) to 'Classpath', instead of to 'Modulepath'\n\n""]",https://stackoverflow.com/questions/47104058/selenium-webdriver-java-eclipse-java-lang-noclassdeffounderror,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to use ADB to send touch events to device using sendevent command?,"
I am trying to send touch events to a device using AndroidDebugBridge, so that I can do some basic automation for UI tests. I have followed the discussion in LINK. I am able to use sendevent to simulate touch on emulators, but unable to do the same on a device. 
Like in above link the emulator seems to send out 6 events for each touch ( xcoord, ycoord, 2 for press,2 for release) and it was easy to use this information to sendevents, but a getevent for the touchscreen for a device seems to generate far too many events.
Has somebody managed to send touch from ADB to a device? Could you please share the solution.
",202k,"
            104
        ","[""\nAndroid comes with an input command-line tool that can simulate miscellaneous input events. To simulate tapping, it's:\ninput tap x y\n\nYou can use the adb shell ( > 2.3.5) to run the command remotely:\nadb shell input tap x y\n\n"", '\nIn order to do a particular action (for example to open the web browser), you need to first figure out where to tap.  To do that, you can first run:\nadb shell getevent -l\n\nOnce you press on the device, at the location that you want, you will see this output:\n<...>\n/dev/input/event3: EV_KEY       BTN_TOUCH            DOWN\n/dev/input/event3: EV_ABS       ABS_MT_POSITION_X    000002f5\n/dev/input/event3: EV_ABS       ABS_MT_POSITION_Y    0000069e\n\nadb is telling you that a key was pressed (button down) at position 2f5, 69e in hex which is 757 and 1694 in decimal.\nIf you now want to generate the same event, you can use the input tap command at the same position:\nadb shell input tap 757 1694\n\nMore info can be found at:\nhttps://source.android.com/devices/input/touch-devices.html\nhttp://source.android.com/devices/input/getevent.html\n', '\n2.3.5 did not have input tap, just input keyevent and input text\nYou can use the monkeyrunner for it: (this is a copy of the answer at https://stackoverflow.com/a/18959385/1587329):\n\nYou might want to use monkeyrunner like this:\n\n$ monkeyrunner\n>>> from com.android.monkeyrunner import MonkeyRunner, MonkeyDevice\n>>> device = MonkeyRunner.waitForConnection()\n>>> device.touch(200, 400, MonkeyDevice.DOWN_AND_UP)\n\n\nYou can also do a drag, start activies etc.\n  Have a look at the api for MonkeyDevice.\n\n', ""\nYou don't need to use \n\nadb shell getevent -l\n\ncommand, you just need to enable in Developer Options on the device [Show Touch data] to get X and Y. \nSome more information can be found in my article here: https://mobileqablog.wordpress.com/2016/08/20/android-automatic-touchscreen-taps-adb-shell-input-touchscreen-tap/\n"", '\nBuilding on top of Tomas\'s answer, this is the best approach of finding the location tap position as an integer I found:\nadb shell getevent -l | grep ABS_MT_POSITION --line-buffered | awk \'{a = substr($0,54,8); sub(/^0+/, """", a); b = sprintf(""0x%s"",a); printf(""%d\\n"",strtonum(b))}\'\n\nUse adb shell getevent -l to get a list of events, the using grep for ABS_MT_POSITION (gets the line with touch events in hex) and finally use awk to get the relevant hex values, strip them of zeros and convert hex to integer. This continuously prints the x and y coordinates in the terminal only when you press on the device.\nYou can then use this adb shell command to send the command:\nadb shell input tap x y\n\n', ""\nConsider using Android's uiautomator, with adb shell uiautomator [...] or directly using the .jar that comes with the SDK.\n""]",https://stackoverflow.com/questions/3437686/how-to-use-adb-to-send-touch-events-to-device-using-sendevent-command,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automating running command on Linux from Windows using PuTTY,"
I have a scenario where I need to run a linux shell command frequently (with different filenames) from windows. I am using PuTTY and WinSCP to do that (requires login name and password).  The file is copied to a predefined folder in the linux machine through WinSCP and then the command is run from PuTTY. Is there a way by which I can automate this through a program. Ideally I would like to right click the file from windows and issue the command which would copy the file to remote machine and run the predefined command (in PuTTy) with the filename as argument.
",267k,"
            71
        ","['\nPutty usually comes with the ""plink"" utility.\nThis is essentially the ""ssh"" command line command implemented as a windows .exe.\nIt pretty well documented in the putty manual under ""Using the command line tool plink"".\nYou just need to wrap a command like:\nplink root@myserver /etc/backups/do-backup.sh\n\nin a .bat script.\nYou can also use common shell constructs, like semicolons to execute multiple commands. e.g:\nplink read@myhost ls -lrt /home/read/files;/etc/backups/do-backup.sh\n\n', '\nThere could be security issues with common methods for auto-login. \nOne of the most easiest ways is documented below:\n\nRunning Putty from the Windows Command Line\n\nAnd as for the part the executes the command\nIn putty UI, Connection>SSH>  there\'s a field for remote command.\n\n4.17 The SSH panel\nThe SSH panel allows you to configure\n  options that only apply to SSH\n  sessions.\n4.17.1 Executing a specific command on the server\nIn SSH, you don\'t have to run a\n  general shell session on the server.\n  Instead, you can choose to run a\n  single specific command (such as a\n  mail user agent, for example). If you\n  want to do this, enter the command in\n  the ""Remote command"" box.\n  http://the.earth.li/~sgtatham/putty/0.53/htmldoc/Chapter4.html\n\nin short, your answers might just as well be similar to the text below:  \n\nlet Putty run command in remote server\n\n', '\nYou can write a TCL script and establish SSH session to that Linux machine and issue commands automatically. Check http://wiki.tcl.tk/11542 for a short tutorial.\n', '\nYou can create a putty session, and auto load the script on the server, when starting the session:\nputty -load ""sessionName"" \n\nAt remote command, point to the remote script.\n', '\nYou can do both tasks (the upload and the command execution) using WinSCP. Use WinSCP script like:\noption batch abort\noption confirm off\nopen your_session\nput %1%\ncall script.sh\nexit\n\nReference for the call command:\nhttps://winscp.net/eng/docs/scriptcommand_call\nReference for the %1% syntax:\nhttps://winscp.net/eng/docs/scripting#syntax\nYou can then run the script like:\nwinscp.exe /console /script=script_path\\upload.txt /parameter file_to_upload.dat\n\nActually, you can put a shortcut to the above command to the Windows Explorer\'s Send To menu, so that you can then just right-click any file and go to the Send To > Upload using WinSCP and Execute Remote Command (=name of the shortcut).\nFor that, go to the folder %USERPROFILE%\\SendTo and create a shortcut with the following target:\nwinscp_path\\winscp.exe /console /script=script_path\\upload.txt /parameter %1\n\nSee Creating entry in Explorer\'s ""Send To"" menu.\n', '\nHere is a totally out of the box solution.\n\nInstall AutoHotKey (ahk)\nMap the script to a key (e.g. F9)\nIn the ahk script,\na) Ftp the commands (.ksh) file to the linux machine\nb) Use plink like below. Plink should be installed if you have putty.\n\n\nplink sessionname -l username -pw password test.ksh\n\nor\n\nplink -ssh example.com -l username -pw password test.ksh\n\nAll the steps will be performed in sequence whenever you press F9 in windows.\n', '\nCode:\nusing System;\nusing System.Diagnostics;\nnamespace playSound\n{\n    class Program\n    {\n        public static void Main(string[] args)\n        {\n            Console.WriteLine(args[0]);\n\n            Process amixerMediaProcess = new Process();\n            amixerMediaProcess.StartInfo.CreateNoWindow = false;\n            amixerMediaProcess.StartInfo.UseShellExecute = false;\n            amixerMediaProcess.StartInfo.ErrorDialog = false;\n            amixerMediaProcess.StartInfo.RedirectStandardOutput = false;\n            amixerMediaProcess.StartInfo.RedirectStandardInput = false;\n            amixerMediaProcess.StartInfo.RedirectStandardError = false;\n            amixerMediaProcess.EnableRaisingEvents = true;\n\n            amixerMediaProcess.StartInfo.Arguments = string.Format(""{0}"",""-ssh username@""+args[0]+"" -pw password -m commands.txt"");\n            amixerMediaProcess.StartInfo.FileName = ""plink.exe"";\n            amixerMediaProcess.Start();\n\n\n            Console.Write(""Presskey to continue . . . "");\n            Console.ReadKey(true);\n    }\n}\n\n}\nSample commands.txt:\nps\nLink: https://huseyincakir.wordpress.com/2015/08/27/send-commands-to-a-remote-device-over-puttyssh-putty-send-command-from-command-line/\n', '\nTry MtPutty,\nyou can automate the ssh login in it. Its a great tool especially if you need to login to multiple servers many times. Try it here\nAnother tool worth trying is TeraTerm. Its really easy to use for the ssh automation stuff. You can get it here. But my favorite one is always MtPutty.\n', '\nIn case you are using Key based authentication, using saved Putty session seems to work great, for example to run a shell script on a remote server(In my case an ec2).Saved configuration will take care of authentication.\nC:\\Users> plink saved_putty_session_name path_to_shell_file/filename.sh\nPlease remember if you save your session with name like(user@hostname), this command would not work as it will be treated as part of the remote command.\n']",https://stackoverflow.com/questions/6147203/automating-running-command-on-linux-from-windows-using-putty,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disposing of Microsoft.Office.Interop.Word.Application,"
(Somewhat of a follow on from the post (which remains unanswered): https://stackoverflow.com/q/6197829/314661)
Using the following code
Application app = new Application();
_Document doc = app.Documents.Open(""myDocPath.docx"", false, false, false);
doc.PrintOut(false);
doc.Close();

I am attempting to open and print a file programmatically.
The problem is each time I run the above code a new WINWORD.exe process is started and obviously this quickly eats up all the memory.
The application class doesn't seem to contain a dispose/close or similar method.
After a bit of research I (realized) and changed the code to the following.
 Application app = new Application();
 _Document doc = app.Documents.Open(fullFilePath + "".doc"", false, false, false);
 doc.PrintOut(false);
 doc.Close();
 int res = System.Runtime.InteropServices.Marshal.ReleaseComObject(doc);
 int res1 = System.Runtime.InteropServices.Marshal.ReleaseComObject(app);

And I can see the remaining reference count is zero but the processes remain? 
PS: I'm using Version 14 of the Microsoft.Office.Interop library.
",77k,"
            42
        ","['\nDo you not need to call Quit?\napp.Quit();\n\n', '\nPerhaps try setting doc = null and calling GC.Collect()\nEdit, not really my own code I forget where I got it but this is what I use to dispose of Excel, and it does the job maybe you can glean something from this:\npublic void DisposeExcelInstance()\n{\n    app.DisplayAlerts = false;\n    workBook.Close(null, null, null);\n    app.Workbooks.Close();\n    app.Quit();\n    if (workSheet != null)\n        System.Runtime.InteropServices.Marshal.ReleaseComObject(workSheet);\n    if (workBook != null)\n        System.Runtime.InteropServices.Marshal.ReleaseComObject(workBook);\n    if (app != null)\n        System.Runtime.InteropServices.Marshal.ReleaseComObject(app);\n    workSheet = null;\n    workBook = null;\n    app = null;\n    GC.Collect(); // force final cleanup!\n}\n\n', '\nI think the main issue, which nobody seems to have picked up on, is that you shouldn\'t be creating a new Application object in the first place if Word is already open.\nThose of us who have been coding since the days of COM and/or VB6 will remember GetActiveObject. Fortunately .Net only requires a ProgID.\nThe recommended way of doing this is as follows:\ntry\n{\n    wordApp = (word.Application) Marshal.GetActiveObject(""Word.Application"");\n}\ncatch(COMException ex) when (ex.HResult == -2147221021)\n{\n    wordApp = new word.Application();\n}\n\n', '\nThe best solution.. last:\ntry {\n\n    Microsoft.Office.Interop.Word.Application appWord = new Microsoft.Office.Interop.Word.Application();\n    appWord.Visible = false;\n    Microsoft.Office.Interop.Word.Document doc = null;\n    wordDocument = appWord.Documents.Open((INP), ReadOnly: true);\n\n    wordDocument.ExportAsFixedFormat(OUTP, Microsoft.Office.Interop.Word.WdExportFormat.wdExportFormatPDF);\n\n    // doc.Close(false); // Close the Word Document.\n    appWord.Quit(false); // Close Word Application.\n} catch (Exception ex) {\n    Console.WriteLine(ex.Message + ""     "" + ex.InnerException);\n}\n\n', '\nYou need to calls app.Quit() to close the application. I used below code & it worked like a charm for me - \ntry\n{\n   Microsoft.Office.Interop.Word.Application wordApp = new Microsoft.Office.Interop.Word.Application();\n   wordApp.Visible = false;\n   Microsoft.Office.Interop.Word.Document doc = null;\n\n   //Your code here...\n\n   doc.Close(false); // Close the Word Document.\n   wordApp.Quit(false); // Close Word Application.\n}\ncatch (Exception ex)\n{\n   MessageBox.Show(ex.Message + ""     "" + ex.InnerException);\n}\nfinally\n{\n   // Release all Interop objects.\n   if (doc != null)\n      System.Runtime.InteropServices.Marshal.ReleaseComObject(doc);\n   if (wordApp != null)\n      System.Runtime.InteropServices.Marshal.ReleaseComObject(wordApp);\n   doc = null;\n   wordApp = null;\n   GC.Collect();\n}\n\n', ""\nAgreed with other posters that GC.Collect() and Marshal.ReleaseComObject() is not needed. If the process still exists after running app.Quit(false), it might be because you're running the app invisible, and there is a prompt that is preventing the application from closing, such as a Document Recovery dialog. If that's the case, you need to add this when creating your application.\napp.DisplayAlerts = false;\n\n"", '\nI close the document, then the application, that works for me, then force garbage collection.\n// Document\nobject saveOptionsObject = saveDocument ? Word.WdSaveOptions.wdSaveChanges : Word.WdSaveOptions.wdDoNotSaveChanges;\nthis.WordDocument.Close(ref saveOptionsObject, ref Missing.Value, ref Missing.Value);\n\n// Application\nobject saveOptionsObject = Word.WdSaveOptions.wdDoNotSaveChanges;\nthis.WordApplication.Quit(ref saveOptionsObject, ref Missing.Value, ref Missing.Value); \n\nGC.Collect();\nGC.WaitForPendingFinalizers();\n\n', '\nTry this..\ndoc.Close(false);\napp.Quit(false);\nif (doc != null)\n    System.Runtime.InteropServices.Marshal.ReleaseComObject(doc);\nif (app != null)\n    System.Runtime.InteropServices.Marshal.ReleaseComObject(app);\ndoc = null;\napp = null;\nGC.Collect();\n\n']",https://stackoverflow.com/questions/6777422/disposing-of-microsoft-office-interop-word-application,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selenium Webdriver: How to Download a PDF File with Python?,"
I am using selenium webdriver to automate downloading several PDF files. I get the PDF preview window (see below), and now I would like to download the file. How can I accomplish this using Google Chrome as the browser?  

",53k,"
            17
        ","['\nTry this code, it worked for me.\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option(\'prefs\', {\n""download.default_directory"": ""C:/Users/XXXX/Desktop"", #Change default directory for downloads\n""download.prompt_for_download"": False, #To auto download the file\n""download.directory_upgrade"": True,\n""plugins.always_open_pdf_externally"": True #It will not show PDF directly in chrome\n})\nself.driver = webdriver.Chrome(options=options)\n\n', '\nI found this piece of code somewhere on Stackoverflow itself and it serves the purpose for me without having to use selenium at all.\nimport urllib.request\n\nresponse = urllib.request.urlopen(URL)    \nfile = open(""FILENAME.pdf"", \'wb\')\nfile.write(response.read())\nfile.close()\n\n', '\nYou can download the pdf (Embeded pdf & Normal pdf) from web using selenium.\nfrom selenium import webdriver\n\ndownload_dir = ""C:\\\\Users\\\\omprakashpk\\\\Documents"" # for linux/*nix, download_dir=""/usr/Public""\noptions = webdriver.ChromeOptions()\n\nprofile = {""plugins.plugins_list"": [{""enabled"": False, ""name"": ""Chrome PDF Viewer""}], # Disable Chrome\'s PDF Viewer\n               ""download.default_directory"": download_dir , ""download.extensions_to_open"": ""applications/pdf""}\noptions.add_experimental_option(""prefs"", profile)\ndriver = webdriver.Chrome(\'C:\\\\chromedriver\\\\chromedriver_2_32.exe\', chrome_options=options)  # Optional argument, if not specified will search path.\n\ndriver.get(`pdf_url`)\n\nIt will download and save the pdf in directory specified. Change the download_dir location and chrome driver location as per your convenience. \nYou can download chrome driver from here.\nHope it helps!\n', '\nI did it and it worked, don\'t ask me how :)\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option(\'prefs\', {\n#""download.default_directory"": ""C:/Users/517/Download"", #Change default directory for downloads\n#""download.prompt_for_download"": False, #To auto download the file\n#""download.directory_upgrade"": True,\n""plugins.always_open_pdf_externally"": True #It will not show PDF directly in chrome \n})\ndriver = webdriver.Chrome(options=options)\n\n', '\nIn My case it worked without any code modification,Just need to disabled the Chrome pdf viewer\nHere are the steps to disable it\n\nGo into Chrome Settings\nScroll to the bottom click on Advanced\nUnder Privacy And Security - Click on ""Site Settings""\nScroll to PDF Documents\nEnable ""Download PDF files instead of automatically opening them in Chrome""\n\n']",https://stackoverflow.com/questions/43149534/selenium-webdriver-how-to-download-a-pdf-file-with-python,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I pass an argument to a PowerShell script?,"
There's a PowerShell script named itunesForward.ps1 that makes iTunes fast forward 30 seconds:
$iTunes = New-Object -ComObject iTunes.Application

if ($iTunes.playerstate -eq 1)
{
  $iTunes.PlayerPosition = $iTunes.PlayerPosition + 30
}

It is executed with a prompt line command:
powershell.exe itunesForward.ps1

Is it possible to pass an argument from the command line and have it applied in the script instead of the hardcoded 30 seconds value?
",848k,"
            551
        ","['\nTested as working:\n#Must be the first statement in your script (not counting comments)\nparam([Int32]$step=30) \n\n$iTunes = New-Object -ComObject iTunes.Application\n\nif ($iTunes.playerstate -eq 1)\n{\n  $iTunes.PlayerPosition = $iTunes.PlayerPosition + $step\n}\n\nCall it with\npowershell.exe -file itunesForward.ps1 -step 15\n\nMultiple parameters syntax (comments are optional, but allowed):\n<#\n    Script description.\n\n    Some notes.\n#>\nparam (\n    # height of largest column without top bar\n    [int]$h = 4000,\n    \n    # name of the output image\n    [string]$image = \'out.png\'\n)\n\nAnd some example for advanced parameters, e.g. Mandatory:\n<#\n    Script description.\n\n    Some notes.\n#>\nparam (\n    # height of largest column without top bar\n    [Parameter(Mandatory=$true)]\n    [int]$h,\n    \n    # name of the output image\n    [string]$image = \'out.png\'\n)\n\nWrite-Host ""$image $h""\n\nA default value will not work with a mandatory parameter. You can omit the =$true for advanced parameters of type boolean [Parameter(Mandatory)].\n', ""\nYou can use also the $args variable (that's like position parameters):\n$step = $args[0]\n\n$iTunes = New-Object -ComObject iTunes.Application\n\nif ($iTunes.playerstate -eq 1)\n{\n  $iTunes.PlayerPosition = $iTunes.PlayerPosition + $step\n}\n\nThen it can be called like:\npowershell.exe -file itunersforward.ps1 15\n\n"", '\nCall the script from a batch file (*.bat) or CMD\nPowerShell Core\npwsh.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 -Param1 Hello -Param2 World""\n\npwsh.exe -NoLogo -ExecutionPolicy Bypass -Command ""path-to-script/Script.ps1 -Param1 Hello -Param2 World""\n\npwsh.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 Hello -Param2 World""\n\npwsh.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 Hello World""\n\npwsh.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 -Param2 World Hello""\n\nPowerShell\npowershell.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 -Param1 Hello -Param2 World""\n\npowershell.exe -NoLogo -ExecutionPolicy Bypass -Command ""path-to-script/Script.ps1 -Param1 Hello -Param2 World""\n\npowershell.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 Hello -Param2 World""\n\npowershell.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 Hello World""\n\npowershell.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 -Param2 World Hello""\n\n\nCall from PowerShell\nPowerShell Core or Windows PowerShell\n& path-to-script/Script.ps1 -Param1 Hello -Param2 World\n& ./Script.ps1 -Param1 Hello -Param2 World\n\n\nScript.ps1 - Script Code\nparam(\n    [Parameter(Mandatory=$True, Position=0, ValueFromPipeline=$false)]\n    [System.String]\n    $Param1,\n\n    [Parameter(Mandatory=$True, Position=1, ValueFromPipeline=$false)]\n    [System.String]\n    $Param2\n)\n\nWrite-Host $Param1\nWrite-Host $Param2\n\n', ""\nLet PowerShell analyze and decide the data type. It internally uses a 'Variant' for this.\nAnd generally it does a good job...\nparam($x)\n$iTunes = New-Object -ComObject iTunes.Application\nif ($iTunes.playerstate -eq 1)\n{\n    $iTunes.PlayerPosition = $iTunes.PlayerPosition + $x\n}\n\nOr if you need to pass multiple parameters:\nparam($x1, $x2)\n$iTunes = New-Object -ComObject iTunes.Application\nif ($iTunes.playerstate -eq 1)\n{\n    $iTunes.PlayerPosition = $iTunes.PlayerPosition + $x1\n    $iTunes.<AnyProperty>  = $x2\n}\n\n"", '\n# ENTRY POINT MAIN()\nParam(\n    [Parameter(Mandatory=$True)]\n    [String] $site,\n    [Parameter(Mandatory=$True)]\n    [String] $application,\n    [Parameter(Mandatory=$True)]\n    [String] $dir,\n    [Parameter(Mandatory=$True)]\n    [String] $applicationPool\n)\n\n# Create Web IIS Application\nfunction ValidateWebSite ([String] $webSiteName)\n{\n    $iisWebSite = Get-Website -Name $webSiteName\n    if($Null -eq $iisWebSite)\n    {\n        Write-Error -Message ""Error: Web Site Name: $($webSiteName) not exists.""  -Category ObjectNotFound\n    }\n    else\n    {\n        return 1\n    }\n}\n\n# Get full path from IIS WebSite\nfunction GetWebSiteDir ([String] $webSiteName)\n{\n    $iisWebSite = Get-Website -Name $webSiteName\n    if($Null -eq $iisWebSite)\n    {\n        Write-Error -Message ""Error: Web Site Name: $($webSiteName) not exists.""  -Category ObjectNotFound\n    }\n    else\n    {\n        return $iisWebSite.PhysicalPath\n    }\n}\n\n# Create Directory\nfunction CreateDirectory([string]$fullPath)\n{\n    $existEvaluation = Test-Path $fullPath -PathType Any\n    if($existEvaluation -eq $false)\n    {\n        new-item $fullPath -itemtype directory\n    }\n    return 1\n}\n\nfunction CreateApplicationWeb\n{\n    Param(\n        [String] $WebSite,\n        [String] $WebSitePath,\n        [String] $application,\n        [String] $applicationPath,\n        [String] $applicationPool\n        )\n    $fullDir = ""$($WebSitePath)\\$($applicationPath)""\n    CreateDirectory($fullDir)\n    New-WebApplication -Site $WebSite -Name $application -PhysicalPath $fullDir -ApplicationPool $applicationPool -Force\n}\n\n$fullWebSiteDir = GetWebSiteDir($Site)f($null -ne $fullWebSiteDir)\n{\n    CreateApplicationWeb -WebSite $Site -WebSitePath $fullWebSiteDir -application $application  -applicationPath $dir -applicationPool $applicationPool\n}\n\n', ""\nCreate a PowerShell script with the following code in the file.\nparam([string]$path)\nGet-ChildItem $path | Where-Object {$_.LinkType -eq 'SymbolicLink'} | select name, target\n\nThis creates a script with a path parameter. It will list all symbolic links within the path provided as well as the specified target of the symbolic link.\n"", ""\nYou can also define a variable directly in the PowerShell command line and then execute the script. The variable will be defined there, too. This helped me in a case where I couldn't modify a signed script.\nExample:\n PS C:\\temp> $stepsize = 30\n PS C:\\temp> .\\itunesForward.ps1\n\nwith iTunesForward.ps1 being\n$iTunes = New-Object -ComObject iTunes.Application\n\nif ($iTunes.playerstate -eq 1)\n{\n  $iTunes.PlayerPosition = $iTunes.PlayerPosition + $stepsize\n}\n\n""]",https://stackoverflow.com/questions/5592531/how-can-i-pass-an-argument-to-a-powershell-script,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"How can I automate the ""generate scripts"" task in SQL Server Management Studio 2008?","
I'd like to automate the script generation in SQL Server Management Studio 2008.
Right now what I do is :

Right click on my database, Tasks, ""Generate Scripts...""
manually select all the export options I need, and hit select all on the ""select object"" tab
Select the export folder
Eventually hit the ""Finish"" button

Is there a way to automate this task?
Edit : I want to generate creation scripts, not change scripts.
",97k,"
            112
        ","['\nSqlPubwiz has very limited options compared to the script generation in SSMS. By contrast the options available with SMO almost exactly match those in SSMS, suggesting it is probably even the same code. (I would hope MS didn\'t write it twice!) There are several examples on MSDN like this one that show scripting tables as individual objects. However if you want everything to script correctly with a \'full\' schema that includes \'DRI\' (Declarative Referential Integrity) objects like foreign keys then scripting tables individually doesn\'t work the dependencies out correctly. I found it is neccessary to collect all the URNs and hand them to the scripter as an array. This code, modified from the example, works for me (though I daresay you could tidy it up and comment it a bit more):\n    using Microsoft.SqlServer.Management.Smo;\n    using Microsoft.SqlServer.Management.Sdk.Sfc;\n    // etc...\n\n    // Connect to the local, default instance of SQL Server. \n    Server srv = new Server();\n\n    // Reference the database.  \n    Database db = srv.Databases[""YOURDBHERE""];\n\n    Scripter scrp = new Scripter(srv);\n    scrp.Options.ScriptDrops = false;\n    scrp.Options.WithDependencies = true;\n    scrp.Options.Indexes = true;   // To include indexes\n    scrp.Options.DriAllConstraints = true;   // to include referential constraints in the script\n    scrp.Options.Triggers = true;\n    scrp.Options.FullTextIndexes = true;\n    scrp.Options.NoCollation = false;\n    scrp.Options.Bindings = true;\n    scrp.Options.IncludeIfNotExists = false;\n    scrp.Options.ScriptBatchTerminator = true;\n    scrp.Options.ExtendedProperties = true;\n\n    scrp.PrefetchObjects = true; // some sources suggest this may speed things up\n\n    var urns = new List<Urn>();\n\n    // Iterate through the tables in database and script each one   \n    foreach (Table tb in db.Tables)\n    {\n        // check if the table is not a system table\n        if (tb.IsSystemObject == false)\n        {\n            urns.Add(tb.Urn);\n        }\n    }\n\n    // Iterate through the views in database and script each one. Display the script.   \n    foreach (View view in db.Views)\n    {\n        // check if the view is not a system object\n        if (view.IsSystemObject == false)\n        {\n            urns.Add(view.Urn);\n        }\n    }\n\n    // Iterate through the stored procedures in database and script each one. Display the script.   \n    foreach (StoredProcedure sp in db.StoredProcedures)\n    {\n        // check if the procedure is not a system object\n        if (sp.IsSystemObject == false)\n        {\n            urns.Add(sp.Urn);\n        }\n    }\n\n    StringBuilder builder = new StringBuilder();\n    System.Collections.Specialized.StringCollection sc = scrp.Script(urns.ToArray());\n    foreach (string st in sc)\n    {\n        // It seems each string is a sensible batch, and putting GO after it makes it work in tools like SSMS.\n        // Wrapping each string in an \'exec\' statement would work better if using SqlCommand to run the script.\n        builder.AppendLine(st);\n        builder.AppendLine(""GO"");\n    }\n\n    return builder.ToString();\n\n', ""\nWhat Brann is mentioning from the Visual Studio 2008 SP1 Team Suite is version 1.4 of the Database Publishing Wizard. It's installed with sql server 2008 (maybe only professional?) to \\Program Files\\Microsoft SQL Server\\90\\Tools\\Publishing\\1.4. The VS call from server explorer is simply calling this. You can achieve the same functionality via the command line like:\nsqlpubwiz help script\n\nI don't know if v1.4 has the same troubles that v1.1 did (users are converted to roles, constraints are not created in the right order), but it is not a solution for me because it doesn't script objects to different files like the Tasks->Generate Scripts option in SSMS does. I'm currently using a modified version of Scriptio (uses the MS SMO API) to act as an improved replacement for the database publishing wizard (sqlpubwiz.exe). It's not currently scriptable from the command line, I might add that contribution in the future.\nScriptio was originally posted on Bill Graziano's blog, but has subsequently been released to CodePlex by Bill and updated by others. Read the discussion to see how to compile for use with SQL Server 2008.\nhttp://scriptio.codeplex.com/\nEDIT: I've since started using RedGate's SQL Compare product to do this. It's a very nice replacement for all that sql publishing wizard should have been. You choose a database, backup, or snapshot as the source, and a folder as the output location and it dumps everything nicely into a folder structure. It happens to be the same format that their other product, SQL Source Control, uses.\n"", ""\nI wrote an open source command line utility named SchemaZen that does this. It's much faster than scripting from management studio and it's output is more version control friendly. It supports scripting both schema and data. \nTo generate scripts run:\nschemazen.exe script --server localhost --database db --scriptDir c:\\somedir\nThen to recreate the database from scripts run:\nschemazen.exe create --server localhost --database db --scriptDir c:\\somedir\n"", '\nYou can use SQL Server Management Object (SMO) to automate SQL Server 2005 management tasks including generating scripts: http://msdn.microsoft.com/en-us/library/ms162169.aspx.\n', ""\nIf you're a developer, definitely go with SMO.  Here's a link to the Scripter class, which is your starting point:\nScripter Class\n"", ""\nI don't see powershell with SQLPSX mentioned in any of these answers... I personally haven't played with it but it looks beautifully simple to use and ideally suited to this type of automation tasks, with tasks like:\nGet-SqlDatabase -dbname test -sqlserver server | Get-SqlTable | Get-SqlScripter | Set-Content -Path C:\\script.sql\nGet-SqlDatabase -dbname test -sqlserver server | Get-SqlStoredProcedure | Get-SqlScripter\nGet-SqlDatabase -dbname test -sqlserver server | Get-SqlView | Get-SqlScripter\n\n(ref: http://www.sqlservercentral.com/Forums/Topic1167710-1550-1.aspx#bm1168100)\nProject page: http://sqlpsx.codeplex.com/\nThe main advantage of this approach is that it combines the configurablity / customizability of using SMO directly, with the convenience and maintainability of using a simple existing tool like the Database Publishing Wizard.\n"", ""\nIn Tools > Options > Designers > Table and Database Designers there's an option for 'Auto generate change scripts' that will generate one for every change you make at the time you save it.\n"", '\nYou can do it with T-SQL code using the INFORMATION_SCHEMA tables.\nThere are also third-party tools - I like Apex SQL Script for precisely the use you are talking about.  I run it completely from the command-line.\n', '\nIf you want to a Microsoft solution you can try: Microsoft SQL Server Database Publishing Wizard 1.1\nhttp://www.microsoft.com/downloads/details.aspx?FamilyId=56E5B1C5-BF17-42E0-A410-371A838E570A&displaylang=en\nIt create a batch process you can run anytime you need to rebuild the scripts.\n', '\nTry new SQL Server command line tools to generate T-SQL scripts and monitor Dynamic Management Views.\nWorked for me like charm. It is a new python based tool from Microsoft that runs from command line.\nEverything works like described on the Microsoft page (see link below)\nWorked for me with SQL 2012 server.\nYou install it with pip:\n\n$pip install mssql-scripter\n\nCommand parameter overview as usual with h for help:\n\nmssql-scripter -h\n\nHint:\nIf you log in to SQL-Server via Windows authentication, just leave away Username and password.\nhttps://cloudblogs.microsoft.com/sqlserver/2017/05/17/try-new-sql-server-command-line-tools-to-generate-t-sql-scripts-and-monitor-dynamic-management-views/\n', ""\nI've been using DB Comparer - Its free and no fuss script entire DB and can compare to another DB and also produce a Diff script . Excellent for Development to Production change scripts. \nhttp://www.dbcomparer.com/\n"", '\nFrom Visual Studio 2008 SP1 TeamSuite :\nIn the Server Explorer / Data Connections tab, there\'s a publish to provider tool which does the same as ""Microsoft SQL Server Database Publishing Wizard"", but which is compatible with MS Sql Server 2008.\n', '\nThere is also this simple command line tool I build for my needs.\nhttp://mycodepad.wordpress.com/2013/11/18/export-ms-sql-database-schema-with-c/\nIt can export an entire db, and it tries to export encrypted objects. Everything is stored in folders and separate sql files for easy file comparison. \nCode is also available on github.\n', '\nI am using VS 2012(for DBs on MSSQL Server 2008) compare database has an option to save it, the comparison and options. This is essentially  what are your settings for delivery. After that you can do update or generate script.\nI just find it it a little bit awkward to load it from file later(drag and drop from windows explorer) as I do not see the file in solution explorer.\n']",https://stackoverflow.com/questions/483568/how-can-i-automate-the-generate-scripts-task-in-sql-server-management-studio-2,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fastest way to interface between live (unsaved) Excel data and C# objects,"
I want to know what the fastest way is of reading and writing data to and from an open Excel workbook to c# objects.   The background is that I want to develop a c# application that is used from Excel and uses data held in excel.  
The business logic will reside in the c# application but the data will reside in an Excel workbook.  The user will be using Excel and will click a button (or do something similar) on the excel workbook to initiate the c# application.  The c# application will then read data off the excel workbook, process the data, and then write data back to the excel workbook.
There may be numerous blocks of data that are required to be read off and written back to the excel workbook but they will normally be of a relatively small size, say 10 rows and 20 columns.  Occasionally a large list of data may need to be processed, of the order of 50,000 rows and 40 columns.
I know that this is relatively easy to do say using VSTO but I want to know what the fastest (but still robust and elegant) solution is and get an idea of the speed.   I don't mind if the solution recommends using third party products or uses C++.
The obvious solution is using VSTO or interop but I don't know what the performance is like versus VBA which I'm currently using to read in the data, or if there are any other solutions.
This was posted on experts exchange saying that VSTO was dramatically slower than VBA but that was a couple of years ago and I don't know if the performance has improved.
http://www.experts-exchange.com/Microsoft/Development/VSTO/Q_23635459.html
Thanks.
",22k,"
            32
        ","['\nI\'ll take this as a challenge, and will bet the fastest way to shuffle your data between Excel and C# is to use Excel-DNA - http://excel-dna.net.\n(Disclaimer: I develop Excel-DNA. But it\'s still true...)\nBecause it uses the native .xll interface it skips all the COM integration overhead that you\'d have with VSTO or another COM-based add-in approach. With Excel-DNA you could make a macro that is hooked up to a menu or ribbon button which reads a range, processes it, and writes it back to a range in Excel. All using the native Excel interface from C# - not a COM object in sight.\nI\'ve made a small test function that takes the current selection into an array, squares every number in the array, and writes the result into Sheet 2 starting from cell A1. You just need to add the (free) Excel-DNA runtime which you can download from http://excel-dna.net.\nI read into C#, process and write back to Excel a million-cell range in under a second. Is this fast enough for you?\nMy function looks like this:\nusing ExcelDna.Integration;\npublic static class RangeTools {\n\n[ExcelCommand(MenuName=""Range Tools"", MenuText=""Square Selection"")]\npublic static void SquareRange()\n{\n    object[,] result;\n    \n    // Get a reference to the current selection\n    ExcelReference selection = (ExcelReference)XlCall.Excel(XlCall.xlfSelection);\n    // Get the value of the selection\n    object selectionContent = selection.GetValue();\n    if (selectionContent is object[,])\n    {\n        object[,] values = (object[,])selectionContent;\n        int rows = values.GetLength(0);\n        int cols = values.GetLength(1);\n        result = new object[rows,cols];\n        \n        // Process the values\n        for (int i = 0; i < rows; i++)\n        {\n            for (int j = 0; j < cols; j++)\n            {\n                if (values[i,j] is double)\n                {\n                    double val = (double)values[i,j];\n                    result[i,j] = val * val;\n                }\n                else\n                {\n                    result[i,j] = values[i,j];\n                }\n            }\n        }\n    }\n    else if (selectionContent is double)\n    {\n        double value = (double)selectionContent;\n        result = new object[,] {{value * value}}; \n    }\n    else\n    {\n        result = new object[,] {{""Selection was not a range or a number, but "" + selectionContent.ToString()}};\n    }\n    \n    // Now create the target reference that will refer to Sheet 2, getting a reference that contains the SheetId first\n    ExcelReference sheet2 = (ExcelReference)XlCall.Excel(XlCall.xlSheetId, ""Sheet2""); // Throws exception if no Sheet2 exists\n    // ... then creating the reference with the right size as new ExcelReference(RowFirst, RowLast, ColFirst, ColLast, SheetId)\n    int resultRows = result.GetLength(0);\n    int resultCols = result.GetLength(1);\n    ExcelReference target = new ExcelReference(0, resultRows-1, 0, resultCols-1, sheet2.SheetId);\n    // Finally setting the result into the target range.\n    target.SetValue(result);\n}\n}\n\n', '\nIf the C# application is a stand-alone application, then you will always have cross-process marshaling involved that will overwhelm any optimizations you can do by switching languages from, say, C# to C++. Stick to your most preferred language in this situation, which sounds like is C#. \nIf you are willing to make an add-in that runs within Excel, however, then your operations will avoid cross-process calls and run about 50x faster.\nIf you run within Excel as an add-in, then VBA is among the fastest options, but it does still involve COM and so C++ calls using an XLL add-in would be fastest. But VBA is still quite fast in terms of calls to the Excel object model. As for actual calculation speed, however, VBA runs as pcode, not as fully compiled code, and so executes about 2-3x slower than native code. This sounds very bad, but it isn\'t because the vast majority of the execution time taken with a typical Excel add-in or application involves calls to the Excel object model, so VBA vs. a fully compiled COM add-in, say using natively compiled VB 6.0, would only be about 5-15% slower, which is not noticeable.\nVB 6.0 is a compiled COM approach, and runs 2-3x faster than VBA for non-Excel related calls, but VB 6.0 is about 12 years old at this point and won\'t run in 64 bit mode, say if installing Office 2010, which can be installed to run 32 bit or 64 bit. Usage of 64 bit Excel is tiny at the moment, but will grow in usage, and so I would avoid VB 6.0 for this reason.\nC#, if running in-process as an Excel add-in would execute calls to the Excel object model as fast as VBA, and execute non-Excel calls 2-3x faster than VBA -- if running unshimmed. The approach recommended by Microsoft, however, is to run fully shimmed, for example, by making use of the COM Shim Wizard. By being shimmed, Excel is protected from your code (if it\'s faulty) and your code is fully protected from other 3rd party add-ins that could otherwise potentially cause problems. The down-side to this, however, is that a shimmed solution runs within a separate AppDomain, which requires cross-AppDomain marshaling that incurrs an execution speed penalty of about 40x -- which is very noticeable in many contexts.\nAdd-ins using Visual Studio Tools for Office (VSTO) are automatically loaded within a shim and executes within a separate AppDomain. There is no avoiding this if using VSTO. Therefore, calls to the Excel object model would also incur an approximately 40x execution speed degradation. VSTO is a gorgeous system for making very rich Excel add-ins, but execution speed is its weakness for applications such as yours.\nExcelDna is a free, open source project that allows you to use C# code, which is then converted for you to an XLL add-in that uses C++ code. That is, ExcelDna parses your C# code and creates the required C++ code for you. I\'ve not used it myself, but I am familiar with the process and it\'s very impressive. ExcelDna gets very good reviews from those that use it. [Edit: Note the following correction as per Govert\'s comments below: ""Hi Mike - I want add a small correction to clarify the Excel-Dna implementation: all the managed-to-Excel glue works at runtime from your managed assembly using reflection - there is no extra pre-compilation step or C++ code generation. Also, even though Excel-Dna uses .NET, there need not be any COM interop involved when talking to Excel - as an .xll the native interface can be used directly from .NET (though you can also use COM if you want). This makes high-performance UDFs and macros possible."" 鈥?Govert]\nYou also might want to look at Add-in Express. It\'s not free, but it would allow you to code in C# and although it shims your solution into a separate AppDomain, I believe that it\'s execution speed is outstanding. If I am understanding its execution speed correctly, then I\'m not sure how Add-in Express doing this, but it might be taking advantage of something called FastPath AppDomain marshaling. Don\'t quote me on any of this, however, as I\'m not very familiar with Add-in Express. You should check it out though and do your own research. [Edit: Reading Charles Williams\' answer, it looks like Add-in Express enables both COM and C API access. And Govert states that Excel DNA also enables both COM and the fastrer C API access. So you\'d probably want to check out both and compare them to ExcelDna.]\nMy advice would be to research Add-in Express and ExcelDna. Both approaches would allow you to code using C#, which you seem most familiar with.\nThe other main issue is how you make your calls. For example, Excel is very fast when handling an entire range of data passed back-and-forth as an array. This is vastly more efficient than looping through the cells individually. For example, the following code makes use of the Excel.Range.set_Value accessor method to assign a 10 x 10 array of values to a 10 x 10 range of cells in one shot:\nvoid AssignArrayToRange()\n{\n    // Create the array.\n    object[,] myArray = new object[10, 10];\n\n    // Initialize the array.\n    for (int i = 0; i < myArray.GetLength(0); i++)\n    {\n        for (int j = 0; j < myArray.GetLength(1); j++)\n        {\n            myArray[i, j] = i + j;\n        }\n    }\n\n    // Create a Range of the correct size:\n    int rows = myArray.GetLength(0);\n    int columns = myArray.GetLength(1);\n    Excel.Range range = myWorksheet.get_Range(""A1"", Type.Missing);\n    range = range.get_Resize(rows, columns);\n\n    // Assign the Array to the Range in one shot:\n    range.set_Value(Type.Missing, myArray);\n}\n\nOne can similarly make use of the Excel.Range.get_Value accessor method to read an array of values from a range in one step. Doing this and then looping through the values within the array is vastly faster than looping trough the values within the cells of the range individually.\n', ""\nFurther to Mike Rosenblum's comments on the use of arrays, I'd like to add that I've been using the very approach (VSTO + arrays) and when I measured it, the actual read speed itself was within milliseconds. Just remember to disable event handling and screen updating prior to the read/write, and remember to re-enable after the operation is complete.\nUsing C#, you can create 1-based arrays exactly the same as Excel VBA itself does.  This is pretty useful, especially because even in VSTO, when you extract the array from an Excel.Range object, the array is 1-based, so keeping the Excel-oriented arrays 1-based helps you avoid needing to always check for whether the array is one-based or zero-based. \n  (If the column position in the array has significance to you, having to deal with 0-based and 1-based arrays can be a real pain).\nGenerally reading the Excel.Range into an array would look something like this:\nvar myArray = (object[,])range.Value2;\n\n\nMy variation of Mike Rosenblum's array-write uses a 1-based array like this:\nint[] lowerBounds = new int[]{ 1, 1 };\nint[] lengths = new int[] { rowCount, columnCount };  \nvar myArray = \n    (object[,])Array.CreateInstance(typeof(object), lengths, lowerBounds);\n\nvar dataRange = GetRangeFromMySources();\n\n// this example is a bit too atomic; you probably want to disable \n// screen updates and events a bit higher up in the call stack...\ndataRange.Application.ScreenUpdating = false;\ndataRange.Application.EnableEvents = false;\n\ndataRange = dataRange.get_Resize(rowCount, columnCount);\ndataRange.set_Value(Excel.XlRangeValueDataType.xlRangeValueDefault, myArray);\n\ndataRange.Application.ScreenUpdating = true;\ndataRange.Application.EnableEvents = true;\n\n"", '\nThe fastest interface to Excel data is the C API. There are a number of products out there that link .NET to Excel using this interface.\n2 products I like that do this are Excel DNA (which is free and open source) and Addin Express (which is a commercial product and has both the C API and COM interface available).\n', '\nFirst off, your solution cannot be an Excel UDF (user-defined function). In our manuals, we give the following definition: ""Excel UDFs are used to build custom functions in Excel for the end user to use them in formulas."" I wouldn\'t mind if you suggest a better definition :) \nThat definition shows that a UDF cannot add a button to the UI (I know that XLLs can modify the CommandBar UI) or intercept keyboard shortcuts as well as Excel events. \nThat is, ExcelDNA is out of scope because it is purposed for developing XLL add-ins. The same applies to Excel-targeted functionality of Add-in Express since it allows developing XLL add-ins and Excel Automation add-ins. \nBecause you need to handle Excel events, your solution can be a standalone application but there are obvious limitations of such approach. The only real way is to create a COM add-in; it allows handling Excel events and adding custom things to the Excel UI. You have three possibilities:\n\nVSTO\nAdd-in Express (COM add-in functionality)\nShared Add-in (see the corresponding item in the New Project dialog in VS)\n\nIf talking about developing an Excel COM add-in, the 3 tools above provide different features: visual designers, shimming, etc. But I don\'t think they differ in the speed of accessing the Excel Object Model. Say, I don\'t know (and cannot imagine) why getting a COM object from the Default AppDomain should differ from getting the same COM object from another AppDomain. BTW, you can check if shimming influences the speed of operation by creating a shared add-in and then using the COM Shim Wizard to shim it. \nSpeed II. As I wrote to you yesterday: ""The best way to speed up reading and writing to a range of cells is to create a variable of the Excel.Range type referring to that range and then read/write an array from/to the Value property of the variable."" But contrary to what Francesco says, I don\'t attribute this to VSTO; this is a feature of the Excel object model.\nSpeed III. The fastest Excel UDFs are written in native C++, not in any .NET language. I haven\'t compared the speed of an XLL add-in produced by ExcelDNA and Add-in Express; I don\'t think you\'ll find any substantial difference here.\nTo sum up. I am convinced you are on a wrong way: COM add-ins based on Add-in Express, VSTO or Shared Add-in should read and write Excel cells at the same speed. I will be glad (sincerely) if someone disproves this statement.\nNow on your other questions. VSTO doesn\'t allow developing a COM add-in supporting Office 2000-2010. It requires three different codebases and at least two versions of Visual Studio to completely support Office 2003-2010; you need to have strong nerves and a portion of good luck to deploy a VSTO-based add-in for Excel 2003. With Add-in Express, you create a COM add-in for all Office versions with a single codebase; Add-in Express provides you with a setup project, which is ready to install your add-in in Excel 2000-2010 (32-bit and 64-bit); ClickOnce deployment is on board too. \nVSTO beats Add-in Express in one area: it allows creating so-called document-level add-ins. Imagine a workbook or template with some .NET code behind it; I wouldn\'t be surprised, however, if deployment of such things is a nightmare.\nOn Excel events. All Excel events are listed in MSDN, for instance, see Excel 2007 events\nRegards from Belarus (GMT+2),\nAndrei Smolin\nAdd-in Express Team Leader\n', ""\nI've used VBA code (macro) to gather & compact the data, and get this data in one call to C#, and vice versa. This will probably be the most performant approach.\nUsing C#, you will always need to use some marshalling. Using VSTO or COM Interop, the underlaying communication layer (marshalling overhead) is the same.\nIn VBA (Visual Basic For Application) you work directly on the objects in Excel. So the access to this data will always be faster.\nBut.... Once you have the data in C#, the manipulation of this data can be a lot faster.\nIf you are using VB6 or C++, you also go through a COM interface, and you will also be facing cross process marshalling.\nSo you are looking for a method to minimize cross process calls and marshalling.\n""]",https://stackoverflow.com/questions/3840270/fastest-way-to-interface-between-live-unsaved-excel-data-and-c-sharp-objects,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R command for setting working directory to source file location in Rstudio,"
I am working out some tutorials in R. Each R code is contained in a specific folder. There are data files and other files in there. I want to open the .r file and source it such that I do not have to change the working directory in Rstudio as shown below:

Is there a way to specify my working directory automatically in R.
",183k,"
            177
        ","['\nTo get the location of a script being sourced, you can use utils::getSrcDirectory or utils::getSrcFilename. These require a function as an input. Create a script with the following lines, and source it to see their usage:\nprint(utils::getSrcDirectory(function(){}))\nprint(utils::getSrcFilename(function(){}, full.names = TRUE))\n\nChanging the working directory to that of the current file can be done with:\nsetwd(getSrcDirectory(function(){})[1])\n\nThis does not work in RStudio if you Run the code rather than Sourceing it.  For that, you need to use rstudioapi::getActiveDocumentContext.\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\n\nThis second solution requires that you are using RStudio as your IDE, of course.\n', ""\nI know this question is outdated, but I was searching for a solution for that as well and Google lists this at the very top:\nthis.dir <- dirname(parent.frame(2)$ofile)\nsetwd(this.dir)\n\nput that somewhere into the file (best would be the beginning, though), so that the wd is changed according to that file.\nAccording to the comments, this might not necessarily work on every platform (Windows seems to work, Linux/Mac for some).\nKeep in mind that this solution is for 'sourcing' the files, not necessarily for running chunks in that file.\nsee also get filename and path of `source`d file\n"", ""\nFor rstudio, you can automatically set your working directory to the script directory using rstudioapi like that:\nlibrary(rstudioapi)\n\n# Getting the path of your current open file\ncurrent_path = rstudioapi::getActiveDocumentContext()$path \nsetwd(dirname(current_path ))\nprint( getwd() )\n\nThis works when Running or Sourceing your file.\nYou need to install the package rstudioapi first.\nNotice I print the path to be 100% sure I'm at the right place, but this is optional.\n"", '\ndirname(rstudioapi::getActiveDocumentContext()$path)\n\nworks for me but if you don\'t want to use rstudioapi and you are not in a proyect, you can use the symbol ~ in your path. The symbol ~ refers to the default RStudio working directory (at least on Windows).\n\nIf your RStudio working directory is ""D:/Documents"", setwd(""~/proyect1"") is the same as setwd(""D:/Documents/proyect1"").\nOnce you set that, you can navigate to a subdirectory: read.csv(""DATA/mydata.csv""). Is the same as read.csv(""D:/Documents/proyect1/DATA/mydata.csv"").\nIf you want to navigate to a parent folder, you can use ""../"". \nFor example: read.csv(""../olddata/DATA/mydata.csv"") which is the same as read.csv(""D:/Documents/oldata/DATA/mydata.csv"")\nThis is the best way for me to code scripts, no matter what computer you are using.\n', '\nI realize that this is an old thread, but I had a similar problem with needing to set the working directory and couldn\'t get any of the solutions to work for me. Here\'s what did work, in case anyone else stumbles across this later on:\n# SET WORKING DIRECTORY TO CURRENT DIRECTORY:\nsystem(""pwd=`pwd`; $pwd 2> dummyfile.txt"")\ndir <- fread(""dummyfile.txt"")\nn<- colnames(dir)[2]\nn2 <- substr(n, 1, nchar(n)-1)\nsetwd(n2)\n\nIt\'s a bit convoluted, but basically this uses system commands to get the working directory and save it to dummyfile.txt, then R reads that file using data.table::fread. The rest is just cleaning up what got printed to the file so that I\'m left with just the directory path.\nI needed to run R on a cluster, so there was no way to know what directory I\'d end up in (jobs get assigned a number and a compute node). This did the trick for me.\n', '\nThis answer can help:\nscript.dir <- dirname(sys.frame(1)$ofile)\n\n\nNote: script must be sourced in order to return correct path\n\nI found it in: https://support.rstudio.com/hc/communities/public/questions/200895567-can-user-obtain-the-path-of-current-Project-s-directory-\nThe BumbleBee麓s answer (with parent.frame instead sys.frame) didn麓t work to me, I always get an error.\n', '\nThe solution \ndirname(parent.frame(2)$ofile)\n\nnot working for me.\nI\'m using a brute force algorithm, but works:\nFile <- ""filename""\nFiles <- list.files(path=file.path(""~""),recursive=T,include.dirs=T)\nPath.file <- names(unlist(sapply(Files,grep,pattern=File))[1])\nDir.wd <- dirname(Path.file)\n\nMore easy when searching a directory:\nDirname <- ""subdir_name""\nDirs <- list.dirs(path=file.path(""~""),recursive=T)\ndir_wd <- names(unlist(sapply(Dirs,grep,pattern=Dirname))[1])\n\n', '\nIf you work on Linux you can try this:\nsetwd(system(""pwd"", intern = T) )\nIt works for me.\n', ""\nThe here package provides the here() function, which returns your project root directory based on some heuristics. \nNot the perfect solution, since it doesn't find the location of the script, but it suffices for some purposes so I thought I'd put it here.\n"", '\nI understand this is outdated, but I couldn\'t get the former answers to work very satisfactorily, so I wanted to contribute my method in case any one else encounters the same error mentioned in the comments to BumbleBee\'s answer.\nMine is based on a simple system command. All you feed the function is the name of your script:\nextractRootDir <- function(x) {\n    abs <- suppressWarnings(system(paste(""find ./ -name"",x), wait=T, intern=T, ignore.stderr=T))[1];\n    path <- paste(""~"",substr(abs, 3, length(strsplit(abs,"""")[[1]])),sep="""");\n    ret <- gsub(x, """", path);\n    return(ret);\n}\n\nsetwd(extractRootDir(""myScript.R""));\n\nThe output from the function would look like ""/Users/you/Path/To/Script"". Hope this helps anyone else who may have gotten stuck.\n', '\nsetwd(this.path::here())\n\nworks both for sourced and ""active"" scripts.\n', '\nI was just looking for a solution to this problem, came to this page. I know its dated but the previous solutions where unsatisfying or didn\'t work for me. Here is my work around if interested.\nfilename = ""your_file.R""\nfilepath = file.choose()  # browse and select your_file.R in the window\ndir = substr(filepath, 1, nchar(filepath)-nchar(filename))\nsetwd(dir)\n\n', '\nI feel like a mocking bird, but I\'m going to say it: I know this post is old, but...\nI just recently learned you cannot call the api when running the script from Task Scheduler and a .bat file. I learned this the hard way. Thought those of you who were using any of the rstudioapi:: methods might like to know that. We run lots of script this way overnight. Just recently changed our path to include the api called so we could ""dynamically"" set the working directory. Then, when the first one we tried with failed when triggered from Task Scheduler, investigation brought that information about.\nThis is the actual code that brought this issue to light: setwd(dirname(rstudioapi::getActiveDocumentContext()$path))\nWorks beautifully if you\'re running the script though!\nJust adding my two cents as I know people still pull these threads up, thought it might be helpful.\n', '\nIn case you use UTF-8 encoding:\npath <- rstudioapi::getActiveDocumentContext()$path\nEncoding(path) <- ""UTF-8""\nsetwd(dirname(path))\n\nYou need to install the package rstudioapi if you haven\'t done it yet.\n', '\nMost GUIs assume that if you are in a directory and ""open"", double-click, or otherwise attempt to execute an .R file, that the directory in which it resides will be the working directory unless otherwise specified. The Mac GUI provides a method to change that default behavior which is changeable in the Startup panel of Preferences that you set in a running session and become effective at the next ""startup"". You should be also looking at:\n?Startup\n\nThe RStudio documentation says:\n""When launched through a file association, RStudio automatically sets the working directory to the directory of the opened file.""  The default setup is for RStudio to be register as a handler for .R files, although there is also mention of ability to set a default ""association"" with RStudio for .Rdata and .R extensions. Whether having \'handler\' status and \'association\' status are the same on Linux, I cannot tell.\nhttp://www.rstudio.com/ide/docs/using/workspaces\n', ""\ndirname(parent.frame(2)$ofile)  \n\ndoesn't work for me either, but the following (as suggested in https://stackoverflow.com/a/35842176/992088) works for me in ubuntu 14.04\ndirname(rstudioapi::getActiveDocumentContext()$path)\n\n"", ""\nHere is another way to do it: \nset2 <- function(name=NULL) {\n  wd <- rstudioapi::getSourceEditorContext()$path\n  if (!is.null(name)) {\n    if (substr(name, nchar(name) - 1, nchar(name)) != '.R') \n      name <- paste0(name, '.R')\n  }\n  else {\n    name <- stringr::word(wd, -1, sep='/')\n  }\n  wd <- gsub(wd, pattern=paste0('/', name), replacement = '')\n  no_print <- eval(expr=setwd(wd), envir = .GlobalEnv)\n}\nset2()\n\n""]",https://stackoverflow.com/questions/13672720/r-command-for-setting-working-directory-to-source-file-location-in-rstudio,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Karate- Need help to assert a single dimension array for date range,"
I am trying to assert the values inside a single dimensional array. I have tried using match but it looks like the date ranges cannot be asserted. 
Below is the object array:
[
""2019-04-24T17:41:28"",
""2019-04-24T17:41:27.975"",
""2019-04-24T17:41:27.954"",
""2019-04-24T17:41:27.93"",
""2019-04-24T17:41:27.907"",
""2019-04-24T17:41:27.886"",
""2019-04-24T17:41:27.862"",
""2019-04-24T17:41:27.84"",
""2019-04-24T17:41:27.816"",
""2019-04-24T17:41:27.792""
]

I am trying to assert each values between the following date ranges:
MinDate:2019-04-24T17:25:00.000000+00:00
MaxDate:2019-04-24T17:50:00.000000+00:00

I have tried the following but none works:
* match dateCreated == '#[]? _.value >= fromDate'
 * eval for(var i = 0; i < responseJson.response.data.TotalItemCount; i++) dateCreated.add(responseJson.response.data.Items[i].DateCreated)  karate.assert(dateCreated[i] >= fromDate)

Any hint/tip on how to go about it.
",1k,"
            2
        ","['\nHere you go:\n* def dateToLong =\n""""""\nfunction(s) {\n  var SimpleDateFormat = Java.type(\'java.text.SimpleDateFormat\');\n  var sdf = new SimpleDateFormat(""yyyy-MM-dd\'T\'HH:mm:ss.SSS"");\n  return sdf.parse(s).time;\n} \n""""""\n* def min = dateToLong(\'2019-04-24T17:25:00.000\')\n* def max = dateToLong(\'2019-04-24T17:50:00.000\')\n* def isValid = function(x){ var temp = dateToLong(x); return temp >= min && temp <= max }\n\n* def response =\n""""""\n[\n""2019-04-24T17:41:27.975"",\n""2019-04-24T17:41:27.954"",\n""2019-04-24T17:41:27.93"",\n""2019-04-24T17:41:27.907"",\n""2019-04-24T17:41:27.886"",\n""2019-04-24T17:41:27.862"",\n""2019-04-24T17:41:27.84"",\n""2019-04-24T17:41:27.816"",\n""2019-04-24T17:41:27.792""\n]\n""""""\n* match each response == \'#? isValid(_)\'\n\nPlease refer the docs if you have doubts about any of the keywords. I removed the first date in the list because it was not consistent, but you have enough info to handle it if needed - you may need some conditional logic somewhere.\nAlso see:\nhttps://stackoverflow.com/a/54114432/143475\nhttps://stackoverflow.com/a/52892797/143475\n']",https://stackoverflow.com/questions/55938266/karate-need-help-to-assert-a-single-dimension-array-for-date-range,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automating telnet session using Bash scripts,"
I am working on automating some telnet related tasks, using Bash scripts.
Once automated, there will be no interaction of the user with telnet (that is, the script will be totally automated).
The scripts looks something like this:
# execute some commands on the local system
# access a remote system with an IP address: 10.1.1.1 (for example)

telnet 10.1.1.1

# execute some commands on the remote system
# log all the activity (in a file) on the local system
# exit telnet
# continue with executing the rest of the script

There are two problems I am facing here:

How to execute the commands on the remote system from the script (without human interaction)?
From my experience with some test code, I was able to deduce that when telnet 10.1.1.1 is executed, telnet goes into an interactive session and the subsequent lines of code in the script are executed on the local system. How can I run the lines of code on the remote system rather than on the local one?

I am unable to get a log file for the activity in the telnet session on the local system. The stdout redirect I used makes a copy on the remote system (I do not want to perform a copy operation to copy the log to the local system). How can I achieve this functionality?


",327k,"
            89
        ","['\nWhile I\'d suggest using expect, too, for non-interactive use the normal shell commands might suffice. telnet accepts its command on stdin, so you just need to pipe or write the commands into it through heredoc:\ntelnet 10.1.1.1 <<EOF\nremotecommand 1\nremotecommand 2\nEOF\n\n(Edit: Judging from the comments, the remote command needs some time to process the inputs or the early SIGHUP is not taken gracefully by telnet. In these cases, you might try a short sleep on the input:)\n{ echo ""remotecommand 1""; echo ""remotecommand 2""; sleep 1; } | telnet 10.1.1.1\n\nIn any case, if it\'s getting interactive or anything, use expect.\n', '\nWrite an expect script.\nHere is an example:\n#!/usr/bin/expect\n\n#If it all goes pear shaped the script will timeout after 20 seconds.\nset timeout 20\n#First argument is assigned to the variable name\nset name [lindex $argv 0]\n#Second argument is assigned to the variable user\nset user [lindex $argv 1]\n#Third argument is assigned to the variable password\nset password [lindex $argv 2]\n#This spawns the telnet program and connects it to the variable name\nspawn telnet $name \n#The script expects login\nexpect ""login:"" \n#The script sends the user variable\nsend ""$user ""\n#The script expects Password\nexpect ""Password:""\n#The script sends the password variable\nsend ""$password ""\n#This hands control of the keyboard over to you (Nice expect feature!)\ninteract\n\nTo run:\n./myscript.expect name user password\n\n', '\nTelnet is often used when you learn the HTTP protocol. I used to use that script as a part of my web scraper:\necho ""open www.example.com 80""\nsleep 2\necho ""GET /index.html HTTP/1.1""\necho ""Host: www.example.com""\necho\necho\nsleep 2\n\nLet\'s say the name of the script is get-page.sh, then this will give you an HTML document:\nget-page.sh | telnet\n\nI hope this will be helpful to someone ;)\n', '\nThis worked for me.. \nI was trying to automate multiple telnet logins which require a username and password. The telnet session needs to run in the background indefinitely since I am saving logs from different servers to my machine.\ntelnet.sh automates telnet login using the \'expect\' command. More info can be found here: http://osix.net/modules/article/?id=30\ntelnet.sh\n#!/usr/bin/expect\nset timeout 20\nset hostName [lindex $argv 0]\nset userName [lindex $argv 1]\nset password [lindex $argv 2]\n\nspawn telnet $hostName\n\nexpect ""User Access Verification""\nexpect ""Username:""\nsend ""$userName\\r""\nexpect ""Password:""\nsend ""$password\\r"";\ninteract\n\nsample_script.sh is used to create a background process for each of the telnet sessions by running telnet.sh. More information can be found in the comments section of the code.\nsample_script.sh\n#!/bin/bash\n#start screen in detached mode with session-name \'default_session\' \nscreen -dmS default_session -t screen_name \n#save the generated logs in a log file \'abc.log\' \nscreen -S default_session -p screen_name -X stuff ""script -f /tmp/abc.log $(printf \\\\r)""\n#start the telnet session and generate logs\nscreen -S default_session -p screen_name -X stuff ""expect telnet.sh hostname username password $(printf \\\\r)""\n\n\nMake sure there is no screen running in the backgroud by using the\ncommand \'screen -ls\'. \nRead\nhttp://www.gnu.org/software/screen/manual/screen.html#Stuff to read\nmore about screen and its options. \n\'-p\' option in sample_script.sh\npreselects and reattaches to a specific window to send a command via\nthe 鈥?X鈥?option otherwise you get a \'No screen session found\' error.\n\n', '\nYou can use expect scripts instaed of bash.\nBelow example show how to telnex into an embedded board having no password\n#!/usr/bin/expect\n\nset ip ""<ip>""\n\nspawn ""/bin/bash""\nsend ""telnet $ip\\r""\nexpect ""\'^]\'.""\nsend ""\\r""\nexpect ""#""\nsleep 2\n\nsend ""ls\\r""\nexpect ""#""\n\nsleep 2\nsend -- ""^]\\r""\nexpect ""telnet>""\nsend  ""quit\\r""\nexpect eof\n\n', '\nFollowing is working for me...\nput all of your IPs you want to telnet in IP_sheet.txt\nwhile true\nread a\ndo\n{\n    sleep 3\n    echo df -kh\n    sleep 3\n    echo exit\n} | telnet $a\ndone<IP_sheet.txt\n\n', ""\nThe answer by @thiton was helpful but I wanted to avoid the sleep command. Also telnet didn't exit the interactive mode, so my script got stuck.\nI solved that by sending telnet command with curl (which seems to wait for the response) and by explicitly telling telnet to quit like this:\ncurl telnet://10.1.1.1:23 <<EOF\nremotecommand 1\nremotecommand 2\nquit\nEOF\n\n"", '\n#!/bin/bash\nping_count=""4""\navg_max_limit=""1500""\nrouter=""sagemcom-fast-2804-v2""\nadress=""192.168.1.1""\nuser=""admin""\npass=""admin""\n\nVAR=$(\nexpect -c "" \n        set timeout 3\n        spawn telnet ""$adress""\n        expect \\""Login:\\"" \n        send \\""$user\\n\\""\n        expect \\""Password:\\""\n        send \\""$pass\\n\\""\n        expect \\""commands.\\""\n        send \\""ping ya.ru -c $ping_count\\n\\""\n        set timeout 9\n        expect \\""transmitted\\""\n        send \\""exit\\""\n        "")\n\ncount_ping=$(echo ""$VAR"" | grep packets | cut -c 1)\navg_ms=$(echo ""$VAR"" | grep round-trip | cut -d \'/\' -f 4 | cut -d \'.\' -f 1)\n\necho ""1_____ping___$count_ping|||____$avg_ms""\necho ""$VAR""\n\n', '\nUse ssh for that purpose. Generate keys without using a password and place it to .authorized_keys at the remote machine. Create the script to be run remotely, copy it to the other machine and then just run it remotely using ssh.\nI used this approach many times with a big success. Also note that it is much more secure than telnet.\n', '\nHere is how to use telnet in bash shell/expect  \n#!/usr/bin/expect\n# just do a chmod 755 one the script\n# ./YOUR_SCRIPT_NAME.sh $YOUHOST $PORT\n# if you get ""Escape character is \'^]\'"" as the output it means got connected otherwise it has failed\n\nset ip [lindex $argv 0]\nset port [lindex $argv 1]\n\nset timeout 5\nspawn telnet $ip $port\nexpect ""\'^]\'.""\n\n', '\nScript for obtain version of CISCO-servers:\n#!/bin/sh\n\nservers=\'\n192.168.34.1\n192.168.34.3\n192.168.34.2\n192.168.34.3\n\'\nuser=\'cisco_login\'\npass=\'cisco_password\'\n\nshow_version() {\nhost=$1\nexpect << EOF\nset timeout 20\nset host $host\nset user $user\nset pass $pass\nspawn telnet $host\nexpect ""Username:""\nsend ""$user\\r""\nexpect ""Password:""\nsend ""$pass\\r""\nexpect -re "".*#""\nsend ""show version\\r""\nexpect -re "".*-More-.*""\nsend "" ""\nexpect -re "".*#""\nsend ""exit\\r""\nEOF\n}\n\nfor ip in $servers; do\n  echo \'---------------------------------------------\'\n  echo ""$ip""\n  show_version $ip | grep -A3 \'SW Version\'\ndone\n\n', '\nHere is a solution that will work with a list of extenders.  This only requires bash - some of the answers above require expect and you may not be able to count on expect being installed.\n#!/bin/bash\n\ndeclare -a  Extenders=(""192.168.1.48"" ""192.168.1.50"" ""192.168.1.51"") \n# ""192.168.1.52"" ""192.168.1.56"" ""192.168.1.58"" ""192.168.1.59"" ""192.168.1.143"")\nsleep 5\n# Iterate the string array using for loop\nfor val in ${Extenders[@]}; do\n   { sleep 0.2; echo ""root""; sleep 0.2; echo ""ls""; sleep 0.2; }  | telnet $val\ndone\n\n', '\nPlay with tcpdump or wireshark and see what commands are sent to the server itself\nTry this\nprintf (printf ""$username\\r\\n$password\\r\\nwhoami\\r\\nexit\\r\\n"") | ncat $target 23\n\nSome servers require a delay with the password as it does not hold lines on the stack\nprintf (printf ""$username\\r\\n"";sleep 1;printf ""$password\\r\\nwhoami\\r\\nexit\\r\\n"") | ncat $target 23**\n\n']",https://stackoverflow.com/questions/7013137/automating-telnet-session-using-bash-scripts,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What are the best ways to automate a GDB debugging session?,"
Does GDB have a built in scripting mechanism, should I code up an expect script, or is there an even better solution out there?  
I'll be sending the same sequence of commands every time and I'll be saving the output of each command to a file (most likely using GDB's built-in logging mechanism, unless someone has a better idea).
",106k,"
            87
        ","['\nBasically, in this example I wanted to get some variable values in particular places of the code; and have them output until the program crashes. So here is first a little program which is guaranteed to crash in a few steps, test.c:\n#include <stdio.h>\n#include <stdlib.h>\n\nint icount = 1; // default value\n\nmain(int argc, char *argv[])\n{\n  int i;\n\n  if (argc == 2) {\n    icount = atoi(argv[1]);\n  }\n\n  i = icount;\n  while (i > -1) {\n    int b = 5 / i;\n    printf("" 5 / %d = %d \\n"", i, b );\n    i = i - 1;\n  }\n\n  printf(""Finished\\n"");\n  return 0;\n}\n\nThe only reason the program accepts command-line arguments is to be able to choose the number of steps before crashing - and to show that gdb ignores --args in batch mode. This I compile with:\ngcc -g test.c -o test.exe\n\nThen, I prepare the following script - the main trick here is to assign a command to each breakpoint, which will eventually continue (see also Automate gdb: show backtrace at every call to function puts). This script I call test.gdb:\n# http://sourceware.org/gdb/wiki/FAQ: to disable the\n# ""---Type <return> to continue, or q <return> to quit---""\n# in batch mode:\nset width 0\nset height 0\nset verbose off\n\n# at entry point - cmd1\nb main\ncommands 1\n  print argc\n  continue\nend\n\n# printf line - cmd2\nb test.c:17\ncommands 2\n  p i\n  p b\n  continue\nend\n\n# int b = line - cmd3\nb test.c:16\ncommands 3\n  p i\n  p b\n  continue\nend\n\n# show arguments for program\nshow args\nprintf ""Note, however: in batch mode, arguments will be ignored!\\n""\n\n# note: even if arguments are shown;\n# must specify cmdline arg for ""run""\n# when running in batch mode! (then they are ignored)\n# below, we specify command line argument ""2"":\nrun 2     # run\n\n#start # alternative to run: runs to main, and stops\n#continue\n\nNote that, if you intend to use it in batch mode, you have to ""start up"" the script at the end, with run or start or something similar.\nWith this script in place, I can call gdb in batch mode - which will generate the following output in the terminal:\n$ gdb --batch --command=test.gdb --args ./test.exe 5\nBreakpoint 1 at 0x804844d: file test.c, line 10.\nBreakpoint 2 at 0x8048485: file test.c, line 17.\nBreakpoint 3 at 0x8048473: file test.c, line 16.\nArgument list to give program being debugged when it is started is ""5"".\nNote, however: in batch mode, arguments will be ignored!\n\nBreakpoint 1, main (argc=2, argv=0xbffff424) at test.c:10\n10    if (argc == 2) {\n$1 = 2\n\nBreakpoint 3, main (argc=2, argv=0xbffff424) at test.c:16\n16      int b = 5 / i;\n$2 = 2\n$3 = 134513899\n\nBreakpoint 2, main (argc=2, argv=0xbffff424) at test.c:17\n17      printf("" 5 / %d = %d \\n"", i, b );\n$4 = 2\n$5 = 2\n 5 / 2 = 2 \n\nBreakpoint 3, main (argc=2, argv=0xbffff424) at test.c:16\n16      int b = 5 / i;\n$6 = 1\n$7 = 2\n\nBreakpoint 2, main (argc=2, argv=0xbffff424) at test.c:17\n17      printf("" 5 / %d = %d \\n"", i, b );\n$8 = 1\n$9 = 5\n 5 / 1 = 5 \n\nBreakpoint 3, main (argc=2, argv=0xbffff424) at test.c:16\n16      int b = 5 / i;\n$10 = 0\n$11 = 5\n\nProgram received signal SIGFPE, Arithmetic exception.\n0x0804847d in main (argc=2, argv=0xbffff424) at test.c:16\n16      int b = 5 / i;\n\nNote that while we specify command line argument 5, the loop still spins only two times (as is the specification of run in the gdb script); if run didn\'t have any arguments, it spins only once (the default value of the program) confirming that --args ./test.exe 5 is ignored.\nHowever, since now this is output in a single call, and without any user interaction, the command line output can easily be captured in a text file using bash redirection, say:\ngdb --batch --command=test.gdb --args ./test.exe 5 > out.txt\n\nThere is also an example of using python for automating gdb in c - GDB auto stepping - automatic printout of lines, while free running?\n', '\ngdb executes file .gdbinit after running.\nSo you can add your commands to this file and see if it is OK for you.\nThis is an example of .gdbinit in order to print backtrace for all f() calls:\nset pagination off\nset logging file gdb.txt\nset logging on\nfile a.out\nb f\ncommands\nbt\ncontinue\nend\ninfo breakpoints\nr\nset logging off\nquit\n\n', '\nIf a -x with a file is too much for you, just use multiple -ex\'s.\nThis is an example to track a running program showing (and saving) the backtrace on crashes\nsudo gdb -p ""$(pidof my-app)"" -batch \\\n  -ex ""set logging on"" \\\n  -ex continue \\\n  -ex ""bt full"" \\\n  -ex quit\n\n']",https://stackoverflow.com/questions/10748501/what-are-the-best-ways-to-automate-a-gdb-debugging-session,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to find the UpgradeCode and ProductCode of an installed application in Windows 7,"
I have an application installed on my machine. I also have its source code but somehow the ProductCode and UpgradeCode of this application were changed.
Now I want to get the UpgradeCode and ProductCode of this installed application.  I feel there must be some tool for this. 
Can anyone kindly let me know how to get the UpgradeCode and ProductCode of an installed application?
",188k,"
            44
        ","['\n\nIMPORTANT: It\'s been a while since this answer was originally posted, and smart people came up with wiser answers. Check How can I find the Upgrade Code for an installed MSI file? from @ Stein 脜smul if you need a solid and comprehensive approach.\n\nHere\'s another way (you don\'t need any tools):\n\nopen system registry and search for HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall key (if it\'s a 32-bit installer on a 64-bit machine, it might be under HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall instead).\nthe GUIDs listed under that key are the products installed on this machine\nfind the one you\'re talking about - just step one by one until you see its name on the right pane\n\nThis GUID you stopped on is the ProductCode.\nNow, if you\'re sure that reinstallation of this application will go fine, you can run the following command line:\n\nmsiexec /i {PRODUCT-CODE-GUID-HERE}\n  REINSTALL=ALL REINSTALLMODE=omus /l*v\n  log.txt\n\nThis will ""repair"" your application. Now look at the log file and search for ""UpgradeCode"". This value is dumped there.\nNOTE: you should only do this if you are sure that reinstall flow is implemented correctly and this won\'t break your installed application. \n', '\nIt takes some time to return results, easily many tens of seconds, but wmic works well and can be scripted:\nwmic product where ""Name like \'%Word%\'"" get Name,Version,IdentifyingNumber\n\nresult:\nIdentifyingNumber                       Name                                      Version\n{90140000-001B-0409-0000-0000000FF1CE}  Microsoft Office Word MUI (English) 2010  14.0.6029.1000\n\nThe IdentifingNumber is the ProductCode. I didn\'t see a property for UpgradeCode, but perhaps it might be buried under something else. See http://quux.wiki.zoho.com/WMIC-Snippets.html for many other examples, including uninstall:\nwmic path win32_product where ""name = \'HP Software Update\'"" call Uninstall\n\nFor UpgradeCode see the excellent and detailed answer to How can I find the Upgrade Code for an installed MSI file?\n', '\nTo everyone using:\nGet-WMIObject win32_product\n\nYou should be aware that this will run a self-heal on every single MSI application installed on the PC. If you were to check eventvwr it will say it has finished reconfiguring each product.\nIn this case i use the following (a mixture of Yan Sklyarenko\'s method):\n$Reg = @( ""HKLM:\\Software\\Wow6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*"", ""HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*"" )\n$InstalledApps = Get-ItemProperty $Reg -EA 0\n$WantedApp = $InstalledApps | Where { $_.DisplayName -like ""*<part of product>*"" }\n\nNow if you were to type:\n$WantedApp.PSChildName\n\nYou would be given the following:\nPS D:\\SCCM> $WantedApp.PSChildName\n{047904BA-C065-40D5-969A-C7D91CA93D62}\n\nIf your organization uses loads of MST\'s whilst installing applications you would want to avoid running self-heals encase they revert some crucial settings.\n\nNote - This will find your product code, then the upgrade can be found as Yan mentioned. I usually, though, just use either \'InstEd It!\' or \'Orca\' then go to the Property table of the MSI and it lists them right at the top.\n\n', ""\nIf you have msi installer open it with Orca (tool from Microsoft), table Property (rows UpgradeCode, ProductCode, Product version etc) or table Upgrade column Upgrade Code.\nTry to find instller via registry: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall find required subkey and watch value InstallSource. Maybe along the way you'll be able to find the MSI file.\n"", '\nPowershell handles tasks like this fairly handily:\n$productCode = (gwmi win32_product | `\n                ? { $_.Name -Like ""<PRODUCT NAME HERE>*"" } | `\n                % { $_.IdentifyingNumber } | `\n                Select-Object -First 1)\n\nYou can then use it to get the uninstall information as well:\n$wow = """"\n$is32BitInstaller = $True # or $False\n\nif($is32BitInstaller -and [System.Environment]::Is64BitOperatingSystem) \n{\n    $wow = ""\\Wow6432Node"" \n}\n\n$regPath = ""HKEY_LOCAL_MACHINE\\SOFTWARE$wow\\Microsoft\\Windows\\CurrentVersion\\Uninstall""\n\ndir ""HKLM:\\SOFTWARE$wow\\Microsoft\\Windows\\CurrentVersion\\Uninstall"" | `\n? { $_.Name -Like ""$regPath\\$productCode""  }\n\n', '\nYou can use the MsiEnumProductsEx and MsiGetProductInfoEx methods to enumerate all the installed applications on your system and match the data to your application\n', '\nIn Windows 10 preview build with PowerShell 5, I can see that you can do:\n$info = Get-Package -Name YourInstalledProduct\n$info.Metadata[""ProductCode""]\n\nNot familiar with even not sure if all products has UpgradeCode, but  according to this post you need to search UpgradeCode from this registry path:\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Installer\\UpgradeCodes\n\nUnfortunately, the registry key values are the ProductCode and the registry keys are the UpgradeCode.\n', '\nIf anyone wants to get installed application package code, just execute below command with your application name in the command prompt. You will be getting product code along with package code.\nwmic product where ""Name like \'%YOUR_APPLICATION_NAME%\'"" get IdentifyingNumber, PackageCode\n', '\nAnother way-too-complicated workaround, with the benefit of not having to re-install the application as the previous workaround required. This requires that you have access to the msi (or a setup.exe with the msi embedded).\nIf you have Visual Studio 2012 (or possibly other editions) and install the free ""InstallShield LE"", then you can create a new setup project using InstallShield.\nOne of the configuration options in the ""Organize your Setup"" step is called ""Upgrade Paths"".  Open the properties for Upgrade Paths, and in the left pane right click ""Upgrade Paths"" and select ""New Upgrade Path"" ... now browse to the msi (or setup.exe containing the msi) and click ""open"".  The upgrade code will be populated for you in the settings page in the right pane which you should now see.\n', '\nHadn\'t found any way of finding out the UpgradeCode from an installed application, before seeing Yan Sklyarenko\'s workaround (currently) above. But if you/anyone else would find a way of finding out (at least) both UpgradeCode and ProductCode from a MSI, read on.\nFrom http://www.dwarfsoft.com/blog/2010/06/22/msi-package-code-fun/, modified to allow (when launched with wscript.exe) one popup box of info per MSI (Trunicated at 1023 chars, due to wscript.echo limitation); able to input MSI(s) from the GUI as well as the CLI; some basic human input validation; removed debug code (\' Set oDatabase) and 1 bug fix (DB.OpenView).\n\'Created by:   Chris Bennett\n\'Created Date: 22/06/2010\n\'Description:\n\'   Opens up MSI file(s) Passed as Arguments & returns ProductName, ProductCode,\n\'   The HKCR key created from ProductCode (a Packed GUID of ProductCode), the \n\'   PackageCode and the UpgradeCode of the MSI. Much quicker than getting these\n\'   out of the MSI\'s the Manual Way.\n\nReferences:\nhttp://msdn.microsoft.com/en-us/library/aa369794%28VS.85%29.aspx\nhttp://www.eggheadcafe.com/forumarchives/platformsdkmsi/Jan2006/post25948124.asp\nif wscript.arguments.count = 0 then\n  MSIs = inputbox(""Enter in * delimited list of MSI\'s to query (Max 254 characters)"", ""MSI Product Details"")\n  MSIs = split(MSIs,""*"")\nelse\n  set MSIs = wscript.arguments\nend if\n\nset objFS = createobject(""scripting.filesystemobject"")\nFor Each MSIPath in MSIs\n  if objFS.fileexists(MSIPath) then\n    Set MSIDetails = EvaluateMSI(MSIPath)\n    MSIDetails = MSIPath & "": "" & vbcrlf & vbcrlf & ""Product Name: "" &_\n    MSIDetails(""ProductName"") & vbcrlf & ""Product Code: "" &_\n    MSIDetails(""ProductCode"") & vbcrlf & ""Product Key : "" &_\n    ""HKCR\\Installer\\Products\\"" & PackGUID(MSIDetails(""ProductCode"")) &_\n    vbcrlf & ""Package Code: "" & MSIDetails(""PackageCode"") & vbcrlf &_\n    ""Upgrade Code: "" & MSIDetails(""UpgradeCode"") & vbcrlf\n    WScript.Echo MSIDetails\n  else\n    wscript.echo ""Inaccessible; Non-existant; or Error in Path for:"" & vbcrlf & MSIPath & vbcrlf & ""... skipping""\n  end if\nNext\n\nFunction EvaluateMSI(MSIPath)\n  On Error Resume Next\n  \' create installer object\n  Set oInstaller = CreateObject(""WindowsInstaller.Installer"")\n  \' open msi in read-only mode\n  Set oDatabase = oInstaller.OpenDatabase(MSIPath, 0)\n  Set objDictionary = CreateObject(""Scripting.Dictionary"")\n  \' Get Package Code from Summary Information Stream   \n  Set streamobj = oDatabase.SummaryInformation(0) \'0 = read only\n  objDictionary(""PackageCode"") = streamobj.Property(9)\n  \' Get Product Name from MSI Database\n  Set View = oDatabase.OpenView(""Select `Value` From Property WHERE `Property`=\'ProductName\'"")\n  View.Execute\n  Set ProductName = View.Fetch\n  objDictionary(""ProductName"") = ProductName.StringData(1)\n\n  \' Get Product Code from MSI Database\n  Set View = oDatabase.OpenView(""Select `Value` From Property WHERE `Property`=\'ProductCode\'"")\n  View.Execute\n  Set ProductCode = View.Fetch\n  objDictionary(""ProductCode"") = ProductCode.StringData(1)\n\n  \' Get Upgrade Code from MSI Database\n  Set View = oDatabase.OpenView(""Select `Value` From Property WHERE `Property`=\'UpgradeCode\'"")\n  View.Execute\n  Set UpgradeCode = View.Fetch\n  objDictionary(""UpgradeCode"") = UpgradeCode.StringData(1)\n\n  Set EvaluateMSI = objDictionary\n  On Error Goto 0\nEnd Function\n\nFunction PackGUID(guid)  \n  PackGUID = """"  \n  \'*  \n  Dim temp  \n  temp = Mid(guid,2,Len(guid)-2)  \n  Dim part  \n  part = Split(temp,""-"")  \n  Dim pack  \n  pack = """"  \n  Dim i, j  \n  For i = LBound(part) To UBound(part)\n    Select Case i\n      Case LBound(part), LBound(part)+1, LBound(part)+2\n        For j = Len(part(i)) To 1 Step -1  \n          pack = pack & Mid(part(i),j,1)  \n        Next  \n      Case Else\n        For j = 1 To Len(part(i)) Step 2  \n          pack = pack & Mid(part(i),j+1,1) & Mid(part(i),j,1)  \n      Next  \n    End Select\n  Next  \n  \'*  \n  PackGUID = pack  \nEnd Function\n\nIf one needs to copy&paste any of the GUID\'s in the popup, I tend to find it easiest to use a subsequent inputbox, like inputbox """","""",MSIDetails\n', ""\nIf you don't have the msi and you need the upgrade code, rather than the product code then the answer is here: How can I find the upgrade code for an installed application in C#?\n""]",https://stackoverflow.com/questions/5063129/how-to-find-the-upgradecode-and-productcode-of-an-installed-application-in-windo,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to print every executed line in GDB automatically until a given breakpoint is reached?,"
I would like to be able to set a breakpoint in GDB, and have it run to that point - and in the process, print out lines it has ""stepped through"".
Here is an example, based on this simple file with a main and a function, and two breakpoints for each: 
$ cat > test.c <<EOF
#include ""stdio.h""

int count=0;

void doFunction(void) {
  // two steps forward
  count += 2;
  // one step back
  count--;
}

int main(void) {
  // some pointless init commands;
  count = 1;
  count += 2;
  count = 0;
  //main loop
  while(1) {
    doFunction();
    printf(""%d\n"", count);
  }
}
EOF

$ gcc -g -Wall test.c -o test.exe
$ chmod +x test.exe
$ gdb -se test.exe
...
Reading symbols from /path/to/test.exe...done.
(gdb) b main
Breakpoint 1 at 0x80483ec: file test.c, line 14.
(gdb) b doFunction
Breakpoint 2 at 0x80483c7: file test.c, line 7.

To start the session, I need to run (r) the program, which will then stop at first breakpoint (main):
(gdb) r
Starting program: /path/to/test.exe 

Breakpoint 1, main () at test.c:14
14    count = 1;
(gdb) 

At this point - I can, for instance, hit continue (c); and the process will run through, not outputing anything, and break at the requested line: 
(gdb) c
Continuing.

Breakpoint 2, doFunction () at test.c:7
7     count += 2;
(gdb)

On the other hand, instead of continue - I can go line by line, either by using step (s) or next (n); for instance:
14    count = 1;
(gdb) n
15    count += 2;
(gdb) s
16    count = 0;
(gdb) s
19      doFunction();
(gdb) s

Breakpoint 2, doFunction () at test.c:7
7     count += 2;
(gdb) s
9     count--;
(gdb) s
10  }
(gdb) s
main () at test.c:20
20      printf(""%d\n"", count);
(gdb) s
...
(gdb) s
_IO_vfprintf_internal (s=Cannot access memory at address 0xe5853361
) at vfprintf.c:210
210 vfprintf.c: No such file or directory.
    in vfprintf.c
(gdb) s
245 in vfprintf.c
(gdb) s
210 in vfprintf.c
(gdb) n
245 in vfprintf.c
...
(gdb) n
2006    in vfprintf.c
(gdb) n
__printf (format=0x80484f0 ""%d\n"") at printf.c:39
39  printf.c: No such file or directory.
    in printf.c
(gdb) n
main () at test.c:21
21    }
(gdb) n
19      doFunction();
(gdb) n

Breakpoint 2, doFunction () at test.c:7
7     count += 2;
(gdb) 

Anyways, I am aware that I can keep Enter pressed, and the last entered command (step or next) will repeat (left a bit longer session in the second case, to show that 'next' remains on same level, 'step' steps inside the functions being called). However, as it can be seen, depending on whether step or next runs, it may take a while until a result is reached - and so, I don't want to sit for 10 minutes with my hand stuck on the Enter button :) 
So, my question is - can I somehow instruct gdb to run to 'breakpoint 2' without further user intervention - while printing out the lines it goes through, as if step (or next) was pressed?
",22k,"
            34
        ","['\nWell, this wasn\'t easy - but I think I somewhat got it :) I went through a bunch of failed attempts (posted here); relevant code is below.\nBasically, the problem in a ""next/step until breakpoint"" is how to determine whether you\'re ""on"" a breakpoint or not, if the debugger is stopped (at a step). Note also I use GDB 7.2-1ubuntu11 (current for Ubuntu 11.04). So, it went like this:\n\nI first found about Convenience Variables, and thought - given there are program counters and such available, there must be some GDB convenience variable that gives the ""breakpoint"" status, and can be used directly in a GDB script. After looking through GDB reference Index for a while, however, I simply cannot find any such variables (my attempts are in nub.gdb)\nIn lack of such a ""breakpoint status"" internal variable - the only thing left to do, is to capture the (\'stdout\') command line output of GDB (in response to commands) as a string, and parse it (looking for ""Breakpoint"")\nThen, I found out about Python API to GDB, and the gdb.execute(""CMDSTR"", toString=True) command - which is seemingly exactly what is needed to capture the output: ""By default, any output produced by command is sent to gdb\'s standard output. If the to_string parameter is True, then output will be collected by gdb.execute and returned as a string[1]""!\n\n\nSo, first I tried to make a script (pygdb-nub.py,gdbwrap) that would utilize gdb.execute in the recommended manner; failed here - because of this:\n\n\nBug 627506 鈥?python: gdb.execute([...], to_string=True) partly prints to stdout/stderr\nBug 10808 鈥?Allow GDB/Python API to capture and store GDB output\n\nThen, I thought I\'d use a python script to subprocess.Popen the GDB program, while replacing its stdin and stdout; and then proceed controlling GDB from there (pygdb-sub.py) - that failed too... (apparently, because I didn\'t redirect stdin/out right)\nThen, I thought I\'d use python scripts to be called from GDB (via source) which would internally fork into a pty whenever gdb.execute should be called, so as to capture its output (pygdb-fork.gdb,pygdb-fork.py)... This almost worked - as there are strings returned; however GDB notices something ain\'t right: ""[tcsetpgrp failed in terminal_inferior: Operation not permitted]"", and the subsequent return strings don\'t seem to change.\n\n\nAnd finally, the approach that worked is: temporarily redirecting the GDB output from a gdb.execute to a logfile in RAM (Linux: /dev/shm); and then reading it back, parsing it and printing it from python - python also handles a simple while loop that steps until a breakpoint is reached.\nThe irony is - most of these bugs, that caused this solution via redirecting the logfile, are actually recently fixed in SVN; meaning those will propagate to the distros in the near future, and one will be able to use gdb.execute(""CMDSTR"", toString=True) directly :/ Yet, as I cannot risk building GDB from source right now (and possibly bumping into possible new incompatibilites), this is good enough for me also :)\n\xa0\nHere are the relevant files (partially also in pygdb-fork.gdb,pygdb-fork.py):\npygdb-logg.gdb is:\n# gdb script: pygdb-logg.gdb\n# easier interface for pygdb-logg.py stuff\n# from within gdb: (gdb) source -v pygdb-logg.gdb\n# from cdmline: gdb -x pygdb-logg.gdb -se test.exe\n\n# first, ""include"" the python file:\nsource -v pygdb-logg.py\n\n# define shorthand for nextUntilBreakpoint():\ndefine nub\n  python nextUntilBreakpoint()\nend\n\n# set up breakpoints for test.exe:\nb main\nb doFunction\n\n# go to main breakpoint\nrun\n\npygdb-logg.py is:\n# gdb will \'recognize\' this as python\n#  upon \'source pygdb-logg.py\'\n# however, from gdb functions still have\n#  to be called like:\n#  (gdb) python print logExecCapture(""bt"")\n\nimport sys\nimport gdb\nimport os\n\ndef logExecCapture(instr):\n  # /dev/shm - save file in RAM\n  ltxname=""/dev/shm/c.log""\n\n  gdb.execute(""set logging file ""+ltxname) # lpfname\n  gdb.execute(""set logging redirect on"")\n  gdb.execute(""set logging overwrite on"")\n  gdb.execute(""set logging on"")\n  gdb.execute(instr)\n  gdb.execute(""set logging off"")\n\n  replyContents = open(ltxname, \'r\').read() # read entire file\n  return replyContents\n\n# next until breakpoint\ndef nextUntilBreakpoint():\n  isInBreakpoint = -1;\n  # as long as we don\'t find ""Breakpoint"" in report:\n  while isInBreakpoint == -1:\n    REP=logExecCapture(""n"")\n    isInBreakpoint = REP.find(""Breakpoint"")\n    print ""LOOP:: "", isInBreakpoint, ""\\n"", REP\n\n\xa0\nBasically, pygdb-logg.gdb loads the pygdb-logg.py python script, sets up the alias nub for nextUntilBreakpoint, and initializes the session - everything else is handled by the python script. And here is a sample session - in respect to the test source in OP:\n$ gdb -x pygdb-logg.gdb -se test.exe\n...\nReading symbols from /path/to/test.exe...done.\nBreakpoint 1 at 0x80483ec: file test.c, line 14.\nBreakpoint 2 at 0x80483c7: file test.c, line 7.\n\nBreakpoint 1, main () at test.c:14\n14    count = 1;\n(gdb) nub\nLOOP::  -1\n15    count += 2;\n\nLOOP::  -1\n16    count = 0;\n\nLOOP::  -1\n19      doFunction();\n\nLOOP::  1\n\nBreakpoint 2, doFunction () at test.c:7\n7     count += 2;\n\n(gdb) nub\nLOOP::  -1\n9     count--;\n\nLOOP::  -1\n10  }\n\nLOOP::  -1\nmain () at test.c:20\n20      printf(""%d\\n"", count);\n\n1\nLOOP::  -1\n21    }\n\nLOOP::  -1\n19      doFunction();\n\nLOOP::  1\n\nBreakpoint 2, doFunction () at test.c:7\n7     count += 2;\n\n(gdb)\n\n... just as I wanted it :P Just don\'t know how reliable it is (and whether it will be possible to use in avr-gdb, which is what I need this for :) EDIT: version of avr-gdb in Ubuntu 11.04 is currently 6.4, which doesn\'t recognize the python command :()\n\xa0\nWell, hope this helps someone,\nCheers!\n\xa0\nHere some references:\n\nGDB: error detected on stdin\nGDB has problems with getting commands piped to STDIN\nRe: [Gdb] How do i use GDB other input?\ngdb doesn\'t accept input on stdin\nUsing gdb in an IDE - comp.os.linux.development.apps | Google Groups\nrmathew: Terminal Sickness\n[TUTORIAL] Calling an external program in C (Linux) - GIDForums\nshell - how to use multiple arguments with a shebang (i.e. #!)? - Stack Overflow\nRedirecting/storing output of shell into GDB variable? - Stack Overflow\nCorey Goldberg: Python - Redirect or Turn Off STDOUT and STDERR\nThe Cliffs of Inanity 鈥?9. Scripting gdb\ngdb python scripting: where has parse_and_eval gone? - Stack Overflow\nshell - Invoke gdb to automatically pass arguments to the program being debugged - Stack Overflow\nStoring Files/Directories In Memory With tmpfs | HowtoForge - Linux Howtos and Tutorials\nsimple way to touch a file if it does not exist | Python | Python\nos.fork() different in cgi-script? - Python\njava - Writing tests that use GDB - how to capture output? - Stack Overflow\nDebugging with GDB: How to create GDB Commands in Python - Wiki\nGDB reference card\n\n', '\nWhat about doing it like this in gdb, using a command file. Change file argument, and loop count as required.\ngdb -x run.gdb\n\nrun.gdb:\nset pagination off\nset logging file gdb.log\nset logging on\nset $i = 0\nfile main\nbreak main\nbreak WriteData\n# sadly, commands not getting executed on reaching breakpoint 2\ncommands 2\n  set $i=1000\n  print ""commands 2 : %d"",$i\nend\nrun\nwhile ( $i < 1000 )\n  step\n  # next\n  # continue\n  set $i = $i + 1\nend\n\n', ""\nBased on the link in @sdaau's answer (http://www.mail-archive.com/gdb@gnu.org/msg00031.html), I created my own script to simply keep sending 's' and reading the output of gdb continuously, while printing output to textfile and terminal, of course, my script can be modified to fit anyone else's needs, however, I hope that the modification I made should fit most people needs.\nhttp://www.codeground.net/coding/gdb-step-into-all-lines-to-get-full-application-flow/\nwget http://www.codeground.net/downloads/gdbwalkthrough.c\ngcc gdbwalkthrough.c -o gdbwalkthrough\n./gdbwalkthrough <application full path> [application arguments]\n\n"", '\nAs a new answer, since the previous is already hogged :) Basically, if the point is to observe execution of source (and/or assembly) code lines as the program as running - as the motivation is often for me when looking into ""automatic printout"" -- then, basically, a very quick way is to use GDB TUI mode; I quote:\nc - gdb behavior : value optimized out - Stack Overflow #1354762\n\nUse the GDB TUI mode. My copy of GDB enables it when I type the minus and Enter. Then type C-x 2 (that is hold down Control and press X, release both and then press 2). That will put it into split source and disassembly display. Then use stepi and nexti to move one machine instruction at a time. Use C-x o to switch between the TUI windows.\n\nThe trick here is that, even if you hit continue - this time source will be shown and indicated on the TUI; and followed as the program runs: \n \n... and this for me avoids many situations where I\'d have to script the breakpoints in ""auto-stepping context"" (although there are still such situations).. Docs about TUI: TUI - Debugging with GDB\nCheers!\n', '\nActually, I have a Github repo with a Python-GDB extension, which does exactly the same thing as You have described, but with some more functionality.\nYou can just clone the repo:\ngit clone https://github.com/Viaceslavus/gdb-debug-until.git\n\nand feed the python script to GDB with the following command inside GDB:\nsource gdb-debug-until/debug_until.py\n\n(Change python script path if necessary)\nAnd now you can use the following command to run through each line of your code until a breakpoint:\ndebug-until somefile.c:100 --args="""" --end=""somefile.c:200""\n\n""somefile.c:100"" here is a starting breakpoint, and ""somefile.c:200"" is the final breakpoint.\n""--args"" specifies a set of arguments to your program (you can omit it if there are no arguments).\nWith this extension you can also run few times through the code (with \'-r\' option) and even specify some events that should be handled while debugging. For more info see:\nhttps://github.com/Viaceslavus/gdb-debug-until\n', '\nThe currently accepted answer includes a lot of file io and does only stop on breakpoints, but watchpoints, signals and possibly even the program end is ignored.\nUsing the python api this can be handled nicely:\n\ndefine a user command (with additional argument to say how fast to auto-step)\noptional: define a parameter for the default (both variants below)\ndo the while loop within python, handle the ""expected"" keyboard interrupt of CTRL-C\nregister a stop event handler that checks for the stop reason and store the kind of step there\nadjust the while loop to stop for a ""not simple"" stop (breakpoint/watchpoint/signal/...)\n\nThe following code may be placed in a gdb-auto-step.py which can be made active with source gdb-auto-step.py whenever you want that (or include in the .gdbinit file to make it always available):\nimport gdb\nimport time\nimport traceback\n\nclass CmdAutoStep (gdb.Command):\n    """"""Auto-Step through the code until something happens or manually interrupted.\nAn argument says how fast auto stepping is done (1-19, default 5).""""""\n    def __init__(self):\n        print(\'Registering command auto-step\')\n        super(CmdAutoStep, self).__init__(""auto-step"", gdb.COMMAND_RUNNING)\n        gdb.events.stop.connect(stop_handler_auto_step)\n    def invoke(self, argument, from_tty):\n        # sanity check - are we even active, prevents a spurious ""no registers"" exception\n        try:\n            gdb.newest_frame()\n        except gdb.error:\n            raise gdb.GdbError(""The program is not being run."")\n\n        # calculate sleep time\n        if argument:\n            if not argument.isdigit():\n                raise gdb.GdbError(""argument must be a digit, not "" + argument)\n            number = int(argument)\n            if number == 0 or number > 19:\n                raise gdb.GdbError(""argument must be a digit between 1 and 19"")   \n        sleep_time = 3.0 / (1.4 ** number)\n\n        # activate GDB scrolling, otherwise we\'d auto-step only one page\n        pagination = gdb.parameter(""pagination"")\n        if pagination:\n            gdb.execute(""set pagination off"", False, False)\n\n        # recognize the kind of stop via stop_handler_auto_step \n        global last_stop_was_simple\n        last_stop_was_simple = True\n\n        # actual auto-stepping\n        try:\n            while last_stop_was_simple:\n                gdb.execute(""step"")\n                time.sleep(sleep_time)\n        # we just quit the loop as requested\n        # pass keyboard and user errors unchanged\n        except (KeyboardInterrupt, gdb.GdbError):\n            raise\n        # that exception is unexpected, but we never know...\n        except Exception:\n            traceback.print_exc()\n        # never leave without cleanup...\n        finally:\n            if pagination:\n                gdb.execute(""set pagination on"", False, False)\n\ndef stop_handler_auto_step(event):\n    # check the type of stop, the following is the common one after step/next,\n    # a more complex one would be a subclass (for example breakpoint or signal)\n    global last_stop_was_simple\n    last_stop_was_simple = type(event) is gdb.StopEvent\n\nCmdAutoStep()\n\n\nTo specify the default with a parameter (aka ""the gdb way"") add another parameter and use it (comes also with 0 = unlimited)\nimport gdb\nimport time\nimport traceback\n\nclass ParameterAutoStep (gdb.Parameter):\n    """"""auto-step default speed (0-19, default 5)""""""\n    def __init__(self):\n        self.set_doc = """"""Set speed for ""auto-step"", internally used to calculate sleep time between ""step""s.\nset ""auto-step 0"" causes there to be no sleeping.""""""\n        self.show_doc = ""Speed value for auto-step.""\n        super(ParameterAutoStep, self).__init__(""auto-step"", gdb.COMMAND_RUNNING, gdb.PARAM_UINTEGER)\n        self.value = 5\n        self.backup = self.value\n\n    def get_set_string (self):\n        try:\n            self.value = int(ParameterAutoStep.validate(self.value))\n        except gdb.GdbError:\n            self.value = int (self.backup)\n            raise\n        self.backup = self.value\n        return """"\n\n    @staticmethod\n    def validate (argument):\n        """"""validation for auto-step speed""""""\n        try:\n            speed = int(argument)\n            if speed < 0 or speed > 19:\n                raise ValueError()\n        except (TypeError, ValueError):\n            raise gdb.GdbError(""speed-argument must be an integer between 1 and 19, or 0"")\n        return speed\n\nclass CmdAutoStep (gdb.Command):\n    """"""Auto-Step through the code until something happens or manually interrupted.\nAn argument says how fast auto stepping is done (see parameter ""auto-step"").""""""\n    def __init__(self):\n        print(\'Registering command and parameter auto-step\')\n        super(CmdAutoStep, self).__init__(""auto-step"", gdb.COMMAND_RUNNING)\n        self.defaultSpeed = ParameterAutoStep()\n        gdb.events.stop.connect(stop_handler_auto_step)\n\n    def invoke(self, argument, from_tty):\n        # sanity check - are we even active, prevents a spurious ""no registers"" exception\n        try:\n            gdb.newest_frame()\n        except gdb.error:\n            raise gdb.GdbError(""The program is not being run."")\n\n        # calculate sleep time\n        if argument:\n            number = ParameterAutoStep.validate(argument) # raises an error if not valid\n        else:\n            number = self.defaultSpeed.value\n        if number:\n            sleep_time = 3.0 / (1.4 ** number)\n        else:\n            sleep_time = 0\n\n        # activate GDB scrolling, otherwise we\'d auto-step only one page\n        pagination = gdb.parameter(""pagination"")\n        if pagination:\n            gdb.execute(""set pagination off"", False, False)\n\n        # recognize the kind of stop via stop_handler_auto_step \n        global last_stop_was_simple\n        last_stop_was_simple = True\n\n        # actual auto-stepping\n        try:\n            while last_stop_was_simple:\n                gdb.execute(""step"")\n                time.sleep(sleep_time)\n        # we just quit the loop as requested\n        # pass keyboard and user errors unchanged\n        except (KeyboardInterrupt, gdb.GdbError):\n            raise\n        # that exception is unexpected, but we never know...\n        except Exception:\n            traceback.print_exc()\n        # never leave without cleanup...\n        finally:\n            if pagination:\n                gdb.execute(""set pagination on"", False, False)\n\ndef stop_handler_auto_step(event):\n    # check the type of stop, the following is the common one after step/next,\n    # a more complex one would be a subclass (for example breakpoint or signal)\n    global last_stop_was_simple\n    last_stop_was_simple = type(event) is gdb.StopEvent\n\nCmdAutoStep()\n\n\n']",https://stackoverflow.com/questions/6947389/how-to-print-every-executed-line-in-gdb-automatically-until-a-given-breakpoint-i,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to use WebBrowser control DocumentCompleted event in C#?,"
Before starting writing this question, i was trying to solve following
// 1. navigate to page
// 2. wait until page is downloaded
// 3. read and write some data from/to iframe 
// 4. submit (post) form

The problem was, that if a iframe exists on a web page, DocumentCompleted event would get fired more then once (after each document has been completed). It was highly likely that program would have tried to read data from DOM that was not completed and naturally - fail.
But suddenly while writing this question 'What if' monster inspired me, and i fix'ed the problem, that i was trying to solve. As i failed Google'ing this, i thought it would be nice to post it here.
    private int iframe_counter = 1; // needs to be 1, to pass DCF test
    public bool isLazyMan = default(bool);

    /// <summary>
    /// LOCK to stop inspecting DOM before DCF
    /// </summary>
    public void waitPolice() {
        while (isLazyMan) Application.DoEvents();
    }

    private void webBrowser1_Navigating(object sender, WebBrowserNavigatingEventArgs e) {
        if(!e.TargetFrameName.Equals(""""))
            iframe_counter --;
        isLazyMan = true;
    }

    private void webBrowser1_DocumentCompleted(object sender, WebBrowserDocumentCompletedEventArgs e) {
        if (!((WebBrowser)sender).Document.Url.Equals(e.Url))
            iframe_counter++;
        if (((WebBrowser)sender).Document.Window.Frames.Count <= iframe_counter) {//DCF test
            DocumentCompletedFully((WebBrowser)sender,e);
            isLazyMan = false; 
        }
    }

    private void DocumentCompletedFully(WebBrowser sender, WebBrowserDocumentCompletedEventArgs e){
        //code here
    }

For now at least, my 5m hack seems to be working fine. 
Maybe i am really failing at querying google or MSDN, but i can not find:
""How to use webbrowser control DocumentCompleted event in C# ?""
Remark: After learning a lot about webcontrol, I found that it does FuNKY stuff. 
Even if you detect that the document has completed, in most cases it wont stay like that forever. Page update can be done in several ways - frame refresh, ajax like request or server side push (you need to have some control that supports asynchronous communication and has html or JavaScript interop). Also some iframes will never load, so it's not best idea to wait for them forever. 
I ended up using:
if (e.Url != wb.Url)

",52k,"
            14
        ","['\nYou might want to know the AJAX calls as well.\nConsider using this:\nprivate void webBrowser_DocumentCompleted(object sender, WebBrowserDocumentCompletedEventArgs e)\n{\n    string url = e.Url.ToString();\n    if (!(url.StartsWith(""http://"") || url.StartsWith(""https://"")))\n    {\n            // in AJAX\n    }\n\n    if (e.Url.AbsolutePath != this.webBrowser.Url.AbsolutePath)\n    {\n            // IFRAME \n    }\n    else\n    {\n            // REAL DOCUMENT COMPLETE\n    }\n}\n\n', '\nI have yet to find a working solution to this problem online. Hopefully this will make it to the top and save everyone the months of tweaking I spent trying to solve it, and the edge cases associated with it. I have fought over this issue over the years as Microsoft has changed the implementation/reliability of isBusy and document.readystate. With IE8, I had to resort to the following solution. It\'s similar to the question/answer from Margus with a few exceptions. My code will handle nested frames, javascript/ajax requests and meta-redirects. I have simplified the code for clarity sake, but I also use a timeout function (not included) to reset the webpage after if 5 minutes domAccess still equals false.\nprivate void m_WebBrowser_BeforeNavigate(object pDisp, ref object URL, ref object Flags, ref object TargetFrameName, ref object PostData, ref object Headers, ref bool Cancel)\n{\n    //Javascript Events Trigger a Before Navigate Twice, but the first event \n    //will contain javascript: in the URL so we can ignore it.\n    if (!URL.ToString().ToUpper().StartsWith(""JAVASCRIPT:""))\n    {\n        //indicate the dom is not available\n        this.domAccess = false;\n        this.activeRequests.Add(URL);\n    }\n}\n\nprivate void m_WebBrowser_DocumentComplete(object pDisp, ref object URL) \n{\n\n    this.activeRequests.RemoveAt(0);\n\n    //if pDisp Matches the main activex instance then we are done.\n    if (pDisp.Equals((SHDocVw.WebBrowser)m_WebBrowser.ActiveXInstance)) \n    {\n        //Top Window has finished rendering \n        //Since it will always render last, clear the active requests.\n        //This solves Meta Redirects causing out of sync request counts\n        this.activeRequests.Clear();\n    }\n    else if (m_WebBrowser.Document != null)\n    {\n        //Some iframe completed dom render\n    }\n\n    //Record the final complete URL for reference\n    if (this.activeRequests.Count == 0)\n    {\n        //Finished downloading page - dom access ready\n        this.domAccess = true;\n    }\n}\n\n', '\nUnlike Thorsten I didn\'t have to use ShDocVw, but what did make the difference for me was adding the loop checking ReadyState and using Application.DoEvents() while not ready.  Here is my code:\n        this.webBrowser.DocumentCompleted += new WebBrowserDocumentCompletedEventHandler(WebBrowser_DocumentCompleted);\n        foreach (var item in this.urlList) // This is a Dictionary<string, string>\n        {\n            this.webBrowser.Navigate(item.Value);\n            while (this.webBrowser1.ReadyState != WebBrowserReadyState.Complete)\n            {\n                Application.DoEvents();\n            }\n        }\n\nAnd I used Yuki\'s solution for checking the results of WebBrowser_DocumentCompleted, though with the last if/else swapped per user\'s comment:\n     private void WebBrowser_DocumentCompleted(object sender, WebBrowserDocumentCompletedEventArgs e)\n    {\n        string url = e.Url.ToString();\n        var browser = (WebBrowser)sender;\n\n        if (!(url.StartsWith(""http://"") || url.StartsWith(""https://"")))     \n        {             \n            // in AJAX     \n        }\n        if (e.Url.AbsolutePath != this.webBrowser.Url.AbsolutePath)     \n        {\n            // IFRAME           \n        }     \n        else     \n        {             \n            // REAL DOCUMENT COMPLETE\n            // Put my code here\n        }\n    }\n\nWorked like a charm :)\n', '\nI had to do something similar. What I do is use ShDocVw directly (adding a reference to all the necessary interop assemblies to my project). Then, I do not add the WebBrowser control to my form, but the AXShDocVw.AxWebBrowser control.\nTo navigate and wait I use to following method:\nprivate void GotoUrlAndWait(AxWebBrowser wb, string url)\n{\n    object dummy = null;\n    wb.Navigate(url, ref dummy, ref dummy, ref dummy, ref dummy);\n\n    // Wait for the control the be initialized and ready.\n    while (wb.ReadyState != SHDocVw.tagREADYSTATE.READYSTATE_COMPLETE)\n        Application.DoEvents();\n}\n\n', '\nJust thought to drop a line or two here about a small improvement which works in conjunction with the code of FeiBao. The idea is to inject a landmark (javascript) variable in the webpage and use that to detect which of the subsequent DocumentComplete events is the real deal. I doubt it\'s bulletproof but it has worked more reliably in general than the approach that lacks it. Any comments welcome. Here is the boilerplate code:\n void WebBrowser_DocumentCompleted(object sender, WebBrowserDocumentCompletedEventArgs e)\n    {\n        string url = e.Url.ToString();\n        var browser = (WebBrowser)sender;\n\n        if (!(url.StartsWith(""http://"") || url.StartsWith(""https://"")))\n        {\n            // in AJAX     \n        }\n        if (e.Url.AbsolutePath != this.webBrowser.Url.AbsolutePath)\n        {\n            // IFRAME           \n        }\n        else if (browser.Document != null && (bool)browser.Document.InvokeScript(""eval"", new object[] { @""typeof window.YourLandMarkJavascriptVariableHere === \'undefined\'"" }))\n        {\n            ((IHTMLWindow2)browser.Document.Window.DomWindow).execScript(""var window.YourLandMarkJavascriptVariableHere = true;"");\n\n            // REAL DOCUMENT COMPLETE\n            // Put my code here\n        }\n    }\n\n']",https://stackoverflow.com/questions/840813/how-to-use-webbrowser-control-documentcompleted-event-in-c,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automate saveas dialogue for IE9 (vba),"
I am trying to download an excel sheet from a website. I have thus far achieved until clicking the download button automatically (web scraping). Now ie9 is popping a save as screen. How do i automate that?
",15k,"
            9
        ","['\nYou may try this as it is worked for me on IE9:\n\n\nCopy file C:\\Windows\\System32\\UIAutomationCore.dll file to users Documents i.e C:\\Users\\admin\\Documents then add reference UIAutomationClient to your macro file. \nPaste below code in your module: \n    Option Explicit\n    Dim ie As InternetExplorer\n    Dim h As LongPtr\n    Private Declare PtrSafe Function FindWindowEx Lib ""user32"" Alias ""FindWindowExA"" (ByVal hWnd1 As LongPtr, ByVal hWnd2 As LongPtr, ByVal lpsz1 As String, ByVal lpsz2 As String) As LongPtr\n\nSub Download()\n    Dim o As IUIAutomation\n    Dim e As IUIAutomationElement\n    Set o = New CUIAutomation\n    h = ie.Hwnd\n    h = FindWindowEx(h, 0, ""Frame Notification Bar"", vbNullString)\n    If h = 0 Then Exit Sub\n\n    Set e = o.ElementFromHandle(ByVal h)\n    Dim iCnd As IUIAutomationCondition\n    Set iCnd = o.CreatePropertyCondition(UIA_NamePropertyId, ""Save"")\n\n    Dim Button As IUIAutomationElement\n    Set Button = e.FindFirst(TreeScope_Subtree, iCnd)\n    Dim InvokePattern As IUIAutomationInvokePattern\n    Set InvokePattern = Button.GetCurrentPattern(UIA_InvokePatternId)\n    InvokePattern.Invoke\nEnd Sub   \n\n\nTry at your end.\n', '\n\'This is a working code for vba in excel 2007 to open a file\n\'But you need to add the ""UIAutomationCore.dll"" to be copied \n\'from ""C:\\Windows\\System32\\UIAutomationCore.dll"" into the \n\'path ""C:\\Users\\admin\\Documents""    \n\'The path where to copy may be different and you can find it when you check on \n\'the box for UIAutomationClient - the location is given under it.\n\'Tools-references\n\nOption Explicit\n  Dim ie As InternetExplorer\n  Dim h As LONG_PTR\n  Private Declare Function FindWindowEx Lib ""user32"" Alias ""FindWindowExA"" (ByVal hWnd1 As LONG_PTR, ByVal hWnd2 As LONG_PTR, ByVal lpsz1 As String, ByVal lpsz2 As String) As LONG_PTR\n\n\nSub click_open()\n  Dim o As IUIAutomation\n  Dim e As IUIAutomationElement\n  Dim sh\n  Dim eachIE\n\nDo\n\n    Set sh = New Shell32.Shell\n     For Each eachIE In sh.Windows\n         \' Check if this is the desired URL\n\n    \' Here you can use your condition except .html\n    \' If you want to use your URL , then put the URL below in the code for condition check.\n    \' This code will reassign your IE object with the same reference for navigation and your issue will resolve.\n         If InStr(1, eachIE.LocationURL, ""<enter your page url>"") Then\n         Set ie = eachIE\n         Exit Do\n         End If\n     Next eachIE\n Loop\n\nSet o = New CUIAutomation\nh = ie.Hwnd\nh = FindWindowEx(h, 0, ""Frame Notification Bar"", vbNullString)\nIf h = 0 Then Exit Sub\n\nSet e = o.ElementFromHandle(ByVal h)\nDim iCnd As IUIAutomationCondition\nSet iCnd = o.CreatePropertyCondition(UIA_NamePropertyId, ""Open"")\n\n\nDim Button As IUIAutomationElement\nSet Button = e.FindFirst(TreeScope_Subtree, iCnd)\nDim InvokePattern As IUIAutomationInvokePattern\nSet InvokePattern = Button.GetCurrentPattern(UIA_InvokePatternId)\nInvokePattern.Invoke\n\nEnd Sub\n\n', '\nI sent the shortcut keys to IE11.\nNote: the code will not run as you expect if IE is not the active window on your machine so it won\'t work while in debug mode. The shortcut keys and how to send them are below.\n\nShortcut key:Alt+S\nVBA: Application.SendKeys ""%{S}""\n\n']",https://stackoverflow.com/questions/26038165/automate-saveas-dialogue-for-ie9-vba,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NAnt or MSBuild, which one to choose and when?","
I am aware there are other NAnt and MSBuild related questions on Stack聽Overflow, but I could not find a direct comparison between the two and so here is the question.
When should one choose NAnt over MSBuild? Which one is better for what? Is NAnt more suitable for home/open source projects and MSBuild for work projects? What is the experience with any of the two?
",32k,"
            162
        ","['\nI\'ve done a similar investigation this week. Here\'s what I\'ve been able to determine:\nNAnt:\n\nCross-platform (supports Linux/Mono). It may be handy for installing a web site to multiple targets (that is,  Linux Apache and Windows IIS), for example.\n95% similar in syntax to Ant (easy for current Ant users or Java builders to pick up)\nIntegration with NUnit for running unit tests as part of the build, and with NDoc for producting documentation.\n\nMSBuild:\n\nBuilt-in to .NET.\nIntegrated with Visual Studio\nEasy to get started with MSBuild in Visual Studio - it\'s all behind the scenes. If you want to get deeper, you can hand edit the files.\n\nSubjective Differences: (YMMV)\n\nNAnt documentation is a little more straightforward. For example, the MSBuild Task Reference lists ""Csc Task - Describes the Csc task and its parameters. "" (thanks for the ""help""?), vs the NAnt Task Reference ""csc - Compiles C# programs."" UPDATE: I\'ve noticed the MSBuild documentation has been improved and is much better now (probably on par with NAnt).\nNot easy to figure out how to edit the build script source (*.*proj file) directly from within Visual Studio. With NAnt I just have Visual Studio treat the .build script as an XML file.\nApparently, in Visual Studio, Web Application Projects don\'t get a *.*proj file by default, so I had great difficulty figuring out how to even get MSBuild to run on mine to create a deployment script.\nNAnt is not built-in to Visual Studio and has to be added, either with an Add-In, or as an ""External Tool"". This is a bit of a pain to set up.\n(Edit:) One of my coworkers brought this up--if you want to set up a build machine using CruiseControl for continuous integration, CruiseControl integrates with NAnt nicely out of the box. UPDATE: CruiseControl also has an MSBuild task.\nPlease see comments below for full and up-to-date discussion of subjective differences.\n\n', '\nOne of the major draws of MSBuild for me (on Windows platforms) is that it comes as part of .NET itself. That means that any Windows machine that is up-to-date with Windows Update will have MSBuild available. Add to this the fact that C# compiler is also part of .NET itself and you have a platform that can build projects on clean machines. No need to install Visual Studio behemoth. NAnt, on the other hand, has to be explicitly installed  before a build can be triggered.\nJust for the record, I\'ve used NMake, Make, Ant, Rake, NAnt and MSBuild on non-trivial builds in the past (in that order). My favourite is MSBuild, hands down (and I do not favour it because ""that\'s what Visual Studio uses""). IMHO, it is a very under-appreciated build tool.\nI would compare NAnt vs. MSBuild to the difference between procedural and functional programming. NAnt is quite straightforward and you-get-what-you-see. MSBuild on the other hand requires a bit more thought. The learning curve is steeper. But once you ""get it"", you can do some amazing things with it.\nSo I would recommend looking at MSBuild if you also gravitate towards functional or logical style programming - if you are willing to invest a bit of time and effort before seeing tangible results (of course, I also strongly believe that the investment eventually pays off and you can do more powerful things more efficiently).\n', '\nPersonally, I use both - for the same project.\nMSBuild is great at building Visual Studio solutions and projects - that\'s what it\'s made for.\nNAnt is more easily hand-editable, in my opinion - particularly if you already know Ant. NAnt can call MSBuild very easily with NAntContrib. So, I hand-craft a NAnt script to do things like copying built files, cleaning up etc - and call MSBuild to do the actual ""turn my C# source code into assemblies"" part.\nIf you want an example of that, look at my Protocol Buffers build file. (I wouldn\'t claim it\'s a fabulous NAnt script, but it does the job.)\n', ""\nNAnt has more features out of the box, but MSBuild has a much better fundamental structure (item metadata rocks) which makes it much easier to build reusable MSBuild scripts.\nMSBuild takes a while to understand, but once you do it's very nice.\nLearning materials:\n\nInside the Microsoft Build Engine: Using MSBuild and Team Foundation Build\nby Sayed Ibrahim Hashimi (Jan, 2009)\nDeploying .NET Applications: Learning MSBuild and ClickOnce by Sayed\nby Y. Hashimi (Sep, 2008)\n\n"", ""\nKISS = Use MSBuild.\nWhy add something else into the mix when you have something that will do a reasonable job out of the box? When MSBuild arrived, NAnt died. And Mono will have an MSBuild implementation, (xbuild).\nDRY = Use MSBuild.\nAsk yourself what do you want out of a build system? I want a build system that is also used by my IDE rather than the maintaining two different configurations.\nPersonally, I'd love to hear some real arguments for NAnt, because I just can't think of any that really hold water.\n"", ""\nOne thing I noticed several posters mention was having to hand edit the .csproj (or .vbproj, etc.) files. \nMSBuild allows customization of these .*proj files through the use of .user files. If I have a project named MyCreativelyNamedProject.csproj and want to customize the MSBuild tasks inside of it, I can create a file named MyCreativelyNamedProject.csproj.user and use the CustomBeforeMicrosoftCommonTargets and CustomAfterMicrosoftCommonTargets to customize those files.\nAlso, both NAnt and MSBuild can be customized to the heart's content through custom MSBuild tasks and through NantContrib extensions.\nSo, using NAnt or MSBuild really comes down to familiarity:\n\nIf you are already familiar with Ant, use NAnt. The learning curve will be very easy.\nIf you are not familiar with either tool, MSBuild is integrated with Visual Studio already and requires no additional tools.\n\nIt is also worth adding that MSBuild is pretty much guaranteed to work with all new versions of .NET and Visual Studio as soon as they are released, whereas NAnt may have some lag.\n"", ""\nI use both in that my NAnt scripts call MSBuild. My main reason for staying with NAnt is isolation. Let me explain why I feel this is important:\n\nAdding dependencies to your project.\nThe NAnt build file is alien to Visual Studio (in my case I consider this a pro) so Visual Studio does not attempt to do anything with it. MSBuild tasks are embedded so part of the solution and can refer to other MSBuild tasks. I've received source code from someone else only to find out I could not build, because the MSBuild community tasks were not installed. What I find particularly frustrating is that Visual Studio just would not build and threw a bunch of errors that made me loose time debugging. This, despite the fact that the build being requested could have gone ahead (as a debug build for instance) without some of the extras of the MSBuild task. In short: I don't like adding dependencies to my project if I can avoid it.\nI don't trust Visual Studio as far as I could throw its development team. This stems back to the early days of Visual Studio when it would massacre my HTML. I still do not use the designer for instance (at a conference recently I found colleagues did the same). I have found that Visual Studio can screw up dependencies and version numbers in the DLL file (I cannot replicate this, but it did happen in a project consistently and caused a lot of grief and lost time). I have resorted to a build procedure that uses Visual Studio to build in debug mode only. For production, I use NAnt so that I control everything externally. Visual Studio just cannot interfere any longer if I build using NAnt.\n\nPS: I'm a web developer and do not do Windows Forms development.\n"", ""\nWhile I'm not very familiar with MsBuild, I'm under the impression that some of key differences on both sides can be supplemented by additions:\n\nMsBuildTasks\nNantContrib\n\nI recently had to build a Silverlight project in Nant. I discovered that life would be easier if I just did this with MsBuild - I ended up calling a MsBuild task from within a Nant script so I suppose it's not too out of the ordinary to mix and match the two.\nBeyond that, I suppose it's going to be a question of personal preference - obviously you can manage some/most of MsBuild's functionality from within Visual Studio, if that's your thing. Nant seems more flexible and better suited if you prefer to write scripts by hand, and if you come from the Java world you'll likely be right at home with it.\n"", ""\nI ended up using both. When redesigning our build system, I was facing a tricky problem. Namely, I couldn't get rid of .vcproj (and family) because we everybody was using VS to update the project files, settings, and configurations. So without a huge duplication and error prone process, we couldn't base our build system on a new set of files.\nFor this reason, I decided to keep the 'proj' files of VS and use MSBuild (they are MSBuild files, at least VS2005 and VS2008 use MSBuild project files). For everything else (custom configuration, unit testing, packaging, preparing documentation...) I used NAnt.\nFor continuous integration, I used CruiseControl. So we had CC scripts that triggered NAnt jobs, which for building used MSBuild.\nOne final note: MSBuild does NOT support Setup projects! So you're stuck with calling DevEnv.com or using Visual Studio directly. That's what I ended up doing, but I disabled the setup project by default from all solution configurations, since developers wouldn't normally need to build them, and if they do, they can manually select to build them.\n"", '\nI have switched from NAnt to MSBuild recently because of its ability to build VS solutions. I still use NAnt occasionally, though.\nYou may also want to check out MSBuild Community Tasks which is like NAntContrib.\n', ""\nThe documentation and tutorials available for NAnt make it easier to begin learning build scripts with NAnt. Once I got the hang of NAnt and creating build scripts I started translating that knowledge to MSBuild (I did X in NAnt, how do I do X in MSBuild?). Microsoft's documentation usually assumes a pretty high level of knowledge before it is useful.\nThe reason for switching from NAnt to MSBuild is because MSBuild is more current. Unfortunately the last release of NAnt was in December 8 2007, while MSBuild 4.0 (.NET 4.0) isn't far off. It looks like the NAnt project has died.\nIf you find good documentation for someone just beginning to learn creating build scripts using MSBuild, then skip NAnt and go straight for MSBuild. If NAnt ever comes out with a new release then I'd consider sticking with NAnt, but they're lagging behind now.\n"", '\nWe use both. NAnt is responsible for all ""scripting"" stuff, like copying, deploying on IIS, creating packages and MSBuild is responsible for building the solution. Then we can avoid problems with non-supported .NET 4.0 by a new version of NAnt.\nNAnt is also more scalable. If we want to migrate the deployment scripts to production servers, we only copy the build file and install a proper version of .NET - no Visual Studio problems with csproj files:)\n', '\nYDeliver by Manoj is a build framework built on top of PSake. It has a rich set of library functions, ability to define workflows, and we have used it to deliver over six enterprise projects to production.\nUse it in conjunction with TeamCity, CruiseControl, or anything that can run PowerShell.\n', '\nWe use FlubuCore. It\'s an open source C# library for building projects and executing deployment scripts using C# code.\nSimple example of how flubu is used:\nprotected override void ConfigureTargets(ITaskContext session)\n{           \n\n    var compile = session.CreateTarget(""compile"")\n        .SetDescription(""Compiles the solution."")\n        .AddTask(x => x.CompileSolutionTask())\n        .DependsOn(""generate.commonassinfo"");\n}\n\nYou can find more information about flubu and how to get started here:\nchoice-for-build-tool-msbuild-nant-or-something-else\n']",https://stackoverflow.com/questions/476163/nant-or-msbuild-which-one-to-choose-and-when,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dead code detection in legacy C/C++ project [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



How would you go about dead code detection in C/C++ code? I have a pretty large code base to work with and at least 10-15% is dead code. Is there any Unix based tool to identify this areas? Some pieces of code still use a lot of preprocessor, can automated process handle that?
",38k,"
            68
        ","[""\nYou could use a code coverage analysis tool for this and look for unused spots in your code.\nA popular tool for the gcc toolchain is gcov, together with the graphical frontend lcov (http://ltp.sourceforge.net/coverage/lcov.php).\nIf you use gcc, you can compile with gcov support, which is enabled by the '--coverage' flag. Next, run your application or run your test suite with this gcov enabled build.\nBasically gcc will emit some extra files during compilation and the application will also emit some coverage data while running. You have to collect all of these (.gcdo and .gcda files). I'm not going in full detail here, but you probably need to set two environment variables to collect the coverage data in a sane way: GCOV_PREFIX and GCOV_PREFIX_STRIP... \nAfter the run, you can put all the coverage data together and run it through the lcov toolsuite. Merging of all the coverage files from different test runs is also possible, albeit a bit involved.\nAnyhow, you end up with a nice set of webpages showing some coverage information, pointing out the pieces of code that have no coverage and hence, were not used.\nOff course, you need to double check if the portions of code are not used in any situation and a lot depends on how good your tests exercise the codebase.  But at least, this will give an idea about possible dead-code candidates...\n"", '\nCompile it under gcc with -Wunreachable-code. \nI think that the more recent the version, the better results you\'ll get, but I may be wrong in my impression that it\'s something they\'ve been actively working on. Note that this does flow analysis, but I don\'t believe it tells you about ""code"" which is already dead by the time it leaves the preprocessor, because that\'s never parsed by the compiler. It also won\'t detect e.g. exported functions which are never called, or special case handling code which just so happen to be impossible because nothing ever calls the function with that parameter - you need code coverage for that (and run the functional tests, not the unit tests. Unit tests are supposed to have 100% code coverage, and hence execute code paths which are \'dead\' as far as the application is concerned). Still, with these limitations in mind it\'s an easy way to get started finding the most completely bollixed routines in the code base.\nThis CERT advisory lists some other tools for static dead code detection\n', '\nFor C code only and assuming that the source code of the whole project\nis available, launch an analysis with the Open Source tool Frama-C.\nAny statement of the program that displays red in the GUI is\ndead code.\nIf you have ""dead code"" problems, you may also be interested in\nremoving ""spare code"", code that is executed but does not\ncontribute to the end result. This requires you to provide\nan accurate modelization of I/O functions (you wouldn\'t want\nto remove a computation that appears to be ""spare"" but\nthat is used as an argument to printf). Frama-C has an option for pointing out spare code.\n', ""\nYour approach depends on the availability (automated) tests. If you have a test suite that you trust to cover a sufficient amount of functionality, you can use a coverage analysis, as previous answers already suggested. \nIf you are not so fortunate, you might want to look into source code analysis tools like SciTools' Understand that can help you analyse your code using a lot of built in analysis reports. My experience with that tool dates from 2 years ago, so I can't give you much detail, but what I do remember is that they had an impressive support with very fast turnaround times of bug fixes and answers to questions.\nI found a page on static source code analysis that lists many other tools as well.\nIf that doesn't help you sufficiently either, and you're specifically interested in finding out the preprocessor-related dead code, I would recommend you post some more details about the code. For example, if it is mostly related to various combinations of #ifdef settings you could write scripts to determine the (combinations of) settings and find out which combinations are never actually built, etc.\n"", '\nBoth Mozilla and Open Office have home-grown solutions.\n', '\ng++ 4.01 -Wunreachable-code warns about code that is unreachable within a function, but does not warn about unused functions.\nint foo() { \n    return 21; // point a\n}\n\nint bar() {\n  int a = 7;\n  return a;\n  a += 9;  // point b\n  return a;\n}\n\nint main(int, char **) {\n    return bar();\n}\n\ng++ 4.01 will issue a warning about point b, but say nothing about foo() (point a) even though it is unreachable in this file.  This behavior is correct although disappointing, because a compiler cannot know that function foo() is not declared extern in some other compilation unit and invoked from there; only a linker can be sure.\n', ""\nDead code analysis like this requires a global analysis of your entire project. You can't get this information by analyzing translation units individually (well, you can detect dead entities if they are entirely within a single translation unit, but I don't think that's what you are really looking for).\nWe've used our DMS Software Reengineering Toolkit to implement exactly this for Java code, by parsing all the compilation-units involved at once, building symbol tables for everything and chasing down all the references. A top level definition with no references and no claim of being an external API item is dead. This tool also automatically strips out the dead code, and at the end you can choose what you want: the report of dead entities, or the code stripped of those entities.\nDMS also parses C++ in a variety of dialects (EDIT Feb 2014: including MS and GCC versions of C++14 [EDIT Nov 2017: now C++17]) and builds all the necessary symbol tables. Tracking down the dead references would be straightforward from that point. DMS could also be used to strip them out. See http://www.semanticdesigns.com/Products/DMS/DMSToolkit.html\n"", '\nBullseye coverage tool would help. It is not free though.\n']",https://stackoverflow.com/questions/229069/dead-code-detection-in-legacy-c-c-project,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check if element is clickable in Selenium Java,"
I'm new to Selenium and need to check if element is clickable in Selenium Java, since element.click() passes both on link and label.
I tried using the following code, but it is not working:
WebDriverWait wait = new WebDriverWait(Scenario1Test.driver, 10);

if(wait.until(ExpectedConditions.elementToBeClickable(By.xpath(""(//div[@id='brandSlider']/div[1]/div/div/div/img)[50]"")))==null)

",216k,"
            29
        ","['\nelementToBeClickable is used for checking an element is visible and enabled such that you can click it. \nExpectedConditions.elementToBeClickable returns WebElement if expected condition is true otherwise it will throw TimeoutException, It never returns null.\nSo if your using ExpectedConditions.elementToBeClickable to find an element which will always gives you the clickable element, so no need to check for null condition, you should try as below :-\nWebDriverWait wait = new WebDriverWait(Scenario1Test.driver, 10); \nWebElement element = wait.until(ExpectedConditions.elementToBeClickable(By.xpath(""(//div[@id=\'brandSlider\']/div[1]/div/div/div/img)[50]"")));\nelement.click();\n\nAs you are saying element.click() passes both on link and label that\'s doesn\'t mean element is not clickable, it means returned element clicked but may be there is no event performs on element by click action.\nNote:- I\'m suggesting you always try first to find elements by id, name, className and other locator. if you faced some difficulty to find then use cssSelector and always give last priority to xpath locator because it is slower than other locator to locate an element.\nHope it helps you..:)\n', '\nThere are instances when element.isDisplayed() && element.isEnabled() will return true but still element will not be clickable, because it is hidden/overlapped by some other element. \nIn such case, Exception caught is: \n\norg.openqa.selenium.WebDriverException: unknown error: Element is not\n  clickable at point (781, 704). Other element would receive the click:\n  <div class=""footer"">...</div> \n\nUse this code instead:\nWebElement  element=driver.findElement(By.xpath"""");  \nJavascriptExecutor ex=(JavascriptExecutor)driver;\nex.executeScript(""arguments[0].click()"", element);\n\nIt will work.\n', ""\nwait.until(ExpectedConditions) won't return null, it will either meet the condition or throw TimeoutException.\nYou can check if the element is displayed and enabled\nWebElement element = driver.findElement(By.xpath);\nif (element.isDisplayed() && element.isEnabled()) {\n    element.click();\n}\n\n"", '\nThere are certain things you have to take care:\n\nWebDriverWait inconjunction with ExpectedConditions as elementToBeClickable() returns the WebElement once it is located and clickable i.e. visible and enabled.\nIn this process, WebDriverWait will ignore instances of NotFoundException that are encountered by default in the until condition.\nOnce the duration of the wait expires on the desired element not being located and clickable, will throw a timeout exception.\nThe different approach to address this issue are:\n\nTo invoke click() as soon as the element is returned, you can use:\nnew WebDriverWait(driver, 10).until(ExpectedConditions.elementToBeClickable(By.xpath(""(//div[@id=\'brandSlider\']/div[1]/div/div/div/img)[50]""))).click();\n\n\nTo simply validate if the element is located and clickable, wrap up the WebDriverWait in a try-catch{} block as follows:\ntry {\n       new WebDriverWait(driver, 10).until(ExpectedConditions.elementToBeClickable(By.xpath(""(//div[@id=\'brandSlider\']/div[1]/div/div/div/img)[50]"")));\n       System.out.println(""Element is clickable"");\n     }\ncatch(TimeoutException e) {\n       System.out.println(""Element isn\'t clickable"");\n    }\n\n\nIf WebDriverWait returns the located and clickable element but the element is still not clickable, you need to invoke executeScript() method as follows:\nWebElement element = new WebDriverWait(driver, 10).until(ExpectedConditions.elementToBeClickable(By.xpath(""(//div[@id=\'brandSlider\']/div[1]/div/div/div/img)[50]""))); \n((JavascriptExecutor)driver).executeScript(""arguments[0].click();"", element);\n\n\n\n\n\n', '\nFrom the source code you will be able to view that, ExpectedConditions.elementToBeClickable(), it will judge the element visible and enabled, so you can use isEnabled() together with isDisplayed(). Following is the source code.\n\n\npublic static ExpectedCondition<WebElement> elementToBeClickable(final WebElement element) {\r\n\t\treturn new ExpectedCondition() {\r\n\t\t\tpublic WebElement apply(WebDriver driver) {\r\n\t\t\t\tWebElement visibleElement = (WebElement) ExpectedConditions.visibilityOf(element).apply(driver);\r\n\r\n\t\t\t\ttry {\r\n\t\t\t\t\treturn visibleElement != null && visibleElement.isEnabled() ? visibleElement : null;\r\n\t\t\t\t} catch (StaleElementReferenceException arg3) {\r\n\t\t\t\t\treturn null;\r\n\t\t\t\t}\r\n\t\t\t}\r\n\r\n\t\t\tpublic String toString() {\r\n\t\t\t\treturn ""element to be clickable: "" + element;\r\n\t\t\t}\r\n\t\t};\r\n\t}\n\n\n', '\nthe class attribute contains disabled when the element is not clickable.\nWebElement webElement = driver.findElement(By.id(""elementId""));\nif(!webElement.getAttribute(""class"").contains(""disabled"")){\n    webElement.click();\n}\n\n', '\nList<WebElement> wb=driver.findElements(By.xpath(newXpath));\n        for(WebElement we: wb){\n            if(we.isDisplayed() && we.isEnabled())\n            {\n                we.click();\n                break;\n            }\n        }\n    }\n\n']",https://stackoverflow.com/questions/38327049/check-if-element-is-clickable-in-selenium-java,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interact with other programs using Python,"
I'm having the idea of writing a program using Python which shall find a lyric of a song whose name I provided. I think the whole process should boil down to couple of things below. These are what I want the program to do when I run it:

prompt me to enter a name of a song
copy that name
open a web browser (google chrome for example)
paste that name in the address bar and find information about the song
open a page that contains the lyrics
copy that lyrics
run a text editor (like Microsoft Word for instance)
paste the lyrics
save the new text file with the name of the song

I am not asking for code, of course. I just want to know the concepts or ideas about how to use python to interact with other programs
To be more specific, I think I want to know, fox example, just how we point out where is the address bar in Google Chrome and tell python to paste the name there. Or how we tell python how to copy the lyrics as well as paste it into the Microsof Word's sheet then save it.
I've been reading (I'm still reading) several books on Python: Byte of python, Learn python the hard way, Python for dummies, Beginning Game Development with Python and Pygame. However, I found out that it seems like I only (or almost only) learn to creat programs that work on itself (I can't tell my program to do things I want with other programs that are already installed on my computer)
I know that my question somehow sounds rather silly, but I really want to know how it works, the way we tell Python to regconize that this part of the Google chrome browser is the address bar and that it should paste the name of the song in it. The whole idea of making python interact with another program is really really vague to me and I just 
extremely want to grasp that.
Thank you everyone, whoever spend their time reading my so-long question.
ttriet204
",97k,"
            28
        ","['\nIf what you\'re really looking into is a good excuse to teach yourself how to interact with other apps, this may not be the best one. Web browsers are messy, the timing is going to be unpredictable, etc. So, you\'ve taken on a very hard task鈥攁nd one that would be very easy if you did it the usual way (talk to the server directly, create the text file directly, etc., all without touching any other programs).\nBut if you do want to interact with other apps, there are a variety of different approaches, and which is appropriate depends on the kinds of apps you need to deal with.\n\nSome apps are designed to be automated from the outside. On Windows, this nearly always means they a COM interface, usually with an IDispatch interface, for which you can use pywin32\'s COM wrappers; on Mac, it means an AppleEvent interface, for which you use ScriptingBridge or appscript; on other platforms there is no universal standard. IE (but probably not Chrome) and Word both have such interfaces.\nSome apps have a non-GUI interface鈥攚hether that\'s a command line you can drive with popen, or a DLL/SO/DYLIB you can load up through ctypes. Or, ideally, someone else has already written Python bindings for you.\nSome apps have nothing but the GUI, and there\'s no way around doing GUI automation. You can do this at a low level, by crafting WM_ messages to send via pywin32 on Windows, using the accessibility APIs on Mac, etc., or at a somewhat higher level with libraries like pywinauto, or possibly at the very high level of selenium or similar tools built to automate specific apps.\n\nSo, you could do this with anything from selenium for Chrome and COM automation for Word, to crafting all the WM_ messages yourself. If this is meant to be a learning exercise, the question is which of those things you want to learn today.\n\nLet\'s start with COM automation. Using pywin32, you directly access the application\'s own scripting interfaces, without having to take control of the GUI from the user, figure out how to navigate menus and dialog boxes, etc. This is the modern version of writing ""Word macros""鈥攖he macros can be external scripts instead of inside Word, and they don\'t have to be written in VB, but they look pretty similar. The last part of your script would look something like this:\nword = win32com.client.dispatch(\'Word.Application\')\nword.Visible = True\ndoc = word.Documents.Add()\ndoc.Selection.TypeText(my_string)\ndoc.SaveAs(r\'C:\\TestFiles\\TestDoc.doc\')\n\nIf you look at Microsoft Word Scripts, you can see a bunch of examples. However, you may notice they\'re written in VBScript. And if you look around for tutorials, they\'re all written for VBScript (or older VB). And the documentation for most apps is written for VBScript (or VB, .NET, or even low-level COM). And all of the tutorials I know of for using COM automation from Python, like Quick Start to Client Side COM and Python, are written for people who already know about COM automation, and just want to know how to do it from Python. The fact that Microsoft keeps changing the name of everything makes it even harder to search for鈥攈ow would you guess that googling for OLE automation, ActiveX scripting, Windows Scripting House, etc. would have anything to do with learning about COM automation? So, I\'m not sure what to recommend for getting started. I can promise that it\'s all as simple as it looks from that example above, once you do learn all the nonsense, but I don\'t know how to get past that initial hurdle. \nAnyway, not every application is automatable. And sometimes, even if it is, describing the GUI actions (what a user would click on the screen) is simpler than thinking in terms of the app\'s object model. ""Select the third paragraph"" is hard to describe in GUI terms, but ""select the whole document"" is easy鈥攋ust hit control-A, or go to the Edit menu and Select All. GUI automation is much harder than COM automation, because you either have to send the app the same messages that Windows itself sends to represent your user actions (e.g., see ""Menu Notifications"") or, worse, craft mouse messages like ""go (32, 4) pixels from the top-left corner, click, mouse down 16 pixels, click again"" to say ""open the File menu, then click New"".\nFortunately, there are tools like pywinauto that wrap up both kinds of GUI automation stuff up to make it a lot simpler. And there are tools like swapy that can help you figure out what commands you want to send. If you\'re not wedded to Python, there are also tools like AutoIt and Actions that are even easier than using swapy and pywinauto, at least when you\'re getting started. Going this way, the last part of your script might look like:\nword.Activate()\nword.MenuSelect(\'File->New\')\nword.KeyStrokes(my_string)\nword.MenuSelect(\'File->Save As\')\nword.Dialogs[-1].FindTextField(\'Filename\').Select()\nword.KeyStrokes(r\'C:\\TestFiles\\TestDoc.doc\')\nword.Dialogs[-1].FindButton(\'OK\').Click()\n\nFinally, even with all of these tools, web browsers are very hard to automate, because each web page has its own menus, buttons, etc. that aren\'t Windows controls, but HTML. Unless you want to go all the way down to the level of ""move the mouse 12 pixels"", it\'s very hard to deal with these. That\'s where selenium comes in鈥攊t scripts web GUIs the same way that pywinauto scripts Windows GUIs.\n', '\nThe following script uses Automa to do exactly what you want (tested on Word 2010):\ndef find_lyrics():\n    print \'Please minimize all other open windows, then enter the song:\'\n    song = raw_input()\n    start(""Google Chrome"")\n    # Disable Google\'s autocompletion and set the language to English:\n    google_address = \'google.com/webhp?complete=0&hl=en\'\n    write(google_address, into=""Address"")\n    press(ENTER)\n    write(song + \' lyrics filetype:txt\')\n    click(""I\'m Feeling Lucky"")\n    press(CTRL + \'a\', CTRL + \'c\')\n    press(ALT + F4)\n    start(""Microsoft Word"")\n    press(CTRL + \'v\')\n    press(CTRL + \'s\')\n    click(""Desktop"")\n    write(song + \' lyrics\', into=""File name"")\n    click(""Save"")\n    press(ALT + F4)\n    print(""\\nThe lyrics have been saved in file \'%s lyrics\' ""\n          ""on your desktop."" % song)\n\nTo try it out for yourself, download Automa.zip from its Download page and unzip into, say, c:\\Program Files. You\'ll get a folder called Automa 1.1.2. Run Automa.exe in that folder. Copy the code above and paste it into Automa by right-clicking into the console window. Press Enter twice to get rid of the last ... in the window and arrive back at the prompt >>>. Close all other open windows and type\n>>> find_lyrics()\n\nThis performs the required steps. \nAutoma is a Python library: To use it as such, you have to add the line\nfrom automa.api import *\n\nto the top of your scripts and the file library.zip from Automa\'s installation directory to your environment variable PYTHONPATH. \nIf you have any other questions, just let me know :-)\n', '\nHere\'s an implementation in Python of @Matteo Italia\'s comment:\n\nYou are approaching the problem from a ""user perspective"" when you\n  should approach it from a ""programmer perspective""; you don\'t need to\n  open a browser, copy the text, open Word or whatever, you need to\n  perform the appropriate HTTP requests, parse the relevant HTML,\n  extract the text and write it to a file from inside your Python\n  script. All the tools to do this are available in Python (in\n  particular you\'ll need urllib2 and BeautifulSoup).\n\n#!/usr/bin/env python\nimport codecs\nimport json\nimport sys\nimport urllib\nimport urllib2\n\nimport bs4  # pip install beautifulsoup4\n\ndef extract_lyrics(page):\n    """"""Extract lyrics text from given lyrics.wikia.com html page.""""""\n    soup = bs4.BeautifulSoup(page)\n    result = []\n    for tag in soup.find(\'div\', \'lyricbox\'):\n        if isinstance(tag, bs4.NavigableString):\n            if not isinstance(tag, bs4.element.Comment):\n                result.append(tag)\n        elif tag.name == \'br\':\n            result.append(\'\\n\')\n    return """".join(result)\n\n# get artist, song to search\nartist = raw_input(""Enter artist:"")\nsong = raw_input(""Enter song:"")\n\n# make request\nquery = urllib.urlencode(dict(artist=artist, song=song, fmt=""realjson""))\nresponse = urllib2.urlopen(""http://lyrics.wikia.com/api.php?"" + query)\ndata = json.load(response)\n\nif data[\'lyrics\'] != \'Not found\':\n    # print short lyrics\n    print(data[\'lyrics\'])\n    # get full lyrics\n    lyrics = extract_lyrics(urllib2.urlopen(data[\'url\']))\n    # save to file\n    filename = ""[%s] [%s] lyrics.txt"" % (data[\'artist\'], data[\'song\'])\n    with codecs.open(filename, \'w\', encoding=\'utf-8\') as output_file:\n        output_file.write(lyrics)\n    print(""written \'%s\'"" % filename)\nelse:\n    sys.exit(\'not found\')\n\nExample\n$ printf ""Queen\\nWe are the Champions"" | python get-lyrics.py \n\nOutput\n\nI\'ve paid my dues\nTime after time\nI\'ve done my sentence\nBut committed no crime\n\nAnd bad mistakes\nI\'ve made a few\nI\'ve had my share of sand kicked [...]\nwritten \'[Queen] [We are the Champions] lyrics.txt\'\n\n', ""\nIf you really want to open a browser, etc, look at selenium. But that's overkill for your purposes. Selenium is used to simulate button clicks, etc for testing the appearance of websites on various browsers, etc. Mechanize is less of an overkill for this\nWhat you really want to do is understand how a browser (or any other program) works under the hood i.e. when you click on the mouse or type on the keyboard or hit Save, what does the program do behind the scenes? It is this behind-the-scenes work that you want your python code to do.\nSo, use urllib, urllib2 or requests (or heck, even scrapy) to request a web page (learn how to put together the url to a google search or the php GET request of a lyrics website). Google also has a search API that you can take advantage of, to perform a google search.\nOnce you have your results from your page request, parse it with xml, beautifulsoup, lxlml, etc and find the section of the request result that has the information you're after.\nNow that you have your lyrics, the simplest thing to do is open a text file and dump the lyrics in there and write to disk. But if you really want to do it with MS Word, then open a doc file in notepad or notepad++ and look at its structure. Now, use python to build a document with similar structure, wherein the content will be the downloaded lyrics.\nIf this method fails, you could look into pywinauto or such to automate the pasting of text into an MS Word doc and clicking on Save\nCitation: Matteo Italia, g.d.d.c from the comments on the OP\n"", '\nYou should look into a package called selenium for interacting with web browsers\n']",https://stackoverflow.com/questions/14288177/interact-with-other-programs-using-python,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMeter : How to record HTTPS traffic?,"
I'm using Apache JMeter 2.3, which now supports ""attempt HTTPS spoofing"" under the Proxy Server element. 
I've tried this on several different servers, and have had no success. 
Has anyone been able to successfully record from an HTTPS source with this setting?
Or barring successfully recording, can anyone share a work-around? When available, I simply have HTTPS turned off at the server level, but this is not always feasible. Thoughts?
",34k,"
            13
        ","['\n\nStarting from JMeter 3.0 default port for the HTTP(S) Test Script Recorder is 8888\nThe easiest way to configure recording is using JMeter Templates feature. From JMeter\'s main menu select:\nFile -> Templates -> Recording -> Create\n\n\n\n\nDon\'t forget to start the recorder :\n\nIn JMeter < 4.0, Expand ""Workbench"", if >= 4.0, ignore this step\nSelect ""HTTP(S) Test Script Recorder""\nClick ""Start"" button\n\n\n\n\nYou will see a message regarding Root CA Certificate. Click OK:\n\n\n\nit is OK, it informs you JMeter has created a Root Certificate Authority that you need to import in your browser to be able to record correctly HTTPS traffic.\n\n\nTo Import this Root CA certificate in Firefox (it is located in jmeter/bin folder) for example:\n\n\n\n\n\n\n\n\nConfigure browser to use JMeter as proxy:\n\n\nIt is now Ok.\n\nYou can navigate to your application, samplers will be created under ""Recording Controller"" which is under ""Thread Group"" element\n\n', '\nWhile the JMeter proxy already has the ability to record HTTPS requests, a Chrome Extension that creates JMeter script came out recently:\nhttps://chrome.google.com/webstore/detail/blazemeter-the-load-testi/mbopgmdnpcbohhpnfglgohlbhfongabi?hl=en\nIt uses a BlazeMeter as the middleman (a commercial JMeter in the cloud service) but you can use their free service forever and still use the plugin to record a JMX script and download it locally to your own machine even if you never use any of the paid plans.\n', ""\nWhat I do is:\n\nGo to my website using my web server's IP-address (i.e. http://2.2.2.2/login.html)\nStart the recorder and run through my test case\nStop recording\nReplace all values of the IP address with the domain name (i.e. replace 2.2.2.2 with yoursite.com) from the HTTP Request Samplers\nSet the protocol to https in the HTTP Request Samplers\n\nIf you have more than a few pages, it's easiest to create an HTTP Request Defaults item, and set your domain name and protocol there.\nFYI, I'm using the latest stable build as of 2010-05-24: Jmeter 2.3.4 r785646.\n"", '\nThe newest version of Jmeter (2.4) now supports HTTPS recording.  Rejoice!\nMore details:\nhttp://wiki.apache.org/jmeter/JMeterFAQ#Can_JMeter_record_HTTPS_requests_using_the_recording_proxy.3F\n', '\nIs there any other way to record HTTPS than Bad boy and Https spoofing?\nYes--use a nightly build of JMeter, e.g. version r922204.\n', '\nHttps recording is successfully working in new version of Jmeter 2.9 as of today. I had to import proxy certificate and play around with Firefox to get this working.\nRefer this link for more information\nHttps recording using Jmeter\n', '\nYes, I have used it with ""attempt HTTPS spoofing"" on. Things are simple enough:\n\nTurn HTTPS Spoofing on (of course).\nMake sure that the browser sends Http request to Jmeter, so that Jmeter can record it and then send the encrypted request back to the server. So, the URL in the browser should start with http:// (and not with https://). The details could be found in my blog. \n\nPlease let me know if it works for you.\n', '\nI am using Webscarab to record https and ajax conversations.\nIt workd fine.  I extended the Webscarab with export function for Jmeter.\nBugzilla 48898.\n']",https://stackoverflow.com/questions/299529/jmeter-how-to-record-https-traffic,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python - Control window with pywinauto while the window is minimized or hidden,"
What I'm trying to do:
I'm trying to create a script in python with pywinauto to automatically install notepad++ in the background (hidden or minimized), notepad++ is just an example since I will edit it to work with other software.
Problem:
The problem is that I want to do it while the installer is hidden or minimized, but if I move my mouse the script will stop working.
Question:
How can I execute this script and make it work, while the notepad++ installer is hidden or minimized.
This is my code so far:
import sys, os, pywinauto

pwa_app = pywinauto.application.Application()

app = pywinauto.Application().Start(r'npp.6.8.3.Installer.exe')

Wizard = app['Installer Language']

Wizard.NextButton.Click()

Wizard = app['Notepad++ v6.8.3 Setup']

Wizard.Wait('visible')

Wizard['Welcome to the Notepad++ v6.8.3 Setup'].Wait('ready')
Wizard.NextButton.Click()

Wizard['License Agreement'].Wait('ready')
Wizard['I &Agree'].Click()

Wizard['Choose Install Location'].Wait('ready')
Wizard.Button2.Click()

Wizard['Choose Components'].Wait('ready')
Wizard.Button2.Click()

Wizard['Create Shortcut on Desktop'].Wait('enabled').CheckByClick()
Wizard.Install.Click()

Wizard['Completing the Notepad++ v6.8.3 Setup'].Wait('ready', timeout=30)
Wizard['CheckBox'].Wait('enabled').Click()
Wizard.Finish.Click()
Wizard.WaitNot('visible')

",22k,"
            9
        ","['\nThe problem is here:\nWizard[\'Create Shortcut on Desktop\'].wait(\'enabled\').check_by_click()\n\ncheck_by_click() uses click_input() method that moves real mouse cursor and performs a realistic click.\nUse check() method instead.\n[EDIT] If the installer doesn\'t handle BM_SETCHECK properly the workaround may look so:\ncheckbox = Wizard[\'Create Shortcut on Desktop\'].wait(\'enabled\')\nif checkbox.get_check_state() != pywinauto.win32defines.BST_CHECKED:\n    checkbox.click()\n\nI will fix it in the next pywinauto release by creating methods check_by_click and check_by_click_input respectively.\n\n[EDIT 2]\nI tried your script with my fix and it works perfectly (and very fast) with and without mouse moves. Win7 x64, 32-bit Python 2.7, pywinauto 0.6.x, run as administrator.\nimport sys\nimport os\nfrom pywinauto import Application\n\napp = Application(backend=""win32"").start(r\'npp.6.8.3.Installer.exe\')\n\nWizard = app[\'Installer Language\']\n\nWizard.minimize()\nWizard.NextButton.click()\n\nWizard = app[\'Notepad++ v6.8.3 Setup\']\n\nWizard.wait(\'visible\')\nWizard.minimize()\n\nWizard[\'Welcome to the Notepad++ v6.8.3 Setup\'].wait(\'ready\')\nWizard.NextButton.click()\n\nWizard.minimize()\nWizard[\'License Agreement\'].wait(\'ready\')\nWizard[\'I &Agree\'].click()\n\nWizard.minimize()\nWizard[\'Choose Install Location\'].wait(\'ready\')\nWizard.Button2.click()\n\nWizard.minimize()\nWizard[\'Choose Components\'].wait(\'ready\')\nWizard.Button2.click()\n\nWizard.minimize()\ncheckbox = Wizard[\'Create Shortcut on Desktop\'].wait(\'enabled\')\nif checkbox.get_check_state() != pywinauto.win32defines.BST_CHECKED:\n    checkbox.click()\nWizard.Install.click()\n\nWizard[\'Completing the Notepad++ v6.8.3 Setup\'].wait(\'ready\', timeout=30)\nWizard.minimize()\nWizard[\'CheckBox\'].wait(\'enabled\').click()\nWizard.Finish.click()\nWizard.wait_not(\'visible\')\n\n']",https://stackoverflow.com/questions/32846550/python-control-window-with-pywinauto-while-the-window-is-minimized-or-hidden,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schedule automatic daily upload with FileZilla [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.


Closed 7 years ago.


The community reviewed whether to reopen this question 1 year ago and left it closed:

Original close reason(s) were not resolved






                        Improve this question
                    



I would like to use FileZilla to automatically upload PDFs to my GoDaddy hosted site daily, replacing the previous day's sheets. Is there any way to do this? I read online that batch files might work, could someone post a sample version of a batch file that would do the trick?
",137k,"
            19
        ","['\nFileZilla does not have any command line arguments (nor any other way) that allow an automatic transfer.\nSome references:\n\nFileZilla Client command-line arguments\nhttps://trac.filezilla-project.org/ticket/2317\nHow do I send a file with FileZilla from the command line?\n\n\nThough you can use any other client that allows automation.\nYou have not specified, what protocol you are using. FTP or SFTP? You will definitely be able to use WinSCP, as it supports all protocols that the free version of FileZilla does (and more).\nCombine WinSCP scripting capabilities with Windows Scheduler:\n\nAutomate file transfers to FTP server or SFTP server;\nSchedule file transfers to FTP/SFTP server\n\nA typical WinSCP script for upload (with SFTP) looks like:\nopen sftp://user:password@example.com/ -hostkey=""ssh-rsa 2048 xxxxxxxxxxx...=""\nput c:\\mypdfs\\*.pdf /home/user/\nclose\n\nWith FTP, just replace the sftp:// with the ftp:// and remove the -hostkey=""..."" switch.\n\nSimilarly for download: How to schedule an automatic FTP download on Windows?\n\nWinSCP can even generate a script from an imported FileZilla session.\nFor details, see the guide to FileZilla automation.\n(I\'m the author of WinSCP)\n\nAnother option, if you are using SFTP, is the psftp.exe client from PuTTY suite.\n']",https://stackoverflow.com/questions/24945709/schedule-automatic-daily-upload-with-filezilla,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executing a PHP script with a CRON Job [closed],"









                        It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and  cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened,  visit the help center.
                        
                    


Closed 9 years ago.



I would like to run a PHP script every day at midnight. After research on how to do this, it appears that the best way to achieve this is to use a CRON job.
If my php script was located at http://example.com/scripts/scriptExample.php, can somebody be able to show the most simple example of what this CRON command would look like?
I have looked through numerous posts but I cannot find a simple enough example for me to learn and build upon.
",94k,"
            24
        ","['\nCrontab needs the full path on your server.\n0 0 * * * php /var/www/vhosts/domain.com/httpdocs/scripts/example.php\n\nThis will execute every day at midnight.\n', '\nSo something like this:\n00 * * * * /usr/local/bin/php /home/john/myscript.php\n\nThe 00 * * * * means hourly \n/usr/local/bin/php - where php main engine is in\n/home/john/myscript.php - the script to run (physical path)\nYou can use also @hourly special key:\n@hourly /usr/local/bin/php /home/john/myscript.php\n\n', '\nIf You have a sudo access to your linux server :-\nThen do the following\nsudo crontab -e\n\nThis will open the cron tab for you on your server.\nNext thing is you have to do a cron entry for the file which you want to execute\n00 00 * * * /usr/local/bin/php ""path of the php file which you want to execute""\n\n00 00 * * * this will run your cron at midnight daily, means at 0hrs and 0mins\n', '\nAre you using a company to host your website? \nAs you should have a icon  in your c panel called cron jobs from there you can tell it what script to execute and when.\n']",https://stackoverflow.com/questions/16144350/executing-a-php-script-with-a-cron-job,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Creating, opening and printing a word file from C++","
I have three related questions. 
I want to create a word file with a name from C++. I want to be able to sent the printing command to this file, so that the file is being printed without the user having to open the document and do it manually and I want to be able to open the document. Opening the document should just open word which then opens the file.
",47k,"
            16
        ","['\nYou can use Office Automation for this task. You can find answers to frequently asked questions about Office Automation with C++ at http://support.microsoft.com/kb/196776 and http://support.microsoft.com/kb/238972 .\nKeep in mind that to do Office Automation with C++, you need to understand how to use COM.\nHere are some examples of how to perform various tasks in word usign C++:\n\nhttp://support.microsoft.com/kb/220911/en-us\nhttp://support.microsoft.com/kb/238393/en-us\nhttp://support.microsoft.com/kb/238611/en-us\n\nMost of these samples show how to do it using MFC, but the concepts of using COM to manipulate Word are the same, even if you use ATL or COM directly.\n', '\nAs posted as an answer to a similar question, I advise you to look at this page where the author explains what solution he took to generate Word documents on a server, without MsWord being available, without automation or thirdparty libraries.\n', '\nWhen you have the file and just want to print it, then look at this entry at Raymond Chen\'s blog. You can use the verb ""print"" for printing.\nSee the shellexecute msdn entry for details.\n', '\nYou can use automation to open MS Word (in background or foreground) and then send the needed commands.\nA good starting place is the knowledge base article Office Automation Using Visual C++\nSome C source code is available in How To Use Visual C++ to Access DocumentProperties with Automation (the title says C++, but it is plain C)\n', '\nI have no experience from integrating with Microsoft Office, but I guess there are some APIs around that you can use for this.\nHowever, if what you want to accomplish is a rudimentary way of printing formatted output and exporting it to a file that can be handled in Word, you might want to look into the RTF format. The format is quite simple to learn, and is supported by the RtfTextBox (or is it RichTextBox?), which also has some printing capabilities. The rtf format is the same format as is used by Windows Wordpad (write.exe).\nThis also has the benefit of not depending on MS Office in order to work.\n', '\nMy solution to this is to use the following command:\nstart /min winword <filename> /q /n /f /mFilePrint /mFileExit\n\nThis allows the user to specify a printer, no. of copies, etc.\nReplace <filename> with the filename. It must be enclosed in double-quotation marks if it contains spaces. (e.g. file.rtf, ""A File.docx"")\nIt can be placed within a system call as in:\nsystem(""start /min winword <filename> /q /n /f /mFilePrint /mFileExit"");\n\nHere is a C++ header file with functions that handle this so you don\'t have to remember all of the switches if you use it frequently:\n/*winword.h\n *Includes functions to print Word files more easily\n */\n\n#ifndef WINWORD_H_\n#define WINWORD_H_\n\n#include <string.h>\n#include <stdlib.h>\n\n//Opens Word minimized, shows the user a dialog box to allow them to\n//select the printer, number of copies, etc., and then closes Word\nvoid wordprint(char* filename){\n   char* command = new char[64 + strlen(filename)];\n   strcpy(command, ""start /min winword \\"""");\n   strcat(command, filename);\n   strcat(command, ""\\"" /q /n /f /mFilePrint /mFileExit"");\n   system(command);\n   delete command;\n}\n\n//Opens the document in Word\nvoid wordopen(char* filename){\n   char* command = new char[64 + strlen(filename)];\n   strcpy(command, ""start /max winword \\"""");\n   strcat(command, filename);\n   strcat(command, ""\\"" /q /n"");\n   system(command);\n   delete command;\n}\n\n//Opens a copy of the document in Word so the user can save a copy\n//without seeing or modifying the original\nvoid wordduplicate(char* filename){\n   char* command = new char[64 + strlen(filename)];\n   strcpy(command, ""start /max winword \\"""");\n   strcat(command, filename);\n   strcat(command, ""\\"" /q /n /f"");\n   system(command);\n   delete command;\n}\n\n#endif\n\n']",https://stackoverflow.com/questions/145573/creating-opening-and-printing-a-word-file-from-c,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Programmatically add trusted sites to Internet Explorer,"
I'm doing an IE automation project using WatiN. 
When a file to be downloaded is clicked, I get the following in the Internet Explorer Information bar:

To help protect your security,
  Internet Explorer has blocked this
  site from downloading files to you
  computer.

In order to download the report, I can manually add the site to Internet Explorer's list of trusted sites, but I would prefer to check programmatically in .NET to see if the site is trusted and add it to the list if it is not. 
FYI, I'm currently using IE7.
",36k,"
            14
        ","['\nHave a look at this\nBasically it looks as if all you have to do is create registry key in \nHKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ZoneMap\\Domains\\DOMAINNAME\n\nthen a REG_DWORD value named ""http"" with value==2\n', '\nHere\'s the implementation that I came up with for writing the registry keys in .NET.\nThanks for setting me in the right direction, Ben.\nusing System;\nusing System.Collections.Generic;\nusing Microsoft.Win32;\n\n\nnamespace ReportManagement\n{\n    class ReportDownloader\n    {\n        [STAThread]\n        static void Main(string[] args)\n        {\n\n            const string domainsKeyLocation = @""Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ZoneMap\\Domains"";\n            const string domain = @""newsite.com"";\n            const int trustedSiteZone = 0x2;\n\n            var subdomains = new Dictionary<string, string>\n                                 {\n                                     {""www"", ""https""},\n                                     {""www"", ""http""},\n                                     {""blog"", ""https""},\n                                     {""blog"", ""http""}\n                                 };\n\n            RegistryKey currentUserKey = Registry.CurrentUser;\n\n            currentUserKey.GetOrCreateSubKey(domainsKeyLocation, domain, false);\n\n            foreach (var subdomain in subdomains)\n            {\n                CreateSubdomainKeyAndValue(currentUserKey, domainsKeyLocation, domain, subdomain, trustedSiteZone);\n            }\n\n            //automation code\n        }\n\n        private static void CreateSubdomainKeyAndValue(RegistryKey currentUserKey, string domainsKeyLocation, \n            string domain, KeyValuePair<string, string> subdomain, int zone)\n        {\n            RegistryKey subdomainRegistryKey = currentUserKey.GetOrCreateSubKey(\n                string.Format(@""{0}\\{1}"", domainsKeyLocation, domain), \n                subdomain.Key, true);\n\n            object objSubDomainValue = subdomainRegistryKey.GetValue(subdomain.Value);\n\n            if (objSubDomainValue == null || Convert.ToInt32(objSubDomainValue) != zone)\n            {\n                subdomainRegistryKey.SetValue(subdomain.Value, zone, RegistryValueKind.DWord);\n            }\n        }\n    }\n\n    public static class RegistryKeyExtensionMethods\n    {\n        public static RegistryKey GetOrCreateSubKey(this RegistryKey registryKey, string parentKeyLocation, \n            string key, bool writable)\n        {\n            string keyLocation = string.Format(@""{0}\\{1}"", parentKeyLocation, key);\n\n            RegistryKey foundRegistryKey = registryKey.OpenSubKey(keyLocation, writable);\n\n            return foundRegistryKey ?? registryKey.CreateSubKey(parentKeyLocation, key);\n        }\n\n        public static RegistryKey CreateSubKey(this RegistryKey registryKey, string parentKeyLocation, string key)\n        {\n            RegistryKey parentKey = registryKey.OpenSubKey(parentKeyLocation, true); //must be writable == true\n            if (parentKey == null) { throw new NullReferenceException(string.Format(""Missing parent key: {0}"", parentKeyLocation)); }\n\n            RegistryKey createdKey = parentKey.CreateSubKey(key);\n            if (createdKey == null) { throw new Exception(string.Format(""Key not created: {0}"", key)); }\n\n            return createdKey;\n        }\n    }\n}\n\n', '\nGlad I came across your postings. The only thing I can add to the excellent contributions already is that a different registry key is used whenever the URI contains an IP address i.e. the address isn\'t a fully qualified domain name.\nIn this instance you have to use an alternative approach:\nImagine I wish to add an IP address to the trusted sites: say 10.0.1.13 and I don\'t care what protocol.\n\nUnder HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ZoneMap\\Ranges, I create a key e.g. ""Range1"" and the inside that create the following values:\nA DWORD with name ""*"" and value 0x2  (for all protocols(*) and trusted site(2))\n    A string with name "":Range"" with value ""10.0.1.13""\n\n', '\nUsing powershell it is quite easy.\n#Setting IExplorer settings\nWrite-Verbose ""Now configuring IE""\n#Add http://website.com as a trusted Site/Domain\n#Navigate to the domains folder in the registry\nset-location ""HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings""\nset-location ZoneMap\\Domains\n\n#Create a new folder with the website name\nnew-item website/ -Force\nset-location website/\nnew-itemproperty . -Name * -Value 2 -Type DWORD -Force\nnew-itemproperty . -Name http -Value 2 -Type DWORD -Force\nnew-itemproperty . -Name https -Value 2 -Type DWORD -Force\n\n', '\nIn addition to adding the domain to the Trusted Sites list, you may also need to change the setting ""Automatically prompt for file downloads"" for the Trusted Sites zone. To do so programatically, you modify the key/value:\n\nHKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet\n  Settings\\Zones\\2@2200\n\nChange the value from 3 (Disable) to 0 (Enable). Here\'s some C# code to do that:\npublic void DisableForTrustedSitesZone()\n{\n    const string ZonesLocation = @""Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\Zones"";\n    const int TrustedSiteZone = 2;\n\n    const string AutoPromptForFileDownloadsValueName = @""2200"";\n    const int AutoPromptForFileDownloadsValueEnable = 0x00;     // Bypass security bar prompt\n\n    using (RegistryKey currentUserKey = Registry.CurrentUser)\n    {\n        RegistryKey trustedSiteZoneKey = currentUserKey.OpenSubKey(string.Format(@""{0}\\{1:d}"", ZonesLocation, TrustedSiteZone), true);\n        trustedSiteZoneKey.SetValue(AutoPromptForFileDownloadsValueName, AutoPromptForFileDownloadsValueEnable, RegistryValueKind.DWord);\n    }\n}\n\n', '\nHere is the implementation of adding trusted sites programmatically to IE - based on Even Mien\'s code. It supports domain name and IP address as well. The limitation is no specific protocol could be defined, instead it simply uses ""*"" for all protocols.\n//  Source : http://support.microsoft.com/kb/182569\nstatic class IeTrustedSite\n{\n    const string DOMAINS_KEY = @""Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ZoneMap\\Domains"";\n    const string RANGES_KEY = @""Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ZoneMap\\Ranges"";\n\n    const int TRUSTED_SITE_CODE = 0x2;\n    const string ALL_PROTOCOL = ""*"";\n    const string RANGE_ADDRESS = "":Range"";\n\n    public static void AddSite(string address)\n    {\n        string[] segmentList = address.Split(new string[] {"".""}, StringSplitOptions.None);\n        if (segmentList.Length == 4)\n            AddIpAddress(segmentList);\n        else\n            AddDomainName(segmentList);\n    }\n\n    static void AddIpAddress(string[] segmentList)\n    {\n        string ipAddress = segmentList[0] + ""."" + segmentList[1] + ""."" + segmentList[2] + ""."" + segmentList[3];\n        RegistryKey rangeKey = GetRangeKey(ipAddress);\n\n        rangeKey.SetValue(ALL_PROTOCOL, TRUSTED_SITE_CODE, RegistryValueKind.DWord);\n        rangeKey.SetValue(RANGE_ADDRESS, ipAddress, RegistryValueKind.String);\n    }\n\n    static RegistryKey GetRangeKey(string ipAddress)\n    {\n        RegistryKey currentUserKey = Registry.CurrentUser;\n        for (int i = 1; i < int.MaxValue; i++)\n        {\n            RegistryKey rangeKey = currentUserKey.GetOrCreateSubKey(RANGES_KEY, ""Range"" + i.ToString());\n\n            object addressValue = rangeKey.GetValue(RANGE_ADDRESS);\n            if (addressValue == null)\n            {\n                return rangeKey;\n            }\n            else\n            {\n                if (Convert.ToString(addressValue) == ipAddress)\n                    return rangeKey;\n            }\n        }\n        throw new Exception(""No range slot can be used."");\n    }\n\n    static void AddDomainName(string[] segmentList)\n    {\n        if (segmentList.Length == 2)\n        {\n            AddTwoSegmentDomainName(segmentList);\n        }\n        else if (segmentList.Length == 3)\n        {\n            AddThreeSegmentDomainName(segmentList);\n        }\n        else\n        {\n            throw new Exception(""Un-supported server address."");\n        }\n    }\n\n    static void AddTwoSegmentDomainName(string[] segmentList)\n    {\n        RegistryKey currentUserKey = Registry.CurrentUser;\n\n        string domain = segmentList[0] + ""."" + segmentList[1];\n        RegistryKey trustedSiteKey = currentUserKey.GetOrCreateSubKey(DOMAINS_KEY, domain);\n\n        SetDomainNameValue(trustedSiteKey);\n    }\n\n    static void AddThreeSegmentDomainName(string[] segmentList)\n    {\n        RegistryKey currentUserKey = Registry.CurrentUser;\n\n        string domain = segmentList[1] + ""."" + segmentList[2];\n        currentUserKey.GetOrCreateSubKey(DOMAINS_KEY, domain);\n\n        string serviceName = segmentList[0];\n        RegistryKey trustedSiteKey = currentUserKey.GetOrCreateSubKey(DOMAINS_KEY + @""\\"" + domain, serviceName);\n\n        SetDomainNameValue(trustedSiteKey);\n    }\n\n    static void SetDomainNameValue(RegistryKey subDomainRegistryKey)\n    {\n        object securityValue = subDomainRegistryKey.GetValue(ALL_PROTOCOL);\n        if (securityValue == null || Convert.ToInt32(securityValue) != TRUSTED_SITE_CODE)\n        {\n            subDomainRegistryKey.SetValue(ALL_PROTOCOL, TRUSTED_SITE_CODE, RegistryValueKind.DWord);\n        }\n    }\n}\n\nstatic class RegistryKeyExtension\n{\n    public static RegistryKey GetOrCreateSubKey(this RegistryKey registryKey, string parentString, string subString)\n    {\n        RegistryKey subKey = registryKey.OpenSubKey(parentString + @""\\"" + subString, true);\n        if (subKey == null)\n            subKey = registryKey.CreateSubKey(parentString, subString);\n\n        return subKey;\n    }\n\n    public static RegistryKey CreateSubKey(this RegistryKey registryKey, string parentString, string subString)\n    {\n        RegistryKey parentKey = registryKey.OpenSubKey(parentString, true);\n        if (parentKey == null)\n            throw new Exception(""BUG : parent key "" + parentString + "" is not exist.""); \n\n        return parentKey.CreateSubKey(subString);\n    }\n}\n\n', ""\n\nIf a website could add itself to the trusted sites, now that would be bad. \n\nI don't quite agree- as long as the browser asks the user for permission, the ability of a site to add itself to trusted sites can greatly simplify the user experience, where the user trusts the domain and wants correct page display. \nThe alternative is the user must manually go into internet options to add the domain, which is, for my users, not viable. \ni'm looking for a php or javascript method for the site to add itself, either through some IE api, or through the registry as you've so helpfully explained above!\nhave found these possible solutions so far:\n\nphp via shell\nothers i'm not allowed to list here because i don't have enough points\n\n""]",https://stackoverflow.com/questions/972345/programmatically-add-trusted-sites-to-internet-explorer,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to delete mysql row after time passes?,"
I have no idea where to start with this one:
I have a database that stores postID and Date.
What I want to do is have my website auto delete all rows where Date is less than today. This script can't have any user input at all.  No button clicks, nothing. The script must run every day at midnight.
I've been looking all over the place for something that does this and I've found absolutely nothing.
",31k,"
            14
        ","['\nYou can use PHP script and use cron job on your cpanel. \nExample:\ncronjobcommand.php\n<?php \n include \'your_db_connection\';\n mysql_query(""DELETE FROM your_table_name WHERE Date < NOW()"");\n?>\n\nI have attached a screenshot below for your more reference.\n\n', ""\nFor those out there who are on a shared hosting, like 1and1's, and can't use cron, here are 2 alternatives :\n\nmysql events enable you to place a time trigger on mysql, which will execute when you'll want, without having to be fired by any kind of user input\nif you cannot create mysql events because you're on 1and1 :(, an alternative is to use webcron\n\nYou just need to tell webcron the url of the php script you'd like to be run, and they'll trigger it for you at the intervals you want\n"", ""\nWhy using cronjobs everyday?? Why not filter data on output. For example in your select check if post date equals today with adding a simple where:\nSELECT * FROM `posts`\nWHERE (DATE(`post_date`) = DATE(NOW()));\n\nThis way you're not required to do your database managements/cronjobs on any special time and it will be used just for database managements. Afterwards you can delete unnecessary data at any time using by mysql command like:\nDELETE FROM `posts` WHERE (\n    DATE(`post_date`) < DATE(NOW())\n)\n\n"", ""\nMost hosts provide a cron(8) service that can execute commands at specific times. You use the crontab(1) program to manage the crontab(5) file the describes when to run which commands.\nThere's a lot of functionality available to you, but if you write a program (shell script, php script, C program, whatever) that runs the appropriate MySQL commands, you can call the program via cron(8) in an entirely hands-off fashion.\nRun crontab -e to edit your current crontab(5) file. If none exists, hopefully you'll get one with a helpful header. If not, copy this:\n# m h  dom mon dow   command\n\nThe columns indicate the minute, hour, day of month, month, and day of week to execute commands. All the numbers in the columns are essentially ANDed together to decide when to run commands.\nThus, midnight every night would look like this:\n0 0 * * * /path/to/executable\n\nIt's remarkably flexible, so put some time into the documentation, and you'll find many uses for it.\n"", '\nYou should set cron job (scheduled tack.) for it. \nA cron job is an automated program setup for Linux and Unix operating systems. It allows the user to execute several commands or functions at a specific time and date.\nyou have cron Job in your cpanel setup. first you need to make a php script with your logic for delete record after each date. take date from server and write script for delete.\nthen go to cron tab in your cpanel and do settings for time interval to run cron and give path of your php script file.\n', ""\nMySQL doesn't have a task scheduler. So you have to use the task scheduler of your Operating System (CRON under Linux), or to lunch a basic task checker sub-script during the script of the main page (on another page that is supposed to display the changing data).\n""]",https://stackoverflow.com/questions/9865393/how-to-delete-mysql-row-after-time-passes,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to make the Java.awt.Robot type unicode characters? (Is it possible?),"
We have a user provided string that may contain unicode characters, and we want the robot to type that string.
How do you convert a string into keyCodes that the robot will use?
How do you do it so it is also java version independant (1.3 -> 1.6)?
What we have working for ""ascii"" chars is
//char c = nextChar();
//char c = 'a'; // this works, and so does 'A'
char c = '谩'; // this doesn't, and neither does '膫'
Robot robot = new Robot();
KeyStroke key = KeyStroke.getKeyStroke(""pressed "" + Character.toUpperCase(c) );
if( null != key ) {
  // should only have to worry about case with standard characters
  if (Character.isUpperCase(c))
  {
    robot.keyPress(KeyEvent.VK_SHIFT);
  }

  robot.keyPress(key.getKeyCode());
  robot.keyRelease(key.getKeyCode());

  if (Character.isUpperCase(c))
  {
    robot.keyRelease(KeyEvent.VK_SHIFT);
  }
}

",16k,"
            12
        ","[""\nBased on javamonkey79's code I've created the following snippet which should work for all Unicode values...\npublic static void pressUnicode(Robot r, int key_code)\n{\n    r.keyPress(KeyEvent.VK_ALT);\n\n    for(int i = 3; i >= 0; --i)\n    {\n        // extracts a single decade of the key-code and adds\n        // an offset to get the required VK_NUMPAD key-code\n        int numpad_kc = key_code / (int) (Math.pow(10, i)) % 10 + KeyEvent.VK_NUMPAD0;\n\n        r.keyPress(numpad_kc);\n        r.keyRelease(numpad_kc);\n    }\n\n    r.keyRelease(KeyEvent.VK_ALT);\n}\n\nThis automatically goes through each decade of the unicode key-code, maps it to the corresponding VK_NUMPAD equivalent and presses/releases the keys accordingly.\n"", ""\nThe KeyEvent Class does not have direct mappings for many unicode classes in JRE 1.5. If you are running this on a Windows box what you may have to do is write a custom handler that does something like this:\nRobot robot = new Robot();\nchar curChar = '脙';\n\n// -- isUnicode( char ) should be pretty easy to figure out\nif ( isUnicode( curChar ) ) {\n   // -- this is an example, exact key combinations will vary\n   robot.keyPress( KeyEvent.VK_ALT );\n\n   robot.keyPress( KeyEvent.VK_NUMBER_SIGN );\n   robot.keyRelease( KeyEvent.VK_NUMBER_SIGN );\n\n   // -- have to apply some logic to know what sequence\n   robot.keyPress( KeyEvent.VK_0 );\n   robot.keyRelease( KeyEvent.VK_0 );\n   robot.keyPress( KeyEvent.VK_1 );\n   robot.keyRelease( KeyEvent.VK_1 );\n   robot.keyPress( KeyEvent.VK_9 );\n   robot.keyRelease( KeyEvent.VK_9 );\n   robot.keyPress( KeyEvent.VK_5 );\n   robot.keyRelease( KeyEvent.VK_5 );\n\n   robot.keyRelease( KeyEvent.VK_ALT );\n}\n\ne.g. Figure out what they key combinations are, and then map them to some sort of Object (maybe a HashMap?) for later lookup and execution.\nHope this helps :)\n"", '\ni think this is a bit late but... \nRobot robot = new Robot();\n\n   robot.keyPress( KeyEvent.VK_DEAD_ACUTE);\n\n   robot.keyPress( KeyEvent.VK_A );\n   robot.keyRelease( KeyEvent.VK_A );\n\n   robot.keyRelease( KeyEvent.VK_DEAD_ACUTE );\n\nthat just type an ""谩""\n', '\nThe best way that i find when solve simulare problem\nimport java.awt.AWTException;\nimport java.awt.Robot;\n\npublic class MyRobot {\n\n    public static void typeString(String s)\n        {\n            try {\n            Robot robik = new Robot();\n            byte[] bytes = s.getBytes();\n            for (byte b : bytes)\n            {\n                int code = b;\n                // keycode only handles [A-Z] (which is ASCII decimal [65-90])\n                if (code > 96 && code < 123) code = code - 32;\n                robik.delay(40);\n                robik.keyPress(code);\n                robik.keyRelease(code);\n            }\n        } catch (AWTException e){\n\n    }\n  }\n\n}\n\nhttp://www.devdaily.com/java/java-robot-class-example-mouse-keystroke\\\n']",https://stackoverflow.com/questions/397113/how-to-make-the-java-awt-robot-type-unicode-characters-is-it-possible,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is there a Python equivalent to Java's AWT Robot class? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 Questions asking us to recommend or find a tool, library or favorite off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, describe the problem and what has been done so far to solve it.


Closed 9 years ago.







                        Improve this question
                    



Does anyone know of a Python class similar to Java Robot? 
Specifically I would like to perform a screen grab in Ubuntu, and eventually track mouse clicks and keyboard presses (although that's a slightly different question).
",12k,"
            12
        ","['\nIf you have GTK, then you can use the gtk.gdk.Display class to do most of the work. It controls the keyboard/mouse pointer grabs a set of gtk.gdk.Screen objects.\n', '\nCheck out GNU LDTP:\n\nGNU/Linux Desktop Testing Project (GNU\n  LDTP) is aimed at producing high\n  quality test automation framework\n  [...]\n\nEspecially Writing LDTP test scripts in Python scripting language\n', '\nAs far as the screen grab, see this answer.  That worked for me.  Other answers to the same question might be of interest as well.\n', '\nCheck out the RobotFramework. I do not know if it will do the same things as JavaRobot, or if it will do more. But it is easy and very flexible to use.\n']",https://stackoverflow.com/questions/860013/is-there-a-python-equivalent-to-javas-awt-robot-class,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Why should I ever use CSS selectors as opposed to XPath for automated testing?,"
Please help me understand why using CSS selectors are even an option for automated testing.  I've been using the tool Ghost Inspector some in my workplace for creating lots of automated tests for our stuff.  This tool gives you the option of using CSS selectors intead of XPath. Why?
XPath is SO much more durable than CSS.  The CSS on any given UI is subject to change almost weekly on some projects/features.  This make the tests extremely brittle and prone to being broken regularly.
Is it because most new test writers don't want to learn about anything XPath and wish to stick to the basics? CSS selectors look prettier than XPath syntax? Please convince me. thanks.
",5k,"
            8
        ","['\nJeffC\'s answer here does plenty to sum up the pros and cons of each locator strategy. But I\'ll address your points specifically.\nFirst off, there is no need for anyone to convince you that selectors are better, because from a purely functional standpoint, they simply aren\'t (and I\'m saying this as someone with a gold css-selectors tag badge and almost 1000 answers to questions with that tag, so you know I\'m not biased). If you\'re more comfortable with XPath, use it 鈥?in terms of features and what you can do, XPath is vastly superior, there really is no contest there. And, as you correctly state, performance is no longer an issue (if it ever was).\nSelectors are there for quick and simple use cases and for users coming from HTML and CSS codebases, such as web developers, who want to get started with automated tests without having to learn another DSL. If you\'re responsible for the CSS of your own site you can also easily copy selectors from your stylesheet into your tests depending on what exactly you\'re testing.\nIf on the other hand you\'re coming from an XML/XSLT/XPath background, wonderful, you get to keep using the XPath you know and love1!\n\nYes, Xpath is way more durable than CSS because it can invoke specific content contains functionality.\n\nHaving a content contains feature doesn\'t make XPath more ""durable"" 鈥?it makes it more versatile. If you rely solely on an element\'s content and that content can potentially change or move around, your XPath becomes no less brittle than a selector that relies solely on an element\'s attributes or its position in the DOM tree.\nYou can do any of a number of things to make your XPath or selector more or less brittle, but that\'s an indicator of how versatile the DSL is, not how brittle it inherently is.\n\n1 Depending on what version of XPath you\'re used to.\n', ""\nOne of the most common conversation in the Selenium Community is which Locator Strategy is better among the two - Css or XPath with respect to performance. Supporters of CSS say that it is more readable and faster while those in favor of XPath says it's ability to transverse the HTML DOM (while CSS cannot). With such a divide based on different perspective it is hard to determine the best performing approach for you and your tests as a beginner. Here are some excerts from the industry experts :\n\nDave Haeffner who maintains Elemental Selenium carried out a test on a page with two HTML data tables, one table is written without helpful attributes (ID and Class), and the other with them. \n\n\nResults with Finding Elements By ID and Class :\nBrowser                 | CSS           | XPath\n----------------------------------------------------\nInternet Explorer 8     | 23 seconds    | 22 seconds\nChrome 31               | 17 seconds    | 16 seconds\nFirefox 26              | 22 seconds    | 22 seconds\nOpera 12                | 17 seconds    | 20 seconds\nSafari 5                | 18 seconds    | 18 seconds\n\nFinding Elements By Traversing :\nBrowser                 | CSS           | XPath\n----------------------------------------------------\nInternet Explorer 8     | not supported | 29 seconds\nChrome 31               | 24 seconds    | 26 seconds\nFirefox 26              | 27 seconds    | 27 seconds\nOpera 12                | 25 seconds    | 25 seconds\nSafari 5                | 23 seconds    | 22 seconds\n\nThe following were the takeaways :\n\nFor starters there is no dramatic difference in performance between XPath and CSS.\nTraversing the DOM in older browsers like IE8 does not work with CSS but is fine with XPath. And XPath can walk up the DOM (e.g. from child to parent), whereas CSS can only traverse down the DOM (e.g. from parent to child). However not being able to traverse the DOM with CSS in older browsers isn't necessarily a bad thing as it is more of an indicator that your page has poor design and could benefit from some helpful markup.\nAn argument in favor of CSS is that they are more readable, brief, and concise while it is a subjective call.\n\nBen Burton mentions you should use CSS because that's how applications are built. This makes the tests easier to write, talk about, and have others help maintain.\nAdam Goucher says to adopt a more hybrid approach -- focusing first on IDs, then CSS, and leveraging XPath only when you need it (e.g. walking up the DOM) and that XPath will always be more powerful for advanced locators.\n\n\nConclusion\nSo it appears to be a tough call to make. Especially now that we are aware with the knowledge that the choice is not as reliant on performance as it once was. The choice is not as permanent as choosing a programming language, and if you are using helpful abstraction (e.g. Page Objects) then leveraging a hybrid approach is simple to implement.\nTrivia\n\nCss Vs. X Path\nCss Vs. X Path, Under a Microscope\nCss Vs. X Path, Under a Microscope (Part 2)\n\n""]",https://stackoverflow.com/questions/51936193/why-should-i-ever-use-css-selectors-as-opposed-to-xpath-for-automated-testing,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
python-pptx - How to replace keyword across multiple runs?,"
I have two PPTs (File1.pptx and File2.pptx) in which I have the below 2 lines
XX NOV 2021, Time: xx:xx 鈥?xx:xx hrs (90mins)
FY21/22 / FY22/23

I wish to replace like below
a) NOV 2021 as NOV 2022.
b) FY21/22 / FY22/23 as FY21/22 or FY22/23.
But the problem is my replacement works in File1.pptx but it doesn't work in File2.pptx.
When I printed the run text, I was able to see that they are represented differently in two slides.
def replace_text(replacements:dict,shapes:list):
    for shape in shapes:
        for match, replacement in replacements.items():
            if shape.has_text_frame:
                if (shape.text.find(match)) != -1:
                    text_frame = shape.text_frame
                    for paragraph in text_frame.paragraphs:
                        for run in paragraph.runs:
                            cur_text = run.text
                            print(cur_text)
                            print(""---"")
                            new_text = cur_text.replace(str(match), str(replacement))
                            run.text = new_text

In File1.pptx, the cur_text looks like below (for 1st keyword). So, my replace works (as it contains the keyword that I am looking for)

But in File2.pptx, the cur_text looks like below (for 1st keyword). So, replace doesn't work (because the cur_text doesn't match with my search term)

The same issue happens for 2nd keyword as well which is FY21/22 / FY22/23.
The problem is the split keyword could be in previous or next run from current run (with no pattern). So, we should be able to compare a search term with previous run term (along with current term as well). Then a match can be found (like Nov 2021) and be replaced.
This issue happens for only 10% of the search terms (and not for all of my search terms) but scary to live with this issue because if the % increases, we may have to do a lot of manual work. How do we avoid this and code correctly?
How do we get/extract/find/identify the word that we are looking for across multiple runs (when they are indeed present) like CTRL+F and replace it with desired keyword?
Any help please?
UPDATE - Incorrect replacements based on matching
Before replacement

After replacement

My replacement keywords can be found below
replacements = { 'How are you?': ""I'm fine!"",
                'FY21/22':'FY22/23',
                'FY_2021':'FY21/22',
                'FY20/21':'FY21/22',
                'GB2021':'GB2022',
                'GB2020':'GB2022',
                'SEP-2022':'SEP-2023',
                'SEP-2021':'SEP-2022',
                'OCT-2021':'OCT-2022',
                'OCT-2020':'OCT-2021',
                'OCT 2021':'OCT 2022',
                'NOV 2021':'NOV 2022',
                'FY2122':'FY22/23',
                'FY2021':'FY21/22',
                'FY1920':'FY20/21',
                'FY_2122':'FY22/23',
                'FY21/22 / FY22/23':'FY21/22 or FY22/23',
                'F21Y22':'FY22/23',
                'your FY20 POS FCST':'your FY22/23 POS FCST',
                'your FY21/22 POS FCST':'your FY22/23 POS FCST',
                'Q2/FY22/23':'Q2-FY22/23',
                'JAN-22':'JAN-23',
                'solution for FY21/22':'solution for FY22/23',
                'achievement in FY20/21':'achievement in FY21/22',
                'FY19/20':'FY20/21'}

",525,"
            4
        ","['\nAs one can find in python-pptx\'s documentation at https://python-pptx.readthedocs.io/en/latest/api/text.html\n\na text frame is made up of paragraphs and\na paragraph is made up of runs and specifies a font configuration that is used as the default for it\'s runs.\nruns specify part of the paragraph\'s text with a certain font configuration - possibly different from the default font configuration in the paragraph\n\nAll three have a field called text:\n\nThe text frame\'s text contains all the text from all it\'s paragraphs concatenated together with the appropriate line-feeds in between the paragraphs.\nThe paragraphs\'s text contains all the texts from all of it\'s runs concatenated to a long string with a vertical tab character (\\v) put wherever there was a so-called soft-break in any of the run\'s text (a soft break is like a line-feed but without terminating the paragraph).\nThe run\'s text contains text that is to be rendered with a certain font configuration (font family, font size, italic/bold/underlined, color etc. pp). It is the lowest level of the font configuration for any text.\n\nNow if you specify a line of text in a text-frame in a PowerPoint presentation, this text-frame will very likely only have one paragraph and that paragraph will have just one run.\nLet\'s say that line says: Hi there! How are you? What is your name? and is all normal (neither italic nor bold) and in size 10.\nNow if you go ahead in PowerPoint and make the questions How are you? What is your name? stand out by making them italic, you will end up with 2 runs in our paragraph:\n\nHello there!  with the default font configuration from the paragraph\nHow are you? What is you name? with the font configuration specifying the additional italic attribute.\n\nNow imagine, we want the How are you? stand out even more by making it bold and italic. We end up with 3 runs:\n\nHello there!  with the default font configuration from the paragraph.\nHow are you? with the font configuration specifying the BOLD and ITALIC attribute\n What is your name? with the font configuration specifying the ITALIC attribute.\n\nOne step further, making the are in How are you? bigger. We get 5 runs:\n\nHello there!  with the default font configuration from the paragraph.\nHow  with the font configuration specifying the BOLD and ITALIC attribute\nare with the font configuration specifying the BOLD and ITALIC attribute and font size 16\n you? with the font configuration specifying the BOLD and ITALIC attribute\n What is your name? with the font configuration specifying the ITALIC attribute.\n\nSo if you try to replace the How are you? with I\'m fine! with the code from your question, you won\'t succeed, because the text How are you? is actually distributed across 3 runs.\nYou can go one level higher and look at the paragraph\'s text, that still says Hello there! How are you? What is your name? since it is the concatenation of all its run\'s texts.\nBut if you go ahead and do the replacement of the paragraph\'s text, it will erase all runs and create one new run with the text Hello there! I\'m fine! What is your name? all the while deleting all the formatting that we put on the What is your name?.\nTherefore, changing text in a paragraph without affecting formatting of the other text in the paragraph is pretty involved. And even if the text you are looking for has all the same formatting, that is no guarantee for it to be within one run. Because if you - in our example above - make the are smaller again, the 5 runs will very likely remain, the runs 2 to 4 just having the same font configuration now.\nHere is the code to produce a test presentation with a text box containing the exact paragraph runs as given in my example above:\nfrom pptx import Presentation\nfrom pptx.chart.data import CategoryChartData\nfrom pptx.enum.chart import XL_CHART_TYPE,XL_LABEL_POSITION\nfrom pptx.util import Inches, Pt\nfrom pptx.dml.color import RGBColor\nfrom pptx.enum.dml import MSO_THEME_COLOR\n\n# create presentation with 1 slide ------\nprs = Presentation()\nslide = prs.slides.add_slide(prs.slide_layouts[5])\ntextbox_shape = slide.shapes.add_textbox(Pt(200),Pt(200),Pt(30),Pt(240))\ntext_frame = textbox_shape.text_frame\np = text_frame.paragraphs[0]\nfont = p.font\nfont.name = \'Arial\'\nfont.size = Pt(10)\nfont.bold = False\nfont.italic = False\nfont.color.rgb = RGBColor(0,0,0)\n\nrun = p.add_run()\nrun.text = \'Hello there! \'\n\nrun = p.add_run()\nrun.text = \'How \'\nfont = run.font\nfont.italic = True\nfont.bold = True\n\nrun = p.add_run()\nrun.text = \'are\'\nfont = run.font\nfont.italic = True\nfont.bold = True\nfont.size = Pt(16)\n\nrun = p.add_run()\nrun.text = \' you?\'\nfont = run.font\nfont.italic = True\nfont.bold = True\n\nrun = p.add_run()\nrun.text = \' What is your name?\'\nrun.font.italic = True\n\nprs.save(\'text-01.pptx\')\n\nAnd this is what it looks like, if you open it in PowerPoint:\n\nNow if you install the python code from my GitHub repository at https://github.com/fschaeck/python-pptx-text-replacer by running the command\npython -m pip install python-pptx-text-replacer\n\nand after successful installation run the command\npython-pptx-text-replacer -m ""How are you?"" -r ""I\'m fine!"" -i text-01.pptx -o text-02.pptx\n\nthe resulting presentation text-02.pptx will look like this:\n\nAs you can see, it mapped the replacement string exactly onto the existing font-configurations, thus if your match and it\'s replacement have the same length, the replacement string will retain the exact format of the match.\nBut - as an important side-note - if the text-frame has auto-size or fit-frame switched on, even all that work won\'t save you from screwing up the formatting, if the text after the replacement needs more or less space!\nIf you got issues with this code, please use the possibly improved version from GitHub first. If your problem remains, use the GitHub issue tracker to report it. The discussion of this question and answer is already getting out of hand. ;-)\n']",https://stackoverflow.com/questions/73219378/python-pptx-how-to-replace-keyword-across-multiple-runs,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gluing (Imposition) PDF documents,"
I have several A4 PDF documents which I would like (two into one) ""glue"" together into A3 format PDF document. So I will get from 2PDFs A4 a single one sided PDF A3.
I have found the excellent utility PDFToolkit and some others but none of them can be used to ""glue"" side by side two documents. 
",24k,"
            20
        ","['\nI just came across a nice tool on superuser.com called PDFjam that can do all of the above in a single command:\npdfjam --nup 2x1 file1.pdf file2.pdf --outfile DONESKI.pdf\n\nIt has other standard features like page size plus a nice syntax for more sophisticated collations of pages (the tricky page re-ordering necessary for true booklet-style page imposition).\nIt\'s built on top of TeX which is, whatever it is. Installing is a breeze on Ubuntu: you can just apt-get install pdfjam. On Mac OS, I recommend getting BasicTeX (google ""mactex basictex""; SO thinks I\'m a spammer and won\'t let me post the link).\nThis is a lot easier and more maintanable than installing both pdftk and Multivalent (on both Mac OS for dev and Ubuntu for deploy), which wasn\'t going so well for me anyway...!\n', '\nFound the following (free and open-source) tool for doing Imposition called Impose (thanks danio for the tip). This solved my problem perfectly. \nEDIT:\nHere is how it\'s done:\nUse PDF Toolkit to joint two PDF files into one (two A4)\npdftk File1.pdf File2.pdf cat output OutputFile.pdf\n\nCreate from this a single page (one A3):\njava -cp Multivalent.jar tool.pdf.Impose -dim 2x1 -verbose -paper-size ""42.2x29.9cm"" -layout ""1,2"" OutputFile.pdf\n\n', ""\nI would like to advertise my pdftools\nIt's written in Python so should run on any platform. It's a wrapper to Latex (the pdfpages packages)  but can do lot of things with a single command line: merge pdf files, nup them (multiple input pages per output page) and number the pages of the output file (you specify the location and the format of the number)\nIt still needs some work but I think it's quite stable to be usable right now :)\n"", '\nThis puts two landscape letter pages onto a single portrait letter sheet, to be ""bound"" (i.e., folded) along the top.\npdftops $1 - | \npsbook | \npstops -w11in -h8.5in \'4:1@.65(.5in,0in)+0@.65(.5in,5.5in),2U@.65(8in,5.5in)+3@.65U(8in,11in)\' | \nps2pdf - $(basename $1 .pdf).psbook.pdf\n\nBy the way, I do this often, so I\'ll probably submit more ""answers"" to this question just to keep track of successful pstops pagespecs. Let me know if this is an inappropriate use of SO.\n', ""\nA nice, powerful, open-source imposition tool is included\nin the PoDoFo package:\n  http://podofo.sourceforge.net/\nIt works for me.  Some imposition plans can be found at:\n  http://www.av8n.com/computer/prepress/\nPoDoFo can do lots of other stuff, not just imposition.\nAnother useful imposition tool is Bookbinder (on the\nquantumelephant site).  It has a GUI that appeals to non-experts.\nIt is not as flexible or powerful as PoDoFo, but it can do\nimposition.\npdftk is more-or-less essential to have, but it will not\ndo imposition.\npdfjam is useless to me, because there are a wide range of\nvalid pdf files that it cannot handle.\nI've never been able to get multivalent to work, either.\n"", '\nWhat you want to do is imposition.  There are commercial tools to impose PDFs such as ARTS crackerjack and Quite imposing but they are pretty expensive (US$500), require a copy of acrobat professional and are overkill for imposing 2 A4 pages to an A3 sheet.\n', ""\nOn the Postscript side, a tool named pstops is able to rearrange pages of a Postscript file in any way you could imagine. I've not heard of such a tool for PDF. But pdf2ps and ps2pdf exist. So a not-so-ideal solution may be a combination of pdf2ps, pstops and ps2pdf.\n"", '\nI would combine the two A4 pages into one 2-page PDF using pdftk.  Then Print to PDF using something like PrimoPDF, and tell it to print to A3 format, two pages per side.\nI just tested this printing some slides from PowerPoint.  It worked great.  I selected A3 as my paper size in PowerPoint, and then chose to print 2 pages per side.  Printed to Primo and voila, I have two A4 slides per A3.\n', '\nYou can put multiple input pages on one output page using BookletImposer.\nAnd you can change page orders and combine multiple pdf files using PDF Mod.\nWith these two tools, you can do almost everything you want with pdf files (except editing their content).\n', '\nI had a similar problem. I tried Impose but it was giving me an\nException in thread ""main"" java.lang.NoClassDefFoundError: tool/pdf/Impose\nCaused by: java.lang.ClassNotFoundException: tool.pdf.Impose\n(...)\nCould not find the main class: tool.pdf.Impose.  Program will exit.\n\nI then tried PDF Snake which isn\'t free or open source, but has a completely unrestricted 30-day trial version. It worked perfectly, after tweaking the parameters to achieve what I wanted. It\'s a great tool. I would definitely buy it if it wasn\'t so expensive! Anyway, I thought I\'d leave my 2 cents in case anyone had the same problem I had with Impose.\n', '\nlook at this\nhttp://sourceforge.net/projects/proposition/\nIt needs laTex to run,\nbut when it does, works really fine\nRegards\n']",https://stackoverflow.com/questions/465271/gluing-imposition-pdf-documents,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get Sikuli working in headless mode,"
If we have a headless test server running sikuli (both ubuntu and windows configurations needed), how to get it work without a physical monitor and preferably for as many screen resolutions as possible.
",24k,"
            19
        ","['\nI successfully got sikuli running in headless mode (no physical monitor connected)\nUbuntu: check Xvfb.\nWindows: install display driver on the machine (to be headless) from virtualbox guest additions display drivers and use TightVNC to remotely set resolution from another machine.\nDetailed steps for windows 7\nAssume that:\n\nMachine A: to be headless machine, windows 7, with vnc server ready (e.g. TightVNC server installed and waiting for connections).\nMachine B: will be used to remotely setup the virtual display driver on machine A.\n\nsteps:\n\nDownload virtualbox guest additions iso file on Machine A from here (for latest version check latest version here and download VBoxGuestAdditions_x.y.z.iso)\n\nExtract iso file (possibly with winrar) into a directory (let us call it folder D)\n\nusing command prompt cd to D folder\nDriver extraction \n-To extract the 32-bit drivers to ""C:\\Drivers"", do the following:\n\nVBoxWindowsAdditions-x86 /extract /D=C:\\Drivers\n\n-For the 64-bit drivers:\n\nVBoxWindowsAdditions-amd64 /extract /D=C:\\Drivers\n\n\nGoto device manager\n\nadd hardware\n\n\n\n\n\n\n\n\n\nRestart and connect with VNC viewer, now you should be able to change screen resolution \n\n\nother valuable info on launchpad.\n', '\nI got SikuliX working in a true headless mode in GCE with a Windows 2016 client system. It takes some duct tape and other Rube Goldberg contraptions to work, but it can be done.\nThe issue is that, for GCE (and probably AWS and other cloud environment Windows clients), you don\'t have a virtual video adapter and display, so, unless there\'s an open RDP connection to the client, it doesn\'t have a screen, and SikuliX/OpenCV will get a 1024x768 black desktop, and fail. \nSo, the question is, how to create an RDP connection without having an actual screen anywhere. I did this using Xvfb (X Windows virtual frame buffer). This does require a second VM, though. Xvfb runs on Linux. The other piece of the puzzle is xfreerdp 2.0. The 2.x version is required for compatibility with recent versions of Windows. 1.x is included with some Linux distros; 2.x may need to be built from sources, depending on what flavor Linux you\'re using. I\'m using CentOS, which did require me to build my own.\nThe commands to establish the headless RDP session, once the pieces are in place, look something like this:\n/usr/bin/Xvfb :0 -screen 0 1920x1080x24 &\nexport DISPLAY=:0.0\n/usr/local/bin/xfreerdp /size:1920x1080 /u:[WindowsUser] /p:""[WindowsPassword]"" /v:[WindowsTarget]\n\nIn our environment we automated this as part of the build job kicked off by Jenkins. For this to work under the Jenkins slave, it was also necessary to run the Jenkins slave as a user process, rather than a service... this can be accomplished by enabling auto admin login and setting the slave launch script as a run (on logon) command.\n', '\nFor those looking to automate on ec2 windows machines, this worked for me: http://www.allianceglobalservices.com/blog/executing-automation-suite-on-disconnectedlocked-machines\nIn summary, I used RDC to connect, put the following code in a batch file on remote desktop, double clicked it, and sikulix  started working remotely (kicking me out of RDC at the same time).  Note that ec2 windows machines default to 1024x768 when tscon takes over which may be too small so TightVnc can be used to increase the resolution to 1280x1024 before running. \ntscon.exe 0 /dest:console\ntscon.exe 1 /dest:console\ntscon.exe 2 /dest:console\ntscon.exe 3 /dest:console\nSTART /DC:\\Sikulix /WAIT /B C:\\Sikulix\\runsikulix.cmd -d 3 -r C:\\test.sikuli -f C:\\Sikulix\\log.txt -d C:\\Sikulix\\userlog.txt\n\n', '\nI just figure out a way to resolve similar issue.\nMy env:\nlocal: windows pc\nremote (for running sikulix + app I would like to test): windows ec2 instance\nMy way:\n1.create a .bat file, its contents:\nping 127.0.0.1 -n 15 > nul\n\nfor /f ""skip=1 tokens=3"" %%s in (\'query user %USERNAME%\') do (\n  %windir%\\System32\\tscon.exe %%s /dest:console\n)\n\ncd ""\\path\\to\\sikulix""\njava -jar sikulixide-2.0.5.jar -r /path/to/sikulix -c > logfile.log\n\n\nprepare your app\nrun the bat (right click > run as administrator)\nping will give your 10s, so that you can bring your app back to front\nyou will be disconnnected from rdp connection\n\nExplanation:\n\nping is like ""sleep""\nfor loop: kick out current user & keep session alive\n\n']",https://stackoverflow.com/questions/26032706/how-to-get-sikuli-working-in-headless-mode,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"VBA Internet Explorer Automation - How to Select ""Open"" When Downloading a File","
This is my first question ever here on stackoverflow!
I've been searching for a solution to this problem for a while and haven't found any help. I may just be using the wrong keywords in my searches, but so far I've had no luck. Here's the question:
In VBA, how can I select the ""Open"" option from the file download dialog in Internet Explorer?
Just for extra clarification, I'm talking about the yellow-orange bar that pops up across the bottom of the screen in IE9 when a file is downloaded.
I'm doing some VBA automation to download hundreds of PDFs from the web using Internet Explorer, but there is an intermediate step where a .fdf file has to be opened before I get to the actual PDF. So I first need to select the ""Open"" option so that I can move on to the next step of the automation. Like I said earlier, I've done a lot of searching and had no luck so far.
I've tried using SendKeys in hopes that hitting Enter would work, and that was a last ditch effort that didn't work.
Thanks in advance for the help!
",48k,"
            14
        ","[""\nI have covered this extensively here.\nTopic: VBA/VB.Net/VB6鈥揅lick Open/Save/Cancel Button on IE Download window 鈥?PART I\nLink: http://www.siddharthrout.com/2011/10/23/vbavb-netvb6click-opensavecancel-button-on-ie-download-window/\nand\n\nEDIT (IMP) If you are using IE 9 Do not forget to read PART 2 as it includes and covers the window structure of IE 9 download window\n\nTopic: VBA/VB.Net/VB6鈥揅lick Open/Save/Cancel Button on IE Download window 鈥?PART II\nLink: http://www.siddharthrout.com/2012/02/02/vbavb-netvb6click-opensavecancel-button-on-ie-download-window-part-ii/\nThe above links discuss on how to use use the API's to achieve what you want.\nFrom the 1st link...\n\nLike you and me, we both have names, similarly windows have 鈥渉andles鈥?(hWnd), Class etc. Once you know what that hWnd is, it is easier to interact with that window.\nFindwindow API finds the hWnd of a particular window by using the class name and the caption of the window (鈥淔ile Download鈥? in this case. The 鈥淥pen鈥? 鈥淪ave鈥?and 鈥淐ancel鈥?buttons are windows in itself but they are child windows of the main window which is 鈥淔ile Download鈥? That means each one of those will also have a hWnd :) To find the child windows, we don鈥檛 use FindWindow but use FindWindowEx. All the three buttons 鈥淥pen鈥? 鈥淪ave鈥?and 鈥淐ancel鈥?have the same class which is 鈥?Button鈥?\n\n"", '\nSimilar post: link\n    Option Explicit\n    Dim ie As InternetExplorer\n    Dim h As LongPtr\n    Private Declare PtrSafe Function FindWindowEx Lib ""user32"" Alias ""FindWindowExA"" (ByVal hWnd1 As LongPtr, ByVal hWnd2 As LongPtr, ByVal lpsz1 As String, ByVal lpsz2 As String) As LongPtr\n\nSub Download()\n    Dim o As IUIAutomation\n    Dim e As IUIAutomationElement\n    Set o = New CUIAutomation\n    h = ie.Hwnd\n    h = FindWindowEx(h, 0, ""Frame Notification Bar"", vbNullString)\n    If h = 0 Then Exit Sub\n\n    Set e = o.ElementFromHandle(ByVal h)\n    Dim iCnd As IUIAutomationCondition\n    Set iCnd = o.CreatePropertyCondition(UIA_NamePropertyId, ""Open"")\n\n    Dim Button As IUIAutomationElement\n    Set Button = e.FindFirst(TreeScope_Subtree, iCnd)\n    Dim InvokePattern As IUIAutomationInvokePattern\n    Set InvokePattern = Button.GetCurrentPattern(UIA_InvokePatternId)\n    InvokePattern.Invoke\nEnd Sub \n\n', '\nI sent the shortcut keys to the application. Here they are for IE11. Sorry I could not test in IE9. If you hold down Alt, it may show you the other key to the combo as IE11 does. \nNote: the code will not run as you expect if IE is not the active window on your machine so it won\'t work while in debug mode.\n\nShortcut key:Alt+O\nVBA: Application.SendKeys ""%{O}""\n\n']",https://stackoverflow.com/questions/10400795/vba-internet-explorer-automation-how-to-select-open-when-downloading-a-file,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accessing Excel Custom Document Properties programmatically,"
I'm trying to add custom properties to a workbook I have created programmatically.  I have a method in place for getting and setting properties, but the problem is the workbook is returning null for the CustomDocumentProperties property.  I cannot figure out how to initialize this property so that I can add and retrieve properties from the workbook.  Microsoft.Office.Core.DocumentProperties is an interface, so I cant go and do the following
if(workbook.CustomDocumentProperties == null)
    workbook.CustomDocumentProperties = new DocumentProperties;

Here is the code I have to get and set the properties:
     private object GetDocumentProperty(string propertyName, MsoDocProperties type)
    {
        object returnVal = null;

        Microsoft.Office.Core.DocumentProperties properties;
        properties = (Microsoft.Office.Core.DocumentProperties)workBk.CustomDocumentProperties;

        foreach (Microsoft.Office.Core.DocumentProperty property in properties)
        {
            if (property.Name == propertyName && property.Type == type)
            {
                returnVal = property.Value;
            }
            DisposeComObject(property);
        }

        DisposeComObject(properties);

        return returnVal;
    }

    protected void SetDocumentProperty(string propertyName, string propertyValue)
    {
        DocumentProperties properties;
        properties = workBk.CustomDocumentProperties as DocumentProperties;

        bool propertyExists = false;
        foreach (DocumentProperty prop in properties)
        {
            if (prop.Name == propertyName)
            {
                prop.Value = propertyValue;
                propertyExists = true;
            }
            DisposeComObject(prop);

            if(propertyExists) break;
        }

        if (!propertyExists)
        {
            properties.Add(propertyName, false, MsoDocProperties.msoPropertyTypeString, propertyValue, Type.Missing);
        }

        DisposeComObject(propertyExists);

    }

The line
    properties = workBk.CustomDocumentProperties as DocumentProperties;
always set properties to null.
This is using Microsoft.Office.Core v12.0.0.0 and Microsoft.Office.Interop.Excell v12.0.0.0 (Office 2007)
",21k,"
            13
        ","['\nIf you are targetting .NET 4.0, you can use the dynamic key word for late binding\n Document doc = GetActiveDocument();\n if ( doc != null )\n {\n     dynamic properties = doc.CustomDocumentProperties;\n     foreach (dynamic p in properties)\n     {\n         Console.WriteLine( p.Name + "" "" + p.Value);\n     }\n }\n\n', '\nI looked at my own code and can see that I access the properties using late binding. I can\'t remember why, but I\'ll post some code in case it helps.\nobject properties = workBk.GetType().InvokeMember(""CustomDocumentProperties"", BindingFlags.Default | BindingFlags.GetProperty, null, workBk, null);\n\nobject property = properties.GetType().InvokeMember(""Item"", BindingFlags.Default | BindingFlags.GetProperty, null, properties, new object[] { propertyIndex });\n\nobject propertyValue = property.GetType().InvokeMember(""Value"", BindingFlags.Default | BindingFlags.GetProperty, null, propertyWrapper.Object, null);\n\nEDIT: ah, now I remember why. :-)\nEDIT 2: Jimbojones\' answer - to use the dynamic keyword - is a better solution (if you value ease-of-use over the performance overhead of using dynamic).\n', '\nI found the solution here.\nHere is the code I ended up with:\n    public void SetDocumentProperty(string propertyName, string propertyValue)\n    {\n        object oDocCustomProps = workBk.CustomDocumentProperties;\n        Type typeDocCustomProps = oDocCustomProps.GetType();\n\n        object[] oArgs = {propertyName,false,\n                 MsoDocProperties.msoPropertyTypeString,\n                 propertyValue};\n\n        typeDocCustomProps.InvokeMember(""Add"", BindingFlags.Default |\n                                   BindingFlags.InvokeMethod, null,\n                                   oDocCustomProps, oArgs);\n\n    }\n\n    private object GetDocumentProperty(string propertyName, MsoDocProperties type)\n    {\n        object returnVal = null;\n\n        object oDocCustomProps = workBk.CustomDocumentProperties;\n        Type typeDocCustomProps = oDocCustomProps.GetType();\n\n\n        object returned = typeDocCustomProps.InvokeMember(""Item"", \n                                    BindingFlags.Default |\n                                   BindingFlags.GetProperty, null,\n                                   oDocCustomProps, new object[] { propertyName });\n\n        Type typeDocAuthorProp = returned.GetType();\n        returnVal = typeDocAuthorProp.InvokeMember(""Value"",\n                                   BindingFlags.Default |\n                                   BindingFlags.GetProperty,\n                                   null, returned,\n                                   new object[] { }).ToString();\n\n        return returnVal;\n    }\n\nSome exception handling is necessary to hand if the property doesnt exist when retrieved\n', '\nLate answer to this question, but I worked out a simpler method for adding custom DocumentProperties that might be of use to someone in the future.\nMy problem was that calling the Add() method with the System type supplied by System.String.GetType() triggered a COMException: Type mismatch. Referring to the link in the previous answers it\'s clear that this method expects an Office-specific type, so the code that ended up working for me was:\nvar custProps = (Office.DocumentProperties)this.CustomDocumentProperties;\ncustProps.Add( ""AProperty"", false, MsoDocProperties.msoPropertyTypeString, ""AStringProperty"" );\n\nBecause it\'s a CustomDocumentProperty Office will add the custom property without difficulty, but if you need to check for existence or validate the value when the CustomDocumentProperty might not exist you\'ll have to catch a System.ArgumentException.\nEDIT\nAs pointed out in Oliver Bock\'s comment, this is an Office 2007 and up only solution, as far as I know.\n']",https://stackoverflow.com/questions/1137763/accessing-excel-custom-document-properties-programmatically,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to access Microsoft Word existing instance using late binding,"
i am developing some code in c# where i will be interacting with Microsoft Word. I want to be able to have the option of re-using an existing instance or as an alternative creating a new instance.
Keeping in mind i want to do all of this using LATE BINDING... it is safe to say i have figured out how to get things working when creating a new instance.. i just call Activator.CreateInstance etc..
The problem i am having is how do i reuse an existing instance, for example, Word is already open and i want to use that instance.
Is there an Activator.UseExistingInstance? or something similar??
Thanks!
",11k,"
            5
        ","['\nYou\'re looking for Marshal.GetActiveObject.\nobject word;\ntry\n{\n    word = System.Runtime.InteropServices.Marshal.GetActiveObject(""Word.Application"");\n}\ncatch (COMException)\n{\n    Type type = Type.GetTypeFromProgID(""Word.Application"");\n    word = System.Activator.CreateInstance(type);\n}\n\n', '\nYou might want to have a look at the AccessibleObjectFromWindow api function defined in Oleacc.dll. Andrew Whitechapel has some articles on how to use it. Based on his articles I wrote an answer to a very similar question (about Excel, not Word), which you can find here:\n\nHow to use use late binding to get Excel instance?\n\nThere you will find an example how to connect to an already running Excel instance and then automating this instance using late binding. \nUpdate: \nHere is a short sample adapted to Word:\nusing System;\nusing System.Reflection;\nusing System.Runtime.InteropServices;\nusing System.Text;\n\nnamespace WordLateBindingSample\n{\n    [ComImport, InterfaceType(ComInterfaceType.InterfaceIsIUnknown), Guid(""00020400-0000-0000-C000-000000000046"")]\n    public interface IDispatch\n    {\n    }\n\n    class Program\n    {\n        [DllImport(""user32.dll"", SetLastError = true)]\n        static extern IntPtr FindWindow(string lpClassName, string lpWindowName);\n\n        [DllImport(""Oleacc.dll"")]\n        static extern int AccessibleObjectFromWindow(int hwnd, uint dwObjectID, byte[] riid, out IDispatch ptr);\n\n        public delegate bool EnumChildCallback(int hwnd, ref int lParam);\n\n        [DllImport(""User32.dll"")]\n        public static extern bool EnumChildWindows(int hWndParent, EnumChildCallback lpEnumFunc, ref int lParam);\n\n        [DllImport(""User32.dll"")]\n        public static extern int GetClassName(int hWnd, StringBuilder lpClassName, int nMaxCount);\n\n        public static bool EnumChildProc(int hwndChild, ref int lParam)\n        {\n            StringBuilder buf = new StringBuilder(128);\n            GetClassName(hwndChild, buf, 128);\n            if (buf.ToString() == ""_WwG"")\n            {\n                lParam = hwndChild;\n                return false;\n            }\n            return true;\n        }\n\n        static void Main(string[] args)\n        {\n            // Use the window class name (""OpusApp"") to retrieve a handle to Word\'s main window.\n            // Alternatively you can get the window handle via the process id:\n            // int hwnd = (int)Process.GetProcessById(wordPid).MainWindowHandle;\n            //\n            int hwnd = (int)FindWindow(""OpusApp"", null);\n\n            if (hwnd != 0)\n            {\n                int hwndChild = 0;\n\n                // Search the accessible child window (it has class name ""_WwG"") \n                // as described in http://msdn.microsoft.com/en-us/library/dd317978%28VS.85%29.aspx\n                //\n                EnumChildCallback cb = new EnumChildCallback(EnumChildProc);\n                EnumChildWindows(hwnd, cb, ref hwndChild);\n\n                if (hwndChild != 0)\n                {\n                    // We call AccessibleObjectFromWindow, passing the constant OBJID_NATIVEOM (defined in winuser.h) \n                    // and IID_IDispatch - we want an IDispatch pointer into the native object model.\n                    //\n                    const uint OBJID_NATIVEOM = 0xFFFFFFF0;\n                    Guid IID_IDispatch = new Guid(""{00020400-0000-0000-C000-000000000046}"");\n                    IDispatch ptr;\n\n                    int hr = AccessibleObjectFromWindow(hwndChild, OBJID_NATIVEOM, IID_IDispatch.ToByteArray(), out ptr);\n\n                    if (hr >= 0)\n                    {\n                        object wordApp = ptr.GetType().InvokeMember(""Application"", BindingFlags.GetProperty, null, ptr, null);\n\n                        object version = wordApp.GetType().InvokeMember(""Version"", BindingFlags.GetField | BindingFlags.InvokeMethod | BindingFlags.GetProperty, null, wordApp, null);\n                        Console.WriteLine(string.Format(""Word version is: {0}"", version));\n                    }\n                }\n            }\n        }\n    }\n}\n\n']",https://stackoverflow.com/questions/2203968/how-to-access-microsoft-word-existing-instance-using-late-binding,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selenium can't open a second page,"
I am using Selenium to open different pages of a site. Have tried multiple times but the browser does not open a second webpage after the initial GET call. Have tried on both Chrome and Safari. Here is my code:
driver = webdriver.Chrome()
driver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-2"")
driver.set_page_load_timeout(30)
driver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-3"")

Here is the error I get for the second call:

The info from Network logs is Error 504, but I have verified that it works perfectly when done on another window of the browser, without automation
",3k,"
            1
        ","['\noptions.add_experimental_option(\n    ""excludeSwitches"", [\'enable-automation\'])\n\noptions.add_argument(\n    ""user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"")\noptions.add_argument(""--remote-debugging-port=9222"")\n\ndriver = webdriver.Chrome(options=options)\ndriver.get(\n    ""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-2"")\ndriver.set_page_load_timeout(30)\ndriver.get(\n    ""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-3"")\n\nthe website is detecting automation use above code :)\nYou can also do this in single line\nJust add below argument:\noptions.add_argument(\'--disable-blink-features=AutomationControlled\')\n\ndisabling enable-automation , or disabling automation controller disables webdriver.navigator which few websites uses to detect automation scripts\n', '\nA bit of more information about your usecase would have helped to construct a more canonical answer. However I was able to access the Page 2 of justdial.com/Chennai/Hr-Consultancy-Services with a minimized code block as follows:\n\nCode Block:\nfrom selenium import webdriver\n\noptions = webdriver.ChromeOptions() \noptions.add_argument(""start-maximized"")\ndriver = webdriver.Chrome(options=options, executable_path=r\'C:\\WebDrivers\\chromedriver.exe\')\ndriver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-2"")\n\n\nBrowser Snapshot:\n\n\n\nBut while sending multiple get() one after another:\ndriver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-2"")\ndriver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-3"")\n\nIt seems ChromeDriver initiated Chrome Browser gets detected and the following error is shown:\nAn error occurred while processing your request.\nReference #97.e5732c31.1612205693.6fd2708\n\n\nSolution\nTo avoid the detection you can add the following option:\n--disable-blink-features=AutomationControlled\n\n\nExample\nfrom selenium import webdriver\n\noptions = webdriver.ChromeOptions() \noptions.add_argument(""start-maximized"")\noptions.add_argument(\'--disable-blink-features=AutomationControlled\')\ndriver = webdriver.Chrome(options=options, executable_path=r\'C:\\WebDrivers\\chromedriver.exe\')\ndriver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-2"")\ndriver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-3"")\n\n']",https://stackoverflow.com/questions/65994908/selenium-cant-open-a-second-page,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Puppeteer wait for all images to load then take screenshot,"
I am using Puppeteer to try to take a screenshot of a website after all images have loaded but can't get it to work.
Here is the code I've got so far, I am using https://www.digg.com as the example website:
const puppeteer = require('puppeteer');

(async () => {
    const browser = await puppeteer.launch();
    const page = await browser.newPage();
    await page.goto('https://www.digg.com/');

    await page.setViewport({width: 1640, height: 800});

    await page.evaluate(() => {
        return Promise.resolve(window.scrollTo(0,document.body.scrollHeight));
    });

    await page.waitFor(1000);

    await page.evaluate(() => {
        var images = document.querySelectorAll('img');

        function preLoad() {

            var promises = [];

            function loadImage(img) {
                return new Promise(function(resolve,reject) {
                    if (img.complete) {
                        resolve(img)
                    }
                    img.onload = function() {
                        resolve(img);
                    };
                    img.onerror = function(e) {
                        resolve(img);
                    };
                })
            }

            for (var i = 0; i < images.length; i++)
            {
                promises.push(loadImage(images[i]));
            }

            return Promise.all(promises);
        }

        return preLoad();
    });

    await page.screenshot({path: 'digg.png', fullPage: true});

    browser.close();
})();

",56k,"
            58
        ","['\nThere is a built-in option for that:\nawait page.goto(\'https://www.digg.com/\', {""waitUntil"" : ""networkidle0""});\n\n\nnetworkidle0 - consider navigation to be finished when there are no more than 0 network connections for at least 500 ms\n\n\nnetworkidle2 - consider navigation to be finished when there are no more than 2 network connections for at least 500 ms.\n\nOf course it won\'t work if you\'re working with endless-scrolling-single-page-applications like Twitter.\nPuppeteer GitHub issue #1552 provides explanation for the motivation behind networkidle2.\n', '\nAnother option, actually evaluate to get callback when all images were loaded\nThis option will also work with setContent that doesn\'t support the wait networkidle0 option\nawait page.evaluate(async () => {\n  const selectors = Array.from(document.querySelectorAll(""img""));\n  await Promise.all(selectors.map(img => {\n    if (img.complete) return;\n    return new Promise((resolve, reject) => {\n      img.addEventListener(\'load\', resolve);\n      img.addEventListener(\'error\', reject);\n    });\n  }));\n})\n\n', ""\nWait for Lazy Loading Images\nYou may want to consider scrolling down first using a method such as Element.scrollIntoView() to account for lazy loading images:\nawait page.goto('https://www.digg.com/', {\n  waitUntil: 'networkidle0', // Wait for all non-lazy loaded images to load\n});\n\nawait page.evaluate(async () => {\n  // Scroll down to bottom of page to activate lazy loading images\n  document.body.scrollIntoView(false);\n\n  // Wait for all remaining lazy loading images to load\n  await Promise.all(Array.from(document.getElementsByTagName('img'), image => {\n    if (image.complete) {\n      return;\n    }\n\n    return new Promise((resolve, reject) => {\n      image.addEventListener('load', resolve);\n      image.addEventListener('error', reject);\n    });\n  }));\n});\n\n"", ""\nI'm facing the exact same issue.\nI have a feeling the solution will involve using:\nawait page.setRequestInterceptionEnabled(true);\n\npage.on('request', interceptedRequest => {\n    //some code here that adds this request to ...\n    //a list and checks whether all list items have ...\n    //been successfully completed!\n});\n\nhttps://github.com/GoogleChrome/puppeteer/blob/master/docs/api.md#pagesetrequestinterceptionenabledvalue\n"", ""\nI found a solution which is applicable to multiple sites using the page.setViewPort(...) method as given below:\nconst puppeteer = require('puppeteer');\n\nasync(() => {\n    const browser = await puppeteer.launch({\n        headless: true, // Set to false while development\n        defaultViewport: null,\n        args: [\n            '--no-sandbox',\n            '--start-maximized', // Start in maximized state\n        ],\n    });\n\n    const page = await = browser.newPage();\n    await page.goto('https://www.digg.com/', {\n        waitUntil: 'networkidle0', timeout: 0\n    });\n\n    // Get scroll width and height of the rendered page and set viewport\n    const bodyWidth = await page.evaluate(() => document.body.scrollWidth);\n    const bodyHeight = await page.evaluate(() => document.body.scrollHeight);\n    await page.setViewport({ width: bodyWidth, height: bodyHeight });\n\n    await page.waitFor(1000);\n    await page.screenshot({path: 'digg-example.png' });\n})();\n\n""]",https://stackoverflow.com/questions/46160929/puppeteer-wait-for-all-images-to-load-then-take-screenshot,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Android 鈥?multiple custom versions of the same app,"
Whats the best way to deploy several customized versions of a Android application? 
Currently I have a script to exchange the resource folder for getting a customized version of my app. It works great, but all custom versions still have the same package name in the AndroidManifest.xml. Therefore it is not possible to install two customized versions of the app at the same time.
This is one solution for this problem, but that has to be done by hand
Can you think of a more easy solution, or how this could be built into a skript?
(btw: it is not for a porn/spam/whatever app, not even a paid one)
",28k,"
            47
        ","['\nPerhaps the built-in Android ""library"" concept was not fully baked at the time of the original post, but it may be the preferred method as of 2011.  Follow these steps for an ant build:\nStarting from a working app (let\'s call it directory ""myOrigApp"", package com.foo.myapp), just add this line to ""default.properties"" to make it a library:\nandroid.library=true\n\nNow create a new app in a sibling directory in any way you prefer (let\'s call it directory ""sibling"", package com.foo.myVariant).  Using Intellij Idea, for example, create a project \'from scratch\' with directory \'sibling\' and it will create all the files/directories you would normally need.\nIn that new, sibling directory edit ""default.properties"" to add the dependency:\nandroid.library.reference.1=../myOrigApp\n\nCopy over the Manifest from the original dir:\ncd sibling\ncp ../myOrigApp/AndroidManifest.xml  ../myOrigApp/local.properties ../myOrigApp/build.properties  .\n\nEdit that copied Manifest file to change its package name to your new variant, ""com.foo.myVarient""; that\'s the only change.\nIf you just run the ant build scripts, you may be done. (I had to just set up signing keys.)\nIf you want to set up an IDE like Idea to have the library project as a dependent of the variant project, follow these steps to add a library project to the variant project (assumes you already have a project set up for both):\n\nOpen the original project, bring up Project Settings, select your Facet and check ""Is Library Project"" and save.\nOpen the variant project, bring up Project Settings, select Modules\nAdd a module\nSelect 鈥淚mport existing module鈥漒nBrowse to the Original directory (myOrigApp) and select its .iml file (IntelliJ project source file)\nClick ""Finish."" (The library project is added as a module within the variant project.)\nIn the modules list click over the Variant project to select it.\nOn the right hand side select the ""Dependencies"" tab.\nClick ""Add鈥?\nChoose """"Module dependency鈥?"" (A list should appear that includes the name of the module/library you previously added to the project--perhaps the only entry in the list). \nSelect the library project you added and press OK. (It will be added to the list of dependencies of your project.)\nPress OK to finish configuring the project. (You should see 2 modules", with the library\'s resources and classes available and recognized in the Variant project.)\n\n'," '\nWhat I did for something similar to this is to just use an antlib task and then go through all java and xml files to replace my old package string to the new package string. It didn\'t matter if the files were not in the correct src paths according to the package. Just doing a regex replace for all the files was enough for me to get this working...\nFor example to replace it in all your java files under the src directory:\n <replaceregexp flags=""""g"""" byline=""""false"""">\n    <regexp pattern=""""old.package.string"""" /> \n    <substitution expression=""""new.package.string"""" />\n    <fileset dir=""""src"""" includes=""""**/*.java"""" /> \n </replaceregexp>\n\n'"," """"\nYou definitely want to use Gradle flavors that comes natively", encouraged even, on Android Studio.\nIt seems to explain all the basics really well. I just finished converting to Gradle today, and it works great. Custom app icons, names, and strings, etc.\nAs the website explains, part of the purpose behind this design was to make it more dynamic and more easily allow multiple APKs to be created with essentially the same code, which sounds similar what you're doing.\nI probably didn't explain it the best," but that website does a pretty good job.\n""""", '\nThe linked-to solution does not have to be done by hand. Bear in mind that the package attribute in the <manifest> element does not have to be where the code resides, so long as you spell out the fully-qualified classes elsewhere in the manifest (e.g.," activity android:name=""""com.commonsware.android.MyActivity"""" rather than activity android:name="""".MyActivity""""). Script your manifest change and use Ant to build a new APK. AFAIK", that should work.\n', '\nSupport Multiple Partners \nPrepare config.xml\n\n\n     Build project for different partner \n<!--partner.dir, pkg.name, ver.code," ver.name are input from command line when execute \'ant\' -->\n\n<!-- set global properties for this build -->\n<property name=""""build.bin"""" location=""""bin""""/>\n<property name=""""build.gen"""" location=""""gen""""/>\n<property name=""""src"""" location=""""src""""/>\n<property name=""""res"""" location=""""res""""/>\n\n<target name=""""preparefiles"""" description=""""Prepare files for different partner"""" >\n    <delete dir=""""${build.bin}"""" />\n    <delete dir=""""${build.gen}"""" />\n\n    <copy todir=""""${res}"""" overwrite=""""true"""" />\n        <fileset dir=""""${partner.dir}/res"""" /> \n    </copy>\n\n    <!-- change the import in all Java source files -->\n    <replaceregexp file=""""AndroidManifest.xml""""\n        match=\'android.versionCode=""""(.*)""""\'\n        replace=\'android.versionCode=""""${ver.code}""""\'\n        byline=""""false"""">\n\n    <replaceregexp file=""""AndroidManifest.xml""""\n        match=\'android.versionName=""""(.*)""""\'\n        replace=\'android.versionName=""""${ver.name}""""\'\n        byline=""""false"""">\n\n    <replaceregexp file=""""AndroidManifest.xml""""\n        match=\'package=""""(.*)""""\'\n        replace=\'package=""""${pkg.name}""""\'\n        byline=""""false"""">\n\n    <!-- change the package name in AndroidManifest -->\n    <replaceregexp flags=""""g"""" byline=""""false"""">\n        <regexp pattern=""""import(.*)com.myproject.com.R;"""" /> \n        <substitution expression=""""import com.${pkg.name}.R;"""" />\n        <fileset dir=""""${src}"""" includes=""""**/*.java"""" /> \n    </replaceregexp>\n\n    <replaceregexp flags=""""g"""" byline=""""false"""">\n        <regexp pattern=""""(package com.myproject.com;)"""" /> \n        <substitution expression=""""\\1&#10;import com.${pkg.name}.R;"""" />\n        <fileset dir=""""${src}"""" includes=""""**/*.java"""" /> \n    </replaceregexp>\n</target>\n\n\nPrepare Files\n$ ant -f config.xml -Dpartner.dir=""""xxx"""" -Dpkg.name=""""xxx"""" -Dver.code=""""xxx"""" -Dver.name=""""xxx"""" preparefiles\nCreate build.xml\nBuild\n$ ant debug\nor\n$ ant release\n'"," """"\nI'm using the maven-android-plugin to achieve this. Specify one AndroidManifest.xml for the generated-sources goal and another AndroidManifest.xml for the final apk goal. That way the source code project retains the actual source code package name during generation of the R class and the build phase"," while the market adapted manifest package name is in the second AndroidManifest.xml which is included in the final apk file.\n""""", '\nI wound up with a script that patches the sources; patching the source sounds risky, but in presence of version control the risk is acceptable.\nSo I made one version, committed the source, made the other version, committed the source, and looking at diffs wrote a patching script in Python.\nI am not sure if it is the best solution. (And the code misses some os.path.joins)\nThe heart of the script is the following function:\n# In the file \'fname\',"\n# find the text matching """"before oldtext after"""" (all occurrences) and\n# replace \'oldtext\' with \'newtext\' (all occurrences).\n# If \'mandatory\' is true", raise an exception if no replacements were made.\ndef fileReplace(fname,before,newtext,after,mandatory=True):\n    with open(fname," \'r+\') as f:\n    read_data = f.read()\n    pattern = r""""(""""+re.escape(before)+r"""")\\w+(""""+re.escape(after)+r"""")""""\n    replacement = r""""\\1""""+newtext+r""""\\2""""\n    new_data",replacements_made = re.subn(pattern,replacement,read_data,"flags=re.MULTILINE)\n    if replacements_made and really:\n        f.seek(0)\n        f.truncate()\n        f.write(new_data)\n        if verbose:\n            print """"patching """"",fname," (""""",replacements_made," occurrence"""""," """"s"""" if 1!=replacements_made else """"""""",")""""\n    elif replacements_made:\n        print fname",":""""\n        print new_data\n    elif mandatory:\n        raise Exception(""""cannot patch the file: """"+fname)\n\nAnd you may find the following one of use:\n# Change the application resource package name everywhere in the src/ tree.\n# Yes", this changes the java files. We hope that if something goes wrong,\n# the version control will save us.\ndef patchResourcePackageNameInSrc(pname):\n    for root, dirs, files in os.walk(\'src\'):\n    if \'.svn\' in dirs:\n        dirs.remove(\'.svn\')\n    for fname in files:\n        fileReplace(os.path.join(root,fname),"import com.xyz.""""",pname,".R;""""",mandatory=False)\n\nThere is also a function that copies assets from x-assets-cfgname to assets (earlier it turned out that for me it is more convenient to have a subdirectory in assets).\ndef copyAssets(vname,"force=False):\n    assets_source = """"x-assets-""""+vname+""""/xxx""""\n    assets_target = """"assets/xxx""""\n    if not os.path.exists(assets_source):\n        raise Exception(""""Invalid variant name: """"+vname+"""" (the assets directory """"+assets_source+"""" does not exist)"""")\n    if os.path.exists(assets_target+""""/.svn""""):\n        raise Exception(""""The assets directory must not be under version control! """"+assets_target+""""/.svn exists!"""")\n    if os.path.exists(assets_target):\n        shutil.rmtree(assets_target)\n    shutil.copytree(assets_source", assets_target, ignore=shutil.ignore_patterns(\'.svn\'))\n\nWell, you get the idea. Now you can write your own script.\n'," """"\nI think the best way is to create a new project and copy the stuff. steps",\n- create new android project without a class\n- create package (package name should be corresponding to the one in the manifest file)," or just copy the package name in the 'gen' folder\n- copy the java files\n- copy the drawable folders\n- copy the layout files\n- copy any other file(s) used in ur project\n- copy manifest file's data\nthis has been simpler for me for the task\n""""]""",https://stackoverflow.com/questions/1222302/android-multiple-custom-versions-of-the-same-app,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upload file to SFTP using PowerShell,"
We were asked to set up an automated upload from one of our servers to an SFTP site. There will be a file that is exported from a database to a filer every Monday morning and they want the file to be uploaded to SFTP on Tuesday. The current authentication method we are using is username and password (I believe there was an option to have key file as well but username/password option was chosen).
The way I am envisioning this is to have a script sitting on a server that will be triggered by Windows Task scheduler to run at a specific time (Tuesday) that will grab the file in question upload it to the SFTP and then move it to a different location for backup purposes.
For example:

Local Directory: C:\FileDump
SFTP Directory: /Outbox/
Backup Directory: C:\Backup

I tried few things at this point WinSCP being one of them as well as SFTP PowerShell Snap-In but nothing has worked for me so far. 
This will be running on Windows Server 2012R2.
When I run Get-Host my console host version is 4.0.
Thanks.
",170k,"
            44
        ","['\nYou didn\'t tell us what particular problem do you have with the WinSCP, so I can really only repeat what\'s in WinSCP documentation.\n\nDownload WinSCP .NET assembly.\nThe latest package as of now is WinSCP-5.21.7-Automation.zip;\n\nExtract the .zip archive along your script;\n\nUse a code like this (based on the official PowerShell upload example):\n# Load WinSCP .NET assembly\nAdd-Type -Path ""WinSCPnet.dll""\n\n# Setup session options\n$sessionOptions = New-Object WinSCP.SessionOptions -Property @{\n    Protocol = [WinSCP.Protocol]::Sftp\n    HostName = ""example.com""\n    UserName = ""user""\n    Password = ""mypassword""\n    SshHostKeyFingerprint = ""ssh-rsa 2048 xxxxxxxxxxx...=""\n}\n\n$session = New-Object WinSCP.Session\n\ntry\n{\n    # Connect\n    $session.Open($sessionOptions)\n\n    # Upload\n    $session.PutFiles(""C:\\FileDump\\export.txt"", ""/Outbox/"").Check()\n}\nfinally\n{\n    # Disconnect, clean up\n    $session.Dispose()\n}\n\n\n\n\nYou can have WinSCP generate the PowerShell script for the upload for you:\n\nLogin to your server with WinSCP GUI;\nNavigate to the target directory in the remote file panel;\nSelect the file for upload in the local file panel;\nInvoke the Upload command;\nOn the Transfer options dialog, go to Transfer Settings > Generate Code;\nOn the Generate transfer code dialog, select the .NET assembly code tab;\nChoose PowerShell language.\n\nYou will get a code like above with all session and transfer settings filled in.\n\n(I\'m the author of WinSCP)\n', '\nThere isn\'t currently a built-in PowerShell method for doing the SFTP part. You\'ll have to use something like psftp.exe or a PowerShell module like Posh-SSH.\nHere is an example using Posh-SSH:\n# Set the credentials\n$Password = ConvertTo-SecureString \'Password1\' -AsPlainText -Force\n$Credential = New-Object System.Management.Automation.PSCredential (\'root\', $Password)\n\n# Set local file path, SFTP path, and the backup location path which I assume is an SMB path\n$FilePath = ""C:\\FileDump\\test.txt""\n$SftpPath = \'/Outbox\'\n$SmbPath = \'\\\\filer01\\Backup\'\n\n# Set the IP of the SFTP server\n$SftpIp = \'10.209.26.105\'\n\n# Load the Posh-SSH module\nImport-Module C:\\Temp\\Posh-SSH\n\n# Establish the SFTP connection\n$ThisSession = New-SFTPSession -ComputerName $SftpIp -Credential $Credential\n\n# Upload the file to the SFTP path\nSet-SFTPFile -SessionId ($ThisSession).SessionId -LocalFile $FilePath -RemotePath $SftpPath\n\n#Disconnect all SFTP Sessions\nGet-SFTPSession | % { Remove-SFTPSession -SessionId ($_.SessionId) }\n\n# Copy the file to the SMB location\nCopy-Item -Path $FilePath -Destination $SmbPath\n\nSome additional notes:\n\nYou\'ll have to download the Posh-SSH module which you can install to your user module directory (e.g. C:\\Users\\jon_dechiro\\Documents\\WindowsPowerShell\\Modules) and just load using the name or put it anywhere and load it like I have in the code above.\nIf having the credentials in the script is not acceptable you\'ll have to use a credential file. If you need help with that I can update with some details or point you to some links.\nChange the paths, IPs, etc. as needed.\n\nThat should give you a decent starting point.\n', ""\nI am able to sftp using PowerShell as below:\nPS C:\\Users\\user\\Desktop> sftp user@aa.bb.cc.dd                                                     \nuser@aa.bb.cc.dd's password:\nConnected to user@aa.bb.cc.dd.\nsftp> ls\ntestFolder\nsftp> cd testFolder\nsftp> ls\ntaj_mahal.jpeg\nsftp> put taj_mahal_1.jpeg\nUploading taj_mahal_1.jpeg to /home/user/testFolder/taj_mahal_1.jpeg\ntaj_mahal_1.jpeg                                                                      100%   11KB  35.6KB/s   00:00\nsftp> ls\ntaj_mahal.jpeg      taj_mahal_1.jpeg\nsftp>\n\nI do not have installed Posh-SSH or anything like that. I am using Windows 10 Pro PowerShell. No additional modules installed.\n"", ""\nUsing PuTTY's pscp.exe (which I have in an $env:path directory):\npscp -sftp -pw passwd c:\\filedump\\* user@host:/Outbox/\nmv c:\\filedump\\* c:\\backup\\*\n\n"", '\n$FilePath = ""C:\\Backup\\xxx.zip""\n$SftpPath = \'/Cloud_Deployment/Backup\'\n$SftpIp = \'mercury.xxx.xx.uk\' #Or IP\n$Password = \'password\'\n$userroot = \'username\'\n$Password = ConvertTo-SecureString $Password -AsPlainText -Force\n$Credential = New-Object System.Management.Automation.PSCredential ($userroot, $Password)\nInstall-Module -Name Posh-SSH  #rus as Admin \n$SFTPSession = New-SFTPSession -ComputerName $SftpIp -Credential $Credential\n#Download file\n#Get-SFTPItem -SessionId $SFTPSession.SessionId -Path $SftpPath/test.txt -Destination c:\\temp\n#Upload file\nSet-SFTPItem -SessionId $SFTPSession.SessionId -Path $FilePath -Destination $SftpPath\n#Disconnect all SFTP Sessions\nRemove-SFTPSession -SFTPSession $SFTPSession\n#or \nGet-SFTPSession | % { Remove-SFTPSession -SessionId ($_.SessionId) }\nRef : powershell-sftp\nIf any how you face error ""PackageManagement\\Install-Package : No match was found for the specified search criteria and module name \'Posh-SSH\'""\nThen please visit Here\n', '\nWell, while using powershell 7, we can simply upload files using sftp with following command\necho ""put localpath/file.txt destinationpath/file.txt"" | sftp username@server\n\nmake sure to add these double quotes.\n']",https://stackoverflow.com/questions/38732025/upload-file-to-sftp-using-powershell,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to programmatically fill input elements built with React?,"
I'm tasked with crawling website built with React. I'm trying to fill in input fields and submitting the form using javascript injects to the page (either selenium or webview in mobile). This works like a charm on every other site + technology but React seems to be a real pain.
so here is a sample code 
var email = document.getElementById( 'email' );
email.value = 'example@mail.com';

I the value changes on the DOM input element, but the React does not trigger the change event.
I've been trying plethora of different ways to get the React to update the state.
var event = new Event('change', { bubbles: true });
email.dispatchEvent( event );

no avail
var event = new Event('input', { bubbles: true });
email.dispatchEvent( event );

not working
email.onChange( event );

not working
I cannot believe interacting with React has been made so difficult. I would greatly appreciate any help. 
Thank you
",24k,"
            37
        ","[""\nThis accepted solution appears not to work in React > 15.6 (including React 16) as a result of changes to de-dupe input and change events.\nYou can see the React discussion here: https://github.com/facebook/react/issues/10135\nAnd the suggested workaround here:\nhttps://github.com/facebook/react/issues/10135#issuecomment-314441175\nReproduced here for convenience:\nInstead of\ninput.value = 'foo';\ninput.dispatchEvent(new Event('input', {bubbles: true}));\n\nYou would use\nfunction setNativeValue(element, value) {\n  const valueSetter = Object.getOwnPropertyDescriptor(element, 'value').set;\n  const prototype = Object.getPrototypeOf(element);\n  const prototypeValueSetter = Object.getOwnPropertyDescriptor(prototype, 'value').set;\n\n  if (valueSetter && valueSetter !== prototypeValueSetter) {\n    prototypeValueSetter.call(element, value);\n  } else {\n    valueSetter.call(element, value);\n  }\n}\n\nand then\nsetNativeValue(input, 'foo');\ninput.dispatchEvent(new Event('input', { bubbles: true }));\n\n"", '\nReact is listening for the input event of text fields.\nYou can change the value and manually trigger an input event, and react\'s onChange handler will trigger:\n\n\nclass Form extends React.Component {\r\n  constructor(props) {\r\n    super(props)\r\n    this.state = {value: \'\'}\r\n  }\r\n  \r\n  handleChange(e) {\r\n    this.setState({value: e.target.value})\r\n    console.log(\'State updated to \', e.target.value);\r\n  }\r\n  \r\n  render() {\r\n    return (\r\n      <div>\r\n        <input\r\n          id=\'textfield\'\r\n          value={this.state.value}\r\n          onChange={this.handleChange.bind(this)}\r\n        />\r\n        <p>{this.state.value}</p>\r\n      </div>      \r\n    )\r\n  }\r\n}\r\n\r\nReactDOM.render(\r\n  <Form />,\r\n  document.getElementById(\'app\')\r\n)\r\n\r\ndocument.getElementById(\'textfield\').value = \'foo\'\r\nconst event = new Event(\'input\', { bubbles: true })\r\ndocument.getElementById(\'textfield\').dispatchEvent(event)\n<script src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react.min.js""></script>\r\n<script src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react-dom.min.js""></script>\r\n\r\n<div id=\'app\'></div>\n\n\n', ""\nHere is the cleanest possible solution for inputs, selects, checkboxes, etc. (works not only for react inputs)\n/**\n * See [Modify React Component's State using jQuery/Plain Javascript from Chrome Extension](https://stackoverflow.com/q/41166005)\n * See https://github.com/facebook/react/issues/11488#issuecomment-347775628\n * See [How to programmatically fill input elements built with React?](https://stackoverflow.com/q/40894637)\n * See https://github.com/facebook/react/issues/10135#issuecomment-401496776\n *\n * @param {HTMLInputElement | HTMLSelectElement} el\n * @param {string} value\n */\nfunction setNativeValue(el, value) {\n  const previousValue = el.value;\n\n  if (el.type === 'checkbox' || el.type === 'radio') {\n    if ((!!value && !el.checked) || (!!!value && el.checked)) {\n      el.click();\n    }\n  } else el.value = value;\n\n  const tracker = el._valueTracker;\n  if (tracker) {\n    tracker.setValue(previousValue);\n  }\n\n  // 'change' instead of 'input', see https://github.com/facebook/react/issues/11488#issuecomment-381590324\n  el.dispatchEvent(new Event('change', { bubbles: true }));\n}\n\nUsage:\nsetNativeValue(document.getElementById('name'), 'Your name');\ndocument.getElementById('radio').click(); // or setNativeValue(document.getElementById('radio'), true)\ndocument.getElementById('checkbox').click(); // or setNativeValue(document.getElementById('checkbox'), true)\n\n"", '\nI noticed the input element had some property with a name along the lines of __reactEventHandlers$..., which had some functions including an onChange.\nThis worked for finding that function and triggering it\nlet getReactEventHandlers = (element) => {\n    // the name of the attribute changes, so we find it using a match.\n    // It\'s something like `element.__reactEventHandlers$...`\n    let reactEventHandlersName = Object.keys(element)\n       .filter(key => key.match(\'reactEventHandler\'));\n    return element[reactEventHandlersName];\n}\n\nlet triggerReactOnChangeEvent = (element) => {\n    let ev = new Event(\'change\');\n    // workaround to set the event target, because `ev.target = element` doesn\'t work\n    Object.defineProperty(ev, \'target\', {writable: false, value: element});\n    getReactEventHandlers(element).onChange(ev);\n}\n\ninput.value = ""some value"";\ntriggerReactOnChangeEvent(input);\n\n', '\nWithout element ids:\nexport default function SomeComponent() {\n    const inputRef = useRef();\n    const [address, setAddress] = useState("""");\n    const onAddressChange = (e) => {\n        setAddress(e.target.value);\n    }\n    const setAddressProgrammatically = (newValue) => {\n        const event = new Event(\'change\', { bubbles: true });\n        const input = inputRef.current;\n        if (input) {\n            setAddress(newValue);\n            input.value = newValue;\n            input.dispatchEvent(event);\n        }\n    }\n    return (\n        ...\n        <input ref={inputRef} type=""text"" value={address} onChange={onAddressChange}/>\n        ...\n    );\n}\n\n', '\nReact 17 works with fibers:\nfunction findReact(dom) {\n    let key = Object.keys(dom).find(key => key.startsWith(""__reactFiber$""));\n    let internalInstance = dom[key];\n    if (internalInstance == null) return ""internalInstance is null: "" + key;\n\n    if (internalInstance.return) { // react 16+\n        return internalInstance._debugOwner\n            ? internalInstance._debugOwner.stateNode\n           : internalInstance.return.stateNode;\n    } else { // react <16\n        return internalInstance._currentElement._owner._instance;\n   }\n}\n\nthen:\nfindReact(domElement).onChangeWrapper(""New value"");\n\nthe domElement in this is the tr with the data-param-name of the field you are trying to change:\nvar domElement = ?.querySelectorAll(\'tr[data-param-name=""<my field name>""]\')\n\n']",https://stackoverflow.com/questions/40894637/how-to-programmatically-fill-input-elements-built-with-react,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using conditional statements inside 'expect',"
I need to automate logging into a TELNET session using expect, but I need to take care of multiple passwords for the same username.
Here's the flow I need to create:

Open TELNET session to an IP
Send user-name
Send password
Wrong password? Send the same user-name again, then a different password
Should have successfully logged-in at this point...

For what it's worth, here's what I've got so far:
#!/usr/bin/expect
spawn telnet 192.168.40.100
expect ""login:""
send ""spongebob\r""
expect ""password:""
send ""squarepants\r""
expect ""login incorrect"" {
  expect ""login:""
  send ""spongebob\r""
  expect ""password:""
  send ""rhombuspants\r""
}
expect ""prompt\>"" {
  send_user ""success!\r""
}
send ""blah...blah...blah\r""

Needless to say this doesn't work, and nor does it look very pretty. From my adventures with Google expect seems to be something of a dark-art. Thanks in advance to anyone for assistance in the matter!
",77k,"
            33
        ","['\nHave to recomment the Exploring Expect book for all expect programmers -- invaluable.\nI\'ve rewritten your code: (untested)\nproc login {user pass} {\n    expect ""login:""\n    send ""$user\\r""\n    expect ""password:""\n    send ""$pass\\r""\n}\n\nset username spongebob \nset passwords {squarepants rhombuspants}\nset index 0\n\nspawn telnet 192.168.40.100\nlogin $username [lindex $passwords $index]\nexpect {\n    ""login incorrect"" {\n        send_user ""failed with $username:[lindex $passwords $index]\\n""\n        incr index\n        if {$index == [llength $passwords]} {\n            error ""ran out of possible passwords""\n        }\n        login $username [lindex $passwords $index]\n        exp_continue\n    }\n    ""prompt>"" \n}\nsend_user ""success!\\n""\n# ...\n\nexp_continue loops back to the beginning of the expect block -- it\'s like a ""redo"" statement.\nNote that send_user ends with \\n not \\r\nYou don\'t have to escape the > character in your prompt: it\'s not special for Tcl.\n', '\nWith a bit of bashing I found a solution. Turns out that expect uses a TCL syntax that I\'m not at all familiar with:\n#!/usr/bin/expect\nset pass(0) ""squarepants""\nset pass(1) ""rhombuspants""\nset pass(2) ""trapezoidpants""\nset count 0\nset prompt ""> ""\nspawn telnet 192.168.40.100\nexpect {\n  ""$prompt"" {\n    send_user ""successfully logged in!\\r""\n  }\n  ""password:"" {\n    send ""$pass($count)\\r""\n    exp_continue\n  }\n  ""login incorrect"" {\n    incr count\n    exp_continue\n  }\n  ""username:"" {\n    send ""spongebob\\r""\n    exp_continue\n  }\n}\nsend ""command1\\r""\nexpect ""$prompt""\nsend ""command2\\r""\nexpect ""$prompt""\nsend ""exit\\r""\nexpect eof\nexit\n\nHopefully this will be useful to others.\n', ""\nIf you know the user ids and passwords, then you ought also to know which userid/password pairs are aligned with which systems.  I think you'd be better off maintaining a map of which userid/password pair goes with which system then extracting that information and simply use the correct one.\nSo -- since you obviously don't like my advice, then I suggest you look at the wikipedia page and implement a procedure that returns 0 if successful and 1 if the expectation times out.  That will allow you to detect when the password supplied failed -- the prompt expectation times out -- and retry.  If this is helpful, you can remove your downvote now that I've edited it.\nIn retrospect, you'd probably want to do this in conjunction with the map anyway since you'd want to detect a failed login if the password was changed.\n""]",https://stackoverflow.com/questions/1538444/using-conditional-statements-inside-expect,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I use persisted cookies from a file using phantomjs,"
I have some authentication requried to hit a particular url. In browser I need to login only once, as for other related urls which can use the session id from the cookie need not required to go to the login page. 
Similarly, can I use the cookie generated in the cookie file using --cookies-file=cookies.txt in the commandline in phantomjs to open other page which requires the same cookie detail.
Please suggest.
",23k,"
            25
        ","['\nPhantom JS and cookies\n--cookies-file=cookies.txt will only store non-session cookies in the cookie jar. Login and authentication is more commonly based on session cookies.\nWhat about session cookies?\nTo save these is quite simple, but you should consider that they will likely expire quickly.\nYou need to write your program logic to consider this. For example\n\nLoad cookies from the cookiejar\nHit a URL to check if the user is logged in\nIf not logged in\nLog in, Save cookies to cookiejar\ncontinue with processing\n\nExample\nvar fs = require(\'fs\');\nvar CookieJar = ""cookiejar.json"";\nvar pageResponses = {};\npage.onResourceReceived = function(response) {\n    pageResponses[response.url] = response.status;\n    fs.write(CookieJar, JSON.stringify(phantom.cookies), ""w"");\n};\nif(fs.isFile(CookieJar))\n    Array.prototype.forEach.call(JSON.parse(fs.read(CookieJar)), function(x){\n        phantom.addCookie(x);\n    });\n\npage.open(LoginCheckURL, function(status){\n // this assumes that when you are not logged in, the server replies with a 303\n if(pageResponses[LoginCheckURL] == 303)\n {  \n    //attempt login\n    //assuming a resourceRequested event is fired the cookies will be written to the jar and on your next load of the script they will be found and used\n }\n\n\n});\n\n', '\nThe file created by the option --cookies-file=cookies.txt is serialized from CookieJar: there are extra characters and it\'s sometimes difficult to parse.\nIt may looks like:\n[General]\ncookies=""@Variant(\\0\\0\\0\\x7f\\0\\0\\0\\x16QList<QNetworkCookie>\\0\\0\\0\\0\\x1\\0\\0\\0\\v\\0\\0\\0{__cfduid=da7fda1ef6dd8b38450c6ad5632...\n\nI used in the past phantom.cookies. This array will be pre-populated by any existing Cookie data stored in the cookie file specified in the PhantomJS startup config/command-line options, if any. But you can also add dynamic cookie by using phantom.addCookie.\nA basic example is \nphantom.addCookie({\n    \'name\':     \'Valid-Cookie-Name\',   /* required property */\n    \'value\':    \'Valid-Cookie-Value\',  /* required property */\n    \'domain\':   \'localhost\',           /* required property */\n    \'path\':     \'/foo\',\n    \'httponly\': true,\n    \'secure\':   false,\n    \'expires\':  (new Date()).getTime() + (1000 * 60 * 60)   /* <-- expires in 1 hour */\n});\n\nWith these methods, it\'s not so difficult to implement your own cookie management logic.\n']",https://stackoverflow.com/questions/18739354/how-can-i-use-persisted-cookies-from-a-file-using-phantomjs,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c# and excel automation - ending the running instance,"
I'm attempting Excel automation through C#. I have followed all the instructions from Microsoft on how to go about this, but I'm still struggling to discard the final reference(s) to Excel for it to close and to enable the GC to collect it.
A code sample follows. When I comment out the code block that contains lines similar to:
Sheet.Cells[iRowCount, 1] = data[""fullname""].ToString();

then the file saves and Excel quits. Otherwise the file saves but Excel is left running as a process. The next time this code runs it creates a new instance and they eventually build up.
Any help is appreciated. Thanks.
This is the barebones of my code:
        Excel.Application xl = null;
        Excel._Workbook wBook = null;
        Excel._Worksheet wSheet = null;
        Excel.Range range = null;

        object m_objOpt = System.Reflection.Missing.Value;

        try
        {
            // open the template
            xl = new Excel.Application();
            wBook = (Excel._Workbook)xl.Workbooks.Open(excelTemplatePath + _report.ExcelTemplate, false, false, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt);
            wSheet = (Excel._Worksheet)wBook.ActiveSheet;

            int iRowCount = 2;

            // enumerate and drop the values straight into the Excel file
            while (data.Read())
            {

                wSheet.Cells[iRowCount, 1] = data[""fullname""].ToString();
                wSheet.Cells[iRowCount, 2] = data[""brand""].ToString();
                wSheet.Cells[iRowCount, 3] = data[""agency""].ToString();
                wSheet.Cells[iRowCount, 4] = data[""advertiser""].ToString();
                wSheet.Cells[iRowCount, 5] = data[""product""].ToString();
                wSheet.Cells[iRowCount, 6] = data[""comment""].ToString();
                wSheet.Cells[iRowCount, 7] = data[""brief""].ToString();
                wSheet.Cells[iRowCount, 8] = data[""responseDate""].ToString();
                wSheet.Cells[iRowCount, 9] = data[""share""].ToString();
                wSheet.Cells[iRowCount, 10] = data[""status""].ToString();
                wSheet.Cells[iRowCount, 11] = data[""startDate""].ToString();
                wSheet.Cells[iRowCount, 12] = data[""value""].ToString();

                iRowCount++;
            }

            DirectoryInfo saveTo = Directory.CreateDirectory(excelTemplatePath + _report.FolderGuid.ToString() + ""\\"");
            _report.ReportLocation = saveTo.FullName + _report.ExcelTemplate;
            wBook.Close(true, _report.ReportLocation, m_objOpt);
            wBook = null;

        }
        catch (Exception ex)
        {
            LogException.HandleException(ex);
        }
        finally
        {
            NAR(wSheet);
            if (wBook != null)
                wBook.Close(false, m_objOpt, m_objOpt);
            NAR(wBook);
            xl.Quit();
            NAR(xl);
            GC.Collect();
        }

private void NAR(object o)
{
    try
    {
        System.Runtime.InteropServices.Marshal.ReleaseComObject(o);
    }
    catch { }
    finally
    {
        o = null;
    }
}


Update
No matter what I try, the 'clean' method or the 'ugly' method (see answers below), the excel instance still hangs around as soon as this line is hit:
wSheet.Cells[iRowCount, 1] = data[""fullname""].ToString();

If I comment that line out (and the other similar ones below it, obviously) the Excel app exits gracefully. As soon as one line per above is uncommented, Excel sticks around.
I think I'm going to have to check if there's a running instance prior to assigning the xl variable and hook into that instead. I forgot to mention that this is a windows service, but that shouldn't matter, should it?

",22k,"
            14
        ","['\nUPDATE (November 2016)\nI\'ve just read a convincing argument by Hans Passant that using GC.Collect is actually the right way to go. I no longer work with Office (thank goodness), but if I did I\'d probably want to give this another try - it would certainly simplify a lot of the (thousands of lines) of code I wrote trying to do things the ""right"" way (as I saw it then).\nI\'ll leave my original answer for posterity...\n\nAs Mike says in his answer, there is an easy way and a hard way to deal with this. Mike suggests using the easy way because... it\'s easier. I don\'t personally believe that\'s a good enough reason, and I don\'t believe it\'s the right way. It smacks of ""turn it off and on again"" to me.\nI have several years experience of developing an Office automation application in .NET, and these COM interop problems plagued me for the first few weeks & months when I first ran into the issue, not least because Microsoft are very coy about admitting there\'s a problem in the first place, and at the time good advice was hard to find on the web.\nI have a way of working that I now use virtually without thinking about it, and it\'s years since I had a problem. It\'s still important to be alive to all the hidden objects that you might be creating - and yes, if you miss one, you might have a leak that only becomes apparent much later. But it\'s no worse than things used to be in the bad old days of malloc/free.\nI do think there\'s something to be said for cleaning up after yourself as you go, rather than at the end. If you\'re only starting Excel to fill in a few cells, then maybe it doesn\'t matter - but if you\'re going to be doing some heavy lifting, then that\'s a different matter.\nAnyway, the technique I use is to use a wrapper class that implements IDisposable, and which in its Dispose method calls ReleaseComObject. That way I can use using statements to ensure that the object is disposed (and the COM object released) as soon as I\'m finished with it.\nCrucially, it\'ll get disposed/released even if my function returns early, or there\'s an Exception, etc. Also, it\'ll only get disposed/released if it was actually created in the first place - call me a pedant but the suggested code that attempts to release objects that may not actually have been created looks to me like sloppy code. I have a similar objection to using FinalReleaseComObject - you should know how many times you caused the creation of a COM reference, and should therefore be able to release it the same number of times.\nA typical snippet of my code might look like this (or it would, if I was using C# v2 and could use generics :-)):\nusing (ComWrapper<Excel.Application> application = new ComWrapper<Excel.Application>(new Excel.Application()))\n{\n  try\n  {\n    using (ComWrapper<Excel.Workbooks> workbooks = new ComWrapper<Excel.Workbooks>(application.ComObject.Workbooks))\n    {\n      using (ComWrapper<Excel.Workbook> workbook = new ComWrapper<Excel.Workbook>(workbooks.ComObject.Open(...)))\n      {\n        using (ComWrapper<Excel.Worksheet> worksheet = new ComWrapper<Excel.Worksheet>(workbook.ComObject.ActiveSheet))\n        {\n          FillTheWorksheet(worksheet);\n        }\n        // Close the workbook here (see edit 2 below)\n      }\n    }\n  }\n  finally\n  {\n    application.ComObject.Quit();\n  }\n}\n\nNow, I\'m not about to pretend that that isn\'t wordy, and the indentation caused by object creation can get out of hand if you don\'t divide stuff into smaller methods. This example is something of a worst case, since all we\'re doing is creating objects. Normally there\'s a lot more going on between the braces and the overhead is much less.\nNote that as per the example above I would always pass the \'wrapped\' objects between methods, never a naked COM object, and it would be the responsibility of the caller to dispose of it (usually with a using statement). Similarly, I would always return a wrapped object, never a naked one, and again it would be the responsibility of the caller to release it. You could use a different protocol, but it\'s important to have clear rules, just as it was when we used to have to do our own memory management.\nThe ComWrapper<T> class used here hopefully requires little explanation. It simply stores a reference to the wrapped COM object, and releases it explicitly (using ReleaseComObject) in its Dispose method. The ComObject method simply returns a typed reference to the wrapped COM object.\nHope this helps!\nEDIT: I\'ve only now followed the link over to Mike\'s answer to another question, and I see that another answer to that question there has a link to a wrapper class, much as I suggest above.\nAlso, with regard to Mike\'s answer to that other question, I have to say I was very nearly seduced by the ""just use GC.Collect"" argument. However, I was mainly drawn to that on a false premise; it looked at first glance like there would be no need to worry about the COM references at all. However, as Mike says you do still need to explicitly release the COM objects associated with all your in-scope variables - and so all you\'ve done is reduce rather than remove the need for COM-object management. Personally, I\'d rather go the whole hog. \nI also note a tendency in lots of answers to write code where everything gets released at the end of a method, in a big block of ReleaseComObject calls. That\'s all very well if everything works as planned, but I would urge anyone writing serious code to consider what would happen if an exception were thrown, or if the method had several exit points (the code would not be executed, and thus the COM objects would not be released). This is why I favor the use of ""wrappers"" and usings. It\'s wordy, but it does make for bulletproof code.\nEDIT2: I\'ve updated the code above to indicate where the workbook should be closed with or without saving changes. Here\'s the code to save changes:\nobject saveChanges = Excel.XlSaveAction.xlSaveChanges;\n\nworkbook.ComObject.Close(saveChanges, Type.Missing, Type.Missing);\n\n...and to not save changes, simply change xlSaveChanges to xlDoNotSaveChanges.\n', '\nWhat is happening is that your call to:\nSheet.Cells[iRowCount, 1] = data[""fullname""].ToString();\n\nIs essentially the same as:\nExcel.Range cell = Sheet.Cells[iRowCount, 1];\ncell.Value = data[""fullname""].ToString();\n\nBy doing it this way, you can see that you are creating an Excel.Range object, and then assigning a value to it. This way also gives us a named reference to our range variable, the cell variable, that allows us to release it directly if we wanted. So you could clean up your objects one of two ways:\n(1) The difficult and ugly way:\nwhile (data.Read())\n{\n    Excel.Range cell = Sheet.Cells[iRowCount, 1];\n    cell.Value = data[""fullname""].ToString();\n    Marshal.FinalReleaseComObject(cell);\n\n    cell = Sheet.Cells[iRowCount, 2];\n    cell.Value = data[""brand""].ToString();\n    Marshal.FinalReleaseComObject(cell);\n\n    cell = Sheet.Cells[iRowCount, 3];\n    cell.Value = data[""agency""].ToString();\n    Marshal.FinalReleaseComObject(cell);\n\n    // etc...\n}\n\nIn the above, we are releasing each range object via a call to Marshal.FinalReleaseComObject(cell) as we go along.\n(2) The easy and clean way: \nLeave your code exactly as you currently have it, and then at the end you can clean up as follows:\nGC.Collect();\nGC.WaitForPendingFinalizers();\n\nif (wSheet != null)\n{\n    Marshal.FinalReleaseComObject(wSheet)\n}\nif (wBook != null)\n{\n    wBook.Close(false, m_objOpt, m_objOpt);\n    Marshal.FinalReleaseComObject(wBook);\n}\nxl.Quit();\nMarshal.FinalReleaseComObject(xl);\n\nIn short, your existing code is extremely close. If you just add calls to GC.Collect() and GC.WaitForPendingFinalizers() before your \'NAR\' calls, I think it should work for you. (In short, both Jamie\'s code and Ahmad\'s code are correct. Jamie\'s is cleaner, but Ahmad\'s code is an easier ""quick fix"" for you because you would only have to add the calls to calls to GC.Collect() and GC.WaitForPendingFinalizers() to your existing code.)\nJamie and Amhad also listed links to the .NET Automation Forum that I participate on (thanks guys!) Here are a couple of related posts that I\'ve made here on StackOverflow :\n(1) How to properly clean up Excel interop objects in C#\n(2) C# Automate PowerPoint Excel -- PowerPoint does not quit\nI hope this helps, Sean...\nMike\n', ""\nAdd the following before your call to xl.Quit():\nGC.Collect(); \nGC.WaitForPendingFinalizers(); \n\nYou can also use Marshal.FinalReleaseComObject() in your NAR method instead of ReleaseComObject. ReleaseComObject decrements the reference count by 1 while FinalReleaseComObject releases all references so the count is 0.\nSo your finally block would look like:\nfinally\n{\n    GC.Collect(); \n    GC.WaitForPendingFinalizers(); \n\n    NAR(wSheet);\n    if (wBook != null)\n        wBook.Close(false, m_objOpt, m_objOpt);\n    NAR(wBook);\n    xl.Quit();\n    NAR(xl);\n}\n\nUpdated NAR method:\nprivate void NAR(object o)\n{\n    try\n    {\n        System.Runtime.InteropServices.Marshal.FinalReleaseComObject(o);\n    }\n    catch { }\n    finally\n    {\n        o = null;\n    }\n}\n\nI had researched this awhile ago and in examples I found usually the GC related calls were at the end after closing the app. However, there's an MVP (Mike Rosenblum) that mentions that it ought to be called in the beginning. I've tried both ways and they've worked. I also tried it without the WaitForPendingFinalizers and it worked although it shouldn't hurt anything. YMMV.\nHere are the relevant links by the MVP I mentioned (they're in VB but it's not that different):\n\nhttp://www.xtremevbtalk.com/showthread.php?p=1157109#post1157109\nhttp://www.xtremevbtalk.com/showthread.php?s=bcdea222412c5cbfa7f02cfaf8f7b33f&p=1156479#post1156479\n\n"", '\nAs others have already covered InterOp i would suggest that if you deal with Excel files with XLSX extension you should use EPPlus which will make your Excel nightmares go away.\n', '\nI have just answered this question here:\nKilling excel process by its main window hWnd\n', '\nIts over 4 years since this was posted but I came across the same problem and was able to solve it.  Apparently just accessing the Cells array creates a COM object. So if you were to do:\n    wSheet = (Excel._Worksheet)wBook.ActiveSheet;\n    Microsoft.Office.Interop.Excel.Range cells = wSheet.Cells;\n\n    int iRowCount = 2;\n\n    // enumerate and drop the values straight into the Excel file\n    while (data.Read())\n    {\n        Microsoft.Office.Interop.Excel.Range cell = cells[iRowCount, 1];\n        cell  = data[""fullname""].ToString();\n        Marshal.FinalReleaseComObject(cell);\n    }\n    Marshal.FinalReleaseComObject(cells);\n\nand then the rest of your cleanup it should fix the problem.\n', '\nWhat I ended up doing to solve a similar problem was get the process Id and kill that as a last resort...\n[DllImport(""user32.dll"", SetLastError = true)]\n   static extern IntPtr GetWindowThreadProcessId(int hWnd, out IntPtr lpdwProcessId);\n\n...\n\nobjApp = new Excel.Application();\n\nIntPtr processID;\nGetWindowThreadProcessId(objApp.Hwnd, out processID);\nexcel = Process.GetProcessById(processID.ToInt32());\n\n...\n\nobjApp.Application.Quit();\nMarshal.FinalReleaseComObject(objApp);\n_excel.Kill();\n\n', ""\nHere's the contents of my hasn't-failed-yet finally block for cleaning up Excel automation. My application leaves Excel open so there's no Quit call. The reference in the comment was my source.\nfinally\n{\n    // Cleanup -- See http://www.xtremevbtalk.com/showthread.php?t=160433\n    GC.Collect();\n    GC.WaitForPendingFinalizers();\n    GC.Collect();\n    GC.WaitForPendingFinalizers();\n    // Calls are needed to avoid memory leak\n    Marshal.FinalReleaseComObject(sheet);\n    Marshal.FinalReleaseComObject(book);\n    Marshal.FinalReleaseComObject(excel);\n}\n\n"", '\nHave you considered using a pure .NET solution such as SpreadsheetGear for .NET? Here is what your code might like like using SpreadsheetGear:\n// open the template            \nusing (IWorkbookSet workbookSet = SpreadsheetGear.Factory.GetWorkbookSet())\n{\n    IWorkbook wBook = workbookSet.Workbooks.Open(excelTemplatePath + _report.ExcelTemplate);\n    IWorksheet wSheet = wBook.ActiveWorksheet;\n    int iRowCount = 2;\n    // enumerate and drop the values straight into the Excel file            \n    while (data.Read())\n    {\n        wSheet.Cells[iRowCount, 1].Value = data[""fullname""].ToString();\n        wSheet.Cells[iRowCount, 2].Value = data[""brand""].ToString();\n        wSheet.Cells[iRowCount, 3].Value = data[""agency""].ToString();\n        wSheet.Cells[iRowCount, 4].Value = data[""advertiser""].ToString();\n        wSheet.Cells[iRowCount, 5].Value = data[""product""].ToString();\n        wSheet.Cells[iRowCount, 6].Value = data[""comment""].ToString();\n        wSheet.Cells[iRowCount, 7].Value = data[""brief""].ToString();\n        wSheet.Cells[iRowCount, 8].Value = data[""responseDate""].ToString();\n        wSheet.Cells[iRowCount, 9].Value = data[""share""].ToString();\n        wSheet.Cells[iRowCount, 10].Value = data[""status""].ToString();\n        wSheet.Cells[iRowCount, 11].Value = data[""startDate""].ToString();\n        wSheet.Cells[iRowCount, 12].Value = data[""value""].ToString();\n        iRowCount++;\n    }\n    DirectoryInfo saveTo = Directory.CreateDirectory(excelTemplatePath + _report.FolderGuid.ToString() + ""\\\\"");\n    _report.ReportLocation = saveTo.FullName + _report.ExcelTemplate;\n    wBook.SaveAs(_report.ReportLocation, FileFormat.OpenXMLWorkbook);\n}\n\nIf you have more than a few rows, you might be shocked at how much faster it runs. And you will never have to worry about a hanging instance of Excel.\nYou can download the free trial here and try it for yourself.\nDisclaimer: I own SpreadsheetGear LLC\n', '\nThe easiest way to paste code is through a question - this doesn\'t mean that I have answered my own question (unfortunately).\nApologies to those trying to help me - I was not able to get back to this until now. It still has me stumped...\nI have completely isolated the Excel code into one function as per\nprivate bool GenerateDailyProposalsReport(ScheduledReport report)\n{\n    // start of test\n\n    Excel.Application xl = null;\n    Excel._Workbook wBook = null;\n    Excel._Worksheet wSheet = null;\n    Excel.Range xlrange = null;\n    object m_objOpt = System.Reflection.Missing.Value;\n\n    xl = new Excel.Application();\n    wBook = (Excel._Workbook)xl.Workbooks.Open(@""E:\\Development\\Romain\\APN\\SalesLinkReportManager\\ExcelTemplates\\DailyProposalReport.xls"", false, false, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt);\n    wSheet = (Excel._Worksheet)wBook.ActiveSheet;\n    xlrange = wSheet.Cells[2, 1] as Excel.Range;\n\n    // PROBLEM LINE ************\n    xlrange.Value2 = ""fullname"";\n    //**************************\n\n    wBook.Close(true, @""c:\\temp\\DailyProposalReport.xls"", m_objOpt);\n    xl.Quit();\n\n    GC.Collect();\n    GC.WaitForPendingFinalizers();\n    GC.Collect();\n    GC.WaitForPendingFinalizers();\n\n    Marshal.FinalReleaseComObject(xlrange);\n    Marshal.FinalReleaseComObject(wSheet);\n    Marshal.FinalReleaseComObject(wBook);\n    Marshal.FinalReleaseComObject(xl);\n\n    xlrange = null;\n    wSheet = null;\n    wBook = null;\n    xl = null;\n\n    // end of test\n    return true;\n\n}\n\nIf I comment out the PROBLEM LINE above, the instance of Excel is released from memory. As it stands, it does not.\nI\'d appreciate any further help on this as time is fleeting and a deadline looms (don\'t they all). \nPlease ask if you need more information.\nThanks in anticipation.\nAddendum\nA bit more information that may or may not shed more light on this. I have resorted to killing the process (stopgap measure) after a certain time lapse (5-10 seconds to give Excel time to finish it\'s processes). I have two reports scheduled - the first report is created and saved to disk and the Excel process is killed, then emailed. The second is created, saved to disk, the process is killed but suddenly there is an error when attempting the email. The error is:\n The process cannot access the file\'....\' etc\nSo even when the Excel app has been killed, the actual Excel file is still being held by the windows service. I have to kill the service to delete the file...\n', '\nI\'m afraid I am running out of ideas here Sean. :-(\nGary could have some thoughts, but although his wrapper approach is very solid, it won\'t actually help you in this case because you are already doing everything pretty much correctly.\nI\'ll list a few thoughts here. I don\'t see how any of them will actually work because your mystery line \nxlrange.Value2 = ""fullname"";\n\nwould not seem to be impacted by any of these ideas, but here goes:\n(1) Don\'t make use of the _Workbook and _Worksheet interfaces. Use Workbook and Worksheet instead. (For more on this see: Excel interop: _Worksheet or Worksheet?.)\n(2) Any time you have two dots (""."") on the same line when accessing an Excel object, break it up into two lines, assigning each object to a named variable. Then, within the cleanup section of your code, explicitly release each variable using Marshal.FinalReleaseComObject().\nFor example, your code here:\n wBook = (Excel._Workbook)xl.Workbooks.Open(@""E:\\Development\\Romain\\APN\\SalesLinkReportManager\\ExcelTemplates\\DailyProposalReport.xls"", false, false, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt);\n\ncould be broken up to:\nExcel.Workbooks wBooks = xl.Workbooks;\nwBook = wBooks.Open(""@""E:\\Development\\...\\DailyProposalReport.xls"", etc...);\n\nAnd then later, within the cleanup section, you would have:\nMarshal.FinalReleaseComObject(xlrange);\nMarshal.FinalReleaseComObject(wSheet);\nMarshal.FinalReleaseComObject(wBook);\nMarshal.FinalReleaseComObject(wBooks); // <-- Added\nMarshal.FinalReleaseComObject(xl);\n\n(3) I am not sure what is going on with your Process.Kill approach. If you call wBook.Close() and then xl.Quit() before calling Process.Kill(), you should have no troubles. Workbook.Close() does not return execution to you until the workbook is closed, and Excel.Quit() will not return execution until Excel has finished shutting down (although it might still be hanging).  \nAfter calling Process.Kill(), you can check the Process.HasExited property in a loop, or, better, call the Process.WaitForExit() method which will pause until it has exited for you. I would guess that this will generally take well under a second to occur. It is better to wait less time and be certain than to wait 5 - 10 seconds and only be guessing.\n(4) You should try these cleanup ideas that I\'ve listed above, but I am starting to suspect that you might have an issue with other processes that might be working with Excel, such as an add-in or anti-virus program. These add-ins can cause Excel to hang if they are not done correctly. If this occurs, it can be very difficult or impossible to get Excel to release. You would need to figure out the offending program and then disable it. Another possibility is that operating as a Windows Service somehow is an issue. I don\'t see why it would be, but I do not have experience automating Excel via a Windows Service, so I can\'t say. If your problems are related to this, then using Process.Kill will likely be your only resort here.\nThis is all I can think of off-hand, Sean. I hope this helps. Let us know how it goes...\n-- Mike\n', '\nSean,\nI\'m going to re-post your code again with my changes (below). I\'ve avoided changing your code too much, so I haven\'t added any exception handling, etc. This code is not robust.\nprivate bool GenerateDailyProposalsReport(ScheduledReport report)\n{\n    Excel.Application xl = null;\n    Excel.Workbooks wBooks = null;\n    Excel.Workbook wBook = null;\n    Excel.Worksheet wSheet = null;\n    Excel.Range xlrange = null;\n    Excel.Range xlcell = null;\n\n    xl = new Excel.Application();\n\n    wBooks = xl.Workbooks;\n\n    wBook = wBooks.Open(@""DailyProposalReport.xls"", false, false, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing);\n\n    wSheet = wBook.ActiveSheet;\n\n    xlrange = wSheet.Cells;\n\n    xlcell = xlrange[2, 1] as Excel.Range;\n\n    xlcell.Value2 = ""fullname"";\n\n    Marshal.ReleaseComObject(xlcell);\n    Marshal.ReleaseComObject(xlrange);\n    Marshal.ReleaseComObject(wSheet);\n\n    wBook.Close(true, @""c:\\temp\\DailyProposalReport.xls"", Type.Missing);\n\n    Marshal.ReleaseComObject(wBook);\n    Marshal.ReleaseComObject(wBooks);\n\n    xl.Quit();\n\n    Marshal.ReleaseComObject(xl);\n\n    return true;\n}\n\nPoints to note:\n\nThe Workbooks method of the\nApplication class creates a\nWorkbooks object which holds a\nreference to the corresponding COM\nobject, so we need to ensure we\nsubsequently release that reference,\nwhich is why I added the variable\nwBooks and the corresponding call to ReleaseComObject.\nSimilarly, the Cells method of the\nWorksheet object returns a Range\nobject with another COM reference,\nso we need to clean that up too.\nHence the need for 2 separate Range variables.\nI\'ve released the COM references\n(using ReleaseComObject) as soon\nas they\'re no longer needed, which I\nthink is good practice even if it\nisn\'t strictly necessary. Also, (and\nthis may be superstition) I\'ve\nreleased all the objects owned by\nthe workbook before closing the\nworkbook, and released the workbook\nbefore closing Excel.\nI\'m not calling GC.Collect etc.\nbecause it shouldn\'t be necessary.\nReally!\nI\'m using ReleaseComObject rather\nthan FinalReleaseComObject, because it should be perfectly sufficient.\nI\'m not null-ing the variables after\nuse; once again, it\'s not doing\nanything worthwhile.\nNot relevant\nhere, but I\'m using Type.Missing\ninstead of\nSystem.Reflection.Missing.Value for\nconvenience. Roll on C#v4 where\noptional parameters will be\nsupported by the compiler!\n\nI\'ve not been able to compile or run this code, but I\'m pretty confident it\'ll work. Good luck!\n', ""\nThere's no need to use the excel com objects from C#. You can use OleDb to modify the sheets.\nhttp://www.codeproject.com/KB/office/excel_using_oledb.aspx\n"", '\nI was having a similar problem.\nI removed _worksheet and _workbook and all was well.\n']",https://stackoverflow.com/questions/1041266/c-sharp-and-excel-automation-ending-the-running-instance,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PsExec Throws Error Messages, but works without any problems","
So we are using PsExec a lot in our automations to install virtual machines, as we can't use ps remote sessions with our windows 2003 machines. Everything works great and there are no Problems, but PsExec keeps throwing errors, even every command is being carried out without correctly. 
For example:
D:\tools\pstools\psexec.exe $guestIP -u $global:default_user -p $global:default_pwd -d -i C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -command ""Enable-PSRemoting -Force""

Enables the PsRemoting on the guest, but also throws this error message:
psexec.exe : 
Bei D:\Scripts\VMware\VMware_Module5.ps1:489 Zeichen:29
+     D:\tools\pstools\psexec.exe <<<<  $guestIP -u $global:default_user -p $global:default_pwd -d -i C:\Windows\System32\WindowsPowerShell\
v1.0\powershell.exe -command ""Enable-PSRemoting -Force""
+ CategoryInfo          : NotSpecified: (:String) [], RemoteException
+ FullyQualifiedErrorId : NativeCommandError

PsExec v1.98 - Execute processes remotely
Copyright (C) 2001-2010 Mark Russinovich
Sysinternals - www.sysinternals.com


Connecting to 172.17.23.95...Starting PsExec service on 172.17.23.95...Connecting with PsExec service on 172.17.23.95...Starting C:\Windows\
System32\WindowsPowerShell\v1.0\powershell.exe on 172.17.23.95...
C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe started on 172.17.23.95 with process ID 2600.

These kinds of error messages apear ALWAYS no matter how i use psexec, like with quotes, with vriables/fixed values, other flags, etc. Does anybody has an idea how i could fix this? It is not a real problem, but it makes finding errors a pain in the ass, because the ""errors"" are everywhere. Disabling the error messages of psexec at all would also help...
",21k,"
            11
        ","['\nThis is because PowerShell sometimes reports a NativeCommandError when a process writes to STDERR. PsExec writes the infoline\nPsExec v1.98 - Execute processes remotely\nCopyright (C) 2001-2010 Mark Russinovich\nSysinternals - www.sysinternals.com\n\nto STDERR which means it can cause this.\nFor more information, see these questions / answers:\n\nhttps://stackoverflow.com/a/1416933/478656\nhttps://stackoverflow.com/a/11826589/478656\nhttps://stackoverflow.com/a/10666208/478656\n\n', ""\nredirect stderr to null worked best for me. see below link\nError when calling 3rd party executable from Powershell when using an IDE\nHere's the relevant section from that link:\nTo avoid this you can redirect stderr to null e.g.:\ndu 2> $null\nEssentially the console host and ISE (as well as remoting) treat the stderr stream differently. On the console host it was important for PowerShell to support applications like edit.com to work along with other applications that write colored output and errors to the screen. If the I/O stream is not redirected on console host, PowerShell gives the native EXE a console handle to write to directly. This bypasses PowerShell so PowerShell can't see that there are errors written so it can't report the error via $error or by writing to PowerShell's stderr stream.\nISE and remoting don't need to support this scenario so they do see the errors on stderr and subsequently write the error and update $error.\n.\\PsExec.exe \\$hostname -u $script:userName -p $script:password /accepteula -h cmd /c $powerShellArgs 2> $null\n"", '\nI have created a psexec wrapper for powershell, which may be helpful to people browsing this question:\nfunction Return-CommandResultsUsingPsexec {\n    param(\n        [Parameter(Mandatory=$true)] [string] $command_str,\n        [Parameter(Mandatory=$true)] [string] $remote_computer,\n        [Parameter(Mandatory=$true)] [string] $psexec_path,\n        [switch] $include_blank_lines\n    )\n\n    begin {\n        $remote_computer_regex_escaped = [regex]::Escape($remote_computer)\n\n        # $ps_exec_header = ""`r`nPsExec v2.2 - Execute processes remotely`r`nCopyright (C) 2001-2016 Mark Russinovich`r`nSysinternals - www.sysinternals.com`r`n""\n\n        $ps_exec_regex_headers_array = @(\n            \'^\\s*PsExec v\\d+(?:\\.\\d+)? - Execute processes remotely\\s*$\',\n            \'^\\s*Copyright \\(C\\) \\d{4}(?:-\\d{4})? Mark Russinovich\\s*$\',\n            \'^\\s*Sysinternals - www\\.sysinternals\\.com\\s*$\'\n        )\n\n        $ps_exec_regex_info_array = @(\n            (\'^\\s*Connecting to \' + $remote_computer_regex_escaped + \'\\.{3}\\s*$\'),\n            (\'^\\s*Starting PSEXESVC service on \' + $remote_computer_regex_escaped + \'\\.{3}\\s*$\'),\n            (\'^\\s*Connecting with PsExec service on \' + $remote_computer_regex_escaped + \'\\.{3}\\s*$\'),\n            (\'^\\s*Starting .+ on \' + $remote_computer_regex_escaped + \'\\.{3}\\s*$\')\n        )\n\n        $bypass_regex_array = $ps_exec_regex_headers_array + $ps_exec_regex_info_array\n\n        $exit_code_regex_str = (\'^.+ exited on \' + $remote_computer_regex_escaped + \' with error code (\\d+)\\.\\s*$\')\n\n        $ps_exec_args_str = (\'""\\\\\' + $remote_computer + \'"" \' + $command_str)\n    }\n\n    process {\n        $return_dict = @{\n            \'std_out\' = (New-Object \'system.collections.generic.list[string]\');\n            \'std_err\' = (New-Object \'system.collections.generic.list[string]\');\n            \'exit_code\' = $null;\n            \'bypassed_std\' = (New-Object \'system.collections.generic.list[string]\');\n        }\n\n        $process_info = New-Object System.Diagnostics.ProcessStartInfo\n        $process_info.RedirectStandardError = $true\n        $process_info.RedirectStandardOutput = $true\n        $process_info.UseShellExecute = $false\n        $process_info.FileName = $psexec_path\n        $process_info.Arguments = $ps_exec_args_str\n\n        $process = New-Object System.Diagnostics.Process\n        $process.StartInfo = $process_info\n        $process.Start() | Out-Null\n\n        $std_dict = [ordered] @{\n            \'std_out\' = New-Object \'system.collections.generic.list[string]\';\n            \'std_err\' = New-Object \'system.collections.generic.list[string]\';\n        }\n\n        # $stdout_str = $process.StandardOutput.ReadToEnd()\n        while ($true) {\n            $line = $process.StandardOutput.ReadLine()\n            if ($line -eq $null) {\n                break\n            }\n            $std_dict[\'std_out\'].Add($line)\n        }\n\n        # $stderr_str = $process.StandardError.ReadToEnd()\n        while ($true) {\n            $line = $process.StandardError.ReadLine()\n            if ($line -eq $null) {\n                break\n            }\n            $std_dict[\'std_err\'].Add($line)\n        }\n\n        $process.WaitForExit()\n\n        ForEach ($std_type in $std_dict.Keys) {\n            ForEach ($line in $std_dict[$std_type]) {\n                if ((-not $include_blank_lines) -and ($line -match \'^\\s*$\')) {\n                    continue\n                }\n\n                $do_continue = $false\n                ForEach ($regex_str in $bypass_regex_array) {\n                    if ($line -match $regex_str) {\n                        $return_dict[\'bypassed_std\'].Add($line)\n                        $do_continue = $true\n                        break\n                    }\n                }\n                if ($do_continue) {\n                    continue\n                }\n\n                $exit_code_regex_match = [regex]::Match($line, $exit_code_regex_str)\n\n                if ($exit_code_regex_match.Success) {\n                    $return_dict[\'exit_code\'] = [int] $exit_code_regex_match.Groups[1].Value\n                } elseif ($std_type -eq \'std_out\') {\n                    $return_dict[\'std_out\'].Add($line)\n                } elseif ($std_type -eq \'std_err\') {\n                    $return_dict[\'std_err\'].Add($line)\n                } else {\n                    throw \'this conditional should never be true; if so, something was coded incorrectly\'\n                }\n            }\n        }\n\n        return $return_dict\n    }\n}\n\n']",https://stackoverflow.com/questions/18380227/psexec-throws-error-messages-but-works-without-any-problems,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"IE9, Automation server can't create object error while using CertEnroll.dll","
In my web page, a JS block like this:
var classFactory = new ActiveXObject(""X509Enrollment.CX509EnrollmentWebClassFactory"");

// Other initialize CertEnroll Objects

It works fine in windows7(32bit or 64bit) with IE8(32bit), as long as I change the IE8 secure setting, enable Initializing and Script ActiveX controls not marked as safe.
But when use IE9(32bit), I try anything I can find on web, it reports error ""Automation server can't create object.""
I even created a static html file, save it in my hard disk, and then open it with IE9(32bit), it worked fine. Then I put the html file on my web site, visit the html file with url, then it came up with the error message again.
I have worked on this problem for 4 days, any suggestion would be appreciated.
3Q. I hope you can read my words as I'm not an native English speaker.
",99k,"
            11
        ","['\na) Go to Tools-->Internet Options\nb) Select security tab\nc) Click on Trusted Sites (or Local Intranet depending on whether your site is trusted or not)\nd) Click on Custom Level\ne) Ensure that ""Initialize and script active x controls is not marked safe for scripting""  is enabled - this comes under Activex controls and plug-ins section towards 1/4th of the scroll bar.\nClick OK, OK.\nOnce this is completed, clear the browser cookies and cache. Close all your browser sessions. Reopen the IE to launch your site. \nTry to disable the setting in step (e) to see if the problem comes back - that should give more insight to the problem.\n']",https://stackoverflow.com/questions/15686040/ie9-automation-server-cant-create-object-error-while-using-certenroll-dll,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to send input to the console as if the user is typing?,"
This is my problem. I have a program that has to run in a TTY, cygwin provides this TTY. When I redirect stdIn the program fails because it does not have a TTY. I cannot modify this program, and need some way of automating it. 
How can I grab the cmd.exe window and send it data and make it think the user is typing it? 
I'm using C#, I believe there is a way to do it with java.awt.Robot but I have to use C# for other reasons.
",13k,"
            7
        ","['\nI have figured out how to send the input to the console.  I used what Jon Skeet said.  I am not 100% sure this is the correct way to implement this.\nIf there are any comments on to make this better I would love to here.  I did this just to see if I could figure it out.\nHere is the program I stared that waited for input form the user\nclass Program\n{\n    static void Main(string[] args)\n    {\n        // This is needed to wait for the other process to wire up.\n        System.Threading.Thread.Sleep(2000);\n\n        Console.WriteLine(""Enter Pharse: "");\n\n        string pharse = Console.ReadLine();\n\n        Console.WriteLine(""The password is \'{0}\'"", pharse);\n\n\n        Console.WriteLine(""Press any key to exit. . ."");\n        string lastLine = Console.ReadLine();\n\n        Console.WriteLine(""Last Line is: \'{0}\'"", lastLine);\n    }\n}\n\nThis is the console app writing to the other one\nclass Program\n{\n    static void Main(string[] args)\n    {\n        // Find the path of the Console to start\n        string readFilePath = System.IO.Path.GetFullPath(@""..\\..\\..\\ReadingConsole\\bin\\Debug\\ReadingConsole.exe"");\n\n        ProcessStartInfo startInfo = new ProcessStartInfo(readFilePath);\n\n        startInfo.RedirectStandardOutput = true;\n        startInfo.RedirectStandardInput = true;\n        startInfo.WindowStyle = ProcessWindowStyle.Hidden;\n        startInfo.CreateNoWindow = true;\n        startInfo.UseShellExecute = false;\n\n        Process readProcess = new Process();\n        readProcess.StartInfo = startInfo;\n\n        // This is the key to send data to the server that I found\n        readProcess.OutputDataReceived += new DataReceivedEventHandler(readProcess_OutputDataReceived);\n\n        // Start the process\n        readProcess.Start();\n\n        readProcess.BeginOutputReadLine();\n\n        // Wait for other process to spin up\n        System.Threading.Thread.Sleep(5000);\n\n        // Send Hello World\n        readProcess.StandardInput.WriteLine(""Hello World"");\n\n        readProcess.StandardInput.WriteLine(""Exit"");\n\n        readProcess.WaitForExit();\n    }\n\n    static void readProcess_OutputDataReceived(object sender, DataReceivedEventArgs e)\n    {\n        // Write what was sent in the event\n        Console.WriteLine(""Data Recieved at {1}: {0}"", e.Data, DateTime.UtcNow.Ticks);\n    }\n}\n\n', '\nThis sounds like a task for SendKeys(). It\'s not C#, but VBScript, but nontheless - you asked for some way of automating it:\nSet Shell = CreateObject(""WScript.Shell"")\n\nShell.Run ""cmd.exe /k title RemoteControlShell""\nWScript.Sleep 250\n\nShell.AppActivate ""RemoteControlShell""\nWScript.Sleep 250\n\nShell.SendKeys ""dir{ENTER}""\n\n', '\nCan you start the program (or cygwin) within your code, using ProcessStartInfo.RedirectStandardInput (and output/error) to control the data flow?\n', '\nI had similar problem some time ago, cygwin should write some useful information (exact cygwin function, error text and WINAPI error code) to error stream, you should redirect it somewhere and read what it writes.\n']",https://stackoverflow.com/questions/451228/how-to-send-input-to-the-console-as-if-the-user-is-typing,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IE11 Frame Notification Bar Save button,"
On a 64-bit system with MS Excel 2010 and IE11 I'm using this code to automate download process from a website:  
hWnd = FindWindowEx(IE.hWnd, 0, ""Frame Notification Bar"", vbNullString)

If hWnd Then
    hWnd = FindWindowEx(hWnd, 0&, ""Button"", ""Save"")
End If

If hWnd Then
    SetForegroundWindow (hWnd)
    Sleep 600
    SendMessage hWnd, BM_CLICK, 0, 0
End If

Everything goes OK until the Frame Notification Bar appears. I'm getting the HWND of this window, but can't get the HWND of the ""Save"" button, so that I can send click to it.
",10k,"
            4
        ","['\nIf somebody is still trying to find the solution, for IE11 it\'s here.\nOn the very first line of the code of Vahagn Sargsyan above, instead of ""Frame Notification Bar"" get the exact title of the dialog box, which might be in English ""View Downloads - Internet Explorer"". This allows you to grab the right hWnd.\nBecause in IE11 there no more button accelerator to Save files, follow the solution posted here by pmr.\nFrom pmr code, just get the following lines:\nSet e = o.ElementFromHandle(ByVal h)\nDim iCnd As IUIAutomationCondition\nSet iCnd = o.CreatePropertyCondition(UIA_NamePropertyId, ""Save"")\n\nDim Button As IUIAutomationElement\nSet Button = e.FindFirst(TreeScope_Subtree, iCnd)\nDim InvokePattern As IUIAutomationInvokePattern\nSet InvokePattern = Button.GetCurrentPattern(UIA_InvokePatternId)\nInvokePattern.Invoke\n\nThis should solve your issue. This unlocked the situation for me with French localisation.\n', ""\nI'm assuming you're talking about that little frame that pops up at the bottom of IE giving you options to Open, Save or Cancel. If so, you might wanna check out another answer to a similar question asked here.\nA secondary solution would be a more complicated one (here), but works nonetheless. You'll have to import the modules from the workbook provided in this forum (you'll need to signup for membership though, but its free so just do it.) and that'll do basically what you need, albeit in a way that allows you more flexibility (choose filepath, filename, etc) and also a little more convoluted. \nEither way, hope I helped. \n""]",https://stackoverflow.com/questions/31489801/ie11-frame-notification-bar-save-button,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VBA:IE-How to assign pathname to file input tag without popup file upload form?,"
I am currently doing automaiton for file uploading
Below is HTML tag for input file tag:
 <input name=""file"" title=""Type the path of the file or click the Browse button to find the file."" id=""file"" type=""file"" size=""20"">

And below is button HTML Tag:
<input name=""Attach"" title=""Attach File (New Window)"" class=""btn"" id=""Attach"" onclick=""javascript:setLastMousePosition(event); window.openPopup('/widg/uploadwaiting.jsp', 'uploadWaiting', 400, 130, 'width=400,height=130,resizable=no,toolbar=no,status=no,scrollbars=no,menubar=no,directories=no,location=no,dependant=no', true);"" type=""submit"" value=""Attach File"">

My VBA coding is:
Dim filee As Object
Set filee = mydoc.getElementById(""file"")
filee.Value = filenamepath

Set attach = mydoc.getElementsByName(""Attach"")
attach(0).Click

When I am running this coding, input filepath box not assign path name so i am getting chose file path.
Find attach screenshot.
 
Finally i have tried below code but that send key not executing  
Dim filee As Object
    Set filee = mydoc.getElementById(""file"")
    filee.Click

obj.SetText filename
obj.PutInClipboard
SendKeys ""^v""
SendKeys ""{ENTER}""

Set attach = mydoc.getElementsByName(""Attach"")
    attach(0).Click

Set finall = mydoc.getElementsByName(""cancel"")
    finall(0).Click

Kindly tell me the windows API program to assign my file name directory in fine name: input box on opened Choose File to Open explorer and click the open button. 
",7k,"
            2
        ","['\nI fixed this issue by running external VBScript contain file path to set it on \'Choose File to Upload\' pop up window using SendKeys method after send Enter Key to close this pop up, and this run successfully because the extranl VBScript run on another process so it will not stuck on VBA code.\nNotes:\n1- I dynamically create the external VBScript from VBA code and save it on Temp folder after that I run this script using WScript.Shell.Run to excute it on another thread\n1- At the begining of the external VBScript I set 1 sec delay to be sure the \'Choose File to Upload\' pop up window already opened from VBA.\nAnd here is the complete code:\n....\n....\n\nSet filee = mydoc.getElementById(""file"")\n\n    CompleteUploadThread MyFilePath\n    filee.Foucs\n    filee.Click\n\n....\n....\n\nPrivate Sub CompleteUploadThread(ByVal fName As String)\n    Dim strScript As String, sFileName As String, wsh As Object\n    Set wsh = VBA.CreateObject(""WScript.Shell"")\n    \'---Create VBscript String---\n    strScript = ""WScript.Sleep 1000"" & vbCrLf & _\n                ""Dim wsh"" & vbCrLf & _\n                ""Set wsh = CreateObject(""""WScript.Shell"""")"" & vbCrLf & _\n                ""wsh.SendKeys """""" & fName & """""""" & vbCrLf & _\n                ""wsh.SendKeys """"{ENTER}"""""" & vbCrLf & _\n                ""Set wsh = Nothing""\n    \'---Save the VBscript String to file---\n    sFileName = wsh.ExpandEnvironmentStrings(""%Temp%"") & ""\\zz_automation.vbs""\n    Open sFileName For Output As #1\n    Print #1, strScript\n    Close #1\n    \'---Execute the VBscript file asynchronously---\n    wsh.Run """""""" & sFileName & """"""""\n    Set wsh = Nothing\nEnd Sub\n\n', '\nAs setting the value of a file input element is disabled due to security reasons, the ""send keys"" method seems to be the only option for automating file uploads using the IE API.\nI just stumbled over the same problem that the code after the Click does not seem to be executed - that is, unless the dialog is closed. This indicates that the Click method is blocking, making it impossible to interact with the dialog from within the macro.\nI could solve that by using a different method to open the dialog: by setting the focus to the file element with Focus, and sending the space key with SendKeys.\nIn your case, replace\nfilee.Click\n\nwith\nfilee.Focus\nSendKeys "" ""\n\n', '\nleemes\'s method(Sending  key to the file selection button on IE) is an easy way to automate the file selection procedure.\nIn addition, if IEObject.Visible sometimes fails to give focus to the IE Window,\n we\'d better send the IE window to the top-most position using Windows API before using \'SendKeys\' like following:\n#If VBA7 Then\n    Declare PtrSafe Function SetForegroundWindow Lib ""user32"" (ByVal hwnd As LongPtr) As LongPtr\n    Declare PtrSafe Function FindWindow Lib ""user32"" Alias ""FindWindowA"" (ByVal lpClassName As Any, ByVal lpWindowName As Any) As LongPtr\n#Else\n    Declare Function SetForegroundWindow Lib ""user32"" (ByVal hwnd As Long) As Long\n    Declare Function FindWindow Lib ""user32"" Alias ""FindWindowA"" (ByVal lpClassName As Any, ByVal lpWindowName As Any) As Long\n#End If\n\nSub Test()\n\n   \'first create or get IE object\n   Set IE= ...\n   ...\n\n   \'second, send IE window to the foreground\n   Dim TargetWnd\n   TargetWnd = FindWindow(""IEFrame"", vbNullString)  \'find IE window\n   If TargetWnd = 0 Then Debug.Print ""Window not found."" \'Else Debug.Print TargetWnd\n   SetForegroundWindow (TargetWnd)\n\n   \'sendkeys\n   set filee = getElement....\n   filee.Focus\n   SendKeys "" ""     \'send Space key instead of .Click method\n   SendKeys ""filePath""     \' ""C:\\path\\filename""  \' Type-in the filename \n   SendKeys ""{Enter}""    \'closes the file dialog\n\n   \'finally submit       \n   ...\n   ...\n\nend Sub\n\n']",https://stackoverflow.com/questions/34803863/vbaie-how-to-assign-pathname-to-file-input-tag-without-popup-file-upload-form,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can you automatically remove trailing whitespace in vim,"
I am getting 'trailing whitespace' errors trying to commit some files in Git.
I want to remove these trailing whitespace characters automatically right before I save Python files.
Can you configure Vim to do this? If so, how?
",75k,"
            219
        ","[""\nI found the answer here.\nAdding the following to my .vimrc file did the trick:\nautocmd BufWritePre *.py :%s/\\s\\+$//e\n\nThe e flag at the end means that the command doesn't issue an error message if the search pattern fails. See :h :s_flags for more.\n"", ""\nCompilation of above plus saving cursor position:\nfunction! <SID>StripTrailingWhitespaces()\n  if !&binary && &filetype != 'diff'\n    let l:save = winsaveview()\n    keeppatterns %s/\\s\\+$//e\n    call winrestview(l:save)\n  endif\nendfun\n\nautocmd FileType c,cpp,java,php,ruby,python autocmd BufWritePre <buffer> :call <SID>StripTrailingWhitespaces()\n\nIf you want to apply this on save to any file, leave out the second autocmd and use a wildcard *:\nautocmd BufWritePre,FileWritePre,FileAppendPre,FilterWritePre *\n  \\ :call <SID>StripTrailingWhitespaces()\n\n"", ""\nI also usually have a :\n\nmatch Todo /\\s\\+$/\n\nin my .vimrc file, so that end of line whitespace are hilighted.\nTodo being a syntax hilighting group-name that is used for hilighting keywords like TODO, FIXME or XXX. It has an annoyingly ugly yellowish background color, and I find it's the best to hilight things you don't want in your code :-)\n"", ""\nI both highlight existing trailing whitespace and also strip trailing whitespace.\nI configure my editor (vim) to show white space at the end, e.g.\n\nwith this at the bottom of my .vimrc:\n\nhighlight ExtraWhitespace ctermbg=red guibg=red\nmatch ExtraWhitespace /\\s\\+$/\nautocmd BufWinEnter * match ExtraWhitespace /\\s\\+$/\nautocmd InsertEnter * match ExtraWhitespace /\\s\\+\\%#\\@<!$/\nautocmd InsertLeave * match ExtraWhitespace /\\s\\+$/\nautocmd BufWinLeave * call clearmatches()\n\nand I 'auto-strip' it from files when saving them, in my case *.rb for ruby files, again in my ~/.vimrc\nfunction! TrimWhiteSpace()\n    %s/\\s\\+$//e\nendfunction\nautocmd BufWritePre     *.rb :call TrimWhiteSpace()\n\n"", ""\nHere's a way to filter by more than one FileType.\n\nautocmd FileType c,cpp,python,ruby,java autocmd BufWritePre <buffer> :%s/\\s\\+$//e\n\n"", '\nI saw this solution in a comment at \nVIM Wikia - Remove unwanted spaces\nI really liked it. Adds a . on the unwanted white spaces.\n\nPut this in your .vimrc\n"" Removes trailing spaces\nfunction TrimWhiteSpace()\n  %s/\\s*$//\n  \'\'\nendfunction\n\nset list listchars=trail:.,extends:>\nautocmd FileWritePre * call TrimWhiteSpace()\nautocmd FileAppendPre * call TrimWhiteSpace()\nautocmd FilterWritePre * call TrimWhiteSpace()\nautocmd BufWritePre * call TrimWhiteSpace()\n\n', '\nCopied and pasted from http://blog.kamil.dworakowski.name/2009/09/unobtrusive-highlighting-of-trailing.html (the link no longer works, but the bit you need is below)\n""This has the advantage of not highlighting each space you type at the end of the line, only when you open a file or leave insert mode. Very neat.""\n\nhighlight ExtraWhitespace ctermbg=red guibg=red\nau ColorScheme * highlight ExtraWhitespace guibg=red\nau BufEnter * match ExtraWhitespace /\\s\\+$/\nau InsertEnter * match ExtraWhitespace /\\s\\+\\%#\\@<!$/\nau InsertLeave * match ExtraWhiteSpace /\\s\\+$/\n\n', '\nThis is how I\'m doing it. I can\'t remember where I stole it from tbh.\n\nautocmd BufWritePre * :call <SID>StripWhite()\nfun! <SID>StripWhite()\n    %s/[ \\t]\\+$//ge\n    %s!^\\( \\+\\)\\t!\\=StrRepeat(""\\t"", 1 + strlen(submatch(1)) / 8)!ge\nendfun\n\n', ""\nA solution which simply strips trailing whitespace from the file is not acceptable in all circumstances. It will work in a project which has had this policy from the start, and so there are no such whitespace that you did not just add yourself in your upcoming commit.\nSuppose you wish merely not to add new instances of trailing whitespace, without affecting existing whitespace in lines that you didn't edit, in order to keep your commit free of changes which are irrelevant to your work.\nIn that case, with git, you can can use a script like this:\n#!/bin/sh\n\nset -e # bail on errors\n\ngit stash save commit-cleanup\ngit stash show -p | sed '/^\\+/s/ *$//' | git apply\ngit stash drop\n\nThat is to say, we stash the changes, and then filter all the + lines in the diff to remove their trailing whitespace as we re-apply the change to the working directory. If this command pipe is successful, we drop the stash.\n"", ""\nThe other approaches here somehow didn't work for me in MacVim when used in the .vimrc file. So here's one that does and highlights trailing spaces:\nset encoding=utf-8\nset listchars=trail:路\nset list\n\n"", '\nFor people who want to run it for specific file types (FileTypes are not always reliable):\nautocmd BufWritePre *.c,*.cpp,*.cc,*.h,*.hpp,*.py,*.m,*.mm :%s/\\s\\+$//e\n\nOr with vim7:\nautocmd BufWritePre *.{c,cpp,cc,h,hpp,py,m,mm} :%s/\\s\\+$//e\n\n', '\nIf you trim whitespace, you should only do it on files that are already clean.  ""When in Rome..."".  This is good etiquette when working on codebases where spurious changes are unwelcome.  \nThis function detects trailing whitespace and turns on trimming only if it was already clean.\nThe credit for this idea goes to a gem of a comment here:  https://github.com/atom/whitespace/issues/10  (longest bug ticket comment stream ever)\nautocmd BufNewFile,BufRead *.test call KarlDetectWhitespace()\n\nfun! KarlDetectWhitespace()\npython << endpython\nimport vim\nnr_unclean = 0\nfor line in vim.current.buffer:\n    if line.rstrip() != line:\n        nr_unclean += 1\n\nprint ""Unclean Lines: %d"" % nr_unclean\nprint ""Name: %s"" % vim.current.buffer.name\ncmd = ""autocmd BufWritePre <buffer> call KarlStripTrailingWhitespace()""\nif nr_unclean == 0:\n    print ""Enabling Whitespace Trimming on Save""\n    vim.command(cmd)\nelse:\n    print ""Whitespace Trimming Disabled""\nendpython\nendfun\n\nfun! KarlStripTrailingWhitespace()\n    let l = line(""."")\n    let c = col(""."")\n    %s/\\s\\+$//e\n    call cursor(l, c)\nendfun\n\n', ""\nautocmd BufWritePre *.py execute 'norm m`' | %s/\\s\\+$//e | norm g``\n\nThis will keep the cursor in the same position as it was just before saving\n"", ""\nautocmd BufWritePre * :%s/\\s\\+$//<CR>:let @/=''<CR>\n""]",https://stackoverflow.com/questions/356126/how-can-you-automatically-remove-trailing-whitespace-in-vim,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WatiN or Selenium? [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



I'm going to start coding some automated tests of our presentation soon. It seems that everyone recommends WatiN and Selenium. Which do you prefer for automated testing of ASP.NET web forms? Which of these products work better for you?
As a side note, I noticed that WatiN 2.0 has been in CTP since March 2008, is that something to be concerned about?
",48k,"
            149
        ","[""\nI'm currently working hard on a beta release of WatiN 2.0 somewhere in Q1 of 2009. It will be a major upgrade to the current CTP 2.0 versions and will basically give you the same functionality to automate FireFox and IE as version 1.3.0 offers for automating IE.\nSo no concerns there.\nJeroen van Menen\nLead dev WatiN\n"", '\nIf you\'re looking to make a serious long-term investment in a framework that will continue to be improved and supported by the community, Selenium is probably your best bet. For example, I just came across this info on Matt Raible\'s blog:\n\nAs of Friday, Google has over 50 teams\n  running over 51K tests per day on\n  internal Selenium Farm. 96% of these\n  tests are handled by Selenium RC and\n  the Farm machines correctly. The other\n  4% are partly due to RC bugs, partly\n  to test errors, but isolating the\n  cause can be difficult. Selenium has\n  been adopted as the primary technology\n  for functional testing of web\n  applications within Google. That\'s the\n  good news.\n\nI also went to one of the Selenium meetups recently and learned that Google is putting serious resources into improving Selenium and integrating it with WebDriver, which is an automated testing tool developed by Simon Stewart. One of the major advantages of WebDriver is that it controls the browser itself rather than running inside the browser as a Javascript application, which means that major stumbling blocks like the ""same origin"" problem will no longer be an issue. \n', ""\nWe've tested both and decided to go with WaTiN.  As others have pointed out, Selenium does have some nice features not found in WaTiN, but we ran into issues getting Selenium working and once we did it was definitely slower when running tests than WaTiN. If I remember correctly, the setup issues we ran into stemmed from the fact that Selenium had a separate app to control the actual browser where WaTiN did everything in process.\n"", '\nI\'ve been trying \'em both out and here are my initial thoughts...\n\nWatiN\nThe Good\n\nFast execution.\nScript creation tools are independent projects; there are 2 that I know of: Wax (Excel based, hosted on CodePlex) and WatiN Test Record (hosted on SourceForge). Neither is as robust as Selenium IDE.\nVery good IE support.  Can attach and detach to/from running instances. Can access native window handles etc. (See script example below).\nNuGet packaged, easy to get running in .NET, Visual Studio style environments and keep updated.\n\nThe Bad\n\nGoogling WatiN (watin xyz) often causes Google to recommend ""watir xyz"" instead. Not that much documentation out there. \nWhat little there is (documentation), it is confusing; for example: at first blush it would appear that there is no native support for CSS selectors. Especially since there are extensions libraries like \'WatiNCssSelectorExtensions\' and many blog articles about alternative techniques (such as injecting jQuery/sizzle into the page). On Stack Overflow, I found a comment by Jeroen van Menen which suggests that there is native support. At least the lead-developer spends time on Stack Overflow :)\nNo native XPath support.\nNo out-of-the-box remote execution/grid based execution.\n\nScript Example (C#). You can\'t do this with Selenium (not that I know off, at least):\nclass IEManager\n{\n    IE _ie = null;\n    object _lock = new object();\n\n    IE GetInstance(string UrlFragment)\n    {\n        lock (_lock)\n        {\n            if (_ie == null)\n            {\n                var instances = new IECollection(true);  //Find all existing IE instances\n                var match = instances.FirstOrDefault(ie=>ie.Url.Contains(UrlFragment));\n                _ie = match ?? new IE();\n                if (match==null)  //we created a new instance, so we should clean it up when done!\n                    _ie.AutoClose = true;\n            }\n        }\n\n        return _ie;\n    }\n}\n\n\nSelenium\n\nSlower than WatiN (especially since a new process has to be created).\nBuilt-in CSS selectors/XPath support.\nSelenium IDE is good (can\'t say great, but it鈥檚 the best in class!).\nFeels more Java-ish than .NET-ish...but really, it\'s programming language agnostic; all commands are sent to an out-of-process \'Driver\'. The driver is really a \'host\' process for the browser instance. All communication must be serialised in/out across process boundaries, which might explain the speed issues relative to WatiN.\nDecoupled processes - ""Driver"" and ""Control""  mean more robustness, more complexity, etc., but also easier to create grids/distributed test environments. Would have really liked it if the ""distribution"" mechanism (i.e. the communication between Driver & Control) were across WebSphere or other existing, robust, message queue manager.\nSupport chrome and other browsers out of the box.\n\nDespite everything, I went with WatiN in the end; I mainly intend to write small screen-scraping applications and want to use LINQPad for development. Attaching to a remote IE instance (one that I did not spawn myself) is a big plus. I can fiddle around in an existing instance...then run a bit of script...then fiddle again etc. This is harder to do with Selenium, though I suppose ""pauses"" could be embedded in the script during which time I could fiddle directly with the browser.\n', ""\nThe biggest difference is that Selenium has support for different browsers (not just IE or FF, see http://seleniumhq.org/about/platforms.html#browsers.\nAlso, Selenium has a remote control server (http://seleniumhq.org/projects/remote-control/), which means that you don't need to run the browser on the same machine the test code is running. You can therefore test your Web app. on different OS platforms.\nIn general I would recommend using Selenium. I've used WatiN few years ago, but I wasn't satisfied with its stability (it has probably improved by now). The biggest plus for Selenium for me is the fact that you can test the Web app. on different browsers.\n"", '\nNeither. Use Coypu. It wraps Selenium. Much more durable. https://github.com/featurist/coypu\nUpdate\nYe Oliver you\'re right. Ok why\'s it better?\nPersonally I\'ve found the Selenium driver for IE in particular to be very fragile - there\'s a number of \'standard\' driver exceptions that I\'ve time again found when driving Selenium for Unit Tests on ajax heavy websites.\nDid I mention I want to write my scripts in c# as a Test Project ? Yes Acceptance Tests within a continous build deployment.\nWell Coypu deals with the above. It\'s a wrapper for Selenium that allows test fixtures such as,\nbrowser.Visit(""file:///C:/users/adiel/localstuff.htm"")\nbrowser.Select(""toyota"").From(""make"");\nbrowser.ClickButton(""Search"");\n\n... which will spin up a (configurable brand of) browser and run the script. It works great with scoped regions and is VERY extendable.\nThere\'s more examples at GitHub and as Olvier below mentions, Adrian\'s video is excellent. I think it\'s the best way to drive browser based tests in the .Net world and tries to follow it\'s Ruby namesake capybara\n', ""\nI've used both, they both seem to work ok.  My nod is for Selenium as it seemed to have better Ajax support.  I believe WaTiN has matured though since last I used it so it should have the same thing.\nThe biggest thing would be which development environment do you like to be in?  Selenium and Watin have recorders but Selenium is in the browser and watin is in visual studio. + and -'s to both of those.\n"", '\nUntil now we are a pure Microsoft Shop for delivering solutions for the enterprise and went with WatiN. This may change in the future.\nAs a more recent source:\nMicrosoft printed in MSDN Magazine 12/2010 a BDD-Primer with the combination of SpecFlow with WatiN (cool BDD-Behavior Driven Development). Its author Brandon Satrom (msft Developer Evangelist) has also posted in December 2010 a Video Webcast teaching in detail 1:1 his above findings.\nThere is a Whitepaper from 04/2011 on Supporting ATDD/BDD with SpecLog, SpecFlow and Team Foundation Server (Acceptance Test Driven Development/Behavior Driven Development) from Christian Hassa, whose team built SpecFlow.\n', ""\nI use Watin, but haven't used Selenium.  I can say I got up and running quickly on Watin and have had few to no problems.  I can't think of anything I have wanted to do that I couldn't figure out with it.  HTH\n"", '\nI generally use Selenium, mainly because I like the Selenium IDE plugin for FireFox for recording starting points for my tests.\n', ""\nI recommend WebAii since that's what I've had any success with and when using it my gripes were few. I never tried Selenium and I don't remember using WaTiN much, at least not to the point where I could get it to succesfully work. I don't know of any framework that deals with Windows dialogs gracefully, although WebAii has an interface for implementing your own dialog handlers.\n"", ""\nI considered using both. I used the recorder for Selenium to build some tests in FF. I tried to do the same in Watin and found that the Watin Recorder (2.0.9.1228) is completely worthless for our sites. It appeared to be rendering the site in IE6 -- making our site effectively unusable for recording. We don't support IE6. I couldn't find any way to change the browser it is using. I only found one Watin Recorder out there. If there's more than one, or one that is kept up to date, please comment. \nThe Selenium Recorder IDE for Firefox is simple to use and ports tests to C#. It isn't great at this. I couldn't get porting test suites to work, despite reading a blog post or two that had workarounds. So there's a bit of manipulation of the generated code. Still, it works 90% and that's better than the alternative. \nFor my money/time, Selenium is superior just for the ease of building new tests. IE doesn't have any good developer toolbars that are anywhere near as good as Firebug, so I'm doing my development in Firefox to begin with, so having a good working recorder in Firefox is a huge bonus. \nMy conclusion here was a lot like that democracy quote by Churchill: Selenium is the worst form of automated UI testing. Except for all the other ones. \n"", ""\nAt the risk of going off on a tangent, I'd recommend Axe/WatiN.  Axe allows tests to be written in Excel by 'Manual' Testers with no knowledge of the underlying test 'language'.  It does need a 'Technician' to write the bespoke actions (IE. Today I had to do a slightly complex Table lookup & cross-reference) but once written the actions can be used in tests by the non-techy testers.\nI also heard that the UK Government Gateway project (which I believe has 6K+ tests automated tests) recently ported all their tests from Axe/Winrunner to Axe/Watin within a week!! And many of the tests are pretty complex - I know as I worked on it a few years ago...\nI'm looking at Selenium at the moment, as a potential Client uses it.  But i do suggest a wee look at Axe as a layer above the 'work horse' tool.\n"", ""\nIf you have to access iframes, modal dialogs and cross domain iframes WatiN is a way to go.  Selenium couldn't handle the iframes it was throwing commandtimeout exceptions.  WatiN you could do lot more things especially if the website uses IE specific stuff like ShowModalDialog etc.. WatiN handles all of them very well.  I could even do cross domain iframe access.\n"", '\nYou will have to do both if you need to do IE and FF testing, but they are only going to work so well for presentation testing. They cant detect if one element is slightly off, just that the elements are present. I dont know of anything that can replace the human eye for UI / presentation testing, though you could do a few things to assist it (take screenshots of the pages at each step for users to review). \n']",https://stackoverflow.com/questions/417380/watin-or-selenium,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
find and delete file or folder older than x days,"
I want to delete file and folder older than 7 days so I tried 
[17:07:14 root@client01.abc.com:~]# find /tmp/ -mindepth 1 -maxdepth 1 -ctime +7 -exec ls -l {} \;

So when I run find /tmp/ -mindepth 1 -maxdepth 1 -ctime +7 -exec ls -l {} \; it doesnt show any dir, but for find /tmp/ -mindepth 1 -maxdepth 2 -ctime +7 -exec ls -l {} \; it does show few files in subdir.
Whats is the right way to delete files/folders older than 7 days in one specific dir ?
",127k,"
            37
        ","['\nYou can make use of this piece of code\nfind /tmp/* -mtime +7 -exec rm {} \\;\n\nExplanation\n\nThe first argument is the path to the files. This can be a path, a directory, or a wildcard as in the example above. I would recommend using the full path, and make sure that you run the command without the exec rm to make sure you are getting the right results.\n\nThe second argument, -mtime, is used to specify the number of days old that the file is. If you enter +7, it will find files older than 7 days.\n\nThe third argument, -exec, allows you to pass in a command such as rm. The {} \\; at the end is required to end the command.\n\n\nSource : http://www.howtogeek.com/howto/ubuntu/delete-files-older-than-x-days-on-linux/\nFor deleting folders, after emptying inside of them you can rmdirinstad of rm in the piece of code, also if you only want to see directories you can add\n-type d\n\nto piece of code such as below:\nfind /tmp/*/* -mtime +7 -type d -exec rmdir {} \\;\n\n', '\nEasier to just do \nfind /tmp/* -mtime +7 -exec rm -rf {} \\; \n\nWhich will del files and dirs\n', '\nmy easy way:\nfind /tmp/* -daystart -mtime +7 -delete\n\nthe daystart option measure times from the beginning of today rather than from 24 hours ago\nref: official_doc\n', '\nfind /tmp/* -mtime +7 -type f -exec rm {} \\;\n\nRemove files.\nfind /tmp/ -empty -type d -delete\n\nRemove empty directories.\n']",https://stackoverflow.com/questions/31389483/find-and-delete-file-or-folder-older-than-x-days,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What is the best automated website UI testing framework [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 9 years ago.



What are the good automated web UI testing tools?
I want to be able to use it in the .Net world - but it doesn't have to written in .net.
Features such as a record mode, integration into build process\ continuous integration would be nice.
Im going to look at:

Watir
Selenium

Are there any others I should look at?
",22k,"
            34
        ","[""\nI definitively recommend Selenium, you can use it from .NET, supports different browsers, works in automatic builds and CI processes (we use it from CCNet). The code is stable. It has a few quirks, but after all they all do.\nWhichever tool you choose, I recommend making your own test facade class(es) around it. The facade should be designed to suite your concrete testing needs, without exposing too much the details of the testing tool's API. This will make the test code easier to write and maintain.\nUPDATE: if you use ASP.NET view state in your app, you could have problems using a pure HTTP test tool. This is where browser-controllers (like Selenium) are much better.\n"", '\nWatiN\nAutomates FF and IE\n[Test] \npublic void SearchForWatiNOnGoogle()\n{\n using (IE ie = new IE(""http://www.google.com""))\n {\n  ie.TextField(Find.ByName(""q"")).TypeText(""WatiN"");\n  ie.Button(Find.ByName(""btnG"")).Click();\n\n  Assert.IsTrue(ie.ContainsText(""WatiN""));\n }\n}\n\nhttp://watin.sourceforge.net/\n', '\nWatin is pretty unstable to use it in serious projects. It often fails with unexpected reasons like ""EI is busy"" or something like ""Error with COM object"". \nSelenium is much more stable and it already has a lot of supporting tools. For example Selenium GRID is a solution which allows significantly decrease run time of tests. (Our smoke tests on Watin takes 6 hours to run).\n', '\nCurrently in my job i use QTP and it so far atleast can handle pretty much anything we throw at it both on the UI and it has a special mode for testing non gui services allowing us to check both and help us narrow down where some problems occur when we change the system. It is in my opinion very configurable and the inclusion of vbscript as its language allows integration with lots and lots of things on windows to allow you  to do pretty much anything you want! For instance we use it to control the excel com object to make custom excel reports of success and failure so the format of the results is the same wether a test was run manually and also on another project used the adodb object to check that when a page submits information to the database that the database contains the correct data for that record!\nAs for integration into the build process i have not tried this myself but it is possible to launch qtp and  a test from a vbs file so i would assume this should be fairly trvial as ms tools tend to allow you to run vbs files pretty easily  from most tools.\nI would reccomend it to anyone assuming you can get someone to buy the license!\n', '\nYou can also try VSTT - http://blogs.msdn.com/b/slumley/archive/2009/05/28/vsts-2010-feature-enhancements-for-web-test-playback-ui.aspx\nTelerik Test Tools - http://www.telerik.com/automated-testing-tools.aspx\nVisual Studio UI Test Extensibility鈥揝cenarios & Guiding Principles - http://blogs.msdn.com/b/mathew_aniyan/archive/2011/03/28/visual-studio-ui-test-extensibility-scenarios-amp-guiding-principles.aspx\nVSTS Web Test Step-by-Step Primer - http://blogs.msdn.com/b/jimmymay/archive/2009/02/23/vsts-web-test-step-by-step-primer-7-minute-video-by-microsoft-a-c-e-performance-engineer-chris-lundquist-with-copious-notes-screen-shots-from-your-humble-correspondent.aspx\n', ""\nyou might also be interested in taking a look at what the ASP.NET team cooked up itself: Lightweight Test Automation Framework.\nThere's also a dedicated forum for it.\n"", ""\nHaving used several different automated testing solutions (TestComplete, QTP, etc), I have to put a vote in for Telerik + Visual Studio.  Telerik has great support forums, and is very compatible with whatever testing framework you come up with.  Our Devs put unique IDs into their HTML code so that our scripts don't need to be rewritten even with pretty drastic UI refactors.  It's definitely more challenging than record and playback, but once you have your unique IDs in place, the automation code requires little or no maintenance.\n"", '\nTry httpunit\n', ""\nDepend on what you would like to achieve. \nYou can use web test built in the Visual Studio Tester Edition. It's quite good and easy to automate. You can use external data as a test data source and it integrates nicely with VS.\nThere is also test tool by Automated QA (forgot the name) which looks good but expensive.\nAnd there is Selenium. That's the one we are using in Symantec. The biggest advantage is that it actually uses a browser you want to test. VS mimic a browser by changing http request parameters only so you may not be able to test your site for cross-browser compatibility. Selenium on the other hand uses browser and automates it so you can actually test your site in IE, Firefox etc. It can be also integrated with VS unit tests so you can see test results in VS.\nSo I would recommend Selenium or VS.\n"", ""\nI've used Selenium. The features were good, and it was usable but it was buggy. \nThe IDE would often record events incorrectly (so tests would need to be manually changed), and test files occasionally became completely unusable for no apparent reason, which meant they would have to be recreated all over again. Also development on Selenium IDE seems to have stopped; there hasn't been any bug fixes and patches for a while, and bug reports seem to go unnoticed. \nMolybdenum is an alternative, built on Selenium that's worth looking into.\nhttp://www.molyb.org/\n"", ""\nJust to throw out another option (of which I haven't tried but I do like Telerik) is Telerik's new WebUI Testing Studio.  I will also echo Selenium up votes.\n"", '\nI forget one nice tools and can find link on it but find this ... http://weblogs.asp.net/bsimser/archive/2008/02/21/automated-ui-testing-with-project-white.aspx maybe can help.\n', '\nIf you are looking for simple, cross-browser tool with record and playback, multithreaded playback, intergration with build processes, powerful scripting, good reporting and excellent support, go for Sahi. It will be much easier for your testers/devs to learn and maintain.\n', '\nyou might want to take in consideration near Selenium also Rational Functional Tester ! whether you are familiar with coding in .Net or Java and want to just play around with record & replay or want to create more sophisticated programmatic testing I would recommend it.\n', '\nWebDriver is another possibility: http://code.google.com/p/webdriver\nThey are working on a .NET wrapper that may be interesting for you.\n', '\nTry QEngine. It has all the features of QTP.\n', '\nYou may want to look at RIATest for cross-platform cross-browser testing of web applications. \nIt works on Windows and Mac, supported browsers are Firefox, IE and Chrome. Automated testing scripts written on one platform/browser can be run against all other supported platforms/browsers.\nIt has the features that you want: user interaction recording mode and integration with CI servers (outputs results in JUnit format which can be consumed by CI servers such as Hudson).\n(Disclaimer: I am a RIATest team member).\n']",https://stackoverflow.com/questions/805910/what-is-the-best-automated-website-ui-testing-framework,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What are the shortcut to Auto-generating toString Method in Eclipse?,"
Is it good or bad practice auto-generating toString methods for some simple classes?
I was thinking of generating something like below where it takes the variable names and produces a toString method that prints the name followed by its value.
private String name;
private int age;
private double height;

public String toString(){
   return String.format(""Name: %s Age: %d Height %f"", name, age, height);
}

",43k,"
            30
        ","[""\nEclipse 3.5.2 (and possibly earlier versions) already provides this feature. If you right-click within the editor, you'll find it under Source -> Generate toString()...\nTo answer your question about whether it's a bad practice to autogenerate toString(), my opinion is that it is not. If the generated code is very similar to the code you would have written yourself, then why bother typing it out?\n"", '\nI personally like to implement a toString method for all objects, as it helps in debugging.\nI would look into using the Apache Commons ToStringBuilder.\nYou can implement a simple toString method using reflection as follows:\npublic String toString() {\n   return ToStringBuilder.reflectionToString(this);\n}\n\nUsing this method, you will not have to update your toString method if/when fields are added.\n', ""\nIf you use lombok they have a @ToString annotation which will generate the toString for you.\nThe reason why this is much better to use instead of generating toString with eclipse for instance is that if you later add,remove or change attributes of the class, you will also have to regenerate the toString. If you use lombok you don't have to do that.\n"", '\nTo add to Steve\'s and Don\'s answers (+1 for them) :\nMake your toString() method simple, make sure it nevers triggers expections (especially be aware of fields that could be null). \nIf possible, don\'t call other methods of your class. At least, be sure that your toString() method doesn\'t modify your object. \nAnd be aware of silly exception-toString loops:\npublic class MyClass { \n       ... \n       public String toString() { \n          // BAD PRACTICE 1: this can throw NPE - just use field1\n            return "" field1="" + field1.toString() \n                + "" extraData="" + getExtraData();\n          // BAD PRACTICE 2: potential exception-toString loop\n       }\n\n       public MyExtraData getExtraData() {\n           try { \n           .... do something\n           } catch(Exception e) {\n              throw new RuntimeException(""error getting extradata - "" + this.toString(),e);\n           }\n\n       }\n\n}\n\n', ""\nIn IntelliJ Idea you can press alt+insert, the Generate popup will open; now select the fields and click the OK button; that's it.\n\n\n\nFurther tip: In the Generate toString dialog, it gives you a choice to select the template by clicking the drop down on the template combo box. Here you can select StringBuffer if you need to or any other template as required. Play with it to get accustomed. I like it :)\n"", '\n\nShortcut to generate toString() method\n\n\nPress Alt + Shift + S + S (double)\nRight click -> Source -> Generate toString() ...\nGo to Source menu -> Generate toString() ...\nGo to Windows menu -> Preferences -> General -> Keys (Write Generate toString on text field)\n\n', ""\nBe clear when adding toString() as to the audience of the generated text.  Some frameworks use the toString() method to generate end user visible text (e.g. certain web frameworks), whereas many people use toString() methods to generate debugging / developer information.  Either way, make sure that you have enough uniqueness in the toString implementation to satisfy your requirements.\nThe default JDK implementation of toString() generates developer info, so that's usually the pattern I recommend if possible, but if you are working on a project with a different idea / expectation you could wind up confused...\n"", '\nJust noticed -In NetBeans IDE you can generate toString() method by selecting fields you want to generate it for right click->insert code or use shortcut ALT+INSERT and then select toString().\nWay it looks is :\n@Override\npublic String toString() {\n    return ""ClassName{""+""fieldName=""+fieldName+\'}\';\n}\n\nIts great way to debug and no need for additional libs.\n', '\nConsidering some old answers including @Steve\'s, I\'d like to add answer as per latest library. \nAdd dependency\n        <dependency>\n            <groupId>org.apache.commons</groupId>\n            <artifactId>commons-lang3</artifactId>\n            <version>3.10</version>\n        </dependency>\n\nIn your class \nimport org.apache.commons.lang3.builder.ReflectionToStringBuilder;\n\npublic class User {\n     ... \n\n     @Override\n     public String toString() {\n          return ReflectionToStringBuilder.toString(this);\n     }\n}\n\nYou can exclude certain fields as below \n    ... \n    @Override\n    public String toString() {\n        return ReflectionToStringBuilder.toStringExclude(this, ""name""); // Name will be excluded from toString output \n    }\n\n']",https://stackoverflow.com/questions/2653268/what-are-the-shortcut-to-auto-generating-tostring-method-in-eclipse,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to add application to Azure AD programmatically?,"
I want to automate the creation of my application in Azure AD and get back the client id generated by Azure AD.
Are there PowerShell commandlets to do this? Is there some other means, like an API of doing this besides the management console?
Can you point me to an example?
Thanks!
",22k,"
            29
        ","['\n', '\n', '\n', '\n']",https://stackoverflow.com/questions/31684821/how-to-add-application-to-azure-ad-programmatically,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Automation Google login with python and selenium shows """"This browser or app may be not secure""""","
I've tried login with Gmail or any Google services but it shows the following ""This browser or app may not be secure"" message:

I also tried to do options like enable less secure app in my acc but it didn't work.
then I made a new google account and it worked with me. but not with my old acc.

how can i solve this ? 
How can i open selenium in the normal chrome browser not the one controlled by automated software
?

This is my code

    from selenium.webdriver import Chrome
    from selenium.webdriver.chrome.options import Options


    browser = webdriver.Chrome()
    browser.get('https://accounts.google.com/servicelogin')
    search_form = browser.find_element_by_id(""identifierId"")
    search_form.send_keys('mygmail')
    nextButton = browser.find_elements_by_xpath('//*[@id =""identifierNext""]') 
    search_form.send_keys('password')
    nextButton[0].click() 

",26k,"
            23
        ","['\nFirst of all don\'t use chrome and chromedriver. You need to use Firefox.(if not installed) Download and install Firefox. Login to Google with normal Firefox.\nYou need to show the Google site that you are not a robot. You can use code like this:\nfrom selenium import webdriver\nimport geckodriver_autoinstaller\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n\ngeckodriver_autoinstaller.install()\n\nprofile = webdriver.FirefoxProfile(\n    \'/Users/<user name>/Library/Application Support/Firefox/Profiles/xxxxx.default-release\')\n\nprofile.set_preference(""dom.webdriver.enabled"", False)\nprofile.set_preference(\'useAutomationExtension\', False)\nprofile.update_preferences()\ndesired = DesiredCapabilities.FIREFOX\n\ndriver = webdriver.Firefox(firefox_profile=profile,\n                           desired_capabilities=desired)\n\nThis can help you find your profile location.\nBut, why Firefox?\nActually there is only one reason, chromedriver was coded by Google.\nThey can easily understand if it is a bot or not. But when we add user data with Firefox, they cannot understand if there is a bot or not.\nYou can fool Google like this. It worked for me too. I tried very hard to do this. Hope it will be resolved in you too.\n', '\nThis is working for me. I found the solution from GitHub.\n   from selenium import webdriver\n   from selenium_stealth import stealth\n\n   options = webdriver.ChromeOptions()\n   options.add_argument(""user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"")\n   options.add_experimental_option(""excludeSwitches"", [""enable-automation""])\n   options.add_experimental_option(\'useAutomationExtension\', False)\n   options.add_argument(\'--disable-blink-features=AutomationControlled\')\n   driver = webdriver.Chrome(options=options)\n   stealth(driver,\n        languages=[""en-US"", ""en""],\n        vendor=""Google Inc."",\n        platform=""Win32"",\n        webgl_vendor=""Intel Inc."",\n        renderer=""Intel Iris OpenGL Engine"",\n        fix_hairline=True,\n        )\n   driver.get(""https://www.google.com"")\n\n', '\nYou can easily bypass the google bot detection with the undetected_chromedriver:\npip install undetected-chromedriver\npip install selenium\n\nCredits: https://github.com/xtekky/google-login-bypass/blob/main/login.py\nimport undetected_chromedriver as uc\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nclass Main:\n  def __init_(self) -> None:\n    self.url    = \'https://accounts.google.com/ServiceLogin\'\n    self.driver = driver = uc.Chrome(use_subprocess=True)\n    self.time   = 10\n    \n  def login(self, email, password):\n    WebDriverWait(self.driver, 20).until(EC.visibility_of_element_located((By.NAME, \'identifier\'))).send_keys(f\'{email}\\n)\n    WebDriverWait(self.driver, 20).until(EC.visibility_of_element_located((By.NAME, \'password\'))).send_keys(f\'{password}\\n)\n                                                                                \n    self.code()\n                                                                                  \n  def code(self):\n    # [ ---------- paste your code here ---------- ]\n    time.sleep(self.time)                                                                                  \n                                                                                  \nif __name__ == ""__main__"":\n  #  ---------- EDIT ----------\n  email = \'email\' # replace email\n  password = \'password\' # replace password\n  #  ---------- EDIT ----------                                                                                                                                                         \n \n  driver = Main()\n  driver.login(email, password) \n\n', '\nFrom terminal pip install undetected-chromedriver\nthen do the following steps, as shown below.\nNOTE: indent your code inside if name == main, as i have done, only then the program will work\nimport undetected_chromedriver as uc\nfrom time import sleep\nfrom selenium.webdriver.common.by import By\n\n\nif __name__ == \'__main__\':\n    \n    driver = uc.Chrome()\n    driver.get(\'https://accounts.google.com/\')\n\n    # add email\n    driver.find_element(By.XPATH, \'//*[@id=""identifierId""]\').send_keys(YOUR EMAIL)\n    driver.find_element(By.XPATH, \'//*[@id=""identifierNext""]/div/button/span\').click()\n    sleep(3)\n    driver.find_element(By.XPATH, \'//*[@id=""password""]/div[1]/div/div[1]/input\').send_keys(YOUR PASSWORD)\n    driver.find_element(By.XPATH, \'//*[@id=""passwordNext""]/div/button/span\').click()\n    sleep(10)\n\n\n\n', '\nIf you\'d prefer Chrome over Firefox, the way to go around Googles automation detection is by using the undetected_chromedriver library.\nYou can install the package using pip install undetected-chromedriver.\nOnce your driver object is initiated you can simply use selenium to operate the driver afterwards.\n# initiate the driver with undetetcted_chromedriver\nimport undetected_chromedriver.v2 as uc\ndriver = uc.Chrome()\n\n# operate the driver as you would with selenium\ndriver.get(\'https://my-url.com\') \n\n# Example use of selenium imports to be used with the driver\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.webdriver.common.by import By\n\ntry:\n    driver.find_element(By.XPATH, \'//*[@id=""my-id""]\').click()\nexcept NoSuchElementException:\n    print(""No Such Element Exception"")\n\nNote: the login of Google is a popup, so don\'t forget to swap window handles to the popup to login and then afterwards to switch back to the main window.\n', '\nYou can use undetected-chromedriver from github\nimport undetected_chromedriver as uc\nfrom selenium import webdriver\n\noptions = uc.ChromeOptions()\noptions.add_argument(""--ignore-certificate-error"")\noptions.add_argument(""--ignore-ssl-errors"")\n# e.g. Chrome path in Mac =/Users/x/Library/xx/Chrome/Default/\noptions.add_argument( ""--user-data-dir=<Your chrome profile>"")\ndriver = uc.Chrome(options=options)\nurl=\'https://accounts.google.com/servicelogin\'\ndriver.get(url)\n\nYour first import need to be undetected Chrome driver.\n', '\nFirstly, I would ask you to use the undetected_chromedriver module\nI have found some issues with the original repo so I have forked a repo that supports user_data_dir and also allows you to provide custom chromedriver path as well.\nUninstall the old module and install this by using git clone and then go the folder and run python setup.py install\nForked repo link : https://github.com/anilabhadatta/undetected-chromedriver\nImport the latest undetected_chromdriver module:\nimport undetected_chromedriver.v2 as ucdriver\nFor using user_data_dir feature write:\noptions.user_data_dir = ""path_to _user-data-dir""\ninstead of using this\noptions.add_argument(f""user_data_dir={path_to _user-data-dir}"")\nprofile_directory name is the same as how we write in selenium\noptions.add_argument(f""--profile-directory=Default"")\nFor using custom chrome path,\noptions.binary_location = chrome_path\nFor using custom chromedriver path,\ndriver = ucdriver.Chrome(executable_path=f\'{path_to_chromedriver}\', options=options) \nRecently Google made some changes for which the authentication part didn\'t work.\nI have tested this in Python 3.9.0, there are reports that it may not work correctly in 3.10.0\nAnd this is tested in both Windows and Linux.\nFinal Code:\ndef load_chrome_driver(headless):\n    chrome_loc = ""/home/ubuntu/Downloads/chromium-browser/""\n    chrome_path = chrome_loc + ""chrome""\n    chromedriver_path = chrome_loc + ""chromedriver""\n    user_data_dir = ""/home/ubuntu/.config/chromium/custom_user""\n    options = webdriver.ChromeOptions()\n    if headless:\n        options.add_argument(\'headless\')\n    options.add_argument(\'--profile-directory=Default\')\n    options.add_argument(""--start-maximized"")\n    options.add_argument(\'--disable-gpu\')\n    options.add_argument(\'--no-sandbox\')\n    options.add_argument(""--disable-dev-shm-usage"")\n    options.add_argument(\'--log-level=3\')\n    options.binary_location = chrome_path\n    options.user_data_dir = user_data_dir\n    driver = ucdriver.Chrome(\n        executable_path=chromedriver_path, options=options)\n    driver.set_window_size(1920, 1080)\n    driver.set_window_position(0, 0)\n    return driver\n\n', '\nGoogle is detecting that you\'re using a bot. You must first of all add this snippet of config (convert it to python since i wrote it in java):\n    options.addArguments(""--no-sandbox"");\n            options.addArguments(""--disable-dev-shm-usage"");\n            options.addArguments(""--disable-blink-features"");\n            options.setExperimentalOption(""excludeSwitches"", Arrays.asList(""enable-automation""));\n            options.addArguments(""--disable-blink-features=AutomationControlled"");\n            options.addArguments(""--disable-infobars"");\n\n        options.addArguments(""--remote-debugging-port=9222"");\n\noptions.setCapability(CapabilityType.UNEXPECTED_ALERT_BEHAVIOUR, UnexpectedAlertBehaviour.IGNORE);\n\ndriver.executeScript(""Object.defineProperty(navigator, \'webdriver\', {get: () => undefined})"");\n\nThe last instruction is the key one, when you launch Selenium, the navigator var of Chrome is set to \'true\', that means that the browser is controlled by a bot, setting it with JS to undefined renders it a ""normal browser"" in view of Google.\nAnother important thing you should do is to change the HEX of the chromedriver binary, i can PM you my already modified one (ubuntu environment), if you trust me obv.\n']",https://stackoverflow.com/questions/66209119/automation-google-login-with-python-and-selenium-shows-this-browser-or-app-may,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Programmatically building htpasswd,"
Is there a programmatic way to build htpasswd files, without depending on OS specific functions (i.e. exec(), passthru())?
",21k,"
            22
        ","[""\n.httpasswd files are just text files with a specific format depending on the hash function specified. If you are using MD5 they look like this:\nfoo:$apr1$y1cXxW5l$3vapv2yyCXaYz8zGoXj241\n\nThat's the login, a colon, ,$apr1$, the salt and 1000 times md5 encoded as base64. If you select SHA1 they look like this:\nfoo:{SHA}BW6v589SIg3i3zaEW47RcMZ+I+M=\n\nThat's the login, a colon, the string {SHA} and the SHA1 hash encoded with base64.\nIf your language has an implementation of either MD5 or SHA1 and base64 you can just create the file like this:\n<?php\n\n$login = 'foo';\n$pass = 'pass';\n$hash = base64_encode(sha1($pass, true));\n\n$contents = $login . ':{SHA}' . $hash;\n\nfile_put_contents('.htpasswd', $contents);\n\n?>\n\nHere's more information on the format:\nhttp://httpd.apache.org/docs/2.2/misc/password_encryptions.html\n"", ""\nFrom what it says on the PHP website, you can use crypt() in the following method:\n<?php\n\n// Set the password & username\n$username = 'user';\n$password = 'mypassword';\n\n// Get the hash, letting the salt be automatically generated\n$hash = crypt($password);\n\n// write to a file\nfile_set_contents('.htpasswd', $username ':' . $contents);\n\n?>\n\nPart of this example can be found: http://ca3.php.net/crypt\nThis will of course overwrite the entire existing file, so you'll want to do some kind of concatination.\nI'm not 100% sure this will work, but I'm pretty sure.\n"", ""\nTrac ships with a Python replacement for htpasswd, which I'm sure you could port to your language of choice: htpasswd.py.\n""]",https://stackoverflow.com/questions/39916/programmatically-building-htpasswd,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Learning and Understanding the Xcode Build System,"
Alright, I'm curious about the build process with Xcode. Setting up multiple Targets, how to automate versioning and generally understanding the system so I can manipulate it to do what I want. 
Does anyone have any books or can point me to some documentation somewhere so that I can figure all of this out? 
Thanks a ton.
Another thing, if anyone actually sees this since changing it.
But any books anyone is aware of that will focus on Xcode 4? There's Xcode 3 Unleashed but I'd be real curious if there are any books that focus heavily on Xcode 4. 
",10k,"
            20
        ","['\nOne thing that is really essential for consistent, reproducible, automatable builds is knowledge of the xcodebuild command.  Sadly I can\'t find any official docs on it apart from the manpage (type man xcodebuild).  There\'s a useful guide to automating iphone builds here that includes building with xcodebuild and versioning with agvtool.  This is just as relevant to general building of Mac apps.\nGenerally building with xcodebuild is very simple: \ncd project_dir\nxcodebuild -project myproject.xcodeproj -configuration Release ARCHS=""x86_64 i386"" build\n\nOnce you can build from a script like this it\'s very easy to slot into an automated build system. \n', '\nXcode build process\n[LLVM], Clang LLVM, Swift LLVM\nXcode uses xcodebuild internally[Example]\n\nObjective-C language\n1. Preprocessing\n[Driver][Preprocessing][Parsing and Semantic Analysis] Parser `.m -> AST`\n    2.Compiling by compiler(Clang)\n    [Code Generation and Optimization](GCC_OPTIMIZATION_LEVEL) LLVM IR Generation `AST -> LLVM IR`. \n    3. Assembling\n    [LLVM backend] LLVM `LLVM IR -> .o`\n4. Static Linking(ld)\n5. Output binary\n\nXcode and Objective-C exposes some of these steps:\n\nPreprocessing:\n\n\nReplace macros\n\nSplits .h and .m.\nIn Xcode, you can look at the preprocessor output of .m file by selecting\n  select .m file -> Product -> Perform Action -> Preprocess\n\n\n\n\nCompiling - translates a low-level intermediate code.\nOften you can see this file when debug a code that you are not owned. Xcode allows you to review the output.\n select .m file -> Product -> Perform Action -> Assemble\n\n\nAssembling(produce .o) - translates code into object file (.o file)[Mach-O] In Xcode, you鈥檒l find these object files inside the <product_name>.build/Objects-normal folder inside the derived data directory.\n\nStatic Linking(produce .app, .a, .framework ...)[About] - It is a part of static linker that has to resolve symbols between object files and libraries/frameworks. This process produce a merged executable file which can contain additional resources and dynamic binary\n\nOutput binary\n\n\nSwift language\nLVVM Frontend\n\n1. Preprocessing\n[Parse module] Parser `.m -> AST`\n[Sema module] Type checking > type-checks AST and annotates it with type information\n    2.Compiling by compiler(Swiftc)\n    [SILGen module] SIL Generator `AST -> raw SIL` > optimizations\n        `Guaranteed Optimization Passes`, `Diagnostic Passes` `raw SIL -> canonical SIL. This optimization is applied in any case \n        `Optimization Passes`(SWIFT_OPTIMIZATION_LEVEL) \n    [IRGen] LLVM IR Generation\n\nLVVM Backend\n    3. Assembling\n    [LLVM backend] LLVM `LLVM IR -> .o`\n4. Static Linking(ld)\n5. Output binary\n\nSwift Example\nimport Foundation\n\nclass ClassA {\n    func foo(param: String) -> Int {\n        return 1\n    }\n}\n\nAST\n-dump-parse            Parse input file(s) and dump AST(s)\n-dump-ast              Parse and type-check input file(s) and dump AST(s)\n-print-ast             Parse and type-check input file(s) and pretty print AST(s)\n\n# xcrun swiftc -dump-parse ""ClassA.swift"" \n\n(source_file ""ClassA.swift""\n  (import_decl range=[ClassA.swift:8:1 - line:8:8] \'Foundation\')\n  (class_decl range=[ClassA.swift:10:1 - line:14:1] ""ClassA""\n    (func_decl range=[ClassA.swift:11:5 - line:13:5] ""foo(param:)""\n      (parameter ""self"")\n      (parameter_list range=[ClassA.swift:11:13 - line:11:27]\n\n...\n\n#xcrun swiftc -dump-ast ""ClassA.swift"" \n\n(source_file ""ClassA.swift""\n  (import_decl range=[ClassA.swift:8:1 - line:8:8] \'Foundation\')\n  (class_decl range=[ClassA.swift:10:1 - line:14:1] ""ClassA"" interface type=\'ClassA.Type\' access=internal non-resilient\n    (func_decl range=[ClassA.swift:11:5 - line:13:5] ""foo(param:)"" interface type=\'(ClassA) -> (String) -> Int\' access=internal\n      (parameter ""self"")\n\n...      \n\n# xcrun swiftc -print-ast ""ClassA.swift""\nimport Foundation\n\ninternal class ClassA {\n  internal func foo(param: String) -> Int\n  @objc deinit\n  internal init()\n}\n\n#xcrun swiftc -frontend -emit-syntax ""ClassA.swift"" | python -m json.tool\n\n{\n    ""kind"": ""SourceFile"",\n    ""layout"": [\n        {\n            ""kind"": ""CodeBlockItemList"",\n            ""layout"": [\n                {\n                    ""kind"": ""CodeBlockItem"",\n                    ""layout"": [\n                        {\n                            ""kind"": ""ImportDecl"",\n                            ""layout"": [\n                                {\n                                   \n...\n\nSIL\n-emit-silgen           Emit raw SIL file(s)\n-emit-sil              Emit canonical SIL file(s)\n\n# xcrun swiftc -emit-silgen  ""ClassA.swift""                      \n\nsil_stage raw\n\nimport Builtin\nimport Swift\nimport SwiftShims\n\nimport Foundation\n\nclass ClassA {\n  func foo(param: String) -> Int\n  @objc deinit\n  init()\n}\n\n// ClassA.foo(param:)\nsil hidden [ossa] @$s6ClassAAAC3foo5paramSiSS_tF : $@convention(method) (@guaranteed String, @guaranteed ClassA) -> Int {\n// %0 ""param""                                     // user: %2\n\n\nsil_vtable ClassA {\n  #ClassA.foo: (ClassA) -> (String) -> Int : @$s6ClassAAAC3foo5paramSiSS_tF // ClassA.foo(param:)\n  #ClassA.init!allocator: (ClassA.Type) -> () -> ClassA : @$s6ClassAAACABycfC // ClassA.__allocating_init()\n  #ClassA.deinit!deallocator: @$s6ClassAAACfD // ClassA.__deallocating_deinit\n}\n\n\n\n// Mappings from \'#fileID\' to \'#filePath\':\n//   \'ClassA/ClassA.swift\' => \'ClassA.swift\'\n\n...\n\nLLVM IR\n-emit-ir               Emit LLVM IR file(s)\n\nxcrun swiftc -emit-ir  ""ClassA.swift""\n; ModuleID = \'<swift-imported-modules>\'\nsource_filename = ""<swift-imported-modules>""\ntarget datalayout = ""e-m:o-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128""\ntarget triple = ""x86_64-apple-macosx12.0.0""\n\n%objc_class = type { %objc_class*, %objc_class*, %swift.opaque*, %swift.opaque*, %swift.opaque* }\n\ndefine i32 @main(i32 %0, i8** %1) #0 {\nentry:\n  %2 = bitcast i8** %1 to i8*\n  ret i32 0\n}\n\n...\n\n-emit-object           Emit object file(s) (-c)\n-emit-executable       Emit a linked executable\n\n[Build With Timing Summary]\nAlso you can use Xcode Report Navigator to learn more about build process. Moreover Xcode v14 includes a great feature to visualisation and analyzation of build process\nShow the Report navigator -> <select build> -> <right_click> -> Show in Timeline\n\n//or\n\nShow the Report navigator -> <select build> -> Editor -> Open Timeline\n\n\n']",https://stackoverflow.com/questions/5490048/learning-and-understanding-the-xcode-build-system,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Outlook email draft using PowerShell,"
I'm creating a PowerShell script to automate a process at work.  This process requires an email to be filled in and sent to someone else.  The email will always roughly follow the same sort of template however it will probably never be the same every time so I want to create an email draft in Outlook and open the email window so the extra details can be filled in before sending.
I've done a bit of searching online but all I can find is some code to send email silently. The code is as follows:
$ol = New-Object -comObject Outlook.Application  
$mail = $ol.CreateItem(0)  
$Mail.Recipients.Add(""XXX@YYY.ZZZ"")  
$Mail.Subject = ""PS1 Script TestMail""  
$Mail.Body = ""  
Test Mail  
""  
$Mail.Send() 

In short, does anyone have any idea how to create and save a new Outlook email draft and immediately open that draft for editing?
",41k,"
            19
        ","['\nBased on the other answers, I have trimmed down the code a bit and use\n$ol = New-Object -comObject Outlook.Application\n\n$mail = $ol.CreateItem(0)\n$mail.Subject = ""<subject>""\n$mail.Body = ""<body>""\n$mail.save()\n\n$inspector = $mail.GetInspector\n$inspector.Display()\n\nThis removes the unnecessary step of retrieving the mail from the drafts folder.  Incidentally, it also removes an error that occurred in Shay Levy\'s code when two draft emails had the same subject.\n', '\n$olFolderDrafts = 16\n$ol = New-Object -comObject Outlook.Application \n$ns = $ol.GetNameSpace(""MAPI"")\n\n# call the save method yo dave the email in the drafts folder\n$mail = $ol.CreateItem(0)\n$null = $Mail.Recipients.Add(""XXX@YYY.ZZZ"")  \n$Mail.Subject = ""PS1 Script TestMail""  \n$Mail.Body = ""  Test Mail  ""\n$Mail.save()\n\n# get it back from drafts and update the body\n$drafts = $ns.GetDefaultFolder($olFolderDrafts)\n$draft = $drafts.Items | where {$_.subject -eq \'PS1 Script TestMail\'}\n$draft.body += ""`n foo bar""\n$draft.save()\n\n# send the message\n#$draft.Send()\n\n', ""\nI think Shay Levy's answer is almost there: the only bit missing is the display of the item.\nTo do this, all you need is to get the relevant inspector object and tell it to display itself, thus:\n$inspector = $draft.GetInspector  \n$inspector.Display()\n\nSee the MSDN help on GetInspector for fancier behaviour.\n"", '\nif you want to use HTML template please use HTMLbody instead of Body , please find sample code below:\n$ol = New-Object -comObject Outlook.Application\n$mail = $ol.CreateItem(0)\n$mail.Subject = ""Top demand apps-SOURCE CLARIFICATION""\n$mail.HTMLBody=""<html><head></head><body><b>Joseph</b></body></Html>""\n$mail.save()\n\n$inspector = $mail.GetInspector\n$inspector.Display()\n\n', '\nThought I would add in to this as well. There are a few steps you can save yourself if you know a lot of the basics (subject, recipients, or other aspects). First create the template of the email and save that, e.g. somewhere maybe with the code? \nAs to the code itself, it follows much the same that others have posted.  \nBorrowing from Jason:\n$ol = New-Object -comObject Outlook.Application\n$msg = $ol.CreateItemFromTemplate(<<Path to template file>>)\n\nModify as needed. Append fields or modify body. The message can still be viewed prior to sending the same way $msg.GetInspector.Display(). Then call $msg.send() to send away!   \n']",https://stackoverflow.com/questions/1453723/create-outlook-email-draft-using-powershell,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GMail is blocking login via Automation (Selenium),"
I am using selenium to automate a mail verification process in a web application. I have a script already in place to login to gmail and read an activation mail received on the account. The script was perfectly working till yesterday but today I am facing a problem.

Additional Screenshot of issue

Gmail is not allowing sign in if the browser is launched with selenium. Says, 

You're using a browser that Google doesn't recognize or that's setup in a way that we don't support.


I have tried upgrading chromedriver version to 76.0.0 as I am using
chrome version 76.0.3809.100(64 bit). (Previously used chromedriver
2.45) Still, the problem persists.
Verified that this issue occurs even if I use Firefox instead of Chrome for automation.
Verified that Javascript is enabled in the browser
Gmail is not asking for any OTP or recovery mail. It is simply
blocking my attempt to login via automation. However I am able to
login to the same account manually.


Software used:  ""webdriverio"": ""^4.14.1"", ""wdio-cucumber-framework"":
  ""^2.2.8""

Any help is appreciated.
",38k,"
            18
        ","['\nAfter some trial and error, found out that this issue happens only in a scenario when multiple gmail accounts have already been created from the same App/IP/Device. Google somehow is marking those accounts and blocks them if they are launched by automation frameworks/extensions. \nTemporary Solutions:\n\nCreate a fresh GMail account using a\ndifferent mobile number from another device (Not recommended).\nWe should be using workarounds like nodemailer\nzeolearn.com/magazine/sending-and-receiving-emails-using-nodejs\n(as mentioned by Rahul L as a suggestion)\nAutomate temporary mail providers like Guerilla Mail or 10 Minute Mail if you are\nworried about only receiving mails\n\nMy humble opinion is to entirely avoid automating the UI of third party Mail applications as you cannot predict how their UI and elements will change. They might block you from launching for security purposes and they have every right to do so!\n', ""\nI just tried something out that worked for me after several hours of trial and error.\nAdding args: ['--disable-web-security', '--user-data-dir', '--allow-running-insecure-content' ] to my config resolved the issue.\nI realized later that this was not what helped me out as I tried with a different email and it didn't work. After some observations, I figured something else out and this has been tried and tested.\nUsing automation:\nGo to https://stackoverflow.com/users/login\nSelect Log in with Google Strategy\nEnter Google username and password\nLogin to Stackoverflow\nGo to https://gmail.com (or whatever Google app you want to access)\nAfter doing this consistently for like a whole day (about 24 hours), try automating your login directly to gmail (or whatever Google app you want to access) directly... I've had at least two other people do this with success.\nPS - You might want to continue with the stackoverflow login until you at least get a captcha request as we all went through that phase as well.\n"", '\nGo to Gmail --> Settings --> Search for ""Less secure app access"" --> Enable\nThis is not a suggested method because it allows access to less secure apps and it\'s easier to get into your account but if it\'s just a testing account and no important data is being transferred to or from, you can give this a try.\n', '\nYou need to open your editor and copy this code and paste them and save with this name as email.py and then open your terminal/cmd/powershell in that directory and type this python\xa0.\\email.py\nNote:\nMake sure your chrome driver in that directory where you save python file\nYou need to copy this code and paste in your editor\nHere is that script:\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport time\nimport pyautogui as pg\n\n\nusername = input(""Enter Your Username: "")\n\npassword = input(""Enter Your Password: "")\n\ndriver = webdriver.Chrome()\ndriver.maximize_window()\ndriver.get(""https://accounts.google.com/ServiceLogin?service=mail&passive=true&rm=false&continue=https://mail.google.com/mail/&ss=1&scc=1&ltmpl=default&ltmplcache=2&emr=1&osid=1#identifier"")\ndriver.maximize_window()\n\n\nmail = WebDriverWait(driver, 100).until(EC.element_to_be_clickable((By.XPATH, ""//*[@id=\'identifierId\']""))).send_keys(username)\n\nlogin = WebDriverWait(driver, 100).until(EC.element_to_be_clickable((By.XPATH, ""//*[@id=\'identifierNext\']/span""))).click()\n\npassw = WebDriverWait(driver, 100).until(EC.element_to_be_clickable((By.XPATH, ""//*[@id=\'password\']/div[1]/div/div[1]/input""))).send_keys(password)\n\nnext = WebDriverWait(driver, 100).until(EC.element_to_be_clickable((By.XPATH, ""//*[@id=\'passwordNext\']/span/span""))).click()\n\n', '\nSteps to login to gmail through stackoverflow :\n\nOpen a browser window and open stackoverflow\nClick on log in\nLogin with google\nEnter email and password\nStackoverflow is logged in with gmail credentials\nNow open mail.google.com (gmail.com)\nYou are now logged into gmail using selenium\n\nYou can copy this code and paste in your editor, you have to enter the path of your chrome driver, email address and password for your gmail where it is asked in the code.\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nimport time\n\ndriver=webdriver.Chrome(\'Enter the path of the chrome driver here\')\ndriver.get(""https://stackoverflow.com/"")\n\ndriver.maximize_window()\n\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'/html/body/header/div/ol[2]/li[2]/a[1]\').click()#Log in button in stackoverflow\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'//*[@id=""openid-buttons""]/button[1]\').click()# Log in with Google button\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'//*[@id=""identifierId""]\').send_keys(""Enter email address"")# Enter email address\n\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'//*[@id=""identifierNext""]/div/button/div[2]\').click() # Click next button after entering email address\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'//*[@id=""password""]/div[1]/div/div[1]/input\').send_keys(""Enter password"")#Enter password\n\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'//*[@id=""passwordNext""]/div/button/div[2]\').click()# Click on next button after entering the password\n\ntime.sleep(5)\n\ndriver.get(""https://mail.google.com"")\n\ntime.sleep(5)\n\ndriver.close()\n\n', '\nFound solution, add these arguements:\noptions.AddArguments(""--disable-web-security"", ""--user-data-dir=true"", ""--allow-running-insecure-content"");\n^^  C# btw\n', '\nI think this is not a problem with the automation framework nor with the automated Chrome application, it is Google server that blocks automated login requests. I don\'t have any clear evidence/really understand how they can do it, but several clues point me to this direction:\n\nFirst, on the login page, after entering your user name and clicking next, a user lookup request is fired, something likes https://accounts.google.com/_/lookup/accountlookup?hl=vi&_reqid=xxxxx&rt=j, the response of this request includes something likes https://accounts.google.com/signin/rejected?rrk=46&hl=vi. In subsequent requests, the browser just tries to resolve & display what the error means to the user.\nSecond, it was stated here that ""Google doesn鈥檛 let you sign in from some browsers. Google might stop sign-ins from browsers that are being controlled through software automation rather than a human"". This means Googles has implemented measures to detect requests coming automated browser, especially Chrome-family browsers in debug mode, which are used by many automation frameworks.\nA thread about similar issue with Taiko also mentioned that Google blocks requests from Chrome running in debug mode.\n\nRecently I tried with Cypress and Taiko, but none works, same ""rejected"" issue, which ruined my initial plan of doing an e2e test for my app (which uses GG OIDC login).\n', '\nBelow code worked for me by using selenium-stealth module and editing chromedriver exe\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium import webdriver \nimport time\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\nfrom selenium_stealth import stealth\nchrome_options = Options()\nchrome_options.add_experimental_option(""useAutomationExtension"", False)\nchrome_options.add_experimental_option(""excludeSwitches"",[""enable-automation""])\nchrome_options.add_argument(""--start-maximized"")\nchrome_options.add_argument(\'--disable-logging\')\nchrome_options.add_argument(\'--no-sandbox\')\nchrome_options.add_argument(\'--disable-dev-shm-usage\')\nchrome_options.add_argument(\'--disable-blink-features=AutomationControlled\')\ncaps = DesiredCapabilities.CHROME\ncaps[\'goog:loggingPrefs\'] = {\'performance\': \'ALL\'}\n\n# for editing chromedriver exe so that its not detected(run only once)\nwith open(""chromedriver.exe"",""rb"") as d:\n    file_data=d.read()\nfile_data=file_data.replace(b""cdc_"",b""tch_"")\nwith open(""chromedriver.exe"",""wb"") as d:\n    d.write(file_data)\ndriver = webdriver.Chrome(\'chromedriver.exe\',chrome_options=chrome_options,desired_capabilities=caps)\n\n# for injecting js to that tab so that browser not detected\nstealth(driver,languages=[""en-US"", ""en""],vendor=""Google Inc."",platform=""Win32"",webgl_vendor=""Intel Inc."",renderer=""Intel Iris OpenGL Engine"",fix_hairline=True,)\ndriver.get(""https://accounts.google.com"")\ntime.sleep(3)\ndriver.switch_to.active_element.send_keys(""myemail@gmail.com\\n"")\ntime.sleep(3)\ndriver.switch_to.active_element.send_keys(""mypassword\\n"")\n\n']",https://stackoverflow.com/questions/57602974/gmail-is-blocking-login-via-automation-selenium,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can a Perl script install its own CPAN dependencies?,"
I have a Perl script that has two dependencies that exist in CPAN. What I'd like to do is have the script itself prompt the user to install the necessary dependencies so the script will run properly. If the user needs to enter in some kind of authentication to install the dependencies that's fine: what I'm trying to avoid is the following workflow:

Run script -> Watch it fail -> Scour CPAN aimlessly -> Lynch the script writer

Instead I'm hoping for something like:

Run script -> Auto-download script dependencies (authenticating as necessary) -> Script succeeds -> Buy the script writer a beer

Can this be done?
",3k,"
            17
        ","[""\nEach of the standard build paradigms has their own way of specifying dependencies. In all of these cases, the build process will attempt to install your dependencies, automatically in some contexts.\nIn ExtUtils::MakeMaker, you pass a hash reference in the PREREQ_PM field to WriteMakefile:\n# Makefile.PL for My::Module\nuse ExtUtils::MakeMaker;\n\nWriteMakefile (\n    NAME => 'My::Module',\n    AUTHOR => ...,\n    ...,\n    PREREQ_PM => {\n        'Some::Dependency' => 0,             # any version\n        'Some::Other::Dependency' => 0.42,   # at least version 0.42\n        ...\n    },\n    ...\n );\n\nIn Module::Build, you pass a hashref to the build_requires field:\n# Build.PL\nuse Module::Build;\n...\nmy $builderclass = Module::Build->subclass( ... customizations ... );\nmy $builder = $builderclass->new(\n    module_name => 'My::Module',\n    ...,\n    build_requires => {\n        'Some::Dependency' => 0,\n        'Some::Other::Dependency' => 0.42,\n    },\n    ...\n);\n$builderclass->create_build_script();\n\nIn Module::Install, you execute one or more requires commands before calling the command to write the Makefile:\n# Makefile.PL\nuse inc::Module::Install;\n...\nrequires 'Some::Dependency' => 0;\nrequires 'Some::Other::Dependency' => 0.42;\ntest_requires 'Test::More' => 0.89;\n...\nWriteAll;\n\n"", ""\nYou can probably just execute this from inside your script.\nperl -MCPAN -e 'install MyModule::MyDepends'\n""]",https://stackoverflow.com/questions/7664829/can-a-perl-script-install-its-own-cpan-dependencies,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
win32: simulate a click without simulating mouse movement?,"
I'm trying to simulate a mouse click on a window. I currently have success doing this as follows (I'm using Python, but it should apply to general win32):
win32api.SetCursorPos((x,y))
win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN,0,0)
win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP,0,0)

This works fine. However, if the click happens while I'm moving the mouse manually, the cursor position gets thrown off. Is there any way to send a click directly to a given (x,y) coordinate without moving the mouse there? I've tried something like the following with not much luck:
nx = x*65535/win32api.GetSystemMetrics(0)
ny = y*65535/win32api.GetSystemMetrics(1)
win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN | \
                     win32con.MOUSEEVENTF_ABSOLUTE,nx,ny)
win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP | \
                     win32con.MOUSEEVENTF_ABSOLUTE,nx,ny)

",21k,"
            10
        ","['\nTry WindowFromPoint() function:\nPOINT pt;\n    pt.x = 30; // This is your click coordinates\n    pt.y = 30;\n\nHWND hWnd = WindowFromPoint(pt);\nLPARAM lParam = MAKELPARAM(pt.x, pt.y);\nPostMessage(hWnd, WM_RBUTTONDOWN, MK_RBUTTON, lParam);\nPostMessage(hWnd, WM_RBUTTONUP, MK_RBUTTON, lParam);\n\n', ""\nThis doesn't answer the question, but it does solve my problem:\nwin32api.ClipCursor((x-1,y-1,x+1,y+1))\nwin32api.SetCursorPos((x,y))\nwin32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN| \\\n                     win32con.MOUSEEVENTF_ABSOLUTE,0,0)\nwin32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP| \\\n                     win32con.MOUSEEVENTF_ABSOLUTE,0,0)\nwin32api.ClipCursor((0,0,0,0))\n\nThe result is that any movements I'm making won't interfere with the click. The downside is that my actual movement will be messed up, so I'm still open to suggestions.\n""]",https://stackoverflow.com/questions/3720968/win32-simulate-a-click-without-simulating-mouse-movement,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UI Automation ""Selected text""","
Anyone knows how to get selected text from other application using UI Automation and .Net?
http://msdn.microsoft.com/en-us/library/ms745158.aspx
",5k,"
            9
        ","['\nprivate void button1_Click(object sender, EventArgs e) {\n        Process[] plist = Process.GetProcesses();\n\n        foreach (Process p in plist) {\n            if (p.ProcessName == ""notepad"") {\n\n                AutomationElement ae = AutomationElement.FromHandle(p.MainWindowHandle);\n\n                AutomationElement npEdit = ae.FindFirst(TreeScope.Descendants, new PropertyCondition(AutomationElement.ClassNameProperty, ""Edit""));\n\n                TextPattern tp = npEdit.GetCurrentPattern(TextPattern.Pattern) as TextPattern;\n\n                TextPatternRange[] trs;\n\n                if (tp.SupportedTextSelection == SupportedTextSelection.None) {\n                    return;\n                }\n                else {\n                    trs = tp.GetSelection();\n                    lblSelectedText.Text = trs[0].GetText(-1);\n                }\n            }\n        }\n    }\n\n', '\nHere is another solution using only UI Automation.\nIt gets the selected text from Notepad and Wordpad.\n// Get only top level windows\nPropertyCondition condition = new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Window);\nAutomationElementCollection windows = AutomationElement.RootElement.FindAll(TreeScope.Children, condition);\nList<AutomationElement> allDocuments = new List<AutomationElement>();\n\nforeach (AutomationElement window in windows)\n{\n    string className = window.Current.ClassName;\n    if (className == ""Notepad"" || className == ""WordPadClass"")\n    {\n        condition = new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Document);\n        AutomationElementCollection documents = window.FindAll(TreeScope.Children, condition);\n\n        if (documents.Count > 0)\n        {\n            allDocuments.Add(documents[0]);\n        }\n    }\n}\n\n// store all pieces of selected text here\nList<string> selectedText = new List<string>();\n\n// iterate through all documents found\nforeach (AutomationElement document in allDocuments)\n{\n    object patternObj = null;\n    if (document.TryGetCurrentPattern(TextPattern.Pattern, out patternObj) == true)\n    {\n        TextPattern textPattern = patternObj as TextPattern;\n        TextPatternRange[] ranges = textPattern.GetSelection();\n\n        foreach (TextPatternRange range in ranges)\n        {\n            string text = range.GetText(-1);\n            if (text.Length > 0)\n            {\n                selectedText.Add(text);\n            }\n        }\n    }\n}\n\n']",https://stackoverflow.com/questions/517694/ui-automation-selected-text,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VBA Internet Explorer wait for web page to load,"
I know questions like this have been asked before, but mine is a bit different and has been fairly troubling.  What I'm dealing with is a web page with a form with a few events that load more of the page when certain items in input boxes are filled out.  When these events fire the page loads again, but remains at the same URL with the same nameprop.  I've been using the following types of methods both seperately and strung together to handle waiting for the page to load, but sometimes the VBA still manages to continue executing and set the HTMLDocument variable to a page without the appropriate information on it causing the macro to debug.  Here are the kinds of things I've been trying so far:
While IE.Busy
    DoEvents
Wend

Do Until IE.statusText = ""Done""
    DoEvents
Loop

Do Until IE.readyState = 4
    DoEvents
Loop

I've even attempted to place these events into a loop like the following, but it didn't quite work because the lastModified property only returns a value down to the second and the macro spins through the fields fast enough that it is returning a new page in the same second:
Do Until IE.statusText = ""Done"" And IE.Busy = False And IE.ReadyState = 4 _
And IE.document.lastModified > LastModified ----or---- IE.document.nameprop = _
""some known and expected name prop here""
    While IE.Busy
        DoEvents
    Wend
    Do Until IE.statusText = ""Done""
        DoEvents
    Loop
    Do Until IE.readyState = 4
        DoEvents
    Loop
Loop

Even that fails to wait long enough to set the HTMLDocument object leading to a debug.  I've contemplated setting the next input element and checking that for nothing to further the code, but even that wouldn't be successful 100% of the time because generally the Input elements exist in the HTML but are hidden until the appropriate event is fired, which wouldn't be a problem but they don't load their possible selections until after the event is fired.  It might be an odd page.
Anyway... not sure what else to add.  If there is something else that might be helpful to see just ask.  I guess what I'm looking for is a way to get VBA to wait until IE knows another page isn't on it's way.  It seems to load a few times before it is completely done.
So... Anyone have any ideas?
EDIT:  Found a few new things to try.  Still, no dice.  It was suggested that I add these attempts.  Here is the code, for some reason the VBE and excel instance become non-responsive when using this approach after firing an event that should populate the options on the select element... thinking about trying xml... here is the code:
intCounter = 0
Do until intCounter > 2
    Do Until IE.Busy = False: DoEvents: Loop
    Do Until IE.ReadyState = 4: DoEvents: Loop
    Set HTMLDoc = IE.Document
    Do Until HTMLDoc.ReadyState = ""complete""
    Set HTMLSelect = HTMLDoc.getElementById(""ctl00$ctl00$MainContent$ChildMainContent$ddlEmployeeBranchCodes"")
    intCounter = 0
    For each Opt in HTMLSelect
        intCounter = intCounter + 1
    Next Opt
Loop

Based on what I can see happening on the web page, I know that it is somewhere in this loop that the VBE and Excel become non-responsive.  
Hope that helps... I know it didn't help me... Drats.
EDIT:  Just thought I'd add this.  When it comes to automating a web page, for the most part, I no longer use IE.  I've found it's much better, and sidesteps this issue of async stuff entirely, to simply perform the posts and gets yourself.  May not be the best solution depending on what you're trying to do, but it works pretty reliably if you look at the traffic closely and parameterize things well.
",50k,"
            7
        ","[""\nAfter an exhaustive search, I've determined that the AJAX request, javascript code that runs asynchronously in the background isn't something I can get a signal from in any way.  It does seem to trigger some event when it finishes with loading the page, which is an option I'd like to explore in the future.  However, for my purposes I simply used the same code I was already using to fill out the form on my page and I have it loop through each field again to check to see if the values are still correct before clicking the submit button.  It isn't an ideal solution, but it is a workaround that appears to have taken care of my issue in this case. \nI'm not sure if my solution would be applicable to someone else dealing with this issue, but there it is if it helps.  I think workarounds for this issue are going to have to be based on the application and web page in question.\n"", '\nOld question I know, but I think this is a good answer that I haven\'t seen about much...\n\nI had a similar problem; waiting for all the images on a google image search to load (which is a tricky thing since image loads are prompted by AJAX and the user scrolling the page). As has been suggested in the comments; my solution was to wait for a certain element to appear in the viewport (this is an area a little larger than the monitor screen, and is treated as what you can actually ""see"").\nThis is achieved with the getBoundingClientRect method\nDim myDiv As HTMLDivElement: Set myDiv = currPage.getElementById(""fbar"") \n\'myDiv should some element in the page which will only be visible once everything else is loaded\nDim elemRect As IHTMLRect: Set elemRect = myDiv.getBoundingClientRect\nDo Until elemRect.bottom > 0 \'If the element is not in the viewport, then this returns 0\n    DoEvents\n    \'Now run the code that triggers the Ajax requests\n    \'For me that was simply scrolling down by a big number\n    Set elemRect = myDiv.getBoundingClientRect\nLoop\nmyDiv.ScrollIntoView\n\nI explain in detail in the linked answer how this works, but essentially the BoundingClientRect.bottom is equal to 0 until the element is in the vieport.\nThe element is something which is loaded straight away (like a frame/template for the page). But you don\'t actually see it until all the content has been loaded, because it\'s right at the bottom. \nIf that element is indeed the last thing to be loaded (for my google search it was the Show More Results button), then as long as you get it into the viewport when it\'s loaded, you should be able to detect when it appears on the page. .bottom then returns a non-zero value (something to do with the actual position of the element on the page  - I didn\'t really care though for my purposes). I finish off with a .ScrollIntoView, but that\'s not essential.\n', '\nI had the same problem with My Webpage..\nWhat is did is...\nthe fist option is\nWhile Ie.**document**.readystate=""complete""\nDoEvents\nWend\n\nthere were few boxes in which options were loaded after a button click/even fire in another box...I jsst placed code like this..\nDo Until IE.document.getelementbyid(""Next box"").Lenght>0\nDoEvents\nLoop\nApplication.wait Now+Timevalue(""00:00:02)\n\n']",https://stackoverflow.com/questions/19933313/vba-internet-explorer-wait-for-web-page-to-load,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to connect to existing instance of Excel from PowerShell?,"
All examples that automate Excel through PowerShell start with this line:
PS> $Excel = New-Object -Com Excel.Application

This seems to be handling a new instance of Excel, e.g. running $Excel.Visible = $true will show an empty, blank Excel window, not switch to the existing workbook.
If there is already an instance of Excel running, is there a way to connect to it?
",7k,"
            7
        ","['\nInstead of the usual New-Object -ComObject excel.application us this\n$excel = [Runtime.Interopservices.Marshal]::GetActiveObject(\'Excel.Application\')\nRest stays the same. \nOne downside. You will only get the excel ""instances"" started by the same user that will initiate the ps1.  \n', '\nYes, you can access the COM object via HWND [Window handle] using this WIN32 API (AccessibleObjectFromWindow). \n\n(See a SO post sample here of using this api via C#)\n\n.\nYou may have to write an assembly in C# and/or manipulate P/Invoke calls via Powershell.\nYou may give a shot at it & see how it goes.\n']",https://stackoverflow.com/questions/11081317/how-to-connect-to-existing-instance-of-excel-from-powershell,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clicking a button on a page using a Greasemonkey/userscript in Chrome,"
I'm going to be as absolutely verbose here as possible as I've run into a few solutions that didn't end up panning out. Please keep in mind that I don't know Javascript. I know basic HTML and CSS. I don't have any actual programming background but I'm trying to learn bit by bit by researching basic tasks like this. Please talk to me like I'm an idiot. Any lingo I throw around in this post I learned while researching this specific issue. I'm writing this userscript as a personal project and to share with some friends.
What I'm trying to do.
I'm trying to write a userscript for Chrome/Greasemonkey (Chrome is my target browser) that will click the Refresh button on the Battlefield 3 server browser. For those of you that don't know, Battlefield 3 uses a web site paired with a browser plugin for VOIP and actually launching the game via a server browser. The bulk of it seems to be fairly straight forward HTML arranged in tables.
The idea is that when viewing the main page for a server that is full, the script will click the Refresh button every three seconds or so until the page reports an open spot on the server, then stop the refresh loop and click the join server button. I've already got the part of the script running that polls the server current and maximum players then assigns them to their own variables.
At this point I'm trying to get a click to work in the console so I can actually put it to some use in my script and am having zero luck.
The code I'm trying to manipulate.
This is the div for the button that I'm trying to click pulled from the Chrome dev tools:
<div class=""serverguide-header-refresh-button alternate show""> 
<div type=""reset"" class=""common-button-medium-grey"">
<p style=""position: relative;"">
<a href=""/bf3/servers/show/c7088bdc-2806-4758-bf93-2106792b34d8/"">Refresh </a>
</p>
</div>
</div>

(That link is not static. It's a link to a specific server page)
What I've tried.
To actually find the button I'm using getElementsByClassName. It doesn't have a unique ID but the class is unique to that element on this particular page so getElementsByClassName(""serverguide-header-refresh-button"")[0] is pulling the proper div each time. It's getting the script to perform any actual action on the button that's the problem.
document.getElementsByClassName(""serverguide-header-refresh-button"")[0].click();

I now realize this didn't work because it's not a conventional submit button. I don't understand the specifics of the standard here but I get that it doesn't support the .click() method.
function addJQuery(callback) {
  var script = document.createElement(""script"");
  script.setAttribute(""src"", ""http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js"");
  script.addEventListener('load', function() {
    var script = document.createElement(""script"");
    script.textContent = ""("" + callback.toString() + "")();"";
    document.body.appendChild(script);
  }, false);
  document.body.appendChild(script);
}

// the guts of this userscript
function main() {
  unsafeWindow.jQuery('.serverguide-header-refresh-button')[0].click();
}

// load jQuery and execute the main function
addJQuery(main);

This is simply unsafeWindow.jQuery('.serverguide-header-refresh-button').click(); wrapped in some code to load jQuery for userscripts. It was a bit I picked up elsewhere but was told it would only work if jQuery was loaded on the page. I figured it was worth a try. This is one of those I have no idea what I'm doing shots in the dark and it didn't work. I tried the same thing below with another snippet of jQuery code I picked up:
function addJQuery(callback) {
  var script = document.createElement(""script"");
  script.setAttribute(""src"", ""http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js"");
  script.addEventListener('load', function() {
    var script = document.createElement(""script"");
    script.textContent = ""("" + callback.toString() + "")();"";
    document.body.appendChild(script);
  }, false);
  document.body.appendChild(script);
}

// the guts of this userscript
function main() {
   var evObj = document.createEvent('Events');
      evObj.initEvent(""click"", true, false);
      document.getElementsByClassName('serverguide-header-refresh-button')[0].dispatchEvent(evObj);
}

    // load jQuery and execute the main function
addJQuery(main);

Both of these return Undefined in the Chrome and Firebug consoles.
So, would anyone be so kind as to help me create a bit of code for this script to press the Refresh button on this page?
",11k,"
            6
        ","['\nNote:\n\njQuery .click() does not work reliably on click events that were not set via jQuery in the first place.\nYou need to create the right kind of event; createEvent(\'Events\') is not the way.\nAs Jim Deville pointed out, the link pseudo-button was not being selected.\nYou do not need jQuery for this (so far).\nMake sure that the ""Refresh"" control is loaded statically for the test code as shown in the question.  If it\'s AJAXed in, the click attempt may fire too soon.\n\nPutting that all together, the following code should work.  But beware that some sites (like certain Google apps) use funky, state-sensitive designs -- which require a sequence of events.\nvar refreshBtn = document.querySelector (\n    ""div.serverguide-header-refresh-button div[type=\'reset\'] a""\n);\nvar clickEvent = document.createEvent (\'MouseEvents\');\nclickEvent.initEvent (\'click\', true, true);\nrefreshBtn.dispatchEvent (clickEvent);\n\n', '\nso, you are on the right track, but you keep grabbing the DIV not the A tag. To click on the ""button"" (link in this case), you have to click on the actual link because it won\'t tunnel down to the elements contained in the DIV.\ndocument.querySelector(\'serverguide-header-refresh-button a\')\n\nShould get you the A element to click on. From jQuery $(\'serverguide-header-refresh-button a\').click(); should work.\n']",https://stackoverflow.com/questions/8192126/clicking-a-button-on-a-page-using-a-greasemonkey-userscript-in-chrome,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
reusing Internet Explorer COM Automation Object,"
I am using VBScript macros to utilize the InternetExplorer.Application COM automation object and I am struggling with reusing an existing instance of this object.
From what I have read, I should be able to use the GetObject() method in vbscript to grab a hold of an existing instance of this object.
When I execute the following code I get an ""Object creation failed - moniker syntax error"".
Is my issue really syntax? 
Is my issue how I am trying to use this object? 
or can what I am trying to accomplish just not be done?
Code:
Dim IEObject as object

Sub Main  
  Set IEObject =  GetObject( ""InternetExplorer.Application"" )

  'Set the window visable
  IEObject.Visible = True

  'Navigate to www.google.com
  IEObject.Navigate( ""www.google.com"" )
End Sub

Also, I have no problem running the CreateObject() which opens up a new internet explorer window and navigates where i want to, but i would rather not have the macro open up multiple instances of Internet Explorer.
",11k,"
            6
        ","['\nTry This:\n\nSet IEObject =  GetObject( ,""InternetExplorer.Application"" )\n\n*Notice the comma before ""InternetExplorer.Application""\nEDIT:\nTry this:\n\nDim IE As SHDocVw.InternetExplorer\n\nSet IE = GetObject(,""InternetExplorer.Application"")\n\nYou can also try this:\n\nDim ShellApp\nSet ShellApp = CreateObject(""Shell.Application"")\nDim ShellWindows\nSet ShellWindows = ShellApp.Windows()\nDim i\nFor i = 0 To ShellWindows.Count - 1\n    If InStr(ShellWindows.Item(i).FullName, ""iexplore.exe"") <> 0 Then\n        Set IEObject = ShellWindows.Item(i) \n    End If\nNext\nIEObject.Navigate2(""http://www.google.com"")\n\nEDIT: What you are trying may not be possible, take a look at this. http://support.microsoft.com/kb/239470\n']",https://stackoverflow.com/questions/941767/reusing-internet-explorer-com-automation-object,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
simulate backspace key with java.awt.Robot,"
There seems to be an issue simulating the backspace key with java.awt.Robot.
This thread seems to confirm this but it does not propose a solution.
This works:
Robot rob = new Robot();
rob.keyPress(KeyEvent.VK_A);
rob.keyRelease(KeyEvent.VK_A);

This doesn't:
Robot rob = new Robot();
rob.keyPress(KeyEvent.VK_BACK_SPACE);
rob.keyRelease(KeyEvent.VK_BACK_SPACE);

Any ideas?
",10k,"
            5
        ","['\nIt seems to work in this test.\nAddendum: Regarding the cited article, ""Aside from those keys that are defined by the Java language (VK_ENTER, VK_BACK_SPACE, and VK_TAB), do not rely on the values of the VK_ constants. Sun reserves the right to change these values as needed to accomodate a wider range of keyboards in the future.""鈥攋ava.awt.event.KeyEvent\npublic class RobotTest {\n\n    public static void main(String[] args) {\n        EventQueue.invokeLater(new Runnable() {\n            public void run() {\n                new RobotTest().create();\n            }\n        });\n    }\n\n    private void create() {\n        JFrame f = new JFrame();\n        f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n        f.setLocationRelativeTo(null);\n        f.setLayout(new FlowLayout());\n        f.add(new JTextField(8));\n        final JButton b = new JButton();\n        f.getRootPane().setDefaultButton(b);\n        b.addActionListener(new ActionListener() {\n            public void actionPerformed(ActionEvent e) {\n                b.setText(""@"" + e.getWhen());\n            }\n        });\n        f.add(b);\n        f.setSize(256, 128);\n        f.setVisible(true);\n        doTest();\n    }\n\n    private void doTest() {\n        try {\n            Robot r = new Robot();\n            int[] keys = {\n                KeyEvent.VK_T, KeyEvent.VK_E,\n                KeyEvent.VK_S, KeyEvent.VK_T,\n                KeyEvent.VK_Z, KeyEvent.VK_BACK_SPACE,\n                KeyEvent.VK_ENTER\n            };\n            for (int code : keys) {\n                r.keyPress(code);\n                r.keyRelease(code);\n            }\n        } catch (AWTException ex) {\n            ex.printStackTrace(System.err);\n        }\n    }\n}\n\n', '\nThe Backspace functionality does not work as expected. I added a Shift key with the Backspace and it works fine for me, here is the pseudo-code for it.\nrobot.keyPress(KeyEvent.VK_SHIFT);\nrobot.keyPress(KeyEvent.VK_BACK_SPACE);\nrobot.keyRelease(KeyEvent.VK_BACK_SPACE);\nrobot.keyRelease(KeyEvent.VK_SHIFT);\n\nThis does not seem to work for the Delete key though.\n']",https://stackoverflow.com/questions/2596641/simulate-backspace-key-with-java-awt-robot,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to call python file in a feature file of karate automation?,"
I need to call a .py file and pass an argument to one of the functions in it and have to match the result returned.
For instance:
xyz.py
KarateXyz.feature

match result == call('xyz.py')# how to specify the method name and pass an argument in it?

",763,"
            1
        ","[""\nYes, you can use karate.exec() to call any OS process. Whether python is installed is up to you. Refer https://github.com/intuit/karate#karate-exec\n* def result = karate.exec('python foo.py bar')\n\nFor more details, refer: https://stackoverflow.com/a/64352676/143475\n""]",https://stackoverflow.com/questions/66546381/how-to-call-python-file-in-a-feature-file-of-karate-automation,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Auto-reload browser when I save changes to html file, in Chrome?","
I'm editing an HTML file in Vim and I want the browser to refresh whenever the file underneath changes. 
Is there a plugin for Google Chrome that will listen for changes to the file and auto refresh the page every time I save a change to the file? I know there's XRefresh for Firefox but I could not get XRefresh to run at all.
How hard would it be to write a script to do this myself?
",123k,"
            147
        ","['\nPure JavaScript solution!\nLive.js\nJust add the following to your <head>:\n<script type=""text/javascript"" src=""https://livejs.com/live.js""></script>\n\n\nHow?\nJust include Live.js and it will monitor the current page including local CSS and Javascript by sending consecutive HEAD requests to the server. Changes to CSS will be applied dynamically and HTML or Javascript changes will reload the page. Try it!\n\n\nWhere?\nLive.js works in Firefox, Chrome, Safari, Opera and IE6+ until proven otherwise. Live.js is independent of the development framework or language you use, whether it be Ruby, Handcraft, Python, Django, NET, Java, Php, Drupal, Joomla or what-have-you.\n\nI copied this answer almost verbatim from here, because I think it\'s easier and more general than the currently accepted answer here.\n', '\nWith the addition of a single meta tag into your document, you can instruct the browser to automatically reload at a provided interval:\n<meta http-equiv=""refresh"" content=""3"" >\n\nPlaced within the head tag of your document, this meta tag will instruct the browser to refresh every three seconds.\n', ""\nI know this is an old question but in case it helps someone, there is a reload npm package that solves it.\nIn case that you are not running it on a server or have received the error Live.js doesn't support the file protocol. It needs http.\nJust install it:\nnpm install reload -g\n\nand then at your index.html directory, run:\nreload -b\n\nIt will start a server that will refresh every time you save your changes.\nThere are many other options in case you're running it on the server or anything else. Check the reference for more details!\n"", '\nHandy Bash one-liner for OS X, assuming that you have installed fswatch (brew install fswatch). It watches an arbitrary path/file and refreshes the active Chrome tab when there are changes:\nfswatch -o ~/path/to/watch | xargs -n1 -I {} osascript -e \'tell application ""Google Chrome"" to tell the active tab of its first window to reload\'\n\nSee more about fswatch here: https://stackoverflow.com/a/13807906/3510611\n', '\nI assume you\'re not on OSX?  Otherwise you could do something like this with applescript:\nhttp://brettterpstra.com/watch-for-file-changes-and-refresh-your-browser-automatically/\nThere is also a plugin for chrome called ""auto refresh plus"" where you can specify a reload every x seconds:\nhttps://chrome.google.com/webstore/detail/auto-refresh-plus-page-mo/hgeljhfekpckiiplhkigfehkdpldcggm?hl=en\n', '\nUpdate: Tincr is dead.\nTincr is a Chrome extension that will refresh the page whenever the file underneath changes.\n', '\nUse Gulp to watch the files and Browsersync to reload the browser.\nThe steps are:\nIn the command line execute\n\nnpm install --save-dev gulp browser-sync\n\nCreate gulpfile.js with the following contents:\nvar gulp = require(\'gulp\');\nvar browserSync = require(\'browser-sync\').create();\nvar reload = browserSync.reload;\n\ngulp.task(\'serve\', function() {\n  browserSync.init({\n    server: {\n      baseDir: ""./""\n    }\n  });\n\n  gulp.watch(""*.html"").on(""change"", reload);\n});\n\nRun\n\ngulp serve\n\nEdit HTML, save and see your browser reload. The magic is done through on-the-fly injection of special  tag into your HTML files.\n', '\nhttp://livereload.com/ - native app for OS X, Alpha version for Windows. Open sourced at https://github.com/livereload/LiveReload2\n', ""\nIf you are on GNU/Linux, you can use a pretty cool browser called Falkon. It's based on the Qt WebEngine. It's just like Firefox or Chromium - except, it auto refreshes the page when a file is updated. The auto refresh doesn't matter much whether you use vim, nano, or atom, vscode, brackets, geany, mousepad etc.\nOn Arch Linux, you can install Falkon pretty easily:\nsudo pacman -S falkon\n\nHere's the snap package.\n"", ""\nFollowing couple of lines can do the trick:\nvar bfr = '';\nsetInterval(function () {\n    fetch(window.location).then((response) => {\n        return response.text();\n    }).then(r => {\n        if (bfr != '' && bfr != r) {\n            window.location.reload();\n        }\n        else {\n            bfr = r;\n        }\n    });\n}, 1000);\n\nThis compares current response text with previously buffered response text after every second and will reload the page if there are any change in source code. You don't need any heavy duty plugins if you are just developing light weight pages.\n"", '\nThere is a java app for os x and Chrome called Refreschro. It will monitor a given set of files on the local file system and reload Chrome when a change is detected:\nhttp://neromi.com/refreschro/\n', '\nThis works for me (in Ubuntu):\n#!/bin/bash\n#\n# Watches the folder or files passed as arguments to the script and when it\n# detects a change it automatically refreshes the current selected Chrome tab or\n# window.\n#\n# Usage:\n# ./chrome-refresher.sh /folder/to/watch\n\nTIME_FORMAT=\'%F %H:%M\'\nOUTPUT_FORMAT=\'%T Event(s): %e fired for file: %w. Refreshing.\'\n\nwhile inotifywait --exclude \'.+\\.swp$\' -e modify -q \\\n    -r --timefmt ""${TIME_FORMAT}"" --format ""${OUTPUT_FORMAT}"" ""$@""; do\n    xdotool search --onlyvisible --class chromium windowactivate --sync key F5 \\\n    search --onlyvisible --class gnome-terminal windowactivate\ndone\n\nYou may need to install inotify and xdotool packages (sudo apt-get install inotify-tools xdotool in Ubuntu) and to change args of --class to the actual names of your preferred browser and terminal.\nStart the script as described and just open index.html in a browser. After each save in vim the script will focus your browser\'s window, refresh it, and then return to the terminal.\n', '\nA quick solution that I sometimes use is to divide the screen into\ntwo, and each time a change is made, click on the document xD .\n<script>\ndocument.addEventListener(""click"", function(){\n    window.location.reload();\n})\n</script>\n\n', '\nIf you are you are using visual studio code (which I highly recommend for Web Development), there is an extension by the name Live Server by Ritwick Dey with more than 9 million downloads. Just install it (recommended to restart vs code after that), and then just right-click on your main HTML file, there will be an option ""open with Live Server"", click it and your Website will be automatically open in a browser on a local server.\n', '\nIn node.js, you can wire-up primus.js (websockets) with gulp.js + gulp-watch (a task runner and change listener, respectively), so that gulp lets your browser window know it should refresh whenever html, js, etc, change. This is OS agnostic and I have it working in a local project. \nHere, the page is served by your web server, not loaded as a file from disk, which is actually more like the real thing.\n', ""\nThe most flexible solution I've found is the chrome LiveReload extension paired with a guard server. \nWatch all files in a project, or only the ones you specify. Here is a sample Guardfile config:\nguard 'livereload' do\n  watch(%r{.*\\.(css|js|html|markdown|md|yml)})\nend\n\nThe downside is that you have to set this up per project and it helps if you're familiar with ruby.\nI have also used the Tincr chrome extension - but it appears to be tightly coupled to frameworks and file structures. (I tried wiring up tincr for a jekyll project but it only allowed me to watch a single file for changes, not accounting for includes, partial or layout changes). Tincr however, works great out of the box with projects like rails that have consistent and predefined file structures. \nTincr would be a great solution if it allowed all inclusive match patterns for reloading, but the project is still limited in its feature set. \n"", '\nThis can be done using a simple python script.\n\nUse pyinotify to monitor a particular folder.\nUse Chrome with debugging enabled. Refresh can be done via a websocket connection.\n\nFull details can be referred here.\n', '\nBased on attekei\'s answer for OSX:\n$ brew install fswatch\n\nChuck all this into reload.scpt:\nfunction run(argv) {\n    if (argv.length < 1) {\n        console.log(""Please supply a (partial) URL to reload"");\n        return;\n    }\n    console.log(""Trying to reload: "" + argv[0]);\n    let a = Application(""Google Chrome"");\n    for (let i = 0; i < a.windows.length; i++) {\n        let win = a.windows[i];\n        for (let j = 0; j < win.tabs.length; j++) {\n            let tab = win.tabs[j];\n            if (tab.url().startsWith(""file://"") && tab.url().endsWith(argv[0])) {\n                console.log(""Reloading URL: "" + tab.url());\n                tab.reload();\n                return;\n            }\n        }\n    }\n    console.log(""Tab not found."");\n}\n\nThat will reload the first tab that it finds that starts with file:// and ends with the first command line argument. You can tweak it as desired.\nFinally, do something like this.\nfswatch -o ~/path/to/watch | xargs -n1 osascript -l JavaScript reload.scpt myindex.html \n\nfswatch -o outputs the number of files that have changed in each change event, one per line. Usually it will just print 1. xargs reads those 1s in and -n1 means it passes each one as an argument to a new execution of osascript (where it will be ignored).\n', ""\nAdd this to your HTML\n<script> window.addEventListener('focus', ()=>{document.location = document.location})</script>\n\nWhile you are editing, your browser page is blurred, by switching back to look at it, the focus event is fired and the page reloads.\n"", '\nInstall and set up chromix\nNow add this to your .vimrc\nautocmd BufWritePost *.html,*.js,*.css :silent ! chromix with http://localhost:4500/ reload\n\nchange the port to what you use\n', ""\nOk, here is my crude Auto Hotkey solution (On Linux, try Auto Key). When the save keyboard shortcut gets pressed, activate the browser, click the reload button, then switch back to the editor. Im just tired of getting other solutions running. Wont work if your editor does autosave.\n^s::   ; '^' means ctrl key. '!' is alt, '+' is shift. Can be combined.\n    MouseGetPos x,y\n    Send ^s\n\n    ; if your IDE is not that fast with saving, increase.\n    Sleep 100\n\n    ; Activate the browser. This may differ on your system. Can be found with AHK Window Spy.\n    WinActivate ahk_class Chrome_WidgetWin_1\n    WinWaitActive ahk_class Chrome_WidgetWin_1\n    Sleep 100   ; better safe than sorry.\n\n    ;~ Send ^F5   ; I dont know why this doesnt work ...\n    Click 92,62   ; ... so lets click the reload button instead.\n\n    ; Switch back to Editor. Can be found with AHK Window Spy.\n    WinActivate ahk_class zc-frame\n    WinWaitActive ahk_class zc-frame\n    Sleep 100   ; better safe than sorry.\n\n    MouseMove x,y\n    return\n\n"", '\nOffline solution using R\nThis code will:\n\nsetup a local server using the given .html-file\nreturn the server adress, so that you can watch it in a browser.\nmake the browser refresh everytime a change is saved to the .html-file.\n\ninstall.packages(""servr"")\nservr::httw(dir = ""c:/users/username/desktop/"")\n\n\n\nSimilar solutions exist for python etc. \n', '\nIf you have Node installed on your computer, then you can use light-server.\nSetp 1: Install light-server using command npm install -g light-server\nStep 2: While current working directory is the folder containing the static HTML page, start light-server using command npx light-server -s . -p 5000 -w ""*.css # # reloadcss"" -w ""*.html # # reloadhtml"" -w ""*.js # # reloadhtml""\nStep 3: Open the web page in browser at http://localhost:5000\n\n\nIn Step 2,\nPort can be changed using -p switch\nFiles that are being watched can be changed using -w switch\nServer directory can be changed using -s switch\n\nDocumentation for light-server is at https://www.npmjs.com/package/light-server\n', '\nAdd this in your ""head"" section.\nchange the time from 3000ms to any value which you prefer.\nA small hack to reload html file every 3secs. This is useful to me when I use vim + browser setup for JS development.\n    <script>\n        function autoreload() {\n            location.reload();\n        }\n        setInterval(autoreload, 3000);\n    </script>\n\n', '\nLive reload works fine on js and css changes. However it was not working for laravel\'s blade templates. So I wrote a small script that checks for page changes and reloads if there is change.\nAm not sure if it is the best way, but it works.\nI used the script together with Live.js.\nHere it is.\n    window.addEventListener(""load"", function(){\n        var current_url = window.location.href;\n        var current_data = httpGet(current_url);\n        var compone = current_data.length;\n        setInterval(function(){\n        var current_data_twos = httpGet(current_url);\n        var componecurrent_data_twos = current_data_twos.length;\n        if(compone !=componecurrent_data_twos ){\n          location.reload();\n        }\n      }, 1000);\n    });\n    function httpGet(theUrl)\n    {\n        let xmlhttp;\n\n        if (window.XMLHttpRequest) { // code for IE7+, Firefox, Chrome, Opera, Safari\n            xmlhttp=new XMLHttpRequest();\n        } else { // code for IE6, IE5\n            xmlhttp=new ActiveXObject(""Microsoft.XMLHTTP"");\n        }\n        xmlhttp.onreadystatechange=function() {\n            if (xmlhttp.readyState==4 && xmlhttp.status==200) {\n                return xmlhttp.responseText;\n            }\n        }\n        xmlhttp.open(""GET"", theUrl, false);\n        xmlhttp.send();\n        return xmlhttp.response;\n    }\n\n', '\nJust use the Live server extension and open the file with live server. extension namelive server\n', '\npip install https://github.com/joh/when-changed/archive/master.zip\n\nalias watch_refresh_chrome="" when-changed -v -r -1 -s ./ osascript -e \'tell application \\""Google Chrome\\"" to tell the active tab of its first window to reload\' ""\n\nthen just enter the directory you want to monitor execute ""watch_refresh_chrome""\n', '\n(function() {\n    setTimeout(function(){\n        window.location.reload(true);\n    }, 100);\n})();\n\nSave this code into a file livereload.js and include it at the bottom of the HTML script like so:\n<script type=""text/javascript"" src=""livereload.js""></script>\n\nWhat will this do is refresh the page every 100 mili-seconds. Any changes you make in code are instantly visible to the eyes.\n', '\nLive Reload Browser Page\n\nLive Reload Browser Page - tool 2022, for auto refreshing the browser page in real time for Google Chrome.\nAdditional features\n\nauto refreshing the browser page in real time\n\nauto HTML validation of the browser page\n\nreal-time alert on the browser page during web development\n\n\n']",https://stackoverflow.com/questions/5588658/auto-reload-browser-when-i-save-changes-to-html-file-in-chrome,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is it possible to modify a registry entry via a .bat/.cmd script?,"
Is it possible to modify a registry value (whether string or DWORD) via a .bat/.cmd script?
",187k,"
            51
        ","['\n@Franci Penov - modify is possible in the sense of overwrite with /f, eg  \nreg add ""HKCU\\Software\\etc\\etc"" /f /v ""value"" /t REG_SZ /d ""Yes""\n\n', '\nYou can use the REG command. From http://www.ss64.com/nt/reg.html:\nSyntax:\n\n   REG QUERY [ROOT\\]RegKey /v ValueName [/s]\n   REG QUERY [ROOT\\]RegKey /ve  --This returns the (default) value\n\n   REG ADD [ROOT\\]RegKey /v ValueName [/t DataType] [/S Separator] [/d Data] [/f]\n   REG ADD [ROOT\\]RegKey /ve [/d Data] [/f]  -- Set the (default) value\n\n   REG DELETE [ROOT\\]RegKey /v ValueName [/f]\n   REG DELETE [ROOT\\]RegKey /ve [/f]  -- Remove the (default) value\n   REG DELETE [ROOT\\]RegKey /va [/f]  -- Delete all values under this key\n\n   REG COPY  [\\\\SourceMachine\\][ROOT\\]RegKey [\\\\DestMachine\\][ROOT\\]RegKey\n\n   REG EXPORT [ROOT\\]RegKey FileName.reg\n   REG IMPORT FileName.reg\n   REG SAVE [ROOT\\]RegKey FileName.hiv\n   REG RESTORE \\\\MachineName\\[ROOT]\\KeyName FileName.hiv\n\n   REG LOAD FileName KeyName\n   REG UNLOAD KeyName\n\n   REG COMPARE [ROOT\\]RegKey [ROOT\\]RegKey [/v ValueName] [Output] [/s]\n   REG COMPARE [ROOT\\]RegKey [ROOT\\]RegKey [/ve] [Output] [/s]\n\nKey:\n   ROOT :\n         HKLM = HKey_Local_machine (default)\n         HKCU = HKey_current_user\n         HKU  = HKey_users\n         HKCR = HKey_classes_root\n\n   ValueName : The value, under the selected RegKey, to edit.\n               (default is all keys and values)\n\n   /d Data   : The actual data to store as a ""String"", integer etc\n\n   /f        : Force an update without prompting ""Value exists, overwrite Y/N""\n\n   \\\\Machine : Name of remote machine - omitting defaults to current machine.\n                Only HKLM and HKU are available on remote machines.\n\n   FileName  : The filename to save or restore a registry hive.\n\n   KeyName   : A key name to load a hive file into. (Creating a new key)\n\n   /S        : Query all subkeys and values.\n\n   /S Separator : Character to use as the separator in REG_MULTI_SZ values\n                  the default is ""\\0"" \n\n   /t DataType  : REG_SZ (default) | REG_DWORD | REG_EXPAND_SZ | REG_MULTI_SZ\n\n   Output    : /od (only differences) /os (only matches) /oa (all) /on (no output)\n\n', '\nYes, you can script using the reg command.\nExample:\nreg add HKCU\\Software\\SomeProduct\nreg add HKCU\\Software\\SomeProduct /v Version /t REG_SZ /d v2.4.6\n\nThis would create key HKEY_CURRENT_USER\\Software\\SomeProduct, and add a String value ""v2.4.6"" named ""Version"" to that key.\nreg /? has the details.\n', '\nThis is how you can modify registry, without yes or no prompt and don\'t forget to run as administrator\nreg add HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\Shell\\etc\\etc   /v Valuename /t REG_SZ /d valuedata  /f \n\nBelow is a real example to set internet explorer as my default browser\nreg add HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\Shell\\Associations\\UrlAssociations\\https\\UserChoice   /v ProgId /t REG_SZ /d IE.HTTPS  /f \n\n\n/f Force: Force an update without prompting ""Value exists, overwrite\n  Y/N""\n/d Data   : The actual data to store as a ""String"", integer etc\n/v Value   : The value name eg ProgId\n/t DataType  : REG_SZ (default) | REG_DWORD | REG_EXPAND_SZ |\n  REG_MULTI_SZ\n\nLearn more about  Read, Set or Delete registry keys and values, save and restore from a .REG file. from here \n', '\nYou can make a .reg file and call start on it.  You can export any part of the registry as a .reg file to see what the format is.  \nFormat here:\nhttp://support.microsoft.com/kb/310516\nThis can be run on any Windows machine without installing other software.\n', '\nYes. You can use reg.exe which comes with the OS to add, delete or query registry values. Reg.exe does not have an explicit modify command, but you can do it by doing delete and then add.\n', '\nIn addition to reg.exe, I highly recommend that you also check out powershell, its vastly more capable in its registry handling.\n', '\nSee http://www.chaminade.org/MIS/Articles/RegistryEdit.htm\n']",https://stackoverflow.com/questions/130193/is-it-possible-to-modify-a-registry-entry-via-a-bat-cmd-script,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Register Variables in Loop in an Ansible Playbook,"
I have two ansible tasks as follows 
  tasks:
 - shell: ifconfig -a | sed 's/[ \t].*//;/^\(lo\|\)$/d'
   register: var1
 - debug: var=var1

 - shell: ethtool -i {{ item }} | grep bus-info | cut -b 16-22
   with_items: var1.stdout_lines
   register: var2
 - debug: var=var2

which is used to get a list of interfaces in a machine (linux) and get the bus address for each. I have one more task as follows in tha same playbook 
 - name: Binding the interfaces
   shell: echo {{ item.item }}
   with_flattened: var2.results
   register: var3

which I expect to iterate over value from var2 and then print the bus numbers. 
var2.results is as follows 
""var2"": {
    ""changed"": true,
    ""msg"": ""All items completed"",
    ""results"": [
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i br0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005778"",
            ""end"": ""2015-04-14 20:29:47.122203"",
            ""invocation"": {
                ""module_args"": ""ethtool -i br0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""br0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:47.116425"",
            ""stderr"": """",
            ""stdout"": """",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp13s0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005862"",
            ""end"": ""2015-04-14 20:29:47.359749"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp13s0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp13s0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:47.353887"",
            ""stderr"": """",
            ""stdout"": ""0d:00.0"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp14s0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005805"",
            ""end"": ""2015-04-14 20:29:47.576674"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp14s0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp14s0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:47.570869"",
            ""stderr"": """",
            ""stdout"": ""0e:00.0"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp15s0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005873"",
            ""end"": ""2015-04-14 20:29:47.875058"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp15s0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp15s0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:47.869185"",
            ""stderr"": """",
            ""stdout"": ""0f:00.0"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp5s0f1: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005870"",
            ""end"": ""2015-04-14 20:29:48.112027"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp5s0f1: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp5s0f1:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:48.106157"",
            ""stderr"": """",
            ""stdout"": ""05:00.1"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp5s0f2: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005863"",
            ""end"": ""2015-04-14 20:29:48.355733"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp5s0f2: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp5s0f2:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:48.349870"",
            ""stderr"": """",
            ""stdout"": ""05:00.2"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp5s0f3: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005829"",
            ""end"": ""2015-04-14 20:29:48.591244"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp5s0f3: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp5s0f3:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:48.585415"",
            ""stderr"": """",
            ""stdout"": ""05:00.3"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp9s0f0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005943"",
            ""end"": ""2015-04-14 20:29:48.910992"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp9s0f0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp9s0f0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:48.905049"",
            ""stderr"": """",
            ""stdout"": ""09:00.0"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp9s0f1: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005863"",
            ""end"": ""2015-04-14 20:29:49.143706"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp9s0f1: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp9s0f1:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:49.137843"",
            ""stderr"": """",
            ""stdout"": ""09:00.1"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i lo: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005856"",
            ""end"": ""2015-04-14 20:29:49.386044"",
            ""invocation"": {
                ""module_args"": ""ethtool -i lo: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""lo:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:49.380188"",
            ""stderr"": ""Cannot get driver information: Operation not supported"",
            ""stdout"": """",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i virbr0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005859"",
            ""end"": ""2015-04-14 20:29:49.632356"",
            ""invocation"": {
                ""module_args"": ""ethtool -i virbr0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""virbr0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:49.626497"",
            ""stderr"": """",
            ""stdout"": """",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i virbr0-nic: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.024850"",
            ""end"": ""2015-04-14 20:29:49.901539"",
            ""invocation"": {
                ""module_args"": ""ethtool -i virbr0-nic: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""virbr0-nic:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:49.876689"",
            ""stderr"": """",
            ""stdout"": """",
            ""warnings"": []
        }
    ]

My objective is to get the value of stdout in each item above for example (""stdout"": ""09:00.0"") . I tried giving something like 
     - name: Binding the interfaces
       shell: echo {{ item.item.stdout}}
       with_flattened: var2.results
#       with_indexed_items: var2.results
       register: var3

But this is not giving the bus values in stdout correctly. Appreciate help in listing the variable of variable value in task as given below when the second variable is and indexed list. I am trying to avoid direct index numbering such as item[0] because the number of interfaces are dynamic and direct indexing may result in unexpected outcomes. 
Thanks 
",110k,"
            39
        ","['\nIs this what you\'re looking for:\n\nVariables registered for a task that has with_items have different format, they contain results for all items.\n\n- hosts: localhost\n  tags: s21\n  gather_facts: no\n  vars:\n    images:\n      - foo\n      - bar\n  tasks:\n    - shell: ""echo result-{{item}}""\n      register: ""r""\n      with_items: ""{{images}}""\n\n    - debug: var=r\n\n    - debug: msg=""item.item={{item.item}}, item.stdout={{item.stdout}}, item.changed={{item.changed}}""\n      with_items: ""{{r.results}}""\n\n    - debug: msg=""Gets printed only if this item changed - {{item}}""\n      when: ""{{item.changed == true}}""\n      with_items: ""{{r.results}}""\n\n\nSource: Register variables in with_items loop in Ansible playbook\n']",https://stackoverflow.com/questions/29635627/register-variables-in-loop-in-an-ansible-playbook,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows Console Application Getting Stuck (Needs Key Press) [duplicate],"






This question already has answers here:
                        
                    



How and why does QuickEdit mode in Command Prompt freeze applications?

                                (2 answers)
                            

Closed 6 years ago.



I have a console program that has different components that run like this:  
void start() {
while(true){
     DoSomething();
     Thread.Sleep(1000*5);
}
}

My main entry point looks like [pseudo-ish code]
Thread.Start(Componenet1.Start);
Thread.Start(Componenet2.Start);

while(true){
     Console.Writeline(""running"");
     Thread.Sleep(1000*5);
}

There are no Console.Reads anywhere. My problem is SOMETIMES the application will be running great but then stop and if I press any key on the window it will start working again. This happens fairly infrequently but I have this program deployed on 100+ VM's running 24/7 in an automated environment.
Also on the computer I have some AHK scripts and other stuff that manipulate the mouse but not sure if that has anything to do with it.
Also note that sometimes the CPU can really be running at 100% on the machines so maybe thread priority is an issue?
SOLUTION: You need to disable quick edit mode. Here is working C# code to do this:
 // http://msdn.microsoft.com/en-us/library/ms686033(VS.85).aspx
    [DllImport(""kernel32.dll"")]
    public static extern bool SetConsoleMode(IntPtr hConsoleHandle, uint dwMode);

    private const uint ENABLE_EXTENDED_FLAGS = 0x0080;

    static void Main(string[] args)
    {
         IntPtr handle = Process.GetCurrentProcess().MainWindowHandle;
         SetConsoleMode(handle, ENABLE_EXTENDED_FLAGS);

",10k,"
            38
        ","['\nIf the user accidentally clicks into the black console window, the cursor changes to a filled white rectangle, and the app hangs at the next Console.Write statement, until another clic is made.\nIt is a generic feature of the Console window when its ""QuickEdit Mode"" is enabled.\nIn order to disable that feature, you should uncheck the ""QuickEdit Mode"" option of your app\'s console window at run-time.\n']",https://stackoverflow.com/questions/4453692/windows-console-application-getting-stuck-needs-key-press,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
element not interactable exception in selenium web automation,"
In the below code i cannot send password keys in the password field, i tried clicking the field, clearing the field and sending the keys. But now working in any of the method. But its working if i debug and test
  public class TestMail {
   protected static WebDriver driver;

   protected static String result;

   @BeforeClass

   public static void setup()  {
              System.setProperty(""webdriver.gecko.driver"",""D:\\geckodriver.exe"");

   driver = new FirefoxDriver();

   driver.manage().timeouts().implicitlyWait(60, TimeUnit.SECONDS);

  }

   @Test

 void Testcase1() {

   driver.get(""http://mail.google.com"");

   WebElement loginfield = driver.findElement(By.name(""Email""));
   if(loginfield.isDisplayed()){
       loginfield.sendKeys(""ragesh@gmail.in"");
   }
   else{
  WebElement newloginfield = driver.findElemnt(By.cssSelector(""#identifierId""));                                      
       newloginfield.sendKeys(""ragesh@gmail.in"");
      // System.out.println(""This is new login"");
   }


    driver.findElement(By.name(""signIn"")).click();

  // driver.findElement(By.cssSelector("".RveJvd"")).click();

   driver.manage().timeouts().implicitlyWait(15, TimeUnit.SECONDS);
 // WebElement pwd = driver.findElement(By.name(""Passwd""));
  WebElement pwd = driver.findElement(By.cssSelector(""#Passwd""));

  pwd.click();
  pwd.clear();
 // pwd.sendKeys(""123"");
 if(pwd.isEnabled()){
     pwd.sendKeys(""123"");
 }
 else{
     System.out.println(""Not Enabled"");
 }

",318k,"
            33
        ","['\nTry setting an implicit wait of maybe 10 seconds.\ngmail.manage().timeouts().implicitlyWait(10, TimeUnit.SECONDS);\n\nOr set an explicit wait. An explicit waits is code you define to wait for a certain condition to occur before proceeding further in the code. In your case, it is the visibility of the password input field. (Thanks to ainlolcat\'s comment)\nWebDriver gmail= new ChromeDriver();\ngmail.get(""https://www.gmail.co.in""); \ngmail.findElement(By.id(""Email"")).sendKeys(""abcd"");\ngmail.findElement(By.id(""next"")).click();\nWebDriverWait wait = new WebDriverWait(gmail, 10);\nWebElement element = wait.until(\nExpectedConditions.visibilityOfElementLocated(By.id(""Passwd"")));\ngmail.findElement(By.id(""Passwd"")).sendKeys(""xyz"");\n\nExplanation: The reason selenium can\'t find the element is because the id of the password input field is initially Passwd-hidden. After you click on the ""Next"" button, Google first verifies the email address entered and then shows the password input field (by changing the id from Passwd-hidden to Passwd). So, when the password field is still hidden (i.e. Google is still verifying the email id), your webdriver starts searching for the password input field with id Passwd which is still hidden. And hence, an exception is thrown.\n', '\n""element not interactable"" error can mean two things :\na.  Element has not properly rendered:\nSolution for this is just to use implicit /explicit wait\n\nImplicit wait :\ndriver.manage().timeouts().implicitlyWait(50, TimeUnit.SECONDS);\n\nExplicit wait :\nWebDriverWait wait=new WebDriverWait(driver, 20);\nelement1 = wait.until(ExpectedConditions.elementToBeClickable(By.className(""fa-stack-1x"")));\n\n\nb. Element has rendered but it is not in the visible part of the screen:\nSolution is just to scroll till the element. Based on the version of Selenium it can be handled in different ways but I will provide a solution that works in all versions :\n    JavascriptExecutor executor = (JavascriptExecutor) driver;\n    executor.executeScript(""arguments[0].scrollIntoView(true);"", element1);\n\n\nSuppose all this fails then another way is to again make use of Javascript executor as following :\nexecutor.executeScript(""arguments[0].click();"", element1);\n\nIf you still can\'t click , then it could again mean two things :\n\n\n1. Iframe\nCheck the DOM to see if the element you are inspecting lives in any frame. If that is true then you would need to switch to this frame before attempting any operation.\n    driver.switchTo().frame(""a077aa5e""); //switching the frame by ID\n    System.out.println(""********We are switching to the iframe*******"");\n    driver.findElement(By.xpath(""html/body/a/img"")).click();\n\n2. New tab\nIf a new tab has opened up and the element exists on it then you again need to code something like below to switch to it before attempting operation.\nString parent = driver.getWindowHandle();\ndriver.findElement(By.partialLinkText(""Continue"")).click();\nSet<String> s = driver.getWindowHandles();\n// Now iterate using Iterator\nIterator<String> I1 = s.iterator();\nwhile (I1.hasNext()) {\nString child_window = I1.next();\nif (!parent.equals(child_window)) {\n    driver.switchTo().window(child_window);\n    element1.click() \n}\n\n', '\nPlease try selecting the password field like this.\n    WebDriverWait wait = new WebDriverWait(driver, 10);\n    WebElement passwordElement = wait.until(ExpectedConditions.elementToBeClickable(By.cssSelector(""#Passwd"")));\n    passwordElement.click();\n  passwordElement.clear();\n     passwordElement.sendKeys(""123"");\n\n', '\nyou may also try full xpath, I had a similar issue where I had to click on an element which has a property javascript onclick function. the full xpath method worked and no interactable exception was thrown.\n', '\nIn my case the element that generated the Exception was a button belonging to a form. I replaced\nWebElement btnLogin = driver.findElement(By.cssSelector(""button""));\nbtnLogin.click();\n\nwith\nbtnLogin.submit();\n\nMy environment was chromedriver windows 10\n', ""\nIn my case, I'm using python-selenium.\nI have two instructions. The second instruction wasn't able to execute.\nI put a time.sleep(1) between two instructions and I'm done.\nIf you want you can change the sleep amount according to your need.\n"", '\nI had the same problem and then figured out the cause. I was trying to type in a span tag instead of an input tag. My XPath was written with a span tag, which was a wrong thing to do. I reviewed the Html for the element and found the problem. All I then did was to find the input tag which happens to be a child element. You can only type in an input field if your XPath is created with an input tagname\n', ""\nI'm going to hedge this answer with this: I know it's crap.. and there's got to be a better way. (See above answers) But I tried all the suggestions here and still got nill. Ended up chasing errors, ripping the code to bits. Then I tried this:\nimport keyboard    \nkeyboard.press_and_release('tab')\nkeyboard.press_and_release('tab')\nkeyboard.press_and_release('tab') #repeat as needed\nkeyboard.press_and_release('space') \n\nIt's pretty insufferable and you've got to make sure that you don't lose focus otherwise you'll just be tabbing and spacing on the wrong thing.\nMy assumption on why the other methods didn't work for me is that I'm trying to click on something the developers didn't want a bot clicking on. So I'm not clicking on it!\n"", '\nI got this error because I was using a wrong CSS selector with the Selenium WebDriver Node.js function By.css().\nYou can check if your selector is correct by using it in the web console of your web browser (Ctrl+Shift+K shortcut), with the JavaScript function document.querySelectorAll().\n', '\nIf it\'s working in the debug, then wait must be the proper solution.\nI will suggest to use the explicit wait, as given below:\nWebDriverWait wait = new WebDriverWait(new ChromeDriver(), 5);\nwait.until(ExpectedConditions.presenceOfElementLocated(By.cssSelector(""#Passwd"")));\n\n', '\nI came across this error too.  I thought it might have been because the field was not visible.  I tried the scroll solution above and although the field became visible in the controlled browser session I still got the exception.  The solution I am committing looks similar to below.  It looks like the event can bubble to the contained input field and the end result is the Selected property becomes true.\nThe field appears in my page something like this.\n<label>\n  <input name=""generic"" type=""checkbox"" ... >\n<label>\n\nThe generic working code looks more or less like this:\nvar checkbox = driver.FindElement(By.Name(""generic""), mustBeVisible: false);\ncheckbox.Selected.Should().BeFalse();\nvar label = checkbox.FindElement(By.XPath(""..""));\nlabel.Click();\ncheckbox.Selected.Should().BeTrue();\n\nYou\'ll need to translate this to your specific language.  I\'m using C# and FluentAssertions.  This solution worked for me with Chrome 94 and Selenium 3.141.0.\n', ""\nI had to hover over the element first for the sub-elements to appear. I didn't take that into account at first.\n    WebElement boardMenu = this.driver.findElement(By.linkText(boardTitle));\n    Actions action = new Actions(this.driver);\n\n    action.moveToElement(boardMenu).perform();\n\nAnother tip is to check that you are having one element of that DOM. Try using Ctrl+F when inspecting the web page and check your xpath there; it should return one element if you are going with the findElement method.\n""]",https://stackoverflow.com/questions/45183797/element-not-interactable-exception-in-selenium-web-automation,automation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can scrapy be used to scrape dynamic content from websites that are using AJAX?,"
I have recently been learning Python and am dipping my hand into building a web-scraper.  It's nothing fancy at all; its only purpose is to get the data off of a betting website and have this data put into Excel.
Most of the issues are solvable and I'm having a good little mess around. However I'm hitting a massive hurdle over one issue. If a site loads a table of horses and lists current betting prices this information is not in any source file. The clue is that this data is live sometimes, with the numbers being updated obviously from some remote server. The HTML on my PC simply has a hole where their servers are pushing through all the interesting data that I need.
Now my experience with dynamic web content is low, so this thing is something I'm having trouble getting my head around. 
I think Java or Javascript is a key, this pops up often. 
The scraper is simply a odds comparison engine.  Some sites have APIs but I need this for those that don't. I'm using the scrapy library with Python 2.7
I do apologize if this question is too open-ended. In short, my question is: how can scrapy be used to scrape this dynamic data so that I can use it?  So that I can scrape this betting odds data in real-time?
",146k,"
            164
        ","['\nHere is a simple example of  scrapy with an AJAX request. Let see the site rubin-kazan.ru.\nAll messages are loaded with an AJAX request. My goal is to fetch these messages with all their attributes (author, date, ...):\n\nWhen I analyze the source code of the page I can\'t see all these messages because the web page uses AJAX technology. But I can with Firebug from Mozilla Firefox (or an equivalent tool in other browsers) to analyze the HTTP request that generate the messages on the web page:\n\nIt doesn\'t reload the whole page but only the parts of the page that contain messages. For this purpose I click an arbitrary number of page on the bottom:\n\nAnd I observe the HTTP request that is responsible for message body:\n\nAfter finish, I analyze the headers of the request (I must quote that this URL I\'ll extract from source page from var section, see the code below):\n\nAnd the form data content of the request (the HTTP method is ""Post""):\n\nAnd the content of response, which is a JSON file:\n\nWhich presents all the information I\'m looking for.\nFrom now, I must implement all this knowledge in scrapy. Let\'s define the spider for this purpose:\nclass spider(BaseSpider):\n    name = \'RubiGuesst\'\n    start_urls = [\'http://www.rubin-kazan.ru/guestbook.html\']\n\n    def parse(self, response):\n        url_list_gb_messages = re.search(r\'url_list_gb_messages=""(.*)""\', response.body).group(1)\n        yield FormRequest(\'http://www.rubin-kazan.ru\' + url_list_gb_messages, callback=self.RubiGuessItem,\n                          formdata={\'page\': str(page + 1), \'uid\': \'\'})\n\n    def RubiGuessItem(self, response):\n        json_file = response.body\n\nIn parse function I have the response for first request.\nIn RubiGuessItem I have the JSON file with all information. \n', ""\nWebkit based browsers (like Google Chrome or Safari) has built-in developer tools. In Chrome you can open it Menu->Tools->Developer Tools. The Network tab allows you to see all information about every request and response:\n\nIn the bottom of the picture you can see that I've filtered request down to XHR - these are requests made by javascript code.\nTip: log is cleared every time you load a page, at the bottom of the picture, the black dot button will preserve log.\nAfter analyzing requests and responses you can simulate these requests from your web-crawler and extract valuable data. In many cases it will be easier to get your data than parsing HTML, because that data does not contain presentation logic and is formatted to be accessed by javascript code.\nFirefox has similar extension, it is called firebug. Some will argue that firebug is even more powerful but I like the simplicity of webkit.\n"", '\nMany times when crawling we run into problems where content that is rendered on the page is generated with Javascript and therefore scrapy is unable to crawl for it (eg. ajax requests, jQuery craziness).\nHowever, if you use Scrapy along with the web testing framework Selenium then we are able to crawl anything displayed in a normal web browser.\nSome things to note:\n\nYou must have the Python version of Selenium RC installed for this to work, and you must have set up Selenium properly.  Also this is just a template crawler.  You could get much crazier and more advanced with things but I just wanted to show the basic idea.  As the code stands now you will be doing two requests for any given url.  One request is made by Scrapy and the other is made by Selenium.  I am sure there are ways around this so that you could possibly just make Selenium do the one and only request but I did not bother to implement that and by doing two requests you get to crawl the page with Scrapy too.\nThis is quite powerful because now you have the entire rendered DOM available for you to crawl and you can still use all the nice crawling features in Scrapy.  This will make for slower crawling of course but depending on how much you need the rendered DOM it might be worth the wait.\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.http import Request\n\nfrom selenium import selenium\n\nclass SeleniumSpider(CrawlSpider):\n    name = ""SeleniumSpider""\n    start_urls = [""http://www.domain.com""]\n\n    rules = (\n        Rule(SgmlLinkExtractor(allow=(\'\\.html\', )), callback=\'parse_page\',follow=True),\n    )\n\n    def __init__(self):\n        CrawlSpider.__init__(self)\n        self.verificationErrors = []\n        self.selenium = selenium(""localhost"", 4444, ""*chrome"", ""http://www.domain.com"")\n        self.selenium.start()\n\n    def __del__(self):\n        self.selenium.stop()\n        print self.verificationErrors\n        CrawlSpider.__del__(self)\n\n    def parse_page(self, response):\n        item = Item()\n\n        hxs = HtmlXPathSelector(response)\n        #Do some XPath selection with Scrapy\n        hxs.select(\'//div\').extract()\n\n        sel = self.selenium\n        sel.open(response.url)\n\n        #Wait for javscript to load in Selenium\n        time.sleep(2.5)\n\n        #Do some crawling of javascript created content with Selenium\n        sel.get_text(""//div"")\n        yield item\n\n# Snippet imported from snippets.scrapy.org (which no longer works)\n# author: wynbennett\n# date  : Jun 21, 2011\n\n\nReference: http://snipplr.com/view/66998/\n', '\nAnother solution would be to implement a download handler or download handler middleware. (see scrapy docs for more information on downloader middleware) The following is an example class using selenium with headless phantomjs webdriver: \n1) Define class within the middlewares.py script.\nfrom selenium import webdriver\nfrom scrapy.http import HtmlResponse\n\nclass JsDownload(object):\n\n    @check_spider_middleware\n    def process_request(self, request, spider):\n        driver = webdriver.PhantomJS(executable_path=\'D:\\phantomjs.exe\')\n        driver.get(request.url)\n        return HtmlResponse(request.url, encoding=\'utf-8\', body=driver.page_source.encode(\'utf-8\'))\n\n2) Add JsDownload() class to variable DOWNLOADER_MIDDLEWARE within settings.py:\nDOWNLOADER_MIDDLEWARES = {\'MyProj.middleware.MiddleWareModule.MiddleWareClass\': 500}\n\n3) Integrate the HTMLResponse within your_spider.py. Decoding the response body will get you the desired output.\nclass Spider(CrawlSpider):\n    # define unique name of spider\n    name = ""spider""\n\n    start_urls = [""https://www.url.de""] \n\n    def parse(self, response):\n        # initialize items\n        item = CrawlerItem()\n\n        # store data as items\n        item[""js_enabled""] = response.body.decode(""utf-8"") \n\nOptional Addon: \nI wanted the ability to tell different spiders which middleware to use so I implemented this wrapper:\ndef check_spider_middleware(method):\n@functools.wraps(method)\ndef wrapper(self, request, spider):\n    msg = \'%%s %s middleware step\' % (self.__class__.__name__,)\n    if self.__class__ in spider.middleware:\n        spider.log(msg % \'executing\', level=log.DEBUG)\n        return method(self, request, spider)\n    else:\n        spider.log(msg % \'skipping\', level=log.DEBUG)\n        return None\n\nreturn wrapper\n\nfor wrapper to work all spiders must have at minimum:\nmiddleware = set([])\n\nto include a middleware:\nmiddleware = set([MyProj.middleware.ModuleName.ClassName])\n\nAdvantage: \nThe main advantage to implementing it this way rather than in the spider is that you only end up making one request. In A T\'s solution for example: The download handler processes the request and then hands off the response to the spider. The spider then makes a brand new request in it\'s parse_page function -- That\'s two requests for the same content.\n', '\nI was using a custom downloader middleware, but wasn\'t very happy with it, as I didn\'t manage to make the cache work with it.\nA better approach was to implement a custom download handler.\nThere is a working example here. It looks like this:\n# encoding: utf-8\nfrom __future__ import unicode_literals\n\nfrom scrapy import signals\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.xlib.pydispatch import dispatcher\nfrom selenium import webdriver\nfrom six.moves import queue\nfrom twisted.internet import defer, threads\nfrom twisted.python.failure import Failure\n\n\nclass PhantomJSDownloadHandler(object):\n\n    def __init__(self, settings):\n        self.options = settings.get(\'PHANTOMJS_OPTIONS\', {})\n\n        max_run = settings.get(\'PHANTOMJS_MAXRUN\', 10)\n        self.sem = defer.DeferredSemaphore(max_run)\n        self.queue = queue.LifoQueue(max_run)\n\n        SignalManager(dispatcher.Any).connect(self._close, signal=signals.spider_closed)\n\n    def download_request(self, request, spider):\n        """"""use semaphore to guard a phantomjs pool""""""\n        return self.sem.run(self._wait_request, request, spider)\n\n    def _wait_request(self, request, spider):\n        try:\n            driver = self.queue.get_nowait()\n        except queue.Empty:\n            driver = webdriver.PhantomJS(**self.options)\n\n        driver.get(request.url)\n        # ghostdriver won\'t response when switch window until page is loaded\n        dfd = threads.deferToThread(lambda: driver.switch_to.window(driver.current_window_handle))\n        dfd.addCallback(self._response, driver, spider)\n        return dfd\n\n    def _response(self, _, driver, spider):\n        body = driver.execute_script(""return document.documentElement.innerHTML"")\n        if body.startswith(""<head></head>""):  # cannot access response header in Selenium\n            body = driver.execute_script(""return document.documentElement.textContent"")\n        url = driver.current_url\n        respcls = responsetypes.from_args(url=url, body=body[:100].encode(\'utf8\'))\n        resp = respcls(url=url, body=body, encoding=""utf-8"")\n\n        response_failed = getattr(spider, ""response_failed"", None)\n        if response_failed and callable(response_failed) and response_failed(resp, driver):\n            driver.close()\n            return defer.fail(Failure())\n        else:\n            self.queue.put(driver)\n            return defer.succeed(resp)\n\n    def _close(self):\n        while not self.queue.empty():\n            driver = self.queue.get_nowait()\n            driver.close()\n\nSuppose your scraper is called ""scraper"". If you put the mentioned code inside a file called handlers.py on the root of the ""scraper"" folder, then you could add to your settings.py:\nDOWNLOAD_HANDLERS = {\n    \'http\': \'scraper.handlers.PhantomJSDownloadHandler\',\n    \'https\': \'scraper.handlers.PhantomJSDownloadHandler\',\n}\n\nAnd voil脿, the JS parsed DOM, with scrapy cache, retries, etc.\n', ""\n\nhow can scrapy be used to scrape this dynamic data so that I can use\n  it?\n\nI wonder why no one has posted the solution using Scrapy only. \nCheck out the blog post from Scrapy team SCRAPING INFINITE SCROLLING PAGES\n. The example scraps http://spidyquotes.herokuapp.com/scroll website which uses infinite scrolling. \nThe idea is to use Developer Tools of your browser and notice the AJAX requests, then based on that information create the requests for Scrapy.\nimport json\nimport scrapy\n\n\nclass SpidyQuotesSpider(scrapy.Spider):\n    name = 'spidyquotes'\n    quotes_base_url = 'http://spidyquotes.herokuapp.com/api/quotes?page=%s'\n    start_urls = [quotes_base_url % 1]\n    download_delay = 1.5\n\n    def parse(self, response):\n        data = json.loads(response.body)\n        for item in data.get('quotes', []):\n            yield {\n                'text': item.get('text'),\n                'author': item.get('author', {}).get('name'),\n                'tags': item.get('tags'),\n            }\n        if data['has_next']:\n            next_page = data['page'] + 1\n            yield scrapy.Request(self.quotes_base_url % next_page)\n\n"", '\nData that generated from external url which is API calls HTML response as POST method.\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass TestSpider(scrapy.Spider):\n    name = \'test\'  \n    def start_requests(self):\n        url = \'https://howlongtobeat.com/search_results?page=1\'\n        payload = ""queryString=&t=games&sorthead=popular&sortd=0&plat=&length_type=main&length_min=&length_max=&v=&f=&g=&detail=&randomize=0""\n        headers = {\n            ""content-type"":""application/x-www-form-urlencoded"",\n            ""user-agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36""\n        }\n\n        yield scrapy.Request(url,method=\'POST\', body=payload,headers=headers,callback=self.parse)\n\n    def parse(self, response):\n        cards = response.css(\'div[class=""search_list_details""]\')\n\n        for card in cards: \n            game_name = card.css(\'a[class=text_white]::attr(title)\').get()\n            yield {\n                ""game_name"":game_name\n            }\n           \n\nif __name__ == ""__main__"":\n    process =CrawlerProcess()\n    process.crawl(TestSpider)\n    process.start()\n\n', ""\nThere are a few more modern alternatives in 2022 that I think should be mentioned, and I would like to list some pros and cons for the methods discussed in the more popular answers to this question.\n\nThe top answer and several others discuss using the browsers dev tools or packet capturing software to try to identify patterns in response url's, and try to re-construct them to use as scrapy.Requests.\n\nPros: This is still the best option in my opinion, and when it is available it is quick and often times simpler than even the traditional approach i.e. extracting content from the HTML using xpath and css selectors.\n\nCons: Unfortunately this is only available on a fraction of dynamic sites and frequently websites have security measures in place that make using this strategy difficult.\n\n\n\nUsing Selenium Webdriver is the other approach mentioned a lot in previous answers.\n\nPros: It's easy to implement, and integrate into the scrapy workflow. Additionally there are a ton of examples, and requires very little configuration if you use 3rd-party extensions like scrapy-selenium\n\nCons: It's slow!  One of scrapy's key features is it's asynchronous workflow that makes it easy to crawl dozens or even hundreds of pages in seconds. Using selenium cuts this down significantly.\n\n\n\n\nThere are two new methods that defenitely worth consideration, scrapy-splash and scrapy-playwright.\nscrapy-splash:\n\nA scrapy plugin that integrates splash, a javascript rendering service created  and maintained by the developers of scrapy, into the scrapy workflow. The plugin can be installed from pypi with pip3 install scrapy-splash, while splash needs to run in it's own process, and is easiest to run from a docker container.\n\nscrapy-playwright:\n\nPlaywright is a browser automation tool kind of like selenium, but without the crippling decrease in speed that comes with using selenium. Playwright has no issues fitting into the asynchronous scrapy workflow making sending requests just as quick as using scrapy alone. It is also much easier to install and integrate than selenium. The scrapy-playwright plugin is maintained by the developers of scrapy as well, and after installing via pypi with pip3 install scrapy-playwright is as easy as running playwright install in the terminal.\n\nMore details and many examples can be found at each of the plugin's github pages https://github.com/scrapy-plugins/scrapy-playwright and https://github.com/scrapy-plugins/scrapy-splash.\np.s.  Both projects tend to work better in a linux environment in my experience. for windows users i recommend using it with The Windows Subsystem for Linux(wsl).\n"", '\nYes, Scrapy can scrape dynamic websites, website that are rendered through JavaScript.\nThere are Two approaches to scrapy these kind of websites.\n\nyou can use splash to render Javascript code and then parse the rendered HTML.\nyou can find the doc and project here Scrapy splash, git\n\nas previously stated, by monitoring the network calls, yes, you can find the API call that fetch the data and mock that call in your scrapy spider might help you to get desired data.\n\n\n', '\nI handle the ajax request by using Selenium and the Firefox web driver. It is not that fast if you need the crawler as a daemon, but much better than any manual solution.\n']",https://stackoverflow.com/questions/8550114/can-scrapy-be-used-to-scrape-dynamic-content-from-websites-that-are-using-ajax,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PhantomJS failing to open HTTPS site,"
I'm using the following code based on loadspeed.js example to open up a https:// site which requires http server authentication as well.
var page = require('webpage').create(), system = require('system'), t, address;

page.settings.userName = 'myusername';
page.settings.password = 'mypassword';

if (system.args.length === 1) {
    console.log('Usage: scrape.js <some URL>');
    phantom.exit();
} else {
    t = Date.now();
    address = system.args[1];
    page.open(address, function (status) {
        if (status !== 'success') {
            console.log('FAIL to load the address');
        } else {
            t = Date.now() - t;
            console.log('Page title is ' + page.evaluate(function () {
                return document.title;
            }));
            console.log('Loading time ' + t + ' msec');
        }
        phantom.exit();
    });
}  

Its failing to load the page all the time. What could be wrong here? Are secured sites to be handled any differently? The site can be accessed successfully from browser though.
I'm just starting with Phantom right now and find it too good to stop playing around even though i'm not moving forward with this issue.
",77k,"
            104
        ","[""\nI tried Fred's and Cameron Tinker's answers, but only --ssl-protocol=any option seem to help me:\nphantomjs --ssl-protocol=any test.js\n\nAlso I think it should be way safer to use --ssl-protocol=any as you still are using encryption, but --ignore-ssl-errors=true will ignore (duh) all ssl errors, including malicious ones.\n"", ""\nThe problem is most likely due to SSL certificate errors. If you start phantomjs with the --ignore-ssl-errors=yes option, it should proceed to load the page as it would if there were no SSL errors:\nphantomjs --ignore-ssl-errors=yes [phantomOptions] script.js [scriptOptions]\n\nI've seen a few websites having problems with incorrectly implementing their SSL certificates or they've expired, etc. A complete list of command line options for phantomjs is available here: http://phantomjs.org/api/command-line.html.\n"", '\nNote that as of 2014-10-16, PhantomJS defaults to using SSLv3 to open HTTPS connections. With the POODLE vulnerability recently announced, many servers are disabling SSLv3 support.\nTo get around that, you should be able to run PhantomJS with:\nphantomjs --ssl-protocol=tlsv1\n\nHopefully, PhantomJS will be updated soon to make TLSv1 the default instead of SSLv3.\n', '\nexperienced same issue...\n--ignore-ssl-errors=yes was not enough to fix it for me,\nhad to do two more things:\n\n1) change user-agent\n\n2) tried all ssl-protocols, the only one that worked was tlsv1 for the page in question\n\nHope this helps...\n', '\nI experienced the same problem (casperjs 1.1.0-beta3/phantomjs 1.9.7). Using --ignore-ssl-errors=yes and --ssl-protocol=tlsv1 solved it. Using only one of the options did not solve it for me.\n', '\nI was receiving \n\nError creating SSL context"" from phantomJS (running on CentOS 6.6)\n\nBuilding from source fixed it for me. Don\'t forget to use the phantomjs that you built. (instead of the /usr/local/bin/phantomjs if you have it)\nsudo yum -y install gcc gcc-c++ make flex bison gperf ruby openssl-devel freetype-devel fontconfig-devel libicu-devel sqlite-devel libpng-devel libjpeg-devel\ngit clone git://github.com/ariya/phantomjs.git\ncd phantomjs\ngit checkout 2.0\n./build.sh\ncd bin/\n./phantomjs <your JS file>\n\n', '\nIf someone is using Phantomjs with Sahi the --ignore-ssl-errors option needs to go in your browser_types.xml file. It worked for me.\n<browserType>\n    <name>phantomjs</name>\n    <displayName>PhantomJS</displayName>\n    <icon>safari.png</icon>\n    <path>/usr/local/Cellar/phantomjs/1.9.2/bin/phantomjs</path>\n    <options>--ignore-ssl-errors=yes --debug=yes --proxy=localhost:9999 /usr/local/Cellar/phantomjs/phantom-sahi.js</options>\n    <processName>""PhantomJS""</processName>\n    <capacity>100</capacity>\n    <force>true</force>\n</browserType>\n\n', ""\nWhat about shebang?\nIf you're using shebang to execute phantomjs scripts, use the following shebang line\n#!/usr/bin/phantomjs --ignore-ssl-errors=yes\n    \nvar system = require('system');\nvar webpage = require('webpage');\n\n// ... rest of your script\n\nUse any of the above answers. i personally like --ignore-ssl-errors=yes since it's irrelevant to validate my loopback web servers' self-signed certificate.\n"", '\nNone of the other answers here helped me; it may be that the specific site(s) I was working with were too picky with their HTTP headers. This is what worked:\nvar page = webpage.create();\npage.customHeaders = {\n    ""Connection"": ""keep-alive""\n};\n\nI found out that PhantomJS was using ""Keep-Alive"" (capitalized), and the connection was not being kept alive. :)\n', '\nI was getting SSL Handshake Failed yesterday. I tried many combinations of phantomJS options (--ignore-ssl-errors=yes etc.), but none of them worked.\nUpgrading to phantomJS 2.1.1 fixed it. \nI used the phantomJS installation instructions at https://gist.github.com/julionc/7476620, changing the phantomJS version to 2.1.1.\n', '\nOn the machine you are trying to run phantomjs on to connect to a remote server, run ""openssl ciphers."" Copy and paste the ciphers listed into the --ssl-ciphers="""" command line option. This tells the connecting web server which ciphers are available to use to communicate with your client. If you don\'t set the ones available on your own machine, it can use any cipher your machine does not understand that the default modern browsers do that are used for the default setting.\n', '\nphantomjs --web-security=false --ignore-ssl-errors=true scripts.js\n', '\nThe only thing that worked for me was upping phantomjs from 1.9x to 2.x ;)\n']",https://stackoverflow.com/questions/12021578/phantomjs-failing-to-open-https-site,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web scraping with Python [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I'd like to grab daily sunrise/sunset times from a web site. Is it possible to scrape web content with Python? what are the modules used? Is there any tutorial available?
",214k,"
            196
        ","[""\nUse urllib2 in combination with the brilliant BeautifulSoup library:\nimport urllib2\nfrom BeautifulSoup import BeautifulSoup\n# or if you're using BeautifulSoup4:\n# from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(urllib2.urlopen('http://example.com').read())\n\nfor row in soup('table', {'class': 'spad'})[0].tbody('tr'):\n    tds = row('td')\n    print tds[0].string, tds[1].string\n    # will print date and sunrise\n\n"", ""\nI'd really recommend Scrapy.\nQuote from a deleted answer:\n\n\nScrapy crawling is fastest than mechanize because uses asynchronous operations (on top of Twisted).\nScrapy has better and fastest support for parsing (x)html on top of libxml2.\nScrapy is a mature framework with full unicode, handles redirections, gzipped responses, odd encodings, integrated http cache, etc.\nOnce you are into Scrapy, you can write a spider in less than 5 minutes that download images, creates thumbnails and export the extracted data directly to csv or json.\n\n\n"", '\nI collected together scripts from my web scraping work into this bit-bucket library.\nExample script for your case:\nfrom webscraping import download, xpath\nD = download.Download()\n\nhtml = D.get(\'http://example.com\')\nfor row in xpath.search(html, \'//table[@class=""spad""]/tbody/tr\'):\n    cols = xpath.search(row, \'/td\')\n    print \'Sunrise: %s, Sunset: %s\' % (cols[1], cols[2])\n\nOutput:\nSunrise: 08:39, Sunset: 16:08\nSunrise: 08:39, Sunset: 16:09\nSunrise: 08:39, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:11\nSunrise: 08:40, Sunset: 16:12\nSunrise: 08:40, Sunset: 16:13\n\n', ""\nI would strongly suggest checking out pyquery. It uses jquery-like (aka css-like) syntax which makes things really easy for those coming from that background.\nFor your case, it would be something like:\nfrom pyquery import *\n\nhtml = PyQuery(url='http://www.example.com/')\ntrs = html('table.spad tbody tr')\n\nfor tr in trs:\n  tds = tr.getchildren()\n  print tds[1].text, tds[2].text\n\nOutput:\n5:16 AM 9:28 PM\n5:15 AM 9:30 PM\n5:13 AM 9:31 PM\n5:12 AM 9:33 PM\n5:11 AM 9:34 PM\n5:10 AM 9:35 PM\n5:09 AM 9:37 PM\n\n"", ""\nYou can use urllib2 to make the HTTP requests, and then you'll have web content.\nYou can get it like this:\nimport urllib2\nresponse = urllib2.urlopen('http://example.com')\nhtml = response.read()\n\nBeautiful Soup is a python HTML parser that is supposed to be good for screen scraping.\nIn particular, here is their tutorial on parsing an HTML document.\nGood luck!\n"", '\nI use a combination of Scrapemark (finding urls - py2) and httlib2 (downloading images - py2+3). The scrapemark.py has 500 lines of code, but uses regular expressions, so it may be not so fast, did not test.\nExample for scraping your website:\n\nimport sys\nfrom pprint import pprint\nfrom scrapemark import scrape\n\npprint(scrape(""""""\n    <table class=""spad"">\n        <tbody>\n            {*\n                <tr>\n                    <td>{{[].day}}</td>\n                    <td>{{[].sunrise}}</td>\n                    <td>{{[].sunset}}</td>\n                    {# ... #}\n                </tr>\n            *}\n        </tbody>\n    </table>\n"""""", url=sys.argv[1] ))\n\nUsage:\npython2 sunscraper.py http://www.example.com/\n\nResult:\n[{\'day\': u\'1. Dez 2012\', \'sunrise\': u\'08:18\', \'sunset\': u\'16:10\'},\n {\'day\': u\'2. Dez 2012\', \'sunrise\': u\'08:19\', \'sunset\': u\'16:10\'},\n {\'day\': u\'3. Dez 2012\', \'sunrise\': u\'08:21\', \'sunset\': u\'16:09\'},\n {\'day\': u\'4. Dez 2012\', \'sunrise\': u\'08:22\', \'sunset\': u\'16:09\'},\n {\'day\': u\'5. Dez 2012\', \'sunrise\': u\'08:23\', \'sunset\': u\'16:08\'},\n {\'day\': u\'6. Dez 2012\', \'sunrise\': u\'08:25\', \'sunset\': u\'16:08\'},\n {\'day\': u\'7. Dez 2012\', \'sunrise\': u\'08:26\', \'sunset\': u\'16:07\'}]\n\n', '\nMake your life easier by using CSS Selectors\nI know I have come late to party but I have a nice suggestion for you.\nUsing BeautifulSoup is already been suggested I would rather prefer using CSS Selectors to scrape data inside HTML\nimport urllib2\nfrom bs4 import BeautifulSoup\n\nmain_url = ""http://www.example.com""\n\nmain_page_html  = tryAgain(main_url)\nmain_page_soup = BeautifulSoup(main_page_html)\n\n# Scrape all TDs from TRs inside Table\nfor tr in main_page_soup.select(""table.class_of_table""):\n   for td in tr.select(""td#id""):\n       print(td.text)\n       # For acnhors inside TD\n       print(td.select(""a"")[0].text)\n       # Value of Href attribute\n       print(td.select(""a"")[0][""href""])\n\n# This is method that scrape URL and if it doesnt get scraped, waits for 20 seconds and then tries again. (I use it because my internet connection sometimes get disconnects)\ndef tryAgain(passed_url):\n    try:\n        page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n        return page\n    except Exception:\n        while 1:\n            print(""Trying again the URL:"")\n            print(passed_url)\n            try:\n                page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n                print(""-------------------------------------"")\n                print(""---- URL was successfully scraped ---"")\n                print(""-------------------------------------"")\n                return page\n            except Exception:\n                time.sleep(20)\n                continue \n\n', '\nIf we think of getting name of items from any specific category then we can do that by specifying the class name of that category using css selector:\nimport requests ; from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(requests.get(\'https://www.flipkart.com/\').text, ""lxml"")\nfor link in soup.select(\'div._2kSfQ4\'):\n    print(link.text)\n\nThis is the partial search results:\nPuma, USPA, Adidas & moreUp to 70% OffMen\'s Shoes\nShirts, T-Shirts...Under 鈧?99For Men\nNike, UCB, Adidas & moreUnder 鈧?99Men\'s Sandals, Slippers\nPhilips & moreStarting 鈧?9LED Bulbs & Emergency Lights\n\n', '\nHere is a simple web crawler, i used BeautifulSoup and we will search for all the links(anchors) who\'s class name is _3NFO0d. I used Flipkar.com, it is an online retailing store.\nimport requests\nfrom bs4 import BeautifulSoup\ndef crawl_flipkart():\n    url = \'https://www.flipkart.com/\'\n    source_code = requests.get(url)\n    plain_text = source_code.text\n    soup = BeautifulSoup(plain_text, ""lxml"")\n    for link in soup.findAll(\'a\', {\'class\': \'_3NFO0d\'}):\n        href = link.get(\'href\')\n        print(href)\n\ncrawl_flipkart()\n\n', '\nPython has good options to scrape the web. The best one with a framework is scrapy. It can be a little tricky for beginners, so here is a little help. \n1. Install python above 3.5 (lower ones till 2.7 will work). \n2. Create a environment in conda ( I did this). \n3. Install scrapy at a location and run in from there. \n4. Scrapy shell will give you an interactive interface to test you code. \n5. Scrapy startproject projectname will create a framework.\n6. Scrapy genspider spidername will create a spider. You can create as many spiders as you want. While doing this make sure you are inside the project directory. \n\n\nThe easier one is to use requests and beautiful soup. Before starting give one hour of time to go through the documentation, it will solve most of your doubts. BS4 offer wide range of parsers that you can opt for. Use user-agent and sleep to make scraping easier. BS4 returns a bs.tag so use variable[0]. If there is js running, you wont be able to scrape using requests and bs4 directly. You  could get the api link then parse the JSON to get the information you need or try selenium.  \n']",https://stackoverflow.com/questions/2081586/web-scraping-with-python,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scrape html generated by javascript with python,"
I need to scrape a site with python. I obtain the source html code with the urlib module, but I need to scrape also some html code that is generated by a javascript function (which is included in the html source). What this functions does ""in"" the site is that when you press a button it outputs some html code. How can I ""press"" this button with python code? Can scrapy help me? I captured the POST request with firebug but when I try to pass it on the url I get a 403 error. Any suggestions?
",18k,"
            18
        ","['\nIn Python, I think Selenium 1.0 is the way to go. It鈥檚 a library that allows you to control a real web browser from your language of choice.\nYou need to have the web browser in question installed on the machine your script runs on, but it looks like the most reliable way to programmatically interrogate websites that use a lot of JavaScript.\n', ""\nSince there is no comprehensive answer here, I'll go ahead and write one.\nTo scrape off JS rendered pages, we will need a browser that has a JavaScript engine (e.i, support JavaScript rendering)\nOptions like Mechanize, url2lib will not work since they DO NOT support JavaScript. \nSo here's what you do:\nSetup PhantomJS to run with Selenium. After installing the dependencies for both of them (refer this), you can use the following code as an example to fetch the fully rendered website.\nfrom selenium import webdriver\n\ndriver = webdriver.PhantomJS()\ndriver.get('http://jokes.cc.com/')\nsoupFromJokesCC = BeautifulSoup(driver.page_source) #page_source fetches page after rendering is complete\ndriver.save_screenshot('screen.png') # save a screenshot to disk\n\ndriver.quit()\n\n"", '\nI have had to do this before (in .NET) and you are basically going to have to host a browser, get it to click the button, and then interrogate the DOM (document object model) of the browser to get at the generated HTML.\nThis is definitely one of the downsides to web apps moving towards an Ajax/Javascript approach to generating HTML client-side.\n', '\nI use webkit, which is the browser renderer behind Chrome and Safari. There are Python bindings to webkit through Qt. And here is a full example to execute JavaScript and extract the final HTML.\n', ""\nFor Scrapy (great python scraping framework) there is scrapyjs: an additional downloader handler / middleware handler able to scraping javascript generated content.\nIt's based on webkit engine by pygtk, python-webkit, and python-jswebkit and it's quite simple.\n""]",https://stackoverflow.com/questions/2148493/scrape-html-generated-by-javascript-with-python,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jsoup posting and cookie,"
I'm trying to use jsoup to login to a site and then scrape information, I am running into in a problem, I can login successfully and create a Document from index.php but I cannot get other pages on the site. I know I need to set a cookie after I post and then load it when I'm trying to open another page on the site. But how do I do this? The following code lets me login and get index.php
Document doc = Jsoup.connect(""http://www.example.com/login.php"")
               .data(""username"", ""myUsername"", 
                     ""password"", ""myPassword"")
               .post();

I know I can use apache httpclient to do this but I don't want to. 
",64k,"
            52
        ","['\nWhen you login to the site, it is probably setting an authorised session cookie that needs to be sent on subsequent requests to maintain the session.\nYou can get the cookie like this:\nConnection.Response res = Jsoup.connect(""http://www.example.com/login.php"")\n    .data(""username"", ""myUsername"", ""password"", ""myPassword"")\n    .method(Method.POST)\n    .execute();\n\nDocument doc = res.parse();\nString sessionId = res.cookie(""SESSIONID""); // you will need to check what the right cookie name is\n\nAnd then send it on the next request like:\nDocument doc2 = Jsoup.connect(""http://www.example.com/otherPage"")\n    .cookie(""SESSIONID"", sessionId)\n    .get();\n\n', '\n//This will get you the response.\nResponse res = Jsoup\n    .connect(""loginPageUrl"")\n    .data(""loginField"", ""login@login.com"", ""passField"", ""pass1234"")\n    .method(Method.POST)\n    .execute();\n\n//This will get you cookies\nMap<String, String> loginCookies = res.cookies();\n\n//And this is the easiest way I\'ve found to remain in session\nDocument doc = Jsoup.connect(""urlYouNeedToBeLoggedInToAccess"")\n      .cookies(loginCookies)\n      .get();\n\n', '\nWhere the code was:\nDocument doc = Jsoup.connect(""urlYouNeedToBeLoggedInToAccess"").cookies().get(); \n\nI was having difficulties until I changed it to:\nDocument doc = Jsoup.connect(""urlYouNeedToBeLoggedInToAccess"").cookies(cookies).get();\n\nNow it is working flawlessly.\n', '\nHere is what you can try...\nimport org.jsoup.Connection;\n\n\nConnection.Response res = null;\n    try {\n        res = Jsoup\n                .connect(""http://www.example.com/login.php"")\n                .data(""username"", ""your login id"", ""password"", ""your password"")\n                .method(Connection.Method.POST)\n                .execute();\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n\nNow save all your cookies and make request to the other page you want.\n//Store Cookies\ncookies = res.cookies();\n\nMaking request to another page.\ntry {\n    Document doc = Jsoup.connect(""your-second-page-link"").cookies(cookies).get();\n}\ncatch(Exception e){\n    e.printStackTrace();\n}\n\nAsk if further help needed.\n', '\nConnection.Response res = Jsoup.connect(""http://www.example.com/login.php"")\n    .data(""username"", ""myUsername"")\n    .data(""password"", ""myPassword"")\n    .method(Connection.Method.POST)\n    .execute();\n//Connecting to the server with login details\nDocument doc = res.parse();\n//This will give the redirected file\nMap<String,String> cooki=res.cookies();\n//This gives the cookies stored into cooki\nDocument docs= Jsoup.connect(""http://www.example.com/otherPage"")\n    .cookies(cooki)\n    .get();\n//This gives the data of the required website\n\n', '\nWhy reconnect?\nif there are any cookies to avoid 403 Status i do so.\n                Document doc = null;\n                int statusCode = -1;\n                String statusMessage = null;\n                String strHTML = null;\n        \n                try {\n    // connect one time.                \n                    Connection con = Jsoup.connect(urlString);\n    // get response.\n                    Connection.Response res = con.execute();        \n    // get cookies\n                    Map<String, String> loginCookies = res.cookies();\n\n    // print cookie content and status message\n                    if (loginCookies != null) {\n                        for (Map.Entry<String, String> entry : loginCookies.entrySet()) {\n                            System.out.println(entry.getKey() + "":"" + entry.getValue().toString() + ""\\n"");\n                        }\n                    }\n        \n                    statusCode = res.statusCode();\n                    statusMessage = res.statusMessage();\n                    System.out.print(""Status CODE\\n"" + statusCode + ""\\n\\n"");\n                    System.out.print(""Status Message\\n"" + statusMessage + ""\\n\\n"");\n        \n    // set login cookies to connection here\n                    con.cookies(loginCookies).userAgent(""Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0"");\n        \n    // now do whatever you want, get document for example\n                    doc = con.get();\n    // get HTML\n                    strHTML = doc.head().html();\n\n                } catch (org.jsoup.HttpStatusException hse) {\n                    hse.printStackTrace();\n                } catch (IOException ioe) {\n                    ioe.printStackTrace();\n                }\n\n']",https://stackoverflow.com/questions/6432970/jsoup-posting-and-cookie,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to implement a web scraper in PHP? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 7 years ago.







                        Improve this question
                    



What built-in PHP functions are useful for web scraping?  What are some good resources (web or print) for getting up to speed on web scraping with PHP?
",81k,"
            61
        ","['\nScraping generally encompasses 3 steps: \n\nfirst you GET or POST your request\nto a specified URL \nnext you receive\n    the html that is returned as the\n    response\nfinally you parse out of\n    that html the text you\'d like to\n    scrape.\n\nTo accomplish steps 1 and 2, below is a simple php class which uses Curl to fetch webpages using either GET or POST.  After you get the HTML back, you just use Regular Expressions to accomplish step 3 by parsing out the text you\'d like to scrape.\nFor regular expressions, my favorite tutorial site is the following:\nRegular Expressions Tutorial\nMy Favorite program for working with RegExs is Regex Buddy.  I would advise you to try the demo of that product even if you have no intention of buying it.  It is an invaluable tool and will even generate code for your regexs you make in your language of choice (including php).\nUsage:\n\n\n$curl = new Curl();\n$html = $curl->get(""http://www.google.com"");\n\n// now, do your regex work against $html\n\nPHP Class:\n\n\n<?php\n\nclass Curl\n{       \n\n    public $cookieJar = """";\n\n    public function __construct($cookieJarFile = \'cookies.txt\') {\n        $this->cookieJar = $cookieJarFile;\n    }\n\n    function setup()\n    {\n\n\n        $header = array();\n        $header[0] = ""Accept: text/xml,application/xml,application/xhtml+xml,"";\n        $header[0] .= ""text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5"";\n        $header[] =  ""Cache-Control: max-age=0"";\n        $header[] =  ""Connection: keep-alive"";\n        $header[] = ""Keep-Alive: 300"";\n        $header[] = ""Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7"";\n        $header[] = ""Accept-Language: en-us,en;q=0.5"";\n        $header[] = ""Pragma: ""; // browsers keep this blank.\n\n\n        curl_setopt($this->curl, CURLOPT_USERAGENT, \'Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US; rv:1.8.1.7) Gecko/20070914 Firefox/2.0.0.7\');\n        curl_setopt($this->curl, CURLOPT_HTTPHEADER, $header);\n        curl_setopt($this->curl,CURLOPT_COOKIEJAR, $this->cookieJar); \n        curl_setopt($this->curl,CURLOPT_COOKIEFILE, $this->cookieJar);\n        curl_setopt($this->curl,CURLOPT_AUTOREFERER, true);\n        curl_setopt($this->curl,CURLOPT_FOLLOWLOCATION, true);\n        curl_setopt($this->curl,CURLOPT_RETURNTRANSFER, true);  \n    }\n\n\n    function get($url)\n    { \n        $this->curl = curl_init($url);\n        $this->setup();\n\n        return $this->request();\n    }\n\n    function getAll($reg,$str)\n    {\n        preg_match_all($reg,$str,$matches);\n        return $matches[1];\n    }\n\n    function postForm($url, $fields, $referer=\'\')\n    {\n        $this->curl = curl_init($url);\n        $this->setup();\n        curl_setopt($this->curl, CURLOPT_URL, $url);\n        curl_setopt($this->curl, CURLOPT_POST, 1);\n        curl_setopt($this->curl, CURLOPT_REFERER, $referer);\n        curl_setopt($this->curl, CURLOPT_POSTFIELDS, $fields);\n        return $this->request();\n    }\n\n    function getInfo($info)\n    {\n        $info = ($info == \'lasturl\') ? curl_getinfo($this->curl, CURLINFO_EFFECTIVE_URL) : curl_getinfo($this->curl, $info);\n        return $info;\n    }\n\n    function request()\n    {\n        return curl_exec($this->curl);\n    }\n}\n\n?>\n\n\n', '\nI recommend Goutte, a simple PHP Web Scraper.\nExample Usage:-\nCreate a Goutte Client instance (which extends\nSymfony\\Component\\BrowserKit\\Client):\nuse Goutte\\Client;\n\n$client = new Client();\n\nMake requests with the request() method:\n$crawler = $client->request(\'GET\', \'http://www.symfony-project.org/\');\n\nThe request method returns a Crawler object\n(Symfony\\Component\\DomCrawler\\Crawler).\nClick on links:\n$link = $crawler->selectLink(\'Plugins\')->link();\n$crawler = $client->click($link);\n\nSubmit forms:\n$form = $crawler->selectButton(\'sign in\')->form();\n$crawler = $client->submit($form, array(\'signin[username]\' => \'fabien\', \'signin[password]\' => \'xxxxxx\'));\n\nExtract data:\n$nodes = $crawler->filter(\'.error_list\');\n\nif ($nodes->count())\n{\n  die(sprintf(""Authentification error: %s\\n"", $nodes->text()));\n}\n\nprintf(""Nb tasks: %d\\n"", $crawler->filter(\'#nb_tasks\')->text());\n\n', '\nScraperWiki is a pretty interesting project.\nHelps you build scrapers online in Python, Ruby or PHP - i was able to get a simple attempt up in a few minutes.\n', ""\nIf you need something that is easy to maintain, rather than fast to execute, it could help to use a scriptable browser, such as SimpleTest's.\n"", ""\nScraping can be pretty complex, depending on what you want to do. Have a read of this tutorial series on The Basics Of Writing A Scraper In PHP and see if you can get to grips with it.\nYou can use similar methods to automate form sign ups, logins, even fake clicking on Ads! The main limitations with using CURL though are that it doesn't support using javascript, so if you are trying to scrape a site that uses AJAX for pagination for example it can become a little tricky...but again there are ways around that!\n"", '\nhere is another one: a simple PHP Scraper without Regex.\n', '\nfile_get_contents() can take a remote URL and give you the source. You can then use regular expressions (with the Perl-compatible functions) to grab what you need.\nOut of curiosity, what are you trying to scrape?\n', ""\nI'd either use libcurl or Perl's LWP (libwww for perl). Is there a libwww for php?\n"", '\nScraper class from my framework:\n<?php\n\n/*\n    Example:\n\n    $site = $this->load->cls(\'scraper\', \'http://www.anysite.com\');\n    $excss = $site->getExternalCSS();\n    $incss = $site->getInternalCSS();\n    $ids = $site->getIds();\n    $classes = $site->getClasses();\n    $spans = $site->getSpans(); \n\n    print \'<pre>\';\n    print_r($excss);\n    print_r($incss);\n    print_r($ids);\n    print_r($classes);\n    print_r($spans);        \n\n*/\n\nclass scraper\n{\n    private $url = \'\';\n\n    public function __construct($url)\n    {\n        $this->url = file_get_contents(""$url"");\n    }\n\n    public function getInternalCSS()\n    {\n        $tmp = preg_match_all(\'/(style="")(.*?)("")/is\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getExternalCSS()\n    {\n        $tmp = preg_match_all(\'/(href="")(\\w.*\\.css)""/i\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getIds()\n    {\n        $tmp = preg_match_all(\'/(id=""(\\w*)"")/is\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getClasses()\n    {\n        $tmp = preg_match_all(\'/(class=""(\\w*)"")/is\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getSpans(){\n        $tmp = preg_match_all(\'/(<span>)(.*)(<\\/span>)/\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n}\n?>\n\n', '\nThe curl library allows you to download web pages. You should look into regular expressions for doing the scraping.\n']",https://stackoverflow.com/questions/26947/how-to-implement-a-web-scraper-in-php,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Headless Browser for Python (Javascript support REQUIRED!) [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 Questions asking us to recommend or find a tool, library or favorite off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, describe the problem and what has been done so far to solve it.


Closed 8 years ago.







                        Improve this question
                    



I need a headless browser which is fairly easy to use (I am still fairly new to Python and programming in general) which will allow me to navigate to a page, log into a form that requires Javascript, and then scrape the resulting web page by searching for results matching certain criteria, clicking check boxes, and clicking to download files. All of this requires Javascript.
I hear a headless browser is what I want - requirements/preferences are that I be able to run it from Python, and preferably that the resultant script will be compilable by py2exe (I am writing this program for other users).
So far Windmill looks like it MIGHT be what I want, but I am not sure.
Any ideas appreciated!
",43k,"
            57
        ","['\nI use webkit as a headless browser in Python via pyqt / pyside:\nhttp://www.riverbankcomputing.co.uk/software/pyqt/download\nhttp://developer.qt.nokia.com/wiki/Category:LanguageBindings::PySide::Downloads\nI particularly like webkit because it is simple to setup. For Ubuntu you just use: sudo apt-get install python-qt4\nHere is an example script:\nhttp://webscraping.com/blog/Scraping-JavaScript-webpages-with-webkit/\n', '\nThe answer to this question was Spynner\n', '\nI\'m in the midst of writing a Python driver for Zombie.js, ""a lightweight framework for testing client-side JavaScript code in a simulated environment"".\nI\'m currently at a standstill on a resolution to a bug in Node.js (before I write more tests and more code), but feel free to keep an eye on my project as it progresses:\nhttps://github.com/ryanpetrello/python-zombie\n', '\nThere are not too many headless browsers yet that support Javascript.\nYou could try Zombie.js or Phantomjs. Those are not Python, but plain Javascript and those really can do the job.\n', '\nTry using phantomjs, it has great javascript support.  Then you could run it as a subprocess of a python script \nhttp://docs.python.org/library/subprocess.html\nthat could boss it around.\n', '\nYou can use HTQL in combination with IRobotSoft webscraper.  Check here for examples: http://htql.net/\n']",https://stackoverflow.com/questions/6025082/headless-browser-for-python-javascript-support-required,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is there a PHP equivalent of Perl's WWW::Mechanize?,"
I'm looking for a library that has functionality similar to Perl's WWW::Mechanize, but for PHP. Basically, it should allow me to submit HTTP GET and POST requests with a simple syntax, and then parse the resulting page and return in a simple format all forms and their fields, along with all links on the page.
I know about CURL, but it's a little too barebones, and the syntax is pretty ugly (tons of curl_foo($curl_handle, ...) statements
Clarification:
I want something more high-level than the answers so far. For example, in Perl, you could do something like:
# navigate to the main page
$mech->get( 'http://www.somesite.com/' ); 

# follow a link that contains the text 'download this'
$mech->follow_link( text_regex => qr/download this/i );

# submit a POST form, to log into the site
$mech->submit_form(
    with_fields      => {
        username    => 'mungo',
        password    => 'lost-and-alone',
    }
);

# save the results as a file
$mech->save_content('somefile.zip');

To do the same thing using HTTP_Client or wget or CURL would be a lot of work, I'd have to manually parse the pages to find the links, find the form URL, extract all the hidden fields, and so on. The reason I'm asking for a PHP solution is that I have no experience with Perl, and I could probably build what I need with a lot of work, but it would be much quicker if I could do the above in PHP.
",15k,"
            26
        ","[""\nSimpleTest's ScriptableBrowser can be used independendly from the testing framework. I've used it for numerous automation-jobs.\n"", '\nI feel compelled to answer this, even though its an old post... I\'ve been working with PHP curl a lot and it is not as good anywhere near comparable to something like WWW:Mechanize, which I am switching to (I think I am going to go with the Ruby language implementation).. Curl is outdated as it requires too much ""grunt work"" to automate anything, the simpletest scriptable browser looked promising to me but in testing it, it won\'t work on most web forms I try it on... honestly, I think PHP is lacking in this category of scraping, web automation so its best to look at a different language, just wanted to post this since I have spent countless hours on this topic and maybe it will save someone else some time in the future.\n', '\nIt\'s 2016 now and there\'s Mink. It even supports different engines from headless pure-PHP ""browser"" (without JavaScript), over Selenium (which needs a browser like Firefox or Chrome) to a headless ""browser.js"" in NPM, which DOES support JavaScript.\n', '\nTry looking in the PEAR library. If all else fails, create an object wrapper for curl.\nYou can so something simple like this:\nclass curl {\n    private $resource;\n\n    public function __construct($url) {\n        $this->resource = curl_init($url);\n    }\n\n    public function __call($function, array $params) {\n        array_unshift($params, $this->resource);\n        return call_user_func_array(""curl_$function"", $params);\n    }\n}\n\n', ""\nTry one of the following:\n\nPEAR's HTTP_Request\nZend_Http_Client\n\n(Yes, it's ZendFramework code, but it doesn't make your class slower using it since it just loads the required libs.)\n"", '\nLook into Snoopy:\nhttp://sourceforge.net/projects/snoopy/\n', '\nCurl is the way to go for simple requests. It runs cross platform, has a PHP extension and is widely adopted and tested.\nI created a nice class that can GET and POST an array of data (INCLUDING FILES!) to a url by just calling CurlHandler::Get($url, $data) || CurlHandler::Post($url, $data). There\'s an optional HTTP User authentication option too :)\n/**\n * CURLHandler handles simple HTTP GETs and POSTs via Curl \n * \n * @package Pork\n * @author SchizoDuckie\n * @copyright SchizoDuckie 2008\n * @version 1.0\n * @access public\n */\nclass CURLHandler\n{\n\n    /**\n     * CURLHandler::Get()\n     * \n     * Executes a standard GET request via Curl.\n     * Static function, so that you can use: CurlHandler::Get(\'http://www.google.com\');\n     * \n     * @param string $url url to get\n     * @return string HTML output\n     */\n    public static function Get($url)\n    {\n       return self::doRequest(\'GET\', $url);\n    }\n\n    /**\n     * CURLHandler::Post()\n     * \n     * Executes a standard POST request via Curl.\n     * Static function, so you can use CurlHandler::Post(\'http://www.google.com\', array(\'q\'=>\'StackOverFlow\'));\n     * If you want to send a File via post (to e.g. PHP\'s $_FILES), prefix the value of an item with an @ ! \n     * @param string $url url to post data to\n     * @param Array $vars Array with key=>value pairs to post.\n     * @return string HTML output\n     */\n    public static function Post($url, $vars, $auth = false) \n    {\n       return self::doRequest(\'POST\', $url, $vars, $auth);\n    }\n\n    /**\n     * CURLHandler::doRequest()\n     * This is what actually does the request\n     * <pre>\n     * - Create Curl handle with curl_init\n     * - Set options like CURLOPT_URL, CURLOPT_RETURNTRANSFER and CURLOPT_HEADER\n     * - Set eventual optional options (like CURLOPT_POST and CURLOPT_POSTFIELDS)\n     * - Call curl_exec on the interface\n     * - Close the connection\n     * - Return the result or throw an exception.\n     * </pre>\n     * @param mixed $method Request Method (Get/ Post)\n     * @param mixed $url URI to get or post to\n     * @param mixed $vars Array of variables (only mandatory in POST requests)\n     * @return string HTML output\n     */\n    public static function doRequest($method, $url, $vars=array(), $auth = false)\n    {\n        $curlInterface = curl_init();\n\n        curl_setopt_array ($curlInterface, array( \n            CURLOPT_URL => $url,\n            CURLOPT_RETURNTRANSFER => 1,\n            CURLOPT_FOLLOWLOCATION =>1,\n            CURLOPT_HEADER => 0));\n        if (strtoupper($method) == \'POST\')\n        {\n            curl_setopt_array($curlInterface, array(\n                CURLOPT_POST => 1,\n                CURLOPT_POSTFIELDS => http_build_query($vars))\n            );  \n        }\n        if($auth !== false)\n        {\n              curl_setopt($curlInterface, CURLOPT_USERPWD, $auth[\'username\'] . "":"" . $auth[\'password\']);\n        }\n        $result = curl_exec ($curlInterface);\n        curl_close ($curlInterface);\n\n        if($result === NULL)\n        {\n            throw new Exception(\'Curl Request Error: \'.curl_errno($curlInterface) . "" - "" . curl_error($curlInterface));\n        }\n        else\n        {\n            return($result);\n        }\n    }\n\n}\n\n?>\n\n[edit] Read the clarification only now... You probably want to go with one of the tools mentioned above that automates stuff. You could also decide to use a clientside firefox extension like ChickenFoot for more flexibility. I\'ll leave the example class above here for future searches.\n', '\nIf you\'re using CakePHP in your project, or if you\'re inclined to extract the relevant library you can use their curl wrapper HttpSocket. It has the simple page-fetching syntax you describe, e.g., \n# This is the sugar for importing the library within CakePHP       \nApp::import(\'Core\', \'HttpSocket\');\n$HttpSocket = new HttpSocket();\n\n$result = $HttpSocket->post($login_url,\narray(\n  ""username"" => ""username"",\n  ""password"" => ""password""\n)\n);\n\n...although it doesn\'t have a way to parse the response page. For that I\'m going to use simplehtmldom: http://net.tutsplus.com/tutorials/php/html-parsing-and-screen-scraping-with-the-simple-html-dom-library/ which describes itself as having a jQuery-like syntax.\nI tend to agree that the bottom line is that PHP doesn\'t have the awesome scraping/automation libraries that Perl/Ruby have.\n', ""\nIf you're on a *nix system you could use shell_exec() with wget, which has a lot of nice options.\n""]",https://stackoverflow.com/questions/199045/is-there-a-php-equivalent-of-perls-wwwmechanize,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executing Javascript from Python,"
I have HTML webpages that I am crawling using xpath. The etree.tostring of a certain node gives me this string:
<script>
<!--
function escramble_758(){
  var a,b,c
  a='+1 '
  b='84-'
  a+='425-'
  b+='7450'
  c='9'
  document.write(a+c+b)
}
escramble_758()
//-->
</script>

I just need the output of escramble_758(). I can write a regex to figure out the whole thing, but I want my code to remain tidy. What is the best alternative?
I am zipping through the following libraries, but I didnt see an exact solution. Most of them are trying to emulate browser, making things snail slow.

http://code.google.com/p/python-spidermonkey/ (clearly says it's not yet possible to call a function defined in Javascript)
http://code.google.com/p/webscraping/ (don't see anything for Javascript, I may be wrong)
http://pypi.python.org/pypi/selenium (Emulating browser)

Edit: An example will be great.. (barebones will do)
",176k,"
            67
        ","['\nYou can also use Js2Py which is written in pure python and is able to both execute and translate javascript to python. Supports virtually whole JavaScript even labels, getters, setters and other rarely used features. \nimport js2py\n\njs = """"""\nfunction escramble_758(){\nvar a,b,c\na=\'+1 \'\nb=\'84-\'\na+=\'425-\'\nb+=\'7450\'\nc=\'9\'\ndocument.write(a+c+b)\n}\nescramble_758()\n"""""".replace(""document.write"", ""return "")\n\nresult = js2py.eval_js(js)  # executing JavaScript and converting the result to python string \n\nAdvantages of Js2Py include portability and extremely easy integration with python (since basically JavaScript is being translated to python). \nTo install:\npip install js2py\n\n', '\nUsing PyV8, I can do this. However, I have to replace document.write with return because there\'s no DOM and therefore no document.\nimport PyV8\nctx = PyV8.JSContext()\nctx.enter()\n\njs = """"""\nfunction escramble_758(){\nvar a,b,c\na=\'+1 \'\nb=\'84-\'\na+=\'425-\'\nb+=\'7450\'\nc=\'9\'\ndocument.write(a+c+b)\n}\nescramble_758()\n""""""\n\nprint ctx.eval(js.replace(""document.write"", ""return ""))\n\nOr you could create a mock document object\nclass MockDocument(object):\n\n    def __init__(self):\n        self.value = \'\'\n\n    def write(self, *args):\n        self.value += \'\'.join(str(i) for i in args)\n\n\nclass Global(PyV8.JSClass):\n    def __init__(self):\n        self.document = MockDocument()\n\nscope = Global()\nctx = PyV8.JSContext(scope)\nctx.enter()\nctx.eval(js)\nprint scope.document.value\n\n', '\nOne more solution as PyV8 seems to be unmaintained and dependent on the old version of libv8. \nPyMiniRacer It\'s a wrapper around the v8 engine and it works with the new version and is actively maintained.\npip install py-mini-racer\nfrom py_mini_racer import py_mini_racer\nctx = py_mini_racer.MiniRacer()\nctx.eval(""""""\nfunction escramble_758(){\n    var a,b,c\n    a=\'+1 \'\n    b=\'84-\'\n    a+=\'425-\'\n    b+=\'7450\'\n    c=\'9\'\n    return a+c+b;\n}\n"""""")\nctx.call(""escramble_758"")\n\nAnd yes, you have to replace document.write with return as others suggested\n', '\nYou can use js2py context to execute your js code and get output from document.write with mock document object:\nimport js2py\n\njs = """"""\nvar output;\ndocument = {\n    write: function(value){\n        output = value;\n    }\n}\n"""""" + your_script\n\ncontext = js2py.EvalJs()\ncontext.execute(js)\nprint(context.output)\n\n', '\nYou can use requests-html which will download and use chromium underneath.\nfrom requests_html import HTML\n\nhtml = HTML(html=""<a href=\'http://www.example.com/\'>"")\n\nscript = """"""\nfunction escramble_758(){\n    var a,b,c\n    a=\'+1 \'\n    b=\'84-\'\n    a+=\'425-\'\n    b+=\'7450\'\n    c=\'9\'\n    return a+c+b;\n}\n""""""\n\nval = html.render(script=script, reload=False)\nprint(val)\n# +1 425-984-7450\n\nMore on this read here\n', '\nquickjs should be the best option after quickjs come out.  Just pip install quickjs and you are ready to go.\nmodify based on the example on README.\nfrom quickjs import Function\n\njs = """"""\nfunction escramble_758(){\nvar a,b,c\na=\'+1 \'\nb=\'84-\'\na+=\'425-\'\nb+=\'7450\'\nc=\'9\'\ndocument.write(a+c+b)\nescramble_758()\n}\n""""""\n\nescramble_758 = Function(\'escramble_758\', js.replace(""document.write"", ""return ""))\n\nprint(escramble_758())\n\nhttps://github.com/PetterS/quickjs\n', '\nReally late to the party but you can use a successor of pyv8 which is regularly maintained by a reputable organization (Subjective) named CloudFlare. Here is the repository URL:\nhttps://github.com/cloudflare/stpyv8\n']",https://stackoverflow.com/questions/10136319/executing-javascript-from-python,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping ajax pages using python,"
I've already seen this question about scraping ajax, but python isn't mentioned there. I considered using scrapy, i believe they have some docs on that subject, but as you can see the website is down. So i don't know what to do. I want to do the following:
I only have one url, example.com you go from page to page by clicking submit, the url doesn't change since they're using ajax to display the content. I want to scrape the content of each page, how to do it? 
Lets say that i want to scrape only the numbers, is there anything other than scrapy that would do it? If not, would you give me a snippet on how to do it, just because their website is down so i can't reach the docs.
",47k,"
            18
        ","['\nFirst of all, scrapy docs are available at https://scrapy.readthedocs.org/en/latest/.\nSpeaking about handling ajax while web scraping. Basically, the idea is rather simple: \n\nopen browser developer tools, network tab\ngo to the target site\nclick submit button and see what XHR request is going to the server\nsimulate this XHR request in your spider\n\nAlso see:\n\nCan scrapy be used to scrape dynamic content from websites that are using AJAX?\nPagination using scrapy\n\nHope that helps.\n', '\nI found the answer very useful but I would like to make it more simple.\nresponse = requests.post(request_url, data=payload, headers=request_headers)\n\nrequest.post takes three parameters url, data and headers. Values for these three attributes can be found in the XHR request.\nCopy the whole request header and form data to load into the above variables and you are good to go\n']",https://stackoverflow.com/questions/16390257/scraping-ajax-pages-using-python,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTML Scraping in Php [duplicate],"






This question already has answers here:
                        
                    



How do you parse and process HTML/XML in PHP?

                                (31 answers)
                            

Closed 9 years ago.



I've been doing some HTML scraping in PHP using regular expressions.  This works, but the result is finicky and fragile.  Has anyone used any packages that provide a more robust solution?  A config driven solution would be ideal, but I'm not picky.
",46k,"
            39
        ","['\nI would recomend PHP Simple HTML DOM Parser after you have scraped the HTML from the page. It supports invalid HTML, and provides a very easy way to handle HTML elements. \n', ""\nIf the page you're scraping is valid X(HT)ML, then any of PHP's built-in XML parsers will do. \nI haven't had much success with PHP libraries for scraping. If you're adventurous though, you can try simplehtmldom. I'd recommend Hpricot for Ruby or Beautiful Soup for Python, which are both excellent parsers for HTML.\n"", ""\nI would also recommend 'Simple HTML DOM Parser.' It is a good option particularly if your familiar with jQuery or JavaScript selectors then you will find yourself at home.\nI have even blogged about it in the past.\n"", '\nI had some fun working with htmlSQL, which is not so much a high end solution, but really simple to work with.\n', ""\nUsing PHP for HTML scraping, I'd recommend cURL + regexp or cURL + some DOM parsers though I personally use cURL + regexp. If you have a profound taste of regexp, it's actually more accurate sometimes.\n"", ""\nI've had very good with results with the Simple Html DOM Parser mentioned above as well. And then there's the \xa0tidy Extension for PHP as well which works really well too.\n"", '\nI had to use curl on my host 1and1.\nhttp://www.quickscrape.com/ is what I came up with using the Simple DOM class!\n']",https://stackoverflow.com/questions/34120/html-scraping-in-php,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What's a good tool to screen-scrape with Javascript support? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



Is there a good test suite or tool set that can automate website navigation -- with Javascript support -- and collect the HTML from the pages?
Of course I can scrape straight HTML with BeautifulSoup.  But this does me no good for sites that require Javascript. :)
",26k,"
            28
        ","[""\nYou could use Selenium or Watir to drive a real browser.\nTher are also some JavaScript-based headless browsers:\n\nPhantomJS is a headless Webkit browser.\n\npjscrape is a scraping framework based on PhantomJS and jQuery.\nCasperJS is a navigation scripting & testing utility bsaed on PhantomJS, if you need to do a little more than point at URLs to be scraped.\n\n\nZombie for Node.js\n\nPersonally, I'm most familiar with Selenium, which has support for writing automation scripts in a good number of languagues and has more mature tooling, such as the excellent Selenium IDE extension for Firefox, which can be used to write and run testcases, and can export test scripts to many languages.\n"", '\nUsing HtmlUnit is also a possibility.\n\nHtmlUnit is a ""GUI-Less browser for\n  Java programs"". It models HTML\n  documents and provides an API that\n  allows you to invoke pages, fill out\n  forms, click links, etc... just like\n  you do in your ""normal"" browser.\nIt has fairly good JavaScript support\n  (which is constantly improving) and is\n  able to work even with quite complex\n  AJAX libraries, simulating either\n  Firefox or Internet Explorer depending\n  on the configuration you want to use.\nIt is typically used for testing\n  purposes or to retrieve information\n  from web sites.\n\n', '\nSelenium now wraps htmlunit so you don麓t need start a browser anymore. The new WebDriver api is very easy to use too. The first example use htmlunit driver \n', ""\nIt would be very difficult to code a solution that would work with any arbitrary site out there.  Each navigation menu implementation can be quite unique.  I've worked a great deal with scrapers, and, provided you know the site you wish to target, here is how I'd approach it.\nUsually, if you analyze the particular javascript used in a nav menu, it is fairly easy to use regular expressions to pull out the entire set of variables that are used to build the navmenu.  I have never used Beautiful Soup, but from your description it sounds like it may only work on HTML elements and not be able to work inside the script tags.\nIf you're still having problems, or need to emulate some form POSTs or ajax, get Firefox and install the LiveHttpHeaders plugin.  This plugin will allow you to manually browse the site and capture the urls being navigated along with any cookies that are being passed during your manual browsing.  That is what you need your scraperbot to send in a request to get a valid response from the target webserver(s).   This will also capture any ajax calls being made, and in many cases the same ajax calls must be implementated in your scraper to get your desired responses.\n"", '\nMozenda is a great tool to use as well.\n', ""\nKeep in mind that and javascript fanciness is messing with the brower's internal DOM model of the page, and does nothing to the raw HTML.\n"", ""\nI've been using Selenium for this and it find that it works great.\nSelenium runs in Browser and will work with Firefox, Webkit and IE.\nhttp://selenium.openqa.org/\n"", '\n@insin Watir is not IE only.\nhttps://stackoverflow.com/questions/81566#83387\n']",https://stackoverflow.com/questions/125177/whats-a-good-tool-to-screen-scrape-with-javascript-support,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Download image file from the HTML page source,"
I am writing a scraper that downloads all the image files from a HTML page and saves them to a specific folder. All the images are part of the HTML page.
",101k,"
            48
        ","['\nHere is some code to download all the images from the supplied URL, and save them in the specified output folder. You can modify it to your own needs.\n""""""\ndumpimages.py\n    Downloads all the images on the supplied URL, and saves them to the\n    specified output file (""/test/"" by default)\n\nUsage:\n    python dumpimages.py http://example.com/ [output]\n""""""\nfrom bs4 import BeautifulSoup as bs\nfrom urllib.request import (\n    urlopen, urlparse, urlunparse, urlretrieve)\nimport os\nimport sys\n\ndef main(url, out_folder=""/test/""):\n    """"""Downloads all the images at \'url\' to /test/""""""\n    soup = bs(urlopen(url))\n    parsed = list(urlparse(url))\n\n    for image in soup.findAll(""img""):\n        print(""Image: %(src)s"" % image)\n        filename = image[""src""].split(""/"")[-1]\n        parsed[2] = image[""src""]\n        outpath = os.path.join(out_folder, filename)\n        if image[""src""].lower().startswith(""http""):\n            urlretrieve(image[""src""], outpath)\n        else:\n            urlretrieve(urlunparse(parsed), outpath)\n\ndef _usage():\n    print(""usage: python dumpimages.py http://example.com [outpath]"")\n\nif __name__ == ""__main__"":\n    url = sys.argv[-1]\n    out_folder = ""/test/""\n    if not url.lower().startswith(""http""):\n        out_folder = sys.argv[-1]\n        url = sys.argv[-2]\n        if not url.lower().startswith(""http""):\n            _usage()\n            sys.exit(-1)\n    main(url, out_folder)\n\nEdit: You can specify the output folder now.\n', '\nRyan\'s solution is good, but fails if the image source URLs are absolute URLs or anything that doesn\'t give a good result when simply concatenated to the main page URL.  urljoin recognizes absolute vs. relative URLs, so replace the loop in the middle with:\nfor image in soup.findAll(""img""):\n    print ""Image: %(src)s"" % image\n    image_url = urlparse.urljoin(url, image[\'src\'])\n    filename = image[""src""].split(""/"")[-1]\n    outpath = os.path.join(out_folder, filename)\n    urlretrieve(image_url, outpath)\n\n', '\nYou have to download the page and parse html document, find your image with regex and download it.. You can use urllib2 for downloading and Beautiful Soup for parsing html file.\n', '\nAnd this is function for download one image:\ndef download_photo(self, img_url, filename):\n    file_path = ""%s%s"" % (DOWNLOADED_IMAGE_PATH, filename)\n    downloaded_image = file(file_path, ""wb"")\n\n    image_on_web = urllib.urlopen(img_url)\n    while True:\n        buf = image_on_web.read(65536)\n        if len(buf) == 0:\n            break\n        downloaded_image.write(buf)\n    downloaded_image.close()\n    image_on_web.close()\n\n    return file_path\n\n', '\nUse htmllib to extract all img tags (override do_img), then use urllib2 to download all the images.\n', ""\nIf the request need an authorization refer to this one:\nr_img = requests.get(img_url, auth=(username, password)) \nf = open('000000.jpg','wb') \nf.write(r_img.content) \nf.close()\n\n"", '\nBased on code here\nRemoving some lines of code, you\'ll get only the images img tags.\nUses Python 3+ Requests, BeautifulSoup and other standard libraries.\nimport os, sys\nimport requests\nfrom urllib import parse\nfrom bs4 import BeautifulSoup\nimport re\n\ndef savePageImages(url, imagespath=\'images\'):\n    def soupfindnSave(pagefolder, tag2find=\'img\', inner=\'src\'):\n        if not os.path.exists(pagefolder): # create only once\n            os.mkdir(pagefolder)\n        for res in soup.findAll(tag2find):  \n            if res.has_attr(inner): # check inner tag (file object) MUST exists\n                try:\n                    filename, ext = os.path.splitext(os.path.basename(res[inner])) # get name and extension\n                    filename = re.sub(\'\\W+\', \'\', filename) + ext # clean special chars from name\n                    fileurl = parse.urljoin(url, res.get(inner))\n                    filepath = os.path.join(pagefolder, filename)\n                    if not os.path.isfile(filepath): # was not downloaded\n                        with open(filepath, \'wb\') as file:\n                            filebin = session.get(fileurl)\n                            file.write(filebin.content)\n                except Exception as exc:\n                    print(exc, file=sys.stderr)   \n    session = requests.Session()\n    #... whatever other requests config you need here\n    response = session.get(url)\n    soup = BeautifulSoup(response.text, ""html.parser"")\n    soupfindnSave(imagespath, \'img\', \'src\')\n\nUse like this bellow to save the google.com page images in a folder google_images:\nsavePageImages(\'https://www.google.com\', \'google_images\')\n\n', '\nimport urllib.request as req\n\nwith req.urlopen(image_link) as d, open(image_location, ""wb"") as image_object:\n    data = d.read()\n    image_object.write(data)\n\n']",https://stackoverflow.com/questions/257409/download-image-file-from-the-html-page-source,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to scroll down with Phantomjs to load dynamic content,"
I am trying to scrape links from a page that generates content dynamically as the user scroll down to the bottom (infinite scrolling). I have tried doing different things with Phantomjs but not able to gather links beyond first page. Let say the element at the bottom which loads content has class .has-more-items. It is available until final content is loaded while scrolling and then becomes unavailable in DOM (display:none). Here are the things I have tried-

Setting viewportSize to a large height right after var page = require('webpage').create();


page.viewportSize = {             width: 1600,            height: 10000,
          };


Using page.scrollPosition = { top: 10000, left: 0 } inside page.open but have no effect like-


page.open('http://example.com/?q=houston', function(status) {
   if (status == ""success"") {
      page.scrollPosition = { top: 10000, left: 0 };  
   }
});



Also tried putting it inside page.evaluate function but that gives 


Reference error: Can't find variable page


Tried using jQuery and JS code inside page.evaluate and page.open but to no avail-


$(""html, body"").animate({ scrollTop: $(document).height() }, 10,
  function() {
          //console.log('check for execution');
      });

as it is and also inside document.ready. Similarly for JS code-
window.scrollBy(0,10000)

as it is and also inside window.onload
I am really struck on it for 2 days now and not able to find a way. Any help or hint would be appreciated.
Update
I have found a helpful piece of code at https://groups.google.com/forum/?fromgroups=#!topic/phantomjs/8LrWRW8ZrA0
var hitRockBottom = false; while (!hitRockBottom) {
    // Scroll the page (not sure if this is the best way to do so...)
    page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };

    // Check if we've hit the bottom
    hitRockBottom = page.evaluate(function() {
        return document.querySelector("".has-more-items"") === null;
    }); }

Where .has-more-items is the element class I want to access which is available at the bottom of the page initially and as we scroll down, it moves further down until all data is loaded and then becomes unavailable.
However, when I tested it is clear that it is running into infinite loops without scrolling down (I render pictures to check). I have tried to replace page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 }; with codes from below as well (one at a time)
window.document.body.scrollTop = '1000';
location.href = "".has-more-items"";
page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };
document.location.href="".has-more-items"";

But nothing seems to work.
",36k,"
            47
        ","['\nFound a way to do it and tried to adapt to your situation. I didn\'t test the best way of finding the bottom of the page because I had a different context, but check the solution below. The thing here is that you have to wait a little for the page to load and javascript works asynchronously so you have to use setInterval or setTimeout (see) to achieve this.\npage.open(\'http://example.com/?q=houston\', function () {\n\n  // Check for the bottom div and scroll down from time to time\n  window.setInterval(function() {\n      // Check if there is a div with class="".has-more-items"" \n      // (not sure if there\'s a better way of doing this)\n      var count = page.content.match(/class="".has-more-items""/g);\n\n      if(count === null) { // Didn\'t find\n        page.evaluate(function() {\n          // Scroll to the bottom of page\n          window.document.body.scrollTop = document.body.scrollHeight;\n        });\n      }\n      else { // Found\n        // Do what you want\n        ...\n        phantom.exit();\n      }\n  }, 500); // Number of milliseconds to wait between scrolls\n\n});\n\n', '\nI know that it has been answered a long time ago, but I also found a solution to my specific scenario. The result is a piece of javascript that scrolls to the bottom of the page. It is optimized to reduce waiting time.\nIt is not written for PhantomJS by default, so that will have to be modified. However, for a beginner or someone who doesn\'t have root access, an Iframe with injected javascript (run Google Chrome with --disable-javascript parameter) is a good alternative method for scraping a smaller set of ajax pages. The main benefit is that it\'s easily debuggable, because you have a visual overview of what\'s going on with your scraper.\nfunction ScrollForAjax () {\n\n    scrollintervals = 50;\n    scrollmaxtime = 1000;\n\n    if(typeof(scrolltime)==""undefined""){\n        scrolltime = 0;\n    }\n\n    scrolldocheight1 = $(iframeselector).contents().find(""body"").height();\n\n    $(""body"").scrollTop(scrolldocheight1);\n    setTimeout(function(){\n\n        scrolldocheight2 = $(""body"").height();\n\n        if(scrolltime===scrollmaxtime || scrolltime>scrollmaxtime){\n            scrolltime = 0;\n            $(""body"").scrollTop(0);\n            ScrapeCurrentPage(iframeselector);\n        }\n\n        else if(scrolldocheight2>scrolldocheight1){\n            scrolltime = 0;\n            ScrollForAjax (iframeselector);\n        }\n\n        else if(scrolldocheight1>=scrolldocheight2){\n            ScrollForAjax (iframeselector);\n        }\n\n    },scrollintervals);\n\n    scrolltime += scrollintervals;\n}\n\nscrollmaxtime is a timeout variable. Hope this is useful to someone :)\n', '\nThe ""correct"" solution didn\'t work for me. And, from what I\'ve read CasperJS doesn\'t use window (but I may be wrong on that), which makes me doubt that window works.\nThe following works for me in the Firefox/Chrome console; but, doesn\'t work in CasperJS (within casper.evaluate function).\n$(document).scrollTop($(document).height());\n\nWhat did work for me in CasperJS was:\ncasper.scrollToBottom();\ncasper.wait(1000, function waitCb() {\n  casper.capture(""loadedContent.png"");\n});\n\nWhich, also worked when moving casper.capture into Casper\'s then function.\nHowever, the above solution won\'t work on some sites like Twitter; jQuery seems to break the casper.scrollToBottom() function, and I had to remove the clientScripts reference to jQuery when working within Twitter.\nvar casper = require(\'casper\').create({\n    clientScripts: [\n       // \'jquery.js\'\n    ]\n});\n\nSome websites (e.g. BoingBoing.net) seem to work fine with jQuery and CasperJS scrollToBottom(). Not sure why some sites work and others don\'t.\n', ""\nThe code snippet below work just fine for pinterest. I researched a lot to scrape pinterest without phantomjs but it is impossible to find the infinite scroll trigger link. I think the code below will help other infinite scroll web page to scrape.\npage.open(pageUrl).then(function (status) {\n    var count = 0;\n    // Scrolls to the bottom of page\n    function scroll2btm() {\n        if (count < 500) {\n            page.evaluate(function(limit) {\n                window.scrollTo(0, document.body.scrollHeight || document.documentElement.scrollHeight);\n                return document.getElementsByClassName('pinWrapper').length; // use desired contents (eg. pin) selector for count presence number\n            }).then(function(c) {\n                count = c;\n                console.log(count); // print no of content found to check\n            });\n            setTimeout(scroll2btm,3000);\n        } else {\n            // required number of item found\n        }\n    }\n    scroll2btm();\n});\n\n""]",https://stackoverflow.com/questions/16561582/how-to-scroll-down-with-phantomjs-to-load-dynamic-content,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simple Screen Scraping using jQuery,"
I have been playing with the idea of using a simple screen-scraper using jQuery and I am wondering if the following is possible.
I have simple HTML page and am making an attempt (if this is possible) to grab the contents of all of the list items from another page, like so:
Main Page:
<!-- jQuery -->
<script type='text/javascript'>
$(document).ready(function(){
$.getJSON(""[URL to other page]"",
  function(data){

    //Iterate through the <li> inside of the URL's data
    $.each(data.items, function(item){
      $(""<li/>"").value().appendTo(""#data"");
    });

  });
});
</script>

<!-- HTML -->
<html>
    <body>
       <div id='data'></div>
    </body>
</html>

Other Page:
//Html
<body>
    <p><b>Items to Scrape</b></p>   
    <ul>
        <li>I want to scrape what is here</li>
        <li>and what is here</li>
        <li>and here as well</li>
        <li>and append it in the main page</li>
    </ul>
</body>

So, is it possible using jQuery to pull all of the list item contents from an external page and append them inside of a div?
",101k,"
            46
        ","['\nUse $.ajax to load the other page into a variable, then create a temporary element and use .html() to set the contents to the value returned. Loop through the  element\'s children of nodeType 1 and keep their first children\'s nodeValues. If the external page is not on your web server you will need to proxy the file with your own web server.\nSomething like this:\n$.ajax({\n     url: ""/thePageToScrape.html"",\n     dataType: \'text\',\n     success: function(data) {\n          var elements = $(""<div>"").html(data)[0].getElementsByTagName(""ul"")[0].getElementsByTagName(""li"");\n          for(var i = 0; i < elements.length; i++) {\n               var theText = elements[i].firstChild.nodeValue;\n               // Do something here\n          }\n     }\n});\n\n', '\nSimple scraping with jQuery...\n// Get HTML from page\n$.get( \'http://example.com/\', function( html ) {\n\n    // Loop through elements you want to scrape content from\n    $(html).find(""ul"").find(""li"").each( function(){\n\n        var text = $(this).text();\n        // Do something with content\n\n    } )\n\n} );\n\n', '\n$.get(""/path/to/other/page"",function(data){\n  $(\'#data\').append($(\'li\',data));\n}\n\n', ""\nIf this is for the same domain then no problem - the jQuery solution is good.\nBut otherwise you can't access content from an arbitrary website because this is considered a security risk. See same origin policy. \nThere are of course server side workarounds such as a web proxy or CORS headers.\nOf if you're lucky they will support jsonp.\nBut if you want a client side solution to work with an arbitrary website and web browser then you are out of luck. There is a proposal to relax this policy, but this won't effect current web browsers.\n"", ""\nYou may want to consider pjscrape:\nhttp://nrabinowitz.github.io/pjscrape/\nIt allows you to do this from the command-line, using javascript and jQuery. It does this by using PhantomJS, which is a headless webkit browser (it has no window, and it exists only for your script's usage, so you can load complex websites that use AJAX and it will work just as if it were a real browser).\nThe examples are self-explanatory and I believe this works on all platforms (including Windows).\n"", '\nUse YQL or Yahoo pipes to make the cross domain request for the raw page html content. The yahoo pipe or YQL query will spit this back as a JSON that can be processed by jquery to extract and display the required data.\nOn the downside: YQL and Yahoo pipes OBEY the robots.txt file for the target domain\nand if the page is to long the Yahoo Pipes regex commands will not run.\n', '\nI am sure you will hit the CORS issue with requests in many cases.\nFrom here try to resolve CORS issue.\nvar name = ""kk"";\nvar url = ""http://anyorigin.com/go?url="" + encodeURIComponent(""https://www.yoursite.xyz/"") + name + ""&callback=?"";\n$.get(url, function(response) {\n  console.log(response);\n});\n\n']",https://stackoverflow.com/questions/5667880/simple-screen-scraping-using-jquery,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
web scraping dynamic content with python,"
I'd like to use Python to scrape the contents of the ""Were you looking for these authors:"" box on web pages like this one: http://academic.research.microsoft.com/Search?query=lander
Unfortunately the contents of the box get loaded dynamically by JavaScript. Usually in this situation I can read the Javascript to figure out what's going on, or I can use an browser extension like Firebug to figure out where the dynamic content is coming from. No such luck this time...the Javascript is pretty convoluted and Firebug doesn't give many clues about how to get at the content.
Are there any tricks that will make this task easy? 
",21k,"
            6
        ","['\nInstead of trying to reverse engineer it, you can use ghost.py to directly interact with JavaScript on the page.\nIf you run the following query in a chrome console, you\'ll see it returns everything you want.\ndocument.getElementsByClassName(\'inline-text-org\');\n\nReturns\n[<div class=\u200b""inline-text-org"" title=\u200b""University of Manchester"">\u200bUniversity of Manchester\u200b</div>, \n <div class=\u200b""inline-text-org"" title=\u200b""University of California Irvine"">\u200bUniversity of California ...\u200b</div>\u200b\n  etc...\n\nYou can run JavaScript through python in a real life DOM using ghost.py.\nThis is really cool:\nfrom ghost import Ghost\nghost = Ghost()\npage, resources = ghost.open(\'http://academic.research.microsoft.com/Search?query=lander\')\nresult, resources = ghost.evaluate(\n    ""document.getElementsByClassName(\'inline-text-org\');"")\n\n', ""\nA very similar question was asked earlier here.\nQuoted is selenium, originally a testing environment for web-apps.\nI usually use Chrome's Developer Mode, which IMHO already gives even more details than Firefox.\n"", ""\nFor scraping dynamic content, you need not a simple scraper but a full-fledged headless browser.\ndhamaniasad/HeadlessBrowsers: A list of (almost) all headless web browsers in existence is the fullest list of these that I've seen; it lists which languages each has bindings for.\n(Note that more than a few of the listed projects are abandoned!)\n""]",https://stackoverflow.com/questions/17608572/web-scraping-dynamic-content-with-python,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How I can get web page's content and save it into the string variable,"
How I can get the content of the web page using ASP.NET?  I need to write a program to get the HTML of a webpage and store it into a string variable.
",182k,"
            78
        ","['\nYou can use the WebClient\nUsing System.Net;\n\nusing(WebClient client = new WebClient()) {\n    string downloadString = client.DownloadString(""http://www.gooogle.com"");\n}\n\n', '\nI\'ve run into issues with Webclient.Downloadstring before. If you do, you can try this:\nWebRequest request = WebRequest.Create(""http://www.google.com"");\nWebResponse response = request.GetResponse();\nStream data = response.GetResponseStream();\nstring html = String.Empty;\nusing (StreamReader sr = new StreamReader(data))\n{\n    html = sr.ReadToEnd();\n}\n\n', '\nI recommend not using WebClient.DownloadString. This is because (at least in .NET 3.5) DownloadString is not smart enough to use/remove the BOM, should it be present. This can result in the BOM (脙炉脗禄脗驴) incorrectly appearing as part of the string when UTF-8 data is returned (at least without a charset) - ick!\nInstead, this slight variation will work correctly with BOMs:\nstring ReadTextFromUrl(string url) {\n    // WebClient is still convenient\n    // Assume UTF8, but detect BOM - could also honor response charset I suppose\n    using (var client = new WebClient())\n    using (var stream = client.OpenRead(url))\n    using (var textReader = new StreamReader(stream, Encoding.UTF8, true)) {\n        return textReader.ReadToEnd();\n    }\n}\n\n', '\nWebclient client = new Webclient();\nstring content = client.DownloadString(url);\n\nPass the URL of page who you want to get. You can parse the result using htmlagilitypack.\n']",https://stackoverflow.com/questions/4510212/how-i-can-get-web-pages-content-and-save-it-into-the-string-variable,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Beautiful Soup cannot find a CSS class if the object has other classes, too","
if a page has <div class=""class1""> and <p class=""class1"">, then soup.findAll(True, 'class1') will find them both.
If it has <p class=""class1 class2"">, though, it will not be found.  How do I find all objects with a certain class, regardless of whether they have other classes, too?
",16k,"
            40
        ","[""\nUnfortunately, BeautifulSoup treats this as a class with a space in it 'class1 class2' rather than two classes ['class1','class2'].  A workaround is to use a regular expression to search for the class instead of a string.\nThis works: \nsoup.findAll(True, {'class': re.compile(r'\\bclass1\\b')})\n\n"", '\nJust in case anybody comes across this question. BeautifulSoup now supports this:\nPython 2.7.5 (default, May 15 2013, 22:43:36) [MSC v.1500 32 bit (Intel)]\nType ""copyright"", ""credits"" or ""license"" for more information.\n\nIn [1]: import bs4\n\nIn [2]: soup = bs4.BeautifulSoup(\'<div class=""foo bar""></div>\')\n\nIn [3]: soup(attrs={\'class\': \'bar\'})\nOut[3]: [<div class=""foo bar""></div>]\n\nAlso, you don\'t have to type findAll anymore.\n', '\nYou should use lxml. It works with multiple class values separated by spaces (\'class1 class2\').\nDespite its name, lxml is also for parsing and scraping HTML. It\'s much, much faster than BeautifulSoup, and it even handles ""broken"" HTML better than BeautifulSoup (their claim to fame). It has a compatibility API for BeautifulSoup too if you don\'t want to learn the lxml API.\nIan Bicking agrees and prefers lxml over BeautifulSoup.\nThere\'s no reason to use BeautifulSoup anymore, unless you\'re on Google App Engine or something where anything not purely Python isn\'t allowed.\nYou can even use CSS selectors with lxml, so it\'s far easier to use than BeautifulSoup. Try playing around with it in an interactive Python console.\n', '\nIt鈥檚 very useful to search for a tag that has a certain CSS class, but the name of the CSS attribute, 鈥渃lass鈥? is a reserved word in Python. Using class as a keyword argument will give you a syntax error. As of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument class_:\nLike:\nsoup.find_all(""a"", class_=""class1"")\n\n']",https://stackoverflow.com/questions/1242755/beautiful-soup-cannot-find-a-css-class-if-the-object-has-other-classes-too,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
file_get_contents() give me 403 Forbidden,"
I have a partner that has created some content for me to scrape.
I can access the page with my browser, but when trying to user file_get_contents, I get a 403 forbidden.
I've tried using stream_context_create, but that's not helping - it might be because I don't know what should go in there.
1) Is there any way for me to scrape the data?
2) If no, and if partner is not allowed to configure server to allow me access, what can I do then?
The code I've tried using:
$opts = array(
  'http'=>array(
    'user_agent' => 'My company name',
    'method'=>""GET"",
    'header'=> implode(""\r\n"", array(
      'Content-type: text/plain;'
    ))
  )
);

$context = stream_context_create($opts);

//Get header content
$_header = file_get_contents($partner_url,false, $context);

",35k,"
            26
        ","['\nThis is not a problem in your script, its a feature in you partners web server security.\nIt\'s hard to say exactly whats blocking you, most likely its some sort of block against scraping. If your partner has access to his web servers setup it might help pinpoint.\nWhat you could do is to ""fake a web browser"" by setting the user-agent headers so that it imitates a standard web browser.\nI would recommend cURL to do this, and it will be easy to find good documentation for doing this.\n    // create curl resource\n    $ch = curl_init();\n\n    // set url\n    curl_setopt($ch, CURLOPT_URL, ""example.com"");\n\n    //return the transfer as a string\n    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\n    curl_setopt($ch,CURLOPT_USERAGENT,\'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.13) Gecko/20080311 Firefox/2.0.0.13\');\n\n    // $output contains the output string\n    $output = curl_exec($ch);\n\n    // close curl resource to free up system resources\n    curl_close($ch); \n\n', ""\n//set User Agent first\nini_set('user_agent','Mozilla/4.0 (compatible; MSIE 6.0)'); \n\n"", ""\nAlso if for some reason you're requesting a http resource but that resource lives on your server you can save yourself some trouble if you just include the file as an absolute path.\nLike: /home/sally/statusReport/myhtmlfile.html \ninstead of \nhttps://example.org/myhtmlfile.html\n"", ""\nI have two things in my mind, If you're opening a URI with special characters, such as spaces, you need to encode the URI with urlencode() and A URL can be used as a filename with this function if the fopen wrappers have been enabled.\n""]",https://stackoverflow.com/questions/11680709/file-get-contents-give-me-403-forbidden,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrape a dynamic website,"
What is the best method to scrape a dynamic website where most of the content is generated by what appears to be ajax requests?  I have previous experience with a Mechanize, BeautifulSoup, and python combo, but I am up for something new.
--Edit--
For more detail: I'm trying to scrape the CNN primary database.  There is a wealth of information there, but there doesn't appear to be an api.
",8k,"
            12
        ","['\nThe best solution that I found was to use Firebug to monitor XmlHttpRequests, and then to use a script to resend them.\n', ""\nThis is a difficult problem because you either have to reverse engineer the JavaScript on a per-site basis, or implement a JavaScript engine and run the scripts (which has its own difficulties and pitfalls).\nIt's a heavy weight solution, but I've seen people doing this with GreaseMonkey scripts - allow Firefox to render everything and run the JavaScript, and then scrape the elements. You can even initiate user actions on the page if needed.\n"", '\nSelenium IDE, a tool for testing, is something I\'ve used for a lot of screen-scraping. There are a few things it doesn\'t handle well (Javascript window.alert() and popup windows in general), but it does its work on a page by actually triggering the click events and typing into the text boxes. Because the IDE portion runs in Firefox, you don\'t have to do all of the management of sessions, etc. as Firefox takes care of it. The IDE records and plays tests back.\nIt also exports C#, PHP, Java, etc. code to build compiled tests/scrapers that are executed on the Selenium server. I\'ve done that for more than a few of my Selenium scripts, which makes things like storing the scraped data in a database much easier.\nScripts are fairly simple to write and alter, being made up of things like (""clickAndWait"",""submitButton""). Worth a look given what you\'re describing.\n', '\nAdam Davis\'s advice is solid.\nI would additionally suggest that you try to ""reverse-engineer"" what the JavaScript is doing, and instead of trying to scrape the page, you issue the HTTP requests that the JavaScript is issuing and interpret the results yourself (most likely in JSON format, nice and easy to parse).  This strategy could be anything from trivial to a total nightmare, depending on the complexity of the JavaScript.\nThe best possibility, of course, would be to convince the website\'s maintainers to implement a developer-friendly API.  All the cool kids are doing it these days 8-)  Of course, they might not  want their data scraped in an automated fashion... in which case you can expect a cat-and-mouse game of making their page increasingly difficult to scrape :-(\n', ""\nThere is a bit of a learning curve, but tools like Pamie (Python) or Watir (Ruby) will let you latch into the IE web browser and get at the elements. This turns out to be easier than Mechanize and other HTTP level tools since you don't have to emulate the browser, you just ask the browser for the html elements. And it's going to be way easier than reverse engineering the Javascript/Ajax calls. If needed you can also use tools like beatiful soup in conjunction with Pamie.\n"", '\nProbably the easiest way is to use IE webbrowser control in C# (or any other language). You have access to all the stuff inside browser out of the box + you dont need to care about cookies, SSL and so on.\n', '\ni found the IE Webbrowser control have all kinds of quirks and workarounds that would justify some high quality software to take care of all those inconsistencies, layered around the shvwdoc.dll api and mshtml and provide a framework. \n', ""\nThis seems like it's a pretty common problem.  I wonder why someone hasn't anyone developed a programmatic browser?  I'm envisioning a Firefox you can call from the command line with a URL as an argument and it will load the page, run all of the initial page load JS events and save the resulting file.\nI mean Firefox, and other browsers already do this, why can't we simply strip off the UI stuff?  \n""]",https://stackoverflow.com/questions/206855/scrape-a-dynamic-website,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Protection from screen scraping [closed],"






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.


Closed 10 years ago.







                        Improve this question
                    



Following on from my question on the Legalities of screen scraping, even if it's illegal people will still try, so:
What technical mechanisms can be employed to prevent or at least disincentivise screen scraping?
Oh and just for grins and to make life difficult, it may well be nice to retain access for search engines. I may well be playing devil's advocate here but there is a serious underlying point.
",24k,"
            31
        ","['\nYou can鈥檛 prevent it.\n', ""\nI've written a blog post about this here:  http://blog.screen-scraper.com/2009/08/17/further-thoughts-on-hindering-screen-scraping/\nTo paraphrase:\nIf you post information on the internet someone can get it, it's just a matter of how many resources they want to invest.  Some means to make the required resources higher are:\nTuring tests\nThe most common implementation of the Turning Test is the old CAPTCHA that tries to ensure a human reads the text in an image, and feeds it into a form.\nWe have found a large number of sites that implement a very weak CAPTCHA that takes only a few minutes to get around. On the other hand, there are some very good implementations of Turing Tests that we would opt not to deal with given the choice, but a sophisticated OCR can sometimes overcome those, or many bulletin board spammers have some clever tricks to get past these.\nData as images\nSometimes you know which parts of your data are valuable. In that case it becomes reasonable to replace such text with an image. As with the Turing Test, there is OCR software that can read it, and there鈥檚 no reason we can鈥檛 save the image and have someone read it later.\nOften times, however, listing data as an image without a text alternate is in violation of the Americans with Disabilities Act (ADA), and can be overcome with a couple of phone calls to a company鈥檚 legal department.\nCode obfuscation\nUsing something like a JavaScript function to show data on the page though it鈥檚 not anywhere in the HTML source is a good trick. Other examples include putting prolific, extraneous comments through the page or having an interactive page that orders things in an unpredictable way (and the example I think of used CSS to make the display the same no matter the arrangement of the code.)\nCSS Sprites\nRecently we鈥檝e encountered some instances where a page has one images containing numbers and letters, and used CSS to display only the characters they desired.  This is in effect a combination of the previous 2 methods.  First we have to get that master-image and read what characters are there, then we鈥檇 need to read the CSS in the site and determine to what character each tag was pointing.\nWhile this is very clever, I suspect this too would run afoul the ADA, though I鈥檝e not tested that yet.\nLimit search results\nMost of the data we want to get at is behind some sort of form. Some are easy, and submitting a blank form will yield all of the results. Some need an asterisk or percent put in the form. The hardest ones are those that will give you only so many results per query. Sometimes we just make a loop that will submit the letters of the alphabet to the form, but if that鈥檚 too general, we must make a loop to submit all combination of 2 or 3 letters鈥搕hat鈥檚 17,576 page requests.\nIP Filtering\nOn occasion, a diligent webmaster will notice a large number of page requests coming from a particular IP address, and block requests from that domain.  There are a number of methods to pass requests through alternate domains, however, so this method isn鈥檛 generally very effective.\nSite Tinkering\nScraping always keys off of certain things in the HTML.  Some sites have the resources to constantly tweak their HTML so that any scrapes are constantly out of date.  Therefore it becomes cost ineffective to continually update the scrape for the constantly changing conditions.\n"", '\nSo, one approach would be to obfuscate the code (rot13, or something), and then have some javascript in the page that do something like document.write(unobfuscate(obfuscated_page)). But this totally blows away search engines (probably!).\nOf course this doesn鈥檛 actually stop someone who wants to steal your data either, but it does make it harder.\nOnce the client has the data it is pretty much game over, so you need to look at something on the server side.\nGiven that search engines are basically screen scrapers things are difficult. You need to look at what the difference between the good screen scrapers and the bad screen scrapers are. And of course, you have just the normal human users as well. So this comes down to a problem of how can you on the server effectively classify as request as coming from a human, a good screen scraper, or a bad screen scraper.\nSo, the place to start would be looking at your log-files and seeing if there is some pattern that allows you to effectively classify requests, and then on determining the pattern see if there is some way that a bad screen scraper, upon knowing this classification, could cloak itself to appear like a human or good screen scraper.\nSome ideas:\n\nYou may be able to determine the good screen scrapers by IP address(es)..\nYou could potentially determine scraper vs. human by number of concurrent connections, total number of connections per time-period, access pattern, etc.\n\nObviously these aren鈥檛 ideal or fool-proof. Another tactic is to determine what measures can you take that are unobtrusive to humans, but (may be) annoying for scrapers. An example might be slowing down the number of requests. (Depends on the time criticality of the request. If they are scraping in real-time, this would effect their end users).\nThe other aspect is to look at serving these users better. Clearly they are scraping because they want the data. If you provide them an easy way in which to directly obtain the data in a useful format then that will be easier for them to do instead of screen scraping. If there is an easy way then access to the data can be regulated. E.g: give requesters a unique key, and then limit the number of requests per key to avoid overload on the server, or charge per 1000 requests, etc.\nOf course there are still people who will want to rip you off, and then there are probably other ways to disincentivise, bu they probably start being non-technical, and require legal avenues to be persued.\n', ""\nIt's pretty hard to prevent screen scraping but if you really, really wanted to you could\nchange your HTML frequently or change the HTML tag names frequently. Most screen scrapers work by using string comparisons with tag names, or regular expressions searching for particular strings etc. If you are changing the underlying HTML it will make them need to change their software.\n"", '\nIt would be very difficult to prevent.  The problem is that Web pages are meant to be parsed by a program (your browser), so they are exceptionally easy to scrape.  The best you can do is be vigilant, and if you find that your site is being scraped, block the IP of the offending program.\n', ""\nDon't prevent it, detect it and retaliate those who try.\nFor example, leave your site open to download but disseminate some links that no sane user would follow. If someone follows that link, is clicking too fast for a human or other suspicious behaviour, react promptly to stop the user from trying. If there is a login system, block the user and contact him regarding unacceptable behaviour. That should make sure they don't try again. If there is no login system, instead of actual pages, return a big warning with fake links to the same warning.\nThis really applies for things like Safari Bookshelf where a user copy-pasting a piece of code or a chapter to mail a colleague is fine while a full download of book is not acceptable. I'm quite sure that they detect when some tries to download their books, block the account and show the culprit that he might get in REAL trouble should he try that again.\nTo make a non-IT analogy, if airport security only made it hard to bring weapons on board of planes, terrorists would try many ways to sneak one past security. But the fact that just trying will get you in deep trouble make it so that nobody is going to try and find the ways to sneak one. The risk of getting caught and punished is too high. Just do the same. If possible.\n"", ""\nSearch engines ARE screen scrapers by definition.  So most things you do to make it harder to screen scrape will also make it harder to index your content.\nWell behaved robots will honour your robots.txt file.\nYou could also block the IP of known offenders or add obfuscating HTML tags into your content when it's not sent to a known good robot.  It's a losing battle though.  I recommend the litigation route for known offenders.\nYou could also hide identifying data in the content to make it easier to track down offenders. Encyclopaedias have been known to to add Fictitious entries to help detect and prosecute copyright infringers.\n"", ""\nPrevent? -- impossible, but you can make it harder.\nDisincentivise? -- possible, but you won't like the answer: provide bulk data exports for interested parties. \nOn the long run, all your competitors will have the same data if you publish it, so you need other means of diversifying your website (e.g. update it more frequently, make it faster or easier to use). Nowdays even Google is using scraped information like user reviews, what do you think you can do about it? Sue them and get booted from their index?\n"", ""\nThe best return on investment is probably to add random newlines and multiple spaces, since most screen scrapers work from the HTML as text rather than as a XML (since most pages don't parse as valid XML).\nThe browser ignores whitespace, so your user's don't notice that \n  Price : 1\n  Price :    2\n  Price\\n:\\n3\n\nare different.  (this comes from my experience scraping government sites with AWK).\nNext step is adding  tags around random elements to mess up the DOM.\n"", '\nOne way is to create an function that takes text and position and then Serverside generate x, y pos for every character in the text, generate divs in random order containing the characters. Generate a javascript that then posision every div on right place on screen. Looks good on screen but in code behind there is no real order to fetch the text if you dont go throuh the trouble to scrape via your javascript (that can be changed dynamically every request)\nToo much work and have possibly many quirks, it depends on how much text and how complicate UI you have on the site and other things.\n', '\nVery few I think given the intention of any site is to publish (i.e. to make public) information. \n\nYou can hide your data behind logins of course, but that\'s a very situational solution. \nI\'ve seen apps which would only serve up content where the request headers indicated a web browser (rather than say anonymous or ""jakarta"") but that\'s easy to spoof and you\'ll lose some genuine humans.\nThen there\'s the possibility that you accept some scrapage but make life insurmountably hard for them by not serving content if requests are coming from the same IP at too high a rate. This suffers from not being full coverage but more importantly there is the ""AOL problem"" that an IP can cover many many unique human users.\n\nBoth of the last two techniques also depend heavily on having traffic intercepting technology which is an inevitable performance and/or financial outlay.\n', ""\nGiven that most sites want a good search engine ranking, and search engines are scraper bots, there's not much you can do that won't harm your SEO. \nYou could make an entirely ajax loaded site or flash based site, which would make it harder for bots, or hide everything behind a login, which would make it harder still, but either of these approaches is going to hurt your search rankings and possibly annoy your users, and if someone really wants it, they'll find a way.  \nThe only guaranteed way of having content that can't be scraped is to not publish it on the web. The nature of the web is such that when you put it out there, it's out there.\n"", '\nIf its not much information you want to protect you can convert it to a picture on the fly. Then they must use OCR wich makes it easier to scrape another site instead of yours..\n', ""\nYou could check the user agent of clients coming to your site. Some third party screen scraping programs have their own user agent so you could block that. Good screen scrapers however spoof their user agent so you won't be able to detect it. Be careful if you do try to block anyone because you don't want to block a legitimate user :)\nThe best you can hope for is to block people using screen scrapers that aren't smart enough to change their user agent.\n"", '\nI tried to ""screen scrape"" some PDF files once, only to find that they\'d actually put the characters in the PDF in semi-random order.  I guess the PDF format allows you to specify a location for each block of text, and they\'d used very small blocks (smaller than a word).  I suspect that the PDFs in question weren\'t trying to prevent screen scraping so much as they were doing something weird with their render engine.\nI wonder if you could do something like that.\n', '\nYou could put everything in flash, but in most cases that would annoy many legitimate users, myself included. It can work for some information such as stock prices or graphs.\n', '\nI suspect there is no good way to do this.\nI suppose you could run all your content through a mechanism to convert text to images rendered using a CAPTCHA-style font and layout, but that would break SEO and annoy your users.\n', '\nWell, before you push the content from the server to the client, remove all the \\r\\n, \\n, \\t and replace everything with nothing but a single space. Now you have 1 long line in your html page. Google does this. This will make it hard for others to read your html or JavaScript.\nThen you can create empty tags and randomly insert them here and there. The will have no effect.\nThen you can log all the IPs and how often they hit your site. If you see one that comes in on time everytime, you mark it as robot and block it.\nMake sure you leave the search engines alone if you want them to come in.\n\nHope this helps\n', ""\nWhat about using the iText library to create PDFs out of your database information? As with Flash, it won't make scraping impossible, but might make it a little more difficult.\nNels\n"", ""\nOld question, but- adding interactivity makes screen scraping much more difficult. If the data isn't in the original response- say, you made an AJAX request to populate a div after page load- most scrapers won't see it.\nFor example- I use the mechanize library to do my scraping. Mechanize doesn't execute Javascript- it isn't a modern browser- it just parses HTML, let's me follow links and extract text, etc. Whenever I run into a page that makes heavy use of Javascript, I choke- without a fully scripted browser (that supports the full gamut of Javascript) I'm stuck.\nThis is the same issue that makes automated testing of highly interactive web applications so difficult.\n"", '\nI never thought that preventing print screen would be possible... well what do you know, checkout the new tech - sivizion.com. With their video buffer technology there is no way to do a print screen, cool, really cool, though hard to use ... I think they license the tech also, check it out. (If I am wrong please post here how it can be hacked.)\nFound it here: How do I prevent print screen\n']",https://stackoverflow.com/questions/396817/protection-from-screen-scraping,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CasperJS passing data back to PHP,"
CasperJS is being called by PHP using an exec() command. After CasperJS does its work such as retrieving parts of a webpage, how can the retrieved data be returned back to PHP?
",11k,"
            9
        ","['\nI think the best way to transfer data from CasperJS to another language such as PHP is running CasperJS script as a service. Because CasperJS has been written over PhantomJS, CasperJS can use an embedded web server module of PhantomJS called Mongoose.\nFor information about how works the embedded web server see here\nHere an example about how a CasperJS script can start a web server.\n//define ip and port to web service\nvar ip_server = \'127.0.0.1:8585\';\n\n//includes web server modules\nvar server = require(\'webserver\').create();\n\n//start web server\nvar service = server.listen(ip_server, function(request, response) {\n\n    var links = [];\n    var casper = require(\'casper\').create();\n\n    function getLinks() {\n        var links = document.querySelectorAll(\'h3.r a\');\n        return Array.prototype.map.call(links, function(e) {\n            return e.getAttribute(\'href\')\n        });\n    }\n\n    casper.start(\'http://google.fr/\', function() {\n        // search for \'casperjs\' from google form\n        this.fill(\'form[action=""/search""]\', { q: \'casperjs\' }, true);\n    });\n\n    casper.then(function() {\n        // aggregate results for the \'casperjs\' search\n        links = this.evaluate(getLinks);\n        // now search for \'phantomjs\' by filling the form again\n        this.fill(\'form[action=""/search""]\', { q: \'phantomjs\' }, true);\n    });\n\n    casper.then(function() {\n        // aggregate results for the \'phantomjs\' search\n        links = links.concat(this.evaluate(getLinks));\n    });\n\n    //\n    casper.run(function() {\n            response.statusCode = 200;\n            //sends results as JSON object\n            response.write(JSON.stringify(links, null, null));\n            response.close();              \n    });\n\n});\nconsole.log(\'Server running at http://\' + ip_server+\'/\');\n\n', ""\nYou can redirect output from stdout to an array.\nOn this page it says you can do:  \nstring exec ( string $command [, array &$output [, int &$return_var ]] )\n\nIt goes on to say: \n\nIf the output argument is present, then the specified array will be filled with every line of output from the command. \n\nSo basically you can do exec('casperjs command here, $array_here);\n""]",https://stackoverflow.com/questions/15852987/casperjs-passing-data-back-to-php,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unable to call firefox from selenium in python on AWS machine,"
I am trying to use selenium from python to scrape some dynamics pages with javascript. However, I cannot call firefox after I followed the instruction of selenium on the pypi page(http://pypi.python.org/pypi/selenium). I installed firefox on AWS ubuntu 12.04. The error message I got is:
In [1]: from selenium import webdriver

In [2]: br = webdriver.Firefox()
---------------------------------------------------------------------------
WebDriverException                        Traceback (most recent call last)
/home/ubuntu/<ipython-input-2-d6a5d754ea44> in <module>()
----> 1 br = webdriver.Firefox()

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/webdriver.pyc in __init__(self, firefox_profile, firefox_binary, timeout)
     49         RemoteWebDriver.__init__(self,
     50             command_executor=ExtensionConnection(""127.0.0.1"", self.profile,
---> 51             self.binary, timeout),
     52             desired_capabilities=DesiredCapabilities.FIREFOX)
     53

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/extension_connection.pyc in __init__(self, host, firefox_profile, firefox_binary, timeout)
     45         self.profile.add_extension()
     46
---> 47         self.binary.launch_browser(self.profile)
     48         _URL = ""http://%s:%d/hub"" % (HOST, PORT)
     49         RemoteConnection.__init__(

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/firefox_binary.pyc in launch_browser(self, profile)
     42
     43         self._start_from_profile_path(self.profile.path)
---> 44         self._wait_until_connectable()
     45
     46     def kill(self):

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/firefox_binary.pyc in _wait_until_connectable(self)
     79                 raise WebDriverException(""The browser appears to have exited ""
     80                       ""before we could connect. The output was: %s"" %
---> 81                       self._get_firefox_output())
     82             if count == 30:
     83                 self.kill()

WebDriverException: Message: 'The browser appears to have exited before we could connect. The output was: Error: no display specified\n'

I did search on the web and found that this problem happened with other people (https://groups.google.com/forum/?fromgroups=#!topic/selenium-users/21sJrOJULZY). But I don't understand the solution, if it is. 
Can anyone help me please? Thanks!
",24k,"
            33
        ","['\n', '\n', '\n', '\n']",https://stackoverflow.com/questions/13039530/unable-to-call-firefox-from-selenium-in-python-on-aws-machine,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scrape websites with infinite scrolling,"
I have written many scrapers but I am not really sure how to handle infinite scrollers. These days most website etc, Facebook, Pinterest has infinite scrollers.
",31k,"
            31
        ","['\nYou can use selenium to scrap the infinite scrolling website like twitter or facebook. \nStep 1 : Install Selenium using pip \npip install selenium \n\nStep 2 : use the code below to automate infinite scroll and extract the source code\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import NoAlertPresentException\nimport sys\n\nimport unittest, time, re\n\nclass Sel(unittest.TestCase):\n    def setUp(self):\n        self.driver = webdriver.Firefox()\n        self.driver.implicitly_wait(30)\n        self.base_url = ""https://twitter.com""\n        self.verificationErrors = []\n        self.accept_next_alert = True\n    def test_sel(self):\n        driver = self.driver\n        delay = 3\n        driver.get(self.base_url + ""/search?q=stckoverflow&src=typd"")\n        driver.find_element_by_link_text(""All"").click()\n        for i in range(1,100):\n            self.driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")\n            time.sleep(4)\n        html_source = driver.page_source\n        data = html_source.encode(\'utf-8\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n\nStep 3 : Print the data if required.\n', ""\nMost sites that have infinite scrolling do (as Lattyware notes) have a proper API as well, and you will likely be better served by using this rather than scraping.\nBut if you must scrape...\nSuch sites are using JavaScript to request additional content from the site when you reach the bottom of the page. All you need to do is figure out the URL of that additional content and you can retrieve it. Figuring out the required URL can be done by inspecting the script, by using the Firefox Web console, or by using a debug proxy.\nFor example, open the Firefox Web Console, turn off all the filter buttons except Net, and load the site you wish to scrape. You'll see all the files as they are loaded. Scroll the page while watching the Web Console and you'll see the URLs being used for the additional requests. Then you can request that URL yourself and see what format the data is in (probably JSON) and get it into your Python script.\n"", '\nFinding the url of the ajax source will be the best option but it can be cumbersome for certain sites. Alternatively you could use a headless browser like QWebKit from PyQt and send keyboard events while reading the data from the DOM tree. QWebKit has a nice and simple api.\n']",https://stackoverflow.com/questions/12519074/scrape-websites-with-infinite-scrolling,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Screen Scraping from a web page with a lot of Javascript [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



I have been asked to write an app which screen scrapes info from an intranet web page and presents the certain info from it in a nice easy to view format. The web page is a real mess and requires the user to click on half a dozen icons to discover if an ordered item has arrived or has been receipted. As you can imagine users find this irritating to say the least and it would be nice to have an app anyone can use that lists the state of their orders in a single screen.
Yes I know a better solution would be to re-write the web app but that would involve calling in the vendor and would cost us as small fortune.
Anyway while looking into this I discovered the web page I want to scrape is mostly Javascript (although it doesn't use any AJAX techniques). Does anyone know if a library or program exists which I could feed with the Javascript and which would then spit out the DOM for my app to parse ? 
I can pretty much write the app in any language but my preference would be JavaFX just so I could have a play with it.
Thanks for your time.
Ian
",18k,"
            17
        ","[""\nYou may consider using HTMLunit\nIt's a java class library made to automate browsing without having to control a browser, and it integrates the Mozilla Rhino Javascript engine to process javascript on the pages it loads. There's also a JRuby wrapper for that, named Celerity. Its javascript support is not really perfect right now, but if your pages don't use many hacks things should work fine the performance should be way better than controlling a browser. Furthermore, you don't have to worry about cookies being persisted after your scraping is over and all the other nasty things connected to controlling a browser (history, autocomplete, temp files etc).\n"", ""\nSince you say that no AJAX is used, then all the info is present at the HTML source. The javascript just renders it based on user clicks. So you need to reverse engineer the way the application works, parse the html and the javascript code and extract the useful information. It is strictly business of text parsing - you shouldn't deal with running javascript and producing a new DOM. This would be much more difficult to do.\nIf AJAX was used, your job would be easier. You could easily find out how the AJAX services work (probably by receiving JSON and XML) and extract the information.\n"", '\nYou could consider using a greasemonkey JS. greasemonkey is a very powerful Firefox add on that allows you to run your own script alongside that of specific web sites. This allows you to modify how the web site is displayed, add or remove content. You can even use it to  do AJAX style lookups and add dynamic content. \nIf your tool is for in house use, and users are all happy to use Firefox then this could be a winner.\nRegards\n', '\nI suggest IRobotSoft web scraper.  It is a dedicated free software for screen scraping with the best javascript support.  You can create and test a robot with its visual interface.  You can also embed it into your own application using its ActiveX control and hide the browser window. \n', ""\nI'd go with Perl's Win32::IE::Mechanize which lets you automate Internet Explorer. You should be able to click on icons and extract text while letting MSIE do the annoying tasks of processing all the JS.\n"", ""\nI agree with kgiannakakis' answer. I'd be suprised if you couldn't reverse engineer the javascript to identify where the information comes from and then write some simple Python scripts using Urllib2 and the Beautiful Soup library to scrape the same information.\nIf Python and scraping are a new idea, there's some excellent tutorials available on how to get going.\n[Edit] Looks like there's a Python version of mechanize too. Time to re-write some scrapers I developed a while back! :-)\n"", '\nI created a project site2archive that uses phantomJs to render including JS stuff and wget to scrape. phantomJs is based on Webkit, that delivers a similar browsing environment as Safari and Google Chrome.\n']",https://stackoverflow.com/questions/857515/screen-scraping-from-a-web-page-with-a-lot-of-javascript,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping javascript website in R,"
I want to scrape the match time and date from this url:
http://www.scoreboard.com/game/rosol-l-goffin-d-2014/8drhX07d/#game-summary
By using the chrome dev tools, I can see this appears to be generated using the following code:
<td colspan=""3"" id=""utime"" class=""mstat-date"">01:20 AM, October 29, 2014</td>

But this is not in the source html.
I think this is because its java (correct me if Im wrong). How can I scrape this information using R?
",8k,"
            9
        ","['\nSo, RSelenium is not the only answer (anymore). If you can install the PhantomJS binary (grab phantomjs binaries from here: http://phantomjs.org/) then you can use it to render the HTML and scrape it with rvest (similar to the RSelenium approach but doesn\'t require java):\nlibrary(rvest)\n\n# render HTML from the site with phantomjs\n\nurl <- ""http://www.scoreboard.com/game/rosol-l-goffin-d-2014/8drhX07d/#game-summary""\n\nwriteLines(sprintf(""var page = require(\'webpage\').create();\npage.open(\'%s\', function () {\n    console.log(page.content); //page source\n    phantom.exit();\n});"", url), con=""scrape.js"")\n\nsystem(""phantomjs scrape.js > scrape.html"", intern = T)\n\n# extract the content you need\npg <- html(""scrape.html"")\npg %>% html_nodes(""#utime"") %>% html_text()\n\n## [1] ""10:20 AM, October 28, 2014""\n\n', '\nYou could also use docker as the web driver (in place of selenium)\nYou will still need to install phantomjs, and docker too. Then run:\nlibrary(RSelenium)\n\nurl <- ""http://www.scoreboard.com/game/rosol-l-goffin-d-2014/8drhX07d/#game-summary""\n\nsystem(\'docker run -d -p 4445:4444 selenium/standalone-chrome\') \nremDr <- remoteDriver(remoteServerAddr = ""localhost"", port = 4445L, browserName = ""chrome"")\nremDr$open()\nremDr$navigate(url)\n\nwriteLines(sprintf(""var page = require(\'webpage\').create();\npage.open(\'%s\', function () {\n    console.log(page.content); //page source\n    phantom.exit();\n});"", url), con=""scrape.js"")\n\nsystem(""phantomjs scrape.js > scrape.html"", intern = T)\n\n# extract the content you need\npg <- read_html(""scrape.html"")\npg %>% html_nodes(""#utime"") %>% html_text()\n\n# [1] ""10:20 AM, October 28, 2014""\n\n']",https://stackoverflow.com/questions/26631511/scraping-javascript-website-in-r,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Perform screen-scape of Webbrowser control in thread,"
I am using the technique shown in 

WebBrowser Control in a new thread

Trying to get a screen-scrape of a webpage I have been able to get the following code to successfully work when the WebBrowser control is placed on a WinForm. However it fails by providing an arbitrary image of the desktop when run inside a thread.
Thread browserThread = new Thread(() =>
{
    WebBrowser br = new WebBrowser();
    br.DocumentCompleted += webBrowser1_DocumentCompleted;
    br.ProgressChanged += webBrowser1_ProgressChanged;
    br.ScriptErrorsSuppressed = true;
    br.Navigate(url);
    Application.Run();
});
browserThread.SetApartmentState(ApartmentState.STA);
browserThread.Start();

private Image TakeSnapShot(WebBrowser browser)
{
    int width;
    int height;

    width = browser.ClientRectangle.Width;
    height = browser.ClientRectangle.Height;

    Bitmap image = new Bitmap(width, height);

    using (Graphics graphics = Graphics.FromImage(image))
    {
        Point p = new Point(0, 0);
        Point upperLeftSource = browser.PointToScreen(p);
        Point upperLeftDestination = new Point(0, 0);

        Size blockRegionSize = browser.ClientRectangle.Size;
        blockRegionSize.Width = blockRegionSize.Width - 15;
        blockRegionSize.Height = blockRegionSize.Height - 15;
        graphics.CopyFromScreen(upperLeftSource, upperLeftDestination, blockRegionSize);
    }

    return image;
}

This obviously happens because of the method Graphics.CopyFromScreen() but I am unaware of any other approach. Is there a way to resolve this issue that anyone could suggest? or is my only option to create a form, add the control, make it visible and then screen-scrape? For obvious reasons I'm hoping to avoid such an approach.
",4k,"
            3
        ","['\nYou can write \nprivate Image TakeSnapShot(WebBrowser browser)\n{\n     browser.Width = browser.Document.Body.ScrollRectangle.Width;\n     browser.Height= browser.Document.Body.ScrollRectangle.Height;\n\n     Bitmap bitmap = new Bitmap(browser.Width - System.Windows.Forms.SystemInformation.VerticalScrollBarWidth, browser.Height);\n\n     browser.DrawToBitmap(bitmap, new Rectangle(0, 0, bitmap.Width, bitmap.Height));\n\n     return bitmap;\n}\n\nA full working code\nvar image = await WebUtils.GetPageAsImageAsync(""http://www.stackoverflow.com"");\nimage.Save(fname , System.Drawing.Imaging.ImageFormat.Bmp);\n\n\npublic class WebUtils\n{\n    public static Task<Image> GetPageAsImageAsync(string url)\n    {\n        var tcs = new TaskCompletionSource<Image>();\n\n        var thread = new Thread(() =>\n        {\n            WebBrowser browser = new WebBrowser();\n            browser.Size = new Size(1280, 768);\n\n            WebBrowserDocumentCompletedEventHandler documentCompleted = null;\n            documentCompleted = async (o, s) =>\n            {\n                browser.DocumentCompleted -= documentCompleted;\n                await Task.Delay(2000); //Run JS a few seconds more\n\n                Bitmap bitmap = TakeSnapshot(browser);\n\n                tcs.TrySetResult(bitmap);\n                browser.Dispose();\n                Application.ExitThread();\n            };\n\n            browser.ScriptErrorsSuppressed = true;\n            browser.DocumentCompleted += documentCompleted;\n            browser.Navigate(url);\n            Application.Run();\n        });\n\n        thread.SetApartmentState(ApartmentState.STA);\n        thread.Start();\n\n        return tcs.Task;\n    }\n\n    private static Bitmap TakeSnapshot(WebBrowser browser)\n    {\n         browser.Width = browser.Document.Body.ScrollRectangle.Width;\n         browser.Height= browser.Document.Body.ScrollRectangle.Height;\n\n         Bitmap bitmap = new Bitmap(browser.Width - System.Windows.Forms.SystemInformation.VerticalScrollBarWidth, browser.Height);\n\n         browser.DrawToBitmap(bitmap, new Rectangle(0, 0, bitmap.Width, bitmap.Height));\n\n         return bitmap;\n    }\n}\n\n', '\nusing (Graphics graphics = Graphics.FromImage(image))\n    {\n        Point p = new Point(0, 0);\n        Point upperLeftSource = browser.PointToScreen(p);\n        Point upperLeftDestination = new Point(0, 0);\n\n        Size blockRegionSize = browser.ClientRectangle.Size;\n        blockRegionSize.Width = blockRegionSize.Width - 15;\n        blockRegionSize.Height = blockRegionSize.Height - 15;\n        graphics.CopyFromScreen(upperLeftSource, upperLeftDestination, blockRegionSize);\n    }\n\nDo you really need using statement ? \nYou also return image but how is copied image assigned to it? \n']",https://stackoverflow.com/questions/18675606/perform-screen-scape-of-webbrowser-control-in-thread,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Looping over urls to do the same thing,"
I am tring to scrape a few sites. Here is my code:
for (var i = 0; i < urls.length; i++) {
    url = urls[i];
    console.log(""Start scraping: "" + url);

    page.open(url, function () {
        waitFor(function() {
            return page.evaluate(function() {
                return document.getElementById(""progressWrapper"").childNodes.length == 1;
            });

        }, function() {
            var price = page.evaluate(function() {
                // do something
                return price;
            });

            console.log(price);
            result = url + "" ; "" + price;
            output = output + ""\r\n"" + result;
        });
    });

}
fs.write('test.txt', output);
phantom.exit();

I want to scrape all sites in the array urls, extract some information and then write this information to a text file.
But there seems to be a problem with the for loop. When scraping only one site without using a loop, all works as I want. But with the loop, first nothing happens, then the line 
console.log(""Start scraping: "" + url);

is shown, but one time too much.
If url = {a,b,c}, then phantomjs does:
Start scraping: a 
Start scraping: b 
Start scraping: c 
Start scraping:

It seems that page.open isn't called at all.
I am newbie to JS so I am sorry for this stupid question.
",4k,"
            1
        ","['\nPhantomJS is asynchronous. By calling page.open() multiple times using a loop, you essentially rush the execution of the callback. You\'re overwriting the current request before it is finished with a new request which is then again overwritten. You need to execute them one after the other, for example like this:\npage.open(url, function () {\n    waitFor(function() {\n       // something\n    }, function() {\n        page.open(url, function () {\n            waitFor(function() {\n               // something\n            }, function() {\n                // and so on\n            });\n        });\n    });\n});\n\nBut this is tedious. There are utilities that can help you with writing nicer code like async.js. You can install it in the directory of the phantomjs script through npm.\nvar async = require(""async""); // install async through npm\nvar tests = urls.map(function(url){\n    return function(callback){\n        page.open(url, function () {\n            waitFor(function() {\n               // something\n            }, function() {\n                callback();\n            });\n        });\n    };\n});\nasync.series(tests, function finish(){\n    fs.write(\'test.txt\', output);\n    phantom.exit();\n});\n\nIf you don\'t want any dependencies, then it is also easy to define your own recursive function (from here):\nvar urls = [/*....*/];\n\nfunction handle_page(url){\n    page.open(url, function(){\n        waitFor(function() {\n           // something\n        }, function() {\n            next_page();\n        });\n    });\n}\n\nfunction next_page(){\n    var url = urls.shift();\n    if(!urls){\n        phantom.exit(0);\n    }\n    handle_page(url);\n}\n\nnext_page();\n\n']",https://stackoverflow.com/questions/26681464/looping-over-urls-to-do-the-same-thing,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping contents of multi web pages of a website using BeautifulSoup and Selenium,"
The website I want to scrap is :
http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061
I want to get the last page number of the above the link for proceeding, which is 499 while taking the screenshot.

My code :
   from bs4 import BeautifulSoup 
   from urllib.request import urlopen as uReq
   from selenium import webdriver;import time
   from selenium.webdriver.common.by import By
   from selenium.webdriver.support.ui import WebDriverWait
   from selenium.webdriver.support import expected_conditions as EC
   from selenium.webdriver.common.desired_capabilities import         DesiredCapabilities

   firefox_capabilities = DesiredCapabilities.FIREFOX
   firefox_capabilities['marionette'] = True
   firefox_capabilities['binary'] = '/etc/firefox'

   driver = webdriver.Firefox(capabilities=firefox_capabilities)
   url = ""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061""

   driver.get(url)
   wait = WebDriverWait(driver, 10)
   soup=BeautifulSoup(driver.page_source,""lxml"")
   containers = soup.findAll(""ul"",{""class"":""pages table""})
   containers[0] = soup.findAll(""li"")
   li_len = len(containers[0])
   for item in soup.find(""ul"",{""class"":""pages table""}) : 
   li_text = item.select(""li"")[li_len].text
   print(""li_text : {}\n"".format(li_text))
   driver.quit()

I need help to figure out the error in my code for getting the last page number. Also, I would be grateful if someone give the alternate solution for the same and suggest ways to achieve my intention.
",1k,"
            0
        ","['\nIf you want to get the last page number of the above the link for proceeding, which is 499 you can use either Selenium or Beautifulsoup as follows :\n\nSelenium :\nfrom selenium import webdriver\n\ndriver = webdriver.Firefox(executable_path=r\'C:\\Utility\\BrowserDrivers\\geckodriver.exe\')\nurl = ""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061""\ndriver.get(url)\nelement = driver.find_element_by_xpath(""//div[@class=\'row pagination\']//p/span[contains(.,\'Reviews on Reliance Jio\')]"")\ndriver.execute_script(""return arguments[0].scrollIntoView(true);"", element)\nprint(driver.find_element_by_xpath(""//ul[@class=\'pagination table\']/li/ul[@class=\'pages table\']//li[last()]/a"").get_attribute(""innerHTML""))\ndriver.quit()\n\nConsole Output :\n499\n\n\nBeautifulsoup :\nimport bs4\nfrom bs4 import BeautifulSoup as soup\nfrom urllib.request import urlopen as uReq\n\nurl = ""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061""\nuClient = uReq(url)\npage_html = uClient.read()\nuClient.close()\npage_soup = soup(page_html, ""html.parser"")\ncontainer = page_soup.find(""ul"",{""class"":""pages table""})\nall_li = container.findAll(""li"")\nlast_div = None\nfor last_div in all_li:pass\nif last_div:\n    content = last_div.getText()\n    print(content)\n\nConsole Output :\n499\n\n']",https://stackoverflow.com/questions/47869382/scraping-contents-of-multi-web-pages-of-a-website-using-beautifulsoup-and-seleni,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrape web pages in real time with Node.js,"
What's a good was to scrape website content using Node.js. I'd like to build something very, very fast that can execute searches in the style of kayak.com, where one query is dispatched to several different sites, the results scraped, and returned to the client as they become available.
Let's assume that this script should just provide the results in JSON format, and we can process them either directly in the browser or in another web application.
A few starting points:
Using node.js and jquery to scrape websites
Anybody have any ideas?
",46k,"
            66
        ","['\nNode.io seems to take the cake :-)\n', '\nAll aforementioned solutions presume running the scraper locally. This means you will be severely limited in performance (due to running them in sequence or in a limited set of threads). A better approach, imho, is to rely on an existing, albeit commercial, scraping grid.\nHere is an example:\nvar bobik = new Bobik(""YOUR_AUTH_TOKEN"");\nbobik.scrape({\n  urls: [\'amazon.com\', \'zynga.com\', \'http://finance.google.com/\', \'http://shopping.yahoo.com\'],\n  queries:  [""//th"", ""//img/@src"", ""return document.title"", ""return $(\'script\').length"", ""#logo"", "".logo""]\n}, function (scraped_data) {\n  if (!scraped_data) {\n    console.log(""Data is unavailable"");\n    return;\n  }\n  var scraped_urls = Object.keys(scraped_data);\n  for (var url in scraped_urls)\n    console.log(""Results from "" + url + "": "" + scraped_data[scraped_urls[url]]);\n});\n\nHere, scraping is performed remotely and a callback is issued to your code only when results are ready (there is also an option to collect results as they become available).\nYou can download Bobik client proxy SDK at https://github.com/emirkin/bobik_javascript_sdk\n', ""\nI've been doing research myself, and https://npmjs.org/package/wscraper boasts itself as a\n\na web scraper agent based on cheerio.js a fast, flexible, and lean\n  implementation of core jQuery; built on top of request.js; inspired by\n  http-agent.js\n\nVery low usage (according to npmjs.org) but worth a look for any interested parties.\n"", ""\nYou don't always need to jQuery. If you play with the DOM returned from jsdom for example you can easily take what you need yourself (also considering you dont have to worry about xbrowser issues.) See: https://gist.github.com/1335009 that's not taking away from node.io at all, just saying you might be able to do it yourself depending...\n"", ""\nThe new way using ES7/promises\nUsually when you're scraping you want to use some method to\n\nGet the resource on the webserver (html document usually)\nRead that resource and work with it as\n\n\nA DOM/tree structure and make it navigable\nparse it as token-document with something like SAS.\n\n\nBoth tree, and token-parsing have advantages, but tree is usually substantially simpler. We'll do that. Check out request-promise, here is how it works:\nconst rp = require('request-promise');\nconst cheerio = require('cheerio'); // Basically jQuery for node.js \n\nconst options = {\n    uri: 'http://www.google.com',\n    transform: function (body) {\n        return cheerio.load(body);\n    }\n};\n\nrp(options)\n    .then(function ($) {\n        // Process html like you would with jQuery... \n    })\n    .catch(function (err) {\n        // Crawling failed or Cheerio \n\nThis is using cheerio which is essentially a lightweight server-side jQuery-esque library (that doesn't need a window object, or jsdom).\nBecause you're using promises, you can also write this in an asychronous function. It'll look synchronous, but it'll be asynchronous with ES7:\nasync function parseDocument() {\n    let $;\n    try {\n      $ = await rp(options);\n    } catch (err) { console.error(err); }\n\n    console.log( $('title').text() ); // prints just the text in the <title>\n}\n\n"", '\ncheck out https://github.com/rc0x03/node-promise-parser\nFast: uses libxml C bindings\nLightweight: no dependencies like jQuery, cheerio, or jsdom\nClean: promise based interface- no more nested callbacks\nFlexible: supports both CSS and XPath selectors\n\n', ""\nI see most answers the right path with cheerio and so forth, however once you get to the point where you need to parse and execute JavaScript (ala SPA's and more), then I'd check out https://github.com/joelgriffith/navalia (I'm the author). Navalia is built to support scraping in a headless-browser context, and it's pretty quick. Thanks!\n"", '\nIt is my easy to use but badly spelled general purpose scraper https://github.com/harish2704/html-scraper written for Node.JS\nIt can extract information based on predefined schemas.\nA schema defnition includes a css selector and a data extraction function.\nIt currently using cheerio for dom parsing..\n']",https://stackoverflow.com/questions/5211486/scrape-web-pages-in-real-time-with-node-js,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I scrape an HTML table to CSV?,"
The Problem
I use a tool at work that lets me do queries and get back HTML tables of info. I do not have any kind of back-end access to it.
A lot of this info would be much more useful if I could put it into a spreadsheet for sorting, averaging, etc. How can I screen-scrape this data to a CSV file?
My First Idea
Since I know jQuery, I thought I might use it to strip out the table formatting onscreen, insert commas and line breaks, and just copy the whole mess into notepad and save as a CSV. Any better ideas?
The Solution
Yes, folks, it really was as easy as copying and pasting. Don't I feel silly.
Specifically, when I pasted into the spreadsheet, I had to select ""Paste Special"" and choose the format ""text."" Otherwise it tried to paste everything into a single cell, even if I highlighted the whole spreadsheet.
",83k,"
            44
        ","[""\n\nSelect the HTML table in your tools's UI and copy it into the clipboard (if that's possible\nPaste it into Excel.\nSave as CSV file\n\nHowever, this is a manual solution not an automated one.\n"", '\nusing python: \nfor example imagine you want to scrape forex quotes in csv form from some site like:fxquotes\nthen...\nfrom BeautifulSoup import BeautifulSoup\nimport urllib,string,csv,sys,os\nfrom string import replace\n\ndate_s = \'&date1=01/01/08\'\ndate_f = \'&date=11/10/08\'\nfx_url = \'http://www.oanda.com/convert/fxhistory?date_fmt=us\'\nfx_url_end = \'&lang=en&margin_fixed=0&format=CSV&redirected=1\'\ncur1,cur2 = \'USD\',\'AUD\'\nfx_url = fx_url + date_f + date_s + \'&exch=\' + cur1 +\'&exch2=\' + cur1\nfx_url = fx_url +\'&expr=\' + cur2 +  \'&expr2=\' + cur2 + fx_url_end\ndata = urllib.urlopen(fx_url).read()\nsoup = BeautifulSoup(data)\ndata = str(soup.findAll(\'pre\', limit=1))\ndata = replace(data,\'[<pre>\',\'\')\ndata = replace(data,\'</pre>]\',\'\')\nfile_location = \'/Users/location_edit_this\'\nfile_name = file_location + \'usd_aus.csv\'\nfile = open(file_name,""w"")\nfile.write(data)\nfile.close()\n\n\nedit: to get values from a table:\nexample from: palewire\nfrom mechanize import Browser\nfrom BeautifulSoup import BeautifulSoup\n\nmech = Browser()\n\nurl = ""http://www.palewire.com/scrape/albums/2007.html""\npage = mech.open(url)\n\nhtml = page.read()\nsoup = BeautifulSoup(html)\n\ntable = soup.find(""table"", border=1)\n\nfor row in table.findAll(\'tr\')[1:]:\n    col = row.findAll(\'td\')\n\n    rank = col[0].string\n    artist = col[1].string\n    album = col[2].string\n    cover_link = col[3].img[\'src\']\n\n    record = (rank, artist, album, cover_link)\n    print ""|"".join(record)\n\n', '\nThis is my python version using the (currently) latest version of BeautifulSoup which can be obtained using, e.g.,\n$ sudo easy_install beautifulsoup4\n\nThe script reads HTML from the standard input, and outputs the text found in all tables in proper CSV format.\n#!/usr/bin/python\nfrom bs4 import BeautifulSoup\nimport sys\nimport re\nimport csv\n\ndef cell_text(cell):\n    return "" "".join(cell.stripped_strings)\n\nsoup = BeautifulSoup(sys.stdin.read())\noutput = csv.writer(sys.stdout)\n\nfor table in soup.find_all(\'table\'):\n    for row in table.find_all(\'tr\'):\n        col = map(cell_text, row.find_all(re.compile(\'t[dh]\')))\n        output.writerow(col)\n    output.writerow([])\n\n', '\nEven easier (because it saves it for you for next time) ...\nIn Excel\nData/Import External Data/New Web Query\nwill take you to a url prompt. Enter your url, and it will delimit available tables on the page to import. Voila.\n', '\nTwo ways come to mind (especially for those of us that don\'t have Excel):\n\nGoogle Spreadsheets has an excellent importHTML function: \n\n=importHTML(""http://example.com/page/with/table"", ""table"", index\nIndex starts at 1\nI recommend a copy and paste values shortly after import\nFile -> Download as -> CSV\n\nPython\'s superb Pandas library has handy read_html and to_csv functions\n\nHere\'s a basic Python3 script that prompts for the URL, which table at that URL, and a filename for the CSV.\n\n\n', '\nQuick and dirty:\nCopy out of browser into Excel, save as CSV.\nBetter solution (for long term use):\nWrite a bit of code in the language of your choice that will pull the html contents down, and scrape out the bits that you want.  You could probably throw in all of the data operations (sorting, averaging, etc) on top of the data retrieval.  That way, you just have to run your code and you get the actual report that you want.\nIt all depends on how often you will be performing this particular task.\n', '\nExcel can open a http page.\nEg:\n\nClick File, Open\nUnder filename, paste the URL  ie: How can I scrape an HTML table to CSV?\nClick ok\n\nExcel does its best to convert the html to a table.\nIts not the most elegant solution, but does work!\n', '\nBasic Python implementation using BeautifulSoup, also considering both rowspan and colspan:\nfrom BeautifulSoup import BeautifulSoup\n\ndef table2csv(html_txt):\n   csvs = []\n   soup = BeautifulSoup(html_txt)\n   tables = soup.findAll(\'table\')\n\n   for table in tables:\n       csv = \'\'\n       rows = table.findAll(\'tr\')\n       row_spans = []\n       do_ident = False\n\n       for tr in rows:\n           cols = tr.findAll([\'th\',\'td\'])\n\n           for cell in cols:\n               colspan = int(cell.get(\'colspan\',1))\n               rowspan = int(cell.get(\'rowspan\',1))\n\n               if do_ident:\n                   do_ident = False\n                   csv += \',\'*(len(row_spans))\n\n               if rowspan > 1: row_spans.append(rowspan)\n\n               csv += \'""{text}""\'.format(text=cell.text) + \',\'*(colspan)\n\n           if row_spans:\n               for i in xrange(len(row_spans)-1,-1,-1):\n                   row_spans[i] -= 1\n                   if row_spans[i] < 1: row_spans.pop()\n\n           do_ident = True if row_spans else False\n\n           csv += \'\\n\'\n\n       csvs.append(csv)\n       #print csv\n\n   return \'\\n\\n\'.join(csvs)\n\n', '\nHere is a tested example that combines grequest and soup to download large quantities of pages from a structured website:\n#!/usr/bin/python\n\nfrom bs4 import BeautifulSoup\nimport sys\nimport re\nimport csv\nimport grequests\nimport time\n\ndef cell_text(cell):\n    return "" "".join(cell.stripped_strings)\n\ndef parse_table(body_html):\n    soup = BeautifulSoup(body_html)\n    for table in soup.find_all(\'table\'):\n        for row in table.find_all(\'tr\'):\n            col = map(cell_text, row.find_all(re.compile(\'t[dh]\')))\n            print(col)\n\ndef process_a_page(response, *args, **kwargs): \n    parse_table(response.content)\n\ndef download_a_chunk(k):\n    chunk_size = 10 #number of html pages\n    x = ""http://www.blahblah....com/inclusiones.php?p=""\n    x2 = ""&name=...""\n    URLS = [x+str(i)+x2 for i in range(k*chunk_size, k*(chunk_size+1)) ]\n    reqs = [grequests.get(url, hooks={\'response\': process_a_page}) for url in URLS]\n    resp = grequests.map(reqs, size=10)\n\n# download slowly so the server does not block you\nfor k in range(0,500):\n    print(""downloading chunk "",str(k))\n    download_a_chunk(k)\n    time.sleep(11)\n\n', ""\nHave you tried opening it with excel?\nIf you save a spreadsheet in excel as html you'll see the format excel uses.\nFrom a web app I wrote I spit out this html format so the user can export to excel.\n"", ""\nIf you're screen scraping and the table you're trying to convert has a given ID, you could always do a regex parse of the html along with some scripting to generate a CSV.\n""]",https://stackoverflow.com/questions/259091/how-can-i-scrape-an-html-table-to-csv,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to download any(!) webpage with correct charset in python?,"
Problem
When screen-scraping a webpage using python one has to know the character encoding of the page. If you get the character encoding wrong than your output will be messed up.
People usually use some rudimentary technique to detect the encoding. They either use the charset from the header or the charset defined in the meta tag or they use an encoding detector (which does not care about meta tags or headers).
By using only one these techniques, sometimes you will not get the same result as you would in a browser.
Browsers do it this way:

Meta tags always takes precedence (or xml definition)
Encoding defined in the header is used when there is no charset defined in a meta tag
If the encoding is not defined at all, than it is time for encoding detection.

(Well... at least that is the way I believe most browsers do it. Documentation is really scarce.)
What I'm looking for is a library that can decide the character set of a page the way a browser would. I'm sure I'm not the first who needs a proper solution to this problem.
Solution (I have not tried it yet...)
According to Beautiful Soup's documentation.
Beautiful Soup tries the following encodings, in order of priority, to turn your document into Unicode:

An encoding you pass in as the
fromEncoding argument to the soup
constructor.
An encoding discovered  in the document itself: for instance,   in an XML declaration or (for HTML   documents) an http-equiv META tag. If   Beautiful Soup finds this kind of   encoding within the document, it   parses the document again from the   beginning and gives the new encoding   a try. The only exception is if you   explicitly specified an encoding, and   that encoding actually worked: then   it will ignore any encoding it finds   in the document.
An encoding sniffed   by looking at the first few bytes of   the file. If an encoding is detected
at this stage, it will be one of the
UTF-* encodings, EBCDIC, or ASCII.
An
encoding sniffed by the chardet
library, if you have it installed.
UTF-8
Windows-1252

",16k,"
            35
        ","[""\nWhen you download a file with urllib or urllib2, you can find out whether a charset header was transmitted:\nfp = urllib2.urlopen(request)\ncharset = fp.headers.getparam('charset')\n\nYou can use BeautifulSoup to locate a meta element in the HTML:\nsoup = BeatifulSoup.BeautifulSoup(data)\nmeta = soup.findAll('meta', {'http-equiv':lambda v:v.lower()=='content-type'})\n\nIf neither is available, browsers typically fall back to user configuration, combined with auto-detection. As rajax proposes, you could use the chardet module. If you have user configuration available telling you that the page should be Chinese (say), you may be able to do better.\n"", '\nUse the Universal Encoding Detector:\n>>> import chardet\n>>> chardet.detect(urlread(""http://google.cn/""))\n{\'encoding\': \'GB2312\', \'confidence\': 0.99}\n\nThe other option would be to just use wget:\n  import os\n  h = os.popen(\'wget -q -O foo1.txt http://foo.html\')\n  h.close()\n  s = open(\'foo1.txt\').read()\n\n', ""\nIt seems like you need a hybrid of the answers presented:\n\nFetch the page using urllib\nFind <meta> tags using beautiful soup or other method\nIf no meta tags exist, check the headers returned by urllib\nIf that still doesn't give you an answer, use the universal encoding detector.\n\nI honestly don't believe you're going to find anything better than that.  \nIn fact if you read further into the FAQ you linked to in the comments on the other answer, that's what the author of detector library advocates.\nIf you believe the FAQ, this is what the browsers do (as requested in your original question) as the detector is a port of the firefox sniffing code.\n"", '\nI would use html5lib for this.\n', ""\nScrapy downloads a page and detects a correct encoding for it, unlike requests.get(url).text or urlopen. To do so it tries to follow browser-like rules - this is the best one can do, because website owners have incentive to make their websites work in a browser. Scrapy needs to take HTTP headers, <meta> tags, BOM marks and differences in encoding names in account. \nContent-based guessing (chardet, UnicodeDammit) on its own is not a correct solution, as it may fail; it should be only used as a last resort when headers or <meta> or BOM marks are not available or provide no information.\nYou don't have to use Scrapy to get its encoding detection functions; they are released (among with some other stuff) in a separate library called w3lib: https://github.com/scrapy/w3lib. \nTo get page encoding and unicode body use w3lib.encoding.html_to_unicode function, with a content-based guessing fallback:\nimport chardet\nfrom w3lib.encoding import html_to_unicode\n\ndef _guess_encoding(data):\n    return chardet.detect(data).get('encoding')\n\ndetected_encoding, html_content_unicode = html_to_unicode(\n    content_type_header,\n    html_content_bytes,\n    default_encoding='utf8', \n    auto_detect_fun=_guess_encoding,\n)\n\n"", '\ninstead of trying to get a page then figuring out the charset the browser would use, why not just use a browser to fetch the page and check what charset it uses.. \nfrom win32com.client import DispatchWithEvents\nimport threading\n\n\nstopEvent=threading.Event()\n\nclass EventHandler(object):\n    def OnDownloadBegin(self):\n        pass\n\ndef waitUntilReady(ie):\n    """"""\n    copypasted from\n    http://mail.python.org/pipermail/python-win32/2004-June/002040.html\n    """"""\n    if ie.ReadyState!=4:\n        while 1:\n            print ""waiting""\n            pythoncom.PumpWaitingMessages()\n            stopEvent.wait(.2)\n            if stopEvent.isSet() or ie.ReadyState==4:\n                stopEvent.clear()\n                break;\n\nie = DispatchWithEvents(""InternetExplorer.Application"", EventHandler)\nie.Visible = 0\nie.Navigate(\'http://kskky.info\')\nwaitUntilReady(ie)\nd = ie.Document\nprint d.CharSet\n\n', '\nBeautifulSoup dose this with UnicodeDammit : Unicode, Dammit\n']",https://stackoverflow.com/questions/1495627/how-to-download-any-webpage-with-correct-charset-in-python,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nokogiri, open-uri, and Unicode Characters","
I'm using Nokogiri and open-uri to grab the contents of the title tag on a webpage, but am having trouble with accented characters.  What's the best way to deal with these?  Here's what I'm doing:
require 'open-uri'
require 'nokogiri'

doc = Nokogiri::HTML(open(link))
title = doc.at_css(""title"")

At this point, the title looks like this:

Rag\303\271

Instead of:

Rag霉

How can I have nokogiri return the proper character (e.g. 霉 in this case)?
Here's an example URL:
http://www.epicurious.com/recipes/food/views/Tagliatelle-with-Duck-Ragu-242037
",23k,"
            27
        ","['\nSummary: When feeding UTF-8 to Nokogiri through open-uri, use open(...).read and pass the resulting string to Nokogiri.\nAnalysis:\nIf I fetch the page using curl, the headers properly show Content-Type: text/html; charset=UTF-8 and the file content includes valid UTF-8, e.g. ""Genealog铆a de Jesucristo"". But even with a magic comment on the Ruby file and setting the doc encoding, it\'s no good:\n# encoding: UTF-8\nrequire \'nokogiri\'\nrequire \'open-uri\'\n\ndoc = Nokogiri::HTML(open(\'http://www.biblegateway.com/passage/?search=Mateo1-2&version=NVI\'))\ndoc.encoding = \'utf-8\'\nh52 = doc.css(\'h5\')[1]\nputs h52.text, h52.text.encoding\n#=> Genealog脙 a de Jesucristo\n#=> UTF-8\n\nWe can see that this is not the fault of open-uri:\nhtml = open(\'http://www.biblegateway.com/passage/?search=Mateo1-2&version=NVI\')\ngene = html.read[/Gene\\S+/]\nputs gene, gene.encoding\n#=> Genealog铆a\n#=> UTF-8\n\nThis is a Nokogiri issue when dealing with open-uri, it seems. This can be worked around by passing the HTML as a raw string to Nokogiri:\n# encoding: UTF-8\nrequire \'nokogiri\'\nrequire \'open-uri\'\n\nhtml = open(\'http://www.biblegateway.com/passage/?search=Mateo1-2&version=NVI\')\ndoc = Nokogiri::HTML(html.read)\ndoc.encoding = \'utf-8\'\nh52 = doc.css(\'h5\')[1].text\nputs h52, h52.encoding, h52 == ""Genealog铆a de Jesucristo""\n#=> Genealog铆a de Jesucristo\n#=> UTF-8\n#=> true\n\n', ""\nI was having the same problem and the Iconv approach wasn't working. Nokogiri::HTML is an alias to Nokogiri::HTML.parse(thing, url, encoding, options).\nSo, you just need to do:\ndoc = Nokogiri::HTML(open(link).read, nil, 'utf-8')\nand it'll convert the page encoding properly to utf-8. You'll see Rag霉 instead of Rag\\303\\271.\n"", '\nWhen you say ""looks like this,"" are you viewing this value IRB? It\'s going to escape non-ASCII range characters with C-style escaping of the byte sequences that represent the characters.\nIf you print them with puts, you\'ll get them back as you expect, presuming your shell console is using the same encoding as the string in question (Apparently UTF-8 in this case, based on the two bytes returned for that character). If you are storing the values in a text file, printing to a handle should also result in UTF-8 sequences.\nIf you need to translate between UTF-8 and other encodings, the specifics depend on whether you\'re in Ruby 1.9 or 1.8.6.\nFor 1.9: http://blog.grayproductions.net/articles/ruby_19s_string\nfor 1.8, you probably need to look at Iconv.\nAlso, if you need to interact with COM components in Windows, you\'ll need to tell ruby to use the correct encoding with something like the following:\nrequire \'win32ole\'\n\nWIN32OLE.codepage = WIN32OLE::CP_UTF8\n\nIf you\'re interacting with mysql, you\'ll need to set the collation on the table to one that supports the encoding that you\'re working with. In general, it\'s best to set the collation to UTF-8, even if some of your content is coming back in other encodings; you\'ll just need to convert as necessary.\nNokogiri has some features for dealing with different encodings (probably through Iconv), but I\'m a little out of practice with that, so I\'ll leave explanation of that to someone else.\n', '\nTry setting the encoding option of Nokogiri, like so:\nrequire \'open-uri\'\nrequire \'nokogiri\'\ndoc = Nokogiri::HTML(open(link))\ndoc.encoding = \'utf-8\'\ntitle = doc.at_css(""title"")\n\n', '\nYou need to convert the response from the website being scraped (here epicurious.com) into utf-8 encoding.\nas per the html content from the page being scraped, its ""ISO-8859-1"" for now. So, you need to do something like this:\nrequire \'iconv\'\ndoc = Nokogiri::HTML(Iconv.conv(\'utf-8//IGNORE\', \'ISO-8859-1\', open(link).read))\n\nRead more about it here: http://www.quarkruby.com/2009/9/22/rails-utf-8-and-html-screen-scraping\n', '\nChanging Nokogiri::HTML(...) to Nokogiri::HTML5(...) fixed issues I was having with parsing certain special character, specifically em-dashes.\n(The accented characters in your link came through fine in both, so don\'t know if this would help you with that.)\nEXAMPLE:\nurl = \'https://www.youtube.com/watch?v=4r6gr7uytQA\'\n\ndoc = Nokogiri::HTML(open(url))\ndoc.title\n=> ""Josh Waitzkin 芒\\u0080\\u0094 How to Cram 2 Months of Learning into 1 Day | The Tim Ferriss Show - YouTube""\n\ndoc = Nokogiri::HTML5(open(url))\ndoc.title\n=> ""Josh Waitzkin 鈥?How to Cram 2 Months of Learning into 1 Day | The Tim Ferriss Show - YouTube""\n\n', '\nJust to add a cross-reference, this SO page gives some related information:\nHow to make Nokogiri transparently return un/encoded Html entities untouched?\n', ""\nTip: you could also use the Scrapifier gem to get metadata, as the page title, from URIs in a very simple way. The data are all encoded in UTF-8.\nCheck it out: https://github.com/tiagopog/scrapifier\nHope it's useful for you.\n""]",https://stackoverflow.com/questions/2572396/nokogiri-open-uri-and-unicode-characters,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using Python and Mechanize to submit form data and authenticate,"
I want to submit login to the website Reddit.com, navigate to a particular area of the page, and submit a comment.  I don't see what's wrong with this code, but it is not working in that no change is reflected on the Reddit site.
import mechanize
import cookielib


def main():

#Browser
br = mechanize.Browser()


# Cookie Jar
cj = cookielib.LWPCookieJar()
br.set_cookiejar(cj)

# Browser options
br.set_handle_equiv(True)
br.set_handle_gzip(True)
br.set_handle_redirect(True)
br.set_handle_referer(True)
br.set_handle_robots(False)

# Follows refresh 0 but not hangs on refresh > 0
br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)

#Opens the site to be navigated
r= br.open('http://www.reddit.com')
html = r.read()

# Select the second (index one) form
br.select_form(nr=1)

# User credentials
br.form['user'] = 'DUMMYUSERNAME'
br.form['passwd'] = 'DUMMYPASSWORD'

# Login
br.submit()

#Open up comment page
r= br.open('http://www.reddit.com/r/PoopSandwiches/comments/f47f8/testing/')
html = r.read()

#Text box is the 8th form on the page (which, I believe, is the text area)
br.select_form(nr=7)

#Change 'text' value to a testing string
br.form['text']= ""this is an automated test""

#Submit the information  
br.submit()

What's wrong with this?
",26k,"
            15
        ","['\nI would definitely suggest trying to use the API if possible, but this works for me (not for your example post, which has been deleted, but for any active one):\n#!/usr/bin/env python\n\nimport mechanize\nimport cookielib\nimport urllib\nimport logging\nimport sys\n\ndef main():\n\n    br = mechanize.Browser()\n    cj = cookielib.LWPCookieJar()\n    br.set_cookiejar(cj)\n\n    br.set_handle_equiv(True)\n    br.set_handle_gzip(True)\n    br.set_handle_redirect(True)\n    br.set_handle_referer(True)\n    br.set_handle_robots(False)\n\n    br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)\n\n    r= br.open(\'http://www.reddit.com\')\n\n    # Select the second (index one) form\n    br.select_form(nr=1)\n\n    # User credentials\n    br.form[\'user\'] = \'user\'\n    br.form[\'passwd\'] = \'passwd\'\n\n    # Login\n    br.submit()\n\n    # Open up comment page\n    posting = \'http://www.reddit.com/r/PoopSandwiches/comments/f47f8/testing/\'\n    rval = \'PoopSandwiches\'\n    # you can get the rval in other ways, but this will work for testing\n\n    r = br.open(posting)\n\n    # You need the \'uh\' value from the first form\n    br.select_form(nr=0)\n    uh = br.form[\'uh\']\n\n    br.select_form(nr=7)\n    thing_id = br.form[\'thing_id\']\n    id = \'#\' + br.form.attrs[\'id\']\n    # The id that gets posted is the form id with a \'#\' prepended.\n\n    data = {\'uh\':uh, \'thing_id\':thing_id, \'id\':id, \'renderstyle\':\'html\', \'r\':rval, \'text\':""Your text here!""}\n    new_data_dict = dict((k, urllib.quote(v).replace(\'%20\', \'+\')) for k, v in data.iteritems())\n\n    # not sure if the replace needs to happen, I did it anyway\n    new_data = \'thing_id=%(thing_id)s&text=%(text)s&id=%(id)s&r=%(r)s&uh=%(uh)s&renderstyle=%(renderstyle)s\' %(new_data_dict)\n\n    # not sure which of these headers are really needed, but it works with all\n    # of them, so why not just include them.\n    req = mechanize.Request(\'http://www.reddit.com/api/comment\', new_data)\n    req.add_header(\'Referer\', posting)\n    req.add_header(\'Accept\', \' application/json, text/javascript, */*\')\n    req.add_header(\'Content-Type\', \'application/x-www-form-urlencoded; charset=UTF-8\')\n    req.add_header(\'X-Requested-With\', \'XMLHttpRequest\')\n    cj.add_cookie_header(req)\n    res = mechanize.urlopen(req)\n\nmain()\n\nIt would be interesting to turn javascript off and see how the reddit comments are handled then.  Right now there is a bunch of magic that happens in an onsubmit function called when making your post.  This is where the uh and id value get added.\n']",https://stackoverflow.com/questions/4720470/using-python-and-mechanize-to-submit-form-data-and-authenticate,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
View Generated Source (After AJAX/JavaScript) in C#,"
Is there a way to view the generated source of a web page (the code after all AJAX calls and JavaScript DOM manipulations have taken place) from a C# application without opening up a browser from the code?
Viewing the initial page using a WebRequest or WebClient object works ok, but if the page makes extensive use of JavaScript to alter the DOM on page load, then these don't provide an accurate picture of the page.
I have tried using Selenium and Watin UI testing frameworks and they work perfectly, supplying the generated source as it appears after all JavaScript manipulations are completed.  Unfortunately, they do this by opening up an actual web browser, which is very slow.  I've implemented a selenium server which offloads this work to another machine, but there is still a substantial delay.
Is there a .Net library that will load and parse a page (like a browser) and spit out the generated code?  Clearly, Google and Yahoo aren't opening up browsers for every page they want to spider (of course they may have more resources than me...).  
Is there such a library or am I out of luck unless I'm willing to dissect the source code of an open source browser?
SOLUTION
Well, thank you everyone for you're help.  I have a working solution that is about 10X faster then Selenium. Woo!
Thanks to this old article from beansoftware I was able to use the System.Windows.Forms.WebBrowser control to download the page and parse it, then give em the generated source.  Even though the control is in Windows.Forms, you can still run it from Asp.Net (which is what I'm doing), just remember to add System.Window.Forms to your project references.
There are two notable things about the code.  First, the WebBrowser control is called in a new thread.  This is because it must run on a single threaded apartment.
Second, the GeneratedSource variable is set in two places.  This is not due to an intelligent design decision :)  I'm still working on it and will update this answer when I'm done.  wb_DocumentCompleted() is called multiple times.  First when the initial HTML is downloaded, then again when the first round of JavaScript completes.  Unfortunately, the site I'm scraping has 3 different loading stages.  1) Load initial HTML 2) Do first round of JavaScript DOM manipulation 3) pause for half a second then do a second round of JS DOM manipulation.
For some reason, the second round isn't cause by the wb_DocumentCompleted() function, but it is always caught when wb.ReadyState == Complete.  So why not remove it from wb_DocumentCompleted()? I'm still not sure why it isn't caught there and that's where the beadsoftware article recommended putting it.  I'm going to keep looking into it.  I just wanted to publish this code so anyone who's interested can use it.  Enjoy!
using System.Threading;
using System.Windows.Forms;

public class WebProcessor
{
    private string GeneratedSource{ get; set; }
    private string URL { get; set; }

    public string GetGeneratedHTML(string url)
    {
        URL = url;

        Thread t = new Thread(new ThreadStart(WebBrowserThread));
        t.SetApartmentState(ApartmentState.STA);
        t.Start();
        t.Join();

        return GeneratedSource;
    }

    private void WebBrowserThread()
    {
        WebBrowser wb = new WebBrowser();
        wb.Navigate(URL);

        wb.DocumentCompleted += 
            new WebBrowserDocumentCompletedEventHandler(
                wb_DocumentCompleted);

        while (wb.ReadyState != WebBrowserReadyState.Complete)
            Application.DoEvents();

        //Added this line, because the final HTML takes a while to show up
        GeneratedSource= wb.Document.Body.InnerHtml;

        wb.Dispose();
    }

    private void wb_DocumentCompleted(object sender, 
        WebBrowserDocumentCompletedEventArgs e)
    {
        WebBrowser wb = (WebBrowser)sender;
        GeneratedSource= wb.Document.Body.InnerHtml;
    }
}

",14k,"
            27
        ","['\nit is possibly using an instance of a browser (in you case: the ie control). you can easily use in your app and open a page. the control will then load it and process any javascript. once this is done you can access the controls dom object and get the ""interpreted"" code.\n', '\nBest way is using PhantomJs. That\'s Great. (sample of that is Article).\nMy solution is look like this:\nvar page = require(\'webpage\').create();\n\npage.open(""https://sample.com"", function(){\n    page.evaluate(function(){\n        var i = 0,\n        oJson = jsonData,\n        sKey;\n        localStorage.clear();\n\n        for (; sKey = Object.keys(oJson)[i]; i++) {\n            localStorage.setItem(sKey,oJson[sKey])\n        }\n    });\n\n    page.open(""https://sample.com"", function(){\n        setTimeout(function(){\n         page.render(""screenshoot.png"") \n            // Where you want to save it    \n           console.log(page.content); //page source\n            // You can access its content using jQuery\n            var fbcomments = page.evaluate(function(){\n                return $(""body"").contents().find("".content"") \n            }) \n            phantom.exit();\n        },10000)\n    });     \n});\n\n', ""\nTheoretically yes, but, at present, no.\nI don't think there is currently a product or OSS project that does this.  Such a product would need to have it's own javascript interpreter and be able to accurately emulate the run-time environment and quirks of every browser it supports.\nGiven that you need something that accurately emulates the server + browser environment in order to produce the final page code, in the long run, I think that using a browser instance is the best way to accurately generate the page in its final state. This is especially true, when you consider that, after the page load completes, the page sources can still change over time in the browser from AJAX/javascript.\n""]",https://stackoverflow.com/questions/1307800/view-generated-source-after-ajax-javascript-in-c-sharp,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to use the WebClient.DownloadDataAsync() method in this context?,"
My plan is to have a user write down a movie title in my program and my program will pull the appropiate information asynchronously so the UI doesn't freeze up.
Here's the code:
public class IMDB
    {
        WebClient WebClientX = new WebClient();
        byte[] Buffer = null;


        public string[] SearchForMovie(string SearchParameter)
        {
            //Format the search parameter so it forms a valid IMDB *SEARCH* url.
            //From within the search website we're going to pull the actual movie
            //link.
            string sitesearchURL = FindURL(SearchParameter);

            //Have a method download asynchronously the ENTIRE source code of the
            //IMDB *search* website.
            Buffer = WebClientX.DownloadDataAsync(sitesearchURL);


            //Pass the IMDB source code to method findInformation().

            //string [] lol = findInformation();

            //????

            //Profit.

            string[] lol = null;
            return lol;
        }

My actual problem lies in the WebClientX.DownloadDataAsync() method. I can't use a string URL for it. How can I use that built in function to download the bytes of the site (for later use I will convert this to string, I know how to do this) and without freezing up my GUI?
Perhaps a clear cut example of the DownloadDataAsync so I can learn how to use it?
Thanks SO, you're always such a great resource.
",52k,"
            21
        ","[""\nThere is a newer DownloadDataTaskAsync method that allows you to await the result. It is simpler to read and easier to wire up by far. I'd use that...\nvar client = new WebClient();\n\nvar data = await client.DownloadDataTaskAsync(new Uri(imageUrl));\n\nawait outstream.WriteAsync(data, 0, data.Length);\n\n"", '\nYou need to handle the DownloadDataCompleted event:\nstatic void Main()\n{\n    string url = ""http://google.com"";\n    WebClient client = new WebClient();\n    client.DownloadDataCompleted += DownloadDataCompleted;\n    client.DownloadDataAsync(new Uri(url));\n    Console.ReadLine();\n}\n\nstatic void DownloadDataCompleted(object sender,\n    DownloadDataCompletedEventArgs e)\n{\n    byte[] raw = e.Result;\n    Console.WriteLine(raw.Length + "" bytes received"");\n}\n\nThe args contains other bits of information relating to error conditions etc - check those too.\nAlso note that you\'ll be coming into DownloadDataCompleted on a different thread; if you are in a UI (winform, wpf, etc) you\'ll need to get to the UI thread before updating the UI. From winforms, use this.Invoke. For WPF, look at the Dispatcher.\n', '\nstatic void Main(string[] args)\n{\n    byte[] data = null;\n    WebClient client = new WebClient();\n    client.DownloadDataCompleted += \n       delegate(object sender, DownloadDataCompletedEventArgs e)\n       {\n            data = e.Result;\n       };\n    Console.WriteLine(""starting..."");\n    client.DownloadDataAsync(new Uri(""http://stackoverflow.com/questions/""));\n    while (client.IsBusy)\n    {\n         Console.WriteLine(""\\twaiting..."");\n         Thread.Sleep(100);\n    }\n    Console.WriteLine(""done. {0} bytes received;"", data.Length);\n}\n\n', '\nIf anyone using above in web application or websites please set Async = ""true"" in the page directive declaration in aspx file. \n', '\nThreadPool.QueueUserWorkItem(state => WebClientX.DownloadDataAsync(sitesearchURL));\n\nhttp://workblog.pilin.name/2009/02/system.html\n', '\n//using ManualResetEvent class\nstatic ManualResetEvent evnts = new ManualResetEvent(false);\nstatic void Main(string[] args)\n{\n    byte[] data = null;\n    WebClient client = new WebClient();\n    client.DownloadDataCompleted += \n        delegate(object sender, DownloadDataCompletedEventArgs e)\n        {\n             data = e.Result;\n             evnts.Set();\n        };\n    Console.WriteLine(""starting..."");\n    evnts.Reset();\n    client.DownloadDataAsync(new Uri(""http://stackoverflow.com/questions/""));\n    evnts.WaitOne(); // wait to download complete\n\n    Console.WriteLine(""done. {0} bytes received;"", data.Length);\n}\n\n']",https://stackoverflow.com/questions/1585985/how-to-use-the-webclient-downloaddataasync-method-in-this-context,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Text Extraction from HTML Java,"
I'm working on a program that downloads HTML pages and then selects some of the information and write it to another file.
I want to extract the information which is intbetween the paragraph tags, but i can only get one line of the paragraph. My code is as follows;
FileReader fileReader = new FileReader(file);
BufferedReader buffRd = new BufferedReader(fileReader);
BufferedWriter out = new BufferedWriter(new FileWriter(newFile.txt));
String s;

while ((s = br.readLine()) !=null) {
    if(s.contains(""<p>"")) {
        try {
            out.write(s);
        } catch (IOException e) {
        }
    }
}

i was trying to add another while loop, which would tell the program to keep writing to file until the line contains the </p> tag, by saying;
while ((s = br.readLine()) !=null) {
    if(s.contains(""<p>"")) {
        while(!s.contains(""</p>"") {
            try {
                out.write(s);
            } catch (IOException e) {
            }
        }
    }
}

But this doesn't work. Could someone please help.
",52k,"
            19
        ","['\njsoup\nAnother html parser I really liked using was jsoup. You could get all the <p> elements in 2 lines of code.\nDocument doc = Jsoup.connect(""http://en.wikipedia.org/"").get();\nElements ps = doc.select(""p"");\n\nThen write it out to a file in one more line\nout.write(ps.text());  //it will append all of the p elements together in one long string\n\nor if you want them on separate lines you can iterate through the elements and write them out separately. \n', '\njericho is one of several posible html parsers that could make this task both easy and safe.\n', '\nJTidy can represent an HTML document (even a malformed one) as a document model, making the process of extracting the contents of a <p> tag a rather more elegant process than manually thunking through the raw text.\n', '\nTry (if you don\'t want to use a HTML parser library):\n\n        FileReader fileReader = new FileReader(file);\n        BufferedReader buffRd = new BufferedReader(fileReader);\n        BufferedWriter out = new BufferedWriter(new FileWriter(newFile.txt));\n        String s;\n        int writeTo = 0;\n        while ((s = br.readLine()) !=null) \n        {\n                if(s.contains(""<p>""))\n                {\n                        writeTo = 1;\n\n                        try \n                        {\n                            out.write(s);\n                    } \n                        catch (IOException e) \n                        {\n\n                    }\n                }\n                if(s.contains(""</p>""))\n                {\n                        writeTo = 0;\n\n                        try \n                        {\n                            out.write(s);\n                    } \n                        catch (IOException e) \n                        {\n\n                    }\n                }\n                else if(writeTo==1)\n                {\n                        try \n                        {\n                            out.write(s);\n                    } \n                        catch (IOException e) \n                        {\n\n                    }\n                }\n}\n\n', ""\nI've had success using TagSoup & XPath to parse HTML.\nhttp://home.ccil.org/~cowan/XML/tagsoup/\n"", '\nUse a ParserCallback. Its a simple class thats included with the JDK. It notifies you every time a new tag is found and then you can extract the text of the tag. Simple example:\nimport java.io.*;\nimport java.net.*;\nimport javax.swing.text.*;\nimport javax.swing.text.html.*;\nimport javax.swing.text.html.parser.*;\n\npublic class ParserCallbackTest extends HTMLEditorKit.ParserCallback\n{\n    private int tabLevel = 1;\n    private int line = 1;\n\n    public void handleComment(char[] data, int pos)\n    {\n        displayData(new String(data));\n    }\n\n    public void handleEndOfLineString(String eol)\n    {\n        System.out.println( line++ );\n    }\n\n    public void handleEndTag(HTML.Tag tag, int pos)\n    {\n        tabLevel--;\n        displayData(""/"" + tag);\n    }\n\n    public void handleError(String errorMsg, int pos)\n    {\n        displayData(pos + "":"" + errorMsg);\n    }\n\n    public void handleMutableTag(HTML.Tag tag, MutableAttributeSet a, int pos)\n    {\n        displayData(""mutable:"" + tag + "": "" + pos + "": "" + a);\n    }\n\n    public void handleSimpleTag(HTML.Tag tag, MutableAttributeSet a, int pos)\n    {\n        displayData( tag + ""::"" + a );\n//      tabLevel++;\n    }\n\n    public void handleStartTag(HTML.Tag tag, MutableAttributeSet a, int pos)\n    {\n        displayData( tag + "":"" + a );\n        tabLevel++;\n    }\n\n    public void handleText(char[] data, int pos)\n    {\n        displayData( new String(data) );\n    }\n\n    private void displayData(String text)\n    {\n        for (int i = 0; i < tabLevel; i++)\n            System.out.print(""\\t"");\n\n        System.out.println(text);\n    }\n\n    public static void main(String[] args)\n    throws IOException\n    {\n        ParserCallbackTest parser = new ParserCallbackTest();\n\n        // args[0] is the file to parse\n\n        Reader reader = new FileReader(args[0]);\n//      URLConnection conn = new URL(args[0]).openConnection();\n//      Reader reader = new InputStreamReader(conn.getInputStream());\n\n        try\n        {\n            new ParserDelegator().parse(reader, parser, true);\n        }\n        catch (IOException e)\n        {\n            System.out.println(e);\n        }\n    }\n}\n\nSo all you need to do is set a boolean flag when the paragraph tag is found. Then in the handleText() method you extract the text.\n', '\nTry this.\n public static void main( String[] args )\n{\n    String url = ""http://en.wikipedia.org/wiki/Big_data"";\n\n    Document document;\n    try {\n        document = Jsoup.connect(url).get();\n        Elements paragraphs = document.select(""p"");\n\n        Element firstParagraph = paragraphs.first();\n        Element lastParagraph = paragraphs.last();\n        Element p;\n        int i=1;\n        p=firstParagraph;\n        System.out.println(""*  "" +p.text());\n        while (p!=lastParagraph){\n            p=paragraphs.get(i);\n            System.out.println(""*  "" +p.text());\n            i++;\n        } \n} catch (IOException e) {\n    // TODO Auto-generated catch block\n    e.printStackTrace();\n}\n}\n\n', '\nYou may just be using the wrong tool for the job:\nperl -ne ""print if m|<p>| .. m|</p>|"" infile.txt >outfile.txt\n\n']",https://stackoverflow.com/questions/1386107/text-extraction-from-html-java,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Screen Scraping a Javascript based webpage in Python,"
I am working on a screen scraping tool in Python. But, as I look through the source of the webpage, I noticed that most of the data is coming through Javascript. 
Any idea, how to scrape javascript based webpage ? Any tool  in Python ?
Thanks
",7k,"
            4
        ","['\nScraping javascript-based webpages is possible with selenium. In particular, try the Selenium WebDriver.\n', '\nI use webkit, which is the browser renderer behind Chrome and Safari. There are Python bindings to webkit through Qt. \nAnd here is a full Python example to execute JavaScript and extract the final HTML.\n', '\nYou can use the QtWebKit module of the PyQt4 library\n']",https://stackoverflow.com/questions/8183682/screen-scraping-a-javascript-based-webpage-in-python,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Click on a javascript link within python?,"
I am navigating a site using python's mechanize module and having trouble clicking on a javascript link for next page.  I did a bit of reading and people suggested I need python-spidermonkey and DOMforms.  I managed to get them installed by I am not sure of the syntax to actually click on the link.
I can identify the code on the page as: 
<a href=""javascript:__doPostBack('ctl00$MainContent$gvSearchResults','Page$2')"">2</a>

Does anyone know how to click on it? or if perhaps there's another tool.
Thanks
",12k,"
            15
        ","['\nI mainly use HtmlUnit under jython for these use cases. Also I published a simple article on the subject: Web Scraping Ajax and Javascript sites.\n', ""\ninstead of struggling with python-spidermonkey try webkit's qt python bindings.\nHere is a full example to execute JavaScript and extract the final HTML.\n"", ""\nhow about calling __doPostBack('ctl00$MainContent$gvSearchResults','Page$'+pageid); (javascript method, via python-spidermonkey)\n""]",https://stackoverflow.com/questions/5207948/click-on-a-javascript-link-within-python,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I use Perl to grab text from a web page that is dynamically generated with JavaScript?,"
There is a website I am trying to pull information from in Perl, however the section of the page I need is being generated using javascript so all you see in the source is:
<div id=""results""></div>

I need to somehow pull out the contents of that div and save it to a file using Perl/proxies/whatever. e.g. the information I want to save would be
document.getElementById('results').innerHTML;

I am not sure if this is possible or if anyone had any ideas or a way to do this.
I was using a lynx source dump for other pages but since I cant straight forward screen scrape this page I came here to ask about it!
If anyone is interested, the page is http://downloadcenter.trendmicro.com/index.php?clk=left_nav&clkval=pattern_file&regs=NABU and the info I am trying to get is the row about the ConsumerOPR
",6k,"
            12
        ","[""\nYou'll need to reverse-engineer what the Javascript is doing. Does it fire off an AJAX request to populate the <div>? If so, it should be pretty easy to sniff the request using Firebug and then duplicate it with LWP::UserAgent or WWW::Mechanize to get the information.\nIf the Javascript is just doing pure DOM manipulation, then that means the data must exist somewhere else in the page or the Javascript already. So figure out where it's coming from and grab it.\nFinally, if none of those options are adequate, you may need to just use a real browser to do it. There are a few options for automating browser behavior, like WWW::Mechanize::Firefox or Win32::IE::Mechanize.\n"", '\nBringing the Browser to the Server by John Resig might be useful.\n', ""\nAs the content of your page is generated by some Javascript, you need the ability to :\n\nExecute some Javascript code\n\n\nEven, possibly, some complex JS code, doing Ajax requests and all that ?\n\nAnd do it with an engine that supports the functions/methods that are present in a browser (like DOM manipulations)\n\n\nA solution could be to actually really start a browser to navigate to that page, and, then, parse the page that's loaded by it, to extract the information ?\nI've never used this for grabbing, but the Selenium suite might help, here : using Selenium RC, you can start a real browser, and pilot it -- then, you have functions to get data from it.\nIt's not quite fast, and it's pretty heavy (it has to start a browser !), but it works quite well : you'll be using Firefox, for example, to navigate to your page -- which means a real Javascript engine, that's used every day by a lot of people ;-)\n"", ""\nThis might be what your looking for (in PHP):\n$url = 'http://downloadcenter.trendmicro.com/ajx/pattern_result.php';\n\n$ch = curl_init();\ncurl_setopt ($ch, CURLOPT_SSL_VERIFYPEER, FALSE);\ncurl_setopt ($ch, CURLOPT_URL, $url);\ncurl_setopt ($ch, CURLOPT_POST, 1);\ncurl_setopt ($ch, CURLOPT_POSTFIELDS, 'q=patresult_page&reg=NABU');\ncurl_setopt ($ch, CURLOPT_RETURNTRANSFER, 1);\n$content = curl_exec($ch);\ncurl_close($ch);\n\necho $content;\nexit;\n\nonce you get the content you can use something like: http://code.google.com/p/phpquery/ to parse the results you need or a similar perl equivalent??? \nAnd/or do the parsing yourself.\nFYI: all I did was use firebug to inspect the requests and recreated it with PHP/CURL...\n"", '\nto work with the dynamically created HTML you can use the FireFox Chickenfoot plugin.\nOr if you need something that works from a command line script use bindings to Perl. I have done this with Python before.\n']",https://stackoverflow.com/questions/2655034/how-can-i-use-perl-to-grab-text-from-a-web-page-that-is-dynamically-generated-wi,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can i grab CData out of BeautifulSoup,"
I have a website that I'm scraping that has a similar structure the following. I'd like to be able to grab the info out of the CData block. 
I'm using BeautifulSoup to pull other info off the page, so if the solution can work with that, it would help keep my learning curve down as I'm a python novice.
Specifically, I want to get at the two different types of data hidden in the CData statement. the first which is just text I'm pretty sure I can throw a regex at it and get what I need. For the second type, if i could drop the data that has html elements into it's own beautifulsoup, I can parse that. 
I'm just learning python and beautifulsoup, so I'm struggling to find the magical incantation that will give me just the CData by itself.
<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN""   ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">
<html xmlns=""http://www.w3.org/1999/xhtml"">  
<head>  
<title>
   Cows and Sheep
  </title>
</head>
<body>
 <div id=""main"">
  <div id=""main-precontents"">
   <div id=""main-contents"" class=""main-contents"">
    <script type=""text/javascript"">
       //<![CDATA[var _ = g_cow;_[7654]={cowname_enus:'cows rule!',leather_quality:99,icon:'cow_level_23'};_[37357]={sheepname_enus:'baa breath',wool_quality:75,icon:'sheep_level_23'};_[39654].cowmeat_enus = '<table><tr><td><b class=""q4"">cows rule!</b><br></br>
       <!--ts-->
       get it now<table width=""100%""><tr><td>NOW</td><th>NOW</th></tr></table><span>244 Cows</span><br></br>67 leather<br></br>68 Brains
       <!--yy-->
       <span class=""q0"">Cow Bonus: +9 Cow Power</span><br></br>Sheep Power 60 / 60<br></br>Sheep 88<br></br>Cow Level 555</td></tr></table>
       <!--?5695:5:40:45-->
       ';
        //]]>
      </script>
     </div>
     </div>
    </div>
 </body>
</html>

",15k,"
            12
        ","['\nOne thing you need to be careful of BeautifulSoup grabbing CData is not to use a lxml parser.\nBy default, the lxml parser will strip CDATA sections from the tree and replace them by their plain text content, Learn more here\n#Trying it with html.parser\n\n\n>>> from bs4 import BeautifulSoup\n>>> import bs4\n>>> s=\'\'\'<?xml version=""1.0"" ?>\n<foo>\n    <bar><![CDATA[\n        aaaaaaaaaaaaa\n    ]]></bar>\n</foo>\'\'\'\n>>> soup = BeautifulSoup(s, ""html.parser"")\n>>> soup.find(text=lambda tag: isinstance(tag, bs4.CData)).string.strip()\n\'aaaaaaaaaaaaa\'\n>>> \n\n', '\nBeautifulSoup sees CData as a special case (subclass) of ""navigable strings"". So for example:\nimport BeautifulSoup\n\ntxt = \'\'\'<foobar>We have\n       <![CDATA[some data here]]>\n       and more.\n       </foobar>\'\'\'\n\nsoup = BeautifulSoup.BeautifulSoup(txt)\nfor cd in soup.findAll(text=True):\n  if isinstance(cd, BeautifulSoup.CData):\n    print \'CData contents: %r\' % cd\n\nIn your case of course you could look in the subtree starting at the div with the \'main-contents\' ID, rather than all over the document tree.\n', '\nYou could try this:\nfrom BeautifulSoup import BeautifulSoup\n\n// source.html contains your html above\nf = open(\'source.html\')\nsoup = BeautifulSoup(\'\'.join(f.readlines()))\ns = soup.findAll(\'script\')\ncdata = s[0].contents[0]\n\nThat should give you the contents of cdata.\nUpdate\nThis may be a little cleaner:\nfrom BeautifulSoup import BeautifulSoup\nimport re\n\n// source.html contains your html above\nf = open(\'source.html\')\nsoup = BeautifulSoup(\'\'.join(f.readlines()))\ncdata = soup.find(text=re.compile(""CDATA""))\n\nJust personal preference, but I like the bottom one a little better.\n', ""\nimport re\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(content)\nfor x in soup.find_all('item'):\n    print re.sub('[\\[CDATA\\]]', '', x.string)\n\n"", ""\nFor anyone using BeautifulSoup4, Alex Martelli's solution works but do this:\nfrom bs4 import BeautifulSoup, CData\n\nsoup = BeautifulSoup(txt)\nfor cd in soup.findAll(text=True):\n  if isinstance(cd, Cdata):\n    print 'CData contents: %r' % cd\n\n""]",https://stackoverflow.com/questions/2032172/how-can-i-grab-cdata-out-of-beautifulsoup,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
php - Fastest way to check presence of text in many domains (above 1000),"
I have a php script running and using cURL to retrieve the content of webpages on which I would like to check for the presence of some text.
Right now it looks like this:
for( $i = 0; $i < $num_target; $i++ ) {
    $ch = curl_init();
    $timeout = 10;
    curl_setopt ($ch, CURLOPT_URL,$target[$i]);
    curl_setopt ($ch, CURLOPT_RETURNTRANSFER, true);
    curl_setopt ($ch, CURLOPT_FORBID_REUSE, true);
    curl_setopt ($ch, CURLOPT_CONNECTTIMEOUT, $timeout);
    $url = curl_exec ($ch);
    curl_close($ch);

    if (preg_match($text,$url,$match)) {
        $match[$i] = $match;
        echo ""text"" . $text . "" found in URL: "" . $url . "": "" . $match .;

        } else {
        $match[$i] = $match;
        echo ""text"" . $text . "" not found in URL: "" . $url . "": no match"";
        }
}

I was wondering if I could use a special cURL setup that makes it faster ( I looked in the php manual chose the options that seemed the best to me but I may have neglected some that could increase the speed and performance of the script).
I was then wondering if using cgi, Perl or python (or another solution) could be faster than php.
Thank you in advance for any help / advice / suggestion.
",953,"
            -2
        ","['\nYou can use curl_multi_init .... which Allows the processing of multiple cURL handles in parallel.\nExample \n$url = array();\n$url[] = \'http://www.huffingtonpost.com\';\n$url[] = \'http://www.yahoo.com\';\n$url[] = \'http://www.google.com\';\n$url[] = \'http://technet.microsoft.com/en-us/\';\n\n$start = microtime(true);\necho ""<pre>"";\nprint_r(checkLinks($url, ""Azure""));\necho ""<h1>"", microtime(true) - $start, ""</h1>"";\n\nOutput\nArray\n(\n    [0] => http://technet.microsoft.com/en-us/\n)\n\n1.2735739707947 <-- Faster\n\nFunction Used\nfunction checkLinks($nodes, $text) {\n    $mh = curl_multi_init();\n    $curl_array = array();\n    foreach ( $nodes as $i => $url ) {\n        $curl_array[$i] = curl_init($url);\n        curl_setopt($curl_array[$i], CURLOPT_RETURNTRANSFER, true);\n        curl_setopt($curl_array[$i], CURLOPT_USERAGENT, \'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.1.2) Gecko/20090729 Firefox/3.5.2 (.NET CLR 3.5.30729)\');\n        curl_setopt($curl_array[$i], CURLOPT_CONNECTTIMEOUT, 5);\n        curl_setopt($curl_array[$i], CURLOPT_TIMEOUT, 15);\n        curl_multi_add_handle($mh, $curl_array[$i]);\n    }\n    $running = NULL;\n    do {\n        usleep(10000);\n        curl_multi_exec($mh, $running);\n    } while ( $running > 0 );\n    $res = array();\n    foreach ( $nodes as $i => $url ) {\n        $curlErrorCode = curl_errno($curl_array[$i]);\n        if ($curlErrorCode === 0) {\n            $info = curl_getinfo($curl_array[$i]);\n            if ($info[\'http_code\'] == 200) {\n                if (stripos(curl_multi_getcontent($curl_array[$i]), $text) !== false) {\n                    $res[] = $info[\'url\'];\n                }\n            }\n        }\n        curl_multi_remove_handle($mh, $curl_array[$i]);\n        curl_close($curl_array[$i]);\n    }\n    curl_multi_close($mh);\n    return $res;\n}\n\n']",https://stackoverflow.com/questions/12891689/php-fastest-way-to-check-presence-of-text-in-many-domains-above-1000,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What's the best way of scraping data from a website? [closed],"






Closed. This question is opinion-based. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 8 years ago.







                        Improve this question
                    



I need to extract contents from a website, but the application doesn鈥檛 provide any application programming interface or another mechanism to access that data programmatically.
I found a useful third-party tool called Import.io that provides click and go functionality for scraping web pages and building data sets, the only thing is I want to keep my data locally and I don't want to subscribe to any subscription plans.
What kind of technique does this company use for scraping the web pages and building their datasets? I found some web scraping frameworks pjscrape & Scrapy could they provide such a feature
",167k,"
            114
        ","['\nYou will definitely want to start with a good web scraping framework. Later on you may decide that they are too limiting and you can put together your own stack of libraries but without a lot of scraping experience your design will be much worse than pjscrape or scrapy.\nNote: I use the terms crawling and scraping basically interchangeable here. This is a copy of my answer to your Quora question, it\'s pretty long.\nTools\nGet very familiar with either Firebug or Chrome dev tools depending on your preferred browser. This will be absolutely necessary as you browse the site you are pulling data from and map out which urls contain the data you are looking for and what data formats make up the responses.\nYou will need a good working knowledge of HTTP as well as HTML and will probably want to find a decent piece of man in the middle proxy software. You will need to be able to inspect HTTP requests and responses and understand how the cookies and session information and query parameters are being passed around. Fiddler (http://www.telerik.com/fiddler) and Charles Proxy (http://www.charlesproxy.com/) are popular tools. I use mitmproxy (http://mitmproxy.org/) a lot as I\'m more of a keyboard guy than a mouse guy.\nSome kind of console/shell/REPL type environment where you can try out various pieces of code with instant feedback will be invaluable. Reverse engineering tasks like this are a lot of trial and error so you will want a workflow that makes this easy.\nLanguage\nPHP is basically out, it\'s not well suited for this task and the library/framework support is poor in this area. Python (Scrapy is a great starting point) and Clojure/Clojurescript (incredibly powerful and productive but a big learning curve) are great languages for this problem. Since you would rather not learn a new language and you already know Javascript I would definitely suggest sticking with JS. I have not used pjscrape but it looks quite good from a quick read of their docs. It\'s well suited and implements an excellent solution to the problem I describe below.\nA note on Regular expressions:\nDO NOT USE REGULAR EXPRESSIONS TO PARSE HTML.\nA lot of beginners do this because they are already familiar with regexes. It\'s a huge mistake, use xpath or css selectors to navigate html and only use regular expressions to extract data from actual text inside an html node. This might already be obvious to you, it becomes obvious quickly if you try it but a lot of people waste a lot of time going down this road for some reason. Don\'t be scared of xpath or css selectors, they are WAY easier to learn than regexes and they were designed to solve this exact problem.\nJavascript-heavy sites\nIn the old days you just had to make an http request and parse the HTML reponse. Now you will almost certainly have to deal with sites that are a mix of standard HTML HTTP request/responses and asynchronous HTTP calls made by the javascript portion of the target site. This is where your proxy software and the network tab of firebug/devtools comes in very handy. The responses to these might be html or they might be json, in rare cases they will be xml or something else.\nThere are two approaches to this problem:\nThe low level approach:\nYou can figure out what ajax urls the site javascript is calling and what those responses look like and make those same requests yourself. So you might pull the html from http://example.com/foobar and extract one piece of data and then have to pull the json response from http://example.com/api/baz?foo=b... to get the other piece of data. You\'ll need to be aware of passing the correct cookies or session parameters. It\'s very rare, but occasionally some required parameters for an ajax call will be the result of some crazy calculation done in the site\'s javascript, reverse engineering this can be annoying.\nThe embedded browser approach:\nWhy do you need to work out what data is in html and what data comes in from an ajax call? Managing all that session and cookie data? You don\'t have to when you browse a site, the browser and the site javascript do that. That\'s the whole point.\nIf you just load the page into a headless browser engine like phantomjs it will load the page, run the javascript and tell you when all the ajax calls have completed. You can inject your own javascript if necessary to trigger the appropriate clicks or whatever is necessary to trigger the site javascript to load the appropriate data.\nYou now have two options, get it to spit out the finished html and parse it or inject some javascript into the page that does your parsing and data formatting and spits the data out (probably in json format). You can freely mix these two options as well.\nWhich approach is best?\nThat depends, you will need to be familiar and comfortable with the low level approach for sure. The embedded browser approach works for anything, it will be much easier to implement and will make some of the trickiest problems in scraping disappear. It\'s also quite a complex piece of machinery that you will need to understand. It\'s not just HTTP requests and responses, it\'s requests, embedded browser rendering, site javascript, injected javascript, your own code and 2-way interaction with the embedded browser process.\nThe embedded browser is also much slower at scale because of the rendering overhead but that will almost certainly not matter unless you are scraping a lot of different domains. Your need to rate limit your requests will make the rendering time completely negligible in the case of a single domain.\nRate Limiting/Bot behaviour\nYou need to be very aware of this. You need to make requests to your target domains at a reasonable rate. You need to write a well behaved bot when crawling websites, and that means respecting robots.txt and not hammering the server with requests. Mistakes or negligence here is very unethical since this can be considered a denial of service attack. The acceptable rate varies depending on who you ask, 1req/s is the max that the Google crawler runs at but you are not Google and you probably aren\'t as welcome as Google. Keep it as slow as reasonable. I would suggest 2-5 seconds between each page request.\nIdentify your requests with a user agent string that identifies your bot and have a webpage for your bot explaining it\'s purpose. This url goes in the agent string.\nYou will be easy to block if the site wants to block you. A smart engineer on their end can easily identify bots and a few minutes of work on their end can cause weeks of work changing your scraping code on your end or just make it impossible. If the relationship is antagonistic then a smart engineer at the target site can completely stymie a genius engineer writing a crawler. Scraping code is inherently fragile and this is easily exploited. Something that would provoke this response is almost certainly unethical anyway, so write a well behaved bot and don\'t worry about this.\nTesting\nNot a unit/integration test person? Too bad. You will now have to become one. Sites change frequently and you will be changing your code frequently. This is a large part of the challenge.\nThere are a lot of moving parts involved in scraping a modern website, good test practices will help a lot. Many of the bugs you will encounter while writing this type of code will be the type that just return corrupted data silently. Without good tests to check for regressions you will find out that you\'ve been saving useless corrupted data to your database for a while without noticing. This project will make you very familiar with data validation (find some good libraries to use) and testing. There are not many other problems that combine requiring comprehensive tests and being very difficult to test.\nThe second part of your tests involve caching and change detection. While writing your code you don\'t want to be hammering the server for the same page over and over again for no reason. While running your unit tests you want to know if your tests are failing because you broke your code or because the website has been redesigned. Run your unit tests against a cached copy of the urls involved. A caching proxy is very useful here but tricky to configure and use properly.\nYou also do want to know if the site has changed. If they redesigned the site and your crawler is broken your unit tests will still pass because they are running against a cached copy! You will need either another, smaller set of integration tests that are run infrequently against the live site or good logging and error detection in your crawling code that logs the exact issues, alerts you to the problem and stops crawling. Now you can update your cache, run your unit tests and see what you need to change.\nLegal Issues\nThe law here can be slightly dangerous if you do stupid things. If the law gets involved you are dealing with people who regularly refer to wget and curl as ""hacking tools"". You don\'t want this.\nThe ethical reality of the situation is that there is no difference between using browser software to request a url and look at some data and using your own software to request a url and look at some data. Google is the largest scraping company in the world and they are loved for it. Identifying your bots name in the user agent and being open about the goals and intentions of your web crawler will help here as the law understands what Google is. If you are doing anything shady, like creating fake user accounts or accessing areas of the site that you shouldn\'t (either ""blocked"" by robots.txt or because of some kind of authorization exploit) then be aware that you are doing something unethical and the law\'s ignorance of technology will be extraordinarily dangerous here. It\'s a ridiculous situation but it\'s a real one.\nIt\'s literally possible to try and build a new search engine on the up and up as an upstanding citizen, make a mistake or have a bug in your software and be seen as a hacker. Not something you want considering the current political reality.\nWho am I to write this giant wall of text anyway?\nI\'ve written a lot of web crawling related code in my life. I\'ve been doing web related software development for more than a decade as a consultant, employee and startup founder. The early days were writing perl crawlers/scrapers and php websites. When we were embedding hidden iframes loading csv data into webpages to do ajax before Jesse James Garrett named it ajax, before XMLHTTPRequest was an idea. Before jQuery, before json. I\'m in my mid-30\'s, that\'s apparently considered ancient for this business.\nI\'ve written large scale crawling/scraping systems twice, once for a large team at a media company (in Perl) and recently for a small team as the CTO of a search engine startup (in Python/Javascript). I currently work as a consultant, mostly coding in Clojure/Clojurescript (a wonderful expert language in general and has libraries that make crawler/scraper problems a delight)\nI\'ve written successful anti-crawling software systems as well. It\'s remarkably easy to write nigh-unscrapable sites if you want to or to identify and sabotage bots you don\'t like.\nI like writing crawlers, scrapers and parsers more than any other type of software. It\'s challenging, fun and can be used to create amazing things.\n', '\nYes you can do it yourself. It is just a matter of grabbing the sources of the page and parsing them the way you want. \nThere are various possibilities. A good combo is using python-requests (built on top of urllib2, it is urllib.request in Python3) and BeautifulSoup4, which has its methods to select elements and also permits CSS selectors:\nimport requests\nfrom BeautifulSoup4 import BeautifulSoup as bs\nrequest = requests.get(""http://foo.bar"")\nsoup = bs(request.text) \nsome_elements = soup.find_all(""div"", class_=""myCssClass"")\n\nSome will prefer xpath parsing or jquery-like pyquery, lxml or something else.\nWhen the data you want is produced by some JavaScript, the above won\'t work. You either need python-ghost or Selenium. I prefer the latter combined with PhantomJS, much lighter and simpler to install, and easy to use:\nfrom selenium import webdriver\nclient = webdriver.PhantomJS()\nclient.get(""http://foo"")\nsoup = bs(client.page_source)\n\nI would advice to start your own solution. You\'ll understand Scrapy\'s benefits doing so.\nps: take a look at scrapely: https://github.com/scrapy/scrapely\npps: take a look at Portia, to start extracting information visually, without programming knowledge: https://github.com/scrapinghub/portia \n']",https://stackoverflow.com/questions/22168883/whats-the-best-way-of-scraping-data-from-a-website,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Headless, scriptable Firefox/Webkit on linux? [closed]","






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I'm looking to automate some web interactions, namely periodic download of files from a secure website. This basically involves entering my username/password and navigating to the appropriate URL.
I tried simple scripting in Python, followed by more sophisticated scripting, only to discover this particular website is using some obnoxious javascript and flash based mechanism for login, rendering my methods useless. 
I then tried HTMLUnit, but that doesn't seem to want to work either. I suspect use of Flash is the issue.
I don't really want to think about it any more, so I'm leaning towards scripting an actual browser to log in and grab the file I need. 
Requirements are:

Run on linux server (ie. no X running). If I really need to have X I can make that happen, but I won't be happy.
Be reliable. I want to start this thing and never think about it again.
Be scriptable. Nothing too sophisticated, but I should be able to tell the browser the various steps to take and pages to visit.

Are there any good toolkits for a headless, X-less scriptable browser? Have you tried something like this and if so do you have any words of wisdom?
",22k,"
            46
        ","['\nWhat about phantomjs?  \n', '\nI did related task with IE embedded browser (although it was gui application with hidden browser component panel). Actually you can take any layout engine and cut output logic. Navigation is should be done via firing script-like events.\nYou can use Crowbar. It is headless version of firefox (Gecko engine). It turns browser into RESTful server that can accept requests (""fetch  url""). So it parse html, represent it as DOM, wait defined delay for all script performed. \nIt works on linux. I suppose you can easily extend it for your goal using JS and rich XULrunner abilities.\n', '\nHave you tried Selenium? It will allow you to record a usage scenario, using an extension for Firefox, which can later be played back using a number of different methods.\nEdit: I just realized this was a very late response. :)\n', '\nHave a look at WebKitDriver. The project includes headless implementation of WebKit.\n', ""\nI don't know how to do flash interactions (and am also interested), but for html/javascript you can use Chickenfoot. \nAnd to get a headless + scriptable browser working on Linux you can use the Qt webkit library. Here is an example use.\n"", ""\nTo accomplish this, I just write Chrome extensions that post to CouchDBs (example and its Futon). Add the Couch to the permissions in the manifest to allow cross-domain XHRs.\n(I arrived at this thread in search of a headless alternative to what I've been doing; having found this thread, I'm going to try Crowbar at some point.)\nAlso, considering the bizarre characteristics of this website, I can't help wondering whether you can exploit some security hole to get around the Flash and Javascript.\n""]",https://stackoverflow.com/questions/2073481/headless-scriptable-firefox-webkit-on-linux,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrapy Python Set up User Agent,"
I tried to override the user-agent of my crawlspider by adding an extra line to the project configuration file. Here is the code:
[settings]
default = myproject.settings
USER_AGENT = ""Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36""


[deploy]
#url = http://localhost:6800/
project = myproject

But when I run the crawler against my own web, I notice the spider did not pick up my customized user agent but the default one ""Scrapy/0.18.2 (+http://scrapy.org)"". 
Can any one explain what I have done wrong. 
Note:
(1). It works when I tried to override the user agent globally: 
scrapy crawl myproject.com -o output.csv -t csv -s USER_AGENT=""Mozilla....""

(2). When I remove the line ""default = myproject.setting"" from the configuration file, and run scrapy crawl myproject.com, it says ""cannot find spider.."", so I feel like the default setting should not be removed in this case.
Thanks a lot for the help in advance.                            
",53k,"
            41
        ","['\nMove your USER_AGENT line to the settings.py file, and not in your scrapy.cfg file. settings.py should be at same level as items.py if you use scrapy startproject command, in your case  it should be something like myproject/settings.py\n', ""\nJust in case anyone lands here that manually controls the scrapy crawl. i.e. you do not use the scrapy crawl process from the shell...\n$ scrapy crawl myproject\n\nBut insted you use CrawlerProcess() or CrawlerRunner()...\nprocess = CrawlerProcess()\n\nor \nprocess = CrawlerRunner()\n\nthen the user agent, along with other settings, can be passed to the crawler in a dictionary of configuration variables. \nLike this...\n    process = CrawlerProcess(\n            {\n                'USER_AGENT': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n            }\n    )\n\n""]",https://stackoverflow.com/questions/18920930/scrapy-python-set-up-user-agent,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to fetch HTML in Java,"
Without the use of any external library, what is the simplest way to fetch a website's HTML content into a String?
",72k,"
            33
        ","['\nI\'m currently using this:\nString content = null;\nURLConnection connection = null;\ntry {\n  connection =  new URL(""http://www.google.com"").openConnection();\n  Scanner scanner = new Scanner(connection.getInputStream());\n  scanner.useDelimiter(""\\\\Z"");\n  content = scanner.next();\n  scanner.close();\n}catch ( Exception ex ) {\n    ex.printStackTrace();\n}\nSystem.out.println(content);\n\nBut not sure if there\'s a better way.\n', '\nThis has worked well for me:\nURL url = new URL(theURL);\nInputStream is = url.openStream();\nint ptr = 0;\nStringBuffer buffer = new StringBuffer();\nwhile ((ptr = is.read()) != -1) {\n    buffer.append((char)ptr);\n}\n\nNot sure at to whether the other solution(s) provided are any more efficient or not.\n', ""\nI just left this post in your other thread, though what you have above might work as well.  I don't think either would be any easier than the other.  The Apache packages can be accessed by just using import org.apache.commons.HttpClient at the top of your code.\nEdit: Forgot the link ;)\n"", '\nWhilst not vanilla-Java, I\'ll offer up a simpler solution. Use Groovy ;-)\nString siteContent = new URL(""http://www.google.com"").text\n\n', '\nIts not library but a tool named curl generally installed in most of the servers or you can easily install in ubuntu by \nsudo apt install curl\n\nThen fetch any html page and store it to your local file like an example \ncurl https://www.facebook.com/ > fb.html\n\nYou will get the home page html.You can run it in your browser as well.\n']",https://stackoverflow.com/questions/31462/how-to-fetch-html-in-java,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping Real Time Visitors from Google Analytics,"
I have a lot of sites and want to build a dashboard showing the number of real time visitors on each of them on a single page. (would anyone else want this?) Right now the only way to view this information is to open a new tab for each site.
Google doesn't have a real-time API, so I'm wondering if it is possible to scrape this data. Eduardo Cereto found out that Google transfers the real-time data over the realtime/bind network request. Anyone more savvy have an idea of how I should start? Here's what I'm thinking:

Figure out how to authenticate programmatically

Inspect all of the realtime/bind requests to see how they change. Does each request have a unique key? Where does that come from? Below is my breakdown of the request:
https://www.google.com/analytics/realtime/bind?VER=8
&key= [What is this? Where does it come from? 21 character lowercase alphanumeric, stays the same each request]
&ds= [What is this? Where does it come from? 21 character lowercase alphanumeric, stays the same each request]
&pageId=rt-standard%2Frt-overview
&q=t%3A0%7C%3A1%3A0%3A%2Ct%3A11%7C%3A1%3A5%3A%2Cot%3A0%3A0%3A4%2Cot%3A0%3A0%3A3%2Ct%3A7%7C%3A1%3A10%3A6%3D%3DREFERRAL%3B%2Ct%3A10%7C%3A1%3A10%3A%2Ct%3A18%7C%3A1%3A10%3A%2Ct%3A4%7C5%7C2%7C%3A1%3A10%3A2!%3Dzz%3B%2C&f
The q variable URI decodes to this (what the?):
t:0|:1:0:,t:11|:1:5:,ot:0:0:4,ot:0:0:3,t:7|:1:10:6==REFERRAL;,t:10|:1:10:,t:18|:1:10:,t:4|5|2|:1:10:2!=zz;,&f
&RID=rpc
&SID= [What is this? Where does it come from? 16 character uppercase alphanumeric, stays the same each request]
&CI=0
&AID= [What is this? Where does it come from? integer, starts at 1, increments weirdly to 150 and then 298]
&TYPE=xmlhttp
&zx= [What is this? Where does it come from? 12 character lowercase alphanumeric, changes each request]
&t=1

Inspect all of the realtime/bind responses to see how they change. How does the data come in? It looks like some altered JSON. How many times do I need to connect to get the data? Where is the active visitors on site number in there? Here is a dump of sample data:


19
[[151,[""noop""]
]
]
388
[[152,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[49,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[0,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3,2,0],""name"":""Total""}]}}]]]
]
388
[[153,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[52,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[2,1,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3,2],""name"":""Total""}]}}]]]
]
388
[[154,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[53,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[0,3,1,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3],""name"":""Total""}]}}]]]
]

Let me know if you can help with any of the items above!

",18k,"
            27
        ","['\nTo get the same, Google has  launched new Real Time API. With this API you can easily retrieve real time online visitors as well as several Google Analytics with following dimensions and metrics. https://developers.google.com/analytics/devguides/reporting/realtime/dimsmets/\nThis is quite similar to Google Analytics API. To start development on this, \nhttps://developers.google.com/analytics/devguides/reporting/realtime/v3/devguide \n', '\nWith Google Chrome I can see the data on the Network Panel.\nThe request endpoint is https://www.google.com/analytics/realtime/bind\nSeems like the connection stays open for 2.5 minutes, and during this time it just keeps getting more and more data. \nAfter about 2.5 minutes the connection is closed and a new one is open.\nOn the Network panel you can only see the data for the connections that are terminated. So leave it open for 5 minutes or so and you can start to see the data.\nI hope that can give you a place to start.\n', '\nHaving google in the loop seems pretty redundant. Suggest you use a common element delivered on demand from the dashboard server and include this item by absolute URL on all pages to be monitored for a given site. The script outputting the item can read the IP of the browser asking and these can all be logged into a database and filtered for uniqueness giving a real time head count.\n<?php\n$user_ip = $_SERVER[""REMOTE_ADDR""];\n/// Some MySQL to insert $user_ip to the database table for website XXX  goes here\n\n\n$file = \'tracking_image.gif\';\n$type = \'image/gif\';\nheader(\'Content-Type:\'.$type);\nheader(\'Content-Length: \' . filesize($file));\nreadfile($file);\n?>\n\nAmmendum:\nA database can also add a timestamp to every row of data it stores. This can be used to further filter results and provide the number of visitors in the last hour or minute. \nClient side Javascript with AJAX for fine tuning or overkill\nThe onblur and onfocus javascript commands  can be used to tell if the the page is visible, pass the data back to the dashboard server via Ajax. http://www.thefutureoftheweb.com/demo/2007-05-16-detect-browser-window-focus/\nWhen a visitor closes a page this can also be detected by the javascript onunload function in the body tag and Ajax can be used to send data back to the server one last time before the browser finally closes the page.\nAs you may also wish to collect some information about the visitor like Google analytics does this page https://panopticlick.eff.org/ has a lot of javascript that can be examined and adapted.\n', '\nI needed/wanted realtime data for personal use so I reverse-engineered their system a little bit.\nInstead of binding to /bind I get data from /getData (no pun intended).\nAt /getData the minimum request is apparently: https://www.google.com/analytics/realtime/realtime/getData?pageId&key={{propertyID}}&q=t:0|:1\nHere\'s a short explanation of the possible query parameters and syntax, please remember that these are all guesses and I don\'t know all of them:\nQuery Syntax: pageId&key=propertyID&q=dataType:dimensions|:page|:limit:filters\nValues: \npageID: Required but seems to only be used for internal analytics.\n\npropertyID: a{{accountID}}w{{webPropertyID}}p{{profileID}}, as specified at the Documentation link below. You can also find this in the URL of all analytics pages in the UI.\n\n\ndataType:\n    t: Current data\n    ot: Overtime/Past\n    c: Unknown, returns only a ""count"" value\n\n\ndimensions (| separated or alone), most values are only applicable for t:\n    1:  Country\n    2:  City\n    3:  Location code?\n    4:  Latitude\n    5:  Longitude\n    6:  Traffic source type (Social, Referral, etc.)\n    7:  Source\n    8:  ?? Returns (not set)\n    9:  Another location code? longer.\n    10: Page URL\n    11: Visitor Type (new/returning)\n    12: ?? Returns (not set)\n    13: ?? Returns (not set)\n    14: Medium\n    15: ?? Returns ""1""\n\npage:\n    At first this seems to work for pagination but after further analysis it looks like it\'s also used to specify which of the 6 pages (Overview, Locations, Traffic Sources, Content, Events and Conversions) to return data for.\n\n    For some reason 0 returns an impossibly high metrictotal\n\nlimit: Result limit per page, maximum of 50\n\nfilters:\n    Syntax is as specified at the Documentation 2 link below except the OR is specified using | instead of a comma.6==CUSTOM;1==United%20States\n\n\nYou can also combine multiple queries in one request by comma separating them (i.e. q=t:1|2|:1|:10,t:6|:1|:10).\nFollowing the above ""documentation"", if you wanted to build a query that requests the page URL and city of the top 10 active visitors with a traffic source type of CUSTOM located in the US you would use this URL: https://www.google.com/analytics/realtime/realtime/getData?key={{propertyID}}&pageId&q=t:10|2|:1|:10:6==CUSTOM;1==United%20States\n\nDocumentation\nDocumentation 2\n\nI hope that my answer is readable and (although it\'s a little late) sufficiently answers your question and helps others in the future.\n']",https://stackoverflow.com/questions/11021554/scraping-real-time-visitors-from-google-analytics,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is there a simple way in R to extract only the text elements of an HTML page?,"
Is there a simple way in R to extract only the text elements of an HTML page?
I think this is known as 'screen scraping' but I have no experience of it, I just need a simple way of extracting the text you'd normally see in a browser when visiting a url.
",27k,"
            26
        ","['\nI had to do this once upon time myself. \nOne way of doing it is to make use of XPath expressions. You will need these packages installed from the repository at http://www.omegahat.org/\nlibrary(RCurl)\nlibrary(RTidyHTML)\nlibrary(XML)\n\nWe use RCurl to connect to the website of interest. It has lots of options which allow you to access websites that the default functions in base R would have difficulty with I think it\'s fair to say. It is an R-interface to the libcurl library.\nWe use RTidyHTML to clean up malformed HTML web pages so that they are easier to parse. It is an R-interface to the libtidy library.\nWe use XML to parse the HTML code with our XPath expressions. It is an R-interface to the libxml2 library.\nAnyways, here\'s what you do (minimal code, but options are available, see help pages of corresponding functions):\nu <- ""http://stackoverflow.com/questions/tagged?tagnames=r"" \ndoc.raw <- getURL(u)\ndoc <- tidyHTML(doc.raw)\nhtml <- htmlTreeParse(doc, useInternal = TRUE)\ntxt <- xpathApply(html, ""//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]"", xmlValue)\ncat(unlist(txt))\n\nThere may be some problems with this approach, but I can\'t remember what they are off the top of my head (I don\'t think my xpath expression works with all web pages, sometimes it might not filter out script code or it may plain just not work with some other pages at all, best to experiment!)\nP.S. Another way, which works almost perfectly I think at web scraping all text from html is the following (basically getting Internet Explorer to do the conversion for you):\nlibrary(RDCOMClient) \nu <- ""http://stackoverflow.com/questions/tagged?tagnames=r""\nie <- COMCreate(""InternetExplorer.Application"") \nie$Navigate(u)\ntxt <- list()\ntxt[[u]] <- ie[[""document""]][[""body""]][[""innerText""]] \nie$Quit() \nprint(txt) \n\nHOWEVER, I\'ve never liked doing this because not only is it slow, but if you vectorise it and apply a vector of URLs, if internet explorer crashes on a bad page, then R might hang or crash itself (I don\'t think ?try helps that much in this case). Also it\'s prone to allowing pop-ups. I don\'t know, it\'s been a while since I\'ve done this, but thought I should point this out.\n', ""\nThe best solution is package htm2txt.\nlibrary(htm2txt)\nurl <- 'https://en.wikipedia.org/wiki/Alan_Turing'\ntext <- gettxt(url)\n\nFor details, see https://CRAN.R-project.org/package=htm2txt.\n"", '\nWell it麓s not exactly a R way of doing it, but it麓s as simple as they come: outwit plugin for firefox. The basic version is for free and helps to extract tables and stuff. \nah and if you really wanna do it the hard way in R, this link is for you:\n', ""\nI've had good luck with the readHTMLTable() function of the XML package. It returns a list of all tables on the page.\nlibrary(XML)\nurl <- 'http://en.wikipedia.org/wiki/World_population'\nallTables <- readHTMLTable(url)\n\nThere can be many tables on each page.\nlength(allTables)\n# [1] 17\n\nSo just select the one you want.\ntbl <- allTables[[3]]\n\nThe biggest hassle can be installing the XML package. It's big, and it needs the libxml2 library (and, under Linux, it needs the xml2-config Debian package, too). The second biggest hassle is that HTML tables often contain junk you don't want, besides the data you do want.\n"", ""\nYou can also use the rvest package and first, select all html nodes/tags containing text (e.g. p, h1, h2, h3) and then extract the text from those:\nrequire(rvest)\nurl = 'https://en.wikipedia.org/wiki/Alan_Turing'\nsite = read_html(url)\ntext = html_text(html_nodes(site, 'p,h1,h2,h3')) # comma separate\n\n"", '\nHere is another approach that can be used :\nlibrary(pagedown)\nlibrary(pdftools)\nchrome_print(input = ""http://stackoverflow.com/questions/tagged?tagnames=r"", \n             output = ""C:/.../test.pdf"")\ntext <- pdf_text(""C:/.../test.pdf"")\n\nIt is also possible to use RSelenium :\nlibrary(RSelenium)\nshell(\'docker run -d -p 4445:4444 selenium/standalone-firefox\')\nremDr <- remoteDriver(remoteServerAddr = ""localhost"", port = 4445L, browserName = ""firefox"")\nremDr$open()\nremDr$navigate(""http://stackoverflow.com/questions/tagged?tagnames=r"")\nremDr$getPageSource()[[1]]\n\n']",https://stackoverflow.com/questions/3195522/is-there-a-simple-way-in-r-to-extract-only-the-text-elements-of-an-html-page,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Programmatic Python Browser with JavaScript,"
I want to screen-scrape a web-site that uses JavaScript. 
There is mechanize, the programmatic web browser for Python. However, it (understandably) doesn't interpret javascript. Is there any programmatic browser for Python which does? If not, is there any JavaScript implementation in Python that I could use to attempt to create one?
",18k,"
            14
        ","['\nYou might be better off using a tool like Selenium to automate the scraping using a web browser, so the JS executes and the page renders just like it would for a real user.\n', '\nThe PyV8 package nicely wraps Google\'s V8 Javascript engine for Python.  It\'s particularly nice because not only can you call from Python to Javascript code, but you can call back from Javascript to Python code.  This makes it quite straightforward to implement the usual browser-supplied objects (that is, everything in the Javascript global namespace: ""window"", ""document"", and so on), which you\'d need to do if you were going to make a Javascript-capable Python browser emulator thing, possibly by hooking this up with mechanize.\n', ""\nMy favorite is PyPhantomJS. It's written using Python and PyQt4. It's completely headless and you can control it completely from JavaScript.\nHowever, if you are looking to actually see the page, you can use QWebView from PyQt4 as well.\n"", '\nThere is also spynner "" a stateful programmatic web browser module for Python with Javascript/AJAX support based on the QtWebkit framework"" : http://code.google.com/p/spynner/\n', '\nYou could also try defining Chickenfoot page triggers on the pages in question, executing whatever operations you want on the page and saving the results of the operation to a local file, and calling Firefox from the command line inside your program, followed by reading the file.\n', '\ni recommend that you take a look at some of the options available to you at http://wiki.python.org/moin/WebBrowserProgramming - surprisingly this is coming up as a common question (i\'ve found three on stackoverflow today, by searching for the words ""python browser"" on google).  if you do the same you\'ll find the other answers i gave.\n', '\nyou may try zope browser\nhttp://pypi.python.org/pypi?:action=display&name=zope.testbrowser\n', ""\nPlaywright or pyppeteer are both reasonably good, and use headless Chromium to render pages and interpret JavaScript.\nI'd pick Playwright out of the two, simply because it's backed by a larger entity, and supports Chromium/Firefox/WebKit out of the box.\n""]",https://stackoverflow.com/questions/1916711/programmatic-python-browser-with-javascript,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run multiple scrapy spiders at once using scrapyd,"
I'm using scrapy for a project where I want to scrape a number of sites - possibly hundreds - and I have to write a specific spider for each site. I can schedule one spider in a project deployed to scrapyd using:
curl http://localhost:6800/schedule.json -d project=myproject -d spider=spider2

But how do I schedule all spiders in a project at once?
All help much appreciated!
",8k,"
            12
        ","['\nMy solution for running 200+ spiders at once has been to create a custom command for the project.  See http://doc.scrapy.org/en/latest/topics/commands.html#custom-project-commands for more information about implementing custom commands.\nYOURPROJECTNAME/commands/allcrawl.py :\nfrom scrapy.command import ScrapyCommand\nimport urllib\nimport urllib2\nfrom scrapy import log\n\nclass AllCrawlCommand(ScrapyCommand):\n\n    requires_project = True\n    default_settings = {\'LOG_ENABLED\': False}\n\n    def short_desc(self):\n        return ""Schedule a run for all available spiders""\n\n    def run(self, args, opts):\n        url = \'http://localhost:6800/schedule.json\'\n        for s in self.crawler.spiders.list():\n            values = {\'project\' : \'YOUR_PROJECT_NAME\', \'spider\' : s}\n            data = urllib.urlencode(values)\n            req = urllib2.Request(url, data)\n            response = urllib2.urlopen(req)\n            log.msg(response)\n\nMake sure to include the following in your settings.py\nCOMMANDS_MODULE = \'YOURPROJECTNAME.commands\'\n\nThen from the command line (in your project directory) you can simply type\nscrapy allcrawl\n\n', ""\nSorry, I know this is an old topic, but I've started learning scrapy recently and stumbled here, and I don't have enough rep yet to post a comment, so posting an answer.\nFrom the common scrapy practices you'll see that if you need to run multiple spiders at once, you'll have to start multiple scrapyd service instances and then distribute your Spider runs among those.\n""]",https://stackoverflow.com/questions/10801093/run-multiple-scrapy-spiders-at-once-using-scrapyd,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Scraping JavaScript using Selenium and Beautiful Soup,"
I'm trying to scrape a JavaScript enables page using BS and Selenium. 
I have the following code so far. It still doesn't somehow detect the JavaScript (and returns a null value). In this case I'm trying to scrape the Facebook comments in the bottom. (Inspect element shows the class as postText)
Thanks for the help!
from selenium import webdriver  
from selenium.common.exceptions import NoSuchElementException  
from selenium.webdriver.common.keys import Keys  
import BeautifulSoup

browser = webdriver.Firefox()  
browser.get('http://techcrunch.com/2012/05/15/facebook-lightbox/')  
html_source = browser.page_source  
browser.quit()

soup = BeautifulSoup.BeautifulSoup(html_source)  
comments = soup(""div"", {""class"":""postText""})  
print comments

",16k,"
            11
        ","['\nThere are some mistakes in your code that are fixed below. However, the class ""postText"" must exist elsewhere, since it is not defined in the original source code.\nMy revised version of your code was tested and is working on multiple websites.\nfrom selenium import webdriver  \nfrom selenium.common.exceptions import NoSuchElementException  \nfrom selenium.webdriver.common.keys import Keys  \nfrom bs4 import BeautifulSoup\n\nbrowser = webdriver.Firefox()  \nbrowser.get(\'http://techcrunch.com/2012/05/15/facebook-lightbox/\')  \nhtml_source = browser.page_source  \nbrowser.quit()\n\nsoup = BeautifulSoup(html_source,\'html.parser\')  \n#class ""postText"" is not defined in the source code\ncomments = soup.findAll(\'div\',{\'class\':\'postText\'})  \nprint comments\n\n']",https://stackoverflow.com/questions/14529849/python-scraping-javascript-using-selenium-and-beautiful-soup,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenGL/D3D: How do I get a screen grab of a game running full screen in Windows?,"
Suppose I have an OpenGL game running full screen (Left 4 Dead 2). I'd like to programmatically get a screen grab of it and then write it to a video file. 
I've tried GDI, D3D, and OpenGL methods (eg glReadPixels) and either receive a blank screen or flickering in the capture stream.
Any ideas?
For what it's worth, a canonical example of something similar to what I'm trying to achieve is Fraps.
",6k,"
            10
        ","['\nThere are a few approaches to this problem. Most of them are icky, and it totally depends on what kind of graphics API you want to target, and which functions the target application uses.\nMost DirectX, GDI+ and OpenGL applications are double or tripple-buffered, so they all call:\nvoid SwapBuffers(HDC hdc)\n\nat some point. They also generate WM_PAINT messages in their message queue whenever the window should be drawn. This gives you two options.\n\nYou can install a global hook or thread-local hook into the target process and capture WM_PAINT messages. This allows you to copy the contents from the device context just before the painting happens. The process can be found by enumerating all the processes on the system and look for a known window name, or a known module handle.\n\nYou can inject code into the target process\'s local copy of SwapBuffers. On Linux this would be easy to do via the LD_PRELOAD environmental variable, or by calling ld-linux.so.2 explicitly, but there is no equivalient on Windows. Luckily there is a framework from Microsoft Research which can do this for you called Detours. You can find this here: link.\n\n\nThe demoscene group Farbrausch made a demo-capturing tool named kkapture which makes use of the Detours library. Their tool targets applications that require no user input however, so they basically run the demos at a fixed framerate by hooking into all the possible time functions, like timeGetTime(), GetTickCount() and QueryPerformanceCounter(). It\'s totally rad. A presentation written by ryg (I think?) regarding kkapture\'s internals can be found here. I think that\'s of interest to you.\nFor more information about Windows hooks, see here and here.\nEDIT:\nThis idea intrigued me, so I used Detours to hook into OpenGL applications and mess with the graphics. Here is Quake 2 with green fog added:  \nSome more information about how Detours works, since I\'ve used it first hand now:\nDetours works on two levels. The actual hooking only works in the same process space as the target process. So Detours has a function for injecting a DLL into a process and force its DLLMain to run too, as well as functions that are supposed to be used in that DLL. When DLLMain is run, the DLL should call DetourAttach() to specify the functions to hook, as well as the ""detour"" function, which is the code you want to override with.\nSo it basically works like this:\n\nYou have a launcher application who\'s only task is to call DetourCreateProcessWithDll(). It works the same way as CreateProcessW, only with a few extra parameters. This injects a DLL into a process and calls its DllMain().\nYou implement a DLL that calls the Detour functions and sets up trampoline functions. That means calling DetourTransactionBegin(), DetourUpdateThread(), DetourAttach() followed by DetourTransactionEnd().\nUse the launcher to inject the DLL you implemented into a process.\n\nThere are some caveats though. When DllMain is run, libraries that are imported later with LoadLibrary() aren\'t visible yet. So you can\'t necessarily set up everything during the DLL attachment event. A workaround is to keep track of all the functions that are overridden so far, and try to initialize the others inside these functions that you can already call. This way you will discover new functions as soon as LoadLibrary have mapped them into the memory space of the process. I\'m not quite sure how well this would work for wglGetProcAddress though. (Perhaps someone else here has ideas regarding this?)\nSome LoadLibrary() calls seem to fail. I tested with Quake 2, and DirectSound and the waveOut API failed to initalize for some reason. I\'m still investigating this.\n', ""\nI found a sourceforge'd project called taksi:\nhttp://taksi.sourceforge.net/\nTaksi does not provide audio capture, though.\n"", ""\nI've written screen grabbers in the past (DirectX7-9 era).  I found good old DirectDraw worked remarkably well and would reliably grab bits of hardware-accelerated/video screen content which other methods (D3D, GDI, OpenGL) seemed to leave blank or scrambled.  It was very fast too.\n""]",https://stackoverflow.com/questions/3486729/opengl-d3d-how-do-i-get-a-screen-grab-of-a-game-running-full-screen-in-windows,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What's the best approach for parsing XML/'screen scraping' in iOS? UIWebview or NSXMLParser?,"
I am creating an iOS app that needs to get some data from a web page. My first though was to use NSXMLParser initWithContentsOfURL: and parse the HTML with the NSXMLParser delegate. However this approach seems like it could quickly become painful (if, for example, the HTML changed I would have to rewrite the parsing code which could be awkward). 
Seeing as I'm loading a web page I took take a look at UIWebView too. It looks like UIWebView may be the way to go. stringByEvaluatingJavaScriptFromString: seems like a very handy way to extract the data and would allow the javascript to be stored in a separate file that would be easy to edit if the HTML changed. However, using UIWebView seems a bit hacky (seeing as UIWebView is a UIView subclass it may block the main thread, and the docs say that the javascript has a limit of 10MB).
Does anyone have any advice regarding parsing XML/HTML before I get stuck in?
UPDATE:
I wrote a blog post about my solution:HTML parsing/screen scraping in iOS
",11k,"
            8
        ","['\nI\'ve done this a few times. The best approach I\'ve found is to use libxml2 which has a mode for HTML. Then you can use XPath to query the document. \nWorking with the libxml2 API is not the most enjoyable. So, I usually bring over the XPathQuery.h/.m files documented on this page:\nhttp://cocoawithlove.com/2008/10/using-libxml2-for-parsing-and-xpath.html\nThen I fetch the data using a NSConnection and query the data with something like this:\nNSArray *tdNodes = PerformHTMLXPathQuery(self.receivedData, @""//td[@class=\'col-name\']/a/span"");\n\nSummary:\n\nAdd libxml2 to your project, here are some quick instructions for XCode4: \nhttp://cmar.me/2011/04/20/adding-libxml2-to-an-xcode-4-project/\nGet the XPathQuery.h/.m\nUse an XPath statement to query the html document.\n\n', ""\nParsing HTML with an XML parser usually does not work anyway because many sites have incorrect HTML, which a web browser will deal with, but a strict XML parser like NSXMLParser will totally fail on.\nFor many scripting languages there are great scraping libraries that are more merciful. Like Python's Beautiful Soup module. Unfortunately I do not know of such modules for Objective-C.\nLoading stuff into a UIWebView might be the simplest way to go here. Note that you do not have to put the UIWebView on screen. You can create a separate UIWindow and add the UIWebView to it, so that you do full off-screen rendering. There was a WWDC2009 video about this I think. As you already mention, it will not be lightweight though.\nDepending on the data that you want and the complexity of the pages that you need to parse, you might also be able to parse it by using regular expressions or even a hand written parser. I have done this many times, and for simple data this works well.\n""]",https://stackoverflow.com/questions/3541615/whats-the-best-approach-for-parsing-xml-screen-scraping-in-ios-uiwebview-or,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Webbrowser behaviour issues,"
I am trying to automate Webbrowser with .NET C#. The issue is that the control or should I say IE browser behaves strange on different computers. For example, I am clickin on link and fillup a Ajax popup form on 1st computer like this, without any error:
private void btn_Start_Click(object sender, RoutedEventArgs e)
{
    webbrowserIE.Navigate(""http://www.test.com/"");
    webbrowserIE.DocumentCompleted += fillup_LoadCompleted; 
}

void fillup_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{
    System.Windows.Forms.HtmlElement ele = web_BrowserIE.Document.GetElementById(""login"");
    if (ele != null)
        ele.InvokeMember(""Click"");

    if (this.web_BrowserIE.ReadyState == System.Windows.Forms.WebBrowserReadyState.Complete)
    {
        web_BrowserIE.Document.GetElementById(""login"").SetAttribute(""value"", myUserName);
        web_BrowserIE.Document.GetElementById(""password"").SetAttribute(""value"", myPassword);

        foreach (System.Windows.Forms.HtmlElement el in web_BrowserIE.Document.GetElementsByTagName(""button""))
        {
            if (el.InnerText == ""Login"")
            {
                el.InvokeMember(""click"");
            }
        }

        web_BrowserIE.DocumentCompleted -= fillup_LoadCompleted;        
    }
}

However, the above code wont work on 2nd pc and the only way to click is like this:
private void btn_Start_Click(object sender, RoutedEventArgs e)
{
    webbrowserIE.DocumentCompleted += click_LoadCompleted;
    webbrowserIE.Navigate(""http://www.test.com/""); 
}

void click_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{
    if (this.webbrowserIE.ReadyState == System.Windows.Forms.WebBrowserReadyState.Complete)
    {
        System.Windows.Forms.HtmlElement ele = webbrowserIE.Document.GetElementById(""login"");
        if (ele != null)
            ele.InvokeMember(""Click"");

        webbrowserIE.DocumentCompleted -= click_LoadCompleted;
        webbrowserIE.DocumentCompleted += fillup_LoadCompleted;
    }
}

void click_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{

        webbrowserIE.Document.GetElementById(""login_login"").SetAttribute(""value"", myUserName);
        webbrowserIE.Document.GetElementById(""login_password"").SetAttribute(""value"", myPassword);

        //If you know the ID of the form you would like to submit:
        foreach (System.Windows.Forms.HtmlElement el in webbrowserIE.Document.GetElementsByTagName(""button""))
        {
            if (el.InnerText == ""Login"")
            {
                el.InvokeMember(""click"");
            }
        }

        webbrowserIE.DocumentCompleted -= click_LoadCompleted;      
}

So, in second solution I have to call two Load Completed Chains. Could someone advise on how should I can handle this issue? Also, a proposal for more robust approach would be very helpfull. Thank you in advance 
",3k,"
            1
        ","['\nI could recommend two things:\n\nDon\'t execute your code upon DocumentComplete event, rather do upon DOM window.onload event.\nTo make sure your web page behaves in WebBrowser control the same way as it would in full Internet Explorer browser, consider implementing Feature Control.\n\n[EDITED] There\'s one more suggestion, based on the structure of your code. Apparently, you perform a series of navigation/handle DocumentComplete actions. It might be more natural and easy to use async/await for this. Here\'s an example of doing this, with or without async/await. It illustrates how to handle onload, too:\nasync Task DoNavigationAsync()\n{\n    bool documentComplete = false;\n    TaskCompletionSource<bool> onloadTcs = null;\n\n    WebBrowserDocumentCompletedEventHandler handler = delegate \n    {\n        if (documentComplete)\n            return; // attach to onload only once per each Document\n        documentComplete = true;\n\n        // now subscribe to DOM onload event\n        this.wb.Document.Window.AttachEventHandler(""onload"", delegate\n        {\n            // each navigation has its own TaskCompletionSource\n            if (onloadTcs.Task.IsCompleted)\n                return; // this should not be happening\n\n            // signal the completion of the page loading\n            onloadTcs.SetResult(true);\n        });\n    };\n\n    // register DocumentCompleted handler\n    this.wb.DocumentCompleted += handler;\n\n    // Navigate to http://www.example.com?i=1\n    documentComplete = false;\n    onloadTcs = new TaskCompletionSource<bool>();\n    this.wb.Navigate(""http://www.example.com?i=1"");\n    await onloadTcs.Task;\n    // the document has been fully loaded, you can access DOM here\n    MessageBox.Show(this.wb.Document.Url.ToString());\n\n    // Navigate to http://example.com?i=2\n    // could do the click() simulation instead\n\n    documentComplete = false;\n    onloadTcs = new TaskCompletionSource<bool>(); // new task for new navigation\n    this.wb.Navigate(""http://example.com?i=2"");\n    await onloadTcs.Task;\n    // the document has been fully loaded, you can access DOM here\n    MessageBox.Show(this.wb.Document.Url.ToString());\n\n    // no more navigation, de-register DocumentCompleted handler\n    this.wb.DocumentCompleted -= handler;\n}\n\nHere\'s the same code without async/await pattern (for .NET 4.0):\nTask DoNavigationAsync()\n{\n    // save the correct continuation context for Task.ContinueWith\n    var continueContext = TaskScheduler.FromCurrentSynchronizationContext(); \n\n    bool documentComplete = false;\n    TaskCompletionSource<bool> onloadTcs = null;\n\n    WebBrowserDocumentCompletedEventHandler handler = delegate \n    {\n        if (documentComplete)\n            return; // attach to onload only once per each Document\n        documentComplete = true;\n\n        // now subscribe to DOM onload event\n        this.wb.Document.Window.AttachEventHandler(""onload"", delegate\n        {\n            // each navigation has its own TaskCompletionSource\n            if (onloadTcs.Task.IsCompleted)\n                return; // this should not be happening\n\n            // signal the completion of the page loading\n            onloadTcs.SetResult(true);\n        });\n    };\n\n    // register DocumentCompleted handler\n    this.wb.DocumentCompleted += handler;\n\n    // Navigate to http://www.example.com?i=1\n    documentComplete = false;\n    onloadTcs = new TaskCompletionSource<bool>();\n    this.wb.Navigate(""http://www.example.com?i=1"");\n\n    return onloadTcs.Task.ContinueWith(delegate \n    {\n        // the document has been fully loaded, you can access DOM here\n        MessageBox.Show(this.wb.Document.Url.ToString());\n\n        // Navigate to http://example.com?i=2\n        // could do the \'click()\' simulation instead\n\n        documentComplete = false;\n        onloadTcs = new TaskCompletionSource<bool>(); // new task for new navigation\n        this.wb.Navigate(""http://example.com?i=2"");\n\n        onloadTcs.Task.ContinueWith(delegate \n        {\n            // the document has been fully loaded, you can access DOM here\n            MessageBox.Show(this.wb.Document.Url.ToString());\n\n            // no more navigation, de-register DocumentCompleted handler\n            this.wb.DocumentCompleted -= handler;\n        }, continueContext);\n\n    }, continueContext);\n}\n\nNote, it both cases it is still a piece of asynchronous code which returns a Task object. Here\'s an example of how to handle the completion of such task:\nprivate void Form1_Load(object sender, EventArgs e)\n{\n    DoNavigationAsync().ContinueWith(_ => {\n        MessageBox.Show(""Navigation complete!"");\n    }, TaskScheduler.FromCurrentSynchronizationContext());\n}\n\nThe benefit of using TAP pattern here is that DoNavigationAsync is a self-contained, independent method. It can be reused and it doesn\'t interfere with the state of parent object (in this case, the main form).\n']",https://stackoverflow.com/questions/18572635/webbrowser-behaviour-issues,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PHP CSS Selector Library? [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



Is there a PHP class/library that would allow me to query an XHTML document with CSS selectors? I need to scrape some pages for data that is very easily accessible if I could somehow use CSS selectors (jQuery has spoiled me!). Any ideas?
",22k,"
            33
        ","[""\nAfter Googling further (initial results weren't very helpful), it seems there is actually a Zend Framework library for this, along with some others:\n\nDOM-Query\nphpQuery\npQuery\nQueryPath\nSimple HTML DOM Parser\nUltimate Web Scraper Toolkit\nZend-Dom\n\n"", '\nXPath is a fairly standard way to access XML (and XHTML) nodes, and provides much more precision than CSS.\n', '\nAnother one:\nhttp://querypath.org/\n', '\nA great one is a component of symfony 2, CssSelector\\Parser\xadIntroduction. It converts CSS selectors into XPath expressions. Take a look =)\nSource code\n', '\nFor jQuery users most interesting may be port of jQuery to PHP, which is phpQuery. Almost all sections of the library are ported. Additionally it contains WebBrowser plugin, which can be used for Web Scraping whole site\'s path/processes (eg accessing data available after logging in). It simply simulates web browser on the server (events and cookies too). Latest versions has experimental support for XML namespaces and CSS3 ""|"" selector.\n', ""\nI ended up using PHP Query Lite, it's very simple and has all I need.\n"", '\nFor document parsing I use DOM.  This can quite easily solve your problem if you know the tag name (in this example ""div""):\n $doc = new DOMDocument();\n $doc->loadHTML($html);\n\n $elements = $doc->getElementsByTagName(""div"");\n foreach ($elements as $e){\n  if ($e->getAttribute(""class"")!=""someclass"") continue;\n\n  //its a div.classname\n }\n\nNot sure if DOM lets you get all elements of a document at once... you might have to do a tree traversal.\n', ""\nI wrote mine, based on Mootools CSS selector engine http://selectors.svn.exyks.org/. it rely on simplexml extension ability (so, it's read-only)\n""]",https://stackoverflow.com/questions/260605/php-css-selector-library,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping websites with Javascript enabled?,"
I'm trying to scrape and submit information to websites that heavily rely on Javascript to do most of its actions. The website won't even work when i disable Javascript in my browser.
I've searched for some solutions on Google and SO and there was someone who suggested i should reverse engineer the Javascript, but i have no idea how to do that. 
So far i've been using Mechanize and it works on websites that don't require Javascript.
Is there any way to access websites that use Javascript by using urllib2 or something similar? 
I'm also willing to learn Javascript, if that's what it takes.
",25k,"
            18
        ","['\nI wrote a small tutorial on this subject, this might help:\nhttp://koaning.io.s3-website.eu-west-2.amazonaws.com/dynamic-scraping-with-python.html\nBasically what you do is you have the selenium library pretend that it is a firefox browser, the browser will wait until all javascript has loaded before it continues passing you the html string. Once you have this string, you can then parse it with beautifulsoup.\n', ""\nI've had exactly the same problem. It is not simple at all, but I finally found a great solution, using PyQt4.QtWebKit.\nYou will find the explanations on this webpage : http://blog.motane.lu/2009/07/07/downloading-a-pages-content-with-python-and-webkit/\nI've tested it, I currently use it, and that's great !\nIts great advantage is that it can run on a server, only using X, without a graphic environment.\n"", ""\nYou should look into using Ghost, a Python library that wraps the PyQt4 + WebKit hack.\nThis makes g the WebKit client:\nimport ghost\ng = ghost.Ghost()\n\nYou can grab a page with g.open(url) and then g.content will evaluate to the document in its current state.\nGhost has other cool features, like injecting JS and some form filling methods, and you can pass the resulting document to BeautifulSoup and so on: soup = bs4.BeautifulSoup(g.content).\nSo far, Ghost is the only thing I've found that makes this kind of thing easy in Python. The only limitation I've come across is that you can't easily create more than one instance of the client object, ghost.Ghost, but you could work around that.\n"", ""\nCheck out crowbar. I haven't had any experience with it, but I was curious about the answer to your question so I started googling around. I'd like to know if this works out for you.\nhttp://grep.codeconsult.ch/2007/02/24/crowbar-scrape-javascript-generated-pages-via-gecko-and-rest/\n"", ""\nMaybe you could use Selenium Webdriver, which has python bindings I believe. I think it's mainly used as a tool for testing websites, but I guess it should be usable for scraping too. \n"", '\nI would actually suggest using Selenium.  Its mainly designed for testing Web-Applications from a ""user perspective however it is basically a ""FireFox"" driver.  I\'ve actually used it for this purpose ... although I was scraping an dynamic AJAX webpage.  As long as the Javascript form has a recognizable ""Anchor Text"" that Selenium can ""click"" everything should sort itself out.\nHope that helps\n']",https://stackoverflow.com/questions/3362859/scraping-websites-with-javascript-enabled,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I read and parse the contents of a webpage in R,"
I'd like to read the contents of a URL (e.q., http://www.haaretz.com/) in R. I am wondering how I can do it
",33k,"
            16
        ","['\nNot really sure how you want to process that page, because it\'s really messy.  As we re-learned in this famous stackoverflow question, it\'s not a good idea to do regex on html, so you will definitely want to parse this with the XML package.  \nHere\'s an example to get you started:\nrequire(RCurl)\nrequire(XML)\nwebpage <- getURL(""http://www.haaretz.com/"")\nwebpage <- readLines(tc <- textConnection(webpage)); close(tc)\npagetree <- htmlTreeParse(webpage, error=function(...){}, useInternalNodes = TRUE)\n# parse the tree by tables\nx <- xpathSApply(pagetree, ""//*/table"", xmlValue)  \n# do some clean up with regular expressions\nx <- unlist(strsplit(x, ""\\n""))\nx <- gsub(""\\t"","""",x)\nx <- sub(""^[[:space:]]*(.*?)[[:space:]]*$"", ""\\\\1"", x, perl=TRUE)\nx <- x[!(x %in% c("""", ""|""))]\n\nThis results in a character vector of mostly just webpage text (along with some javascript):\n> head(x)\n[1] ""Subscribe to Print Edition""              ""Fri., December 04, 2009 Kislev 17, 5770"" ""Israel Time:脗\xa016:48脗\xa0(EST+7)""           \n[4] ""脗\xa0脗\xa0Make Haaretz your homepage""          ""/*check the search form*/""               ""function chkSearch()"" \n\n', '\nYour best bet may be the XML package -- see for example this previous question.\n', '\nI know you asked for R. But maybe python+beautifullsoup is the way forward here? Then do your analysis with R you have scraped the screen with beautifullsoup?\n']",https://stackoverflow.com/questions/1844829/how-can-i-read-and-parse-the-contents-of-a-webpage-in-r,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scrapy how to set referer url,"
I need to set the referer url, before scraping a site, the site uses refering url based Authentication, so it does not allow me to login if the referer is not valid.
Could someone tell how to do this in Scrapy?
",20k,"
            11
        ","[""\nIf you want to change the referer in your spider's request, you can change DEFAULT_REQUEST_HEADERS in the settings.py file:\nDEFAULT_REQUEST_HEADERS = {\n    'Referer': 'http://www.google.com' \n}\n\n"", '\nYou should do exactly as @warwaruk indicated, below is my example elaboration for a crawl spider:\nfrom scrapy.spiders import CrawlSpider\nfrom scrapy import Request\n\nclass MySpider(CrawlSpider):\n  name = ""myspider""\n  allowed_domains = [""example.com""]\n  start_urls = [\n      \'http://example.com/foo\'\n      \'http://example.com/bar\'\n      \'http://example.com/baz\'\n      ]\n  rules = [(...)]\n\n  def start_requests(self):\n    requests = []\n    for item in self.start_urls:\n      requests.append(Request(url=item, headers={\'Referer\':\'http://www.example.com/\'}))\n    return requests    \n\n  def parse_me(self, response):\n    (...)\n\nThis should generate following logs in your terminal:\n(...)\n[myspider] DEBUG: Crawled (200) <GET http://example.com/foo> (referer: http://www.example.com/)\n(...)\n[myspider] DEBUG: Crawled (200) <GET http://example.com/bar> (referer: http://www.example.com/)\n(...)\n[myspider] DEBUG: Crawled (200) <GET http://example.com/baz> (referer: http://www.example.com/)\n(...)\n\nWill work same with BaseSpider. In the end start_requests method is BaseSpider method, from which CrawlSpider inherits from.\nDocumentation explains more options to be set in Request apart from headers, such as: cookies , callback function, priority of the request etc.\n', ""\nJust set Referer url in the Request headers\n\nclass scrapy.http.Request(url[, method='GET', body, headers, ...\nheaders (dict) 鈥?the headers of this request. The dict values can be strings (for single valued headers) or lists (for multi-valued headers).\n\nExample:\nreturn Request(url=your_url,\n               headers={'Referer':'http://your_referer_url'})\n"", '\nOverride BaseSpider.start_requests and create there your custom Request passing it your referer header.\n']",https://stackoverflow.com/questions/13069854/scrapy-how-to-set-referer-url,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extracting table contents from html with python and BeautifulSoup,"
I want to extract certain information out of an html document. E.g. it contains a table 
(among other tables with other contents) like this:
    <table class=""details"">
            <tr>
                    <th>Advisory:</th>
                    <td>RHBA-2013:0947-1</td>
            </tr>
            <tr>    
                    <th>Type:</th>
                    <td>Bug Fix Advisory</td>
            </tr>
            <tr>
                    <th>Severity:</th>
                    <td>N/A</td>
            </tr>
            <tr>    
                    <th>Issued on:</th>
                    <td>2013-06-13</td>
            </tr>
            <tr>    
                    <th>Last updated on:</th>
                    <td>2013-06-13</td>
            </tr>

            <tr>
                    <th valign=""top"">Affected Products:</th>
                    <td><a href=""#Red Hat Enterprise Linux ELS (v. 4)"">Red Hat Enterprise Linux ELS (v. 4)</a></td>
            </tr>


    </table>

I want to extract Information like the date of ""Issued on:"". It looks like BeautifulSoup4
could do this easyly, but somehow I don't manage to get it right.
My code so far:
    from bs4 import BeautifulSoup
    soup=BeautifulSoup(unicodestring_containing_the_entire_htlm_doc)
    table_tag=soup.table
    if table_tag['class'] == ['details']:
            print table_tag.tr.th.get_text() + "" "" + table_tag.tr.td.get_text()
            a=table_tag.next_sibling
            print  unicode(a)
            print table_tag.contents

This gets me the contents of the first table row, and also a listing of the contents. 
But the next sibling thing is not working right, I guess I am just using it wrong.
Of course I could just parse the contents thingy, but it seems to me that beautiful soup
was designed to prevent us from doing exactly this (if I start parsing myself, I might as
well parse the whole doc ...). If someone could enlighten me on how to acomplish this, I 
would be gratefull. If there is a better way then BeautifulSoup, I would be interested to 
hear about it.
",41k,"
            11
        ","[""\n>>> from bs4 import BeautifulSoup\n>>> soup = BeautifulSoup(unicodestring_containing_the_entire_htlm_doc)\n>>> table = soup.find('table', {'class': 'details'})\n>>> th = table.find('th', text='Issued on:')\n>>> th\n<th>Issued on:</th>\n>>> td = th.findNext('td')\n>>> td\n<td>2013-06-13</td>\n>>> td.text\nu'2013-06-13'\n\n""]",https://stackoverflow.com/questions/17196018/extracting-table-contents-from-html-with-python-and-beautifulsoup,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping javascript-generated data using Python,"
I want to scrape some data of following url using Python.
http://www.hankyung.com/stockplus/main.php?module=stock&mode=stock_analysis_infomation&itemcode=078340
It's about a summary of company information. 
What I want to scrape is not shown on the first page. 
By clicking tab named ""鞛鞝滍憸"", you can access financial statement. And clicking tab named ""順勱笀頋愲響?, you can access ""Cash Flow"". 
I want to scrape the ""Cash Flow"" data. 
However, Cash flow data is generated by javascript across the url.
The following link is that url which is hidden, http://stock.kisline.com/compinfo/financial/main.action?vhead=N&vfoot=N&vstay=&omit=&vwidth=
Cash flow data is generated by submitting some option value and cookie to this url.
As you perceived, itemcode=078340 in the first link means stock code and there are as many as 1680 stocks that I want gather cash flow data. I want make it a loop structure.
Is there good way to scrape cash flow data?
I tried scrapy but scrapy is difficult to cope with my another scraping code already I'm using.
",11k,"
            7
        ","[""\nThere's also dryscape (a library written by me, so the recommendation is a bit biased, obviously :) which uses a fast Webkit-based in-memory browser to navigate around. It understands Javascript, too, but is a lot more lightweight than Selenium.\n"", '\nIf you need to scape the page content which is updated with AJAX and you are not in the control of this AJAX interface I would use Selenium browser automator for the task:\nhttp://code.google.com/p/selenium/\n\nSelenium has Python bindings\nIt launches a real browser instance so it can do and scrape 100% the same thing as you see with your own eyes\nGet HTML document content after AJAX updates thru Selenium API\nUse lxml + xpath / CSS selectors to parse out the relevant parts out of the document\n\n']",https://stackoverflow.com/questions/10052465/scraping-javascript-generated-data-using-python,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to replace words with span tag using jsoup?,"
Assume I have the following html:
<html>
<head>
</head>
<body>
    <div id=""wrapper"" >
         <div class=""s2"">I am going <a title=""some title"" href="""">by flying</a>
           <p>mr tt</p>
         </div> 
    </div>
</body>    
</html>

Any words in the text nodes that are equal to or greater than 4 characters for example the word 'going' is replaced with html content (not text) <span>going<span> in the original html without changing anything else.
If I try do something like element.html(replacement), the problem is if lets the current element is <div class=""s2""> it will also wipe off <a title=""some title"" 
",6k,"
            7
        ","['\nIn this case you must traverse your document as suggested by this answer. Here\'s a way of doing it using Jsoup APIs:\n\nNodeTraversor and NodeVisitor allow you to traverse the DOM\nNode.replaceWith(...) allows for replacing a node in the DOM\n\nHere\'s the code:\npublic class JsoupReplacer {\n\n  public static void main(String[] args) {\n    so6527876();\n  }\n\n  public static void so6527876() {\n    String html = \n    ""<html>"" +\n    ""<head>"" +\n    ""</head>"" +\n    ""<body>"" +\n    ""    <div id=\\""wrapper\\"" >"" +\n    ""         <div class=\\""s2\\"">I am going <a title=\\""some title\\"" href=\\""\\"">by flying</a>"" +\n    ""           <p>mr tt</p>"" +\n    ""         </div> "" +\n    ""    </div>"" +\n    ""</body>    "" +\n    ""</html>"";\n    Document doc = Jsoup.parse(html);\n\n    final List<TextNode> nodesToChange = new ArrayList<TextNode>();\n\n    NodeTraversor nd  = new NodeTraversor(new NodeVisitor() {\n\n      @Override\n      public void tail(Node node, int depth) {\n        if (node instanceof TextNode) {\n          TextNode textNode = (TextNode) node;\n          String text = textNode.getWholeText();\n          String[] words = text.trim().split("" "");\n          for (String word : words) {\n            if (word.length() > 4) {\n              nodesToChange.add(textNode);\n              break;\n            }\n          }\n        }\n      }\n\n      @Override\n      public void head(Node node, int depth) {        \n      }\n    });\n\n    nd.traverse(doc.body());\n\n    for (TextNode textNode : nodesToChange) {\n      Node newNode = buildElementForText(textNode);\n      textNode.replaceWith(newNode);\n    }\n\n    System.out.println(""result: "");\n    System.out.println();\n    System.out.println(doc);\n  }\n\n  private static Node buildElementForText(TextNode textNode) {\n    String text = textNode.getWholeText();\n    String[] words = text.trim().split("" "");\n    Set<String> longWords = new HashSet<String>();\n    for (String word : words) {\n      if (word.length() > 4) {\n        longWords.add(word);\n      } \n    }\n    String newText = text;\n    for (String longWord : longWords) {\n      newText = newText.replaceAll(longWord, \n          ""<span>"" + longWord + ""</span>"");\n    }\n    return new DataNode(newText, textNode.baseUri());\n  }\n\n}\n\n', '\nI think you need to traverse the tree.  The result of text() on an Element will be all of the Element\'s text including text within child elements. Hopefully something like the following code will be helpful to you:\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.StringTokenizer;\nimport org.apache.commons.io.FileUtils;\nimport org.jsoup.Jsoup;\nimport org.jsoup.nodes.Document;\nimport org.jsoup.nodes.Element;\nimport org.jsoup.nodes.Node;\nimport org.jsoup.nodes.TextNode;\n\npublic class ScreenScrape {\n\n    public static void main(String[] args) throws IOException {\n        String content = FileUtils.readFileToString(new File(""test.html""));\n        Document doc = Jsoup.parse(content);\n        Element body = doc.body();\n        //System.out.println(body.toString());\n\n        StringBuilder sb = new StringBuilder();\n        traverse(body, sb);\n\n        System.out.println(sb.toString());\n    }\n\n    private static void traverse(Node n, StringBuilder sb) {\n        if (n instanceof Element) {\n            sb.append(\'<\');\n            sb.append(n.nodeName());            \n            if (n.attributes().size() > 0) {\n                sb.append(n.attributes().toString());\n            }\n            sb.append(\'>\');\n        }\n        if (n instanceof TextNode) {\n            TextNode tn = (TextNode) n;\n            if (!tn.isBlank()) {\n                sb.append(spanifyText(tn.text()));\n            }\n        }\n        for (Node c : n.childNodes()) {\n            traverse(c, sb);\n        }\n        if (n instanceof Element) {\n            sb.append(""</"");\n            sb.append(n.nodeName());\n            sb.append(\'>\');\n        }        \n    }\n\n    private static String spanifyText(String text){\n        StringBuilder sb = new StringBuilder();\n        StringTokenizer st = new StringTokenizer(text);\n        String token;\n        while (st.hasMoreTokens()) {\n             token = st.nextToken();\n             if(token.length() > 3){\n                 sb.append(""<span>"");\n                 sb.append(token);\n                 sb.append(""</span>"");\n             } else {\n                 sb.append(token);\n             }             \n             sb.append(\' \');\n        }\n        return sb.substring(0, sb.length() - 1).toString();\n    }\n\n}\n\n\nUPDATE\nUsing Jonathan\'s new Jsoup List element.textNode() method and combining it with MarcoS\'s suggested NodeTraversor/NodeVisitor technique I came up with (although I am modifying the tree whilst traversing it - probably a bad idea):\nDocument doc = Jsoup.parse(content);\nElement body = doc.body();\nNodeTraversor nd = new NodeTraversor(new NodeVisitor() {\n\n    @Override\n    public void tail(Node node, int depth) {\n        if (node instanceof Element) {\n            boolean foundLongWord;\n            Element elem = (Element) node;\n            Element span;\n            String token;\n            StringTokenizer st;\n            ArrayList<Node> changedNodes;\n            Node currentNode;\n            for (TextNode tn : elem.textNodes()) {\n                foundLongWord = Boolean.FALSE;\n                changedNodes = new ArrayList<Node>();\n                st = new StringTokenizer(tn.text());\n                while (st.hasMoreTokens()) {\n                    token = st.nextToken();\n                    if (token.length() > 3) {\n                        foundLongWord = Boolean.TRUE;\n                        span = new Element(Tag.valueOf(""span""), elem.baseUri());\n                        span.appendText(token);\n                        changedNodes.add(span);\n                    } else {\n                        changedNodes.add(new TextNode(token + "" "", elem.baseUri()));\n                    }\n                }\n                if (foundLongWord) {\n                    currentNode = changedNodes.remove(0);\n                    tn.replaceWith(currentNode);\n                    for (Node n : changedNodes) {\n                        currentNode.after(n);\n                        currentNode = n;\n                    }\n                }\n            }\n        }\n    }\n\n    @Override\n    public void head(Node node, int depth) {\n    }\n});    \nnd.traverse(body);\nSystem.out.println(body.toString());\n\n', '\nI am replacing word hello with hello(span tag)\nDocument doc = Jsoup.parse(content);\n    Element test =  doc.body();\n    Elements elemenets = test.getAllElements();\n    for(int i =0 ;i <elemenets .size();i++){\n        String elementText = elemenets .get(i).text();\n        if(elementText.contains(""hello""))\n            elemenets .get(i).html(l.get(i).text().replaceAll(""hello"",""<span style=\\""color:blue\\"">hello</span>""));\n    }\n\n']",https://stackoverflow.com/questions/6527876/how-to-replace-words-with-span-tag-using-jsoup,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scrapy log handler,"
I seek your help in the following 2 questions - How do I set the handler for the different log levels like in python. Currently, I have 
STATS_ENABLED = True
STATS_DUMP = True 

LOG_FILE = 'crawl.log'

But the debug messages generated by Scrapy are also added into the log files. Those are very long and ideally, I would  like the DEBUG level messages to left on standard error and INFO messages to be dump to my LOG_FILE.
Secondly, in the docs, it says The logging service must be explicitly started through the scrapy.log.start() function. My question is, where do I run this scrapy.log.start()? Is it inside my spider?
",7k,"
            3
        ","['\n\nSecondly, in the docs, it says The logging service must be explicitly\n  started through the scrapy.log.start() function. My question is, where\n  do I run this scrapy.log.start()? Is it inside my spider?\n\nIf you run a spider using scrapy crawl my_spider -- the log is started automatically if STATS_ENABLED = True\nIf you start the crawler process manually, you can do scrapy.log.start() before starting the crawler process.\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.conf import settings\n\n\nsettings.overrides.update({}) # your settings\n\ncrawlerProcess = CrawlerProcess(settings)\ncrawlerProcess.install()\ncrawlerProcess.configure()\n\ncrawlerProcess.crawl(spider) # your spider here\n\nlog.start() # depends on LOG_ENABLED\n\nprint ""Starting crawler.""\ncrawlerProcess.start()\nprint ""Crawler stopped.""\n\nThe little knowledge I have about your first question:\nBecause you have to start the scrapy log manually, this allows you to use your own logger. \nI think you can copy module scrapy/scrapy/log.py in scrapy sources, modify it, import it instead of scrapy.log and run start() - scrapy will use your log. In it there is a line in function start() which says log.startLoggingWithObserver(sflo.emit, setStdout=logstdout). \nMake your own observer (http://docs.python.org/howto/logging-cookbook.html#logging-to-multiple-destinations) and use it there.\n', ""\n\nI would like the DEBUG level messages to left on standard error and INFO messages to be dump to my LOG_FILE.\n\nYou can set LOG_LEVEL = 'INFO' in settings.py, but it will completely disable DEBUG messages.\n"", '\nHmm, \nJust wanted to update that I am able to get the logging file handler to file by using\nfrom twisted.python import log\nimport logging\nlogging.basicConfig(level=logging.INFO, filemode=\'w\', filename=\'log.txt\'"""""")\nobserver = log.PythonLoggingObserver()\nobserver.start()\n\nhowever I am unable to get the log to display the spiders\' name like from twisted in standard error. I posted this question. \n', ""\nscrapy some-scrapy's-args -L 'INFO' -s LOG_FILE=log1.log\n\noutputs will be redirected to a logname file .\n""]",https://stackoverflow.com/questions/8320730/scrapy-log-handler,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate javascript on a local html file (without browser),"
This is part of a project I am working on for work.
I want to automate a Sharepoint site, specifically to pull data out of a database that I and my coworkers only have front-end access to.
I FINALLY managed to get mechanize (in python) to accomplish this using Python-NTLM, and by patching part of it's source code to fix a reoccurring error.
Now, I am at what I would hope is my final roadblock: Part of the form I need to submit seems to be output of a JavaScript function :| and lo and behold... Mechanize does not support javascript. I don't want to emulate the javascript functionality myself in python because I would ideally like a reusable solution...
So, does anyone know how I could evaluate the javascript on the local html I download from sharepoint? I just want to run the javascript somehow (to complete the loading of the page), but without a browser.
I have already looked into selenium, but it's pretty slow for the amount of work I need to get done... I am currently looking into PyV8 to try and evaluate the javascript myself... but surely there must be an app or library (or anything) that can do this??
",2k,"
            3
        ","['\nWell, in the end I came down to the following possible solutions:\n\nRun Chrome headless and collect the html output (thanks to koenp for the link!)\nRun PhantomJS, a headless browser with a javascript api\nRun HTMLUnit; same thing but for Java\nUse Ghost.py, a python-based headless browser (that I haven\'t seen suggested anyyyywhere for some reason!)\nWrite a DOM-based javascript interpreter based on Pyv8 (Google v8 javascript engine) and add this to my current ""half-solution"" with mechanize.\n\nFor now, I have decided to use either use Ghost.py or my own modification of the PySide/PyQT Webkit (how ghost works) to evaluate the javascript, as apparently they can run quite fast if you optimize them to not download images and disable the GUI.\nHopefully others will find this list useful!\n', ""\nWell you will need something that both understands the DOM and understand Javascript, so that comes down to a headless browser of some sort. Maybe you can take a look at the selenium webdriver, but I guess you already did that. I don't hink there is an easy way of doing this without running the stuff in an actually browser engine.\n""]",https://stackoverflow.com/questions/16375251/evaluate-javascript-on-a-local-html-file-without-browser,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CasperJS click event having AJAX call,"
I am trying to fetch data from a site by simulating events using CasperJS with phantomJS 1.7.0.
I am able to simulate normal click events and select events. But my code fails in following scenario:
When I click on button / anchor etc on remote page, the click on remote page  initiates an AJAX call / JS call(depending on how that page is implemented by programmer.). 
In case of JS call, my code works and I get changed data. But for clicks where is AJAX call is initiated, I do not get updated data.
For debugging, I tried to get the page source of the element container(before and after), but I see no change in code. 
I tried to set wait time from 10 sec to 1 ms range, but that to does not reflect any changes in behavior. 
Below is my piece of code for clicking. I am using an array of CSS Paths, which represents which element(s) to click.
/*Click on array of clickable elements using CSS Paths.*/
fn_click = function(){
casper.each(G_TAGS,function(casper, cssPath, count1) 
                    {
                            casper.then ( function() {
                            casper.click(cssPath);

                            this.echo('DEBUG AFTER CLICKING -START HTML ');
                            //this.echo(this.getHTML(""CONTAINER WHERE DETAILS CHANGE""));
                            this.echo('DEBUG AFTER CLICKING -START HTML');
                            casper.wait(5000, function() 
                                                    {   

                                                        casper.then(fn_getData);
                                                    } 
                                    );
                            });     
                    });
};

UPDATE:
I tried to use remote-debug option from phantomJS, to debug above script. 
It is not working. I am on windows. I will try to run remote debugging on Ubuntu as well. 
Please help me. I would appreciate any help on this. 
UPDATE:
Please have a look at following code as a sample. 
https://gist.github.com/4441570

Content before click and after click are same. 
I am clicking on sorting options provided under tag (votes / activity etc.). 
",5k,"
            2
        ","[""\nI had the same problem today. I found this post, which put me in the direction of jQuery.\nAfter some testing I found out that there was already a jQuery loaded on that webpage. (A pretty old version though)\nLoading another jQuery on top of that broke any js calls made, so also the link that does an Ajax call.\nTo solve this I found http://api.jquery.com/jQuery.noConflict/\nand I added the following to my code:\n    this.evaluate(function () { jq = $.noConflict(true) } ); \n\nAnything that was formerly assigned to $ will be restored that way. And the jQuery that you injected is now available under 'jq'.\nHope this helps you guys.\n""]",https://stackoverflow.com/questions/14098483/casperjs-click-event-having-ajax-call,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C# WebClient - View source question,"
I'm using  a C# WebClient to post login details to a page and read the all the results.
The page I am trying to load includes flash (which, in the browser, translates into HTML). I'm guessing it's flash to avoid being picked up by search engines???
The flash I am interested in is just text (not an image/video) etc and when I ""View Selection Source"" in firefox I do actually see the text, within HTML, that I want to see.
(Interestingly when I view the source for the whole page I do not see the text, within HTML, that I want to see. Could this be related?)
Currently after I have posted my login details, and loaded the HTML back, I see the page which does NOT show the flash HTML (as if I had viewed source for the whole page).
Thanks in advance,
Jim
PS: I should point out that the POST is actually working, my log in is successful.
",3k,"
            2
        ","['\nFiddler (or similar tool) is invaluable to track down screen-scraping problems like this.  Using a normal browser and with fiddler active, look at all the requests being made as you go through the login and navigation process to get to the data you want.  In between, you will likely see one or more things that your code is doing differently which the server is responding to and hence showing you different HTML than a real client.\nThe list of stuff below (think of it as ""scraping 101"") is what you want to look for.  Most of the stuff below is probably stuff you\'re already doing, but I included everything for completeness. \nIn order to scrape effectively, you may need to deal with one or more of the following:\n\ncookies and/or hidden fields. when you show up at any page on a site, you\'ll typically get a session cookie and/or hidden form field which (in a normal browser) would be propagated back to the server on all subsequent requests. You will likely also get a persistent cookie.  On many sites, if a requests shows up without a proper cookie (or form field for sites using ""cookieless sessions""), the site will redirect the user to a ""no cookies"" UI, a login page, or another undesirable location (from the scraper app\'s perspective).  always make sure you capture the cookies set on the initial request and faithfully send them back to the server on subsequent requests, except if one of those subsequent requests changes a cookie (in which case propagate that new cookie instead).\nauthentication tokens a special case of above is forms-authentication cookies or hidden fields. make sure you\'re capturing the login token (usually a cookie) and sending it back.\nPOST vs. GET this is obvious, but make sure you\'re using the same HTTP method that a real browser does. \nform fields (esp. hidden ones!) I\'m sure you\'re doing this already, but make sure to send all form fields that a real browser does, not just the visible fields. make sure fields are HTML-encoded properly.\nHTTP headers. you already checked this, but it may make sense to check again just to make sure the (non-cookie) headers are identical. I always start with the exact same headers and then start pulling out headers one by one, and only keep the ones that cause the request to fail or return bogus data. this approach simplifies your scraping code. \nredirects. These can either come from the server, or from client script (e.g. ""if user doesn\'t have flash plug-in loaded, redirect to a non-flash page"").  See WebRequest: How to find a postal code using a WebRequest against this ContentType=""application/xhtml+xml, text/xml, text/html; charset=utf-8""? for a crazy example of how redirection can trip up a screen-scraper.  Note that if you\'re using .NET for scraping, you\'ll need to use HttpWebRequest (not WebClient) for redirect-dependent scraping, because by default WebClient doesn\'t provide a way for your code to attach cookies and headers to the second (post-redirect) request.  See the thread above for more details.\nsub-requests (frames, ajax, flash, etc.) - often, page elements (not the main HTTP requests) will end up fetching the data you want to scrape.  you\'ll be able to figure this out by looking which HTTP response contains the text you want, and then working backwards until you find what on the page is actually making the request for that content. A few sites do really crazy things in sub-requests, like requesting compressed or encrypted text via ajax, and then using client-side script to decrypt it.  if this is the case, you\'ll need to do a bit more work like reverse-engineering what the client script is doing.\nordering - this one is obvious: make HTTP requests in the same order that a browser client does. that doesn\'t mean you need to make every request (e.g. images). Typically you only need to make the requests which return text/html content type, unless the data you want is not in the HTML and is in an ajax/flash/etc. request. \n\n', '\n\n(Interestingly when I view the source for the whole page I do not see the text, within HTML, that I want to see. Could this be related?)\n\nThis usually means that the discrepancy is caused by some DOM manipulations via javascript after the page has loaded.  Try turning off javascript and see what it looks like.\n']",https://stackoverflow.com/questions/1471062/c-sharp-webclient-view-source-question,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping YouTube links from a webpage,"
I've been trying to scrape YouTube links from a webpage, but nothing has worked.
This is a picture of what I've been trying to scrape.:

This is the code I tried most recently:
youtube_link = soup.find(""a"", class_=""ytp-title-link yt-uix-sessionlink"")

And this is the link to the website the YouTube link is in: https://www.electronic-festivals.com/event/i-am-hardstyle-germany
",2k,"
            1
        ","['\nMost of the youtube links are within an iframe and javascript also needs to run. Try using selenium. The following extracts any src or href containing youtube. I only enter the key iframe hosting the youtube clip. You could loop all iframes checking.\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.by import By\n\ndef addItems(links, final):\n    for link in links:\n        ref = link.get_attribute(\'src\') if link.get_attribute(\'src\') is not None else link.get_attribute(\'href\')\n        final.append(ref)\n    return final\n\nurl = ""https://www.electronic-festivals.com/event/i-am-hardstyle-germany"" \ndriver = webdriver.Chrome()\ndriver.get(url)\ndriver.switch_to.frame(driver.find_element_by_css_selector(\'.media-youtube-player\'))\nfinal = []\n\ntry:\n    links = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ""[href*=youtube] , [src*=youtube]"")))\n    addItems(links, final)\nexcept:\n    pass\nfinally:\n    driver.switch_to.default_content()\n\nlinks = driver.find_elements_by_css_selector(\'[href*=youtube] , [src*=youtube]\')\naddItems(links, final)\n\nfor link in set(final):\n    print(link)\n\ndriver.quit()\n\n', '\nyou better use tools instead, youtube frontend or code will change, so better use command link tools, or apis or https://onlinetool.app/ext/youtube_dl\n', '\nIf you mean by scraping downloading, try\npip install youtube-dl\n\nin your shell.\n']",https://stackoverflow.com/questions/54973419/scraping-youtube-links-from-a-webpage,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How does a site like kayak.com aggregate content? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



Greetings,
I've been toying with an idea for a new project and was wondering if anyone has any idea on how a service like Kayak.com is able to aggregate data from so many sources so quickly and accurately. More specifically, do you think Kayak.com is interacting with APIs or are they crawling/scraping airline and hotel websites in order to fulfill user requests? I know there isn't one right answer for this sort of thing but I'm curious to know what others think would be a good way to go about this. If it helps, pretend you are going to create kayak.com tomorrow ... where is your data coming from?
",79k,"
            84
        ","['\nI\'m working in travel industry as a software architect / project lead on the precisely kind of project you describe - in our region we work with suppliers directly, but for outgoing we connect to several aggregators.\nTo answer your question... some data you have, some you get in various ways, and some you have to torture and twist until it confesses.\nWhat\'s your angle?\nThe questions you have to ask are... Do you want to sell advertising like Kayak or do you take a cut like Expedia? Are you into search or into selling travel services? Do you target niche (for example, just air travel) or everything (accommodation, airlines, rent-a-car, additional services like transport/sightseeing/conferences etc)? Do you target region (US or part of US) or the world? How deep do you go - do you just show several sites on a single screen, or do you bundle different services together and package them dynamically?\nGetting the data\nIf you\'re going with Kayak business model, you technically don\'t need site\'s permission... but a lot of sites have affiliate programs with IFrames or other simple ways to direct the customer to their site. On the plus side, you don\'t have to deal with payments/complaints and travelers themselves. As for the cons... if you want to compare prices yourself and present the cheapest option to the user, you\'ll have to integrate on a deeper level, and that means APIs and web scraping.\nAs for web scraping... avoid it. It sucks. Really. Just don\'t do it. Trust me on this one. For example, some things like lowcosters you can\'t get without web scraping. Low cost airlines live from value added services. If the user doesn\'t see their website, they don\'t sell extra stuff, and they don\'t earn anything. Therefore, they don\'t have affiliates, they don\'t offer APIs, and they change their site layout almost constantly. However, there are companies which earn a living by web scraping lowcoster\'s sites and wrapping them into nice APIs. If you can afford them, you can give your users cost-comparison of low cost flights and that\'s huge.\nOn the other hand, there are ""normal"" carriers which offer APIs. It\'s not that big of a problem to get to airlines since they\'re all united under IATA; basically, you buy from IATA, and IATA distributes the money to carriers. However, you probably don\'t want to connect directly to carrier network. They have web services and SOAP these days, but believe me when I say that there are SOAP protocols which are just an insanely thin wrappers around a text prompt through which you can interact with a mainframe with an 80es-style protocol (think of a Unix prompt where you\'re billed per command; and it takes about 20 commands to do one search). That\'s why you probably want to connect to somebody a bit more down the food chain, with a better API.\nAirlines are thus on both extremes of Gaussian curve; on one side are individual suppliers, and on the other highly centralized systems where you implement one API and you\'re able to fly anywhere in the world. Accommodation and the rest of travel products are in between. There are several big players which aggregate hotels, and a ton of small suppliers with a lot of aggregators which cover only part of a spectrum. For example, you can rent a lighthouse and it\'s even not that expensive - but you won\'t be able to compare the prices of different lighthouses in one place.\nIf you\'re into Kayak business model, you\'ll probably end up scraping websites. If you\'re into integrating different providers, you\'ll often work with APIs, some of which are pretty good, and most of which are tolerable. I haven\'t worked with RSS but there\'s not a lot of difference between RSS and web scraping. There is also a fourth option not mentioned in Jeff\'s answer... the one where you get your data nightly, for example .CSV files through FTP and similar.\nLife sucks (mini-rant)\nAnd then there\'s complexity. The more value you want to add, the more complexity you\'ll have to handle. Can you search accommodations which allow pets? For a hostel which is located less than 5 km from the town center? Are you combining flights, and are you able to guarantee that the traveler will have enough time to get from one airport to another... can you sell the transport in advance? A famous cellist doesn\'t want to part from his precious 18th century cello; can you sell him another seat for the cello (yep, not making this one up)?\nWant to compare prices? Sure, the room is EUR 30 per night. But you can either get one double for 30 and one single for 20, or you can get one extra bed in a double and get 70% off for third person. But only if it\'s a child under 12 years of age; our extra beds are not for adults. And you don\'t get the price for extra bed in search results - only when you calculate the final price.\nAnd don\'t even get me started on dynamic packaging. Want to sell accommodation + rent-a-car? No problem; integrate with two different providers, and off you go... manually updating list of locations in the city (from rent-a-car provider) to match with hotels (from accommodation provider, who gives you only the city for each hotel). Of course, provided that you\'ve already matched the list of cities from the two, since there is no international standard for city codes.\nUnlike a lot of other industries which have many products, travel industry has many very complex products. Amazon has it easy; selling books and selling potatoes, it\'s the same thing; you can even ship them in the same box. They combine easily and aren\'t assembled from many parts. :)\nP.S. Linking to an interesting recent thread on Hacker News with some insider info regarding flights.\nP.P.S. Recently stumbled on a great albeit rather old blogpost on IATA\'s NDC protocol with overview of how travel industry is connected and a history lesson how this came to be.\n', '\nThey use a software package like ITA Software, which is one of the companies Google is in the process of picking up.\n', ""\nOnly 3 ways I know of to get data from websites.\nRSS Feeds - We use rss feeds a lot at my company to integrate existing site's data with our apps.  It's fast and most sites already have an RSS feed available.  The problem with this is not all sites implement the RSS standard properly so if you're pulling data from many RSS feeds across many sites then make sure you write your code so that you can add exceptions and filters easily. \nAPIs - These are nice if they are designed well and have all the information you need, however that's not always the case, plus if the sites are not using a standard api format then you'll have to support multiple API's.\nWeb Scraping - This method would be the most unreliable as well as the most expensive to maintain.  But if you're left with nothing else it can be done.  \n"", ""\nThis article says that Kayak was asked to stop scrapping a certain airlines page. That leads me to believe that they probably do scraping on sites that they don't have a relationship with (and a data feed that comes with that relationship).\n"", '\nTravelport offer a product called ""Universal API"" which connects to flights and hotels and car rental companies and copes with package deals and all the various complexities to do with taxes and exchange rates:\nhttps://developer.travelport.com/app/developer-network/resource-centre-uapi\nI\'ve just started using it and it seems fine so far.  The queries are a little slow, but then so is every query on every OTA (Online travel agent)\'s site.\n', ""\nThere's two good APIs I've found from flight comparison websites recently\nThere's one from Wego, and one from Skyscanner. Both seem to have a good range and breadth of data from a number of airlines and good documentation too.\nWego pays each time a user clicks from your app to a booking website and Skyscanner pay affiliates 50% of 'revenue' (I assume that means the commission they make from airlines)\n"", ""\nThis is an old post but I thought I'd just add. I'm a data architect who works for a company that feeds these travel sites with content. This company enters into contracts with many hotel brands, individual hotels and other content providers. We aggregate this information then pass it onto the different channels. They then aggregate again in to their system.\nThe Large GDS systems are also content providers.\nAggregation is done by many methods... matching algorithms(in-house) and keys. Being an aggregation service, we need to communicate on the client level.\nHope this helps! cheers!\n""]",https://stackoverflow.com/questions/4607141/how-does-a-site-like-kayak-com-aggregate-content,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Screen scraping: getting around ""HTTP Error 403: request disallowed by robots.txt""","
Is there a way to get around the following?
httperror_seek_wrapper: HTTP Error 403: request disallowed by robots.txt

Is the only way around this to contact the site-owner (barnesandnoble.com).. i'm building a site that would bring them more sales, not sure why they would deny access at a certain depth.
I'm using mechanize and BeautifulSoup on Python2.6.
hoping for a work-around
",49k,"
            54
        ","['\noh you need to ignore the robots.txt\nbr = mechanize.Browser()\nbr.set_handle_robots(False)\n\n', '\nYou can try lying about your user agent (e.g., by trying to make believe you\'re a human being and not a robot) if you want to get in possible legal trouble with Barnes & Noble.  Why not instead get in touch with their business development department and convince them to authorize you specifically?  They\'re no doubt just trying to avoid getting their site scraped by some classes of robots such as price comparison engines, and if you can convince them that you\'re not one, sign a contract, etc, they may well be willing to make an exception for you.\nA ""technical"" workaround that just breaks their policies as encoded in robots.txt is a high-legal-risk approach that I would never recommend.  BTW, how does their robots.txt read?\n', ""\nThe code to make a correct request:\nbr = mechanize.Browser()\nbr.set_handle_robots(False)\nbr.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\nresp = br.open(url)\nprint resp.info()  # headers\nprint resp.read()  # content\n\n"", ""\nMechanize automatically follows robots.txt, but it can be disabled assuming you have permission, or you have thought the ethics through ..\nSet a flag in your browser: \nbrowser.set_handle_equiv(False) \n\nThis ignores robots.txt.\nAlso, make sure you throttle your requests, so you don't put too much load on their site. (Note, this also makes it less likely that they will detect and ban you).\n"", ""\nThe error you're receiving is not related to the user agent.  mechanize by default checks robots.txt directives automatically when you use it to navigate to a site.  Use the .set_handle_robots(false) method of mechanize.browser to disable this behavior.\n"", ""\nSet your User-Agent header to match some real IE/FF User-Agent.\nHere's my IE8 useragent string:\nMozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; AskTB5.6)\n\n"", '\nWithout debating the ethics of this you could modify the headers to look like the googlebot for example, or is the googlebot blocked as well?\n', '\nAs it seems, you have to do less work to bypass robots.txt, at least says this article. So you might have to remove some code to ignore the filter.\n']",https://stackoverflow.com/questions/2846105/screen-scraping-getting-around-http-error-403-request-disallowed-by-robots-tx,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BeautifulSoup: How do I extract all the <li>s from a list of <ul>s that contains some nested <ul>s?,"
I'm a newbie programmer trying to jump in to Python by building a script that scrapes http://en.wikipedia.org/wiki/2000s_in_film and extracts a list of ""Movie Title (Year)"".
My HTML source looks like:
<h3>Header3 (Start here)</h3>
<ul>
    <li>List items</li>
    <li>Etc...</li>
</ul>
<h3>Header 3</h3>
<ul>
    <li>List items</li>
    <ul>
        <li>Nested list items</li>
        <li>Nested list items</li></ul>
    <li>List items</li>
</ul>
<h2>Header 2 (end here)</h2>

I'd like all the li tags following the first h3 tag and stopping at the next h2 tag, including all nested li tags.
firstH3 = soup.find('h3')

...correctly finds the place I'd like to start.
firstH3 = soup.find('h3') # Start here
uls = []
for nextSibling in firstH3.findNextSiblings():
    if nextSibling.name == 'h2':
        break
    if nextSibling.name == 'ul':
        uls.append(nextSibling)

...gives me a list uls, each with li contents that I need.
Excerpt of the uls list:
<ul>
...
    <li><i><a href=""/wiki/Agent_Cody_Banks"" title=""Agent Cody Banks"">Agent Cody Banks</a></i> (2003)</li>
    <li><i><a href=""/wiki/Agent_Cody_Banks_2:_Destination_London"" title=""Agent Cody Banks 2: Destination London"">Agent Cody Banks 2: Destination London</a></i> (2004)</li>
    <li>Air Bud series:
        <ul>
            <li><i><a href=""/wiki/Air_Bud:_World_Pup"" title=""Air Bud: World Pup"">Air Bud: World Pup</a></i> (2000)</li>
            <li><i><a href=""/wiki/Air_Bud:_Seventh_Inning_Fetch"" title=""Air Bud: Seventh Inning Fetch"">Air Bud: Seventh Inning Fetch</a></i> (2002)</li>
            <li><i><a href=""/wiki/Air_Bud:_Spikes_Back"" title=""Air Bud: Spikes Back"">Air Bud: Spikes Back</a></i> (2003)</li>
            <li><i><a href=""/wiki/Air_Buddies"" title=""Air Buddies"">Air Buddies</a></i> (2006)</li>
        </ul>
    </li>
    <li><i><a href=""/wiki/Akeelah_and_the_Bee"" title=""Akeelah and the Bee"">Akeelah and the Bee</a></i> (2006)</li>
...
</ul>

But I'm unsure of where to go from here.

Update:
Final Code:
lis = []
    for ul in uls:
        for li in ul.findAll('li'):
            if li.find('ul'):
                break
            lis.append(li)

    for li in lis:
        print li.text.encode(""utf-8"")

The if...break throws out the LI's that contain UL's since the nested LI's are now duplicated.
Print output is now:


102 Dalmatians(2000)
10th & Wolf(2006)
11:14(2006)
12:08 East of Bucharest(2006)
13 Going on 30(2004)
1408(2007)
...


",80k,"
            35
        ","[""\n.findAll() works for nested li elements:\nfor ul in uls:\n    for li in ul.findAll('li'):\n        print(li)\n\nOutput:\n<li>List items</li>\n<li>Etc...</li>\n<li>List items</li>\n<li>Nested list items</li>\n<li>Nested list items</li>\n<li>List items</li>\n\n"", ""\nA list comprehension could work, too.\nlis = [li for ul in uls for li in ul.findAll('li')]\n\n"", '\nimport requests\nfrom bs4 import BeautifulSoup\nr = requests.get(""https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_list_test"")\nsoup =   BeautifulSoup(r.content,""lxml"")\nw3schollsList = soup.find_all(\'body\')\nfor w3scholl in w3schollsList:\n    ulList = w3scholl.find_all(\'li\')\n    for li in ulList:\n        print(li)\n\nNote: here is to get the ""li"" inside the div we made\n']",https://stackoverflow.com/questions/4362981/beautifulsoup-how-do-i-extract-all-the-lis-from-a-list-of-uls-that-contains,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"XPath to Parse ""SRC"" from IMG tag?","
Right now I successfully grabbed the full  element from an HTML page with this:
//img[@class='photo-large']

for example it would return this: 
<img src=""http://example.com/img.jpg"" class='photo-large' />

But I only need the SRC url (http://example.com/img.jpg). Any help?
",69k,"
            29
        ","[""\nYou are so close to answering this yourself that I am somewhat reluctant to answer it for you. However, the following XPath should provide what you want (provided the source is XHTML, of course).\n//img[@class='photo-large']/@src\n\nFor further tips, check out W3 Schools. They have excellent tutorials on such things and a great reference too.\n"", '\nUsing Hpricot this works:\ndoc.at(\'//img[@class=""photo-large""]\')[\'src\']\n\nIn case you have more than one image, the following gives an array:\ndoc.search(\'//img[@class=""photo-large""]\').map do |e| e[\'src\'] end\n\nHowever, Nokogiri is many times faster and it 鈥渃an be used as a drop in replacement鈥?for Hpricot.\nHere the version for Nokogiri, in which this XPath for selecting attributes works:\ndoc.at(\'//img[@class=""photo-large""]/@src\').to_s\n\nor for many images:\ndoc.search(\'//img[@class=""photo-large""]/@src\').to_a\n\n', '\n//img/@src\nyou can just go with this if you want a link of the image.\nexample:\n<img alt="""" class=""avatar width-full rounded-2"" height=""230"" src=""https://avatars3.githubusercontent.com/...;s=460"" width=""230"">\n\n']",https://stackoverflow.com/questions/1179641/xpath-to-parse-src-from-img-tag,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Puppeteer waitForSelector on multiple selectors,"
I have Puppeteer controlling a website with a lookup form that can either return a result or a ""No records found"" message. How can I tell which was returned? 
waitForSelector seems to wait for only one at a time, while waitForNavigation doesn't seem to work because it is returned using Ajax.
I am using a try catch, but it is tricky to get right and slows everything way down.
try {
    await page.waitForSelector(SELECTOR1,{timeout:1000}); 
}
catch(err) { 
    await page.waitForSelector(SELECTOR2);
}

",25k,"
            23
        ","[""\nMaking any of the elements exists\nYou can use querySelectorAll and waitForFunction together to solve this problem. Using all selectors with comma will return all nodes that matches any of the selector.\nawait page.waitForFunction(() => \n  document.querySelectorAll('Selector1, Selector2, Selector3').length\n);\n\nNow this will only return true if there is some element, it won't return which selector matched which elements.\n"", ""\nhow about using Promise.race() like something I did in the below code snippet, and don't forget the { visible: true } option in page.waitForSelector() method.\npublic async enterUsername(username:string) : Promise<void> {\n    const un = await Promise.race([\n        this.page.waitForSelector(selector_1, { timeout: 4000, visible: true })\n        .catch(),\n        this.page.waitForSelector(selector_2, { timeout: 4000, visible: true })\n        .catch(),\n    ]);\n\n    await un.focus();\n    await un.type(username);\n}\n\n"", ""\nAn alternative and simple solution would be to approach this from a more CSS perspective.  waitForSelector seems to follow the CSS selector list rules. So essentially you can select multiple CSS elements by just using a comma.\ntry {    \n    await page.waitForSelector('.selector1, .selector2',{timeout:1000})\n} catch (error) {\n    // handle error\n}\n\n"", '\nUsing Md. Abu Taher\'s suggestion, I ended up with this:\n// One of these SELECTORs should appear, we don\'t know which\nawait page.waitForFunction((sel) => { \n    return document.querySelectorAll(sel).length;\n},{timeout:10000},SELECTOR1 + "", "" + SELECTOR2); \n\n// Now see which one appeared:\ntry {\n    await page.waitForSelector(SELECTOR1,{timeout:10});\n}\ncatch(err) {\n    //check for ""not found"" \n    let ErrMsg = await page.evaluate((sel) => {\n        let element = document.querySelector(sel);\n        return element? element.innerHTML: null;\n    },SELECTOR2);\n    if(ErrMsg){\n        //SELECTOR2 found\n    }else{\n        //Neither found, try adjusting timeouts until you never get this...\n    }\n};\n//SELECTOR1 found\n\n', ""\nI had a similar issue and went for this simple solution:\nhelpers.waitForAnySelector = (page, selectors) => new Promise((resolve, reject) => {\n  let hasFound = false\n  selectors.forEach(selector => {\n    page.waitFor(selector)\n      .then(() => {\n        if (!hasFound) {\n          hasFound = true\n          resolve(selector)\n        }\n      })\n      .catch((error) => {\n        // console.log('Error while looking up selector ' + selector, error.message)\n      })\n  })\n})\n\nAnd then to use it:\nconst selector = await helpers.waitForAnySelector(page, [\n  '#inputSmsCode', \n  '#buttonLogOut'\n])\n\nif (selector === '#inputSmsCode') {\n  // We need to enter the 2FA sms code. \n} else if (selector === '#buttonLogOut') {\n  // We successfully logged in\n}\n\n"", '\nIn puppeteer you can simply use multiple selectors separated by coma like this:\nconst foundElement = await page.waitForSelector(\'.class_1, .class_2\');\n\nThe returned element will be an elementHandle of the first element found in the page.\nNext if you want to know which element was found you can get the class name like so:\nconst className = await page.evaluate(el => el.className, foundElement);\n\nin your case a code similar to this should work:\nconst foundElement = await page.waitForSelector([SELECTOR1,SELECTOR2].join(\',\'));\nconst responseMsg = await page.evaluate(el => el.innerText, foundElement);\nif (responseMsg == ""No records found""){ // Your code here }\n\n', ""\nOne step further using Promise.race() by wrapping it and just check index for further logic:\n// Typescript\nexport async function racePromises(promises: Promise<any>[]): Promise<number> {\n  const indexedPromises: Array<Promise<number>> = promises.map((promise, index) => new Promise<number>((resolve) => promise.then(() => resolve(index))));\n  return Promise.race(indexedPromises);\n}\n\n// Javascript\nexport async function racePromises(promises) {\n  const indexedPromises = promises.map((promise, index) => new Promise((resolve) => promise.then(() => resolve(index))));\n  return Promise.race(indexedPromises);\n}\n\nUsage:\nconst navOutcome = await racePromises([\n  page.waitForSelector('SELECTOR1'),\n  page.waitForSelector('SELECTOR2')\n]);\nif (navigationOutcome === 0) {\n  //logic for 'SELECTOR1'\n} else if (navigationOutcome === 1) {\n  //logic for 'SELECTOR2'\n}\n\n\n\n"", ""\nCombining some elements from above into a helper method, I've built a command that allows me to create multiple possible selector outcomes and have the first to resolve be handled.\n\n\n/**\r\n * @typedef {import('puppeteer').ElementHandle} PuppeteerElementHandle\r\n * @typedef {import('puppeteer').Page} PuppeteerPage\r\n */\r\n\r\n/** Description of the function\r\n  @callback OutcomeHandler\r\n  @async\r\n  @param {PuppeteerElementHandle} element matched element\r\n  @returns {Promise<*>} can return anything, will be sent to handlePossibleOutcomes\r\n*/\r\n\r\n/**\r\n * @typedef {Object} PossibleOutcome\r\n * @property {string} selector The selector to trigger this outcome\r\n * @property {OutcomeHandler} handler handler will be called if selector is present\r\n */\r\n\r\n/**\r\n * Waits for a number of selectors (Outcomes) on a Puppeteer page, and calls the handler on first to appear,\r\n * Outcome Handlers should be ordered by preference, as if multiple are present, only the first occuring handler\r\n * will be called.\r\n * @param {PuppeteerPage} page Puppeteer page object\r\n * @param {[PossibleOutcome]} outcomes each possible selector, and the handler you'd like called.\r\n * @returns {Promise<*>} returns the result from outcome handler\r\n */\r\nasync function handlePossibleOutcomes(page, outcomes)\r\n{\r\n  var outcomeSelectors = outcomes.map(outcome => {\r\n    return outcome.selector;\r\n  }).join(', ');\r\n  return page.waitFor(outcomeSelectors)\r\n  .then(_ => {\r\n    let awaitables = [];\r\n    outcomes.forEach(outcome => {\r\n      let await = page.$(outcome.selector)\r\n      .then(element => {\r\n        if (element) {\r\n          return [outcome, element];\r\n        }\r\n        return null;\r\n      });\r\n      awaitables.push(await);\r\n    });\r\n    return Promise.all(awaitables);\r\n  })\r\n  .then(checked => {\r\n    let found = null;\r\n    checked.forEach(check => {\r\n      if(!check) return;\r\n      if(found) return;\r\n      let outcome = check[0];\r\n      let element = check[1];\r\n      let p = outcome.handler(element);\r\n      found = p;\r\n    });\r\n    return found;\r\n  });\r\n}\n\n\nTo use it, you just have to call and provide an array of Possible Outcomes and their selectors / handlers:\n await handlePossibleOutcomes(page, [\n    {\n      selector: '#headerNavUserButton',\n      handler: element => {\n        console.log('Logged in',element);\n        loggedIn = true;\n        return true;\n      }\n    },\n    {\n      selector: '#email-login-password_error',\n      handler: element => {\n        console.log('password error',element);\n        return false;\n      }\n    }\n  ]).then(result => {\n    if (result) {\n      console.log('Logged in!',result);\n    } else {\n      console.log('Failed :(');\n    }\n  })\n\n"", ""\nI just started with Puppeteer, and have encountered the same issue, therefore I wanted to make a custom function which fulfills the same use-case.\nThe function goes as follows:\nasync function waitForMySelectors(selectors, page){\n    for (let i = 0; i < selectors.length; i++) {\n        await page.waitForSelector(selectors[i]);\n    }\n}\n\nThe first parameter in the function recieves an array of selectors, the second parameter is the page that we're inside to preform the waiting process with.\ncalling the function as the example below:\nvar SelectorsArray = ['#username', '#password'];\nawait waitForMySelectors(SelectorsArray, page);\n\nthough I have not preformed any tests on it yet, it seems functional.\n"", '\nIf you want to wait for the first of multiple selectors and get the matched element(s), you can start with waitForFunction:\nconst matches = await page.waitForFunction(() => {\n  const matches = [...document.querySelectorAll(YOUR_SELECTOR)];\n  return matches.length ? matches : null;\n});\n\nwaitForFunction will return an ElementHandle but not an array of them. If you only need native DOM methods, it\'s not necessary to get handles. For example, to get text from this array:\nconst contents = await matches.evaluate(els => els.map(e => e.textContent));\n\nIn other words, matches acts a lot like the array passed to $$eval by Puppeteer.\nOn the other hand, if you do need an array of handles, the following demonstration code makes the conversion and shows the handles being used as normal:\nconst puppeteer = require(""puppeteer""); // ^16.2.0\n\nconst html = `\n<!DOCTYPE html>\n<html>\n<head>\n<style>\nh1 {\n  display: none;\n}\n</style>\n</head>\n<body>\n<script>\nsetTimeout(() => {\n\n  // add initial batch of 3 elements\n  for (let i = 0; i < 3; i++) {\n    const h1 = document.createElement(""button"");\n    h1.textContent = \\`first batch #\\${i + 1}\\`;\n    h1.addEventListener(""click"", () => {\n      h1.textContent = \\`#\\${i + 1} clicked\\`;\n    });\n    document.body.appendChild(h1);\n  }\n\n  // add another element 1 second later to show it won\'t appear in the first batch\n  setTimeout(() => {\n    const h1 = document.createElement(""h1"");\n    h1.textContent = ""this won\'t be found in the first batch"";\n    document.body.appendChild(h1);\n  }, 1000);\n\n}, 3000); // delay before first batch of elements are added\n</script>\n</body>\n</html>\n`;\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch({headless: true});\n  const [page] = await browser.pages();\n  await page.setContent(html);\n\n  const matches = await page.waitForFunction(() => {\n    const matches = [...document.querySelectorAll(""button"")];\n    return matches.length ? matches : null;\n  });\n  const length = await matches.evaluate(e => e.length);\n  const handles = await Promise.all([...Array(length)].map((e, i) =>\n    page.evaluateHandle((m, i) => m[i], matches, i)\n  ));\n  await handles[1].click(); // show that the handles work\n  const contents = await matches.evaluate(els => els.map(e => e.textContent));\n  console.log(contents);\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close())\n;\n\nUnfortunately, it\'s a bit verbose, but this can be made into a helper.\nSee also Wait for first visible among multiple elements matching selector if you\'re interested in integrating the {visible: true} option.\n', ""\nPuppeteer methods might throw errors if they are unable to fufill a request. For example, page.waitForSelector(selector[, options]) might fail if the selector doesn't match any nodes during the given timeframe.\nFor certain types of errors Puppeteer uses specific error classes. These classes are available via require('puppeteer/Errors').\nList of supported classes:\nTimeoutError\nAn example of handling a timeout error:\nconst {TimeoutError} = require('puppeteer/Errors');\n\n// ...\n\ntry {\n  await page.waitForSelector('.foo');\n} catch (e) {\n  if (e instanceof TimeoutError) {\n    // Do something if this is a timeout.\n  }\n}\n\n""]",https://stackoverflow.com/questions/49946728/puppeteer-waitforselector-on-multiple-selectors,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Scrapy, scraping data inside a Javascript","
I am using scrapy to screen scrape data from a website. However, the data I wanted wasn't inside the html itself, instead, it is from a javascript. So, my question is:
How to get the values (text values) of such cases? 
This, is the site I'm trying to screen scrape:
https://www.mcdonalds.com.sg/locate-us/
Attributes I'm trying to get:
Address, Contact, Operating hours.
If you do a ""right click"", ""view source"" inside a chrome browser you will see that such values aren't available itself in the HTML.

Edit
Sry paul, i did what you told me to, found the admin-ajax.php and saw the body but, I'm really stuck now.
How do I retrieve the values from the json object and store it into a variable field of my own? It would be good, if you could share how to do just one attribute for the public and to those who just started scrapy as well.
Here's my code so far
Items.py 
class McDonaldsItem(Item):
name = Field()
address = Field()
postal = Field()
hours = Field()

McDonalds.py
from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector
import re

from fastfood.items import McDonaldsItem

class McDonaldSpider(BaseSpider):
name = ""mcdonalds""
allowed_domains = [""mcdonalds.com.sg""]
start_urls = [""https://www.mcdonalds.com.sg/locate-us/""]

def parse_json(self, response):

    js = json.loads(response.body)
    pprint.pprint(js)

Sry for long edit, so in short, how do i store the json value into my attribute? for eg
***item['address'] = * how to retrieve ****
P.S, not sure if this helps but, i run these scripts on the cmd line using
scrapy crawl mcdonalds -o McDonalds.json -t json ( to save all my data into a json file )
I cannot stress enough on how thankful i feel. I know it's kind of unreasonable to ask this of u, will totally be okay even if you dont have time for this.
",21k,"
            23
        ","['\n(I posted this to scrapy-users mailing list but by Paul\'s suggestion I\'m posting it here as it complements the answer with the shell command interaction.)\nGenerally, websites that use a third party service to render some data visualization (map, table, etc) have to send the data somehow, and in most cases this data is accessible from the browser.\nFor this case, an inspection (i.e. exploring the requests made by the browser) shows that the data is loaded from a POST request to https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php\nSo, basically you have there all the data you want in a nice json format ready for consuming. \nScrapy provides the shell command which is very convenient to thinker with the website before writing the spider:\n$ scrapy shell https://www.mcdonalds.com.sg/locate-us/\n2013-09-27 00:44:14-0400 [scrapy] INFO: Scrapy 0.16.5 started (bot: scrapybot)\n...\n\nIn [1]: from scrapy.http import FormRequest\n\nIn [2]: url = \'https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php\'\n\nIn [3]: payload = {\'action\': \'ws_search_store_location\', \'store_name\':\'0\', \'store_area\':\'0\', \'store_type\':\'0\'}\n\nIn [4]: req = FormRequest(url, formdata=payload)\n\nIn [5]: fetch(req)\n2013-09-27 00:45:13-0400 [default] DEBUG: Crawled (200) <POST https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php> (referer: None)\n...\n\nIn [6]: import json\n\nIn [7]: data = json.loads(response.body)\n\nIn [8]: len(data[\'stores\'][\'listing\'])\nOut[8]: 127\n\nIn [9]: data[\'stores\'][\'listing\'][0]\nOut[9]: \n{u\'address\': u\'678A Woodlands Avenue 6<br/>#01-05<br/>Singapore 731678\',\n u\'city\': u\'Singapore\',\n u\'id\': 78,\n u\'lat\': u\'1.440409\',\n u\'lon\': u\'103.801489\',\n u\'name\': u""McDonald\'s Admiralty"",\n u\'op_hours\': u\'24 hours<br>\\r\\nDessert Kiosk: 0900-0100\',\n u\'phone\': u\'68940513\',\n u\'region\': u\'north\',\n u\'type\': [u\'24hrs\', u\'dessert_kiosk\'],\n u\'zip\': u\'731678\'}\n\nIn short: in your spider you have to return the FormRequest(...) above, then in the callback load the json object from response.body and finally for each store\'s data in the list data[\'stores\'][\'listing\'] create an item with the wanted values.\nSomething like this:\nclass McDonaldSpider(BaseSpider):\n    name = ""mcdonalds""\n    allowed_domains = [""mcdonalds.com.sg""]\n    start_urls = [""https://www.mcdonalds.com.sg/locate-us/""]\n\n    def parse(self, response):\n        # This receives the response from the start url. But we don\'t do anything with it.\n        url = \'https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php\'\n        payload = {\'action\': \'ws_search_store_location\', \'store_name\':\'0\', \'store_area\':\'0\', \'store_type\':\'0\'}\n        return FormRequest(url, formdata=payload, callback=self.parse_stores)\n\n    def parse_stores(self, response):\n        data = json.loads(response.body)\n        for store in data[\'stores\'][\'listing\']:\n            yield McDonaldsItem(name=store[\'name\'], address=store[\'address\'])\n\n', '\nWhen you open https://www.mcdonalds.com.sg/locate-us/ in your browser of choice, open up the ""inspect"" tool (hopefully it has one, e.g. Chrome or Firefox), and look for the ""Network"" tab.\nYou can further filter for ""XHR"" (XMLHttpRequest) events, and you\'ll see a POST request to https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php with this body\naction=ws_search_store_location&store_name=0&store_area=0&store_type=0\n\nThe response to that POST request is a JSON object with all the information you want \nimport json\nimport pprint\n...\nclass MySpider(BaseSpider):\n...\n    def parse_json(self, response):\n\n        js = json.loads(response.body)\n        pprint.pprint(js)\n\nThis would output something like:\n{u\'flagicon\': u\'https://www.mcdonalds.com.sg/wp-content/themes/mcd/images/storeflag.png\',\n u\'stores\': {u\'listing\': [{u\'address\': u\'678A Woodlands Avenue 6<br/>#01-05<br/>Singapore 731678\',\n                           u\'city\': u\'Singapore\',\n                           u\'id\': 78,\n                           u\'lat\': u\'1.440409\',\n                           u\'lon\': u\'103.801489\',\n                           u\'name\': u""McDonald\'s Admiralty"",\n                           u\'op_hours\': u\'24 hours<br>\\r\\nDessert Kiosk: 0900-0100\',\n                           u\'phone\': u\'68940513\',\n                           u\'region\': u\'north\',\n                           u\'type\': [u\'24hrs\', u\'dessert_kiosk\'],\n                           u\'zip\': u\'731678\'},\n                          {u\'address\': u\'383 Bukit Timah Road<br/>#01-09B<br/>Alocassia Apartments<br/>Singapore 259727\',\n                           u\'city\': u\'Singapore\',\n                           u\'id\': 97,\n                           u\'lat\': u\'1.319752\',\n                           u\'lon\': u\'103.827398\',\n                           u\'name\': u""McDonald\'s Alocassia"",\n                           u\'op_hours\': u\'Daily: 0630-0100\',\n                           u\'phone\': u\'68874961\',\n                           u\'region\': u\'central\',\n                           u\'type\': [u\'24hrs_weekend\',\n                                     u\'drive_thru\',\n                                     u\'mccafe\'],\n                           u\'zip\': u\'259727\'},\n\n                        ...\n                          {u\'address\': u\'60 Yishuan Avenue 4 <br/>#01-11<br/><br/>Singapore 769027\',\n                           u\'city\': u\'Singapore\',\n                           u\'id\': 1036,\n                           u\'lat\': u\'1.423924\',\n                           u\'lon\': u\'103.840628\',\n                           u\'name\': u""McDonald\'s Yishun Safra"",\n                           u\'op_hours\': u\'24 hours\',\n                           u\'phone\': u\'67585632\',\n                           u\'region\': u\'north\',\n                           u\'type\': [u\'24hrs\',\n                                     u\'drive_thru\',\n                                     u\'live_screening\',\n                                     u\'mccafe\',\n                                     u\'bday_party\'],\n                           u\'zip\': u\'769027\'}],\n             u\'region\': u\'all\'}}\n\nI\'ll leave you to extract the fields you want.\nIn the FormRequest() you send with Scrapy you probably need to add a ""X-Requested-With: XMLHttpRequest"" header (your browser sends that if you look at the request headers in the inspect tool)\n']",https://stackoverflow.com/questions/19021541/scrapy-scraping-data-inside-a-javascript,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scraping in Python - Preventing IP ban,"
I am using Python to scrape pages. Until now I didn't have any complicated issues.
The site that I'm trying to scrape uses a lot of security checks and have some mechanism to prevent scraping. 
Using Requests and lxml I was able to scrape about 100-150 pages before getting banned by IP. Sometimes I even get ban on first request (new IP, not used before, different C block). I have tried with spoofing headers, randomize time between requests, still the same.
I have tried with Selenium and I got much better results. With Selenium I was able to scrape about 600-650 pages before getting banned. Here I have also tried to randomize requests (between 3-5 seconds, and make time.sleep(300) call on every 300th request). Despite that, Im getting banned.
From here I can conclude that site have some mechanism where they ban IP if it requested more than X pages in one open browser session or something like that.
Based on your experience what else should I try?
Will closing and opening browser in Selenium help (for example after every 100th requests close and open browser). I was thinking about trying with proxies but there are about million of pages and it will be very expansive.
",36k,"
            19
        ","['\nIf you would switch to the Scrapy web-scraping framework, you would be able to reuse a number of things that were made to prevent and tackle banning:\n\nthe built-in AutoThrottle extension:\n\n\nThis is an extension for automatically throttling crawling speed based on load of both the Scrapy server and the website you are crawling.\n\n\nrotating user agents with scrapy-fake-useragent middleware:\n\n\nUse a random User-Agent provided by fake-useragent every request\n\n\nrotating IP addresses:\n\nSetting Scrapy proxy middleware to rotate on each request\nscrapy-proxies\n\nyou can also run it via local proxy & TOR:\n\nScrapy: Run Using TOR and Multiple Agents\n\n\n', '\nI had this problem too. I used urllib with tor in python3.\n\ndownload and install tor browser\ntesting tor\n\nopen terminal and type:\ncurl --socks5-hostname localhost:9050 <http://site-that-blocked-you.com>\n\nif you see result it\'s worked.\n\nNow we should test in python. Now run this code\n\nimport socks\nimport socket\nfrom urllib.request import Request, urlopen\nfrom bs4 import BeautifulSoup\n\n#set socks5 proxy to use tor\n\nsocks.set_default_proxy(socks.SOCKS5, ""localhost"", 9050)\nsocket.socket = socks.socksocket\nreq = Request(\'http://check.torproject.org\', headers={\'User-Agent\': \'Mozilla/5.0\', })\nhtml = urlopen(req).read()\nsoup = BeautifulSoup(html, \'html.parser\')\nprint(soup(\'title\')[0].get_text())\n\nif you see \n\nCongratulations. This browser is configured to use Tor. \n\nit worked in python too and this means you are using tor for web scraping.\n', '\nYou could use proxies.\nYou can buy several hundred IPs for very cheap, and use selenium as you previously have done.\nFurthermore I suggest varying the browser your use and other user-agent parameters.\nYou could iterate over using a single IP address to load only x number of pages and stopping prior to getting banned.\ndef load_proxy(PROXY_HOST,PROXY_PORT):\n        fp = webdriver.FirefoxProfile()\n        fp.set_preference(""network.proxy.type"", 1)\n        fp.set_preference(""network.proxy.http"",PROXY_HOST)\n        fp.set_preference(""network.proxy.http_port"",int(PROXY_PORT))\n        fp.set_preference(""general.useragent.override"",""whater_useragent"")\n        fp.update_preferences()\n        return webdriver.Firefox(firefox_profile=fp)\n\n']",https://stackoverflow.com/questions/35133200/scraping-in-python-preventing-ip-ban,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maintaining cookies between Mechanize requests,"
I'm trying to use the Ruby version of Mechanize to extract my employer's tickets from a ticket management system that we're moving away from that does not supply an API.
Problem is, it seems Mechanize isn't keeping the cookies between the post call and the get call shown below:
require 'rubygems'
require 'nokogiri'
require 'mechanize'

@agent = Mechanize.new

page = @agent.post('http://<url>.com/user_session', {
                                            'authenticity_token' => '<token>',
                                            'user_session[login]' => '<login>',
                                            'user_session[password]' => '<password>',
                                            'user_session[remember_me]' => '0',
                                            'commit' => 'Login'
})

page = @agent.get 'http://<url>.com/<organization>/<repo-name>/tickets/1'
puts page.title

user_session is the URL to which the site's login page POSTs, and I've verified that this indeed logs me in. But the page that returns from the get call is the 'Oops, you're not logged in!' page.
I've verified that clicking links on the page that returns from the post call works, but I can't actually get to where I need to go without JavaScript. And of course I've done this successfully on the browser with the same login.
What am I doing wrong?
",13k,"
            15
        ","[""\nOkay this might help you - first of all what version of mechanize are you using? You need to identify, if this problem is due to the cookies being overwritten/cleaned by mechanize between the requests or if the cookies are wrong/not being set in the first place. You can do that by adding a puts @agent.cookie_jar.jar inbetween the two requests, to see what is stored.\nIf its a overwriting issue, you might be able to solve it by collecting the cookies from the first request, and applying them to the second. There are many ways to do this:\nOne way is to just do a temp_jar = agent.cookie_jar.jar an then just going through each cookie and add it again using the .add method \nHOWEVER - the easiest way is by just installing the latest 2.1 pre release of mechanize (many fixes), because you will then be able to do it very simply.\nTo install the latest do a gem install mechanize --pre and make sure to get rid of the old version of mechanize gem uninstall mechanize 'some_version' after this, you can simply do as follows:\nrequire 'rubygems'\nrequire 'nokogiri'\nrequire 'mechanize'\n\n@agent = Mechanize.new\n\npage = @agent.post('http://<url>.com/user_session', {\n                                        'authenticity_token' => '<token>',\n                                        'user_session[login]' => '<login>',\n                                        'user_session[password]' => '<password>',\n                                        'user_session[remember_me]' => '0',\n                                        'commit' => 'Login'\n})\ntemp_jar = @agent.cookie_jar\n#Do whatever you need an use the cookies again in a new session after that\n@agent = Mechanize.new\n@agent.cookie_jar = temp_jar\n\npage = @agent.get 'http://<url>.com/<organization>/<repo-name>/tickets/1'\nputs page.title\n\nBTW the documentation is here http://mechanize.rubyforge.org/index.html\n"", ""\nMechanize would automatically send cookies obtained from the response in the consecutive request. You can use the same agent without re-new.\nrequire 'mechanize'\n\n@agent = Mechanize.new\n@agent.post(create_sessions_url, params, headers)\n@agent.get(ticket_url)\n\nTested with mechanize 2.7.6.\n""]",https://stackoverflow.com/questions/7046535/maintaining-cookies-between-mechanize-requests,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Screen-scraping a windows application in c# [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 2 years ago.







                        Improve this question
                    



I need to scrape data from a windows application to run a query in another program. Does anyone know of a good starting point for me to do this in .NET?
",29k,"
            13
        ","[""\nYou may want to look into the WM_GETTEXT message. This can be used to read text from other windows -- it's an archaic part of the Windows API, and if you're in C#, you'll need to p/invoke for it. \nCheck out this page for an example of doing this in C#. \nBasically, you first FindControlEx() to get the handle of the window that you want (by caption).\nSecond, you recursively enumerate the controls on that window with EnumChildWindows() to find all of the window's child controls, and all of those children's children until you have a complete map of the target form. \nHere is a selected portion of Theta-ga's excellent explanation from Google Answers:\nTo get the contents of any textbox or listbox control, all we need is it's window handle.  If you have already obtained the window handle then move to part 2 of the explaination.\nPART 1: Obtaining the control handle\n\nTo obtain the handle of a control, we first obtain the handle of it?s parent window. We can do this by using the Win32 FindControlEx() method. This method takes in the window caption (such as 'Calculator') and/or its class name, and return its handle.\nOnce we have the parent window handle, we can call the Win32 EnumChildWindows method. This method takes in a callback method, which it calls with the handle of every child control it finds for the specified parent. For eg., if we call this method with the handle of the Calculator window, it will call the callback method with the handle of the textbox control, and then again with the handles of each of the buttons on the Calculator window, and so on.\nSince we are only interested in the handle of the textbox control, we can check the class of the window in the callback method. The Win32 method GetClassName() can be used for this. This method takes in a window handle and provides us with a string containing the class name. So a textbox belongs to the ?Edit? class, a listbox to the 'ListBox' class and so on. Once you have determined that you have the handle for the right control, you can read its contents.\n\nPART 2: Reading the contents of a control\n\nYou can read in the contents of a control by using the Win32 SendMessage() function, and using it to pass the WM_GETTEXT message to the target control. This will give you the text content of the control. This method will work for a textbox, button, or static control.\nHowever, the above approach will fail if you try to read the contents of a listbox. To get the contents of a listbox, we need to first use SendMessage() with the LB_GETCOUNT message to get the count of list items. Then we need to call SendMessage() with the LB_GETTEXT message for each item in the list.\n\n""]",https://stackoverflow.com/questions/375117/screen-scraping-a-windows-application-in-c-sharp,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selenium Scroll inside of popup div,"
I am using selenium and trying to scroll inside the popup div on instagram.
I get to a page like 'https://www.instagram.com/kimkardashian/', click followers, and then I can't get the followers list to scroll down.
I tried using hover, click_and_hold, and a few other tricks to select the div but none of them worked.
What would the best way be to get this selected?
This is what I tried so far:
driver.find_elements_by_xpath(""//*[contains(text(), 'followers')]"")[0].click()
element_to_hover_over = driver.find_elements_by_xpath(""//*[contains(text(), 'Follow')]"")[12]
hover = ActionChains(webdriver).move_to_element(element_to_hover_over)
hover.click_and_hold()
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")

",23k,"
            11
        ","['\nThe exact code is as follow. You first have to find the new iframe which contains the name of followers:\nscr1 = driver.find_element_by_xpath(\'/html/body/div[2]/div/div[2]/div/div[2]\')\ndriver.execute_script(""arguments[0].scrollTop = arguments[0].scrollHeight"", scr1)\n\nThis will automatically scroll down the page but you have make a for loop for it until it reaches to the end of page. You can see my Instagram crawler here.\n', '\nYou would need to use jQuery to execute a function on the div.  Here\'s the way I figured to do it.  It was easier to solve it with jQuery and execute a script than handle it with the api.\nheight = 2000\nquery = \'jQuery(""div"").filter((i, div) => jQuery(div).css(""overflow-y"") == ""scroll"")[0].scrollTop = %s\' %height\ndriver.execute_script(query)\n\n']",https://stackoverflow.com/questions/38041974/selenium-scroll-inside-of-popup-div,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pass the user-agent through webdriver in Selenium,"
I am working on a website scraping project using Selenium in Python. When I open the homepage through a browser, it opens properly.
But, when I try to open the webpage through webdriver() in Selenium, it opens a completely different page.
I think, it is able to detect the user-agent( not sure what it is called) and is able to check the properties of the browser or something.
Is it possible to pass the properties though the webdriver() so that the right homepage is loaded.
Thanks
",13k,"
            10
        ","['\nChanging the user agent in the python version of webdriver is done by altering your browser\'s profile. I have only done this for webdriver.Firefox() by passing a profile parameter. You need to do the following:\nfrom selenium import webdriver\nprofile = webdriver.FirefoxProfile()\nprofile.set_preference(""general.useragent.override"",""your_user_agent_string"")\ndriver=webdriver.Firefox(profile)\n\nEvery time you wish to change the user agent you will need to restart your web browser (i.e. call driver=webdriver.Firefox(profile) again)\nIf you are unsure to what your user agent string is do a search for ""what is my user agent"" on a browser that displays the page properly and just copy and paste that one.\nHope that sorts it.\n', '\nAssuming the user-agent is the problem, in Java you can modify it like this:\nFirefoxProfile profile = new FirefoxProfile();\nprofile.addAdditionalPreference(""general.useragent.override"", ""some UA string"");\nWebDriver driver = new FirefoxDriver(profile);\n\nSee documentation here.\n']",https://stackoverflow.com/questions/8286127/pass-the-user-agent-through-webdriver-in-selenium,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Save all image files from a website,"
I'm creating a small app for myself where I run a Ruby script and save all of the images off of my blog.
I can't figure out how to save the image files after I've identified them. Any help would be much appreciated.
require 'rubygems'
require 'nokogiri'
require 'open-uri'

url = '[my blog url]'
doc = Nokogiri::HTML(open(url))

doc.css(""img"").each do |item|
  #something
end

",11k,"
            8
        ","['\nURL = \'[my blog url]\'\n\nrequire \'nokogiri\' # gem install nokogiri\nrequire \'open-uri\' # already part of your ruby install\n\nNokogiri::HTML(open(URL)).xpath(""//img/@src"").each do |src|\n  uri = URI.join( URL, src ).to_s # make absolute uri\n  File.open(File.basename(uri),\'wb\'){ |f| f.write(open(uri).read) }\nend\n\nUsing the code to convert to absolute paths from here: How can I get the absolute URL when extracting links using Nokogiri?\n', ""\nassuming the src attribute is an absolute url, maybe something like:\nif item['src'] =~ /([^\\/]+)$/\n    File.open($1, 'wb') {|f| f.write(open(item['src']).read)}\nend\n\n"", ""\nTip: there's a simple way to get images from a page's head/body using the Scrapifier gem. The cool thing is that you can also define which type of image you want it to be returned (jpg, png, gif). \nGive it a try: https://github.com/tiagopog/scrapifier\nHope you enjoy.\n"", ""\nsystem %x{ wget #{item['src']} }\n\nEdit: This is assuming you're on a unix system with wget :)\nEdit 2: Updated code for grabbing the img src from nokogiri.\n""]",https://stackoverflow.com/questions/7926675/save-all-image-files-from-a-website,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
web scraping to fill out (and retrieve) search forms?,"
I was wondering if it is possible to ""automate"" the task of typing in entries to search forms and extracting matches from the results. For instance, I have a list of journal articles for which I would like to get DOI's (digital object identifier); manually for this I would go to the journal articles search page (e.g., http://pubs.acs.org/search/advanced), type in the authors/title/volume (etc.) and then find the article out of its list of returned results, and pick out the DOI and paste that into my reference list. I use R and Python for data analysis regularly (I was inspired by a post on RCurl) but don't know much about web protocols... is such a thing possible (for instance using something like Python's BeautifulSoup?). Are there any good references for doing anything remotely similar to this task? I'm just as much interested in learning about web scraping and tools for web scraping in general as much as getting this particular task done... Thanks for your time!
",14k,"
            8
        ","['\nBeautiful Soup is great for parsing webpages- that\'s half of what you want to do.  Python, Perl, and Ruby all have a version of Mechanize, and that\'s the other half:\nhttp://wwwsearch.sourceforge.net/mechanize/\nMechanize let\'s you control a browser:\n# Follow a link\nbrowser.follow_link(link_node)\n\n# Submit a form\nbrowser.select_form(name=""search"")\nbrowser[""authors""] = [""author #1"", ""author #2""]\nbrowser[""volume""] = ""any""\nsearch_response = br.submit()\n\nWith Mechanize and Beautiful Soup you have a great start.  One extra tool I\'d consider is Firebug, as used in this quick ruby scraping guide:\nhttp://www.igvita.com/2007/02/04/ruby-screen-scraper-in-60-seconds/\nFirebug can speed your construction of xpaths for parsing documents, saving you some serious time.\nGood luck!\n', '\nPython Code: for search forms.\n# import \nfrom selenium import webdriver\n\nfrom selenium.common.exceptions import TimeoutException\n\nfrom selenium.webdriver.support.ui import WebDriverWait # available since 2.4.0\n\nfrom selenium.webdriver.support import expected_conditions as EC # available since 2.26.0\n\n# Create a new instance of the Firefox driver\ndriver = webdriver.Firefox()\n\n# go to the google home page\ndriver.get(""http://www.google.com"")\n\n# the page is ajaxy so the title is originally this:\nprint driver.title\n\n# find the element that\'s name attribute is q (the google search box)\ninputElement = driver.find_element_by_name(""q"")\n\n# type in the search\ninputElement.send_keys(""cheese!"")\n\n# submit the form (although google automatically searches now without submitting)\ninputElement.submit()\n\ntry:\n    # we have to wait for the page to refresh, the last thing that seems to be updated is the title\n    WebDriverWait(driver, 10).until(EC.title_contains(""cheese!""))\n\n    # You should see ""cheese! - Google Search""\n    print driver.title\n\nfinally:\n    driver.quit()\n\nSource: https://www.seleniumhq.org/docs/03_webdriver.jsp\n', '\nWebRequest req = WebRequest.Create(""http://www.URLacceptingPOSTparams.com"");\n\nreq.Proxy = null;\nreq.Method = ""POST"";\nreq.ContentType = ""application/x-www-form-urlencoded"";\n\n//\n// add POST data\nstring reqString = ""searchtextbox=webclient&searchmode=simple&OtherParam=???"";\nbyte[] reqData = Encoding.UTF8.GetBytes (reqString);\nreq.ContentLength = reqData.Length;\n//\n// send request\nusing (Stream reqStream = req.GetRequestStream())\n  reqStream.Write (reqData, 0, reqData.Length);\n\nstring response;\n//\n// retrieve response\nusing (WebResponse res = req.GetResponse())\nusing (Stream resSteam = res.GetResponseStream())\nusing (StreamReader sr = new StreamReader (resSteam))\n  response = sr.ReadToEnd();\n\n// use a regular expression to break apart response\n// OR you could load the HTML response page as a DOM \n\n(Adapted from Joe Albahri\'s ""C# in a nutshell"")\n', '\nThere are many tools for web scraping. There is a good firefox plugin called iMacros. It works great and needs no programming knowledge at all. The free version can be downloaded from here:\nhttps://addons.mozilla.org/en-US/firefox/addon/imacros-for-firefox/\nThe best thing about iMacros, is that it can get you started in minutes, and it can also be launched from the bash command line, and can also be called from within bash scripts.\nA more advanced step would be selenium webdrive. The reason I chose selenium is that it is documented in a great way suiting beginners. reading just the following page:\nwould get you upand running in no time.\nSelenium supports java, python, php , c so if you are familiar with any of these languages, you would be familiar with all the commands needed. I prefer webdrive variation of selenium, as it opens a browser, so that you can check the fields and outputs. After setting up the script using webdrive, you can easily migrate the script to IDE, thus running headless.\nTo install selenium you can do by typing the command\nsudo easy_install selenium\n\nThis will take care of the dependencies and everything needed for you.\nIn order to run your script interactively, just open a terminal, and type \npython\n\nyou will see the python prompt, >>> and you can type in the commands.\nHere is a sample code which you can paste in the terminal, it will search google for the word cheeses\npackage org.openqa.selenium.example;\n\nimport org.openqa.selenium.By;\nimport org.openqa.selenium.WebDriver;\nimport org.openqa.selenium.WebElement;\nimport org.openqa.selenium.firefox.FirefoxDriver;\nimport org.openqa.selenium.support.ui.ExpectedCondition;\nimport org.openqa.selenium.support.ui.WebDriverWait;\n\npublic class Selenium2Example  {\n    public static void main(String[] args) {\n        // Create a new instance of the Firefox driver\n        // Notice that the remainder of the code relies on the interface, \n        // not the implementation.\n        WebDriver driver = new FirefoxDriver();\n\n        // And now use this to visit Google\n        driver.get(""http://www.google.com"");\n        // Alternatively the same thing can be done like this\n        // driver.navigate().to(""http://www.google.com"");\n\n        // Find the text input element by its name\n        WebElement element = driver.findElement(By.name(""q""));\n\n        // Enter something to search for\n        element.sendKeys(""Cheese!"");\n\n        // Now submit the form. WebDriver will find the form for us from the element\n        element.submit();\n\n        // Check the title of the page\n        System.out.println(""Page title is: "" + driver.getTitle());\n\n        // Google\'s search is rendered dynamically with JavaScript.\n        // Wait for the page to load, timeout after 10 seconds\n        (new WebDriverWait(driver, 10)).until(new ExpectedCondition<Boolean>() {\n            public Boolean apply(WebDriver d) {\n                return d.getTitle().toLowerCase().startsWith(""cheese!"");\n            }\n        });\n\n        // Should see: ""cheese! - Google Search""\n        System.out.println(""Page title is: "" + driver.getTitle());\n\n        //Close the browser\n        driver.quit();\n    }}\n\nI hope that this can give you a head start.\nCheers :)\n']",https://stackoverflow.com/questions/1170120/web-scraping-to-fill-out-and-retrieve-search-forms,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python web scraping involving HTML tags with attributes,"
I'm trying to make a web scraper that will parse a web-page of publications and extract the authors. The skeletal structure of the web-page is the following:
<html>
<body>
<div id=""container"">
<div id=""contents"">
<table>
<tbody>
<tr>
<td class=""author"">####I want whatever is located here ###</td>
</tr>
</tbody>
</table>
</div>
</div>
</body>
</html>

I've been trying to use BeautifulSoup and lxml thus far to accomplish this task, but I'm not sure how to handle the two div tags and td tag because they have attributes. In addition to this, I'm not sure whether I should rely more on BeautifulSoup or lxml or a combination of both. What should I do?
At the moment, my code looks like what is below:
    import re
    import urllib2,sys
    import lxml
    from lxml import etree
    from lxml.html.soupparser import fromstring
    from lxml.etree import tostring
    from lxml.cssselect import CSSSelector
    from BeautifulSoup import BeautifulSoup, NavigableString

    address='http://www.example.com/'
    html = urllib2.urlopen(address).read()
    soup = BeautifulSoup(html)
    html=soup.prettify()
    html=html.replace('&nbsp', '&#160')
    html=html.replace('&iacute','&#237')
    root=fromstring(html)

I realize that a lot of the import statements may be redundant, but I just copied whatever I currently had in more source file.
EDIT: I suppose that I didn't make this quite clear, but I have multiple  tags in page that I want to scrape. 
",9k,"
            8
        ","[""\nIt's not clear to me from your question why you need to worry about the div tags -- what about doing just:\nsoup = BeautifulSoup(html)\nthetd = soup.find('td', attrs={'class': 'author'})\nprint thetd.string\n\nOn the HTML you give, running this emits exactly:\n####I want whatever is located here ###\n\nwhich appears to be what you want.  Maybe you can specify better exactly what it is you need and this super-simple snippet doesn't do -- multiple td tags all of class author of which you need to consider (all? just some? which ones?), possibly missing any such tag (what do you want to do in that case), and the like. It's hard to infer what exactly are your specs, just from this simple example and overabundant code;-).\nEdit: if, as per the OP's latest comment, there are multiple such td tags, one per author:\nthetds = soup.findAll('td', attrs={'class': 'author'})\nfor thetd in thetds:\n    print thetd.string\n\n...i.e., not much harder at all!-)\n"", ""\nor you could be using pyquery, since BeautifulSoup is not actively maintained anymore, see http://www.crummy.com/software/BeautifulSoup/3.1-problems.html\nfirst, install pyquery with \neasy_install pyquery\n\nthen your script could be as simple as\nfrom pyquery import PyQuery\nd = PyQuery('http://mywebpage/')\nallauthors = [ td.text() for td in d('td.author') ]\n\npyquery uses the css selector syntax familiar from jQuery which I find more intuitive than BeautifulSoup's. It uses lxml underneath, and is much faster than BeautifulSoup. But BeautifulSoup is pure python, and thus works on Google's app engine as well\n"", '\nThe lxml library is now the standard for parsing html in python.  The interface can seem awkward at first, but it is very serviceable for what it does.  \nYou should let the libary handle the xml specialism, such as those escaped &entities;\nimport lxml.html\n\nhtml = """"""<html><body><div id=""container""><div id=""contents""><table><tbody><tr>\n          <td class=""author"">####I want whatever is located here, eh? &iacute; ###</td>\n          </tr></tbody></table></div></div></body></html>""""""\n\nroot = lxml.html.fromstring(html)\ntds = root.cssselect(""div#contents td.author"")\n\nprint tds           # gives [<Element td at 84ee2cc>]\nprint tds[0].text   # what you want, including the \'铆\'\n\n', '\nBeautifulSoup is certainly the canonical HTML parser/processor.  But if you have just this kind of snippet you need to match, instead of building a whole hierarchical object representing the HTML, pyparsing makes it easy to define leading and trailing HTML tags as part of creating a larger search expression:\nfrom pyparsing import makeHTMLTags, withAttribute, SkipTo\n\nauthor_td, end_td = makeHTMLTags(""td"")\n\n# only interested in <td>\'s where class=""author""\nauthor_td.setParseAction(withAttribute((""class"",""author"")))\n\nsearch = author_td + SkipTo(end_td)(""body"") + end_td\n\nfor match in search.searchString(html):\n    print match.body\n\nPyparsing\'s makeHTMLTags function does a lot more than just emit ""<tag>"" and ""</tag>"" expressions.  It also handles:\n\ncaseless matching of tags\n""<tag/>"" syntax\nzero or more attribute in the opening tag\nattributes defined in arbitrary order\nattribute names with namespaces\nattribute values in single, double, or no quotes\nintervening whitespace between tag and symbols, or attribute name, \'=\', and value\nattributes are accessible after parsing as named results\n\nThese are the common pitfalls when considering using a regex for HTML scraping.\n']",https://stackoverflow.com/questions/1391657/python-web-scraping-involving-html-tags-with-attributes,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BeautifulSoup get_text does not strip all tags and JavaScript,"
I am trying to use BeautifulSoup to get text from web pages.
Below is a script I've written to do so. It takes two arguments, first is the input HTML or XML file, the second output file.
import sys
from bs4 import BeautifulSoup

def stripTags(s): return BeautifulSoup(s).get_text()

def stripTagsFromFile(inFile, outFile):
    open(outFile, 'w').write(stripTags(open(inFile).read()).encode(""utf-8""))

def main(argv):
    if len(sys.argv) <> 3:
        print 'Usage:\t\t', sys.argv[0], 'input.html output.txt'
        return 1
    stripTagsFromFile(sys.argv[1], sys.argv[2])
    return 0

if __name__ == ""__main__"":
    sys.exit(main(sys.argv))

Unfortunately, for many web pages, for example: http://www.greatjobsinteaching.co.uk/career/134112/Education-Manager-Location
I get something like this (I'm showing only few first lines):
html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd""
    Education Manager  Job In London With  Caleeda | Great Jobs In Teaching

var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-15255540-21']);
_gaq.push(['_trackPageview']);
_gaq.push(['_trackPageLoadTime']);

Is there anything wrong with my script? I was trying to pass 'xml' as the second argument to BeautifulSoup's constructor, as well as 'html5lib' and 'lxml', but it doesn't help.
Is there an alternative to BeautifulSoup which would work better for this task? All I want is to extract the text which would be rendered in a browser for this web page.
Any help will be much appreciated. 
",15k,"
            7
        ","[""\nnltk's clean_html() is quite good at this!\nAssuming that your already have your html stored in a variable html like\nhtml = urllib.urlopen(address).read()\n\nthen just use\nimport nltk\nclean_text = nltk.clean_html(html)\n\nUPDATE\nSupport for clean_html and clean_url will be dropped for future versions of nltk. Please use BeautifulSoup for now...it's very unfortunate.\nAn example on how to achieve this is on this page:\nBeatifulSoup4 get_text still has javascript\n"", ""\nHere's an approach which is based on the answer here: BeautifulSoup Grab Visible Webpage Text by jbochi. This approach allows for comments embedded in elements containing page text, and does a bit to clean up the output by stripping newlines, consolidating space, etc.\nhtml = urllib.urlopen(address).read()\nsoup = BeautifulSoup.BeautifulSoup(html)\ntexts = soup.findAll(text=True)\n\ndef visible_text(element):\n    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n        return ''\n    result = re.sub('<!--.*-->|\\r|\\n', '', str(element), flags=re.DOTALL)\n    result = re.sub('\\s{2,}|&nbsp;', ' ', result)\n    return result\n\nvisible_elements = [visible_text(elem) for elem in texts]\nvisible_text = ''.join(visible_elements)\nprint(visible_text)\n\n"", ""\nThis was the problem I was having.  no solution seemed to be able to return the text (the text that would actually be rendered in the web broswer).  Other solutions mentioned that BS is not ideal for rendering and that html2text was a good approach.  I tried both html2text and nltk.clean_html and was surprised by the timing results so thought they warranted an answer for posterity.  Of course, the speed delta might highly depend on the contents of the data...\nOne answer here from @Helge was about using nltk of all things.  \nimport nltk\n\n%timeit nltk.clean_html(html)\nwas returning 153 us per loop\n\nIt worked really well to return a string with rendered html.  This nltk module was faster than even html2text, though perhaps html2text is more robust. \nbetterHTML = html.decode(errors='ignore')\n%timeit html2text.html2text(betterHTML)\n%3.09 ms per loop\n\n""]",https://stackoverflow.com/questions/10524387/beautifulsoup-get-text-does-not-strip-all-tags-and-javascript,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I screen scrape with Perl?,"
I need to display some values that are stored in a website, for that I need to scrape the website and fetch the content from the table. Any ideas?
",16k,"
            7
        ","['\nIf you are familiar with jQuery you might want to check out pQuery, which makes this very easy:\n## print every <h2> tag in page\nuse pQuery;\n\npQuery(""http://google.com/search?q=pquery"")\n    ->find(""h2"")\n    ->each(sub {\n        my $i = shift;\n        print $i + 1, "") "", pQuery($_)->text, ""\\n"";\n    });\n\nThere\'s also HTML::DOM.\nWhatever you do, though, don\'t use regular expressions for this.\n', '\nI have used HTML Table Extract in the past.\nI personally find it a bit clumsy to use, but maybe I did not understand the object model well.\nI usually use this part of the manual to examine the data:\n use HTML::TableExtract;\n $te = HTML::TableExtract->new();\n $te->parse($html_string);\n\n     # Examine all matching tables\n     foreach $ts ($te->tables) {\n       print ""Table ("", join(\',\', $ts->coords), ""):\\n"";\n       foreach $row ($ts->rows) {\n          print join(\',\', @$row), ""\\n"";\n       }\n     }`\n\n', ""\nAlthough I've generally done this with LWP/LWP::Simple, the current 'preferred' module for any sort of webpage scraping in Perl is WWW::Mechanize.\n"", ""\nIf you're familiar with XPath, you can also use HTML::TreeBuilder::XPath. And if you're not... well you should be ;--)\n"", '\nYou could also use this simple perl module WEB::Scraper, this is simple to understand and make life easy for me. follow this example for more information.   \nhttp://teusje.wordpress.com/2010/05/02/web-scraping-with-perl/\n', '\nFor similar Stackoverflow questions have a look at....\n\nHow can I extract URLs from a web page in Perl\nHow can I extract XML of a website and save in a file using Perl鈥檚 LWP?\n\nI do like using pQuery for things like this however Web::Scraper does look interesting.\n', ""\nI don't mean to drag up a dead thread but anyone googling across this thread should also checkout WWW::Scripter - 'For scripting web sites that have scripts'\nhappy remote data aggregating ;)\n"", ""\nTake a look at the magical Web::Scraper, it's THE tool for web scraping.\n"", '\nI use LWP::UserAgent for most of my screen scraping needs. You can also Couple that with HTTP::Cookies if you need Cookies support.\nHere\'s a simple example on how to get source.\nuse LWP;\nuse HTTP::Cookies;\nmy $cookie_jar = HTTP::Cookies->new;\nmy $browser = LWP::UserAgent->new;\n$browser->cookie_jar($cookie_jar);\n\n$resp = $browser->get(""https://www.stackoverflow.com"");\nif($resp->is_success) {\n   # Play with your source here\n   $source = $resp->content;\n   $source =~ s/^.*<table>/<table>/i; # this is just an example \n   print $source;                     # not a solution to your problem.\n}\n\n', '\nCheck out this little example of web scraping with perl:\nlink text\n']",https://stackoverflow.com/questions/713827/how-can-i-screen-scrape-with-perl,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using Nokogiri to Split Content on BR tags,"
I have a snippet of code im trying to parse with nokogiri that looks like this:
<td class=""j"">
    <a title=""title text1"" href=""http://link1.com"">Link 1</a> (info1), Blah 1,<br>
    <a title=""title text2"" href=""http://link2.com"">Link 2</a> (info1), Blah 1,<br>
    <a title=""title text2"" href=""http://link3.com"">Link 3</a> (info2), Blah 1 Foo 2,<br>
</td>

I have access to the source of the td.j using something like this:
data_items = doc.css(""td.j"")
My goal is to split each of those lines up into an array of hashes.  The only logical splitting point i can see is to split on the BRs and then use some regex on the string.  
I was wondering if there's a Better way to do this maybe using nokogiri only?  Even if i could use nokogiri to suck out the 3 line items it would make things easier for me as i could just do some regex parsing on the .content result. 
Not sure how to use Nokogiri to grab lines ending with br though -- should i be using xpaths? any direction is appreciated! thank you
",4k,"
            6
        ","['\nI\'m not sure about the point of using an array of hashes, and without an example I can\'t suggest something. However, for splitting the text on <br> tags, I\'d go about it this way:\nrequire \'nokogiri\'\n\ndoc = Nokogiri::HTML(\'<td class=""j"">\n    <a title=""title text1"" href=""http://link1.com"">Link 1</a> (info1), Blah 1,<br>\n    <a title=""title text2"" href=""http://link2.com"">Link 2</a> (info1), Blah 1,<br>\n    <a title=""title text2"" href=""http://link3.com"">Link 3</a> (info2), Blah 1 Foo 2,<br>\n</td>\')\n\ndoc.search(\'br\').each do |n|\n  n.replace(""\\n"")\nend\ndoc.at(\'tr.j\').text.split(""\\n"") # => ["""", ""    Link 1 (info1), Blah 1,"", ""Link 2 (info1), Blah 1,"", ""Link 3 (info2), Blah 1 Foo 2,""]\n\nThis will get you closer to a hash:\nHash[*doc.at(\'td.j\').text.split(""\\n"")[1 .. -1].map{ |t| t.strip.split(\',\')[0 .. 1] }.flatten] # => {""Link 1 (info1)""=>"" Blah 1"", ""Link 2 (info1)""=>"" Blah 1"", ""Link 3 (info2)""=>"" Blah 1 Foo 2""}\n\n', '\nIf your data really is that regular and you don\'t need the attributes from the <a> elements, then you could parse the text form of each table cell without having to worry about the <br> elements at all.\nGiven some HTML like this in html:\n<table>\n    <tbody>\n        <tr>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://link1.com"">Link 1</a> (info1), Blah 1,<br>\n                <a title=""title text2"" href=""http://link2.com"">Link 2</a> (info1), Blah 1,<br>\n                <a title=""title text2"" href=""http://link3.com"">Link 3</a> (info2), Blah 1 Foo 2,<br>\n            </td>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://link4.com"">Link 4</a> (info1), Blah 2,<br>\n                <a title=""title text2"" href=""http://link5.com"">Link 5</a> (info1), Blah 2,<br>\n                <a title=""title text2"" href=""http://link6.com"">Link 6</a> (info2), Blah 2 Foo 2,<br>\n            </td>\n        </tr>\n        <tr>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://link7.com"">Link 7</a> (info1), Blah 3,<br>\n                <a title=""title text2"" href=""http://link8.com"">Link 8</a> (info1), Blah 3,<br>\n                <a title=""title text2"" href=""http://link9.com"">Link 9</a> (info2), Blah 3 Foo 2,<br>\n            </td>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://linkA.com"">Link A</a> (info1), Blah 4,<br>\n                <a title=""title text2"" href=""http://linkB.com"">Link B</a> (info1), Blah 4,<br>\n                <a title=""title text2"" href=""http://linkC.com"">Link C</a> (info2), Blah 4 Foo 2,<br>\n            </td>\n        </tr>\n    </tbody>\n</table>\n\nYou could do this:\nchunks = doc.search(\'.j\').map { |td| td.text.strip.scan(/[^,]+,[^,]+/) }\n\nand have this:\n[\n    [ ""Link 1 (info1), Blah 1"", ""Link 2 (info1), Blah 1"", ""Link 3 (info2), Blah 1 Foo 2"" ],\n    [ ""Link 4 (info1), Blah 2"", ""Link 5 (info1), Blah 2"", ""Link 6 (info2), Blah 2 Foo 2"" ],\n    [ ""Link 7 (info1), Blah 3"", ""Link 8 (info1), Blah 3"", ""Link 9 (info2), Blah 3 Foo 2"" ],\n    [ ""Link A (info1), Blah 4"", ""Link B (info1), Blah 4"", ""Link C (info2), Blah 4 Foo 2"" ]\n]\n\nin chunks. Then you could convert that to whatever hash form you needed.\n']",https://stackoverflow.com/questions/7058922/using-nokogiri-to-split-content-on-br-tags,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Why does scrapy throw an error for me when trying to spider and parse a site?,"
The following code
class SiteSpider(BaseSpider):
    name = ""some_site.com""
    allowed_domains = [""some_site.com""]
    start_urls = [
        ""some_site.com/something/another/PRODUCT-CATEGORY1_10652_-1__85667"",
    ]
    rules = (
        Rule(SgmlLinkExtractor(allow=('some_site.com/something/another/PRODUCT-CATEGORY_(.*)', ))),

        # Extract links matching 'item.php' and parse them with the spider's method parse_item
        Rule(SgmlLinkExtractor(allow=('some_site.com/something/another/PRODUCT-DETAIL(.*)', )), callback=""parse_item""),
    )
    def parse_item(self, response):
.... parse stuff

Throws the following error
Traceback (most recent call last):
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/base.py"", line 1174, in mainLoop
    self.runUntilCurrent()
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/base.py"", line 796, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 318, in callback
    self._startRunCallbacks(result)
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 424, in _startRunCallbacks
    self._runCallbacks()
--- <exception caught here> ---
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 441, in _runCallbacks
    self.result = callback(self.result, *args, **kw)
  File ""/usr/lib/pymodules/python2.6/scrapy/spider.py"", line 62, in parse
    raise NotImplementedError
exceptions.NotImplementedError: 

When I change the callback to ""parse"" and the function to ""parse"" i don't get any errors, but nothing is scraped. I changed it to ""parse_items"" thinking I might be overriding the parse method by accident. Perhaps I'm setting up the link extractor wrong?
What I want to do is parse each ITEM link on the CATEGORY page. Am I doing this totally wrong?
",9k,"
            6
        ","['\nI needed to change BaseSpider to CrawlSpider. Thanks srapy users!\nhttp://groups.google.com/group/scrapy-users/browse_thread/thread/4adaba51f7bcd0af#\n\nHi Bob,\nPerhaps it might work if you change\n  from BaseSpider to CrawlSpider? The\n  BaseSpider seems not implement Rule,\n  see:\nhttp://doc.scrapy.org/topics/spiders.html?highlight=rule#scrapy.contr...\n-M\n\n', '\nBy default scrapy searches for parse function in the class. Here in your spider, parse function is missing. Instead of parse you have given parse_item. The problem will be solved if parse_item is replace with parse.\nOr you can override the parse method in spider.py with that of parse_item.\n']",https://stackoverflow.com/questions/5264829/why-does-scrapy-throw-an-error-for-me-when-trying-to-spider-and-parse-a-site,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Issue scraping page with ""Load more"" button with rvest","
I want to obtain the links to the atms listed on this page: https://coinatmradar.com/city/345/bitcoin-atm-birmingham-uk/
Would I need to do something about the 'load more' button at the bottom of the page?
I have been using the selector tool you can download for chrome to select the CSS path. 
I've written the below code block and it only seems to retrieve the first ten links. 
library(rvest)

base <- ""https://coinatmradar.com/city/345/bitcoin-atm-birmingham-uk/""
base_read <- read_html(base)
atm_urls <- html_nodes(base_read, "".place > a"")
all_urls_final <- html_attr(atm_urls, ""href"" )
print(all_urls_final)

I expected to be able to retrieve all links to the atms listed in the area but my R code has not done so.
Any help would be great. Sorry if this is a really simple question.
",2k,"
            6
        ","['\nYou should give RSelenium a try. I\'m able to get the links with the following code:\n# install.packages(""RSelenium"")\nlibrary(RSelenium)\nlibrary(rvest)\n\n# Download binaries, start driver, and get client object.\nrd <- rsDriver(browser = ""firefox"", port = 4444L)\nffd <- rd$client\n\n# Navigate to page.\nffd$navigate(""https://coinatmradar.com/city/345/bitcoin-atm-birmingham-uk/"")\n\n# Find the load button and assign, then send click event.\nload_btn <- ffd$findElement(using = ""css selector"", "".load-more .btn"")\nload_btn$clickElement()\n\n# Wait for elements to load.\nSys.sleep(2)\n\n# Get HTML data and parse\nhtml_data <- ffd$getPageSource()[[1]]\nhtml_data %>% \n    read_html() %>% \n    html_nodes("".place a:not(.operator-link)"") %>% \n    html_attr(""href"")\n\n#### OUTPUT ####\n\n#  [1] ""/bitcoin_atm/5969/bitcoin-atm-shitcoins-club-birmingham-uk-bitcoin-embassy/""                   \n#  [2] ""/bitcoin_atm/7105/bitcoin-atm-general-bytes-northampton-costcutter/""                           \n#  [3] ""/bitcoin_atm/4759/bitcoin-atm-general-bytes-birmingham-uk-costcutter/""                         \n#  [4] ""/bitcoin_atm/2533/bitcoin-atm-general-bytes-birmingham-uk-londis-# convenience/""                 \n#  [5] ""/bitcoin_atm/5458/bitcoin-atm-general-bytes-coventry-agg-african-restaurant/""                  \n#  [6] ""/bitcoin_atm/711/bitcoin-atm-general-bytes-coventry-bigs-barbers/""                             \n#  [7] ""/bitcoin_atm/5830/bitcoin-atm-general-bytes-telford-bpred-lion-service-station/""               \n#  [8] ""/bitcoin_atm/5466/bitcoin-atm-general-bytes-nottingham-24-express-off-licence/""                \n#  [9] ""/bitcoin_atm/4615/bitcoin-atm-general-bytes-northampton-costcutter/""                           \n# [10] ""/bitcoin_atm/4841/bitcoin-atm-lamassu-worcester-computer-house/""                               \n# [11] ""/bitcoin_atm/3150/bitcoin-atm-bitxatm-leicester-keshs-wines-and-newsagents-braustone/""         \n# [12] ""/bitcoin_atm/2948/bitcoin-atm-bitxatm-coventry-nisa-local/""                                    \n# [13] ""/bitcoin_atm/4742/bitcoin-atm-bitxatm-birmingham-uk-custcutter-coventry-road-hay-mills/""       \n# [14] ""/bitcoin_atm/4741/bitcoin-atm-bitxatm-derby-michaels-drink-store-alvaston/""                    \n# [15] ""/bitcoin_atm/4740/bitcoin-atm-bitxatm-birmingham-uk-nisa-local-crabtree-# hockley/""              \n# [16] ""/bitcoin_atm/4739/bitcoin-atm-bitxatm-birmingham-uk-nisa-local-subway-boldmere/""               \n# [17] ""/bitcoin_atm/4738/bitcoin-atm-bitxatm-birmingham-uk-ashtree-convenience-store/""                \n# [18] ""/bitcoin_atm/4737/bitcoin-atm-bitxatm-birmingham-uk-nisa-local-finnemore-road-bordesley-green/""\n# [19] ""/bitcoin_atm/3160/bitcoin-atm-bitxatm-birmingham-uk-costcutter/"" \n\n', ""\nWhen you click show more the page does an XHR POST request for more results using an offset of 10 (suggesting results come in batches of 10) from current set. You can mimic this so long as you have the followings params in the post body (I suspect only the bottom 3 are essential)\n'direction' : 1\n'sort' : 1\n'offset' : 10\n'pagetype' : 'city'\n'pageid' : 345\n\nAnd the following request header is required (at least in Python implementations) \n'X-Requested-With' : 'XMLHttpRequest'\n\nYou send that correctly and you will get a response containing the additional content. Note: content is wrapped in  ![CDATA[]] as instruction that content should not be interpreted as xml - you will need to account for that by extracting content within for parsing.\nThe total number of atms is returned from original  page you have and with css selector\n.atm-number\n\nYou can split on  &nbsp; and take the upper bound value from the split and convert to int. You then can calculate each offset required to meet that total (being used in a loop as consecutive offset param until total achieved) e.g. 19 results will be 2 requests total with 1 request at offset 10 for additional content.\n""]",https://stackoverflow.com/questions/56118999/issue-scraping-page-with-load-more-button-with-rvest,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PHP equivalent of PyQuery or Nokogiri? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



Basically, I want to do some HTML screen scraping, but figuring out if it is possible in PHP.
In Python, I would use
PyQuery.
In Ruby, I would use Nokogiri.
",4k,"
            6
        ","[""\nIn PHP you can use phpQuery\nP.S. it's kinda ironic, I came to this page looking for phpQuery equivalent in Python :)\n"", '\nIn PHP for screen scraping you can use Snoopy (http://sourceforge.net/projects/snoopy/) or Simple HTML DOM Parser (http://simplehtmldom.sourceforge.net/)\n', ""\nHere's a PHP port of Ruby nokogiri\n"", '\nThe closest thing to Nokogiri in PHP is this one.\n', '\nThere are also the built in PHP DOM libraries.  They include DomDocument with its loadHTML() method for parsing html from a string and DomElement and DomNode for building up HTML via an object. \n']",https://stackoverflow.com/questions/2815726/php-equivalent-of-pyquery-or-nokogiri,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Count number of results for a particular word on Twitter,"
To further a personal project of mine, I have been pondering how to count the number of results for a user specified word on Twitter.  I have used their API extensively, but have not been able to come up with an efficient or even halfway practical way to count the occurrences of a particular word.  The actual results are not critical, just the overall count.  I'll keep scratching my head.  Any ideas or direction pointing would be most appreciated.
e.g. http://search.twitter.com/search?q=tomatoes
",7k,"
            5
        ","[""\nI'm able to go back about a week. I start my search with the parameters that Adam posted and then key off of the smallest id in the set of search results, like so,\nhttp://search.twitter.com/search.atom?lang=en&q=iphone&rpp=100&max_id=\nwhere max_id = the min(id) of the 100 results I just pulled.\n"", '\nnet but I have made recursive function to call search query again and again until I don\'t find word ""page="" in result.\n']",https://stackoverflow.com/questions/580369/count-number-of-results-for-a-particular-word-on-twitter,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Convert a relative URL to an absolute URL with Simple HTML DOM?,"
When I'm scraping content from some pages, the script gives a relative URL. Is it possible to get a absolute URL with Simple HTML DOM?
",5k,"
            4
        ","[""\nI don鈥檛 think that the Simple HTML DOM Parser can do that.\nBut you can do that on your own. First you need to distinguish the base URI that is the URI of the document if not declared otherwise (see BASE element). Than get each URI reference and apply the algorithms to resolve a relative URI as described in RFC 3986 (there already are classes you can use for that like the PEAR package Net_URL2).\nSo, using these two classes, you could do something like this:\n$uri = new Net_URL2('http://example.com/foo/bar'); // URI of the resource\n$baseURI = $uri;\nforeach ($html->find('base[href]') as $elem) {\n    $baseURI = $uri->resolve($elem->href);\n}\n\nforeach ($html->find('*[src]') as $elem) {\n    $elem->src = $baseURI->resolve($elem->src)->__toString();\n}\nforeach ($html->find('*[href]') as $elem) {\n    if (strtoupper($elem->tag) === 'BASE') continue;\n    $elem->href = $baseURI->resolve($elem->href)->__toString();\n}\nforeach ($html->find('form[action]') as $elem) {\n    $elem->action = $baseURI->resolve($elem->action)->__toString();\n}\n\nRepeat the substitution for any other attribute containing a URI like background, cite, classid, codebase, data, longdesc, profile and usemap (see index of attributes in HTML 4.01).\n"", '\nIn addition to @Artefacto\'s answer, and if you are outputting the scraped HTML somewhere, you could simply add <base href=""http://example.com""> to the head of the document, which will establish the base URL for all relative URLs in the document as the specified href. Have a look at http://www.w3schools.com/tags/tag_base.asp\n', ""\nEDIT See Gumbo's answer for a formally correct answer. This is a simplified algorithm that will work in the vast majority of cases, but fail on some.\nSure. Do this:\n\nTake the relative URL (a URL that doesn't start with http://, https://, or any other protocol, and also doesn't start with /).\nTake the URL of the page.\nRemove the query string from it (if any). One simple way is to explode around ? and then take the first element of the resulting array (take element with index 0 or use reset).\n\n\nIf the URL of the page ends in /, append it the relative URL and you have the final URL.\nIf the URL doesn't end in /, take dirname of it, and append it the relative URL. You now have the final URL.\n\n\n""]",https://stackoverflow.com/questions/3329499/convert-a-relative-url-to-an-absolute-url-with-simple-html-dom,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can a cURL based HTTP request imitate a browser based request completely?,"
This is a two part question.
Q1: Can cURL based request 100% imitate a browser based request? 
Q2: If yes, what all options should be set. If not what extra does the browser do that cannot bee imitated by cURL?
I have a website and I see thousands of request being made from a single IP in a very short time. These requests harvest all my data. When looked at the log to identify the agent used, it looks like a request from browser. So was curious to know if its a bot and not a user.
Thanks in advance
",8k,"
            4
        ","['\nThis page has all the answers to your questions. You can imitate the things mostly.\n', ""\nR1 : I suppose, if you set all the correct headers, that, yes, a curl-based request can imitate a browser-based one : after all, both send an HTTP request, which is just a couple of lines of text following a specific convention (namely, the HTTP RFC)\n\nR2 : The best way to answer that question is to take a look at what your browser is sending ; with Firefox, for instance, you can use either Firebug or LiveHTTPHeaders to get that.\nFor instance, to get this page, Firefox sent those request headers :\nGET /questions/1926876/can-a-curl-based-http-request-imitate-a-browser-based-request-completely HTTP/1.1\nHost: stackoverflow.com\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; fr; rv:1.9.2b4) Gecko/20091124 Firefox/3.6b4\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: fr,fr-fr;q=0.8,en-us;q=0.5,en;q=0.3\nAccept-Encoding: gzip,deflate\nAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7\nKeep-Alive: 115\nConnection: keep-alive\nReferer: http://stackoverflow.com/questions/1926876/can-a-curl-based-http-request-imitate-a-browser-based-request-completely/1926889\nCookie: .......\nCache-Control: max-age=0\n\n(I Just removed a couple of informations -- but you get the idea ;-) )\nUsing curl, you can work with curl_setopt to set the HTTP headers ; here, you'd probably have to use a combination of CURLOPT_HTTPHEADER, CURLOPT_COOKIE, CURLOPT_USERAGENT, ...\n""]",https://stackoverflow.com/questions/1926876/can-a-curl-based-http-request-imitate-a-browser-based-request-completely,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get instagram followers,"
I want to parse a website's followers count with BeautifulSoup. This is what I have so far:
username_extract = 'lazada_my'

url = 'https://www.instagram.com/'+ username_extract
r = requests.get(url)
soup = BeautifulSoup(r.content,'lxml')
f = soup.find('head', attrs={'class':'count'})

This is the part I want to parse:

Something within my soup.find() function is wrong, but I can't wrap my head around it. When returning f, it is empty. Any idea what I am doing wrong?
",5k,"
            3
        ","['\nI think you can use re module to search the correct count.\nimport requests\nimport re\n\nusername_extract = \'lazada_my\'\n\nurl = \'https://www.instagram.com/\'+ username_extract\nr = requests.get(url)\nm = re.search(r\'""followed_by"":\\{""count"":([0-9]+)\\}\', str(r.content))\nprint(m.group(1))\n\n', '\nsoup.find(\'head\', attrs={\'class\':\'count\'}) searches for something that looks like <head class=""count"">, which doesn\'t exist anywhere in the HTML. The data you\'re after is contained in the <script> tag that starts with window._sharedData:\nscript = soup.find(\'script\', text=lambda t: t.startswith(\'window._sharedData\'))\n\nFrom there, you can just strip off the variable assignment and the semicolon to get valid JSON:\n# <script>window._sharedData = ...;</script>\n#                              ^^^\n#                              JSON\n\npage_json = script.text.split(\' = \', 1)[1].rstrip(\';\')\n\nParse it and everything you need is contained in the object:\nimport json\n\ndata = json.loads(page_json)\nfollower_count = data[\'entry_data\'][\'ProfilePage\'][0][\'user\'][\'followed_by\'][\'count\']\n\n', '\nMost of the content is dynamically generated with JS. That\'s the reason you\'re getting empty results.\nBut, the followers count is present in the page source. Only thing is, it is not directly available in the form you want. You can see it here:\n<meta content=""407.4k Followers, 27 Following, 2,740 Posts - See Instagram photos and videos from Lazada Malaysia (@lazada_my)"" name=""description"" />\n\nIf you want to scrape the followers count without regex, you can use this:\n>>> followers = soup.find(\'meta\', {\'name\': \'description\'})[\'content\']\n>>> followers\n\'407.4k Followers, 27 Following, 2,740 Posts - See Instagram photos and videos from Lazada Malaysia (@lazada_my)\'\n>>> followers_count = followers.split(\'Followers\')[0]\n>>> followers_count\n\'407.4k \'\n\n', '\nYou have to look for the scripts, Then look for the \'window._sharedData\' exits in it. If exits then perform the regular expression operation. \nimport re\n\nusername_extract = \'lazada_my\'\nurl = \'https://www.instagram.com/\'+ username_extract\nr = requests.get(url)\nsoup = BeautifulSoup(r.content,\'lxml\')\ns = re.compile(r\'""followed_by"":{""count"":\\d*}\')\nfor i in soup.find_all(\'script\'):\n     if \'window._sharedData\' in str(i):\n         print s.search(str(i.contents)).group()\n\nResult,\n""followed_by"":{""count"":407426}\n\n', '\nThank you all, I ended up using William\'s solution. In case anybody will have future projects, here is my complete code for scraping a bunch of URL\'s for their follower count:\nimport requests\nimport csv \nimport pandas as pd\nimport re\n\ninsta = pd.read_csv(\'Instagram.csv\')\n\nusername = []\n\nbad_urls = [] \n\nfor lines in insta[\'Instagram\'][0:250]:\n    lines = lines.split(""/"")\n    username.append(lines[3])\n\nwith open(\'insta_output.csv\', \'w\') as csvfile:\nt = csv.writer(csvfile, delimiter=\',\')     #   ----> COMMA Seperated\nfor user in username:\n   try:\n       url = \'https://www.instagram.com/\'+ user\n       r = requests.get(url)\n       m = re.search(r\'""followed_by"":\\{""count"":([0-9]+)\\}\', str(r.content))\n       num_followers = m.group(1)\n       t.writerow([user,num_followers])    #  ----> Adding Rows\n   except:\n       bad_urls.append(url)\n\n']",https://stackoverflow.com/questions/49043857/get-instagram-followers,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"BeautifulSoup subpages of list with ""load more"" pagination","
Quite new here, so apologies in advance. I'm looking to get a list of all company descriptions from https://angel.co/companies to play around with. The web-based parsing tools I've tried aren't cutting it, so I'm looking to write a simple python script. Should I start by getting an array of all the company URLs then loop through them? Any resources or direction would be helpful--I've looked around BeautifulSoup's documentation and a few posts/video tutorials, but I'm getting hung up on simulating the json request, among other things (see here: Get all links with BeautifulSoup from a single page website ('Load More' feature))
I see a script that I believe is calling additional listings:
o.on(""company_filter_fetch_page_complete"", function(e) {
    return t.ajax({
        url: ""/companies/startups"",
        data: e,
        dataType: ""json"",
        success: function(t) {
            return t.html ? 
                (E().find("".more"").empty().replaceWith(t.html),
                 c()) : void 0
        }
    })
}),

Thanks!
",4k,"
            3
        ","['\nThe data you want to scrape is dynamically loaded using ajax, you need to do a lot of work to get to the html you actually want:  \nimport requests\nfrom bs4 import BeautifulSoup\n\nheader = {\n    ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36"",\n    ""X-Requested-With"": ""XMLHttpRequest"",\n    }\n\nwith requests.Session() as s:\n    r = s.get(""https://angel.co/companies"").content\n    csrf = BeautifulSoup(r).select_one(""meta[name=csrf-token]"")[""content""]\n    header[""X-CSRF-Token""] = csrf\n    ids = s.post(""https://angel.co/company_filters/search_data"", data={""sort"": ""signal""}, headers=header).json()\n    _ids = """".join([""ids%5B%5D={}&"".format(i)  for i in ids.pop(""ids"")])\n    rest = ""&"".join([""{}={}"".format(k,v) for k,v in ids.items()])\n    url = ""https://angel.co/companies/startups?{}{}"".format(_ids, rest)\n    rsp = s.get(url, headers=header)\n    print(rsp.json())\n\nWe first need to get a valid csrf-token which is what the initial request does, then we need to post to https://angel.co/company_filters/search_data:\n\nwhich gives us:\n{""ids"":[296769,297064,60,63,112,119,130,160,167,179,194,236,281,287,312,390,433,469,496,516],""total"":908164,""page"":1,""sort"":""signal"",""new"":false,""hexdigest"":""3f4980479bd6dca37e485c80d415e848a57c43ae""}\n\nThey are the params needed for our get to https://angel.co/companies/startups i.e our last request:\n\nThat request then gives us more json which holds the html and all the company info:\n{""html"":""<div class=\\"" dc59 frs86 _a _jm\\"" data-_tn=\\""companies/results ...........\n\nThere is way too much to post but that is what you will need to parse.\nSo putting it all together:\nIn [3]: header = {\n   ...:     ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36"",\n   ...:     ""X-Requested-With"": ""XMLHttpRequest"",\n   ...: }\n\nIn [4]: with requests.Session() as s:\n   ...:         r = s.get(""https://angel.co/companies"").content\n   ...:         csrf = BeautifulSoup(r, ""lxml"").select_one(""meta[name=csrf-token]"")[""content""]\n   ...:         header[""X-CSRF-Token""] = csrf\n   ...:         ids = s.post(""https://angel.co/company_filters/search_data"", data={""sort"": ""signal""}, headers=header).json()\n   ...:         _ids = """".join([""ids%5B%5D={}&"".format(i) for i in ids.pop(""ids"")])\n   ...:         rest = ""&"".join([""{}={}"".format(k, v) for k, v in ids.items()])\n   ...:         url = ""https://angel.co/companies/startups?{}{}"".format(_ids, rest)\n   ...:         rsp = s.get(url, headers=header)\n   ...:         soup = BeautifulSoup(rsp.json()[""html""], ""lxml"")\n   ...:         for comp in soup.select(""div.base.startup""):\n   ...:                 text = comp.select_one(""div.text"")\n   ...:                 print(text.select_one(""div.name"").text.strip())\n   ...:                 print(text.select_one(""div.pitch"").text.strip())\n   ...:         \nFrontback\nMe, now.\nOutbound\nOptimizely for messages\nAdaptly\nThe Easiest Way to Advertise Across The Social Web.\nDraft\nWords with Friends for Fantasy (w/ real money)\nGraphicly\nan automated ebook publishing and distribution platform\nAppstores\nApp Distribution Platform\neVenues\nOnline Marketplace & Booking Engine for Unique Meeting Spaces\nWePow\nVideo & Mobile Recruitment\nDoubleDutch\nEvent Marketing Automation Software\necomom\nIt\'s all good\nBackType\nAcquired by Twitter\nStipple\nNative advertising for the visual web\nPinterest\nA Universal Social Catalog\nSocialize\nIdentify and reward your most influential users with our drop-in social platform.\nStyleSeat\nLargest and fastest growing marketplace in the $400B beauty and wellness industry\nLawPivot\n99 Designs for legal\nOstrovok\nLeading hotel booking platform for Russian-speakers\nThumb\nLeading mobile social network that helps people get instant opinions\nAppFog\nMaking developing applications on the cloud easier than ever before\nArtsy\nMaking all the world鈥檚 art accessible to anyone with an Internet connection.\n\nAs far as the pagination goes, you are limited to 20 pages per day but to get all 20 pages is simply a case of adding page:page_no to our form data to get the new params needed, data={""sort"": ""signal"",""page"":page}, when you click load more you can see what is posted:\n\nSo the final code:\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef parse(soup):\n\n        for comp in soup.select(""div.base.startup""):\n            text = comp.select_one(""div.text"")\n            yield (text.select_one(""div.name"").text.strip()), text.select_one(""div.pitch"").text.strip()\n\ndef connect(page):\n    header = {\n        ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36"",\n        ""X-Requested-With"": ""XMLHttpRequest"",\n    }\n\n    with requests.Session() as s:\n        r = s.get(""https://angel.co/companies"").content\n        csrf = BeautifulSoup(r, ""lxml"").select_one(""meta[name=csrf-token]"")[""content""]\n        header[""X-CSRF-Token""] = csrf\n        ids = s.post(""https://angel.co/company_filters/search_data"", data={""sort"": ""signal"",""page"":page}, headers=header).json()\n        _ids = """".join([""ids%5B%5D={}&"".format(i) for i in ids.pop(""ids"")])\n        rest = ""&"".join([""{}={}"".format(k, v) for k, v in ids.items()])\n        url = ""https://angel.co/companies/startups?{}{}"".format(_ids, rest)\n        rsp = s.get(url, headers=header)\n        soup = BeautifulSoup(rsp.json()[""html""], ""lxml"")\n        for n, p in parse(soup):\n            yield n, p\nfor i in range(1, 21):\n    for name, pitch in connect(i):\n        print(name, pitch)\n\nObviously what you parse is up to you but everything you see in your browser in the results will be available.\n']",https://stackoverflow.com/questions/37799149/beautifulsoup-subpages-of-list-with-load-more-pagination,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Close a scrapy spider when a condition is met and return the output object,"
I have made a spider to get reviews from a page like this here using scrapy. I want product reviews only till a certain date(2nd July 2016 in this case). I want to close my spider as soon as the review date goes earlier than the given date and return the items list.
Spider is working well but my problem is that i am not able to close my spider if the condition is met..if i raise an exception, spider closes without returning anything.
Please suggest the best way to close the spider manually. Here is my code:
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from scrapy import Selector
from tars.items import FlipkartProductReviewsItem
import re as r
import unicodedata
from datetime import datetime 

class Freviewspider(CrawlSpider):
    name = ""frs""
    allowed_domains = [""flipkart.com""]
    def __init__(self, *args, **kwargs):
        super(Freviewspider, self).__init__(*args, **kwargs)
        self.start_urls = [kwargs.get('start_url')]


    rules = (
        Rule(LinkExtractor(allow=(), restrict_xpaths=('//a[@class=""nav_bar_next_prev""]')), callback=""parse_start_url"", follow= True),
)


    def parse_start_url(self, response):

        hxs = Selector(response)
        titles = hxs.xpath('//div[@class=""fclear fk-review fk-position-relative line ""]')

        items = []

        for i in titles:

            item = FlipkartProductReviewsItem()

            #x-paths:

            title_xpath = ""div[2]/div[1]/strong/text()""
            review_xpath = ""div[2]/p/span/text()""
            date_xpath = ""div[1]/div[3]/text()""



            #field-values-extraction:

            item[""date""] = (''.join(i.xpath(date_xpath).extract())).replace('\n ', '')
            item[""title""] = (''.join(i.xpath(title_xpath).extract())).replace('\n ', '')

            review_list = i.xpath(review_xpath).extract()
            temp_list = []
            for element in review_list:
                temp_list.append(element.replace('\n ', '').replace('\n', ''))

            item[""review""] = ' '.join(temp_list)

            xxx = datetime.strptime(item[""date""], '%d %b %Y ')
            comp_date = datetime.strptime('02 Jul 2016 ', '%d %b %Y ')
            if xxx>comp_date:
                items.append(item)
            else:
                break

        return(items)

",6k,"
            2
        ",['\nTo force spider to close you can use raise CloseSpider exception as described here in scrapy docs. Just be sure to return/yield your items before you raise the exception.\n'],https://stackoverflow.com/questions/38331428/close-a-scrapy-spider-when-a-condition-is-met-and-return-the-output-object,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to scrape charts from a website with python?,"
EDIT: 
So I have save the script codes below to a text file but using re to extract the data still doesn't return me anything. My code is: 
file_object = open('source_test_script.txt', mode=""r"")
soup = BeautifulSoup(file_object, ""html.parser"")
pattern = re.compile(r""^var (chart[0-9]+) = new Highcharts.Chart\(({.*?})\);$"", re.MULTILINE | re.DOTALL)
scripts = soup.find(""script"", text=pattern)
profile_text = pattern.search(scripts.text).group(1)
profile = json.loads(profile_text)

print profile[""data""], profile[""categories""]


I would like to extract the chart's data from a website. The following is the source code of the chart. 
  <script type=""text/javascript"">
    jQuery(function() {

    var chart1 = new Highcharts.Chart({

          chart: {
             renderTo: 'chart1',
              defaultSeriesType: 'column',
            borderWidth: 2
          },
          title: {
             text: 'Productions'
          },
          legend: {
            enabled: false
          },
          xAxis: [{
             categories: [1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016],

          }],
          yAxis: {
             min: 0,
             title: {
             text: 'Productions'
          }
          },

          series: [{
               name: 'Productions',
               data: [1,1,0,1,6,4,9,15,15,19,24,18,53,42,54,53,61,36]
               }]
       });
    });

    </script>

There are several charts like that from the website, called ""chart1"", ""chart2"", etc. I would like to extract the following data: the categories line and the data line, for each chart: 
categories: [1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016]

data: [1,1,0,1,6,4,9,15,15,19,24,18,53,42,54,53,61,36]

",5k,"
            2
        ","['\nAnother way is to use Highcharts\' JavaScript Library as one would in the console and pull that using Selenium. \nimport time\nfrom selenium import webdriver\n\nwebsite = """"\n\ndriver = webdriver.Firefox()\ndriver.get(website)\ntime.sleep(5)\n\ntemp = driver.execute_script(\'return window.Highcharts.charts[0]\'\n                             \'.series[0].options.data\')\ndata = [item[1] for item in temp]\nprint(data)\n\nDepending on what chart and series you are trying to pull your case might be slightly different.\n', '\nI\'d go a combination of regex and yaml parser.  Quick and dirty below - you may need to tweek the regex but it works with example:\nimport re\nimport sys\nimport yaml\n\nchart_matcher = re.compile(r\'^var (chart[0-9]+) = new Highcharts.Chart\\(({.*?})\\);$\',\n        re.MULTILINE | re.DOTALL)\n\nscript = sys.stdin.read()\n\nm = chart_matcher.findall(script)\n\nfor name, data in m:\n    print name\n    try:\n        chart = yaml.safe_load(data)\n        print ""categories:"", chart[\'xAxis\'][0][\'categories\']\n        print ""data:"", chart[\'series\'][0][\'data\']\n    except Exception, e:\n        print e\n\nRequires the yaml library (pip install PyYAML) and you should use BeautifulSoup to extract the correct <script> tag before passing it to the regex.\nEDIT - full example\nSorry I didn\'t make myself clear.  You use BeautifulSoup to parse the HTML and extract the <script> elements, and then use PyYAML to parse the javascript object declaration.  You can\'t use the built in json library because its not valid JSON but plain javascript object declarations (ie with no functions) are a subset of YAML.\nfrom bs4 import BeautifulSoup\nimport yaml\nimport re\n\nfile_object = open(\'source_test_script.txt\', mode=""r"")\nsoup = BeautifulSoup(file_object, ""html.parser"")\n\npattern = re.compile(r""var (chart[0-9]+) = new Highcharts.Chart\\(({.*?})\\);"", re.MULTILINE | re.DOTALL | re.UNICODE)\n\ncharts = {}\n\n# find every <script> tag in the source using beautifulsoup\nfor tag in soup.find_all(\'script\'):\n\n    # tabs are special in yaml so remove them first\n    script = tag.text.replace(\'\\t\', \'\')\n\n    # find each object declaration\n    for name, obj_declaration in pattern.findall(script):\n        try:\n            # parse the javascript declaration\n            charts[name] = yaml.safe_load(obj_declaration)\n        except Exception, e:\n            print ""Failed to parse {0}: {1}"".format(name, e)\n\n# extract the data you want\nfor name in charts:\n    print ""## {0} ##"".format(name);\n    print ""categories:"", charts[name][\'xAxis\'][0][\'categories\']\n    print ""data:"", charts[name][\'series\'][0][\'data\']\n    print\n\nOutput:\n## chart1 ##\ncategories: [1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]\ndata: [22, 1, 0, 1, 6, 4, 9, 15, 15, 19, 24, 18, 53, 42, 54, 53, 61, 36]\n\nNote I had to tweek the regex to make it handle the unicode output and whitespace from BeautifulSoup - in my original example I just piped your source directly to the regex.\nEDIT 2 - no yaml\nGiven that the javascript looks to be partially generated the best you can hope for is to grab the lines - not elegant but will probably work for you.\nfrom bs4 import BeautifulSoup\nimport json\nimport re\n\nfile_object = open(\'citec.repec.org_p_c_pcl20.html\', mode=""r"")\nsoup = BeautifulSoup(file_object, ""html.parser"")\n\npattern = re.compile(r""var (chart[0-9]+) = new Highcharts.Chart\\(({.*?})\\);"", re.MULTILINE | re.DOTALL | re.UNICODE)\n\ncharts = {}\n\nfor tag in soup.find_all(\'script\'):\n\n    # tabs are special in yaml so remove them first\n    script = tag.text\n\n    values = {}\n\n    # find each object declaration\n    for name, obj_declaration in pattern.findall(script):\n        for line in obj_declaration.split(\'\\n\'):\n            line = line.strip(\'\\t\\n ,;\')\n            for field in (\'data\', \'categories\'):\n                if line.startswith(field + "":""):\n                    data = line[len(field)+1:]\n                    try:\n                        values[field] = json.loads(data)\n                    except:\n                        print ""Failed to parse %r for %s"" % (data, name)\n\n        charts[name] = values\n\nprint charts\n\nNote that it fails for chart7 because that references another variable.\n']",https://stackoverflow.com/questions/39864796/how-to-scrape-charts-from-a-website-with-python,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Screen Scraping of Image Links in PHP,"
I have a website that contains many different pages of products and each page has a certain amount of images in the same format across all pages. I want to be able to screen scrap each page's url so I can retrieve the url of each image from each page. The idea is to make a gallery for each page made up of hotlinked images.
I know this can be done in php, but I am not sure how to scrap the page for multiple links. Any ideas?
",5k,"
            2
        ","[""\nI would recommend using a DOM parser, such as PHP's very own DOMDocument. Example:\n$page = file_get_contents('http://example.com/images.php');\n$doc = new DOMDocument(); \n$doc->loadHTML($page);\n$images = $doc->getElementsByTagName('img'); \nforeach($images as $image) {\n    echo $image->getAttribute('src') . '<br />';\n}\n\n"", '\nYou can use a regular expression (regex) to go through the page source and parse all the IMG tags. \nThis regex will do the job quite nicely: <img[^>]+src=""(.*?)"" \nHow does this work? \n// <img[^>]+src=""(.*?)""\n// \n// Match the characters ""<img"" literally 芦<img禄\n// Match any character that is not a "">"" 芦[^>]+禄\n//    Between one and unlimited times, as many times as possible, giving back as needed (greedy) 芦+禄\n// Match the characters ""src="""" literally 芦src=""禄\n// Match the regular expression below and capture its match into backreference number 1 芦(.*?)禄\n//    Match any single character that is not a line break character 芦.*?禄\n//       Between zero and unlimited times, as few times as possible, expanding as needed (lazy) 芦*?禄\n// Match the character """""" literally 芦""禄\n\nSample PHP code: \npreg_match_all(\'/<img[^>]+src=""(.*?)""/i\', $subject, $result, PREG_PATTERN_ORDER);\nfor ($i = 0; $i < count($result[0]); $i++) {\n    // image URL is in $result[0][$i];\n}\n\nYou\'ll have to do a bit more work to resolve things like relative URLs.\n', ""\nI really like PHP Simple HTML DOM Parser for things like this. An example of grabbing images is right there on the front page:\n// Create DOM from URL or file\n$html = file_get_html('http://www.google.com/');\n\n// Find all images\nforeach($html->find('img') as $element)\n       echo $element->src . '<br>';\n\n"", '\nYou can you this to scrap pages.\nhttp://simplehtmldom.sourceforge.net/\nbut it requires PHP 5+.\n']",https://stackoverflow.com/questions/3261820/screen-scraping-of-image-links-in-php,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What prevents me from using $.ajax to load another domain's html?,"
My domain:
<!DOCTYPE html>  
<html>
<head>
<title>scrape</title>
<script src=""http://code.jquery.com/jquery-1.7.1.min.js""></script>
</head>
<body>
    <script>
        $.ajax({url:'http://their-domain.com/index.html',
        dataType:'html',
            success:function(data){console.log(data);}
        });
    </script>
</body>
</html>

What prevents me from being able to scrape their-domain? Any work around?
Addendum: thank you all for the suggestions to use a server side script, but I am for the moment interested in solving this problem exclusively using the client.
If I format the request using ""jsonp"" I do at least get a response, but with the following error:""Uncaught SyntaxError: Unexpected token <"". So I am getting a response from their-domain but the parser expects it to be json. (As well it should.) I am hacking through this trying to see if their is a way to trick the client into accepting this response. Please understand that I know this is atypical.
<!DOCTYPE html>  
<html>
<head>
<title>scrape</title>
<script src=""http://code.jquery.com/jquery-1.7.1.min.js""></script>
</head>
<body>
    <script>
        $.ajax({url:'http://their-domain.com/index.html',
        dataType:'jsonp',
            success:function(data){console.log(data);}
        });
    </script>
</body>
</html>

",2k,"
            2
        ","[""\nThere are Four ways to get around Same Origin Policy \n\nProxy - You request it from your server, your server requests it from other domain, your server returns it to the browser\nFlash cross domain policy - other domain must add a crossdomain.xml file to their site\nCross domain HTTP header - other domain must add an Access-Control-Allow-Origin header to their page \nJSONP - It's a json web service that provides a callback function.  Other domain must implement this.\n\nNote: The ONLY way to do it without the other domain's help is #1, routing it through your own server.\n"", '\nthe Same Origin Policy prevents client side scripts from getting data from domains that are not from the originator for the request. You would need a server side script to act as a proxy\n', ""\nIt's the Same Origin Policy, which prevents cross-domain requests.  If you want to scrape html, you are better off writing a server side process to get the content, then use ajax to make a request against your server, which contains the harvested data.  \n"", '\nOne workaround is to make a server-side script (eg. PHP) to get the page, and have $.ajax call that.\n']",https://stackoverflow.com/questions/8944656/what-prevents-me-from-using-ajax-to-load-another-domains-html,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Screen scraping web page after delay,"
I'm trying to scrape a web page using C#, however after the page loads, it executes some JavaScript which loads more elements into the DOM which I need to scrape. A standard scraper simply grabs the html of the page on load and doesn't pick up the DOM changes made via JavaScript. How do I put in some sort of functionality to wait for a second or two and then grab the source?
Here is my current code:
private string ScrapeWebpage(string url, DateTime? updateDate)
{
    HttpWebRequest request = null;
    HttpWebResponse response = null;
    Stream responseStream = null;
    StreamReader reader = null;
    string html = null;
    try
    {
        //create request (which supports http compression)
        request = (HttpWebRequest)WebRequest.Create(url);
        request.Pipelined = true;
        request.Headers.Add(HttpRequestHeader.AcceptEncoding, ""gzip,deflate"");
        if (updateDate != null)
            request.IfModifiedSince = updateDate.Value;
        //get response.
        response = (HttpWebResponse)request.GetResponse();
        responseStream = response.GetResponseStream();
        if (response.ContentEncoding.ToLower().Contains(""gzip""))
            responseStream = new GZipStream(responseStream,
                CompressionMode.Decompress);
        else if (response.ContentEncoding.ToLower().Contains(""deflate""))
            responseStream = new DeflateStream(responseStream,
                CompressionMode.Decompress);
        //read html.
        reader = new StreamReader(responseStream, Encoding.Default);
        html = reader.ReadToEnd();
    }
    catch
    {
        throw;
    }
    finally
    {
        //dispose of objects.
        request = null;
        if (response != null)
        {
            response.Close();
            response = null;
        }
        if (responseStream != null)
        {
            responseStream.Close();
            responseStream.Dispose();
        }
        if (reader != null)
        {
            reader.Close();
            reader.Dispose();
        }
    }
    return html;
}

Here's a sample URL:
http://www.realtor.com/realestateandhomes-search/geneva_ny#listingType-any/pg-4
You'll see when the page first loads it says 134 listings found, then after a second it says 187 properties found.
",4k,"
            2
        ","[""\nTo execute the JavaScript I use webkit to render the page, which is the engine used by Chrome and Safari. Here is an example using its Python bindings.\nWebkit also has .NET bindings but I haven't used them.\n"", ""\nThe approach you have will not work regardless how long you wait, you need a browser to execute the javascript (or something that understands javascript).\nTry this question:\nWhat's a good tool to screen-scrape with Javascript support?\n"", '\nYou would need to execute the javascript yourself to get this functionality. Currently, your code only receives whatever the server replies with at the URL you request. The rest of the listings are ""showing up"" because the browser downloads, parses, and executes the accompanying javascript.\n', '\nThe answer to this similar question says to use a web browser control to read the page in and process it before scraping it. Perhaps with some kind of timer delay to give the javascript some time to execute and return results.\n']",https://stackoverflow.com/questions/5636921/screen-scraping-web-page-after-delay,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Why is contains(text(), ""string"" ) not working in XPath?","
I have written this expression //*[contains(text(), ""Brand:"" )] for the below HTML code.


<div class=""info-product mt-3"">
  <h3>Informazioni prodotto</h3>


  Brand: <span class=""brand_title font-weight-bold text-uppercase""><a href=""https://mammapack.com/brand/ava"">Ava</a></span><br> SKU: 8002910009960<br> Peso Lordo: 0.471 kg <br> Dimensioni: 44.00 脳 145.00 脳 153.00 mm<br>

  <p class=""mt-2"">
    AVA BUCATO A MANO E2 GR.380</p>
</div>


The xpath that I have written is not working I want to select Node that contains text Brand:. Can someone tell me my mistake?
",3k,"
            1
        ","['\nYour XPath,\n//*[contains(text(), ""Brand:"")]\n\nin XPath 1.0 will select all elements whose first text node child contains a ""Brand:"" substring.  In XPath 2.0 it is an error to call contains() with a sequence of more than one item as the first argument.\nThis XPath,\n//*[text()[contains(., ""Brand:"")]]\n\nwill select all elements with a text node child whose string value contains a ""Brand:"" substring.\nSee also\n\nXPath 1.0 vs 2.0+ different contains() behavior explanation\nTesting text() nodes vs string values in XPath\n\n']",https://stackoverflow.com/questions/71253563/why-is-containstext-string-not-working-in-xpath,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Why is this HtmlAgilityPack operation invalid when there are, indeed, matching elements?","
I get ""InvalidOperationException > Message=Sequence contains no matching element"" with the following code:
private void buttonLoadHTML_Click(object sender, EventArgs e)
{
    GetParagraphsListFromHtml(@""C:\PlatypiRUs\fitt.html"");
}

// This code adapted from Kirk Woll's answer at 
   http://stackoverflow.com/questions/4752840/html-agility-pack-c-sharp-paragraph-
   parsing-problem
public List<string> GetParagraphsListFromHtml(string sourceHtml)
{
    var pars = new List<string>();
    HtmlAgilityPack.HtmlDocument doc = new HtmlAgilityPack.HtmlDocument();
    doc.LoadHtml(sourceHtml);
    foreach (var par in doc.DocumentNode
        .DescendantNodes()
        .Single(x => x.Id == ""body"")
        .DescendantNodes()
        .Where(x => x.Name == ""p""))
        //.Where(x => x.Name == ""h1"" || x.Name == ""h2"" || x.Name == ""h3"" || x.Name 
           == ""hp"" || )) <-- This is what I'd really like to do, but I don't know if   
           this is possible or, if it is, if the syntax is correct
    {
        pars.Add(par.InnerText);
    }
    // test
    foreach (string s in pars)
    {
        MessageBox.Show(s);
    }
    return pars;
}

Why is the code not finding the paragraphs?
I really want to find all the text (h1..3 or higher vals, too), but this is a start.
BTW: The html file I'm testing with does have some paragraph elements.
UPDATE
In response to Amy's implied request, and in the interest of full disclosure/ultimate illumination, here is the entire test html file:
<style>
body {
    background-color: orange;
    font-family: Verdana, sans-serif;
}

h1 {
    color: Blue;   
    font-family: 'Segoe UI', Verdana, sans-serif;
}

h2 {
    color: white;    
    font-family: 'Palatino Linotype', 'Palatino', sans-serif;
}

h3 {
    display: inline-block;
}
</style>

<h1>Found in the Translation</h1>
<h2>Bilingual Editions of Classic Literature</h2>
<div><label>Contact: </label><a href=""mailto:axx3andspace@gmail.com"">Found in the Translation</a></div>

<h2><cite>Around the World in 80 Days</cite> by Jules Verne (French &amp; English Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495308081"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51BCZUX2-dL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I0DOYRE"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51BCZUX2-dL._SL160_.jpg"" /></a>

<h2><cite>Gulliver's Travels</cite> by Jonathan Swift (English &amp; French Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495374688"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/517O76OyaWL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I5319ZO"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/517O76OyaWL._SL160_.jpg"" /></a>

<h2><cite>Journey to the Center of the Earth</cite> by Jules Verne (French &amp; English Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495409031"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/41hosXOIw8L._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I6LG25M"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/41qj8DfrihL._SL160_.jpg"" /></a>

<h2><cite>Treasure Island</cite> by Robert Louis Stevenson (English &amp; Finnish Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495418936"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51veMV3OiOL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00IA5V4KC"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51XNUWbA07L._SL160_.jpg"" /></a>

<h2><cite>Robinson Crusoe</cite> by Daniel Defoe (English &amp; French Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495448053"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51QQMRPrP9L._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I9IE8OY"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5128hqiw3DL._SL160_.jpg"" /></a>

<h2><cite>Don Quixote</cite> by Miguel de Cervantes Saavedra (Spanish &amp; English Side by Side)</h2>
<h3>Paperback</h3></br>
<h3>Volume I</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/149474967X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51HqjOPXLVL._SL160_.jpg"" /></a>
<h3>Volume II</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1494803445"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51NONygEMYL._SL160_.jpg"" /></a>
<h3>Volume III</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1494841983"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51G%2BW3ICHkL._SL160_.jpg"" /></a></br>
<h3>Kindle</h3></br>
<h3>Volume I</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HQMWPQ2"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51HqjOPXLVL._SL160_.jpg"" /></a>
<h3>Volume II</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HYN2QGM"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51NONygEMYL._SL160_.jpg"" /></a>
<h3>Volume III</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HLX519E"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51G%2BW3ICHkL._SL160_.jpg"" /></a></br>

<h2><cite>Alice's Adventures in Wonderland</cite> by Lewis Carroll (English &amp; German Side by Side)</h2>
<h3>Coming soon; for now, see:</h3></br/>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/193659420X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5143vIpQ2YL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00ESLTIYQ"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51%2BX0Dy7uNL._SL160_.jpg"" /></a>

<h2><cite>Alice's Adventures in Wonderland</cite> by Lewis Carroll (English &amp; Italian Side by Side)</h2>
<h3>Coming soon; for now, see:</h3></br/>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/193659420X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5143vIpQ2YL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00ESLTIYQ"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51%2BX0Dy7uNL._SL160_.jpg"" /></a>

<h2>Other Sites:</h2>
<p><a href=""http://usamaporama.azurewebsites.net/""  target=""_blank"">USA Map-O-Rama</a></p>
<p><a href=""http://www.awardwinnersonly.com/""  target=""_blank"">Award-winning Movies, Books, and Music</a></p>
<p><a href=""http://www.bigsurgarrapata.com/""  target=""_blank"">Garrapata State Park in Big Sur Throughout the Seasons</a></p>

UPDATE 2
This works (although it is with ""live"" web pages, and not html files saved to disk):
public List<string> GetParagraphsListFromHtml(string sourceHtml)
{
    var pars = new List<string>();
    HtmlAgilityPack.HtmlDocument doc = new HtmlAgilityPack.HtmlDocument();
    doc.LoadHtml(sourceHtml);

    var getHtmlWeb = new HtmlWeb();
    var document = getHtmlWeb.Load(""http://www.montereycountyweekly.com/opinion/letters/article_e333a222-942d-11e3-ba9c-001a4bcf6878.html""); 
    //http://www.bigsurgarrapata.com/ only returned one paragraph
    // http://usamaporama.azurewebsites.net/ <-- none
    // http://www.awardwinnersonly.com/ <- same as bigsurgarrapata
    var pTags = document.DocumentNode.SelectNodes(""//p"");
    int counter = 1;
    if (pTags != null)
    {
        foreach (var pTag in pTags)
        {
            pars.Add(pTag.InnerText);
            MessageBox.Show(pTag.InnerText);
            counter++;
        }
    }
    MessageBox.Show(""done!"");
    return pars;
}

",331,"
            0
        ","['\nIt turns out to be pretty easy; this is not complete, but this, inspired by this answer, is enough to get started:\nHtmlAgilityPack.HtmlDocument htmlDoc = new HtmlAgilityPack.HtmlDocument();\n\n// There are various options, set as needed\nhtmlDoc.OptionFixNestedTags = true;\n\nhtmlDoc.Load(@""C:\\Platypus\\dplatypus.htm"");\n\nif (htmlDoc.DocumentNode != null)\n{\n    IEnumerable<HtmlAgilityPack.HtmlNode> textNodes = htmlDoc.DocumentNode.SelectNodes(""//text()"");\n    foreach (HtmlNode node in textNodes)\n    {\n        if (!string.IsNullOrWhiteSpace(node.InnerText))\n        {\n            MessageBox.Show(node.InnerText);\n        }\n    }\n}\n\n']",https://stackoverflow.com/questions/21788078/why-is-this-htmlagilitypack-operation-invalid-when-there-are-indeed-matching-e,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parse a .Net Page with Postbacks,"
I need to read data from an online database that's displayed using an aspx page from the UN. I've done HTML parsing before, but it was always by manipulating query-string values. In this case, the site uses asp.net postbacks. So, you click on a value in box one, then box two shows, click on a value in box 2 and click a button to get your results.
Does anybody know how I could automate that process? 
Thanks,
Mike
",2k,"
            0
        ","[""\nYou may still only need to send one request, but that one request can be rather complicated.  ASP.Net is notoriously difficult (though not impossible) to screen scrape.  Between event validation and the ViewState, it's tricky to get your requests just right.  The simplest way to do it is often to use a sniffer tool like fiddler to see exactly what the http request looks like, and then just mimic that request.\nIf you do still need to send two requests, it's because the first request also places some state in a session somewhere, and that means whatever you use to send those requests needs to be able to send them with the same session.  This often means supporting cookies.\n"", '\nWatin would be my first choice.  You would code the selecting and clicking, then parse the HTML after.\n', ""\nI'd look at HtmlAgilityPack with the FormProcessor addon.\n""]",https://stackoverflow.com/questions/1245782/parse-a-net-page-with-postbacks,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Options for web scraping - C++ version only,"
I'm looking for a good C++ library for web scraping.
It has to be C/C++ and nothing else so please do not direct me to Options for HTML scraping or other SO questions/answers where C++ is not even mentioned.
",60k,"
            46
        ","['\n\nlibcurl to download the html file\nlibtidy to convert to valid xml\nlibxml to parse/navigate the xml\n\n', '\nUse myhtml C/C++ parser here; dead simple, very fast. No dependencies except C99. And has CSS selectors built in (example here)\n', '\nI recommend Qt5.6.2, this powerful library offer us\n\nHigh level, intuitive, asynchronous network api like QNetworkAccessManager, QNetworkReply, QNetworkProxy etc\nPowerful regex class like QRegularExpression\nDecent web engine like QtWebEngine\nRobust, mature gui like QWidgets\nMost of the Qt5 api are well designed, signal and slot make writing asynchronous codes become much easier too\nGreat unicode support\nFeature rich file system library. Whether create, remove, rename or find standard path to save files is piece of cake in Qt5\nAsynchronous api of QNetworkAccessManager make it easy to spawn many download request at once\nCross major desktop platforms, windows, mac os and linux, write once compiled anywhere, one code bases only.\nEasy to deploy on windows and mac(linux?maybe linuxdeployqt can save us tons of troubles)    \nEasy to install on windows, mac and linux\nAnd so on\n\nI already wrote an image scraper apps by Qt5, this app can scrape almost every image searched by Google, Bing and Yahoo. \nTo know more details about it, please visit my github project.\nI wrote down high level overview about how to scrape data by Qt5 on \nmy blogs(it is too long to post at stack overflow).\n\nDownload Bing images by Qt5\nCreate a better images downloader(Google, Bing and Yahoo) by Qt5\n\n', '\n// download winhttpclient.h\n// --------------------------------\n#include <winhttp\\WinHttpClient.h>\nusing namespace std;\ntypedef unsigned char byte;\n#define foreach         BOOST_FOREACH\n#define reverse_foreach BOOST_REVERSE_FOREACH\n\nbool substrexvealue(const std::wstring& html,const std::string& tg1,const std::string& tg2,std::string& value, long& next) {\n    long p1,p2;\n    std::wstring wtmp;\n    std::wstring wtg1(tg1.begin(),tg1.end());\n    std::wstring wtg2(tg2.begin(),tg2.end());\n\n    p1=html.find(wtg1,next);\n    if(p1!=std::wstring::npos) {\n        p2=html.find(wtg2,next);\n        if(p2!=std::wstring::npos) {\n            p1+=wtg1.size();\n            wtmp=html.substr(p1,p2-p1-1);\n            value=std::string(wtmp.begin(),wtmp.end());\n            boost::trim(value);\n            next=p1+1;\n        }\n    }\n    return p1!=std::wstring::npos;\n}\nbool extractvalue(const std::wstring& html,const std::string& tag,std::string& value, long& next) {\n    long p1,p2,p3;\n    std::wstring wtmp;\n    std::wstring wtag(tag.begin(),tag.end());\n\n    p1=html.find(wtag,next);\n    if(p1!=std::wstring::npos) {\n        p2=html.find(L"">"",p1+wtag.size()-1);\n        p3=html.find(L""<"",p2+1);\n        wtmp=html.substr(p2+1,p3-p2-1);\n        value=std::string(wtmp.begin(),wtmp.end());\n        boost::trim(value);\n        next=p1+1;\n    }\n    return p1!=std::wstring::npos;\n}\nbool GetHTML(const std::string& url,std::wstring& header,std::wstring& hmtl) {\n    std::wstring wurl = std::wstring(url.begin(),url.end());\n    bool ret=false;\n    try {\n        WinHttpClient client(wurl.c_str());\n        std::string url_protocol=url.substr(0,5);\n        std::transform(url_protocol.begin(), url_protocol.end(), url_protocol.begin(), (int (*)(int))std::toupper);\n        if(url_protocol==""HTTPS"")    client.SetRequireValidSslCertificates(false);\n        client.SetUserAgent(L""User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:19.0) Gecko/20100101 Firefox/19.0"");\n        if(client.SendHttpRequest()) {\n            header = client.GetResponseHeader();\n            hmtl = client.GetResponseContent();\n            ret=true;\n        }\n    }catch(...) {\n        header=L""Error"";\n        hmtl=L"""";\n    }\n    return ret;\n}\nint main() {\n    std::string url = ""http://www.google.fr"";\n    std::wstring header,html;\n    GetHTML(url,header,html));\n}\n\n']",https://stackoverflow.com/questions/834768/options-for-web-scraping-c-version-only,screen-scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TypeError: can't use a string pattern on a bytes-like object in re.findall(),"
I am trying to learn how to automatically fetch urls from a page. In the following code I am trying to get the title of the webpage:
import urllib.request
import re

url = ""http://www.google.com""
regex = r'<title>(,+?)</title>'
pattern  = re.compile(regex)

with urllib.request.urlopen(url) as response:
   html = response.read()

title = re.findall(pattern, html)
print(title)

And I get this unexpected error:
Traceback (most recent call last):
  File ""path\to\file\Crawler.py"", line 11, in <module>
    title = re.findall(pattern, html)
  File ""C:\Python33\lib\re.py"", line 201, in findall
    return _compile(pattern, flags).findall(string)
TypeError: can't use a string pattern on a bytes-like object

What am I doing wrong?
",276k,"
            164
        ","[""\nYou want to convert html (a byte-like object) into a string using .decode, e.g.  html = response.read().decode('utf-8'). \nSee Convert bytes to a Python String\n"", ""\nThe problem is that your regex is a string, but html is bytes:\n>>> type(html)\n<class 'bytes'>\n\nSince python doesn't know how those bytes are encoded, it throws an exception when you try to use a string regex on them.\nYou can either decode the bytes to a string:\nhtml = html.decode('ISO-8859-1')  # encoding may vary!\ntitle = re.findall(pattern, html)  # no more error\n\nOr use a bytes regex:\nregex = rb'<title>(,+?)</title>'\n#        ^\n\n\nIn this particular context, you can get the encoding from the response headers:\nwith urllib.request.urlopen(url) as response:\n    encoding = response.info().get_param('charset', 'utf8')\n    html = response.read().decode(encoding)\n\nSee the urlopen documentation for more details.\n"", ""\nBased upon last one, this was smimple to do when pdf read was done .\ntext = text.decode('ISO-8859-1') \n\nThanks @Aran-fey\n""]",https://stackoverflow.com/questions/31019854/typeerror-cant-use-a-string-pattern-on-a-bytes-like-object-in-re-findall,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Sending ""User-agent"" using Requests library in Python","
I want to send a value for ""User-agent"" while requesting a webpage using Python Requests.  I am not sure is if it is okay to send this as a part of the header, as in the code below:
debug = {'verbose': sys.stderr}
user_agent = {'User-agent': 'Mozilla/5.0'}
response  = requests.get(url, headers = user_agent, config=debug)

The debug information isn't showing the headers being sent during the request.
Is it acceptable to send this information in the header?  If not, how can I send it?
",441k,"
            318
        ","[""\nThe user-agent should be specified as a field in the header.\nHere is a list of HTTP header fields, and you'd probably be interested in request-specific fields, which includes User-Agent.\nIf you're using requests v2.13 and newer\nThe simplest way to do what you want is to create a dictionary and specify your headers directly, like so:\nimport requests\n\nurl = 'SOME URL'\n\nheaders = {\n    'User-Agent': 'My User Agent 1.0',\n    'From': 'youremail@domain.example'  # This is another valid field\n}\n\nresponse = requests.get(url, headers=headers)\n\nIf you're using requests v2.12.x and older\nOlder versions of requests clobbered default headers, so you'd want to do the following to preserve default headers and then add your own to them.\nimport requests\n\nurl = 'SOME URL'\n\n# Get a copy of the default headers that requests would use\nheaders = requests.utils.default_headers()\n\n# Update the headers with your custom ones\n# You don't have to worry about case-sensitivity with\n# the dictionary keys, because default_headers uses a custom\n# CaseInsensitiveDict implementation within requests' source code.\nheaders.update(\n    {\n        'User-Agent': 'My User Agent 1.0',\n    }\n)\n\nresponse = requests.get(url, headers=headers)\n\n"", ""\nIt's more convenient to use a session, this way you don't have to remember to set headers each time:\nsession = requests.Session()\nsession.headers.update({'User-Agent': 'Custom user agent'})\n\nsession.get('https://httpbin.org/headers')\n\nBy default, session also manages cookies for you. In case you want to disable that, see this question.\n"", ""\nIt will send the request like browser\nimport requests\n\nurl = 'https://Your-url'\nheaders={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}\n\nresponse= requests.get(url.strip(), headers=headers, timeout=10)\n\n""]",https://stackoverflow.com/questions/10606133/sending-user-agent-using-requests-library-in-python,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I scrape pages with dynamic content using node.js?,"
I am trying to scrape a website but I don't get some of the elements, because these elements are dynamically created.
I use the cheerio in node.js and My code is below.
var request = require('request');
var cheerio = require('cheerio');
var url = ""http://www.bdtong.co.kr/index.php?c_category=C02"";

request(url, function (err, res, html) {
    var $ = cheerio.load(html);
    $('.listMain > li').each(function () {
        console.log($(this).find('a').attr('href'));
    });
});

This code returns empty response, because when the page is loaded, the <ul id=""store_list"" class=""listMain""> is empty. 
The content has not been appended yet. 
How can I get these elements using node.js? How can I scrape pages with dynamic content?
",33k,"
            30
        ","['\nHere you go;\nvar phantom = require(\'phantom\');\n\nphantom.create(function (ph) {\n  ph.createPage(function (page) {\n    var url = ""http://www.bdtong.co.kr/index.php?c_category=C02"";\n    page.open(url, function() {\n      page.includeJs(""http://ajax.googleapis.com/ajax/libs/jquery/1.6.1/jquery.min.js"", function() {\n        page.evaluate(function() {\n          $(\'.listMain > li\').each(function () {\n            console.log($(this).find(\'a\').attr(\'href\'));\n          });\n        }, function(){\n          ph.exit()\n        });\n      });\n    });\n  });\n});\n\n', ""\nCheck out GoogleChrome/puppeteer\n\nHeadless Chrome Node API\n\nIt makes scraping pretty trivial. The following example will scrape the headline over at npmjs.com (assuming .npm-expansions remains)\nconst puppeteer = require('puppeteer');\n\n(async () => {\n  const browser = await puppeteer.launch();\n  const page = await browser.newPage();\n\n  await page.goto('https://www.npmjs.com/');\n\n  const textContent = await page.evaluate(() => {\n    return document.querySelector('.npm-expansions').textContent\n  });\n\n  console.log(textContent); /* No Problem Mate */\n\n  browser.close();\n})();\n\nevaluate will allow for the inspection of the dynamic element as this will run scripts on the page.\n"", ""\nUse the new npm module x-ray, with a pluggable web driver x-ray-phantom.\nExamples in the pages above, but here's how to do dynamic scraping:\nvar phantom = require('x-ray-phantom');\nvar Xray = require('x-ray');\n\nvar x = Xray()\n  .driver(phantom());\n\nx('http://google.com', 'title')(function(err, str) {\n  if (err) return done(err);\n  assert.equal('Google', str);\n  done();\n})\n\n"", '\nAnswering this as a canonical, an alternative to Puppeteer for scraping dynamic sites which is also well-supported as of 2023 is Playwright. Here\'s a simple example:\nconst playwright = require(""playwright""); // ^1.28.1\n\nlet browser;\n(async () => {\n  browser = await playwright.chromium.launch();\n  const page = await browser.newPage();\n  await page.goto(""https://example.com"");\n  const text = await page.locator(\'h1:text(""Example"")\').textContent();\n  console.log(text); // => Example Domain\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close());\n\n', '\nEasiest and reliable solution is to use puppeteer. As mentioned in https://pusher.com/tutorials/web-scraper-node which is suitable for both static + dynamic scraping.\nOnly change the timeout in Browser.js, TimeoutSettings.js, Launcher.js 300000 to 3000000\n']",https://stackoverflow.com/questions/28739098/how-can-i-scrape-pages-with-dynamic-content-using-node-js,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
how to detect search engine bots with php?,"
How can one detect the search engine bots using php?
",153k,"
            144
        ","[""\nI use the following code which seems to be working fine:\nfunction _bot_detected() {\n\n  return (\n    isset($_SERVER['HTTP_USER_AGENT'])\n    && preg_match('/bot|crawl|slurp|spider|mediapartners/i', $_SERVER['HTTP_USER_AGENT'])\n  );\n}\n\nupdate 16-06-2017 \nhttps://support.google.com/webmasters/answer/1061943?hl=en\nadded mediapartners\n"", '\nHere\'s a Search Engine Directory of Spider names\nThen you use $_SERVER[\'HTTP_USER_AGENT\']; to check if the agent is said spider.\nif(strstr(strtolower($_SERVER[\'HTTP_USER_AGENT\']), ""googlebot""))\n{\n    // what to do\n}\n\n', ""\nCheck the $_SERVER['HTTP_USER_AGENT'] for some of the strings listed here:\nhttp://www.useragentstring.com/pages/useragentstring.php\nOr more specifically for crawlers:\nhttp://www.useragentstring.com/pages/useragentstring.php?typ=Crawler\nIf you want to -say- log the number of visits of most common search engine crawlers, you could use\n$interestingCrawlers = array( 'google', 'yahoo' );\n$pattern = '/(' . implode('|', $interestingCrawlers) .')/';\n$matches = array();\n$numMatches = preg_match($pattern, strtolower($_SERVER['HTTP_USER_AGENT']), $matches, 'i');\nif($numMatches > 0) // Found a match\n{\n  // $matches[1] contains an array of all text matches to either 'google' or 'yahoo'\n}\n\n"", '\nYou can checkout if it\'s a search engine with this function :\n<?php\nfunction crawlerDetect($USER_AGENT)\n{\n$crawlers = array(\n\'Google\' => \'Google\',\n\'MSN\' => \'msnbot\',\n      \'Rambler\' => \'Rambler\',\n      \'Yahoo\' => \'Yahoo\',\n      \'AbachoBOT\' => \'AbachoBOT\',\n      \'accoona\' => \'Accoona\',\n      \'AcoiRobot\' => \'AcoiRobot\',\n      \'ASPSeek\' => \'ASPSeek\',\n      \'CrocCrawler\' => \'CrocCrawler\',\n      \'Dumbot\' => \'Dumbot\',\n      \'FAST-WebCrawler\' => \'FAST-WebCrawler\',\n      \'GeonaBot\' => \'GeonaBot\',\n      \'Gigabot\' => \'Gigabot\',\n      \'Lycos spider\' => \'Lycos\',\n      \'MSRBOT\' => \'MSRBOT\',\n      \'Altavista robot\' => \'Scooter\',\n      \'AltaVista robot\' => \'Altavista\',\n      \'ID-Search Bot\' => \'IDBot\',\n      \'eStyle Bot\' => \'eStyle\',\n      \'Scrubby robot\' => \'Scrubby\',\n      \'Facebook\' => \'facebookexternalhit\',\n  );\n  // to get crawlers string used in function uncomment it\n  // it is better to save it in string than use implode every time\n  // global $crawlers\n   $crawlers_agents = implode(\'|\',$crawlers);\n  if (strpos($crawlers_agents, $USER_AGENT) === false)\n      return false;\n    else {\n    return TRUE;\n    }\n}\n?>\n\nThen you can use it like :\n<?php $USER_AGENT = $_SERVER[\'HTTP_USER_AGENT\'];\n  if(crawlerDetect($USER_AGENT)) return ""no need to lang redirection"";?>\n\n', ""\nI'm using this to detect bots:\nif (preg_match('/bot|crawl|curl|dataprovider|search|get|spider|find|java|majesticsEO|google|yahoo|teoma|contaxe|yandex|libwww-perl|facebookexternalhit/i', $_SERVER['HTTP_USER_AGENT'])) {\n    // is bot\n}\n\nIn addition I use a whitelist to block unwanted bots:\nif (preg_match('/apple|baidu|bingbot|facebookexternalhit|googlebot|-google|ia_archiver|msnbot|naverbot|pingdom|seznambot|slurp|teoma|twitter|yandex|yeti/i', $_SERVER['HTTP_USER_AGENT'])) {\n    // allowed bot\n}\n\nAn unwanted bot (= false-positive user) is then able to solve a captcha to unblock himself for 24 hours. And as no one solves this captcha, I know it does not produce false-positives. So the bot detection seem to work perfectly.\nNote: My whitelist is based on Facebooks robots.txt.\n"", ""\nBecause any client can set the user-agent to what they want, looking for 'Googlebot', 'bingbot' etc is only half the job.\nThe 2nd part is verifying the client's IP. In the old days this required maintaining IP lists. All the lists you find online are outdated. The top search engines officially support verification through DNS, as explained by Google https://support.google.com/webmasters/answer/80553 and Bing http://www.bing.com/webmaster/help/how-to-verify-bingbot-3905dc26\nAt first perform a reverse DNS lookup of the client IP. For Google this brings a host name under googlebot.com, for Bing it's under search.msn.com. Then, because someone could set such a reverse DNS on his IP, you need to verify with a forward DNS lookup on that hostname. If the resulting IP is the same as the one of the site's visitor, you're sure it's a crawler from that search engine.\nI've written a library in Java that performs these checks for you. Feel free to port it to PHP. It's on GitHub: https://github.com/optimaize/webcrawler-verifier\n"", '\nIf you really need to detect GOOGLE engine bots you should never rely on ""user_agent"" or ""IP"" address because ""user_agent"" can be changed  and acording to what google said in: Verifying Googlebot\n\nTo verify Googlebot as the caller:\n1.Run a reverse DNS lookup on the accessing IP address from your logs, using the host command.\n2.Verify that the domain name is in either googlebot.com or google.com\n3.Run a forward DNS lookup on the domain name retrieved in step 1 using the host command on the retrieved domain name. Verify that it is the same as the original accessing IP address from your logs.\n\nHere is my tested code :\n<?php\n$remote_add=$_SERVER[\'REMOTE_ADDR\'];\n$hostname = gethostbyaddr($remote_add);\n$googlebot = \'googlebot.com\';\n$google = \'google.com\';\nif (stripos(strrev($hostname), strrev($googlebot)) === 0 or stripos(strrev($hostname),strrev($google)) === 0 ) \n{\n//add your code\n}\n\n?>\n\nIn this code we check ""hostname"" which should contain ""googlebot.com"" or ""google.com"" at the end of ""hostname"" which is really important to check exact domain not subdomain.\nI hope you enjoy ;)\n', ""\nI use this function ... part of the regex comes from prestashop but I added some more bot to it.   \n    public function isBot()\n{\n    $bot_regex = '/BotLink|bingbot|AhrefsBot|ahoy|AlkalineBOT|anthill|appie|arale|araneo|AraybOt|ariadne|arks|ATN_Worldwide|Atomz|bbot|Bjaaland|Ukonline|borg\\-bot\\/0\\.9|boxseabot|bspider|calif|christcrawler|CMC\\/0\\.01|combine|confuzzledbot|CoolBot|cosmos|Internet Cruiser Robot|cusco|cyberspyder|cydralspider|desertrealm, desert realm|digger|DIIbot|grabber|downloadexpress|DragonBot|dwcp|ecollector|ebiness|elfinbot|esculapio|esther|fastcrawler|FDSE|FELIX IDE|ESI|fido|H锟絤锟絟锟絢ki|KIT\\-Fireball|fouineur|Freecrawl|gammaSpider|gazz|gcreep|golem|googlebot|griffon|Gromit|gulliver|gulper|hambot|havIndex|hotwired|htdig|iajabot|INGRID\\/0\\.1|Informant|InfoSpiders|inspectorwww|irobot|Iron33|JBot|jcrawler|Teoma|Jeeves|jobo|image\\.kapsi\\.net|KDD\\-Explorer|ko_yappo_robot|label\\-grabber|larbin|legs|Linkidator|linkwalker|Lockon|logo_gif_crawler|marvin|mattie|mediafox|MerzScope|NEC\\-MeshExplorer|MindCrawler|udmsearch|moget|Motor|msnbot|muncher|muninn|MuscatFerret|MwdSearch|sharp\\-info\\-agent|WebMechanic|NetScoop|newscan\\-online|ObjectsSearch|Occam|Orbsearch\\/1\\.0|packrat|pageboy|ParaSite|patric|pegasus|perlcrawler|phpdig|piltdownman|Pimptrain|pjspider|PlumtreeWebAccessor|PortalBSpider|psbot|Getterrobo\\-Plus|Raven|RHCS|RixBot|roadrunner|Robbie|robi|RoboCrawl|robofox|Scooter|Search\\-AU|searchprocess|Senrigan|Shagseeker|sift|SimBot|Site Valet|skymob|SLCrawler\\/2\\.0|slurp|ESI|snooper|solbot|speedy|spider_monkey|SpiderBot\\/1\\.0|spiderline|nil|suke|http:\\/\\/www\\.sygol\\.com|tach_bw|TechBOT|templeton|titin|topiclink|UdmSearch|urlck|Valkyrie libwww\\-perl|verticrawl|Victoria|void\\-bot|Voyager|VWbot_K|crawlpaper|wapspider|WebBandit\\/1\\.0|webcatcher|T\\-H\\-U\\-N\\-D\\-E\\-R\\-S\\-T\\-O\\-N\\-E|WebMoose|webquest|webreaper|webs|webspider|WebWalker|wget|winona|whowhere|wlm|WOLP|WWWC|none|XGET|Nederland\\.zoek|AISearchBot|woriobot|NetSeer|Nutch|YandexBot|YandexMobileBot|SemrushBot|FatBot|MJ12bot|DotBot|AddThis|baiduspider|SeznamBot|mod_pagespeed|CCBot|openstat.ru\\/Bot|m2e/i';\n    $userAgent = empty($_SERVER['HTTP_USER_AGENT']) ? FALSE : $_SERVER['HTTP_USER_AGENT'];\n    $isBot = !$userAgent || preg_match($bot_regex, $userAgent);\n\n    return $isBot;\n}\n\nAnyway take care that some bots uses browser like user agent to fake their identity\n ( I got many russian ip that has this behaviour on my site )\nOne distinctive feature of most of the bot is that they don't carry any cookie and so no session is attached to them.\n( I am not sure how but this is for sure the best way to track them ) \n"", ""\nYou could analyse the user agent ($_SERVER['HTTP_USER_AGENT']) or compare the client鈥檚 IP address ($_SERVER['REMOTE_ADDR']) with a list of IP addresses of search engine bots.\n"", '\nUse Device Detector open source library, it offers a isBot() function: https://github.com/piwik/device-detector\n', ""\nI made one good and fast function for this\nfunction is_bot(){\n\n        if(isset($_SERVER['HTTP_USER_AGENT']))\n        {\n            return preg_match('/rambler|abacho|acoi|accona|aspseek|altavista|estyle|scrubby|lycos|geona|ia_archiver|alexa|sogou|skype|facebook|twitter|pinterest|linkedin|naver|bing|google|yahoo|duckduckgo|yandex|baidu|teoma|xing|java\\/1.7.0_45|bot|crawl|slurp|spider|mediapartners|\\sask\\s|\\saol\\s/i', $_SERVER['HTTP_USER_AGENT']);\n        }\n\n        return false;\n    }\n\nThis cover 99% of all possible bots, search engines etc.\n"", '\n <?php // IPCLOACK HOOK\nif (CLOAKING_LEVEL != 4) {\n    $lastupdated = date(""Ymd"", filemtime(FILE_BOTS));\n    if ($lastupdated != date(""Ymd"")) {\n        $lists = array(\n        \'http://labs.getyacg.com/spiders/google.txt\',\n        \'http://labs.getyacg.com/spiders/inktomi.txt\',\n        \'http://labs.getyacg.com/spiders/lycos.txt\',\n        \'http://labs.getyacg.com/spiders/msn.txt\',\n        \'http://labs.getyacg.com/spiders/altavista.txt\',\n        \'http://labs.getyacg.com/spiders/askjeeves.txt\',\n        \'http://labs.getyacg.com/spiders/wisenut.txt\',\n        );\n        foreach($lists as $list) {\n            $opt .= fetch($list);\n        }\n        $opt = preg_replace(""/(^[\\r\\n]*|[\\r\\n]+)[\\s\\t]*[\\r\\n]+/"", ""\\n"", $opt);\n        $fp =  fopen(FILE_BOTS,""w"");\n        fwrite($fp,$opt);\n        fclose($fp);\n    }\n    $ip = isset($_SERVER[\'REMOTE_ADDR\']) ? $_SERVER[\'REMOTE_ADDR\'] : \'\';\n    $ref = isset($_SERVER[\'HTTP_REFERER\']) ? $_SERVER[\'HTTP_REFERER\'] : \'\';\n    $agent = isset($_SERVER[\'HTTP_USER_AGENT\']) ? $_SERVER[\'HTTP_USER_AGENT\'] : \'\';\n    $host = strtolower(gethostbyaddr($ip));\n    $file = implode("" "", file(FILE_BOTS));\n    $exp = explode(""."", $ip);\n    $class = $exp[0].\'.\'.$exp[1].\'.\'.$exp[2].\'.\';\n    $threshold = CLOAKING_LEVEL;\n    $cloak = 0;\n    if (stristr($host, ""googlebot"") && stristr($host, ""inktomi"") && stristr($host, ""msn"")) {\n        $cloak++;\n    }\n    if (stristr($file, $class)) {\n        $cloak++;\n    }\n    if (stristr($file, $agent)) {\n        $cloak++;\n    }\n    if (strlen($ref) > 0) {\n        $cloak = 0;\n    }\n\n    if ($cloak >= $threshold) {\n        $cloakdirective = 1;\n    } else {\n        $cloakdirective = 0;\n    }\n}\n?>\n\nThat would be the ideal way to cloak for spiders. It\'s from an open source script called [YACG] - http://getyacg.com\nNeeds a bit of work, but definitely the way to go.\n', ""\n100% Working Bot detector. It is working on my website successfully.\nfunction isBotDetected() {\n\n    if ( preg_match('/abacho|accona|AddThis|AdsBot|ahoy|AhrefsBot|AISearchBot|alexa|altavista|anthill|appie|applebot|arale|araneo|AraybOt|ariadne|arks|aspseek|ATN_Worldwide|Atomz|baiduspider|baidu|bbot|bingbot|bing|Bjaaland|BlackWidow|BotLink|bot|boxseabot|bspider|calif|CCBot|ChinaClaw|christcrawler|CMC\\/0\\.01|combine|confuzzledbot|contaxe|CoolBot|cosmos|crawler|crawlpaper|crawl|curl|cusco|cyberspyder|cydralspider|dataprovider|digger|DIIbot|DotBot|downloadexpress|DragonBot|DuckDuckBot|dwcp|EasouSpider|ebiness|ecollector|elfinbot|esculapio|ESI|esther|eStyle|Ezooms|facebookexternalhit|facebook|facebot|fastcrawler|FatBot|FDSE|FELIX IDE|fetch|fido|find|Firefly|fouineur|Freecrawl|froogle|gammaSpider|gazz|gcreep|geona|Getterrobo-Plus|get|girafabot|golem|googlebot|\\-google|grabber|GrabNet|griffon|Gromit|gulliver|gulper|hambot|havIndex|hotwired|htdig|HTTrack|ia_archiver|iajabot|IDBot|Informant|InfoSeek|InfoSpiders|INGRID\\/0\\.1|inktomi|inspectorwww|Internet Cruiser Robot|irobot|Iron33|JBot|jcrawler|Jeeves|jobo|KDD\\-Explorer|KIT\\-Fireball|ko_yappo_robot|label\\-grabber|larbin|legs|libwww-perl|linkedin|Linkidator|linkwalker|Lockon|logo_gif_crawler|Lycos|m2e|majesticsEO|marvin|mattie|mediafox|mediapartners|MerzScope|MindCrawler|MJ12bot|mod_pagespeed|moget|Motor|msnbot|muncher|muninn|MuscatFerret|MwdSearch|NationalDirectory|naverbot|NEC\\-MeshExplorer|NetcraftSurveyAgent|NetScoop|NetSeer|newscan\\-online|nil|none|Nutch|ObjectsSearch|Occam|openstat.ru\\/Bot|packrat|pageboy|ParaSite|patric|pegasus|perlcrawler|phpdig|piltdownman|Pimptrain|pingdom|pinterest|pjspider|PlumtreeWebAccessor|PortalBSpider|psbot|rambler|Raven|RHCS|RixBot|roadrunner|Robbie|robi|RoboCrawl|robofox|Scooter|Scrubby|Search\\-AU|searchprocess|search|SemrushBot|Senrigan|seznambot|Shagseeker|sharp\\-info\\-agent|sift|SimBot|Site Valet|SiteSucker|skymob|SLCrawler\\/2\\.0|slurp|snooper|solbot|speedy|spider_monkey|SpiderBot\\/1\\.0|spiderline|spider|suke|tach_bw|TechBOT|TechnoratiSnoop|templeton|teoma|titin|topiclink|twitterbot|twitter|UdmSearch|Ukonline|UnwindFetchor|URL_Spider_SQL|urlck|urlresolver|Valkyrie libwww\\-perl|verticrawl|Victoria|void\\-bot|Voyager|VWbot_K|wapspider|WebBandit\\/1\\.0|webcatcher|WebCopier|WebFindBot|WebLeacher|WebMechanic|WebMoose|webquest|webreaper|webspider|webs|WebWalker|WebZip|wget|whowhere|winona|wlm|WOLP|woriobot|WWWC|XGET|xing|yahoo|YandexBot|YandexMobileBot|yandex|yeti|Zeus/i', $_SERVER['HTTP_USER_AGENT'])\n    ) {\n        return true; // 'Above given bots detected'\n    }\n\n    return false;\n\n} // End :: isBotDetected()\n\n"", '\nI\'m using this code, pretty good. You will very easy to know user-agents visitted your site. This code is opening a file and write the user_agent down the file. You can check each day this file by go to yourdomain.com/useragent.txt and know about new user_agents and put them in your condition of if clause.\n$user_agent = strtolower($_SERVER[\'HTTP_USER_AGENT\']);\nif(!preg_match(""/Googlebot|MJ12bot|yandexbot/i"", $user_agent)){\n    // if not meet the conditions then\n    // do what you need\n\n    // here open a file and write the user_agent down the file. You can check each day this file useragent.txt and know about new user_agents and put them in your condition of if clause\n    if($user_agent!=""""){\n        $myfile = fopen(""useragent.txt"", ""a"") or die(""Unable to open file useragent.txt!"");\n        fwrite($myfile, $user_agent);\n        $user_agent = ""\\n"";\n        fwrite($myfile, $user_agent);\n        fclose($myfile);\n    }\n}\n\nThis is the content of useragent.txt\nMozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\nMozilla/5.0 (compatible; MJ12bot/v1.4.6; http://mj12bot.com/)Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\nMozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.96 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)mozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (iphone; cpu iphone os 9_3 like mac os x) applewebkit/601.1.46 (khtml, like gecko) version/9.0 mobile/13e198 safari/601.1\nmozilla/5.0 (windows nt 6.1; wow64) applewebkit/537.36 (khtml, like gecko) chrome/53.0.2785.143 safari/537.36\nmozilla/5.0 (compatible; linkdexbot/2.2; +http://www.linkdex.com/bots/)\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64) applewebkit/537.36 (khtml, like gecko) chrome/53.0.2785.143 safari/537.36\nmozilla/5.0 (windows nt 6.1; wow64) applewebkit/537.36 (khtml, like gecko) chrome/53.0.2785.143 safari/537.36\nmozilla/5.0 (compatible; baiduspider/2.0; +http://www.baidu.com/search/spider.html)\nzoombot (linkbot 1.0 http://suite.seozoom.it/bot.html)\nmozilla/5.0 (windows nt 10.0; wow64) applewebkit/537.36 (khtml, like gecko) chrome/44.0.2403.155 safari/537.36 opr/31.0.1889.174\nmozilla/5.0 (windows nt 10.0; wow64) applewebkit/537.36 (khtml, like gecko) chrome/44.0.2403.155 safari/537.36 opr/31.0.1889.174\nsogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)\nmozilla/5.0 (windows nt 10.0; wow64) applewebkit/537.36 (khtml, like gecko) chrome/44.0.2403.155 safari/537.36 opr/31.0.1889.174\n\n', ""\nFor Google i'm using this method.\nfunction is_google() {\n    $ip   = $_SERVER['REMOTE_ADDR'];\n    $host = gethostbyaddr( $ip );\n    if ( strpos( $host, '.google.com' ) !== false || strpos( $host, '.googlebot.com' ) !== false ) {\n\n        $forward_lookup = gethostbyname( $host );\n\n        if ( $forward_lookup == $ip ) {\n            return true;\n        }\n\n        return false;\n    } else {\n        return false;\n    }\n\n}\n\nvar_dump( is_google() );\n\nCredits: https://support.google.com/webmasters/answer/80553\n"", '\nVerifying Googlebot\nAs useragent can be changed...\n\nthe only official supported way to identify a google bot is to run a\nreverse DNS lookup on the accessing IP address and run a forward DNS\nlookup on the result to verify that it points to accessing IP address\nand the resulting domain name is in either googlebot.com or google.com\ndomain.\n\nTaken from here.\n\nso you must run a DNS lookup\n\nBoth, reverse and forward.\nSee this guide on Google Search Central.\n', ""\nfunction bot_detected() {\n\n  if(preg_match('/bot|crawl|slurp|spider|mediapartners/i', $_SERVER['HTTP_USER_AGENT']){\n    return true;\n  }\n  else{\n    return false;\n  }\n}\n\n"", '\nmight be late, but what about a hidden a link. All bots will use the rel attribute follow, only bad bots will use the nofollow rel attribute.\n<a style=""display:none;"" rel=""follow"" href=""javascript:void(0);"" onclick=""isabot();"">.</a>\n\nfunction isabot(){\n//define a variable to pass with ajax to php\n// || send bots info direct to where ever.\nisabot = true;\n}\n\nfor a bad bot you can use this:\n<a style=""display:none;"" href=""javascript:void(0);"" rel=""nofollow"" onclick=""isBadbot();"">.</a>\n\nfor PHP specific you can remove the onclick attribute and replace the href attribute with a link to your ip detector/ bot detector like so:\n<a style=""display:none;"" rel=""follow"" href=""https://somedomain.com/botdetector.php"">.</a>\n\nOR\n<a style=""display:none;"" rel=""nofollow"" href=""https://somedomain.com/badbotdetector.php"">.</a>\n\nyou can work with it and maybe use both, one detects a bot, while the other proves it to be a bad bot.\nhope you find this useful\n']",https://stackoverflow.com/questions/677419/how-to-detect-search-engine-bots-with-php,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do I make a simple crawler in PHP? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    


Closed 3 years ago.










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                        
                    





I have a web page with a bunch of links. I want to write a script which would dump all the data contained in those links in a local file.
Has anybody done that with PHP? General guidelines and gotchas would suffice as an answer.
",202k,,"['\nMeh. Don\'t parse HTML with regexes.\nHere\'s a DOM version inspired by Tatu\'s:\n<?php\nfunction crawl_page($url, $depth = 5)\n{\n    static $seen = array();\n    if (isset($seen[$url]) || $depth === 0) {\n        return;\n    }\n\n    $seen[$url] = true;\n\n    $dom = new DOMDocument(\'1.0\');\n    @$dom->loadHTMLFile($url);\n\n    $anchors = $dom->getElementsByTagName(\'a\');\n    foreach ($anchors as $element) {\n        $href = $element->getAttribute(\'href\');\n        if (0 !== strpos($href, \'http\')) {\n            $path = \'/\' . ltrim($href, \'/\');\n            if (extension_loaded(\'http\')) {\n                $href = http_build_url($url, array(\'path\' => $path));\n            } else {\n                $parts = parse_url($url);\n                $href = $parts[\'scheme\'] . \'://\';\n                if (isset($parts[\'user\']) && isset($parts[\'pass\'])) {\n                    $href .= $parts[\'user\'] . \':\' . $parts[\'pass\'] . \'@\';\n                }\n                $href .= $parts[\'host\'];\n                if (isset($parts[\'port\'])) {\n                    $href .= \':\' . $parts[\'port\'];\n                }\n                $href .= dirname($parts[\'path\'], 1).$path;\n            }\n        }\n        crawl_page($href, $depth - 1);\n    }\n    echo ""URL:"",$url,PHP_EOL,""CONTENT:"",PHP_EOL,$dom->saveHTML(),PHP_EOL,PHP_EOL;\n}\ncrawl_page(""http://hobodave.com"", 2);\n\nEdit: I fixed some bugs from Tatu\'s version (works with relative URLs now).\nEdit: I added a new bit of functionality that prevents it from following the same URL twice.\nEdit: echoing output to STDOUT now so you can redirect it to whatever file you want\nEdit: Fixed a bug pointed out by George in his answer. Relative urls will no longer append to the end of the url path, but overwrite it. Thanks to George for this. Note that George\'s answer doesn\'t account for any of: https, user, pass, or port. If you have the http PECL extension loaded this is quite simply done using http_build_url. Otherwise, I have to manually glue together using parse_url. Thanks again George.\n', '\nHere my implementation based on the above example/answer.\n\nIt is class based \nuses Curl\nsupport HTTP Auth\nSkip Url not belonging to the base domain\nReturn Http header Response Code for each page\nReturn time for each page\n\nCRAWL CLASS:\nclass crawler\n{\n    protected $_url;\n    protected $_depth;\n    protected $_host;\n    protected $_useHttpAuth = false;\n    protected $_user;\n    protected $_pass;\n    protected $_seen = array();\n    protected $_filter = array();\n\n    public function __construct($url, $depth = 5)\n    {\n        $this->_url = $url;\n        $this->_depth = $depth;\n        $parse = parse_url($url);\n        $this->_host = $parse[\'host\'];\n    }\n\n    protected function _processAnchors($content, $url, $depth)\n    {\n        $dom = new DOMDocument(\'1.0\');\n        @$dom->loadHTML($content);\n        $anchors = $dom->getElementsByTagName(\'a\');\n\n        foreach ($anchors as $element) {\n            $href = $element->getAttribute(\'href\');\n            if (0 !== strpos($href, \'http\')) {\n                $path = \'/\' . ltrim($href, \'/\');\n                if (extension_loaded(\'http\')) {\n                    $href = http_build_url($url, array(\'path\' => $path));\n                } else {\n                    $parts = parse_url($url);\n                    $href = $parts[\'scheme\'] . \'://\';\n                    if (isset($parts[\'user\']) && isset($parts[\'pass\'])) {\n                        $href .= $parts[\'user\'] . \':\' . $parts[\'pass\'] . \'@\';\n                    }\n                    $href .= $parts[\'host\'];\n                    if (isset($parts[\'port\'])) {\n                        $href .= \':\' . $parts[\'port\'];\n                    }\n                    $href .= $path;\n                }\n            }\n            // Crawl only link that belongs to the start domain\n            $this->crawl_page($href, $depth - 1);\n        }\n    }\n\n    protected function _getContent($url)\n    {\n        $handle = curl_init($url);\n        if ($this->_useHttpAuth) {\n            curl_setopt($handle, CURLOPT_HTTPAUTH, CURLAUTH_ANY);\n            curl_setopt($handle, CURLOPT_USERPWD, $this->_user . "":"" . $this->_pass);\n        }\n        // follows 302 redirect, creates problem wiht authentication\n//        curl_setopt($handle, CURLOPT_FOLLOWLOCATION, TRUE);\n        // return the content\n        curl_setopt($handle, CURLOPT_RETURNTRANSFER, TRUE);\n\n        /* Get the HTML or whatever is linked in $url. */\n        $response = curl_exec($handle);\n        // response total time\n        $time = curl_getinfo($handle, CURLINFO_TOTAL_TIME);\n        /* Check for 404 (file not found). */\n        $httpCode = curl_getinfo($handle, CURLINFO_HTTP_CODE);\n\n        curl_close($handle);\n        return array($response, $httpCode, $time);\n    }\n\n    protected function _printResult($url, $depth, $httpcode, $time)\n    {\n        ob_end_flush();\n        $currentDepth = $this->_depth - $depth;\n        $count = count($this->_seen);\n        echo ""N::$count,CODE::$httpcode,TIME::$time,DEPTH::$currentDepth URL::$url <br>"";\n        ob_start();\n        flush();\n    }\n\n    protected function isValid($url, $depth)\n    {\n        if (strpos($url, $this->_host) === false\n            || $depth === 0\n            || isset($this->_seen[$url])\n        ) {\n            return false;\n        }\n        foreach ($this->_filter as $excludePath) {\n            if (strpos($url, $excludePath) !== false) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    public function crawl_page($url, $depth)\n    {\n        if (!$this->isValid($url, $depth)) {\n            return;\n        }\n        // add to the seen URL\n        $this->_seen[$url] = true;\n        // get Content and Return Code\n        list($content, $httpcode, $time) = $this->_getContent($url);\n        // print Result for current Page\n        $this->_printResult($url, $depth, $httpcode, $time);\n        // process subPages\n        $this->_processAnchors($content, $url, $depth);\n    }\n\n    public function setHttpAuth($user, $pass)\n    {\n        $this->_useHttpAuth = true;\n        $this->_user = $user;\n        $this->_pass = $pass;\n    }\n\n    public function addFilterPath($path)\n    {\n        $this->_filter[] = $path;\n    }\n\n    public function run()\n    {\n        $this->crawl_page($this->_url, $this->_depth);\n    }\n}\n\nUSAGE:\n// USAGE\n$startURL = \'http://YOUR_URL/\';\n$depth = 6;\n$username = \'YOURUSER\';\n$password = \'YOURPASS\';\n$crawler = new crawler($startURL, $depth);\n$crawler->setHttpAuth($username, $password);\n// Exclude path with the following structure to be processed \n$crawler->addFilterPath(\'customer/account/login/referer\');\n$crawler->run();\n\n', '\nCheck out PHP Crawler\nhttp://sourceforge.net/projects/php-crawler/\nSee if it helps.\n', '\nIn it\'s simplest form:\nfunction crawl_page($url, $depth = 5) {\n    if($depth > 0) {\n        $html = file_get_contents($url);\n\n        preg_match_all(\'~<a.*?href=""(.*?)"".*?>~\', $html, $matches);\n\n        foreach($matches[1] as $newurl) {\n            crawl_page($newurl, $depth - 1);\n        }\n\n        file_put_contents(\'results.txt\', $newurl.""\\n\\n"".$html.""\\n\\n"", FILE_APPEND);\n    }\n}\n\ncrawl_page(\'http://www.domain.com/index.php\', 5);\n\nThat function will get contents from a page, then crawl all found links and save the contents to \'results.txt\'. The functions accepts an second parameter, depth, which defines how long the links should be followed. Pass 1 there if you want to parse only links from the given page.\n', '\nWhy use PHP for this, when you can use wget, e.g.\nwget -r -l 1 http://www.example.com\n\nFor how to parse the contents, see Best Methods to parse HTML and use the search function for examples. How to parse HTML has been answered multiple times before.\n', '\nWith some little changes to hobodave\'s code, here is a codesnippet you can use to crawl pages. This needs the curl extension to be enabled in your server.\n<?php\n//set_time_limit (0);\nfunction crawl_page($url, $depth = 5){\n$seen = array();\nif(($depth == 0) or (in_array($url, $seen))){\n    return;\n}   \n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_URL, $url);\ncurl_setopt($ch, CURLOPT_TIMEOUT, 30);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER,1);\n$result = curl_exec ($ch);\ncurl_close ($ch);\nif( $result ){\n    $stripped_file = strip_tags($result, ""<a>"");\n    preg_match_all(""/<a[\\s]+[^>]*?href[\\s]?=[\\s\\""\\\']+"".""(.*?)[\\""\\\']+.*?>"".""([^<]+|.*?)?<\\/a>/"", $stripped_file, $matches, PREG_SET_ORDER ); \n    foreach($matches as $match){\n        $href = $match[1];\n            if (0 !== strpos($href, \'http\')) {\n                $path = \'/\' . ltrim($href, \'/\');\n                if (extension_loaded(\'http\')) {\n                    $href = http_build_url($href , array(\'path\' => $path));\n                } else {\n                    $parts = parse_url($href);\n                    $href = $parts[\'scheme\'] . \'://\';\n                    if (isset($parts[\'user\']) && isset($parts[\'pass\'])) {\n                        $href .= $parts[\'user\'] . \':\' . $parts[\'pass\'] . \'@\';\n                    }\n                    $href .= $parts[\'host\'];\n                    if (isset($parts[\'port\'])) {\n                        $href .= \':\' . $parts[\'port\'];\n                    }\n                    $href .= $path;\n                }\n            }\n            crawl_page($href, $depth - 1);\n        }\n}   \necho ""Crawled {$href}"";\n}   \ncrawl_page(""http://www.sitename.com/"",3);\n?>\n\nI have explained this tutorial in this crawler script tutorial\n', '\nHobodave you were very close. The only thing I have changed is within the if statement that checks to see if the href attribute of the found anchor tag begins with \'http\'. Instead of simply adding the $url variable which would contain the page that was passed in you must first strip it down to the host which can be done using the parse_url php function.\n<?php\nfunction crawl_page($url, $depth = 5)\n{\n  static $seen = array();\n  if (isset($seen[$url]) || $depth === 0) {\n    return;\n  }\n\n  $seen[$url] = true;\n\n  $dom = new DOMDocument(\'1.0\');\n  @$dom->loadHTMLFile($url);\n\n  $anchors = $dom->getElementsByTagName(\'a\');\n  foreach ($anchors as $element) {\n    $href = $element->getAttribute(\'href\');\n    if (0 !== strpos($href, \'http\')) {\n       /* this is where I changed hobodave\'s code */\n        $host = ""http://"".parse_url($url,PHP_URL_HOST);\n        $href = $host. \'/\' . ltrim($href, \'/\');\n    }\n    crawl_page($href, $depth - 1);\n  }\n\n  echo ""New Page:<br /> "";\n  echo ""URL:"",$url,PHP_EOL,""<br />"",""CONTENT:"",PHP_EOL,$dom->saveHTML(),PHP_EOL,PHP_EOL,""  <br /><br />"";\n}\n\ncrawl_page(""http://hobodave.com/"", 5);\n?>\n\n', ""\nAs mentioned, there are crawler frameworks all ready for customizing out there, but if what you're doing is as simple as you mentioned, you could make it from scratch pretty easily.\nScraping the links: http://www.phpro.org/examples/Get-Links-With-DOM.html\nDumping results to a file: http://www.tizag.com/phpT/filewrite.php\n"", ""\nI used @hobodave's code, with this little tweak to prevent re-crawling all fragment variants of the same URL:\n<?php\nfunction crawl_page($url, $depth = 5)\n{\n  $parts = parse_url($url);\n  if(array_key_exists('fragment', $parts)){\n    unset($parts['fragment']);\n    $url = http_build_url($parts);\n  }\n\n  static $seen = array();\n  ...\n\nThen you can also omit the $parts = parse_url($url); line within the for loop.\n"", '\nYou can try this it may be help to you\n$search_string = \'american golf News: Fowler beats stellar field in Abu Dhabi\';\n$html = file_get_contents(url of the site);\n$dom = new DOMDocument;\n$titalDom = new DOMDocument;\n$tmpTitalDom = new DOMDocument;\nlibxml_use_internal_errors(true);\n@$dom->loadHTML($html);\nlibxml_use_internal_errors(false);\n$xpath = new DOMXPath($dom);\n$videos = $xpath->query(\'//div[@class=""primary-content""]\');\nforeach ($videos as $key => $video) {\n$newdomaindom = new DOMDocument;    \n$newnode = $newdomaindom->importNode($video, true);\n$newdomaindom->appendChild($newnode);\n@$titalDom->loadHTML($newdomaindom->saveHTML());\n$xpath1 = new DOMXPath($titalDom);\n$titles = $xpath1->query(\'//div[@class=""listingcontainer""]/div[@class=""list""]\');\nif(strcmp(preg_replace(\'!\\s+!\',\' \',  $titles->item(0)->nodeValue),$search_string)){     \n    $tmpNode = $tmpTitalDom->importNode($video, true);\n    $tmpTitalDom->appendChild($tmpNode);\n    break;\n}\n}\necho $tmpTitalDom->saveHTML();\n\n', '\nThank you @hobodave.\nHowever I found two weaknesses in your code.\nYour parsing of the original url to get the ""host"" segment stops at the first single slash. This presumes that all relative links start in the root directory. This only true sometimes.\noriginal url   :  http://example.com/game/index.html\nhref in <a> tag:  highscore.html\nauthor\'s intent:  http://example.com/game/highscore.html  <-200->\ncrawler result :  http://example.com/highscore.html       <-404->\n\nfix this by breaking at the last single slash not the first\na second unrelated bug, is that $depth does not really track recursion depth, it tracks breadth of the first level of recursion. \nIf I believed this page were in active use I might debug this second issue, but I suspect the text I am writing now will never be read by anyone, human or robot, since this issue is six years old and I do not even have enough reputation to notify +hobodave directly about these defects by commmenting on his code. Thanks anyway hobodave.\n', '\nI came up with the following spider code.\nI adapted it a bit from the following:\nPHP - Is the there a safe way to perform deep recursion?\nit seems fairly rapid....\n    <?php\nfunction  spider( $base_url , $search_urls=array() ) {\n    $queue[] = $base_url;\n    $done           =   array();\n    $found_urls     =   array();\n    while($queue) {\n            $link = array_shift($queue);\n            if(!is_array($link)) {\n                $done[] = $link;\n                foreach( $search_urls as $s) { if (strstr( $link , $s )) { $found_urls[] = $link; } }\n                if( empty($search_urls)) { $found_urls[] = $link; }\n                if(!empty($link )) {\necho \'LINK:::\'.$link;\n                      $content =    file_get_contents( $link );\n//echo \'P:::\'.$content;\n                    preg_match_all(\'~<a.*?href=""(.*?)"".*?>~\', $content, $sublink);\n                    if (!in_array($sublink , $done) && !in_array($sublink , $queue)  ) {\n                           $queue[] = $sublink;\n                    }\n                }\n            } else {\n                    $result=array();\n                    $return = array();\n                    // flatten multi dimensional array of URLs to one dimensional.\n                    while(count($link)) {\n                         $value = array_shift($link);\n                         if(is_array($value))\n                             foreach($value as $sub)\n                                $link[] = $sub;\n                         else\n                               $return[] = $value;\n                     }\n                     // now loop over one dimensional array.\n                     foreach($return as $link) {\n                                // echo \'L::\'.$link;\n                                // url may be in form <a href.. so extract what\'s in the href bit.\n                                preg_match_all(\'/<a[^>]+href=([\\\'""])(?<href>.+?)\\1[^>]*>/i\', $link, $result);\n                                if ( isset( $result[\'href\'][0] )) { $link = $result[\'href\'][0]; }\n                                // add the new URL to the queue.\n                                if( (!strstr( $link , ""http"")) && (!in_array($base_url.$link , $done)) && (!in_array($base_url.$link , $queue)) ) {\n                                     $queue[]=$base_url.$link;\n                                } else {\n                                    if ( (strstr( $link , $base_url  ))  && (!in_array($base_url.$link , $done)) && (!in_array($base_url.$link , $queue)) ) {\n                                         $queue[] = $link;\n                                    }\n                                }\n                      }\n            }\n    }\n\n\n    return $found_urls;\n}    \n\n\n    $base_url       =   \'https://www.houseofcheese.co.uk/\';\n    $search_urls    =   array(  $base_url.\'acatalog/\' );\n    $done = spider( $base_url  , $search_urls  );\n\n    //\n    // RESULT\n    //\n    //\n    echo \'<br /><br />\';\n    echo \'RESULT:::\';\n    foreach(  $done as $r )  {\n        echo \'URL:::\'.$r.\'<br />\';\n    }\n\n', '\nIts worth remembering that when crawling external links (I do appreciate the OP relates to a users own page) you should be aware of robots.txt. I have found the following which will hopefully help http://www.the-art-of-web.com/php/parse-robots/.\n', '\nI created a small class to grab data from the provided url, then extract html elements of your choice. The class makes use of CURL and DOMDocument.\nphp class:\nclass crawler {\n\n\n   public static $timeout = 2;\n   public static $agent   = \'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\';\n\n\n   public static function http_request($url) {\n      $ch = curl_init();\n      curl_setopt($ch, CURLOPT_URL,            $url);\n      curl_setopt($ch, CURLOPT_USERAGENT,      self::$agent);\n      curl_setopt($ch, CURLOPT_CONNECTTIMEOUT, self::$timeout);\n      curl_setopt($ch, CURLOPT_TIMEOUT,        self::$timeout);\n      curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n      $response = curl_exec($ch);\n      curl_close($ch);\n      return $response;\n   }\n\n\n   public static function strip_whitespace($data) {\n      $data = preg_replace(\'/\\s+/\', \' \', $data);\n      return trim($data);\n   }\n\n\n   public static function extract_elements($tag, $data) {\n      $response = array();\n      $dom      = new DOMDocument;\n      @$dom->loadHTML($data);\n      foreach ( $dom->getElementsByTagName($tag) as $index => $element ) {\n         $response[$index][\'text\'] = self::strip_whitespace($element->nodeValue);\n         foreach ( $element->attributes as $attribute ) {\n            $response[$index][\'attributes\'][strtolower($attribute->nodeName)] = self::strip_whitespace($attribute->nodeValue);\n         }\n      }\n      return $response;\n   }\n\n\n}\n\n\nexample usage:\n$data  = crawler::http_request(\'https://stackoverflow.com/questions/2313107/how-do-i-make-a-simple-crawler-in-php\');\n$links = crawler::extract_elements(\'a\', $data);\nif ( count($links) > 0 ) {\n   file_put_contents(\'links.json\', json_encode($links, JSON_PRETTY_PRINT));\n}\n\n\nexample response:\n[\n    {\n        ""text"": ""Stack Overflow"",\n        ""attributes"": {\n            ""href"": ""https:\\/\\/stackoverflow.com"",\n            ""class"": ""-logo js-gps-track"",\n            ""data-gps-track"": ""top_nav.click({is_current:false, location:2, destination:8})""\n        }\n    },\n    {\n        ""text"": ""Questions"",\n        ""attributes"": {\n            ""id"": ""nav-questions"",\n            ""href"": ""\\/questions"",\n            ""class"": ""-link js-gps-track"",\n            ""data-gps-track"": ""top_nav.click({is_current:true, location:2, destination:1})""\n        }\n    },\n    {\n        ""text"": ""Developer Jobs"",\n        ""attributes"": {\n            ""id"": ""nav-jobs"",\n            ""href"": ""\\/jobs?med=site-ui&ref=jobs-tab"",\n            ""class"": ""-link js-gps-track"",\n            ""data-gps-track"": ""top_nav.click({is_current:false, location:2, destination:6})""\n        }\n    }\n]\n\n', '\nIt\'s an old question. A lot of good things happened since then. Here are my two cents on this topic:\n\nTo accurately track the visited pages you have to normalize URI first. The normalization algorithm includes multiple steps:\n\nSort query parameters. For example, the following URIs are equivalent after normalization:\n\nGET http://www.example.com/query?id=111&cat=222\nGET http://www.example.com/query?cat=222&id=111\n\nConvert the empty path.\nExample: http://example.org 鈫?http://example.org/\nCapitalize percent encoding. All letters within a percent-encoding triplet (e.g., ""%3A"") are case-insensitive.\nExample: http://example.org/a%c2%B1b 鈫?http://example.org/a%C2%B1b\nRemove unnecessary dot-segments.\nExample: http://example.org/../a/b/../c/./d.html 鈫?http://example.org/a/c/d.html\nPossibly some other normalization rules\n\nNot only <a> tag has href attribute, <area> tag has it too https://html.com/tags/area/. If you don\'t want to miss anything, you have to scrape <area> tag too.\nTrack crawling progress. If the website is small, it is not a problem. Contrarily it might be very frustrating if you crawl half of the site and it failed. Consider using a database or a filesystem to store the progress.\nBe kind to the site owners.\nIf you are ever going to use your crawler outside of your website, you have to use delays. Without delays, the script is too fast and might significantly slow down some small sites. From sysadmins perspective, it looks like a DoS attack. A static delay between the requests will do the trick.\n\nIf you don\'t want to deal with that, try Crawlzone and let me know your feedback. Also, check out the article I wrote a while back https://www.codementor.io/zstate/this-is-how-i-crawl-n98s6myxm\n']",,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to run Scrapy from within a Python script,"
I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:
http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/
http://snipplr.com/view/67006/using-scrapy-from-a-script/
I can't figure out where I should put my spider code and how to call it from the main function. Please help. This is the example code:
# This snippet can be used to run scrapy spiders independent of scrapyd or the scrapy command line tool and use it from a script. 
# 
# The multiprocessing library is used in order to work around a bug in Twisted, in which you cannot restart an already running reactor or in this case a scrapy instance.
# 
# [Here](http://groups.google.com/group/scrapy-users/browse_thread/thread/f332fc5b749d401a) is the mailing-list discussion for this snippet. 

#!/usr/bin/python
import os
os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'project.settings') #Must be at the top before other imports

from scrapy import log, signals, project
from scrapy.xlib.pydispatch import dispatcher
from scrapy.conf import settings
from scrapy.crawler import CrawlerProcess
from multiprocessing import Process, Queue

class CrawlerScript():

    def __init__(self):
        self.crawler = CrawlerProcess(settings)
        if not hasattr(project, 'crawler'):
            self.crawler.install()
        self.crawler.configure()
        self.items = []
        dispatcher.connect(self._item_passed, signals.item_passed)

    def _item_passed(self, item):
        self.items.append(item)

    def _crawl(self, queue, spider_name):
        spider = self.crawler.spiders.create(spider_name)
        if spider:
            self.crawler.queue.append_spider(spider)
        self.crawler.start()
        self.crawler.stop()
        queue.put(self.items)

    def crawl(self, spider):
        queue = Queue()
        p = Process(target=self._crawl, args=(queue, spider,))
        p.start()
        p.join()
        return queue.get(True)

# Usage
if __name__ == ""__main__"":
    log.start()

    """"""
    This example runs spider1 and then spider2 three times. 
    """"""
    items = list()
    crawler = CrawlerScript()
    items.append(crawler.crawl('spider1'))
    for i in range(3):
        items.append(crawler.crawl('spider2'))
    print items

# Snippet imported from snippets.scrapy.org (which no longer works)
# author: joehillen
# date  : Oct 24, 2010

Thank you.
",83k,"
            85
        ","[""\nAll other answers reference Scrapy v0.x. According to the updated docs, Scrapy 1.0 demands:\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider(scrapy.Spider):\n    # Your spider definition\n    ...\n\nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\n\nprocess.crawl(MySpider)\nprocess.start() # the script will block here until the crawling is finished\n\n"", '\nSimply we can use\nfrom scrapy.crawler import CrawlerProcess\nfrom project.spiders.test_spider import SpiderName\n\nprocess = CrawlerProcess()\nprocess.crawl(SpiderName, arg1=val1,arg2=val2)\nprocess.start()\n\nUse these arguments inside spider __init__ function with the global scope.\n', ""\nThough I haven't tried it I think the answer can be found within the scrapy documentation. To quote directly from it:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import Settings\nfrom scrapy import log\nfrom testspiders.spiders.followall import FollowAllSpider\n\nspider = FollowAllSpider(domain='scrapinghub.com')\ncrawler = Crawler(Settings())\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here\n\nFrom what I gather this is a new development in the library which renders some of the earlier approaches online (such as that in the question) obsolete.\n"", '\nIn scrapy 0.19.x you should do this:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy import log, signals\nfrom testspiders.spiders.followall import FollowAllSpider\nfrom scrapy.utils.project import get_project_settings\n\nspider = FollowAllSpider(domain=\'scrapinghub.com\')\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.signals.connect(reactor.stop, signal=signals.spider_closed)\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here until the spider_closed signal was sent\n\nNote these lines     \nsettings = get_project_settings()\ncrawler = Crawler(settings)\n\nWithout it your spider won\'t use your settings and will not save the items.\nTook me a while to figure out why the example in documentation wasn\'t saving my items. I sent a pull request to fix the doc example.\nOne more to do so is just call command directly from you script\nfrom scrapy import cmdline\ncmdline.execute(""scrapy crawl followall"".split())  #followall is the spider\'s name\n\nCopied this answer from my first answer in here:\nhttps://stackoverflow.com/a/19060485/1402286\n', '\nWhen there are multiple crawlers need to be run inside one python script, the reactor stop needs to be handled with caution as the reactor can only be stopped once and cannot be restarted. \nHowever, I found while doing my project that using \nos.system(""scrapy crawl yourspider"")\n\nis the easiest. This will save me from handling all sorts of signals especially when I have multiple spiders.\nIf Performance is a concern, you can use multiprocessing to run your spiders in parallel, something like:\ndef _crawl(spider_name=None):\n    if spider_name:\n        os.system(\'scrapy crawl %s\' % spider_name)\n    return None\n\ndef run_crawler():\n\n    spider_names = [\'spider1\', \'spider2\', \'spider2\']\n\n    pool = Pool(processes=len(spider_names))\n    pool.map(_crawl, spider_names)\n\n', '\nit  is an improvement of\nScrapy throws an error when run using crawlerprocess\nand https://github.com/scrapy/scrapy/issues/1904#issuecomment-205331087\nFirst create your usual spider for successful command line running. it is very very important that it should run and export data or image or file\nOnce it is over, do just like pasted in my program above spider class definition and below __name __ to invoke settings.\nit will get necessary settings which ""from scrapy.utils.project import get_project_settings"" failed to do which is recommended by many\nboth above and below portions should be there together. only one don\'t run.\nSpider will run in scrapy.cfg folder not any other folder\ntree  diagram may be displayed by the moderators for reference\n#Tree\n[enter image description here][1]\n\n#spider.py\nimport sys\nsys.path.append(r\'D:\\ivana\\flow\') #folder where scrapy.cfg is located\n\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.settings import Settings\nfrom flow import settings as my_settings\n\n#----------------Typical Spider Program starts here-----------------------------\n\n          spider class definition here\n\n#----------------Typical Spider Program ends here-------------------------------\n\nif __name__ == ""__main__"":\n\n    crawler_settings = Settings()\n    crawler_settings.setmodule(my_settings)\n\n    process = CrawlerProcess(settings=crawler_settings)\n    process.crawl(FlowSpider) # it is for class FlowSpider(scrapy.Spider):\n    process.start(stop_after_crawl=True)\n\n', ""\n# -*- coding: utf-8 -*-\nimport sys\nfrom scrapy.cmdline import execute\n\n\ndef gen_argv(s):\n    sys.argv = s.split()\n\n\nif __name__ == '__main__':\n    gen_argv('scrapy crawl abc_spider')\n    execute()\n\nPut this code to the path you can run scrapy crawl abc_spider from command line. (Tested with Scrapy==0.24.6)\n"", ""\nIf you want to run a simple crawling, It's easy by just running command: \nscrapy crawl . \nThere is another options to export your results to store in some formats like: \nJson, xml, csv. \nscrapy crawl  -o result.csv or result.json or result.xml. \nyou may want to try it\n""]",https://stackoverflow.com/questions/13437402/how-to-run-scrapy-from-within-a-python-script,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrapy - Reactor not Restartable [duplicate],"






This question already has answers here:
                        
                    



ReactorNotRestartable error in while loop with scrapy

                                (10 answers)
                            

Closed 3 years ago.



with:
from twisted.internet import reactor
from scrapy.crawler import CrawlerProcess

I've always ran this process sucessfully:
process = CrawlerProcess(get_project_settings())
process.crawl(*args)
# the script will block here until the crawling is finished
process.start() 

but since I've moved this code into a web_crawler(self) function, like so:
def web_crawler(self):
    # set up a crawler
    process = CrawlerProcess(get_project_settings())
    process.crawl(*args)
    # the script will block here until the crawling is finished
    process.start() 

    # (...)

    return (result1, result2) 

and started calling the method using class instantiation, like:
def __call__(self):
    results1 = test.web_crawler()[1]
    results2 = test.web_crawler()[0]

and running:
test()

I am getting the following error:
Traceback (most recent call last):
  File ""test.py"", line 573, in <module>
    print (test())
  File ""test.py"", line 530, in __call__
    artists = test.web_crawler()
  File ""test.py"", line 438, in web_crawler
    process.start() 
  File ""/Library/Python/2.7/site-packages/scrapy/crawler.py"", line 280, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 1194, in run
    self.startRunning(installSignalHandlers=installSignalHandlers)
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 1174, in startRunning
    ReactorBase.startRunning(self)
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 684, in startRunning
    raise error.ReactorNotRestartable()
twisted.internet.error.ReactorNotRestartable

what is wrong?
",38k,"
            29
        ","['\nYou cannot restart the reactor, but you should be able to run it more times by forking a separate process:\nimport scrapy\nimport scrapy.crawler as crawler\nfrom scrapy.utils.log import configure_logging\nfrom multiprocessing import Process, Queue\nfrom twisted.internet import reactor\n\n# your spider\nclass QuotesSpider(scrapy.Spider):\n    name = ""quotes""\n    start_urls = [\'http://quotes.toscrape.com/tag/humor/\']\n\n    def parse(self, response):\n        for quote in response.css(\'div.quote\'):\n            print(quote.css(\'span.text::text\').extract_first())\n\n\n# the wrapper to make it run more times\ndef run_spider(spider):\n    def f(q):\n        try:\n            runner = crawler.CrawlerRunner()\n            deferred = runner.crawl(spider)\n            deferred.addBoth(lambda _: reactor.stop())\n            reactor.run()\n            q.put(None)\n        except Exception as e:\n            q.put(e)\n\n    q = Queue()\n    p = Process(target=f, args=(q,))\n    p.start()\n    result = q.get()\n    p.join()\n\n    if result is not None:\n        raise result\n\nRun it twice:\nconfigure_logging()\n\nprint(\'first run:\')\nrun_spider(QuotesSpider)\n\nprint(\'\\nsecond run:\')\nrun_spider(QuotesSpider)\n\nResult:\nfirst run:\n鈥淭he person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.鈥漒n鈥淎 day without sunshine is like, you know, night.鈥漒n...\n\nsecond run:\n鈥淭he person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.鈥漒n鈥淎 day without sunshine is like, you know, night.鈥漒n...\n\n', '\nThis is what helped for me to win the battle against ReactorNotRestartable error: last answer from the author of the question\n0) pip install crochet\n1) import from crochet import setup\n2) setup() - at the top of the file\n3) remove 2 lines:\na) d.addBoth(lambda _: reactor.stop())\nb) reactor.run()\n\nI had the same problem with this error, and spend 4+ hours to solve this problem, read all questions here about it. Finally found that one - and share it. That is how i solved this. The only meaningful lines from Scrapy docs left are 2 last lines in this my code:\n#some more imports\nfrom crochet import setup\nsetup()\n\ndef run_spider(spiderName):\n    module_name=""first_scrapy.spiders.{}"".format(spiderName)\n    scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   \n    spiderObj=scrapy_var.mySpider()           #get mySpider-object from spider module\n    crawler = CrawlerRunner(get_project_settings())   #from Scrapy docs\n    crawler.crawl(spiderObj)                          #from Scrapy docs\n\nThis code allows me to select what spider to run just with its name passed to run_spider function and after scrapping finishes - select another spider and run it again. \nHope this will help somebody, as it helped for me :)\n', '\nAs per the Scrapy documentation, the start() method of the CrawlerProcess class does the following:\n\n""[...] starts a Twisted reactor, adjusts its pool size to REACTOR_THREADPOOL_MAXSIZE, and installs a DNS cache based on DNSCACHE_ENABLED and DNSCACHE_SIZE.""\n\nThe error you are receiving is being thrown by Twisted, because a Twisted reactor cannot be restarted.  It uses a ton of globals, and even if you do jimmy-rig some sort of code to restart it (I\'ve seen it done), there\'s no guarantee it will work.  \nHonestly, if you think you need to restart the reactor, you\'re likely doing something wrong.\nDepending on what you want to do, I would also review the Running Scrapy from a Script portion of the documentation, too.\n', '\nAs some people pointed out already: You shouldn\'t need to restart the reactor.\nIdeally if you want to chain your processes (crawl1 then crawl2 then crawl3) you simply add callbacks.\nFor example, I\'ve been using this loop spider that follows this pattern:\n1. Crawl A\n2. Sleep N\n3. goto 1\n\nAnd this is how it looks in scrapy:\nimport time\n\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.project import get_project_settings\nfrom twisted.internet import reactor\n\nclass HttpbinSpider(scrapy.Spider):\n    name = \'httpbin\'\n    allowed_domains = [\'httpbin.org\']\n    start_urls = [\'http://httpbin.org/ip\']\n\n    def parse(self, response):\n        print(response.body)\n\ndef sleep(_, duration=5):\n    print(f\'sleeping for: {duration}\')\n    time.sleep(duration)  # block here\n\n\ndef crawl(runner):\n    d = runner.crawl(HttpbinSpider)\n    d.addBoth(sleep)\n    d.addBoth(lambda _: crawl(runner))\n    return d\n\n\ndef loop_crawl():\n    runner = CrawlerRunner(get_project_settings())\n    crawl(runner)\n    reactor.run()\n\n\nif __name__ == \'__main__\':\n    loop_crawl()\n\nTo explain the process more the crawl function schedules a crawl and adds two extra callbacks that are being called when crawling is over: blocking sleep and recursive call to itself (schedule another crawl).\n$ python endless_crawl.py \nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\n\n', '\nThe mistake is in this code:\ndef __call__(self):\n    result1 = test.web_crawler()[1]\n    result2 = test.web_crawler()[0] # here\n\nweb_crawler() returns two results, and for that purpose it is trying to start the process twice, restarting the Reactor, as pointed by @Rejected.\nobtaining results running one single process, and storing both results in a tuple, is the way to go here:\ndef __call__(self):\n    result1, result2 = test.web_crawler()\n\n', '\nThis solved my problem,put below code after reactor.run() or process.start():\ntime.sleep(0.5)\n\nos.execl(sys.executable, sys.executable, *sys.argv)\n\n']",https://stackoverflow.com/questions/41495052/scrapy-reactor-not-restartable,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to pass a user defined argument in scrapy spider,"
I am trying to pass a user defined argument to a scrapy's spider. Can anyone suggest on how to do that?
I read about a parameter -a somewhere but have no idea how to use it.
",75k,"
            125
        ","[""\nSpider arguments are passed in the crawl command using the -a option. For example:\nscrapy crawl myspider -a category=electronics -a domain=system\n\nSpiders can access arguments as attributes:\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def __init__(self, category='', **kwargs):\n        self.start_urls = [f'http://www.example.com/{category}']  # py36\n        super().__init__(**kwargs)  # python3\n\n    def parse(self, response)\n        self.log(self.domain)  # system\n\nTaken from the Scrapy doc: http://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments\nUpdate 2013: Add second argument\nUpdate 2015: Adjust wording\nUpdate 2016: Use newer base class and add super, thanks @Birla\nUpdate 2017: Use Python3 super\n# previously\nsuper(MySpider, self).__init__(**kwargs)  # python2\n\nUpdate 2018: As @eLRuLL points out, spiders can access arguments as attributes\n"", ""\nPrevious answers were correct, but you don't have to declare the constructor (__init__) every time you want to code a scrapy's spider, you could just specify the parameters as before:\nscrapy crawl myspider -a parameter1=value1 -a parameter2=value2\n\nand in your spider code you can just use them as spider arguments:\nclass MySpider(Spider):\n    name = 'myspider'\n    ...\n    def parse(self, response):\n        ...\n        if self.parameter1 == value1:\n            # this is True\n\n        # or also\n        if getattr(self, parameter2) == value2:\n            # this is also True\n\nAnd it just works.\n"", '\nTo pass arguments with crawl command\n\nscrapy crawl myspider -a category=\'mycategory\' -a domain=\'example.com\'\n\nTo pass arguments to run on scrapyd replace -a with -d\n\ncurl http://your.ip.address.here:port/schedule.json -d \n   spider=myspider -d category=\'mycategory\' -d domain=\'example.com\'\n\nThe spider will receive arguments in its constructor.\n\nclass MySpider(Spider):\n    name=""myspider""\n    def __init__(self,category=\'\',domain=\'\', *args,**kwargs):\n        super(MySpider, self).__init__(*args, **kwargs)\n        self.category = category\n        self.domain = domain\nScrapy puts all the arguments as spider attributes and you can skip the init method completely. Beware use getattr method for getting those attributes so your code does not break.\n\nclass MySpider(Spider):\n    name=""myspider""\n    start_urls = (\'https://httpbin.org/ip\',)\n\n    def parse(self,response):\n        print getattr(self,\'category\',\'\')\n        print getattr(self,\'domain\',\'\')\n\n\n', '\nSpider arguments are passed while running the crawl command using the -a option. For example if i want to pass a domain name as argument to my spider then i will do this-\n\nscrapy crawl myspider -a domain=""http://www.example.com""\n\nAnd receive arguments in spider\'s constructors:\nclass MySpider(BaseSpider):\n    name = \'myspider\'\n    def __init__(self, domain=\'\', *args, **kwargs):\n        super(MySpider, self).__init__(*args, **kwargs)\n        self.start_urls = [domain]\n        #\n\n...\nit will work :)\n', '\nAlternatively we can use ScrapyD which expose an API where we can pass the start_url and spider name. ScrapyD has api\'s to stop/start/status/list the spiders.\npip install scrapyd scrapyd-deploy\nscrapyd\nscrapyd-deploy local -p default\n\nscrapyd-deploy will deploy the spider in the form of egg into the daemon and even it maintains the version of the spider. While starting the spider you can mention which version of spider to use.\nclass MySpider(CrawlSpider):\n\n    def __init__(self, start_urls, *args, **kwargs):\n        self.start_urls = start_urls.split(\'|\')\n        super().__init__(*args, **kwargs)\n    name = testspider\n\ncurl http://localhost:6800/schedule.json -d project=default -d spider=testspider -d start_urls=""https://www.anyurl...|https://www.anyurl2""\nAdded advantage is you can build your own UI to accept the url and other params from the user and schedule a task using the above scrapyd schedule API\nRefer scrapyd API documentation for more details\n']",https://stackoverflow.com/questions/15611605/how-to-pass-a-user-defined-argument-in-scrapy-spider,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
python: [Errno 10054] An existing connection was forcibly closed by the remote host,"
I am writing python to crawl Twitter space using Twitter-py. I have set the crawler to sleep for a while (2 seconds) between each request to api.twitter.com. However, after some times of running (around 1), when the Twitter's rate limit not exceeded yet, I got this error.
[Errno 10054] An existing connection was forcibly closed by the remote host.

What are possible causes of this problem and how to solve this?
I have searched through and found that the Twitter server itself may force to close the connection due to many requests.
Thank you very much in advance.
",213k,"
            70
        ","[""\nThis can be caused by the two sides of the connection disagreeing over whether the connection timed out or not during a keepalive. (Your code tries to reused the connection just as the server is closing it because it has been idle for too long.) You should basically just retry the operation over a new connection. (I'm surprised your library doesn't do this automatically.)\n"", ""\nI know this is a very old question but it may be that you need to set the request headers. This solved it for me.\nFor example 'user-agent', 'accept' etc. here is an example with user-agent:\nurl = 'your-url-here'\nheaders = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\nr = requests.get(url, headers=headers)\n\n"", '\nthere are many causes such as \n\nThe network link between server and client may be temporarily going down.\nrunning out of system resources.\nsending malformed data.\n\nTo examine the problem in detail, you can use Wireshark.\nor you can just re-request or re-connect again.\n', '\nFor me this problem arised while trying to connect to the SAP Hana database. When I got this error, \nOperationalError: Lost connection to HANA server (ConnectionResetError(10054, \'An existing connection was forcibly closed by the remote host\', None, 10054, None))\nI tried to run the code for connection(mentioned below), which created that error, again and it worked. \n\n\n    import pyhdb\n    connection = pyhdb.connect(host=""example.com"",port=30015,user=""user"",password=""secret"")\n    cursor = connection.cursor()\n    cursor.execute(""SELECT \'Hello Python World\' FROM DUMMY"")\n    cursor.fetchone()\n    connection.close()\n\n\nIt was because the server refused to connect. It might require you to wait for a while and try again. Try closing the Hana Studio by logging off and then logging in again. Keep running the code for a number of times.\n', '\nI got the same error ([WinError 10054] An existing connection was forcibly closed by the remote host) with websocket-client after setting ping_interval = 2 in websocket.run_forever(). (I had multiple threads connecting to the same host.)\nSetting ping_interval = 10 and ping_timeout = 9 solved the issue. May be you need to reduce the amount of requests and stop making host busy otherwise it will forcibly disconnect you.\n', '\nI fixed it with a while try loop, waiting for the response to set the variable in order to exit the loop.\nWhen the connection has an exception, it waits five seconds, and continues looking for the response from the connection.\nMy code before fix, with the failed response HTTPSConnectionPool(host=\'etc.com\', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E9955A2050>, \'Connection to example.net timed out. (connect timeout=None)\'))\n \n\nfrom __future__ import print_function\nimport sys\nimport requests\n\n\ndef condition_questions(**kwargs):\n    proxies = {\'https\': \'example.com\', \'http\': \'example.com:3128\'}\n    print(kwargs, file=sys.stdout)\n    headers = {\'etc\':\'etc\',}\n    body = f\'\'\'<etc>\n                </etc>\'\'\'\n\n    try:\n        response_xml = requests.post(\'https://example.com\', data=body, headers=headers, proxies=proxies)\n    except Exception as ex:\n        print(""exception"", ex, file=sys.stdout)\n        log.exception(ex)\n    finally:\n        print(""response_xml"", response_xml, file=sys.stdout)\n        return response_xml\n\nAfter fix, with successful response response_xml <Response [200]>:\n\nimport time\n...\n\nresponse_xml = \'\'\n    while response_xml == \'\':\n        try:\n            response_xml = requests.post(\'https://example.com\', data=body, headers=headers, proxies=proxies)\n            break\n        except Exception as ex:\n            print(""exception"", ex, file=sys.stdout)\n            log.exception(ex)\n            time.sleep(5)\n            continue\n        finally:\n            print(""response_xml"", response_xml, file=sys.stdout)\n            return response_xml\n\nbased on Jatin\'s answer here  --""Just do this,\nimport time\n\npage = \'\'\nwhile page == \'\':\n    try:\n        page = requests.get(url)\n        break\n    except:\n        print(""Connection refused by the server.."")\n        print(""Let me sleep for 5 seconds"")\n        print(""ZZzzzz..."")\n        time.sleep(5)\n        print(""Was a nice sleep, now let me continue..."")\n        continue\n\nYou\'re welcome :)""\n']",https://stackoverflow.com/questions/8814802/python-errno-10054-an-existing-connection-was-forcibly-closed-by-the-remote-h,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anyone know of a good Python based web crawler that I could use?,"









Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                        
                    





I'm half-tempted to write my own, but I don't really have enough time right now.  I've seen the Wikipedia list of open source crawlers but I'd prefer something written in Python.  I realize that I could probably just use one of the tools on the Wikipedia page and wrap it in Python.  I might end up doing that - if anyone has any advice about any of those tools, I'm open to hearing about them.  I've used Heritrix via its web interface and I found it to be quite cumbersome.  I definitely won't be using a browser API for my upcoming project.
Thanks in advance.  Also, this is my first SO question!
",98k,,"[""\n\nMechanize is my favorite; great high-level browsing capabilities (super-simple form filling and submission).\nTwill is a simple scripting language built on top of Mechanize\nBeautifulSoup + urllib2 also works quite nicely.\nScrapy looks like an extremely promising project; it's new.\n\n"", '\nUse Scrapy.\nIt is a twisted-based web crawler framework. Still under heavy development but it works already. Has many goodies:\n\nBuilt-in support for parsing HTML, XML, CSV, and Javascript\nA media pipeline for scraping items with images (or any other media) and download the image files as well\nSupport for extending Scrapy by plugging your own functionality using middlewares, extensions, and pipelines\nWide range of built-in middlewares and extensions for handling of compression, cache, cookies, authentication, user-agent spoofing, robots.txt handling, statistics, crawl depth restriction, etc\nInteractive scraping shell console, very useful for developing and debugging\nWeb management console for monitoring and controlling your bot\nTelnet console for low-level access to the Scrapy process\n\nExample code to extract information about all torrent files added today in the mininova torrent site, by using a XPath selector on the HTML returned:\nclass Torrent(ScrapedItem):\n    pass\n\nclass MininovaSpider(CrawlSpider):\n    domain_name = \'mininova.org\'\n    start_urls = [\'http://www.mininova.org/today\']\n    rules = [Rule(RegexLinkExtractor(allow=[\'/tor/\\d+\']), \'parse_torrent\')]\n\n    def parse_torrent(self, response):\n        x = HtmlXPathSelector(response)\n        torrent = Torrent()\n\n        torrent.url = response.url\n        torrent.name = x.x(""//h1/text()"").extract()\n        torrent.description = x.x(""//div[@id=\'description\']"").extract()\n        torrent.size = x.x(""//div[@id=\'info-left\']/p[2]/text()[2]"").extract()\n        return [torrent]\n\n', '\nCheck the HarvestMan, a multi-threaded web-crawler written in Python, also give a look to the spider.py module.\nAnd here you can find code samples to build a simple web-crawler.\n', ""\nI've used Ruya and found it pretty good.\n"", '\nI hacked the above script to include a login page as I needed it to access a drupal site. Not pretty but may help someone out there.\n#!/usr/bin/python\n\nimport httplib2\nimport urllib\nimport urllib2\nfrom cookielib import CookieJar\nimport sys\nimport re\nfrom HTMLParser import HTMLParser\n\nclass miniHTMLParser( HTMLParser ):\n\n  viewedQueue = []\n  instQueue = []\n  headers = {}\n  opener = """"\n\n  def get_next_link( self ):\n    if self.instQueue == []:\n      return \'\'\n    else:\n      return self.instQueue.pop(0)\n\n\n  def gethtmlfile( self, site, page ):\n    try:\n        url = \'http://\'+site+\'\'+page\n        response = self.opener.open(url)\n        return response.read()\n    except Exception, err:\n        print "" Error retrieving: ""+page\n        sys.stderr.write(\'ERROR: %s\\n\' % str(err))\n    return """" \n\n    return resppage\n\n  def loginSite( self, site_url ):\n    try:\n    cj = CookieJar()\n    self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n\n    url = \'http://\'+site_url \n        params = {\'name\': \'customer_admin\', \'pass\': \'customer_admin123\', \'opt\': \'Log in\', \'form_build_id\': \'form-3560fb42948a06b01d063de48aa216ab\', \'form_id\':\'user_login_block\'}\n    user_agent = \'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\'\n    self.headers = { \'User-Agent\' : user_agent }\n\n    data = urllib.urlencode(params)\n    response = self.opener.open(url, data)\n    print ""Logged in""\n    return response.read() \n\n    except Exception, err:\n    print "" Error logging in""\n    sys.stderr.write(\'ERROR: %s\\n\' % str(err))\n\n    return 1\n\n  def handle_starttag( self, tag, attrs ):\n    if tag == \'a\':\n      newstr = str(attrs[0][1])\n      print newstr\n      if re.search(\'http\', newstr) == None:\n        if re.search(\'mailto\', newstr) == None:\n          if re.search(\'#\', newstr) == None:\n            if (newstr in self.viewedQueue) == False:\n              print ""  adding"", newstr\n              self.instQueue.append( newstr )\n              self.viewedQueue.append( newstr )\n          else:\n            print ""  ignoring"", newstr\n        else:\n          print ""  ignoring"", newstr\n      else:\n        print ""  ignoring"", newstr\n\n\ndef main():\n\n  if len(sys.argv)!=3:\n    print ""usage is ./minispider.py site link""\n    sys.exit(2)\n\n  mySpider = miniHTMLParser()\n\n  site = sys.argv[1]\n  link = sys.argv[2]\n\n  url_login_link = site+""/node?destination=node""\n  print ""\\nLogging in"", url_login_link\n  x = mySpider.loginSite( url_login_link )\n\n  while link != \'\':\n\n    print ""\\nChecking link "", link\n\n    # Get the file from the site and link\n    retfile = mySpider.gethtmlfile( site, link )\n\n    # Feed the file into the HTML parser\n    mySpider.feed(retfile)\n\n    # Search the retfile here\n\n    # Get the next link in level traversal order\n    link = mySpider.get_next_link()\n\n  mySpider.close()\n\n  print ""\\ndone\\n""\n\nif __name__ == ""__main__"":\n  main()\n\n', '\nTrust me nothing is better than curl.. . the following code can crawl 10,000 urls in parallel in less than 300 secs on Amazon EC2\nCAUTION: Don\'t hit the same domain at such a high speed.. .\n#! /usr/bin/env python\n# -*- coding: iso-8859-1 -*-\n# vi:ts=4:et\n# $Id: retriever-multi.py,v 1.29 2005/07/28 11:04:13 mfx Exp $\n\n#\n# Usage: python retriever-multi.py <file with URLs to fetch> [<# of\n#          concurrent connections>]\n#\n\nimport sys\nimport pycurl\n\n# We should ignore SIGPIPE when using pycurl.NOSIGNAL - see\n# the libcurl tutorial for more info.\ntry:\n    import signal\n    from signal import SIGPIPE, SIG_IGN\n    signal.signal(signal.SIGPIPE, signal.SIG_IGN)\nexcept ImportError:\n    pass\n\n\n# Get args\nnum_conn = 10\ntry:\n    if sys.argv[1] == ""-"":\n        urls = sys.stdin.readlines()\n    else:\n        urls = open(sys.argv[1]).readlines()\n    if len(sys.argv) >= 3:\n        num_conn = int(sys.argv[2])\nexcept:\n    print ""Usage: %s <file with URLs to fetch> [<# of concurrent connections>]"" % sys.argv[0]\n    raise SystemExit\n\n\n# Make a queue with (url, filename) tuples\nqueue = []\nfor url in urls:\n    url = url.strip()\n    if not url or url[0] == ""#"":\n        continue\n    filename = ""doc_%03d.dat"" % (len(queue) + 1)\n    queue.append((url, filename))\n\n\n# Check args\nassert queue, ""no URLs given""\nnum_urls = len(queue)\nnum_conn = min(num_conn, num_urls)\nassert 1 <= num_conn <= 10000, ""invalid number of concurrent connections""\nprint ""PycURL %s (compiled against 0x%x)"" % (pycurl.version, pycurl.COMPILE_LIBCURL_VERSION_NUM)\nprint ""----- Getting"", num_urls, ""URLs using"", num_conn, ""connections -----""\n\n\n# Pre-allocate a list of curl objects\nm = pycurl.CurlMulti()\nm.handles = []\nfor i in range(num_conn):\n    c = pycurl.Curl()\n    c.fp = None\n    c.setopt(pycurl.FOLLOWLOCATION, 1)\n    c.setopt(pycurl.MAXREDIRS, 5)\n    c.setopt(pycurl.CONNECTTIMEOUT, 30)\n    c.setopt(pycurl.TIMEOUT, 300)\n    c.setopt(pycurl.NOSIGNAL, 1)\n    m.handles.append(c)\n\n\n# Main loop\nfreelist = m.handles[:]\nnum_processed = 0\nwhile num_processed < num_urls:\n    # If there is an url to process and a free curl object, add to multi stack\n    while queue and freelist:\n        url, filename = queue.pop(0)\n        c = freelist.pop()\n        c.fp = open(filename, ""wb"")\n        c.setopt(pycurl.URL, url)\n        c.setopt(pycurl.WRITEDATA, c.fp)\n        m.add_handle(c)\n        # store some info\n        c.filename = filename\n        c.url = url\n    # Run the internal curl state machine for the multi stack\n    while 1:\n        ret, num_handles = m.perform()\n        if ret != pycurl.E_CALL_MULTI_PERFORM:\n            break\n    # Check for curl objects which have terminated, and add them to the freelist\n    while 1:\n        num_q, ok_list, err_list = m.info_read()\n        for c in ok_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print ""Success:"", c.filename, c.url, c.getinfo(pycurl.EFFECTIVE_URL)\n            freelist.append(c)\n        for c, errno, errmsg in err_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print ""Failed: "", c.filename, c.url, errno, errmsg\n            freelist.append(c)\n        num_processed = num_processed + len(ok_list) + len(err_list)\n        if num_q == 0:\n            break\n    # Currently no more I/O is pending, could do something in the meantime\n    # (display a progress bar, etc.).\n    # We just call select() to sleep until some more data is available.\n    m.select(1.0)\n\n\n# Cleanup\nfor c in m.handles:\n    if c.fp is not None:\n        c.fp.close()\n        c.fp = None\n    c.close()\nm.close()\n\n', ""\nAnother simple spider \nUses BeautifulSoup and urllib2. Nothing too sophisticated, just reads all a href's builds a list and goes though it.\n"", '\npyspider.py\n']",,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
how to extract links and titles from a .html page?,"
for my website, i'd like to add a new functionality.
I would like user to be able to upload his bookmarks backup file (from any browser if possible) so I can upload it to their profile and they don't have to insert all of them manually...
the only part i'm missing to do this it's the part of extracting title and URL from the uploaded file.. can anyone give a clue where to start or where to read? 
used search option and (How to extract data from a raw HTML file?) this is the most related question for mine and it doesn't talk about it..
I really don't mind if its using jquery or php
Thank you very much.
",73k,"
            41
        ","['\nThank you everyone, I GOT IT! \nThe final code:\n$html = file_get_contents(\'bookmarks.html\');\n//Create a new DOM document\n$dom = new DOMDocument;\n\n//Parse the HTML. The @ is used to suppress any parsing errors\n//that will be thrown if the $html string isn\'t valid XHTML.\n@$dom->loadHTML($html);\n\n//Get all links. You could also use any other tag name here,\n//like \'img\' or \'table\', to extract other tags.\n$links = $dom->getElementsByTagName(\'a\');\n\n//Iterate over the extracted links and display their URLs\nforeach ($links as $link){\n    //Extract and show the ""href"" attribute.\n    echo $link->nodeValue;\n    echo $link->getAttribute(\'href\'), \'<br>\';\n}\n\nThis shows you the anchor text assigned and the href for all  links in a .html file.\nAgain, thanks a lot.\n', '\nThis is probably sufficient:\n$dom = new DOMDocument;\n$dom->loadHTML($html);\nforeach ($dom->getElementsByTagName(\'a\') as $node)\n{\n  echo $node->nodeValue.\': \'.$node->getAttribute(""href"").""\\n"";\n}\n\n', ""\nAssuming the stored links are in a html file the best solution is probably to use a html parser such as PHP Simple HTML DOM Parser (never tried it myself). (The other option is to search using basic string search or regexp, and you should probably never use regexp to parse html).\nAfter reading the html file using the parser use it's functions to find the a tags:\nfrom the tutorial:\n// Find all links\nforeach($html->find('a') as $element)\n       echo $element->href . '<br>'; \n\n"", '\n$html = file_get_contents(\'your file path\');\n\n$dom = new DOMDocument;\n\n@$dom->loadHTML($html);\n\n$styles = $dom->getElementsByTagName(\'link\');\n\n$links = $dom->getElementsByTagName(\'a\');\n\n$scripts = $dom->getElementsByTagName(\'script\');\n\nforeach($styles as $style)\n{\n\n    if($style->getAttribute(\'href\')!=""#"")\n\n    {\n        echo $style->getAttribute(\'href\');\n        echo\'<br>\';\n    }\n}\n\nforeach ($links as $link){\n\n    if($link->getAttribute(\'href\')!=""#"")\n    {\n        echo $link->getAttribute(\'href\');\n        echo\'<br>\';\n    }\n}\n\nforeach($scripts as $script)\n{\n\n        echo $script->getAttribute(\'src\');\n        echo\'<br>\';\n\n}\n\n', ""\nI wanted to create a CSV of link paths and their text from html pages so I could rip menus etc from sites.\nIn this example you specify the domain you are interested in so you don't get off site links and then it produces a CSV per document\n/**\n * Extracts links to the given domain from the files and creates CSVs of the links\n */\n\n\n$LinkExtractor = new LinkExtractor('https://www.example.co.uk');\n\n$LinkExtractor->extract(__DIR__ . '/hamburger.htm');\n$LinkExtractor->extract(__DIR__ . '/navbar.htm');\n$LinkExtractor->extract(__DIR__ . '/footer.htm');\n\nclass LinkExtractor {\n    public $domain;\n\n    public function __construct($domain) {\n      $this->domain = $domain;\n    }\n\n    public function extract($file) {\n        $html = file_get_contents($file);\n        //Create a new DOM document\n        $dom = new DOMDocument;\n\n        //Parse the HTML. The @ is used to suppress any parsing errors\n        //that will be thrown if the $html string isn't valid XHTML.\n        @$dom->loadHTML($html);\n\n        //Get all links. You could also use any other tag name here,\n        //like 'img' or 'table', to extract other tags.\n        $links = $dom->getElementsByTagName('a');\n\n        $results = [];\n        //Iterate over the extracted links and display their URLs\n        foreach ($links as $link){\n            //Extract and sput the matching links in an array for the CSV\n            $href = $link->getAttribute('href');\n            $parts = parse_url($href);\n            if (!empty($parts['path']) && strpos($this->domain, $parts['host']) !== false) {\n                $results[$parts['path']] = [$parts['path'], $link->nodeValue];\n            }\n        }\n\n        asort($results);\n        // Make the CSV\n        $fp = fopen($file .'.csv', 'w');\n        foreach ($results as $fields) {\n            fputcsv($fp, $fields);\n        }\n        fclose($fp);\n    }\n}\n\n"", '\nHere is my work for one of my client and make it as a function to use everywhere.\nfunction getValidUrlsFrompage($source)\n  {\n    $links = [];\n    $content = file_get_contents($source);\n    $content = strip_tags($content, ""<a>"");\n    $subString = preg_split(""/<\\/a>/"", $content);\n    foreach ($subString as $val) {\n      if (strpos($val, ""<a href="") !== FALSE) {\n        $val = preg_replace(""/.*<a\\s+href=\\""/sm"", """", $val);\n        $val = preg_replace(""/\\"".*/"", """", $val);\n        $val = trim($val);\n      }\n      if (strlen($val) > 0 && filter_var($val, FILTER_VALIDATE_URL)) {\n        if (!in_array($val, $links)) {\n          $links[] = $val;\n        }\n      }\n    }\n    return $links;\n  }\n\nAnd use it like\n$links = getValidUrlsFrompage(""https://www.w3resource.com/"");\n\nAnd The expected output is get 99 URLs in an array,\nArray ( [0] => https://www.w3resource.com [1] => https://www.w3resource.com/html/HTML-tutorials.php [2] => https://www.w3resource.com/css/CSS-tutorials.php [3] => https://www.w3resource.com/javascript/javascript.php [4] => https://www.w3resource.com/html5/introduction.php [5] => https://www.w3resource.com/schema.org/introduction.php [6] => https://www.w3resource.com/phpjs/use-php-functions-in-javascript.php [7] => https://www.w3resource.com/twitter-bootstrap/tutorial.php [8] => https://www.w3resource.com/responsive-web-design/overview.php [9] => https://www.w3resource.com/zurb-foundation3/introduction.php [10] => https://www.w3resource.com/pure/ [11] => https://www.w3resource.com/html5-canvas/ [12] => https://www.w3resource.com/course/javascript-course.html [13] => https://www.w3resource.com/icon/ [14] => https://www.w3resource.com/linux-system-administration/installation.php [15] => https://www.w3resource.com/linux-system-administration/linux-commands-introduction.php [16] => https://www.w3resource.com/php/php-home.php [17] => https://www.w3resource.com/python/python-tutorial.php [18] => https://www.w3resource.com/java-tutorial/ [19] => https://www.w3resource.com/node.js/node.js-tutorials.php [20] => https://www.w3resource.com/ruby/ [21] => https://www.w3resource.com/c-programming/programming-in-c.php [22] => https://www.w3resource.com/sql/tutorials.php [23] => https://www.w3resource.com/mysql/mysql-tutorials.php [24] => https://w3resource.com/PostgreSQL/tutorial.php [25] => https://www.w3resource.com/sqlite/ [26] => https://www.w3resource.com/mongodb/nosql.php [27] => https://www.w3resource.com/API/google-plus/tutorial.php [28] => https://www.w3resource.com/API/youtube/tutorial.php [29] => https://www.w3resource.com/API/google-maps/index.php [30] => https://www.w3resource.com/API/flickr/tutorial.php [31] => https://www.w3resource.com/API/last.fm/tutorial.php [32] => https://www.w3resource.com/API/twitter-rest-api/ [33] => https://www.w3resource.com/xml/xml.php [34] => https://www.w3resource.com/JSON/introduction.php [35] => https://www.w3resource.com/ajax/introduction.php [36] => https://www.w3resource.com/html-css-exercise/index.php [37] => https://www.w3resource.com/javascript-exercises/ [38] => https://www.w3resource.com/jquery-exercises/ [39] => https://www.w3resource.com/jquery-ui-exercises/ [40] => https://www.w3resource.com/coffeescript-exercises/ [41] => https://www.w3resource.com/php-exercises/ [42] => https://www.w3resource.com/python-exercises/ [43] => https://www.w3resource.com/c-programming-exercises/ [44] => https://www.w3resource.com/csharp-exercises/ [45] => https://www.w3resource.com/java-exercises/ [46] => https://www.w3resource.com/sql-exercises/ [47] => https://www.w3resource.com/oracle-exercises/ [48] => https://www.w3resource.com/mysql-exercises/ [49] => https://www.w3resource.com/sqlite-exercises/ [50] => https://www.w3resource.com/postgresql-exercises/ [51] => https://www.w3resource.com/mongodb-exercises/ [52] => https://www.w3resource.com/twitter-bootstrap/examples.php [53] => https://www.w3resource.com/euler-project/ [54] => https://w3resource.com/w3skills/html5-quiz/ [55] => https://w3resource.com/w3skills/php-fundamentals/ [56] => https://w3resource.com/w3skills/sql-beginner/ [57] => https://w3resource.com/w3skills/python-beginner-quiz/ [58] => https://w3resource.com/w3skills/mysql-basic-quiz/ [59] => https://w3resource.com/w3skills/javascript-basic-skill-test/ [60] => https://w3resource.com/w3skills/javascript-advanced-quiz/ [61] => https://w3resource.com/w3skills/javascript-quiz-part-iii/ [62] => https://w3resource.com/w3skills/mongodb-basic-quiz/ [63] => https://www.w3resource.com/form-template/ [64] => https://www.w3resource.com/slides/ [65] => https://www.w3resource.com/convert/number/binary-to-decimal.php [66] => https://www.w3resource.com/excel/ [67] => https://www.w3resource.com/video-tutorial/php/some-basics-of-php.php [68] => https://www.w3resource.com/video-tutorial/javascript/list-of-tutorial.php [69] => https://www.w3resource.com/web-development-tools/firebug-tutorials.php [70] => https://www.w3resource.com/web-development-tools/useful-web-development-tools.php [71] => https://www.facebook.com/w3resource [72] => https://twitter.com/w3resource [73] => https://plus.google.com/+W3resource [74] => https://in.linkedin.com/in/w3resource [75] => https://feeds.feedburner.com/W3resource [76] => https://www.w3resource.com/ruby-exercises/ [77] => https://www.w3resource.com/graphics/matplotlib/ [78] => https://www.w3resource.com/python-exercises/numpy/index.php [79] => https://www.w3resource.com/python-exercises/pandas/index.php [80] => https://w3resource.com/plsql-exercises/ [81] => https://w3resource.com/swift-programming-exercises/ [82] => https://www.w3resource.com/angular/getting-started-with-angular.php [83] => https://www.w3resource.com/react/react-js-overview.php [84] => https://www.w3resource.com/vue/installation.php [85] => https://www.w3resource.com/jest/jest-getting-started.php [86] => https://www.w3resource.com/numpy/ [87] => https://www.w3resource.com/php/composer/a-gentle-introduction-to-composer.php [88] => https://www.w3resource.com/php/PHPUnit/a-gentle-introduction-to-unit-test-and-testing.php [89] => https://www.w3resource.com/laravel/laravel-tutorial.php [90] => https://www.w3resource.com/oracle/index.php [91] => https://www.w3resource.com/redis/index.php [92] => https://www.w3resource.com/cpp-exercises/ [93] => https://www.w3resource.com/r-programming-exercises/ [94] => https://w3resource.com/w3skills/ [95] => https://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US [96] => https://www.w3resource.com/privacy.php [97] => https://www.w3resource.com/about.php [98] => https://www.w3resource.com/contact.php [99] => https://www.w3resource.com/feedback.php [100] => https://www.w3resource.com/advertise.php )\n\nHope, this will help someone. And here is a gist - \nhttps://gist.github.com/ManiruzzamanAkash/74cffb9ffdfc92f57bd9cf214cf13491\n']",https://stackoverflow.com/questions/4423272/how-to-extract-links-and-titles-from-a-html-page,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pulling data from a webpage, parsing it for specific pieces, and displaying it","
I've been using this site for a long time to find answers to my questions, but I wasn't able to find the answer on this one.
I am working with a small group on a class project. We're to build a small ""game trading"" website that allows people to register, put in a game they have they want to trade, and accept trades from others or request a trade.
We have the site functioning long ahead of schedule so we're trying to add more to the site. One thing I want to do myself is to link the games that are put in to Metacritic.
Here's what I need to do. I need to (using asp and c# in visual studio 2012) get the correct game page on metacritic, pull its data, parse it for specific parts, and then display the data on our page. 
Essentially when you choose a game you want to trade for we want a small div to display with the game's information and rating. I'm wanting to do it this way to learn more and get something out of this project I didn't have to start with. 
I was wondering if anyone could tell me where to start. I don't know how to pull data from a page. I'm still trying to figure out if I need to try and write something to automatically search for the game's title and find the page that way or if I can find some way to go straight to the game's page. And once I've gotten the data, I don't know how to pull the specific information I need from it.
One of the things that doesn't make this easy is that I'm learning c++ along with c# and asp so I keep getting my wires crossed. If someone could point me in the right direction it would be a big help. Thanks
",102k,"
            19
        ","['\nThis small example uses HtmlAgilityPack, and using XPath selectors to get to the desired elements.\nprotected void Page_Load(object sender, EventArgs e)\n{\n    string url = ""http://www.metacritic.com/game/pc/halo-spartan-assault"";\n    var web = new HtmlAgilityPack.HtmlWeb();\n    HtmlDocument doc = web.Load(url);\n\n    string metascore = doc.DocumentNode.SelectNodes(""//*[@id=\\""main\\""]/div[3]/div/div[2]/div[1]/div[1]/div/div/div[2]/a/span[1]"")[0].InnerText;\n    string userscore = doc.DocumentNode.SelectNodes(""//*[@id=\\""main\\""]/div[3]/div/div[2]/div[1]/div[2]/div[1]/div/div[2]/a/span[1]"")[0].InnerText;\n    string summary = doc.DocumentNode.SelectNodes(""//*[@id=\\""main\\""]/div[3]/div/div[2]/div[2]/div[1]/ul/li/span[2]/span/span[1]"")[0].InnerText;\n}\n\nAn easy way to obtain the XPath for a given element is by using your web browser (I use Chrome) Developer Tools:\n\nOpen the Developer Tools (F12 or Ctrl + Shift + C on Windows or Command + Shift + C for Mac).\nSelect the element in the page that you want the XPath for.\nRight click the element in the ""Elements"" tab.\nClick on ""Copy as XPath"".\n\nYou can paste it exactly like that in c# (as shown in my code), but make sure to escape the quotes.\nYou have to make sure you use some error handling techniques because Web scraping can cause errors if they change the HTML formatting of the page.\nEdit\nPer @knocte\'s suggestion,  here is the link to the Nuget package for HTMLAgilityPack:\nhttps://www.nuget.org/packages/HtmlAgilityPack/\n', '\nI looked and Metacritic.com doesn\'t have an API.\nYou can use an HttpWebRequest to get the contents of a website as a string. \nusing System.Net;\nusing System.IO;\nusing System.Windows.Forms;\n\nstring result = null;\nstring url = ""http://www.stackoverflow.com"";\nWebResponse response = null;\nStreamReader reader = null;\n\ntry\n{\n    HttpWebRequest request = (HttpWebRequest)WebRequest.Create(url);\n    request.Method = ""GET"";\n    response = request.GetResponse();\n    reader = new StreamReader(response.GetResponseStream(), Encoding.UTF8);\n    result = reader.ReadToEnd();\n}\ncatch (Exception ex)\n{\n    // handle error\n    MessageBox.Show(ex.Message);\n}\nfinally\n{\n    if (reader != null)\n        reader.Close();\n    if (response != null)\n        response.Close();\n}\n\nThen you can parse the string for the data that you want by taking advantage of Metacritic\'s use of meta tags. Here\'s the information they have available in meta tags:\n\nog:title\nog:type\nog:url\nog:image\nog:site_name\nog:description\n\nThe format of each tag is: meta name=""og:title"" content=""In a World...""\n', '\nI recommend Dcsoup.  There\'s a nuget package for it and it uses CSS selectors so it is familiar if you use jquery.  I\'ve tried others but it is the best and easiest to use that I\'ve found.  There\'s not much documentation, but it\'s open source and a port of the java jsoup library that has good documentation. (Documentation for the .NET API here.) I absolutely love it.\nvar timeoutInMilliseconds = 5000;\nvar uri = new Uri(""http://www.metacritic.com/game/pc/fallout-4"");\nvar doc = Supremes.Dcsoup.Parse(uri, timeoutInMilliseconds);\n\n// <span itemprop=""ratingValue"">86</span>\nvar ratingSpan = doc.Select(""span[itemprop=ratingValue]"");\nint ratingValue = int.Parse(ratingSpan.Text);\n\n// selectors match both critic and user scores\nvar scoreDiv = doc.Select(""div.score_summary"");\nvar scoreAnchor = scoreDiv.Select(""a.metascore_anchor"");\nint criticRating = int.Parse(scoreAnchor[0].Text);\nfloat userRating = float.Parse(scoreAnchor[1].Text);\n\n', '\nI\'d recomend you WebsiteParser - it\'s based on HtmlAgilityPack (mentioned by Hanlet Esca帽o) but it makes web scraping easier with attributes and css selectors:\nclass PersonModel\n{\n    [Selector(""#BirdthDate"")]\n    [Converter(typeof(DateTimeConverter))]\n    public DateTime BirdthDate { get; set; }\n}\n\n// ...\n\nPersonModel person = WebContentParser.Parse<PersonModel>(html);\n\nNuget link\n']",https://stackoverflow.com/questions/18065526/pulling-data-from-a-webpage-parsing-it-for-specific-pieces-and-displaying-it,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parse HTML content in VBA,"
I have a question relating to HTML parsing. I have a website with some products and I would like to catch text within page into my current spreadsheet. This spreadsheet is quite big but contains ItemNbr in 3rd column, I expect the text in the 14th column and one row corresponds to one product (item).
My idea is to fetch the 'Material' on the webpage which is inside the Innertext after  tag. The id number changes from one page to page (sometimes ).
Here is the structure of the website:
<div style=""position:relative;"">
    <div></div>
    <table id=""list-table"" width=""100%"" tabindex=""1"" cellspacing=""0"" cellpadding=""0"" border=""0"" role=""grid"" aria-multiselectable=""false"" aria-labelledby=""gbox_list-table"" class=""ui-jqgrid-btable"" style=""width: 930px;"">
        <tbody>
            <tr class=""jqgfirstrow"" role=""row"" style=""height:auto"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""1"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""2"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""3"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""4"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""5"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""6"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""7"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td role=""gridcell"" style=""padding-left:10px"" title=""Material"" aria-describedby=""list-table_"">Material</td>
                <td role=""gridcell"" style="""" title=""600D polyester."" aria-describedby=""list-table_"">600D polyester.</td>
            </tr>           
            <tr ...>
            </tr>
        </tbody>
    </table> </div>

I would like to get ""600D Polyester"" as a result.
My (not working) code snippet is as is:
Sub ParseMaterial()

    Dim Cell As Integer
    Dim ItemNbr As String

    Dim AElement As Object
    Dim AElements As IHTMLElementCollection
Dim IE As MSXML2.XMLHTTP60
Set IE = New MSXML2.XMLHTTP60

Dim HTMLDoc As MSHTML.HTMLDocument
Dim HTMLBody As MSHTML.HTMLBody

Set HTMLDoc = New MSHTML.HTMLDocument
Set HTMLBody = HTMLDoc.body

For Cell = 1 To 5                            'I iterate through the file row by row

    ItemNbr = Cells(Cell, 3).Value           'ItemNbr isin the 3rd Column of my spreadsheet

    IE.Open ""GET"", ""http://www.example.com/?item="" & ItemNbr, False
    IE.send

    While IE.ReadyState <> 4
        DoEvents
    Wend

    HTMLBody.innerHTML = IE.responseText

    Set AElements = HTMLDoc.getElementById(""list-table"").getElementsByTagName(""tr"")
    For Each AElement In AElements
        If AElement.Title = ""Material"" Then
            Cells(Cell, 14) = AElement.nextNode.value     'I write the material in the 14th column
        End If
    Next AElement

        Application.Wait (Now + TimeValue(""0:00:2""))

Next Cell

Thanks for your help !
",83k,"
            15
        ","['\nJust a couple things that hopefully will get you in the right direction:\n\nclean up a bit: remove the readystate property testing loop. The value returned by the readystate property will never change in this context - code will pause after the send instruction, to resume only once the server response is received, or has failed to do so. The readystate property will be set accordingly, and the code will resume execution. You should still test for the ready state, but the loop is just unnecessary\n\ntarget the right HTML elements: you are searching through the tr elements - while the logic of how you use these elements in your code actually looks to point to td elements\n\nmake sure the properties are actually available for the objects you are using them on: to help you with this, try and declare all your variable as specific objects instead of the generic Object. This will activate intellisense. If you have a difficult time finding the actual name of your object as defined in the relevant library in a first place, declare it as the generic Object, run your code, and then inspect the type of the object - by printing typename(your_object) to the debug window for instance. This should put you on your way\n\n\nI have also included some code below that may help. If you still can\'t get this to work and you can share your urls - plz do that.\nSub getInfoWeb()\n\n    Dim cell As Integer\n    Dim xhr As MSXML2.XMLHTTP60\n    Dim doc As MSHTML.HTMLDocument\n    Dim table As MSHTML.HTMLTable\n    Dim tableCells As MSHTML.IHTMLElementCollection\n    \n    Set xhr = New MSXML2.XMLHTTP60\n   \n    For cell = 1 To 5\n        \n        ItemNbr = Cells(cell, 3).Value\n        \n        With xhr\n        \n            .Open ""GET"", ""http://www.example.com/?item="" & ItemNbr, False\n            .send\n            \n            If .readyState = 4 And .Status = 200 Then\n                Set doc = New MSHTML.HTMLDocument\n                doc.body.innerHTML = .responseText\n            Else\n                MsgBox ""Error"" & vbNewLine & ""Ready state: "" & .readyState & _\n                vbNewLine & ""HTTP request status: "" & .Status\n            End If\n            \n        End With\n        \n        Set table = doc.getElementById(""list-table"")\n        Set tableCells = table.getElementsByTagName(""td"")\n        \n        For Each tableCell In tableCells\n            If tableCell.getAttribute(""title"") = ""Material"" Then\n                Cells(cell, 14).Value = tableCell.NextSibling.innerHTML\n            End If\n        Next tableCell\n    \n    Next cell\n    \nEnd Sub\n\nEDIT: as a follow-up to the further information you provided in the comment below - and the additional comments I have added\n\'Determine your product number\n    \'Open an xhr for your source url, and retrieve the product number from there - search for the tag which\n    \'text include the ""productnummer:"" substring, and extract the product number from the outerstring\n    \'OR\n    \'if the product number consistently consists of the fctkeywords you are entering in your source url\n    \'with two ""0"" appended - just build the product number like that\n\'Open an new xhr for this url ""http://www.pfconcept.com/cgi-bin/wspd_pcdb_cgi.sh/y/y2productspec-ajax.p?itemc="" & product_number & ""&_search=false&rows=-1&page=1&sidx=&sord=asc""\n\'Load the response in an XML document, and retrieve the material information\n\nSub getInfoWeb()\n\n    Dim xhr As MSXML2.XMLHTTP60\n    Dim doc As MSXML2.DOMDocument60\n    Dim xmlCell As MSXML2.IXMLDOMElement\n    Dim xmlCells As MSXML2.IXMLDOMNodeList\n    Dim materialValueElement As MSXML2.IXMLDOMElement\n    \n    Set xhr = New MSXML2.XMLHTTP60\n        \n        With xhr\n            \n            .Open ""GET"", ""http://www.pfconcept.com/cgi-bin/wspd_pcdb_cgi.sh/y/y2productspec-ajax.p?itemc=10031700&_search=false&rows=-1&page=1&sidx=&sord=asc"", False\n            .send\n            \n            If .readyState = 4 And .Status = 200 Then\n                Set doc = New MSXML2.DOMDocument60\n                doc.LoadXML .responseText\n            Else\n                MsgBox ""Error"" & vbNewLine & ""Ready state: "" & .readyState & _\n                vbNewLine & ""HTTP request status: "" & .Status\n            End If\n            \n        End With\n        \n        Set xmlCells = doc.getElementsByTagName(""cell"")\n\n        For Each xmlCell In xmlCells\n            If xmlCell.Text = ""Materiaal"" Then\n                Set materialValueElement = xmlCell.NextSibling\n            End If\n        Next\n        \n        MsgBox materialValueElement.Text\n    \nEnd Sub\n\nEDIT2: an alternative automating IE\nSub searchWebViaIE()\n    Dim ie As SHDocVw.InternetExplorer\n    Dim doc As MSHTML.HTMLDocument\n    Dim anchors As MSHTML.IHTMLElementCollection\n    Dim anchor As MSHTML.HTMLAnchorElement\n    Dim prodSpec As MSHTML.HTMLAnchorElement\n    Dim tableCells As MSHTML.IHTMLElementCollection\n    Dim materialValueElement As MSHTML.HTMLTableCell\n    Dim tableCell As MSHTML.HTMLTableCell\n    \n    Set ie = New SHDocVw.InternetExplorer\n    \n    With ie\n        .navigate ""http://www.pfconcept.com/cgi-bin/wspd_pcdb_cgi.sh/y/y2facetmain.p?fctkeywords=100317&world=general#tabs-4""\n        .Visible = True\n        \n        Do While .readyState <> READYSTATE_COMPLETE Or .Busy = True\n            DoEvents\n        Loop\n        \n        Set doc = .document\n        \n        Set anchors = doc.getElementsByTagName(""a"")\n        \n        For Each anchor In anchors\n            If InStr(anchor.innerHTML, ""Product Specificatie"") <> 0 Then\n                anchor.Click\n                Exit For\n            End If\n        Next anchor\n        \n        Do While .readyState <> READYSTATE_COMPLETE Or .Busy = True\n            DoEvents\n        Loop\n    \n    End With\n        \n    For Each anchor In anchors\n        If InStr(anchor.innerHTML, ""Product Specificatie"") <> 0 Then\n            Set prodSpec = anchor\n        End If\n    Next anchor\n    \n    Set tableCells = doc.getElementById(""list-table"").getElementsByTagName(""td"")\n    \n    If Not tableCells Is Nothing Then\n        For Each tableCell In tableCells\n            If tableCell.innerHTML = ""Materiaal"" Then\n                Set materialValueElement = tableCell.NextSibling\n            End If\n        Next tableCell\n    End If\n    \n    MsgBox materialValueElement.innerHTML\n    \nEnd Sub\n\n', '\nNot related to tables or Excel ( I use MS-Access 2013) but directly related to the topic title. My solution is \nPrivate Sub Sample(urlSource)\nDim httpRequest As New WinHttpRequest\nDim doc As MSHTML.HTMLDocument\nDim tags As MSHTML.IHTMLElementCollection\nDim tag As MSHTML.HTMLHtmlElement\nhttpRequest.Option(WinHttpRequestOption_UserAgentString) = ""Mozilla/4.0 (compatible;MSIE 7.0; Windows NT 6.0)""\nhttpRequest.Open ""GET"", urlSource\nhttpRequest.send \' fetching webpage\nSet doc = New MSHTML.HTMLDocument\ndoc.body.innerHTML = httpRequest.responseText\nSet tags = doc.getElementsByTagName(""a"")\ni = 1\nFor Each tag In tags\n  Debug.Print i\n  Debug.Print tag.href\n  Debug.Print tag.innerText\n  \'Debug.Print tag.Attributes(""any other attributes you need"")() \' may return an object\n  i = i + 1\n  If i Mod 50 = 0 Then Stop\n  \' or code to store results in a table\nNext\nEnd Sub\n\n']",https://stackoverflow.com/questions/25488687/parse-html-content-in-vba,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do Google's crawlers interpret Javascript? What if I load a page through AJAX? [closed],"






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.


Closed 10 years ago.







                        Improve this question
                    



When a user enters my page, I have to make another AJAX call...to load data inside a div.
That's just how my application works.
The problem is...when I view the source of this code, it does not contain the source of that AJAX.  Of course, when I do wget URL ...it also does not show the AJAX HTML. Makes sense.
But what about Google? Will Google be able to crawl the content, as if it's a browser?  How do I allow Google to crawl my page just like a user would see it?
",13k,"
            15
        ","['\nDespite the answers above, apparently it does interpret JavaScript, to an extent, according to Matt Cutts:\n\n""For a while, we were scanning within JavaScript, and we were looking for links. Google has gotten smarter about JavaScript and can execute some JavaScript. I wouldn\'t say that we execute all JavaScript, so there are some conditions in which we don\'t execute JavaScript. Certainly there are some common, well-known JavaScript things like Google Analytics, which you wouldn\'t even want to execute because you wouldn\'t want to try to generate phantom visits from Googlebot into your Google Analytics"".\n\n(Why answer an answered question? Mostly because I just saw it because of a duplicate question posted today, and didn\'t see this info here.)\n', '\nActually... Google does have a solution for crawling Ajax applications...\nhttp://code.google.com/web/ajaxcrawling/docs/getting-started.html\n', '\nUpdated: From the answer to this question about ""Ajax generated content, crawling and black listing"" I found this document about the way Google crawls AJAX requests which is part of a collection of documents about Making AJAX Applications Crawlable. \nIn short, it means you need to use <a href=""#!data"">...</a> rather than <a href=""#data"">...</a> and then supply a real server-side answer to the URL path/to/path?_escaped_fragment_=data.\nAlso consider a <link/> tag to supply crawlers with a hint to SEO-friendly content. <link rel=""canonical""/>, which this article explains a bit, is a good candidate\nNote: I took the answer from: https://stackoverflow.com/questions/10006825/search-engine-misunderstanting/10006925#comment12792862_10006925  because it seems I can\'t delete mine here.\n', '\nWhat I do in this situation is always initially populate the page with content based upon the default parameters of whatever the Ajax call is doing. Then I only use the ajax javascript to do updates to the page.\n', '\nAs other answers say, Google\'s crawler (and I believe those of other search engines) does not interpret Javascript -- and you should not try to differentiate by user-agent or the like (at the risk of having your site downgraded or blocked for presenting different contents to users vs robots).  Rather, do offer some (perhaps minimal) level of content to visitors that have Javascript blocked for whatever reason (including the cases where the reason is ""being robots"";-) -- after all, that\'s the very reason the noscript tag exists... to make it very, very easy to offer such ""minimal level of content"" (or, more than minimal, if you so choose;-) to non-users of Javascript!\n', '\nWeb crawlers have a difficult time with ajax and javascript that dynamically loads content.  This site has some ideas that show you how to help google index your site http://www.softwaredeveloper.com/features/google-ajax-play-nice-061907/\n', '\nIf you make your pages such that they will work with OR without javascript (i.e. fall back to using frames or standard GET / POST requests to the server if javascript fails, either automatically, or via a ""display as plain html"" link ), it will be much easier for search engines to crawl the page.\nIt makes sense for them not to crawl ""dynamic"" content - because it is just that...dynamic.\nMy understanding is that in most situations, Google does not crawl the client-side-dynamic-content.\n', '\nNow It looks ike Google bot is not limited to simple lynx like browser.\nGoogle bot tries to grab the Humanly visible and Humanly contrasting text to give importance in different sectors of page. So it renders the page with a Layout Engine just like another browser like FF or Chrome have.\nIt might even have v8 Javascript Engine support. and the bot might load the page and wait till dom is ready and may even wait for few seconds for the page to come to a stable view. and then crop the contrasting text.\n']",https://stackoverflow.com/questions/2061844/do-googles-crawlers-interpret-javascript-what-if-i-load-a-page-through-ajax,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetch contents(loaded through AJAX call) of a web page,"
I am a beginner to crawling. I have a requirement to fetch the posts and comments from a link. I want to automate this process. I considered using webcrawler and jsoup for this but was told that webcrawlers are mostly used for websites with greater depth. 
Sample for a page: Jive community website
For this page, when I view the source of the page, I can see only the post and not the comments. Think this is because comments are fetched through an AJAX call to the server. 
Hence, when I use jsoup, it doesn't fetch the comments. 
So how can I automate the process of fetching posts and comments?
",10k,"
            6
        ","[""\nJsoup is a html parser only. Unfortunately it's not possible to parse any javascript / ajax content, since jsoup can't execute those.\nThe solution: using a library which can handle Scripts.\nHere are some examples i know:\n\nHtmlUnit\nJava Script Engine\nApache Commons BSF\nRhino\n\nIf such a library doesn't support parsing or selectors, you can at least use them to get Html out of the scripts (which then can be parsed by jsoup).\n"", '\nJsoup does not handle with Javascript and Ajax, so you need to use Htmlunit or selenium.\nAfter loading page using Htmlunit or any you can use jsoup for rest of task.\n']",https://stackoverflow.com/questions/20633294/fetch-contentsloaded-through-ajax-call-of-a-web-page,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I handle Javascript in a Perl web crawler?,"
I would like to crawl a website, the problem is, that its full of JavaScript things, such as buttons and such that when they are pressed, they do not change the URL, but the data on the page is changed. 
Usually I use LWP / Mechanize etc to crawl sites, but neither support JavaScript.
any idea?
",5k,"
            19
        ","['\nAnother option might be Selenium with WWW::Selenium module\n', ""\nThe WWW::Scripter module has a JavaScript plugin that may be useful. Can't say I've used it myself, however.\n"", '\nWWW::Mechanize::Firefox might be of use.  that way you can have Firefox handle the complex JavaScript issues and then extract the resultant html.\n', '\nI would suggest HtmlUnit and Perl wrapper: WWW::HtmlUnit.\n']",https://stackoverflow.com/questions/3769015/how-can-i-handle-javascript-in-a-perl-web-crawler,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Detecting 'stealth' web-crawlers,"
What options are there to detect web-crawlers that do not want to be detected?
(I know that listing detection techniques will allow the smart stealth-crawler programmer to make a better spider, but I do not think that we will ever be able to block smart stealth-crawlers anyway, only the ones that make mistakes.)
I'm not talking about the nice crawlers such as Googlebot and Yahoo! Slurp.
I consider a bot nice if it:

identifies itself as a bot in the user agent string
reads robots.txt (and obeys it)

I'm talking about the bad crawlers, hiding behind common user agents, using my bandwidth and never giving me anything in return.
There are some trapdoors that can be constructed updated list (thanks Chris, gs):

Adding a directory only listed (marked as disallow) in the robots.txt,
Adding invisible links (possibly marked as rel=""nofollow""?),

style=""display: none;"" on link or parent container
placed underneath another element with higher z-index


detect who doesn't understand CaPiTaLiSaTioN,
detect who tries to post replies but always fail the Captcha.
detect GET requests to POST-only resources
detect interval between requests
detect order of pages requested
detect who (consistently) requests HTTPS resources over HTTP
detect who does not request image file (this in combination with a list of user-agents of known image capable browsers works surprisingly nice)

Some traps would be triggered by both 'good' and 'bad' bots.
you could combine those with a whitelist:

It trigger a trap
It request robots.txt?
It doest not trigger another trap because it obeyed robots.txt

One other important thing here is:
Please consider blind people using a screen readers: give people a way to contact you, or solve a (non-image) Captcha to continue browsing.
What methods are there to automatically detect the web crawlers trying to mask themselves as normal human visitors.
The question is not: How do I catch every crawler. The question is: How can I maximize the chance of detecting a crawler.
Some spiders are really good, and actually parse and understand HTML, xhtml, CSS JavaScript, VBScript etc...
I have no illusions: I won't be able to beat them.
You would however be surprised how stupid some crawlers are. With the best example  of stupidity (in my opinion) being: cast all URLs to lower case before requesting them.
And then there is a whole bunch of crawlers that are just 'not good enough' to avoid the various trapdoors.
",27k,"
            112
        ","[""\nA while back, I worked with a smallish hosting company to help them implement a solution to this.  The system I developed examined web server logs for excessive activity from any given IP address and issued firewall rules to block offenders.  It included whitelists of IP addresses/ranges based on http://www.iplists.com/, which were then updated automatically as needed by checking claimed user-agent strings and, if the client claimed to be a legitimate spider but not on the whitelist, it performed DNS/reverse-DNS lookups to verify that the source IP address corresponds to the claimed owner of the bot.  As a failsafe, these actions were reported to the admin by email, along with links to black/whitelist the address in case of an incorrect assessment.\nI haven't talked to that client in 6 months or so, but, last I heard, the system was performing quite effectively.\nSide point:  If you're thinking about doing a similar detection system based on hit-rate-limiting, be sure to use at least one-minute (and preferably at least five-minute) totals.  I see a lot of people talking about these kinds of schemes who want to block anyone who tops 5-10 hits in a second, which may generate false positives on image-heavy pages (unless images are excluded from the tally) and will generate false positives when someone like me finds an interesting site that he wants to read all of, so he opens up all the links in tabs to load in the background while he reads the first one.\n"", '\nSee Project Honeypot - they\'re setting up bot traps on large scale (and have DNSRBL with their IPs).\nUse tricky URLs and HTML:\n<a href=""//example.com/""> = http://example.com/ on http pages.\n<a href=""page&amp;&#x23;hash""> = page& + #hash\n\nIn HTML you can use plenty of tricks with comments, CDATA elements, entities, etc:\n<a href=""foo<!--bar-->""> (comment should not be removed)\n<script>var haha = \'<a href=""bot"">\'</script>\n<script>// <!-- </script> <!--><a href=""bot""> <!-->\n\n', '\nAn easy solution is to create a link and make it invisible\n<a href=""iamabot.script"" style=""display:none;"">Don\'t click me!</a>\n\nOf course you should expect that some people who look at the source code follow that link just to see where it leads. But you could present those users with a captcha...\nValid crawlers would, of course, also follow the link. But you should not implement a rel=nofollow, but look for the sign of a valid crawler. (like the user agent)\n', ""\nOne thing you didn't list, that are used commonly to detect bad crawlers.\nHit speed, good web crawlers will break their hits up so they don't deluge a site with requests.  Bad ones will do one of three things:\n\nhit sequential links one after the other\nhit sequential links in some paralell sequence (2 or more at a time.)\nhit sequential links at a fixed interval\n\nAlso, some offline browsing programs will slurp up a number of pages, I'm not sure what kind of threshold you'd want to use, to start blocking by IP address.\nThis method will also catch mirroring programs like fmirror or wget.\nIf the bot randomizes the time interval, you could check to see if the links are traversed in a sequential or depth-first manner, or you can see if the bot is traversing a huge amount of text (as in words to read) in a too-short period of time.  Some sites limit the number of requests per hour, also.\nActually, I heard an idea somewhere, I don't remember where, that if a user gets too much data, in terms of kilobytes, they can be presented with a captcha asking them to prove they aren't a bot.  I've never seen that implemented though.\n\nUpdate on Hiding Links\n\nAs far as hiding links goes, you can put a div under another, with CSS (placing it first in the draw order) and possibly setting the z-order.  A bot could not ignore that, without parsing all your javascript to see if it is a menu.  To some extent, links inside invisible DIV elements also can't be ignored without the bot parsing all the javascript.\nTaking that idea to completion, uncalled javascript which could potentially show the hidden elements would possilby fool a subset of javascript parsing bots.  And, it is not a lot of work to implement.\n"", ""\nIt's not actually that easy to keep up with the good user agent strings. Browser versions come and go. Making a statistic about user agent strings by different behaviors can reveal interesting things.\nI don't know how far this could be automated, but at least it is one differentiating thing.\n"", '\nOne simple bot detection method I\'ve heard of for forms is the hidden input technique. If you are trying to secure a form put a input in the form with an id that looks completely legit. Then use css in an external file to hide it. Or if you are really paranoid, setup something like jquery to hide the input box on page load. If you do this right I imagine it would be very hard for a bot to figure out. You know those bots have it in there nature to fill out everything on a page especially if you give your hidden input an id of something like id=""fname"", etc.\n', '\nUntested, but here is a nice list of user-agents you could make a regular expression out of.  Could get you most of the way there:\nADSARobot|ah-ha|almaden|aktuelles|Anarchie|amzn_assoc|ASPSeek|ASSORT|ATHENS|Atomz|attach|attache|autoemailspider|BackWeb|Bandit|BatchFTP|bdfetch|big.brother|BlackWidow|bmclient|Boston\\ Project|BravoBrian\\ SpiderEngine\\ MarcoPolo|Bot\\ mailto:craftbot@yahoo.com|Buddy|Bullseye|bumblebee|capture|CherryPicker|ChinaClaw|CICC|clipping|Collector|Copier|Crescent|Crescent\\ Internet\\ ToolPak|Custo|cyberalert|DA$|Deweb|diagem|Digger|Digimarc|DIIbot|DISCo|DISCo\\ Pump|DISCoFinder|Download\\ Demon|Download\\ Wonder|Downloader|Drip|DSurf15a|DTS.Agent|EasyDL|eCatch|ecollector|efp@gmx\\.net|Email\\ Extractor|EirGrabber|email|EmailCollector|EmailSiphon|EmailWolf|Express\\ WebPictures|ExtractorPro|EyeNetIE|FavOrg|fastlwspider|Favorites\\ Sweeper|Fetch|FEZhead|FileHound|FlashGet\\ WebWasher|FlickBot|fluffy|FrontPage|GalaxyBot|Generic|Getleft|GetRight|GetSmart|GetWeb!|GetWebPage|gigabaz|Girafabot|Go\\!Zilla|Go!Zilla|Go-Ahead-Got-It|GornKer|gotit|Grabber|GrabNet|Grafula|Green\\ Research|grub-client|Harvest|hhjhj@yahoo|hloader|HMView|HomePageSearch|http\\ generic|HTTrack|httpdown|httrack|ia_archiver|IBM_Planetwide|Image\\ Stripper|Image\\ Sucker|imagefetch|IncyWincy|Indy*Library|Indy\\ Library|informant|Ingelin|InterGET|Internet\\ Ninja|InternetLinkagent|Internet\\ Ninja|InternetSeer\\.com|Iria|Irvine|JBH*agent|JetCar|JOC|JOC\\ Web\\ Spider|JustView|KWebGet|Lachesis|larbin|LeechFTP|LexiBot|lftp|libwww|likse|Link|Link*Sleuth|LINKS\\ ARoMATIZED|LinkWalker|LWP|lwp-trivial|Mag-Net|Magnet|Mac\\ Finder|Mag-Net|Mass\\ Downloader|MCspider|Memo|Microsoft.URL|MIDown\\ tool|Mirror|Missigua\\ Locator|Mister\\ PiX|MMMtoCrawl\\/UrlDispatcherLLL|^Mozilla$|Mozilla.*Indy|Mozilla.*NEWT|Mozilla*MSIECrawler|MS\\ FrontPage*|MSFrontPage|MSIECrawler|MSProxy|multithreaddb|nationaldirectory|Navroad|NearSite|NetAnts|NetCarta|NetMechanic|netprospector|NetResearchServer|NetSpider|Net\\ Vampire|NetZIP|NetZip\\ Downloader|NetZippy|NEWT|NICErsPRO|Ninja|NPBot|Octopus|Offline\\ Explorer|Offline\\ Navigator|OpaL|Openfind|OpenTextSiteCrawler|OrangeBot|PageGrabber|Papa\\ Foto|PackRat|pavuk|pcBrowser|PersonaPilot|Ping|PingALink|Pockey|Proxy|psbot|PSurf|puf|Pump|PushSite|QRVA|RealDownload|Reaper|Recorder|ReGet|replacer|RepoMonkey|Robozilla|Rover|RPT-HTTPClient|Rsync|Scooter|SearchExpress|searchhippo|searchterms\\.it|Second\\ Street\\ Research|Seeker|Shai|Siphon|sitecheck|sitecheck.internetseer.com|SiteSnagger|SlySearch|SmartDownload|snagger|Snake|SpaceBison|Spegla|SpiderBot|sproose|SqWorm|Stripper|Sucker|SuperBot|SuperHTTP|Surfbot|SurfWalker|Szukacz|tAkeOut|tarspider|Teleport\\ Pro|Templeton|TrueRobot|TV33_Mercator|UIowaCrawler|UtilMind|URLSpiderPro|URL_Spider_Pro|Vacuum|vagabondo|vayala|visibilitygap|VoidEYE|vspider|Web\\ Downloader|w3mir|Web\\ Data\\ Extractor|Web\\ Image\\ Collector|Web\\ Sucker|Wweb|WebAuto|WebBandit|web\\.by\\.mail|Webclipping|webcollage|webcollector|WebCopier|webcraft@bea|webdevil|webdownloader|Webdup|WebEMailExtrac|WebFetch|WebGo\\ IS|WebHook|Webinator|WebLeacher|WEBMASTERS|WebMiner|WebMirror|webmole|WebReaper|WebSauger|Website|Website\\ eXtractor|Website\\ Quester|WebSnake|Webster|WebStripper|websucker|webvac|webwalk|webweasel|WebWhacker|WebZIP|Wget|Whacker|whizbang|WhosTalking|Widow|WISEbot|WWWOFFLE|x-Tractor|^Xaldon\\ WebSpider|WUMPUS|Xenu|XGET|Zeus.*Webster|Zeus [NC]\n\nTaken from:\nhttp://perishablepress.com/press/2007/10/15/ultimate-htaccess-blacklist-2-compressed-version/\n', '\nYou can also check referrals. No referral could raise bot suspition. Bad referral means certainly it is not browser.\n\nAdding invisible links (possibly marked as rel=""nofollow""?),\n\n* style=""display: none;"" on link or parent container\n* placed underneath another element with higher z-index\n\nI would\'nt do that. You can end up blacklisted by google for black hat SEO :)\n', ""\nI currently work for a company that scans web sites in order to classify them. We also check sites for malware.\nIn my experience the number one blockers of our web crawler (which of course uses a IE or Firefox UA and does not obey robots.txt. Duh.) are sites intentionally hosting malware.  It's a pain because the site then falls back to a human who has to manually load the site, classify it and check it for malware.\nI'm just saying, by blocking web crawlers you're putting yourself in some bad company.\nOf course, if they are horribly rude and suck up tons of your bandwidth it's a different story because then you've got a good reason.\n"", ""\nPeople keep addressing broad crawlers but not crawlers that are specialized for your website.\nI write stealth crawlers and if they are individually built no amount of honey pots or hidden links will have any effect whatsoever - the only real way to detect specialised crawlers is by inspecting connection patterns. \nThe best systems use AI (e.g. Linkedin) use AI to address this.\nThe easiest solution is write log parsers that analyze IP connections and simply blacklist those IPs or serve captcha, at least temporary. \ne.g.\nif IP X is seen every 2 seconds connecting to foo.com/cars/*.html but not any other pages - it's most likely a bot or a hungry power user.\nAlternatively there are various javascript challenges that act as protection (e.g. Cloudflare's anti-bot system), but those are easily solvable, you can write something custom and that might be enough deterrent to make it not worth the effort for the crawler.\nHowever you must ask a question are you willing to false-positive legit users and introduce inconvenience for them to prevent bot traffic. Protecting public data is an impossible paradox.\n"", ""\nshort answer: if a mid level programmer knows what he's doing you won't be able to detect a crawler without affecting the real user. Having your information publicly you won't be able to defend it against a crawler... it's like the 1st amendment right :)\n""]",https://stackoverflow.com/questions/233192/detecting-stealth-web-crawlers,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Click a Button in Scrapy,"
I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking).
I found out that Scrapy can handle forms (like logins) as shown here. But the problem is that there is no form to fill out, so it's not exactly what I need.
How can I simply click a button, which then shows the information I need?
Do I have to use an external library like mechanize or lxml?
",76k,"
            67
        ","[""\nScrapy cannot interpret javascript.\nIf you absolutely must interact with the javascript on the page, you want to be using Selenium.\nIf using Scrapy, the solution to the problem depends on what the button is doing.\nIf it's just showing content that was previously hidden, you can scrape the data without a problem, it doesn't matter that it wouldn't appear in the browser, the HTML is still there.\nIf it's fetching the content dynamically via AJAX when the button is pressed, the best thing to do is to view the HTTP request that goes out when you press the button using a tool like Firebug. You can then just request the data directly from that URL.\n\nDo I have to use an external library like mechanize or lxml?\n\nIf you want to interpret javascript, yes you need to use a different library, although neither of those two fit the bill. Neither of them know anything about javascript. Selenium is the way to go.\nIf you can give the URL of the page you're working on scraping I can take a look.\n"", '\nSelenium browser provide very nice solution. Here is an example (pip install -U selenium):\nfrom selenium import webdriver\n\nclass northshoreSpider(Spider):\n    name = \'xxx\'\n    allowed_domains = [\'www.example.org\']\n    start_urls = [\'https://www.example.org\']\n\n    def __init__(self):\n        self.driver = webdriver.Firefox()\n\n    def parse(self,response):\n            self.driver.get(\'https://www.example.org/abc\')\n\n            while True:\n                try:\n                    next = self.driver.find_element_by_xpath(\'//*[@id=""BTN_NEXT""]\')\n                    url = \'http://www.example.org/abcd\'\n                    yield Request(url,callback=self.parse2)\n                    next.click()\n                except:\n                    break\n\n            self.driver.close()\n\n    def parse2(self,response):\n        print \'you are here!\'\n\n', '\nTo properly and fully use JavaScript you need a full browser engine and this is possible only with Watir/WatiN/Selenium etc.\n', ""\nAlthough it's an old thread I've found quite useful to use Helium (built on top of Selenium) for this purpose and far more easier/simpler than using Selenium. It will be something like the following:\nfrom helium import *\n\nstart_firefox('your_url')\ns = S('path_to_your_button')\nclick(s)\n...\n\n\n""]",https://stackoverflow.com/questions/6682503/click-a-button-in-scrapy,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to give URL to scrapy for crawling?,"
I want to use scrapy for crawling web pages. Is there a way to pass the start URL from the terminal itself?
It is given in the documentation that either the name of the spider or the URL can be given, but when i given the url it throws an error:
//name of my spider is example, but i am giving url instead of my spider name(It works fine if i give spider name).

scrapy crawl example.com                 

ERROR:

File
  ""/usr/local/lib/python2.7/dist-packages/Scrapy-0.14.1-py2.7.egg/scrapy/spidermanager.py"",
  line 43, in create
      raise KeyError(""Spider not found: %s"" % spider_name) KeyError: 'Spider not found: example.com'

How can i make scrapy to use my spider on the url given in the terminal??
",24k,"
            35
        ","['\nI\'m  not really sure about the commandline option. However, you could write your spider like this.\nclass MySpider(BaseSpider):\n\n    name = \'my_spider\'    \n\n    def __init__(self, *args, **kwargs): \n      super(MySpider, self).__init__(*args, **kwargs) \n\n      self.start_urls = [kwargs.get(\'start_url\')] \n\nAnd start it like:\nscrapy crawl my_spider -a start_url=""http://some_url""\n', '\nAn even easier way to allow multiple url-arguments than what Peter suggested is by giving them as a string with the urls separated by a comma, like this:\n-a start_urls=""http://example1.com,http://example2.com""\n\nIn the spider you would then simply split the string on \',\' and get an array of urls:\nself.start_urls = kwargs.get(\'start_urls\').split(\',\')\n\n', '\nUse scrapy parse command. You can parse a url with your spider. url is passed from command.\n$ scrapy parse http://www.example.com/ --spider=spider-name\n\nhttp://doc.scrapy.org/en/latest/topics/commands.html#parse\n', '\nSjaak Trekhaak has the right idea and here is how to allow multiples:\nclass MySpider(scrapy.Spider):\n    """"""\n    This spider will try to crawl whatever is passed in `start_urls` which\n    should be a comma-separated string of fully qualified URIs.\n\n    Example: start_urls=http://localhost,http://example.com\n    """"""\n    def __init__(self, name=None, **kwargs):\n        if \'start_urls\' in kwargs:\n            self.start_urls = kwargs.pop(\'start_urls\').split(\',\')\n        super(Spider, self).__init__(name, **kwargs)\n\n', ""\nThis is an extension to the approach given by Sjaak Trekhaak in this thread. The approach as it is so far only works if you provide exactly one url. For example, if you want to provide more than one url like this, for instance: \n-a start_url=http://url1.com,http://url2.com\n\nthen Scrapy (I'm using the current stable version 0.14.4) will terminate with the following exception:\nerror: running 'scrapy crawl' with more than one spider is no longer supported\n\nHowever, you can circumvent this problem by choosing a different variable for each start url, together with an argument that holds the number of passed urls. Something like this:\n-a start_url1=http://url1.com \n-a start_url2=http://url2.com \n-a urls_num=2\n\nYou can then do the following in your spider:\nclass MySpider(BaseSpider):\n\n    name = 'my_spider'    \n\n    def __init__(self, *args, **kwargs): \n        super(MySpider, self).__init__(*args, **kwargs) \n\n        urls_num = int(kwargs.get('urls_num'))\n\n        start_urls = []\n        for i in xrange(1, urls_num):\n            start_urls.append(kwargs.get('start_url{0}'.format(i)))\n\n        self.start_urls = start_urls\n\nThis is a somewhat ugly hack but it works. Of course, it's tedious to explicitly write down all command line arguments for each url. Therefore, it makes sense to wrap the scrapy crawl command in a Python subprocess and generate the command line arguments in a loop or something.\nHope it helps. :) \n"", '\nYou can also try this:\n>>> scrapy view http://www.sitename.com\n\nIt will open a window in browser of requested URL.\n']",https://stackoverflow.com/questions/9681114/how-to-give-url-to-scrapy-for-crawling,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spider a Website and Return URLs Only,"
I'm looking for a way to pseudo-spider a website. The key is that I don't actually want the content, but rather a simple list of URIs. I can get reasonably close to this idea with Wget using the --spider option, but when piping that output through a grep, I can't seem to find the right magic to make it work:
wget --spider --force-html -r -l1 http://somesite.com | grep 'Saving to:'

The grep filter seems to have absolutely no affect on the wget output. Have I got something wrong or is there another tool I should try that's more geared towards providing this kind of limited result set?
UPDATE
So I just found out offline that, by default, wget writes to stderr. I missed that in the man pages (in fact, I still haven't found it if it's in there). Once I piped the return to stdout, I got closer to what I need:
wget --spider --force-html -r -l1 http://somesite.com 2>&1 | grep 'Saving to:'

I'd still be interested in other/better means for doing this kind of thing, if any exist.
",88k,"
            71
        ","[""\nThe absolute last thing I want to do is download and parse all of the content myself (i.e. create my own spider). Once I learned that Wget writes to stderr by default, I was able to redirect it to stdout and filter the output appropriately.\nwget --spider --force-html -r -l2 $url 2>&1 \\\n  | grep '^--' | awk '{ print $3 }' \\\n  | grep -v '\\.\\(css\\|js\\|png\\|gif\\|jpg\\)$' \\\n  > urls.m3u\n\nThis gives me a list of the content resource (resources that aren't images, CSS or JS source files) URIs that are spidered. From there, I can send the URIs off to a third party tool for processing to meet my needs.\nThe output still needs to be streamlined slightly (it produces duplicates as it's shown above), but it's almost there and I haven't had to do any parsing myself.\n"", '\nCreate a few regular expressions to extract the addresses from all\n<a href=""(ADDRESS_IS_HERE)"">.\n\nHere is the solution I would use:\nwget -q http://example.com -O - | \\\n    tr ""\\t\\r\\n\'"" \'   ""\' | \\\n    grep -i -o \'<a[^>]\\+href[ ]*=[ \\t]*""\\(ht\\|f\\)tps\\?:[^""]\\+""\' | \\\n    sed -e \'s/^.*""\\([^""]\\+\\)"".*$/\\1/g\'\n\nThis will output all http, https, ftp, and ftps links from a webpage.  It will not give you relative urls, only full urls.\nExplanation regarding the options used in the series of piped commands:\nwget -q makes it not have excessive output (quiet mode).\nwget -O - makes it so that the downloaded file is echoed to stdout, rather than saved to disk.\ntr is the unix character translator, used in this example to translate newlines and tabs to spaces, as well as convert single quotes into double quotes so we can simplify our regular expressions.\ngrep -i makes the search case-insensitive\ngrep -o makes it output only the matching portions.\nsed is the Stream EDitor unix utility which allows for filtering and transformation operations.\nsed -e just lets you feed it an expression.\nRunning this little script on ""http://craigslist.org"" yielded quite a long list of links:\nhttp://blog.craigslist.org/\nhttp://24hoursoncraigslist.com/subs/nowplaying.html\nhttp://craigslistfoundation.org/\nhttp://atlanta.craigslist.org/\nhttp://austin.craigslist.org/\nhttp://boston.craigslist.org/\nhttp://chicago.craigslist.org/\nhttp://cleveland.craigslist.org/\n...\n\n', '\nI\'ve used a tool called xidel \nxidel http://server -e \'//a/@href\' | \ngrep -v ""http"" | \nsort -u | \nxargs -L1 -I {}  xidel http://server/{} -e \'//a/@href\' | \ngrep -v ""http"" | sort -u\n\nA little hackish but gets you closer! This is only the first level. Imagine packing this up into a self recursive script!\n']",https://stackoverflow.com/questions/2804467/spider-a-website-and-return-urls-only,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selenium wait for Ajax content to load - universal approach,"
Is there a universal approach for Selenium to wait till all ajax content has loaded? (not tied to a specific website - so it works for every ajax website)
",32k,"
            23
        ","['\nYou need to wait for Javascript and jQuery to finish loading. Execute Javascript to check if jQuery.active is 0 and document.readyState is complete, which means the JS and jQuery load is complete.\npublic boolean waitForJSandJQueryToLoad() {\n\n    WebDriverWait wait = new WebDriverWait(driver, 30);\n\n    // wait for jQuery to load\n    ExpectedCondition<Boolean> jQueryLoad = new ExpectedCondition<Boolean>() {\n      @Override\n      public Boolean apply(WebDriver driver) {\n        try {\n          return ((Long)((JavascriptExecutor)getDriver()).executeScript(""return jQuery.active"") == 0);\n        }\n        catch (Exception e) {\n          // no jQuery present\n          return true;\n        }\n      }\n    };\n\n    // wait for Javascript to load\n    ExpectedCondition<Boolean> jsLoad = new ExpectedCondition<Boolean>() {\n      @Override\n      public Boolean apply(WebDriver driver) {\n        return ((JavascriptExecutor)getDriver()).executeScript(""return document.readyState"")\n        .toString().equals(""complete"");\n      }\n    };\n\n  return wait.until(jQueryLoad) && wait.until(jsLoad);\n}\n\n', '\nAs Mark Collin described in his book ""Mastering Selenium Webdriver"", use JavascriptExecutor let you figure out whether a website using jQuery has finished making AJAX calls\npublic class AdditionalConditions {\n\n  public static ExpectedCondition<Boolean> jQueryAJAXCallsHaveCompleted() {\n    return new ExpectedCondition<Boolean>() {\n\n        @Override\n        public Boolean apply(WebDriver driver) {\n            return (Boolean) ((JavascriptExecutor) driver).executeScript(""return (window.jQuery != null) && (jQuery.active === 0);"");\n        }\n    };\n  }\n}\n\n', '\nI have been using this simple do while to iterate until an AJAX is finished. \nIt consistently works for me. \npublic void waitForAjax() throws InterruptedException{\n    while (true)\n    {\n        Boolean ajaxIsComplete = (Boolean) ((JavascriptExecutor)driver).executeScript(""return jQuery.active == 0"");\n        if (ajaxIsComplete){\n            info(""Ajax Call completed. "");\n            break;\n        }\n        Thread.sleep(150);\n    }\n}\n\n', ""\nI don't believe that there is a universal approach out of the box. I typically make a method that does a .waituntilrowcount(2) or waituntilvisible() that polls an element.\n""]",https://stackoverflow.com/questions/33348600/selenium-wait-for-ajax-content-to-load-universal-approach,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrapy CrawlSpider doesn't crawl the first landing page,"
I am new to Scrapy and I am working on a scraping exercise and I am using the CrawlSpider.
Although the Scrapy framework works beautifully and it follows the relevant links, I can't seem to make the CrawlSpider to scrape the very first link (the home page / landing page). Instead it goes directly to scrape the links determined by the rule but doesn't scrape the landing page on which the links are. I don't know how to fix this since it is not recommended to overwrite the parse method for a CrawlSpider. Modifying follow=True/False also doesn't yield any good results. Here is the snippet of code:
class DownloadSpider(CrawlSpider):
    name = 'downloader'
    allowed_domains = ['bnt-chemicals.de']
    start_urls = [
        ""http://www.bnt-chemicals.de""        
        ]
    rules = (   
        Rule(SgmlLinkExtractor(aloow='prod'), callback='parse_item', follow=True),
        )
    fname = 1

    def parse_item(self, response):
        open(str(self.fname)+ '.txt', 'a').write(response.url)
        open(str(self.fname)+ '.txt', 'a').write(','+ str(response.meta['depth']))
        open(str(self.fname)+ '.txt', 'a').write('\n')
        open(str(self.fname)+ '.txt', 'a').write(response.body)
        open(str(self.fname)+ '.txt', 'a').write('\n')
        self.fname = self.fname + 1

",7k,"
            18
        ","['\nJust change your callback to parse_start_url and override it:\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n\nclass DownloadSpider(CrawlSpider):\n    name = \'downloader\'\n    allowed_domains = [\'bnt-chemicals.de\']\n    start_urls = [\n        ""http://www.bnt-chemicals.de"",\n    ]\n    rules = (\n        Rule(SgmlLinkExtractor(allow=\'prod\'), callback=\'parse_start_url\', follow=True),\n    )\n    fname = 0\n\n    def parse_start_url(self, response):\n        self.fname += 1\n        fname = \'%s.txt\' % self.fname\n\n        with open(fname, \'w\') as f:\n            f.write(\'%s, %s\\n\' % (response.url, response.meta.get(\'depth\', 0)))\n            f.write(\'%s\\n\' % response.body)\n\n', '\nThere\'s a number of ways of doing this, but one of the simplest is to implement parse_start_url and then modify start_urls\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\n\nclass DownloadSpider(CrawlSpider):\n    name = \'downloader\'\n    allowed_domains = [\'bnt-chemicals.de\']\n    start_urls = [""http://www.bnt-chemicals.de/tunnel/index.htm""]\n    rules = (\n        Rule(SgmlLinkExtractor(allow=\'prod\'), callback=\'parse_item\', follow=True),\n        )\n    fname = 1\n\n    def parse_start_url(self, response):\n        return self.parse_item(response)\n\n\n    def parse_item(self, response):\n        open(str(self.fname)+ \'.txt\', \'a\').write(response.url)\n        open(str(self.fname)+ \'.txt\', \'a\').write(\',\'+ str(response.meta[\'depth\']))\n        open(str(self.fname)+ \'.txt\', \'a\').write(\'\\n\')\n        open(str(self.fname)+ \'.txt\', \'a\').write(response.body)\n        open(str(self.fname)+ \'.txt\', \'a\').write(\'\\n\')\n        self.fname = self.fname + 1\n\n']",https://stackoverflow.com/questions/15836062/scrapy-crawlspider-doesnt-crawl-the-first-landing-page,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to find all links / pages on a website,"
Is it possible to find all the pages and links on ANY given website? I'd like to enter a URL and produce a directory tree of all links from that site?
I've looked at HTTrack but that downloads the whole site and I simply need the directory tree.
",532k,"
            125
        ","['\nCheck out linkchecker鈥攊t will crawl the site (while obeying robots.txt) and generate a report. From there, you can script up a solution for creating the directory tree.\n', ""\nIf you have the developer console (JavaScript) in your browser, you can type this code in:\nurls = document.querySelectorAll('a'); for (url in urls) console.log(urls[url].href);\n\nShortened:\nn=$$('a');for(u in n)console.log(n[u].href)\n\n"", '\nAnother alternative might be\nArray.from(document.querySelectorAll(""a"")).map(x => x.href)\n\nWith your $$( its even shorter\nArray.from($$(""a"")).map(x => x.href)\n\n', '\nIf this is a programming question, then I would suggest you write your own regular expression to parse all the retrieved contents. Target tags are IMG and A for standard HTML. For JAVA, \nfinal String openingTags = ""(<a [^>]*href=[\'\\""]?|<img[^> ]* src=[\'\\""]?)"";\n\nthis along with Pattern and Matcher classes should detect the beginning of the tags. Add LINK tag if you also want CSS.\nHowever, it is not as easy as you may have intially thought. Many web pages are not well-formed. Extracting all the links programmatically that human being can ""recognize"" is really difficult if you need to take into account all the irregular expressions.\nGood luck!\n', '\nfunction getalllinks($url) {\n    $links = array();\n    if ($fp = fopen($url, \'r\')) {\n        $content = \'\';\n        while ($line = fread($fp, 1024)) {\n            $content. = $line;\n        }\n    }\n    $textLen = strlen($content);\n    if ($textLen > 10) {\n        $startPos = 0;\n        $valid = true;\n        while ($valid) {\n            $spos = strpos($content, \'<a \', $startPos);\n            if ($spos < $startPos) $valid = false;\n            $spos = strpos($content, \'href\', $spos);\n            $spos = strpos($content, \'""\', $spos) + 1;\n            $epos = strpos($content, \'""\', $spos);\n            $startPos = $epos;\n            $link = substr($content, $spos, $epos - $spos);\n            if (strpos($link, \'http://\') !== false) $links[] = $link;\n        }\n    }\n    return $links;\n}\n\ntry this code....\n']",https://stackoverflow.com/questions/1439326/how-to-find-all-links-pages-on-a-website,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
how to filter duplicate requests based on url in scrapy,"
I am writing a crawler for a website using scrapy with CrawlSpider.
Scrapy provides an in-built duplicate-request filter which filters duplicate requests based on urls. Also, I can filter requests using rules member of CrawlSpider. 
What I want to do is to filter requests like:
http:://www.abc.com/p/xyz.html?id=1234&refer=5678

If I have already visited
http:://www.abc.com/p/xyz.html?id=1234&refer=4567


NOTE: refer is a parameter that doesn't affect the response I get, so I don't care if the value of that parameter changes.

Now, if I have a set which accumulates all ids I could ignore it in my callback function parse_item (that's my callback function) to achieve this functionality.
But that would mean I am still at least fetching that page, when I don't need to.
So what is the way in which I can tell scrapy that it shouldn't send a particular request based on the url?
",19k,"
            42
        ","['\nYou can write custom middleware for duplicate removal and add it in settings\nimport os\n\nfrom scrapy.dupefilter import RFPDupeFilter\n\nclass CustomFilter(RFPDupeFilter):\n""""""A dupe filter that considers specific ids in the url""""""\n\n    def __getid(self, url):\n        mm = url.split(""&refer"")[0] #or something like that\n        return mm\n\n    def request_seen(self, request):\n        fp = self.__getid(request.url)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + os.linesep)\n\nThen you need to set the correct DUPFILTER_CLASS in settings.py\nDUPEFILTER_CLASS = \'scraper.duplicate_filter.CustomFilter\'\n\nIt should work after that\n', '\nFollowing ytomar\'s lead, I wrote this filter that filters based purely on URLs that have already been seen by checking an in-memory set.  I\'m a Python noob so let me know if I screwed something up, but it seems to work all right:\nfrom scrapy.dupefilter import RFPDupeFilter\n\nclass SeenURLFilter(RFPDupeFilter):\n    """"""A dupe filter that considers the URL""""""\n\n    def __init__(self, path=None):\n        self.urls_seen = set()\n        RFPDupeFilter.__init__(self, path)\n\n    def request_seen(self, request):\n        if request.url in self.urls_seen:\n            return True\n        else:\n            self.urls_seen.add(request.url)\n\nAs ytomar mentioned, be sure to add the DUPEFILTER_CLASS constant to settings.py:\nDUPEFILTER_CLASS = \'scraper.custom_filters.SeenURLFilter\'\n\n', '\nhttps://github.com/scrapinghub/scrapylib/blob/master/scrapylib/deltafetch.py\nThis file might help you. This file creates a database of unique delta fetch key from the url ,a user pass in a scrapy.Reqeust(meta={\'deltafetch_key\':uniqe_url_key}).\nThis this let you avoid duplicate requests you already have visited in the past.\nA sample mongodb implementation using deltafetch.py \n        if isinstance(r, Request):\n            key = self._get_key(r)\n            key = key+spider.name\n\n            if self.db[\'your_collection_to_store_deltafetch_key\'].find_one({""_id"":key}):\n                spider.log(""Ignoring already visited: %s"" % r, level=log.INFO)\n                continue\n        elif isinstance(r, BaseItem):\n\n            key = self._get_key(response.request)\n            key = key+spider.name\n            try:\n                self.db[\'your_collection_to_store_deltafetch_key\'].insert({""_id"":key,""time"":datetime.now()})\n            except:\n                spider.log(""Ignoring already visited: %s"" % key, level=log.ERROR)\n        yield r\n\neg. id = 345\nscrapy.Request(url,meta={deltafetch_key:345},callback=parse)\n', ""\nHere is my custom filter base on scrapy 0.24.6.\nIn this filter, it only cares id in the url. for example\nhttp://www.example.com/products/cat1/1000.html?p=1\nhttp://www.example.com/products/cat2/1000.html?p=2\nare treated as same url. But\nhttp://www.example.com/products/cat2/all.html\nwill not.\nimport re\nimport os\nfrom scrapy.dupefilter import RFPDupeFilter\n\n\nclass MyCustomURLFilter(RFPDupeFilter):\n\n    def _get_id(self, url):\n        m = re.search(r'(\\d+)\\.html', url)\n        return None if m is None else m.group(1)\n\n    def request_fingerprint(self, request):\n        style_id = self._get_id(request.url)\n        return style_id\n\n"", ""\nIn the latest scrapy, we can use the default duplication filter or extend and have custom one.\ndefine the below config in spider settings\nDUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'\n""]",https://stackoverflow.com/questions/12553117/how-to-filter-duplicate-requests-based-on-url-in-scrapy,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
crawl site that has infinite scrolling using python,"
I have been doing research and so far I found out the python package that I will plan on using its scrapy, now I am trying to find out what is a good way to build a scraper using scrapy to crawl site with infinite scrolling. After digging around I found out that there is a package call selenium and it has python module. I have a feeling someone has already done that using Scrapy and Selenium to scrape site with infinite scrolling. It would be great if someone can point towards to an example. 
",28k,"
            10
        ","['\nYou can use selenium to scrap the infinite scrolling website like twitter or facebook. \nStep 1 : Install Selenium using pip \npip install selenium \n\nStep 2 : use the code below to automate infinite scroll and extract the source code\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import NoAlertPresentException\nimport sys\n\nimport unittest, time, re\n\nclass Sel(unittest.TestCase):\n    def setUp(self):\n        self.driver = webdriver.Firefox()\n        self.driver.implicitly_wait(30)\n        self.base_url = ""https://twitter.com""\n        self.verificationErrors = []\n        self.accept_next_alert = True\n    def test_sel(self):\n        driver = self.driver\n        delay = 3\n        driver.get(self.base_url + ""/search?q=stackoverflow&src=typd"")\n        driver.find_element_by_link_text(""All"").click()\n        for i in range(1,100):\n            self.driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")\n            time.sleep(4)\n        html_source = driver.page_source\n        data = html_source.encode(\'utf-8\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n\nThe for loop allows you to parse through the infinite scrolls and post which you can extract the loaded data.\nStep 3 : Print the data if required.\n', '\nThis is short & simple code which is working for me:\nSCROLL_PAUSE_TIME = 20\n\n# Get scroll height\nlast_height = driver.execute_script(""return document.body.scrollHeight"")\n\nwhile True:\n    # Scroll down to bottom\n    driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")\n\n    # Wait to load page\n    time.sleep(SCROLL_PAUSE_TIME)\n\n    # Calculate new scroll height and compare with last scroll height\n    new_height = driver.execute_script(""return document.body.scrollHeight"")\n    if new_height == last_height:\n        break\n    last_height = new_height\n\nposts = driver.find_elements_by_class_name(""post-text"")\n\nfor block in posts:\n    print(block.text)\n\n']",https://stackoverflow.com/questions/22702277/crawl-site-that-has-infinite-scrolling-using-python,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
find a word on a website and get its page link,"
I want to scrape a few websites and see if the word ""katalog"" is present there. If yes, I want to retrieve the link of all the tabs/sub pages where that word is present. Is it possible to do so?
I tried following this tutorial but the wordlist.csv I get at the end is empty even though the word catalog does exist on the website.
https://www.phooky.com/blog/find-specific-words-on-web-pages-with-scrapy/
        wordlist = [
            ""katalog"",
            ""downloads"",
            ""download""
            ]

def find_all_substrings(string, sub):
    starts = [match.start() for match in re.finditer(re.escape(sub), string)]
    return starts

class WebsiteSpider(CrawlSpider):

    name = ""webcrawler""
    allowed_domains = [""www.reichelt.com/""]
    start_urls = [""https://www.reichelt.com/""]
    rules = [Rule(LinkExtractor(), follow=True, callback=""check_buzzwords"")]

    crawl_count = 0
    words_found = 0                                 

    def check_buzzwords(self, response):

        self.__class__.crawl_count += 1

        crawl_count = self.__class__.crawl_count

        url = response.url
        contenttype = response.headers.get(""content-type"", """").decode('utf-8').lower()
        data = response.body.decode('utf-8')

        for word in wordlist:
                substrings = find_all_substrings(data, word)
                print(""substrings"", substrings)
                for pos in substrings:
                        ok = False
                        if not ok:
                                self.__class__.words_found += 1
                                print(word + "";"" + url + "";"")
        return Item()

    def _requests_to_follow(self, response):
        if getattr(response, ""encoding"", None) != None:
                return CrawlSpider._requests_to_follow(self, response)
        else:
                return []

How can I find all instances of a word on a website and obtain the link of the page where the word is founded?
",2k,"
            2
        ","['\nMain problem is wrong allowed_domain - it has to be without path /\n    allowed_domains = [""www.reichelt.com""]\n\nOther problems can be this tutorial is 3 years old (there is link to documentation for Scarpy 1.5 but newest version is 2.5.0).\nIt also uses some useless lines of code.\nIt gets contenttype but never use it to decode request.body. Your url  uses iso8859-1 for original language and utf-8 for ?LANGUAGE=PL - but you can simply use request.text and it will automatically decode it.\nIt also uses ok = False and later check it but it is totally useless.\n\nMinimal working code - you can copy it to single file and run as python script.py without creating project.\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\nimport re\n\nwordlist = [\n    ""katalog"",\n    ""catalog"",\n    ""downloads"",\n    ""download"",\n]\n\ndef find_all_substrings(string, sub):\n    return [match.start() for match in re.finditer(re.escape(sub), string)]\n\nclass WebsiteSpider(CrawlSpider):\n\n    name = ""webcrawler""\n    \n    allowed_domains = [""www.reichelt.com""]\n    start_urls = [""https://www.reichelt.com/""]\n    #start_urls = [""https://www.reichelt.com/?LANGUAGE=PL""]\n    \n    rules = [Rule(LinkExtractor(), follow=True, callback=""check_buzzwords"")]\n\n    #crawl_count = 0\n    #words_found = 0                                 \n\n    def check_buzzwords(self, response):\n        print(\'[check_buzzwords] url:\', response.url)\n        \n        #self.crawl_count += 1\n\n        #content_type = response.headers.get(""content-type"", """").decode(\'utf-8\').lower()\n        #print(\'content_type:\', content_type)\n        #data = response.body.decode(\'utf-8\')\n        \n        data = response.text\n\n        for word in wordlist:\n            print(\'[check_buzzwords] check word:\', word)\n            substrings = find_all_substrings(data, word)\n            print(\'[check_buzzwords] substrings:\', substrings)\n            \n            for pos in substrings:\n                #self.words_found += 1\n                # only display\n                print(\'[check_buzzwords] word: {} | pos: {} | sub: {} | url: {}\'.format(word, pos, data[pos-20:pos+20], response.url))\n                # send to file\n                yield {\'word\': word, \'pos\': pos, \'sub\': data[pos-20:pos+20], \'url\': response.url}\n\n# --- run without project and save in `output.csv` ---\n\nfrom scrapy.crawler import CrawlerProcess\n\nc = CrawlerProcess({\n    \'USER_AGENT\': \'Mozilla/5.0\',\n    # save in file CSV, JSON or XML\n    \'FEEDS\': {\'output.csv\': {\'format\': \'csv\'}},  # new in 2.1\n})\nc.crawl(WebsiteSpider)\nc.start() \n\n\nEDIT:\nI added data[pos-20:pos+20] to yielded data to see where is substring and sometimes it is in URL like .../elements/adw_2018/catalog/... or other place like <img alt=""""catalog"""" - so using regex doesn\'t have to be good idea. Maybe better is to use xpath or css selector to search text only in some places or in links.\n\nEDIT:\nVersion which search links with words from list. It uses response.xpath to search all linsk and later it check if there is word in href - so it doesn\'t need regex.\nProblem can be that it treats link with -downloads- (with s) as link with word download and downloads so it would need more complex method to check (ie. using regex) to treats it only as link with word downloads\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\n\nwordlist = [\n    ""katalog"",\n    ""catalog"",\n    ""downloads"",\n    ""download"",\n]\n\nclass WebsiteSpider(CrawlSpider):\n\n    name = ""webcrawler""\n    \n    allowed_domains = [""www.reichelt.com""]\n    start_urls = [""https://www.reichelt.com/""]\n    \n    rules = [Rule(LinkExtractor(), follow=True, callback=""check_buzzwords"")]\n\n    def check_buzzwords(self, response):\n        print(\'[check_buzzwords] url:\', response.url)\n        \n        links = response.xpath(\'//a[@href]\')\n        \n        for word in wordlist:\n            \n            for link in links:\n                url = link.attrib.get(\'href\')\n                if word in url:\n                    print(\'[check_buzzwords] word: {} | url: {} | page: {}\'.format(word, url, response.url))\n                    # send to file\n                    yield {\'word\': word, \'url\': url, \'page\': response.url}\n\n# --- run without project and save in `output.csv` ---\n\nfrom scrapy.crawler import CrawlerProcess\n\nc = CrawlerProcess({\n    \'USER_AGENT\': \'Mozilla/5.0\',\n    # save in file CSV, JSON or XML\n    \'FEEDS\': {\'output.csv\': {\'format\': \'csv\'}},  # new in 2.1\n})\nc.crawl(WebsiteSpider)\nc.start() \n\n', '\nYou can do it with requests-html and rendering the page:\nfrom requests_html import HTMLSession\n\nsession = HTMLSession()\nurl = ""https://www.reichelt.com/""\n\nr = session.get(url)\nr.html.render(sleep=2)\n\nif ""your_word"" in r.html.text: #or r.html.html if you want it in raw html\n    print([link for link in r.html.absolute_links if ""your_word"" in link])\n\n']",https://stackoverflow.com/questions/68193300/find-a-word-on-a-website-and-get-its-page-link,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodejs: Async request with a list of URL,"
I am working on a crawler. I have a list of URL need to be requested. There are several hundreds of request at the same time if I don't set it to be async. I am afraid that it would explode my bandwidth or produce to much network access to the target website. What should I do?
Here is what I am doing: 
urlList.forEach((url, index) => {

    console.log('Fetching ' + url);
    request(url, function(error, response, body) {
        //do sth for body

    });
});

I want one request is called after one request is completed.
",967,"
            0
        ","['\nYou can use something like Promise library e.g. snippet\nconst Promise = require(""bluebird"");\nconst axios = require(""axios"");\n\n//Axios wrapper for error handling\nconst axios_wrapper = (options) => {\n    return axios(...options)\n        .then((r) => {\n            return Promise.resolve({\n                data: r.data,\n                error: null,\n            });\n        })\n        .catch((e) => {\n            return Promise.resolve({\n                data: null,\n                error: e.response ? e.response.data : e,\n            });\n        });\n};\n\nPromise.map(\n    urls,\n    (k) => {\n        return axios_wrapper({\n            method: ""GET"",\n            url: k,\n        });\n    },\n    { concurrency: 1 } // Here 1 represents how many requests you want to run in parallel\n)\n    .then((r) => {\n        console.log(r);\n        //Here r will be an array of objects like {data: [{}], error: null}, where if the request was successfull it will have data value present otherwise error value will be non-null\n    })\n    .catch((e) => {\n        console.error(e);\n    });\n\n', ""\nThe things you need to watch for are:\n\nWhether the target site has rate limiting and you may be blocked from access if you try to request too much too fast?\nHow many simultaneous requests the target site can handle without degrading its performance?\nHow much bandwidth your server has on its end of things?\nHow many simultaneous requests your own server can have in flight and process without causing excess memory usage or a pegged CPU.\n\nIn general, the scheme for managing all this is to create a way to tune how many requests you launch.  There are many different ways to control this by number of simultaneous requests, number of requests per second, amount of data used, etc...\nThe simplest way to start would be to just control how many simultaneous requests you make.  That can be done like this:\nfunction runRequests(arrayOfData, maxInFlight, fn) {\n    return new Promise((resolve, reject) => {\n        let index = 0;\n        let inFlight = 0;\n\n        function next() {\n            while (inFlight < maxInFlight && index < arrayOfData.length) {\n                ++inFlight;\n                fn(arrayOfData[index++]).then(result => {\n                    --inFlight;\n                    next();\n                }).catch(err => {\n                    --inFlight;\n                    console.log(err);\n                    // purposely eat the error and let the rest of the processing continue\n                    // if you want to stop further processing, you can call reject() here\n                    next();\n                });\n            }\n            if (inFlight === 0) {\n                // all done\n                resolve();\n            }\n        }\n        next();\n    });\n}\n\nAnd, then you would use that like this:\nconst rp = require('request-promise');\n\n// run the whole urlList, no more than 10 at a time\nrunRequests(urlList, 10, function(url) {\n    return rp(url).then(function(data) {\n        // process fetched data here for one url\n    }).catch(function(err) {\n        console.log(url, err);\n    });\n}).then(function() {\n    // all requests done here\n});\n\nThis can be made as sophisticated as you want by adding a time element to it (no more than N requests per second) or even a bandwidth element to it.\n\nI want one request is called after one request is completed.\n\nThat's a very slow way to do things.  If you really want that, then you can just pass a 1 for the maxInFlight parameter to the above function, but typically, things would work a lot faster and not cause problems by allowing somewhere between 5 and 50 simultaneous requests.  Only testing would tell you where the sweet spot is for your particular target sites and your particular server infrastructure and amount of processing you need to do on the results.\n"", '\nyou can use set timeout function to process all request within loop. for that you must know maximum time to process a request.\n']",https://stackoverflow.com/questions/47299174/nodejs-async-request-with-a-list-of-url,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I use different pipelines for different spiders in a single Scrapy project,"
I have a scrapy project which contains multiple spiders.
Is there any way I can define which pipelines to use for which spider? Not all the pipelines i have defined are applicable for every spider.
Thanks
",32k,"
            97
        ","[""\nJust remove all pipelines from main settings and use this inside spider.\nThis will define the pipeline to user per spider\nclass testSpider(InitSpider):\n    name = 'test'\n    custom_settings = {\n        'ITEM_PIPELINES': {\n            'app.MyPipeline': 400\n        }\n    }\n\n"", ""\nBuilding on the solution from Pablo Hoffman, you can use the following decorator on the process_item method of a Pipeline object so that it checks the pipeline attribute of your spider for whether or not it should be executed. For example:\ndef check_spider_pipeline(process_item_method):\n\n    @functools.wraps(process_item_method)\n    def wrapper(self, item, spider):\n\n        # message template for debugging\n        msg = '%%s %s pipeline step' % (self.__class__.__name__,)\n\n        # if class is in the spider's pipeline, then use the\n        # process_item method normally.\n        if self.__class__ in spider.pipeline:\n            spider.log(msg % 'executing', level=log.DEBUG)\n            return process_item_method(self, item, spider)\n\n        # otherwise, just return the untouched item (skip this step in\n        # the pipeline)\n        else:\n            spider.log(msg % 'skipping', level=log.DEBUG)\n            return item\n\n    return wrapper\n\nFor this decorator to work correctly, the spider must have a pipeline attribute with a container of the Pipeline objects that you want to use to process the item, for example:\nclass MySpider(BaseSpider):\n\n    pipeline = set([\n        pipelines.Save,\n        pipelines.Validate,\n    ])\n\n    def parse(self, response):\n        # insert scrapy goodness here\n        return item\n\nAnd then in a pipelines.py file:\nclass Save(object):\n\n    @check_spider_pipeline\n    def process_item(self, item, spider):\n        # do saving here\n        return item\n\nclass Validate(object):\n\n    @check_spider_pipeline\n    def process_item(self, item, spider):\n        # do validating here\n        return item\n\nAll Pipeline objects should still be defined in ITEM_PIPELINES in settings (in the correct order -- would be nice to change so that the order could be specified on the Spider, too).\n"", '\nThe other solutions given here are good, but I think they could be slow, because we are not really not using the pipeline per spider, instead we are checking if a pipeline exists every time an item is returned (and in some cases this could reach millions).\nA good way to completely disable (or enable) a feature per spider is using custom_setting and from_crawler for all extensions like this:\npipelines.py\nfrom scrapy.exceptions import NotConfigured\n\nclass SomePipeline(object):\n    def __init__(self):\n        pass\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool(\'SOMEPIPELINE_ENABLED\'):\n            # if this isn\'t specified in settings, the pipeline will be completely disabled\n            raise NotConfigured\n        return cls()\n\n    def process_item(self, item, spider):\n        # change my item\n        return item\n\nsettings.py\nITEM_PIPELINES = {\n   \'myproject.pipelines.SomePipeline\': 300,\n}\nSOMEPIPELINE_ENABLED = True # you could have the pipeline enabled by default\n\nspider1.py\nclass Spider1(Spider):\n\n    name = \'spider1\'\n\n    start_urls = [""http://example.com""]\n\n    custom_settings = {\n        \'SOMEPIPELINE_ENABLED\': False\n    }\n\nAs you check, we have specified custom_settings that will override the things specified in settings.py, and we are disabling SOMEPIPELINE_ENABLED for this spider.\nNow when you run this spider, check for something like:\n[scrapy] INFO: Enabled item pipelines: []\n\nNow scrapy has completely disabled the pipeline, not bothering of its existence for the whole run. Check that this also works for scrapy extensions and middlewares.\n', ""\nYou can use the name attribute of the spider in your pipeline\nclass CustomPipeline(object)\n\n    def process_item(self, item, spider)\n         if spider.name == 'spider1':\n             # do something\n             return item\n         return item\n\nDefining all pipelines this way can accomplish what you want.\n"", ""\nI can think of at least four approaches:\n\nUse a different scrapy project per set of spiders+pipelines (might be appropriate if your spiders are different enough warrant being in different projects)\nOn the scrapy tool command line, change the pipeline setting with scrapy settings in between each invocation of your spider\nIsolate your spiders into their own scrapy tool commands, and define the default_settings['ITEM_PIPELINES'] on your command class to the pipeline list you want for that command. See line 6 of this example.\nIn the pipeline classes themselves, have process_item() check what spider it's running against, and do nothing if it should be ignored for that spider. See the example using resources per spider to get you started. (This seems like an ugly solution because it tightly couples spiders and item pipelines. You probably shouldn't use this one.)\n\n"", ""\nThe most simple and effective solution is to set custom settings in each spider itself.\ncustom_settings = {'ITEM_PIPELINES': {'project_name.pipelines.SecondPipeline': 300}}\n\nAfter that you need to set them in the settings.py file\nITEM_PIPELINES = {\n   'project_name.pipelines.FistPipeline': 300,\n   'project_name.pipelines.SecondPipeline': 400\n}\n\nin that way each spider will use the respective pipeline.\n"", '\nYou can just set the item pipelines settings inside of the spider like this:\nclass CustomSpider(Spider):\n    name = \'custom_spider\'\n    custom_settings = {\n        \'ITEM_PIPELINES\': {\n            \'__main__.PagePipeline\': 400,\n            \'__main__.ProductPipeline\': 300,\n        },\n        \'CONCURRENT_REQUESTS_PER_DOMAIN\': 2\n    }\n\nI can then split up a pipeline (or even use multiple pipelines) by adding a value to the loader/returned item that identifies which part of the spider sent items over. This way I won鈥檛 get any KeyError exceptions and I know which items should be available. \n    ...\n    def scrape_stuff(self, response):\n        pageloader = PageLoader(\n                PageItem(), response=response)\n\n        pageloader.add_xpath(\'entire_page\', \'/html//text()\')\n        pageloader.add_value(\'item_type\', \'page\')\n        yield pageloader.load_item()\n\n        productloader = ProductLoader(\n                ProductItem(), response=response)\n\n        productloader.add_xpath(\'product_name\', \'//span[contains(text(), ""Example"")]\')\n        productloader.add_value(\'item_type\', \'product\')\n        yield productloader.load_item()\n\nclass PagePipeline:\n    def process_item(self, item, spider):\n        if item[\'item_type\'] == \'product\':\n            # do product stuff\n\n        if item[\'item_type\'] == \'page\':\n            # do page stuff\n\n', ""\nI am using two pipelines, one for image download (MyImagesPipeline) and second for save data in mongodb (MongoPipeline).\nsuppose we have many spiders(spider1,spider2,...........),in my example spider1 and spider5 can not use MyImagesPipeline\nsettings.py\nITEM_PIPELINES = {'scrapycrawler.pipelines.MyImagesPipeline' : 1,'scrapycrawler.pipelines.MongoPipeline' : 2}\nIMAGES_STORE = '/var/www/scrapycrawler/dowload'\n\nAnd bellow complete code of pipeline\nimport scrapy\nimport string\nimport pymongo\nfrom scrapy.pipelines.images import ImagesPipeline\n\nclass MyImagesPipeline(ImagesPipeline):\n    def process_item(self, item, spider):\n        if spider.name not in ['spider1', 'spider5']:\n            return super(ImagesPipeline, self).process_item(item, spider)\n        else:\n           return item \n\n    def file_path(self, request, response=None, info=None):\n        image_name = string.split(request.url, '/')[-1]\n        dir1 = image_name[0]\n        dir2 = image_name[1]\n        return dir1 + '/' + dir2 + '/' +image_name\n\nclass MongoPipeline(object):\n\n    collection_name = 'scrapy_items'\n    collection_url='snapdeal_urls'\n\n    def __init__(self, mongo_uri, mongo_db):\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_uri=crawler.settings.get('MONGO_URI'),\n            mongo_db=crawler.settings.get('MONGO_DATABASE', 'scraping')\n        )\n\n    def open_spider(self, spider):\n        self.client = pymongo.MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n\n    def close_spider(self, spider):\n        self.client.close()\n\n    def process_item(self, item, spider):\n        #self.db[self.collection_name].insert(dict(item))\n        collection_name=item.get( 'collection_name', self.collection_name )\n        self.db[collection_name].insert(dict(item))\n        data = {}\n        data['base_id'] = item['base_id']\n        self.db[self.collection_url].update({\n            'base_id': item['base_id']\n        }, {\n            '$set': {\n            'image_download': 1\n            }\n        }, upsert=False, multi=True)\n        return item\n\n"", '\nwe can use some conditions in pipeline as this\n    # -*- coding: utf-8 -*-\nfrom scrapy_app.items import x\n\nclass SaveItemPipeline(object):\n    def process_item(self, item, spider):\n        if isinstance(item, x,):\n            item.save()\n        return item\n\n', ""\nSimple but still useful solution.\nSpider code\n    def parse(self, response):\n        item = {}\n        ... do parse stuff\n        item['info'] = {'spider': 'Spider2'}\n\npipeline code\n    def process_item(self, item, spider):\n        if item['info']['spider'] == 'Spider1':\n            logging.error('Spider1 pipeline works')\n        elif item['info']['spider'] == 'Spider2':\n            logging.error('Spider2 pipeline works')\n        elif item['info']['spider'] == 'Spider3':\n            logging.error('Spider3 pipeline works')\n\nHope this save some time for somebody!\n"", ""\nOverriding 'ITEM_PIPELINES' with custom settings per spider, as others have suggested, works well. However, I found I had a few distinct groups of pipelines I wanted to use for different categories of spiders. I wanted to be able to easily define the pipeline for a particular category of spider without a lot of thought, and I wanted to be able to update a pipeline category without editing each spider in that category individually.\nSo I created a new file called pipeline_definitions.py in the same directory as settings.py. pipeline_definitions.py contains functions like this:\ndef episode_pipelines():\n    return {\n        'radio_scrape.pipelines.SaveEpisode': 100,\n    }\n\ndef show_pipelines():\n    return {\n        'radio_scrape.pipelines.SaveShow': 100,\n    }\n\nThen in each spider I would import the specific function relevant for the spider:\nfrom radio_scrape.pipeline_definitions import episode_pipelines\n\nI then use that function in the custom settings assignment:\nclass RadioStationAEspisodesSpider(scrapy.Spider):\n    name = 'radio_station_A_episodes'        \n    custom_settings = {\n        'ITEM_PIPELINES': episode_pipelines()\n    }\n\n""]",https://stackoverflow.com/questions/8372703/how-can-i-use-different-pipelines-for-different-spiders-in-a-single-scrapy-proje,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getting Forbidden by robots.txt: scrapy,"
while crawling website like https://www.netflix.com, getting Forbidden by robots.txt: https://www.netflix.com/>
ERROR: No response downloaded for: https://www.netflix.com/
",53k,"
            69
        ","['\nIn the new version (scrapy 1.1) launched 2016-05-11 the crawl first downloads robots.txt before crawling. To change this behavior change in your settings.py with ROBOTSTXT_OBEY\nROBOTSTXT_OBEY = False\n\nHere are the release notes\n', ""\nNetflix's Terms of Use state:\n\nYou also agree not to circumvent, remove, alter, deactivate, degrade or thwart any of the content protections in the Netflix service; use any robot, spider, scraper or other automated means to access the Netflix service;\n\nThey have their robots.txt set up to block web scrapers. If you override the setting in settings.py to ROBOTSTXT_OBEY=False then you are violating their terms of use which can result in a law suit.\n"", '\nFirst thing you need to ensure is that you change your user agent in the request, otherwise default user agent will be blocked for sure.\n']",https://stackoverflow.com/questions/37274835/getting-forbidden-by-robots-txt-scrapy,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python: maximum recursion depth exceeded while calling a Python object,"
I've built a crawler that had to run on about 5M pages (by increasing the url ID) and then parses the pages which contain the info' I need.
after using an algorithm which run on the urls (200K) and saved the good and bad results I found that the I'm wasting a lot of time. I could see that there are a a few returning subtrahends which I can use to check the next valid url.
you can see the subtrahends quite fast (a little ex' of the few first ""good IDs"") -
510000011 # +8
510000029 # +18
510000037 # +8
510000045 # +8
510000052 # +7
510000060 # +8
510000078 # +18
510000086 # +8
510000094 # +8
510000102 # +8
510000110 # etc'
510000128
510000136
510000144
510000151
510000169
510000177
510000185
510000193
510000201

after crawling about 200K urls which gave me only 14K good results I knew I was wasting my time and need to optimize it, so I run some statistics and built a function that will check the urls while increasing the id with 8\18\17\8 (top returning subtrahends ) etc'.
this is the function - 
def checkNextID(ID):
    global numOfRuns, curRes, lastResult
    while ID < lastResult:
        try:
            numOfRuns += 1
            if numOfRuns % 10 == 0:
                time.sleep(3) # sleep every 10 iterations
            if isValid(ID + 8):
                parseHTML(curRes)
                checkNextID(ID + 8)
                return 0
            if isValid(ID + 18):
                parseHTML(curRes)
                checkNextID(ID + 18)
                return 0
            if isValid(ID + 7):
                parseHTML(curRes)
                checkNextID(ID + 7)
                return 0
            if isValid(ID + 17):
                parseHTML(curRes)
                checkNextID(ID + 17)
                return 0
            if isValid(ID+6):
                parseHTML(curRes)
                checkNextID(ID + 6)
                return 0
            if isValid(ID + 16):
                parseHTML(curRes)
                checkNextID(ID + 16)
                return 0
            else:
                checkNextID(ID + 1)
                return 0
        except Exception, e:
            print ""somethin went wrong: "" + str(e)

what is basically does is -checkNextID(ID) is getting the first id I know that contain the data minus 8 so the first iteration will match the first ""if isValid"" clause (isValid(ID + 8) will return True).
lastResult is a variable which saves the last known url id, so we'll run until numOfRuns is
isValid() is a function that gets an ID + one of the subtrahends and returns True if the url contains what I need and saves a soup object of the url to a global varibale named - 'curRes', it returns False if the url doesn't contain the data I need.
parseHTML is a function that gets the soup object (curRes), parses the data I need and then saves the data to a csv, then returns True.
if isValid() returns True, we'll call parseHTML() and then try to check the next ID+the subtrahends (by calling checkNextID(ID + subtrahends), if none of them will return what I'm looking for I'll increase it with 1 and check again until I'll find the next valid url.
you can see the rest of the code here
after running the code I got about 950~ good results and suddenly an exception had raised -

""somethin went wrong: maximum recursion depth exceeded while calling a
  Python object""

I could see on WireShark that the scipt stuck on id - 510009541 (I started my script with 510000003), the script tried getting the url with that ID a few times before I noticed the error and stopped it.
I was really exciting to see that I got the same results but 25x-40x times faster then my old script, with fewer HTTP requests, it's very precise, I have missed only 1 result for 1000 good results, which is find by me, it's impossible to rum 5M times, I had my old script running for 30 hours and got 14-15K results when my new script gave me 960~ results in 5-10 minutes.
I read about stack limitations, but there must be a solution for the algorithm I'm trying to implement in Python (I can't go back to my old ""algorithm"", it will never end).
Thanks!
",253k,"
            65
        ","[""\nPython don't have a great support for recursion because of it's lack of TRE (Tail Recursion Elimination).\nThis means that each call to your recursive function will create a function call stack and because there is a limit of stack depth (by default is 1000) that you can check out by sys.getrecursionlimit (of course you can change it using sys.setrecursionlimit but it's not recommended) your program will end up by crashing when it hits this limit.\nAs other answer has already give you a much nicer way for how to solve this in your case (which is to replace recursion by simple loop) there is another solution if you still want to use recursion which is to use one of the many recipes of implementing TRE in python like this one.\nN.B: My answer is meant to give you more insight on why you get the error, and I'm not advising you to use the TRE as i already explained because in your case a loop will be much better and easy to read.\n"", '\nYou can increase the capacity of the stack by the following :\nimport sys\nsys.setrecursionlimit(10000)\n\n', '\nthis turns the recursion in to a loop:\ndef checkNextID(ID):\n    global numOfRuns, curRes, lastResult\n    while ID < lastResult:\n        try:\n            numOfRuns += 1\n            if numOfRuns % 10 == 0:\n                time.sleep(3) # sleep every 10 iterations\n            if isValid(ID + 8):\n                parseHTML(curRes)\n                ID = ID + 8\n            elif isValid(ID + 18):\n                parseHTML(curRes)\n                ID = ID + 18\n            elif isValid(ID + 7):\n                parseHTML(curRes)\n                ID = ID + 7\n            elif isValid(ID + 17):\n                parseHTML(curRes)\n                ID = ID + 17\n            elif isValid(ID+6):\n                parseHTML(curRes)\n                ID = ID + 6\n            elif isValid(ID + 16):\n                parseHTML(curRes)\n                ID = ID + 16\n            else:\n                ID = ID + 1\n        except Exception, e:\n            print ""somethin went wrong: "" + str(e)\n\n', '\nYou can increase the recursion depth and thread stack size.\nimport sys, threading\nsys.setrecursionlimit(10**7) # max depth of recursion\nthreading.stack_size(2**27)  # new thread will get stack of such size\n\n', '\nInstead of doing recursion, the parts of the code with checkNextID(ID + 18) and similar could be replaced with ID+=18, and then if you remove all instances of return 0, then it should do the same thing but as a simple loop. You should then put a return 0 at the end and make your variables non-global.\n', ""\n\nuse try and except but don't print your error in except just run your function again in except statement\n\n""]",https://stackoverflow.com/questions/6809402/python-maximum-recursion-depth-exceeded-while-calling-a-python-object,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Detecting honest web crawlers,"
I would like to detect (on the server side) which requests are from bots.  I don't care about malicious bots at this point, just the ones that are playing nice.  I've seen a few approaches that mostly involve matching the user agent string against keywords like 'bot'.  But that seems awkward, incomplete, and unmaintainable.  So does anyone have any more solid approaches?  If not, do you have any resources you use to keep up to date with all the friendly user agents?
If you're curious: I'm not trying to do anything against any search engine policy.  We have a section of the site where a user is randomly presented with one of several slightly different versions of a page.  However if a web crawler is detected, we'd always give them the same version so that the index is consistent.
Also I'm using Java, but I would imagine the approach would be similar for any server-side technology.
",20k,"
            46
        ","['\nYou said matching the user agent on 鈥榖ot鈥?may be awkward, but we鈥檝e found it to be a pretty good match. Our studies have shown that it will cover about 98% of the hits you receive. We also haven鈥檛 come across any false positive matches yet either. If you want to raise this up to 99.9% you can include a few other well-known matches such as 鈥榗rawler鈥? 鈥榖aiduspider鈥? 鈥榠a_archiver鈥? 鈥榗url鈥?etc. We鈥檝e tested this on our production systems over millions of hits. \nHere are a few c# solutions for you:\n1) Simplest\nIs the fastest when processing a miss. i.e. traffic from a non-bot 鈥?a normal user.\nCatches 99+% of crawlers.\nbool iscrawler = Regex.IsMatch(Request.UserAgent, @""bot|crawler|baiduspider|80legs|ia_archiver|voyager|curl|wget|yahoo! slurp|mediapartners-google"", RegexOptions.IgnoreCase);\n\n2) Medium\nIs the fastest when processing a hit. i.e. traffic from a bot. Pretty fast for misses too.\nCatches close to 100% of crawlers.\nMatches 鈥榖ot鈥? 鈥榗rawler鈥? 鈥榮pider鈥?upfront. \nYou can add to it any other known crawlers.\nList<string> Crawlers3 = new List<string>()\n{\n    ""bot"",""crawler"",""spider"",""80legs"",""baidu"",""yahoo! slurp"",""ia_archiver"",""mediapartners-google"",\n    ""lwp-trivial"",""nederland.zoek"",""ahoy"",""anthill"",""appie"",""arale"",""araneo"",""ariadne"",            \n    ""atn_worldwide"",""atomz"",""bjaaland"",""ukonline"",""calif"",""combine"",""cosmos"",""cusco"",\n    ""cyberspyder"",""digger"",""grabber"",""downloadexpress"",""ecollector"",""ebiness"",""esculapio"",\n    ""esther"",""felix ide"",""hamahakki"",""kit-fireball"",""fouineur"",""freecrawl"",""desertrealm"",\n    ""gcreep"",""golem"",""griffon"",""gromit"",""gulliver"",""gulper"",""whowhere"",""havindex"",""hotwired"",\n    ""htdig"",""ingrid"",""informant"",""inspectorwww"",""iron33"",""teoma"",""ask jeeves"",""jeeves"",\n    ""image.kapsi.net"",""kdd-explorer"",""label-grabber"",""larbin"",""linkidator"",""linkwalker"",\n    ""lockon"",""marvin"",""mattie"",""mediafox"",""merzscope"",""nec-meshexplorer"",""udmsearch"",""moget"",\n    ""motor"",""muncher"",""muninn"",""muscatferret"",""mwdsearch"",""sharp-info-agent"",""webmechanic"",\n    ""netscoop"",""newscan-online"",""objectssearch"",""orbsearch"",""packrat"",""pageboy"",""parasite"",\n    ""patric"",""pegasus"",""phpdig"",""piltdownman"",""pimptrain"",""plumtreewebaccessor"",""getterrobo-plus"",\n    ""raven"",""roadrunner"",""robbie"",""robocrawl"",""robofox"",""webbandit"",""scooter"",""search-au"",\n    ""searchprocess"",""senrigan"",""shagseeker"",""site valet"",""skymob"",""slurp"",""snooper"",""speedy"",\n    ""curl_image_client"",""suke"",""www.sygol.com"",""tach_bw"",""templeton"",""titin"",""topiclink"",""udmsearch"",\n    ""urlck"",""valkyrie libwww-perl"",""verticrawl"",""victoria"",""webscout"",""voyager"",""crawlpaper"",\n    ""webcatcher"",""t-h-u-n-d-e-r-s-t-o-n-e"",""webmoose"",""pagesinventory"",""webquest"",""webreaper"",\n    ""webwalker"",""winona"",""occam"",""robi"",""fdse"",""jobo"",""rhcs"",""gazz"",""dwcp"",""yeti"",""fido"",""wlm"",\n    ""wolp"",""wwwc"",""xget"",""legs"",""curl"",""webs"",""wget"",""sift"",""cmc""\n};\nstring ua = Request.UserAgent.ToLower();\nbool iscrawler = Crawlers3.Exists(x => ua.Contains(x));\n\n3) Paranoid\nIs pretty fast, but a little slower than options 1 and 2.\nIt鈥檚 the most accurate, and allows you to maintain the lists if you want.\nYou can maintain a separate list of names with 鈥榖ot鈥?in them if you are afraid of false positives in future.\nIf we get a short match we log it and check it for a false positive.\n// crawlers that have \'bot\' in their useragent\nList<string> Crawlers1 = new List<string>()\n{\n    ""googlebot"",""bingbot"",""yandexbot"",""ahrefsbot"",""msnbot"",""linkedinbot"",""exabot"",""compspybot"",\n    ""yesupbot"",""paperlibot"",""tweetmemebot"",""semrushbot"",""gigabot"",""voilabot"",""adsbot-google"",\n    ""botlink"",""alkalinebot"",""araybot"",""undrip bot"",""borg-bot"",""boxseabot"",""yodaobot"",""admedia bot"",\n    ""ezooms.bot"",""confuzzledbot"",""coolbot"",""internet cruiser robot"",""yolinkbot"",""diibot"",""musobot"",\n    ""dragonbot"",""elfinbot"",""wikiobot"",""twitterbot"",""contextad bot"",""hambot"",""iajabot"",""news bot"",\n    ""irobot"",""socialradarbot"",""ko_yappo_robot"",""skimbot"",""psbot"",""rixbot"",""seznambot"",""careerbot"",\n    ""simbot"",""solbot"",""mail.ru_bot"",""spiderbot"",""blekkobot"",""bitlybot"",""techbot"",""void-bot"",\n    ""vwbot_k"",""diffbot"",""friendfeedbot"",""archive.org_bot"",""woriobot"",""crystalsemanticsbot"",""wepbot"",\n    ""spbot"",""tweetedtimes bot"",""mj12bot"",""who.is bot"",""psbot"",""robot"",""jbot"",""bbot"",""bot""\n};\n\n// crawlers that don\'t have \'bot\' in their useragent\nList<string> Crawlers2 = new List<string>()\n{\n    ""baiduspider"",""80legs"",""baidu"",""yahoo! slurp"",""ia_archiver"",""mediapartners-google"",""lwp-trivial"",\n    ""nederland.zoek"",""ahoy"",""anthill"",""appie"",""arale"",""araneo"",""ariadne"",""atn_worldwide"",""atomz"",\n    ""bjaaland"",""ukonline"",""bspider"",""calif"",""christcrawler"",""combine"",""cosmos"",""cusco"",""cyberspyder"",\n    ""cydralspider"",""digger"",""grabber"",""downloadexpress"",""ecollector"",""ebiness"",""esculapio"",""esther"",\n    ""fastcrawler"",""felix ide"",""hamahakki"",""kit-fireball"",""fouineur"",""freecrawl"",""desertrealm"",\n    ""gammaspider"",""gcreep"",""golem"",""griffon"",""gromit"",""gulliver"",""gulper"",""whowhere"",""portalbspider"",\n    ""havindex"",""hotwired"",""htdig"",""ingrid"",""informant"",""infospiders"",""inspectorwww"",""iron33"",\n    ""jcrawler"",""teoma"",""ask jeeves"",""jeeves"",""image.kapsi.net"",""kdd-explorer"",""label-grabber"",\n    ""larbin"",""linkidator"",""linkwalker"",""lockon"",""logo_gif_crawler"",""marvin"",""mattie"",""mediafox"",\n    ""merzscope"",""nec-meshexplorer"",""mindcrawler"",""udmsearch"",""moget"",""motor"",""muncher"",""muninn"",\n    ""muscatferret"",""mwdsearch"",""sharp-info-agent"",""webmechanic"",""netscoop"",""newscan-online"",\n    ""objectssearch"",""orbsearch"",""packrat"",""pageboy"",""parasite"",""patric"",""pegasus"",""perlcrawler"",\n    ""phpdig"",""piltdownman"",""pimptrain"",""pjspider"",""plumtreewebaccessor"",""getterrobo-plus"",""raven"",\n    ""roadrunner"",""robbie"",""robocrawl"",""robofox"",""webbandit"",""scooter"",""search-au"",""searchprocess"",\n    ""senrigan"",""shagseeker"",""site valet"",""skymob"",""slcrawler"",""slurp"",""snooper"",""speedy"",\n    ""spider_monkey"",""spiderline"",""curl_image_client"",""suke"",""www.sygol.com"",""tach_bw"",""templeton"",\n    ""titin"",""topiclink"",""udmsearch"",""urlck"",""valkyrie libwww-perl"",""verticrawl"",""victoria"",\n    ""webscout"",""voyager"",""crawlpaper"",""wapspider"",""webcatcher"",""t-h-u-n-d-e-r-s-t-o-n-e"",\n    ""webmoose"",""pagesinventory"",""webquest"",""webreaper"",""webspider"",""webwalker"",""winona"",""occam"",\n    ""robi"",""fdse"",""jobo"",""rhcs"",""gazz"",""dwcp"",""yeti"",""crawler"",""fido"",""wlm"",""wolp"",""wwwc"",""xget"",\n    ""legs"",""curl"",""webs"",""wget"",""sift"",""cmc""\n};\n\nstring ua = Request.UserAgent.ToLower();\nstring match = null;\n\nif (ua.Contains(""bot"")) match = Crawlers1.FirstOrDefault(x => ua.Contains(x));\nelse match = Crawlers2.FirstOrDefault(x => ua.Contains(x));\n\nif (match != null && match.Length < 5) Log(""Possible new crawler found: "", ua);\n\nbool iscrawler = match != null;\n\nNotes:\n\nIt鈥檚 tempting to just keep adding names to the regex option 1. But if you do this it will become slower. If you want a more complete list then linq with lambda is faster.\nMake sure .ToLower() is outside of your linq method 鈥?remember the method is a loop and you would be modifying the string during each iteration.\nAlways put the heaviest bots at the start of the list, so they match sooner.\nPut the lists into a static class so that they are not rebuilt on every pageview.\n\nHoneypots\nThe only real alternative to this is to create a 鈥榟oneypot鈥?link on your site that only a bot will reach. You then log the user agent strings that hit the honeypot page to a database. You can then use those logged strings to classify crawlers.\nPostives: It will match some unknown crawlers that aren鈥檛 declaring themselves.\nNegatives: Not all crawlers dig deep enough to hit every link on your site, and so they may not reach your honeypot.\n', '\nYou can find a very thorough database of data on known ""good"" web crawlers in the robotstxt.org Robots Database.  Utilizing this data would be far more effective than just matching bot in the user-agent.\n', '\nOne suggestion is to create an empty anchor on your page that only a bot would follow.  Normal users wouldn\'t see the link, leaving spiders and bots to follow.  For example, an empty anchor tag that points to a subfolder would record a get request in your logs...\n<a href=""dontfollowme.aspx""></a>\n\nMany people use this method while running a HoneyPot to catch malicious bots that aren\'t following the robots.txt file.  I use the empty anchor method in an ASP.NET honeypot solution I wrote to trap and block those creepy crawlers...\n', '\nAny visitor whose entry page is /robots.txt is probably a bot.\n', '\nSomething quick and dirty like this might be a good start:\nreturn if request.user_agent =~ /googlebot|msnbot|baidu|curl|wget|Mediapartners-Google|slurp|ia_archiver|Gigabot|libwww-perl|lwp-trivial/i\n\nNote: rails code, but regex is generally applicable.\n', ""\nI'm pretty sure a large proportion of bots don't use robots.txt, however that was my first thought.\nIt seems to me that the best way to detect a bot is with time between requests, if the time between requests is consistently fast then its a bot.\n"", '\nvoid CheckBrowserCaps()\n    {\n        String labelText = """";\n        System.Web.HttpBrowserCapabilities myBrowserCaps = Request.Browser;\n        if (((System.Web.Configuration.HttpCapabilitiesBase)myBrowserCaps).Crawler)\n        {\n            labelText = ""Browser is a search engine."";\n        }\n        else\n        {\n            labelText = ""Browser is not a search engine."";\n        }\n\n        Label1.Text = labelText;\n    }\n\nHttpCapabilitiesBase.Crawler Property\n']",https://stackoverflow.com/questions/544450/detecting-honest-web-crawlers,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to programmatically fill input elements built with React?,"
I'm tasked with crawling website built with React. I'm trying to fill in input fields and submitting the form using javascript injects to the page (either selenium or webview in mobile). This works like a charm on every other site + technology but React seems to be a real pain.
so here is a sample code 
var email = document.getElementById( 'email' );
email.value = 'example@mail.com';

I the value changes on the DOM input element, but the React does not trigger the change event.
I've been trying plethora of different ways to get the React to update the state.
var event = new Event('change', { bubbles: true });
email.dispatchEvent( event );

no avail
var event = new Event('input', { bubbles: true });
email.dispatchEvent( event );

not working
email.onChange( event );

not working
I cannot believe interacting with React has been made so difficult. I would greatly appreciate any help. 
Thank you
",24k,"
            37
        ","[""\nThis accepted solution appears not to work in React > 15.6 (including React 16) as a result of changes to de-dupe input and change events.\nYou can see the React discussion here: https://github.com/facebook/react/issues/10135\nAnd the suggested workaround here:\nhttps://github.com/facebook/react/issues/10135#issuecomment-314441175\nReproduced here for convenience:\nInstead of\ninput.value = 'foo';\ninput.dispatchEvent(new Event('input', {bubbles: true}));\n\nYou would use\nfunction setNativeValue(element, value) {\n  const valueSetter = Object.getOwnPropertyDescriptor(element, 'value').set;\n  const prototype = Object.getPrototypeOf(element);\n  const prototypeValueSetter = Object.getOwnPropertyDescriptor(prototype, 'value').set;\n\n  if (valueSetter && valueSetter !== prototypeValueSetter) {\n    prototypeValueSetter.call(element, value);\n  } else {\n    valueSetter.call(element, value);\n  }\n}\n\nand then\nsetNativeValue(input, 'foo');\ninput.dispatchEvent(new Event('input', { bubbles: true }));\n\n"", '\nReact is listening for the input event of text fields.\nYou can change the value and manually trigger an input event, and react\'s onChange handler will trigger:\n\n\nclass Form extends React.Component {\r\n  constructor(props) {\r\n    super(props)\r\n    this.state = {value: \'\'}\r\n  }\r\n  \r\n  handleChange(e) {\r\n    this.setState({value: e.target.value})\r\n    console.log(\'State updated to \', e.target.value);\r\n  }\r\n  \r\n  render() {\r\n    return (\r\n      <div>\r\n        <input\r\n          id=\'textfield\'\r\n          value={this.state.value}\r\n          onChange={this.handleChange.bind(this)}\r\n        />\r\n        <p>{this.state.value}</p>\r\n      </div>      \r\n    )\r\n  }\r\n}\r\n\r\nReactDOM.render(\r\n  <Form />,\r\n  document.getElementById(\'app\')\r\n)\r\n\r\ndocument.getElementById(\'textfield\').value = \'foo\'\r\nconst event = new Event(\'input\', { bubbles: true })\r\ndocument.getElementById(\'textfield\').dispatchEvent(event)\n<script src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react.min.js""></script>\r\n<script src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react-dom.min.js""></script>\r\n\r\n<div id=\'app\'></div>\n\n\n', ""\nHere is the cleanest possible solution for inputs, selects, checkboxes, etc. (works not only for react inputs)\n/**\n * See [Modify React Component's State using jQuery/Plain Javascript from Chrome Extension](https://stackoverflow.com/q/41166005)\n * See https://github.com/facebook/react/issues/11488#issuecomment-347775628\n * See [How to programmatically fill input elements built with React?](https://stackoverflow.com/q/40894637)\n * See https://github.com/facebook/react/issues/10135#issuecomment-401496776\n *\n * @param {HTMLInputElement | HTMLSelectElement} el\n * @param {string} value\n */\nfunction setNativeValue(el, value) {\n  const previousValue = el.value;\n\n  if (el.type === 'checkbox' || el.type === 'radio') {\n    if ((!!value && !el.checked) || (!!!value && el.checked)) {\n      el.click();\n    }\n  } else el.value = value;\n\n  const tracker = el._valueTracker;\n  if (tracker) {\n    tracker.setValue(previousValue);\n  }\n\n  // 'change' instead of 'input', see https://github.com/facebook/react/issues/11488#issuecomment-381590324\n  el.dispatchEvent(new Event('change', { bubbles: true }));\n}\n\nUsage:\nsetNativeValue(document.getElementById('name'), 'Your name');\ndocument.getElementById('radio').click(); // or setNativeValue(document.getElementById('radio'), true)\ndocument.getElementById('checkbox').click(); // or setNativeValue(document.getElementById('checkbox'), true)\n\n"", '\nI noticed the input element had some property with a name along the lines of __reactEventHandlers$..., which had some functions including an onChange.\nThis worked for finding that function and triggering it\nlet getReactEventHandlers = (element) => {\n    // the name of the attribute changes, so we find it using a match.\n    // It\'s something like `element.__reactEventHandlers$...`\n    let reactEventHandlersName = Object.keys(element)\n       .filter(key => key.match(\'reactEventHandler\'));\n    return element[reactEventHandlersName];\n}\n\nlet triggerReactOnChangeEvent = (element) => {\n    let ev = new Event(\'change\');\n    // workaround to set the event target, because `ev.target = element` doesn\'t work\n    Object.defineProperty(ev, \'target\', {writable: false, value: element});\n    getReactEventHandlers(element).onChange(ev);\n}\n\ninput.value = ""some value"";\ntriggerReactOnChangeEvent(input);\n\n', '\nWithout element ids:\nexport default function SomeComponent() {\n    const inputRef = useRef();\n    const [address, setAddress] = useState("""");\n    const onAddressChange = (e) => {\n        setAddress(e.target.value);\n    }\n    const setAddressProgrammatically = (newValue) => {\n        const event = new Event(\'change\', { bubbles: true });\n        const input = inputRef.current;\n        if (input) {\n            setAddress(newValue);\n            input.value = newValue;\n            input.dispatchEvent(event);\n        }\n    }\n    return (\n        ...\n        <input ref={inputRef} type=""text"" value={address} onChange={onAddressChange}/>\n        ...\n    );\n}\n\n', '\nReact 17 works with fibers:\nfunction findReact(dom) {\n    let key = Object.keys(dom).find(key => key.startsWith(""__reactFiber$""));\n    let internalInstance = dom[key];\n    if (internalInstance == null) return ""internalInstance is null: "" + key;\n\n    if (internalInstance.return) { // react 16+\n        return internalInstance._debugOwner\n            ? internalInstance._debugOwner.stateNode\n           : internalInstance.return.stateNode;\n    } else { // react <16\n        return internalInstance._currentElement._owner._instance;\n   }\n}\n\nthen:\nfindReact(domElement).onChangeWrapper(""New value"");\n\nthe domElement in this is the tr with the data-param-name of the field you are trying to change:\nvar domElement = ?.querySelectorAll(\'tr[data-param-name=""<my field name>""]\')\n\n']",https://stackoverflow.com/questions/40894637/how-to-programmatically-fill-input-elements-built-with-react,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to identify web-crawler?,"
How can I filter out hits from webcrawlers etc. Hits which not is human..
I use maxmind.com to request the city from the IP.. It is not quite cheap if I have to pay for ALL hits including webcrawlers, robots etc.
",27k,"
            36
        ","['\nThere are two general ways to detect robots and I would call them ""Polite/Passive"" and ""Aggressive"". Basically, you have to give your web site a psychological disorder.\nPolite\nThese are ways to politely tell crawlers that they shouldn\'t crawl your site and to limit how often you are crawled. Politeness is ensured through robots.txt file in which you specify which bots, if any, should be allowed to crawl your website and how often your website can be crawled. This assumes that the robot you\'re dealing with is polite.\nAggressive\nAnother way to keep bots off your site is to get aggressive. \nUser Agent\nSome aggressive behavior includes (as previously mentioned by other users) the filtering of user-agent strings. This is probably the simplest, but also the least reliable way to detect if it\'s a user or not. A lot of bots tend to spoof user agents and some do it for legitimate reasons (i.e. they only want to crawl mobile content), while others simply don\'t want to be identified as bots. Even worse, some bots spoof legitimate/polite bot agents, such as the user agents of google, microsoft, lycos and other crawlers which are generally considered polite. Relying on the user agent can be helpful, but not by itself.\nThere are more aggressive ways to deal with robots that spoof user agents AND don\'t abide by your robots.txt file:\nBot Trap\nI like to think of this as a ""Venus Fly Trap,"" and it basically punishes any bot that wants to play tricks with you. \nA bot trap is probably the most effective way to find bots that don\'t adhere to your robots.txt file without actually impairing the usability of your website. Creating a bot trap ensures that only bots are captured and not real users. The basic way to do it is to setup a directory which you specifically mark as off limits in your robots.txt file, so any robot that is polite will not fall into the trap. The second thing you do is to place a ""hidden"" link from your website to the bot trap directory (this ensures that real users will never go there, since real users never click on invisible links). Finally, you ban any IP address that goes to the bot trap directory. \nHere are some instructions on how to achieve this:\nCreate a bot trap (or in your case: a PHP bot trap).\nNote: of course, some bots are smart enough to read your robots.txt file, see all the directories which you\'ve marked as ""off limits"" and STILL ignore your politeness settings (such as crawl rate and allowed bots). Those bots will probably not fall into your bot trap despite the fact that they are not polite.\nViolent\nI think this is actually too aggressive for the general audience (and general use), so if there are any kids under the age of 18, then please take them to another room!\nYou can make the bot trap ""violent"" by simply not specifying a robots.txt file. In this situation ANY BOT that crawls the hidden links will probably end up in the bot trap and you can ban all bots, period! \nThe reason this is not recommended is that you may actually want some bots to crawl your website (such as Google, Microsoft or other bots for site indexing). Allowing your website to be politely crawled by the bots from Google, Microsoft, Lycos, etc. will ensure that your site gets indexed and it shows up when people search for it on their favorite search engine.\nSelf Destructive\nYet another way to limits what bots can crawl on your website, is to serve CAPTCHAs or other challenges which a bot cannot solve. This comes at an expense of your users and I would think that anything which makes your website less usable (such as a CAPTCHA) is ""self destructive."" This, of course, will not actually block the bot from repeatedly trying to crawl your website, it will simply make your website very uninteresting to them. There are ways to ""get around"" the CAPTCHAs, but they\'re difficult to implement so I\'m not going to delve into this too much.\nConclusion\nFor your purposes, probably the best way to deal with bots is to employ a combination of the above mentioned strategies:\n\nFilter user agents.\nSetup a bot trap (the violent one).\n\nCatch all the bots that go into the violent bot trap and simply black-list their IPs (but don\'t block them). This way you will still get the ""benefits"" of being crawled by bots, but you will not have to pay to check the IP addresses that are black-listed due to going to your bot trap.\n', ""\nYou can check USER_AGENT, something like:\nfunction crawlerDetect($USER_AGENT)\n{\n    $crawlers = array(\n    array('Google', 'Google'),\n    array('msnbot', 'MSN'),\n    array('Rambler', 'Rambler'),\n    array('Yahoo', 'Yahoo'),\n    array('AbachoBOT', 'AbachoBOT'),\n    array('accoona', 'Accoona'),\n    array('AcoiRobot', 'AcoiRobot'),\n    array('ASPSeek', 'ASPSeek'),\n    array('CrocCrawler', 'CrocCrawler'),\n    array('Dumbot', 'Dumbot'),\n    array('FAST-WebCrawler', 'FAST-WebCrawler'),\n    array('GeonaBot', 'GeonaBot'),\n    array('Gigabot', 'Gigabot'),\n    array('Lycos', 'Lycos spider'),\n    array('MSRBOT', 'MSRBOT'),\n    array('Scooter', 'Altavista robot'),\n    array('AltaVista', 'Altavista robot'),\n    array('IDBot', 'ID-Search Bot'),\n    array('eStyle', 'eStyle Bot'),\n    array('Scrubby', 'Scrubby robot')\n    );\n\n    foreach ($crawlers as $c)\n    {\n        if (stristr($USER_AGENT, $c[0]))\n        {\n            return($c[1]);\n        }\n    }\n\n    return false;\n}\n\n// example\n\n$crawler = crawlerDetect($_SERVER['HTTP_USER_AGENT']);\n\n"", ""\nThe user agent ($_SERVER['HTTP_USER_AGENT']) often identifies whether the connecting agent is a browser or a robot. Review logs/analytics for the user agents of crawlers that visit your site. Filter accordingly.\nTake note that the user agent is a header supplied by the client application. As such it can be pretty much anything and shouldn't be trusted 100%. Plan accordingly.\n"", ""\nChecking the User-Agent will protect you from legitimate bots like Google and Yahoo.\nHowever, if you're also being hit with spam bots, then chances are User-Agent comparison won't protect you since those bots typically forge a common User-Agent string anyway.  In that instance, you would need to imploy more sophisticated measures.  If user input is required, a simple image verification scheme like ReCaptcha will work.\nIf you're looking to filter out all page hits from a bot, unfortunately, there's no 100% reliable way to do this if the bot is forging its credentials.  This is just an annoying fact of life on the internet that web admins have to put up with.\n"", ""\nI found this package, it's actively being developed and I'm quite liking it so far:\nhttps://github.com/JayBizzle/Crawler-Detect\nIt's simple as this:\nuse Jaybizzle\\CrawlerDetect\\CrawlerDetect;\n\n$CrawlerDetect = new CrawlerDetect;\n\n// Check the user agent of the current 'visitor'\nif($CrawlerDetect->isCrawler()) {\n    // true if crawler user agent detected\n}\n\n// Pass a user agent as a string\nif($CrawlerDetect->isCrawler('Mozilla/5.0 (compatible; Sosospider/2.0; +http://help.soso.com/webspider.htm)')) {\n    // true if crawler user agent detected\n}\n\n// Output the name of the bot that matched (if any)\necho $CrawlerDetect->getMatches();\n\n"", '\nuseragentstring.com is serving a lilst that you can use to analyze the userstring:\n$api_request=""http://www.useragentstring.com/?uas="".urlencode($_SERVER[\'HTTP_USER_AGENT\']).""&getJSON=all"";\n$ua=json_decode(file_get_contents($api_request), true);\nif($ua[""agent_type""]==""Crawler"") die();\n\n']",https://stackoverflow.com/questions/8404775/how-to-identify-web-crawler,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Send Post Request in Scrapy,"
I am trying to crawl the latest reviews from google play store and to get that I need to make a post request.
With the Postman, it works and I get desired response.

but a post request in terminal gives me a server error
For ex: this page https://play.google.com/store/apps/details?id=com.supercell.boombeach
curl -H ""Content-Type: application/json"" -X POST -d '{""id"": ""com.supercell.boombeach"", ""reviewType"": '0', ""reviewSortOrder"": '0', ""pageNum"":'0'}' https://play.google.com/store/getreviews

gives a server error and
Scrapy just ignores this line:
frmdata = {""id"": ""com.supercell.boombeach"", ""reviewType"": 0, ""reviewSortOrder"": 0, ""pageNum"":0}
        url = ""https://play.google.com/store/getreviews""
        yield Request(url, callback=self.parse, method=""POST"", body=urllib.urlencode(frmdata))

",63k,"
            31
        ","[""\nThe answer above do not really solved the problem. They are sending the data as paramters instead of JSON data as the body of the request.\nFrom http://bajiecc.cc/questions/1135255/scrapy-formrequest-sending-json:\nmy_data = {'field1': 'value1', 'field2': 'value2'}\nrequest = scrapy.Request( url, method='POST', \n                          body=json.dumps(my_data), \n                          headers={'Content-Type':'application/json'} )\n\n"", '\nMake sure that each element in your formdata is of type string/unicode\nfrmdata = {""id"": ""com.supercell.boombeach"", ""reviewType"": \'0\', ""reviewSortOrder"": \'0\', ""pageNum"":\'0\'}\nurl = ""https://play.google.com/store/getreviews""\nyield FormRequest(url, callback=self.parse, formdata=frmdata)\n\nI think this will do\nIn [1]: from scrapy.http import FormRequest\n\nIn [2]: frmdata = {""id"": ""com.supercell.boombeach"", ""reviewType"": \'0\', ""reviewSortOrder"": \'0\', ""pageNum"":\'0\'}\n\nIn [3]: url = ""https://play.google.com/store/getreviews""\n\nIn [4]: r = FormRequest(url, formdata=frmdata)\n\nIn [5]: fetch(r)\n 2015-05-20 14:40:09+0530 [default] DEBUG: Crawled (200) <POST      https://play.google.com/store/getreviews> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7f3ea4258890>\n[s]   item       {}\n[s]   r          <POST https://play.google.com/store/getreviews>\n[s]   request    <POST https://play.google.com/store/getreviews>\n[s]   response   <200 https://play.google.com/store/getreviews>\n[s]   settings   <scrapy.settings.Settings object at 0x7f3eaa205450>\n[s]   spider     <Spider \'default\' at 0x7f3ea3449cd0>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\n', '\nSample Page Traversing using Post in Scrapy:\ndef directory_page(self,response):\n    if response:\n        profiles = response.xpath(""//div[@class=\'heading-h\']/h3/a/@href"").extract()\n        for profile in profiles:\n            yield Request(urljoin(response.url,profile),callback=self.profile_collector)\n\n        page = response.meta[\'page\'] + 1\n        if page :\n            yield FormRequest(\'https://rotmanconnect.com/AlumniDirectory/getmorerecentjoineduser\',\n                                        formdata={\'isSortByName\':\'false\',\'pageNumber\':str(page)},\n                                        callback= self.directory_page,\n                                        meta={\'page\':page})\n    else:\n         print ""No more page available""\n\n']",https://stackoverflow.com/questions/30342243/send-post-request-in-scrapy,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
how do web crawlers handle javascript,"
Today a lot of content on Internet is generated using JavaScript (specifically by background AJAX calls). I was wondering how web crawlers like Google handle them. Are they aware of JavaScript? Do they have a built-in JavaScript engine? Or do they simple ignore all JavaScript generated content in the page (I guess quite unlikely). Do people use specific techniques for getting their content indexed which would otherwise be available through background AJAX requests to a normal Internet user? 
",11k,"
            22
        ","[""\nJavaScript is handled by both Bing and Google crawlers. Yahoo uses the Bing crawler data, so it should be handled as well. I didn't look into other search engines, so if you care about them, you should look them up.\nBing published guidance in March 2014 as to how to create JavaScript-based websites that work with their crawler (mostly related to pushState) that are good practices in general:\n\nAvoid creating broken links with pushState\nAvoid creating two different links that link to the same content with pushState\nAvoid cloaking. (Here's an article Bing published about their cloaking detection in 2007)\nSupport browsers (and crawlers) that can't handle pushState.\n\nGoogle later published guidance in May 2014 as to how to create JavaScript-based websites that work with their crawler, and their recommendations are also recommended:\n\nDon't block the JavaScript (and CSS) in the robots.txt file.\nMake sure you can handle the load of the crawlers.\nIt's a good idea to support browsers and crawlers that can't handle (or users and organizations that won't allow) JavaScript\nTricky JavaScript that relies on arcane or specific features of the language might not work with the crawlers.\nIf your JavaScript removes content from the page, it might not get indexed.\naround.\n\n"", ""\nMost of them don't handle Javascript in any way. (At least, all the major search engines' crawlers don't.)\nThis is why it's still important to have your site gracefully handle navigation without Javascript.\n"", ""\nI have tested this by putting pages on my site only reachable by Javascript and then observing their presence in search indexes.\nPages on my site which were reachable only by Javascript were subsequently indexed by Google.\nThe content was reached through Javascript with a 'classic' technique or constructing a URL and setting the window.location accordingly.\n"", ""\nPrecisely what Ben S said. And anyone accessing your site with Lynx won't execute JavaScript either. If your site is intended for general public use, it should generally be usable without JavaScript.\nAlso, related: if there are pages that you would want a search engine to find, and which would normally arise only from JavaScript, you might consider generating static versions of them, reachable by a crawlable site map, where these static pages use JavaScript to load the current version when hit by a JavaScript-enabled browser (in case a human with a browser follows your site map). The search engine will see the static form of the page, and can index it.\n"", '\nCrawlers doesn\'t parse Javascript to find out what it does.\nThey may be built to recognise some classic snippets like  onchange=""window.location.href=this.options[this.selectedIndex].value;"" or onclick=""window.location.href=\'blah.html\';"", but they don\'t bother with things like content fetched using AJAX. At least not yet, and content fetched like that will always be secondary anyway.\nSo, Javascript should be used only for additional functionality. The main content taht you want the crawlers to find should still be plain text in the page and regular links that the crawlers easily can follow.\n', ""\ncrawlers can handle javascript or ajax calls if they are using some kind of frameworks like 'htmlunit' or 'selenium'\n""]",https://stackoverflow.com/questions/1785083/how-do-web-crawlers-handle-javascript,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTTPWebResponse + StreamReader Very Slow,"
I'm trying to implement a limited web crawler in C# (for a few hundred sites only)
using HttpWebResponse.GetResponse() and Streamreader.ReadToEnd() , also tried using StreamReader.Read() and a loop to build my HTML string.
I'm only downloading pages which are about 5-10K. 
It's all very slow! For example, the average GetResponse() time is about half a second, while the average StreamREader.ReadToEnd() time is about 5 seconds!
All sites should be very fast, as they are very close to my location, and have fast servers. (in Explorer takes practically nothing to D/L) and I am not using any proxy.
My Crawler has about 20 threads reading simultaneously from the same site. Could this be causing a problem?
How do I reduce StreamReader.ReadToEnd times DRASTICALLY?
",24k,"
            21
        ","['\nHttpWebRequest may be taking a while to detect your proxy settings. Try adding this to your application config:\n<system.net>\n  <defaultProxy enabled=""false"">\n    <proxy/>\n    <bypasslist/>\n    <module/>\n  </defaultProxy>\n</system.net>\n\nYou might also see a slight performance gain from buffering your reads to reduce the number of calls made to the underlying operating system socket:\nusing (BufferedStream buffer = new BufferedStream(stream))\n{\n  using (StreamReader reader = new StreamReader(buffer))\n  {\n    pageContent = reader.ReadToEnd();\n  }\n}\n\n', '\nWebClient\'s DownloadString is a simple wrapper for HttpWebRequest, could you try using that temporarily and see if the speed improves? If things get much faster, could you share your code so we can have a look at what may be wrong with it?\nEDIT:\nIt seems HttpWebRequest observes IE\'s \'max concurrent connections\' setting, are these URLs on the same domain? You could try increasing the connections limit to see if that helps? I found this article about the problem:\n\nBy default, you can\'t perform more\n  than 2-3 async HttpWebRequest (depends\n  on the OS). In order to override it\n  (the easiest way, IMHO) don\'t forget\n  to add this under \n  section in the application\'s config\n  file:\n\n<system.net>\n  <connectionManagement>\n     <add address=""*"" maxconnection=""65000"" />\n  </connectionManagement>\n</system.net>\n\n', ""\nI had the same problem, but when I sat the HttpWebRequest's Proxy parameter to null, it solved the problem.\nUriBuilder ub = new UriBuilder(url);\nHttpWebRequest request = (HttpWebRequest)WebRequest.Create( ub.Uri );\nrequest.Proxy = null;\nHttpWebResponse response = (HttpWebResponse)request.GetResponse();\n\n"", '\nHave you tried ServicePointManager.maxConnections?  I usually set it to 200 for things similar to this.\n', '\nI had problem the same problem but worst.\nresponse = (HttpWebResponse)webRequest.GetResponse(); in my code\ndelayed about 10 seconds before running more code and after this the download saturated my connection.\nkurt\'s answer defaultProxy enabled=""false"" \nsolved the problem. now the response is almost instantly and i can download any http file at my connections maximum speed :)\nsorry for bad english\n', '\nI found the Application Config method did not work, but the problem was still due to the proxy settings.  My simple request used to take up to 30 seconds, now it takes 1.\npublic string GetWebData()\n{\n            string DestAddr = ""http://mydestination.com"";\n            System.Net.WebClient myWebClient = new System.Net.WebClient();\n            WebProxy myProxy = new WebProxy();\n            myProxy.IsBypassed(new Uri(DestAddr));\n            myWebClient.Proxy = myProxy;\n            return myWebClient.DownloadString(DestAddr);\n}\n\n', ""\nThank you all for answers, they've helped me to dig in proper direction. I've faced with the same performance issue, though proposed solution to change application config file (as I understood that solution is for web applications) doesn't fit my needs, my solution is shown below:\nHttpWebRequest webRequest;\n\nwebRequest = (HttpWebRequest)System.Net.WebRequest.Create(fullUrl);\nwebRequest.Method = WebRequestMethods.Http.Post;\n\nif (useDefaultProxy)\n{\n    webRequest.Proxy = System.Net.WebRequest.DefaultWebProxy;\n    webRequest.Credentials = CredentialCache.DefaultCredentials;\n}\nelse\n{\n    System.Net.WebRequest.DefaultWebProxy = null;\n    webRequest.Proxy = System.Net.WebRequest.DefaultWebProxy;\n}\n\n"", ""\nWhy wouldn't multithreading solve this issue? Multithreading would minimize the network wait times, and since you'd be storing the contents of the buffer in system memory (RAM), there would be no IO bottleneck from dealing with a filesystem. Thus, your 82 pages that take 82 seconds to download and parse, should take like 15 seconds (assuming a 4x processor). Correct me if I'm missing something. \n____ DOWNLOAD THREAD_____*\nDownload Contents\nForm Stream\nRead Contents\n_________________________*\n"", '\nTry to add cookie(AspxAutoDetectCookieSupport=1) to your request like this\nrequest.CookieContainer = new CookieContainer();         \nrequest.CookieContainer.Add(new Cookie(""AspxAutoDetectCookieSupport"", ""1"") { Domain = target.Host });\n\n']",https://stackoverflow.com/questions/901323/httpwebresponse-streamreader-very-slow,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web crawler that can interpret JavaScript [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



I want to write a web crawler that can interpret JavaScript. Basically its a program in Java or PHP that takes a URL as input and outputs the DOM tree which is similar to the output in Firebug HTML window. The best example is Kayak.com where you can not see the resulting DOM displayed on the browser when you 'view source' but can save the resulting HTML though Firebug. 
How would I go about doing this? What tools exist that would help me?
",21k,"
            18
        ","['\nRuby\'s Capybara is an integration test library, but it can also be used to write stand-alone web-crawlers. Given that it uses backends like Selenium or headless WebKit, it interprets javascript out-of-the-box:\nrequire \'capybara/dsl\'\nrequire \'capybara-webkit\'\n\ninclude Capybara::DSL\nCapybara.current_driver = :webkit\nCapybara.app_host = ""http://www.google.com""\npage.visit(""/"")\nputs(page.html)\n\n', ""\nI've been using HtmlUnit (Java). This was originally designed for unit testing pages. It's not perfect javascript, but it hasn't failed me in my limited usage. According to the site, it can run the following JS frameworks to a reasonable degree:\n\njQuery 1.2.6\nMochiKit 1.4.1\nGWT 2.0.0\nSarissa 0.9.9.3\nMooTools 1.2.1\nPrototype 1.6.0\nExt JS 2.2\nDojo 1.0.2\nYUI 2.3.0\n\n"", ""\nYou are more likely to have success in Java than in PHP.  There is a pre-existing Javascript interpreter for Java called Rhino.  It's a reference implementation, and well-documented.\nRhino is used in lots of existing Java apps to provide Javascript scripting ability within the app.  I have also heard of it used to assist with performing automated tests in Javascript.\nI also know that Java includes code that can parse and render HTML, though someone who knows more about Java than me can probably advise more on that.  I am not denying it would be very difficult to achieve something like this; you'd essentially be re-implementing a lot of what a browser does.\n"", ""\nYou could use Mozilla's rendering engine Gecko:\nhttps://developer.mozilla.org/en/Gecko\n"", '\nGive a look here: http://snippets.scrapy.org/snippets/22/\nit\'s a python screen scraping and web crawling framework used with webdrivers that open a page, render all the things you need and gives you the possibilities to ""capture"" anything you want in the page via \n']",https://stackoverflow.com/questions/2670082/web-crawler-that-can-interpret-javascript,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Submit form with no submit button in rvest,"
I'm trying write a crawler to download some information, similar to this Stack Overflow post.  The answer is useful for creating the filled-in form, but I'm struggling to find a way to submit the form when a submit button is not part of the form.  Here is an example:
session <- html_session(""www.chase.com"")
form <- html_form(session)[[3]]

filledform <- set_values(form, `user_name` = user_name, `usr_password` = usr_password)
session <- submit_form(session, filledform)

At this point, I receive this error:
Error in names(submits)[[1]] : subscript out of bounds

How can I make this form submit?
",3k,"
            8
        ","['\nHere\'s a dirty hack that works for me: After studying the submit_form source code, I figured that I could work around the problem by injecting a fake submit button into my code version of the form, and then the submit_form function would call that. It works, except that it gives a warning that often lists an inappropriate input object (not in the example below, though). However, despite the warning, the code works for me:\nsession <- html_session(""www.chase.com"")\nform <- html_form(session)[[3]]\n\n# Form on home page has no submit button,\n# so inject a fake submit button or else rvest cannot submit it.\n# When I do this, rvest gives a warning ""Submitting with \'___\'"", where ""___"" is\n# often an irrelevant field item.\n# This warning might be an rvest (version 0.3.2) bug, but the code works.\nfake_submit_button <- list(name = NULL,\n                           type = ""submit"",\n                           value = NULL,\n                           checked = NULL,\n                           disabled = NULL,\n                           readonly = NULL,\n                           required = FALSE)\nattr(fake_submit_button, ""class"") <- ""input""\nform[[""fields""]][[""submit""]] <- fake_submit_button\n\nuser_name <- ""user""\nusr_password <- ""password""\n\nfilledform <- set_values(form, `user_name` = user_name, `usr_password` = usr_password)\nsession <- submit_form(session, filledform)\n\nThe successful result displays the following warning, which I simply ignore:\n> Submitting with \'submit\'\n\n']",https://stackoverflow.com/questions/33885629/submit-form-with-no-submit-button-in-rvest,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HtmlUnit Only Displays Host HTML Page for GWT App,"
I am using HtmlUnit API to add crawler support to my GWT app as follows:
PrintWriter out = null;
try {
    resp.setCharacterEncoding(CHAR_ENCODING);
    resp.setContentType(""text/html"");

    url = buildUrl(req);
    out = resp.getWriter();

    WebClient webClient = webClientProvider.get();

    // set options
    WebClientOptions options = webClient.getOptions();
    options.setCssEnabled(false);
    options.setThrowExceptionOnScriptError(false);
    options.setThrowExceptionOnFailingStatusCode(false);
    options.setRedirectEnabled(true);
    options.setJavaScriptEnabled(true);

    // set timeouts
    webClient.setJavaScriptTimeout(0);
    webClient.waitForBackgroundJavaScript(20000);

    // ajax controller
    webClient.setAjaxController(new NicelyResynchronizingAjaxController());

    // render page
    HtmlPage page = webClient.getPage(url);

    webClient.getJavaScriptEngine().pumpEventLoop(timeoutMillis);

    out.println(page.asXml());

    webClient.closeAllWindows();
}
...

However; only the bare HTML host page for my GWT app is produced and sent to the client.

UPDATE: Here is the output from Chrome DevTools:
Request URL:http://127.0.0.1:8888/MyApp.html?gwt.codesvr=127.0.0.1:9997&_escaped_fragment_=myobject%3Bid%3D507ac730e4b0e3b7a73b1b81
Request Method:GET
Status Code:200 OK
Request Headersview source
Accept:text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Charset:ISO-8859-1,utf-8;q=0.7,*;q=0.3
Accept-Encoding:gzip,deflate,sdch
Accept-Language:en-GB,en-US;q=0.8,en;q=0.6
Cache-Control:max-age=0
Connection:keep-alive
Cookie:__utma=96992031.428505342.1351707614.1351707614.1356355174.2; __utmb=96992031.1.10.1356355174; __utmc=96992031; __utmz=96992031.1351707614.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)
Host:127.0.0.1:8888
User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11
Query String Parametersview URL encoded
gwt.codesvr:127.0.0.1:9997
_escaped_fragment_:myobject;id=507ac730e4b0e3b7a73b1b81
Response Headersview source
Content-Type:text/html; charset=utf-8
Server:Jetty(6.1.x)
Transfer-Encoding:chunked

Why isn't the GWT code being executed?
",3k,"
            5
        ","['\nI had to try many variants before I finally got it to work.  One key is to leave enough time for the javascript to fully run.  But there were a few other subtleties I don\'t recall -- you can find below my filter version that seems to work for me, look at the parameters I set, some were keys to get this thing to work.  Other than the timer parameters that depend upon what the code to execute (and server ability to run it quickly too), it is pretty generic, so I don\'t understand why Google does not package such a function once and for all!\n/**\n * Special URL token that gets passed from the crawler to the servlet filter.\n * This token is used in case there are already existing query parameters.\n */\nprivate static final String ESCAPED_FRAGMENT_FORMAT1 = ""_escaped_fragment_="";\nprivate static final int ESCAPED_FRAGMENT_LENGTH1 = ESCAPED_FRAGMENT_FORMAT1.length();\n/**\n * Special URL token that gets passed from the crawler to the servlet filter.\n * This token is used in case there are not already existing query parameters.\n */\nprivate static final String ESCAPED_FRAGMENT_FORMAT2 = ""&""+ESCAPED_FRAGMENT_FORMAT1;\nprivate static final int ESCAPED_FRAGMENT_LENGTH2 = ESCAPED_FRAGMENT_FORMAT2.length();\n\nprivate class SyncAllAjaxController extends NicelyResynchronizingAjaxController\n{\n  private static final long serialVersionUID = 1L;\n  @Override\n  public boolean processSynchron(HtmlPage page, WebRequest request, boolean async)\n  {\n      return true;\n  }\n}\n\nprivate WebClient webClient = null;\n\nprivate static final long _pumpEventLoopTimeoutMillis = 200;\nprivate static final long _jsTimeoutMillis = 200;\nprivate static final long _pageWaitMillis = 100;\nfinal int _maxLoopChecks = 2;\n\npublic void destroy()\n{\n  if (webClient != null)\n    webClient.closeAllWindows();\n}\n\npublic void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain)\n    throws IOException, ServletException\n{\n  // Grab the request uri and query strings.\n  final HttpServletRequest httpRequest = (HttpServletRequest) request;\n  final String requestURI = httpRequest.getRequestURI();\n  final String queryString = httpRequest.getQueryString();\n  final HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n  if ((queryString != null) && (queryString.contains(ESCAPED_FRAGMENT_FORMAT1)))\n  {\n    // This is a Googlebot crawler request, let\'s return a static indexable html page\n    // post javascript execution, as rendered in the browser.\n\n    final String domain = httpRequest.getServerName();\n    final int port = httpRequest.getServerPort();\n\n    // Rewrite the URL back to the original #! version\n    //   -- basically remove _escaped_fragment_ from the query. \n    // Unescape any %XX characters as need be.\n    final String urlStringWithHashFragment = requestURI + rewriteQueryString(queryString);\n    final String protocol = httpRequest.getProtocol();\n    final URL urlWithHashFragment = new URL(protocol, domain, port, urlStringWithHashFragment);\n    final WebRequest webRequest = new WebRequest(urlWithHashFragment);\n\n    // Use the headless browser to obtain an HTML snapshot.\n    webClient = new WebClient(BrowserVersion.FIREFOX_3_6);\n    webClient.getCache().clear();\n    webClient.setJavaScriptEnabled(true);\n    webClient.setThrowExceptionOnScriptError(false);\n    webClient.setRedirectEnabled(false);\n    webClient.setAjaxController(new SyncAllAjaxController());\n    webClient.setCssErrorHandler(new SilentCssErrorHandler());\n\n    if (_logger.isInfoEnabled())\n      _logger.info(""HtmlUnit starting webClient.getPage(webRequest) where webRequest = "" + webRequest.toString());\n    final HtmlPage page = webClient.getPage(webRequest);\n\n    // Important!  Give the headless browser enough time to execute JavaScript\n    // The exact time to wait may depend on your application.\n\n    webClient.getJavaScriptEngine().pumpEventLoop(_pumpEventLoopTimeoutMillis);\n\n    int waitForBackgroundJavaScript = webClient.waitForBackgroundJavaScript(_jsTimeoutMillis);\n    int loopCount = 0;\n    while (waitForBackgroundJavaScript > 0 && loopCount < _maxLoopChecks)\n    {\n      ++loopCount;\n      waitForBackgroundJavaScript = webClient.waitForBackgroundJavaScript(_jsTimeoutMillis);\n      if (waitForBackgroundJavaScript == 0)\n      {\n        if (_logger.isTraceEnabled())\n          _logger.trace(""HtmlUnit exits background javascript at loop counter "" + loopCount);\n        break;\n      }\n      synchronized (page) \n      {\n        if (_logger.isTraceEnabled())\n            _logger.trace(""HtmlUnit waits for background javascript at loop counter "" + loopCount);\n        try\n        {\n          page.wait(_pageWaitMillis);\n        }\n        catch (InterruptedException e)\n        {\n          _logger.error(""HtmlUnit ERROR on page.wait at loop counter "" + loopCount);\n          e.printStackTrace();\n        }\n      }\n    }\n    webClient.getAjaxController().processSynchron(page, webRequest, false);\n    if (webClient.getJavaScriptEngine().isScriptRunning())\n    {\n      _logger.warn(""HtmlUnit webClient.getJavaScriptEngine().shutdownJavaScriptExecutor()"");\n      webClient.getJavaScriptEngine().shutdownJavaScriptExecutor();\n    }\n\n    // Return the static snapshot.\n    final String staticSnapshotHtml = page.asXml();\n    httpResponse.setContentType(""text/html;charset=UTF-8"");\n    final PrintWriter out = httpResponse.getWriter();\n    out.println(""<hr />"");\n    out.println(""<center><h3>Page non-interactive pour le crawler."");\n    out.println(""La page interactive est: <a href=\\""""\n        + urlWithHashFragment\n        + ""\\"">""\n        + urlWithHashFragment + ""</a></h3></center>"");\n    out.println(""<hr />"");\n    out.println(staticSnapshotHtml);\n    // Close web client.\n    webClient.closeAllWindows();\n    out.println("""");\n    out.flush();\n    out.close();\n    if (_logger.isInfoEnabled())\n      _logger.info(""HtmlUnit completed webClient.getPage(webRequest) where webRequest = "" + webRequest.toString());\n  }\n  else\n  {\n    if (requestURI.contains("".nocache.""))\n    {\n      // Ensure the gwt nocache bootstrapping file is never cached.\n      // References:\n      //   http://stackoverflow.com/questions/4274053/how-to-clear-cache-in-gwt\n      //   http://seewah.blogspot.com/2009/02/gwt-tips-2-nocachejs-getting-cached-in.html\n      // \n      final Date now = new Date();\n      httpResponse.setDateHeader(""Date"", now.getTime());\n      httpResponse.setDateHeader(""Expires"", now.getTime() - 86400000L); // One day old.\n      httpResponse.setHeader(""Pragma"", ""no-cache"");\n      httpResponse.setHeader(""Cache-control"", ""no-cache, no-store, must-revalidate"");\n    }\n\n    filterChain.doFilter(request, response);\n  }\n}\n\n/**\n * Maps from the query string that contains _escaped_fragment_ to one that\n * doesn\'t, but is instead followed by a hash fragment. It also unescapes any\n * characters that were escaped by the crawler. If the query string does not\n * contain _escaped_fragment_, it is not modified.\n * \n * @param queryString\n * @return A modified query string followed by a hash fragment if applicable.\n *         The non-modified query string otherwise.\n * @throws UnsupportedEncodingException\n */\nprivate static String rewriteQueryString(String queryString)\n    throws UnsupportedEncodingException\n{\n  // Seek the escaped fragment.\n  int index = queryString.indexOf(ESCAPED_FRAGMENT_FORMAT2);\n  int length = ESCAPED_FRAGMENT_LENGTH2;\n  if (index == -1)\n  {\n    index = queryString.indexOf(ESCAPED_FRAGMENT_FORMAT1);\n    length = ESCAPED_FRAGMENT_LENGTH1;\n  }\n  if (index != -1)\n  {\n    // Found the escaped fragment, so build back the original decoded one.\n    final StringBuilder queryStringSb = new StringBuilder();\n    // Add url parameters if any.\n    if (index > 0)\n    {\n      queryStringSb.append(""?"");\n      queryStringSb.append(queryString.substring(0, index));\n    }\n    // Add the hash fragment as a replacement for the escaped fragment.\n    queryStringSb.append(""#!"");\n    // Add the decoded token.\n    final String token2Decode = queryString.substring(index + length, queryString.length());\n    final String tokenDecoded = URLDecoder.decode(token2Decode, ""UTF-8"");\n    queryStringSb.append(tokenDecoded);\n    return queryStringSb.toString();\n  }\n  return queryString;\n}\n\n']",https://stackoverflow.com/questions/13997424/htmlunit-only-displays-host-html-page-for-gwt-app,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
selenium implicitly wait doesn't work,"
This is the first time I use selenium and headless browser as I want to crawl some web page using ajax tech.
The effect is great, but for some case it takes too much time to load the whole page(especially when some resource is unavailable),so I have to set a time out for the selenium.
First of all I tried set_page_load_timeout() and set_script_timeout(),but when I set these timeouts, I won't get any page source if the page doesn't load completely, as the codes below:
driver = webdriver.Chrome(chrome_options=options)
driver.set_page_load_timeout(5)
driver.set_script_timeout(5)
try:
    driver.get(url)
except Exception:
    driver.execute_script('window.stop()')

print driver.page_source.encode('utf-8')  # raise TimeoutException this line.

so I try to using Implicitly Wait and Conditional Wait, like this:
driver = webdriver.Firefox(firefox_options=options, executable_path=path)
print(""Firefox Headless Browser Invoked"")
wait = WebDriverWait(driver, timeout=10)
driver.implicitly_wait(2)
start = time.time()
driver.get(url)
end = time.time()
print 'time used: %s s' % str(end - start)
try:
    WebDriverWait(driver, 2, 0.5).until(expected.presence_of_element_located((By.TAG_NAME, 'body')))
    print driver.find_element_by_tag_name('body').text
except Exception:
    driver.execute_script('window.stop()')

This time I got the content that I want.However,it takes a very long time(40+ seconds),that means the timeout I set for 2 seconds doesn't work at all.
In my view, it seems like the driver.get() call ends until the browser stop loading the page, only after that the codes below can work, and you can not kill the get() call or you'll get nothing.
But this is very different from the selenium docs, I REALLY wonder where is the mistake.
environment: OSX 10.12, selenium 3.0.9 with FireFox & GoogleChrome Headless(both latest version.)
--- update ----
Thanks for help.I change the code as below, using WebDriverWait() alone, but there still exist cases that the call last for a very long time, far more than the timeout that I set.
Wonder if I can stop the page load immediately as the time is out?
driver = webdriver.Firefox(firefox_options=options, executable_path=path)
print(""Firefox Headless Browser Invoked"")
start = time.time()
driver.get('url')
end = time.time()
print 'time used: %s s' % str(end - start)
try:
    WebDriverWait(driver, 2, 0.5).until(expected.presence_of_element_located((By.TAG_NAME, 'body')))
    print driver.find_element_by_tag_name('body').text
except Exception:
    driver.execute_script('window.stop()')
driver.quit()

Here is a terminal output in test:
Firefox Headless Browser Invoked
time used: 44.6049938202 s

according to the code this means the driver.get() call takes 44 seconds to finish call, which is unexpected,I wonder if I misunderstood the behavior of the headless browsers?
",3k,"
            2
        ","['\nAs you mentioned in your question it takes too much time to load the whole page(especially when some resource is unavailable) is pretty much possible if the Application Under Test (AUT) uses JavaScript or AJAX Calls.\n\nIn your first scenario you have induced both set_page_load_timeout(5) and set_script_timeout(5)\n\nset_page_load_timeout(time_to_wait) : Sets the amount of time to wait for a page load to complete before throwing an exception.\nset_script_timeout(time_to_wait) : Sets the amount of time that the script should wait during an execute_async_script call before throwing an exception.\n\n\nHence the Application Under Test being dependent on JavaScript or AJAX Calls in presence of both the conditions raises TimeoutException.\n\nIn your second scenario you have induced both implicitly_wait(2) and WebDriverWait(driver, 2, 0.5).\n\nimplicitly_wait(time_to_wait) : Sets the timeout to implicitly wait for an element to be found or a command to complete.\nWebDriverWait(driver, timeout, poll_frequency=0.5, ignored_exceptions=None) : Sets the timeout in-conjunction with different expected_conditions\nBut you are experiancing a very long timeout(40+ seconds) as it is clearly mentioned in the docs Do not mix implicit and explicit waits which can cause unpredictable wait times\n\n\n\nWARNING : Do not mix implicit and explicit waits. Doing so can cause unpredictable wait times. For example setting an implicit wait of 10 seconds and an explicit wait of 15 seconds, could cause a timeout to occur after 20 seconds.\n\nSolution :\nThe best solution would be to remove all the instance of implicitly_wait(time_to_wait) and replace with WebDriverWait() for a stable behavior of the Application Under Test (AUT).\n\nUpdate\nAs per your counter question, the current code block looks perfect. The measurement of time which you are seeing as time used: 44.6049938202 s is the time required for the Web Page to load completely and functionally that is the time required for the Client (i.e. the Web Browser) to return back the control to the WebDriver instance once \'document.readyState\' equals to ""complete"" is achieved. Selenium or as an user you have no control on this rendering process. However for a better performance you may follow the best practices as follows :\n\nKeep your JDK version updated currently Java SE Development Kit 8u162\nKeep your Selenium Client version updated currently selenium 3.9.0\nKeep your WebDriver version updated.\nKeep your Web Browser version updated.\nClean you Project Workspace within your IDE regularly to build your project with required dependencies only.\nUse CCleaner tool to wipe away the OS chores before and after your Test Suite execution.\nIf your Web Browser base version is too old uninstall the Web Browser through Revo Uninstaller and install a recent GA released version of the Web Browser.\nExecute your Test.\n\n']",https://stackoverflow.com/questions/48989984/selenium-implicitly-wait-doesnt-work,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can I scrape tooltips value from a Tableau graph embedded in a webpage,"
I am trying to figure out if there is a way and how to scrape tooltip values from a Tableau embedded graph in a webpage using python.
Here is an example of a graph with tooltips when user hovers over the bars:
https://public.tableau.com/views/NumberofCOVID-19patientsadmittedordischarged/DASHPublicpage_patientsdischarges?:embed=y&:showVizHome=no&:host_url=https%3A%2F%2Fpublic.tableau.com%2F&:embed_code_version=3&:tabs=no&:toolbar=yes&:animate_transition=yes&:display_static_image=no&:display_spinner=no&:display_overlay=yes&:display_count=yes&publish=yes&:loadOrderID=1
I grabbed this url from the original webpage that I want to scrape from:
https://covid19.colorado.gov/hospital-data
Any help is appreciated.
",2k,"
            2
        ","['\nEdit\nI\'ve made a python library to scrape tableau dashboard. The implementation is more straightforward :\nfrom tableauscraper import TableauScraper as TS\n\nurl = ""https://public.tableau.com/views/Colorado_COVID19_Data/CO_Home""\n\nts = TS()\nts.loads(url)\ndashboard = ts.getDashboard()\n\nfor t in dashboard.worksheets:\n    #show worksheet name\n    print(f""WORKSHEET NAME : {t.name}"")\n    #show dataframe for this worksheet\n    print(t.data)\n\nrun this on repl.it\n\nOld answer\nThe graphic seems to be generated in JS from the result of an API which looks like :\nPOST https://public.tableau.com/TITLE/bootstrapSession/sessions/SESSION_ID \n\nThe SESSION_ID parameter is located (among other things) in tsConfigContainer textarea in the URL used to build the iframe.\nStarting from https://covid19.colorado.gov/hospital-data :\n\ncheck element with class tableauPlaceholder\nget the param element with attribute name\nit gives you the url : https://public.tableau.com/views/{urlPath}\nthe previous link gives you a textarea with id tsConfigContainer with a bunch of json values\nextract the session_id and root path (vizql_root)\nmake a POST on https://public.tableau.com/ROOT_PATH/bootstrapSession/sessions/SESSION_ID with the sheetId as form data\nextract the json from the result (result is not json)\n\nCode :\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport re\n\nr = requests.get(""https://covid19.colorado.gov/hospital-data"")\nsoup = BeautifulSoup(r.text, ""html.parser"")\n\n# get the second tableau link\ntableauContainer = soup.findAll(""div"", { ""class"": ""tableauPlaceholder""})[1]\nurlPath = tableauContainer.find(""param"", { ""name"": ""name""})[""value""]\n\nr = requests.get(\n    f""https://public.tableau.com/views/{urlPath}"",\n    params= {\n        "":showVizHome"":""no"",\n    }\n)\nsoup = BeautifulSoup(r.text, ""html.parser"")\n\ntableauData = json.loads(soup.find(""textarea"",{""id"": ""tsConfigContainer""}).text)\n\ndataUrl = f\'https://public.tableau.com{tableauData[""vizql_root""]}/bootstrapSession/sessions/{tableauData[""sessionid""]}\'\n\nr = requests.post(dataUrl, data= {\n    ""sheet_id"": tableauData[""sheetId""],\n})\n\ndataReg = re.search(\'\\d+;({.*})\\d+;({.*})\', r.text, re.MULTILINE)\ninfo = json.loads(dataReg.group(1))\ndata = json.loads(dataReg.group(2))\n\nprint(data[""secondaryInfo""][""presModelMap""][""dataDictionary""][""presModelHolder""][""genDataDictionaryPresModel""][""dataSegments""][""0""][""dataColumns""])\n\nFrom there you have all the data. You will need to look for the way the data is splitted as it seems all the data is dumped through a single list. Probably looking at the other fields in the JSON object would be useful for that.\n']",https://stackoverflow.com/questions/61962611/how-can-i-scrape-tooltips-value-from-a-tableau-graph-embedded-in-a-webpage,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Finding the layers and layer sizes for each Docker image,"
For research purposes I'm trying to crawl the public Docker registry ( https://registry.hub.docker.com/ ) and find out 1) how many layers an average image has and 2) the sizes of these layers to get an idea of the distribution.
However I studied the API and public libraries as well as the details on the github but I cant find any method to:

retrieve all the public repositories/images (even if those are thousands I still need a starting list to iterate through)
find all the layers of an image
find the size for a layer (so not an image but for the individual layer).

Can anyone help me find a way to retrieve this information?
Thank you!
EDIT: is anyone able to verify that searching for '*' in Docker registry is returning all the repositories and not just anything that mentions '*' anywhere? https://registry.hub.docker.com/search?q=*
",187k,"
            205
        ","['\nCheck out dive written in golang. \n\nAwesome tool!\n', ""\nYou can first find the image ID using:\n$ docker images -a\n\nThen find the image's layers and their sizes:\n$ docker history --no-trunc <Image ID>\n\nNote: I'm using Docker version 1.13.1\n$ docker -v\nDocker version 1.13.1, build 092cba3\n\n"", '\nYou can find the layers of the images in the folder /var/lib/docker/aufs/layers; provide if you configured for storage-driver as aufs (default option) \nExample:\n docker ps -a\n CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n 0ca502fa6aae        ubuntu              ""/bin/bash""         44 minutes ago      Exited (0) 44 seconds ago                       DockerTest\n\nNow to view the layers of the containers that were created with the image ""Ubuntu""; go to /var/lib/docker/aufs/layers directory and cat the file starts with the container ID (here it is 0ca502fa6aae*)\n root@viswesn-vm2:/var/lib/docker/aufs/layers# cat    0ca502fa6aaefc89f690736609b54b2f0fdebfe8452902ca383020e3b0d266f9-init \n d2a0ecffe6fa4ef3de9646a75cc629bbd9da7eead7f767cb810f9808d6b3ecb6\n 29460ac934423a55802fcad24856827050697b4a9f33550bd93c82762fb6db8f\n b670fb0c7ecd3d2c401fbfd1fa4d7a872fbada0a4b8c2516d0be18911c6b25d6\n 83e4dde6b9cfddf46b75a07ec8d65ad87a748b98cf27de7d5b3298c1f3455ae4\n\nThis will show the result of same by running \nroot@viswesn-vm2:/var/lib/docker/aufs/layers# docker history ubuntu\nIMAGE               CREATED             CREATED BY                                         SIZE                COMMENT\nd2a0ecffe6fa        13 days ago         /bin/sh -c #(nop) CMD [""/bin/bash""]             0 B                 \n29460ac93442        13 days ago         /bin/sh -c sed -i \'s/^#\\s*\\   (deb.*universe\\)$/   1.895 kB            \nb670fb0c7ecd        13 days ago         /bin/sh -c echo \'#!/bin/sh\' > /usr/sbin/polic   194.5 kB            \n83e4dde6b9cf        13 days ago         /bin/sh -c #(nop) ADD file:c8f078961a543cdefa   188.2 MB \n\nTo view the full layer ID; run with --no-trunc option as part of history command.\ndocker history --no-trunc ubuntu\n\n', '\nIn my opinion, docker history <image> is sufficient. This returns the size of each layer:\n$ docker history jenkinsci-jnlp-slave:2019-1-9c\nIMAGE        CREATED    CREATED BY                                    SIZE  COMMENT\n93f48953d298 42 min ago /bin/sh -c #(nop)  USER jenkins               0B\n6305b07d4650 42 min ago /bin/sh -c chown jenkins:jenkins -R /home/je鈥?1.45GB\n\n', '\nThey have a very good answer here:\nhttps://stackoverflow.com/a/32455275/165865\nJust run below images:\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock nate/dockviz images -t\n\n', ""\nThis will inspect the docker image and print the layers:\n$ docker image inspect nginx -f '{{.RootFS.Layers}}'\n[sha256:d626a8ad97a1f9c1f2c4db3814751ada64f60aed927764a3f994fcd88363b659 sha256:82b81d779f8352b20e52295afc6d0eab7e61c0ec7af96d85b8cda7800285d97d sha256:7ab428981537aa7d0c79bc1acbf208c71e57d9678f7deca4267cc03fba26b9c8]\n\n"", '\none more tool : https://github.com/CenturyLinkLabs/dockerfile-from-image\nGUI using ImageLayers.io \n', ""\n\nhttps://hub.docker.com/search?q=* shows all the images in the entire Docker hub, it's not possible to get this via the search command as it doesnt accept wildcards.\nAs of v1.10 you can find all the layers in an image by pulling it and using these commands:\ndocker pull ubuntu\nID=$(sudo docker inspect -f {{.Id}} ubuntu)\njq .rootfs.diff_ids /var/lib/docker/image/aufs/imagedb/content/$(echo $ID|tr ':' '/')\n\n\n3) The size can be found in /var/lib/docker/image/aufs/layerdb/sha256/{LAYERID}/size although LAYERID != the diff_ids found with the previous command. For this you need to look at /var/lib/docker/image/aufs/layerdb/sha256/{LAYERID}/diff and compare with the previous command output to properly match the correct diff_id and size.\n"", ""\nIt's indeed doable to query the manifest or blob info from docker registry server without pulling the image to local disk.\nYou can refer to the Registry v2 API to fetch the manifest of image.\nGET /v2/<name>/manifests/<reference>\n\nNote, you have to handle different manifest version. For v2 you can directly get the size of layer and digest of blob. For v1 manifest, you can HEAD the blob download url to get the actual layer size.\nThere is a simple script for handling above cases that will be continuously maintained. \n"", '\nTo find all the layers of an image and to find the size for a layer, you can display the manifest from the docker hub registry via the ""manifest"" experimental feature:\ndocker manifest inspect ubuntu\n\nThe result is a JSON file (only the first lines are shown here):\n{\n   ""schemaVersion"": 2,\n   ""mediaType"": ""application/vnd.docker.distribution.manifest.list.v2+json"",\n   ""manifests"": [\n      {\n         ""mediaType"": ""application/vnd.docker.distribution.manifest.v2+json"",\n         ""size"": 529,\n         ""digest"": ""sha256:10cbddb6cf8568f56584ccb6c866203e68ab8e621bb87038e254f6f27f955bbe"",\n         ""platform"": {\n            ""architecture"": ""amd64"",\n            ""os"": ""linux""\n         }\n      },\n      {\n         ""mediaType"": ""application/vnd.docker.distribution.manifest.v2+json"",\n         ""size"": 529,\n         ""digest"": ""sha256:dd375524d7eda25a69f9f9790cd3e28855be7908e04162360dd462794035ebf7"",\n         ""platform"": {\n            ""architecture"": ""arm"",\n            ""os"": ""linux"",\n            ""variant"": ""v7""\n\n', '\nNot exactly the original question but to find the sum total of all the images without double-counting shared layers, the following is useful (ubuntu 18):\nsudo du -h -d1  /var/lib/docker/overlay2 | sort -h\n\n', ""\nI've solved this problem by using the search function on Docker's website where '*' is a valid search that returns 200k repositories and then I crawled each invididual page. HTML parsing allows me to extract all the image names on each page.\n""]",https://stackoverflow.com/questions/29696656/finding-the-layers-and-layer-sizes-for-each-docker-image,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to write a crawler?,"
I have had thoughts of trying to write a simple crawler that might crawl and produce a list of its findings for our NPO's websites and content.
Does anybody have any thoughts on how to do this? Where do you point the crawler to get started? How does it send back its findings and still keep crawling? How does it know what it finds, etc,etc.
",60k,"
            64
        ","[""\nYou'll be reinventing the wheel, to be sure. But here's the basics:\n\nA list of unvisited URLs - seed this with one or more starting pages\nA list of visited URLs - so you don't go around in circles\nA set of rules for URLs you're not interested in - so you don't index the whole Internet\n\nPut these in persistent storage, so you can stop and start the crawler without losing state.\nAlgorithm is:\nwhile(list of unvisited URLs is not empty) {\n    take URL from list\n    remove it from the unvisited list and add it to the visited list\n    fetch content\n    record whatever it is you want to about the content\n    if content is HTML {\n        parse out URLs from links\n        foreach URL {\n           if it matches your rules\n              and it's not already in either the visited or unvisited list\n              add it to the unvisited list\n        }\n    }\n}\n\n"", '\nThe complicated part of a crawler is if you want to scale it to a huge number of websites/requests.\nIn this situation you will have to deal with some issues like:\n\nImpossibility to keep info all in one database.\nNot enough RAM to deal with huge index(s)\nMultithread performance and concurrency\nCrawler traps (infinite loop created by changing urls, calendars, sessions ids...) and duplicated content.\nCrawl from more than one computer\nMalformed HTML codes\nConstant http errors from servers\nDatabases without compression, wich make your need for space about 8x bigger.\nRecrawl routines and priorities.\nUse requests with compression (Deflate/gzip) (good for any kind of crawler).\n\nAnd some important things\n\nRespect robots.txt\nAnd a crawler delay on each request to dont suffocate web servers.\n\n', ""\nMultithreaded Web Crawler\nIf you want to crawl large sized website then you should write a multi-threaded crawler.\nconnecting,fetching and writing crawled information in files/database - these are the three steps of crawling but if you use a single threaded than your CPU and network utilization will be pour.\nA multi threaded web crawler needs two data structures- linksVisited(this should be implemented as a hashmap or trai) and linksToBeVisited(this is a queue). \nWeb crawler uses BFS to traverse world wide web.\nAlgorithm of a basic web crawler:-\n\nAdd one or more seed urls to linksToBeVisited. The method to add a url to linksToBeVisited must be synchronized.\nPop an element from linksToBeVisited and add this to linksVisited. This pop method to pop url from linksToBeVisited must be synchronized.\nFetch the page from internet.\nParse the file and add any till now not visited link found in the page to linksToBeVisited. URL's can be filtered if needed. The user can give a set of rules to filter which url's to be scanned.\nThe necessary information found on the page is saved in database or file.\nrepeat step 2 to 5 until queue is linksToBeVisited empty.\nHere is a code snippet on how to synchronize the threads....\n public void add(String site) {\n   synchronized (this) {\n   if (!linksVisited.contains(site)) {\n     linksToBeVisited.add(site);\n     }\n   }\n }\n\n public String next() {\n    if (linksToBeVisited.size() == 0) {\n    return null;\n    }\n       synchronized (this) {\n        // Need to check again if size has changed\n       if (linksToBeVisited.size() > 0) {\n          String s = linksToBeVisited.get(0);\n          linksToBeVisited.remove(0);\n          linksVisited.add(s);\n          return s;\n       }\n     return null;\n     }\n  }\n\n\n\n"", ""\nCrawlers are simple in concept.\nYou get a root page via a HTTP GET, parse it to find URLs and put them on a queue unless they've been parsed already (so you need a global record of pages you have already parsed).\nYou can use the Content-type header to find out what the type of content is, and limit your crawler to only parsing the HTML types.\nYou can strip out the HTML tags to get the plain text, which you can do text analysis on (to get tags, etc, the meat of the page). You could even do that on the alt/title tags for images if you got that advanced.\nAnd in the background you can have a pool of threads eating URLs from the Queue and doing the same. You want to limit the number of threads of course.\n"", ""\nIf your NPO's sites are relatively big or complex (having dynamic pages that'll effectively create a 'black hole' like a calendar with a 'next day' link) you'd be better using a real web crawler, like Heritrix.\nIf the sites total a few number of pages you can get away with just using curl or wget or your own. Just remember if they start to get big or you start making your script more complex to just use a real crawler or at least look at its source to see what are they doing and why.\nSome issues (there are more):\n\nBlack holes (as described)\nRetries (what if you get a 500?)\nRedirects\nFlow control (else you can be a burden on the sites)\nrobots.txt implementation\n\n"", '\nWikipedia has a good article about web crawlers, covering many of the algorithms and considerations.\nHowever, I wouldn\'t bother writing my own crawler.  It\'s a lot of work, and since you only need a ""simple crawler"", I\'m thinking all you really need is an off-the-shelf crawler.  There are a lot of free and open-source crawlers that will likely do everything you need, with very little work on your part.\n', '\nYou could make a list of words and make a thread for each word searched at google. Then each thread will create a new thread for each link it find in the page. Each thread should write what it finds in a database. When each thread finishes reading the page, it terminates.And there you have a very big database of links in your database.\n', '\nUse wget, do a recursive web suck, which will dump all the files onto your harddrive, then write another script to go through all the downloaded files and analyze them.\nEdit: or maybe curl instead of wget, but I am not familiar with curl, I do not know if it does recursive downloads like wget.\n', ""\nI'm using Open search server for my company internal search, try this : http://open-search-server.com its also open soruce.\n"", '\ni did a simple web crawler using reactive extension in .net.\nhttps://github.com/Misterhex/WebCrawler\npublic class Crawler\n    {\n    class ReceivingCrawledUri : ObservableBase<Uri>\n    {\n        public int _numberOfLinksLeft = 0;\n\n        private ReplaySubject<Uri> _subject = new ReplaySubject<Uri>();\n        private Uri _rootUri;\n        private IEnumerable<IUriFilter> _filters;\n\n        public ReceivingCrawledUri(Uri uri)\n            : this(uri, Enumerable.Empty<IUriFilter>().ToArray())\n        { }\n\n        public ReceivingCrawledUri(Uri uri, params IUriFilter[] filters)\n        {\n            _filters = filters;\n\n            CrawlAsync(uri).Start();\n        }\n\n        protected override IDisposable SubscribeCore(IObserver<Uri> observer)\n        {\n            return _subject.Subscribe(observer);\n        }\n\n        private async Task CrawlAsync(Uri uri)\n        {\n            using (HttpClient client = new HttpClient() { Timeout = TimeSpan.FromMinutes(1) })\n            {\n                IEnumerable<Uri> result = new List<Uri>();\n\n                try\n                {\n                    string html = await client.GetStringAsync(uri);\n                    result = CQ.Create(html)[""a""].Select(i => i.Attributes[""href""]).SafeSelect(i => new Uri(i));\n                    result = Filter(result, _filters.ToArray());\n\n                    result.ToList().ForEach(async i =>\n                    {\n                        Interlocked.Increment(ref _numberOfLinksLeft);\n                        _subject.OnNext(i);\n                        await CrawlAsync(i);\n                    });\n                }\n                catch\n                { }\n\n                if (Interlocked.Decrement(ref _numberOfLinksLeft) == 0)\n                    _subject.OnCompleted();\n            }\n        }\n\n        private static List<Uri> Filter(IEnumerable<Uri> uris, params IUriFilter[] filters)\n        {\n            var filtered = uris.ToList();\n            foreach (var filter in filters.ToList())\n            {\n                filtered = filter.Filter(filtered);\n            }\n            return filtered;\n        }\n    }\n\n    public IObservable<Uri> Crawl(Uri uri)\n    {\n        return new ReceivingCrawledUri(uri, new ExcludeRootUriFilter(uri), new ExternalUriFilter(uri), new AlreadyVisitedUriFilter());\n    }\n\n    public IObservable<Uri> Crawl(Uri uri, params IUriFilter[] filters)\n    {\n        return new ReceivingCrawledUri(uri, filters);\n    }\n}\n\nand you can use it as follows:\nCrawler crawler = new Crawler();\nIObservable observable = crawler.Crawl(new Uri(""http://www.codinghorror.com/""));\nobservable.Subscribe(onNext: Console.WriteLine, \nonCompleted: () => Console.WriteLine(""Crawling completed""));\n\n']",https://stackoverflow.com/questions/102631/how-to-write-a-crawler,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python: Disable images in Selenium Google ChromeDriver,"
I spend a lot of time searching about this.
At the end of the day I combined a number of answers and it works. I share my answer and I'll appreciate it if anyone edits it or provides us with an easier way to do this.
1- The answer in Disable images in Selenium Google ChromeDriver works in Java. So we should do the same thing in Python:
opt = webdriver.ChromeOptions()
opt.add_extension(""Block-image_v1.1.crx"")
browser = webdriver.Chrome(chrome_options=opt)

2- But downloading ""Block-image_v1.1.crx"" is a little bit tricky, because there is no direct way to do that. For this purpose, instead of going to: https://chrome.google.com/webstore/detail/block-image/pehaalcefcjfccdpbckoablngfkfgfgj
you can go to http://chrome-extension-downloader.com/
and paste the extension url there to be able to download the extension file.
3- Then you will be able to use the above mentioned code with the path to the extension file that you have downloaded.
",65k,"
            58
        ","['\nHere is another way to disable images:\nfrom selenium import webdriver\n\nchrome_options = webdriver.ChromeOptions()\nprefs = {""profile.managed_default_content_settings.images"": 2}\nchrome_options.add_experimental_option(""prefs"", prefs)\ndriver = webdriver.Chrome(chrome_options=chrome_options)\n\nI found it below:\nhttp://nullege.com/codes/show/src@o@s@osintstalker-HEAD@fbstalker1.py/56/selenium.webdriver.ChromeOptions.add_experimental_option\n', '\nJava:\nWith this Chrome nor Firefox would load images. The syntax is different but the strings on the parameters are the same.\n    chromeOptions = new ChromeOptions();\n    HashMap<String, Object> images = new HashMap<String, Object>();\n    images.put(""images"", 2);\n    HashMap<String, Object> prefs = new HashMap<String, Object>();\n    prefs.put(""profile.default_content_setting_values"", images);\n    chromeOptions.setExperimentalOption(""prefs"", prefs);\n    driver=new ChromeDriver(chromeOptions);\n\n    firefoxOpt = new FirefoxOptions();\n    FirefoxProfile profile = new FirefoxProfile();\n    profile.setPreference(""permissions.default.image"", 2);\n    firefoxOpt.setProfile(profile);\n\n', '\nThere is another way that comes probably to mind to everyone to access chrome://settings and then go through the settings with selenium I started this way just for didactic curiosity, but then I hit a forest of shadow-roots elements now when you encounter more than 3 shadow root element combined with dynamic content is clearly a way to obfuscate and make it impossible to automate, although might sound at least theoretically possible this approach looks more like a dead end, I will leave this answer with the example code, just for purely learning purposes to advert the people tempted to go to the challenge..  Not only was hard to find just the content settings due to the shadowroots and dynamic change when you find the button is not clickable at this point.  \ndriver = webdriver.Chrome()\n\n\ndef expand_shadow_element(element):\n  shadow_root = driver.execute_script(\'return arguments[0].shadowRoot\', element)\n  return shadow_root\n\ndriver.get(""chrome://settings"")\nroot1 = driver.find_element_by_tag_name(\'settings-ui\')\nshadow_root1 = expand_shadow_element(root1)\n\nroot2 = shadow_root1.find_element_by_css_selector(\'[page-name=""Settings""]\')\nshadow_root2 = expand_shadow_element(root2)\n\nroot3 = shadow_root2.find_element_by_id(\'search\')\nshadow_root3 = expand_shadow_element(root3)\n\nsearch_button = shadow_root3.find_element_by_id(""searchTerm"")\nsearch_button.click()\n\ntext_area = shadow_root3.find_element_by_id(\'searchInput\')\ntext_area.send_keys(""content settings"")\n\nroot0 = shadow_root1.find_element_by_id(\'main\')\nshadow_root0_s = expand_shadow_element(root0)\n\n\nroot1_p = shadow_root0_s.find_element_by_css_selector(\'settings-basic-page\')\nshadow_root1_p = expand_shadow_element(root1_p)\n\n\nroot1_s = shadow_root1_p.find_element_by_css_selector(\'settings-privacy-page\')\nshadow_root1_s = expand_shadow_element(root1_s)\n\ncontent_settings_div = shadow_root1_s.find_element_by_css_selector(\'#site-settings-subpage-trigger\')\ncontent_settings = content_settings_div.find_element_by_css_selector(""button"")\ncontent_settings.click()\n\n']",https://stackoverflow.com/questions/28070315/python-disable-images-in-selenium-google-chromedriver,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I need a Powerful Web Scraper library [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



I need a powerful web scraper library  for mining contents from web. That can be paid or free both will be fine for me. Please suggest me a library or better way for mining the data  and store in my preferred database. I have searched but i didn't find any good solution for this. I need a good suggestion from experts. Please help me out.
",67k,"
            29
        ","[""\nScraping is easy really, you just have to parse the content you are downloading and get all the associated links.\nThe most important piece though is the part that processes the HTML.  Because most browsers don't require the cleanest (or standards-compliant) HTML in order to be rendered, you need an HTML parser that is going to be able to make sense of HTML that is not always well-formed.\nI recommend you use the HTML Agility Pack for this purpose.  It does very well at handling non-well-formed HTML, and provides an easy interface for you to use XPath queries to get nodes in the resulting document.\nBeyond that, you just need to pick a data store to hold your processed data (you can use any database technology for that) and a way to download content from the web, which .NET provides two high-level mechanisms for, the WebClient and HttpWebRequest/HttpWebResponse classes.\n"", '\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\n\nnamespace SoftCircuits.Parsing\n{\n    public class HtmlTag\n    {\n        /// <summary>\n        /// Name of this tag\n        /// </summary>\n        public string Name { get; set; }\n\n        /// <summary>\n        /// Collection of attribute names and values for this tag\n        /// </summary>\n        public Dictionary<string, string> Attributes { get; set; }\n\n        /// <summary>\n        /// True if this tag contained a trailing forward slash\n        /// </summary>\n        public bool TrailingSlash { get; set; }\n\n        /// <summary>\n        /// Indicates if this tag contains the specified attribute. Note that\n        /// true is returned when this tag contains the attribute even when the\n        /// attribute has no value\n        /// </summary>\n        /// <param name=""name"">Name of attribute to check</param>\n        /// <returns>True if tag contains attribute or false otherwise</returns>\n        public bool HasAttribute(string name)\n        {\n            return Attributes.ContainsKey(name);\n        }\n    };\n\n    public class HtmlParser : TextParser\n    {\n        public HtmlParser()\n        {\n        }\n\n        public HtmlParser(string html) : base(html)\n        {\n        }\n\n        /// <summary>\n        /// Parses the next tag that matches the specified tag name\n        /// </summary>\n        /// <param name=""name"">Name of the tags to parse (""*"" = parse all tags)</param>\n        /// <param name=""tag"">Returns information on the next occurrence of the specified tag or null if none found</param>\n        /// <returns>True if a tag was parsed or false if the end of the document was reached</returns>\n        public bool ParseNext(string name, out HtmlTag tag)\n        {\n            // Must always set out parameter\n            tag = null;\n\n            // Nothing to do if no tag specified\n            if (String.IsNullOrEmpty(name))\n                return false;\n\n            // Loop until match is found or no more tags\n            MoveTo(\'<\');\n            while (!EndOfText)\n            {\n                // Skip over opening \'<\'\n                MoveAhead();\n\n                // Examine first tag character\n                char c = Peek();\n                if (c == \'!\' && Peek(1) == \'-\' && Peek(2) == \'-\')\n                {\n                    // Skip over comments\n                    const string endComment = ""-->"";\n                    MoveTo(endComment);\n                    MoveAhead(endComment.Length);\n                }\n                else if (c == \'/\')\n                {\n                    // Skip over closing tags\n                    MoveTo(\'>\');\n                    MoveAhead();\n                }\n                else\n                {\n                    bool result, inScript;\n\n                    // Parse tag\n                    result = ParseTag(name, ref tag, out inScript);\n                    // Because scripts may contain tag characters, we have special\n                    // handling to skip over script contents\n                    if (inScript)\n                        MovePastScript();\n                    // Return true if requested tag was found\n                    if (result)\n                        return true;\n                }\n                // Find next tag\n                MoveTo(\'<\');\n            }\n            // No more matching tags found\n            return false;\n        }\n\n        /// <summary>\n        /// Parses the contents of an HTML tag. The current position should be at the first\n        /// character following the tag\'s opening less-than character.\n        /// \n        /// Note: We parse to the end of the tag even if this tag was not requested by the\n        /// caller. This ensures subsequent parsing takes place after this tag\n        /// </summary>\n        /// <param name=""reqName"">Name of the tag the caller is requesting, or ""*"" if caller\n        /// is requesting all tags</param>\n        /// <param name=""tag"">Returns information on this tag if it\'s one the caller is\n        /// requesting</param>\n        /// <param name=""inScript"">Returns true if tag began, and did not end, and script\n        /// block</param>\n        /// <returns>True if data is being returned for a tag requested by the caller\n        /// or false otherwise</returns>\n        protected bool ParseTag(string reqName, ref HtmlTag tag, out bool inScript)\n        {\n            bool doctype, requested;\n            doctype = inScript = requested = false;\n\n            // Get name of this tag\n            string name = ParseTagName();\n\n            // Special handling\n            if (String.Compare(name, ""!DOCTYPE"", true) == 0)\n                doctype = true;\n            else if (String.Compare(name, ""script"", true) == 0)\n                inScript = true;\n\n            // Is this a tag requested by caller?\n            if (reqName == ""*"" || String.Compare(name, reqName, true) == 0)\n            {\n                // Yes\n                requested = true;\n                // Create new tag object\n                tag = new HtmlTag();\n                tag.Name = name;\n                tag.Attributes = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n            }\n\n            // Parse attributes\n            MovePastWhitespace();\n            while (Peek() != \'>\' && Peek() != NullChar)\n            {\n                if (Peek() == \'/\')\n                {\n                    // Handle trailing forward slash\n                    if (requested)\n                        tag.TrailingSlash = true;\n                    MoveAhead();\n                    MovePastWhitespace();\n                    // If this is a script tag, it was closed\n                    inScript = false;\n                }\n                else\n                {\n                    // Parse attribute name\n                    name = (!doctype) ? ParseAttributeName() : ParseAttributeValue();\n                    MovePastWhitespace();\n                    // Parse attribute value\n                    string value = String.Empty;\n                    if (Peek() == \'=\')\n                    {\n                        MoveAhead();\n                        MovePastWhitespace();\n                        value = ParseAttributeValue();\n                        MovePastWhitespace();\n                    }\n                    // Add attribute to collection if requested tag\n                    if (requested)\n                    {\n                        // This tag replaces existing tags with same name\n                        if (tag.Attributes.ContainsKey(name))\n                            tag.Attributes.Remove(name);\n                        tag.Attributes.Add(name, value);\n                    }\n                }\n            }\n            // Skip over closing \'>\'\n            MoveAhead();\n\n            return requested;\n        }\n\n        /// <summary>\n        /// Parses a tag name. The current position should be the first character of the name\n        /// </summary>\n        /// <returns>Returns the parsed name string</returns>\n        protected string ParseTagName()\n        {\n            int start = Position;\n            while (!EndOfText && !Char.IsWhiteSpace(Peek()) && Peek() != \'>\')\n                MoveAhead();\n            return Substring(start, Position);\n        }\n\n        /// <summary>\n        /// Parses an attribute name. The current position should be the first character\n        /// of the name\n        /// </summary>\n        /// <returns>Returns the parsed name string</returns>\n        protected string ParseAttributeName()\n        {\n            int start = Position;\n            while (!EndOfText && !Char.IsWhiteSpace(Peek()) && Peek() != \'>\' && Peek() != \'=\')\n                MoveAhead();\n            return Substring(start, Position);\n        }\n\n        /// <summary>\n        /// Parses an attribute value. The current position should be the first non-whitespace\n        /// character following the equal sign.\n        /// \n        /// Note: We terminate the name or value if we encounter a new line. This seems to\n        /// be the best way of handling errors such as values missing closing quotes, etc.\n        /// </summary>\n        /// <returns>Returns the parsed value string</returns>\n        protected string ParseAttributeValue()\n        {\n            int start, end;\n            char c = Peek();\n            if (c == \'""\' || c == \'\\\'\')\n            {\n                // Move past opening quote\n                MoveAhead();\n                // Parse quoted value\n                start = Position;\n                MoveTo(new char[] { c, \'\\r\', \'\\n\' });\n                end = Position;\n                // Move past closing quote\n                if (Peek() == c)\n                    MoveAhead();\n            }\n            else\n            {\n                // Parse unquoted value\n                start = Position;\n                while (!EndOfText && !Char.IsWhiteSpace(c) && c != \'>\')\n                {\n                    MoveAhead();\n                    c = Peek();\n                }\n                end = Position;\n            }\n            return Substring(start, end);\n        }\n\n        /// <summary>\n        /// Locates the end of the current script and moves past the closing tag\n        /// </summary>\n        protected void MovePastScript()\n        {\n            const string endScript = ""</script"";\n\n            while (!EndOfText)\n            {\n                MoveTo(endScript, true);\n                MoveAhead(endScript.Length);\n                if (Peek() == \'>\' || Char.IsWhiteSpace(Peek()))\n                {\n                    MoveTo(\'>\');\n                    MoveAhead();\n                    break;\n                }\n            }\n        }\n    }\n}\n\n', '\nFor simple websites ( = plain html only), Mechanize works really well and fast. For sites that use Javascript, AJAX or even Flash, you need a real browser solution such as iMacros.\n', ""\nMy Advice:\nYou could look around for a HTML Parser and then use it to parse out information from sites. (Like here). Then all you would need to do is save that data into your database however you see fit.\nI've made my own scraper a few times, it's pretty easy and allow you to customize the data that is saved. \nData Mining Tools\nIf you really just want to get a tool to do this then you should have no problem finding some. \n""]",https://stackoverflow.com/questions/4377355/i-need-a-powerful-web-scraper-library,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scrapy- how to stop Redirect (302),"
I'm trying to crawl a url using Scrapy. But it redirects me to page that doesn't exist. 
Redirecting (302) to <GET http://www.shop.inonit.in/mobile/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/1275197> from <GET http://www.shop.inonit.in/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/pid-1275197.aspx>

The problem is http://www.shop.inonit.in/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/pid-1275197.aspx exists, but http://www.shop.inonit.in/mobile/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/1275197 doesn't, so the crawler cant find this. I've crawled many other websites as well but didn't have this problem anywhere else. Is there a way I can stop this redirect?
Any help would be much appreciated. Thanks.
Update: This is my spider class
class Inon_Spider(BaseSpider):
name = 'Inon'
allowed_domains = ['www.shop.inonit.in']

start_urls = ['http://www.shop.inonit.in/Products/Inonit-Gadget-Accessories-Mobile-Covers/-The-Red-Tag/Samsung-Note-2-Dead-Mau/pid-2656465.aspx']

def parse(self, response):

    item = DealspiderItem()
    hxs = HtmlXPathSelector(response)

    title = hxs.select('//div[@class=""aboutproduct""]/div[@class=""container9""]/div[@class=""ctl_aboutbrand""]/h1/text()').extract()
    price = hxs.select('//span[@id=""ctl00_ContentPlaceHolder1_Price_ctl00_spnWebPrice""]/span[@class=""offer""]/span[@id=""ctl00_ContentPlaceHolder1_Price_ctl00_lblOfferPrice""]/text()').extract()
    prc = price[0].replace(""Rs.  "","""")
    description = []

    item['price'] = prc
    item['title'] = title
    item['description'] = description
    item['url'] = response.url

    return item

",29k,"
            25
        ","['\nyes you can do this simply by adding meta values like\nmeta={\'dont_redirect\': True}\n\nalso you can stop redirected for a particular response code like\nmeta={\'dont_redirect\': True,""handle_httpstatus_list"": [302]}\n\nit will stop redirecting only 302 response codes. you can add as many http status code you want to avoid redirecting them.\nexample\nyield Request(\'some url\',\n    meta = {\n        \'dont_redirect\': True,\n        \'handle_httpstatus_list\': [302]\n    },\n    callback= self.some_call_back)\n\n', ""\nAfter looking at the documentation and looking through the relevant source, I was able to figure it out. If you look in the source for start_requests, you'll see that it calls make_requests_from_url for all URLs.\nInstead of modifying start_requests, I modified make_requests_from_url\ndef make_requests_from_url(self, url):\n    return Request(url, dont_filter=True, meta = {\n        'dont_redirect': True,\n        'handle_httpstatus_list': [301, 302]\n    })\n\nAnd added this as part of my spider, right above parse().\n"", '\nBy default, Scrapy use RedirectMiddleware to handle redirection. You can set REDIRECT_ENABLED to False to disable redirection.\nSee documentation. \n', ""\nAs explained here: Scrapy docs\nUse Request Meta\nrequest =  scrapy.Request(link.url, callback=self.parse2)\nrequest.meta['dont_redirect'] = True\nyield request\n\n""]",https://stackoverflow.com/questions/15476587/scrapy-how-to-stop-redirect-302,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Writing items to a MySQL database in Scrapy,"
I am new to Scrapy, I had the spider code
class Example_spider(BaseSpider):
   name = ""example""
   allowed_domains = [""www.example.com""]

   def start_requests(self):
       yield self.make_requests_from_url(""http://www.example.com/bookstore/new"")

   def parse(self, response):
       hxs = HtmlXPathSelector(response)
       urls = hxs.select('//div[@class=""bookListingBookTitle""]/a/@href').extract()
       for i in urls:
           yield Request(urljoin(""http://www.example.com/"", i[1:]), callback=self.parse_url)

   def parse_url(self, response):
           hxs = HtmlXPathSelector(response)
           main =   hxs.select('//div[@id=""bookshelf-bg""]')
           items = []
           for i in main:
           item = Exampleitem()
           item['book_name'] = i.select('div[@class=""slickwrap full""]/div[@id=""bookstore_detail""]/div[@class=""book_listing clearfix""]/div[@class=""bookstore_right""]/div[@class=""title_and_byline""]/p[@class=""book_title""]/text()')[0].extract()
           item['price'] = i.select('div[@id=""book-sidebar-modules""]/div[@class=""add_to_cart_wrapper slickshadow""]/div[@class=""panes""]/div[@class=""pane clearfix""]/div[@class=""inner""]/div[@class=""add_to_cart 0""]/form/div[@class=""line-item""]/div[@class=""line-item-price""]/text()').extract()
           items.append(item)
       return items

And pipeline code is:
class examplePipeline(object):

    def __init__(self):               
        self.dbpool = adbapi.ConnectionPool('MySQLdb',
                db='blurb',
                user='root',
                passwd='redhat',
                cursorclass=MySQLdb.cursors.DictCursor,
                charset='utf8',
                use_unicode=True
            )
def process_item(self, spider, item):
    # run db query in thread pool
    assert isinstance(item, Exampleitem)
    query = self.dbpool.runInteraction(self._conditional_insert, item)
    query.addErrback(self.handle_error)
    return item
def _conditional_insert(self, tx, item):
    print ""db connected-=========>""
    # create record if doesn't exist. 
    tx.execute(""select * from example_book_store where book_name = %s"", (item['book_name']) )
    result = tx.fetchone()
    if result:
        log.msg(""Item already stored in db: %s"" % item, level=log.DEBUG)
    else:
        tx.execute(""""""INSERT INTO example_book_store (book_name,price)
                    VALUES (%s,%s)"""""",   
                            (item['book_name'],item['price'])
                    )
        log.msg(""Item stored in db: %s"" % item, level=log.DEBUG)            

def handle_error(self, e):
    log.err(e)          

After running this I am getting the following error 
exceptions.NameError: global name 'Exampleitem' is not defined

I got the above error when I added the below code in process_item method
assert isinstance(item, Exampleitem)

and without adding this line I am getting 
**exceptions.TypeError: 'Example_spider' object is not subscriptable

Can anyone make this code run and make sure that all the items saved into database?
",40k,"
            21
        ","['\nTry the following code in your pipeline\nimport sys\nimport MySQLdb\nimport hashlib\nfrom scrapy.exceptions import DropItem\nfrom scrapy.http import Request\n\nclass MySQLStorePipeline(object):\n    def __init__(self):\n        self.conn = MySQLdb.connect(\'host\', \'user\', \'passwd\', \n                                    \'dbname\', charset=""utf8"",\n                                    use_unicode=True)\n        self.cursor = self.conn.cursor()\n\n    def process_item(self, item, spider):    \n        try:\n            self.cursor.execute(""""""INSERT INTO example_book_store (book_name, price)  \n                        VALUES (%s, %s)"""""", \n                       (item[\'book_name\'].encode(\'utf-8\'), \n                        item[\'price\'].encode(\'utf-8\')))            \n            self.conn.commit()            \n        except MySQLdb.Error, e:\n            print ""Error %d: %s"" % (e.args[0], e.args[1])\n        return item\n\n', ""\nYour process_item method should be declared as: def process_item(self, item, spider): instead of def process_item(self, spider, item): -> you switched the arguments around.\nThis exception: exceptions.NameError: global name 'Exampleitem' is not defined indicates you didn't import the Exampleitem in your pipeline.\nTry adding: from myspiders.myitems import Exampleitem (with correct names/paths ofcourse).\n"", '\nI think this way is better and more concise:\n#Item\nclass pictureItem(scrapy.Item):\n    topic_id=scrapy.Field()\n    url=scrapy.Field()\n\n#SQL\nself.save_picture=""insert into picture(`url`,`id`) values(%(url)s,%(id)s);""\n\n#usage\ncur.execute(self.save_picture,dict(item))\n\nIt\'s just like\ncur.execute(""insert into picture(`url`,`id`) values(%(url)s,%(id)s)"" % {""url"":someurl,""id"":1})\n\nCause (you can read more about Items in Scrapy)\n\nThe Field class is just an alias to the built-in dict class and doesn鈥檛 provide any extra functionality or attributes. In other words, Field objects are plain-old Python dicts. \n\n']",https://stackoverflow.com/questions/10845839/writing-items-to-a-mysql-database-in-scrapy,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Locally run all of the spiders in Scrapy,"
Is there a way to run all of the spiders in a Scrapy project without using the Scrapy daemon? There used to be a way to run multiple spiders with scrapy crawl, but that syntax was removed and Scrapy's code changed quite a bit.
I tried creating my own command:
from scrapy.command import ScrapyCommand
from scrapy.utils.misc import load_object
from scrapy.conf import settings

class Command(ScrapyCommand):
    requires_project = True

    def syntax(self):
        return '[options]'

    def short_desc(self):
        return 'Runs all of the spiders'

    def run(self, args, opts):
        spman_cls = load_object(settings['SPIDER_MANAGER_CLASS'])
        spiders = spman_cls.from_settings(settings)

        for spider_name in spiders.list():
            spider = self.crawler.spiders.create(spider_name)
            self.crawler.crawl(spider)

        self.crawler.start()

But once a spider is registered with self.crawler.crawl(), I get assertion errors for all of the other spiders:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/scrapy/cmdline.py"", line 138, in _run_command
    cmd.run(args, opts)
  File ""/home/blender/Projects/scrapers/store_crawler/store_crawler/commands/crawlall.py"", line 22, in run
    self.crawler.crawl(spider)
  File ""/usr/lib/python2.7/site-packages/scrapy/crawler.py"", line 47, in crawl
    return self.engine.open_spider(spider, requests)
  File ""/usr/lib/python2.7/site-packages/twisted/internet/defer.py"", line 1214, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File ""/usr/lib/python2.7/site-packages/twisted/internet/defer.py"", line 1071, in _inlineCallbacks
    result = g.send(result)
  File ""/usr/lib/python2.7/site-packages/scrapy/core/engine.py"", line 215, in open_spider
    spider.name
exceptions.AssertionError: No free spider slots when opening 'spidername'

Is there any way to do this? I'd rather not start subclassing core Scrapy components just to run all of my spiders like this.
",11k,"
            18
        ","[""\nWhy didn't you just use something like:\nscrapy list|xargs -n 1 scrapy crawl\n\n?\n"", ""\nHere is an example that does not run inside a custom command, but runs the Reactor manually and creates a new Crawler for each spider:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\n# scrapy.conf.settings singlton was deprecated last year\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy import log\n\ndef setup_crawler(spider_name):\n    crawler = Crawler(settings)\n    crawler.configure()\n    spider = crawler.spiders.create(spider_name)\n    crawler.crawl(spider)\n    crawler.start()\n\nlog.start()\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.configure()\n\nfor spider_name in crawler.spiders.list():\n    setup_crawler(spider_name)\n\nreactor.run()\n\nYou will have to design some signal system to stop the reactor when all spiders are finished.\nEDIT: And here is how you can run multiple spiders in a custom command:\nfrom scrapy.command import ScrapyCommand\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import Crawler\n\nclass Command(ScrapyCommand):\n\n    requires_project = True\n\n    def syntax(self):\n        return '[options]'\n\n    def short_desc(self):\n        return 'Runs all of the spiders'\n\n    def run(self, args, opts):\n        settings = get_project_settings()\n\n        for spider_name in self.crawler.spiders.list():\n            crawler = Crawler(settings)\n            crawler.configure()\n            spider = crawler.spiders.create(spider_name)\n            crawler.crawl(spider)\n            crawler.start()\n\n        self.crawler.start()\n\n"", ""\nthe answer of @Steven Almeroth will be failed in Scrapy 1.0, and you should edit the script like this:\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nclass Command(ScrapyCommand):\n\n    requires_project = True\n    excludes = ['spider1']\n\n    def syntax(self):\n        return '[options]'\n\n    def short_desc(self):\n        return 'Runs all of the spiders'\n\n    def run(self, args, opts):\n        settings = get_project_settings()\n        crawler_process = CrawlerProcess(settings) \n\n        for spider_name in crawler_process.spider_loader.list():\n            if spider_name in self.excludes:\n                continue\n            spider_cls = crawler_process.spider_loader.load(spider_name) \n            crawler_process.crawl(spider_cls)\n        crawler_process.start()\n\n"", '\nthis code is works on My scrapy version is 1.3.3 (save it in same directory in scrapy.cfg):\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spiders.list():\n    print (""Running spider %s"" % (spider_name))\n    process.crawl(spider_name,query=""dvh"") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n\nfor scrapy 1.5.x (so you don\'t get the deprecation warning)\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spider_loader.list():\n    print (""Running spider %s"" % (spider_name))\n    process.crawl(spider_name,query=""dvh"") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n\n', '\nLinux script\n#!/bin/bash\nfor spider in $(scrapy list)\ndo\nscrapy crawl ""$spider"" -o ""$spider"".json\ndone\n\n', ""\nRunning all spiders in project using python\n# Run all spiders in project implemented using Scrapy 2.7.0\n\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\n\ndef main():\n    settings = get_project_settings()\n    process = CrawlerProcess(settings)\n    spiders_names = process.spider_loader.list()\n    for s in spiders_names:\n        process.crawl(s)\n    process.start()\n\n\nif __name__ == '__main__':\n    main()\n\n""]",https://stackoverflow.com/questions/15564844/locally-run-all-of-the-spiders-in-scrapy,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating a generic scrapy spider,"
My question is really how to do the same thing as a previous question, but in Scrapy 0.14.
Using one Scrapy spider for several websites
Basically, I have GUI that takes parameters like domain, keywords, tag names, etc. and I want to create a generic spider to crawl those domains for those keywords in those tags.  I've read conflicting things, using older versions of scrapy, by either overriding the spider manager class or by dynamically creating a spider.  Which method is preferred and how do I implement and invoke the proper solution?  Thanks in advance.
Here is the code that I want to make generic.  It also uses BeautifulSoup.  I paired it down so hopefully didn't remove anything crucial to understand it.
class MySpider(CrawlSpider):

name = 'MySpider'
allowed_domains = ['somedomain.com', 'sub.somedomain.com']
start_urls = ['http://www.somedomain.com']

rules = (
    Rule(SgmlLinkExtractor(allow=('/pages/', ), deny=('', ))),

    Rule(SgmlLinkExtractor(allow=('/2012/03/')), callback='parse_item'),
)

def parse_item(self, response):
    contentTags = []

    soup = BeautifulSoup(response.body)

    contentTags = soup.findAll('p', itemprop=""myProp"")

    for contentTag in contentTags:
        matchedResult = re.search('Keyword1|Keyword2', contentTag.text)
        if matchedResult:
            print('URL Found: ' + response.url)

    pass

",7k,"
            17
        ","['\nYou could create a run-time spider which is evaluated by the interpreter. This code piece could be evaluated at runtime like so:\na = open(""test.py"")\nfrom compiler import compile\nd = compile(a.read(), \'spider.py\', \'exec\')\neval(d)\n\nMySpider\n<class \'__main__.MySpider\'>\nprint MySpider.start_urls\n[\'http://www.somedomain.com\']\n\n', '\nI use the Scrapy Extensions approach to extend the Spider class to a class named Masterspider that includes a generic parser.\nBelow is the very ""short"" version of my generic extended parser. Note that you\'ll need to implement a renderer with a Javascript engine (such as Selenium or BeautifulSoup) a as soon as you start working on pages using AJAX. And a lot of additional code to manage differences between sites (scrap based on column title, handle relative vs long URL, manage different kind of data containers, etc...).\nWhat is interresting with the Scrapy Extension approach is that you can still override the generic parser method if something does not fit but I never had to. The Masterspider class checks if some methods have been created (eg. parser_start, next_url_parser...) under the site specific spider class to allow the management of specificies: send a form, construct the next_url request from elements in the page, etc.\nAs I\'m scraping very different sites, there\'s always specificities to manage. That\'s why I prefer to keep a class for each scraped site so that I can write some specific methods to handle it (pre-/post-processing except PipeLines, Request generators...).\nmasterspider/sitespider/settings.py\nEXTENSIONS = {\n    \'masterspider.masterspider.MasterSpider\': 500\n}\n\nmasterspider/masterspdier/masterspider.py\n# -*- coding: utf8 -*-\nfrom scrapy.spider import Spider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request\nfrom sitespider.items import genspiderItem\n\nclass MasterSpider(Spider):\n\n    def start_requests(self):\n        if hasattr(self,\'parse_start\'): # First page requiring a specific parser\n            fcallback = self.parse_start\n        else:\n            fcallback = self.parse\n        return [ Request(self.spd[\'start_url\'],\n                     callback=fcallback,\n                     meta={\'itemfields\': {}}) ]\n\n    def parse(self, response):\n        sel = Selector(response)\n        lines = sel.xpath(self.spd[\'xlines\'])\n        # ...\n        for line in lines:\n            item = genspiderItem(response.meta[\'itemfields\'])               \n            # ...\n            # Get request_url of detailed page and scrap basic item info\n            # ... \n            yield  Request(request_url,\n                   callback=self.parse_item,\n                   meta={\'item\':item, \'itemfields\':response.meta[\'itemfields\']})\n\n        for next_url in sel.xpath(self.spd[\'xnext_url\']).extract():\n            if hasattr(self,\'next_url_parser\'): # Need to process the next page URL before?\n                yield self.next_url_parser(next_url, response)\n            else:\n                yield Request(\n                    request_url,\n                    callback=self.parse,\n                    meta=response.meta)\n\n    def parse_item(self, response):\n        sel = Selector(response)\n        item = response.meta[\'item\']\n        for itemname, xitemname in self.spd[\'x_ondetailpage\'].iteritems():\n            item[itemname] = ""\\n"".join(sel.xpath(xitemname).extract())\n        return item\n\nmasterspider/sitespider/spiders/somesite_spider.py\n# -*- coding: utf8 -*-\nfrom scrapy.spider import Spider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request\nfrom sitespider.items import genspiderItem\nfrom masterspider.masterspider import MasterSpider\n\nclass targetsiteSpider(MasterSpider):\n    name = ""targetsite""\n    allowed_domains = [""www.targetsite.com""]\n    spd = {\n        \'start_url\' : ""http://www.targetsite.com/startpage"", # Start page\n        \'xlines\' : ""//td[something...]"",\n        \'xnext_url\' : ""//a[contains(@href,\'something?page=\')]/@href"", # Next pages\n        \'x_ondetailpage\' : {\n            ""itemprop123"" :      u""id(\'someid\')//text()""\n            }\n    }\n\n#     def next_url_parser(self, next_url, response): # OPTIONAL next_url regexp pre-processor\n#          ...\n\n', ""\nInstead of having the variables name,allowed_domains, start_urls and rules attached to the class, you should write a MySpider.__init__, call CrawlSpider.__init__ from that passing the necessary arguments, and setting name, allowed_domains etc. per object. \nMyProp and keywords also should be set within your __init__. So in the end you should have something like below. You don't have to add name to the arguments, as name is set by BaseSpider itself from kwargs: \nclass MySpider(CrawlSpider):\n\n    def __init__(self, allowed_domains=[], start_urls=[], \n            rules=[], findtag='', finditemprop='', keywords='', **kwargs):\n        CrawlSpider.__init__(self, **kwargs)\n        self.allowed_domains = allowed_domains\n        self.start_urls = start_urls\n        self.rules = rules\n        self.findtag = findtag\n        self.finditemprop = finditemprop\n        self.keywords = keywords\n\n    def parse_item(self, response):\n        contentTags = []\n\n        soup = BeautifulSoup(response.body)\n\n        contentTags = soup.findAll(self.findtag, itemprop=self.finditemprop)\n\n        for contentTag in contentTags:\n            matchedResult = re.search(self.keywords, contentTag.text)\n            if matchedResult:\n                print('URL Found: ' + response.url)\n\n"", '\nI am not sure which way is preferred, but I will tell you what I have done in the past. I am in no way sure that this is the best (or correct) way of doing this and I would be interested to learn what other people think.\nI usually just override the parent class (CrawlSpider) and either pass in arguments and then initialize the parent class via super(MySpider, self).__init__() from within my own init-function or I pull in that data from a database where I have saved a list of links to be appended to start_urls earlier.\n', '\nAs far as crawling specific domains passed as arguments goes, I just override Spider.__init__:\nclass MySpider(scrapy.Spider):\n    """"""\n    This spider will try to crawl whatever is passed in `start_urls` which\n    should be a comma-separated string of fully qualified URIs.\n\n    Example: start_urls=http://localhost,http://example.com\n    """"""\n    def __init__(self, name=None, **kwargs):\n        if \'start_urls\' in kwargs:\n            self.start_urls = kwargs.pop(\'start_urls\').split(\',\')\n        super(Spider, self).__init__(name, **kwargs)\n\n']",https://stackoverflow.com/questions/9814827/creating-a-generic-scrapy-spider,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do I stop all spiders and the engine immediately after a condition in a pipeline is met?,"
We have a system written with scrapy to crawl a few websites. There are several spiders, and a few cascaded pipelines for all items passed by all crawlers.
One of the pipeline components queries the google servers for geocoding addresses.
Google imposes a limit of 2500 requests per day per IP address, and threatens to ban an IP address if it continues querying google even after google has responded with a warning message: 'OVER_QUERY_LIMIT'.
Hence I want to know about any mechanism which I can invoke from within the pipeline that will completely and immediately stop all further crawling/processing of all spiders and also the main engine.
I have checked other similar questions and their answers have not worked:

Force my scrapy spider to stop crawling


from scrapy.project import crawler
crawler._signal_shutdown(9,0) #Run this if the cnxn fails.


this does not work as it takes time for the spider to stop execution and hence many more requests are made to google (which could potentially ban my IP address)

import sys
sys.exit(""SHUT DOWN EVERYTHING!"")


this one doesn't work at all; items keep getting generated and passed to the pipeline, although the log vomits sys.exit() -> exceptions.SystemExit raised (to no effect)

How can I make scrapy crawl break and exit when encountering the first exception?


crawler.engine.close_spider(self, 'log message')


this one has the same problem as the first case mentioned above.
I tried:

scrapy.project.crawler.engine.stop()


To no avail
EDIT:
If I do in the pipeline:

from scrapy.contrib.closespider import CloseSpider

what should I pass as the 'crawler' argument to the CloseSpider's init() from the scope of my pipeline?
",9k,"
            13
        ","['\nYou can raise a CloseSpider exception to close down a spider.\nHowever, I don\'t think this will work from a pipeline. \nEDIT: avaleske notes in the comments to this answer that he was able to raise a CloseSpider exception from a pipeline. Most wise would be to use this.\nA similar situation has been described on the Scrapy Users group, in this thread.\nI quote:\n\nTo close an spider for any part of your code you should use \n  engine.close_spider  method. See this extension for an usage\n  example: \n  https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/closespider.py#L61\n\nYou could write your own extension, whilst looking at closespider.py as an example, which will shut down a spider if a certain condition has been met.\nAnother ""hack"" would be to set a flag on the spider in the pipeline. For example:\npipeline:\ndef process_item(self, item, spider):\n    if some_flag:\n        spider.close_down = True\n\nspider:\ndef parse(self, response):\n    if self.close_down:\n        raise CloseSpider(reason=\'API usage exceeded\')\n\n']",https://stackoverflow.com/questions/9699049/how-do-i-stop-all-spiders-and-the-engine-immediately-after-a-condition-in-a-pipe,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to print html source to console with phantomjs,"
I just downloaed and installed phantomjs on my machine. I copy and pasted the following script into a file called hello.js:
var page = require('webpage').create();
var url = 'https://www.google.com'

page.onLoadStarted = function () {
    console.log('Start loading...');
};

page.onLoadFinished = function (status) {
    console.log('Loading finished.');
phantom.exit();
};

page.open(url);

I'd like to print the complete html source (in this case from the google page) to a file or to the console. How do I do this?
",28k,"
            12
        ","[""\nSpent some time to read the documentation, it should be obvious afterwards.\nvar page = require('webpage').create();\npage.open('http://google.com', function () {\n    console.log(page.content);\n    phantom.exit();\n});\n\n""]",https://stackoverflow.com/questions/12450868/how-to-print-html-source-to-console-with-phantomjs,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to get a web page's source code from Java [duplicate],"






This question already has answers here:
                        
                    



How do you Programmatically Download a Webpage in Java

                                (11 answers)
                            

Closed 7 years ago.



I just want to retrieve any web page's source code from Java. I found lots of solutions so far, but I couldn't find any code that works for all the links below: 

http://www.cumhuriyet.com.tr?hn=298710
http://www.fotomac.com.tr/Yazarlar/Olcay%20%C3%87ak%C4%B1r/2011/11/23/hesap-makinesi 
http://www.sabah.com.tr/Gundem/2011/12/23/basbakan-konferansta-konusuyor#

The main problem for me is that some codes retrieve web page source code, but with missing ones. For example the code below does not work for the first link.
InputStream is = fURL.openStream(); //fURL can be one of the links above
BufferedReader buffer = null;
buffer = new BufferedReader(new InputStreamReader(is, ""iso-8859-9""));

int byteRead;
while ((byteRead = buffer.read()) != -1) {
    builder.append((char) byteRead);
}
buffer.close();
System.out.println(builder.toString());

",110k,"
            11
        ","['\nTry the following code with an added request property:\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.net.URL;\nimport java.net.URLConnection;\n\npublic class SocketConnection\n{\n    public static String getURLSource(String url) throws IOException\n    {\n        URL urlObject = new URL(url);\n        URLConnection urlConnection = urlObject.openConnection();\n        urlConnection.setRequestProperty(""User-Agent"", ""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.95 Safari/537.11"");\n\n        return toString(urlConnection.getInputStream());\n    }\n\n    private static String toString(InputStream inputStream) throws IOException\n    {\n        try (BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream, ""UTF-8"")))\n        {\n            String inputLine;\n            StringBuilder stringBuilder = new StringBuilder();\n            while ((inputLine = bufferedReader.readLine()) != null)\n            {\n                stringBuilder.append(inputLine);\n            }\n\n            return stringBuilder.toString();\n        }\n    }\n}\n\n', '\nURL yahoo = new URL(""http://www.yahoo.com/"");\nBufferedReader in = new BufferedReader(\n            new InputStreamReader(\n            yahoo.openStream()));\n\nString inputLine;\n\nwhile ((inputLine = in.readLine()) != null)\n    System.out.println(inputLine);\n\nin.close();\n\n', '\nI am sure that you have found a solution somewhere over the past 2 years but the following is a solution that works for your requested site\npackage javasandbox;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.MalformedURLException;\nimport java.net.URL;\n\n/**\n*\n* @author Ryan.Oglesby\n*/\npublic class JavaSandbox {\n\nprivate static String sURL;\n\n/**\n * @param args the command line arguments\n */\npublic static void main(String[] args) throws MalformedURLException, IOException {\n    sURL = ""http://www.cumhuriyet.com.tr/?hn=298710"";\n    System.out.println(sURL);\n    URL url = new URL(sURL);\n    HttpURLConnection httpCon = (HttpURLConnection) url.openConnection();\n    //set http request headers\n            httpCon.addRequestProperty(""Host"", ""www.cumhuriyet.com.tr"");\n            httpCon.addRequestProperty(""Connection"", ""keep-alive"");\n            httpCon.addRequestProperty(""Cache-Control"", ""max-age=0"");\n            httpCon.addRequestProperty(""Accept"", ""text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"");\n            httpCon.addRequestProperty(""User-Agent"", ""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36"");\n            httpCon.addRequestProperty(""Accept-Encoding"", ""gzip,deflate,sdch"");\n            httpCon.addRequestProperty(""Accept-Language"", ""en-US,en;q=0.8"");\n            //httpCon.addRequestProperty(""Cookie"", ""JSESSIONID=EC0F373FCC023CD3B8B9C1E2E2F7606C; lang=tr; __utma=169322547.1217782332.1386173665.1386173665.1386173665.1; __utmb=169322547.1.10.1386173665; __utmc=169322547; __utmz=169322547.1386173665.1.1.utmcsr=stackoverflow.com|utmccn=(referral)|utmcmd=referral|utmcct=/questions/8616781/how-to-get-a-web-pages-source-code-from-java; __gads=ID=3ab4e50d8713e391:T=1386173664:S=ALNI_Mb8N_wW0xS_wRa68vhR0gTRl8MwFA; scrElm=body"");\n            HttpURLConnection.setFollowRedirects(false);\n            httpCon.setInstanceFollowRedirects(false);\n            httpCon.setDoOutput(true);\n            httpCon.setUseCaches(true);\n\n            httpCon.setRequestMethod(""GET"");\n\n            BufferedReader in = new BufferedReader(new InputStreamReader(httpCon.getInputStream(), ""UTF-8""));\n            String inputLine;\n            StringBuilder a = new StringBuilder();\n            while ((inputLine = in.readLine()) != null)\n                a.append(inputLine);\n            in.close();\n\n            System.out.println(a.toString());\n\n            httpCon.disconnect();\n}\n}\n\n']",https://stackoverflow.com/questions/8616781/how-to-get-a-web-pages-source-code-from-java,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Crawling the Google Play store,"
I want to crawl the Google Play store to download the web pages of all the android application (All the webpages with the following base url: https://play.google.com/store/apps/). I checked the robots.txt file of the play store and it disallows crawling these URLs. 
Also, when I browse the Google Play store I can only see top applications up to 3 pages for each of the categories. How can I get the other application pages?
If anyone has tried crawling the Google Play please let me know the following things:
a) Were you successful in crawling the play store. If yes, please let me know how you did that.
b) How to crawl the hidden application pages not visible in top apps for each of the categories?
c) Is there a techniques to download the applications also and not just the webpages?
I already searched around and found the following links:
a) https://code.google.com/p/android-market-api/ 
b) https://code.google.com/p/android-marketplace-crawler/source/checkout 
c) http://mohsin-junaid.blogspot.co.uk/2012/12/how-to-install-android-marketplace.html 
d) http://mohsin-junaid.blogspot.in/2012/12/how-to-download-multiple-android-apks.html

Thanks!
",15k,"
            11
        ","['\nFirst of all, Google Play\'s robots.txt does NOT disallow the pages with base ""/store/apps"".\nIf you want to crawl Google Play you would need to develop your own web crawler, parse the HTML page and extract the app meta-data you need (e.g. title, descriptions, price, etc). This topic has been covered in this other question. There are libraries helping with that, for instance:\n\nJava: https://jsoup.org\nPython: https://scrapy.org\n\nThe harder part is to ""find"" the app-pages to crawl. You could use 1) the Google Play Sitemap or 2) follow the app-links you find in every page you crawl as explained in the Link Extractor documentation (in case you plan to use Scrapy).\nAnother option is to use an open-source library based on ProtoBuf to fetch meta-data about an app, here the link to the project: https://code.google.com/archive/p/android-market-api.\nThis library fetches app meta-data from Google Play on behalf of a valid Google account, but also in this case you need a crawler to ""find"" which apps are available and schedule their meta-data retrieval. This other open-source project can help you with that: https://code.google.com/archive/p/android-marketplace-crawler.\nIf you don\'t want to implement all this by yourself, you could use a third-party managed service to access Android apps meta-data through a JSON-based API. For instance, 42matters.com (the company I work for) offers an API for both Android and iOS to retrieve apps\' meta-data, here more details:\nhttps://42matters.com/app-market-data\nIn order to get the Title, Icon, Description, Downloads for an app you can use the ""lookup"" endpoint as documented here:\nhttps://42matters.com/docs/app-market-data/android/apps/lookup\nThis is an example of the JSON response for the ""Angry Birds Space Premium"" app:\n{\n    ""package_name"": ""com.rovio.angrybirdsspace.premium"",\n    ""title"": ""Angry Birds Space Premium"",\n    ""description"": ""Play over 300 interstellar levels across 10 planets..."",\n    ""short_desc"": ""The #1 mobile game of all time blasts off into space!"",\n    ""rating"": 4.3046236038208,\n    ""category"": ""Arcade"",\n    ""cat_key"": ""GAME_ARCADE"",\n    ""cat_keys"": [\n        ""GAME_ARCADE"",\n        ""GAME"",\n        ""FAMILY_EDUCATION"",\n        ""FAMILY""\n    ],\n    ""price"": ""$1.15"",\n    ""downloads"": ""1,000,000 - 5,000,000"",\n    ""version"": ""2.2.1"",\n    ""content_rating"": ""Everyone"",\n    ""promo_video"": ""https://www.youtube.com/embed/g6AL9YqRHaI?ps=play&vq=large&rel=0&autohide=1&showinfo=0&autoplay=1"",\n    ""market_update"": ""2015-07-03T00:00:00+00:00"",\n    ""screenshots"": [\n        ""https://lh3.googleusercontent.com/ZmuBQzIy1G74coPrQ1R7fCeKdJmjTdpJhNrIHBOaFyM0N2EYdUPwZaQjnQUtiUDGmac=h310"",\n        ""https://lh3.googleusercontent.com/Xg2Aq70ZH0SnNhtSKH7xg9jCfisWgmmq3C7xQbx6YMhTVAIRqlRJeH8GYtjxapb_qR4=h310"",\n        ""https://lh3.googleusercontent.com/T4o5-2_UP82sj4fSSegbjrGmslNHlfvtEYuZacXMSOC55-7eyiKySw05lNF1QQGO2FeU=h310"",\n        ""https://lh3.googleusercontent.com/f2ennaLdivFu5cQQaVPKsRcWxB8FS5T4Bkoy3l0iPW9-GDDnTVRhvR5kz6l4m8FL1c8=h310"",\n        ""https://lh3.googleusercontent.com/H-9M03_-O9Df1nHr2-rUdjtk2aeBY3bAxnqSX3m2zh_aV8-K1t0qU1DxLXnK0GrDAw=h310""\n    ],\n    ""created"": ""2012-03-22T08:24:00+00:00"",\n    ""developer"": ""Rovio Entertainment Ltd."",\n    ""number_ratings"": 20812,\n    ""price_currency"": ""$"",\n    ""icon"": ""https://lh3.ggpht.com/aQaIEGrmba1ENSEgUtArdm3yhJUug7BRWlu_WaspoJusZyHv1rjlWtYqe_qRjE_Kmh1E=w300"",\n    ""icon_72"": ""https://lh3.ggpht.com/aQaIEGrmba1ENSEgUtArdm3yhJUug7BRWlu_WaspoJusZyHv1rjlWtYqe_qRjE_Kmh1E=w72"",\n    ""market_url"": ""https://play.google.com/store/apps/details?id=com.rovio.angrybirdsspace.premium&referrer=utm_source%3D42matters.com%26utm_medium%3Dapi""\n}\n\nI hope this helps, otherwise feel free to get in touch with me. I know this topic quite well and can point you in the right direction.\nRegards,\nAndrea\n', '\nI have did the job in Python before, what you need is a web auto test lib called selenium, it can execute Javascript code and return the result to Python, with Javascript, you can click the ""show more"" button by the program itself. And when you get all links for a single category page, you can get some info for the app. The simple demo here. Hope helpful.\n', '\nGoogle doesn\'t disallow crawling of /store/apps pages. \nThere is no mention about ""/store/apps"" in the robot.txt\nSee https://play.google.com/robots.txt\n']",https://stackoverflow.com/questions/17002181/crawling-the-google-play-store,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
crawling a html page using php?,"
This website lists over 250 courses in one list. I want to get the name of each course and insert that into my mysql database using php. The courses are listed like this:
<td> computer science</td>
<td> media studeies</td>
鈥?
Is there a way to do that in PHP, instead of me  having a mad data entry nightmare?
",4k,"
            4
        ","['\nRegular expressions work well.\n$page = // get the page\n$page = preg_split(""/\\n/"", $page);\nfor ($text in $page) {\n    $matches = array();\n    preg_match(""/^<td>(.*)<\\/td>$/"", $text, $matches);\n    // insert $matches[1] into the database\n}\n\nSee the documentation for preg_match.\n', ""\nHow to parse HTML has been asked and answered countless times before. While (for your specific UseCase) Regular Expressions will work, it is - in general - better and more reliable to use a proper parser for this task. Below is how to do it with DOM:\n$dom = new DOMDocument;\n$dom->loadHTMLFile('http://courses.westminster.ac.uk/CourseList.aspx');\nforeach($dom->getElementsByTagName('td') as $title) {\n    echo $title->nodeValue;\n}\n\nFor inserting the data into MySql, you should use the mysqli extension. Examples are plentiful on StackOverflow. so please use the search function.\n"", '\nYou can use this HTML parsing php library to achieve this :http://simplehtmldom.sourceforge.net/\n', '\nI encountered the same problem.\nHere is a good class library called the html dom\nhttp://simplehtmldom.sourceforge.net/.\nThis like jquery\n', '\nJust for fun, here\'s a quick shell script to do the same thing.\ncurl http://courses.westminster.ac.uk/CourseList.aspx \\\n| sed \'/<td>\\(.*\\)<\\/td>/ { s/.*"">\\(.*\\)<\\/a>.*/\\1/; b }; d;\' \\\n| uniq > courses.txt\n\n']",https://stackoverflow.com/questions/3946506/crawling-a-html-page-using-php,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrapy Linkextractor duplicating(?),"
I have the crawler implemented as below.
It is working and it would go through sites regulated under the link extractor.
Basically what I am trying to do is to extract information from different places in the page:
- href and text() under the class 'news' ( if exists)
- image url under the class 'think block' ( if exists)
I have three problems for my scrapy:
1) duplicating linkextractor
It seems that it will duplicate processed page.  ( I check against the export file and found that the same ~.img appeared many times while it is hardly possible)
And the fact is , for every page in the website, there are hyperlinks at the bottom that facilitate users to direct to the topic they are interested in, while my objective is to extract information from the topic's page ( here listed several passages's title under the same topic ) and the images found within a passage's page( you can arrive to the passage's page by clicking on the passage's title found at topic page).
I suspect link extractor would loop the same page over again in this case.
( maybe solve with depth_limit?)
2) Improving parse_item
I think it is quite not efficient for parse_item. How could I improve it? I need to extract information from different places in the web ( for sure it only extracts if  it exists).Beside, it looks like that the parse_item could only progress HkejImage but not HkejItem (again I checked with the output file). How should I tackle this?
3) I need the spiders to be able to read Chinese.
I am crawling a site in HK and it would be essential to be capable to read Chinese.
The site:

http://www1.hkej.com/dailynews/headline/article/1105148/IMF%E5%82%B3%E4%BF%83%E4%B8%AD%E5%9C%8B%E9%80%80%E5%87%BA%E6%95%91%E5%B8%82

As long as it belongs to 'dailynews', that's the thing I want.
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.selector import Selector
from scrapy.http import Request, FormRequest
from scrapy.contrib.linkextractors import LinkExtractor
import items


class EconjournalSpider(CrawlSpider):
    name = ""econJournal""
    allowed_domains = [""hkej.com""]
    login_page = 'http://www.hkej.com/template/registration/jsp/login.jsp'
    start_urls =  'http://www.hkej.com/dailynews'

    rules=(Rule(LinkExtractor(allow=('dailynews', ),unique=True), callback='parse_item', follow =True),
           )


    def start_requests(self):
         yield Request(
         url=self.login_page,
         callback=self.login,
         dont_filter=True
         )
# name column
    def login(self, response):
        return FormRequest.from_response(response,
                    formdata={'name': 'users', 'password': 'my password'},
                    callback=self.check_login_response)

    def check_login_response(self, response):
        """"""Check the response returned by a login request to see if we are
        successfully logged in.
        """"""
        if ""username"" in response.body:       
            self.log(""\n\n\nSuccessfully logged in. Let's start crawling!\n\n\n"")
            return Request(url=self.start_urls)
        else:
            self.log(""\n\n\nYou are not logged in.\n\n\n"")
            # Something went wrong, we couldn't log in, so nothing happens

    def parse_item(self, response):
        hxs = Selector(response)
        news=hxs.xpath(""//div[@class='news']"")
        images=hxs.xpath('//p')

        for image in images:
            allimages=items.HKejImage()
            allimages['image'] = image.xpath('a/img[not(@data-original)]/@src').extract()
            yield allimages

        for new in news:
            allnews = items.HKejItem()
            allnews['news_title']=new.xpath('h2/@text()').extract()
            allnews['news_url'] = new.xpath('h2/@href').extract()
            yield allnews

Thank you very much and I would appreciate any help!
",2k,"
            3
        ","[""\nFirst, to set settings, make it on the settings.py file or you can specify the custom_settings parameter on the spider, like:\ncustom_settings = {\n    'DEPTH_LIMIT': 3,\n}\n\nThen, you have to make sure the spider is reaching the parse_item method (which I think it doesn't, haven't tested yet). And also you can't specify the callback and follow parameters on a rule, because they don't work together.\nFirst remove the follow on your rule, or add another rule, to check which links to follow, and which links to return as items.\nSecond on your parse_item method, you are getting incorrect xpath, to get all the images, maybe you could use something like:\nimages=hxs.xpath('//img')\n\nand then to get the image url:\nallimages['image'] = image.xpath('./@src').extract()\n\nfor the news, it looks like this could work:\nallnews['news_title']=new.xpath('.//a/text()').extract()\nallnews['news_url'] = new.xpath('.//a/@href').extract()\n\nNow, as and understand your problem, this isn't a Linkextractor duplicating error, but only poor rules specifications, also make sure you have valid xpath, because your question didn't indicate you needed xpath correction.\n""]",https://stackoverflow.com/questions/31630771/scrapy-linkextractor-duplicating,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HtmlAgilityPack HtmlWeb.Load returning empty Document,"
I have been using HtmlAgilityPack for the last 2 months in a Web Crawler Application with no issues loading a webpage.
Now when I try to load a this particular webpage, the document OuterHtml is empty, so this test fails
var url = ""http://www.prettygreen.com/"";
var htmlWeb = new HtmlWeb();
var htmlDoc = htmlWeb.Load(url);
var outerHtml = htmlDoc.DocumentNode.OuterHtml;
Assert.AreNotEqual("""", pageHtml);

I can load another page from the site with no problems, such as setting
url = ""http://www.prettygreen.com/news/"";

In the past I once had an issue with encodings, I played around with htmlWeb.OverrideEncoding and htmlWeb.AutoDetectEncoding with no luck.  I have no idea what could be the issue here with this webpage.
",12k,"
            1
        ","['\nIt seems this website requires cookies to be enabled. So creating a cookie container for your web request should solve the issue:\nvar url = ""http://www.prettygreen.com/"";\nvar htmlWeb = new HtmlWeb();\nhtmlWeb.PreRequest += request =>\n    {\n        request.CookieContainer = new System.Net.CookieContainer();\n        return true;\n    };\nvar htmlDoc = htmlWeb.Load(url);\nvar outerHtml = htmlDoc.DocumentNode.OuterHtml;\nAssert.AreNotEqual("""", outerHtml);\n\n']",https://stackoverflow.com/questions/13400493/htmlagilitypack-htmlweb-load-returning-empty-document,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hide Email Address from Bots - Keep mailto:,"
tl;dr
Hide email address from bots without using scripts and maintain mailto: functionality. Method must also support screen-readers.

Summary

Email obfuscation without using scripts or contact forms

Email address needs to be completely visible to human viewers and maintain mailto: functionality

Email Address must not be in image form.

Email address must be ""completely"" hidden from spam-crawlers and spam-bots and any other harvester type



Desired Effect:

No scripts, please. There are no scripts used in the project and I'd like to keep it that way.

Email address is either displayed on the page or can be easily displayed after some sort of user interaction, like opening a modal.

The user can click on on the email address which in turn would trigger the mailto: functionality.

Clicking the email will open the user's email application.
In other words, mailto: functionality must work.

The email address in not visible or not identified as an email address to bots (This includes the page source)

I don't have an inbox that's full of spam



What does NOT Work

Adding a contact form - or anything similar - instead of the email address

I hate contact forms. I rarely fill up a contact form. If there's no email address, I look for a phone number, and if that's not there, I start looking for an alternative service. I would only fill up a contact form if I absolutely have to.

Replacing the address with an image of the address

This creates a HUGE disadvantage to someone using a screenreader (please remember the visually impaired in your future projects)
It also removes the mailto: functionality unless you make the image clickable and then add the mailto: functionality as the href for the link, but that defeats the purpose and now the email is visible to bots.

What might work:

Clever usage of pseudo-elements in CSS

Solutions that make use of base64 encoding

Breaking up the email address and spreading the parts across the document then putting them back together in a modal when the user clicks a button (This will probably involve multiple CSS classes and the usage of anchor tags)

Alterting html attributes via CSS


@MortezaAsadi gracefully brought up the possibility in the comments below. This is the link to the full - The article is from 2012:
What if We Could Use CSS to Alter HTML Attributes?

Other creative solutions that are beyond my scope of knowledge.


Similar Questions / Fixes

JavaScript: Protect your email address by Joe Maller

(This a great fix suggested by Joe Maller, it works well but it's script based. Here's what it looks like;


<SCRIPT TYPE=""text/javascript"">

  emailE = 'example.com'

  emailE = ('yourname' + '@' + emailE)

  document.write('<A href=""mailto:' + emailE + '"">' + emailE + '</a>')

</script>

<NOSCRIPT>

  Email address protected by JavaScript

</NOSCRIPT>



Looking for a PHP only email address obfuscator function
(A Clever solution using both PHP and CSS to first reverse the email using PHP then reverse it back with CSS) A very promising solution that Works great! But it's too easy to solve.

Is it worth obfuscating email addresses on the web these days?


(JavaScript fix)

Best way to obfuscate an e-mail address on a website?

The selected answer works. It actually works really well. It involves encoding the email as html entities. Can it be improved?
Here's what it looks like;


<A HREF=""mailto:

&#121;&#111;&#117;&#114;&#110;&#097;&#109;&#101;&#064;&#100;&#111;&#109;&#097;&#105;&#110;&#046;&#099;&#111;&#109;"">

&#121;&#111;&#117;&#114;&#110;&#097;&#109;&#101;&#064;&#100;&#111;&#109;&#097;&#105;&#110;&#046;&#099;&#111;&#109;

</A>



Does e-mail address obfuscation actually work?

(The selected answer to this SuperUser question is great and it presents a study of the amount of spam received by using different obfuscation methods.
It seems that manipulating the email address with CSS to make it rtl does work. This is the same method used in the first question I linked to in this section.
I am uncertain what effects adding mailto: functionality to the fix would have on the results.

There are also many other questions on SO which all have similar answers. I have not found anything that fits my desired effect


The Question:
Would it be possible to increase the efficiency (ie as little spam as possible) of the email obfuscation methods above by combining two or more of the fixes (or even adding new fixes) while:
A- Maintaining mailto: functionality; and
B- Supporting screen-readers

Many of the answers and comments below pose a very good question while indicating the impossibility of doing this without some sort of js
The question that's asked/implied is:

Why not use js?

The answer is that I am allergic to js
Joking aside though,
The three main reasons I asked this question are:

Contact forms are becoming more and more accepted as a replacement
for providing an email address - which they should not.

If it can be done without scripting then it should be done without
scripting.

Curiosity: (as I am in fact using one of the js fixes currently) I wanted to see if discussing the matter would lead to a better way of doing it.


",66k,"
            105
        ","['\nThe issue with your request is specifically the ""Supporting screen-readers"", as by definition screen readers are a ""bot"" of some sort. If a screen-reader needs to be able to interpret the email address, then a page-crawler would be able to interpret it as well.\nAlso, the point of the mailto attribute is to be the standard of how to do email addresses on the web. Asking if there is a second way to do that is sort of asking if there is a second standard.\nDoing it through scripts will still have the same issue as once the page is loaded, the script would have been run and the email address rendered in the DOM (unless you populate the email address on click or something). Either way, screen readers will still have issues with this since it\'s not already loaded.\nHonestly, just get an email service with a half decent spam filter and specify a default subject line that is easy for you to sort in your inbox.\n<a href=""mailto:no-one@example.com?subject=Something to filter on"">Email me</a>\n\nWhat you\'re asking for is if the standard has two ways to do something, one for bots and the other for non-bots. The answer is it doesn\'t, and you have to just fight the bots as best you can.\n', '\nDefeating email bots is a tough one. You may want to check out the Email Address Harvesting countermeasures section on Wikipedia.\nMy back-story is that I\'ve written a search bot. It crawled 105,000+ URLs during it\'s initial run many years ago. From what I\'ve learned from doing that is that web crawling bots literally see EVERYTHING that is text, which appears on a web page. Bots read everything except images.\nSpam can\'t be easily stopped via code for these reasons:\n\nCSS & JS are irrelevant when using the mailto: tag. Bots specifically look at HTML pages for that ""mailto:"" keyword. Everything from that colon to the next single quote or double quote (whichever comes first) is seen as an email address. HTML entity email addresses - like the example above - can be quickly translated using a reverse ASCII method/function. Running the JavaScript code snippet above, quickly turns the string which starts with: &#121;&#111;&#117;&#114;... into... yourname@example.com. (My search bot threw away hrefs with mailto:email addresses, as I wanted URLs for web pages & not email addresses.)\n\nIf a page crashes a bot, the bot author will tune the bot to fix the crash with that page in mind, so that the bot won\'t crash at that page again in the future. Thus making their bot smarter.\n\nBot authors can write bots, which generate all known variations of email addresses... without crawling pages & never using any starter email addresses. While it may not be feasible to do that, it\'s not inconceivable with today\'s high-core count CPUs (which are hyper-threaded & run at 4+ GHz), plus the availability of using distributed cloud-based computing & even super computers. It\'s conceivable that someone can now create a bot-farm to spam everyone, without knowing anyone\'s email address. 20 years ago, that would have been incomprehensible.\n\nFree email providers have had a history of selling their free user accounts to their advertisers. In the past, simply signing up for a free email account automatically guaranteed them a green light to start delivering spam to that email address... without ever using that email address online. I\'ve seen that happen multiple times, with famous company names. (I won\'t mention any names.)\n\nThe mailto: keyword is part of this IETF RFC, where browsers are built to automatically launch the default email clients, from links with that keyword in them. JavaScript has to be used to interrupt that application launching process, when it happens.\n\n\nI don\'t think it\'s possible to stop 100% of spam while using traditional email servers, without using filters on the email server and possibly using images.\nThere is one alternative... You can also build a chat-like email client, which runs internally on a website. It would be like Facebook\'s chat client. It\'s ""kind of like email"", but not really email. It\'s simply 1-to-1 instant messaging with an archiving feature... that auto-loads upon login. Since it has document attachment + link features, it works kind of like email... but without the spam. As long as you don\'t build an externally accessible API, then it\'s a closed system where people can\'t send spam into it.\nIf you\'re planning to stick with strictly traditional email, then your best bet may be to run something like Apache\'s SpamAssassin on a company\'s email server.\nYou can also try combining multiple strategies as you\'ve listed above, to make it harder for email harvesters to glean email addresses from your web pages. They won\'t stop 100% of the spam, 100% of the time... while also allowing 100% of the screen readers to work for blind visitors.\nYou\'ve created a really good starting look at what\'s wrong with traditional email! Kudos to you for that!\nA good screen reader is JAWS from Freedom Scientific. I\'ve used that before to listen to how my webpages are read by blind users. (If you hear a male voice reading both actions [like clicking on a link] & text, try changing 1 voice to female so that 1 voice reads actions & another reads text. That makes it easier to hear how the web page is read for the visually impared.)\nGood luck with your Email Address Harvesting countermeasure endeavours!\n', '\nHere is an approach that does make use of JavaScript, but with a rather small foot-print. It\'s also very ""ghetto"", and generally I would not recommend an approach with inline JS in the HTML except you have an extreme reluctance to use JS, at all.\n\n\n<a\n  href=""#""\n  data-contact=""bGUtZW1haWxAdGhlLWRvbWFpbi5jb20=""\n  data-subj=""QW4gQW1hemluZyBTdWJqZWN0""\n  onfocus=""this.href = \'mailto:\' + atob(this.dataset.contact) + \'?subject=\' + atob(this.dataset.subj || \'\')""\n  >\n  Send an email\n</a>\n\n\ndata-contact is the base64 encoded email address. And, data-subj is an optional base64 encoded subject.\nThe main challenge with doing this without JS is that CSS can\'t alter HTML attributes. (The article you linked is a ""pie-in-the-sky"" musing and does not have any bearing on what is possible today or in the near future.)\nThe HTML entities approach you mentioned, or some variation of it, is likely the simplest option that will have some efficacy. Additionally, the iframe approach is clever and the server redirect approach is pretty awesome. But, all three are vulnerable to bots:\n\nThe HTML entities just need to be converted (and detecting that is simple)\nThe document referenced by the iframe might simply be followed\nThe server redirect might simply be followed, as well\n\nWith the approach outlined above, the use of a base64 encoded email address in a data-contact attribute is very ""one-off"" 鈥?as long as the scraper is not specifically designed for your site, it should work.\n', '\nSimple + Lot of @ + Editable without tools\n\n\n<a href=""mailto:user@domain@@com""\r\n   onmouseover=""this.href=this.href.replace(\'@@\',\'.\')"">\r\n   Send email\r\n</a>\n\n\n', '\nHave you considered using google\'s recaptcha mailhide?\nhttps://www.google.com/recaptcha/admin#mailhide\nThe idea is that when a user clicks the checkbox (see nocaptcha below), the full e-mail address is displayed.\nWhile recaptcha is traditionally not only hard for screen readers but also humans as well, with the roleout of google\'s nocaptcha recaptcha which you can read about\nhere as they relate to accessibility tests. It appears to show promise with to screen readers as it renders as a traditional checkbox from their view.\n\nExample #1 - Not secure but for easy illustration of the idea\nHere is some code as an example without using mailhide but implementing something using recaptcha yourself: https://jsfiddle.net/43fad8pf/36/\n<div class=""container"">\n    <div id=""recaptcha""></div>\n</div>\n<div id=""email"">\n    Verify captcha to get e-mail\n</div>\n\nfunction createRecaptcha() {\n    grecaptcha.render(""recaptcha"", {sitekey: ""6LcgSAMTAAAAACc2C7rc6HB9ZmEX4SyB0bbAJvTG"", theme: ""light"", callback: showEmail});\n}\n createRecaptcha();\n\nfunction showEmail() {\n    // ideally you would do server side verification of the captcha and then the server would return the e-mail\n  document.getElementById(""email"").innerHTML = ""email@example.com"";\n}\n\nNote: In my example I have the e-mail in a JavaScript function. Ideally you would have the recaptcha validated on the server end, and return the e-mail, otherwise the bot can simply get it in the code.\nExample #2 - Server side validation and returning of e-mail\nIf we use an example more like this, we get additional security: https://designracy.com/recaptcha-using-ajax-php-and-jquery/\nfunction showEmail() {\n    /* Check if the captcha is complete */\n    if ($(""#g-recaptcha-response"").val()) {\n        $.ajax({\n            type: 鈥楶OST鈥?\n            url: ""verify.php"", // The file we鈥檙e making the request to\n            dataType: 鈥榟tml鈥?\n            async: true,\n            data: {\n                captchaResponse: $(""#g-recaptcha-response"").val() // The generated response from the widget sent as a POST parameter\n        },\n        success: function (data) {\n            alert(""everything looks ok. Here is where we would take \'data\' which contains the e-mail and put it somewhere in the document"");\n        },\n        error: function (XMLHttpRequest, textStatus, errorThrown) {\n            alert(""You鈥檙e a bot"");\n        }\n    });\n} else {\n    alert(""Please fill the captcha!"");\n}\n});\n\nWhere verify.php is:\n$captcha = filter_input(INPUT_POST, 鈥榗aptchaResponse鈥?; // get the captchaResponse parameter sent from our ajax\n\n/* Check if captcha is filled */\nif (!$captcha) {\n    http_response_code(401); // Return error code if there is no captcha\n}\n$response =     file_get_contents(""https://www.google.com/recaptcha/api/siteverify?secret=YOUR-SECRET-KEY-HERE&amp;amp;response="" . $captcha);\nif ($response . success == false) {\necho 鈥楽PAM鈥?\nhttp_response_code(401); // It鈥檚 SPAM! RETURN SOME KIND OF ERROR\n} else {\n// Everything is ok, should output this in json or something better, but this is an example\n    echo \'email@example.com\';\n}\n\n', '\nPeople who write scrapers want to make their scrapers as efficient as possible. Therefore, they won\'t download styles, scripts, and other external resources. There\'s no method that I know of to set a mailto link using CSS. In addition, you specifically said you didn\'t want to set the link using Javascript.\nIf you think about what other types of resources there are, there\'s also external documents (i.e. HTML documents using iframes). Almost no scrapers would bother downloading the contents of iframes. Therefore, you can simply do:\nindex.html:\n<iframe src=""frame.html"" style=""height: 1em; width: 100%; border: 0;""></iframe>\n\nframe.html:\nMy email is <a href=""mailto:me@example.com"" target=""_top"">me@example.com</a>\n\nTo human users, the iframe looks just like normal text. Iframes are inline and transparent by default, so we just need set its border and dimensions. You can\'t make the size of the iframe match its content\'s size without using Javascript, so the best we can do is giving it predefined dimensions.\n', ""\nFirst, I don't think doing anything with CSS will work. All bots (except Google's crawler) simply ignore all styling on websites. Any solution has to work with JS or server-side.\nA server-side solution could be making an <a> that links to a new tab, which simply redirects to the desired mailto:\nThat's all my ideas for now. Hope it helps.\n"", ""\nShort answer to fulfill all your requirements is that it's impossible\nSome of the script-based options answered here may work for certain bots, but you wanted no-script, so, no, you can't. \n"", '\nbased on the code of MaanooAk, here is my version:\n\n\n<a href=""mailto: Mike Myers""\nonclick=""this.href=this.href.replace(\' Mike \',\'MikeMy\'); this.href=this.href.replace(\'Myers\',\'ers@vwx.yz\')"">&#9993; Send Email</a>\n\n\nThe difference to MaanookAks version is, that on hover you don\'t see mailto: and a broken email adress but mailto: and the name of contact. And when you click on it, the name is replaced by the email adress.\nIn the code the email adress is splitted into two parts. Nowhere in the code the email adress is visible complete.\n', ""\nHere is my new solution for this. I first build the email adress string by addition of small pieces and then use this string also as title:\n\n\nadress = 'mailt' + 'o:MikeM' + 'yers@v' + 'wx.yz';\ndocument.getElementsByClassName('Email')[0].title = adress;\nfunction mail(){window.location.href = adress;}\n<a class='Email' onclick='mail()'>&#9993; Send Email</a>\n\n\nI use this in a footer of a website. Many pages with all the same footer.\n"", '\nPHP solution\nfunction printEmail($email){\n    $email = \'<a href=""mailto:\'.$email.\'"">\'.$email.\'</a>\';\n    $a = str_split($email);\n    return ""<script>document.write(\'"".implode(""\'+\'"",$a).""\');</script>"";\n}\n\nUse\necho printEmail(\'test@example.com\');\n\nResult\n<script>document.write(\'<\'+\'a\'+\' \'+\'h\'+\'r\'+\'e\'+\'f\'+\'=\'+\'""\'+\'m\'+\'a\'+\'i\'+\'l\'+\'t\'+\'o\'+\':\'+\'t\'+\'e\'+\'s\'+\'t\'+\'@\'+\'g\'+\'m\'+\'a\'+\'i\'+\'l\'+\'.\'+\'c\'+\'o\'+\'m\'+\'""\'+\'>\'+\'t\'+\'e\'+\'s\'+\'t\'+\'@\'+\'g\'+\'m\'+\'a\'+\'i\'+\'l\'+\'.\'+\'c\'+\'o\'+\'m\'+\'<\'+\'/\'+\'a\'+\'>\');</script>\n\nP.S. Requirement: user must have JavaScript enabled\n', '\nThe one method I found effective is using it with CSS like below:\n<a href=""mailto:myemail@ignore-domain.com"">myemail@<span style=""display:none;"">ignore-</span>example.com\nand then write a JavaScript to remove the ignoreme- word from the href=""mailto:..."" attribute with regex. This will hide email from bot as it will append ignore- word before real domain and this will work on screen reader and when user clicks on the link custom JS function will remove the ignore- word from href attribute so it will open the real email.\nThis method has been working very effectively for me till date. you can read more on this - http://techblog.tilllate.com/2008/07/20/ten-methods-to-obfuscate-e-mail-addresses-compared/\n']",https://stackoverflow.com/questions/41318987/hide-email-address-from-bots-keep-mailto,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Designing a web crawler,"
I have come across an interview question ""If you were designing a web crawler, how would you avoid getting into infinite loops? "" and I am trying to answer it.
How does it all begin from the beginning.
Say Google started with some hub pages say hundreds of them (How these hub pages were found in the first place is a different sub-question).
As Google follows links from a page and so on, does it keep making  a hash table to make sure that it doesn't follow the earlier visited pages.
What if the same page has 2 names (URLs) say in these days when we have URL shorteners etc..
I have taken Google as an example. Though Google doesn't leak how its web crawler algorithms and page ranking etc work, but any guesses?
",46k,"
            74
        ","['\nIf you want to get a detailed answer take a look at section 3.8 this paper, which describes the URL-seen test of a modern scraper:\n\nIn the course of extracting links, any\nWeb crawler will encounter multiple\nlinks to the same document. To avoid\ndownloading and processing a document\nmultiple times, a URL-seen test must\nbe performed on each extracted link\nbefore adding it to the URL frontier.\n(An alternative design would be to\ninstead perform the URL-seen test when\nthe URL is removed from the frontier,\nbut this approach would result in a\nmuch larger frontier.)\nTo perform the\nURL-seen test, we store all of the\nURLs seen by Mercator in canonical\nform in a large table called the URL\nset. Again, there are too many entries\nfor them all to fit in memory, so like\nthe document fingerprint set, the URL\nset is stored mostly on disk.\nTo save\nspace, we do not store the textual\nrepresentation of each URL in the URL\nset, but rather a fixed-sized\nchecksum. Unlike the fingerprints\npresented to the content-seen test鈥檚\ndocument fingerprint set, the stream\nof URLs tested against the URL set has\na non-trivial amount of locality. To\nreduce the number of operations on the\nbacking disk file, we therefore keep\nan in-memory cache of popular URLs.\nThe intuition for this cache is that\nlinks to some URLs are quite common,\nso caching the popular ones in memory\nwill lead to a high in-memory hit\nrate.\nIn fact, using an in-memory\ncache of 2^18 entries and the LRU-like\nclock replacement policy, we achieve\nan overall hit rate on the in-memory\ncache of 66.2%, and a hit rate of 9.5%\non the table of recently-added URLs,\nfor a net hit rate of 75.7%. Moreover,\nof the 24.3% of requests that miss in\nboth the cache of popular URLs and the\ntable of recently-added URLs, about\n1=3 produce hits on the buffer in our\nrandom access file implementation,\nwhich also resides in user-space. The\nnet result of all this buffering is\nthat each membership test we perform\non the URL set results in an average\nof 0.16 seek and 0.17 read kernel\ncalls (some fraction of which are\nserved out of the kernel鈥檚 file system\nbuffers). So, each URL set membership\ntest induces one-sixth as many kernel\ncalls as a membership test on the\ndocument fingerprint set. These\nsavings are purely due to the amount\nof URL locality (i.e., repetition of\npopular URLs) inherent in the stream\nof URLs encountered during a crawl.\n\nBasically they hash all of the URLs with a hashing function that guarantees unique hashes for each URL and due to the locality of URLs, it becomes very easy to find URLs. Google even open-sourced their hashing function: CityHash\nWARNING!\nThey might also be talking about bot traps!!! A bot trap is a section of a page that keeps generating new links with unique URLs and you will essentially get trapped in an ""infinite loop"" by following the links that are being served by that page. This is not exactly a loop, because a loop would be the result of visiting the same URL, but it\'s an infinite chain of URLs which you should avoid crawling.\nUpdate 12/13/2012- the day after the world was supposed to end :)\nPer Fr0zenFyr\'s comment: if one uses the AOPIC algorithm for selecting pages, then it\'s fairly easy to avoid bot-traps of the infinite loop kind. Here is a summary of how AOPIC works:\n\nGet a set of N seed pages.\nAllocate X amount of credit to each page, such that each page has X/N credit (i.e. equal amount of credit) before crawling has started.\nSelect a page P, where the P has the highest amount of credit (or if all pages have the same amount of credit, then crawl a random page).\nCrawl page P (let\'s say that P had 100 credits when it was crawled).\nExtract all the links from page P (let\'s say there are 10 of them).\nSet the credits of P to 0.\nTake a 10% ""tax"" and allocate it to a Lambda page.\nAllocate an equal amount of credits each link found on page P from P\'s original credit - the tax: so (100 (P credits) - 10 (10% tax))/10 (links) = 9 credits per each link.\nRepeat from step 3.\n\nSince the Lambda page continuously collects tax, eventually it will be the page with the largest amount of credit and we\'ll have to ""crawl"" it. I say ""crawl"" in quotes, because we don\'t actually make an HTTP request for the Lambda page, we just take its credits and distribute them equally to all of the pages in our database.\nSince bot traps only give internal links credits and they rarely get credit from the outside, they will continually leak credits (from taxation) to the Lambda page. The Lambda page will distribute that credits out to all of the pages in the database evenly and upon each cycle the bot trap page will lose more and more credits, until it has so little credits that it almost never gets crawled again. This will not happen with good pages, because they often get credits from back-links found on other pages. This also results in a dynamic page rank and what you will notice is that any time you take a snapshot of your database, order the pages by the amount of credits they have, then they will most likely be ordered roughly according to their true page rank.\nThis only avoid bot traps of the infinite-loop kind, but there are many other bot traps which you should watch out for and there are ways to get around them too.\n', ""\nWhile everybody here already suggested how to create your web crawler, here is how how Google ranks pages.\nGoogle gives each page a rank based on the number of callback links (how many links on other websites point to a specific website/page). This is called relevance score. This is based on the fact that if a page has many other pages link to it, it's probably an important page.\nEach site/page is viewed as a node in a graph. Links to other pages are directed edges. A degree of a vertex is defined as the number of incoming edges. Nodes with a higher number of incoming edges are ranked higher.\nHere's how the PageRank is determined. Suppose that page Pj has Lj links. If one of those links is to page Pi, then Pj will pass on 1/Lj of its importance to Pi. The importance ranking of Pi is then the sum of all the contributions made by pages linking to it. So if we denote the set of pages linking to Pi by Bi, then we have this formula:\nImportance(Pi)= sum( Importance(Pj)/Lj ) for all links from Pi to Bi\n\nThe ranks are placed in a matrix called hyperlink matrix: H[i,j]\nA row in this matrix is either 0, or 1/Lj if there is a link from Pi to Bi. Another property of this matrix is that if we sum all rows in a column we get 1. \nNow we need multiply this matrix by an Eigen vector, named I (with eigen value 1) such that:\nI = H*I\n\nNow we start iterating: IH, IIH, IIIH .... I^k *H until the solution converges. ie we get pretty much the same numbers in the matrix in step k and k+1.\nNow whatever is left in the I vector is the importance of each page.\nFor a simple class homework example see http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html\n\nAs for solving the duplicate issue in your interview question, do a checksum on the entire page and use either that or a bash of the checksum as your key in a map to keep track of visited pages.\n\n"", ""\nDepends on how deep their question was intended to be.  If they were just trying to avoid following the same links back and forth, then hashing the URL's would be sufficient.\nWhat about content that has literally thousands of URL's that lead to the same content?  Like a QueryString parameter that doesn't affect anything, but can have an infinite number of iterations.  I suppose you could hash the contents of the page as well and compare URL's to see if they are similar to catch content that is identified by multiple URL's.  See for example, Bot Traps mentioned in @Lirik's post.\n"", ""\nYou'd have to have some sort of hash table to store the results in, you'd just have to check it before each page load.   \n"", '\nThe problem here is not to crawl duplicated URLS, wich is resolved by a index using a hash obtained from urls.  The problem is to crawl DUPLICATED CONTENT.  Each url of a ""Crawler Trap"" is different (year, day, SessionID...).\nThere is not a ""perfect"" solution... but you can use some of this strategies:\n鈥?Keep a field of wich level the url is inside the website. For each cicle of getting urls from a page, increase the level. It will be like a tree.  You can stop to crawl at certain level, like 10 (i think google use this). \n鈥?You can try to create a kind of HASH wich can be compared to find similar documents, since you cant compare with each document in your database. There are SimHash from google, but i could not find any implementation to use. Then i麓ve created my own. My hash count low and high frequency characters inside the html code and generate a 20bytes hash, wich is compared with a small cache of last crawled pages inside a AVLTree with an NearNeighbors search with some tolerance (about 2). You cant use any reference to characters locations in this hash. After ""recognize"" the trap, you can record the url pattern of the duplicate content and start to ignore pages with that too.\n鈥?Like google, you can create a ranking to each website and ""trust"" more in one than others.\n', '\nThe web crawler is a computer program which used to collect/crawling following key values(HREF links, Image links, Meta Data .etc) from given website URL. It is designed like intelligent to follow different HREF links which are already fetched from the previous URL, so in this way, Crawler can jump from one website to other websites. Usually, it called as a Web spider or Web Bot. This mechanism always acts as a backbone of the Web search engine.\nPlease find the source code from my tech blog - http://www.algonuts.info/how-to-built-a-simple-web-crawler-in-php.html\n<?php\nclass webCrawler\n{\n    public $siteURL;\n    public $error;\n\n    function __construct()\n    {\n        $this->siteURL = """";\n        $this->error = """";\n    }\n\n    function parser()   \n    {\n        global $hrefTag,$hrefTagCountStart,$hrefTagCountFinal,$hrefTagLengthStart,$hrefTagLengthFinal,$hrefTagPointer;\n        global $imgTag,$imgTagCountStart,$imgTagCountFinal,$imgTagLengthStart,$imgTagLengthFinal,$imgTagPointer;\n        global $Url_Extensions,$Document_Extensions,$Image_Extensions,$crawlOptions;\n\n        $dotCount = 0;\n        $slashCount = 0;\n        $singleSlashCount = 0;\n        $doubleSlashCount = 0;\n        $parentDirectoryCount = 0;\n\n        $linkBuffer = array();\n\n        if(($url = trim($this->siteURL)) != """")\n        {\n            $crawlURL = rtrim($url,""/"");\n            if(($directoryURL = dirname($crawlURL)) == ""http:"")\n            {   $directoryURL = $crawlURL;  }\n            $urlParser = preg_split(""/\\//"",$crawlURL);\n\n            //-- Curl Start --\n            $curlObject = curl_init($crawlURL);\n            curl_setopt_array($curlObject,$crawlOptions);\n            $webPageContent = curl_exec($curlObject);\n            $errorNumber = curl_errno($curlObject);\n            curl_close($curlObject);\n            //-- Curl End --\n\n            if($errorNumber == 0)\n            {\n                $webPageCounter = 0;\n                $webPageLength = strlen($webPageContent);\n                while($webPageCounter < $webPageLength)\n                {\n                    $character = $webPageContent[$webPageCounter];\n                    if($character == """")\n                    {   \n                        $webPageCounter++;  \n                        continue;\n                    }\n                    $character = strtolower($character);\n                    //-- Href Filter Start --\n                    if($hrefTagPointer[$hrefTagLengthStart] == $character)\n                    {\n                        $hrefTagLengthStart++;\n                        if($hrefTagLengthStart == $hrefTagLengthFinal)\n                        {\n                            $hrefTagCountStart++;\n                            if($hrefTagCountStart == $hrefTagCountFinal)\n                            {\n                                if($hrefURL != """")\n                                {\n                                    if($parentDirectoryCount >= 1 || $singleSlashCount >= 1 || $doubleSlashCount >= 1)\n                                    {\n                                        if($doubleSlashCount >= 1)\n                                        {   $hrefURL = ""http://"".$hrefURL;  }\n                                        else if($parentDirectoryCount >= 1)\n                                        {\n                                            $tempData = 0;\n                                            $tempString = """";\n                                            $tempTotal = count($urlParser) - $parentDirectoryCount;\n                                            while($tempData < $tempTotal)\n                                            {\n                                                $tempString .= $urlParser[$tempData].""/"";\n                                                $tempData++;\n                                            }\n                                            $hrefURL = $tempString."""".$hrefURL;\n                                        }\n                                        else if($singleSlashCount >= 1)\n                                        {   $hrefURL = $urlParser[0].""/"".$urlParser[1].""/"".$urlParser[2].""/"".$hrefURL;  }\n                                    }\n                                    $host = """";\n                                    $hrefURL = urldecode($hrefURL);\n                                    $hrefURL = rtrim($hrefURL,""/"");\n                                    if(filter_var($hrefURL,FILTER_VALIDATE_URL) == true)\n                                    {   \n                                        $dump = parse_url($hrefURL);\n                                        if(isset($dump[""host""]))\n                                        {   $host = trim(strtolower($dump[""host""]));    }\n                                    }\n                                    else\n                                    {\n                                        $hrefURL = $directoryURL.""/"".$hrefURL;\n                                        if(filter_var($hrefURL,FILTER_VALIDATE_URL) == true)\n                                        {   \n                                            $dump = parse_url($hrefURL);    \n                                            if(isset($dump[""host""]))\n                                            {   $host = trim(strtolower($dump[""host""]));    }\n                                        }\n                                    }\n                                    if($host != """")\n                                    {\n                                        $extension = pathinfo($hrefURL,PATHINFO_EXTENSION);\n                                        if($extension != """")\n                                        {\n                                            $tempBuffer ="""";\n                                            $extensionlength = strlen($extension);\n                                            for($tempData = 0; $tempData < $extensionlength; $tempData++)\n                                            {\n                                                if($extension[$tempData] != ""?"")\n                                                {   \n                                                    $tempBuffer = $tempBuffer.$extension[$tempData];\n                                                    continue;\n                                                }\n                                                else\n                                                {\n                                                    $extension = trim($tempBuffer);\n                                                    break;\n                                                }\n                                            }\n                                            if(in_array($extension,$Url_Extensions))\n                                            {   $type = ""domain"";   }\n                                            else if(in_array($extension,$Image_Extensions))\n                                            {   $type = ""image"";    }\n                                            else if(in_array($extension,$Document_Extensions))\n                                            {   $type = ""document""; }\n                                            else\n                                            {   $type = ""unknown"";  }\n                                        }\n                                        else\n                                        {   $type = ""domain"";   }\n\n                                        if($hrefURL != """")\n                                        {\n                                            if($type == ""domain"" && !in_array($hrefURL,$this->linkBuffer[""domain""]))\n                                            {   $this->linkBuffer[""domain""][] = $hrefURL;   }\n                                            if($type == ""image"" && !in_array($hrefURL,$this->linkBuffer[""image""]))\n                                            {   $this->linkBuffer[""image""][] = $hrefURL;    }\n                                            if($type == ""document"" && !in_array($hrefURL,$this->linkBuffer[""document""]))\n                                            {   $this->linkBuffer[""document""][] = $hrefURL; }\n                                            if($type == ""unknown"" && !in_array($hrefURL,$this->linkBuffer[""unknown""]))\n                                            {   $this->linkBuffer[""unknown""][] = $hrefURL;  }\n                                        }\n                                    }\n                                }\n                                $hrefTagCountStart = 0;\n                            }\n                            if($hrefTagCountStart == 3)\n                            {\n                                $hrefURL = """";\n                                $dotCount = 0;\n                                $slashCount = 0;\n                                $singleSlashCount = 0;\n                                $doubleSlashCount = 0;\n                                $parentDirectoryCount = 0;\n                                $webPageCounter++;\n                                while($webPageCounter < $webPageLength)\n                                {\n                                    $character = $webPageContent[$webPageCounter];\n                                    if($character == """")\n                                    {   \n                                        $webPageCounter++;  \n                                        continue;\n                                    }\n                                    if($character == ""\\"""" || $character == ""\'"")\n                                    {\n                                        $webPageCounter++;\n                                        while($webPageCounter < $webPageLength)\n                                        {\n                                            $character = $webPageContent[$webPageCounter];\n                                            if($character == """")\n                                            {   \n                                                $webPageCounter++;  \n                                                continue;\n                                            }\n                                            if($character == ""\\"""" || $character == ""\'"" || $character == ""#"")\n                                            {   \n                                                $webPageCounter--;  \n                                                break;  \n                                            }\n                                            else if($hrefURL != """")\n                                            {   $hrefURL .= $character; }\n                                            else if($character == ""."" || $character == ""/"")\n                                            {\n                                                if($character == ""."")\n                                                {\n                                                    $dotCount++;\n                                                    $slashCount = 0;\n                                                }\n                                                else if($character == ""/"")\n                                                {\n                                                    $slashCount++;\n                                                    if($dotCount == 2 && $slashCount == 1)\n                                                    $parentDirectoryCount++;\n                                                    else if($dotCount == 0 && $slashCount == 1)\n                                                    $singleSlashCount++;\n                                                    else if($dotCount == 0 && $slashCount == 2)\n                                                    $doubleSlashCount++;\n                                                    $dotCount = 0;\n                                                }\n                                            }\n                                            else\n                                            {   $hrefURL .= $character; }\n                                            $webPageCounter++;\n                                        }\n                                        break;\n                                    }\n                                    $webPageCounter++;\n                                }\n                            }\n                            $hrefTagLengthStart = 0;\n                            $hrefTagLengthFinal = strlen($hrefTag[$hrefTagCountStart]);\n                            $hrefTagPointer =& $hrefTag[$hrefTagCountStart];\n                        }\n                    }\n                    else\n                    {   $hrefTagLengthStart = 0;    }\n                    //-- Href Filter End --\n                    //-- Image Filter Start --\n                    if($imgTagPointer[$imgTagLengthStart] == $character)\n                    {\n                        $imgTagLengthStart++;\n                        if($imgTagLengthStart == $imgTagLengthFinal)\n                        {\n                            $imgTagCountStart++;\n                            if($imgTagCountStart == $imgTagCountFinal)\n                            {\n                                if($imgURL != """")\n                                {\n                                    if($parentDirectoryCount >= 1 || $singleSlashCount >= 1 || $doubleSlashCount >= 1)\n                                    {\n                                        if($doubleSlashCount >= 1)\n                                        {   $imgURL = ""http://"".$imgURL;    }\n                                        else if($parentDirectoryCount >= 1)\n                                        {\n                                            $tempData = 0;\n                                            $tempString = """";\n                                            $tempTotal = count($urlParser) - $parentDirectoryCount;\n                                            while($tempData < $tempTotal)\n                                            {\n                                                $tempString .= $urlParser[$tempData].""/"";\n                                                $tempData++;\n                                            }\n                                            $imgURL = $tempString."""".$imgURL;\n                                        }\n                                        else if($singleSlashCount >= 1)\n                                        {   $imgURL = $urlParser[0].""/"".$urlParser[1].""/"".$urlParser[2].""/"".$imgURL;    }\n                                    }\n                                    $host = """";\n                                    $imgURL = urldecode($imgURL);\n                                    $imgURL = rtrim($imgURL,""/"");\n                                    if(filter_var($imgURL,FILTER_VALIDATE_URL) == true)\n                                    {   \n                                        $dump = parse_url($imgURL); \n                                        $host = trim(strtolower($dump[""host""]));\n                                    }\n                                    else\n                                    {\n                                        $imgURL = $directoryURL.""/"".$imgURL;\n                                        if(filter_var($imgURL,FILTER_VALIDATE_URL) == true)\n                                        {   \n                                            $dump = parse_url($imgURL); \n                                            $host = trim(strtolower($dump[""host""]));\n                                        }   \n                                    }\n                                    if($host != """")\n                                    {\n                                        $extension = pathinfo($imgURL,PATHINFO_EXTENSION);\n                                        if($extension != """")\n                                        {\n                                            $tempBuffer ="""";\n                                            $extensionlength = strlen($extension);\n                                            for($tempData = 0; $tempData < $extensionlength; $tempData++)\n                                            {\n                                                if($extension[$tempData] != ""?"")\n                                                {   \n                                                    $tempBuffer = $tempBuffer.$extension[$tempData];\n                                                    continue;\n                                                }\n                                                else\n                                                {\n                                                    $extension = trim($tempBuffer);\n                                                    break;\n                                                }\n                                            }\n                                            if(in_array($extension,$Url_Extensions))\n                                            {   $type = ""domain"";   }\n                                            else if(in_array($extension,$Image_Extensions))\n                                            {   $type = ""image"";    }\n                                            else if(in_array($extension,$Document_Extensions))\n                                            {   $type = ""document""; }\n                                            else\n                                            {   $type = ""unknown"";  }\n                                        }\n                                        else\n                                        {   $type = ""domain"";   }\n\n                                        if($imgURL != """")\n                                        {\n                                            if($type == ""domain"" && !in_array($imgURL,$this->linkBuffer[""domain""]))\n                                            {   $this->linkBuffer[""domain""][] = $imgURL;    }\n                                            if($type == ""image"" && !in_array($imgURL,$this->linkBuffer[""image""]))\n                                            {   $this->linkBuffer[""image""][] = $imgURL; }\n                                            if($type == ""document"" && !in_array($imgURL,$this->linkBuffer[""document""]))\n                                            {   $this->linkBuffer[""document""][] = $imgURL;  }\n                                            if($type == ""unknown"" && !in_array($imgURL,$this->linkBuffer[""unknown""]))\n                                            {   $this->linkBuffer[""unknown""][] = $imgURL;   }\n                                        }\n                                    }\n                                }\n                                $imgTagCountStart = 0;\n                            }\n                            if($imgTagCountStart == 3)\n                            {\n                                $imgURL = """";\n                                $dotCount = 0;\n                                $slashCount = 0;\n                                $singleSlashCount = 0;\n                                $doubleSlashCount = 0;\n                                $parentDirectoryCount = 0;\n                                $webPageCounter++;\n                                while($webPageCounter < $webPageLength)\n                                {\n                                    $character = $webPageContent[$webPageCounter];\n                                    if($character == """")\n                                    {   \n                                        $webPageCounter++;  \n                                        continue;\n                                    }\n                                    if($character == ""\\"""" || $character == ""\'"")\n                                    {\n                                        $webPageCounter++;\n                                        while($webPageCounter < $webPageLength)\n                                        {\n                                            $character = $webPageContent[$webPageCounter];\n                                            if($character == """")\n                                            {   \n                                                $webPageCounter++;  \n                                                continue;\n                                            }\n                                            if($character == ""\\"""" || $character == ""\'"" || $character == ""#"")\n                                            {   \n                                                $webPageCounter--;  \n                                                break;  \n                                            }\n                                            else if($imgURL != """")\n                                            {   $imgURL .= $character;  }\n                                            else if($character == ""."" || $character == ""/"")\n                                            {\n                                                if($character == ""."")\n                                                {\n                                                    $dotCount++;\n                                                    $slashCount = 0;\n                                                }\n                                                else if($character == ""/"")\n                                                {\n                                                    $slashCount++;\n                                                    if($dotCount == 2 && $slashCount == 1)\n                                                    $parentDirectoryCount++;\n                                                    else if($dotCount == 0 && $slashCount == 1)\n                                                    $singleSlashCount++;\n                                                    else if($dotCount == 0 && $slashCount == 2)\n                                                    $doubleSlashCount++;\n                                                    $dotCount = 0;\n                                                }\n                                            }\n                                            else\n                                            {   $imgURL .= $character;  }\n                                            $webPageCounter++;\n                                        }\n                                        break;\n                                    }\n                                    $webPageCounter++;\n                                }\n                            }\n                            $imgTagLengthStart = 0;\n                            $imgTagLengthFinal = strlen($imgTag[$imgTagCountStart]);\n                            $imgTagPointer =& $imgTag[$imgTagCountStart];\n                        }\n                    }\n                    else\n                    {   $imgTagLengthStart = 0; }\n                    //-- Image Filter End --\n                    $webPageCounter++;\n                }\n            }\n            else\n            {   $this->error = ""Unable to proceed, permission denied"";  }\n        }\n        else\n        {   $this->error = ""Please enter ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"rl"""";  }\n\n        if($this->error != """""""")\n        {   $this->linkBuffer[""""error""""] = $this->error;  }\n\n        return $this->linkBuffer;\n    }   \n}\n?>\n\n'"," """"\nWell the web is basically a directed graph"," so you can construct a graph out of the urls and then do a BFS or DFS traversal while marking the visited nodes so you don't visit the same page twice. \n"""""," '\nThis is a web crawler example. Which can be used to collect mac Addresses for mac spoofing. \n#!/usr/bin/env python\n\nimport sys\nimport os\nimport urlparse\nimport urllib\nfrom bs4 import BeautifulSoup\n\ndef mac_addr_str(f_data):\nglobal fptr\nglobal mac_list\nword_array = f_data.split("""" """")\n\n    for word in word_array:\n        if len(word) == 17 and \':\' in word[2] and \':\' in word[5] and \':\' in word[8] and \':\' in word[11] and \':\' in word[14]:\n            if word not in mac_list:\n                mac_list.append(word)\n                fptr.writelines(word +""""\\n"""")\n                print word\n\n\n\nurl = """"http://stackoverflow.com/questions/tagged/mac-address""""\n\nurl_list = [url]\nvisited = [url]\npwd = os.getcwd();\npwd = pwd + """"/internet_mac.txt"""";\n\nfptr = open(pwd"," """"a"""")\nmac_list = []\n\nwhile len(url_list) > 0:\n    try:\n        htmltext = urllib.urlopen(url_list[0]).read()\n    except:\n        url_list[0]\n    mac_addr_str(htmltext)\n    soup = BeautifulSoup(htmltext)\n    url_list.pop(0)\n    for tag in soup.findAll(\'a\'",href=True):\n        tag[\'href\'] = urlparse.urljoin(url,"tag[\'href\'])\n        if url in tag[\'href\'] and tag[\'href\'] not in visited:\n            url_list.append(tag[\'href\'])\n            visited.append(tag[\'href\'])\n\nChange the url to crawl more sites......good luck\n']""",https://stackoverflow.com/questions/5834808/designing-a-web-crawler,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrapy Python Set up User Agent,"
I tried to override the user-agent of my crawlspider by adding an extra line to the project configuration file. Here is the code:
[settings]
default = myproject.settings
USER_AGENT = ""Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36""


[deploy]
#url = http://localhost:6800/
project = myproject

But when I run the crawler against my own web, I notice the spider did not pick up my customized user agent but the default one ""Scrapy/0.18.2 (+http://scrapy.org)"". 
Can any one explain what I have done wrong. 
Note:
(1). It works when I tried to override the user agent globally: 
scrapy crawl myproject.com -o output.csv -t csv -s USER_AGENT=""Mozilla....""

(2). When I remove the line ""default = myproject.setting"" from the configuration file, and run scrapy crawl myproject.com, it says ""cannot find spider.."", so I feel like the default setting should not be removed in this case.
Thanks a lot for the help in advance.                            
",53k,"
            41
        ","['\nMove your USER_AGENT line to the settings.py file, and not in your scrapy.cfg file. settings.py should be at same level as items.py if you use scrapy startproject command, in your case  it should be something like myproject/settings.py\n', ""\nJust in case anyone lands here that manually controls the scrapy crawl. i.e. you do not use the scrapy crawl process from the shell...\n$ scrapy crawl myproject\n\nBut insted you use CrawlerProcess() or CrawlerRunner()...\nprocess = CrawlerProcess()\n\nor \nprocess = CrawlerRunner()\n\nthen the user agent, along with other settings, can be passed to the crawler in a dictionary of configuration variables. \nLike this...\n    process = CrawlerProcess(\n            {\n                'USER_AGENT': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n            }\n    )\n\n""]",https://stackoverflow.com/questions/18920930/scrapy-python-set-up-user-agent,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Difference between find and filter in jquery,"
I'm working on fetching data from wiki pages. I'm using a combination of php and jquery to do this. First I am using curl in php to fetch page contents and echoing the content. The filename is content.php:
$url = $_GET['url'];
$url = trim($url,"" "");
$url = urldecode($url);
$url = str_replace("" "",""%20"",$url);

echo ""<a class='urlmax'>"".$_GET['title'].""</a>"";
echo crawl($url);

Then jQuery is used to find the matched elements. 
$.get(""content.php"",{url:""http://en.wikipedia.org/w/index.php?action=render&title=""+str_replace("" "",""_"",data[x]),title:str_replace("" "",""_"",data[x])},function(hdata){
                        var imgs = $(hdata).find('a.image img');
                        var ent = $(hdata).filter('a.urlmax');


                        ent = $(ent[0]).text();


});

I was able to successfully get images but for the variable ent when I use find instead of filter, it's returning an empty array. Only filter is working. Why is this?
Edit: I know the basic difference between find and filter. Here both the a.image img and a.urlmax are descendats of the hdata. Then why find does not work on a.urlmax. Not a.urlmax alone it's not working on any other class or id
",26k,"
            29
        ","[""\n.find()\nhttp://api.jquery.com/find/\n\nGet the descendants of each element in the current set of matched elements, filtered by a selector, jQuery object, or element.\n\nFilter, on the other hand, works on the currently matched elements. That's why filter worked but find did not (you needed to look at the current element).\n"", '\nfilter will select subset of element from the selected element\nfind will select descendent/children of selected element\nTo make it more clear filter will search through all element whereas find will search only in the descendent list\n', '\n.find()\nIt will returns descendant elements of the selected element.\nExemple (jsfiddle):\n    <style>\n      .Cell{\n         margin: 15px;\n         width: 400px;\n         border: 2px solid lightgrey;\n    \n      }\n      .Cell * {\n         display: block;\n         border: 2px solid lightgrey;\n         color: lightgrey;\n         padding: 5px;\n         margin: 10px;\n    }\n    </style>\n\n    <div class=\'Cell Plus\'>div (1)\n      <div class=\'Plus\'>Child</div>\n    </div>\n    \n    <div class=\'Cell Plus\'>div (2)\n      <div class=\'Plus\'>Child</div>\n    </div>\n\nJS:\n$("".Cell"").find("".Plus"").css({""color"": ""red"", ""border"": ""2px solid red""});\n\nResult:\n\n\n.filter()\nIt will returns elements that match a certain criteria.\nUsing the same html above:\n$("".Cell"").filter("".Plus"").css({""color"": ""red"", ""border"": ""2px solid red""});\n\nResult:\n\n']",https://stackoverflow.com/questions/10378757/difference-between-find-and-filter-in-jquery,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What are some good Ruby-based web crawlers? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



I am looking at writing my own, but I am wondering if there are any good web crawlers out there which are written in Ruby.
Short of a full-blown web crawler, any gems that might be helpful in building a web crawler would be useful. I know this part of the question is touched upon in a couple of places, but a list of gems applicable to building a web crawler would be a great resource as well.
",22k,"
            21
        ","[""\nI used to write spiders, page scrapers and site analyzers for my job, and still write them periodically to scratch some itch I get.\nRuby has some excellent gems to make it easy:\n\nNokogiri is my #1 choice for the HTML parser. I used to use Hpricot, but found some sites that made it explode in flames. I switched to Nokogiri afterwards and have been very happy with it. I regularly use it for parsing HTML, RDF/RSS/Atom and XML. Ox looks interesting too, so that might be another candidate, though I find searching the DOM a lot easier than trying to walk through a big hash, such as what is returned by Ox.\n\nOpenURI is good as a simple HTTP client, but it can get in the way when you want to do more complex things or need to have multiple requests firing at once. I'd recommend looking at HTTPClient or Typhoeus with Hydra for modest to heavyweight jobs. Curb is good too, because it uses the cURL library, but the interface isn't as intuitive to me. It's worth looking at though. HTTPclient is also worth looking at, but I lean toward the previously mentioned ones.\nNote: OpenURI has some flaws and vulnerabilities that can affect unsuspecting programmers so it's fallen out of favor somewhat. RestClient is a very worthy successor.\n\nYou'll need a backing database, and some way to talk to it. This isn't a task for Rails per se, but you could use ActiveRecord, detached from Rails, to talk to the database. I've done that a couple times and it works all right. Instead, I really like Sequel for my ORM. It's very flexible in how it lets you talk to the database, from using straight SQL to using Sequel's ability to programmatically build a query, to modeling the database and using migrations. Once you have the database built, you could use Rails to act as a front-end to the data though.\n\nIf you are going to navigate sites in any way beyond simply grabbing pages and following links, you'll want to look at Mechanize. It makes it easy to fill out forms and submit pages. As an added bonus, you can grab the content of a page as a Nokogiri HTML document and parse away using Nokogiri's multitude of tricks.\n\nFor massaging/mangling URLs I really like Addressable::URI. It's more full-featured than the built-in URI module. One thing that URI does that's nice is it has the URI#extract method to scan a string for URLs. If that string happened to be the body of a web page it would be an alternate way of locating links, but its downside is you'll also get links to images, videos, ads, etc., and you'll have to filter those out, probably resulting in more work than if you use a parser and look for <a> tags exclusively. For that matter, Mechanize also has the links method which returns all the links in a page, but you'll still have to filter them to determine whether you want to follow or ignore them.\n\nIf you think you'll need to deal with Javascript manipulated pages, or pages that get their content dynamically from AJAX, you should look into using one of the WATIR variants. There are flavors for the different browsers on different OSes, such as Firewatir, Safariwatir and Operawatir, so you'll have to figure out what works for you.\n\nYou do NOT want to rely on keeping your list of URLs to visit, or visited URLs, in memory. Design a database schema and store that information there. Spend some time up front designing the schema, thinking about what things you'll want to know as you collect links on a site. SQLite3, MySQL and Postgres are all excellent choices, depending on how big you think your database needs will be. One of my site analyzers was custom designed to help us recommend SEO changes for a Fortune 50 company. It ran for over three weeks covering about twenty different sites before we had enough data and stopped it. Imagine what would have happened if we had a power-outage and all that data went in the bit-bucket.\n\n\nAfter all that you'll want to also make your code be aware of proper spidering etiquette: What are the key considerations when creating a web crawler?\n"", '\nI am building wombat, a Ruby DSL to crawl web pages and extract content. Check it out on github https://github.com/felipecsl/wombat\nIt is still in an early stage but is already functional with basic functionality. More stuff will be added really soon.\n', '\nSo you want a good Ruby-based web crawler?\nTry spider or anemone. Both have solid usage according to RubyGems download counts.\nThe other answers, so far, are detailed and helpful but they don\'t have a laser-like focus on the question, which asks for ruby libraries for web crawlers. It would seem that this distinction can get muddled: see my answer to ""Crawling vs. Web-Scraping?""\n', '\nTin Man\'s comprehensive list  is good but partly outdated for me.\nMost websites my customers deal with are heavily AJAX/Javascript dependent.\nI\'ve been using Watir / watir-webdriver / selenium for a few years too, but the overhead of having to load up a hidden web browser on the backend to render that DOM stuff just isn\'t viable, let alone that all this time they still haven\'t implemented a useable ""browser session reuse"" to let new code execution reuse an old browser in memory for this purpose, shooting down tickets that might have worked their way up the API layers eventually. (refering to https://code.google.com/p/selenium/issues/detail?id=18 ) **\nhttps://rubygems.org/gems/phantomjs\nis what we\'re migrating new projects over to now, to let the necessary data get rendered without even any sort of invisible Xvfb memory & CPU heavy web browser.\n** Alternative approaches also failed to pan out:\n\nhow to serialize an object using TCPServer inside?\nCan a watir browser object be re-used in a later Ruby process?\n\n', '\nIf you don\'t want to write your own, then use any ordinary web crawler.  There are dozens out there.\nIf you do want to write your own, then write your own. A web crawler isn\'t exactly a complicated activity, it consists of:\n\nDownloading a website.\nLocating URLs in that website, filtered however you dang well please.\nFor each URL in that website, repeat step 1.\n\nOh, and this seems to be a duplicate of ""Web crawler in ruby"".\n']",https://stackoverflow.com/questions/4981379/what-are-some-good-ruby-based-web-crawlers,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrapy - how to identify already scraped urls,"
Im using scrapy to crawl a news website on a daily basis. How do i  restrict scrapy from scraping already scraped URLs. Also is there any clear documentation or examples on SgmlLinkExtractor.
",10k,"
            15
        ","[""\nYou can actually do this quite easily with the scrapy snippet located here: http://snipplr.com/view/67018/middleware-to-avoid-revisiting-already-visited-items/\nTo use it, copy the code from the link and put it into some file in your scrapy project. \nTo reference it, add a line in your settings.py to reference it:\nSPIDER_MIDDLEWARES = { 'project.middlewares.ignore.IgnoreVisitedItems': 560 }\n\nThe specifics on WHY you pick the number that you do can be read up here: http://doc.scrapy.org/en/latest/topics/downloader-middleware.html\nFinally, you'll need to modify your items.py so that each item class has the following fields:\nvisit_id = Field()\nvisit_status = Field()\n\nAnd I think that's it.  The next time you run your spider it should automatically try to start avoiding the same sites.  \nGood luck!\n"", '\nThis is straight forward. Maintain all your previously crawled urls in python dict. So when you try to try them next time, see if that url is there in the dict. else crawl.\ndef load_urls(prev_urls):\n    prev = dict()\n    for url in prev_urls:\n        prev[url] = True\n    return prev\n\ndef fresh_crawl(prev_urls, new_urls):\n    for url in new_urls:\n        if url not in prev_urls:\n            crawl(url)\n    return\n\ndef main():\n    purls = load_urls(prev_urls)\n    fresh_crawl(purls, nurls)\n    return\n\nThe above code was typed in SO text editor aka browser. Might have syntax errors. You might also need to make a few changes. But the logic is there...\nNOTE: But beware that some websites constantly keep changing their content. So sometimes you might have to recrawl a particular webpage (i.e. same url) just to get the updated content.\n', ""\nI think jama22's answer is a little incomplete. \nIn the snippet if self.FILTER_VISITED in x.meta:, you can see that you require FILTER_VISITED in your Request instance in order for that request to be ignored. This is to ensure that you can differentiate between links that you want to traverse and move around and item links that well, you don't want to see again.\n"", '\nScrapy can auto-filter urls which are scraped, isn\'t it? Some different urls point to the same page will not be filtered, such as ""www.xxx.com/home/"" and ""www.xxx.com/home/index.html"".\n', ""\nFor today (2019), this post is the best answer for this problem. \nhttps://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016\nIt's a lib to handle MIDDLEWARES automatcally.\nHope to help someone. I've spent a lot of time seaching for this.\n""]",https://stackoverflow.com/questions/3871613/scrapy-how-to-identify-already-scraped-urls,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simple web crawler in C#,"
I have created a simple web crawler but I want to add the recursion function so that every page that is opened I can get the URLs in this page, but I have no idea how I can do that and I want also to include threads to make it faster.
Here is my code
namespace Crawler
{
    public partial class Form1 : Form
    {
        String Rstring;

        public Form1()
        {
            InitializeComponent();
        }

        private void button1_Click(object sender, EventArgs e)
        {
            
            WebRequest myWebRequest;
            WebResponse myWebResponse;
            String URL = textBox1.Text;

            myWebRequest =  WebRequest.Create(URL);
            myWebResponse = myWebRequest.GetResponse();//Returns a response from an Internet resource

            Stream streamResponse = myWebResponse.GetResponseStream();//return the data stream from the internet
                                                                       //and save it in the stream

            StreamReader sreader = new StreamReader(streamResponse);//reads the data stream
            Rstring = sreader.ReadToEnd();//reads it to the end
            String Links = GetContent(Rstring);//gets the links only
            
            textBox2.Text = Rstring;
            textBox3.Text = Links;
            streamResponse.Close();
            sreader.Close();
            myWebResponse.Close();




        }

        private String GetContent(String Rstring)
        {
            String sString="""";
            HTMLDocument d = new HTMLDocument();
            IHTMLDocument2 doc = (IHTMLDocument2)d;
            doc.write(Rstring);
            
            IHTMLElementCollection L = doc.links;
           
            foreach (IHTMLElement links in  L)
            {
                sString += links.getAttribute(""href"", 0);
                sString += ""/n"";
            }
            return sString;
        }

",69k,"
            13
        ","['\nI fixed your GetContent method as follow to get new links from crawled page:\npublic ISet<string> GetNewLinks(string content)\n{\n    Regex regexLink = new Regex(""(?<=<a\\\\s*?href=(?:\'|\\""))[^\'\\""]*?(?=(?:\'|\\""))"");\n\n    ISet<string> newLinks = new HashSet<string>();    \n    foreach (var match in regexLink.Matches(content))\n    {\n        if (!newLinks.Contains(match.ToString()))\n            newLinks.Add(match.ToString());\n    }\n\n    return newLinks;\n}\n\nUpdated\nFixed: regex should be regexLink. Thanks @shashlearner for pointing this out (my mistype).\n', '\ni have created something similar using Reactive Extension.\nhttps://github.com/Misterhex/WebCrawler\ni hope it can help you.\nCrawler crawler = new Crawler();\n\nIObservable observable = crawler.Crawl(new Uri(""http://www.codinghorror.com/""));\n\nobservable.Subscribe(onNext: Console.WriteLine, \nonCompleted: () => Console.WriteLine(""Crawling completed""));\n\n', '\nThe following includes an answer/recommendation.\nI believe you should use a dataGridView instead of a textBox as when you look at it in GUI it is easier to see the links (URLs) found.\nYou could change:\ntextBox3.Text = Links;\n\nto \n dataGridView.DataSource = Links;  \n\nNow for the question, you haven\'t included: \nusing System.  ""\'s""\n\nwhich ones were used, as it would be appreciated if I could get them as can\'t figure it out.\n', ""\nFrom a design standpoint, I've written a few webcrawlers.  Basically you want to implement a Depth First Search using a Stack data structure.   You can use Breadth First Search also, but you'll likely come into stack memory issues.  Good luck.\n""]",https://stackoverflow.com/questions/10452749/simple-web-crawler-in-c-sharp,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Asp.net Request.Browser.Crawler - Dynamic Crawler List?,"
I learned Why Request.Browser.Crawler is Always False in C# (http://www.digcode.com/default.aspx?page=ed51cde3-d979-4daf-afae-fa6192562ea9&article=bc3a7a4f-f53e-4f88-8e9c-c9337f6c05a0).
Does anyone uses some method to dynamically update the Crawler's list, so Request.Browser.Crawler will be really useful?
",10k,"
            8
        ","[""\nI've been happy the the results supplied by Ocean's Browsercaps. It supports crawlers that Microsoft's config files has not bothered detecting. It will even parse out what version of the crawler is on your site, not that I really need that level of detail.\n"", '\nYou could check (regex) against Request.UserAgent.\nPeter Bromberg wrote a nice article about writing an ASP.NET Request Logger and Crawler Killer in ASP.NET.\nHere is the method he uses in his Logger class:\npublic static bool IsCrawler(HttpRequest request)\n{\n   // set next line to ""bool isCrawler = false; to use this to deny certain bots\n   bool isCrawler = request.Browser.Crawler;\n   // Microsoft doesn\'t properly detect several crawlers\n   if (!isCrawler)\n   {\n       // put any additional known crawlers in the Regex below\n       // you can also use this list to deny certain bots instead, if desired:\n       // just set bool isCrawler = false; for first line in method \n       // and only have the ones you want to deny in the following Regex list\n       Regex regEx = new Regex(""Slurp|slurp|ask|Ask|Teoma|teoma"");\n       isCrawler = regEx.Match(request.UserAgent).Success;\n   }\n   return isCrawler;\n}\n\n']",https://stackoverflow.com/questions/431765/asp-net-request-browser-crawler-dynamic-crawler-list,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do I save the origin html file with Apache Nutch,"
I'm new to search engines and web crawlers. Now I want to store all the original pages in a particular web site as html files, but with Apache Nutch I can only get the binary database files. How do I get the original html files with Nutch? 
Does Nutch support it? If not, what other tools can I use to achieve my goal.(The tools that support distributed crawling are better.)
",6k,"
            5
        ","['\nWell, nutch will write the crawled data in binary form so if if you want that to be saved in html format, you will have to modify the code. (this will be painful if you are new to nutch).\nIf you want quick and easy solution for getting html pages:\n\nIf the list of pages/urls that you intend to have is quite low, then better get it done with a script which invokes wget for each url.\nOR use HTTrack tool. \n\nEDIT:\nWriting a your own nutch plugin will be great. Your problem will get solved plus you can contribute to nutch by submitting your work !!! If you are new to nutch (in terms of code & design), then you will have to invest lot of time building a new plugin ... else its easy to do. \nFew pointers for helping your initiative:\nHere is a page which talks about writing own nutch plugin.\nStart with Fetcher.java. See lines 647-648. That is the place where you can get the fetched content on per url basis (for those pages which got fetched successfully).\npstatus = output(fit.url, fit.datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS);\nupdateStatus(content.getContent().length);\n\nYou should add code right after this to invoke your plugin. Pass content object to it. By now, you would have guessed that content.getContent() is the content for url you want. Inside the plugin code, write it to some file. Filename should be based on the url name else it will be difficult to work with that. Url can be obtained by fit.url.\n', '\nYou must do modifications in run Nutch in Eclipse.\nWhen you are able to run, open Fetcher.java and add the lines between ""content saver"" command lines. \ncase ProtocolStatus.SUCCESS:        // got a page\n            pstatus = output(fit.url, fit.datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS, fit.outlinkDepth);\n            updateStatus(content.getContent().length);\'\n\n\n            //------------------------------------------- content saver ---------------------------------------------\\\\\n            String filename = ""savedsites//"" + content.getUrl().replace(\'/\', \'-\');  \n\n            File file = new File(filename);\n            file.getParentFile().mkdirs();\n            boolean exist = file.createNewFile();\n            if (!exist) {\n                System.out.println(""File exists."");\n            } else {\n                FileWriter fstream = new FileWriter(file);\n                BufferedWriter out = new BufferedWriter(fstream);\n                out.write(content.toString().substring(content.toString().indexOf(""<!DOCTYPE html"")));\n                out.close();\n                System.out.println(""File created successfully."");\n            }\n            //------------------------------------------- content saver ---------------------------------------------\\\\\n\n', '\nTo update this answer -\nIt is possible to post process the data from your crawldb segment folder, and read in the html (including other data nutch has stored) directly.\n    Configuration conf = NutchConfiguration.create();\n    FileSystem fs = FileSystem.get(conf);\n\n    Path file = new Path(segment, Content.DIR_NAME + ""/part-00000/data"");\n    SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);\n\n    try\n    {\n            Text key = new Text();\n            Content content = new Content();\n\n            while (reader.next(key, content)) \n            {\n                    System.out.println(new String(content.GetContent()));\n            }\n    }\n    catch (Exception e)\n    {\n\n    }\n\n', '\nThe answers here are obsolete. Now, it is simply possible to get the plain HTML-files with nutch dump. Please see this answer.\n', '\nIn apache Nutch 2.3.1\nYou can save the raw HTML by edit the Nutch code firstly run the nutch in eclipse by following https://wiki.apache.org/nutch/RunNutchInEclipse\nAfter you finish ruunning nutch in eclipse edit file FetcherReducer.java , add this code to the output method, run ant eclipse again to rebuild the class\nFinally the raw html will added to reportUrl column in your database\nif (content != null) {\nByteBuffer raw = fit.page.getContent();\nif (raw != null) {\n    ByteArrayInputStream arrayInputStream = new ByteArrayInputStream(raw.array(), raw.arrayOffset() + raw.position(), raw.remaining());\n    Scanner scanner = new Scanner(arrayInputStream);\n    scanner.useDelimiter(""\\\\Z"");//To read all scanner content in one String\n    String data = """";\n    if (scanner.hasNext()) {\n        data = scanner.next();\n    }\n    fit.page.setReprUrl(StringUtil.cleanField(data));\n    scanner.close();\n}\n\n']",https://stackoverflow.com/questions/10007178/how-do-i-save-the-origin-html-file-with-apache-nutch,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wildcards in robots.txt,"
If in WordPress website I have categories in this order:
-Parent
--Child
---Subchild

I have permalinks set to:
%category%/%postname%
Let use an example.
I create post with post name ""Sport game"".
It's tag is sport-game.
It's full url is: domain.com/parent/child/subchild/sport-game
Why I use this kind of permalinks is exactly to block some content easier in robots.txt.
And now this is the part I have question for.
In robots.txt:
User-agent: Googlebot
Disallow: /parent/*
Disallow: /parent/*/*
Disallow: /parent/*/*/*

Disallow: /parent/* Is meaning of this rule that it's blocking domain.com/parent/child but not domain.com/parent/child/subchild and not domain.com/parent/?
Disallow: /parent/*/*/* Is meaning of this that it's blocking domain.com/parent/child/subchild/, that it's blocking only subchild, not child, not parent, and not posts under subchild?
",4k,"
            4
        ","['\nNote that the * wildcard in Disallow is not part of the original robots.txt specification. Some parsers support it, but as there is no specification, they might all handle it differently.\nAs you seem to be interested in Googlebot, have a look at Google鈥檚 robots.txt documentation.\nIn the examples it becomes clear that * means\n\nany string\n\n""Any string"" may, of course, also contain /.\nSo your first line Disallow: /parent/* should block every URL whose path starts with /parent/, including path segments separated by slashes.\nNote that this would be the same as Disallow: /parent/ in the original robots.txt specification, which also blocks any URL whose paths starts with /parent/, for example:\n\nhttp://example.com/parent/\nhttp://example.com/parent/foo\nhttp://example.com/parent/foo.html\nhttp://example.com/parent/foo/bar\nhttp://example.com/parent/foo/bar/\nhttp://example.com/parent/foo/bar/foo.html\n\n']",https://stackoverflow.com/questions/22134608/wildcards-in-robots-txt,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YouTube Data API to crawl all comments and replies,"
I have been desperately seeking a solution to crawl all comments and corresponding replies for my research. Am having a very hard time creating a data frame that includes comment data in correct and corresponding orders.
I am gonna share my code here so you professionals can take a look and give me some insights.
def get_video_comments(service, **kwargs):
    comments = []
    results = service.commentThreads().list(**kwargs).execute()

    while results:
        for item in results['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comment2 = item['snippet']['topLevelComment']['snippet']['publishedAt']
            comment3 = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
            comment4 = item['snippet']['topLevelComment']['snippet']['likeCount']
            if 'replies' in item.keys():
                for reply in item['replies']['comments']:
                    rauthor = reply['snippet']['authorDisplayName']
                    rtext = reply['snippet']['textDisplay']
                    rtime = reply['snippet']['publishedAt']
                    rlike = reply['snippet']['likeCount']
                    data = {'Reply ID': [rauthor], 'Reply Time': [rtime], 'Reply Comments': [rtext], 'Reply Likes': [rlike]}
                    print(rauthor)
                    print(rtext)
            data = {'Comment':[comment],'Date':[comment2],'ID':[comment3], 'Likes':[comment4]}
            result = pd.DataFrame(data)
            result.to_csv('youtube.csv', mode='a',header=False)
            print(comment)
            print(comment2)
            print(comment3)
            print(comment4)
            print('==============================')
            comments.append(comment)
                
        # Check if another page exists
        if 'nextPageToken' in results:
            kwargs['pageToken'] = results['nextPageToken']
            results = service.commentThreads().list(**kwargs).execute()
        else:
            break

    return comments

When I do this, my crawler collects comments but doesn't collect some of the replies that are under certain comments.
How can I make it collect comments and their corresponding replies and put them in a single data frame?
Update
So, somehow I managed to pull the information I wanted at the output section of Jupyter Notebook. All I have to do now is to append the result at the data frame.
Here is my updated code:
def get_video_comments(service, **kwargs):
    comments = []
    results = service.commentThreads().list(**kwargs).execute()

    while results:
        for item in results['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comment2 = item['snippet']['topLevelComment']['snippet']['publishedAt']
            comment3 = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
            comment4 = item['snippet']['topLevelComment']['snippet']['likeCount']
            if 'replies' in item.keys():
                for reply in item['replies']['comments']:
                    rauthor = reply['snippet']['authorDisplayName']
                    rtext = reply['snippet']['textDisplay']
                    rtime = reply['snippet']['publishedAt']
                    rlike = reply['snippet']['likeCount']
                    print(rtext)
                    print(rtime)
                    print(rauthor)
                    print('Likes: ', rlike)
                    
            print(comment)
            print(comment2)
            print(comment3)
            print(""Likes: "", comment4)

            print('==============================')
            comments.append(comment)
                
        # Check if another page exists
        if 'nextPageToken' in results:
            kwargs['pageToken'] = results['nextPageToken']
            results = service.commentThreads().list(**kwargs).execute()
        else:
            break

    return comments

The result is:

As you can see, the comments grouped under ======== lines are the comment and corresponding replies underneath.
What would be a good way to append the result into the data frame?
",2k,"
            4
        ","[""\nAccording to the official doc, the property replies.comments[] of CommentThreads resource has the following specification:\n\nreplies.comments[] (list)\nA list of one or more replies to the top-level comment. Each item in the list is a comment resource.\nThe list contains a limited number of replies, and unless the number of items in the list equals the value of the snippet.totalReplyCount property, the list of replies is only a subset of the total number of replies available for the top-level comment. To retrieve all of the replies for the top-level comment, you need to call the Comments.list method and use the parentId request parameter to identify the comment for which you want to retrieve replies.\n\nConsequently, if wanting to obtain all reply entries associated to a given top-level comment, you will have to use the Comments.list API endpoint queried appropriately.\nI recommend you to read my answer to a very much related question; there are three sections:\n\nTop-Level Comments and Associated Replies,\nThe property nextPageToken and the parameter pageToken, and\nAPI Limitations Imposed by Design.\n\nFrom the get go, you'll have to acknowledge that the API (as currently implemented) does not allow to obtain all top-level comments associated to a given video when the number of those comments exceeds a certain (unspecified) upper bound.\n\nFor what concerns a Python implementation, I would suggest that you do structure the code as follows:\ndef get_video_comments(service, video_id):\n    request = service.commentThreads().list(\n        videoId = video_id,\n        part = 'id,snippet,replies',\n        maxResults = 100\n    )\n    comments = []\n\n    while request:\n        response = request.execute()\n\n        for comment in response['items']:\n            reply_count = comment['snippet'] \\\n                ['totalReplyCount']\n            replies = comment.get('replies')\n            if replies is not None and \\\n               reply_count != len(replies['comments']):\n               replies['comments'] = get_comment_replies(\n                   service, comment['id'])\n\n            # 'comment' is a 'CommentThreads Resource' that has it's\n            # 'replies.comments' an array of 'Comments Resource'\n\n            # Do fill in the 'comments' data structure \n            # to be provided by this function:\n            ...\n\n        request = service.commentThreads().list_next(\n            request, response)\n\n    return comments\n\ndef get_comment_replies(service, comment_id):\n    request = service.comments().list(\n        parentId = comment_id,\n        part = 'id,snippet',\n        maxResults = 100\n    )\n    replies = []\n\n    while request:\n        response = request.execute()\n        replies.extend(response['items'])\n        request = service.comments().list_next(\n            request, response)\n\n    return replies\n\nNote that the ellipsis dots above -- ... -- would have to be replaced with actual code that fills in the array of structures to be returned by get_video_comments to its caller.\nThe simplest way (useful for quick testing) would be to have ... replaced with comments.append(comment) and then the caller of get_video_comments to simply pretty print (using json.dump) the object obtained from that function.\n"", '\nBased on stvar\' answer and the original publication here I built this code:\nimport os\nimport pickle\nimport csv\nimport json\nimport google.oauth2.credentials\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom google.auth.transport.requests import Request\n\nCLIENT_SECRETS_FILE = ""client_secret.json"" # for more information  to create your credentials json please visit https://python.gotrained.com/youtube-api-extracting-comments/\nSCOPES = [\'https://www.googleapis.com/auth/youtube.force-ssl\']\nAPI_SERVICE_NAME = \'youtube\'\nAPI_VERSION = \'v3\'\n\ndef get_authenticated_service():\n    credentials = None\n    if os.path.exists(\'token.pickle\'):\n        with open(\'token.pickle\', \'rb\') as token:\n            credentials = pickle.load(token)\n    #  Check if the credentials are invalid or do not exist\n    if not credentials or not credentials.valid:\n        # Check if the credentials have expired\n        if credentials and credentials.expired and credentials.refresh_token:\n            credentials.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                CLIENT_SECRETS_FILE, SCOPES)\n            credentials = flow.run_console()\n\n        # Save the credentials for the next run\n        with open(\'token.pickle\', \'wb\') as token:\n            pickle.dump(credentials, token)\n\n    return build(API_SERVICE_NAME, API_VERSION, credentials = credentials)\n\ndef get_video_comments(service, **kwargs):\n    request = service.commentThreads().list(**kwargs)\n    comments = []\n\n    while request:\n        response = request.execute()\n\n        for comment in response[\'items\']:\n            reply_count = comment[\'snippet\'] \\\n                [\'totalReplyCount\']\n            replies = comment.get(\'replies\')\n            if replies is not None and \\\n               reply_count != len(replies[\'comments\']):\n               replies[\'comments\'] = get_comment_replies(\n                   service, comment[\'id\'])\n\n            # \'comment\' is a \'CommentThreads Resource\' that has it\'s\n            # \'replies.comments\' an array of \'Comments Resource\'\n\n            # Do fill in the \'comments\' data structure \n            # to be provided by this function:\n            comments.append(comment)\n\n        request = service.commentThreads().list_next(\n            request, response)\n\n    return comments\ndef get_comment_replies(service, comment_id):\n    request = service.comments().list(\n        parentId = comment_id,\n        part = \'id,snippet\',\n        maxResults = 1000\n    )\n    replies = []\n\n    while request:\n        response = request.execute()\n        replies.extend(response[\'items\'])\n        request = service.comments().list_next(\n            request, response)\n\n    return replies\n\n\nif __name__ == \'__main__\':\n    # When running locally, disable OAuthlib\'s HTTPs verification. When\n    # running in production *do not* leave this option enabled.\n    os.environ[\'OAUTHLIB_INSECURE_TRANSPORT\'] = \'1\'\n    service = get_authenticated_service()\n    videoId = input(\'Enter Video id : \') # video id here (the video id of https://www.youtube.com/watch?v=vedLpKXzZqE -> is vedLpKXzZqE)\n    comments = get_video_comments(service, videoId=videoId, part=\'id,snippet,replies\', maxResults = 1000)\n\n\nwith open(\'youtube_comments\', \'w\', encoding=\'UTF8\') as f:\n    writer = csv.writer(f, delimiter=\',\', quotechar=\'""\', quoting=csv.QUOTE_MINIMAL)\n    for row in comments:\n            # convert the tuple to a list and write to the output file\n            writer.writerow([row])\n\n\nit returns a file called youtube_comments with this format:\n""{\'kind\': \'youtube#commentThread\', \'etag\': \'gvhv4hkH0H2OqQAHQKxzfA-K_tA\', \'id\': \'UgzSgI1YEvwcuF4cPwN4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'topLevelComment\': {\'kind\': \'youtube#comment\', \'etag\': \'qpuKZcuD4FKf6BHgRlMunersEeU\', \'id\': \'UgzSgI1YEvwcuF4cPwN4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'This is a comment\', \'textOriginal\': \'This is a comment\', \'authorDisplayName\': \'Gabriell Magana\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLRGBvo2ZncDP1xGjlX6anfUufNYi9b3w9kYZFDl=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UCKAa4FYftXsN7VKaPSlCivg\', \'authorChannelId\': {\'value\': \'UCKAa4FYftXsN7VKaPSlCivg\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 8, \'publishedAt\': \'2019-05-22T12:38:34Z\', \'updatedAt\': \'2019-05-22T12:38:34Z\'}}, \'canReply\': True, \'totalReplyCount\': 0, \'isPublic\': True}}""\n""{\'kind\': \'youtube#commentThread\', \'etag\': \'DsgDziMk7mB7xN4OoX7cmqlbDYE\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'topLevelComment\': {\'kind\': \'youtube#comment\', \'etag\': \'NYjvYM9W_umBafAfQkdg1P9apgg\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'This is another comment\', \'textOriginal\': \'This is another comment\', \'authorDisplayName\': \'Mary Montes\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLTg1b1yw8BX8Af0PoTR_t5OOwP9Cfl9_qL-o1iikw=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UC_GP_8HxDPsqJjJ3Fju_UeA\', \'authorChannelId\': {\'value\': \'UC_GP_8HxDPsqJjJ3Fju_UeA\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 9, \'publishedAt\': \'2019-05-15T05:10:49Z\', \'updatedAt\': \'2019-05-15T05:10:49Z\'}}, \'canReply\': True, \'totalReplyCount\': 3, \'isPublic\': True}, \'replies\': {\'comments\': [{\'kind\': \'youtube#comment\', \'etag\': \'Tu41ENCZYNJ2KBpYeYz4qgre0H8\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg.8uwduw6ppF79DbfJ9zMKxM\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'this is first reply\', \'parentId\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'authorDisplayName\': \'JULIO EMPRESARIO\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/eYP4MBcZ4bON_pHtdbtVsyWnsKbpNKye2wTPhgkffkMYk3ZbN0FL6Aa1o22YlFjn2RVUAkSQYw=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UCrpB9oZZZfmBv1aQsxrk66w\', \'authorChannelId\': {\'value\': \'UCrpB9oZZZfmBv1aQsxrk66w\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 2, \'publishedAt\': \'2020-09-15T04:06:50Z\', \'updatedAt\': \'2020-09-15T04:06:50Z\'}}, {\'kind\': \'youtube#comment\', \'etag\': \'OrpbnJddwzlzwGArCgtuuBsYr94\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg.8uwduw6ppF795E1w8RV1DJ\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'the second replay\', \'textOriginal\': \'the second replay\', \'parentId\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'authorDisplayName\': \'Anatolio27 Diaz\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLR1hOySIxEkvRCySExHjo3T6zGBNkvuKpPkqA=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UC04N8BM5aUwDJf-PNFxKI-g\', \'authorChannelId\': {\'value\': \'UC04N8BM5aUwDJf-PNFxKI-g\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 2, \'publishedAt\': \'2020-02-19T18:21:06Z\', \'updatedAt\': \'2020-02-19T18:21:06Z\'}}, {\'kind\': \'youtube#comment\', \'etag\': \'sPmIwerh3DTZshLiDVwOXn_fJx0\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg.8uwduw6ppF78wwH6Aabh4y\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'A third reply\', \'textOriginal\': \'A third reply\', \'parentId\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'authorDisplayName\': \'Voy detr谩s de mi pasi贸n\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLTgzZ3ZFvkmmAlMzA77ApM-2uGFfvOBnzxegYEX=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UCvv6QMokO7KcJCDpK6qZg3Q\', \'authorChannelId\': {\'value\': \'UCvv6QMokO7KcJCDpK6qZg3Q\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 2, \'publishedAt\': \'2019-07-03T18:45:34Z\', \'updatedAt\': \'2019-07-03T18:45:34Z\'}}]}}""\n\nNow it is necessary a second step in order to information required. For this I a set of bash script toos like cut, awk and set:\ncut -d "":"" -f 10- youtube_comments | sed -e ""s/\', \'/\\n/g"" -e ""s/\'//g"" | awk \'/replies/{print ""------------------------****---------:::   Replies: ""$6""  :::---------******--------------------------------""}!/replies/{print}\' |sed \'/^textOriginal:/,/^authorDisplayName:/{/^authorDisplayName/!d}\' |sed \'/^authorProfileImageUrl:\\|^authorChannelUrl:\\|^authorChannelId:\\|^etag:\\|^updatedAt:\\|^parentId:\\|^id:/d\' |sed \'s/<[^>]*>//g\' | sed \'s/{textDisplay/{\\ntextDisplay/\' |sed \'/^snippet:/d\' | awk -F"":"" \'(NF==1){print ""========================================COMMENT===========================================""}(NF>1){a=0; print $0}\' | sed \'s/textDisplay: //g\' | sed \'s/authorDisplayName/User/g\' | sed \'s/T[0-9]\\{2\\}:[0-9]\\{2\\}:[0-9]\\{2\\}Z//g\' | sed \'s/likeCount: /Likes:/g\' | sed \'s/publishedAt: //g\' > output_file\n\nThe final result is a file called output_file with this format:\n========================================COMMENT===========================================\nThis is a comment\nUser: Robert Everest\nLikes:8, 2019-05-22\n========================================COMMENT===========================================\nThis is another comment\nUser: Anna Davis\nLikes:9, 2019-05-15\n------------------------****---------:::   Replies: 3,  :::---------******--------------------------------\nthis is first reply\nUser: John Doe\nLikes:2, 2020-09-15\nthe second replay\nUser: Caraqueno\nLikes:2, 2020-02-19\nA third reply\nUser: Rebeca\nLikes:2, 2019-07-03\n\nThe python script requires of the file token.pickle to work, it is generated the first time the python script run and when it expired, it have to be deleted and generated again.\n', '\nI had a similar issue that the OP does and managed to solve it, but someone in the community closed my question after I solved it and can\'t post there. I\'m posting it here for fidelity.\nThe YouTube API doesn\'t allow users to grab nested replies to comments. What it does allow is you to get the replies to the comments and all the comments i.e. Video --> Comments --> Comment Replies ---> Reply To Reply et al. Knowing this limitation we can write code to get all the top Comments, and then break into those comments to get the first-level replies.\nModuels\nimport os\nimport googleapiclient.discovery #required for using googleapi\nimport pandas as pd #require for data munging. We use pd.json_normalize to create the tables\nimport numpy as np #just good to have\nimport json # the requests are returned as json objects. \nfrom datetime import datetime #good to have for date modification\n\nGet All Comments Function\nFor a given vidId, this function will get the first 100 comments and place them into a df. It then use a while loop to check to see if the response api contains nextPageToken. While it does, it will continue to run to get all the comments until either all the comments are pulled or you run out of credits, whichever happens first.\ndef vidcomments(vidId):\n    # Disable OAuthlib\'s HTTPS verification when running locally.\n    # *DO NOT* leave this option enabled in production.\n    os.environ[""OAUTHLIB_INSECURE_TRANSPORT""] = ""1""\n\n    api_service_name = ""youtube""\n    api_version = ""v3""\n    DEVELOPER_KEY = ""yourapikey"" #<--- insert API key here\n\n    youtube = googleapiclient.discovery.build(\n        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n\n    request = youtube.commentThreads().list(\n        part=""snippet, replies"",\n        order=""time"",\n        maxResults=100,\n        textFormat=""plainText"",\n        videoId=vidId\n    )\n    \n    response = request.execute()\n    full = pd.json_normalize(response, record_path=[\'items\'])\n    while response:\n        \n        if \'nextPageToken\' in response:\n            response = youtube.commentThreads().list(\n                part=""snippet"",\n                maxResults=100,\n                textFormat=\'plainText\',\n                order=\'time\',\n                videoId=vidId,\n                pageToken=response[\'nextPageToken\']\n            ).execute()\n            \n            df2 = pd.json_normalize(response, record_path=[\'items\'])\n            full = full.append(df2)\n            \n        else:\n            break\n    return full\n\nGet All Replies To Comments Function\nFor a particular parentId, get all the first-level replies. Like the vidcomments() function noted above, it will run until all replies to all comments are pulled or you run out of credits, whichever happens first.\n    def repliesto(parentId):\n        # Disable OAuthlib\'s HTTPS verification when running locally.\n        # *DO NOT* leave this option enabled in production.\n        os.environ[""OAUTHLIB_INSECURE_TRANSPORT""] = ""1""\n\n        api_service_name = ""youtube""\n        api_version = ""v3""\n        DEVELOPER_KEY = DevKey #your dev key\n\n        youtube = googleapiclient.discovery.build(\n            api_service_name, api_version, developerKey = DEVELOPER_KEY)\n\n        request = youtube.comments().list(\n            part=""snippet"",\n            maxResults=100,\n            parentId=parentId,\n            textFormat=""plainText""\n        )\n        response = request.execute()\n\n        replies = pd.json_normalize(response, record_path=[\'items\'])\n        while response:\n\n            if \'nextPageToken\' in response:\n                response = youtube.comments().list(\n                    part=""snippet"",\n                    maxResults=100,\n                    parentId=parentId,\n                    textFormat=""plainText"",\n                    pageToken=response[\'nextPageToken\']                \n                ).execute()\n\n                df2 = pd.json_normalize(response, record_path=[\'items\'])\n                replies = pd.concat([replies, df2], sort=False)\n\n            else:\n                break\n        return replies\n\n\nPutting It Together\nFirst, run the vidcomments function to get all the comments information. Then use the code below to get all the reply information using a for loop to pull in each topLevelComment.id into a list, then use the list and another for loop to build the replies dataframe. This will create two separate Dataframes, one for Comments and another for Replies. After creating both of these Dataframes you can then join them in a way that makes sense for your purpose, either concat/union or a join/merge.\n    replyto = []\n    for reply in full[(full[\'snippet.totalReplyCount\']>0)] \n    [\'snippet.topLevelComment.id\']:\n        replyto.append(reply)\n\n    # create an empty DF to store all the replies\n    # use a for loop to place each item in our replyto list into the function defined above\n    \n    replies = pd.DataFrame()\n    for reply in replyto:\n        df = repliesto(reply)\n        replies = pd.concat([replies, df], ignore_index=True)\n\n']",https://stackoverflow.com/questions/64275331/youtube-data-api-to-crawl-all-comments-and-replies,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to request Google to re-crawl my website? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 This question does not appear to be about programming within the scope defined in the help center.


Closed 7 years ago.







                        Improve this question
                    



Does someone know a way to request Google to re-crawl a website? If possible, this shouldn't last months. My site is showing an old title in Google's search results. How can I show it with the correct title and description? 
",450k,"
            240
        ","['\nThere are two options. The first (and better) one is using the Fetch as Google option in Webmaster Tools that Mike Flynn commented  about. Here are detailed instructions:\n\nGo to: https://www.google.com/webmasters/tools/ and log in\nIf you haven\'t already, add and verify the site with the ""Add a Site"" button\nClick on the site name for the one you want to manage\nClick Crawl -> Fetch as Google\nOptional: if you want to do a specific page only, type in the URL\nClick Fetch\nClick Submit to Index\nSelect either ""URL"" or ""URL and its direct links""\nClick OK and you\'re done.\n\nWith the option above, as long as every page can be reached from some link on the initial page or a page that it links to, Google should recrawl the whole thing. If you want to explicitly tell it a list of pages to crawl on the domain, you can follow the directions to submit a sitemap.\nYour second (and generally slower) option is, as seanbreeden pointed out, submitting here: http://www.google.com/addurl/\nUpdate 2019:\n\nLogin to - Google Search Console\nAdd a site and verify it with the available methods.\nAfter verification from the console, click on URL Inspection.\nIn the Search bar on top, enter your website URL or custom URLs for inspection and enter.\nAfter Inspection, it\'ll show an option to Request Indexing\nClick on it and GoogleBot will add your website in a Queue for crawling.\n\n', '\nThe usual way is to either resubmit your site in your Google Webmaster Tools or submit it here:  http://www.google.com/addurl/\n', '\nGoogle says that it is unable to control when your site is re-crawled. Regardless, you could also check this post on ""forcing rewcrawls"", I haven\'t tried it myself but it\'s worth a shot if you\'re desperate. \nOn another note, I might add that you make sure you have a sitemap.xml up as this will also help with SEO.\n', '\nAs far I know, if you resubmit a sitemap it will trigger and crawler of your site. \n', ""\nNowadays, the revisiting of a website pretty much depends on its popularity, authority and how often its content changes. Having a sitemap.xml containing all URLs is always better. You can also set the lastmod tag of each URL entries. If you don't abuse it, crawlers will take it into account.\n""]",https://stackoverflow.com/questions/9466360/how-to-request-google-to-re-crawl-my-website,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What is the difference between web-crawling and web-scraping? [duplicate],"






This question already has answers here:
                        
                    



crawler vs scraper

                                (6 answers)
                            

Closed 4 years ago.



Is there a difference between Crawling and Web-scraping?
If there's a difference, what's the best method to use in order to collect some web data to supply a database for later use in a customised search engine?
",73k,"
            101
        ","[""\nCrawling would be essentially what Google, Yahoo, MSN, etc. do, looking for ANY information.  Scraping is generally targeted at certain websites, for specfic data, e.g. for price comparison, so are coded quite differently.\nUsually a scraper will be bespoke to the websites it is supposed to be scraping, and would be doing things a (good) crawler wouldn't do, i.e.:\n\nHave no regard for robots.txt\nIdentify itself as a browser\nSubmit forms with data \nExecute Javascript (if required to\nact like a user)\n\n"", ""\nYes, they are different. In practice, you may need to use both.\n(I have to jump in because, so far, the other answers don't get to the essence of it. They use examples but don't make the distinctions clear. Granted, they are from 2010!)\nWeb scraping, to use a minimal definition, is the process of processing a web document and extracting information out of it. You can do web scraping without doing web crawling. \nWeb crawling, to use a minimal definition, is the process of iteratively finding and fetching web links starting from a list of seed URL's. Strictly speaking, to do web crawling, you have to do some degree of web scraping (to extract the URL's.)\nTo clear up some concepts mentioned in the other answers:\n\nrobots.txt is intended to apply to any automated process that accesses a web page. So it applies to both crawlers and scrapers.\n'Proper' crawlers and scrapers, both, should identify themselves accurately.\n\nSome references:\n\nWikipedia on web scraping\nWikipedia on web crawlers\nWikipedia on robots.txt\n\n"", '\nAFAIK Web Crawling is what Google does - it goes around a website looking at links and building a database of the layout of that site and sites it links to\nWeb Scraping would be the progamatic analysis of a web page to load some data off of it, EG loading up BBC weather and ripping (scraping) the weather forcast off of it and placing it elsewhere or using it in another program.\n', ""\nThere's a fundamental difference between these two. \nFor those looking to dig deeper, I suggest you read this - \nWeb scraper, Web Crawler\nThis post goes into detail. A good summary is in this chart from the article:\n\n"", ""\nThere's definitely a difference between these two. One refers to visiting a site, the other to extracting.\n"", '\nWe crawl sites to have broad perspective how the site is structured, what are connections between pages, to estimate how much time we need to visit all pages we are interested in. Scraping is often harder to implement, but it鈥檚 an essence of data extraction. Let鈥檚 think of scraping as of covering website with sheet of paper with some rectangles cut out. We can now see only things we need, completely ignoring parts of website that are common for all pages (like navigation, footer, ads), or extraneous informations as comments or breadcrumbs.\nMore about differences between crawling and scrapping you find here: https://tarantoola.io/web-scraping-vs-web-crawling/\n']",https://stackoverflow.com/questions/4327392/what-is-the-difference-between-web-crawling-and-web-scraping,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to force scrapy to crawl duplicate url?,"
I am learning Scrapy a web crawling framework.
by default it does not crawl duplicate urls or urls which scrapy have already crawled.  
How to make Scrapy to crawl duplicate urls or urls which have already crawled?
I tried to find out on internet but could not find relevant help.  
I found DUPEFILTER_CLASS = RFPDupeFilter and SgmlLinkExtractor from Scrapy - Spider crawls duplicate urls but this question is opposite of what I am looking
",22k,"
            29
        ","[""\nYou're probably looking for the dont_filter=True argument on Request().\nSee http://doc.scrapy.org/en/latest/topics/request-response.html#request-objects\n"", ""\nA more elegant solution is to disable the duplicate filter altogether:\n# settings.py\nDUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'\n\nThis way you don't have to clutter all your Request creation code with dont_filter=True. Another side effect: this only disables duplicate filtering and not any other filters like offsite filtering.\nIf you want to use this setting selectively for only one or some of multiple spiders in your project, you can set it via custom_settings in the spider implementation:\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    custom_settings = {\n        'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n    }\n\n""]",https://stackoverflow.com/questions/23131283/how-to-force-scrapy-to-crawl-duplicate-url,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to generate the start_urls dynamically in crawling?,"
I am crawling a site which may contain a lot of start_urls, like:
http://www.a.com/list_1_2_3.htm

I want to populate start_urls like [list_\d+_\d+_\d+\.htm],
and extract items from URLs like [node_\d+\.htm] during crawling. 
Can I use CrawlSpider to realize this function?
And how can I generate the start_urls dynamically in crawling?
",20k,"
            27
        ","[""\nThe best way to generate URLs dynamically is to override the start_requests method of the spider:  \n\nfrom scrapy.http.request import Request\n\ndef start_requests(self):\n      with open('urls.txt', 'rb') as urls:\n          for url in urls:\n              yield Request(url, self.parse)\n\n\n"", ""\nThere are two questions:\n1)yes you can realize this functionality by using Rules e.g ,\nrules =(Rule(SgmlLinkExtractor(allow = ('node_\\d+.htm')) ,callback = 'parse'))\n\nsuggested reading\n2) yes you can generate start_urls dynamically ,  start_urls is a \n\nlist\n\ne.g >>> start_urls = ['http://www.a.com/%d_%d_%d' %(n,n+1,n+2) for n in range(0, 26)]\n>>> start_urls\n\n['http://www.a.com/0_1_2', 'http://www.a.com/1_2_3', 'http://www.a.com/2_3_4', 'http://www.a.com/3_4_5', 'http://www.a.com/4_5_6', 'http://www.a.com/5_6_7',  'http://www.a.com/6_7_8', 'http://www.a.com/7_8_9', 'http://www.a.com/8_9_10','http://www.a.com/9_10_11', 'http://www.a.com/10_11_12', 'http://www.a.com/11_12_13', 'http://www.a.com/12_13_14', 'http://www.a.com/13_14_15', 'http://www.a.com/14_15_16', 'http://www.a.com/15_16_17', 'http://www.a.com/16_17_18', 'http://www.a.com/17_18_19', 'http://www.a.com/18_19_20', 'http://www.a.com/19_20_21', 'http://www.a.com/20_21_22', 'http://www.a.com/21_22_23', 'http://www.a.com/22_23_24', 'http://www.a.com/23_24_25', 'http://www.a.com/24_25_26', 'http://www.a.com/25_26_27']\n\n""]",https://stackoverflow.com/questions/9322219/how-to-generate-the-start-urls-dynamically-in-crawling,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Save complete web page (incl css, images) using python/selenium","
I am using Python/Selenium to submit genetic sequences to an online database, and want to save the full page of results I get back. Below is the code that gets me to the results I want:
from selenium import webdriver

URL = 'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome'
SEQUENCE = 'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA' #'GAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGA'
CHROME_WEBDRIVER_LOCATION = '/home/max/Downloads/chromedriver' # update this for your machine

# open page with selenium
# (first need to download Chrome webdriver, or a firefox webdriver, etc)
driver = webdriver.Chrome(executable_path=CHROME_WEBDRIVER_LOCATION)
driver.get(URL)
time.sleep(5)

# enter sequence into the query field and hit 'blast' button to search
seq_query_field = driver.find_element_by_id(""seq"")
seq_query_field.send_keys(SEQUENCE)

blast_button = driver.find_element_by_id(""b1"")
blast_button.click()
time.sleep(60)

At that point I have a page that I can manually click ""save as,"" and get a local file (with a corresponding folder of image/js assets) that lets me view the whole returned page locally (minus content which is generated dynamically from scrolling down the page, which is fine). I assumed there would be a simple way to mimic this 'save as' function in python/selenium but haven't found one. The code to save the page below just saves html, and does not leave me with a local file that looks like it does in the web browser, with images, etc.
content = driver.page_source
with open('webpage.html', 'w') as f:
    f.write(content)

I've also found this question/answer on SO, but the accepted answer just brings up the 'save as' box, and does not provide a way to click it (as two commenters point out)
Is there a simple way to 'save [full page] as' using python? Ideally I'd prefer an answer using selenium since selenium makes the crawling part so straightforward, but I'm open to using another library if there's a better tool for this job. Or maybe I just need to specify all of the images/tables I want to download in code, and there is no shortcut to emulating the right-click 'save as' functionality?
UPDATE - Follow up question for James' answer
So I ran James' code to generate a page.html (and associated files) and compared it to the html file I got from manually clicking save-as. The page.html saved via James' script is great and has everything I need, but when opened in a browser it also shows a lot of extra formatting text that's hidden in the manually save'd page. See attached screenshot (manually saved page on the left, script-saved page with extra formatting text shown on right). 

This is especially surprising to me because the raw html of the page saved by James' script seems to indicate those fields should still be hidden. See e.g. the html below, which appears the same in both files, but the text at issue only appears in the browser-rendered page on the one saved by James' script:
<p class=""helpbox ui-ncbitoggler-slave ui-ncbitoggler"" id=""hlp1"" aria-hidden=""true"">
These options control formatting of alignments in results pages. The
default is HTML, but other formats (including plain text) are available.
PSSM and PssmWithParameters are representations of Position Specific Scoring Matrices and are only available for PSI-BLAST. 
The Advanced view option allows the database descriptions to be sorted by various indices in a table.
</p>

Any idea why this is happening?
",19k,"
            25
        ","['\nAs you noted, Selenium cannot interact with the browser\'s context menu to use Save as..., so instead to do so, you could use an external automation library like pyautogui.\npyautogui.hotkey(\'ctrl\', \'s\')\ntime.sleep(1)\npyautogui.typewrite(SEQUENCE + \'.html\')\npyautogui.hotkey(\'enter\')\n\nThis code opens the Save as... window through its keyboard shortcut CTRL+S and then saves the webpage and its assets into the default downloads location by pressing enter. This code also names the file as the sequence in order to give it a unique name, though you could change this for your use case. If needed, you could additionally change the download location through some extra work with the tab and arrow keys.\nTested on Ubuntu 18.10; depending on your OS you may need to modify the key combination sent.\n\nFull code, in which I also added conditional waits to improve speed:\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.expected_conditions import visibility_of_element_located\nfrom selenium.webdriver.support.ui import WebDriverWait\nimport pyautogui\n\nURL = \'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome\'\nSEQUENCE = \'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA\' #\'GAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGA\'\n\n# open page with selenium\n# (first need to download Chrome webdriver, or a firefox webdriver, etc)\ndriver = webdriver.Chrome()\ndriver.get(URL)\n\n# enter sequence into the query field and hit \'blast\' button to search\nseq_query_field = driver.find_element_by_id(""seq"")\nseq_query_field.send_keys(SEQUENCE)\n\nblast_button = driver.find_element_by_id(""b1"")\nblast_button.click()\n\n# wait until results are loaded\nWebDriverWait(driver, 60).until(visibility_of_element_located((By.ID, \'grView\')))\n\n# open \'Save as...\' to save html and assets\npyautogui.hotkey(\'ctrl\', \'s\')\ntime.sleep(1)\npyautogui.typewrite(SEQUENCE + \'.html\')\npyautogui.hotkey(\'enter\')\n\n', '\nThis is not a perfect solution, but it will get you most of what you need.  You can replicate the behavior of ""save as full web page (complete)"" by parsing the html and downloading any loaded files (images, css, js, etc.) to their same relative path.  \nMost of the javascript won\'t work due to cross origin request blocking.  But the content will look (mostly) the same.\nThis uses requests to save the loaded files, lxml to parse the html, and os for the path legwork.\nfrom selenium import webdriver\nimport chromedriver_binary\nfrom lxml import html\nimport requests\nimport os\n\ndriver = webdriver.Chrome()\nURL = \'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome\'\nSEQUENCE = \'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA\' \nbase = \'https://blast.ncbi.nlm.nih.gov/\'\n\ndriver.get(URL)\nseq_query_field = driver.find_element_by_id(""seq"")\nseq_query_field.send_keys(SEQUENCE)\nblast_button = driver.find_element_by_id(""b1"")\nblast_button.click()\n\ncontent = driver.page_source\n# write the page content\nos.mkdir(\'page\')\nwith open(\'page/page.html\', \'w\') as fp:\n    fp.write(content)\n\n# download the referenced files to the same path as in the html\nsess = requests.Session()\nsess.get(base)            # sets cookies\n\n# parse html\nh = html.fromstring(content)\n# get css/js files loaded in the head\nfor hr in h.xpath(\'head//@href\'):\n    if not hr.startswith(\'http\'):\n        local_path = \'page/\' + hr\n        hr = base + hr\n    res = sess.get(hr)\n    if not os.path.exists(os.path.dirname(local_path)):\n        os.makedirs(os.path.dirname(local_path))\n    with open(local_path, \'wb\') as fp:\n        fp.write(res.content)\n\n# get image/js files from the body.  skip anything loaded from outside sources\nfor src in h.xpath(\'//@src\'):\n    if not src or src.startswith(\'http\'):\n        continue\n    local_path = \'page/\' + src\n    print(local_path)\n    src = base + src\n    res = sess.get(hr)\n    if not os.path.exists(os.path.dirname(local_path)):\n        os.makedirs(os.path.dirname(local_path))\n    with open(local_path, \'wb\') as fp:\n        fp.write(res.content)  \n\nYou should have a folder called page with a file called page.html in it with the content you are after.\n', '\nInspired by FThompson\'s answer above, I came up with the following tool that can download full/complete html for a given page url (see: https://github.com/markfront/SinglePageFullHtml)\nUPDATE - follow up with Max\'s suggestion, below are steps to use the tool:\n\nClone the project, then run maven to build:\n\n$> git clone https://github.com/markfront/SinglePageFullHtml.git\n\n$> cd ~/git/SinglePageFullHtml\n$> mvn clean compile package\n\n\nFind the generated jar file in target folder: SinglePageFullHtml-1.0-SNAPSHOT-jar-with-dependencies.jar\n\nRun the jar in command line like:\n\n\n$> java -jar .target/SinglePageFullHtml-1.0-SNAPSHOT-jar-with-dependencies.jar <page_url>\n\n\nThe result file name will have a prefix ""FP, followed by the hashcode of the page url, with file extension "".html"". It will be found in either folder ""/tmp"" (which you can get by System.getProperty(""java.io.tmp""). If not, try find it in your home dir or System.getProperty(""user.home"") in Java).\n\nThe result file will be a big fat self-contained html file that includes everything (css, javascript, images, etc.) referred to by the original html source.\n\n\n', '\nI\'ll advise u to have a try on sikulix which is an image based automation tool for operate any widgets within PC OS, it supports python grammar and run with command line and maybe the simplest way to solve ur problem.\nAll u need to do is just give it a screenshot, call sikulix script in ur python automation script(with OS.system(""xxxx"") or subprocess...).\n']",https://stackoverflow.com/questions/53729201/save-complete-web-page-incl-css-images-using-python-selenium,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Does solr do web crawling?,"
I am interested to do web crawling. I was looking at solr. 
Does solr do web crawling, or what are the steps to do web crawling?
",30k,"
            18
        ","[""\nSolr 5+ DOES in fact now do web crawling!\nhttp://lucene.apache.org/solr/\nOlder Solr versions do not do web crawling alone, as historically it's a search server that provides full text search capabilities.  It builds on top of Lucene.\nIf you need to crawl web pages using another Solr project then you have a number of options including:\n\nNutch - http://lucene.apache.org/nutch/\nWebsphinx - http://www.cs.cmu.edu/~rcm/websphinx/\nJSpider - http://j-spider.sourceforge.net/\nHeritrix - http://crawler.archive.org/\n\nIf you want to make use of the search facilities provided by Lucene or SOLR you'll need to build indexes from the web crawl results.\nSee this also: \nLucene crawler (it needs to build lucene index)\n"", '\nSolr does not in of itself have a web crawling feature.\nNutch is the ""de-facto"" crawler (and then some) for Solr.\n', '\nSolr 5 started supporting simple webcrawling (Java Doc). If want search, Solr is the tool, if you want to crawl, Nutch/Scrapy is better :) \nTo get it up and running, you can take a detail look at here. However, here is how to get it up and running in one line: \njava \n-classpath <pathtosolr>/dist/solr-core-5.4.1.jar \n-Dauto=yes \n-Dc=gettingstarted     -> collection: gettingstarted\n-Ddata=web             -> web crawling and indexing\n-Drecursive=3          -> go 3 levels deep\n-Ddelay=0              -> for the impatient use 10+ for production\norg.apache.solr.util.SimplePostTool   -> SimplePostTool\nhttp://datafireball.com/      -> a testing wordpress blog\n\nThe crawler here is very ""naive"" where you can find all the code from this Apache Solr\'s github repo.\nHere is how the response looks like: \nSimplePostTool version 5.0.0\nPosting web pages to Solr url http://localhost:8983/solr/gettingstarted/update/extract\nEntering auto mode. Indexing pages with content-types corresponding to file endings xml,json,csv,pdf,doc,docx,ppt,pptx,xls,xlsx,odt,odp,ods,ott,otp,ots,rtf,htm,html,txt,log\nSimplePostTool: WARNING: Never crawl an external web site faster than every 10 seconds, your IP will probably be blocked\nEntering recursive mode, depth=3, delay=0s\nEntering crawl at level 0 (1 links total, 1 new)\nPOSTed web resource http://datafireball.com (depth: 0)\nEntering crawl at level 1 (52 links total, 51 new)\nPOSTed web resource http://datafireball.com/2015/06 (depth: 1)\n...\nEntering crawl at level 2 (266 links total, 215 new)\n...\nPOSTed web resource http://datafireball.com/2015/08/18/a-few-functions-about-python-path (depth: 2)\n...\nEntering crawl at level 3 (846 links total, 656 new)\nPOSTed web resource http://datafireball.com/2014/09/06/node-js-web-scraping-using-cheerio (depth: 3)\nSimplePostTool: WARNING: The URL http://datafireball.com/2014/09/06/r-lattice-trellis-another-framework-for-data-visualization/?share=twitter returned a HTTP result status of 302\n423 web pages indexed.\nCOMMITting Solr index changes to http://localhost:8983/solr/gettingstarted/update/extract...\nTime spent: 0:05:55.059\n\nIn the end, you can see all the data are indexed properly. \n \n', '\nYou might also want to take a look at \nhttp://www.crawl-anywhere.com/\nVery powerful crawler that is compatible with Solr. \n', ""\nI have been using Nutch with Solr on my latest project and it seems to work quite nicely.\nIf you are using a Windows machine then I would strongly recommend following the 'No cygwin' instructions given by Jason Riffel too!\n"", '\nYes, I agree with the other posts here, use Apache Nutch\n\nbin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 3 -topN 5\n\nAlthough your solr version has the match the correct version of Nutch, because older versions of solr stores the indices in a different format\nIts tutorial:\nhttp://wiki.apache.org/nutch/NutchTutorial\n', ""\nI know it's been a while, but in case someone else is searching for a Solr crawler like me, there is a new open-source crawler called Norconex HTTP Collector \n"", ""\nI know this question is quite old, but I'll respond anyway for the newcomer that will wonder here.\nIn order to use Solr, you can use a web crawler that is capable of storing documents in Solr.\nFor instance, The Norconex HTTP Collector is a flexible and powerful open-source web crawler that is compatible with Solr.\nTo use Solr with the Norconex HTTP Collector you will need the Norconex HTTP Collector which is used to crawl the website that you want to collect data from, and you will need to install the Norconex Apache Solr Committer to store collected documents into Solr. When the committer is installed, you will need to configure the XML configuration file of the crawler. I would recommend that you follow this link to get started test how the crawler works and here to know how to configure the configuration file. Finally, you will need this link to configure the committer section of the configuration file with Solr.\nNote that if your goal is not to crawl web pages, Norconex also has a Filesystem Collector that can be used with the Sorl Committer as well.\n"", '\nDef Nutch !\nNutch also has a basic web front end which will let you query your search results. You might not even need to bother with SOLR depending on your requirements. If you do a Nutch/SOLR combination you should be able to take advantage of the recent work done to integrate SOLR and Nutch ...  http://issues.apache.org/jira/browse/NUTCH-442 \n']",https://stackoverflow.com/questions/1781247/does-solr-do-web-crawling,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Submit data via web form and extract the results,"
My python level is Novice. I have never written a web scraper or crawler. I have written a python code to connect to an api and extract the data that I want. But for some the extracted data I want to get the gender of the author. I found this web site http://bookblog.net/gender/genie.php but downside is there isn't an api available. I was wondering how to write a python to submit data to the form in the page and extract the return data. It would be a great help if I could get some guidance on this.
This is the form dom: 
<form action=""analysis.php"" method=""POST"">
<textarea cols=""75"" rows=""13"" name=""text""></textarea>
<div class=""copyright"">(NOTE: The genie works best on texts of more than 500 words.)</div>
<p>
<b>Genre:</b>
<input type=""radio"" value=""fiction"" name=""genre"">
fiction&nbsp;&nbsp;
<input type=""radio"" value=""nonfiction"" name=""genre"">
nonfiction&nbsp;&nbsp;
<input type=""radio"" value=""blog"" name=""genre"">
blog entry
</p>
<p>
</form>

results page dom:
<p>
<b>The Gender Genie thinks the author of this passage is:</b>
male!
</p>

",49k,"
            17
        ","['\nNo need to use mechanize, just send the correct form data in a POST request.\nAlso, using regular expression to parse HTML is a bad idea. You would be better off using a HTML parser like lxml.html.\nimport requests\nimport lxml.html as lh\n\n\ndef gender_genie(text, genre):\n    url = \'http://bookblog.net/gender/analysis.php\'\n    caption = \'The Gender Genie thinks the author of this passage is:\'\n\n    form_data = {\n        \'text\': text,\n        \'genre\': genre,\n        \'submit\': \'submit\',\n    }\n\n    response = requests.post(url, data=form_data)\n\n    tree = lh.document_fromstring(response.content)\n\n    return tree.xpath(""//b[text()=$caption]"", caption=caption)[0].tail.strip()\n\n\nif __name__ == \'__main__\':\n    print gender_genie(\'I have a beard!\', \'blog\')\n\n', '\nYou can use mechanize to submit and retrieve content, and the re module for getting what you want. For example, the script below does it for the text of your own question:\nimport re\nfrom mechanize import Browser\n\ntext = """"""\nMy python level is Novice. I have never written a web scraper \nor crawler. I have written a python code to connect to an api and \nextract the data that I want. But for some the extracted data I want to \nget the gender of the author. I found this web site \nhttp://bookblog.net/gender/genie.php but downside is there isn\'t an api \navailable. I was wondering how to write a python to submit data to the \nform in the page and extract the return data. It would be a great help \nif I could get some guidance on this.""""""\n\nbrowser = Browser()\nbrowser.open(""http://bookblog.net/gender/genie.php"")\n\nbrowser.select_form(nr=0)\nbrowser[\'text\'] = text\nbrowser[\'genre\'] = [\'nonfiction\']\n\nresponse = browser.submit()\n\ncontent = response.read()\n\nresult = re.findall(\n    r\'<b>The Gender Genie thinks the author of this passage is:</b> (\\w*)!\', content)\n\nprint result[0]\n\nWhat does it do? It creates a mechanize.Browser and goes to the given URL:\nbrowser = Browser()\nbrowser.open(""http://bookblog.net/gender/genie.php"")\n\nThen it selects the form (since there is only one form to be filled, it will be the first):\nbrowser.select_form(nr=0)\n\nAlso, it sets the entries of the form...\nbrowser[\'text\'] = text\nbrowser[\'genre\'] = [\'nonfiction\']\n\n... and submit it:\nresponse = browser.submit()\n\nNow, we get the result:\ncontent = response.read()\n\nWe know that the result is in the form:\n<b>The Gender Genie thinks the author of this passage is:</b> male!\n\nSo we create a regex for matching and use re.findall():\nresult = re.findall(\n    r\'<b>The Gender Genie thinks the author of this passage is:</b> (\\w*)!\',\n    content)\n\nNow the result is available for your use:\nprint result[0]\n\n', '\nYou can use mechanize, see examples for details.\nfrom mechanize import ParseResponse, urlopen, urljoin\n\nuri = ""http://bookblog.net""\n\nresponse = urlopen(urljoin(uri, ""/gender/genie.php""))\nforms = ParseResponse(response, backwards_compat=False)\nform = forms[0]\n\n#print form\n\nform[\'text\'] = \'cheese\'\nform[\'genre\'] = [\'fiction\']\n\nprint urlopen(form.click()).read()\n\n']",https://stackoverflow.com/questions/8377055/submit-data-via-web-form-and-extract-the-results,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to make a polygon radar (spider) chart in python,"
import matplotlib.pyplot as plt
import numpy as np

labels=['Siege', 'Initiation', 'Crowd_control', 'Wave_clear', 'Objective_damage']
markers = [0, 1, 2, 3, 4, 5]
str_markers = [""0"", ""1"", ""2"", ""3"", ""4"", ""5""]

def make_radar_chart(name, stats, attribute_labels=labels,
                     plot_markers=markers, plot_str_markers=str_markers):

    labels = np.array(attribute_labels)

    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)
    stats = np.concatenate((stats,[stats[0]]))
    angles = np.concatenate((angles,[angles[0]]))

    fig = plt.figure()
    ax = fig.add_subplot(111, polar=True)
    ax.plot(angles, stats, 'o-', linewidth=2)
    ax.fill(angles, stats, alpha=0.25)
    ax.set_thetagrids(angles * 180/np.pi, labels)
    plt.yticks(markers)
    ax.set_title(name)
    ax.grid(True)

    fig.savefig(""static/images/%s.png"" % name)

    return plt.show()

make_radar_chart(""Agni"", [2,3,4,4,5]) # example



Basically I want the chart to be a pentagon instead of circle. Can anyone help with this. I am using python matplotlib to save an image which will stored and displayed later. I want my chart to have the form of the second picture
EDIT:
    gridlines = ax.yaxis.get_gridlines()
    for gl in gridlines:
        gl.get_path()._interpolation_steps = 5

adding this section of code from answer below helped a lot. I am getting this chart. Still need to figure out how to get rid of the outer most ring: 
",27k,"
            17
        ","['\nThe radar chart demo shows how to make the a radar chart. The result looks like this:\n\nHere, the outer spine is polygon shaped as desired. However the inner grid lines are circular. \nSo the open question is how to make the gridlines the same shape as the spines.\nThis can be done by overriding the draw method and setting the gridlines\' path interpolation step variable to the number of variables of the RadarAxes class.\ngridlines = self.yaxis.get_gridlines()\nfor gl in gridlines:\n    gl.get_path()._interpolation_steps = num_vars\n\nComplete example:\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, RegularPolygon\nfrom matplotlib.path import Path\nfrom matplotlib.projections.polar import PolarAxes\nfrom matplotlib.projections import register_projection\nfrom matplotlib.spines import Spine\nfrom matplotlib.transforms import Affine2D\n\n\ndef radar_factory(num_vars, frame=\'circle\'):\n    """"""Create a radar chart with `num_vars` axes.\n\n    This function creates a RadarAxes projection and registers it.\n\n    Parameters\n    ----------\n    num_vars : int\n        Number of variables for radar chart.\n    frame : {\'circle\' | \'polygon\'}\n        Shape of frame surrounding axes.\n\n    """"""\n    # calculate evenly-spaced axis angles\n    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n\n    class RadarAxes(PolarAxes):\n\n        name = \'radar\'\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            # rotate plot such that the first axis is at the top\n            self.set_theta_zero_location(\'N\')\n\n        def fill(self, *args, closed=True, **kwargs):\n            """"""Override fill so that line is closed by default""""""\n            return super().fill(closed=closed, *args, **kwargs)\n\n        def plot(self, *args, **kwargs):\n            """"""Override plot so that line is closed by default""""""\n            lines = super().plot(*args, **kwargs)\n            for line in lines:\n                self._close_line(line)\n\n        def _close_line(self, line):\n            x, y = line.get_data()\n            # FIXME: markers at x[0], y[0] get doubled-up\n            if x[0] != x[-1]:\n                x = np.concatenate((x, [x[0]]))\n                y = np.concatenate((y, [y[0]]))\n                line.set_data(x, y)\n\n        def set_varlabels(self, labels):\n            self.set_thetagrids(np.degrees(theta), labels)\n\n        def _gen_axes_patch(self):\n            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n            # in axes coordinates.\n            if frame == \'circle\':\n                return Circle((0.5, 0.5), 0.5)\n            elif frame == \'polygon\':\n                return RegularPolygon((0.5, 0.5), num_vars,\n                                      radius=.5, edgecolor=""k"")\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n        def draw(self, renderer):\n            """""" Draw. If frame is polygon, make gridlines polygon-shaped """"""\n            if frame == \'polygon\':\n                gridlines = self.yaxis.get_gridlines()\n                for gl in gridlines:\n                    gl.get_path()._interpolation_steps = num_vars\n            super().draw(renderer)\n\n\n        def _gen_axes_spines(self):\n            if frame == \'circle\':\n                return super()._gen_axes_spines()\n            elif frame == \'polygon\':\n                # spine_type must be \'left\'/\'right\'/\'top\'/\'bottom\'/\'circle\'.\n                spine = Spine(axes=self,\n                              spine_type=\'circle\',\n                              path=Path.unit_regular_polygon(num_vars))\n                # unit_regular_polygon gives a polygon of radius 1 centered at\n                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n                # 0.5) in axes coordinates.\n                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n                                    + self.transAxes)\n\n\n                return {\'polar\': spine}\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n    register_projection(RadarAxes)\n    return theta\n\n\ndata = [[\'Sulfate\', \'Nitrate\', \'EC\', \'OC1\', \'OC2\', \'OC3\', \'OP\', \'CO\', \'O3\'],\n        (\'Basecase\', [\n            [0.88, 0.01, 0.03, 0.03, 0.00, 0.06, 0.01, 0.00, 0.00],\n            [0.07, 0.95, 0.04, 0.05, 0.00, 0.02, 0.01, 0.00, 0.00],\n            [0.01, 0.02, 0.85, 0.19, 0.05, 0.10, 0.00, 0.00, 0.00],\n            [0.02, 0.01, 0.07, 0.01, 0.21, 0.12, 0.98, 0.00, 0.00],\n            [0.01, 0.01, 0.02, 0.71, 0.74, 0.70, 0.00, 0.00, 0.00]])]\n\nN = len(data[0])\ntheta = radar_factory(N, frame=\'polygon\')\n\nspoke_labels = data.pop(0)\ntitle, case_data = data[0]\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection=\'radar\'))\nfig.subplots_adjust(top=0.85, bottom=0.05)\n\nax.set_rgrids([0.2, 0.4, 0.6, 0.8])\nax.set_title(title,  position=(0.5, 1.1), ha=\'center\')\n\nfor d in case_data:\n    line = ax.plot(theta, d)\n    ax.fill(theta, d,  alpha=0.25)\nax.set_varlabels(spoke_labels)\n\nplt.show()\n\n\n', '\nAs shown in this other post the answer from @ImportanceOfBeingErnest doesn\'t work in matplotlib>3.2.2 in that you get circular grids. As shown in this PR you can use the following\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, RegularPolygon\nfrom matplotlib.path import Path\nfrom matplotlib.projections.polar import PolarAxes\nfrom matplotlib.projections import register_projection\nfrom matplotlib.spines import Spine\nfrom matplotlib.transforms import Affine2D\n\n\ndef radar_factory(num_vars, frame=\'circle\'):\n    """"""Create a radar chart with `num_vars` axes.\n\n    This function creates a RadarAxes projection and registers it.\n\n    Parameters\n    ----------\n    num_vars : int\n        Number of variables for radar chart.\n    frame : {\'circle\' | \'polygon\'}\n        Shape of frame surrounding axes.\n\n    """"""\n    # calculate evenly-spaced axis angles\n    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n    \n    class RadarTransform(PolarAxes.PolarTransform):\n        def transform_path_non_affine(self, path):\n            # Paths with non-unit interpolation steps correspond to gridlines,\n            # in which case we force interpolation (to defeat PolarTransform\'s\n            # autoconversion to circular arcs).\n            if path._interpolation_steps > 1:\n                path = path.interpolated(num_vars)\n            return Path(self.transform(path.vertices), path.codes)\n\n    class RadarAxes(PolarAxes):\n\n        name = \'radar\'\n        \n        PolarTransform = RadarTransform\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            # rotate plot such that the first axis is at the top\n            self.set_theta_zero_location(\'N\')\n\n        def fill(self, *args, closed=True, **kwargs):\n            """"""Override fill so that line is closed by default""""""\n            return super().fill(closed=closed, *args, **kwargs)\n\n        def plot(self, *args, **kwargs):\n            """"""Override plot so that line is closed by default""""""\n            lines = super().plot(*args, **kwargs)\n            for line in lines:\n                self._close_line(line)\n\n        def _close_line(self, line):\n            x, y = line.get_data()\n            # FIXME: markers at x[0], y[0] get doubled-up\n            if x[0] != x[-1]:\n                x = np.concatenate((x, [x[0]]))\n                y = np.concatenate((y, [y[0]]))\n                line.set_data(x, y)\n\n        def set_varlabels(self, labels):\n            self.set_thetagrids(np.degrees(theta), labels)\n\n        def _gen_axes_patch(self):\n            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n            # in axes coordinates.\n            if frame == \'circle\':\n                return Circle((0.5, 0.5), 0.5)\n            elif frame == \'polygon\':\n                return RegularPolygon((0.5, 0.5), num_vars,\n                                      radius=.5, edgecolor=""k"")\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n        def draw(self, renderer):\n            """""" Draw. If frame is polygon, make gridlines polygon-shaped """"""\n            if frame == \'polygon\':\n                gridlines = self.yaxis.get_gridlines()\n                for gl in gridlines:\n                    gl.get_path()._interpolation_steps = num_vars\n            super().draw(renderer)\n\n\n        def _gen_axes_spines(self):\n            if frame == \'circle\':\n                return super()._gen_axes_spines()\n            elif frame == \'polygon\':\n                # spine_type must be \'left\'/\'right\'/\'top\'/\'bottom\'/\'circle\'.\n                spine = Spine(axes=self,\n                              spine_type=\'circle\',\n                              path=Path.unit_regular_polygon(num_vars))\n                # unit_regular_polygon gives a polygon of radius 1 centered at\n                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n                # 0.5) in axes coordinates.\n                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n                                    + self.transAxes)\n\n\n                return {\'polar\': spine}\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n    register_projection(RadarAxes)\n    return theta\n\n\ndata = [[\'Sulfate\', \'Nitrate\', \'EC\', \'OC1\', \'OC2\', \'OC3\', \'OP\', \'CO\', \'O3\'],\n        (\'Basecase\', [\n            [0.88, 0.01, 0.03, 0.03, 0.00, 0.06, 0.01, 0.00, 0.00],\n            [0.07, 0.95, 0.04, 0.05, 0.00, 0.02, 0.01, 0.00, 0.00],\n            [0.01, 0.02, 0.85, 0.19, 0.05, 0.10, 0.00, 0.00, 0.00],\n            [0.02, 0.01, 0.07, 0.01, 0.21, 0.12, 0.98, 0.00, 0.00],\n            [0.01, 0.01, 0.02, 0.71, 0.74, 0.70, 0.00, 0.00, 0.00]])]\n\nN = len(data[0])\ntheta = radar_factory(N, frame=\'polygon\')\n\nspoke_labels = data.pop(0)\ntitle, case_data = data[0]\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection=\'radar\'))\nfig.subplots_adjust(top=0.85, bottom=0.05)\n\nax.set_rgrids([0.2, 0.4, 0.6, 0.8])\nax.set_title(title,  position=(0.5, 1.1), ha=\'center\')\n\nfor d in case_data:\n    line = ax.plot(theta, d)\n    ax.fill(theta, d, alpha=0.25, label=\'_nolegend_\')\nax.set_varlabels(spoke_labels)\n\nplt.show()\n\nto get the desired:\n\n']",https://stackoverflow.com/questions/52910187/how-to-make-a-polygon-radar-spider-chart-in-python,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get Scrapy crawler output/results in script file function,"
I am using a script file to run a spider within scrapy project and spider is logging the crawler output/results. But i want to use spider output/results in that script file in some function .I did not want to save output/results in any file or DB.
Here is Script code get from https://doc.scrapy.org/en/latest/topics/practices.html#run-from-script
from twisted.internet import reactor
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging
from scrapy.utils.project import get_project_settings

configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})
runner = CrawlerRunner(get_project_settings())


d = runner.crawl('my_spider')
d.addBoth(lambda _: reactor.stop())
reactor.run()

def spider_output(output):
#     do something to that output

How can i get spider output in 'spider_output' method. It is possible to get output/results.
",9k,"
            15
        ","[""\nHere is the solution that get all output/results in a list\nfrom scrapy import signals\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\nfrom scrapy.signalmanager import dispatcher\n\n\ndef spider_results():\n    results = []\n\n    def crawler_results(signal, sender, item, response, spider):\n        results.append(item)\n\n    dispatcher.connect(crawler_results, signal=signals.item_scraped)\n\n    process = CrawlerProcess(get_project_settings())\n    process.crawl(MySpider)\n    process.start()  # the script will block here until the crawling is finished\n    return results\n\n\nif __name__ == '__main__':\n    print(spider_results())\n\n"", '\nThis is an old question, but for future reference. If you are working with python 3.6+ I recommend using scrapyscript that allows you to run your Spiders and get the results in a super simple way:\nfrom scrapyscript import Job, Processor\nfrom scrapy.spiders import Spider\nfrom scrapy import Request\nimport json\n\n# Define a Scrapy Spider, which can accept *args or **kwargs\n# https://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments\nclass PythonSpider(Spider):\n    name = \'myspider\'\n\n    def start_requests(self):\n        yield Request(self.url)\n\n    def parse(self, response):\n        title = response.xpath(\'//title/text()\').extract()\n        return {\'url\': response.request.url, \'title\': title}\n\n# Create jobs for each instance. *args and **kwargs supplied here will\n# be passed to the spider constructor at runtime\ngithubJob = Job(PythonSpider, url=\'http://www.github.com\')\npythonJob = Job(PythonSpider, url=\'http://www.python.org\')\n\n# Create a Processor, optionally passing in a Scrapy Settings object.\nprocessor = Processor(settings=None)\n\n# Start the reactor, and block until all spiders complete.\ndata = processor.run([githubJob, pythonJob])\n\n# Print the consolidated results\nprint(json.dumps(data, indent=4))\n\n[\n    {\n        ""title"": [\n            ""Welcome to Python.org""\n        ],\n        ""url"": ""https://www.python.org/""\n    },\n    {\n        ""title"": [\n            ""The world\'s leading software development platform \\u00b7 GitHub"",\n            ""1clr-code-hosting""\n        ],\n        ""url"": ""https://github.com/""\n    }\n]\n\n', ""\nAFAIK there is no way to do this, since crawl():\n\nReturns a deferred that is fired when the crawling is finished.\n\nAnd the crawler doesn't store results anywhere other than outputting them to logger.\nHowever returning ouput would conflict with the whole asynchronious nature and structure of scrapy, so saving to file then reading it is a prefered approach here.\nYou can simply devise pipeline that saves your items to file and simply read the file in your spider_output. You will receive your results since reactor.run() is blocking your script untill the output file is complete anyways.\n"", ""\nMy advice is to use the Python subprocess module to run spider from the script rather than using the method provided in the scrapy docs to run spider from python script. The reason for that is that with the subprocess module, you can capture the output/logs and even statements that you print from inside the spider.\nIn Python 3, execute the spider with the run method. Ex.\nimport subprocess\nprocess = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nif process.returncode == 0:\n    result = process.stdout.decode('utf-8')\nelse:\n    # code to check error using 'process.stderr'\n\nSetting the stdout/stderr to subprocess.PIPE will allow capture of output so it's very important to set this flag.\nHere command should be a sequence or a string (It it's a string, then call the run method with 1 more param: shell=True). For example:\ncommand = ['scrapy', 'crawl', 'website', '-a', 'customArg=blahblah']\n# or\ncommand = 'scrapy crawl website -a customArg=blahblah' # with shell=True\n#or\nimport shlex\ncommand = shlex.split('scrapy crawl website -a customArg=blahblah') # without shell=True\n\nAlso, process.stdout will contain the output from the script but it will be of type bytes. You need to convert it to str using decode('utf-8')\n"", ""\nIt's going to return all the results of a Spider within a list.\nfrom scrapyscript import Job, Processor\nfrom scrapy.utils.project import get_project_settings\n\n\ndef get_spider_output(spider, **kwargs):\n    job = Job(spider, **kwargs)\n    processor = Processor(settings=get_project_settings())\n    return processor.run([job])\n\n""]",https://stackoverflow.com/questions/40237952/get-scrapy-crawler-output-results-in-script-file-function,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using one Scrapy spider for several websites,"
I need to create a user configurable web spider/crawler, and I'm thinking about using Scrapy. But, I can't hard-code the domains and allowed URL regex:es -- this will instead be configurable in a GUI.
How do I (as simple as possible) create a spider or a set of spiders with Scrapy where the domains and allowed URL regex:es are dynamically configurable? E.g. I write the configuration to a file, and the spider reads it somehow.
",7k,"
            12
        ","[""\nWARNING: This answer was for Scrapy v0.7, spider manager api changed a lot since then.\nOverride default SpiderManager class, load your custom rules from a database or somewhere else and instanciate a custom spider with your own rules/regexes and domain_name\nin mybot/settings.py:\nSPIDER_MANAGER_CLASS = 'mybot.spidermanager.MySpiderManager'\n\nin mybot/spidermanager.py:\nfrom mybot.spider import MyParametrizedSpider\n\nclass MySpiderManager(object):\n    loaded = True\n\n    def fromdomain(self, name):\n        start_urls, extra_domain_names, regexes = self._get_spider_info(name)\n        return MyParametrizedSpider(name, start_urls, extra_domain_names, regexes)\n\n    def close_spider(self, spider):\n        # Put here code you want to run before spiders is closed\n        pass\n\n    def _get_spider_info(self, name):\n        # query your backend (maybe a sqldb) using `name` as primary key, \n        # and return start_urls, extra_domains and regexes\n        ...\n        return (start_urls, extra_domains, regexes)\n\nand now your custom spider class, in mybot/spider.py:\nfrom scrapy.spider import BaseSpider\n\nclass MyParametrizedSpider(BaseSpider):\n\n    def __init__(self, name, start_urls, extra_domain_names, regexes):\n        self.domain_name = name\n        self.start_urls = start_urls\n        self.extra_domain_names = extra_domain_names\n        self.regexes = regexes\n\n     def parse(self, response):\n         ...\n\nNotes:\n\nYou can extend CrawlSpider too if you want to take advantage of its Rules system\nTo run a spider use:  ./scrapy-ctl.py crawl <name>, where name is passed to SpiderManager.fromdomain and is the key to retreive more spider info from the backend system\nAs solution overrides default SpiderManager, coding a classic spider (a python module per SPIDER) doesn't works, but, I think this is not an issue for you. More info on default spiders manager TwistedPluginSpiderManager\n\n"", ""\nWhat you need is to dynamically create spider classes, subclassing your favorite generic spider class as supplied by scrapy (CrawlSpider subclasses with your rules added, or XmlFeedSpider, or whatever) and adding domain_name, start_urls, and possibly extra_domain_names (and/or start_requests(), etc), as you get or deduce them from your GUI (or config file, or whatever).\nPython makes it easy to perform such dynamic creation of class objects; a very simple example might be:\nfrom scrapy import spider\n\ndef makespider(domain_name, start_urls,\n               basecls=spider.BaseSpider):\n  return type(domain_name + 'Spider',\n              (basecls,),\n              {'domain_name': domain_name,\n               'start_urls': start_urls})\n\nallspiders = []\nfor domain, urls in listofdomainurlpairs:\n  allspiders.append(makespider(domain, urls))\n\nThis gives you a list of very bare-bone spider classes -- you'll probably want to add parse methods to them before you instantiate them.  Season to taste...;-).\n"", ""\nShameless self promotion on domo! you'll need to instantiate the crawler as given in the examples, for your project.\nAlso you'll need to make the crawler configurable on runtime, which is simply passing the configuration to crawler, and overriding the settings on runtime, when configuration changed. \n"", '\nNow it is extremely easy to configure scrapy for these purposes:\n\nAbout the first urls to visit, you can pass it as an attribute on the spider call with -a, and use the start_requests function to setup how to start the spider\nYou don\'t need to setup the allowed_domains variable for the spiders. If you don\'t include that class variable, the spider will be able to allow every domain.\n\nIt should end up to something like:\nclass MySpider(Spider):\n\n    name = ""myspider""\n\n    def start_requests(self):\n        yield Request(self.start_url, callback=self.parse)\n\n\n    def parse(self, response):\n        ...\n\nand you should call it with:\nscrapy crawl myspider -a start_url=""http://example.com""\n\n']",https://stackoverflow.com/questions/2396529/using-one-scrapy-spider-for-several-websites,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removing all spaces in text file with Python 3.x,"
So I have this crazy long text file made by my crawler and it for some reason added some spaces inbetween the links, like this:
https://example.com/asdf.html                                (note the spaces)
https://example.com/johndoe.php                              (again)

I want to get rid of that, but keep the new line. Keep in mind that the text file is 4.000+ lines long. I tried to do it myself but figured that I have no idea how to loop through new lines in files.
",51k,"
            9
        ","[""\nSeems like you can't directly edit a python file, so here is my suggestion:\n# first get all lines from file\nwith open('file.txt', 'r') as f:\n    lines = f.readlines()\n\n# remove spaces\nlines = [line.replace(' ', '') for line in lines]\n\n# finally, write lines in the file\nwith open('file.txt', 'w') as f:\n    f.writelines(lines)\n\n"", ""\nYou can open file and read line by line and remove white space - \nPython 3.x:\nwith open('filename') as f:\n    for line in f:\n        print(line.strip())\n\nPython 2.x:\nwith open('filename') as f:\n    for line in f:\n        print line.strip()\n\nIt will remove space from each line and print it.\nHope it helps!\n"", ""\nRead text from file, remove spaces, write text to file:\nwith open('file.txt', 'r') as f:\n    txt = f.read().replace(' ', '')\n\nwith open('file.txt', 'w') as f:\n    f.write(txt)\n\nIn @Leonardo Chiriv矛's solution it's unnecessary to create a list to store file contents when a string is sufficient and more memory efficient.  The .replace(' ', '') operation is only called once on the string, which is more efficient than iterating through a list performing replace for each line individually.\nTo avoid opening the file twice:\nwith open('file.txt', 'r+') as f:\n    txt = f.read().replace(' ', '')\n    f.seek(0)\n    f.write(txt)\n    f.truncate()\n\nIt would be more efficient to only open the file once.  This requires moving the file pointer back to the start of the file after reading, as well as truncating any possibly remaining content left over after you write back to the file.  A drawback to this solution however is that is not as easily readable.\n"", ""\nI had something similar that I'd been dealing with.\nThis is what worked for me (Note: This converts from 2+ spaces into a comma, but if you read below the code block, I explain how you can get rid of ALL whitespaces):\nimport re\n\n# read the file\nwith open('C:\\\\path\\\\to\\\\test_file.txt') as f:\n    read_file = f.read()\n    print(type(read_file)) # to confirm that it's a string\n\nread_file = re.sub(r'\\s{2,}', ',', read_file) # find/convert 2+ whitespace into ','\n\n# write the file\nwith open('C:\\\\path\\\\to\\\\test_file.txt', 'w') as f:\n    f.writelines('read_file')\n\nThis helped me then send the updated data to a CSV, which suited my need, but it can help for you as well, so instead of converting it to a comma (','), you can convert it to an empty string (''), and then [or] use a read_file.replace(' ', '') method if you don't need any whitespaces at all.\n"", ""\nLets not forget about adding back the \\n to go to the next row.\nThe complete function would be :\nwith open(str_path, 'r') as file :\n    str_lines = file.readlines()\n\n# remove spaces    \nif bl_right is True:    \n    str_lines = [line.rstrip() + '\\n' for line in str_lines]\nelif bl_left is True:   \n    str_lines = [line.lstrip() + '\\n' for line in str_lines]\nelse:                   \n    str_lines = [line.strip() + '\\n' for line in str_lines]\n\n# Write the file out again\nwith open(str_path, 'w') as file:\n    file.writelines(str_lines)\n\n""]",https://stackoverflow.com/questions/43447221/removing-all-spaces-in-text-file-with-python-3-x,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Concurrent downloads - Python,"
the plan is this:
I download a webpage, collect a list of images parsed in the DOM and then download these. After this I would iterate through the images in order to evaluate which image is best suited to represent the webpage.
Problem is that images are downloaded 1 by 1 and this can take quite some time.

It would be great if someone could point me in some direction regarding the topic.
Help would be very much appreciated.
",7k,"
            9
        ","['\nSpeeding up crawling is basically Eventlet\'s main use case.  It\'s deeply fast -- we have an application that has to hit 2,000,000 urls in a few minutes.  It makes use of the fastest event interface on your system (epoll, generally), and uses greenthreads (which are built on top of coroutines and are very inexpensive) to make it easy to write.\nHere\'s an example from the docs:\nurls = [""http://www.google.com/intl/en_ALL/images/logo.gif"",\n     ""https://wiki.secondlife.com/w/images/secondlife.jpg"",\n     ""http://us.i1.yimg.com/us.yimg.com/i/ww/beta/y3.gif""]\n\nimport eventlet\nfrom eventlet.green import urllib2  \n\ndef fetch(url):\n  body = urllib2.urlopen(url).read()\n  return url, body\n\npool = eventlet.GreenPool()\nfor url, body in pool.imap(fetch, urls):\n  print ""got body from"", url, ""of length"", len(body)\n\nThis is a pretty good starting point for developing a more fully-featured crawler.  Feel free to pop in to #eventlet on Freenode to ask for help.\n[update: I added a more-complex recursive web crawler example to the docs.  I swear it was in the works before this question was asked, but the question did finally inspire me to finish it.  :)]\n', ""\nWhile threading is certainly a possibility, I would instead suggest asyncore -- there's an excellent example here which shows exactly the simultaneous fetching of two URLs (easy to generalize to any list of URLs!).\n"", '\nHere is an article on threading which uses url fetching as an example.\n', '\nNowadays there are excellent Python libs you might want to use - urllib3 and requests\n']",https://stackoverflow.com/questions/2360291/concurrent-downloads-python,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache HTTPClient throws java.net.SocketException: Connection reset for many domains,"
I'm creating a (well behaved) web spider and I notice that some servers are causing Apache HttpClient to give me a SocketException -- specifically:
java.net.SocketException: Connection reset

The code that causes this is:
// Execute the request
HttpResponse response; 
try {
    response = httpclient.execute(httpget); //httpclient is of type HttpClient
} catch (NullPointerException e) {
    return;//deep down in apache http sometimes throws a null pointer...  
}

For most servers it's just fine.  But for others, it immediately throws a SocketException.
Example of site that causes immediate SocketException: http://www.bhphotovideo.com/
Works great (as do most websites): http://www.google.com/
Now, as you can see, www.bhphotovideo.com loads fine in a web browser.  It also loads fine when I don't use Apache's HTTP Client.  (Code like this:)
 HttpURLConnection c = (HttpURLConnection)url.openConnection();  
 BufferedInputStream in = new BufferedInputStream(c.getInputStream());  
 Reader r = new InputStreamReader(in);     

 int i;  
 while ((i = r.read()) != -1) {  
      source.append((char) i);  
 }  

So, why don't I just use this code instead?  Well there are some key features in Apache's HTTP Client that I need to use.
Does anyone know what causes some servers to cause this exception?
Research so far:

Problem occurs on my local Mac dev machines AND an AWS EC2 Instance, so it's not a local firewall.
It seems the error isn't caused by the remote machine because the exception doesn't say ""by peer""
This stack overflow seems relavent java.net.SocketException: Connection reset but the answers don't show why this would happen only from Apache HTTP Client and not other approaches.

Bonus question: I'm doing a fair amount of crawling with this system.  Is there generally a better Java class for this other than Apache HTTP Client?  I've found a number of issues (such as the NullPointerException I have to catch in the code above).  It seems that HTTPClient is very picky about server communications -- more picky than I'd like for a crawler that can't just break when a server doesn't behave.
Thanks all! 
Solution
Honestly, I don't have a perfect solution, but it works, so that's good enough for me.
As pointed out by oleg below, Bixo has created a crawler that customizes HttpClient to be more forgiving to servers.  To ""get around"" the issue more than fix it, I just used SimpleHttpFetcher provided by Bixo here:
(linked removed - SO thinks I'm a spammer, so you'll have to google it yourself)
SimpleHttpFetcher fetch = new SimpleHttpFetcher(new UserAgent(""botname"",""contact@yourcompany.com"",""ENTER URL""));
try {
    FetchedResult result = fetch.fetch(""ENTER URL"");
    System.out.println(new String(result.getContent()));
} catch (BaseFetchException e) {
    e.printStackTrace();
}

The down side to this solution is that there are a lot of dependencies for Bixo -- so this may not be a good work around for everyone.  However, you can always just work through their use of DefaultHttpClient and see how they instantiated it to get it to work.  I decided to use the whole class because it handles some things for me, like automatic redirect following (and reporting the final destination url) that are helpful.
Thanks for the help all.
Edit: TinyBixo
Hi all.  So, I loved how Bixo worked, but didn't like that it had so many dependencies (including all of Hadoop).  So, I created a vastly simplified Bixo, without all the dependencies.  If you're running into the problems above, I would recommend using it (and feel free to make pull requests if you'd like to update it!)
It's available here: https://github.com/juliuss/TinyBixo 
",44k,"
            9
        ","[""\nFirst, to answer your question:\nThe connection reset was caused by a problem on the server side. Most likely the server failed to parse the request or was unable to process it and dropped the connection as a result without returning a valid response. There is likely something in the HTTP requests generated by HttpClient that causes server side logic to fail, probably due to a server side bug. Just because the error message does not say 'by peer' does not mean the connection reset took place on the client side. \nA few remarks:\n(1) Several popular web crawlers such as bixo http://openbixo.org/ use HttpClient without major issues but pretty much of them had to tweak HttpClient behavior to make it more lenient about common HTTP protocol violations. Per default HttpClient is rather strict about the HTTP protocol compliance.\n(2) Why did not you report the NPE problem or any other problem you have been experiencing to the HttpClient project?\n"", '\nThese two settings will sometimes help:\n client.getParams().setParameter(""http.socket.timeout"", new Integer(0));\n client.getParams().setParameter(""http.connection.stalecheck"", new  Boolean(true));\n\nThe first sets the socket timeout to be infinite.\n', '\nTry getting a network trace using wireshark, and augment that with log4j logging of the HTTPClient. That should show why the connection is being reset\n']",https://stackoverflow.com/questions/5280577/apache-httpclient-throws-java-net-socketexception-connection-reset-for-many-dom,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web Crawling (Ajax/JavaScript enabled pages) using java,"
I am very new to this web crawling. I am using crawler4j to crawl the websites. I am collecting the required information by crawling these sites. My problem here is I was unable to crawl the content for the following site. http://www.sciencedirect.com/science/article/pii/S1568494612005741. I want to crawl the following information from the aforementioned site (Please take a look at the attached screenshot).

If you observe the attached screenshot it has three names (Highlighted in red boxes). If you click one of the link you will see a popup and that popup contains the whole information about that author. I want to crawl the information which are there in that popup.
I am using the following code to crawl the content.
public class WebContentDownloader {

private Parser parser;
private PageFetcher pageFetcher;

public WebContentDownloader() {
    CrawlConfig config = new CrawlConfig();
    parser = new Parser(config);
    pageFetcher = new PageFetcher(config);
}

private Page download(String url) {
    WebURL curURL = new WebURL();
    curURL.setURL(url);
    PageFetchResult fetchResult = null;
    try {
        fetchResult = pageFetcher.fetchHeader(curURL);
        if (fetchResult.getStatusCode() == HttpStatus.SC_OK) {
            try {
                Page page = new Page(curURL);
                fetchResult.fetchContent(page);
                if (parser.parse(page, curURL.getURL())) {
                    return page;
                }
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    } finally {
        if (fetchResult != null) {
            fetchResult.discardContentIfNotConsumed();
        }
    }
    return null;
}

private String processUrl(String url) {
    System.out.println(""Processing: "" + url);
    Page page = download(url);
    if (page != null) {
        ParseData parseData = page.getParseData();
        if (parseData != null) {
            if (parseData instanceof HtmlParseData) {
                HtmlParseData htmlParseData = (HtmlParseData) parseData;
                return htmlParseData.getHtml();
            }
        } else {
            System.out.println(""Couldn't parse the content of the page."");
        }
    } else {
        System.out.println(""Couldn't fetch the content of the page."");
    }
    return null;
}

public String getHtmlContent(String argUrl) {
    return this.processUrl(argUrl);
}
}

I was able to crawl the content from the aforementioned link/site. But it doesn't have the information what I marked in the red boxes. I think those are the dynamic links.

My question is how can I crawl the content from the aforementioned link/website...???
How to crawl the content from Ajax/JavaScript based websites...???

Please can anyone help me on this.
Thanks & Regards,
Amar
",20k,"
            9
        ","['\nHi I found the workaround with the another library. I used \nSelinium WebDriver (org.openqa.selenium.WebDriver) library to extract the dynamic content. Here is the sample code.\npublic class CollectUrls {\n\nprivate WebDriver driver;\n\npublic CollectUrls() {\n    this.driver = new FirefoxDriver();\n    this.driver.manage().timeouts().implicitlyWait(30, TimeUnit.SECONDS);\n}\n\nprotected void next(String url, List<String> argUrlsList) {\n    this.driver.get(url);\n    String htmlContent = this.driver.getPageSource();\n}\n\nHere the ""htmlContent"" is the required one. Please let me know if you face any issues...???\nThanks,\nAmar\n', ""\nSimply said, Crawler4j is static crawler. Meaning that it can't parse the JavaScript on a page. So there is no way of getting the content you want by crawling that specific page you mentioned. Of course there are some workarounds to get it working.\nIf it is just this page you want to crawl, you could use a connection debugger. Check out this question for some tools. Find out which page the AJAX-request calls, and crawl that page.\nIf you have various websites which have dynamic content (JavaScript/ajax), you should consider using a dynamic-content-enabled crawler, like Crawljax (also written in Java).\n"", '\nI have find out the Solution of Dynamic Web page Crawling using Aperture and Selenium.Web Driver.\nAperture is Crawling Tools and Selenium is Testing Tools which can able to rendering Inspect Element. \n\n1. Extract the Aperture- core Jar file by Decompiler Tools and Create a Simple Web Crawling Java program. (https://svn.code.sf.net/p/aperture/code/aperture/trunk/)\n2. Download Selenium. WebDriver Jar Files and Added to Your Program.\n3. Go to CreatedDataObjec() method in org.semanticdesktop.aperture.accessor.http.HttpAccessor.(Aperture Decompiler).\nAdded Below Coding \n\n   WebDriver driver = new FirefoxDriver();\n   String baseurl=uri.toString();\n   driver.get(uri.toString());\n   String str = driver.getPageSource();\n        driver.close();\n stream= new ByteArrayInputStream(str.getBytes());\n\n']",https://stackoverflow.com/questions/24365154/web-crawling-ajax-javascript-enabled-pages-using-java,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mass Downloading of Webpages C#,"
My application requires that I download a large amount of webpages into memory for further parsing and processing. What is the fastest way to do it? My current method (shown below) seems to be too slow and occasionally results in timeouts.
for (int i = 1; i<=pages; i++)
{
    string page_specific_link = baseurl + ""&page="" + i.ToString();

    try
    {    
        WebClient client = new WebClient();
        var pagesource = client.DownloadString(page_specific_link);
        client.Dispose();
        sourcelist.Add(pagesource);
    }
    catch (Exception)
    {
    }
}

",5k,"
            8
        ","['\nThe way you approach this problem is going to depend very much on how many pages you want to download, and how many sites you\'re referencing.\nI\'ll use a good round number like 1,000. If you want to download that many pages from a single site, it\'s going to take a lot longer than if you want to download 1,000 pages that are spread out across dozens or hundreds of sites. The reason is that if you hit a single site with a whole bunch of concurrent requests, you\'ll probably end up getting blocked.\nSo you have to implement a type of ""politeness policy,"" that issues a delay between multiple requests on a single site. The length of that delay depends on a number of things. If the site\'s robots.txt file has a crawl-delay entry, you should respect that. If they don\'t want you accessing more than one page per minute, then that\'s as fast as you should crawl. If there\'s no crawl-delay, you should base your delay on how long it takes a site to respond. For example, if you can download a page from the site in 500 milliseconds, you set your delay to X. If it takes a full second, set your delay to 2X. You can probably cap your delay to 60 seconds (unless crawl-delay is longer), and I would recommend that you set a minimum delay of 5 to 10 seconds.\nI wouldn\'t recommend using Parallel.ForEach for this. My testing has shown that it doesn\'t do a good job. Sometimes it over-taxes the connection and often it doesn\'t allow enough concurrent connections. I would instead create a queue of WebClient instances and then write something like:\n// Create queue of WebClient instances\nBlockingCollection<WebClient> ClientQueue = new BlockingCollection<WebClient>();\n// Initialize queue with some number of WebClient instances\n\n// now process urls\nforeach (var url in urls_to_download)\n{\n    var worker = ClientQueue.Take();\n    worker.DownloadStringAsync(url, ...);\n}\n\nWhen you initialize the WebClient instances that go into the queue, set their OnDownloadStringCompleted event handlers to point to a completed event handler. That handler should save the string to a file (or perhaps you should just use DownloadFileAsync), and then the client, adds itself back to the ClientQueue.\nIn my testing, I\'ve been able to support 10 to 15 concurrent connections with this method. Any more than that and I run into problems with DNS resolution (`DownloadStringAsync\'  doesn\'t do the DNS resolution asynchronously). You can get more connections, but doing so is a lot of work.\nThat\'s the approach I\'ve taken in the past, and it\'s worked very well for downloading thousands of pages quickly. It\'s definitely not the approach I took with my high performance Web crawler, though.\nI should also note that there is a huge difference in resource usage between these two blocks of code:\nWebClient MyWebClient = new WebClient();\nforeach (var url in urls_to_download)\n{\n    MyWebClient.DownloadString(url);\n}\n\n---------------\n\nforeach (var url in urls_to_download)\n{\n    WebClient MyWebClient = new WebClient();\n    MyWebClient.DownloadString(url);\n}\n\nThe first allocates a single WebClient instance that is used for all requests. The second allocates one WebClient for each request. The difference is huge. WebClient uses a lot of system resources, and allocating thousands of them in a relatively short time is going to impact performance. Believe me ... I\'ve run into this. You\'re better off allocating just 10 or 20 WebClients (as many as you need for concurrent processing), rather than allocating one per request.\n', '\nWhy not just use a web crawling framework. It can handle all the stuff for you like (multithreading, httprequests, parsing links, scheduling, politeness, etc..). \nAbot (https://code.google.com/p/abot/) handles all that stuff for you and is written in c#.\n', '\nIn addition to @Davids perfectly valid answer, I want to add a slightly cleaner ""version"" of his approach.\nvar pages = new List<string> { ""http://bing.com"", ""http://stackoverflow.com"" };\nvar sources = new BlockingCollection<string>();\n\nParallel.ForEach(pages, x =>\n{\n    using(var client = new WebClient())\n    {\n        var pagesource = client.DownloadString(x);\n        sources.Add(pagesource);\n    }\n});\n\n\nYet another approach, that uses async:\nstatic IEnumerable<string> GetSources(List<string> pages)\n{\n    var sources = new BlockingCollection<string>();\n    var latch = new CountdownEvent(pages.Count);\n\n    foreach (var p in pages)\n    {\n        using (var wc = new WebClient())\n        {\n            wc.DownloadStringCompleted += (x, e) =>\n            {\n                sources.Add(e.Result);\n                latch.Signal();\n            };\n\n            wc.DownloadStringAsync(new Uri(p));\n        }\n    }\n\n    latch.Wait();\n\n    return sources;\n}\n\n', '\nYou should use parallel programming for this purpose.\nThere are a lot of ways to achieve what u want; the easiest would be something like this:\nvar pageList = new List<string>();\n\nfor (int i = 1; i <= pages; i++)\n{\n  pageList.Add(baseurl + ""&page="" + i.ToString());\n}\n\n\n// pageList  is a list of urls\nParallel.ForEach<string>(pageList, (page) =>\n{\n  try\n    {\n      WebClient client = new WebClient();\n      var pagesource = client.DownloadString(page);\n      client.Dispose();\n      lock (sourcelist)\n      sourcelist.Add(pagesource);\n    }\n\n    catch (Exception) {}\n});\n\n', '\nI Had a similar Case ,and that\'s how i solved \nusing System;\n    using System.Threading;\n    using System.Collections.Generic;\n    using System.Net;\n    using System.IO;\n\nnamespace WebClientApp\n{\nclass MainClassApp\n{\n    private static int requests = 0;\n    private static object requests_lock = new object();\n\n    public static void Main() {\n\n        List<string> urls = new List<string> { ""http://www.google.com"", ""http://www.slashdot.org""};\n        foreach(var url in urls) {\n            ThreadPool.QueueUserWorkItem(GetUrl, url);\n        }\n\n        int cur_req = 0;\n\n        while(cur_req<urls.Count) {\n\n            lock(requests_lock) {\n                cur_req = requests; \n            }\n\n            Thread.Sleep(1000);\n        }\n\n        Console.WriteLine(""Done"");\n    }\n\nprivate static void GetUrl(Object the_url) {\n\n        string url = (string)the_url;\n        WebClient client = new WebClient();\n        Stream data = client.OpenRead (url);\n\n        StreamReader reader = new StreamReader(data);\n        string html = reader.ReadToEnd ();\n\n        /// Do something with html\n        Console.WriteLine(html);\n\n        lock(requests_lock) {\n            //Maybe you could add here the HTML to SourceList\n            requests++; \n        }\n    }\n}\n\nYou should think using Paralel\'s because the slow speed is because you\'re software is waiting for I/O and why not while a thread i waiting for I/O another one get started.\n', '\nWhile the other answers are perfectly valid, all of them (at the time of this writing) are neglecting something very important: calls to the web are IO bound, having a thread wait on an operation like this is going to strain system resources and have an impact on your system resources.\nWhat you really want to do is take advantage of the async methods on the WebClient class (as some have pointed out) as well as the Task Parallel Library\'s ability to handle the Event-Based Asynchronous Pattern.\nFirst, you would get the urls that you want to download:\nIEnumerable<Uri> urls = pages.Select(i => new Uri(baseurl + \n    ""&page="" + i.ToString(CultureInfo.InvariantCulture)));\n\nThen, you would create a new WebClient instance for each url, using the TaskCompletionSource<T> class to handle the calls asynchronously (this won\'t burn a thread):\nIEnumerable<Task<Tuple<Uri, string>> tasks = urls.Select(url => {\n    // Create the task completion source.\n    var tcs = new TaskCompletionSource<Tuple<Uri, string>>();\n\n    // The web client.\n    var wc = new WebClient();\n\n    // Attach to the DownloadStringCompleted event.\n    client.DownloadStringCompleted += (s, e) => {\n        // Dispose of the client when done.\n        using (wc)\n        {\n            // If there is an error, set it.\n            if (e.Error != null) \n            {\n                tcs.SetException(e.Error);\n            }\n            // Otherwise, set cancelled if cancelled.\n            else if (e.Cancelled) \n            {\n                tcs.SetCanceled();\n            }\n            else \n            {\n                // Set the result.\n                tcs.SetResult(new Tuple<string, string>(url, e.Result));\n            }\n        }\n    };\n\n    // Start the process asynchronously, don\'t burn a thread.\n    wc.DownloadStringAsync(url);\n\n    // Return the task.\n    return tcs.Task;\n});\n\nNow you have an IEnumerable<T> which you can convert to an array and wait on all of the results using Task.WaitAll:\n// Materialize the tasks.\nTask<Tuple<Uri, string>> materializedTasks = tasks.ToArray();\n\n// Wait for all to complete.\nTask.WaitAll(materializedTasks);\n\nThen, you can just use Result property on the Task<T> instances to get the pair of the url and the content:\n// Cycle through each of the results.\nforeach (Tuple<Uri, string> pair in materializedTasks.Select(t => t.Result))\n{\n    // pair.Item1 will contain the Uri.\n    // pair.Item2 will contain the content.\n}\n\nNote that the above code has the caveat of not having an error handling.\nIf you wanted to get even more throughput, instead of waiting for the entire list to be finished, you could process the content of a single page after it\'s done downloading; Task<T> is meant to be used like a pipeline, when you\'ve completed your unit of work, have it continue to the next one instead of waiting for all of the items to be done (if they can be done in an asynchronous manner).\n', '\nI am using an active Threads count and a arbitrary limit:\nprivate static volatile int activeThreads = 0;\n\npublic static void RecordData()\n{\n  var nbThreads = 10;\n  var source = db.ListOfUrls; // Thousands urls\n  var iterations = source.Length / groupSize; \n  for (int i = 0; i < iterations; i++)\n  {\n    var subList = source.Skip(groupSize* i).Take(groupSize);\n    Parallel.ForEach(subList, (item) => RecordUri(item)); \n    //I want to wait here until process further data to avoid overload\n    while (activeThreads > 30) Thread.Sleep(100);\n  }\n}\n\nprivate static async Task RecordUri(Uri uri)\n{\n   using (WebClient wc = new WebClient())\n   {\n      Interlocked.Increment(ref activeThreads);\n      wc.DownloadStringCompleted += (sender, e) => Interlocked.Decrement(ref iterationsCount);\n      var jsonData = """";\n      RootObject root;\n      jsonData = await wc.DownloadStringTaskAsync(uri);\n      var root = JsonConvert.DeserializeObject<RootObject>(jsonData);\n      RecordData(root)\n    }\n}\n\n']",https://stackoverflow.com/questions/7474413/mass-downloading-of-webpages-c-sharp,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running Multiple spiders in scrapy for 1 website in parallel?,"
I want to crawl a website with 2 parts and my script is not as fast as I need.
Is it possible to launch 2 spiders, one for scraping the first part and the second one for the second part? 
I tried to have 2 different classes, and run them
scrapy crawl firstSpider
scrapy crawl secondSpider

but i think that it is not smart.
I read the documentation of scrapyd but I don't know if it's good for my case.
",21k,"
            7
        ","['\nI think what you are looking for is something like this:\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider1(scrapy.Spider):\n    # Your first spider definition\n    ...\n\nclass MySpider2(scrapy.Spider):\n    # Your second spider definition\n    ...\n\nprocess = CrawlerProcess()\nprocess.crawl(MySpider1)\nprocess.crawl(MySpider2)\nprocess.start() # the script will block here until all crawling jobs are finished\n\nYou can read more at: running-multiple-spiders-in-the-same-process.\n', '\nOr you can run with like this, you need to save this code at the same directory with scrapy.cfg (My scrapy version is 1.3.3) :\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spiders.list():\n    print (""Running spider %s"" % (spider_name))\n    process.crawl(spider_name,query=""dvh"") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n\n', '\nBetter solution is (if you have multiple spiders) it dynamically get spiders and run them.\nfrom scrapy import spiderloader\nfrom scrapy.utils import project\nfrom twisted.internet.defer import inlineCallbacks\n\n\n@inlineCallbacks\ndef crawl():\n    settings = project.get_project_settings()\n    spider_loader = spiderloader.SpiderLoader.from_settings(settings)\n    spiders = spider_loader.list()\n    classes = [spider_loader.load(name) for name in spiders]\n    for my_spider in classes:\n        yield runner.crawl(my_spider)\n    reactor.stop()\n\ncrawl()\nreactor.run()\n\n(Second Solution):\nBecause spiders.list() is deprecated in Scrapy 1.4 Yuda solution should be converted to something like\nfrom scrapy import spiderloader    \nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsettings = get_project_settings()\nprocess = CrawlerProcess(settings)\nspider_loader = spiderloader.SpiderLoader.from_settings(settings)\n\nfor spider_name in spider_loader.list():\n    print(""Running spider %s"" % (spider_name))\n    process.crawl(spider_name)\nprocess.start()\n\n']",https://stackoverflow.com/questions/39365131/running-multiple-spiders-in-scrapy-for-1-website-in-parallel,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Send parallel requests but only one per host with HttpClient and Polly to gracefully handle 429 responses,"
Intro:
I am building a single-node web crawler to simply validate URLs are 200 OK in a .NET Core console application. I have a collection of URLs at different hosts to which I am sending requests with HttpClient. I am fairly new to using Polly and TPL Dataflow.
Requirements:

I want to support sending multiple HTTP requests in parallel with a
configurable MaxDegreeOfParallelism.
I want to limit the number of parallel requests to any given host to 1 (or configurable). This is in order to gracefully handle per-host 429 TooManyRequests responses with a Polly policy. Alternatively, I could maybe use a Circuit Breaker to cancel concurrent requests to the same host on receipt of one 429 response and then proceed one-at-a-time to that specific host?
I am perfectly fine with not using TPL Dataflow at all in favor of maybe using a Polly Bulkhead or some other mechanism for throttled parallel requests, but I am not sure what that configuration would look like in order to implement requirement #2.

Current Implementation:
My current implementation works, except that I often see that I'll have x parallel requests to the same host return 429 at about the same time... Then, they all pause for the retry policy... Then, they all slam the same host again at the same time often still receiving 429s. Even if I distribute multiple instances of the same host evenly throughout the queue, my URL collection is overweighted with a few specific hosts that still start generating 429s eventually.
After receiving a 429, I think I only want to send one concurrent request to that host going forward to respect the remote host and pursue 200s. 
Validator Method:
public async Task<int> GetValidCount(IEnumerable<Uri> urls, CancellationToken cancellationToken)
{
    var validator = new TransformBlock<Uri, bool>(
        async u => (await _httpClient.GetAsync(u, HttpCompletionOption.ResponseHeadersRead, cancellationToken)).IsSuccessStatusCode,
        new ExecutionDataflowBlockOptions {MaxDegreeOfParallelism = MaxDegreeOfParallelism}
    );
    foreach (var url in urls)
        await validator.SendAsync(url, cancellationToken);
    validator.Complete();
    var validUrlCount = 0;
    while (await validator.OutputAvailableAsync(cancellationToken))
    {
        if(await validator.ReceiveAsync(cancellationToken))
            validUrlCount++;
    }
    await validator.Completion;
    return validUrlCount;
}

The Polly policy applied to the HttpClient instance used in GetValidCount() above.
IAsyncPolicy<HttpResponseMessage> waitAndRetryTooManyRequests = Policy
    .HandleResult<HttpResponseMessage>(r => r.StatusCode == HttpStatusCode.TooManyRequests)
    .WaitAndRetryAsync(3,
        (retryCount, response, context) =>
            response.Result?.Headers.RetryAfter.Delta ?? TimeSpan.FromMilliseconds(120),
        async (response, timespan, retryCount, context) =>
        {
            // log stuff
        });

Question:
How can I modify or replace this solution to add satisfaction of requirement #2?
",1k,"
            6
        ","['\nI\'d try to introduce some sort of a flag LimitedMode  to detect that this particular client is entered in limited mode. Below I declare two policies - one simple retry policy just to catch TooManyRequests and set the flag. The second policy is a out-of-the-box BulkHead policy.\n    public void ConfigureServices(IServiceCollection services)\n    {\n        /* other configuration */\n\n        var registry = services.AddPolicyRegistry();\n\n        var catchPolicy = Policy.HandleResult<HttpResponseMessage>(r =>\n            {\n                LimitedMode = r.StatusCode == HttpStatusCode.TooManyRequests;\n                return false;\n            })\n            .WaitAndRetryAsync(1, i => TimeSpan.FromSeconds(3)); \n\n        var bulkHead = Policy.BulkheadAsync<HttpResponseMessage>(1, 10, OnBulkheadRejectedAsync);\n\n        registry.Add(""catchPolicy"", catchPolicy);\n        registry.Add(""bulkHead"", bulkHead);\n\n        services.AddHttpClient<CrapyWeatherApiClient>((client) =>\n        {\n            client.BaseAddress = new Uri(""hosturl"");\n        }).AddPolicyHandlerFromRegistry(PolicySelector);\n    }\n\nThen you may want to dynamically decide on which policy to apply using the PolicySelector mechanism: in case the limited mode is active - wrap bulk head policy with catch 429 policy. If the success status code received - switch back to regular mode without a bulkhead.\n    private IAsyncPolicy<HttpResponseMessage> PolicySelector(IReadOnlyPolicyRegistry<string> registry, HttpRequestMessage request)\n    {\n        var catchPolicy = registry.Get<IAsyncPolicy<HttpResponseMessage>>(""catchPolicy"");\n        var bulkHead = registry.Get<IAsyncPolicy<HttpResponseMessage>>(""bulkHead"");\n        if (LimitedMode)\n        {\n            return catchPolicy.WrapAsync(bulkHead);\n        }\n\n        return catchPolicy;\n    }        \n\n', '\nHere is a method that creates a TransformBlock which prevents concurrent execution for messages with the same key. The key of each message is obtained by invoking the supplied keySelector function. Messages with the same key are processed sequentially to each other (not in parallel). The key is also passed as an argument to the transform function, because it can be useful in some cases.\npublic static TransformBlock<TInput, TOutput>\n    CreateExclusivePerKeyTransformBlock<TInput, TKey, TOutput>(\n    Func<TInput, TKey, Task<TOutput>> transform,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (transform == null) throw new ArgumentNullException(nameof(transform));\n    if (keySelector == null) throw new ArgumentNullException(nameof(keySelector));\n    if (dataflowBlockOptions == null)\n        throw new ArgumentNullException(nameof(dataflowBlockOptions));\n    keyComparer = keyComparer ?? EqualityComparer<TKey>.Default;\n\n    var internalCTS = CancellationTokenSource\n        .CreateLinkedTokenSource(dataflowBlockOptions.CancellationToken);\n\n    var maxDOP = dataflowBlockOptions.MaxDegreeOfParallelism;\n    var taskScheduler = dataflowBlockOptions.TaskScheduler;\n\n    var perKeySemaphores = new ConcurrentDictionary<TKey, SemaphoreSlim>(\n        keyComparer);\n\n    SemaphoreSlim maxDopSemaphore;\n    if (maxDOP == DataflowBlockOptions.Unbounded)\n    {\n        maxDopSemaphore = new SemaphoreSlim(Int32.MaxValue);\n    }\n    else\n    {\n        maxDopSemaphore = new SemaphoreSlim(maxDOP, maxDOP);\n\n        // The degree of parallelism is controlled by the semaphore\n        dataflowBlockOptions.MaxDegreeOfParallelism = DataflowBlockOptions.Unbounded;\n\n        // Use a limited-concurrency scheduler for preserving the processing order\n        dataflowBlockOptions.TaskScheduler = new ConcurrentExclusiveSchedulerPair(\n            taskScheduler, maxDOP).ConcurrentScheduler;\n    }\n\n    var block = new TransformBlock<TInput, TOutput>(async item =>\n    {\n        var key = keySelector(item);\n        var perKeySemaphore = perKeySemaphores\n            .GetOrAdd(key, _ => new SemaphoreSlim(1, 1));\n\n        // Continue on captured context before invoking the transform\n        await perKeySemaphore.WaitAsync(internalCTS.Token);\n        try\n        {\n            await maxDopSemaphore.WaitAsync(internalCTS.Token);\n            try\n            {\n                return await transform(item, key).ConfigureAwait(false);\n            }\n            catch (Exception ex) when (!(ex is OperationCanceledException))\n            {\n                internalCTS.Cancel(); // The block has failed\n                throw;\n            }\n            finally\n            {\n                maxDopSemaphore.Release();\n            }\n        }\n        finally\n        {\n            perKeySemaphore.Release();\n        }\n    }, dataflowBlockOptions);\n\n    dataflowBlockOptions.MaxDegreeOfParallelism = maxDOP; // Restore initial value\n    dataflowBlockOptions.TaskScheduler = taskScheduler; // Restore initial value\n    return block;\n}\n\nUsage example:\nvar validator = CreateExclusivePerKeyTransformBlock<Uri, string, bool>(\n    async (uri, host) =>\n    {\n        return (await _httpClient.GetAsync(uri, HttpCompletionOption\n            .ResponseHeadersRead, token)).IsSuccessStatusCode;\n    },\n    new ExecutionDataflowBlockOptions\n    {\n        MaxDegreeOfParallelism = 30,\n        CancellationToken = token,\n    },\n    keySelector: uri => uri.Host,\n    keyComparer: StringComparer.OrdinalIgnoreCase);\n\nAll execution options are supported (MaxDegreeOfParallelism, BoundedCapacity, CancellationToken, EnsureOrdered etc).\nBelow is an overload of the CreateExclusivePerKeyTransformBlock that accepts a synchronous delegate, and another method+overload that returns an ActionBlock instead of a TransformBlock, with the same behavior.\npublic static TransformBlock<TInput, TOutput>\n    CreateExclusivePerKeyTransformBlock<TInput, TKey, TOutput>(\n    Func<TInput, TKey, TOutput> transform,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (transform == null) throw new ArgumentNullException(nameof(transform));\n    return CreateExclusivePerKeyTransformBlock(\n        (item, key) => Task.FromResult(transform(item, key)),\n        dataflowBlockOptions, keySelector, keyComparer);\n}\n\n// An ITargetBlock is similar to an ActionBlock\npublic static ITargetBlock<TInput>\n    CreateExclusivePerKeyActionBlock<TInput, TKey>(\n    Func<TInput, TKey, Task> action,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (action == null) throw new ArgumentNullException(nameof(action));\n    var block = CreateExclusivePerKeyTransformBlock(async (item, key) =>\n        { await action(item, key).ConfigureAwait(false); return (object)null; },\n        dataflowBlockOptions, keySelector, keyComparer);\n    block.LinkTo(DataflowBlock.NullTarget<object>());\n    return block;\n}\n\npublic static ITargetBlock<TInput>\n    CreateExclusivePerKeyActionBlock<TInput, TKey>(\n    Action<TInput, TKey> action,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (action == null) throw new ArgumentNullException(nameof(action));\n    return CreateExclusivePerKeyActionBlock(\n        (item, key) => { action(item, key); return Task.CompletedTask; },\n        dataflowBlockOptions, keySelector, keyComparer);\n}\n\n\nCaution: This class allocates one SemaphoreSlim per key, and keeps a reference to it until the class instance is finally garbage collected. This could be an issue in case the number of different keys is huge. There is an implementation of a less allocatey async lock here, that stores internally only the SemaphoreSlims that are currently in use (plus a small pool of released SemaphoreSlims that can be reused), which could replace the ConcurrentDictionary<TKey, SemaphoreSlim> used by this implementation.\n']",https://stackoverflow.com/questions/57022754/send-parallel-requests-but-only-one-per-host-with-httpclient-and-polly-to-gracef,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to follow all links in CasperJS?,"
I'm having trouble clicking all JavaScript based links in a DOM and saving the
output. The links have the form 
<a id=""html"" href=""javascript:void(0);"" onclick=""goToHtml();"">HTML</a>

the following code works great:
var casper = require('casper').create();

var fs = require('fs');

var firstUrl = 'http://www.testurl.com/test.html';

var css_selector = '#jan_html';

casper.start(firstUrl);

casper.thenClick(css_selector, function(){
    console.log(""whoop"");
});

casper.waitFor(function check() {
    return this.getCurrentUrl() != firstUrl;
}, function then() {
    console.log(this.getCurrentUrl());
    var file_title = this.getTitle().split(' ').join('_') + '.html';
    fs.write(file_title, this.getPageContent());
});

casper.run();

However, how can I get this to work with a selector of ""a"", clicking all
available links and saving content? I'm not sure how to get the clickWhileSelector to remove nodes from the selector as is done here: Click on all links matching a selector
",9k,"
            5
        ","['\nI have this script that first will get all links from a page then save \'href\' attributes to an array, then will iterate over this array and then open each link one by one and echo the url :\nvar casper = require(\'casper\').create({\n    logLevel:""verbose"",\n    debug:true\n});\nvar links;\n\ncasper.start(\'http://localhost:8000\');\n\ncasper.then(function getLinks(){\n     links = this.evaluate(function(){\n        var links = document.getElementsByTagName(\'a\');\n        links = Array.prototype.map.call(links,function(link){\n            return link.getAttribute(\'href\');\n        });\n        return links;\n    });\n});\ncasper.then(function(){\n    this.each(links,function(self,link){\n        self.thenOpen(link,function(a){\n            this.echo(this.getCurrentUrl());\n        });\n    });\n});\ncasper.run(function(){\n    this.exit();\n});\n\n', '\nrusln\'s answer works great if all the links have a meaningful href attribute (actual URL). If you want to click every a that also triggers a javascript function, you may need to iterate some other way over the elements.\nI propose using the XPath generator from stijn de ryck for an element. \n\nYou can then sample all XPaths that are on the page. \nThen you open the page for every a that you have the XPath for and click it by XPath. \nWait a little if it is a single page application\nDo something\n\nvar startURL = \'http://localhost:8000\',\n    xPaths\n    x = require(\'casper\').selectXPath;\n\ncasper.start(startURL);\n\ncasper.then(function getLinks(){\n    xPaths = this.evaluate(function(){\n        // copied from https://stackoverflow.com/a/5178132/1816580\n        function createXPathFromElement(elm) {\n            var allNodes = document.getElementsByTagName(\'*\'); \n            for (var segs = []; elm && elm.nodeType == 1; elm = elm.parentNode) { \n                if (elm.hasAttribute(\'id\')) { \n                        var uniqueIdCount = 0; \n                        for (var n=0;n < allNodes.length;n++) { \n                            if (allNodes[n].hasAttribute(\'id\') && allNodes[n].id == elm.id) uniqueIdCount++; \n                            if (uniqueIdCount > 1) break; \n                        }; \n                        if ( uniqueIdCount == 1) { \n                            segs.unshift(\'id(""\' + elm.getAttribute(\'id\') + \'"")\'); \n                            return segs.join(\'/\'); \n                        } else { \n                            segs.unshift(elm.localName.toLowerCase() + \'[@id=""\' + elm.getAttribute(\'id\') + \'""]\'); \n                        } \n                } else if (elm.hasAttribute(\'class\')) { \n                    segs.unshift(elm.localName.toLowerCase() + \'[@class=""\' + elm.getAttribute(\'class\') + \'""]\'); \n                } else { \n                    for (i = 1, sib = elm.previousSibling; sib; sib = sib.previousSibling) { \n                        if (sib.localName == elm.localName)  i++; }; \n                        segs.unshift(elm.localName.toLowerCase() + \'[\' + i + \']\'); \n                }; \n            }; \n            return segs.length ? \'/\' + segs.join(\'/\') : null; \n        };\n        var links = document.getElementsByTagName(\'a\');\n        var xPaths = Array.prototype.map.call(links, createXPathFromElement);\n        return xPaths;\n    });\n});\ncasper.then(function(){\n    this.each(xPaths, function(self, xpath){\n        self.thenOpen(startURL);\n        self.thenClick(x(xpath));\n        // waiting some time may be necessary for single page applications\n        self.wait(1000);\n        self.then(function(a){\n            // do something meaningful here\n            this.echo(this.getCurrentUrl());\n        });\n\n        // Uncomment the following line in case each click opens a new page instead of staying at the same page\n        //self.back()\n    });\n});\ncasper.run(function(){\n    this.exit();\n});\n\n']",https://stackoverflow.com/questions/20224687/how-to-follow-all-links-in-casperjs,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrapy upload file,"
I am making a form request to a website using scrapy. The form requires to upload a pdf file, How can we do it in Scrapy. I am trying this like -
FormRequest(url,callback=self.parseSearchResponse,method=""POST"",formdata={'filename':'abc.xyz','file':'path to file/abc.xyz'})

",2k,"
            4
        ","['\nAt this very moment Scrapy has no built-in support for uploading files.\nFile uploading via forms in HTTP was specified in RFC1867. According to the spec, an HTTP request with Content-Type: multipart/form-data is required (in your code it would be application/x-www-form-urlencoded).\nTo achieve file uploading with Scrapy, you would need to:\n\nGet familiar with the basic concepts of HTTP file uploading.\nStart with scrapy.Request (instead of FormRequest).\nGive it a proper Content-Type header value.\nBuild the request body yourself.\n\nSee also: How does HTTP file upload work?\n', '\nI just spent an entire day trying to figure out how to implement this.\nFinally, I came upon a Scrapy pull request from 2016 that was never merged, with an implementation of a multipart form request:\nfrom scrapy import FormRequest\nfrom six.moves.urllib.parse import urljoin, urlencode\nimport lxml.html\nfrom parsel.selector import create_root_node\nimport six\nimport string\nimport random\nfrom scrapy.http.request import Request\nfrom scrapy.utils.python import to_bytes, is_listlike\nfrom scrapy.utils.response import get_base_url\n\n\nclass MultipartFormRequest(FormRequest):\n\n    def __init__(self, *args, **kwargs):\n        formdata = kwargs.pop(\'formdata\', None)\n\n        kwargs.setdefault(\'method\', \'POST\')\n\n        super(MultipartFormRequest, self).__init__(*args, **kwargs)\n\n        content_type = self.headers.setdefault(b\'Content-Type\', [b\'multipart/form-data\'])[0]\n        method = kwargs.get(\'method\').upper()\n        if formdata and method == \'POST\' and content_type == b\'multipart/form-data\':\n            items = formdata.items() if isinstance(formdata, dict) else formdata\n            self._boundary = \'\'\n\n            # encode the data using multipart spec\n            self._boundary = to_bytes(\'\'.join(\n                random.choice(string.digits + string.ascii_letters) for i in range(20)), self.encoding)\n            self.headers[b\'Content-Type\'] = b\'multipart/form-data; boundary=\' + self._boundary\n            request_data = _multpart_encode(items, self._boundary, self.encoding)\n            self._set_body(request_data)\n\n\nclass MultipartFile(object):\n\n    def __init__(self, name, content, mimetype=\'application/octet-stream\'):\n        self.name = name\n        self.content = content\n        self.mimetype = mimetype\n\n\ndef _get_form_url(form, url):\n    if url is None:\n        return urljoin(form.base_url, form.action)\n    return urljoin(form.base_url, url)\n\n\ndef _urlencode(seq, enc):\n    values = [(to_bytes(k, enc), to_bytes(v, enc))\n              for k, vs in seq\n              for v in (vs if is_listlike(vs) else [vs])]\n    return urlencode(values, doseq=1)\n\n\ndef _multpart_encode(items, boundary, enc):\n    body = []\n\n    for name, value in items:\n        body.append(b\'--\' + boundary)\n        if isinstance(value, MultipartFile):\n            file_name = value.name\n            content = value.content\n            content_type = value.mimetype\n\n            body.append(\n                b\'Content-Disposition: form-data; name=""\' + to_bytes(name, enc) + b\'""; filename=""\' + to_bytes(file_name,\n                                                                                                              enc) + b\'""\')\n            body.append(b\'Content-Type: \' + to_bytes(content_type, enc))\n            body.append(b\'\')\n            body.append(to_bytes(content, enc))\n        else:\n            body.append(b\'Content-Disposition: form-data; name=""\' + to_bytes(name, enc) + b\'""\')\n            body.append(b\'\')\n            body.append(to_bytes(value, enc))\n\n    body.append(b\'--\' + boundary + b\'--\')\n    return b\'\\r\\n\'.join(body)\n\n\ndef _get_form(response, formname, formid, formnumber, formxpath):\n    """"""Find the form element """"""\n    root = create_root_node(response.text, lxml.html.HTMLParser,\n                            base_url=get_base_url(response))\n    forms = root.xpath(\'//form\')\n    if not forms:\n        raise ValueError(""No <form> element found in %s"" % response)\n\n    if formname is not None:\n        f = root.xpath(\'//form[@name=""%s""]\' % formname)\n        if f:\n            return f[0]\n\n    if formid is not None:\n        f = root.xpath(\'//form[@id=""%s""]\' % formid)\n        if f:\n            return f[0]\n\n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == \'form\':\n                    return el\n                el = el.getparent()\n                if el is None:\n                    break\n        encoded = formxpath if six.PY3 else formxpath.encode(\'unicode_escape\')\n        raise ValueError(\'No <form> element found with %s\' % encoded)\n\n    # If we get here, it means that either formname was None\n    # or invalid\n    if formnumber is not None:\n        try:\n            form = forms[formnumber]\n        except IndexError:\n            raise IndexError(""Form number %d not found in %s"" %\n                             (formnumber, response))\n        else:\n            return form\n\n\ndef _get_inputs(form, formdata, dont_click, clickdata, response):\n    try:\n        formdata = dict(formdata or ())\n    except (ValueError, TypeError):\n        raise ValueError(\'formdata should be a dict or iterable of tuples\')\n\n    inputs = form.xpath(\'descendant::textarea\'\n                        \'|descendant::select\'\n                        \'|descendant::input[not(@type) or @type[\'\n                        \' not(re:test(., ""^(?:submit|image|reset)$"", ""i""))\'\n                        \' and (../@checked or\'\n                        \'  not(re:test(., ""^(?:checkbox|radio)$"", ""i"")))]]\',\n                        namespaces={\n                            ""re"": ""http://exslt.org/regular-expressions""})\n    values = [(k, u\'\' if v is None else v)\n              for k, v in (_value(e) for e in inputs)\n              if k and k not in formdata]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n            values.append(clickable)\n\n    values.extend(formdata.items())\n    return values\n\n\ndef _value(ele):\n    n = ele.name\n    v = ele.value\n    if ele.tag == \'select\':\n        return _select_value(ele, n, v)\n    return n, v\n\n\ndef _select_value(ele, n, v):\n    multiple = ele.multiple\n    if v is None and not multiple:\n        # Match browser behaviour on simple select tag without options selected\n        # And for select tags wihout options\n        o = ele.value_options\n        return (n, o[0]) if o else (None, None)\n    elif v is not None and multiple:\n        # This is a workround to bug in lxml fixed 2.3.1\n        # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n        selected_options = ele.xpath(\'.//option[@selected]\')\n        v = [(o.get(\'value\') or o.text or u\'\').strip() for o in selected_options]\n    return n, v\n\n\ndef _get_clickable(clickdata, form):\n    """"""\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    """"""\n    clickables = [\n        el for el in form.xpath(\n            \'descendant::*[(self::input or self::button)\'\n            \' and re:test(@type, ""^submit$"", ""i"")]\'\n            \'|descendant::button[not(@type)]\',\n            namespaces={""re"": ""http://exslt.org/regular-expressions""})\n    ]\n    if not clickables:\n        return\n\n    # If we don\'t have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.get(\'name\'), el.get(\'value\') or \'\')\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get(\'nr\', None)\n    if nr is not None:\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (el.get(\'name\'), el.get(\'value\') or \'\')\n\n    # We didn\'t find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = u\'.//*\' + \\\n            u\'\'.join(u\'[@%s=""%s""]\' % c for c in six.iteritems(clickdata))\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].get(\'name\'), el[0].get(\'value\') or \'\')\n    elif len(el) > 1:\n        raise ValueError(""Multiple elements found (%r) matching the criteria ""\n                         ""in clickdata: %r"" % (el, clickdata))\n    else:\n        raise ValueError(\'No clickable element matching clickdata: %r\' % (clickdata,))\n\nThis is the code I used to call the request (in my case I needed to upload an image):\nwith open(img_path, \'rb\') as file:\n    img = file.read()\n    file_name = os.path.basename(img_path)\n    multipart_file = MultipartFile(file_name, img, ""image/png"")\n    form_data = {\n        ""param"": ""value"", # this is an example of a text parameter\n        ""PicUpload"": multipart_file\n    }\n    yield MultipartFormRequest(url=upload_url, formdata=form_data,\n                               callback=self.my_callback)\n\nIt\'s a shame that so much time has passed and Scrapy still doesn\'t have a built in way to do this, especially since someone wrote a very simple implementation years ago.\n']",https://stackoverflow.com/questions/39303851/scrapy-upload-file,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make a JavaScript-aware Crawler,"
I want to make a script that's crawling a website and it should return the locations of all the banners showed on that page.
The locations of banners are most of the time from known domains. But banners are not in the HTML as an easy image or swf-file. Most of the times a Javascript is used to show the banner.
So if a .swf-file or image-file is loaded from a banner-domain, it should return that url.
Is that possible to do? And how could I do that roughly?
Best would be if it can also returns the landing page of that ad. How to solve that?
",2k,"
            2
        ","[""\nYou could use selenium to open the pages in a real browser and then access the DOM.\nPhantomJS might also be worth a look - it's a headless version of WebKit (the engine behind Chrome, Safari, etc.).\nHowever, none of those solutions are pure php - if that's a requirement, you'll probably have to write your own JavaScript engine in PHP (which is nothing I'd ask my worst enemy to do ;))\n"", '\nIn order to get the output of the JavaScript you will need a JavaScript engine (such as Google\'s V8 Engine). The V8 engine is written in C++ but there are some resources that tell you embed the V8 engine into PHP.\nWith that said, you have to study the output ""by hand"" and determine exactly what can be scraped and how to identify it. Once you\'ve identified some common syntax for the advertisement banners, then you can write a script to extract the banner and the landing page which is referenced.\nNone of this is easy work, but if you have an example of an ad you\'d like to collect then I can give you more advice.\n']",https://stackoverflow.com/questions/8326301/make-a-javascript-aware-crawler,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Are Robots.txt and metadata tags enough to stop search engines to index dynamic pages that are dependent of $_GET variables?,"
I created a php page that is only accessible by means of token/pass received through $_GET
Therefore if you go to the following url you'll get a generic or blank page

http://fakepage11.com/secret_page.php

However if you used the link with the token it shows you special content

http://fakepage11.com/secret_page.php?token=344ee833bde0d8fa008de206606769e4

Of course this is not as safe as a login page, but my only concern is to create a dynamic page that is not indexable and only accessed through the provided link.
Are dynamic pages that are dependent of $_GET variables indexed by google and other search engines?
If so, will include the following be enough to hide it?

Robots.txt User-agent: * Disallow: /

metadata: <META NAME=""ROBOTS"" CONTENT=""NOINDEX"">


Even if I type into google:

site:fakepage11.com/

Thank you!
",936,"
            2
        ","['\nIf a search engine bot finds the link with the token somehow鹿, it may crawl and index it.\nIf you use robots.txt to disallow crawling the page, conforming search engine bots won鈥檛 crawl the page, but they may still index its URL (which then might appear in a site: search).\nIf you use meta-robots to disallow indexing the page, conforming search engine bots won鈥檛 index the page, but they may still crawl it.\nYou can鈥檛 have both: If you disallow crawling, conforming bots can never learn that you also disallow indexing, because they are not allowed to visit the page to see your meta-robots element. \n鹿 There are countless ways how search engines might find a link. For example, a user that visits the page might use a browser toolbar that automatically sends all visited URLs to a search engine.\n', '\nIf your page isn\'t discoverable then it will not be indexed.\nby ""discoverable"" we mean:\n\nit is a standard web page, i.e. index.*\nit is referenced by another link either yours or from another site\n\nSo in your case by using the get parameter for access, you achieve 1 but not necessarily 2 since someone may reference that link and hence the ""hidden"" page.\nYou can use the robots.txt that you gave and in that case the page will not get indexed by a bot that respects that (not all will do). Not indexing your page doesn\'t mean of course that the ""hidden"" page URL will not be in the wild.\nFurthermore another issue - depending on your requirements - is that you use unencrypted HTTP, that means that your ""hidden"" URLs and content of pages are visible to every server between your server and the user.\nApart from search engines take care that certain services are caching/resolving content when URLs are exchanged for example in Skype or Facebook messenger. In that cases they will visit the URL and try to extract metadata and  maybe cache it if applicable. Of course this scenario does not expose your URL to the public but it is exposed to the systems of those services and with them the content that you have ""hidden"".\nUPDATE:\nAnother issue to consider is the exposing of a ""hidden"" page by linking to another page. In that case in the logs of the server that hosts the linked URL your page will be seen as a referral and thus be visible, that expands also to Google Analytics etc. Thus if you want to remain stealth do not link to another pages from the hidden page.\n']",https://stackoverflow.com/questions/35504252/are-robots-txt-and-metadata-tags-enough-to-stop-search-engines-to-index-dynamic,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
redirect all bots using htaccess apache,"
What .htaccess rewriterule should i use to detect known bots, for example the big ones:
altavista, google, bing, yahoo
I know i can check for their ips, or hosts, but is there a better way?
",3k,"
            1
        ",['\nRewriteCond %{HTTP_USER_AGENT} AltaVista [OR]\nRewriteCond %{HTTP_USER_AGENT} Googlebot [OR]\nRewriteCond %{HTTP_USER_AGENT} msnbot [OR]\nRewriteCond %{HTTP_USER_AGENT} Slurp\nRewriteRule ^.*$ IHateBots.html [L]\n\n'],https://stackoverflow.com/questions/2691956/redirect-all-bots-using-htaccess-apache,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Puppeteer not giving accurate HTML code for page with shadow roots,"
I am trying to download the HTML code for the website intersight.com/help/. But puppeteer is not returning the HTML code with hrefs as we can see in the page (example https://intersight.com/help/getting_started is not present in the downloaded HTML). On inspecting the HTML in browser I came to know that all the missing HTML is present inside the <an-hulk></an-hulk> tags. I don't know what these tags mean.
const puppeteer = require('puppeteer');
const fs = require('fs');
(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  const data = await page.goto('https://intersight.com/help/', { waitUntil: 'domcontentloaded' });
  // Tried all the below lines, neither worked
  // await page.waitForSelector('.helplet-links')
  // document.querySelector(""#app > an-hulk"").shadowRoot.querySelector(""#content"").shadowRoot.querySelector(""#main > div > div > div > an-hulk-home"").shadowRoot.querySelector(""div > div > div:nth-child(1) > div:nth-child(1) > div.helplet-links > ul > li:nth-child(1) > a > span"")
  // await page.evaluateHandle(`document.querySelector(""#app > an-hulk"").shadowRoot.querySelector(""#content"").shadowRoot.querySelector(""#main > div > div > div > an-hulk-home"")`);
  await page.evaluateHandle(`document.querySelector(""an-hulk"").shadowRoot.querySelector(""#aside"").shadowRoot.querySelectorAll("".item"")`)
  const result = await page.content()
  fs.writeFile('./intersight.html', result, (err) => {
    if (err) console.log(err)
    else console.log('done!!')
  })
  // console.log(result)
  await browser.close();
})();

",938,"
            1
        ","['\nAs mentioned in the comments, you\'re dealing with a page that uses shadow roots. Traditional selectors that attempt to pierce shadow roots won\'t work through the console or Puppeteer without help. Short of using a library, the idea is to identify any shadow root elements by their .shadowRoot property, then dive into them recursively and repeat the process until you get the data you\'re after.\nThis code should grab all of the hrefs on the page (I didn\'t do a manual count) following this strategy:\nconst puppeteer = require(""puppeteer"");\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch({headless: true});\n  const [page] = await browser.pages();\n  const url = ""https://intersight.com/help/"";\n  const data = await page.goto(url, {\n    waitUntil: ""networkidle0""\n  });\n  await page.waitForSelector(""an-hulk"", {visible: true});\n  const hrefs = await page.evaluate(() => {\n    const walk = root => [\n      ...[...root.querySelectorAll(""a[href]"")]\n        .map(e => e.getAttribute(""href"")),\n      ...[...root.querySelectorAll(""*"")]\n        .filter(e => e.shadowRoot)\n        .flatMap(e => walk(e.shadowRoot))\n    ];\n    return walk(document);\n  });\n  console.log(hrefs);\n  console.log(hrefs.length); // => 44 at the time I ran this\n\n  // Bonus example of diving manually into shadow roots...\n  //const html = await page.evaluate(() =>\n  //  document\n  //    .querySelector(""#app > an-hulk"")\n  //    .shadowRoot\n  //    .querySelector(""#content"")\n  //    .shadowRoot\n  //    .querySelector(""#main an-hulk-home"")\n  //    .shadowRoot\n  //    .querySelector("".content"")\n  //    .innerHTML\n  //);\n  //console.log(html);\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close());\n;\n\nNote that the sidebar and other parts of the page use event listeners on spans and divs to implement links, so these don\'t count as hrefs as far as the above code is concerned. If you want to access these URLs, there are a variety of strategies you can try, including clicking them and extracting the URL after navigation. This is speculative since it\'s not clear that you want to do this.\n\nA few remarks about your code:\n\nPuppeteer wait until page is completely loaded is an important resource. { waitUntil: \'domcontentloaded\' } is a weaker condition than { waitUntil: \'networkidle0\' }. Using page.waitForSelector(selector, {visible: true}) and page.waitForFunction(predicate) are important to use to ensure the elements have been rendered before you begin manipulating them. Even without the shadow root, it\'s not clear to me that the top-level ""an-hulk"" is going to be available by the time you run evaluate.\nAdd console listeners to your page to help debug. Try your queries one step at a time and break them into multiple stages to see where they go wrong.\nfs.writeFile should be await fs.promises.writeFile since you\'re in an async function.\n\n\nAdditional resources and similar threads:\n\nWhat is shadow root\nPuppeteer: Query nodes within shadow roots #858\nHow to get text from shadow root element?\nSelect element within shadow root\npuppeteer: clicking button in shadowroot\nManipulate / set style shadowRoot using Puppeteer\nPuppeteer: get full HTML content of a webpage, like innerHTML, but including any shadow roots?\nPopup form visible, but html code missing in Puppeteer\nCan\'t locate and click on a terms of conditions button\n\n']",https://stackoverflow.com/questions/68525115/puppeteer-not-giving-accurate-html-code-for-page-with-shadow-roots,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Difference between BeautifulSoup and Scrapy crawler?,"
I want to make a website that shows the comparison between amazon and e-bay product price.
Which of these will work better and why? I am somewhat familiar with BeautifulSoup but not so much with Scrapy crawler.
",89k,"
            159
        ","['\nScrapy is a Web-spider or web scraper framework, You give Scrapy a root URL to start crawling, then you can specify constraints on how many (number of) URLs you want to crawl and fetch,etc. It is a complete framework for web-scraping or crawling.\nWhile\nBeautifulSoup is a parsing library which also does a pretty good job of fetching contents from URL and allows you to parse certain parts of them without any hassle. It only fetches the contents of the URL that you give and then stops. It does not crawl unless you manually put it inside an infinite loop with certain criteria.\nIn simple words, with Beautiful Soup you can build something similar to Scrapy.\nBeautiful Soup is a library while Scrapy is a complete framework.\nSource\n', ""\nI think both are good... im doing a project right now that use both. First i scrap all the pages using scrapy and save that on a mongodb collection using their pipelines, also downloading the images that exists on the page.\nAfter that i use BeautifulSoup4 to make a pos-processing where i must change attributes values and get some special tags.\nIf you don't know which pages products you want, a good tool will be scrapy since you can use their crawlers to run all amazon/ebay website looking for the products without making a explicit for loop.\nTake a look at the scrapy documentation, it's very simple to use.\n"", ""\nScrapy\nIt is a web scraping framework which comes with tons of goodies which make scraping from easier so that we can focus on crawling logic only. Some of my favourite things scrapy takes care for us are below.\n\nFeed exports: It basically allows us to save data in various formats like CSV,JSON,jsonlines and XML. \nAsynchronous scraping: Scrapy uses twisted framework which gives us power to visit multiple urls at once where each request is processed in non blocking way(Basically we don't have to wait for a request to finish before sending another request).\nSelectors: This is where we can compare scrapy with beautiful soup. Selectors are what allow us to select particular data from the webpage like heading, certain div with a class name etc.). Scrapy uses lxml for parsing which is extremely fast than beautiful soup.\nSetting proxy,user agent ,headers etc: scrapy allows us to set and rotate proxy,and other headers dynamically.\nItem Pipelines: Pipelines enable us to process data after extraction. For example we can configure pipeline to push data to your mysql server.\nCookies: scrapy automatically handles cookies for us.\n\netc.\n\nTLDR: scrapy is a framework that provides everything that one might\n  need to build large scale crawls. It provides various features that\n  hide complexity of crawling the webs. one can simply start writing web\n  crawlers without worrying about the setup burden.\n\nBeautiful soup\nBeautiful Soup is a Python package for parsing HTML and XML documents. So with Beautiful soup you can parse a webpage that has been already downloaded. BS4 is very popular and old. Unlike scrapy,You cannot use beautiful soup only to make crawlers. You will need other libraries like requests,urllib etc to make crawlers with bs4. Again, this means you would need to manage the list of urls being crawled,to be crawled, handle cookies , manage proxy, handle errors, create your own functions to push data to CSV,JSON,XML etc. If you want to speed up than you will have to use other libraries like multiprocessing.\nTo sum up.\n\nScrapy is a rich framework that you can use to start writing crawlers\nwithout any hassale.\nBeautiful soup is a library that you can use to parse a webpage. It\ncannot be used alone to scrape web.\n\nYou should definitely use scrapy for your amazon and e-bay product price comparison website. You could build a database of urls and run the crawler every day(cron jobs,Celery for scheduling crawls) and update the price on your database.This way your website will always pull from the database and crawler and database will act as individual components.\n"", '\nBoth are using to parse data.\nScrapy:\n\nScrapy is a fast high-level web crawling and web scraping framework,\nused to crawl websites and extract structured data from their pages.\nBut it has some limitations when data comes from java script or\nloading dynamicaly, we can over come it by using packages like splash, \nselenium etc.\n\nBeautifulSoup:\n\nBeautiful Soup is a Python library for pulling data out of HTML and\nXML files.\nwe can use this package for getting data from java script or \ndynamically loading pages.\n\nScrapy with BeautifulSoup is one of the best combo we can work with for scraping static and dynamic contents \n', ""\nThe way I do it is to use the eBay/Amazon API's rather than scrapy, and then parse the results using BeautifulSoup.\nThe APIs gives you an official way of getting the same data that you would have got from scrapy crawler, with no need to worry about hiding your identity, mess about with proxies,etc.\n"", '\nBeautifulSoup is a library that lets you extract information from a web page.\nScrapy on the other hand is a framework, which does the above thing and many more things you probably need in your scraping project like pipelines for saving data.\nYou can check this blog to get started with Scrapy\nhttps://www.inkoop.io/blog/web-scraping-using-python-and-scrapy/\n', ""\nUsing scrapy you can save tons of code and start with structured programming, If you dont like any of the scapy's pre-written methods then BeautifulSoup can be used in the place of scrapy method.\nBig project takes both advantages.\n"", '\nBeautifulsoup is web scraping small library. it does your job but sometime it does not satisfy your needs.i mean if you scrape  websites in large amount of data  so here in this case beautifulsoup fails.\nIn this case  you should use Scrapy which is  a complete  scraping framework  which will do you job.\nAlso scrapy has support for databases(all kind of databases) so it is a huge\nof scrapy over other web  scraping libraries.\n', '\nThe differences are many and selection of any tool/technology depends on individual needs.\nFew major differences are:\n\nBeautifulSoup is comparatively is easy to learn than Scrapy. \nThe extensions, support, community is larger for Scrapy than for BeautifulSoup.\nScrapy should be considered as a Spider while BeautifulSoup is a Parser.\n\n']",https://stackoverflow.com/questions/19687421/difference-between-beautifulsoup-and-scrapy-crawler,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get a list of URLs from a site [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don鈥檛 allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 7 years ago.







                        Improve this question
                    



I'm deploying a replacement site for a client but they don't want all their old pages to end in 404s. Keeping the old URL structure wasn't possible because it was hideous.
So I'm writing a 404 handler that should look for an old page being requested and do a permanent redirect to the new page. Problem is, I need a list of all the old page URLs.
I could do this manually, but I'd be interested if there are any apps that would provide me a list of relative (eg: /page/path, not http:/.../page/path) URLs just given the home page. Like a spider but one that doesn't care about the content other than to find deeper pages.
",496k,"
            117
        ","[""\nI didn't mean to answer my own question but I just thought about running a sitemap generator. First one I found http://www.xml-sitemaps.com has a nice text output. Perfect for my needs.\n"", ""\ndo wget -r -l0 www.oldsite.com\nThen just find www.oldsite.com would reveal all urls, I believe.\nAlternatively, just serve that custom not-found page on every 404 request!\nI.e. if someone used the wrong link, he would get the page telling that page wasn't found, and making some hints about site's content.\n"", '\nHere is a list of sitemap generators (from which obviously you can get the list of URLs from a site): http://code.google.com/p/sitemap-generators/wiki/SitemapGenerators\n\nWeb Sitemap Generators\nThe following are links to tools that generate or maintain files in\n  the XML Sitemaps format, an open standard defined on sitemaps.org and\n  supported by the search engines such as Ask, Google, Microsoft Live\n  Search and Yahoo!. Sitemap files generally contain a collection of\n  URLs on a website along with some meta-data for these URLs. The\n  following tools generally generate ""web-type"" XML Sitemap and URL-list\n  files (some may also support other formats).\nPlease Note: Google has not tested or verified the features or\n  security of the third party software listed on this site. Please\n  direct any questions regarding the software to the software\'s author.\n  We hope you enjoy these tools!\nServer-side Programs\n\nEnarion phpSitemapsNG (PHP)\nGoogle Sitemap Generator (Linux/Windows, 32/64bit, open-source)\nOutil en PHP (French, PHP)\nPerl Sitemap Generator (Perl)\nPython Sitemap Generator (Python)\nSimple Sitemaps (PHP)\nSiteMap XML Dynamic Sitemap Generator (PHP) $\nSitemap generator for OS/2 (REXX-script)\nXML Sitemap Generator (PHP) $\n\nCMS and Other Plugins:\n\nASP.NET - Sitemaps.Net\nDotClear (Spanish)\nDotClear (2)\nDrupal\nECommerce Templates (PHP) $\nEcommerce Templates (PHP or ASP) $\nLifeType\nMediaWiki Sitemap generator\nmnoGoSearch\nOS Commerce\nphpWebSite\nPlone\nRapidWeaver\nTextpattern\nvBulletin\nWikka Wiki (PHP)\nWordPress\n\nDownloadable Tools\n\nGSiteCrawler (Windows)\nGWebCrawler & Sitemap Creator (Windows)\nG-Mapper (Windows)\nInspyder Sitemap Creator (Windows) $\nIntelliMapper (Windows) $\nMicrosys A1 Sitemap Generator (Windows) $\nRage Google Sitemap Automator $ (OS-X)\nScreaming Frog SEO Spider and Sitemap generator (Windows/Mac) $\nSite Map Pro (Windows) $\nSitemap Writer (Windows) $\nSitemap Generator by DevIntelligence (Windows)\nSorrowmans Sitemap Tools (Windows)\nTheSiteMapper (Windows) $\nVigos Gsitemap (Windows)\nVisual SEO Studio (Windows)\nWebDesignPros Sitemap Generator (Java Webstart Application)\nWeblight (Windows/Mac) $\nWonderWebWare Sitemap Generator (Windows)\n\nOnline Generators/Services\n\nAuditMyPc.com Sitemap Generator\nAutoMapIt\nAutositemap $\nEnarion phpSitemapsNG\nFree Sitemap Generator\nNeuroticweb.com Sitemap Generator\nROR Sitemap Generator\nScriptSocket Sitemap Generator\nSeoUtility Sitemap Generator (Italian)\nSitemapDoc\nSitemapspal\nSitemapSubmit\nSmart-IT-Consulting Google Sitemaps XML Validator\nXML Sitemap Generator\nXML-Sitemaps Generator\n\nCMS with integrated Sitemap generators\n\nConcrete5\n\nGoogle News Sitemap Generators   The following plugins allow\n  publishers to update Google News Sitemap files, a variant of the\n  sitemaps.org protocol that we describe in our Help Center. In addition\n  to the normal properties of Sitemap files, Google News Sitemaps allow\n  publishers to describe the types of content they publish, along with\n  specifying levels of access for individual articles. More information\n  about Google News can be found in our Help Center and Help Forums.\n\nWordPress Google News plugin\n\nCode Snippets / Libraries\n\nASP script\nEmacs Lisp script\nJava library\nPerl script\nPHP class\nPHP generator script\n\nIf you believe that a tool should be added or removed for a legitimate\n  reason, please leave a comment in the Webmaster Help Forum.\n\n', '\nThe best on I have found is http://www.auditmypc.com/xml-sitemap.asp which uses Java, and has no limit on pages, and even lets you export results as a raw URL list.\nIt also uses sessions, so if you are using a CMS, make sure you are logged out before you run the crawl.\n', '\nSo, in an ideal world you\'d have a spec for all pages in your site. You would also have a test infrastructure that could hit all your pages to test them.\nYou\'re presumably not in an ideal world. Why not do this...?\n\nCreate a mapping between the well\nknown old URLs and the new ones.\nRedirect when you see an old URL.\nI\'d possibly consider presenting a\n""this page has moved, it\'s new url\nis XXX, you\'ll be redirected\nshortly"".\nIf you have no mapping, present a\n    ""sorry - this page has moved. Here\'s\n    a link to the home page"" message and\n    redirect them if you like.\nLog all redirects - especially the\n    ones with no mapping. Over time, add\n    mappings for pages that are\n    important.\n\n', ""\nwget from a linux box might also be a good option as there are switches to spider and change it's output.\nEDIT: wget is also available on Windows: http://gnuwin32.sourceforge.net/packages/wget.htm\n"", '\nWrite a spider which reads in every html from disk and outputs every ""href"" attribute of an ""a"" element (can be done with a parser). Keep in mind which links belong to a certain page (this is common task for a MultiMap datastructre). After this you can produce a mapping file which acts as the input for the 404 handler.\n', '\nI would look into any number of online sitemap generation tools.  Personally, I\'ve used this one (java based)in the past, but if you do a google search for ""sitemap builder"" I\'m sure you\'ll find lots of different options.\n']",https://stackoverflow.com/questions/857653/get-a-list-of-urls-from-a-site,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
crawler vs scraper,"
Can somebody distinguish between a crawler and scraper in terms of scope and functionality.
",40k,"
            78
        ","[""\nA crawler gets web pages -- i.e., given a starting address (or set of starting addresses) and some conditions (e.g., how many links deep to go, types of files to ignore) it downloads whatever is linked to from the starting point(s).\nA scraper takes pages that have been downloaded or, in a more general sense, data that's formatted for display, and (attempts to) extract data from those pages, so that it can (for example) be stored in a database and manipulated as desired.\nDepending on how you use the result, scraping may well violate the rights of the owner of the information and/or user agreements about use of web sites (crawling violates the latter in some cases as well). Many sites include a file named robots.txt in their root (i.e. having the URL http://server/robots.txt) to specify how (and if) crawlers should treat that site -- in particular, it can list (partial) URLs that a crawler should not attempt to visit. These can be specified separately per crawler (user-agent) if desired.\n"", ""\nCrawlers surf the web, following links.  An example would be the Google robot that gets pages to index.  Scrapers extract values from forms, but don't necessarily have anything to do with the web.\n"", '\nWeb crawler gets links (Urls - Pages) in a logic and scraper get values (extracting) from HTML.\nThere are so many web crawler tools. Visit page to see some. Any XML - HTML parser can used to extract (scrape) data from crawled pages. (I recommend Jsoup for parsing and extracting data)\n', ""\nGenerally, crawlers would follow the links to reach numerous pages while scrapers is, in some sense, just pulling the contents displayed online and would not reach the deeper links. \nThe most typical crawler is google bots, which would follow the links to reach all the web pages on your website and would index the contents if they found it useful(that's why you need robots.txt to tell which contents you do not want to be indexed). So we could search such kind of contents on its website. While the purpose of scrapers is just to pull the contents for personal uses and would not have much effects on others. \nHowever, there's no distinct difference about crawlers and scrapers now as some automated web scraping tools also allow you to crawl the website by following the links, like Octoparse and import.io. They are not the crawlers like google bots, but they are able to automatically crawl the websites to get numerous data without coding.\n"", '\nScrapers and crawlers do not always distinguish, I mean - you can find crawlers which scrape, in fact, Scraper Crawler is doing both and is named accordingly:\n\nit crawls to a URL i.e. indexes all the URL in that main URL\ndepth of crawling is how far the indexing goes in the URL tree\nthen it scrapes whatever you define in a regexp\n\n', ""\nI know this question is quite old, but I'll respond anyway for the newcomer that will wonder here.\nFrom what I can gather and understand it seems that those two terms are often confused with each other due to their similarity and people will often refer to them as the same thing.\nHowever, they are not quite the same. A crawler(or spider) will follow each link in the page it crawls from the starter page. This is why it is also referred to as a spider bot since it will create a kind of a spider web of pages.\nA scraper will extract the data from a page, usually from the pages downloaded with the crawler.\nIf you are interested in either of those, you can try the Norconex HTTP Collector.\n""]",https://stackoverflow.com/questions/3207418/crawler-vs-scraper,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Detect Search Crawlers via JavaScript,"
I am wondering how would I go abouts in detecting search crawlers? The reason I ask is because I want to suppress certain JavaScript calls if the user agent is a bot.
I have found an example of how to to detect a certain browser, but am unable to find examples of how to detect a search crawler: 
/MSIE (\d+\.\d+);/.test(navigator.userAgent); //test for MSIE x.x
Example of search crawlers I want to block:
Google 
Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html) 
Googlebot/2.1 (+http://www.googlebot.com/bot.html) 
Googlebot/2.1 (+http://www.google.com/bot.html) 

Baidu 
Baiduspider+(+http://www.baidu.com/search/spider_jp.html) 
Baiduspider+(+http://www.baidu.com/search/spider.htm) 
BaiDuSpider 

",46k,"
            62
        ","['\nThis is the regex the ruby UA agent_orange library uses to test if a userAgent looks to be a bot. You can narrow it down for specific bots by referencing the bot userAgent list here:\n/bot|crawler|spider|crawling/i\n\nFor example you have some object, util.browser, you can store what type of device a user is on:\nutil.browser = {\n   bot: /bot|googlebot|crawler|spider|robot|crawling/i.test(navigator.userAgent),\n   mobile: ...,\n   desktop: ...\n}\n\n', '\nTry this. It\'s based on the crawlers list on available on https://github.com/monperrus/crawler-user-agents\nvar botPattern = ""(googlebot\\/|bot|Googlebot-Mobile|Googlebot-Image|Google favicon|Mediapartners-Google|bingbot|slurp|java|wget|curl|Commons-HttpClient|Python-urllib|libwww|httpunit|nutch|phpcrawl|msnbot|jyxobot|FAST-WebCrawler|FAST Enterprise Crawler|biglotron|teoma|convera|seekbot|gigablast|exabot|ngbot|ia_archiver|GingerCrawler|webmon |httrack|webcrawler|grub.org|UsineNouvelleCrawler|antibot|netresearchserver|speedy|fluffy|bibnum.bnf|findlink|msrbot|panscient|yacybot|AISearchBot|IOI|ips-agent|tagoobot|MJ12bot|dotbot|woriobot|yanga|buzzbot|mlbot|yandexbot|purebot|Linguee Bot|Voyager|CyberPatrol|voilabot|baiduspider|citeseerxbot|spbot|twengabot|postrank|turnitinbot|scribdbot|page2rss|sitebot|linkdex|Adidxbot|blekkobot|ezooms|dotbot|Mail.RU_Bot|discobot|heritrix|findthatfile|europarchive.org|NerdByNature.Bot|sistrix crawler|ahrefsbot|Aboundex|domaincrawler|wbsearchbot|summify|ccbot|edisterbot|seznambot|ec2linkfinder|gslfbot|aihitbot|intelium_bot|facebookexternalhit|yeti|RetrevoPageAnalyzer|lb-spider|sogou|lssbot|careerbot|wotbox|wocbot|ichiro|DuckDuckBot|lssrocketcrawler|drupact|webcompanycrawler|acoonbot|openindexspider|gnam gnam spider|web-archive-net.com.bot|backlinkcrawler|coccoc|integromedb|content crawler spider|toplistbot|seokicks-robot|it2media-domain-crawler|ip-web-crawler.com|siteexplorer.info|elisabot|proximic|changedetection|blexbot|arabot|WeSEE:Search|niki-bot|CrystalSemanticsBot|rogerbot|360Spider|psbot|InterfaxScanBot|Lipperhey SEO Service|CC Metadata Scaper|g00g1e.net|GrapeshotCrawler|urlappendbot|brainobot|fr-crawler|binlar|SimpleCrawler|Livelapbot|Twitterbot|cXensebot|smtbot|bnf.fr_bot|A6-Indexer|ADmantX|Facebot|Twitterbot|OrangeBot|memorybot|AdvBot|MegaIndex|SemanticScholarBot|ltx71|nerdybot|xovibot|BUbiNG|Qwantify|archive.org_bot|Applebot|TweetmemeBot|crawler4j|findxbot|SemrushBot|yoozBot|lipperhey|y!j-asr|Domain Re-Animator Bot|AddThis)"";\nvar re = new RegExp(botPattern, \'i\');\nvar userAgent = navigator.userAgent; \nif (re.test(userAgent)) {\n    console.log(\'the user agent is a crawler!\');\n}\n\n', ""\nThe following regex will match the biggest search engines according to this post.\n/bot|google|baidu|bing|msn|teoma|slurp|yandex/i\n    .test(navigator.userAgent)\n\nThe matches search engines are:\n\nBaidu\nBingbot/MSN\nDuckDuckGo (duckduckbot)\nGoogle\nTeoma\nYahoo!\nYandex\n\nAdditionally, I've added bot as a catchall for smaller crawlers/bots.\n"", '\nThis might help to detect the robots user agents while also keeping things more organized:\nJavascript\nconst detectRobot = (userAgent) => {\n  const robots = new RegExp([\n    /bot/,/spider/,/crawl/,                            // GENERAL TERMS\n    /APIs-Google/,/AdsBot/,/Googlebot/,                // GOOGLE ROBOTS\n    /mediapartners/,/Google Favicon/,\n    /FeedFetcher/,/Google-Read-Aloud/,\n    /DuplexWeb-Google/,/googleweblight/,\n    /bing/,/yandex/,/baidu/,/duckduck/,/yahoo/,        // OTHER ENGINES\n    /ecosia/,/ia_archiver/,\n    /facebook/,/instagram/,/pinterest/,/reddit/,       // SOCIAL MEDIA\n    /slack/,/twitter/,/whatsapp/,/youtube/,\n    /semrush/,                                         // OTHER\n  ].map((r) => r.source).join(""|""),""i"");               // BUILD REGEXP + ""i"" FLAG\n\n  return robots.test(userAgent);\n};\n\nTypescript\nconst detectRobot = (userAgent: string): boolean => {\n  const robots = new RegExp(([\n    /bot/,/spider/,/crawl/,                               // GENERAL TERMS\n    /APIs-Google/,/AdsBot/,/Googlebot/,                   // GOOGLE ROBOTS\n    /mediapartners/,/Google Favicon/,\n    /FeedFetcher/,/Google-Read-Aloud/,\n    /DuplexWeb-Google/,/googleweblight/,\n    /bing/,/yandex/,/baidu/,/duckduck/,/yahoo/,           // OTHER ENGINES\n    /ecosia/,/ia_archiver/,\n    /facebook/,/instagram/,/pinterest/,/reddit/,          // SOCIAL MEDIA\n    /slack/,/twitter/,/whatsapp/,/youtube/,\n    /semrush/,                                            // OTHER\n  ] as RegExp[]).map((r) => r.source).join(""|""),""i"");     // BUILD REGEXP + ""i"" FLAG\n\n  return robots.test(userAgent);\n};\n\n\nUse on server:\nconst userAgent = req.get(\'user-agent\');\nconst isRobot = detectRobot(userAgent);\n\nUse on ""client"" / some phantom browser a bot might be using:\nconst userAgent = navigator.userAgent;\nconst isRobot = detectRobot(userAgent);\n\nOverview of Google crawlers:\nhttps://developers.google.com/search/docs/advanced/crawling/overview-google-crawlers\n', ""\nisTrusted property could help you.\n\nThe isTrusted read-only property of the Event interface is a Boolean\nthat is true when the event was generated by a user action, and false\nwhen the event was created or modified by a script or dispatched via\nEventTarget.dispatchEvent().\n\neg:\nisCrawler() {\n  return event.isTrusted;\n}\n\n鈿?Note that IE isn't compatible.\nRead more from doc: https://developer.mozilla.org/en-US/docs/Web/API/Event/isTrusted\n"", '\nPeople might light to check out the new navigator.webdriver property, which allows bots to inform you that they are bots:\nhttps://developer.mozilla.org/en-US/docs/Web/API/Navigator/webdriver\n\nThe webdriver read-only property of the navigator interface indicates whether the user agent is controlled by automation.\n\n\nIt defines a standard way for co-operating user agents to inform the document that it is controlled by WebDriver, for example, so that alternate code paths can be triggered during automation.\n\nIt is supported by all major browsers and respected by major browser automation software like Puppeteer. Users of automation software can of course disable it, and so it should only be used to detect ""good"" bots.\n', '\nI combined some of the above and removed some redundancy. I use this in .htaccess on a semi-private site:\n(google|bot|crawl|spider|slurp|baidu|bing|msn|teoma|yandex|java|wget|curl|Commons-HttpClient|Python-urllib|libwww|httpunit|nutch|biglotron|convera|gigablast|archive|webmon|httrack|grub|netresearchserver|speedy|fluffy|bibnum|findlink|panscient|IOI|ips-agent|yanga|Voyager|CyberPatrol|postrank|page2rss|linkdex|ezooms|heritrix|findthatfile|Aboundex|summify|ec2linkfinder|facebook|slack|instagram|pinterest|reddit|twitter|whatsapp|yeti|RetrevoPageAnalyzer|sogou|wotbox|ichiro|drupact|coccoc|integromedb|siteexplorer|proximic|changedetection|WeSEE|scrape|scaper|g00g1e|binlar|indexer|MegaIndex|ltx71|BUbiNG|Qwantify|lipperhey|y!j-asr|AddThis)\n', '\nThe ""test for MSIE x.x"" example is just code for testing the userAgent against a Regular Expression. In your example the Regexp is the\n/MSIE (\\d+\\.\\d+);/\n\npart. Just replace it with your own Regexp you want to test the user agent against. It would be something like\n/Google|Baidu|Baiduspider/.test(navigator.userAgent)\n\nwhere the vertical bar is the ""or"" operator to match the user agent against all of your mentioned robots. For more information about Regular Expression you can refer to this site since javascript uses perl-style RegExp.\n', ""\nI found this isbot package that has the built-in isbot() function. It seams to me that the package is properly maintained and that they keep everything up-to-date.\nUSAGE:\nconst isBot = require('isbot');\n\n...\n\nisBot(req.get('user-agent'));\n\nPackage: https://www.npmjs.com/package/isbot\n""]",https://stackoverflow.com/questions/20084513/detect-search-crawlers-via-javascript,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node.JS: How to pass variables to asynchronous callbacks? [duplicate],"






This question already has answers here:
                        
                    



JavaScript closure inside loops 鈥?simple practical example

                                (44 answers)
                            

Closed 6 years ago.



I'm sure my problem is based on a lack of understanding of asynch programming in node.js but here goes.
For example: I have a list of links I want to crawl. When each asynch request returns I want to know which URL it is for. But, presumably because of race conditions, each request returns with the URL set to the last value in the list.
var links = ['http://google.com', 'http://yahoo.com'];
for (link in links) {
    var url = links[link];
    require('request')(url, function() {
        console.log(url);
    });
}

Expected output:
http://google.com
http://yahoo.com

Actual output:
http://yahoo.com
http://yahoo.com

So my question is either:

How do I pass url (by value) to the call back function? OR
What is the proper way of chaining the HTTP requests so they run sequentially? OR
Something else I'm missing?

PS: For 1. I don't want a solution which examines the callback's parameters but a general way of a callback knowing about variables 'from above'.
",49k,"
            46
        ","[""\nYour url variable is not scoped to the for loop as JavaScript only supports global and function scoping.  So you need to create a function scope for your request call to capture the url value in each iteration of the loop by using an immediate function:\nvar links = ['http://google.com', 'http://yahoo.com'];\nfor (link in links) {\n    (function(url) {\n        require('request')(url, function() {\n            console.log(url);\n        });\n    })(links[link]);\n}\n\nBTW, embedding a require in the middle of loop isn't good practice.  It should probably be re-written as:\nvar request = require('request');\nvar links = ['http://google.com', 'http://yahoo.com'];\nfor (link in links) {\n    (function(url) {\n        request(url, function() {\n            console.log(url);\n        });\n    })(links[link]);\n}\n\n"", ""\nCheck this blog out. A variable can be passed by using .bind() method. In your case it would be like this:\nvar links = ['http://google.com', 'http://yahoo.com'];\nfor (link in links) {\nvar url = links[link];\n\nrequire('request')(url, function() {\n\n    console.log(this.urlAsy);\n\n}.bind({urlAsy:url}));\n}\n\n"", ""\nSee https://stackoverflow.com/a/11747331/243639 for a general discussion of this issue.\nI'd suggest something like\nvar links = ['http://google.com', 'http://yahoo.com'];\n\nfunction createCallback(_url) {\n    return function() {\n        console.log(_url);\n    }\n};\n\nfor (link in links) {\n    var url = links[link];\n    require('request')(url, createCallback(url));\n}\n\n""]",https://stackoverflow.com/questions/13221769/node-js-how-to-pass-variables-to-asynchronous-callbacks,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do I lock read/write to MySQL tables so that I can select and then insert without other programs reading/writing to the database?,"
I am running many instances of a webcrawler in parallel.
Each crawler selects a domain from a table, inserts that url and a start time into a log table, and then starts crawling the domain.
Other parallel crawlers check the log table to see what domains are already being crawled before selecting their own domain to crawl.
I need to prevent other crawlers from selecting a domain that has just been selected by another crawler but doesn't have a log entry yet.  My best guess at how to do this is to lock the database from all other read/writes while one crawler selects a domain and inserts a row in the log table (two queries).
How the heck does one do this?  I'm afraid this is terribly complex and relies on many other things.  Please help get me started.

This code seems like a good solution (see the error below, however):
INSERT INTO crawlLog (companyId, timeStartCrawling)
VALUES
(
    (
        SELECT companies.id FROM companies
        LEFT OUTER JOIN crawlLog
        ON companies.id = crawlLog.companyId
        WHERE crawlLog.companyId IS NULL
        LIMIT 1
    ),
    now()
)

but I keep getting the following mysql error:
You can't specify target table 'crawlLog' for update in FROM clause

Is there a way to accomplish the same thing without this problem?  I've tried a couple different ways.  Including this:
INSERT INTO crawlLog (companyId, timeStartCrawling)
VALUES
(
    (
        SELECT id
        FROM companies
        WHERE id NOT IN (SELECT companyId FROM crawlLog) LIMIT 1
    ),
    now()
)

",104k,"
            38
        ","['\nYou can lock tables using the MySQL LOCK TABLES command like this:\nLOCK TABLES tablename WRITE;\n\n# Do other queries here\n\nUNLOCK TABLES;\n\nSee:\nhttp://dev.mysql.com/doc/refman/5.5/en/lock-tables.html\n', '\nWell, table locks are one way to deal with that; but this makes parallel requests impossible. If the table is InnoDB you could force a row lock instead, using SELECT ... FOR UPDATE within a transaction. \nBEGIN;\n\nSELECT ... FROM your_table WHERE domainname = ... FOR UPDATE\n\n# do whatever you have to do\n\nCOMMIT;\n\nPlease note that you will need an index on domainname (or whatever column you use in the WHERE-clause) for this to work, but this makes sense in general and I assume you will have that anyway.\n', '\nYou probably don\'t want to lock the table.  If you do that you\'ll have to worry about trapping errors when the other crawlers try to write to the database - which is what you were thinking when you said ""...terribly complex and relies on many other things.""\nInstead you should probably wrap the group of queries in a MySQL transaction (see http://dev.mysql.com/doc/refman/5.0/en/commit.html) like this:\nSTART TRANSACTION;\nSELECT @URL:=url FROM tablewiththeurls WHERE uncrawled=1 ORDER BY somecriterion LIMIT 1;\nINSERT INTO loggingtable SET url=@URL;\nCOMMIT;\n\nOr something close to that.\n[edit]  I just realized - you could probably do everything you need in a single query and not even have to worry about transactions.  Something like this:\nINSERT INTO loggingtable (url) SELECT url FROM tablewithurls u LEFT JOIN loggingtable l ON l.url=t.url WHERE {some criterion used to pick the url to work on} AND l.url IS NULL.\n\n', ""\nI got some inspiration from @Eljakim's answer and started this new thread where I figured out a great trick.  It doesn't involve locking anything and is very simple.\nINSERT INTO crawlLog (companyId, timeStartCrawling)\nSELECT id, now()\nFROM companies\nWHERE id NOT IN\n(\n    SELECT companyId\n    FROM crawlLog AS crawlLogAlias\n)\nLIMIT 1\n\n"", ""\nI wouldn't use locking, or transactions.\nThe easiest way to go is to INSERT a record in the logging table if it's not yet present, and then check for that record.\nAssume you have tblcrawels (cra_id) that is filled with your crawlers and tblurl (url_id) that is filled with the URLs, and a table tbllogging (log_cra_id, log_url_id) for your logfile.\nYou would run the following query if crawler 1 wants to start crawling url 2:\nINSERT INTO tbllogging (log_cra_id, log_url_id) \nSELECT 1, url_id FROM tblurl LEFT JOIN tbllogging on url_id=log_url \nWHERE url_id=2 AND log_url_id IS NULL;\n\nThe next step is to check whether this record has been inserted.\nSELECT * FROM tbllogging WHERE log_url_id=2 AND log_cra_id=1\n\nIf you get any results then crawler 1 can crawl this url. If you don't get any results this means that another crawler has inserted in the same line and is already crawling.\n"", ""\nIt's better to use row lock or transactional based query so that other parallel request context can access the table.\n""]",https://stackoverflow.com/questions/6621303/how-do-i-lock-read-write-to-mysql-tables-so-that-i-can-select-and-then-insert-wi,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
how to totally ignore 'debugger' statement in chrome?,"
'never pause here' can not work

after I continue锛?
still paused
",29k,"
            38
        ","['\nTo totally ignore all breakpoints in Chrome, you must do as follows:\n\nOpen your page in the Chrome browser.\n\nPress F12 or right-click on the page and select Inspect.\n\nIn the Source panel, press Ctrl+F8 to deactivate all breakpoints. (or: At the top-right corner, select deactivate breakpoints.)\n\n\nAll breakpoints and debugger statements will be deactivated.\nI tested it in Chrome 79.0.3945.88 (64-bit) and I found that the debugger statement is ignored.\n\n', '\nTo stop hitting debugger statements, you must either set a ""never pause here"" breakpoint, OR you must pause stopping on exceptions.\nThis works because debugger breakpoints are considered exceptions by the browser.\n\n']",https://stackoverflow.com/questions/45767855/how-to-totally-ignore-debugger-statement-in-chrome,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Passing arguments to process.crawl in Scrapy python,"
I would like to get the same result as this command line :
scrapy crawl linkedin_anonymous -a first=James -a last=Bond -o output.json
My script is as follows :
import scrapy
from linkedin_anonymous_spider import LinkedInAnonymousSpider
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

spider = LinkedInAnonymousSpider(None, ""James"", ""Bond"")
process = CrawlerProcess(get_project_settings())
process.crawl(spider) ## <-------------- (1)
process.start()

I found out that process.crawl() in (1) is creating another LinkedInAnonymousSpider where first and last are None (printed in (2)), if so, then there is no point of creating the object spider and how is it possible to pass the arguments first and last to process.crawl()?
linkedin_anonymous :
from logging import INFO

import scrapy

class LinkedInAnonymousSpider(scrapy.Spider):
    name = ""linkedin_anonymous""
    allowed_domains = [""linkedin.com""]
    start_urls = []

    base_url = ""https://www.linkedin.com/pub/dir/?first=%s&last=%s&search=Search""

    def __init__(self, input = None, first= None, last=None):
        self.input = input  # source file name
        self.first = first
        self.last = last

    def start_requests(self):
        print self.first ## <------------- (2)
        if self.first and self.last: # taking input from command line parameters
                url = self.base_url % (self.first, self.last)
                yield self.make_requests_from_url(url)

    def parse(self, response): . . .

",14k,"
            35
        ","[""\npass the spider arguments on the process.crawl method:\nprocess.crawl(spider, input='inputargument', first='James', last='Bond')\n\n"", '\nYou can do it the easy way:\nfrom scrapy import cmdline\n\ncmdline.execute(""scrapy crawl linkedin_anonymous -a first=James -a last=Bond -o output.json"".split())\n\n', ""\nif you have Scrapyd and you want to schedule the spider, do this\ncurl http://localhost:6800/schedule.json -d project=projectname -d spider=spidername -d first='James' -d last='Bond'\n""]",https://stackoverflow.com/questions/34382356/passing-arguments-to-process-crawl-in-scrapy-python,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How do I use the Python Scrapy module to list all the URLs from my website?,"
I want to use the Python Scrapy module to scrape all the URLs from my website and write the list to a file. I looked in the examples but didn't see any simple example to do this.
",22k,"
            24
        ","[""\nHere's the python program that worked for me:\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.spider import BaseSpider\nfrom scrapy.http import Request\n\nDOMAIN = 'example.com'\nURL = 'http://%s' % DOMAIN\n\nclass MySpider(BaseSpider):\n    name = DOMAIN\n    allowed_domains = [DOMAIN]\n    start_urls = [\n        URL\n    ]\n\n    def parse(self, response):\n        hxs = HtmlXPathSelector(response)\n        for url in hxs.select('//a/@href').extract():\n            if not ( url.startswith('http://') or url.startswith('https://') ):\n                url= URL + url \n            print url\n            yield Request(url, callback=self.parse)\n\nSave this in a file called spider.py.\nYou can then use a shell pipeline to post process this text:\nbash$ scrapy runspider spider.py > urls.out\nbash$ cat urls.out| grep 'example.com' |sort |uniq |grep -v '#' |grep -v 'mailto' > example.urls\n\nThis gives me a list of all the unique urls in my site.\n"", '\nsomething cleaner (and maybe more useful) would be using LinkExtractor\nfrom scrapy.linkextractors import LinkExtractor\n\n    def parse(self, response):\n        le = LinkExtractor() # empty for getting everything, check different options on documentation\n        for link in le.extract_links(response):\n            yield Request(link.url, callback=self.parse)\n\n']",https://stackoverflow.com/questions/9561020/how-do-i-use-the-python-scrapy-module-to-list-all-the-urls-from-my-website,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java Web Crawler Libraries,"
I wanted to make a Java based web crawler for an experiment. I heard that making a Web Crawler in Java was the way to go if this is your first time. However, I have two important questions.

How will my program 'visit' or 'connect' to web pages? Please give a brief explanation. (I understand the basics of the layers of abstraction from the hardware up to the software, here I am interested in the Java abstractions)
What libraries should I use? I would assume I need a library for connecting to web pages, a library for HTTP/HTTPS protocol, and a library for HTML parsing.

",45k,"
            22
        ","['\nCrawler4j is the best solution for you,\nCrawler4j is an open source Java crawler which provides a simple interface for crawling the Web. You can setup a multi-threaded web crawler in 5 minutes!\nAlso visit. for more java based web crawler tools and brief explanation for each.\n', '\nThis is How your program \'visit\' or \'connect\' to web pages.  \n    URL url;\n    InputStream is = null;\n    DataInputStream dis;\n    String line;\n\n    try {\n        url = new URL(""http://stackoverflow.com/"");\n        is = url.openStream();  // throws an IOException\n        dis = new DataInputStream(new BufferedInputStream(is));\n\n        while ((line = dis.readLine()) != null) {\n            System.out.println(line);\n        }\n    } catch (MalformedURLException mue) {\n         mue.printStackTrace();\n    } catch (IOException ioe) {\n         ioe.printStackTrace();\n    } finally {\n        try {\n            is.close();\n        } catch (IOException ioe) {\n            // nothing to see here\n        }\n    }\n\nThis will download source of html page.\nFor HTML parsing see this\nAlso take a look at jSpider and jsoup\n', ""\nRight now there is a inclusion of many java based HTML parser that support visiting and parsing the HTML pages.\n\nJsoup\nJaunt API\nHtmlCleaner\nJTidy\nNekoHTML\nTagSoup\n\nHere's the complete list of HTML parser with basic comparison.\n"", '\nHave a look at these existing projects if you want to learn how it can be done:\n\nApache Nutch\ncrawler4j\ngecco\nNorconex HTTP Collector\nvidageek crawler\nwebmagic\nWebmuncher\n\nA typical crawler process is a loop consisting of fetching, parsing, link extraction, and processing of the output (storing, indexing). Though the devil is in the details, i.e. how to be ""polite"" and respect robots.txt, meta tags, redirects, rate limits, URL canonicalization, infinite depth, retries, revisits, etc.\n\nFlow diagram courtesy of Norconex HTTP Collector.\n', ""\nFor parsing content, I'm using Apache Tika.\n"", '\nI come up with another solution to propose that no one mention. There is a library called Selenum it is is an open-source automating testing tool used for automating web applications for testing purposes, but is certainly not limited to only this . You can write a web crawler and get benefited from this automation testing tool just as a human would do.\nAs an illustration, i will provide to you a quick tutorial to get a better look of how it works. if you are being bored to read this post take a look at this Video to understand what capabilities this library can offer in order to crawl web pages.\nSelenium Components\nTo begin with Selenium consist of various components that coexisted in a unique process and perform their action on the java program. This main component is called Webdriver and it must be included in your program in order to make it working properly.\nGo to the following site here and download the latest release for your computer OS (Windows, Linux, or MacOS). It is a ZIP archive containing chromedriver.exe. Save it on your computer and then extract it to a convenient location just as C:\\WebDrivers\\User\\chromedriver.exe We will use this location later in the java program.\nThe next step is to inlude the jar library. Assuming you are using maven project to build the java programm you need to add the follow dependency to your pom.xml\n<dependency>\n <groupId>org.seleniumhq.selenium</groupId>\n <artifactId>selenium-java</artifactId>\n <version>3.8.1</version>\n</dependency>\n\nSelenium Web driver Setup\nLet us get started with Selenium. The first step is to create a ChromeDriver instance:\nSystem.setProperty(""webdriver.chrome.driver"", ""C:\\WebDrivers\\User\\chromedriver.exe);\nWebDriver driver = new ChromeDriver();\n\nNow its time to get deeper in code.The following example shows a simple programma that open a web page and extract some useful Html components. It is easy to understand, as it has comments that explain the steps clearly. Please take a brief look to understand how to capture the objects\n//Launch website\n      driver.navigate().to(""http://www.calculator.net/"");\n\n      //Maximize the browser\n      driver.manage().window().maximize();\n\n      // Click on Math Calculators\n      driver.findElement(By.xpath("".//*[@id = \'menu\']/div[3]/a"")).click();\n\n      // Click on Percent Calculators\n      driver.findElement(By.xpath("".//*[@id = \'menu\']/div[4]/div[3]/a"")).click();\n\n      // Enter value 10 in the first number of the percent Calculator\n      driver.findElement(By.id(""cpar1"")).sendKeys(""10"");\n\n      // Enter value 50 in the second number of the percent Calculator\n      driver.findElement(By.id(""cpar2"")).sendKeys(""50"");\n\n      // Click Calculate Button\n      driver.findElement(By.xpath("".//*[@id = \'content\']/table/tbody/tr[2]/td/input[2]"")).click();\n\n\n      // Get the Result Text based on its xpath\n      String result =\n         driver.findElement(By.xpath("".//*[@id = \'content\']/p[2]/font/b"")).getText();\n\n\n      // Print a Log In message to the screen\n      System.out.println("" The Result is "" + result);\n\nOnce you are done with your work, the browser window can be closed with:\ndriver.quit();\n\nSelenium Browser Options\nThere too much functionality you can implement when you working with this library, For example, assuming you are using chrome you can add in your code\nChromeOptions options = new ChromeOptions();\n\nTake look at how we can use WebDriver to open Chrome extensions using ChromeOptions\noptions.addExtensions(new File(""src\\test\\resources\\extensions\\extension.crx""));\n\nThis is for using Incognito mode\noptions.addArguments(""--incognito"");\n\nthis one for disabling javascript and info bars\noptions.addArguments(""--disable-infobars"");\noptions.addArguments(""--disable-javascript"");\n\nthis one if you want to make the browser scraping silently and hide browser crawling in the background\noptions.addArguments(""--headless"");\n\nonce you have done with it then\nWebDriver driver = new ChromeDriver(options);\n\nTo sum up let\'s see what Selenium has to offer and make it a unique choice compared with the other solutions that proposed on this post thus far.\n\nLanguage and Framework Support\nOpen Source Availability\nMulti-Browser Support\nSupport Across Various Operating Systems\nEase Of Implementation\nReusability and Integrations\nParallel Test Execution and Faster Go-to-Market\nEasy to Learn and Use\nConstant Updates\n\n', '\nI recommend you to use the HttpClient library. You can found examples here.\n', '\nI think jsoup is better than others, jsoup runs on Java 1.5 and up, Scala, Android, OSGi, and Google App Engine.\n', '\nI would prefer crawler4j. Crawler4j is an open source Java crawler which provides a simple interface for crawling the Web. You can setup a multi-threaded web crawler in few hours. \n', '\nYou can explore.apache droid or apache nutch to  get the feel of java based crawler\n', '\nThough mainly used for Unit Testing web applications, HttpUnit traverses a website, clicks links, analyzes tables and form elements, and gives you meta data about all the pages.  I use it for Web Crawling, not just for Unit Testing. - http://httpunit.sourceforge.net/\n', '\nHere is a list of available crawler:\nhttps://java-source.net/open-source/crawlers\nBut I suggest using Apache Nutch\n']",https://stackoverflow.com/questions/11282503/java-web-crawler-libraries,web-crawler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
