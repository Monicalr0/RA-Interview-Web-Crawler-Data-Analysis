Title,Description,Views,Votes,Answers
TypeError: can't use a string pattern on a bytes-like object in re.findall(),"
I am trying to learn how to automatically fetch urls from a page. In the following code I am trying to get the title of the webpage:
import urllib.request
import re

url = ""http://www.google.com""
regex = r'<title>(,+?)</title>'
pattern  = re.compile(regex)

with urllib.request.urlopen(url) as response:
   html = response.read()

title = re.findall(pattern, html)
print(title)

And I get this unexpected error:
Traceback (most recent call last):
  File ""path\to\file\Crawler.py"", line 11, in <module>
    title = re.findall(pattern, html)
  File ""C:\Python33\lib\re.py"", line 201, in findall
    return _compile(pattern, flags).findall(string)
TypeError: can't use a string pattern on a bytes-like object

What am I doing wrong?
",275k,"
            164
        ","[""\nYou want to convert html (a byte-like object) into a string using .decode, e.g.  html = response.read().decode('utf-8'). \nSee Convert bytes to a Python String\n"", ""\nThe problem is that your regex is a string, but html is bytes:\n>>> type(html)\n<class 'bytes'>\n\nSince python doesn't know how those bytes are encoded, it throws an exception when you try to use a string regex on them.\nYou can either decode the bytes to a string:\nhtml = html.decode('ISO-8859-1')  # encoding may vary!\ntitle = re.findall(pattern, html)  # no more error\n\nOr use a bytes regex:\nregex = rb'<title>(,+?)</title>'\n#        ^\n\n\nIn this particular context, you can get the encoding from the response headers:\nwith urllib.request.urlopen(url) as response:\n    encoding = response.info().get_param('charset', 'utf8')\n    html = response.read().decode(encoding)\n\nSee the urlopen documentation for more details.\n"", ""\nBased upon last one, this was smimple to do when pdf read was done .\ntext = text.decode('ISO-8859-1') \n\nThanks @Aran-fey\n""]"
"Sending ""User-agent"" using Requests library in Python","
I want to send a value for ""User-agent"" while requesting a webpage using Python Requests.  I am not sure is if it is okay to send this as a part of the header, as in the code below:
debug = {'verbose': sys.stderr}
user_agent = {'User-agent': 'Mozilla/5.0'}
response  = requests.get(url, headers = user_agent, config=debug)

The debug information isn't showing the headers being sent during the request.
Is it acceptable to send this information in the header?  If not, how can I send it?
",438k,"
            316
        ","[""\nThe user-agent should be specified as a field in the header.\nHere is a list of HTTP header fields, and you'd probably be interested in request-specific fields, which includes User-Agent.\nIf you're using requests v2.13 and newer\nThe simplest way to do what you want is to create a dictionary and specify your headers directly, like so:\nimport requests\n\nurl = 'SOME URL'\n\nheaders = {\n    'User-Agent': 'My User Agent 1.0',\n    'From': 'youremail@domain.example'  # This is another valid field\n}\n\nresponse = requests.get(url, headers=headers)\n\nIf you're using requests v2.12.x and older\nOlder versions of requests clobbered default headers, so you'd want to do the following to preserve default headers and then add your own to them.\nimport requests\n\nurl = 'SOME URL'\n\n# Get a copy of the default headers that requests would use\nheaders = requests.utils.default_headers()\n\n# Update the headers with your custom ones\n# You don't have to worry about case-sensitivity with\n# the dictionary keys, because default_headers uses a custom\n# CaseInsensitiveDict implementation within requests' source code.\nheaders.update(\n    {\n        'User-Agent': 'My User Agent 1.0',\n    }\n)\n\nresponse = requests.get(url, headers=headers)\n\n"", ""\nIt's more convenient to use a session, this way you don't have to remember to set headers each time:\nsession = requests.Session()\nsession.headers.update({'User-Agent': 'Custom user agent'})\n\nsession.get('https://httpbin.org/headers')\n\nBy default, session also manages cookies for you. In case you want to disable that, see this question.\n"", ""\nIt will send the request like browser\nimport requests\n\nurl = 'https://Your-url'\nheaders={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}\n\nresponse= requests.get(url.strip(), headers=headers, timeout=10)\n\n""]"
How can I scrape pages with dynamic content using node.js?,"
I am trying to scrape a website but I don't get some of the elements, because these elements are dynamically created.
I use the cheerio in node.js and My code is below.
var request = require('request');
var cheerio = require('cheerio');
var url = ""http://www.bdtong.co.kr/index.php?c_category=C02"";

request(url, function (err, res, html) {
    var $ = cheerio.load(html);
    $('.listMain > li').each(function () {
        console.log($(this).find('a').attr('href'));
    });
});

This code returns empty response, because when the page is loaded, the <ul id=""store_list"" class=""listMain""> is empty. 
The content has not been appended yet. 
How can I get these elements using node.js? How can I scrape pages with dynamic content?
",32k,"
            30
        ","['\nHere you go;\nvar phantom = require(\'phantom\');\n\nphantom.create(function (ph) {\n  ph.createPage(function (page) {\n    var url = ""http://www.bdtong.co.kr/index.php?c_category=C02"";\n    page.open(url, function() {\n      page.includeJs(""http://ajax.googleapis.com/ajax/libs/jquery/1.6.1/jquery.min.js"", function() {\n        page.evaluate(function() {\n          $(\'.listMain > li\').each(function () {\n            console.log($(this).find(\'a\').attr(\'href\'));\n          });\n        }, function(){\n          ph.exit()\n        });\n      });\n    });\n  });\n});\n\n', ""\nCheck out GoogleChrome/puppeteer\n\nHeadless Chrome Node API\n\nIt makes scraping pretty trivial. The following example will scrape the headline over at npmjs.com (assuming .npm-expansions remains)\nconst puppeteer = require('puppeteer');\n\n(async () => {\n  const browser = await puppeteer.launch();\n  const page = await browser.newPage();\n\n  await page.goto('https://www.npmjs.com/');\n\n  const textContent = await page.evaluate(() => {\n    return document.querySelector('.npm-expansions').textContent\n  });\n\n  console.log(textContent); /* No Problem Mate */\n\n  browser.close();\n})();\n\nevaluate will allow for the inspection of the dynamic element as this will run scripts on the page.\n"", ""\nUse the new npm module x-ray, with a pluggable web driver x-ray-phantom.\nExamples in the pages above, but here's how to do dynamic scraping:\nvar phantom = require('x-ray-phantom');\nvar Xray = require('x-ray');\n\nvar x = Xray()\n  .driver(phantom());\n\nx('http://google.com', 'title')(function(err, str) {\n  if (err) return done(err);\n  assert.equal('Google', str);\n  done();\n})\n\n"", '\nAnswering this as a canonical, an alternative to Puppeteer for scraping dynamic sites which is also well-supported as of 2023 is Playwright. Here\'s a simple example:\nconst playwright = require(""playwright""); // ^1.28.1\n\nlet browser;\n(async () => {\n  browser = await playwright.chromium.launch();\n  const page = await browser.newPage();\n  await page.goto(""https://example.com"");\n  const text = await page.locator(\'h1:text(""Example"")\').textContent();\n  console.log(text); // => Example Domain\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close());\n\n', '\nEasiest and reliable solution is to use puppeteer. As mentioned in https://pusher.com/tutorials/web-scraper-node which is suitable for both static + dynamic scraping.\nOnly change the timeout in Browser.js, TimeoutSettings.js, Launcher.js 300000 to 3000000\n']"
how to detect search engine bots with php?,"
How can one detect the search engine bots using php?
",153k,"
            144
        ","[""\nI use the following code which seems to be working fine:\nfunction _bot_detected() {\n\n  return (\n    isset($_SERVER['HTTP_USER_AGENT'])\n    && preg_match('/bot|crawl|slurp|spider|mediapartners/i', $_SERVER['HTTP_USER_AGENT'])\n  );\n}\n\nupdate 16-06-2017 \nhttps://support.google.com/webmasters/answer/1061943?hl=en\nadded mediapartners\n"", '\nHere\'s a Search Engine Directory of Spider names\nThen you use $_SERVER[\'HTTP_USER_AGENT\']; to check if the agent is said spider.\nif(strstr(strtolower($_SERVER[\'HTTP_USER_AGENT\']), ""googlebot""))\n{\n    // what to do\n}\n\n', ""\nCheck the $_SERVER['HTTP_USER_AGENT'] for some of the strings listed here:\nhttp://www.useragentstring.com/pages/useragentstring.php\nOr more specifically for crawlers:\nhttp://www.useragentstring.com/pages/useragentstring.php?typ=Crawler\nIf you want to -say- log the number of visits of most common search engine crawlers, you could use\n$interestingCrawlers = array( 'google', 'yahoo' );\n$pattern = '/(' . implode('|', $interestingCrawlers) .')/';\n$matches = array();\n$numMatches = preg_match($pattern, strtolower($_SERVER['HTTP_USER_AGENT']), $matches, 'i');\nif($numMatches > 0) // Found a match\n{\n  // $matches[1] contains an array of all text matches to either 'google' or 'yahoo'\n}\n\n"", '\nYou can checkout if it\'s a search engine with this function :\n<?php\nfunction crawlerDetect($USER_AGENT)\n{\n$crawlers = array(\n\'Google\' => \'Google\',\n\'MSN\' => \'msnbot\',\n      \'Rambler\' => \'Rambler\',\n      \'Yahoo\' => \'Yahoo\',\n      \'AbachoBOT\' => \'AbachoBOT\',\n      \'accoona\' => \'Accoona\',\n      \'AcoiRobot\' => \'AcoiRobot\',\n      \'ASPSeek\' => \'ASPSeek\',\n      \'CrocCrawler\' => \'CrocCrawler\',\n      \'Dumbot\' => \'Dumbot\',\n      \'FAST-WebCrawler\' => \'FAST-WebCrawler\',\n      \'GeonaBot\' => \'GeonaBot\',\n      \'Gigabot\' => \'Gigabot\',\n      \'Lycos spider\' => \'Lycos\',\n      \'MSRBOT\' => \'MSRBOT\',\n      \'Altavista robot\' => \'Scooter\',\n      \'AltaVista robot\' => \'Altavista\',\n      \'ID-Search Bot\' => \'IDBot\',\n      \'eStyle Bot\' => \'eStyle\',\n      \'Scrubby robot\' => \'Scrubby\',\n      \'Facebook\' => \'facebookexternalhit\',\n  );\n  // to get crawlers string used in function uncomment it\n  // it is better to save it in string than use implode every time\n  // global $crawlers\n   $crawlers_agents = implode(\'|\',$crawlers);\n  if (strpos($crawlers_agents, $USER_AGENT) === false)\n      return false;\n    else {\n    return TRUE;\n    }\n}\n?>\n\nThen you can use it like :\n<?php $USER_AGENT = $_SERVER[\'HTTP_USER_AGENT\'];\n  if(crawlerDetect($USER_AGENT)) return ""no need to lang redirection"";?>\n\n', ""\nI'm using this to detect bots:\nif (preg_match('/bot|crawl|curl|dataprovider|search|get|spider|find|java|majesticsEO|google|yahoo|teoma|contaxe|yandex|libwww-perl|facebookexternalhit/i', $_SERVER['HTTP_USER_AGENT'])) {\n    // is bot\n}\n\nIn addition I use a whitelist to block unwanted bots:\nif (preg_match('/apple|baidu|bingbot|facebookexternalhit|googlebot|-google|ia_archiver|msnbot|naverbot|pingdom|seznambot|slurp|teoma|twitter|yandex|yeti/i', $_SERVER['HTTP_USER_AGENT'])) {\n    // allowed bot\n}\n\nAn unwanted bot (= false-positive user) is then able to solve a captcha to unblock himself for 24 hours. And as no one solves this captcha, I know it does not produce false-positives. So the bot detection seem to work perfectly.\nNote: My whitelist is based on Facebooks robots.txt.\n"", ""\nBecause any client can set the user-agent to what they want, looking for 'Googlebot', 'bingbot' etc is only half the job.\nThe 2nd part is verifying the client's IP. In the old days this required maintaining IP lists. All the lists you find online are outdated. The top search engines officially support verification through DNS, as explained by Google https://support.google.com/webmasters/answer/80553 and Bing http://www.bing.com/webmaster/help/how-to-verify-bingbot-3905dc26\nAt first perform a reverse DNS lookup of the client IP. For Google this brings a host name under googlebot.com, for Bing it's under search.msn.com. Then, because someone could set such a reverse DNS on his IP, you need to verify with a forward DNS lookup on that hostname. If the resulting IP is the same as the one of the site's visitor, you're sure it's a crawler from that search engine.\nI've written a library in Java that performs these checks for you. Feel free to port it to PHP. It's on GitHub: https://github.com/optimaize/webcrawler-verifier\n"", '\nIf you really need to detect GOOGLE engine bots you should never rely on ""user_agent"" or ""IP"" address because ""user_agent"" can be changed  and acording to what google said in: Verifying Googlebot\n\nTo verify Googlebot as the caller:\n1.Run a reverse DNS lookup on the accessing IP address from your logs, using the host command.\n2.Verify that the domain name is in either googlebot.com or google.com\n3.Run a forward DNS lookup on the domain name retrieved in step 1 using the host command on the retrieved domain name. Verify that it is the same as the original accessing IP address from your logs.\n\nHere is my tested code :\n<?php\n$remote_add=$_SERVER[\'REMOTE_ADDR\'];\n$hostname = gethostbyaddr($remote_add);\n$googlebot = \'googlebot.com\';\n$google = \'google.com\';\nif (stripos(strrev($hostname), strrev($googlebot)) === 0 or stripos(strrev($hostname),strrev($google)) === 0 ) \n{\n//add your code\n}\n\n?>\n\nIn this code we check ""hostname"" which should contain ""googlebot.com"" or ""google.com"" at the end of ""hostname"" which is really important to check exact domain not subdomain.\nI hope you enjoy ;)\n', ""\nI use this function ... part of the regex comes from prestashop but I added some more bot to it.   \n    public function isBot()\n{\n    $bot_regex = '/BotLink|bingbot|AhrefsBot|ahoy|AlkalineBOT|anthill|appie|arale|araneo|AraybOt|ariadne|arks|ATN_Worldwide|Atomz|bbot|Bjaaland|Ukonline|borg\\-bot\\/0\\.9|boxseabot|bspider|calif|christcrawler|CMC\\/0\\.01|combine|confuzzledbot|CoolBot|cosmos|Internet Cruiser Robot|cusco|cyberspyder|cydralspider|desertrealm, desert realm|digger|DIIbot|grabber|downloadexpress|DragonBot|dwcp|ecollector|ebiness|elfinbot|esculapio|esther|fastcrawler|FDSE|FELIX IDE|ESI|fido|H�m�h�kki|KIT\\-Fireball|fouineur|Freecrawl|gammaSpider|gazz|gcreep|golem|googlebot|griffon|Gromit|gulliver|gulper|hambot|havIndex|hotwired|htdig|iajabot|INGRID\\/0\\.1|Informant|InfoSpiders|inspectorwww|irobot|Iron33|JBot|jcrawler|Teoma|Jeeves|jobo|image\\.kapsi\\.net|KDD\\-Explorer|ko_yappo_robot|label\\-grabber|larbin|legs|Linkidator|linkwalker|Lockon|logo_gif_crawler|marvin|mattie|mediafox|MerzScope|NEC\\-MeshExplorer|MindCrawler|udmsearch|moget|Motor|msnbot|muncher|muninn|MuscatFerret|MwdSearch|sharp\\-info\\-agent|WebMechanic|NetScoop|newscan\\-online|ObjectsSearch|Occam|Orbsearch\\/1\\.0|packrat|pageboy|ParaSite|patric|pegasus|perlcrawler|phpdig|piltdownman|Pimptrain|pjspider|PlumtreeWebAccessor|PortalBSpider|psbot|Getterrobo\\-Plus|Raven|RHCS|RixBot|roadrunner|Robbie|robi|RoboCrawl|robofox|Scooter|Search\\-AU|searchprocess|Senrigan|Shagseeker|sift|SimBot|Site Valet|skymob|SLCrawler\\/2\\.0|slurp|ESI|snooper|solbot|speedy|spider_monkey|SpiderBot\\/1\\.0|spiderline|nil|suke|http:\\/\\/www\\.sygol\\.com|tach_bw|TechBOT|templeton|titin|topiclink|UdmSearch|urlck|Valkyrie libwww\\-perl|verticrawl|Victoria|void\\-bot|Voyager|VWbot_K|crawlpaper|wapspider|WebBandit\\/1\\.0|webcatcher|T\\-H\\-U\\-N\\-D\\-E\\-R\\-S\\-T\\-O\\-N\\-E|WebMoose|webquest|webreaper|webs|webspider|WebWalker|wget|winona|whowhere|wlm|WOLP|WWWC|none|XGET|Nederland\\.zoek|AISearchBot|woriobot|NetSeer|Nutch|YandexBot|YandexMobileBot|SemrushBot|FatBot|MJ12bot|DotBot|AddThis|baiduspider|SeznamBot|mod_pagespeed|CCBot|openstat.ru\\/Bot|m2e/i';\n    $userAgent = empty($_SERVER['HTTP_USER_AGENT']) ? FALSE : $_SERVER['HTTP_USER_AGENT'];\n    $isBot = !$userAgent || preg_match($bot_regex, $userAgent);\n\n    return $isBot;\n}\n\nAnyway take care that some bots uses browser like user agent to fake their identity\n ( I got many russian ip that has this behaviour on my site )\nOne distinctive feature of most of the bot is that they don't carry any cookie and so no session is attached to them.\n( I am not sure how but this is for sure the best way to track them ) \n"", ""\nYou could analyse the user agent ($_SERVER['HTTP_USER_AGENT']) or compare the client’s IP address ($_SERVER['REMOTE_ADDR']) with a list of IP addresses of search engine bots.\n"", '\nUse Device Detector open source library, it offers a isBot() function: https://github.com/piwik/device-detector\n', ""\nI made one good and fast function for this\nfunction is_bot(){\n\n        if(isset($_SERVER['HTTP_USER_AGENT']))\n        {\n            return preg_match('/rambler|abacho|acoi|accona|aspseek|altavista|estyle|scrubby|lycos|geona|ia_archiver|alexa|sogou|skype|facebook|twitter|pinterest|linkedin|naver|bing|google|yahoo|duckduckgo|yandex|baidu|teoma|xing|java\\/1.7.0_45|bot|crawl|slurp|spider|mediapartners|\\sask\\s|\\saol\\s/i', $_SERVER['HTTP_USER_AGENT']);\n        }\n\n        return false;\n    }\n\nThis cover 99% of all possible bots, search engines etc.\n"", '\n <?php // IPCLOACK HOOK\nif (CLOAKING_LEVEL != 4) {\n    $lastupdated = date(""Ymd"", filemtime(FILE_BOTS));\n    if ($lastupdated != date(""Ymd"")) {\n        $lists = array(\n        \'http://labs.getyacg.com/spiders/google.txt\',\n        \'http://labs.getyacg.com/spiders/inktomi.txt\',\n        \'http://labs.getyacg.com/spiders/lycos.txt\',\n        \'http://labs.getyacg.com/spiders/msn.txt\',\n        \'http://labs.getyacg.com/spiders/altavista.txt\',\n        \'http://labs.getyacg.com/spiders/askjeeves.txt\',\n        \'http://labs.getyacg.com/spiders/wisenut.txt\',\n        );\n        foreach($lists as $list) {\n            $opt .= fetch($list);\n        }\n        $opt = preg_replace(""/(^[\\r\\n]*|[\\r\\n]+)[\\s\\t]*[\\r\\n]+/"", ""\\n"", $opt);\n        $fp =  fopen(FILE_BOTS,""w"");\n        fwrite($fp,$opt);\n        fclose($fp);\n    }\n    $ip = isset($_SERVER[\'REMOTE_ADDR\']) ? $_SERVER[\'REMOTE_ADDR\'] : \'\';\n    $ref = isset($_SERVER[\'HTTP_REFERER\']) ? $_SERVER[\'HTTP_REFERER\'] : \'\';\n    $agent = isset($_SERVER[\'HTTP_USER_AGENT\']) ? $_SERVER[\'HTTP_USER_AGENT\'] : \'\';\n    $host = strtolower(gethostbyaddr($ip));\n    $file = implode("" "", file(FILE_BOTS));\n    $exp = explode(""."", $ip);\n    $class = $exp[0].\'.\'.$exp[1].\'.\'.$exp[2].\'.\';\n    $threshold = CLOAKING_LEVEL;\n    $cloak = 0;\n    if (stristr($host, ""googlebot"") && stristr($host, ""inktomi"") && stristr($host, ""msn"")) {\n        $cloak++;\n    }\n    if (stristr($file, $class)) {\n        $cloak++;\n    }\n    if (stristr($file, $agent)) {\n        $cloak++;\n    }\n    if (strlen($ref) > 0) {\n        $cloak = 0;\n    }\n\n    if ($cloak >= $threshold) {\n        $cloakdirective = 1;\n    } else {\n        $cloakdirective = 0;\n    }\n}\n?>\n\nThat would be the ideal way to cloak for spiders. It\'s from an open source script called [YACG] - http://getyacg.com\nNeeds a bit of work, but definitely the way to go.\n', ""\n100% Working Bot detector. It is working on my website successfully.\nfunction isBotDetected() {\n\n    if ( preg_match('/abacho|accona|AddThis|AdsBot|ahoy|AhrefsBot|AISearchBot|alexa|altavista|anthill|appie|applebot|arale|araneo|AraybOt|ariadne|arks|aspseek|ATN_Worldwide|Atomz|baiduspider|baidu|bbot|bingbot|bing|Bjaaland|BlackWidow|BotLink|bot|boxseabot|bspider|calif|CCBot|ChinaClaw|christcrawler|CMC\\/0\\.01|combine|confuzzledbot|contaxe|CoolBot|cosmos|crawler|crawlpaper|crawl|curl|cusco|cyberspyder|cydralspider|dataprovider|digger|DIIbot|DotBot|downloadexpress|DragonBot|DuckDuckBot|dwcp|EasouSpider|ebiness|ecollector|elfinbot|esculapio|ESI|esther|eStyle|Ezooms|facebookexternalhit|facebook|facebot|fastcrawler|FatBot|FDSE|FELIX IDE|fetch|fido|find|Firefly|fouineur|Freecrawl|froogle|gammaSpider|gazz|gcreep|geona|Getterrobo-Plus|get|girafabot|golem|googlebot|\\-google|grabber|GrabNet|griffon|Gromit|gulliver|gulper|hambot|havIndex|hotwired|htdig|HTTrack|ia_archiver|iajabot|IDBot|Informant|InfoSeek|InfoSpiders|INGRID\\/0\\.1|inktomi|inspectorwww|Internet Cruiser Robot|irobot|Iron33|JBot|jcrawler|Jeeves|jobo|KDD\\-Explorer|KIT\\-Fireball|ko_yappo_robot|label\\-grabber|larbin|legs|libwww-perl|linkedin|Linkidator|linkwalker|Lockon|logo_gif_crawler|Lycos|m2e|majesticsEO|marvin|mattie|mediafox|mediapartners|MerzScope|MindCrawler|MJ12bot|mod_pagespeed|moget|Motor|msnbot|muncher|muninn|MuscatFerret|MwdSearch|NationalDirectory|naverbot|NEC\\-MeshExplorer|NetcraftSurveyAgent|NetScoop|NetSeer|newscan\\-online|nil|none|Nutch|ObjectsSearch|Occam|openstat.ru\\/Bot|packrat|pageboy|ParaSite|patric|pegasus|perlcrawler|phpdig|piltdownman|Pimptrain|pingdom|pinterest|pjspider|PlumtreeWebAccessor|PortalBSpider|psbot|rambler|Raven|RHCS|RixBot|roadrunner|Robbie|robi|RoboCrawl|robofox|Scooter|Scrubby|Search\\-AU|searchprocess|search|SemrushBot|Senrigan|seznambot|Shagseeker|sharp\\-info\\-agent|sift|SimBot|Site Valet|SiteSucker|skymob|SLCrawler\\/2\\.0|slurp|snooper|solbot|speedy|spider_monkey|SpiderBot\\/1\\.0|spiderline|spider|suke|tach_bw|TechBOT|TechnoratiSnoop|templeton|teoma|titin|topiclink|twitterbot|twitter|UdmSearch|Ukonline|UnwindFetchor|URL_Spider_SQL|urlck|urlresolver|Valkyrie libwww\\-perl|verticrawl|Victoria|void\\-bot|Voyager|VWbot_K|wapspider|WebBandit\\/1\\.0|webcatcher|WebCopier|WebFindBot|WebLeacher|WebMechanic|WebMoose|webquest|webreaper|webspider|webs|WebWalker|WebZip|wget|whowhere|winona|wlm|WOLP|woriobot|WWWC|XGET|xing|yahoo|YandexBot|YandexMobileBot|yandex|yeti|Zeus/i', $_SERVER['HTTP_USER_AGENT'])\n    ) {\n        return true; // 'Above given bots detected'\n    }\n\n    return false;\n\n} // End :: isBotDetected()\n\n"", '\nI\'m using this code, pretty good. You will very easy to know user-agents visitted your site. This code is opening a file and write the user_agent down the file. You can check each day this file by go to yourdomain.com/useragent.txt and know about new user_agents and put them in your condition of if clause.\n$user_agent = strtolower($_SERVER[\'HTTP_USER_AGENT\']);\nif(!preg_match(""/Googlebot|MJ12bot|yandexbot/i"", $user_agent)){\n    // if not meet the conditions then\n    // do what you need\n\n    // here open a file and write the user_agent down the file. You can check each day this file useragent.txt and know about new user_agents and put them in your condition of if clause\n    if($user_agent!=""""){\n        $myfile = fopen(""useragent.txt"", ""a"") or die(""Unable to open file useragent.txt!"");\n        fwrite($myfile, $user_agent);\n        $user_agent = ""\\n"";\n        fwrite($myfile, $user_agent);\n        fclose($myfile);\n    }\n}\n\nThis is the content of useragent.txt\nMozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\nMozilla/5.0 (compatible; MJ12bot/v1.4.6; http://mj12bot.com/)Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\nMozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.96 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)mozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (iphone; cpu iphone os 9_3 like mac os x) applewebkit/601.1.46 (khtml, like gecko) version/9.0 mobile/13e198 safari/601.1\nmozilla/5.0 (windows nt 6.1; wow64) applewebkit/537.36 (khtml, like gecko) chrome/53.0.2785.143 safari/537.36\nmozilla/5.0 (compatible; linkdexbot/2.2; +http://www.linkdex.com/bots/)\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64) applewebkit/537.36 (khtml, like gecko) chrome/53.0.2785.143 safari/537.36\nmozilla/5.0 (windows nt 6.1; wow64) applewebkit/537.36 (khtml, like gecko) chrome/53.0.2785.143 safari/537.36\nmozilla/5.0 (compatible; baiduspider/2.0; +http://www.baidu.com/search/spider.html)\nzoombot (linkbot 1.0 http://suite.seozoom.it/bot.html)\nmozilla/5.0 (windows nt 10.0; wow64) applewebkit/537.36 (khtml, like gecko) chrome/44.0.2403.155 safari/537.36 opr/31.0.1889.174\nmozilla/5.0 (windows nt 10.0; wow64) applewebkit/537.36 (khtml, like gecko) chrome/44.0.2403.155 safari/537.36 opr/31.0.1889.174\nsogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)\nmozilla/5.0 (windows nt 10.0; wow64) applewebkit/537.36 (khtml, like gecko) chrome/44.0.2403.155 safari/537.36 opr/31.0.1889.174\n\n', ""\nFor Google i'm using this method.\nfunction is_google() {\n    $ip   = $_SERVER['REMOTE_ADDR'];\n    $host = gethostbyaddr( $ip );\n    if ( strpos( $host, '.google.com' ) !== false || strpos( $host, '.googlebot.com' ) !== false ) {\n\n        $forward_lookup = gethostbyname( $host );\n\n        if ( $forward_lookup == $ip ) {\n            return true;\n        }\n\n        return false;\n    } else {\n        return false;\n    }\n\n}\n\nvar_dump( is_google() );\n\nCredits: https://support.google.com/webmasters/answer/80553\n"", '\nVerifying Googlebot\nAs useragent can be changed...\n\nthe only official supported way to identify a google bot is to run a\nreverse DNS lookup on the accessing IP address and run a forward DNS\nlookup on the result to verify that it points to accessing IP address\nand the resulting domain name is in either googlebot.com or google.com\ndomain.\n\nTaken from here.\n\nso you must run a DNS lookup\n\nBoth, reverse and forward.\nSee this guide on Google Search Central.\n', ""\nfunction bot_detected() {\n\n  if(preg_match('/bot|crawl|slurp|spider|mediapartners/i', $_SERVER['HTTP_USER_AGENT']){\n    return true;\n  }\n  else{\n    return false;\n  }\n}\n\n"", '\nmight be late, but what about a hidden a link. All bots will use the rel attribute follow, only bad bots will use the nofollow rel attribute.\n<a style=""display:none;"" rel=""follow"" href=""javascript:void(0);"" onclick=""isabot();"">.</a>\n\nfunction isabot(){\n//define a variable to pass with ajax to php\n// || send bots info direct to where ever.\nisabot = true;\n}\n\nfor a bad bot you can use this:\n<a style=""display:none;"" href=""javascript:void(0);"" rel=""nofollow"" onclick=""isBadbot();"">.</a>\n\nfor PHP specific you can remove the onclick attribute and replace the href attribute with a link to your ip detector/ bot detector like so:\n<a style=""display:none;"" rel=""follow"" href=""https://somedomain.com/botdetector.php"">.</a>\n\nOR\n<a style=""display:none;"" rel=""nofollow"" href=""https://somedomain.com/badbotdetector.php"">.</a>\n\nyou can work with it and maybe use both, one detects a bot, while the other proves it to be a bad bot.\nhope you find this useful\n']"
How do I make a simple crawler in PHP? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    


Closed 3 years ago.










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                        
                    





I have a web page with a bunch of links. I want to write a script which would dump all the data contained in those links in a local file.
Has anybody done that with PHP? General guidelines and gotchas would suffice as an answer.
",202k,,"['\nMeh. Don\'t parse HTML with regexes.\nHere\'s a DOM version inspired by Tatu\'s:\n<?php\nfunction crawl_page($url, $depth = 5)\n{\n    static $seen = array();\n    if (isset($seen[$url]) || $depth === 0) {\n        return;\n    }\n\n    $seen[$url] = true;\n\n    $dom = new DOMDocument(\'1.0\');\n    @$dom->loadHTMLFile($url);\n\n    $anchors = $dom->getElementsByTagName(\'a\');\n    foreach ($anchors as $element) {\n        $href = $element->getAttribute(\'href\');\n        if (0 !== strpos($href, \'http\')) {\n            $path = \'/\' . ltrim($href, \'/\');\n            if (extension_loaded(\'http\')) {\n                $href = http_build_url($url, array(\'path\' => $path));\n            } else {\n                $parts = parse_url($url);\n                $href = $parts[\'scheme\'] . \'://\';\n                if (isset($parts[\'user\']) && isset($parts[\'pass\'])) {\n                    $href .= $parts[\'user\'] . \':\' . $parts[\'pass\'] . \'@\';\n                }\n                $href .= $parts[\'host\'];\n                if (isset($parts[\'port\'])) {\n                    $href .= \':\' . $parts[\'port\'];\n                }\n                $href .= dirname($parts[\'path\'], 1).$path;\n            }\n        }\n        crawl_page($href, $depth - 1);\n    }\n    echo ""URL:"",$url,PHP_EOL,""CONTENT:"",PHP_EOL,$dom->saveHTML(),PHP_EOL,PHP_EOL;\n}\ncrawl_page(""http://hobodave.com"", 2);\n\nEdit: I fixed some bugs from Tatu\'s version (works with relative URLs now).\nEdit: I added a new bit of functionality that prevents it from following the same URL twice.\nEdit: echoing output to STDOUT now so you can redirect it to whatever file you want\nEdit: Fixed a bug pointed out by George in his answer. Relative urls will no longer append to the end of the url path, but overwrite it. Thanks to George for this. Note that George\'s answer doesn\'t account for any of: https, user, pass, or port. If you have the http PECL extension loaded this is quite simply done using http_build_url. Otherwise, I have to manually glue together using parse_url. Thanks again George.\n', '\nHere my implementation based on the above example/answer.\n\nIt is class based \nuses Curl\nsupport HTTP Auth\nSkip Url not belonging to the base domain\nReturn Http header Response Code for each page\nReturn time for each page\n\nCRAWL CLASS:\nclass crawler\n{\n    protected $_url;\n    protected $_depth;\n    protected $_host;\n    protected $_useHttpAuth = false;\n    protected $_user;\n    protected $_pass;\n    protected $_seen = array();\n    protected $_filter = array();\n\n    public function __construct($url, $depth = 5)\n    {\n        $this->_url = $url;\n        $this->_depth = $depth;\n        $parse = parse_url($url);\n        $this->_host = $parse[\'host\'];\n    }\n\n    protected function _processAnchors($content, $url, $depth)\n    {\n        $dom = new DOMDocument(\'1.0\');\n        @$dom->loadHTML($content);\n        $anchors = $dom->getElementsByTagName(\'a\');\n\n        foreach ($anchors as $element) {\n            $href = $element->getAttribute(\'href\');\n            if (0 !== strpos($href, \'http\')) {\n                $path = \'/\' . ltrim($href, \'/\');\n                if (extension_loaded(\'http\')) {\n                    $href = http_build_url($url, array(\'path\' => $path));\n                } else {\n                    $parts = parse_url($url);\n                    $href = $parts[\'scheme\'] . \'://\';\n                    if (isset($parts[\'user\']) && isset($parts[\'pass\'])) {\n                        $href .= $parts[\'user\'] . \':\' . $parts[\'pass\'] . \'@\';\n                    }\n                    $href .= $parts[\'host\'];\n                    if (isset($parts[\'port\'])) {\n                        $href .= \':\' . $parts[\'port\'];\n                    }\n                    $href .= $path;\n                }\n            }\n            // Crawl only link that belongs to the start domain\n            $this->crawl_page($href, $depth - 1);\n        }\n    }\n\n    protected function _getContent($url)\n    {\n        $handle = curl_init($url);\n        if ($this->_useHttpAuth) {\n            curl_setopt($handle, CURLOPT_HTTPAUTH, CURLAUTH_ANY);\n            curl_setopt($handle, CURLOPT_USERPWD, $this->_user . "":"" . $this->_pass);\n        }\n        // follows 302 redirect, creates problem wiht authentication\n//        curl_setopt($handle, CURLOPT_FOLLOWLOCATION, TRUE);\n        // return the content\n        curl_setopt($handle, CURLOPT_RETURNTRANSFER, TRUE);\n\n        /* Get the HTML or whatever is linked in $url. */\n        $response = curl_exec($handle);\n        // response total time\n        $time = curl_getinfo($handle, CURLINFO_TOTAL_TIME);\n        /* Check for 404 (file not found). */\n        $httpCode = curl_getinfo($handle, CURLINFO_HTTP_CODE);\n\n        curl_close($handle);\n        return array($response, $httpCode, $time);\n    }\n\n    protected function _printResult($url, $depth, $httpcode, $time)\n    {\n        ob_end_flush();\n        $currentDepth = $this->_depth - $depth;\n        $count = count($this->_seen);\n        echo ""N::$count,CODE::$httpcode,TIME::$time,DEPTH::$currentDepth URL::$url <br>"";\n        ob_start();\n        flush();\n    }\n\n    protected function isValid($url, $depth)\n    {\n        if (strpos($url, $this->_host) === false\n            || $depth === 0\n            || isset($this->_seen[$url])\n        ) {\n            return false;\n        }\n        foreach ($this->_filter as $excludePath) {\n            if (strpos($url, $excludePath) !== false) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    public function crawl_page($url, $depth)\n    {\n        if (!$this->isValid($url, $depth)) {\n            return;\n        }\n        // add to the seen URL\n        $this->_seen[$url] = true;\n        // get Content and Return Code\n        list($content, $httpcode, $time) = $this->_getContent($url);\n        // print Result for current Page\n        $this->_printResult($url, $depth, $httpcode, $time);\n        // process subPages\n        $this->_processAnchors($content, $url, $depth);\n    }\n\n    public function setHttpAuth($user, $pass)\n    {\n        $this->_useHttpAuth = true;\n        $this->_user = $user;\n        $this->_pass = $pass;\n    }\n\n    public function addFilterPath($path)\n    {\n        $this->_filter[] = $path;\n    }\n\n    public function run()\n    {\n        $this->crawl_page($this->_url, $this->_depth);\n    }\n}\n\nUSAGE:\n// USAGE\n$startURL = \'http://YOUR_URL/\';\n$depth = 6;\n$username = \'YOURUSER\';\n$password = \'YOURPASS\';\n$crawler = new crawler($startURL, $depth);\n$crawler->setHttpAuth($username, $password);\n// Exclude path with the following structure to be processed \n$crawler->addFilterPath(\'customer/account/login/referer\');\n$crawler->run();\n\n', '\nCheck out PHP Crawler\nhttp://sourceforge.net/projects/php-crawler/\nSee if it helps.\n', '\nIn it\'s simplest form:\nfunction crawl_page($url, $depth = 5) {\n    if($depth > 0) {\n        $html = file_get_contents($url);\n\n        preg_match_all(\'~<a.*?href=""(.*?)"".*?>~\', $html, $matches);\n\n        foreach($matches[1] as $newurl) {\n            crawl_page($newurl, $depth - 1);\n        }\n\n        file_put_contents(\'results.txt\', $newurl.""\\n\\n"".$html.""\\n\\n"", FILE_APPEND);\n    }\n}\n\ncrawl_page(\'http://www.domain.com/index.php\', 5);\n\nThat function will get contents from a page, then crawl all found links and save the contents to \'results.txt\'. The functions accepts an second parameter, depth, which defines how long the links should be followed. Pass 1 there if you want to parse only links from the given page.\n', '\nWhy use PHP for this, when you can use wget, e.g.\nwget -r -l 1 http://www.example.com\n\nFor how to parse the contents, see Best Methods to parse HTML and use the search function for examples. How to parse HTML has been answered multiple times before.\n', '\nWith some little changes to hobodave\'s code, here is a codesnippet you can use to crawl pages. This needs the curl extension to be enabled in your server.\n<?php\n//set_time_limit (0);\nfunction crawl_page($url, $depth = 5){\n$seen = array();\nif(($depth == 0) or (in_array($url, $seen))){\n    return;\n}   \n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_URL, $url);\ncurl_setopt($ch, CURLOPT_TIMEOUT, 30);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER,1);\n$result = curl_exec ($ch);\ncurl_close ($ch);\nif( $result ){\n    $stripped_file = strip_tags($result, ""<a>"");\n    preg_match_all(""/<a[\\s]+[^>]*?href[\\s]?=[\\s\\""\\\']+"".""(.*?)[\\""\\\']+.*?>"".""([^<]+|.*?)?<\\/a>/"", $stripped_file, $matches, PREG_SET_ORDER ); \n    foreach($matches as $match){\n        $href = $match[1];\n            if (0 !== strpos($href, \'http\')) {\n                $path = \'/\' . ltrim($href, \'/\');\n                if (extension_loaded(\'http\')) {\n                    $href = http_build_url($href , array(\'path\' => $path));\n                } else {\n                    $parts = parse_url($href);\n                    $href = $parts[\'scheme\'] . \'://\';\n                    if (isset($parts[\'user\']) && isset($parts[\'pass\'])) {\n                        $href .= $parts[\'user\'] . \':\' . $parts[\'pass\'] . \'@\';\n                    }\n                    $href .= $parts[\'host\'];\n                    if (isset($parts[\'port\'])) {\n                        $href .= \':\' . $parts[\'port\'];\n                    }\n                    $href .= $path;\n                }\n            }\n            crawl_page($href, $depth - 1);\n        }\n}   \necho ""Crawled {$href}"";\n}   \ncrawl_page(""http://www.sitename.com/"",3);\n?>\n\nI have explained this tutorial in this crawler script tutorial\n', '\nHobodave you were very close. The only thing I have changed is within the if statement that checks to see if the href attribute of the found anchor tag begins with \'http\'. Instead of simply adding the $url variable which would contain the page that was passed in you must first strip it down to the host which can be done using the parse_url php function.\n<?php\nfunction crawl_page($url, $depth = 5)\n{\n  static $seen = array();\n  if (isset($seen[$url]) || $depth === 0) {\n    return;\n  }\n\n  $seen[$url] = true;\n\n  $dom = new DOMDocument(\'1.0\');\n  @$dom->loadHTMLFile($url);\n\n  $anchors = $dom->getElementsByTagName(\'a\');\n  foreach ($anchors as $element) {\n    $href = $element->getAttribute(\'href\');\n    if (0 !== strpos($href, \'http\')) {\n       /* this is where I changed hobodave\'s code */\n        $host = ""http://"".parse_url($url,PHP_URL_HOST);\n        $href = $host. \'/\' . ltrim($href, \'/\');\n    }\n    crawl_page($href, $depth - 1);\n  }\n\n  echo ""New Page:<br /> "";\n  echo ""URL:"",$url,PHP_EOL,""<br />"",""CONTENT:"",PHP_EOL,$dom->saveHTML(),PHP_EOL,PHP_EOL,""  <br /><br />"";\n}\n\ncrawl_page(""http://hobodave.com/"", 5);\n?>\n\n', ""\nAs mentioned, there are crawler frameworks all ready for customizing out there, but if what you're doing is as simple as you mentioned, you could make it from scratch pretty easily.\nScraping the links: http://www.phpro.org/examples/Get-Links-With-DOM.html\nDumping results to a file: http://www.tizag.com/phpT/filewrite.php\n"", ""\nI used @hobodave's code, with this little tweak to prevent re-crawling all fragment variants of the same URL:\n<?php\nfunction crawl_page($url, $depth = 5)\n{\n  $parts = parse_url($url);\n  if(array_key_exists('fragment', $parts)){\n    unset($parts['fragment']);\n    $url = http_build_url($parts);\n  }\n\n  static $seen = array();\n  ...\n\nThen you can also omit the $parts = parse_url($url); line within the for loop.\n"", '\nYou can try this it may be help to you\n$search_string = \'american golf News: Fowler beats stellar field in Abu Dhabi\';\n$html = file_get_contents(url of the site);\n$dom = new DOMDocument;\n$titalDom = new DOMDocument;\n$tmpTitalDom = new DOMDocument;\nlibxml_use_internal_errors(true);\n@$dom->loadHTML($html);\nlibxml_use_internal_errors(false);\n$xpath = new DOMXPath($dom);\n$videos = $xpath->query(\'//div[@class=""primary-content""]\');\nforeach ($videos as $key => $video) {\n$newdomaindom = new DOMDocument;    \n$newnode = $newdomaindom->importNode($video, true);\n$newdomaindom->appendChild($newnode);\n@$titalDom->loadHTML($newdomaindom->saveHTML());\n$xpath1 = new DOMXPath($titalDom);\n$titles = $xpath1->query(\'//div[@class=""listingcontainer""]/div[@class=""list""]\');\nif(strcmp(preg_replace(\'!\\s+!\',\' \',  $titles->item(0)->nodeValue),$search_string)){     \n    $tmpNode = $tmpTitalDom->importNode($video, true);\n    $tmpTitalDom->appendChild($tmpNode);\n    break;\n}\n}\necho $tmpTitalDom->saveHTML();\n\n', '\nThank you @hobodave.\nHowever I found two weaknesses in your code.\nYour parsing of the original url to get the ""host"" segment stops at the first single slash. This presumes that all relative links start in the root directory. This only true sometimes.\noriginal url   :  http://example.com/game/index.html\nhref in <a> tag:  highscore.html\nauthor\'s intent:  http://example.com/game/highscore.html  <-200->\ncrawler result :  http://example.com/highscore.html       <-404->\n\nfix this by breaking at the last single slash not the first\na second unrelated bug, is that $depth does not really track recursion depth, it tracks breadth of the first level of recursion. \nIf I believed this page were in active use I might debug this second issue, but I suspect the text I am writing now will never be read by anyone, human or robot, since this issue is six years old and I do not even have enough reputation to notify +hobodave directly about these defects by commmenting on his code. Thanks anyway hobodave.\n', '\nI came up with the following spider code.\nI adapted it a bit from the following:\nPHP - Is the there a safe way to perform deep recursion?\nit seems fairly rapid....\n    <?php\nfunction  spider( $base_url , $search_urls=array() ) {\n    $queue[] = $base_url;\n    $done           =   array();\n    $found_urls     =   array();\n    while($queue) {\n            $link = array_shift($queue);\n            if(!is_array($link)) {\n                $done[] = $link;\n                foreach( $search_urls as $s) { if (strstr( $link , $s )) { $found_urls[] = $link; } }\n                if( empty($search_urls)) { $found_urls[] = $link; }\n                if(!empty($link )) {\necho \'LINK:::\'.$link;\n                      $content =    file_get_contents( $link );\n//echo \'P:::\'.$content;\n                    preg_match_all(\'~<a.*?href=""(.*?)"".*?>~\', $content, $sublink);\n                    if (!in_array($sublink , $done) && !in_array($sublink , $queue)  ) {\n                           $queue[] = $sublink;\n                    }\n                }\n            } else {\n                    $result=array();\n                    $return = array();\n                    // flatten multi dimensional array of URLs to one dimensional.\n                    while(count($link)) {\n                         $value = array_shift($link);\n                         if(is_array($value))\n                             foreach($value as $sub)\n                                $link[] = $sub;\n                         else\n                               $return[] = $value;\n                     }\n                     // now loop over one dimensional array.\n                     foreach($return as $link) {\n                                // echo \'L::\'.$link;\n                                // url may be in form <a href.. so extract what\'s in the href bit.\n                                preg_match_all(\'/<a[^>]+href=([\\\'""])(?<href>.+?)\\1[^>]*>/i\', $link, $result);\n                                if ( isset( $result[\'href\'][0] )) { $link = $result[\'href\'][0]; }\n                                // add the new URL to the queue.\n                                if( (!strstr( $link , ""http"")) && (!in_array($base_url.$link , $done)) && (!in_array($base_url.$link , $queue)) ) {\n                                     $queue[]=$base_url.$link;\n                                } else {\n                                    if ( (strstr( $link , $base_url  ))  && (!in_array($base_url.$link , $done)) && (!in_array($base_url.$link , $queue)) ) {\n                                         $queue[] = $link;\n                                    }\n                                }\n                      }\n            }\n    }\n\n\n    return $found_urls;\n}    \n\n\n    $base_url       =   \'https://www.houseofcheese.co.uk/\';\n    $search_urls    =   array(  $base_url.\'acatalog/\' );\n    $done = spider( $base_url  , $search_urls  );\n\n    //\n    // RESULT\n    //\n    //\n    echo \'<br /><br />\';\n    echo \'RESULT:::\';\n    foreach(  $done as $r )  {\n        echo \'URL:::\'.$r.\'<br />\';\n    }\n\n', '\nIts worth remembering that when crawling external links (I do appreciate the OP relates to a users own page) you should be aware of robots.txt. I have found the following which will hopefully help http://www.the-art-of-web.com/php/parse-robots/.\n', '\nI created a small class to grab data from the provided url, then extract html elements of your choice. The class makes use of CURL and DOMDocument.\nphp class:\nclass crawler {\n\n\n   public static $timeout = 2;\n   public static $agent   = \'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\';\n\n\n   public static function http_request($url) {\n      $ch = curl_init();\n      curl_setopt($ch, CURLOPT_URL,            $url);\n      curl_setopt($ch, CURLOPT_USERAGENT,      self::$agent);\n      curl_setopt($ch, CURLOPT_CONNECTTIMEOUT, self::$timeout);\n      curl_setopt($ch, CURLOPT_TIMEOUT,        self::$timeout);\n      curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n      $response = curl_exec($ch);\n      curl_close($ch);\n      return $response;\n   }\n\n\n   public static function strip_whitespace($data) {\n      $data = preg_replace(\'/\\s+/\', \' \', $data);\n      return trim($data);\n   }\n\n\n   public static function extract_elements($tag, $data) {\n      $response = array();\n      $dom      = new DOMDocument;\n      @$dom->loadHTML($data);\n      foreach ( $dom->getElementsByTagName($tag) as $index => $element ) {\n         $response[$index][\'text\'] = self::strip_whitespace($element->nodeValue);\n         foreach ( $element->attributes as $attribute ) {\n            $response[$index][\'attributes\'][strtolower($attribute->nodeName)] = self::strip_whitespace($attribute->nodeValue);\n         }\n      }\n      return $response;\n   }\n\n\n}\n\n\nexample usage:\n$data  = crawler::http_request(\'https://stackoverflow.com/questions/2313107/how-do-i-make-a-simple-crawler-in-php\');\n$links = crawler::extract_elements(\'a\', $data);\nif ( count($links) > 0 ) {\n   file_put_contents(\'links.json\', json_encode($links, JSON_PRETTY_PRINT));\n}\n\n\nexample response:\n[\n    {\n        ""text"": ""Stack Overflow"",\n        ""attributes"": {\n            ""href"": ""https:\\/\\/stackoverflow.com"",\n            ""class"": ""-logo js-gps-track"",\n            ""data-gps-track"": ""top_nav.click({is_current:false, location:2, destination:8})""\n        }\n    },\n    {\n        ""text"": ""Questions"",\n        ""attributes"": {\n            ""id"": ""nav-questions"",\n            ""href"": ""\\/questions"",\n            ""class"": ""-link js-gps-track"",\n            ""data-gps-track"": ""top_nav.click({is_current:true, location:2, destination:1})""\n        }\n    },\n    {\n        ""text"": ""Developer Jobs"",\n        ""attributes"": {\n            ""id"": ""nav-jobs"",\n            ""href"": ""\\/jobs?med=site-ui&ref=jobs-tab"",\n            ""class"": ""-link js-gps-track"",\n            ""data-gps-track"": ""top_nav.click({is_current:false, location:2, destination:6})""\n        }\n    }\n]\n\n', '\nIt\'s an old question. A lot of good things happened since then. Here are my two cents on this topic:\n\nTo accurately track the visited pages you have to normalize URI first. The normalization algorithm includes multiple steps:\n\nSort query parameters. For example, the following URIs are equivalent after normalization:\n\nGET http://www.example.com/query?id=111&cat=222\nGET http://www.example.com/query?cat=222&id=111\n\nConvert the empty path.\nExample: http://example.org → http://example.org/\nCapitalize percent encoding. All letters within a percent-encoding triplet (e.g., ""%3A"") are case-insensitive.\nExample: http://example.org/a%c2%B1b → http://example.org/a%C2%B1b\nRemove unnecessary dot-segments.\nExample: http://example.org/../a/b/../c/./d.html → http://example.org/a/c/d.html\nPossibly some other normalization rules\n\nNot only <a> tag has href attribute, <area> tag has it too https://html.com/tags/area/. If you don\'t want to miss anything, you have to scrape <area> tag too.\nTrack crawling progress. If the website is small, it is not a problem. Contrarily it might be very frustrating if you crawl half of the site and it failed. Consider using a database or a filesystem to store the progress.\nBe kind to the site owners.\nIf you are ever going to use your crawler outside of your website, you have to use delays. Without delays, the script is too fast and might significantly slow down some small sites. From sysadmins perspective, it looks like a DoS attack. A static delay between the requests will do the trick.\n\nIf you don\'t want to deal with that, try Crawlzone and let me know your feedback. Also, check out the article I wrote a while back https://www.codementor.io/zstate/this-is-how-i-crawl-n98s6myxm\n']"
How to run Scrapy from within a Python script,"
I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:
http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/
http://snipplr.com/view/67006/using-scrapy-from-a-script/
I can't figure out where I should put my spider code and how to call it from the main function. Please help. This is the example code:
# This snippet can be used to run scrapy spiders independent of scrapyd or the scrapy command line tool and use it from a script. 
# 
# The multiprocessing library is used in order to work around a bug in Twisted, in which you cannot restart an already running reactor or in this case a scrapy instance.
# 
# [Here](http://groups.google.com/group/scrapy-users/browse_thread/thread/f332fc5b749d401a) is the mailing-list discussion for this snippet. 

#!/usr/bin/python
import os
os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'project.settings') #Must be at the top before other imports

from scrapy import log, signals, project
from scrapy.xlib.pydispatch import dispatcher
from scrapy.conf import settings
from scrapy.crawler import CrawlerProcess
from multiprocessing import Process, Queue

class CrawlerScript():

    def __init__(self):
        self.crawler = CrawlerProcess(settings)
        if not hasattr(project, 'crawler'):
            self.crawler.install()
        self.crawler.configure()
        self.items = []
        dispatcher.connect(self._item_passed, signals.item_passed)

    def _item_passed(self, item):
        self.items.append(item)

    def _crawl(self, queue, spider_name):
        spider = self.crawler.spiders.create(spider_name)
        if spider:
            self.crawler.queue.append_spider(spider)
        self.crawler.start()
        self.crawler.stop()
        queue.put(self.items)

    def crawl(self, spider):
        queue = Queue()
        p = Process(target=self._crawl, args=(queue, spider,))
        p.start()
        p.join()
        return queue.get(True)

# Usage
if __name__ == ""__main__"":
    log.start()

    """"""
    This example runs spider1 and then spider2 three times. 
    """"""
    items = list()
    crawler = CrawlerScript()
    items.append(crawler.crawl('spider1'))
    for i in range(3):
        items.append(crawler.crawl('spider2'))
    print items

# Snippet imported from snippets.scrapy.org (which no longer works)
# author: joehillen
# date  : Oct 24, 2010

Thank you.
",82k,"
            85
        ","[""\nAll other answers reference Scrapy v0.x. According to the updated docs, Scrapy 1.0 demands:\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider(scrapy.Spider):\n    # Your spider definition\n    ...\n\nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\n\nprocess.crawl(MySpider)\nprocess.start() # the script will block here until the crawling is finished\n\n"", '\nSimply we can use\nfrom scrapy.crawler import CrawlerProcess\nfrom project.spiders.test_spider import SpiderName\n\nprocess = CrawlerProcess()\nprocess.crawl(SpiderName, arg1=val1,arg2=val2)\nprocess.start()\n\nUse these arguments inside spider __init__ function with the global scope.\n', ""\nThough I haven't tried it I think the answer can be found within the scrapy documentation. To quote directly from it:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import Settings\nfrom scrapy import log\nfrom testspiders.spiders.followall import FollowAllSpider\n\nspider = FollowAllSpider(domain='scrapinghub.com')\ncrawler = Crawler(Settings())\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here\n\nFrom what I gather this is a new development in the library which renders some of the earlier approaches online (such as that in the question) obsolete.\n"", '\nIn scrapy 0.19.x you should do this:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy import log, signals\nfrom testspiders.spiders.followall import FollowAllSpider\nfrom scrapy.utils.project import get_project_settings\n\nspider = FollowAllSpider(domain=\'scrapinghub.com\')\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.signals.connect(reactor.stop, signal=signals.spider_closed)\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here until the spider_closed signal was sent\n\nNote these lines     \nsettings = get_project_settings()\ncrawler = Crawler(settings)\n\nWithout it your spider won\'t use your settings and will not save the items.\nTook me a while to figure out why the example in documentation wasn\'t saving my items. I sent a pull request to fix the doc example.\nOne more to do so is just call command directly from you script\nfrom scrapy import cmdline\ncmdline.execute(""scrapy crawl followall"".split())  #followall is the spider\'s name\n\nCopied this answer from my first answer in here:\nhttps://stackoverflow.com/a/19060485/1402286\n', '\nWhen there are multiple crawlers need to be run inside one python script, the reactor stop needs to be handled with caution as the reactor can only be stopped once and cannot be restarted. \nHowever, I found while doing my project that using \nos.system(""scrapy crawl yourspider"")\n\nis the easiest. This will save me from handling all sorts of signals especially when I have multiple spiders.\nIf Performance is a concern, you can use multiprocessing to run your spiders in parallel, something like:\ndef _crawl(spider_name=None):\n    if spider_name:\n        os.system(\'scrapy crawl %s\' % spider_name)\n    return None\n\ndef run_crawler():\n\n    spider_names = [\'spider1\', \'spider2\', \'spider2\']\n\n    pool = Pool(processes=len(spider_names))\n    pool.map(_crawl, spider_names)\n\n', '\nit  is an improvement of\nScrapy throws an error when run using crawlerprocess\nand https://github.com/scrapy/scrapy/issues/1904#issuecomment-205331087\nFirst create your usual spider for successful command line running. it is very very important that it should run and export data or image or file\nOnce it is over, do just like pasted in my program above spider class definition and below __name __ to invoke settings.\nit will get necessary settings which ""from scrapy.utils.project import get_project_settings"" failed to do which is recommended by many\nboth above and below portions should be there together. only one don\'t run.\nSpider will run in scrapy.cfg folder not any other folder\ntree  diagram may be displayed by the moderators for reference\n#Tree\n[enter image description here][1]\n\n#spider.py\nimport sys\nsys.path.append(r\'D:\\ivana\\flow\') #folder where scrapy.cfg is located\n\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.settings import Settings\nfrom flow import settings as my_settings\n\n#----------------Typical Spider Program starts here-----------------------------\n\n          spider class definition here\n\n#----------------Typical Spider Program ends here-------------------------------\n\nif __name__ == ""__main__"":\n\n    crawler_settings = Settings()\n    crawler_settings.setmodule(my_settings)\n\n    process = CrawlerProcess(settings=crawler_settings)\n    process.crawl(FlowSpider) # it is for class FlowSpider(scrapy.Spider):\n    process.start(stop_after_crawl=True)\n\n', ""\n# -*- coding: utf-8 -*-\nimport sys\nfrom scrapy.cmdline import execute\n\n\ndef gen_argv(s):\n    sys.argv = s.split()\n\n\nif __name__ == '__main__':\n    gen_argv('scrapy crawl abc_spider')\n    execute()\n\nPut this code to the path you can run scrapy crawl abc_spider from command line. (Tested with Scrapy==0.24.6)\n"", ""\nIf you want to run a simple crawling, It's easy by just running command: \nscrapy crawl . \nThere is another options to export your results to store in some formats like: \nJson, xml, csv. \nscrapy crawl  -o result.csv or result.json or result.xml. \nyou may want to try it\n""]"
Scrapy - Reactor not Restartable [duplicate],"






This question already has answers here:
                        
                    



ReactorNotRestartable error in while loop with scrapy

                                (10 answers)
                            

Closed 3 years ago.



with:
from twisted.internet import reactor
from scrapy.crawler import CrawlerProcess

I've always ran this process sucessfully:
process = CrawlerProcess(get_project_settings())
process.crawl(*args)
# the script will block here until the crawling is finished
process.start() 

but since I've moved this code into a web_crawler(self) function, like so:
def web_crawler(self):
    # set up a crawler
    process = CrawlerProcess(get_project_settings())
    process.crawl(*args)
    # the script will block here until the crawling is finished
    process.start() 

    # (...)

    return (result1, result2) 

and started calling the method using class instantiation, like:
def __call__(self):
    results1 = test.web_crawler()[1]
    results2 = test.web_crawler()[0]

and running:
test()

I am getting the following error:
Traceback (most recent call last):
  File ""test.py"", line 573, in <module>
    print (test())
  File ""test.py"", line 530, in __call__
    artists = test.web_crawler()
  File ""test.py"", line 438, in web_crawler
    process.start() 
  File ""/Library/Python/2.7/site-packages/scrapy/crawler.py"", line 280, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 1194, in run
    self.startRunning(installSignalHandlers=installSignalHandlers)
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 1174, in startRunning
    ReactorBase.startRunning(self)
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 684, in startRunning
    raise error.ReactorNotRestartable()
twisted.internet.error.ReactorNotRestartable

what is wrong?
",38k,"
            29
        ","['\nYou cannot restart the reactor, but you should be able to run it more times by forking a separate process:\nimport scrapy\nimport scrapy.crawler as crawler\nfrom scrapy.utils.log import configure_logging\nfrom multiprocessing import Process, Queue\nfrom twisted.internet import reactor\n\n# your spider\nclass QuotesSpider(scrapy.Spider):\n    name = ""quotes""\n    start_urls = [\'http://quotes.toscrape.com/tag/humor/\']\n\n    def parse(self, response):\n        for quote in response.css(\'div.quote\'):\n            print(quote.css(\'span.text::text\').extract_first())\n\n\n# the wrapper to make it run more times\ndef run_spider(spider):\n    def f(q):\n        try:\n            runner = crawler.CrawlerRunner()\n            deferred = runner.crawl(spider)\n            deferred.addBoth(lambda _: reactor.stop())\n            reactor.run()\n            q.put(None)\n        except Exception as e:\n            q.put(e)\n\n    q = Queue()\n    p = Process(target=f, args=(q,))\n    p.start()\n    result = q.get()\n    p.join()\n\n    if result is not None:\n        raise result\n\nRun it twice:\nconfigure_logging()\n\nprint(\'first run:\')\nrun_spider(QuotesSpider)\n\nprint(\'\\nsecond run:\')\nrun_spider(QuotesSpider)\n\nResult:\nfirst run:\n“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n“A day without sunshine is like, you know, night.”\n...\n\nsecond run:\n“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n“A day without sunshine is like, you know, night.”\n...\n\n', '\nThis is what helped for me to win the battle against ReactorNotRestartable error: last answer from the author of the question\n0) pip install crochet\n1) import from crochet import setup\n2) setup() - at the top of the file\n3) remove 2 lines:\na) d.addBoth(lambda _: reactor.stop())\nb) reactor.run()\n\nI had the same problem with this error, and spend 4+ hours to solve this problem, read all questions here about it. Finally found that one - and share it. That is how i solved this. The only meaningful lines from Scrapy docs left are 2 last lines in this my code:\n#some more imports\nfrom crochet import setup\nsetup()\n\ndef run_spider(spiderName):\n    module_name=""first_scrapy.spiders.{}"".format(spiderName)\n    scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   \n    spiderObj=scrapy_var.mySpider()           #get mySpider-object from spider module\n    crawler = CrawlerRunner(get_project_settings())   #from Scrapy docs\n    crawler.crawl(spiderObj)                          #from Scrapy docs\n\nThis code allows me to select what spider to run just with its name passed to run_spider function and after scrapping finishes - select another spider and run it again. \nHope this will help somebody, as it helped for me :)\n', '\nAs per the Scrapy documentation, the start() method of the CrawlerProcess class does the following:\n\n""[...] starts a Twisted reactor, adjusts its pool size to REACTOR_THREADPOOL_MAXSIZE, and installs a DNS cache based on DNSCACHE_ENABLED and DNSCACHE_SIZE.""\n\nThe error you are receiving is being thrown by Twisted, because a Twisted reactor cannot be restarted.  It uses a ton of globals, and even if you do jimmy-rig some sort of code to restart it (I\'ve seen it done), there\'s no guarantee it will work.  \nHonestly, if you think you need to restart the reactor, you\'re likely doing something wrong.\nDepending on what you want to do, I would also review the Running Scrapy from a Script portion of the documentation, too.\n', '\nAs some people pointed out already: You shouldn\'t need to restart the reactor.\nIdeally if you want to chain your processes (crawl1 then crawl2 then crawl3) you simply add callbacks.\nFor example, I\'ve been using this loop spider that follows this pattern:\n1. Crawl A\n2. Sleep N\n3. goto 1\n\nAnd this is how it looks in scrapy:\nimport time\n\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.project import get_project_settings\nfrom twisted.internet import reactor\n\nclass HttpbinSpider(scrapy.Spider):\n    name = \'httpbin\'\n    allowed_domains = [\'httpbin.org\']\n    start_urls = [\'http://httpbin.org/ip\']\n\n    def parse(self, response):\n        print(response.body)\n\ndef sleep(_, duration=5):\n    print(f\'sleeping for: {duration}\')\n    time.sleep(duration)  # block here\n\n\ndef crawl(runner):\n    d = runner.crawl(HttpbinSpider)\n    d.addBoth(sleep)\n    d.addBoth(lambda _: crawl(runner))\n    return d\n\n\ndef loop_crawl():\n    runner = CrawlerRunner(get_project_settings())\n    crawl(runner)\n    reactor.run()\n\n\nif __name__ == \'__main__\':\n    loop_crawl()\n\nTo explain the process more the crawl function schedules a crawl and adds two extra callbacks that are being called when crawling is over: blocking sleep and recursive call to itself (schedule another crawl).\n$ python endless_crawl.py \nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\n\n', '\nThe mistake is in this code:\ndef __call__(self):\n    result1 = test.web_crawler()[1]\n    result2 = test.web_crawler()[0] # here\n\nweb_crawler() returns two results, and for that purpose it is trying to start the process twice, restarting the Reactor, as pointed by @Rejected.\nobtaining results running one single process, and storing both results in a tuple, is the way to go here:\ndef __call__(self):\n    result1, result2 = test.web_crawler()\n\n', '\nThis solved my problem,put below code after reactor.run() or process.start():\ntime.sleep(0.5)\n\nos.execl(sys.executable, sys.executable, *sys.argv)\n\n']"
How to pass a user defined argument in scrapy spider,"
I am trying to pass a user defined argument to a scrapy's spider. Can anyone suggest on how to do that?
I read about a parameter -a somewhere but have no idea how to use it.
",75k,"
            125
        ","[""\nSpider arguments are passed in the crawl command using the -a option. For example:\nscrapy crawl myspider -a category=electronics -a domain=system\n\nSpiders can access arguments as attributes:\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def __init__(self, category='', **kwargs):\n        self.start_urls = [f'http://www.example.com/{category}']  # py36\n        super().__init__(**kwargs)  # python3\n\n    def parse(self, response)\n        self.log(self.domain)  # system\n\nTaken from the Scrapy doc: http://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments\nUpdate 2013: Add second argument\nUpdate 2015: Adjust wording\nUpdate 2016: Use newer base class and add super, thanks @Birla\nUpdate 2017: Use Python3 super\n# previously\nsuper(MySpider, self).__init__(**kwargs)  # python2\n\nUpdate 2018: As @eLRuLL points out, spiders can access arguments as attributes\n"", ""\nPrevious answers were correct, but you don't have to declare the constructor (__init__) every time you want to code a scrapy's spider, you could just specify the parameters as before:\nscrapy crawl myspider -a parameter1=value1 -a parameter2=value2\n\nand in your spider code you can just use them as spider arguments:\nclass MySpider(Spider):\n    name = 'myspider'\n    ...\n    def parse(self, response):\n        ...\n        if self.parameter1 == value1:\n            # this is True\n\n        # or also\n        if getattr(self, parameter2) == value2:\n            # this is also True\n\nAnd it just works.\n"", '\nTo pass arguments with crawl command\n\nscrapy crawl myspider -a category=\'mycategory\' -a domain=\'example.com\'\n\nTo pass arguments to run on scrapyd replace -a with -d\n\ncurl http://your.ip.address.here:port/schedule.json -d \n   spider=myspider -d category=\'mycategory\' -d domain=\'example.com\'\n\nThe spider will receive arguments in its constructor.\n\nclass MySpider(Spider):\n    name=""myspider""\n    def __init__(self,category=\'\',domain=\'\', *args,**kwargs):\n        super(MySpider, self).__init__(*args, **kwargs)\n        self.category = category\n        self.domain = domain\nScrapy puts all the arguments as spider attributes and you can skip the init method completely. Beware use getattr method for getting those attributes so your code does not break.\n\nclass MySpider(Spider):\n    name=""myspider""\n    start_urls = (\'https://httpbin.org/ip\',)\n\n    def parse(self,response):\n        print getattr(self,\'category\',\'\')\n        print getattr(self,\'domain\',\'\')\n\n\n', '\nSpider arguments are passed while running the crawl command using the -a option. For example if i want to pass a domain name as argument to my spider then i will do this-\n\nscrapy crawl myspider -a domain=""http://www.example.com""\n\nAnd receive arguments in spider\'s constructors:\nclass MySpider(BaseSpider):\n    name = \'myspider\'\n    def __init__(self, domain=\'\', *args, **kwargs):\n        super(MySpider, self).__init__(*args, **kwargs)\n        self.start_urls = [domain]\n        #\n\n...\nit will work :)\n', '\nAlternatively we can use ScrapyD which expose an API where we can pass the start_url and spider name. ScrapyD has api\'s to stop/start/status/list the spiders.\npip install scrapyd scrapyd-deploy\nscrapyd\nscrapyd-deploy local -p default\n\nscrapyd-deploy will deploy the spider in the form of egg into the daemon and even it maintains the version of the spider. While starting the spider you can mention which version of spider to use.\nclass MySpider(CrawlSpider):\n\n    def __init__(self, start_urls, *args, **kwargs):\n        self.start_urls = start_urls.split(\'|\')\n        super().__init__(*args, **kwargs)\n    name = testspider\n\ncurl http://localhost:6800/schedule.json -d project=default -d spider=testspider -d start_urls=""https://www.anyurl...|https://www.anyurl2""\nAdded advantage is you can build your own UI to accept the url and other params from the user and schedule a task using the above scrapyd schedule API\nRefer scrapyd API documentation for more details\n']"
python: [Errno 10054] An existing connection was forcibly closed by the remote host,"
I am writing python to crawl Twitter space using Twitter-py. I have set the crawler to sleep for a while (2 seconds) between each request to api.twitter.com. However, after some times of running (around 1), when the Twitter's rate limit not exceeded yet, I got this error.
[Errno 10054] An existing connection was forcibly closed by the remote host.

What are possible causes of this problem and how to solve this?
I have searched through and found that the Twitter server itself may force to close the connection due to many requests.
Thank you very much in advance.
",211k,"
            69
        ","[""\nThis can be caused by the two sides of the connection disagreeing over whether the connection timed out or not during a keepalive. (Your code tries to reused the connection just as the server is closing it because it has been idle for too long.) You should basically just retry the operation over a new connection. (I'm surprised your library doesn't do this automatically.)\n"", ""\nI know this is a very old question but it may be that you need to set the request headers. This solved it for me.\nFor example 'user-agent', 'accept' etc. here is an example with user-agent:\nurl = 'your-url-here'\nheaders = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\nr = requests.get(url, headers=headers)\n\n"", '\nthere are many causes such as \n\nThe network link between server and client may be temporarily going down.\nrunning out of system resources.\nsending malformed data.\n\nTo examine the problem in detail, you can use Wireshark.\nor you can just re-request or re-connect again.\n', '\nFor me this problem arised while trying to connect to the SAP Hana database. When I got this error, \nOperationalError: Lost connection to HANA server (ConnectionResetError(10054, \'An existing connection was forcibly closed by the remote host\', None, 10054, None))\nI tried to run the code for connection(mentioned below), which created that error, again and it worked. \n\n\n    import pyhdb\n    connection = pyhdb.connect(host=""example.com"",port=30015,user=""user"",password=""secret"")\n    cursor = connection.cursor()\n    cursor.execute(""SELECT \'Hello Python World\' FROM DUMMY"")\n    cursor.fetchone()\n    connection.close()\n\n\nIt was because the server refused to connect. It might require you to wait for a while and try again. Try closing the Hana Studio by logging off and then logging in again. Keep running the code for a number of times.\n', '\nI got the same error ([WinError 10054] An existing connection was forcibly closed by the remote host) with websocket-client after setting ping_interval = 2 in websocket.run_forever(). (I had multiple threads connecting to the same host.)\nSetting ping_interval = 10 and ping_timeout = 9 solved the issue. May be you need to reduce the amount of requests and stop making host busy otherwise it will forcibly disconnect you.\n', '\nI fixed it with a while try loop, waiting for the response to set the variable in order to exit the loop.\nWhen the connection has an exception, it waits five seconds, and continues looking for the response from the connection.\nMy code before fix, with the failed response HTTPSConnectionPool(host=\'etc.com\', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E9955A2050>, \'Connection to example.net timed out. (connect timeout=None)\'))\n \n\nfrom __future__ import print_function\nimport sys\nimport requests\n\n\ndef condition_questions(**kwargs):\n    proxies = {\'https\': \'example.com\', \'http\': \'example.com:3128\'}\n    print(kwargs, file=sys.stdout)\n    headers = {\'etc\':\'etc\',}\n    body = f\'\'\'<etc>\n                </etc>\'\'\'\n\n    try:\n        response_xml = requests.post(\'https://example.com\', data=body, headers=headers, proxies=proxies)\n    except Exception as ex:\n        print(""exception"", ex, file=sys.stdout)\n        log.exception(ex)\n    finally:\n        print(""response_xml"", response_xml, file=sys.stdout)\n        return response_xml\n\nAfter fix, with successful response response_xml <Response [200]>:\n\nimport time\n...\n\nresponse_xml = \'\'\n    while response_xml == \'\':\n        try:\n            response_xml = requests.post(\'https://example.com\', data=body, headers=headers, proxies=proxies)\n            break\n        except Exception as ex:\n            print(""exception"", ex, file=sys.stdout)\n            log.exception(ex)\n            time.sleep(5)\n            continue\n        finally:\n            print(""response_xml"", response_xml, file=sys.stdout)\n            return response_xml\n\nbased on Jatin\'s answer here  --""Just do this,\nimport time\n\npage = \'\'\nwhile page == \'\':\n    try:\n        page = requests.get(url)\n        break\n    except:\n        print(""Connection refused by the server.."")\n        print(""Let me sleep for 5 seconds"")\n        print(""ZZzzzz..."")\n        time.sleep(5)\n        print(""Was a nice sleep, now let me continue..."")\n        continue\n\nYou\'re welcome :)""\n']"
Anyone know of a good Python based web crawler that I could use?,"









Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                        
                    





I'm half-tempted to write my own, but I don't really have enough time right now.  I've seen the Wikipedia list of open source crawlers but I'd prefer something written in Python.  I realize that I could probably just use one of the tools on the Wikipedia page and wrap it in Python.  I might end up doing that - if anyone has any advice about any of those tools, I'm open to hearing about them.  I've used Heritrix via its web interface and I found it to be quite cumbersome.  I definitely won't be using a browser API for my upcoming project.
Thanks in advance.  Also, this is my first SO question!
",98k,,"[""\n\nMechanize is my favorite; great high-level browsing capabilities (super-simple form filling and submission).\nTwill is a simple scripting language built on top of Mechanize\nBeautifulSoup + urllib2 also works quite nicely.\nScrapy looks like an extremely promising project; it's new.\n\n"", '\nUse Scrapy.\nIt is a twisted-based web crawler framework. Still under heavy development but it works already. Has many goodies:\n\nBuilt-in support for parsing HTML, XML, CSV, and Javascript\nA media pipeline for scraping items with images (or any other media) and download the image files as well\nSupport for extending Scrapy by plugging your own functionality using middlewares, extensions, and pipelines\nWide range of built-in middlewares and extensions for handling of compression, cache, cookies, authentication, user-agent spoofing, robots.txt handling, statistics, crawl depth restriction, etc\nInteractive scraping shell console, very useful for developing and debugging\nWeb management console for monitoring and controlling your bot\nTelnet console for low-level access to the Scrapy process\n\nExample code to extract information about all torrent files added today in the mininova torrent site, by using a XPath selector on the HTML returned:\nclass Torrent(ScrapedItem):\n    pass\n\nclass MininovaSpider(CrawlSpider):\n    domain_name = \'mininova.org\'\n    start_urls = [\'http://www.mininova.org/today\']\n    rules = [Rule(RegexLinkExtractor(allow=[\'/tor/\\d+\']), \'parse_torrent\')]\n\n    def parse_torrent(self, response):\n        x = HtmlXPathSelector(response)\n        torrent = Torrent()\n\n        torrent.url = response.url\n        torrent.name = x.x(""//h1/text()"").extract()\n        torrent.description = x.x(""//div[@id=\'description\']"").extract()\n        torrent.size = x.x(""//div[@id=\'info-left\']/p[2]/text()[2]"").extract()\n        return [torrent]\n\n', '\nCheck the HarvestMan, a multi-threaded web-crawler written in Python, also give a look to the spider.py module.\nAnd here you can find code samples to build a simple web-crawler.\n', ""\nI've used Ruya and found it pretty good.\n"", '\nI hacked the above script to include a login page as I needed it to access a drupal site. Not pretty but may help someone out there.\n#!/usr/bin/python\n\nimport httplib2\nimport urllib\nimport urllib2\nfrom cookielib import CookieJar\nimport sys\nimport re\nfrom HTMLParser import HTMLParser\n\nclass miniHTMLParser( HTMLParser ):\n\n  viewedQueue = []\n  instQueue = []\n  headers = {}\n  opener = """"\n\n  def get_next_link( self ):\n    if self.instQueue == []:\n      return \'\'\n    else:\n      return self.instQueue.pop(0)\n\n\n  def gethtmlfile( self, site, page ):\n    try:\n        url = \'http://\'+site+\'\'+page\n        response = self.opener.open(url)\n        return response.read()\n    except Exception, err:\n        print "" Error retrieving: ""+page\n        sys.stderr.write(\'ERROR: %s\\n\' % str(err))\n    return """" \n\n    return resppage\n\n  def loginSite( self, site_url ):\n    try:\n    cj = CookieJar()\n    self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n\n    url = \'http://\'+site_url \n        params = {\'name\': \'customer_admin\', \'pass\': \'customer_admin123\', \'opt\': \'Log in\', \'form_build_id\': \'form-3560fb42948a06b01d063de48aa216ab\', \'form_id\':\'user_login_block\'}\n    user_agent = \'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\'\n    self.headers = { \'User-Agent\' : user_agent }\n\n    data = urllib.urlencode(params)\n    response = self.opener.open(url, data)\n    print ""Logged in""\n    return response.read() \n\n    except Exception, err:\n    print "" Error logging in""\n    sys.stderr.write(\'ERROR: %s\\n\' % str(err))\n\n    return 1\n\n  def handle_starttag( self, tag, attrs ):\n    if tag == \'a\':\n      newstr = str(attrs[0][1])\n      print newstr\n      if re.search(\'http\', newstr) == None:\n        if re.search(\'mailto\', newstr) == None:\n          if re.search(\'#\', newstr) == None:\n            if (newstr in self.viewedQueue) == False:\n              print ""  adding"", newstr\n              self.instQueue.append( newstr )\n              self.viewedQueue.append( newstr )\n          else:\n            print ""  ignoring"", newstr\n        else:\n          print ""  ignoring"", newstr\n      else:\n        print ""  ignoring"", newstr\n\n\ndef main():\n\n  if len(sys.argv)!=3:\n    print ""usage is ./minispider.py site link""\n    sys.exit(2)\n\n  mySpider = miniHTMLParser()\n\n  site = sys.argv[1]\n  link = sys.argv[2]\n\n  url_login_link = site+""/node?destination=node""\n  print ""\\nLogging in"", url_login_link\n  x = mySpider.loginSite( url_login_link )\n\n  while link != \'\':\n\n    print ""\\nChecking link "", link\n\n    # Get the file from the site and link\n    retfile = mySpider.gethtmlfile( site, link )\n\n    # Feed the file into the HTML parser\n    mySpider.feed(retfile)\n\n    # Search the retfile here\n\n    # Get the next link in level traversal order\n    link = mySpider.get_next_link()\n\n  mySpider.close()\n\n  print ""\\ndone\\n""\n\nif __name__ == ""__main__"":\n  main()\n\n', '\nTrust me nothing is better than curl.. . the following code can crawl 10,000 urls in parallel in less than 300 secs on Amazon EC2\nCAUTION: Don\'t hit the same domain at such a high speed.. .\n#! /usr/bin/env python\n# -*- coding: iso-8859-1 -*-\n# vi:ts=4:et\n# $Id: retriever-multi.py,v 1.29 2005/07/28 11:04:13 mfx Exp $\n\n#\n# Usage: python retriever-multi.py <file with URLs to fetch> [<# of\n#          concurrent connections>]\n#\n\nimport sys\nimport pycurl\n\n# We should ignore SIGPIPE when using pycurl.NOSIGNAL - see\n# the libcurl tutorial for more info.\ntry:\n    import signal\n    from signal import SIGPIPE, SIG_IGN\n    signal.signal(signal.SIGPIPE, signal.SIG_IGN)\nexcept ImportError:\n    pass\n\n\n# Get args\nnum_conn = 10\ntry:\n    if sys.argv[1] == ""-"":\n        urls = sys.stdin.readlines()\n    else:\n        urls = open(sys.argv[1]).readlines()\n    if len(sys.argv) >= 3:\n        num_conn = int(sys.argv[2])\nexcept:\n    print ""Usage: %s <file with URLs to fetch> [<# of concurrent connections>]"" % sys.argv[0]\n    raise SystemExit\n\n\n# Make a queue with (url, filename) tuples\nqueue = []\nfor url in urls:\n    url = url.strip()\n    if not url or url[0] == ""#"":\n        continue\n    filename = ""doc_%03d.dat"" % (len(queue) + 1)\n    queue.append((url, filename))\n\n\n# Check args\nassert queue, ""no URLs given""\nnum_urls = len(queue)\nnum_conn = min(num_conn, num_urls)\nassert 1 <= num_conn <= 10000, ""invalid number of concurrent connections""\nprint ""PycURL %s (compiled against 0x%x)"" % (pycurl.version, pycurl.COMPILE_LIBCURL_VERSION_NUM)\nprint ""----- Getting"", num_urls, ""URLs using"", num_conn, ""connections -----""\n\n\n# Pre-allocate a list of curl objects\nm = pycurl.CurlMulti()\nm.handles = []\nfor i in range(num_conn):\n    c = pycurl.Curl()\n    c.fp = None\n    c.setopt(pycurl.FOLLOWLOCATION, 1)\n    c.setopt(pycurl.MAXREDIRS, 5)\n    c.setopt(pycurl.CONNECTTIMEOUT, 30)\n    c.setopt(pycurl.TIMEOUT, 300)\n    c.setopt(pycurl.NOSIGNAL, 1)\n    m.handles.append(c)\n\n\n# Main loop\nfreelist = m.handles[:]\nnum_processed = 0\nwhile num_processed < num_urls:\n    # If there is an url to process and a free curl object, add to multi stack\n    while queue and freelist:\n        url, filename = queue.pop(0)\n        c = freelist.pop()\n        c.fp = open(filename, ""wb"")\n        c.setopt(pycurl.URL, url)\n        c.setopt(pycurl.WRITEDATA, c.fp)\n        m.add_handle(c)\n        # store some info\n        c.filename = filename\n        c.url = url\n    # Run the internal curl state machine for the multi stack\n    while 1:\n        ret, num_handles = m.perform()\n        if ret != pycurl.E_CALL_MULTI_PERFORM:\n            break\n    # Check for curl objects which have terminated, and add them to the freelist\n    while 1:\n        num_q, ok_list, err_list = m.info_read()\n        for c in ok_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print ""Success:"", c.filename, c.url, c.getinfo(pycurl.EFFECTIVE_URL)\n            freelist.append(c)\n        for c, errno, errmsg in err_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print ""Failed: "", c.filename, c.url, errno, errmsg\n            freelist.append(c)\n        num_processed = num_processed + len(ok_list) + len(err_list)\n        if num_q == 0:\n            break\n    # Currently no more I/O is pending, could do something in the meantime\n    # (display a progress bar, etc.).\n    # We just call select() to sleep until some more data is available.\n    m.select(1.0)\n\n\n# Cleanup\nfor c in m.handles:\n    if c.fp is not None:\n        c.fp.close()\n        c.fp = None\n    c.close()\nm.close()\n\n', ""\nAnother simple spider \nUses BeautifulSoup and urllib2. Nothing too sophisticated, just reads all a href's builds a list and goes though it.\n"", '\npyspider.py\n']"
how to extract links and titles from a .html page?,"
for my website, i'd like to add a new functionality.
I would like user to be able to upload his bookmarks backup file (from any browser if possible) so I can upload it to their profile and they don't have to insert all of them manually...
the only part i'm missing to do this it's the part of extracting title and URL from the uploaded file.. can anyone give a clue where to start or where to read? 
used search option and (How to extract data from a raw HTML file?) this is the most related question for mine and it doesn't talk about it..
I really don't mind if its using jquery or php
Thank you very much.
",73k,"
            41
        ","['\nThank you everyone, I GOT IT! \nThe final code:\n$html = file_get_contents(\'bookmarks.html\');\n//Create a new DOM document\n$dom = new DOMDocument;\n\n//Parse the HTML. The @ is used to suppress any parsing errors\n//that will be thrown if the $html string isn\'t valid XHTML.\n@$dom->loadHTML($html);\n\n//Get all links. You could also use any other tag name here,\n//like \'img\' or \'table\', to extract other tags.\n$links = $dom->getElementsByTagName(\'a\');\n\n//Iterate over the extracted links and display their URLs\nforeach ($links as $link){\n    //Extract and show the ""href"" attribute.\n    echo $link->nodeValue;\n    echo $link->getAttribute(\'href\'), \'<br>\';\n}\n\nThis shows you the anchor text assigned and the href for all  links in a .html file.\nAgain, thanks a lot.\n', '\nThis is probably sufficient:\n$dom = new DOMDocument;\n$dom->loadHTML($html);\nforeach ($dom->getElementsByTagName(\'a\') as $node)\n{\n  echo $node->nodeValue.\': \'.$node->getAttribute(""href"").""\\n"";\n}\n\n', ""\nAssuming the stored links are in a html file the best solution is probably to use a html parser such as PHP Simple HTML DOM Parser (never tried it myself). (The other option is to search using basic string search or regexp, and you should probably never use regexp to parse html).\nAfter reading the html file using the parser use it's functions to find the a tags:\nfrom the tutorial:\n// Find all links\nforeach($html->find('a') as $element)\n       echo $element->href . '<br>'; \n\n"", '\n$html = file_get_contents(\'your file path\');\n\n$dom = new DOMDocument;\n\n@$dom->loadHTML($html);\n\n$styles = $dom->getElementsByTagName(\'link\');\n\n$links = $dom->getElementsByTagName(\'a\');\n\n$scripts = $dom->getElementsByTagName(\'script\');\n\nforeach($styles as $style)\n{\n\n    if($style->getAttribute(\'href\')!=""#"")\n\n    {\n        echo $style->getAttribute(\'href\');\n        echo\'<br>\';\n    }\n}\n\nforeach ($links as $link){\n\n    if($link->getAttribute(\'href\')!=""#"")\n    {\n        echo $link->getAttribute(\'href\');\n        echo\'<br>\';\n    }\n}\n\nforeach($scripts as $script)\n{\n\n        echo $script->getAttribute(\'src\');\n        echo\'<br>\';\n\n}\n\n', ""\nI wanted to create a CSV of link paths and their text from html pages so I could rip menus etc from sites.\nIn this example you specify the domain you are interested in so you don't get off site links and then it produces a CSV per document\n/**\n * Extracts links to the given domain from the files and creates CSVs of the links\n */\n\n\n$LinkExtractor = new LinkExtractor('https://www.example.co.uk');\n\n$LinkExtractor->extract(__DIR__ . '/hamburger.htm');\n$LinkExtractor->extract(__DIR__ . '/navbar.htm');\n$LinkExtractor->extract(__DIR__ . '/footer.htm');\n\nclass LinkExtractor {\n    public $domain;\n\n    public function __construct($domain) {\n      $this->domain = $domain;\n    }\n\n    public function extract($file) {\n        $html = file_get_contents($file);\n        //Create a new DOM document\n        $dom = new DOMDocument;\n\n        //Parse the HTML. The @ is used to suppress any parsing errors\n        //that will be thrown if the $html string isn't valid XHTML.\n        @$dom->loadHTML($html);\n\n        //Get all links. You could also use any other tag name here,\n        //like 'img' or 'table', to extract other tags.\n        $links = $dom->getElementsByTagName('a');\n\n        $results = [];\n        //Iterate over the extracted links and display their URLs\n        foreach ($links as $link){\n            //Extract and sput the matching links in an array for the CSV\n            $href = $link->getAttribute('href');\n            $parts = parse_url($href);\n            if (!empty($parts['path']) && strpos($this->domain, $parts['host']) !== false) {\n                $results[$parts['path']] = [$parts['path'], $link->nodeValue];\n            }\n        }\n\n        asort($results);\n        // Make the CSV\n        $fp = fopen($file .'.csv', 'w');\n        foreach ($results as $fields) {\n            fputcsv($fp, $fields);\n        }\n        fclose($fp);\n    }\n}\n\n"", '\nHere is my work for one of my client and make it as a function to use everywhere.\nfunction getValidUrlsFrompage($source)\n  {\n    $links = [];\n    $content = file_get_contents($source);\n    $content = strip_tags($content, ""<a>"");\n    $subString = preg_split(""/<\\/a>/"", $content);\n    foreach ($subString as $val) {\n      if (strpos($val, ""<a href="") !== FALSE) {\n        $val = preg_replace(""/.*<a\\s+href=\\""/sm"", """", $val);\n        $val = preg_replace(""/\\"".*/"", """", $val);\n        $val = trim($val);\n      }\n      if (strlen($val) > 0 && filter_var($val, FILTER_VALIDATE_URL)) {\n        if (!in_array($val, $links)) {\n          $links[] = $val;\n        }\n      }\n    }\n    return $links;\n  }\n\nAnd use it like\n$links = getValidUrlsFrompage(""https://www.w3resource.com/"");\n\nAnd The expected output is get 99 URLs in an array,\nArray ( [0] => https://www.w3resource.com [1] => https://www.w3resource.com/html/HTML-tutorials.php [2] => https://www.w3resource.com/css/CSS-tutorials.php [3] => https://www.w3resource.com/javascript/javascript.php [4] => https://www.w3resource.com/html5/introduction.php [5] => https://www.w3resource.com/schema.org/introduction.php [6] => https://www.w3resource.com/phpjs/use-php-functions-in-javascript.php [7] => https://www.w3resource.com/twitter-bootstrap/tutorial.php [8] => https://www.w3resource.com/responsive-web-design/overview.php [9] => https://www.w3resource.com/zurb-foundation3/introduction.php [10] => https://www.w3resource.com/pure/ [11] => https://www.w3resource.com/html5-canvas/ [12] => https://www.w3resource.com/course/javascript-course.html [13] => https://www.w3resource.com/icon/ [14] => https://www.w3resource.com/linux-system-administration/installation.php [15] => https://www.w3resource.com/linux-system-administration/linux-commands-introduction.php [16] => https://www.w3resource.com/php/php-home.php [17] => https://www.w3resource.com/python/python-tutorial.php [18] => https://www.w3resource.com/java-tutorial/ [19] => https://www.w3resource.com/node.js/node.js-tutorials.php [20] => https://www.w3resource.com/ruby/ [21] => https://www.w3resource.com/c-programming/programming-in-c.php [22] => https://www.w3resource.com/sql/tutorials.php [23] => https://www.w3resource.com/mysql/mysql-tutorials.php [24] => https://w3resource.com/PostgreSQL/tutorial.php [25] => https://www.w3resource.com/sqlite/ [26] => https://www.w3resource.com/mongodb/nosql.php [27] => https://www.w3resource.com/API/google-plus/tutorial.php [28] => https://www.w3resource.com/API/youtube/tutorial.php [29] => https://www.w3resource.com/API/google-maps/index.php [30] => https://www.w3resource.com/API/flickr/tutorial.php [31] => https://www.w3resource.com/API/last.fm/tutorial.php [32] => https://www.w3resource.com/API/twitter-rest-api/ [33] => https://www.w3resource.com/xml/xml.php [34] => https://www.w3resource.com/JSON/introduction.php [35] => https://www.w3resource.com/ajax/introduction.php [36] => https://www.w3resource.com/html-css-exercise/index.php [37] => https://www.w3resource.com/javascript-exercises/ [38] => https://www.w3resource.com/jquery-exercises/ [39] => https://www.w3resource.com/jquery-ui-exercises/ [40] => https://www.w3resource.com/coffeescript-exercises/ [41] => https://www.w3resource.com/php-exercises/ [42] => https://www.w3resource.com/python-exercises/ [43] => https://www.w3resource.com/c-programming-exercises/ [44] => https://www.w3resource.com/csharp-exercises/ [45] => https://www.w3resource.com/java-exercises/ [46] => https://www.w3resource.com/sql-exercises/ [47] => https://www.w3resource.com/oracle-exercises/ [48] => https://www.w3resource.com/mysql-exercises/ [49] => https://www.w3resource.com/sqlite-exercises/ [50] => https://www.w3resource.com/postgresql-exercises/ [51] => https://www.w3resource.com/mongodb-exercises/ [52] => https://www.w3resource.com/twitter-bootstrap/examples.php [53] => https://www.w3resource.com/euler-project/ [54] => https://w3resource.com/w3skills/html5-quiz/ [55] => https://w3resource.com/w3skills/php-fundamentals/ [56] => https://w3resource.com/w3skills/sql-beginner/ [57] => https://w3resource.com/w3skills/python-beginner-quiz/ [58] => https://w3resource.com/w3skills/mysql-basic-quiz/ [59] => https://w3resource.com/w3skills/javascript-basic-skill-test/ [60] => https://w3resource.com/w3skills/javascript-advanced-quiz/ [61] => https://w3resource.com/w3skills/javascript-quiz-part-iii/ [62] => https://w3resource.com/w3skills/mongodb-basic-quiz/ [63] => https://www.w3resource.com/form-template/ [64] => https://www.w3resource.com/slides/ [65] => https://www.w3resource.com/convert/number/binary-to-decimal.php [66] => https://www.w3resource.com/excel/ [67] => https://www.w3resource.com/video-tutorial/php/some-basics-of-php.php [68] => https://www.w3resource.com/video-tutorial/javascript/list-of-tutorial.php [69] => https://www.w3resource.com/web-development-tools/firebug-tutorials.php [70] => https://www.w3resource.com/web-development-tools/useful-web-development-tools.php [71] => https://www.facebook.com/w3resource [72] => https://twitter.com/w3resource [73] => https://plus.google.com/+W3resource [74] => https://in.linkedin.com/in/w3resource [75] => https://feeds.feedburner.com/W3resource [76] => https://www.w3resource.com/ruby-exercises/ [77] => https://www.w3resource.com/graphics/matplotlib/ [78] => https://www.w3resource.com/python-exercises/numpy/index.php [79] => https://www.w3resource.com/python-exercises/pandas/index.php [80] => https://w3resource.com/plsql-exercises/ [81] => https://w3resource.com/swift-programming-exercises/ [82] => https://www.w3resource.com/angular/getting-started-with-angular.php [83] => https://www.w3resource.com/react/react-js-overview.php [84] => https://www.w3resource.com/vue/installation.php [85] => https://www.w3resource.com/jest/jest-getting-started.php [86] => https://www.w3resource.com/numpy/ [87] => https://www.w3resource.com/php/composer/a-gentle-introduction-to-composer.php [88] => https://www.w3resource.com/php/PHPUnit/a-gentle-introduction-to-unit-test-and-testing.php [89] => https://www.w3resource.com/laravel/laravel-tutorial.php [90] => https://www.w3resource.com/oracle/index.php [91] => https://www.w3resource.com/redis/index.php [92] => https://www.w3resource.com/cpp-exercises/ [93] => https://www.w3resource.com/r-programming-exercises/ [94] => https://w3resource.com/w3skills/ [95] => https://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US [96] => https://www.w3resource.com/privacy.php [97] => https://www.w3resource.com/about.php [98] => https://www.w3resource.com/contact.php [99] => https://www.w3resource.com/feedback.php [100] => https://www.w3resource.com/advertise.php )\n\nHope, this will help someone. And here is a gist - \nhttps://gist.github.com/ManiruzzamanAkash/74cffb9ffdfc92f57bd9cf214cf13491\n']"
"Pulling data from a webpage, parsing it for specific pieces, and displaying it","
I've been using this site for a long time to find answers to my questions, but I wasn't able to find the answer on this one.
I am working with a small group on a class project. We're to build a small ""game trading"" website that allows people to register, put in a game they have they want to trade, and accept trades from others or request a trade.
We have the site functioning long ahead of schedule so we're trying to add more to the site. One thing I want to do myself is to link the games that are put in to Metacritic.
Here's what I need to do. I need to (using asp and c# in visual studio 2012) get the correct game page on metacritic, pull its data, parse it for specific parts, and then display the data on our page. 
Essentially when you choose a game you want to trade for we want a small div to display with the game's information and rating. I'm wanting to do it this way to learn more and get something out of this project I didn't have to start with. 
I was wondering if anyone could tell me where to start. I don't know how to pull data from a page. I'm still trying to figure out if I need to try and write something to automatically search for the game's title and find the page that way or if I can find some way to go straight to the game's page. And once I've gotten the data, I don't know how to pull the specific information I need from it.
One of the things that doesn't make this easy is that I'm learning c++ along with c# and asp so I keep getting my wires crossed. If someone could point me in the right direction it would be a big help. Thanks
",102k,"
            19
        ","['\nThis small example uses HtmlAgilityPack, and using XPath selectors to get to the desired elements.\nprotected void Page_Load(object sender, EventArgs e)\n{\n    string url = ""http://www.metacritic.com/game/pc/halo-spartan-assault"";\n    var web = new HtmlAgilityPack.HtmlWeb();\n    HtmlDocument doc = web.Load(url);\n\n    string metascore = doc.DocumentNode.SelectNodes(""//*[@id=\\""main\\""]/div[3]/div/div[2]/div[1]/div[1]/div/div/div[2]/a/span[1]"")[0].InnerText;\n    string userscore = doc.DocumentNode.SelectNodes(""//*[@id=\\""main\\""]/div[3]/div/div[2]/div[1]/div[2]/div[1]/div/div[2]/a/span[1]"")[0].InnerText;\n    string summary = doc.DocumentNode.SelectNodes(""//*[@id=\\""main\\""]/div[3]/div/div[2]/div[2]/div[1]/ul/li/span[2]/span/span[1]"")[0].InnerText;\n}\n\nAn easy way to obtain the XPath for a given element is by using your web browser (I use Chrome) Developer Tools:\n\nOpen the Developer Tools (F12 or Ctrl + Shift + C on Windows or Command + Shift + C for Mac).\nSelect the element in the page that you want the XPath for.\nRight click the element in the ""Elements"" tab.\nClick on ""Copy as XPath"".\n\nYou can paste it exactly like that in c# (as shown in my code), but make sure to escape the quotes.\nYou have to make sure you use some error handling techniques because Web scraping can cause errors if they change the HTML formatting of the page.\nEdit\nPer @knocte\'s suggestion,  here is the link to the Nuget package for HTMLAgilityPack:\nhttps://www.nuget.org/packages/HtmlAgilityPack/\n', '\nI looked and Metacritic.com doesn\'t have an API.\nYou can use an HttpWebRequest to get the contents of a website as a string. \nusing System.Net;\nusing System.IO;\nusing System.Windows.Forms;\n\nstring result = null;\nstring url = ""http://www.stackoverflow.com"";\nWebResponse response = null;\nStreamReader reader = null;\n\ntry\n{\n    HttpWebRequest request = (HttpWebRequest)WebRequest.Create(url);\n    request.Method = ""GET"";\n    response = request.GetResponse();\n    reader = new StreamReader(response.GetResponseStream(), Encoding.UTF8);\n    result = reader.ReadToEnd();\n}\ncatch (Exception ex)\n{\n    // handle error\n    MessageBox.Show(ex.Message);\n}\nfinally\n{\n    if (reader != null)\n        reader.Close();\n    if (response != null)\n        response.Close();\n}\n\nThen you can parse the string for the data that you want by taking advantage of Metacritic\'s use of meta tags. Here\'s the information they have available in meta tags:\n\nog:title\nog:type\nog:url\nog:image\nog:site_name\nog:description\n\nThe format of each tag is: meta name=""og:title"" content=""In a World...""\n', '\nI recommend Dcsoup.  There\'s a nuget package for it and it uses CSS selectors so it is familiar if you use jquery.  I\'ve tried others but it is the best and easiest to use that I\'ve found.  There\'s not much documentation, but it\'s open source and a port of the java jsoup library that has good documentation. (Documentation for the .NET API here.) I absolutely love it.\nvar timeoutInMilliseconds = 5000;\nvar uri = new Uri(""http://www.metacritic.com/game/pc/fallout-4"");\nvar doc = Supremes.Dcsoup.Parse(uri, timeoutInMilliseconds);\n\n// <span itemprop=""ratingValue"">86</span>\nvar ratingSpan = doc.Select(""span[itemprop=ratingValue]"");\nint ratingValue = int.Parse(ratingSpan.Text);\n\n// selectors match both critic and user scores\nvar scoreDiv = doc.Select(""div.score_summary"");\nvar scoreAnchor = scoreDiv.Select(""a.metascore_anchor"");\nint criticRating = int.Parse(scoreAnchor[0].Text);\nfloat userRating = float.Parse(scoreAnchor[1].Text);\n\n', '\nI\'d recomend you WebsiteParser - it\'s based on HtmlAgilityPack (mentioned by Hanlet Escaño) but it makes web scraping easier with attributes and css selectors:\nclass PersonModel\n{\n    [Selector(""#BirdthDate"")]\n    [Converter(typeof(DateTimeConverter))]\n    public DateTime BirdthDate { get; set; }\n}\n\n// ...\n\nPersonModel person = WebContentParser.Parse<PersonModel>(html);\n\nNuget link\n']"
Parse HTML content in VBA,"
I have a question relating to HTML parsing. I have a website with some products and I would like to catch text within page into my current spreadsheet. This spreadsheet is quite big but contains ItemNbr in 3rd column, I expect the text in the 14th column and one row corresponds to one product (item).
My idea is to fetch the 'Material' on the webpage which is inside the Innertext after  tag. The id number changes from one page to page (sometimes ).
Here is the structure of the website:
<div style=""position:relative;"">
    <div></div>
    <table id=""list-table"" width=""100%"" tabindex=""1"" cellspacing=""0"" cellpadding=""0"" border=""0"" role=""grid"" aria-multiselectable=""false"" aria-labelledby=""gbox_list-table"" class=""ui-jqgrid-btable"" style=""width: 930px;"">
        <tbody>
            <tr class=""jqgfirstrow"" role=""row"" style=""height:auto"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""1"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""2"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""3"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""4"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""5"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""6"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""7"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td role=""gridcell"" style=""padding-left:10px"" title=""Material"" aria-describedby=""list-table_"">Material</td>
                <td role=""gridcell"" style="""" title=""600D polyester."" aria-describedby=""list-table_"">600D polyester.</td>
            </tr>           
            <tr ...>
            </tr>
        </tbody>
    </table> </div>

I would like to get ""600D Polyester"" as a result.
My (not working) code snippet is as is:
Sub ParseMaterial()

    Dim Cell As Integer
    Dim ItemNbr As String

    Dim AElement As Object
    Dim AElements As IHTMLElementCollection
Dim IE As MSXML2.XMLHTTP60
Set IE = New MSXML2.XMLHTTP60

Dim HTMLDoc As MSHTML.HTMLDocument
Dim HTMLBody As MSHTML.HTMLBody

Set HTMLDoc = New MSHTML.HTMLDocument
Set HTMLBody = HTMLDoc.body

For Cell = 1 To 5                            'I iterate through the file row by row

    ItemNbr = Cells(Cell, 3).Value           'ItemNbr isin the 3rd Column of my spreadsheet

    IE.Open ""GET"", ""http://www.example.com/?item="" & ItemNbr, False
    IE.send

    While IE.ReadyState <> 4
        DoEvents
    Wend

    HTMLBody.innerHTML = IE.responseText

    Set AElements = HTMLDoc.getElementById(""list-table"").getElementsByTagName(""tr"")
    For Each AElement In AElements
        If AElement.Title = ""Material"" Then
            Cells(Cell, 14) = AElement.nextNode.value     'I write the material in the 14th column
        End If
    Next AElement

        Application.Wait (Now + TimeValue(""0:00:2""))

Next Cell

Thanks for your help !
",82k,"
            15
        ","['\nJust a couple things that hopefully will get you in the right direction:\n\nclean up a bit: remove the readystate property testing loop. The value returned by the readystate property will never change in this context - code will pause after the send instruction, to resume only once the server response is received, or has failed to do so. The readystate property will be set accordingly, and the code will resume execution. You should still test for the ready state, but the loop is just unnecessary\n\ntarget the right HTML elements: you are searching through the tr elements - while the logic of how you use these elements in your code actually looks to point to td elements\n\nmake sure the properties are actually available for the objects you are using them on: to help you with this, try and declare all your variable as specific objects instead of the generic Object. This will activate intellisense. If you have a difficult time finding the actual name of your object as defined in the relevant library in a first place, declare it as the generic Object, run your code, and then inspect the type of the object - by printing typename(your_object) to the debug window for instance. This should put you on your way\n\n\nI have also included some code below that may help. If you still can\'t get this to work and you can share your urls - plz do that.\nSub getInfoWeb()\n\n    Dim cell As Integer\n    Dim xhr As MSXML2.XMLHTTP60\n    Dim doc As MSHTML.HTMLDocument\n    Dim table As MSHTML.HTMLTable\n    Dim tableCells As MSHTML.IHTMLElementCollection\n    \n    Set xhr = New MSXML2.XMLHTTP60\n   \n    For cell = 1 To 5\n        \n        ItemNbr = Cells(cell, 3).Value\n        \n        With xhr\n        \n            .Open ""GET"", ""http://www.example.com/?item="" & ItemNbr, False\n            .send\n            \n            If .readyState = 4 And .Status = 200 Then\n                Set doc = New MSHTML.HTMLDocument\n                doc.body.innerHTML = .responseText\n            Else\n                MsgBox ""Error"" & vbNewLine & ""Ready state: "" & .readyState & _\n                vbNewLine & ""HTTP request status: "" & .Status\n            End If\n            \n        End With\n        \n        Set table = doc.getElementById(""list-table"")\n        Set tableCells = table.getElementsByTagName(""td"")\n        \n        For Each tableCell In tableCells\n            If tableCell.getAttribute(""title"") = ""Material"" Then\n                Cells(cell, 14).Value = tableCell.NextSibling.innerHTML\n            End If\n        Next tableCell\n    \n    Next cell\n    \nEnd Sub\n\nEDIT: as a follow-up to the further information you provided in the comment below - and the additional comments I have added\n\'Determine your product number\n    \'Open an xhr for your source url, and retrieve the product number from there - search for the tag which\n    \'text include the ""productnummer:"" substring, and extract the product number from the outerstring\n    \'OR\n    \'if the product number consistently consists of the fctkeywords you are entering in your source url\n    \'with two ""0"" appended - just build the product number like that\n\'Open an new xhr for this url ""http://www.pfconcept.com/cgi-bin/wspd_pcdb_cgi.sh/y/y2productspec-ajax.p?itemc="" & product_number & ""&_search=false&rows=-1&page=1&sidx=&sord=asc""\n\'Load the response in an XML document, and retrieve the material information\n\nSub getInfoWeb()\n\n    Dim xhr As MSXML2.XMLHTTP60\n    Dim doc As MSXML2.DOMDocument60\n    Dim xmlCell As MSXML2.IXMLDOMElement\n    Dim xmlCells As MSXML2.IXMLDOMNodeList\n    Dim materialValueElement As MSXML2.IXMLDOMElement\n    \n    Set xhr = New MSXML2.XMLHTTP60\n        \n        With xhr\n            \n            .Open ""GET"", ""http://www.pfconcept.com/cgi-bin/wspd_pcdb_cgi.sh/y/y2productspec-ajax.p?itemc=10031700&_search=false&rows=-1&page=1&sidx=&sord=asc"", False\n            .send\n            \n            If .readyState = 4 And .Status = 200 Then\n                Set doc = New MSXML2.DOMDocument60\n                doc.LoadXML .responseText\n            Else\n                MsgBox ""Error"" & vbNewLine & ""Ready state: "" & .readyState & _\n                vbNewLine & ""HTTP request status: "" & .Status\n            End If\n            \n        End With\n        \n        Set xmlCells = doc.getElementsByTagName(""cell"")\n\n        For Each xmlCell In xmlCells\n            If xmlCell.Text = ""Materiaal"" Then\n                Set materialValueElement = xmlCell.NextSibling\n            End If\n        Next\n        \n        MsgBox materialValueElement.Text\n    \nEnd Sub\n\nEDIT2: an alternative automating IE\nSub searchWebViaIE()\n    Dim ie As SHDocVw.InternetExplorer\n    Dim doc As MSHTML.HTMLDocument\n    Dim anchors As MSHTML.IHTMLElementCollection\n    Dim anchor As MSHTML.HTMLAnchorElement\n    Dim prodSpec As MSHTML.HTMLAnchorElement\n    Dim tableCells As MSHTML.IHTMLElementCollection\n    Dim materialValueElement As MSHTML.HTMLTableCell\n    Dim tableCell As MSHTML.HTMLTableCell\n    \n    Set ie = New SHDocVw.InternetExplorer\n    \n    With ie\n        .navigate ""http://www.pfconcept.com/cgi-bin/wspd_pcdb_cgi.sh/y/y2facetmain.p?fctkeywords=100317&world=general#tabs-4""\n        .Visible = True\n        \n        Do While .readyState <> READYSTATE_COMPLETE Or .Busy = True\n            DoEvents\n        Loop\n        \n        Set doc = .document\n        \n        Set anchors = doc.getElementsByTagName(""a"")\n        \n        For Each anchor In anchors\n            If InStr(anchor.innerHTML, ""Product Specificatie"") <> 0 Then\n                anchor.Click\n                Exit For\n            End If\n        Next anchor\n        \n        Do While .readyState <> READYSTATE_COMPLETE Or .Busy = True\n            DoEvents\n        Loop\n    \n    End With\n        \n    For Each anchor In anchors\n        If InStr(anchor.innerHTML, ""Product Specificatie"") <> 0 Then\n            Set prodSpec = anchor\n        End If\n    Next anchor\n    \n    Set tableCells = doc.getElementById(""list-table"").getElementsByTagName(""td"")\n    \n    If Not tableCells Is Nothing Then\n        For Each tableCell In tableCells\n            If tableCell.innerHTML = ""Materiaal"" Then\n                Set materialValueElement = tableCell.NextSibling\n            End If\n        Next tableCell\n    End If\n    \n    MsgBox materialValueElement.innerHTML\n    \nEnd Sub\n\n', '\nNot related to tables or Excel ( I use MS-Access 2013) but directly related to the topic title. My solution is \nPrivate Sub Sample(urlSource)\nDim httpRequest As New WinHttpRequest\nDim doc As MSHTML.HTMLDocument\nDim tags As MSHTML.IHTMLElementCollection\nDim tag As MSHTML.HTMLHtmlElement\nhttpRequest.Option(WinHttpRequestOption_UserAgentString) = ""Mozilla/4.0 (compatible;MSIE 7.0; Windows NT 6.0)""\nhttpRequest.Open ""GET"", urlSource\nhttpRequest.send \' fetching webpage\nSet doc = New MSHTML.HTMLDocument\ndoc.body.innerHTML = httpRequest.responseText\nSet tags = doc.getElementsByTagName(""a"")\ni = 1\nFor Each tag In tags\n  Debug.Print i\n  Debug.Print tag.href\n  Debug.Print tag.innerText\n  \'Debug.Print tag.Attributes(""any other attributes you need"")() \' may return an object\n  i = i + 1\n  If i Mod 50 = 0 Then Stop\n  \' or code to store results in a table\nNext\nEnd Sub\n\n']"
Do Google's crawlers interpret Javascript? What if I load a page through AJAX? [closed],"






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.


Closed 10 years ago.







                        Improve this question
                    



When a user enters my page, I have to make another AJAX call...to load data inside a div.
That's just how my application works.
The problem is...when I view the source of this code, it does not contain the source of that AJAX.  Of course, when I do wget URL ...it also does not show the AJAX HTML. Makes sense.
But what about Google? Will Google be able to crawl the content, as if it's a browser?  How do I allow Google to crawl my page just like a user would see it?
",13k,"
            15
        ","['\nDespite the answers above, apparently it does interpret JavaScript, to an extent, according to Matt Cutts:\n\n""For a while, we were scanning within JavaScript, and we were looking for links. Google has gotten smarter about JavaScript and can execute some JavaScript. I wouldn\'t say that we execute all JavaScript, so there are some conditions in which we don\'t execute JavaScript. Certainly there are some common, well-known JavaScript things like Google Analytics, which you wouldn\'t even want to execute because you wouldn\'t want to try to generate phantom visits from Googlebot into your Google Analytics"".\n\n(Why answer an answered question? Mostly because I just saw it because of a duplicate question posted today, and didn\'t see this info here.)\n', '\nActually... Google does have a solution for crawling Ajax applications...\nhttp://code.google.com/web/ajaxcrawling/docs/getting-started.html\n', '\nUpdated: From the answer to this question about ""Ajax generated content, crawling and black listing"" I found this document about the way Google crawls AJAX requests which is part of a collection of documents about Making AJAX Applications Crawlable. \nIn short, it means you need to use <a href=""#!data"">...</a> rather than <a href=""#data"">...</a> and then supply a real server-side answer to the URL path/to/path?_escaped_fragment_=data.\nAlso consider a <link/> tag to supply crawlers with a hint to SEO-friendly content. <link rel=""canonical""/>, which this article explains a bit, is a good candidate\nNote: I took the answer from: https://stackoverflow.com/questions/10006825/search-engine-misunderstanting/10006925#comment12792862_10006925  because it seems I can\'t delete mine here.\n', '\nWhat I do in this situation is always initially populate the page with content based upon the default parameters of whatever the Ajax call is doing. Then I only use the ajax javascript to do updates to the page.\n', '\nAs other answers say, Google\'s crawler (and I believe those of other search engines) does not interpret Javascript -- and you should not try to differentiate by user-agent or the like (at the risk of having your site downgraded or blocked for presenting different contents to users vs robots).  Rather, do offer some (perhaps minimal) level of content to visitors that have Javascript blocked for whatever reason (including the cases where the reason is ""being robots"";-) -- after all, that\'s the very reason the noscript tag exists... to make it very, very easy to offer such ""minimal level of content"" (or, more than minimal, if you so choose;-) to non-users of Javascript!\n', '\nWeb crawlers have a difficult time with ajax and javascript that dynamically loads content.  This site has some ideas that show you how to help google index your site http://www.softwaredeveloper.com/features/google-ajax-play-nice-061907/\n', '\nIf you make your pages such that they will work with OR without javascript (i.e. fall back to using frames or standard GET / POST requests to the server if javascript fails, either automatically, or via a ""display as plain html"" link ), it will be much easier for search engines to crawl the page.\nIt makes sense for them not to crawl ""dynamic"" content - because it is just that...dynamic.\nMy understanding is that in most situations, Google does not crawl the client-side-dynamic-content.\n', '\nNow It looks ike Google bot is not limited to simple lynx like browser.\nGoogle bot tries to grab the Humanly visible and Humanly contrasting text to give importance in different sectors of page. So it renders the page with a Layout Engine just like another browser like FF or Chrome have.\nIt might even have v8 Javascript Engine support. and the bot might load the page and wait till dom is ready and may even wait for few seconds for the page to come to a stable view. and then crop the contrasting text.\n']"
Fetch contents(loaded through AJAX call) of a web page,"
I am a beginner to crawling. I have a requirement to fetch the posts and comments from a link. I want to automate this process. I considered using webcrawler and jsoup for this but was told that webcrawlers are mostly used for websites with greater depth. 
Sample for a page: Jive community website
For this page, when I view the source of the page, I can see only the post and not the comments. Think this is because comments are fetched through an AJAX call to the server. 
Hence, when I use jsoup, it doesn't fetch the comments. 
So how can I automate the process of fetching posts and comments?
",10k,"
            6
        ","[""\nJsoup is a html parser only. Unfortunately it's not possible to parse any javascript / ajax content, since jsoup can't execute those.\nThe solution: using a library which can handle Scripts.\nHere are some examples i know:\n\nHtmlUnit\nJava Script Engine\nApache Commons BSF\nRhino\n\nIf such a library doesn't support parsing or selectors, you can at least use them to get Html out of the scripts (which then can be parsed by jsoup).\n"", '\nJsoup does not handle with Javascript and Ajax, so you need to use Htmlunit or selenium.\nAfter loading page using Htmlunit or any you can use jsoup for rest of task.\n']"
How can I handle Javascript in a Perl web crawler?,"
I would like to crawl a website, the problem is, that its full of JavaScript things, such as buttons and such that when they are pressed, they do not change the URL, but the data on the page is changed. 
Usually I use LWP / Mechanize etc to crawl sites, but neither support JavaScript.
any idea?
",5k,"
            19
        ","['\nAnother option might be Selenium with WWW::Selenium module\n', ""\nThe WWW::Scripter module has a JavaScript plugin that may be useful. Can't say I've used it myself, however.\n"", '\nWWW::Mechanize::Firefox might be of use.  that way you can have Firefox handle the complex JavaScript issues and then extract the resultant html.\n', '\nI would suggest HtmlUnit and Perl wrapper: WWW::HtmlUnit.\n']"
Detecting 'stealth' web-crawlers,"
What options are there to detect web-crawlers that do not want to be detected?
(I know that listing detection techniques will allow the smart stealth-crawler programmer to make a better spider, but I do not think that we will ever be able to block smart stealth-crawlers anyway, only the ones that make mistakes.)
I'm not talking about the nice crawlers such as Googlebot and Yahoo! Slurp.
I consider a bot nice if it:

identifies itself as a bot in the user agent string
reads robots.txt (and obeys it)

I'm talking about the bad crawlers, hiding behind common user agents, using my bandwidth and never giving me anything in return.
There are some trapdoors that can be constructed updated list (thanks Chris, gs):

Adding a directory only listed (marked as disallow) in the robots.txt,
Adding invisible links (possibly marked as rel=""nofollow""?),

style=""display: none;"" on link or parent container
placed underneath another element with higher z-index


detect who doesn't understand CaPiTaLiSaTioN,
detect who tries to post replies but always fail the Captcha.
detect GET requests to POST-only resources
detect interval between requests
detect order of pages requested
detect who (consistently) requests HTTPS resources over HTTP
detect who does not request image file (this in combination with a list of user-agents of known image capable browsers works surprisingly nice)

Some traps would be triggered by both 'good' and 'bad' bots.
you could combine those with a whitelist:

It trigger a trap
It request robots.txt?
It doest not trigger another trap because it obeyed robots.txt

One other important thing here is:
Please consider blind people using a screen readers: give people a way to contact you, or solve a (non-image) Captcha to continue browsing.
What methods are there to automatically detect the web crawlers trying to mask themselves as normal human visitors.
The question is not: How do I catch every crawler. The question is: How can I maximize the chance of detecting a crawler.
Some spiders are really good, and actually parse and understand HTML, xhtml, CSS JavaScript, VBScript etc...
I have no illusions: I won't be able to beat them.
You would however be surprised how stupid some crawlers are. With the best example  of stupidity (in my opinion) being: cast all URLs to lower case before requesting them.
And then there is a whole bunch of crawlers that are just 'not good enough' to avoid the various trapdoors.
",27k,"
            112
        ","[""\nA while back, I worked with a smallish hosting company to help them implement a solution to this.  The system I developed examined web server logs for excessive activity from any given IP address and issued firewall rules to block offenders.  It included whitelists of IP addresses/ranges based on http://www.iplists.com/, which were then updated automatically as needed by checking claimed user-agent strings and, if the client claimed to be a legitimate spider but not on the whitelist, it performed DNS/reverse-DNS lookups to verify that the source IP address corresponds to the claimed owner of the bot.  As a failsafe, these actions were reported to the admin by email, along with links to black/whitelist the address in case of an incorrect assessment.\nI haven't talked to that client in 6 months or so, but, last I heard, the system was performing quite effectively.\nSide point:  If you're thinking about doing a similar detection system based on hit-rate-limiting, be sure to use at least one-minute (and preferably at least five-minute) totals.  I see a lot of people talking about these kinds of schemes who want to block anyone who tops 5-10 hits in a second, which may generate false positives on image-heavy pages (unless images are excluded from the tally) and will generate false positives when someone like me finds an interesting site that he wants to read all of, so he opens up all the links in tabs to load in the background while he reads the first one.\n"", '\nSee Project Honeypot - they\'re setting up bot traps on large scale (and have DNSRBL with their IPs).\nUse tricky URLs and HTML:\n<a href=""//example.com/""> = http://example.com/ on http pages.\n<a href=""page&amp;&#x23;hash""> = page& + #hash\n\nIn HTML you can use plenty of tricks with comments, CDATA elements, entities, etc:\n<a href=""foo<!--bar-->""> (comment should not be removed)\n<script>var haha = \'<a href=""bot"">\'</script>\n<script>// <!-- </script> <!--><a href=""bot""> <!-->\n\n', '\nAn easy solution is to create a link and make it invisible\n<a href=""iamabot.script"" style=""display:none;"">Don\'t click me!</a>\n\nOf course you should expect that some people who look at the source code follow that link just to see where it leads. But you could present those users with a captcha...\nValid crawlers would, of course, also follow the link. But you should not implement a rel=nofollow, but look for the sign of a valid crawler. (like the user agent)\n', ""\nOne thing you didn't list, that are used commonly to detect bad crawlers.\nHit speed, good web crawlers will break their hits up so they don't deluge a site with requests.  Bad ones will do one of three things:\n\nhit sequential links one after the other\nhit sequential links in some paralell sequence (2 or more at a time.)\nhit sequential links at a fixed interval\n\nAlso, some offline browsing programs will slurp up a number of pages, I'm not sure what kind of threshold you'd want to use, to start blocking by IP address.\nThis method will also catch mirroring programs like fmirror or wget.\nIf the bot randomizes the time interval, you could check to see if the links are traversed in a sequential or depth-first manner, or you can see if the bot is traversing a huge amount of text (as in words to read) in a too-short period of time.  Some sites limit the number of requests per hour, also.\nActually, I heard an idea somewhere, I don't remember where, that if a user gets too much data, in terms of kilobytes, they can be presented with a captcha asking them to prove they aren't a bot.  I've never seen that implemented though.\n\nUpdate on Hiding Links\n\nAs far as hiding links goes, you can put a div under another, with CSS (placing it first in the draw order) and possibly setting the z-order.  A bot could not ignore that, without parsing all your javascript to see if it is a menu.  To some extent, links inside invisible DIV elements also can't be ignored without the bot parsing all the javascript.\nTaking that idea to completion, uncalled javascript which could potentially show the hidden elements would possilby fool a subset of javascript parsing bots.  And, it is not a lot of work to implement.\n"", ""\nIt's not actually that easy to keep up with the good user agent strings. Browser versions come and go. Making a statistic about user agent strings by different behaviors can reveal interesting things.\nI don't know how far this could be automated, but at least it is one differentiating thing.\n"", '\nOne simple bot detection method I\'ve heard of for forms is the hidden input technique. If you are trying to secure a form put a input in the form with an id that looks completely legit. Then use css in an external file to hide it. Or if you are really paranoid, setup something like jquery to hide the input box on page load. If you do this right I imagine it would be very hard for a bot to figure out. You know those bots have it in there nature to fill out everything on a page especially if you give your hidden input an id of something like id=""fname"", etc.\n', '\nUntested, but here is a nice list of user-agents you could make a regular expression out of.  Could get you most of the way there:\nADSARobot|ah-ha|almaden|aktuelles|Anarchie|amzn_assoc|ASPSeek|ASSORT|ATHENS|Atomz|attach|attache|autoemailspider|BackWeb|Bandit|BatchFTP|bdfetch|big.brother|BlackWidow|bmclient|Boston\\ Project|BravoBrian\\ SpiderEngine\\ MarcoPolo|Bot\\ mailto:craftbot@yahoo.com|Buddy|Bullseye|bumblebee|capture|CherryPicker|ChinaClaw|CICC|clipping|Collector|Copier|Crescent|Crescent\\ Internet\\ ToolPak|Custo|cyberalert|DA$|Deweb|diagem|Digger|Digimarc|DIIbot|DISCo|DISCo\\ Pump|DISCoFinder|Download\\ Demon|Download\\ Wonder|Downloader|Drip|DSurf15a|DTS.Agent|EasyDL|eCatch|ecollector|efp@gmx\\.net|Email\\ Extractor|EirGrabber|email|EmailCollector|EmailSiphon|EmailWolf|Express\\ WebPictures|ExtractorPro|EyeNetIE|FavOrg|fastlwspider|Favorites\\ Sweeper|Fetch|FEZhead|FileHound|FlashGet\\ WebWasher|FlickBot|fluffy|FrontPage|GalaxyBot|Generic|Getleft|GetRight|GetSmart|GetWeb!|GetWebPage|gigabaz|Girafabot|Go\\!Zilla|Go!Zilla|Go-Ahead-Got-It|GornKer|gotit|Grabber|GrabNet|Grafula|Green\\ Research|grub-client|Harvest|hhjhj@yahoo|hloader|HMView|HomePageSearch|http\\ generic|HTTrack|httpdown|httrack|ia_archiver|IBM_Planetwide|Image\\ Stripper|Image\\ Sucker|imagefetch|IncyWincy|Indy*Library|Indy\\ Library|informant|Ingelin|InterGET|Internet\\ Ninja|InternetLinkagent|Internet\\ Ninja|InternetSeer\\.com|Iria|Irvine|JBH*agent|JetCar|JOC|JOC\\ Web\\ Spider|JustView|KWebGet|Lachesis|larbin|LeechFTP|LexiBot|lftp|libwww|likse|Link|Link*Sleuth|LINKS\\ ARoMATIZED|LinkWalker|LWP|lwp-trivial|Mag-Net|Magnet|Mac\\ Finder|Mag-Net|Mass\\ Downloader|MCspider|Memo|Microsoft.URL|MIDown\\ tool|Mirror|Missigua\\ Locator|Mister\\ PiX|MMMtoCrawl\\/UrlDispatcherLLL|^Mozilla$|Mozilla.*Indy|Mozilla.*NEWT|Mozilla*MSIECrawler|MS\\ FrontPage*|MSFrontPage|MSIECrawler|MSProxy|multithreaddb|nationaldirectory|Navroad|NearSite|NetAnts|NetCarta|NetMechanic|netprospector|NetResearchServer|NetSpider|Net\\ Vampire|NetZIP|NetZip\\ Downloader|NetZippy|NEWT|NICErsPRO|Ninja|NPBot|Octopus|Offline\\ Explorer|Offline\\ Navigator|OpaL|Openfind|OpenTextSiteCrawler|OrangeBot|PageGrabber|Papa\\ Foto|PackRat|pavuk|pcBrowser|PersonaPilot|Ping|PingALink|Pockey|Proxy|psbot|PSurf|puf|Pump|PushSite|QRVA|RealDownload|Reaper|Recorder|ReGet|replacer|RepoMonkey|Robozilla|Rover|RPT-HTTPClient|Rsync|Scooter|SearchExpress|searchhippo|searchterms\\.it|Second\\ Street\\ Research|Seeker|Shai|Siphon|sitecheck|sitecheck.internetseer.com|SiteSnagger|SlySearch|SmartDownload|snagger|Snake|SpaceBison|Spegla|SpiderBot|sproose|SqWorm|Stripper|Sucker|SuperBot|SuperHTTP|Surfbot|SurfWalker|Szukacz|tAkeOut|tarspider|Teleport\\ Pro|Templeton|TrueRobot|TV33_Mercator|UIowaCrawler|UtilMind|URLSpiderPro|URL_Spider_Pro|Vacuum|vagabondo|vayala|visibilitygap|VoidEYE|vspider|Web\\ Downloader|w3mir|Web\\ Data\\ Extractor|Web\\ Image\\ Collector|Web\\ Sucker|Wweb|WebAuto|WebBandit|web\\.by\\.mail|Webclipping|webcollage|webcollector|WebCopier|webcraft@bea|webdevil|webdownloader|Webdup|WebEMailExtrac|WebFetch|WebGo\\ IS|WebHook|Webinator|WebLeacher|WEBMASTERS|WebMiner|WebMirror|webmole|WebReaper|WebSauger|Website|Website\\ eXtractor|Website\\ Quester|WebSnake|Webster|WebStripper|websucker|webvac|webwalk|webweasel|WebWhacker|WebZIP|Wget|Whacker|whizbang|WhosTalking|Widow|WISEbot|WWWOFFLE|x-Tractor|^Xaldon\\ WebSpider|WUMPUS|Xenu|XGET|Zeus.*Webster|Zeus [NC]\n\nTaken from:\nhttp://perishablepress.com/press/2007/10/15/ultimate-htaccess-blacklist-2-compressed-version/\n', '\nYou can also check referrals. No referral could raise bot suspition. Bad referral means certainly it is not browser.\n\nAdding invisible links (possibly marked as rel=""nofollow""?),\n\n* style=""display: none;"" on link or parent container\n* placed underneath another element with higher z-index\n\nI would\'nt do that. You can end up blacklisted by google for black hat SEO :)\n', ""\nI currently work for a company that scans web sites in order to classify them. We also check sites for malware.\nIn my experience the number one blockers of our web crawler (which of course uses a IE or Firefox UA and does not obey robots.txt. Duh.) are sites intentionally hosting malware.  It's a pain because the site then falls back to a human who has to manually load the site, classify it and check it for malware.\nI'm just saying, by blocking web crawlers you're putting yourself in some bad company.\nOf course, if they are horribly rude and suck up tons of your bandwidth it's a different story because then you've got a good reason.\n"", ""\nPeople keep addressing broad crawlers but not crawlers that are specialized for your website.\nI write stealth crawlers and if they are individually built no amount of honey pots or hidden links will have any effect whatsoever - the only real way to detect specialised crawlers is by inspecting connection patterns. \nThe best systems use AI (e.g. Linkedin) use AI to address this.\nThe easiest solution is write log parsers that analyze IP connections and simply blacklist those IPs or serve captcha, at least temporary. \ne.g.\nif IP X is seen every 2 seconds connecting to foo.com/cars/*.html but not any other pages - it's most likely a bot or a hungry power user.\nAlternatively there are various javascript challenges that act as protection (e.g. Cloudflare's anti-bot system), but those are easily solvable, you can write something custom and that might be enough deterrent to make it not worth the effort for the crawler.\nHowever you must ask a question are you willing to false-positive legit users and introduce inconvenience for them to prevent bot traffic. Protecting public data is an impossible paradox.\n"", ""\nshort answer: if a mid level programmer knows what he's doing you won't be able to detect a crawler without affecting the real user. Having your information publicly you won't be able to defend it against a crawler... it's like the 1st amendment right :)\n""]"
Click a Button in Scrapy,"
I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking).
I found out that Scrapy can handle forms (like logins) as shown here. But the problem is that there is no form to fill out, so it's not exactly what I need.
How can I simply click a button, which then shows the information I need?
Do I have to use an external library like mechanize or lxml?
",75k,"
            67
        ","[""\nScrapy cannot interpret javascript.\nIf you absolutely must interact with the javascript on the page, you want to be using Selenium.\nIf using Scrapy, the solution to the problem depends on what the button is doing.\nIf it's just showing content that was previously hidden, you can scrape the data without a problem, it doesn't matter that it wouldn't appear in the browser, the HTML is still there.\nIf it's fetching the content dynamically via AJAX when the button is pressed, the best thing to do is to view the HTTP request that goes out when you press the button using a tool like Firebug. You can then just request the data directly from that URL.\n\nDo I have to use an external library like mechanize or lxml?\n\nIf you want to interpret javascript, yes you need to use a different library, although neither of those two fit the bill. Neither of them know anything about javascript. Selenium is the way to go.\nIf you can give the URL of the page you're working on scraping I can take a look.\n"", '\nSelenium browser provide very nice solution. Here is an example (pip install -U selenium):\nfrom selenium import webdriver\n\nclass northshoreSpider(Spider):\n    name = \'xxx\'\n    allowed_domains = [\'www.example.org\']\n    start_urls = [\'https://www.example.org\']\n\n    def __init__(self):\n        self.driver = webdriver.Firefox()\n\n    def parse(self,response):\n            self.driver.get(\'https://www.example.org/abc\')\n\n            while True:\n                try:\n                    next = self.driver.find_element_by_xpath(\'//*[@id=""BTN_NEXT""]\')\n                    url = \'http://www.example.org/abcd\'\n                    yield Request(url,callback=self.parse2)\n                    next.click()\n                except:\n                    break\n\n            self.driver.close()\n\n    def parse2(self,response):\n        print \'you are here!\'\n\n', '\nTo properly and fully use JavaScript you need a full browser engine and this is possible only with Watir/WatiN/Selenium etc.\n', ""\nAlthough it's an old thread I've found quite useful to use Helium (built on top of Selenium) for this purpose and far more easier/simpler than using Selenium. It will be something like the following:\nfrom helium import *\n\nstart_firefox('your_url')\ns = S('path_to_your_button')\nclick(s)\n...\n\n\n""]"
How to give URL to scrapy for crawling?,"
I want to use scrapy for crawling web pages. Is there a way to pass the start URL from the terminal itself?
It is given in the documentation that either the name of the spider or the URL can be given, but when i given the url it throws an error:
//name of my spider is example, but i am giving url instead of my spider name(It works fine if i give spider name).

scrapy crawl example.com                 

ERROR:

File
  ""/usr/local/lib/python2.7/dist-packages/Scrapy-0.14.1-py2.7.egg/scrapy/spidermanager.py"",
  line 43, in create
      raise KeyError(""Spider not found: %s"" % spider_name) KeyError: 'Spider not found: example.com'

How can i make scrapy to use my spider on the url given in the terminal??
",24k,"
            35
        ","['\nI\'m  not really sure about the commandline option. However, you could write your spider like this.\nclass MySpider(BaseSpider):\n\n    name = \'my_spider\'    \n\n    def __init__(self, *args, **kwargs): \n      super(MySpider, self).__init__(*args, **kwargs) \n\n      self.start_urls = [kwargs.get(\'start_url\')] \n\nAnd start it like:\nscrapy crawl my_spider -a start_url=""http://some_url""\n', '\nAn even easier way to allow multiple url-arguments than what Peter suggested is by giving them as a string with the urls separated by a comma, like this:\n-a start_urls=""http://example1.com,http://example2.com""\n\nIn the spider you would then simply split the string on \',\' and get an array of urls:\nself.start_urls = kwargs.get(\'start_urls\').split(\',\')\n\n', '\nUse scrapy parse command. You can parse a url with your spider. url is passed from command.\n$ scrapy parse http://www.example.com/ --spider=spider-name\n\nhttp://doc.scrapy.org/en/latest/topics/commands.html#parse\n', '\nSjaak Trekhaak has the right idea and here is how to allow multiples:\nclass MySpider(scrapy.Spider):\n    """"""\n    This spider will try to crawl whatever is passed in `start_urls` which\n    should be a comma-separated string of fully qualified URIs.\n\n    Example: start_urls=http://localhost,http://example.com\n    """"""\n    def __init__(self, name=None, **kwargs):\n        if \'start_urls\' in kwargs:\n            self.start_urls = kwargs.pop(\'start_urls\').split(\',\')\n        super(Spider, self).__init__(name, **kwargs)\n\n', ""\nThis is an extension to the approach given by Sjaak Trekhaak in this thread. The approach as it is so far only works if you provide exactly one url. For example, if you want to provide more than one url like this, for instance: \n-a start_url=http://url1.com,http://url2.com\n\nthen Scrapy (I'm using the current stable version 0.14.4) will terminate with the following exception:\nerror: running 'scrapy crawl' with more than one spider is no longer supported\n\nHowever, you can circumvent this problem by choosing a different variable for each start url, together with an argument that holds the number of passed urls. Something like this:\n-a start_url1=http://url1.com \n-a start_url2=http://url2.com \n-a urls_num=2\n\nYou can then do the following in your spider:\nclass MySpider(BaseSpider):\n\n    name = 'my_spider'    \n\n    def __init__(self, *args, **kwargs): \n        super(MySpider, self).__init__(*args, **kwargs) \n\n        urls_num = int(kwargs.get('urls_num'))\n\n        start_urls = []\n        for i in xrange(1, urls_num):\n            start_urls.append(kwargs.get('start_url{0}'.format(i)))\n\n        self.start_urls = start_urls\n\nThis is a somewhat ugly hack but it works. Of course, it's tedious to explicitly write down all command line arguments for each url. Therefore, it makes sense to wrap the scrapy crawl command in a Python subprocess and generate the command line arguments in a loop or something.\nHope it helps. :) \n"", '\nYou can also try this:\n>>> scrapy view http://www.sitename.com\n\nIt will open a window in browser of requested URL.\n']"
Spider a Website and Return URLs Only,"
I'm looking for a way to pseudo-spider a website. The key is that I don't actually want the content, but rather a simple list of URIs. I can get reasonably close to this idea with Wget using the --spider option, but when piping that output through a grep, I can't seem to find the right magic to make it work:
wget --spider --force-html -r -l1 http://somesite.com | grep 'Saving to:'

The grep filter seems to have absolutely no affect on the wget output. Have I got something wrong or is there another tool I should try that's more geared towards providing this kind of limited result set?
UPDATE
So I just found out offline that, by default, wget writes to stderr. I missed that in the man pages (in fact, I still haven't found it if it's in there). Once I piped the return to stdout, I got closer to what I need:
wget --spider --force-html -r -l1 http://somesite.com 2>&1 | grep 'Saving to:'

I'd still be interested in other/better means for doing this kind of thing, if any exist.
",88k,"
            71
        ","[""\nThe absolute last thing I want to do is download and parse all of the content myself (i.e. create my own spider). Once I learned that Wget writes to stderr by default, I was able to redirect it to stdout and filter the output appropriately.\nwget --spider --force-html -r -l2 $url 2>&1 \\\n  | grep '^--' | awk '{ print $3 }' \\\n  | grep -v '\\.\\(css\\|js\\|png\\|gif\\|jpg\\)$' \\\n  > urls.m3u\n\nThis gives me a list of the content resource (resources that aren't images, CSS or JS source files) URIs that are spidered. From there, I can send the URIs off to a third party tool for processing to meet my needs.\nThe output still needs to be streamlined slightly (it produces duplicates as it's shown above), but it's almost there and I haven't had to do any parsing myself.\n"", '\nCreate a few regular expressions to extract the addresses from all\n<a href=""(ADDRESS_IS_HERE)"">.\n\nHere is the solution I would use:\nwget -q http://example.com -O - | \\\n    tr ""\\t\\r\\n\'"" \'   ""\' | \\\n    grep -i -o \'<a[^>]\\+href[ ]*=[ \\t]*""\\(ht\\|f\\)tps\\?:[^""]\\+""\' | \\\n    sed -e \'s/^.*""\\([^""]\\+\\)"".*$/\\1/g\'\n\nThis will output all http, https, ftp, and ftps links from a webpage.  It will not give you relative urls, only full urls.\nExplanation regarding the options used in the series of piped commands:\nwget -q makes it not have excessive output (quiet mode).\nwget -O - makes it so that the downloaded file is echoed to stdout, rather than saved to disk.\ntr is the unix character translator, used in this example to translate newlines and tabs to spaces, as well as convert single quotes into double quotes so we can simplify our regular expressions.\ngrep -i makes the search case-insensitive\ngrep -o makes it output only the matching portions.\nsed is the Stream EDitor unix utility which allows for filtering and transformation operations.\nsed -e just lets you feed it an expression.\nRunning this little script on ""http://craigslist.org"" yielded quite a long list of links:\nhttp://blog.craigslist.org/\nhttp://24hoursoncraigslist.com/subs/nowplaying.html\nhttp://craigslistfoundation.org/\nhttp://atlanta.craigslist.org/\nhttp://austin.craigslist.org/\nhttp://boston.craigslist.org/\nhttp://chicago.craigslist.org/\nhttp://cleveland.craigslist.org/\n...\n\n', '\nI\'ve used a tool called xidel \nxidel http://server -e \'//a/@href\' | \ngrep -v ""http"" | \nsort -u | \nxargs -L1 -I {}  xidel http://server/{} -e \'//a/@href\' | \ngrep -v ""http"" | sort -u\n\nA little hackish but gets you closer! This is only the first level. Imagine packing this up into a self recursive script!\n']"
Selenium wait for Ajax content to load - universal approach,"
Is there a universal approach for Selenium to wait till all ajax content has loaded? (not tied to a specific website - so it works for every ajax website)
",32k,"
            23
        ","['\nYou need to wait for Javascript and jQuery to finish loading. Execute Javascript to check if jQuery.active is 0 and document.readyState is complete, which means the JS and jQuery load is complete.\npublic boolean waitForJSandJQueryToLoad() {\n\n    WebDriverWait wait = new WebDriverWait(driver, 30);\n\n    // wait for jQuery to load\n    ExpectedCondition<Boolean> jQueryLoad = new ExpectedCondition<Boolean>() {\n      @Override\n      public Boolean apply(WebDriver driver) {\n        try {\n          return ((Long)((JavascriptExecutor)getDriver()).executeScript(""return jQuery.active"") == 0);\n        }\n        catch (Exception e) {\n          // no jQuery present\n          return true;\n        }\n      }\n    };\n\n    // wait for Javascript to load\n    ExpectedCondition<Boolean> jsLoad = new ExpectedCondition<Boolean>() {\n      @Override\n      public Boolean apply(WebDriver driver) {\n        return ((JavascriptExecutor)getDriver()).executeScript(""return document.readyState"")\n        .toString().equals(""complete"");\n      }\n    };\n\n  return wait.until(jQueryLoad) && wait.until(jsLoad);\n}\n\n', '\nAs Mark Collin described in his book ""Mastering Selenium Webdriver"", use JavascriptExecutor let you figure out whether a website using jQuery has finished making AJAX calls\npublic class AdditionalConditions {\n\n  public static ExpectedCondition<Boolean> jQueryAJAXCallsHaveCompleted() {\n    return new ExpectedCondition<Boolean>() {\n\n        @Override\n        public Boolean apply(WebDriver driver) {\n            return (Boolean) ((JavascriptExecutor) driver).executeScript(""return (window.jQuery != null) && (jQuery.active === 0);"");\n        }\n    };\n  }\n}\n\n', '\nI have been using this simple do while to iterate until an AJAX is finished. \nIt consistently works for me. \npublic void waitForAjax() throws InterruptedException{\n    while (true)\n    {\n        Boolean ajaxIsComplete = (Boolean) ((JavascriptExecutor)driver).executeScript(""return jQuery.active == 0"");\n        if (ajaxIsComplete){\n            info(""Ajax Call completed. "");\n            break;\n        }\n        Thread.sleep(150);\n    }\n}\n\n', ""\nI don't believe that there is a universal approach out of the box. I typically make a method that does a .waituntilrowcount(2) or waituntilvisible() that polls an element.\n""]"
Scrapy CrawlSpider doesn't crawl the first landing page,"
I am new to Scrapy and I am working on a scraping exercise and I am using the CrawlSpider.
Although the Scrapy framework works beautifully and it follows the relevant links, I can't seem to make the CrawlSpider to scrape the very first link (the home page / landing page). Instead it goes directly to scrape the links determined by the rule but doesn't scrape the landing page on which the links are. I don't know how to fix this since it is not recommended to overwrite the parse method for a CrawlSpider. Modifying follow=True/False also doesn't yield any good results. Here is the snippet of code:
class DownloadSpider(CrawlSpider):
    name = 'downloader'
    allowed_domains = ['bnt-chemicals.de']
    start_urls = [
        ""http://www.bnt-chemicals.de""        
        ]
    rules = (   
        Rule(SgmlLinkExtractor(aloow='prod'), callback='parse_item', follow=True),
        )
    fname = 1

    def parse_item(self, response):
        open(str(self.fname)+ '.txt', 'a').write(response.url)
        open(str(self.fname)+ '.txt', 'a').write(','+ str(response.meta['depth']))
        open(str(self.fname)+ '.txt', 'a').write('\n')
        open(str(self.fname)+ '.txt', 'a').write(response.body)
        open(str(self.fname)+ '.txt', 'a').write('\n')
        self.fname = self.fname + 1

",7k,"
            18
        ","['\nJust change your callback to parse_start_url and override it:\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n\nclass DownloadSpider(CrawlSpider):\n    name = \'downloader\'\n    allowed_domains = [\'bnt-chemicals.de\']\n    start_urls = [\n        ""http://www.bnt-chemicals.de"",\n    ]\n    rules = (\n        Rule(SgmlLinkExtractor(allow=\'prod\'), callback=\'parse_start_url\', follow=True),\n    )\n    fname = 0\n\n    def parse_start_url(self, response):\n        self.fname += 1\n        fname = \'%s.txt\' % self.fname\n\n        with open(fname, \'w\') as f:\n            f.write(\'%s, %s\\n\' % (response.url, response.meta.get(\'depth\', 0)))\n            f.write(\'%s\\n\' % response.body)\n\n', '\nThere\'s a number of ways of doing this, but one of the simplest is to implement parse_start_url and then modify start_urls\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\n\nclass DownloadSpider(CrawlSpider):\n    name = \'downloader\'\n    allowed_domains = [\'bnt-chemicals.de\']\n    start_urls = [""http://www.bnt-chemicals.de/tunnel/index.htm""]\n    rules = (\n        Rule(SgmlLinkExtractor(allow=\'prod\'), callback=\'parse_item\', follow=True),\n        )\n    fname = 1\n\n    def parse_start_url(self, response):\n        return self.parse_item(response)\n\n\n    def parse_item(self, response):\n        open(str(self.fname)+ \'.txt\', \'a\').write(response.url)\n        open(str(self.fname)+ \'.txt\', \'a\').write(\',\'+ str(response.meta[\'depth\']))\n        open(str(self.fname)+ \'.txt\', \'a\').write(\'\\n\')\n        open(str(self.fname)+ \'.txt\', \'a\').write(response.body)\n        open(str(self.fname)+ \'.txt\', \'a\').write(\'\\n\')\n        self.fname = self.fname + 1\n\n']"
How to find all links / pages on a website,"
Is it possible to find all the pages and links on ANY given website? I'd like to enter a URL and produce a directory tree of all links from that site?
I've looked at HTTrack but that downloads the whole site and I simply need the directory tree.
",531k,"
            125
        ","['\nCheck out linkchecker—it will crawl the site (while obeying robots.txt) and generate a report. From there, you can script up a solution for creating the directory tree.\n', ""\nIf you have the developer console (JavaScript) in your browser, you can type this code in:\nurls = document.querySelectorAll('a'); for (url in urls) console.log(urls[url].href);\n\nShortened:\nn=$$('a');for(u in n)console.log(n[u].href)\n\n"", '\nAnother alternative might be\nArray.from(document.querySelectorAll(""a"")).map(x => x.href)\n\nWith your $$( its even shorter\nArray.from($$(""a"")).map(x => x.href)\n\n', '\nIf this is a programming question, then I would suggest you write your own regular expression to parse all the retrieved contents. Target tags are IMG and A for standard HTML. For JAVA, \nfinal String openingTags = ""(<a [^>]*href=[\'\\""]?|<img[^> ]* src=[\'\\""]?)"";\n\nthis along with Pattern and Matcher classes should detect the beginning of the tags. Add LINK tag if you also want CSS.\nHowever, it is not as easy as you may have intially thought. Many web pages are not well-formed. Extracting all the links programmatically that human being can ""recognize"" is really difficult if you need to take into account all the irregular expressions.\nGood luck!\n', '\nfunction getalllinks($url) {\n    $links = array();\n    if ($fp = fopen($url, \'r\')) {\n        $content = \'\';\n        while ($line = fread($fp, 1024)) {\n            $content. = $line;\n        }\n    }\n    $textLen = strlen($content);\n    if ($textLen > 10) {\n        $startPos = 0;\n        $valid = true;\n        while ($valid) {\n            $spos = strpos($content, \'<a \', $startPos);\n            if ($spos < $startPos) $valid = false;\n            $spos = strpos($content, \'href\', $spos);\n            $spos = strpos($content, \'""\', $spos) + 1;\n            $epos = strpos($content, \'""\', $spos);\n            $startPos = $epos;\n            $link = substr($content, $spos, $epos - $spos);\n            if (strpos($link, \'http://\') !== false) $links[] = $link;\n        }\n    }\n    return $links;\n}\n\ntry this code....\n']"
how to filter duplicate requests based on url in scrapy,"
I am writing a crawler for a website using scrapy with CrawlSpider.
Scrapy provides an in-built duplicate-request filter which filters duplicate requests based on urls. Also, I can filter requests using rules member of CrawlSpider. 
What I want to do is to filter requests like:
http:://www.abc.com/p/xyz.html?id=1234&refer=5678

If I have already visited
http:://www.abc.com/p/xyz.html?id=1234&refer=4567


NOTE: refer is a parameter that doesn't affect the response I get, so I don't care if the value of that parameter changes.

Now, if I have a set which accumulates all ids I could ignore it in my callback function parse_item (that's my callback function) to achieve this functionality.
But that would mean I am still at least fetching that page, when I don't need to.
So what is the way in which I can tell scrapy that it shouldn't send a particular request based on the url?
",19k,"
            42
        ","['\nYou can write custom middleware for duplicate removal and add it in settings\nimport os\n\nfrom scrapy.dupefilter import RFPDupeFilter\n\nclass CustomFilter(RFPDupeFilter):\n""""""A dupe filter that considers specific ids in the url""""""\n\n    def __getid(self, url):\n        mm = url.split(""&refer"")[0] #or something like that\n        return mm\n\n    def request_seen(self, request):\n        fp = self.__getid(request.url)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + os.linesep)\n\nThen you need to set the correct DUPFILTER_CLASS in settings.py\nDUPEFILTER_CLASS = \'scraper.duplicate_filter.CustomFilter\'\n\nIt should work after that\n', '\nFollowing ytomar\'s lead, I wrote this filter that filters based purely on URLs that have already been seen by checking an in-memory set.  I\'m a Python noob so let me know if I screwed something up, but it seems to work all right:\nfrom scrapy.dupefilter import RFPDupeFilter\n\nclass SeenURLFilter(RFPDupeFilter):\n    """"""A dupe filter that considers the URL""""""\n\n    def __init__(self, path=None):\n        self.urls_seen = set()\n        RFPDupeFilter.__init__(self, path)\n\n    def request_seen(self, request):\n        if request.url in self.urls_seen:\n            return True\n        else:\n            self.urls_seen.add(request.url)\n\nAs ytomar mentioned, be sure to add the DUPEFILTER_CLASS constant to settings.py:\nDUPEFILTER_CLASS = \'scraper.custom_filters.SeenURLFilter\'\n\n', '\nhttps://github.com/scrapinghub/scrapylib/blob/master/scrapylib/deltafetch.py\nThis file might help you. This file creates a database of unique delta fetch key from the url ,a user pass in a scrapy.Reqeust(meta={\'deltafetch_key\':uniqe_url_key}).\nThis this let you avoid duplicate requests you already have visited in the past.\nA sample mongodb implementation using deltafetch.py \n        if isinstance(r, Request):\n            key = self._get_key(r)\n            key = key+spider.name\n\n            if self.db[\'your_collection_to_store_deltafetch_key\'].find_one({""_id"":key}):\n                spider.log(""Ignoring already visited: %s"" % r, level=log.INFO)\n                continue\n        elif isinstance(r, BaseItem):\n\n            key = self._get_key(response.request)\n            key = key+spider.name\n            try:\n                self.db[\'your_collection_to_store_deltafetch_key\'].insert({""_id"":key,""time"":datetime.now()})\n            except:\n                spider.log(""Ignoring already visited: %s"" % key, level=log.ERROR)\n        yield r\n\neg. id = 345\nscrapy.Request(url,meta={deltafetch_key:345},callback=parse)\n', ""\nHere is my custom filter base on scrapy 0.24.6.\nIn this filter, it only cares id in the url. for example\nhttp://www.example.com/products/cat1/1000.html?p=1\nhttp://www.example.com/products/cat2/1000.html?p=2\nare treated as same url. But\nhttp://www.example.com/products/cat2/all.html\nwill not.\nimport re\nimport os\nfrom scrapy.dupefilter import RFPDupeFilter\n\n\nclass MyCustomURLFilter(RFPDupeFilter):\n\n    def _get_id(self, url):\n        m = re.search(r'(\\d+)\\.html', url)\n        return None if m is None else m.group(1)\n\n    def request_fingerprint(self, request):\n        style_id = self._get_id(request.url)\n        return style_id\n\n"", ""\nIn the latest scrapy, we can use the default duplication filter or extend and have custom one.\ndefine the below config in spider settings\nDUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'\n""]"
crawl site that has infinite scrolling using python,"
I have been doing research and so far I found out the python package that I will plan on using its scrapy, now I am trying to find out what is a good way to build a scraper using scrapy to crawl site with infinite scrolling. After digging around I found out that there is a package call selenium and it has python module. I have a feeling someone has already done that using Scrapy and Selenium to scrape site with infinite scrolling. It would be great if someone can point towards to an example. 
",28k,"
            10
        ","['\nYou can use selenium to scrap the infinite scrolling website like twitter or facebook. \nStep 1 : Install Selenium using pip \npip install selenium \n\nStep 2 : use the code below to automate infinite scroll and extract the source code\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import NoAlertPresentException\nimport sys\n\nimport unittest, time, re\n\nclass Sel(unittest.TestCase):\n    def setUp(self):\n        self.driver = webdriver.Firefox()\n        self.driver.implicitly_wait(30)\n        self.base_url = ""https://twitter.com""\n        self.verificationErrors = []\n        self.accept_next_alert = True\n    def test_sel(self):\n        driver = self.driver\n        delay = 3\n        driver.get(self.base_url + ""/search?q=stackoverflow&src=typd"")\n        driver.find_element_by_link_text(""All"").click()\n        for i in range(1,100):\n            self.driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")\n            time.sleep(4)\n        html_source = driver.page_source\n        data = html_source.encode(\'utf-8\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n\nThe for loop allows you to parse through the infinite scrolls and post which you can extract the loaded data.\nStep 3 : Print the data if required.\n', '\nThis is short & simple code which is working for me:\nSCROLL_PAUSE_TIME = 20\n\n# Get scroll height\nlast_height = driver.execute_script(""return document.body.scrollHeight"")\n\nwhile True:\n    # Scroll down to bottom\n    driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")\n\n    # Wait to load page\n    time.sleep(SCROLL_PAUSE_TIME)\n\n    # Calculate new scroll height and compare with last scroll height\n    new_height = driver.execute_script(""return document.body.scrollHeight"")\n    if new_height == last_height:\n        break\n    last_height = new_height\n\nposts = driver.find_elements_by_class_name(""post-text"")\n\nfor block in posts:\n    print(block.text)\n\n']"
find a word on a website and get its page link,"
I want to scrape a few websites and see if the word ""katalog"" is present there. If yes, I want to retrieve the link of all the tabs/sub pages where that word is present. Is it possible to do so?
I tried following this tutorial but the wordlist.csv I get at the end is empty even though the word catalog does exist on the website.
https://www.phooky.com/blog/find-specific-words-on-web-pages-with-scrapy/
        wordlist = [
            ""katalog"",
            ""downloads"",
            ""download""
            ]

def find_all_substrings(string, sub):
    starts = [match.start() for match in re.finditer(re.escape(sub), string)]
    return starts

class WebsiteSpider(CrawlSpider):

    name = ""webcrawler""
    allowed_domains = [""www.reichelt.com/""]
    start_urls = [""https://www.reichelt.com/""]
    rules = [Rule(LinkExtractor(), follow=True, callback=""check_buzzwords"")]

    crawl_count = 0
    words_found = 0                                 

    def check_buzzwords(self, response):

        self.__class__.crawl_count += 1

        crawl_count = self.__class__.crawl_count

        url = response.url
        contenttype = response.headers.get(""content-type"", """").decode('utf-8').lower()
        data = response.body.decode('utf-8')

        for word in wordlist:
                substrings = find_all_substrings(data, word)
                print(""substrings"", substrings)
                for pos in substrings:
                        ok = False
                        if not ok:
                                self.__class__.words_found += 1
                                print(word + "";"" + url + "";"")
        return Item()

    def _requests_to_follow(self, response):
        if getattr(response, ""encoding"", None) != None:
                return CrawlSpider._requests_to_follow(self, response)
        else:
                return []

How can I find all instances of a word on a website and obtain the link of the page where the word is founded?
",2k,"
            2
        ","['\nMain problem is wrong allowed_domain - it has to be without path /\n    allowed_domains = [""www.reichelt.com""]\n\nOther problems can be this tutorial is 3 years old (there is link to documentation for Scarpy 1.5 but newest version is 2.5.0).\nIt also uses some useless lines of code.\nIt gets contenttype but never use it to decode request.body. Your url  uses iso8859-1 for original language and utf-8 for ?LANGUAGE=PL - but you can simply use request.text and it will automatically decode it.\nIt also uses ok = False and later check it but it is totally useless.\n\nMinimal working code - you can copy it to single file and run as python script.py without creating project.\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\nimport re\n\nwordlist = [\n    ""katalog"",\n    ""catalog"",\n    ""downloads"",\n    ""download"",\n]\n\ndef find_all_substrings(string, sub):\n    return [match.start() for match in re.finditer(re.escape(sub), string)]\n\nclass WebsiteSpider(CrawlSpider):\n\n    name = ""webcrawler""\n    \n    allowed_domains = [""www.reichelt.com""]\n    start_urls = [""https://www.reichelt.com/""]\n    #start_urls = [""https://www.reichelt.com/?LANGUAGE=PL""]\n    \n    rules = [Rule(LinkExtractor(), follow=True, callback=""check_buzzwords"")]\n\n    #crawl_count = 0\n    #words_found = 0                                 \n\n    def check_buzzwords(self, response):\n        print(\'[check_buzzwords] url:\', response.url)\n        \n        #self.crawl_count += 1\n\n        #content_type = response.headers.get(""content-type"", """").decode(\'utf-8\').lower()\n        #print(\'content_type:\', content_type)\n        #data = response.body.decode(\'utf-8\')\n        \n        data = response.text\n\n        for word in wordlist:\n            print(\'[check_buzzwords] check word:\', word)\n            substrings = find_all_substrings(data, word)\n            print(\'[check_buzzwords] substrings:\', substrings)\n            \n            for pos in substrings:\n                #self.words_found += 1\n                # only display\n                print(\'[check_buzzwords] word: {} | pos: {} | sub: {} | url: {}\'.format(word, pos, data[pos-20:pos+20], response.url))\n                # send to file\n                yield {\'word\': word, \'pos\': pos, \'sub\': data[pos-20:pos+20], \'url\': response.url}\n\n# --- run without project and save in `output.csv` ---\n\nfrom scrapy.crawler import CrawlerProcess\n\nc = CrawlerProcess({\n    \'USER_AGENT\': \'Mozilla/5.0\',\n    # save in file CSV, JSON or XML\n    \'FEEDS\': {\'output.csv\': {\'format\': \'csv\'}},  # new in 2.1\n})\nc.crawl(WebsiteSpider)\nc.start() \n\n\nEDIT:\nI added data[pos-20:pos+20] to yielded data to see where is substring and sometimes it is in URL like .../elements/adw_2018/catalog/... or other place like <img alt=""""catalog"""" - so using regex doesn\'t have to be good idea. Maybe better is to use xpath or css selector to search text only in some places or in links.\n\nEDIT:\nVersion which search links with words from list. It uses response.xpath to search all linsk and later it check if there is word in href - so it doesn\'t need regex.\nProblem can be that it treats link with -downloads- (with s) as link with word download and downloads so it would need more complex method to check (ie. using regex) to treats it only as link with word downloads\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\n\nwordlist = [\n    ""katalog"",\n    ""catalog"",\n    ""downloads"",\n    ""download"",\n]\n\nclass WebsiteSpider(CrawlSpider):\n\n    name = ""webcrawler""\n    \n    allowed_domains = [""www.reichelt.com""]\n    start_urls = [""https://www.reichelt.com/""]\n    \n    rules = [Rule(LinkExtractor(), follow=True, callback=""check_buzzwords"")]\n\n    def check_buzzwords(self, response):\n        print(\'[check_buzzwords] url:\', response.url)\n        \n        links = response.xpath(\'//a[@href]\')\n        \n        for word in wordlist:\n            \n            for link in links:\n                url = link.attrib.get(\'href\')\n                if word in url:\n                    print(\'[check_buzzwords] word: {} | url: {} | page: {}\'.format(word, url, response.url))\n                    # send to file\n                    yield {\'word\': word, \'url\': url, \'page\': response.url}\n\n# --- run without project and save in `output.csv` ---\n\nfrom scrapy.crawler import CrawlerProcess\n\nc = CrawlerProcess({\n    \'USER_AGENT\': \'Mozilla/5.0\',\n    # save in file CSV, JSON or XML\n    \'FEEDS\': {\'output.csv\': {\'format\': \'csv\'}},  # new in 2.1\n})\nc.crawl(WebsiteSpider)\nc.start() \n\n', '\nYou can do it with requests-html and rendering the page:\nfrom requests_html import HTMLSession\n\nsession = HTMLSession()\nurl = ""https://www.reichelt.com/""\n\nr = session.get(url)\nr.html.render(sleep=2)\n\nif ""your_word"" in r.html.text: #or r.html.html if you want it in raw html\n    print([link for link in r.html.absolute_links if ""your_word"" in link])\n\n']"
Nodejs: Async request with a list of URL,"
I am working on a crawler. I have a list of URL need to be requested. There are several hundreds of request at the same time if I don't set it to be async. I am afraid that it would explode my bandwidth or produce to much network access to the target website. What should I do?
Here is what I am doing: 
urlList.forEach((url, index) => {

    console.log('Fetching ' + url);
    request(url, function(error, response, body) {
        //do sth for body

    });
});

I want one request is called after one request is completed.
",960,"
            0
        ","['\nYou can use something like Promise library e.g. snippet\nconst Promise = require(""bluebird"");\nconst axios = require(""axios"");\n\n//Axios wrapper for error handling\nconst axios_wrapper = (options) => {\n    return axios(...options)\n        .then((r) => {\n            return Promise.resolve({\n                data: r.data,\n                error: null,\n            });\n        })\n        .catch((e) => {\n            return Promise.resolve({\n                data: null,\n                error: e.response ? e.response.data : e,\n            });\n        });\n};\n\nPromise.map(\n    urls,\n    (k) => {\n        return axios_wrapper({\n            method: ""GET"",\n            url: k,\n        });\n    },\n    { concurrency: 1 } // Here 1 represents how many requests you want to run in parallel\n)\n    .then((r) => {\n        console.log(r);\n        //Here r will be an array of objects like {data: [{}], error: null}, where if the request was successfull it will have data value present otherwise error value will be non-null\n    })\n    .catch((e) => {\n        console.error(e);\n    });\n\n', ""\nThe things you need to watch for are:\n\nWhether the target site has rate limiting and you may be blocked from access if you try to request too much too fast?\nHow many simultaneous requests the target site can handle without degrading its performance?\nHow much bandwidth your server has on its end of things?\nHow many simultaneous requests your own server can have in flight and process without causing excess memory usage or a pegged CPU.\n\nIn general, the scheme for managing all this is to create a way to tune how many requests you launch.  There are many different ways to control this by number of simultaneous requests, number of requests per second, amount of data used, etc...\nThe simplest way to start would be to just control how many simultaneous requests you make.  That can be done like this:\nfunction runRequests(arrayOfData, maxInFlight, fn) {\n    return new Promise((resolve, reject) => {\n        let index = 0;\n        let inFlight = 0;\n\n        function next() {\n            while (inFlight < maxInFlight && index < arrayOfData.length) {\n                ++inFlight;\n                fn(arrayOfData[index++]).then(result => {\n                    --inFlight;\n                    next();\n                }).catch(err => {\n                    --inFlight;\n                    console.log(err);\n                    // purposely eat the error and let the rest of the processing continue\n                    // if you want to stop further processing, you can call reject() here\n                    next();\n                });\n            }\n            if (inFlight === 0) {\n                // all done\n                resolve();\n            }\n        }\n        next();\n    });\n}\n\nAnd, then you would use that like this:\nconst rp = require('request-promise');\n\n// run the whole urlList, no more than 10 at a time\nrunRequests(urlList, 10, function(url) {\n    return rp(url).then(function(data) {\n        // process fetched data here for one url\n    }).catch(function(err) {\n        console.log(url, err);\n    });\n}).then(function() {\n    // all requests done here\n});\n\nThis can be made as sophisticated as you want by adding a time element to it (no more than N requests per second) or even a bandwidth element to it.\n\nI want one request is called after one request is completed.\n\nThat's a very slow way to do things.  If you really want that, then you can just pass a 1 for the maxInFlight parameter to the above function, but typically, things would work a lot faster and not cause problems by allowing somewhere between 5 and 50 simultaneous requests.  Only testing would tell you where the sweet spot is for your particular target sites and your particular server infrastructure and amount of processing you need to do on the results.\n"", '\nyou can use set timeout function to process all request within loop. for that you must know maximum time to process a request.\n']"
How can I use different pipelines for different spiders in a single Scrapy project,"
I have a scrapy project which contains multiple spiders.
Is there any way I can define which pipelines to use for which spider? Not all the pipelines i have defined are applicable for every spider.
Thanks
",32k,"
            97
        ","[""\nJust remove all pipelines from main settings and use this inside spider.\nThis will define the pipeline to user per spider\nclass testSpider(InitSpider):\n    name = 'test'\n    custom_settings = {\n        'ITEM_PIPELINES': {\n            'app.MyPipeline': 400\n        }\n    }\n\n"", ""\nBuilding on the solution from Pablo Hoffman, you can use the following decorator on the process_item method of a Pipeline object so that it checks the pipeline attribute of your spider for whether or not it should be executed. For example:\ndef check_spider_pipeline(process_item_method):\n\n    @functools.wraps(process_item_method)\n    def wrapper(self, item, spider):\n\n        # message template for debugging\n        msg = '%%s %s pipeline step' % (self.__class__.__name__,)\n\n        # if class is in the spider's pipeline, then use the\n        # process_item method normally.\n        if self.__class__ in spider.pipeline:\n            spider.log(msg % 'executing', level=log.DEBUG)\n            return process_item_method(self, item, spider)\n\n        # otherwise, just return the untouched item (skip this step in\n        # the pipeline)\n        else:\n            spider.log(msg % 'skipping', level=log.DEBUG)\n            return item\n\n    return wrapper\n\nFor this decorator to work correctly, the spider must have a pipeline attribute with a container of the Pipeline objects that you want to use to process the item, for example:\nclass MySpider(BaseSpider):\n\n    pipeline = set([\n        pipelines.Save,\n        pipelines.Validate,\n    ])\n\n    def parse(self, response):\n        # insert scrapy goodness here\n        return item\n\nAnd then in a pipelines.py file:\nclass Save(object):\n\n    @check_spider_pipeline\n    def process_item(self, item, spider):\n        # do saving here\n        return item\n\nclass Validate(object):\n\n    @check_spider_pipeline\n    def process_item(self, item, spider):\n        # do validating here\n        return item\n\nAll Pipeline objects should still be defined in ITEM_PIPELINES in settings (in the correct order -- would be nice to change so that the order could be specified on the Spider, too).\n"", '\nThe other solutions given here are good, but I think they could be slow, because we are not really not using the pipeline per spider, instead we are checking if a pipeline exists every time an item is returned (and in some cases this could reach millions).\nA good way to completely disable (or enable) a feature per spider is using custom_setting and from_crawler for all extensions like this:\npipelines.py\nfrom scrapy.exceptions import NotConfigured\n\nclass SomePipeline(object):\n    def __init__(self):\n        pass\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool(\'SOMEPIPELINE_ENABLED\'):\n            # if this isn\'t specified in settings, the pipeline will be completely disabled\n            raise NotConfigured\n        return cls()\n\n    def process_item(self, item, spider):\n        # change my item\n        return item\n\nsettings.py\nITEM_PIPELINES = {\n   \'myproject.pipelines.SomePipeline\': 300,\n}\nSOMEPIPELINE_ENABLED = True # you could have the pipeline enabled by default\n\nspider1.py\nclass Spider1(Spider):\n\n    name = \'spider1\'\n\n    start_urls = [""http://example.com""]\n\n    custom_settings = {\n        \'SOMEPIPELINE_ENABLED\': False\n    }\n\nAs you check, we have specified custom_settings that will override the things specified in settings.py, and we are disabling SOMEPIPELINE_ENABLED for this spider.\nNow when you run this spider, check for something like:\n[scrapy] INFO: Enabled item pipelines: []\n\nNow scrapy has completely disabled the pipeline, not bothering of its existence for the whole run. Check that this also works for scrapy extensions and middlewares.\n', ""\nYou can use the name attribute of the spider in your pipeline\nclass CustomPipeline(object)\n\n    def process_item(self, item, spider)\n         if spider.name == 'spider1':\n             # do something\n             return item\n         return item\n\nDefining all pipelines this way can accomplish what you want.\n"", ""\nI can think of at least four approaches:\n\nUse a different scrapy project per set of spiders+pipelines (might be appropriate if your spiders are different enough warrant being in different projects)\nOn the scrapy tool command line, change the pipeline setting with scrapy settings in between each invocation of your spider\nIsolate your spiders into their own scrapy tool commands, and define the default_settings['ITEM_PIPELINES'] on your command class to the pipeline list you want for that command. See line 6 of this example.\nIn the pipeline classes themselves, have process_item() check what spider it's running against, and do nothing if it should be ignored for that spider. See the example using resources per spider to get you started. (This seems like an ugly solution because it tightly couples spiders and item pipelines. You probably shouldn't use this one.)\n\n"", ""\nThe most simple and effective solution is to set custom settings in each spider itself.\ncustom_settings = {'ITEM_PIPELINES': {'project_name.pipelines.SecondPipeline': 300}}\n\nAfter that you need to set them in the settings.py file\nITEM_PIPELINES = {\n   'project_name.pipelines.FistPipeline': 300,\n   'project_name.pipelines.SecondPipeline': 400\n}\n\nin that way each spider will use the respective pipeline.\n"", '\nYou can just set the item pipelines settings inside of the spider like this:\nclass CustomSpider(Spider):\n    name = \'custom_spider\'\n    custom_settings = {\n        \'ITEM_PIPELINES\': {\n            \'__main__.PagePipeline\': 400,\n            \'__main__.ProductPipeline\': 300,\n        },\n        \'CONCURRENT_REQUESTS_PER_DOMAIN\': 2\n    }\n\nI can then split up a pipeline (or even use multiple pipelines) by adding a value to the loader/returned item that identifies which part of the spider sent items over. This way I won’t get any KeyError exceptions and I know which items should be available. \n    ...\n    def scrape_stuff(self, response):\n        pageloader = PageLoader(\n                PageItem(), response=response)\n\n        pageloader.add_xpath(\'entire_page\', \'/html//text()\')\n        pageloader.add_value(\'item_type\', \'page\')\n        yield pageloader.load_item()\n\n        productloader = ProductLoader(\n                ProductItem(), response=response)\n\n        productloader.add_xpath(\'product_name\', \'//span[contains(text(), ""Example"")]\')\n        productloader.add_value(\'item_type\', \'product\')\n        yield productloader.load_item()\n\nclass PagePipeline:\n    def process_item(self, item, spider):\n        if item[\'item_type\'] == \'product\':\n            # do product stuff\n\n        if item[\'item_type\'] == \'page\':\n            # do page stuff\n\n', ""\nI am using two pipelines, one for image download (MyImagesPipeline) and second for save data in mongodb (MongoPipeline).\nsuppose we have many spiders(spider1,spider2,...........),in my example spider1 and spider5 can not use MyImagesPipeline\nsettings.py\nITEM_PIPELINES = {'scrapycrawler.pipelines.MyImagesPipeline' : 1,'scrapycrawler.pipelines.MongoPipeline' : 2}\nIMAGES_STORE = '/var/www/scrapycrawler/dowload'\n\nAnd bellow complete code of pipeline\nimport scrapy\nimport string\nimport pymongo\nfrom scrapy.pipelines.images import ImagesPipeline\n\nclass MyImagesPipeline(ImagesPipeline):\n    def process_item(self, item, spider):\n        if spider.name not in ['spider1', 'spider5']:\n            return super(ImagesPipeline, self).process_item(item, spider)\n        else:\n           return item \n\n    def file_path(self, request, response=None, info=None):\n        image_name = string.split(request.url, '/')[-1]\n        dir1 = image_name[0]\n        dir2 = image_name[1]\n        return dir1 + '/' + dir2 + '/' +image_name\n\nclass MongoPipeline(object):\n\n    collection_name = 'scrapy_items'\n    collection_url='snapdeal_urls'\n\n    def __init__(self, mongo_uri, mongo_db):\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_uri=crawler.settings.get('MONGO_URI'),\n            mongo_db=crawler.settings.get('MONGO_DATABASE', 'scraping')\n        )\n\n    def open_spider(self, spider):\n        self.client = pymongo.MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n\n    def close_spider(self, spider):\n        self.client.close()\n\n    def process_item(self, item, spider):\n        #self.db[self.collection_name].insert(dict(item))\n        collection_name=item.get( 'collection_name', self.collection_name )\n        self.db[collection_name].insert(dict(item))\n        data = {}\n        data['base_id'] = item['base_id']\n        self.db[self.collection_url].update({\n            'base_id': item['base_id']\n        }, {\n            '$set': {\n            'image_download': 1\n            }\n        }, upsert=False, multi=True)\n        return item\n\n"", '\nwe can use some conditions in pipeline as this\n    # -*- coding: utf-8 -*-\nfrom scrapy_app.items import x\n\nclass SaveItemPipeline(object):\n    def process_item(self, item, spider):\n        if isinstance(item, x,):\n            item.save()\n        return item\n\n', ""\nSimple but still useful solution.\nSpider code\n    def parse(self, response):\n        item = {}\n        ... do parse stuff\n        item['info'] = {'spider': 'Spider2'}\n\npipeline code\n    def process_item(self, item, spider):\n        if item['info']['spider'] == 'Spider1':\n            logging.error('Spider1 pipeline works')\n        elif item['info']['spider'] == 'Spider2':\n            logging.error('Spider2 pipeline works')\n        elif item['info']['spider'] == 'Spider3':\n            logging.error('Spider3 pipeline works')\n\nHope this save some time for somebody!\n"", ""\nOverriding 'ITEM_PIPELINES' with custom settings per spider, as others have suggested, works well. However, I found I had a few distinct groups of pipelines I wanted to use for different categories of spiders. I wanted to be able to easily define the pipeline for a particular category of spider without a lot of thought, and I wanted to be able to update a pipeline category without editing each spider in that category individually.\nSo I created a new file called pipeline_definitions.py in the same directory as settings.py. pipeline_definitions.py contains functions like this:\ndef episode_pipelines():\n    return {\n        'radio_scrape.pipelines.SaveEpisode': 100,\n    }\n\ndef show_pipelines():\n    return {\n        'radio_scrape.pipelines.SaveShow': 100,\n    }\n\nThen in each spider I would import the specific function relevant for the spider:\nfrom radio_scrape.pipeline_definitions import episode_pipelines\n\nI then use that function in the custom settings assignment:\nclass RadioStationAEspisodesSpider(scrapy.Spider):\n    name = 'radio_station_A_episodes'        \n    custom_settings = {\n        'ITEM_PIPELINES': episode_pipelines()\n    }\n\n""]"
getting Forbidden by robots.txt: scrapy,"
while crawling website like https://www.netflix.com, getting Forbidden by robots.txt: https://www.netflix.com/>
ERROR: No response downloaded for: https://www.netflix.com/
",53k,"
            69
        ","['\nIn the new version (scrapy 1.1) launched 2016-05-11 the crawl first downloads robots.txt before crawling. To change this behavior change in your settings.py with ROBOTSTXT_OBEY\nROBOTSTXT_OBEY = False\n\nHere are the release notes\n', ""\nNetflix's Terms of Use state:\n\nYou also agree not to circumvent, remove, alter, deactivate, degrade or thwart any of the content protections in the Netflix service; use any robot, spider, scraper or other automated means to access the Netflix service;\n\nThey have their robots.txt set up to block web scrapers. If you override the setting in settings.py to ROBOTSTXT_OBEY=False then you are violating their terms of use which can result in a law suit.\n"", '\nFirst thing you need to ensure is that you change your user agent in the request, otherwise default user agent will be blocked for sure.\n']"
Python: maximum recursion depth exceeded while calling a Python object,"
I've built a crawler that had to run on about 5M pages (by increasing the url ID) and then parses the pages which contain the info' I need.
after using an algorithm which run on the urls (200K) and saved the good and bad results I found that the I'm wasting a lot of time. I could see that there are a a few returning subtrahends which I can use to check the next valid url.
you can see the subtrahends quite fast (a little ex' of the few first ""good IDs"") -
510000011 # +8
510000029 # +18
510000037 # +8
510000045 # +8
510000052 # +7
510000060 # +8
510000078 # +18
510000086 # +8
510000094 # +8
510000102 # +8
510000110 # etc'
510000128
510000136
510000144
510000151
510000169
510000177
510000185
510000193
510000201

after crawling about 200K urls which gave me only 14K good results I knew I was wasting my time and need to optimize it, so I run some statistics and built a function that will check the urls while increasing the id with 8\18\17\8 (top returning subtrahends ) etc'.
this is the function - 
def checkNextID(ID):
    global numOfRuns, curRes, lastResult
    while ID < lastResult:
        try:
            numOfRuns += 1
            if numOfRuns % 10 == 0:
                time.sleep(3) # sleep every 10 iterations
            if isValid(ID + 8):
                parseHTML(curRes)
                checkNextID(ID + 8)
                return 0
            if isValid(ID + 18):
                parseHTML(curRes)
                checkNextID(ID + 18)
                return 0
            if isValid(ID + 7):
                parseHTML(curRes)
                checkNextID(ID + 7)
                return 0
            if isValid(ID + 17):
                parseHTML(curRes)
                checkNextID(ID + 17)
                return 0
            if isValid(ID+6):
                parseHTML(curRes)
                checkNextID(ID + 6)
                return 0
            if isValid(ID + 16):
                parseHTML(curRes)
                checkNextID(ID + 16)
                return 0
            else:
                checkNextID(ID + 1)
                return 0
        except Exception, e:
            print ""somethin went wrong: "" + str(e)

what is basically does is -checkNextID(ID) is getting the first id I know that contain the data minus 8 so the first iteration will match the first ""if isValid"" clause (isValid(ID + 8) will return True).
lastResult is a variable which saves the last known url id, so we'll run until numOfRuns is
isValid() is a function that gets an ID + one of the subtrahends and returns True if the url contains what I need and saves a soup object of the url to a global varibale named - 'curRes', it returns False if the url doesn't contain the data I need.
parseHTML is a function that gets the soup object (curRes), parses the data I need and then saves the data to a csv, then returns True.
if isValid() returns True, we'll call parseHTML() and then try to check the next ID+the subtrahends (by calling checkNextID(ID + subtrahends), if none of them will return what I'm looking for I'll increase it with 1 and check again until I'll find the next valid url.
you can see the rest of the code here
after running the code I got about 950~ good results and suddenly an exception had raised -

""somethin went wrong: maximum recursion depth exceeded while calling a
  Python object""

I could see on WireShark that the scipt stuck on id - 510009541 (I started my script with 510000003), the script tried getting the url with that ID a few times before I noticed the error and stopped it.
I was really exciting to see that I got the same results but 25x-40x times faster then my old script, with fewer HTTP requests, it's very precise, I have missed only 1 result for 1000 good results, which is find by me, it's impossible to rum 5M times, I had my old script running for 30 hours and got 14-15K results when my new script gave me 960~ results in 5-10 minutes.
I read about stack limitations, but there must be a solution for the algorithm I'm trying to implement in Python (I can't go back to my old ""algorithm"", it will never end).
Thanks!
",251k,"
            63
        ","[""\nPython don't have a great support for recursion because of it's lack of TRE (Tail Recursion Elimination).\nThis means that each call to your recursive function will create a function call stack and because there is a limit of stack depth (by default is 1000) that you can check out by sys.getrecursionlimit (of course you can change it using sys.setrecursionlimit but it's not recommended) your program will end up by crashing when it hits this limit.\nAs other answer has already give you a much nicer way for how to solve this in your case (which is to replace recursion by simple loop) there is another solution if you still want to use recursion which is to use one of the many recipes of implementing TRE in python like this one.\nN.B: My answer is meant to give you more insight on why you get the error, and I'm not advising you to use the TRE as i already explained because in your case a loop will be much better and easy to read.\n"", '\nYou can increase the capacity of the stack by the following :\nimport sys\nsys.setrecursionlimit(10000)\n\n', '\nthis turns the recursion in to a loop:\ndef checkNextID(ID):\n    global numOfRuns, curRes, lastResult\n    while ID < lastResult:\n        try:\n            numOfRuns += 1\n            if numOfRuns % 10 == 0:\n                time.sleep(3) # sleep every 10 iterations\n            if isValid(ID + 8):\n                parseHTML(curRes)\n                ID = ID + 8\n            elif isValid(ID + 18):\n                parseHTML(curRes)\n                ID = ID + 18\n            elif isValid(ID + 7):\n                parseHTML(curRes)\n                ID = ID + 7\n            elif isValid(ID + 17):\n                parseHTML(curRes)\n                ID = ID + 17\n            elif isValid(ID+6):\n                parseHTML(curRes)\n                ID = ID + 6\n            elif isValid(ID + 16):\n                parseHTML(curRes)\n                ID = ID + 16\n            else:\n                ID = ID + 1\n        except Exception, e:\n            print ""somethin went wrong: "" + str(e)\n\n', '\nYou can increase the recursion depth and thread stack size.\nimport sys, threading\nsys.setrecursionlimit(10**7) # max depth of recursion\nthreading.stack_size(2**27)  # new thread will get stack of such size\n\n', '\nInstead of doing recursion, the parts of the code with checkNextID(ID + 18) and similar could be replaced with ID+=18, and then if you remove all instances of return 0, then it should do the same thing but as a simple loop. You should then put a return 0 at the end and make your variables non-global.\n', ""\n\nuse try and except but don't print your error in except just run your function again in except statement\n\n""]"
Detecting honest web crawlers,"
I would like to detect (on the server side) which requests are from bots.  I don't care about malicious bots at this point, just the ones that are playing nice.  I've seen a few approaches that mostly involve matching the user agent string against keywords like 'bot'.  But that seems awkward, incomplete, and unmaintainable.  So does anyone have any more solid approaches?  If not, do you have any resources you use to keep up to date with all the friendly user agents?
If you're curious: I'm not trying to do anything against any search engine policy.  We have a section of the site where a user is randomly presented with one of several slightly different versions of a page.  However if a web crawler is detected, we'd always give them the same version so that the index is consistent.
Also I'm using Java, but I would imagine the approach would be similar for any server-side technology.
",20k,"
            45
        ","['\nYou said matching the user agent on ‘bot’ may be awkward, but we’ve found it to be a pretty good match. Our studies have shown that it will cover about 98% of the hits you receive. We also haven’t come across any false positive matches yet either. If you want to raise this up to 99.9% you can include a few other well-known matches such as ‘crawler’, ‘baiduspider’, ‘ia_archiver’, ‘curl’ etc. We’ve tested this on our production systems over millions of hits. \nHere are a few c# solutions for you:\n1) Simplest\nIs the fastest when processing a miss. i.e. traffic from a non-bot – a normal user.\nCatches 99+% of crawlers.\nbool iscrawler = Regex.IsMatch(Request.UserAgent, @""bot|crawler|baiduspider|80legs|ia_archiver|voyager|curl|wget|yahoo! slurp|mediapartners-google"", RegexOptions.IgnoreCase);\n\n2) Medium\nIs the fastest when processing a hit. i.e. traffic from a bot. Pretty fast for misses too.\nCatches close to 100% of crawlers.\nMatches ‘bot’, ‘crawler’, ‘spider’ upfront. \nYou can add to it any other known crawlers.\nList<string> Crawlers3 = new List<string>()\n{\n    ""bot"",""crawler"",""spider"",""80legs"",""baidu"",""yahoo! slurp"",""ia_archiver"",""mediapartners-google"",\n    ""lwp-trivial"",""nederland.zoek"",""ahoy"",""anthill"",""appie"",""arale"",""araneo"",""ariadne"",            \n    ""atn_worldwide"",""atomz"",""bjaaland"",""ukonline"",""calif"",""combine"",""cosmos"",""cusco"",\n    ""cyberspyder"",""digger"",""grabber"",""downloadexpress"",""ecollector"",""ebiness"",""esculapio"",\n    ""esther"",""felix ide"",""hamahakki"",""kit-fireball"",""fouineur"",""freecrawl"",""desertrealm"",\n    ""gcreep"",""golem"",""griffon"",""gromit"",""gulliver"",""gulper"",""whowhere"",""havindex"",""hotwired"",\n    ""htdig"",""ingrid"",""informant"",""inspectorwww"",""iron33"",""teoma"",""ask jeeves"",""jeeves"",\n    ""image.kapsi.net"",""kdd-explorer"",""label-grabber"",""larbin"",""linkidator"",""linkwalker"",\n    ""lockon"",""marvin"",""mattie"",""mediafox"",""merzscope"",""nec-meshexplorer"",""udmsearch"",""moget"",\n    ""motor"",""muncher"",""muninn"",""muscatferret"",""mwdsearch"",""sharp-info-agent"",""webmechanic"",\n    ""netscoop"",""newscan-online"",""objectssearch"",""orbsearch"",""packrat"",""pageboy"",""parasite"",\n    ""patric"",""pegasus"",""phpdig"",""piltdownman"",""pimptrain"",""plumtreewebaccessor"",""getterrobo-plus"",\n    ""raven"",""roadrunner"",""robbie"",""robocrawl"",""robofox"",""webbandit"",""scooter"",""search-au"",\n    ""searchprocess"",""senrigan"",""shagseeker"",""site valet"",""skymob"",""slurp"",""snooper"",""speedy"",\n    ""curl_image_client"",""suke"",""www.sygol.com"",""tach_bw"",""templeton"",""titin"",""topiclink"",""udmsearch"",\n    ""urlck"",""valkyrie libwww-perl"",""verticrawl"",""victoria"",""webscout"",""voyager"",""crawlpaper"",\n    ""webcatcher"",""t-h-u-n-d-e-r-s-t-o-n-e"",""webmoose"",""pagesinventory"",""webquest"",""webreaper"",\n    ""webwalker"",""winona"",""occam"",""robi"",""fdse"",""jobo"",""rhcs"",""gazz"",""dwcp"",""yeti"",""fido"",""wlm"",\n    ""wolp"",""wwwc"",""xget"",""legs"",""curl"",""webs"",""wget"",""sift"",""cmc""\n};\nstring ua = Request.UserAgent.ToLower();\nbool iscrawler = Crawlers3.Exists(x => ua.Contains(x));\n\n3) Paranoid\nIs pretty fast, but a little slower than options 1 and 2.\nIt’s the most accurate, and allows you to maintain the lists if you want.\nYou can maintain a separate list of names with ‘bot’ in them if you are afraid of false positives in future.\nIf we get a short match we log it and check it for a false positive.\n// crawlers that have \'bot\' in their useragent\nList<string> Crawlers1 = new List<string>()\n{\n    ""googlebot"",""bingbot"",""yandexbot"",""ahrefsbot"",""msnbot"",""linkedinbot"",""exabot"",""compspybot"",\n    ""yesupbot"",""paperlibot"",""tweetmemebot"",""semrushbot"",""gigabot"",""voilabot"",""adsbot-google"",\n    ""botlink"",""alkalinebot"",""araybot"",""undrip bot"",""borg-bot"",""boxseabot"",""yodaobot"",""admedia bot"",\n    ""ezooms.bot"",""confuzzledbot"",""coolbot"",""internet cruiser robot"",""yolinkbot"",""diibot"",""musobot"",\n    ""dragonbot"",""elfinbot"",""wikiobot"",""twitterbot"",""contextad bot"",""hambot"",""iajabot"",""news bot"",\n    ""irobot"",""socialradarbot"",""ko_yappo_robot"",""skimbot"",""psbot"",""rixbot"",""seznambot"",""careerbot"",\n    ""simbot"",""solbot"",""mail.ru_bot"",""spiderbot"",""blekkobot"",""bitlybot"",""techbot"",""void-bot"",\n    ""vwbot_k"",""diffbot"",""friendfeedbot"",""archive.org_bot"",""woriobot"",""crystalsemanticsbot"",""wepbot"",\n    ""spbot"",""tweetedtimes bot"",""mj12bot"",""who.is bot"",""psbot"",""robot"",""jbot"",""bbot"",""bot""\n};\n\n// crawlers that don\'t have \'bot\' in their useragent\nList<string> Crawlers2 = new List<string>()\n{\n    ""baiduspider"",""80legs"",""baidu"",""yahoo! slurp"",""ia_archiver"",""mediapartners-google"",""lwp-trivial"",\n    ""nederland.zoek"",""ahoy"",""anthill"",""appie"",""arale"",""araneo"",""ariadne"",""atn_worldwide"",""atomz"",\n    ""bjaaland"",""ukonline"",""bspider"",""calif"",""christcrawler"",""combine"",""cosmos"",""cusco"",""cyberspyder"",\n    ""cydralspider"",""digger"",""grabber"",""downloadexpress"",""ecollector"",""ebiness"",""esculapio"",""esther"",\n    ""fastcrawler"",""felix ide"",""hamahakki"",""kit-fireball"",""fouineur"",""freecrawl"",""desertrealm"",\n    ""gammaspider"",""gcreep"",""golem"",""griffon"",""gromit"",""gulliver"",""gulper"",""whowhere"",""portalbspider"",\n    ""havindex"",""hotwired"",""htdig"",""ingrid"",""informant"",""infospiders"",""inspectorwww"",""iron33"",\n    ""jcrawler"",""teoma"",""ask jeeves"",""jeeves"",""image.kapsi.net"",""kdd-explorer"",""label-grabber"",\n    ""larbin"",""linkidator"",""linkwalker"",""lockon"",""logo_gif_crawler"",""marvin"",""mattie"",""mediafox"",\n    ""merzscope"",""nec-meshexplorer"",""mindcrawler"",""udmsearch"",""moget"",""motor"",""muncher"",""muninn"",\n    ""muscatferret"",""mwdsearch"",""sharp-info-agent"",""webmechanic"",""netscoop"",""newscan-online"",\n    ""objectssearch"",""orbsearch"",""packrat"",""pageboy"",""parasite"",""patric"",""pegasus"",""perlcrawler"",\n    ""phpdig"",""piltdownman"",""pimptrain"",""pjspider"",""plumtreewebaccessor"",""getterrobo-plus"",""raven"",\n    ""roadrunner"",""robbie"",""robocrawl"",""robofox"",""webbandit"",""scooter"",""search-au"",""searchprocess"",\n    ""senrigan"",""shagseeker"",""site valet"",""skymob"",""slcrawler"",""slurp"",""snooper"",""speedy"",\n    ""spider_monkey"",""spiderline"",""curl_image_client"",""suke"",""www.sygol.com"",""tach_bw"",""templeton"",\n    ""titin"",""topiclink"",""udmsearch"",""urlck"",""valkyrie libwww-perl"",""verticrawl"",""victoria"",\n    ""webscout"",""voyager"",""crawlpaper"",""wapspider"",""webcatcher"",""t-h-u-n-d-e-r-s-t-o-n-e"",\n    ""webmoose"",""pagesinventory"",""webquest"",""webreaper"",""webspider"",""webwalker"",""winona"",""occam"",\n    ""robi"",""fdse"",""jobo"",""rhcs"",""gazz"",""dwcp"",""yeti"",""crawler"",""fido"",""wlm"",""wolp"",""wwwc"",""xget"",\n    ""legs"",""curl"",""webs"",""wget"",""sift"",""cmc""\n};\n\nstring ua = Request.UserAgent.ToLower();\nstring match = null;\n\nif (ua.Contains(""bot"")) match = Crawlers1.FirstOrDefault(x => ua.Contains(x));\nelse match = Crawlers2.FirstOrDefault(x => ua.Contains(x));\n\nif (match != null && match.Length < 5) Log(""Possible new crawler found: "", ua);\n\nbool iscrawler = match != null;\n\nNotes:\n\nIt’s tempting to just keep adding names to the regex option 1. But if you do this it will become slower. If you want a more complete list then linq with lambda is faster.\nMake sure .ToLower() is outside of your linq method – remember the method is a loop and you would be modifying the string during each iteration.\nAlways put the heaviest bots at the start of the list, so they match sooner.\nPut the lists into a static class so that they are not rebuilt on every pageview.\n\nHoneypots\nThe only real alternative to this is to create a ‘honeypot’ link on your site that only a bot will reach. You then log the user agent strings that hit the honeypot page to a database. You can then use those logged strings to classify crawlers.\nPostives: It will match some unknown crawlers that aren’t declaring themselves.\nNegatives: Not all crawlers dig deep enough to hit every link on your site, and so they may not reach your honeypot.\n', '\nYou can find a very thorough database of data on known ""good"" web crawlers in the robotstxt.org Robots Database.  Utilizing this data would be far more effective than just matching bot in the user-agent.\n', '\nOne suggestion is to create an empty anchor on your page that only a bot would follow.  Normal users wouldn\'t see the link, leaving spiders and bots to follow.  For example, an empty anchor tag that points to a subfolder would record a get request in your logs...\n<a href=""dontfollowme.aspx""></a>\n\nMany people use this method while running a HoneyPot to catch malicious bots that aren\'t following the robots.txt file.  I use the empty anchor method in an ASP.NET honeypot solution I wrote to trap and block those creepy crawlers...\n', '\nAny visitor whose entry page is /robots.txt is probably a bot.\n', '\nSomething quick and dirty like this might be a good start:\nreturn if request.user_agent =~ /googlebot|msnbot|baidu|curl|wget|Mediapartners-Google|slurp|ia_archiver|Gigabot|libwww-perl|lwp-trivial/i\n\nNote: rails code, but regex is generally applicable.\n', ""\nI'm pretty sure a large proportion of bots don't use robots.txt, however that was my first thought.\nIt seems to me that the best way to detect a bot is with time between requests, if the time between requests is consistently fast then its a bot.\n"", '\nvoid CheckBrowserCaps()\n    {\n        String labelText = """";\n        System.Web.HttpBrowserCapabilities myBrowserCaps = Request.Browser;\n        if (((System.Web.Configuration.HttpCapabilitiesBase)myBrowserCaps).Crawler)\n        {\n            labelText = ""Browser is a search engine."";\n        }\n        else\n        {\n            labelText = ""Browser is not a search engine."";\n        }\n\n        Label1.Text = labelText;\n    }\n\nHttpCapabilitiesBase.Crawler Property\n']"
How to programmatically fill input elements built with React?,"
I'm tasked with crawling website built with React. I'm trying to fill in input fields and submitting the form using javascript injects to the page (either selenium or webview in mobile). This works like a charm on every other site + technology but React seems to be a real pain.
so here is a sample code 
var email = document.getElementById( 'email' );
email.value = 'example@mail.com';

I the value changes on the DOM input element, but the React does not trigger the change event.
I've been trying plethora of different ways to get the React to update the state.
var event = new Event('change', { bubbles: true });
email.dispatchEvent( event );

no avail
var event = new Event('input', { bubbles: true });
email.dispatchEvent( event );

not working
email.onChange( event );

not working
I cannot believe interacting with React has been made so difficult. I would greatly appreciate any help. 
Thank you
",23k,"
            37
        ","[""\nThis accepted solution appears not to work in React > 15.6 (including React 16) as a result of changes to de-dupe input and change events.\nYou can see the React discussion here: https://github.com/facebook/react/issues/10135\nAnd the suggested workaround here:\nhttps://github.com/facebook/react/issues/10135#issuecomment-314441175\nReproduced here for convenience:\nInstead of\ninput.value = 'foo';\ninput.dispatchEvent(new Event('input', {bubbles: true}));\n\nYou would use\nfunction setNativeValue(element, value) {\n  const valueSetter = Object.getOwnPropertyDescriptor(element, 'value').set;\n  const prototype = Object.getPrototypeOf(element);\n  const prototypeValueSetter = Object.getOwnPropertyDescriptor(prototype, 'value').set;\n\n  if (valueSetter && valueSetter !== prototypeValueSetter) {\n    prototypeValueSetter.call(element, value);\n  } else {\n    valueSetter.call(element, value);\n  }\n}\n\nand then\nsetNativeValue(input, 'foo');\ninput.dispatchEvent(new Event('input', { bubbles: true }));\n\n"", '\nReact is listening for the input event of text fields.\nYou can change the value and manually trigger an input event, and react\'s onChange handler will trigger:\n\n\nclass Form extends React.Component {\r\n  constructor(props) {\r\n    super(props)\r\n    this.state = {value: \'\'}\r\n  }\r\n  \r\n  handleChange(e) {\r\n    this.setState({value: e.target.value})\r\n    console.log(\'State updated to \', e.target.value);\r\n  }\r\n  \r\n  render() {\r\n    return (\r\n      <div>\r\n        <input\r\n          id=\'textfield\'\r\n          value={this.state.value}\r\n          onChange={this.handleChange.bind(this)}\r\n        />\r\n        <p>{this.state.value}</p>\r\n      </div>      \r\n    )\r\n  }\r\n}\r\n\r\nReactDOM.render(\r\n  <Form />,\r\n  document.getElementById(\'app\')\r\n)\r\n\r\ndocument.getElementById(\'textfield\').value = \'foo\'\r\nconst event = new Event(\'input\', { bubbles: true })\r\ndocument.getElementById(\'textfield\').dispatchEvent(event)\n<script src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react.min.js""></script>\r\n<script src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react-dom.min.js""></script>\r\n\r\n<div id=\'app\'></div>\n\n\n', ""\nHere is the cleanest possible solution for inputs, selects, checkboxes, etc. (works not only for react inputs)\n/**\n * See [Modify React Component's State using jQuery/Plain Javascript from Chrome Extension](https://stackoverflow.com/q/41166005)\n * See https://github.com/facebook/react/issues/11488#issuecomment-347775628\n * See [How to programmatically fill input elements built with React?](https://stackoverflow.com/q/40894637)\n * See https://github.com/facebook/react/issues/10135#issuecomment-401496776\n *\n * @param {HTMLInputElement | HTMLSelectElement} el\n * @param {string} value\n */\nfunction setNativeValue(el, value) {\n  const previousValue = el.value;\n\n  if (el.type === 'checkbox' || el.type === 'radio') {\n    if ((!!value && !el.checked) || (!!!value && el.checked)) {\n      el.click();\n    }\n  } else el.value = value;\n\n  const tracker = el._valueTracker;\n  if (tracker) {\n    tracker.setValue(previousValue);\n  }\n\n  // 'change' instead of 'input', see https://github.com/facebook/react/issues/11488#issuecomment-381590324\n  el.dispatchEvent(new Event('change', { bubbles: true }));\n}\n\nUsage:\nsetNativeValue(document.getElementById('name'), 'Your name');\ndocument.getElementById('radio').click(); // or setNativeValue(document.getElementById('radio'), true)\ndocument.getElementById('checkbox').click(); // or setNativeValue(document.getElementById('checkbox'), true)\n\n"", '\nI noticed the input element had some property with a name along the lines of __reactEventHandlers$..., which had some functions including an onChange.\nThis worked for finding that function and triggering it\nlet getReactEventHandlers = (element) => {\n    // the name of the attribute changes, so we find it using a match.\n    // It\'s something like `element.__reactEventHandlers$...`\n    let reactEventHandlersName = Object.keys(element)\n       .filter(key => key.match(\'reactEventHandler\'));\n    return element[reactEventHandlersName];\n}\n\nlet triggerReactOnChangeEvent = (element) => {\n    let ev = new Event(\'change\');\n    // workaround to set the event target, because `ev.target = element` doesn\'t work\n    Object.defineProperty(ev, \'target\', {writable: false, value: element});\n    getReactEventHandlers(element).onChange(ev);\n}\n\ninput.value = ""some value"";\ntriggerReactOnChangeEvent(input);\n\n', '\nWithout element ids:\nexport default function SomeComponent() {\n    const inputRef = useRef();\n    const [address, setAddress] = useState("""");\n    const onAddressChange = (e) => {\n        setAddress(e.target.value);\n    }\n    const setAddressProgrammatically = (newValue) => {\n        const event = new Event(\'change\', { bubbles: true });\n        const input = inputRef.current;\n        if (input) {\n            setAddress(newValue);\n            input.value = newValue;\n            input.dispatchEvent(event);\n        }\n    }\n    return (\n        ...\n        <input ref={inputRef} type=""text"" value={address} onChange={onAddressChange}/>\n        ...\n    );\n}\n\n', '\nReact 17 works with fibers:\nfunction findReact(dom) {\n    let key = Object.keys(dom).find(key => key.startsWith(""__reactFiber$""));\n    let internalInstance = dom[key];\n    if (internalInstance == null) return ""internalInstance is null: "" + key;\n\n    if (internalInstance.return) { // react 16+\n        return internalInstance._debugOwner\n            ? internalInstance._debugOwner.stateNode\n           : internalInstance.return.stateNode;\n    } else { // react <16\n        return internalInstance._currentElement._owner._instance;\n   }\n}\n\nthen:\nfindReact(domElement).onChangeWrapper(""New value"");\n\nthe domElement in this is the tr with the data-param-name of the field you are trying to change:\nvar domElement = ?.querySelectorAll(\'tr[data-param-name=""<my field name>""]\')\n\n']"
How to identify web-crawler?,"
How can I filter out hits from webcrawlers etc. Hits which not is human..
I use maxmind.com to request the city from the IP.. It is not quite cheap if I have to pay for ALL hits including webcrawlers, robots etc.
",27k,"
            36
        ","['\nThere are two general ways to detect robots and I would call them ""Polite/Passive"" and ""Aggressive"". Basically, you have to give your web site a psychological disorder.\nPolite\nThese are ways to politely tell crawlers that they shouldn\'t crawl your site and to limit how often you are crawled. Politeness is ensured through robots.txt file in which you specify which bots, if any, should be allowed to crawl your website and how often your website can be crawled. This assumes that the robot you\'re dealing with is polite.\nAggressive\nAnother way to keep bots off your site is to get aggressive. \nUser Agent\nSome aggressive behavior includes (as previously mentioned by other users) the filtering of user-agent strings. This is probably the simplest, but also the least reliable way to detect if it\'s a user or not. A lot of bots tend to spoof user agents and some do it for legitimate reasons (i.e. they only want to crawl mobile content), while others simply don\'t want to be identified as bots. Even worse, some bots spoof legitimate/polite bot agents, such as the user agents of google, microsoft, lycos and other crawlers which are generally considered polite. Relying on the user agent can be helpful, but not by itself.\nThere are more aggressive ways to deal with robots that spoof user agents AND don\'t abide by your robots.txt file:\nBot Trap\nI like to think of this as a ""Venus Fly Trap,"" and it basically punishes any bot that wants to play tricks with you. \nA bot trap is probably the most effective way to find bots that don\'t adhere to your robots.txt file without actually impairing the usability of your website. Creating a bot trap ensures that only bots are captured and not real users. The basic way to do it is to setup a directory which you specifically mark as off limits in your robots.txt file, so any robot that is polite will not fall into the trap. The second thing you do is to place a ""hidden"" link from your website to the bot trap directory (this ensures that real users will never go there, since real users never click on invisible links). Finally, you ban any IP address that goes to the bot trap directory. \nHere are some instructions on how to achieve this:\nCreate a bot trap (or in your case: a PHP bot trap).\nNote: of course, some bots are smart enough to read your robots.txt file, see all the directories which you\'ve marked as ""off limits"" and STILL ignore your politeness settings (such as crawl rate and allowed bots). Those bots will probably not fall into your bot trap despite the fact that they are not polite.\nViolent\nI think this is actually too aggressive for the general audience (and general use), so if there are any kids under the age of 18, then please take them to another room!\nYou can make the bot trap ""violent"" by simply not specifying a robots.txt file. In this situation ANY BOT that crawls the hidden links will probably end up in the bot trap and you can ban all bots, period! \nThe reason this is not recommended is that you may actually want some bots to crawl your website (such as Google, Microsoft or other bots for site indexing). Allowing your website to be politely crawled by the bots from Google, Microsoft, Lycos, etc. will ensure that your site gets indexed and it shows up when people search for it on their favorite search engine.\nSelf Destructive\nYet another way to limits what bots can crawl on your website, is to serve CAPTCHAs or other challenges which a bot cannot solve. This comes at an expense of your users and I would think that anything which makes your website less usable (such as a CAPTCHA) is ""self destructive."" This, of course, will not actually block the bot from repeatedly trying to crawl your website, it will simply make your website very uninteresting to them. There are ways to ""get around"" the CAPTCHAs, but they\'re difficult to implement so I\'m not going to delve into this too much.\nConclusion\nFor your purposes, probably the best way to deal with bots is to employ a combination of the above mentioned strategies:\n\nFilter user agents.\nSetup a bot trap (the violent one).\n\nCatch all the bots that go into the violent bot trap and simply black-list their IPs (but don\'t block them). This way you will still get the ""benefits"" of being crawled by bots, but you will not have to pay to check the IP addresses that are black-listed due to going to your bot trap.\n', ""\nYou can check USER_AGENT, something like:\nfunction crawlerDetect($USER_AGENT)\n{\n    $crawlers = array(\n    array('Google', 'Google'),\n    array('msnbot', 'MSN'),\n    array('Rambler', 'Rambler'),\n    array('Yahoo', 'Yahoo'),\n    array('AbachoBOT', 'AbachoBOT'),\n    array('accoona', 'Accoona'),\n    array('AcoiRobot', 'AcoiRobot'),\n    array('ASPSeek', 'ASPSeek'),\n    array('CrocCrawler', 'CrocCrawler'),\n    array('Dumbot', 'Dumbot'),\n    array('FAST-WebCrawler', 'FAST-WebCrawler'),\n    array('GeonaBot', 'GeonaBot'),\n    array('Gigabot', 'Gigabot'),\n    array('Lycos', 'Lycos spider'),\n    array('MSRBOT', 'MSRBOT'),\n    array('Scooter', 'Altavista robot'),\n    array('AltaVista', 'Altavista robot'),\n    array('IDBot', 'ID-Search Bot'),\n    array('eStyle', 'eStyle Bot'),\n    array('Scrubby', 'Scrubby robot')\n    );\n\n    foreach ($crawlers as $c)\n    {\n        if (stristr($USER_AGENT, $c[0]))\n        {\n            return($c[1]);\n        }\n    }\n\n    return false;\n}\n\n// example\n\n$crawler = crawlerDetect($_SERVER['HTTP_USER_AGENT']);\n\n"", ""\nThe user agent ($_SERVER['HTTP_USER_AGENT']) often identifies whether the connecting agent is a browser or a robot. Review logs/analytics for the user agents of crawlers that visit your site. Filter accordingly.\nTake note that the user agent is a header supplied by the client application. As such it can be pretty much anything and shouldn't be trusted 100%. Plan accordingly.\n"", ""\nChecking the User-Agent will protect you from legitimate bots like Google and Yahoo.\nHowever, if you're also being hit with spam bots, then chances are User-Agent comparison won't protect you since those bots typically forge a common User-Agent string anyway.  In that instance, you would need to imploy more sophisticated measures.  If user input is required, a simple image verification scheme like ReCaptcha will work.\nIf you're looking to filter out all page hits from a bot, unfortunately, there's no 100% reliable way to do this if the bot is forging its credentials.  This is just an annoying fact of life on the internet that web admins have to put up with.\n"", ""\nI found this package, it's actively being developed and I'm quite liking it so far:\nhttps://github.com/JayBizzle/Crawler-Detect\nIt's simple as this:\nuse Jaybizzle\\CrawlerDetect\\CrawlerDetect;\n\n$CrawlerDetect = new CrawlerDetect;\n\n// Check the user agent of the current 'visitor'\nif($CrawlerDetect->isCrawler()) {\n    // true if crawler user agent detected\n}\n\n// Pass a user agent as a string\nif($CrawlerDetect->isCrawler('Mozilla/5.0 (compatible; Sosospider/2.0; +http://help.soso.com/webspider.htm)')) {\n    // true if crawler user agent detected\n}\n\n// Output the name of the bot that matched (if any)\necho $CrawlerDetect->getMatches();\n\n"", '\nuseragentstring.com is serving a lilst that you can use to analyze the userstring:\n$api_request=""http://www.useragentstring.com/?uas="".urlencode($_SERVER[\'HTTP_USER_AGENT\']).""&getJSON=all"";\n$ua=json_decode(file_get_contents($api_request), true);\nif($ua[""agent_type""]==""Crawler"") die();\n\n']"
Send Post Request in Scrapy,"
I am trying to crawl the latest reviews from google play store and to get that I need to make a post request.
With the Postman, it works and I get desired response.

but a post request in terminal gives me a server error
For ex: this page https://play.google.com/store/apps/details?id=com.supercell.boombeach
curl -H ""Content-Type: application/json"" -X POST -d '{""id"": ""com.supercell.boombeach"", ""reviewType"": '0', ""reviewSortOrder"": '0', ""pageNum"":'0'}' https://play.google.com/store/getreviews

gives a server error and
Scrapy just ignores this line:
frmdata = {""id"": ""com.supercell.boombeach"", ""reviewType"": 0, ""reviewSortOrder"": 0, ""pageNum"":0}
        url = ""https://play.google.com/store/getreviews""
        yield Request(url, callback=self.parse, method=""POST"", body=urllib.urlencode(frmdata))

",63k,"
            31
        ","[""\nThe answer above do not really solved the problem. They are sending the data as paramters instead of JSON data as the body of the request.\nFrom http://bajiecc.cc/questions/1135255/scrapy-formrequest-sending-json:\nmy_data = {'field1': 'value1', 'field2': 'value2'}\nrequest = scrapy.Request( url, method='POST', \n                          body=json.dumps(my_data), \n                          headers={'Content-Type':'application/json'} )\n\n"", '\nMake sure that each element in your formdata is of type string/unicode\nfrmdata = {""id"": ""com.supercell.boombeach"", ""reviewType"": \'0\', ""reviewSortOrder"": \'0\', ""pageNum"":\'0\'}\nurl = ""https://play.google.com/store/getreviews""\nyield FormRequest(url, callback=self.parse, formdata=frmdata)\n\nI think this will do\nIn [1]: from scrapy.http import FormRequest\n\nIn [2]: frmdata = {""id"": ""com.supercell.boombeach"", ""reviewType"": \'0\', ""reviewSortOrder"": \'0\', ""pageNum"":\'0\'}\n\nIn [3]: url = ""https://play.google.com/store/getreviews""\n\nIn [4]: r = FormRequest(url, formdata=frmdata)\n\nIn [5]: fetch(r)\n 2015-05-20 14:40:09+0530 [default] DEBUG: Crawled (200) <POST      https://play.google.com/store/getreviews> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7f3ea4258890>\n[s]   item       {}\n[s]   r          <POST https://play.google.com/store/getreviews>\n[s]   request    <POST https://play.google.com/store/getreviews>\n[s]   response   <200 https://play.google.com/store/getreviews>\n[s]   settings   <scrapy.settings.Settings object at 0x7f3eaa205450>\n[s]   spider     <Spider \'default\' at 0x7f3ea3449cd0>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\n', '\nSample Page Traversing using Post in Scrapy:\ndef directory_page(self,response):\n    if response:\n        profiles = response.xpath(""//div[@class=\'heading-h\']/h3/a/@href"").extract()\n        for profile in profiles:\n            yield Request(urljoin(response.url,profile),callback=self.profile_collector)\n\n        page = response.meta[\'page\'] + 1\n        if page :\n            yield FormRequest(\'https://rotmanconnect.com/AlumniDirectory/getmorerecentjoineduser\',\n                                        formdata={\'isSortByName\':\'false\',\'pageNumber\':str(page)},\n                                        callback= self.directory_page,\n                                        meta={\'page\':page})\n    else:\n         print ""No more page available""\n\n']"
how do web crawlers handle javascript,"
Today a lot of content on Internet is generated using JavaScript (specifically by background AJAX calls). I was wondering how web crawlers like Google handle them. Are they aware of JavaScript? Do they have a built-in JavaScript engine? Or do they simple ignore all JavaScript generated content in the page (I guess quite unlikely). Do people use specific techniques for getting their content indexed which would otherwise be available through background AJAX requests to a normal Internet user? 
",11k,"
            22
        ","[""\nJavaScript is handled by both Bing and Google crawlers. Yahoo uses the Bing crawler data, so it should be handled as well. I didn't look into other search engines, so if you care about them, you should look them up.\nBing published guidance in March 2014 as to how to create JavaScript-based websites that work with their crawler (mostly related to pushState) that are good practices in general:\n\nAvoid creating broken links with pushState\nAvoid creating two different links that link to the same content with pushState\nAvoid cloaking. (Here's an article Bing published about their cloaking detection in 2007)\nSupport browsers (and crawlers) that can't handle pushState.\n\nGoogle later published guidance in May 2014 as to how to create JavaScript-based websites that work with their crawler, and their recommendations are also recommended:\n\nDon't block the JavaScript (and CSS) in the robots.txt file.\nMake sure you can handle the load of the crawlers.\nIt's a good idea to support browsers and crawlers that can't handle (or users and organizations that won't allow) JavaScript\nTricky JavaScript that relies on arcane or specific features of the language might not work with the crawlers.\nIf your JavaScript removes content from the page, it might not get indexed.\naround.\n\n"", ""\nMost of them don't handle Javascript in any way. (At least, all the major search engines' crawlers don't.)\nThis is why it's still important to have your site gracefully handle navigation without Javascript.\n"", ""\nI have tested this by putting pages on my site only reachable by Javascript and then observing their presence in search indexes.\nPages on my site which were reachable only by Javascript were subsequently indexed by Google.\nThe content was reached through Javascript with a 'classic' technique or constructing a URL and setting the window.location accordingly.\n"", ""\nPrecisely what Ben S said. And anyone accessing your site with Lynx won't execute JavaScript either. If your site is intended for general public use, it should generally be usable without JavaScript.\nAlso, related: if there are pages that you would want a search engine to find, and which would normally arise only from JavaScript, you might consider generating static versions of them, reachable by a crawlable site map, where these static pages use JavaScript to load the current version when hit by a JavaScript-enabled browser (in case a human with a browser follows your site map). The search engine will see the static form of the page, and can index it.\n"", '\nCrawlers doesn\'t parse Javascript to find out what it does.\nThey may be built to recognise some classic snippets like  onchange=""window.location.href=this.options[this.selectedIndex].value;"" or onclick=""window.location.href=\'blah.html\';"", but they don\'t bother with things like content fetched using AJAX. At least not yet, and content fetched like that will always be secondary anyway.\nSo, Javascript should be used only for additional functionality. The main content taht you want the crawlers to find should still be plain text in the page and regular links that the crawlers easily can follow.\n', ""\ncrawlers can handle javascript or ajax calls if they are using some kind of frameworks like 'htmlunit' or 'selenium'\n""]"
HTTPWebResponse + StreamReader Very Slow,"
I'm trying to implement a limited web crawler in C# (for a few hundred sites only)
using HttpWebResponse.GetResponse() and Streamreader.ReadToEnd() , also tried using StreamReader.Read() and a loop to build my HTML string.
I'm only downloading pages which are about 5-10K. 
It's all very slow! For example, the average GetResponse() time is about half a second, while the average StreamREader.ReadToEnd() time is about 5 seconds!
All sites should be very fast, as they are very close to my location, and have fast servers. (in Explorer takes practically nothing to D/L) and I am not using any proxy.
My Crawler has about 20 threads reading simultaneously from the same site. Could this be causing a problem?
How do I reduce StreamReader.ReadToEnd times DRASTICALLY?
",24k,"
            21
        ","['\nHttpWebRequest may be taking a while to detect your proxy settings. Try adding this to your application config:\n<system.net>\n  <defaultProxy enabled=""false"">\n    <proxy/>\n    <bypasslist/>\n    <module/>\n  </defaultProxy>\n</system.net>\n\nYou might also see a slight performance gain from buffering your reads to reduce the number of calls made to the underlying operating system socket:\nusing (BufferedStream buffer = new BufferedStream(stream))\n{\n  using (StreamReader reader = new StreamReader(buffer))\n  {\n    pageContent = reader.ReadToEnd();\n  }\n}\n\n', '\nWebClient\'s DownloadString is a simple wrapper for HttpWebRequest, could you try using that temporarily and see if the speed improves? If things get much faster, could you share your code so we can have a look at what may be wrong with it?\nEDIT:\nIt seems HttpWebRequest observes IE\'s \'max concurrent connections\' setting, are these URLs on the same domain? You could try increasing the connections limit to see if that helps? I found this article about the problem:\n\nBy default, you can\'t perform more\n  than 2-3 async HttpWebRequest (depends\n  on the OS). In order to override it\n  (the easiest way, IMHO) don\'t forget\n  to add this under \n  section in the application\'s config\n  file:\n\n<system.net>\n  <connectionManagement>\n     <add address=""*"" maxconnection=""65000"" />\n  </connectionManagement>\n</system.net>\n\n', ""\nI had the same problem, but when I sat the HttpWebRequest's Proxy parameter to null, it solved the problem.\nUriBuilder ub = new UriBuilder(url);\nHttpWebRequest request = (HttpWebRequest)WebRequest.Create( ub.Uri );\nrequest.Proxy = null;\nHttpWebResponse response = (HttpWebResponse)request.GetResponse();\n\n"", '\nHave you tried ServicePointManager.maxConnections?  I usually set it to 200 for things similar to this.\n', '\nI had problem the same problem but worst.\nresponse = (HttpWebResponse)webRequest.GetResponse(); in my code\ndelayed about 10 seconds before running more code and after this the download saturated my connection.\nkurt\'s answer defaultProxy enabled=""false"" \nsolved the problem. now the response is almost instantly and i can download any http file at my connections maximum speed :)\nsorry for bad english\n', '\nI found the Application Config method did not work, but the problem was still due to the proxy settings.  My simple request used to take up to 30 seconds, now it takes 1.\npublic string GetWebData()\n{\n            string DestAddr = ""http://mydestination.com"";\n            System.Net.WebClient myWebClient = new System.Net.WebClient();\n            WebProxy myProxy = new WebProxy();\n            myProxy.IsBypassed(new Uri(DestAddr));\n            myWebClient.Proxy = myProxy;\n            return myWebClient.DownloadString(DestAddr);\n}\n\n', ""\nThank you all for answers, they've helped me to dig in proper direction. I've faced with the same performance issue, though proposed solution to change application config file (as I understood that solution is for web applications) doesn't fit my needs, my solution is shown below:\nHttpWebRequest webRequest;\n\nwebRequest = (HttpWebRequest)System.Net.WebRequest.Create(fullUrl);\nwebRequest.Method = WebRequestMethods.Http.Post;\n\nif (useDefaultProxy)\n{\n    webRequest.Proxy = System.Net.WebRequest.DefaultWebProxy;\n    webRequest.Credentials = CredentialCache.DefaultCredentials;\n}\nelse\n{\n    System.Net.WebRequest.DefaultWebProxy = null;\n    webRequest.Proxy = System.Net.WebRequest.DefaultWebProxy;\n}\n\n"", ""\nWhy wouldn't multithreading solve this issue? Multithreading would minimize the network wait times, and since you'd be storing the contents of the buffer in system memory (RAM), there would be no IO bottleneck from dealing with a filesystem. Thus, your 82 pages that take 82 seconds to download and parse, should take like 15 seconds (assuming a 4x processor). Correct me if I'm missing something. \n____ DOWNLOAD THREAD_____*\nDownload Contents\nForm Stream\nRead Contents\n_________________________*\n"", '\nTry to add cookie(AspxAutoDetectCookieSupport=1) to your request like this\nrequest.CookieContainer = new CookieContainer();         \nrequest.CookieContainer.Add(new Cookie(""AspxAutoDetectCookieSupport"", ""1"") { Domain = target.Host });\n\n']"
Web crawler that can interpret JavaScript [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



I want to write a web crawler that can interpret JavaScript. Basically its a program in Java or PHP that takes a URL as input and outputs the DOM tree which is similar to the output in Firebug HTML window. The best example is Kayak.com where you can not see the resulting DOM displayed on the browser when you 'view source' but can save the resulting HTML though Firebug. 
How would I go about doing this? What tools exist that would help me?
",21k,"
            18
        ","['\nRuby\'s Capybara is an integration test library, but it can also be used to write stand-alone web-crawlers. Given that it uses backends like Selenium or headless WebKit, it interprets javascript out-of-the-box:\nrequire \'capybara/dsl\'\nrequire \'capybara-webkit\'\n\ninclude Capybara::DSL\nCapybara.current_driver = :webkit\nCapybara.app_host = ""http://www.google.com""\npage.visit(""/"")\nputs(page.html)\n\n', ""\nI've been using HtmlUnit (Java). This was originally designed for unit testing pages. It's not perfect javascript, but it hasn't failed me in my limited usage. According to the site, it can run the following JS frameworks to a reasonable degree:\n\njQuery 1.2.6\nMochiKit 1.4.1\nGWT 2.0.0\nSarissa 0.9.9.3\nMooTools 1.2.1\nPrototype 1.6.0\nExt JS 2.2\nDojo 1.0.2\nYUI 2.3.0\n\n"", ""\nYou are more likely to have success in Java than in PHP.  There is a pre-existing Javascript interpreter for Java called Rhino.  It's a reference implementation, and well-documented.\nRhino is used in lots of existing Java apps to provide Javascript scripting ability within the app.  I have also heard of it used to assist with performing automated tests in Javascript.\nI also know that Java includes code that can parse and render HTML, though someone who knows more about Java than me can probably advise more on that.  I am not denying it would be very difficult to achieve something like this; you'd essentially be re-implementing a lot of what a browser does.\n"", ""\nYou could use Mozilla's rendering engine Gecko:\nhttps://developer.mozilla.org/en/Gecko\n"", '\nGive a look here: http://snippets.scrapy.org/snippets/22/\nit\'s a python screen scraping and web crawling framework used with webdrivers that open a page, render all the things you need and gives you the possibilities to ""capture"" anything you want in the page via \n']"
Submit form with no submit button in rvest,"
I'm trying write a crawler to download some information, similar to this Stack Overflow post.  The answer is useful for creating the filled-in form, but I'm struggling to find a way to submit the form when a submit button is not part of the form.  Here is an example:
session <- html_session(""www.chase.com"")
form <- html_form(session)[[3]]

filledform <- set_values(form, `user_name` = user_name, `usr_password` = usr_password)
session <- submit_form(session, filledform)

At this point, I receive this error:
Error in names(submits)[[1]] : subscript out of bounds

How can I make this form submit?
",3k,"
            8
        ","['\nHere\'s a dirty hack that works for me: After studying the submit_form source code, I figured that I could work around the problem by injecting a fake submit button into my code version of the form, and then the submit_form function would call that. It works, except that it gives a warning that often lists an inappropriate input object (not in the example below, though). However, despite the warning, the code works for me:\nsession <- html_session(""www.chase.com"")\nform <- html_form(session)[[3]]\n\n# Form on home page has no submit button,\n# so inject a fake submit button or else rvest cannot submit it.\n# When I do this, rvest gives a warning ""Submitting with \'___\'"", where ""___"" is\n# often an irrelevant field item.\n# This warning might be an rvest (version 0.3.2) bug, but the code works.\nfake_submit_button <- list(name = NULL,\n                           type = ""submit"",\n                           value = NULL,\n                           checked = NULL,\n                           disabled = NULL,\n                           readonly = NULL,\n                           required = FALSE)\nattr(fake_submit_button, ""class"") <- ""input""\nform[[""fields""]][[""submit""]] <- fake_submit_button\n\nuser_name <- ""user""\nusr_password <- ""password""\n\nfilledform <- set_values(form, `user_name` = user_name, `usr_password` = usr_password)\nsession <- submit_form(session, filledform)\n\nThe successful result displays the following warning, which I simply ignore:\n> Submitting with \'submit\'\n\n']"
HtmlUnit Only Displays Host HTML Page for GWT App,"
I am using HtmlUnit API to add crawler support to my GWT app as follows:
PrintWriter out = null;
try {
    resp.setCharacterEncoding(CHAR_ENCODING);
    resp.setContentType(""text/html"");

    url = buildUrl(req);
    out = resp.getWriter();

    WebClient webClient = webClientProvider.get();

    // set options
    WebClientOptions options = webClient.getOptions();
    options.setCssEnabled(false);
    options.setThrowExceptionOnScriptError(false);
    options.setThrowExceptionOnFailingStatusCode(false);
    options.setRedirectEnabled(true);
    options.setJavaScriptEnabled(true);

    // set timeouts
    webClient.setJavaScriptTimeout(0);
    webClient.waitForBackgroundJavaScript(20000);

    // ajax controller
    webClient.setAjaxController(new NicelyResynchronizingAjaxController());

    // render page
    HtmlPage page = webClient.getPage(url);

    webClient.getJavaScriptEngine().pumpEventLoop(timeoutMillis);

    out.println(page.asXml());

    webClient.closeAllWindows();
}
...

However; only the bare HTML host page for my GWT app is produced and sent to the client.

UPDATE: Here is the output from Chrome DevTools:
Request URL:http://127.0.0.1:8888/MyApp.html?gwt.codesvr=127.0.0.1:9997&_escaped_fragment_=myobject%3Bid%3D507ac730e4b0e3b7a73b1b81
Request Method:GET
Status Code:200 OK
Request Headersview source
Accept:text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Charset:ISO-8859-1,utf-8;q=0.7,*;q=0.3
Accept-Encoding:gzip,deflate,sdch
Accept-Language:en-GB,en-US;q=0.8,en;q=0.6
Cache-Control:max-age=0
Connection:keep-alive
Cookie:__utma=96992031.428505342.1351707614.1351707614.1356355174.2; __utmb=96992031.1.10.1356355174; __utmc=96992031; __utmz=96992031.1351707614.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)
Host:127.0.0.1:8888
User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11
Query String Parametersview URL encoded
gwt.codesvr:127.0.0.1:9997
_escaped_fragment_:myobject;id=507ac730e4b0e3b7a73b1b81
Response Headersview source
Content-Type:text/html; charset=utf-8
Server:Jetty(6.1.x)
Transfer-Encoding:chunked

Why isn't the GWT code being executed?
",3k,"
            5
        ","['\nI had to try many variants before I finally got it to work.  One key is to leave enough time for the javascript to fully run.  But there were a few other subtleties I don\'t recall -- you can find below my filter version that seems to work for me, look at the parameters I set, some were keys to get this thing to work.  Other than the timer parameters that depend upon what the code to execute (and server ability to run it quickly too), it is pretty generic, so I don\'t understand why Google does not package such a function once and for all!\n/**\n * Special URL token that gets passed from the crawler to the servlet filter.\n * This token is used in case there are already existing query parameters.\n */\nprivate static final String ESCAPED_FRAGMENT_FORMAT1 = ""_escaped_fragment_="";\nprivate static final int ESCAPED_FRAGMENT_LENGTH1 = ESCAPED_FRAGMENT_FORMAT1.length();\n/**\n * Special URL token that gets passed from the crawler to the servlet filter.\n * This token is used in case there are not already existing query parameters.\n */\nprivate static final String ESCAPED_FRAGMENT_FORMAT2 = ""&""+ESCAPED_FRAGMENT_FORMAT1;\nprivate static final int ESCAPED_FRAGMENT_LENGTH2 = ESCAPED_FRAGMENT_FORMAT2.length();\n\nprivate class SyncAllAjaxController extends NicelyResynchronizingAjaxController\n{\n  private static final long serialVersionUID = 1L;\n  @Override\n  public boolean processSynchron(HtmlPage page, WebRequest request, boolean async)\n  {\n      return true;\n  }\n}\n\nprivate WebClient webClient = null;\n\nprivate static final long _pumpEventLoopTimeoutMillis = 200;\nprivate static final long _jsTimeoutMillis = 200;\nprivate static final long _pageWaitMillis = 100;\nfinal int _maxLoopChecks = 2;\n\npublic void destroy()\n{\n  if (webClient != null)\n    webClient.closeAllWindows();\n}\n\npublic void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain)\n    throws IOException, ServletException\n{\n  // Grab the request uri and query strings.\n  final HttpServletRequest httpRequest = (HttpServletRequest) request;\n  final String requestURI = httpRequest.getRequestURI();\n  final String queryString = httpRequest.getQueryString();\n  final HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n  if ((queryString != null) && (queryString.contains(ESCAPED_FRAGMENT_FORMAT1)))\n  {\n    // This is a Googlebot crawler request, let\'s return a static indexable html page\n    // post javascript execution, as rendered in the browser.\n\n    final String domain = httpRequest.getServerName();\n    final int port = httpRequest.getServerPort();\n\n    // Rewrite the URL back to the original #! version\n    //   -- basically remove _escaped_fragment_ from the query. \n    // Unescape any %XX characters as need be.\n    final String urlStringWithHashFragment = requestURI + rewriteQueryString(queryString);\n    final String protocol = httpRequest.getProtocol();\n    final URL urlWithHashFragment = new URL(protocol, domain, port, urlStringWithHashFragment);\n    final WebRequest webRequest = new WebRequest(urlWithHashFragment);\n\n    // Use the headless browser to obtain an HTML snapshot.\n    webClient = new WebClient(BrowserVersion.FIREFOX_3_6);\n    webClient.getCache().clear();\n    webClient.setJavaScriptEnabled(true);\n    webClient.setThrowExceptionOnScriptError(false);\n    webClient.setRedirectEnabled(false);\n    webClient.setAjaxController(new SyncAllAjaxController());\n    webClient.setCssErrorHandler(new SilentCssErrorHandler());\n\n    if (_logger.isInfoEnabled())\n      _logger.info(""HtmlUnit starting webClient.getPage(webRequest) where webRequest = "" + webRequest.toString());\n    final HtmlPage page = webClient.getPage(webRequest);\n\n    // Important!  Give the headless browser enough time to execute JavaScript\n    // The exact time to wait may depend on your application.\n\n    webClient.getJavaScriptEngine().pumpEventLoop(_pumpEventLoopTimeoutMillis);\n\n    int waitForBackgroundJavaScript = webClient.waitForBackgroundJavaScript(_jsTimeoutMillis);\n    int loopCount = 0;\n    while (waitForBackgroundJavaScript > 0 && loopCount < _maxLoopChecks)\n    {\n      ++loopCount;\n      waitForBackgroundJavaScript = webClient.waitForBackgroundJavaScript(_jsTimeoutMillis);\n      if (waitForBackgroundJavaScript == 0)\n      {\n        if (_logger.isTraceEnabled())\n          _logger.trace(""HtmlUnit exits background javascript at loop counter "" + loopCount);\n        break;\n      }\n      synchronized (page) \n      {\n        if (_logger.isTraceEnabled())\n            _logger.trace(""HtmlUnit waits for background javascript at loop counter "" + loopCount);\n        try\n        {\n          page.wait(_pageWaitMillis);\n        }\n        catch (InterruptedException e)\n        {\n          _logger.error(""HtmlUnit ERROR on page.wait at loop counter "" + loopCount);\n          e.printStackTrace();\n        }\n      }\n    }\n    webClient.getAjaxController().processSynchron(page, webRequest, false);\n    if (webClient.getJavaScriptEngine().isScriptRunning())\n    {\n      _logger.warn(""HtmlUnit webClient.getJavaScriptEngine().shutdownJavaScriptExecutor()"");\n      webClient.getJavaScriptEngine().shutdownJavaScriptExecutor();\n    }\n\n    // Return the static snapshot.\n    final String staticSnapshotHtml = page.asXml();\n    httpResponse.setContentType(""text/html;charset=UTF-8"");\n    final PrintWriter out = httpResponse.getWriter();\n    out.println(""<hr />"");\n    out.println(""<center><h3>Page non-interactive pour le crawler."");\n    out.println(""La page interactive est: <a href=\\""""\n        + urlWithHashFragment\n        + ""\\"">""\n        + urlWithHashFragment + ""</a></h3></center>"");\n    out.println(""<hr />"");\n    out.println(staticSnapshotHtml);\n    // Close web client.\n    webClient.closeAllWindows();\n    out.println("""");\n    out.flush();\n    out.close();\n    if (_logger.isInfoEnabled())\n      _logger.info(""HtmlUnit completed webClient.getPage(webRequest) where webRequest = "" + webRequest.toString());\n  }\n  else\n  {\n    if (requestURI.contains("".nocache.""))\n    {\n      // Ensure the gwt nocache bootstrapping file is never cached.\n      // References:\n      //   http://stackoverflow.com/questions/4274053/how-to-clear-cache-in-gwt\n      //   http://seewah.blogspot.com/2009/02/gwt-tips-2-nocachejs-getting-cached-in.html\n      // \n      final Date now = new Date();\n      httpResponse.setDateHeader(""Date"", now.getTime());\n      httpResponse.setDateHeader(""Expires"", now.getTime() - 86400000L); // One day old.\n      httpResponse.setHeader(""Pragma"", ""no-cache"");\n      httpResponse.setHeader(""Cache-control"", ""no-cache, no-store, must-revalidate"");\n    }\n\n    filterChain.doFilter(request, response);\n  }\n}\n\n/**\n * Maps from the query string that contains _escaped_fragment_ to one that\n * doesn\'t, but is instead followed by a hash fragment. It also unescapes any\n * characters that were escaped by the crawler. If the query string does not\n * contain _escaped_fragment_, it is not modified.\n * \n * @param queryString\n * @return A modified query string followed by a hash fragment if applicable.\n *         The non-modified query string otherwise.\n * @throws UnsupportedEncodingException\n */\nprivate static String rewriteQueryString(String queryString)\n    throws UnsupportedEncodingException\n{\n  // Seek the escaped fragment.\n  int index = queryString.indexOf(ESCAPED_FRAGMENT_FORMAT2);\n  int length = ESCAPED_FRAGMENT_LENGTH2;\n  if (index == -1)\n  {\n    index = queryString.indexOf(ESCAPED_FRAGMENT_FORMAT1);\n    length = ESCAPED_FRAGMENT_LENGTH1;\n  }\n  if (index != -1)\n  {\n    // Found the escaped fragment, so build back the original decoded one.\n    final StringBuilder queryStringSb = new StringBuilder();\n    // Add url parameters if any.\n    if (index > 0)\n    {\n      queryStringSb.append(""?"");\n      queryStringSb.append(queryString.substring(0, index));\n    }\n    // Add the hash fragment as a replacement for the escaped fragment.\n    queryStringSb.append(""#!"");\n    // Add the decoded token.\n    final String token2Decode = queryString.substring(index + length, queryString.length());\n    final String tokenDecoded = URLDecoder.decode(token2Decode, ""UTF-8"");\n    queryStringSb.append(tokenDecoded);\n    return queryStringSb.toString();\n  }\n  return queryString;\n}\n\n']"
selenium implicitly wait doesn't work,"
This is the first time I use selenium and headless browser as I want to crawl some web page using ajax tech.
The effect is great, but for some case it takes too much time to load the whole page(especially when some resource is unavailable),so I have to set a time out for the selenium.
First of all I tried set_page_load_timeout() and set_script_timeout(),but when I set these timeouts, I won't get any page source if the page doesn't load completely, as the codes below:
driver = webdriver.Chrome(chrome_options=options)
driver.set_page_load_timeout(5)
driver.set_script_timeout(5)
try:
    driver.get(url)
except Exception:
    driver.execute_script('window.stop()')

print driver.page_source.encode('utf-8')  # raise TimeoutException this line.

so I try to using Implicitly Wait and Conditional Wait, like this:
driver = webdriver.Firefox(firefox_options=options, executable_path=path)
print(""Firefox Headless Browser Invoked"")
wait = WebDriverWait(driver, timeout=10)
driver.implicitly_wait(2)
start = time.time()
driver.get(url)
end = time.time()
print 'time used: %s s' % str(end - start)
try:
    WebDriverWait(driver, 2, 0.5).until(expected.presence_of_element_located((By.TAG_NAME, 'body')))
    print driver.find_element_by_tag_name('body').text
except Exception:
    driver.execute_script('window.stop()')

This time I got the content that I want.However,it takes a very long time(40+ seconds),that means the timeout I set for 2 seconds doesn't work at all.
In my view, it seems like the driver.get() call ends until the browser stop loading the page, only after that the codes below can work, and you can not kill the get() call or you'll get nothing.
But this is very different from the selenium docs, I REALLY wonder where is the mistake.
environment: OSX 10.12, selenium 3.0.9 with FireFox & GoogleChrome Headless(both latest version.)
--- update ----
Thanks for help.I change the code as below, using WebDriverWait() alone, but there still exist cases that the call last for a very long time, far more than the timeout that I set.
Wonder if I can stop the page load immediately as the time is out?
driver = webdriver.Firefox(firefox_options=options, executable_path=path)
print(""Firefox Headless Browser Invoked"")
start = time.time()
driver.get('url')
end = time.time()
print 'time used: %s s' % str(end - start)
try:
    WebDriverWait(driver, 2, 0.5).until(expected.presence_of_element_located((By.TAG_NAME, 'body')))
    print driver.find_element_by_tag_name('body').text
except Exception:
    driver.execute_script('window.stop()')
driver.quit()

Here is a terminal output in test:
Firefox Headless Browser Invoked
time used: 44.6049938202 s

according to the code this means the driver.get() call takes 44 seconds to finish call, which is unexpected,I wonder if I misunderstood the behavior of the headless browsers?
",3k,"
            2
        ","['\nAs you mentioned in your question it takes too much time to load the whole page(especially when some resource is unavailable) is pretty much possible if the Application Under Test (AUT) uses JavaScript or AJAX Calls.\n\nIn your first scenario you have induced both set_page_load_timeout(5) and set_script_timeout(5)\n\nset_page_load_timeout(time_to_wait) : Sets the amount of time to wait for a page load to complete before throwing an exception.\nset_script_timeout(time_to_wait) : Sets the amount of time that the script should wait during an execute_async_script call before throwing an exception.\n\n\nHence the Application Under Test being dependent on JavaScript or AJAX Calls in presence of both the conditions raises TimeoutException.\n\nIn your second scenario you have induced both implicitly_wait(2) and WebDriverWait(driver, 2, 0.5).\n\nimplicitly_wait(time_to_wait) : Sets the timeout to implicitly wait for an element to be found or a command to complete.\nWebDriverWait(driver, timeout, poll_frequency=0.5, ignored_exceptions=None) : Sets the timeout in-conjunction with different expected_conditions\nBut you are experiancing a very long timeout(40+ seconds) as it is clearly mentioned in the docs Do not mix implicit and explicit waits which can cause unpredictable wait times\n\n\n\nWARNING : Do not mix implicit and explicit waits. Doing so can cause unpredictable wait times. For example setting an implicit wait of 10 seconds and an explicit wait of 15 seconds, could cause a timeout to occur after 20 seconds.\n\nSolution :\nThe best solution would be to remove all the instance of implicitly_wait(time_to_wait) and replace with WebDriverWait() for a stable behavior of the Application Under Test (AUT).\n\nUpdate\nAs per your counter question, the current code block looks perfect. The measurement of time which you are seeing as time used: 44.6049938202 s is the time required for the Web Page to load completely and functionally that is the time required for the Client (i.e. the Web Browser) to return back the control to the WebDriver instance once \'document.readyState\' equals to ""complete"" is achieved. Selenium or as an user you have no control on this rendering process. However for a better performance you may follow the best practices as follows :\n\nKeep your JDK version updated currently Java SE Development Kit 8u162\nKeep your Selenium Client version updated currently selenium 3.9.0\nKeep your WebDriver version updated.\nKeep your Web Browser version updated.\nClean you Project Workspace within your IDE regularly to build your project with required dependencies only.\nUse CCleaner tool to wipe away the OS chores before and after your Test Suite execution.\nIf your Web Browser base version is too old uninstall the Web Browser through Revo Uninstaller and install a recent GA released version of the Web Browser.\nExecute your Test.\n\n']"
How can I scrape tooltips value from a Tableau graph embedded in a webpage,"
I am trying to figure out if there is a way and how to scrape tooltip values from a Tableau embedded graph in a webpage using python.
Here is an example of a graph with tooltips when user hovers over the bars:
https://public.tableau.com/views/NumberofCOVID-19patientsadmittedordischarged/DASHPublicpage_patientsdischarges?:embed=y&:showVizHome=no&:host_url=https%3A%2F%2Fpublic.tableau.com%2F&:embed_code_version=3&:tabs=no&:toolbar=yes&:animate_transition=yes&:display_static_image=no&:display_spinner=no&:display_overlay=yes&:display_count=yes&publish=yes&:loadOrderID=1
I grabbed this url from the original webpage that I want to scrape from:
https://covid19.colorado.gov/hospital-data
Any help is appreciated.
",2k,"
            2
        ","['\nEdit\nI\'ve made a python library to scrape tableau dashboard. The implementation is more straightforward :\nfrom tableauscraper import TableauScraper as TS\n\nurl = ""https://public.tableau.com/views/Colorado_COVID19_Data/CO_Home""\n\nts = TS()\nts.loads(url)\ndashboard = ts.getDashboard()\n\nfor t in dashboard.worksheets:\n    #show worksheet name\n    print(f""WORKSHEET NAME : {t.name}"")\n    #show dataframe for this worksheet\n    print(t.data)\n\nrun this on repl.it\n\nOld answer\nThe graphic seems to be generated in JS from the result of an API which looks like :\nPOST https://public.tableau.com/TITLE/bootstrapSession/sessions/SESSION_ID \n\nThe SESSION_ID parameter is located (among other things) in tsConfigContainer textarea in the URL used to build the iframe.\nStarting from https://covid19.colorado.gov/hospital-data :\n\ncheck element with class tableauPlaceholder\nget the param element with attribute name\nit gives you the url : https://public.tableau.com/views/{urlPath}\nthe previous link gives you a textarea with id tsConfigContainer with a bunch of json values\nextract the session_id and root path (vizql_root)\nmake a POST on https://public.tableau.com/ROOT_PATH/bootstrapSession/sessions/SESSION_ID with the sheetId as form data\nextract the json from the result (result is not json)\n\nCode :\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport re\n\nr = requests.get(""https://covid19.colorado.gov/hospital-data"")\nsoup = BeautifulSoup(r.text, ""html.parser"")\n\n# get the second tableau link\ntableauContainer = soup.findAll(""div"", { ""class"": ""tableauPlaceholder""})[1]\nurlPath = tableauContainer.find(""param"", { ""name"": ""name""})[""value""]\n\nr = requests.get(\n    f""https://public.tableau.com/views/{urlPath}"",\n    params= {\n        "":showVizHome"":""no"",\n    }\n)\nsoup = BeautifulSoup(r.text, ""html.parser"")\n\ntableauData = json.loads(soup.find(""textarea"",{""id"": ""tsConfigContainer""}).text)\n\ndataUrl = f\'https://public.tableau.com{tableauData[""vizql_root""]}/bootstrapSession/sessions/{tableauData[""sessionid""]}\'\n\nr = requests.post(dataUrl, data= {\n    ""sheet_id"": tableauData[""sheetId""],\n})\n\ndataReg = re.search(\'\\d+;({.*})\\d+;({.*})\', r.text, re.MULTILINE)\ninfo = json.loads(dataReg.group(1))\ndata = json.loads(dataReg.group(2))\n\nprint(data[""secondaryInfo""][""presModelMap""][""dataDictionary""][""presModelHolder""][""genDataDictionaryPresModel""][""dataSegments""][""0""][""dataColumns""])\n\nFrom there you have all the data. You will need to look for the way the data is splitted as it seems all the data is dumped through a single list. Probably looking at the other fields in the JSON object would be useful for that.\n']"
Finding the layers and layer sizes for each Docker image,"
For research purposes I'm trying to crawl the public Docker registry ( https://registry.hub.docker.com/ ) and find out 1) how many layers an average image has and 2) the sizes of these layers to get an idea of the distribution.
However I studied the API and public libraries as well as the details on the github but I cant find any method to:

retrieve all the public repositories/images (even if those are thousands I still need a starting list to iterate through)
find all the layers of an image
find the size for a layer (so not an image but for the individual layer).

Can anyone help me find a way to retrieve this information?
Thank you!
EDIT: is anyone able to verify that searching for '*' in Docker registry is returning all the repositories and not just anything that mentions '*' anywhere? https://registry.hub.docker.com/search?q=*
",187k,"
            204
        ","['\nCheck out dive written in golang. \n\nAwesome tool!\n', ""\nYou can first find the image ID using:\n$ docker images -a\n\nThen find the image's layers and their sizes:\n$ docker history --no-trunc <Image ID>\n\nNote: I'm using Docker version 1.13.1\n$ docker -v\nDocker version 1.13.1, build 092cba3\n\n"", '\nYou can find the layers of the images in the folder /var/lib/docker/aufs/layers; provide if you configured for storage-driver as aufs (default option) \nExample:\n docker ps -a\n CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n 0ca502fa6aae        ubuntu              ""/bin/bash""         44 minutes ago      Exited (0) 44 seconds ago                       DockerTest\n\nNow to view the layers of the containers that were created with the image ""Ubuntu""; go to /var/lib/docker/aufs/layers directory and cat the file starts with the container ID (here it is 0ca502fa6aae*)\n root@viswesn-vm2:/var/lib/docker/aufs/layers# cat    0ca502fa6aaefc89f690736609b54b2f0fdebfe8452902ca383020e3b0d266f9-init \n d2a0ecffe6fa4ef3de9646a75cc629bbd9da7eead7f767cb810f9808d6b3ecb6\n 29460ac934423a55802fcad24856827050697b4a9f33550bd93c82762fb6db8f\n b670fb0c7ecd3d2c401fbfd1fa4d7a872fbada0a4b8c2516d0be18911c6b25d6\n 83e4dde6b9cfddf46b75a07ec8d65ad87a748b98cf27de7d5b3298c1f3455ae4\n\nThis will show the result of same by running \nroot@viswesn-vm2:/var/lib/docker/aufs/layers# docker history ubuntu\nIMAGE               CREATED             CREATED BY                                         SIZE                COMMENT\nd2a0ecffe6fa        13 days ago         /bin/sh -c #(nop) CMD [""/bin/bash""]             0 B                 \n29460ac93442        13 days ago         /bin/sh -c sed -i \'s/^#\\s*\\   (deb.*universe\\)$/   1.895 kB            \nb670fb0c7ecd        13 days ago         /bin/sh -c echo \'#!/bin/sh\' > /usr/sbin/polic   194.5 kB            \n83e4dde6b9cf        13 days ago         /bin/sh -c #(nop) ADD file:c8f078961a543cdefa   188.2 MB \n\nTo view the full layer ID; run with --no-trunc option as part of history command.\ndocker history --no-trunc ubuntu\n\n', '\nIn my opinion, docker history <image> is sufficient. This returns the size of each layer:\n$ docker history jenkinsci-jnlp-slave:2019-1-9c\nIMAGE        CREATED    CREATED BY                                    SIZE  COMMENT\n93f48953d298 42 min ago /bin/sh -c #(nop)  USER jenkins               0B\n6305b07d4650 42 min ago /bin/sh -c chown jenkins:jenkins -R /home/je… 1.45GB\n\n', '\nThey have a very good answer here:\nhttps://stackoverflow.com/a/32455275/165865\nJust run below images:\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock nate/dockviz images -t\n\n', ""\nThis will inspect the docker image and print the layers:\n$ docker image inspect nginx -f '{{.RootFS.Layers}}'\n[sha256:d626a8ad97a1f9c1f2c4db3814751ada64f60aed927764a3f994fcd88363b659 sha256:82b81d779f8352b20e52295afc6d0eab7e61c0ec7af96d85b8cda7800285d97d sha256:7ab428981537aa7d0c79bc1acbf208c71e57d9678f7deca4267cc03fba26b9c8]\n\n"", '\none more tool : https://github.com/CenturyLinkLabs/dockerfile-from-image\nGUI using ImageLayers.io \n', ""\n\nhttps://hub.docker.com/search?q=* shows all the images in the entire Docker hub, it's not possible to get this via the search command as it doesnt accept wildcards.\nAs of v1.10 you can find all the layers in an image by pulling it and using these commands:\ndocker pull ubuntu\nID=$(sudo docker inspect -f {{.Id}} ubuntu)\njq .rootfs.diff_ids /var/lib/docker/image/aufs/imagedb/content/$(echo $ID|tr ':' '/')\n\n\n3) The size can be found in /var/lib/docker/image/aufs/layerdb/sha256/{LAYERID}/size although LAYERID != the diff_ids found with the previous command. For this you need to look at /var/lib/docker/image/aufs/layerdb/sha256/{LAYERID}/diff and compare with the previous command output to properly match the correct diff_id and size.\n"", ""\nIt's indeed doable to query the manifest or blob info from docker registry server without pulling the image to local disk.\nYou can refer to the Registry v2 API to fetch the manifest of image.\nGET /v2/<name>/manifests/<reference>\n\nNote, you have to handle different manifest version. For v2 you can directly get the size of layer and digest of blob. For v1 manifest, you can HEAD the blob download url to get the actual layer size.\nThere is a simple script for handling above cases that will be continuously maintained. \n"", '\nTo find all the layers of an image and to find the size for a layer, you can display the manifest from the docker hub registry via the ""manifest"" experimental feature:\ndocker manifest inspect ubuntu\n\nThe result is a JSON file (only the first lines are shown here):\n{\n   ""schemaVersion"": 2,\n   ""mediaType"": ""application/vnd.docker.distribution.manifest.list.v2+json"",\n   ""manifests"": [\n      {\n         ""mediaType"": ""application/vnd.docker.distribution.manifest.v2+json"",\n         ""size"": 529,\n         ""digest"": ""sha256:10cbddb6cf8568f56584ccb6c866203e68ab8e621bb87038e254f6f27f955bbe"",\n         ""platform"": {\n            ""architecture"": ""amd64"",\n            ""os"": ""linux""\n         }\n      },\n      {\n         ""mediaType"": ""application/vnd.docker.distribution.manifest.v2+json"",\n         ""size"": 529,\n         ""digest"": ""sha256:dd375524d7eda25a69f9f9790cd3e28855be7908e04162360dd462794035ebf7"",\n         ""platform"": {\n            ""architecture"": ""arm"",\n            ""os"": ""linux"",\n            ""variant"": ""v7""\n\n', '\nNot exactly the original question but to find the sum total of all the images without double-counting shared layers, the following is useful (ubuntu 18):\nsudo du -h -d1  /var/lib/docker/overlay2 | sort -h\n\n', ""\nI've solved this problem by using the search function on Docker's website where '*' is a valid search that returns 200k repositories and then I crawled each invididual page. HTML parsing allows me to extract all the image names on each page.\n""]"
How to write a crawler?,"
I have had thoughts of trying to write a simple crawler that might crawl and produce a list of its findings for our NPO's websites and content.
Does anybody have any thoughts on how to do this? Where do you point the crawler to get started? How does it send back its findings and still keep crawling? How does it know what it finds, etc,etc.
",60k,"
            64
        ","[""\nYou'll be reinventing the wheel, to be sure. But here's the basics:\n\nA list of unvisited URLs - seed this with one or more starting pages\nA list of visited URLs - so you don't go around in circles\nA set of rules for URLs you're not interested in - so you don't index the whole Internet\n\nPut these in persistent storage, so you can stop and start the crawler without losing state.\nAlgorithm is:\nwhile(list of unvisited URLs is not empty) {\n    take URL from list\n    remove it from the unvisited list and add it to the visited list\n    fetch content\n    record whatever it is you want to about the content\n    if content is HTML {\n        parse out URLs from links\n        foreach URL {\n           if it matches your rules\n              and it's not already in either the visited or unvisited list\n              add it to the unvisited list\n        }\n    }\n}\n\n"", '\nThe complicated part of a crawler is if you want to scale it to a huge number of websites/requests.\nIn this situation you will have to deal with some issues like:\n\nImpossibility to keep info all in one database.\nNot enough RAM to deal with huge index(s)\nMultithread performance and concurrency\nCrawler traps (infinite loop created by changing urls, calendars, sessions ids...) and duplicated content.\nCrawl from more than one computer\nMalformed HTML codes\nConstant http errors from servers\nDatabases without compression, wich make your need for space about 8x bigger.\nRecrawl routines and priorities.\nUse requests with compression (Deflate/gzip) (good for any kind of crawler).\n\nAnd some important things\n\nRespect robots.txt\nAnd a crawler delay on each request to dont suffocate web servers.\n\n', ""\nMultithreaded Web Crawler\nIf you want to crawl large sized website then you should write a multi-threaded crawler.\nconnecting,fetching and writing crawled information in files/database - these are the three steps of crawling but if you use a single threaded than your CPU and network utilization will be pour.\nA multi threaded web crawler needs two data structures- linksVisited(this should be implemented as a hashmap or trai) and linksToBeVisited(this is a queue). \nWeb crawler uses BFS to traverse world wide web.\nAlgorithm of a basic web crawler:-\n\nAdd one or more seed urls to linksToBeVisited. The method to add a url to linksToBeVisited must be synchronized.\nPop an element from linksToBeVisited and add this to linksVisited. This pop method to pop url from linksToBeVisited must be synchronized.\nFetch the page from internet.\nParse the file and add any till now not visited link found in the page to linksToBeVisited. URL's can be filtered if needed. The user can give a set of rules to filter which url's to be scanned.\nThe necessary information found on the page is saved in database or file.\nrepeat step 2 to 5 until queue is linksToBeVisited empty.\nHere is a code snippet on how to synchronize the threads....\n public void add(String site) {\n   synchronized (this) {\n   if (!linksVisited.contains(site)) {\n     linksToBeVisited.add(site);\n     }\n   }\n }\n\n public String next() {\n    if (linksToBeVisited.size() == 0) {\n    return null;\n    }\n       synchronized (this) {\n        // Need to check again if size has changed\n       if (linksToBeVisited.size() > 0) {\n          String s = linksToBeVisited.get(0);\n          linksToBeVisited.remove(0);\n          linksVisited.add(s);\n          return s;\n       }\n     return null;\n     }\n  }\n\n\n\n"", ""\nCrawlers are simple in concept.\nYou get a root page via a HTTP GET, parse it to find URLs and put them on a queue unless they've been parsed already (so you need a global record of pages you have already parsed).\nYou can use the Content-type header to find out what the type of content is, and limit your crawler to only parsing the HTML types.\nYou can strip out the HTML tags to get the plain text, which you can do text analysis on (to get tags, etc, the meat of the page). You could even do that on the alt/title tags for images if you got that advanced.\nAnd in the background you can have a pool of threads eating URLs from the Queue and doing the same. You want to limit the number of threads of course.\n"", ""\nIf your NPO's sites are relatively big or complex (having dynamic pages that'll effectively create a 'black hole' like a calendar with a 'next day' link) you'd be better using a real web crawler, like Heritrix.\nIf the sites total a few number of pages you can get away with just using curl or wget or your own. Just remember if they start to get big or you start making your script more complex to just use a real crawler or at least look at its source to see what are they doing and why.\nSome issues (there are more):\n\nBlack holes (as described)\nRetries (what if you get a 500?)\nRedirects\nFlow control (else you can be a burden on the sites)\nrobots.txt implementation\n\n"", '\nWikipedia has a good article about web crawlers, covering many of the algorithms and considerations.\nHowever, I wouldn\'t bother writing my own crawler.  It\'s a lot of work, and since you only need a ""simple crawler"", I\'m thinking all you really need is an off-the-shelf crawler.  There are a lot of free and open-source crawlers that will likely do everything you need, with very little work on your part.\n', '\nYou could make a list of words and make a thread for each word searched at google. Then each thread will create a new thread for each link it find in the page. Each thread should write what it finds in a database. When each thread finishes reading the page, it terminates.And there you have a very big database of links in your database.\n', '\nUse wget, do a recursive web suck, which will dump all the files onto your harddrive, then write another script to go through all the downloaded files and analyze them.\nEdit: or maybe curl instead of wget, but I am not familiar with curl, I do not know if it does recursive downloads like wget.\n', ""\nI'm using Open search server for my company internal search, try this : http://open-search-server.com its also open soruce.\n"", '\ni did a simple web crawler using reactive extension in .net.\nhttps://github.com/Misterhex/WebCrawler\npublic class Crawler\n    {\n    class ReceivingCrawledUri : ObservableBase<Uri>\n    {\n        public int _numberOfLinksLeft = 0;\n\n        private ReplaySubject<Uri> _subject = new ReplaySubject<Uri>();\n        private Uri _rootUri;\n        private IEnumerable<IUriFilter> _filters;\n\n        public ReceivingCrawledUri(Uri uri)\n            : this(uri, Enumerable.Empty<IUriFilter>().ToArray())\n        { }\n\n        public ReceivingCrawledUri(Uri uri, params IUriFilter[] filters)\n        {\n            _filters = filters;\n\n            CrawlAsync(uri).Start();\n        }\n\n        protected override IDisposable SubscribeCore(IObserver<Uri> observer)\n        {\n            return _subject.Subscribe(observer);\n        }\n\n        private async Task CrawlAsync(Uri uri)\n        {\n            using (HttpClient client = new HttpClient() { Timeout = TimeSpan.FromMinutes(1) })\n            {\n                IEnumerable<Uri> result = new List<Uri>();\n\n                try\n                {\n                    string html = await client.GetStringAsync(uri);\n                    result = CQ.Create(html)[""a""].Select(i => i.Attributes[""href""]).SafeSelect(i => new Uri(i));\n                    result = Filter(result, _filters.ToArray());\n\n                    result.ToList().ForEach(async i =>\n                    {\n                        Interlocked.Increment(ref _numberOfLinksLeft);\n                        _subject.OnNext(i);\n                        await CrawlAsync(i);\n                    });\n                }\n                catch\n                { }\n\n                if (Interlocked.Decrement(ref _numberOfLinksLeft) == 0)\n                    _subject.OnCompleted();\n            }\n        }\n\n        private static List<Uri> Filter(IEnumerable<Uri> uris, params IUriFilter[] filters)\n        {\n            var filtered = uris.ToList();\n            foreach (var filter in filters.ToList())\n            {\n                filtered = filter.Filter(filtered);\n            }\n            return filtered;\n        }\n    }\n\n    public IObservable<Uri> Crawl(Uri uri)\n    {\n        return new ReceivingCrawledUri(uri, new ExcludeRootUriFilter(uri), new ExternalUriFilter(uri), new AlreadyVisitedUriFilter());\n    }\n\n    public IObservable<Uri> Crawl(Uri uri, params IUriFilter[] filters)\n    {\n        return new ReceivingCrawledUri(uri, filters);\n    }\n}\n\nand you can use it as follows:\nCrawler crawler = new Crawler();\nIObservable observable = crawler.Crawl(new Uri(""http://www.codinghorror.com/""));\nobservable.Subscribe(onNext: Console.WriteLine, \nonCompleted: () => Console.WriteLine(""Crawling completed""));\n\n']"
Python: Disable images in Selenium Google ChromeDriver,"
I spend a lot of time searching about this.
At the end of the day I combined a number of answers and it works. I share my answer and I'll appreciate it if anyone edits it or provides us with an easier way to do this.
1- The answer in Disable images in Selenium Google ChromeDriver works in Java. So we should do the same thing in Python:
opt = webdriver.ChromeOptions()
opt.add_extension(""Block-image_v1.1.crx"")
browser = webdriver.Chrome(chrome_options=opt)

2- But downloading ""Block-image_v1.1.crx"" is a little bit tricky, because there is no direct way to do that. For this purpose, instead of going to: https://chrome.google.com/webstore/detail/block-image/pehaalcefcjfccdpbckoablngfkfgfgj
you can go to http://chrome-extension-downloader.com/
and paste the extension url there to be able to download the extension file.
3- Then you will be able to use the above mentioned code with the path to the extension file that you have downloaded.
",65k,"
            58
        ","['\nHere is another way to disable images:\nfrom selenium import webdriver\n\nchrome_options = webdriver.ChromeOptions()\nprefs = {""profile.managed_default_content_settings.images"": 2}\nchrome_options.add_experimental_option(""prefs"", prefs)\ndriver = webdriver.Chrome(chrome_options=chrome_options)\n\nI found it below:\nhttp://nullege.com/codes/show/src@o@s@osintstalker-HEAD@fbstalker1.py/56/selenium.webdriver.ChromeOptions.add_experimental_option\n', '\nJava:\nWith this Chrome nor Firefox would load images. The syntax is different but the strings on the parameters are the same.\n    chromeOptions = new ChromeOptions();\n    HashMap<String, Object> images = new HashMap<String, Object>();\n    images.put(""images"", 2);\n    HashMap<String, Object> prefs = new HashMap<String, Object>();\n    prefs.put(""profile.default_content_setting_values"", images);\n    chromeOptions.setExperimentalOption(""prefs"", prefs);\n    driver=new ChromeDriver(chromeOptions);\n\n    firefoxOpt = new FirefoxOptions();\n    FirefoxProfile profile = new FirefoxProfile();\n    profile.setPreference(""permissions.default.image"", 2);\n    firefoxOpt.setProfile(profile);\n\n', '\nThere is another way that comes probably to mind to everyone to access chrome://settings and then go through the settings with selenium I started this way just for didactic curiosity, but then I hit a forest of shadow-roots elements now when you encounter more than 3 shadow root element combined with dynamic content is clearly a way to obfuscate and make it impossible to automate, although might sound at least theoretically possible this approach looks more like a dead end, I will leave this answer with the example code, just for purely learning purposes to advert the people tempted to go to the challenge..  Not only was hard to find just the content settings due to the shadowroots and dynamic change when you find the button is not clickable at this point.  \ndriver = webdriver.Chrome()\n\n\ndef expand_shadow_element(element):\n  shadow_root = driver.execute_script(\'return arguments[0].shadowRoot\', element)\n  return shadow_root\n\ndriver.get(""chrome://settings"")\nroot1 = driver.find_element_by_tag_name(\'settings-ui\')\nshadow_root1 = expand_shadow_element(root1)\n\nroot2 = shadow_root1.find_element_by_css_selector(\'[page-name=""Settings""]\')\nshadow_root2 = expand_shadow_element(root2)\n\nroot3 = shadow_root2.find_element_by_id(\'search\')\nshadow_root3 = expand_shadow_element(root3)\n\nsearch_button = shadow_root3.find_element_by_id(""searchTerm"")\nsearch_button.click()\n\ntext_area = shadow_root3.find_element_by_id(\'searchInput\')\ntext_area.send_keys(""content settings"")\n\nroot0 = shadow_root1.find_element_by_id(\'main\')\nshadow_root0_s = expand_shadow_element(root0)\n\n\nroot1_p = shadow_root0_s.find_element_by_css_selector(\'settings-basic-page\')\nshadow_root1_p = expand_shadow_element(root1_p)\n\n\nroot1_s = shadow_root1_p.find_element_by_css_selector(\'settings-privacy-page\')\nshadow_root1_s = expand_shadow_element(root1_s)\n\ncontent_settings_div = shadow_root1_s.find_element_by_css_selector(\'#site-settings-subpage-trigger\')\ncontent_settings = content_settings_div.find_element_by_css_selector(""button"")\ncontent_settings.click()\n\n']"
I need a Powerful Web Scraper library [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



I need a powerful web scraper library  for mining contents from web. That can be paid or free both will be fine for me. Please suggest me a library or better way for mining the data  and store in my preferred database. I have searched but i didn't find any good solution for this. I need a good suggestion from experts. Please help me out.
",66k,"
            29
        ","[""\nScraping is easy really, you just have to parse the content you are downloading and get all the associated links.\nThe most important piece though is the part that processes the HTML.  Because most browsers don't require the cleanest (or standards-compliant) HTML in order to be rendered, you need an HTML parser that is going to be able to make sense of HTML that is not always well-formed.\nI recommend you use the HTML Agility Pack for this purpose.  It does very well at handling non-well-formed HTML, and provides an easy interface for you to use XPath queries to get nodes in the resulting document.\nBeyond that, you just need to pick a data store to hold your processed data (you can use any database technology for that) and a way to download content from the web, which .NET provides two high-level mechanisms for, the WebClient and HttpWebRequest/HttpWebResponse classes.\n"", '\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\n\nnamespace SoftCircuits.Parsing\n{\n    public class HtmlTag\n    {\n        /// <summary>\n        /// Name of this tag\n        /// </summary>\n        public string Name { get; set; }\n\n        /// <summary>\n        /// Collection of attribute names and values for this tag\n        /// </summary>\n        public Dictionary<string, string> Attributes { get; set; }\n\n        /// <summary>\n        /// True if this tag contained a trailing forward slash\n        /// </summary>\n        public bool TrailingSlash { get; set; }\n\n        /// <summary>\n        /// Indicates if this tag contains the specified attribute. Note that\n        /// true is returned when this tag contains the attribute even when the\n        /// attribute has no value\n        /// </summary>\n        /// <param name=""name"">Name of attribute to check</param>\n        /// <returns>True if tag contains attribute or false otherwise</returns>\n        public bool HasAttribute(string name)\n        {\n            return Attributes.ContainsKey(name);\n        }\n    };\n\n    public class HtmlParser : TextParser\n    {\n        public HtmlParser()\n        {\n        }\n\n        public HtmlParser(string html) : base(html)\n        {\n        }\n\n        /// <summary>\n        /// Parses the next tag that matches the specified tag name\n        /// </summary>\n        /// <param name=""name"">Name of the tags to parse (""*"" = parse all tags)</param>\n        /// <param name=""tag"">Returns information on the next occurrence of the specified tag or null if none found</param>\n        /// <returns>True if a tag was parsed or false if the end of the document was reached</returns>\n        public bool ParseNext(string name, out HtmlTag tag)\n        {\n            // Must always set out parameter\n            tag = null;\n\n            // Nothing to do if no tag specified\n            if (String.IsNullOrEmpty(name))\n                return false;\n\n            // Loop until match is found or no more tags\n            MoveTo(\'<\');\n            while (!EndOfText)\n            {\n                // Skip over opening \'<\'\n                MoveAhead();\n\n                // Examine first tag character\n                char c = Peek();\n                if (c == \'!\' && Peek(1) == \'-\' && Peek(2) == \'-\')\n                {\n                    // Skip over comments\n                    const string endComment = ""-->"";\n                    MoveTo(endComment);\n                    MoveAhead(endComment.Length);\n                }\n                else if (c == \'/\')\n                {\n                    // Skip over closing tags\n                    MoveTo(\'>\');\n                    MoveAhead();\n                }\n                else\n                {\n                    bool result, inScript;\n\n                    // Parse tag\n                    result = ParseTag(name, ref tag, out inScript);\n                    // Because scripts may contain tag characters, we have special\n                    // handling to skip over script contents\n                    if (inScript)\n                        MovePastScript();\n                    // Return true if requested tag was found\n                    if (result)\n                        return true;\n                }\n                // Find next tag\n                MoveTo(\'<\');\n            }\n            // No more matching tags found\n            return false;\n        }\n\n        /// <summary>\n        /// Parses the contents of an HTML tag. The current position should be at the first\n        /// character following the tag\'s opening less-than character.\n        /// \n        /// Note: We parse to the end of the tag even if this tag was not requested by the\n        /// caller. This ensures subsequent parsing takes place after this tag\n        /// </summary>\n        /// <param name=""reqName"">Name of the tag the caller is requesting, or ""*"" if caller\n        /// is requesting all tags</param>\n        /// <param name=""tag"">Returns information on this tag if it\'s one the caller is\n        /// requesting</param>\n        /// <param name=""inScript"">Returns true if tag began, and did not end, and script\n        /// block</param>\n        /// <returns>True if data is being returned for a tag requested by the caller\n        /// or false otherwise</returns>\n        protected bool ParseTag(string reqName, ref HtmlTag tag, out bool inScript)\n        {\n            bool doctype, requested;\n            doctype = inScript = requested = false;\n\n            // Get name of this tag\n            string name = ParseTagName();\n\n            // Special handling\n            if (String.Compare(name, ""!DOCTYPE"", true) == 0)\n                doctype = true;\n            else if (String.Compare(name, ""script"", true) == 0)\n                inScript = true;\n\n            // Is this a tag requested by caller?\n            if (reqName == ""*"" || String.Compare(name, reqName, true) == 0)\n            {\n                // Yes\n                requested = true;\n                // Create new tag object\n                tag = new HtmlTag();\n                tag.Name = name;\n                tag.Attributes = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n            }\n\n            // Parse attributes\n            MovePastWhitespace();\n            while (Peek() != \'>\' && Peek() != NullChar)\n            {\n                if (Peek() == \'/\')\n                {\n                    // Handle trailing forward slash\n                    if (requested)\n                        tag.TrailingSlash = true;\n                    MoveAhead();\n                    MovePastWhitespace();\n                    // If this is a script tag, it was closed\n                    inScript = false;\n                }\n                else\n                {\n                    // Parse attribute name\n                    name = (!doctype) ? ParseAttributeName() : ParseAttributeValue();\n                    MovePastWhitespace();\n                    // Parse attribute value\n                    string value = String.Empty;\n                    if (Peek() == \'=\')\n                    {\n                        MoveAhead();\n                        MovePastWhitespace();\n                        value = ParseAttributeValue();\n                        MovePastWhitespace();\n                    }\n                    // Add attribute to collection if requested tag\n                    if (requested)\n                    {\n                        // This tag replaces existing tags with same name\n                        if (tag.Attributes.ContainsKey(name))\n                            tag.Attributes.Remove(name);\n                        tag.Attributes.Add(name, value);\n                    }\n                }\n            }\n            // Skip over closing \'>\'\n            MoveAhead();\n\n            return requested;\n        }\n\n        /// <summary>\n        /// Parses a tag name. The current position should be the first character of the name\n        /// </summary>\n        /// <returns>Returns the parsed name string</returns>\n        protected string ParseTagName()\n        {\n            int start = Position;\n            while (!EndOfText && !Char.IsWhiteSpace(Peek()) && Peek() != \'>\')\n                MoveAhead();\n            return Substring(start, Position);\n        }\n\n        /// <summary>\n        /// Parses an attribute name. The current position should be the first character\n        /// of the name\n        /// </summary>\n        /// <returns>Returns the parsed name string</returns>\n        protected string ParseAttributeName()\n        {\n            int start = Position;\n            while (!EndOfText && !Char.IsWhiteSpace(Peek()) && Peek() != \'>\' && Peek() != \'=\')\n                MoveAhead();\n            return Substring(start, Position);\n        }\n\n        /// <summary>\n        /// Parses an attribute value. The current position should be the first non-whitespace\n        /// character following the equal sign.\n        /// \n        /// Note: We terminate the name or value if we encounter a new line. This seems to\n        /// be the best way of handling errors such as values missing closing quotes, etc.\n        /// </summary>\n        /// <returns>Returns the parsed value string</returns>\n        protected string ParseAttributeValue()\n        {\n            int start, end;\n            char c = Peek();\n            if (c == \'""\' || c == \'\\\'\')\n            {\n                // Move past opening quote\n                MoveAhead();\n                // Parse quoted value\n                start = Position;\n                MoveTo(new char[] { c, \'\\r\', \'\\n\' });\n                end = Position;\n                // Move past closing quote\n                if (Peek() == c)\n                    MoveAhead();\n            }\n            else\n            {\n                // Parse unquoted value\n                start = Position;\n                while (!EndOfText && !Char.IsWhiteSpace(c) && c != \'>\')\n                {\n                    MoveAhead();\n                    c = Peek();\n                }\n                end = Position;\n            }\n            return Substring(start, end);\n        }\n\n        /// <summary>\n        /// Locates the end of the current script and moves past the closing tag\n        /// </summary>\n        protected void MovePastScript()\n        {\n            const string endScript = ""</script"";\n\n            while (!EndOfText)\n            {\n                MoveTo(endScript, true);\n                MoveAhead(endScript.Length);\n                if (Peek() == \'>\' || Char.IsWhiteSpace(Peek()))\n                {\n                    MoveTo(\'>\');\n                    MoveAhead();\n                    break;\n                }\n            }\n        }\n    }\n}\n\n', '\nFor simple websites ( = plain html only), Mechanize works really well and fast. For sites that use Javascript, AJAX or even Flash, you need a real browser solution such as iMacros.\n', ""\nMy Advice:\nYou could look around for a HTML Parser and then use it to parse out information from sites. (Like here). Then all you would need to do is save that data into your database however you see fit.\nI've made my own scraper a few times, it's pretty easy and allow you to customize the data that is saved. \nData Mining Tools\nIf you really just want to get a tool to do this then you should have no problem finding some. \n""]"
scrapy- how to stop Redirect (302),"
I'm trying to crawl a url using Scrapy. But it redirects me to page that doesn't exist. 
Redirecting (302) to <GET http://www.shop.inonit.in/mobile/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/1275197> from <GET http://www.shop.inonit.in/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/pid-1275197.aspx>

The problem is http://www.shop.inonit.in/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/pid-1275197.aspx exists, but http://www.shop.inonit.in/mobile/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/1275197 doesn't, so the crawler cant find this. I've crawled many other websites as well but didn't have this problem anywhere else. Is there a way I can stop this redirect?
Any help would be much appreciated. Thanks.
Update: This is my spider class
class Inon_Spider(BaseSpider):
name = 'Inon'
allowed_domains = ['www.shop.inonit.in']

start_urls = ['http://www.shop.inonit.in/Products/Inonit-Gadget-Accessories-Mobile-Covers/-The-Red-Tag/Samsung-Note-2-Dead-Mau/pid-2656465.aspx']

def parse(self, response):

    item = DealspiderItem()
    hxs = HtmlXPathSelector(response)

    title = hxs.select('//div[@class=""aboutproduct""]/div[@class=""container9""]/div[@class=""ctl_aboutbrand""]/h1/text()').extract()
    price = hxs.select('//span[@id=""ctl00_ContentPlaceHolder1_Price_ctl00_spnWebPrice""]/span[@class=""offer""]/span[@id=""ctl00_ContentPlaceHolder1_Price_ctl00_lblOfferPrice""]/text()').extract()
    prc = price[0].replace(""Rs.  "","""")
    description = []

    item['price'] = prc
    item['title'] = title
    item['description'] = description
    item['url'] = response.url

    return item

",29k,"
            25
        ","['\nyes you can do this simply by adding meta values like\nmeta={\'dont_redirect\': True}\n\nalso you can stop redirected for a particular response code like\nmeta={\'dont_redirect\': True,""handle_httpstatus_list"": [302]}\n\nit will stop redirecting only 302 response codes. you can add as many http status code you want to avoid redirecting them.\nexample\nyield Request(\'some url\',\n    meta = {\n        \'dont_redirect\': True,\n        \'handle_httpstatus_list\': [302]\n    },\n    callback= self.some_call_back)\n\n', ""\nAfter looking at the documentation and looking through the relevant source, I was able to figure it out. If you look in the source for start_requests, you'll see that it calls make_requests_from_url for all URLs.\nInstead of modifying start_requests, I modified make_requests_from_url\ndef make_requests_from_url(self, url):\n    return Request(url, dont_filter=True, meta = {\n        'dont_redirect': True,\n        'handle_httpstatus_list': [301, 302]\n    })\n\nAnd added this as part of my spider, right above parse().\n"", '\nBy default, Scrapy use RedirectMiddleware to handle redirection. You can set REDIRECT_ENABLED to False to disable redirection.\nSee documentation. \n', ""\nAs explained here: Scrapy docs\nUse Request Meta\nrequest =  scrapy.Request(link.url, callback=self.parse2)\nrequest.meta['dont_redirect'] = True\nyield request\n\n""]"
Writing items to a MySQL database in Scrapy,"
I am new to Scrapy, I had the spider code
class Example_spider(BaseSpider):
   name = ""example""
   allowed_domains = [""www.example.com""]

   def start_requests(self):
       yield self.make_requests_from_url(""http://www.example.com/bookstore/new"")

   def parse(self, response):
       hxs = HtmlXPathSelector(response)
       urls = hxs.select('//div[@class=""bookListingBookTitle""]/a/@href').extract()
       for i in urls:
           yield Request(urljoin(""http://www.example.com/"", i[1:]), callback=self.parse_url)

   def parse_url(self, response):
           hxs = HtmlXPathSelector(response)
           main =   hxs.select('//div[@id=""bookshelf-bg""]')
           items = []
           for i in main:
           item = Exampleitem()
           item['book_name'] = i.select('div[@class=""slickwrap full""]/div[@id=""bookstore_detail""]/div[@class=""book_listing clearfix""]/div[@class=""bookstore_right""]/div[@class=""title_and_byline""]/p[@class=""book_title""]/text()')[0].extract()
           item['price'] = i.select('div[@id=""book-sidebar-modules""]/div[@class=""add_to_cart_wrapper slickshadow""]/div[@class=""panes""]/div[@class=""pane clearfix""]/div[@class=""inner""]/div[@class=""add_to_cart 0""]/form/div[@class=""line-item""]/div[@class=""line-item-price""]/text()').extract()
           items.append(item)
       return items

And pipeline code is:
class examplePipeline(object):

    def __init__(self):               
        self.dbpool = adbapi.ConnectionPool('MySQLdb',
                db='blurb',
                user='root',
                passwd='redhat',
                cursorclass=MySQLdb.cursors.DictCursor,
                charset='utf8',
                use_unicode=True
            )
def process_item(self, spider, item):
    # run db query in thread pool
    assert isinstance(item, Exampleitem)
    query = self.dbpool.runInteraction(self._conditional_insert, item)
    query.addErrback(self.handle_error)
    return item
def _conditional_insert(self, tx, item):
    print ""db connected-=========>""
    # create record if doesn't exist. 
    tx.execute(""select * from example_book_store where book_name = %s"", (item['book_name']) )
    result = tx.fetchone()
    if result:
        log.msg(""Item already stored in db: %s"" % item, level=log.DEBUG)
    else:
        tx.execute(""""""INSERT INTO example_book_store (book_name,price)
                    VALUES (%s,%s)"""""",   
                            (item['book_name'],item['price'])
                    )
        log.msg(""Item stored in db: %s"" % item, level=log.DEBUG)            

def handle_error(self, e):
    log.err(e)          

After running this I am getting the following error 
exceptions.NameError: global name 'Exampleitem' is not defined

I got the above error when I added the below code in process_item method
assert isinstance(item, Exampleitem)

and without adding this line I am getting 
**exceptions.TypeError: 'Example_spider' object is not subscriptable

Can anyone make this code run and make sure that all the items saved into database?
",40k,"
            21
        ","['\nTry the following code in your pipeline\nimport sys\nimport MySQLdb\nimport hashlib\nfrom scrapy.exceptions import DropItem\nfrom scrapy.http import Request\n\nclass MySQLStorePipeline(object):\n    def __init__(self):\n        self.conn = MySQLdb.connect(\'host\', \'user\', \'passwd\', \n                                    \'dbname\', charset=""utf8"",\n                                    use_unicode=True)\n        self.cursor = self.conn.cursor()\n\n    def process_item(self, item, spider):    \n        try:\n            self.cursor.execute(""""""INSERT INTO example_book_store (book_name, price)  \n                        VALUES (%s, %s)"""""", \n                       (item[\'book_name\'].encode(\'utf-8\'), \n                        item[\'price\'].encode(\'utf-8\')))            \n            self.conn.commit()            \n        except MySQLdb.Error, e:\n            print ""Error %d: %s"" % (e.args[0], e.args[1])\n        return item\n\n', ""\nYour process_item method should be declared as: def process_item(self, item, spider): instead of def process_item(self, spider, item): -> you switched the arguments around.\nThis exception: exceptions.NameError: global name 'Exampleitem' is not defined indicates you didn't import the Exampleitem in your pipeline.\nTry adding: from myspiders.myitems import Exampleitem (with correct names/paths ofcourse).\n"", '\nI think this way is better and more concise:\n#Item\nclass pictureItem(scrapy.Item):\n    topic_id=scrapy.Field()\n    url=scrapy.Field()\n\n#SQL\nself.save_picture=""insert into picture(`url`,`id`) values(%(url)s,%(id)s);""\n\n#usage\ncur.execute(self.save_picture,dict(item))\n\nIt\'s just like\ncur.execute(""insert into picture(`url`,`id`) values(%(url)s,%(id)s)"" % {""url"":someurl,""id"":1})\n\nCause (you can read more about Items in Scrapy)\n\nThe Field class is just an alias to the built-in dict class and doesn’t provide any extra functionality or attributes. In other words, Field objects are plain-old Python dicts. \n\n']"
Locally run all of the spiders in Scrapy,"
Is there a way to run all of the spiders in a Scrapy project without using the Scrapy daemon? There used to be a way to run multiple spiders with scrapy crawl, but that syntax was removed and Scrapy's code changed quite a bit.
I tried creating my own command:
from scrapy.command import ScrapyCommand
from scrapy.utils.misc import load_object
from scrapy.conf import settings

class Command(ScrapyCommand):
    requires_project = True

    def syntax(self):
        return '[options]'

    def short_desc(self):
        return 'Runs all of the spiders'

    def run(self, args, opts):
        spman_cls = load_object(settings['SPIDER_MANAGER_CLASS'])
        spiders = spman_cls.from_settings(settings)

        for spider_name in spiders.list():
            spider = self.crawler.spiders.create(spider_name)
            self.crawler.crawl(spider)

        self.crawler.start()

But once a spider is registered with self.crawler.crawl(), I get assertion errors for all of the other spiders:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/scrapy/cmdline.py"", line 138, in _run_command
    cmd.run(args, opts)
  File ""/home/blender/Projects/scrapers/store_crawler/store_crawler/commands/crawlall.py"", line 22, in run
    self.crawler.crawl(spider)
  File ""/usr/lib/python2.7/site-packages/scrapy/crawler.py"", line 47, in crawl
    return self.engine.open_spider(spider, requests)
  File ""/usr/lib/python2.7/site-packages/twisted/internet/defer.py"", line 1214, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File ""/usr/lib/python2.7/site-packages/twisted/internet/defer.py"", line 1071, in _inlineCallbacks
    result = g.send(result)
  File ""/usr/lib/python2.7/site-packages/scrapy/core/engine.py"", line 215, in open_spider
    spider.name
exceptions.AssertionError: No free spider slots when opening 'spidername'

Is there any way to do this? I'd rather not start subclassing core Scrapy components just to run all of my spiders like this.
",11k,"
            18
        ","[""\nWhy didn't you just use something like:\nscrapy list|xargs -n 1 scrapy crawl\n\n?\n"", ""\nHere is an example that does not run inside a custom command, but runs the Reactor manually and creates a new Crawler for each spider:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\n# scrapy.conf.settings singlton was deprecated last year\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy import log\n\ndef setup_crawler(spider_name):\n    crawler = Crawler(settings)\n    crawler.configure()\n    spider = crawler.spiders.create(spider_name)\n    crawler.crawl(spider)\n    crawler.start()\n\nlog.start()\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.configure()\n\nfor spider_name in crawler.spiders.list():\n    setup_crawler(spider_name)\n\nreactor.run()\n\nYou will have to design some signal system to stop the reactor when all spiders are finished.\nEDIT: And here is how you can run multiple spiders in a custom command:\nfrom scrapy.command import ScrapyCommand\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import Crawler\n\nclass Command(ScrapyCommand):\n\n    requires_project = True\n\n    def syntax(self):\n        return '[options]'\n\n    def short_desc(self):\n        return 'Runs all of the spiders'\n\n    def run(self, args, opts):\n        settings = get_project_settings()\n\n        for spider_name in self.crawler.spiders.list():\n            crawler = Crawler(settings)\n            crawler.configure()\n            spider = crawler.spiders.create(spider_name)\n            crawler.crawl(spider)\n            crawler.start()\n\n        self.crawler.start()\n\n"", ""\nthe answer of @Steven Almeroth will be failed in Scrapy 1.0, and you should edit the script like this:\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nclass Command(ScrapyCommand):\n\n    requires_project = True\n    excludes = ['spider1']\n\n    def syntax(self):\n        return '[options]'\n\n    def short_desc(self):\n        return 'Runs all of the spiders'\n\n    def run(self, args, opts):\n        settings = get_project_settings()\n        crawler_process = CrawlerProcess(settings) \n\n        for spider_name in crawler_process.spider_loader.list():\n            if spider_name in self.excludes:\n                continue\n            spider_cls = crawler_process.spider_loader.load(spider_name) \n            crawler_process.crawl(spider_cls)\n        crawler_process.start()\n\n"", '\nthis code is works on My scrapy version is 1.3.3 (save it in same directory in scrapy.cfg):\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spiders.list():\n    print (""Running spider %s"" % (spider_name))\n    process.crawl(spider_name,query=""dvh"") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n\nfor scrapy 1.5.x (so you don\'t get the deprecation warning)\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spider_loader.list():\n    print (""Running spider %s"" % (spider_name))\n    process.crawl(spider_name,query=""dvh"") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n\n', '\nLinux script\n#!/bin/bash\nfor spider in $(scrapy list)\ndo\nscrapy crawl ""$spider"" -o ""$spider"".json\ndone\n\n', ""\nRunning all spiders in project using python\n# Run all spiders in project implemented using Scrapy 2.7.0\n\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\n\ndef main():\n    settings = get_project_settings()\n    process = CrawlerProcess(settings)\n    spiders_names = process.spider_loader.list()\n    for s in spiders_names:\n        process.crawl(s)\n    process.start()\n\n\nif __name__ == '__main__':\n    main()\n\n""]"
Creating a generic scrapy spider,"
My question is really how to do the same thing as a previous question, but in Scrapy 0.14.
Using one Scrapy spider for several websites
Basically, I have GUI that takes parameters like domain, keywords, tag names, etc. and I want to create a generic spider to crawl those domains for those keywords in those tags.  I've read conflicting things, using older versions of scrapy, by either overriding the spider manager class or by dynamically creating a spider.  Which method is preferred and how do I implement and invoke the proper solution?  Thanks in advance.
Here is the code that I want to make generic.  It also uses BeautifulSoup.  I paired it down so hopefully didn't remove anything crucial to understand it.
class MySpider(CrawlSpider):

name = 'MySpider'
allowed_domains = ['somedomain.com', 'sub.somedomain.com']
start_urls = ['http://www.somedomain.com']

rules = (
    Rule(SgmlLinkExtractor(allow=('/pages/', ), deny=('', ))),

    Rule(SgmlLinkExtractor(allow=('/2012/03/')), callback='parse_item'),
)

def parse_item(self, response):
    contentTags = []

    soup = BeautifulSoup(response.body)

    contentTags = soup.findAll('p', itemprop=""myProp"")

    for contentTag in contentTags:
        matchedResult = re.search('Keyword1|Keyword2', contentTag.text)
        if matchedResult:
            print('URL Found: ' + response.url)

    pass

",7k,"
            17
        ","['\nYou could create a run-time spider which is evaluated by the interpreter. This code piece could be evaluated at runtime like so:\na = open(""test.py"")\nfrom compiler import compile\nd = compile(a.read(), \'spider.py\', \'exec\')\neval(d)\n\nMySpider\n<class \'__main__.MySpider\'>\nprint MySpider.start_urls\n[\'http://www.somedomain.com\']\n\n', '\nI use the Scrapy Extensions approach to extend the Spider class to a class named Masterspider that includes a generic parser.\nBelow is the very ""short"" version of my generic extended parser. Note that you\'ll need to implement a renderer with a Javascript engine (such as Selenium or BeautifulSoup) a as soon as you start working on pages using AJAX. And a lot of additional code to manage differences between sites (scrap based on column title, handle relative vs long URL, manage different kind of data containers, etc...).\nWhat is interresting with the Scrapy Extension approach is that you can still override the generic parser method if something does not fit but I never had to. The Masterspider class checks if some methods have been created (eg. parser_start, next_url_parser...) under the site specific spider class to allow the management of specificies: send a form, construct the next_url request from elements in the page, etc.\nAs I\'m scraping very different sites, there\'s always specificities to manage. That\'s why I prefer to keep a class for each scraped site so that I can write some specific methods to handle it (pre-/post-processing except PipeLines, Request generators...).\nmasterspider/sitespider/settings.py\nEXTENSIONS = {\n    \'masterspider.masterspider.MasterSpider\': 500\n}\n\nmasterspider/masterspdier/masterspider.py\n# -*- coding: utf8 -*-\nfrom scrapy.spider import Spider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request\nfrom sitespider.items import genspiderItem\n\nclass MasterSpider(Spider):\n\n    def start_requests(self):\n        if hasattr(self,\'parse_start\'): # First page requiring a specific parser\n            fcallback = self.parse_start\n        else:\n            fcallback = self.parse\n        return [ Request(self.spd[\'start_url\'],\n                     callback=fcallback,\n                     meta={\'itemfields\': {}}) ]\n\n    def parse(self, response):\n        sel = Selector(response)\n        lines = sel.xpath(self.spd[\'xlines\'])\n        # ...\n        for line in lines:\n            item = genspiderItem(response.meta[\'itemfields\'])               \n            # ...\n            # Get request_url of detailed page and scrap basic item info\n            # ... \n            yield  Request(request_url,\n                   callback=self.parse_item,\n                   meta={\'item\':item, \'itemfields\':response.meta[\'itemfields\']})\n\n        for next_url in sel.xpath(self.spd[\'xnext_url\']).extract():\n            if hasattr(self,\'next_url_parser\'): # Need to process the next page URL before?\n                yield self.next_url_parser(next_url, response)\n            else:\n                yield Request(\n                    request_url,\n                    callback=self.parse,\n                    meta=response.meta)\n\n    def parse_item(self, response):\n        sel = Selector(response)\n        item = response.meta[\'item\']\n        for itemname, xitemname in self.spd[\'x_ondetailpage\'].iteritems():\n            item[itemname] = ""\\n"".join(sel.xpath(xitemname).extract())\n        return item\n\nmasterspider/sitespider/spiders/somesite_spider.py\n# -*- coding: utf8 -*-\nfrom scrapy.spider import Spider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request\nfrom sitespider.items import genspiderItem\nfrom masterspider.masterspider import MasterSpider\n\nclass targetsiteSpider(MasterSpider):\n    name = ""targetsite""\n    allowed_domains = [""www.targetsite.com""]\n    spd = {\n        \'start_url\' : ""http://www.targetsite.com/startpage"", # Start page\n        \'xlines\' : ""//td[something...]"",\n        \'xnext_url\' : ""//a[contains(@href,\'something?page=\')]/@href"", # Next pages\n        \'x_ondetailpage\' : {\n            ""itemprop123"" :      u""id(\'someid\')//text()""\n            }\n    }\n\n#     def next_url_parser(self, next_url, response): # OPTIONAL next_url regexp pre-processor\n#          ...\n\n', ""\nInstead of having the variables name,allowed_domains, start_urls and rules attached to the class, you should write a MySpider.__init__, call CrawlSpider.__init__ from that passing the necessary arguments, and setting name, allowed_domains etc. per object. \nMyProp and keywords also should be set within your __init__. So in the end you should have something like below. You don't have to add name to the arguments, as name is set by BaseSpider itself from kwargs: \nclass MySpider(CrawlSpider):\n\n    def __init__(self, allowed_domains=[], start_urls=[], \n            rules=[], findtag='', finditemprop='', keywords='', **kwargs):\n        CrawlSpider.__init__(self, **kwargs)\n        self.allowed_domains = allowed_domains\n        self.start_urls = start_urls\n        self.rules = rules\n        self.findtag = findtag\n        self.finditemprop = finditemprop\n        self.keywords = keywords\n\n    def parse_item(self, response):\n        contentTags = []\n\n        soup = BeautifulSoup(response.body)\n\n        contentTags = soup.findAll(self.findtag, itemprop=self.finditemprop)\n\n        for contentTag in contentTags:\n            matchedResult = re.search(self.keywords, contentTag.text)\n            if matchedResult:\n                print('URL Found: ' + response.url)\n\n"", '\nI am not sure which way is preferred, but I will tell you what I have done in the past. I am in no way sure that this is the best (or correct) way of doing this and I would be interested to learn what other people think.\nI usually just override the parent class (CrawlSpider) and either pass in arguments and then initialize the parent class via super(MySpider, self).__init__() from within my own init-function or I pull in that data from a database where I have saved a list of links to be appended to start_urls earlier.\n', '\nAs far as crawling specific domains passed as arguments goes, I just override Spider.__init__:\nclass MySpider(scrapy.Spider):\n    """"""\n    This spider will try to crawl whatever is passed in `start_urls` which\n    should be a comma-separated string of fully qualified URIs.\n\n    Example: start_urls=http://localhost,http://example.com\n    """"""\n    def __init__(self, name=None, **kwargs):\n        if \'start_urls\' in kwargs:\n            self.start_urls = kwargs.pop(\'start_urls\').split(\',\')\n        super(Spider, self).__init__(name, **kwargs)\n\n']"
How do I stop all spiders and the engine immediately after a condition in a pipeline is met?,"
We have a system written with scrapy to crawl a few websites. There are several spiders, and a few cascaded pipelines for all items passed by all crawlers.
One of the pipeline components queries the google servers for geocoding addresses.
Google imposes a limit of 2500 requests per day per IP address, and threatens to ban an IP address if it continues querying google even after google has responded with a warning message: 'OVER_QUERY_LIMIT'.
Hence I want to know about any mechanism which I can invoke from within the pipeline that will completely and immediately stop all further crawling/processing of all spiders and also the main engine.
I have checked other similar questions and their answers have not worked:

Force my scrapy spider to stop crawling


from scrapy.project import crawler
crawler._signal_shutdown(9,0) #Run this if the cnxn fails.


this does not work as it takes time for the spider to stop execution and hence many more requests are made to google (which could potentially ban my IP address)

import sys
sys.exit(""SHUT DOWN EVERYTHING!"")


this one doesn't work at all; items keep getting generated and passed to the pipeline, although the log vomits sys.exit() -> exceptions.SystemExit raised (to no effect)

How can I make scrapy crawl break and exit when encountering the first exception?


crawler.engine.close_spider(self, 'log message')


this one has the same problem as the first case mentioned above.
I tried:

scrapy.project.crawler.engine.stop()


To no avail
EDIT:
If I do in the pipeline:

from scrapy.contrib.closespider import CloseSpider

what should I pass as the 'crawler' argument to the CloseSpider's init() from the scope of my pipeline?
",9k,"
            13
        ","['\nYou can raise a CloseSpider exception to close down a spider.\nHowever, I don\'t think this will work from a pipeline. \nEDIT: avaleske notes in the comments to this answer that he was able to raise a CloseSpider exception from a pipeline. Most wise would be to use this.\nA similar situation has been described on the Scrapy Users group, in this thread.\nI quote:\n\nTo close an spider for any part of your code you should use \n  engine.close_spider  method. See this extension for an usage\n  example: \n  https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/closespider.py#L61\n\nYou could write your own extension, whilst looking at closespider.py as an example, which will shut down a spider if a certain condition has been met.\nAnother ""hack"" would be to set a flag on the spider in the pipeline. For example:\npipeline:\ndef process_item(self, item, spider):\n    if some_flag:\n        spider.close_down = True\n\nspider:\ndef parse(self, response):\n    if self.close_down:\n        raise CloseSpider(reason=\'API usage exceeded\')\n\n']"
How to print html source to console with phantomjs,"
I just downloaed and installed phantomjs on my machine. I copy and pasted the following script into a file called hello.js:
var page = require('webpage').create();
var url = 'https://www.google.com'

page.onLoadStarted = function () {
    console.log('Start loading...');
};

page.onLoadFinished = function (status) {
    console.log('Loading finished.');
phantom.exit();
};

page.open(url);

I'd like to print the complete html source (in this case from the google page) to a file or to the console. How do I do this?
",28k,"
            12
        ","[""\nSpent some time to read the documentation, it should be obvious afterwards.\nvar page = require('webpage').create();\npage.open('http://google.com', function () {\n    console.log(page.content);\n    phantom.exit();\n});\n\n""]"
Crawling the Google Play store,"
I want to crawl the Google Play store to download the web pages of all the android application (All the webpages with the following base url: https://play.google.com/store/apps/). I checked the robots.txt file of the play store and it disallows crawling these URLs. 
Also, when I browse the Google Play store I can only see top applications up to 3 pages for each of the categories. How can I get the other application pages?
If anyone has tried crawling the Google Play please let me know the following things:
a) Were you successful in crawling the play store. If yes, please let me know how you did that.
b) How to crawl the hidden application pages not visible in top apps for each of the categories?
c) Is there a techniques to download the applications also and not just the webpages?
I already searched around and found the following links:
a) https://code.google.com/p/android-market-api/ 
b) https://code.google.com/p/android-marketplace-crawler/source/checkout 
c) http://mohsin-junaid.blogspot.co.uk/2012/12/how-to-install-android-marketplace.html 
d) http://mohsin-junaid.blogspot.in/2012/12/how-to-download-multiple-android-apks.html

Thanks!
",15k,"
            11
        ","['\nFirst of all, Google Play\'s robots.txt does NOT disallow the pages with base ""/store/apps"".\nIf you want to crawl Google Play you would need to develop your own web crawler, parse the HTML page and extract the app meta-data you need (e.g. title, descriptions, price, etc). This topic has been covered in this other question. There are libraries helping with that, for instance:\n\nJava: https://jsoup.org\nPython: https://scrapy.org\n\nThe harder part is to ""find"" the app-pages to crawl. You could use 1) the Google Play Sitemap or 2) follow the app-links you find in every page you crawl as explained in the Link Extractor documentation (in case you plan to use Scrapy).\nAnother option is to use an open-source library based on ProtoBuf to fetch meta-data about an app, here the link to the project: https://code.google.com/archive/p/android-market-api.\nThis library fetches app meta-data from Google Play on behalf of a valid Google account, but also in this case you need a crawler to ""find"" which apps are available and schedule their meta-data retrieval. This other open-source project can help you with that: https://code.google.com/archive/p/android-marketplace-crawler.\nIf you don\'t want to implement all this by yourself, you could use a third-party managed service to access Android apps meta-data through a JSON-based API. For instance, 42matters.com (the company I work for) offers an API for both Android and iOS to retrieve apps\' meta-data, here more details:\nhttps://42matters.com/app-market-data\nIn order to get the Title, Icon, Description, Downloads for an app you can use the ""lookup"" endpoint as documented here:\nhttps://42matters.com/docs/app-market-data/android/apps/lookup\nThis is an example of the JSON response for the ""Angry Birds Space Premium"" app:\n{\n    ""package_name"": ""com.rovio.angrybirdsspace.premium"",\n    ""title"": ""Angry Birds Space Premium"",\n    ""description"": ""Play over 300 interstellar levels across 10 planets..."",\n    ""short_desc"": ""The #1 mobile game of all time blasts off into space!"",\n    ""rating"": 4.3046236038208,\n    ""category"": ""Arcade"",\n    ""cat_key"": ""GAME_ARCADE"",\n    ""cat_keys"": [\n        ""GAME_ARCADE"",\n        ""GAME"",\n        ""FAMILY_EDUCATION"",\n        ""FAMILY""\n    ],\n    ""price"": ""$1.15"",\n    ""downloads"": ""1,000,000 - 5,000,000"",\n    ""version"": ""2.2.1"",\n    ""content_rating"": ""Everyone"",\n    ""promo_video"": ""https://www.youtube.com/embed/g6AL9YqRHaI?ps=play&vq=large&rel=0&autohide=1&showinfo=0&autoplay=1"",\n    ""market_update"": ""2015-07-03T00:00:00+00:00"",\n    ""screenshots"": [\n        ""https://lh3.googleusercontent.com/ZmuBQzIy1G74coPrQ1R7fCeKdJmjTdpJhNrIHBOaFyM0N2EYdUPwZaQjnQUtiUDGmac=h310"",\n        ""https://lh3.googleusercontent.com/Xg2Aq70ZH0SnNhtSKH7xg9jCfisWgmmq3C7xQbx6YMhTVAIRqlRJeH8GYtjxapb_qR4=h310"",\n        ""https://lh3.googleusercontent.com/T4o5-2_UP82sj4fSSegbjrGmslNHlfvtEYuZacXMSOC55-7eyiKySw05lNF1QQGO2FeU=h310"",\n        ""https://lh3.googleusercontent.com/f2ennaLdivFu5cQQaVPKsRcWxB8FS5T4Bkoy3l0iPW9-GDDnTVRhvR5kz6l4m8FL1c8=h310"",\n        ""https://lh3.googleusercontent.com/H-9M03_-O9Df1nHr2-rUdjtk2aeBY3bAxnqSX3m2zh_aV8-K1t0qU1DxLXnK0GrDAw=h310""\n    ],\n    ""created"": ""2012-03-22T08:24:00+00:00"",\n    ""developer"": ""Rovio Entertainment Ltd."",\n    ""number_ratings"": 20812,\n    ""price_currency"": ""$"",\n    ""icon"": ""https://lh3.ggpht.com/aQaIEGrmba1ENSEgUtArdm3yhJUug7BRWlu_WaspoJusZyHv1rjlWtYqe_qRjE_Kmh1E=w300"",\n    ""icon_72"": ""https://lh3.ggpht.com/aQaIEGrmba1ENSEgUtArdm3yhJUug7BRWlu_WaspoJusZyHv1rjlWtYqe_qRjE_Kmh1E=w72"",\n    ""market_url"": ""https://play.google.com/store/apps/details?id=com.rovio.angrybirdsspace.premium&referrer=utm_source%3D42matters.com%26utm_medium%3Dapi""\n}\n\nI hope this helps, otherwise feel free to get in touch with me. I know this topic quite well and can point you in the right direction.\nRegards,\nAndrea\n', '\nI have did the job in Python before, what you need is a web auto test lib called selenium, it can execute Javascript code and return the result to Python, with Javascript, you can click the ""show more"" button by the program itself. And when you get all links for a single category page, you can get some info for the app. The simple demo here. Hope helpful.\n', '\nGoogle doesn\'t disallow crawling of /store/apps pages. \nThere is no mention about ""/store/apps"" in the robot.txt\nSee https://play.google.com/robots.txt\n']"
How to get a web page's source code from Java [duplicate],"






This question already has answers here:
                        
                    



How do you Programmatically Download a Webpage in Java

                                (11 answers)
                            

Closed 7 years ago.



I just want to retrieve any web page's source code from Java. I found lots of solutions so far, but I couldn't find any code that works for all the links below: 

http://www.cumhuriyet.com.tr?hn=298710
http://www.fotomac.com.tr/Yazarlar/Olcay%20%C3%87ak%C4%B1r/2011/11/23/hesap-makinesi 
http://www.sabah.com.tr/Gundem/2011/12/23/basbakan-konferansta-konusuyor#

The main problem for me is that some codes retrieve web page source code, but with missing ones. For example the code below does not work for the first link.
InputStream is = fURL.openStream(); //fURL can be one of the links above
BufferedReader buffer = null;
buffer = new BufferedReader(new InputStreamReader(is, ""iso-8859-9""));

int byteRead;
while ((byteRead = buffer.read()) != -1) {
    builder.append((char) byteRead);
}
buffer.close();
System.out.println(builder.toString());

",110k,"
            11
        ","['\nTry the following code with an added request property:\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.net.URL;\nimport java.net.URLConnection;\n\npublic class SocketConnection\n{\n    public static String getURLSource(String url) throws IOException\n    {\n        URL urlObject = new URL(url);\n        URLConnection urlConnection = urlObject.openConnection();\n        urlConnection.setRequestProperty(""User-Agent"", ""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.95 Safari/537.11"");\n\n        return toString(urlConnection.getInputStream());\n    }\n\n    private static String toString(InputStream inputStream) throws IOException\n    {\n        try (BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream, ""UTF-8"")))\n        {\n            String inputLine;\n            StringBuilder stringBuilder = new StringBuilder();\n            while ((inputLine = bufferedReader.readLine()) != null)\n            {\n                stringBuilder.append(inputLine);\n            }\n\n            return stringBuilder.toString();\n        }\n    }\n}\n\n', '\nURL yahoo = new URL(""http://www.yahoo.com/"");\nBufferedReader in = new BufferedReader(\n            new InputStreamReader(\n            yahoo.openStream()));\n\nString inputLine;\n\nwhile ((inputLine = in.readLine()) != null)\n    System.out.println(inputLine);\n\nin.close();\n\n', '\nI am sure that you have found a solution somewhere over the past 2 years but the following is a solution that works for your requested site\npackage javasandbox;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.MalformedURLException;\nimport java.net.URL;\n\n/**\n*\n* @author Ryan.Oglesby\n*/\npublic class JavaSandbox {\n\nprivate static String sURL;\n\n/**\n * @param args the command line arguments\n */\npublic static void main(String[] args) throws MalformedURLException, IOException {\n    sURL = ""http://www.cumhuriyet.com.tr/?hn=298710"";\n    System.out.println(sURL);\n    URL url = new URL(sURL);\n    HttpURLConnection httpCon = (HttpURLConnection) url.openConnection();\n    //set http request headers\n            httpCon.addRequestProperty(""Host"", ""www.cumhuriyet.com.tr"");\n            httpCon.addRequestProperty(""Connection"", ""keep-alive"");\n            httpCon.addRequestProperty(""Cache-Control"", ""max-age=0"");\n            httpCon.addRequestProperty(""Accept"", ""text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"");\n            httpCon.addRequestProperty(""User-Agent"", ""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36"");\n            httpCon.addRequestProperty(""Accept-Encoding"", ""gzip,deflate,sdch"");\n            httpCon.addRequestProperty(""Accept-Language"", ""en-US,en;q=0.8"");\n            //httpCon.addRequestProperty(""Cookie"", ""JSESSIONID=EC0F373FCC023CD3B8B9C1E2E2F7606C; lang=tr; __utma=169322547.1217782332.1386173665.1386173665.1386173665.1; __utmb=169322547.1.10.1386173665; __utmc=169322547; __utmz=169322547.1386173665.1.1.utmcsr=stackoverflow.com|utmccn=(referral)|utmcmd=referral|utmcct=/questions/8616781/how-to-get-a-web-pages-source-code-from-java; __gads=ID=3ab4e50d8713e391:T=1386173664:S=ALNI_Mb8N_wW0xS_wRa68vhR0gTRl8MwFA; scrElm=body"");\n            HttpURLConnection.setFollowRedirects(false);\n            httpCon.setInstanceFollowRedirects(false);\n            httpCon.setDoOutput(true);\n            httpCon.setUseCaches(true);\n\n            httpCon.setRequestMethod(""GET"");\n\n            BufferedReader in = new BufferedReader(new InputStreamReader(httpCon.getInputStream(), ""UTF-8""));\n            String inputLine;\n            StringBuilder a = new StringBuilder();\n            while ((inputLine = in.readLine()) != null)\n                a.append(inputLine);\n            in.close();\n\n            System.out.println(a.toString());\n\n            httpCon.disconnect();\n}\n}\n\n']"
crawling a html page using php?,"
This website lists over 250 courses in one list. I want to get the name of each course and insert that into my mysql database using php. The courses are listed like this:
<td> computer science</td>
<td> media studeies</td>
…

Is there a way to do that in PHP, instead of me  having a mad data entry nightmare?
",4k,"
            4
        ","['\nRegular expressions work well.\n$page = // get the page\n$page = preg_split(""/\\n/"", $page);\nfor ($text in $page) {\n    $matches = array();\n    preg_match(""/^<td>(.*)<\\/td>$/"", $text, $matches);\n    // insert $matches[1] into the database\n}\n\nSee the documentation for preg_match.\n', ""\nHow to parse HTML has been asked and answered countless times before. While (for your specific UseCase) Regular Expressions will work, it is - in general - better and more reliable to use a proper parser for this task. Below is how to do it with DOM:\n$dom = new DOMDocument;\n$dom->loadHTMLFile('http://courses.westminster.ac.uk/CourseList.aspx');\nforeach($dom->getElementsByTagName('td') as $title) {\n    echo $title->nodeValue;\n}\n\nFor inserting the data into MySql, you should use the mysqli extension. Examples are plentiful on StackOverflow. so please use the search function.\n"", '\nYou can use this HTML parsing php library to achieve this :http://simplehtmldom.sourceforge.net/\n', '\nI encountered the same problem.\nHere is a good class library called the html dom\nhttp://simplehtmldom.sourceforge.net/.\nThis like jquery\n', '\nJust for fun, here\'s a quick shell script to do the same thing.\ncurl http://courses.westminster.ac.uk/CourseList.aspx \\\n| sed \'/<td>\\(.*\\)<\\/td>/ { s/.*"">\\(.*\\)<\\/a>.*/\\1/; b }; d;\' \\\n| uniq > courses.txt\n\n']"
Scrapy Linkextractor duplicating(?),"
I have the crawler implemented as below.
It is working and it would go through sites regulated under the link extractor.
Basically what I am trying to do is to extract information from different places in the page:
- href and text() under the class 'news' ( if exists)
- image url under the class 'think block' ( if exists)
I have three problems for my scrapy:
1) duplicating linkextractor
It seems that it will duplicate processed page.  ( I check against the export file and found that the same ~.img appeared many times while it is hardly possible)
And the fact is , for every page in the website, there are hyperlinks at the bottom that facilitate users to direct to the topic they are interested in, while my objective is to extract information from the topic's page ( here listed several passages's title under the same topic ) and the images found within a passage's page( you can arrive to the passage's page by clicking on the passage's title found at topic page).
I suspect link extractor would loop the same page over again in this case.
( maybe solve with depth_limit?)
2) Improving parse_item
I think it is quite not efficient for parse_item. How could I improve it? I need to extract information from different places in the web ( for sure it only extracts if  it exists).Beside, it looks like that the parse_item could only progress HkejImage but not HkejItem (again I checked with the output file). How should I tackle this?
3) I need the spiders to be able to read Chinese.
I am crawling a site in HK and it would be essential to be capable to read Chinese.
The site:

http://www1.hkej.com/dailynews/headline/article/1105148/IMF%E5%82%B3%E4%BF%83%E4%B8%AD%E5%9C%8B%E9%80%80%E5%87%BA%E6%95%91%E5%B8%82

As long as it belongs to 'dailynews', that's the thing I want.
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.selector import Selector
from scrapy.http import Request, FormRequest
from scrapy.contrib.linkextractors import LinkExtractor
import items


class EconjournalSpider(CrawlSpider):
    name = ""econJournal""
    allowed_domains = [""hkej.com""]
    login_page = 'http://www.hkej.com/template/registration/jsp/login.jsp'
    start_urls =  'http://www.hkej.com/dailynews'

    rules=(Rule(LinkExtractor(allow=('dailynews', ),unique=True), callback='parse_item', follow =True),
           )


    def start_requests(self):
         yield Request(
         url=self.login_page,
         callback=self.login,
         dont_filter=True
         )
# name column
    def login(self, response):
        return FormRequest.from_response(response,
                    formdata={'name': 'users', 'password': 'my password'},
                    callback=self.check_login_response)

    def check_login_response(self, response):
        """"""Check the response returned by a login request to see if we are
        successfully logged in.
        """"""
        if ""username"" in response.body:       
            self.log(""\n\n\nSuccessfully logged in. Let's start crawling!\n\n\n"")
            return Request(url=self.start_urls)
        else:
            self.log(""\n\n\nYou are not logged in.\n\n\n"")
            # Something went wrong, we couldn't log in, so nothing happens

    def parse_item(self, response):
        hxs = Selector(response)
        news=hxs.xpath(""//div[@class='news']"")
        images=hxs.xpath('//p')

        for image in images:
            allimages=items.HKejImage()
            allimages['image'] = image.xpath('a/img[not(@data-original)]/@src').extract()
            yield allimages

        for new in news:
            allnews = items.HKejItem()
            allnews['news_title']=new.xpath('h2/@text()').extract()
            allnews['news_url'] = new.xpath('h2/@href').extract()
            yield allnews

Thank you very much and I would appreciate any help!
",2k,"
            3
        ","[""\nFirst, to set settings, make it on the settings.py file or you can specify the custom_settings parameter on the spider, like:\ncustom_settings = {\n    'DEPTH_LIMIT': 3,\n}\n\nThen, you have to make sure the spider is reaching the parse_item method (which I think it doesn't, haven't tested yet). And also you can't specify the callback and follow parameters on a rule, because they don't work together.\nFirst remove the follow on your rule, or add another rule, to check which links to follow, and which links to return as items.\nSecond on your parse_item method, you are getting incorrect xpath, to get all the images, maybe you could use something like:\nimages=hxs.xpath('//img')\n\nand then to get the image url:\nallimages['image'] = image.xpath('./@src').extract()\n\nfor the news, it looks like this could work:\nallnews['news_title']=new.xpath('.//a/text()').extract()\nallnews['news_url'] = new.xpath('.//a/@href').extract()\n\nNow, as and understand your problem, this isn't a Linkextractor duplicating error, but only poor rules specifications, also make sure you have valid xpath, because your question didn't indicate you needed xpath correction.\n""]"
HtmlAgilityPack HtmlWeb.Load returning empty Document,"
I have been using HtmlAgilityPack for the last 2 months in a Web Crawler Application with no issues loading a webpage.
Now when I try to load a this particular webpage, the document OuterHtml is empty, so this test fails
var url = ""http://www.prettygreen.com/"";
var htmlWeb = new HtmlWeb();
var htmlDoc = htmlWeb.Load(url);
var outerHtml = htmlDoc.DocumentNode.OuterHtml;
Assert.AreNotEqual("""", pageHtml);

I can load another page from the site with no problems, such as setting
url = ""http://www.prettygreen.com/news/"";

In the past I once had an issue with encodings, I played around with htmlWeb.OverrideEncoding and htmlWeb.AutoDetectEncoding with no luck.  I have no idea what could be the issue here with this webpage.
",12k,"
            1
        ","['\nIt seems this website requires cookies to be enabled. So creating a cookie container for your web request should solve the issue:\nvar url = ""http://www.prettygreen.com/"";\nvar htmlWeb = new HtmlWeb();\nhtmlWeb.PreRequest += request =>\n    {\n        request.CookieContainer = new System.Net.CookieContainer();\n        return true;\n    };\nvar htmlDoc = htmlWeb.Load(url);\nvar outerHtml = htmlDoc.DocumentNode.OuterHtml;\nAssert.AreNotEqual("""", outerHtml);\n\n']"
Hide Email Address from Bots - Keep mailto:,"
tl;dr
Hide email address from bots without using scripts and maintain mailto: functionality. Method must also support screen-readers.

Summary

Email obfuscation without using scripts or contact forms

Email address needs to be completely visible to human viewers and maintain mailto: functionality

Email Address must not be in image form.

Email address must be ""completely"" hidden from spam-crawlers and spam-bots and any other harvester type



Desired Effect:

No scripts, please. There are no scripts used in the project and I'd like to keep it that way.

Email address is either displayed on the page or can be easily displayed after some sort of user interaction, like opening a modal.

The user can click on on the email address which in turn would trigger the mailto: functionality.

Clicking the email will open the user's email application.
In other words, mailto: functionality must work.

The email address in not visible or not identified as an email address to bots (This includes the page source)

I don't have an inbox that's full of spam



What does NOT Work

Adding a contact form - or anything similar - instead of the email address

I hate contact forms. I rarely fill up a contact form. If there's no email address, I look for a phone number, and if that's not there, I start looking for an alternative service. I would only fill up a contact form if I absolutely have to.

Replacing the address with an image of the address

This creates a HUGE disadvantage to someone using a screenreader (please remember the visually impaired in your future projects)
It also removes the mailto: functionality unless you make the image clickable and then add the mailto: functionality as the href for the link, but that defeats the purpose and now the email is visible to bots.

What might work:

Clever usage of pseudo-elements in CSS

Solutions that make use of base64 encoding

Breaking up the email address and spreading the parts across the document then putting them back together in a modal when the user clicks a button (This will probably involve multiple CSS classes and the usage of anchor tags)

Alterting html attributes via CSS


@MortezaAsadi gracefully brought up the possibility in the comments below. This is the link to the full - The article is from 2012:
What if We Could Use CSS to Alter HTML Attributes?

Other creative solutions that are beyond my scope of knowledge.


Similar Questions / Fixes

JavaScript: Protect your email address by Joe Maller

(This a great fix suggested by Joe Maller, it works well but it's script based. Here's what it looks like;


<SCRIPT TYPE=""text/javascript"">

  emailE = 'example.com'

  emailE = ('yourname' + '@' + emailE)

  document.write('<A href=""mailto:' + emailE + '"">' + emailE + '</a>')

</script>

<NOSCRIPT>

  Email address protected by JavaScript

</NOSCRIPT>



Looking for a PHP only email address obfuscator function
(A Clever solution using both PHP and CSS to first reverse the email using PHP then reverse it back with CSS) A very promising solution that Works great! But it's too easy to solve.

Is it worth obfuscating email addresses on the web these days?


(JavaScript fix)

Best way to obfuscate an e-mail address on a website?

The selected answer works. It actually works really well. It involves encoding the email as html entities. Can it be improved?
Here's what it looks like;


<A HREF=""mailto:

&#121;&#111;&#117;&#114;&#110;&#097;&#109;&#101;&#064;&#100;&#111;&#109;&#097;&#105;&#110;&#046;&#099;&#111;&#109;"">

&#121;&#111;&#117;&#114;&#110;&#097;&#109;&#101;&#064;&#100;&#111;&#109;&#097;&#105;&#110;&#046;&#099;&#111;&#109;

</A>



Does e-mail address obfuscation actually work?

(The selected answer to this SuperUser question is great and it presents a study of the amount of spam received by using different obfuscation methods.
It seems that manipulating the email address with CSS to make it rtl does work. This is the same method used in the first question I linked to in this section.
I am uncertain what effects adding mailto: functionality to the fix would have on the results.

There are also many other questions on SO which all have similar answers. I have not found anything that fits my desired effect


The Question:
Would it be possible to increase the efficiency (ie as little spam as possible) of the email obfuscation methods above by combining two or more of the fixes (or even adding new fixes) while:
A- Maintaining mailto: functionality; and
B- Supporting screen-readers

Many of the answers and comments below pose a very good question while indicating the impossibility of doing this without some sort of js
The question that's asked/implied is:

Why not use js?

The answer is that I am allergic to js
Joking aside though,
The three main reasons I asked this question are:

Contact forms are becoming more and more accepted as a replacement
for providing an email address - which they should not.

If it can be done without scripting then it should be done without
scripting.

Curiosity: (as I am in fact using one of the js fixes currently) I wanted to see if discussing the matter would lead to a better way of doing it.


",66k,"
            105
        ","['\nThe issue with your request is specifically the ""Supporting screen-readers"", as by definition screen readers are a ""bot"" of some sort. If a screen-reader needs to be able to interpret the email address, then a page-crawler would be able to interpret it as well.\nAlso, the point of the mailto attribute is to be the standard of how to do email addresses on the web. Asking if there is a second way to do that is sort of asking if there is a second standard.\nDoing it through scripts will still have the same issue as once the page is loaded, the script would have been run and the email address rendered in the DOM (unless you populate the email address on click or something). Either way, screen readers will still have issues with this since it\'s not already loaded.\nHonestly, just get an email service with a half decent spam filter and specify a default subject line that is easy for you to sort in your inbox.\n<a href=""mailto:no-one@example.com?subject=Something to filter on"">Email me</a>\n\nWhat you\'re asking for is if the standard has two ways to do something, one for bots and the other for non-bots. The answer is it doesn\'t, and you have to just fight the bots as best you can.\n', '\nDefeating email bots is a tough one. You may want to check out the Email Address Harvesting countermeasures section on Wikipedia.\nMy back-story is that I\'ve written a search bot. It crawled 105,000+ URLs during it\'s initial run many years ago. From what I\'ve learned from doing that is that web crawling bots literally see EVERYTHING that is text, which appears on a web page. Bots read everything except images.\nSpam can\'t be easily stopped via code for these reasons:\n\nCSS & JS are irrelevant when using the mailto: tag. Bots specifically look at HTML pages for that ""mailto:"" keyword. Everything from that colon to the next single quote or double quote (whichever comes first) is seen as an email address. HTML entity email addresses - like the example above - can be quickly translated using a reverse ASCII method/function. Running the JavaScript code snippet above, quickly turns the string which starts with: &#121;&#111;&#117;&#114;... into... yourname@example.com. (My search bot threw away hrefs with mailto:email addresses, as I wanted URLs for web pages & not email addresses.)\n\nIf a page crashes a bot, the bot author will tune the bot to fix the crash with that page in mind, so that the bot won\'t crash at that page again in the future. Thus making their bot smarter.\n\nBot authors can write bots, which generate all known variations of email addresses... without crawling pages & never using any starter email addresses. While it may not be feasible to do that, it\'s not inconceivable with today\'s high-core count CPUs (which are hyper-threaded & run at 4+ GHz), plus the availability of using distributed cloud-based computing & even super computers. It\'s conceivable that someone can now create a bot-farm to spam everyone, without knowing anyone\'s email address. 20 years ago, that would have been incomprehensible.\n\nFree email providers have had a history of selling their free user accounts to their advertisers. In the past, simply signing up for a free email account automatically guaranteed them a green light to start delivering spam to that email address... without ever using that email address online. I\'ve seen that happen multiple times, with famous company names. (I won\'t mention any names.)\n\nThe mailto: keyword is part of this IETF RFC, where browsers are built to automatically launch the default email clients, from links with that keyword in them. JavaScript has to be used to interrupt that application launching process, when it happens.\n\n\nI don\'t think it\'s possible to stop 100% of spam while using traditional email servers, without using filters on the email server and possibly using images.\nThere is one alternative... You can also build a chat-like email client, which runs internally on a website. It would be like Facebook\'s chat client. It\'s ""kind of like email"", but not really email. It\'s simply 1-to-1 instant messaging with an archiving feature... that auto-loads upon login. Since it has document attachment + link features, it works kind of like email... but without the spam. As long as you don\'t build an externally accessible API, then it\'s a closed system where people can\'t send spam into it.\nIf you\'re planning to stick with strictly traditional email, then your best bet may be to run something like Apache\'s SpamAssassin on a company\'s email server.\nYou can also try combining multiple strategies as you\'ve listed above, to make it harder for email harvesters to glean email addresses from your web pages. They won\'t stop 100% of the spam, 100% of the time... while also allowing 100% of the screen readers to work for blind visitors.\nYou\'ve created a really good starting look at what\'s wrong with traditional email! Kudos to you for that!\nA good screen reader is JAWS from Freedom Scientific. I\'ve used that before to listen to how my webpages are read by blind users. (If you hear a male voice reading both actions [like clicking on a link] & text, try changing 1 voice to female so that 1 voice reads actions & another reads text. That makes it easier to hear how the web page is read for the visually impared.)\nGood luck with your Email Address Harvesting countermeasure endeavours!\n', '\nHere is an approach that does make use of JavaScript, but with a rather small foot-print. It\'s also very ""ghetto"", and generally I would not recommend an approach with inline JS in the HTML except you have an extreme reluctance to use JS, at all.\n\n\n<a\n  href=""#""\n  data-contact=""bGUtZW1haWxAdGhlLWRvbWFpbi5jb20=""\n  data-subj=""QW4gQW1hemluZyBTdWJqZWN0""\n  onfocus=""this.href = \'mailto:\' + atob(this.dataset.contact) + \'?subject=\' + atob(this.dataset.subj || \'\')""\n  >\n  Send an email\n</a>\n\n\ndata-contact is the base64 encoded email address. And, data-subj is an optional base64 encoded subject.\nThe main challenge with doing this without JS is that CSS can\'t alter HTML attributes. (The article you linked is a ""pie-in-the-sky"" musing and does not have any bearing on what is possible today or in the near future.)\nThe HTML entities approach you mentioned, or some variation of it, is likely the simplest option that will have some efficacy. Additionally, the iframe approach is clever and the server redirect approach is pretty awesome. But, all three are vulnerable to bots:\n\nThe HTML entities just need to be converted (and detecting that is simple)\nThe document referenced by the iframe might simply be followed\nThe server redirect might simply be followed, as well\n\nWith the approach outlined above, the use of a base64 encoded email address in a data-contact attribute is very ""one-off"" – as long as the scraper is not specifically designed for your site, it should work.\n', '\nSimple + Lot of @ + Editable without tools\n\n\n<a href=""mailto:user@domain@@com""\r\n   onmouseover=""this.href=this.href.replace(\'@@\',\'.\')"">\r\n   Send email\r\n</a>\n\n\n', '\nHave you considered using google\'s recaptcha mailhide?\nhttps://www.google.com/recaptcha/admin#mailhide\nThe idea is that when a user clicks the checkbox (see nocaptcha below), the full e-mail address is displayed.\nWhile recaptcha is traditionally not only hard for screen readers but also humans as well, with the roleout of google\'s nocaptcha recaptcha which you can read about\nhere as they relate to accessibility tests. It appears to show promise with to screen readers as it renders as a traditional checkbox from their view.\n\nExample #1 - Not secure but for easy illustration of the idea\nHere is some code as an example without using mailhide but implementing something using recaptcha yourself: https://jsfiddle.net/43fad8pf/36/\n<div class=""container"">\n    <div id=""recaptcha""></div>\n</div>\n<div id=""email"">\n    Verify captcha to get e-mail\n</div>\n\nfunction createRecaptcha() {\n    grecaptcha.render(""recaptcha"", {sitekey: ""6LcgSAMTAAAAACc2C7rc6HB9ZmEX4SyB0bbAJvTG"", theme: ""light"", callback: showEmail});\n}\n createRecaptcha();\n\nfunction showEmail() {\n    // ideally you would do server side verification of the captcha and then the server would return the e-mail\n  document.getElementById(""email"").innerHTML = ""email@example.com"";\n}\n\nNote: In my example I have the e-mail in a JavaScript function. Ideally you would have the recaptcha validated on the server end, and return the e-mail, otherwise the bot can simply get it in the code.\nExample #2 - Server side validation and returning of e-mail\nIf we use an example more like this, we get additional security: https://designracy.com/recaptcha-using-ajax-php-and-jquery/\nfunction showEmail() {\n    /* Check if the captcha is complete */\n    if ($(""#g-recaptcha-response"").val()) {\n        $.ajax({\n            type: ‘POST’,\n            url: ""verify.php"", // The file we’re making the request to\n            dataType: ‘html’,\n            async: true,\n            data: {\n                captchaResponse: $(""#g-recaptcha-response"").val() // The generated response from the widget sent as a POST parameter\n        },\n        success: function (data) {\n            alert(""everything looks ok. Here is where we would take \'data\' which contains the e-mail and put it somewhere in the document"");\n        },\n        error: function (XMLHttpRequest, textStatus, errorThrown) {\n            alert(""You’re a bot"");\n        }\n    });\n} else {\n    alert(""Please fill the captcha!"");\n}\n});\n\nWhere verify.php is:\n$captcha = filter_input(INPUT_POST, ‘captchaResponse’); // get the captchaResponse parameter sent from our ajax\n\n/* Check if captcha is filled */\nif (!$captcha) {\n    http_response_code(401); // Return error code if there is no captcha\n}\n$response =     file_get_contents(""https://www.google.com/recaptcha/api/siteverify?secret=YOUR-SECRET-KEY-HERE&amp;amp;response="" . $captcha);\nif ($response . success == false) {\necho ‘SPAM’;\nhttp_response_code(401); // It’s SPAM! RETURN SOME KIND OF ERROR\n} else {\n// Everything is ok, should output this in json or something better, but this is an example\n    echo \'email@example.com\';\n}\n\n', '\nPeople who write scrapers want to make their scrapers as efficient as possible. Therefore, they won\'t download styles, scripts, and other external resources. There\'s no method that I know of to set a mailto link using CSS. In addition, you specifically said you didn\'t want to set the link using Javascript.\nIf you think about what other types of resources there are, there\'s also external documents (i.e. HTML documents using iframes). Almost no scrapers would bother downloading the contents of iframes. Therefore, you can simply do:\nindex.html:\n<iframe src=""frame.html"" style=""height: 1em; width: 100%; border: 0;""></iframe>\n\nframe.html:\nMy email is <a href=""mailto:me@example.com"" target=""_top"">me@example.com</a>\n\nTo human users, the iframe looks just like normal text. Iframes are inline and transparent by default, so we just need set its border and dimensions. You can\'t make the size of the iframe match its content\'s size without using Javascript, so the best we can do is giving it predefined dimensions.\n', ""\nFirst, I don't think doing anything with CSS will work. All bots (except Google's crawler) simply ignore all styling on websites. Any solution has to work with JS or server-side.\nA server-side solution could be making an <a> that links to a new tab, which simply redirects to the desired mailto:\nThat's all my ideas for now. Hope it helps.\n"", ""\nShort answer to fulfill all your requirements is that it's impossible\nSome of the script-based options answered here may work for certain bots, but you wanted no-script, so, no, you can't. \n"", '\nbased on the code of MaanooAk, here is my version:\n\n\n<a href=""mailto: Mike Myers""\nonclick=""this.href=this.href.replace(\' Mike \',\'MikeMy\'); this.href=this.href.replace(\'Myers\',\'ers@vwx.yz\')"">&#9993; Send Email</a>\n\n\nThe difference to MaanookAks version is, that on hover you don\'t see mailto: and a broken email adress but mailto: and the name of contact. And when you click on it, the name is replaced by the email adress.\nIn the code the email adress is splitted into two parts. Nowhere in the code the email adress is visible complete.\n', ""\nHere is my new solution for this. I first build the email adress string by addition of small pieces and then use this string also as title:\n\n\nadress = 'mailt' + 'o:MikeM' + 'yers@v' + 'wx.yz';\ndocument.getElementsByClassName('Email')[0].title = adress;\nfunction mail(){window.location.href = adress;}\n<a class='Email' onclick='mail()'>&#9993; Send Email</a>\n\n\nI use this in a footer of a website. Many pages with all the same footer.\n"", '\nPHP solution\nfunction printEmail($email){\n    $email = \'<a href=""mailto:\'.$email.\'"">\'.$email.\'</a>\';\n    $a = str_split($email);\n    return ""<script>document.write(\'"".implode(""\'+\'"",$a).""\');</script>"";\n}\n\nUse\necho printEmail(\'test@example.com\');\n\nResult\n<script>document.write(\'<\'+\'a\'+\' \'+\'h\'+\'r\'+\'e\'+\'f\'+\'=\'+\'""\'+\'m\'+\'a\'+\'i\'+\'l\'+\'t\'+\'o\'+\':\'+\'t\'+\'e\'+\'s\'+\'t\'+\'@\'+\'g\'+\'m\'+\'a\'+\'i\'+\'l\'+\'.\'+\'c\'+\'o\'+\'m\'+\'""\'+\'>\'+\'t\'+\'e\'+\'s\'+\'t\'+\'@\'+\'g\'+\'m\'+\'a\'+\'i\'+\'l\'+\'.\'+\'c\'+\'o\'+\'m\'+\'<\'+\'/\'+\'a\'+\'>\');</script>\n\nP.S. Requirement: user must have JavaScript enabled\n', '\nThe one method I found effective is using it with CSS like below:\n<a href=""mailto:myemail@ignore-domain.com"">myemail@<span style=""display:none;"">ignore-</span>example.com\nand then write a JavaScript to remove the ignoreme- word from the href=""mailto:..."" attribute with regex. This will hide email from bot as it will append ignore- word before real domain and this will work on screen reader and when user clicks on the link custom JS function will remove the ignore- word from href attribute so it will open the real email.\nThis method has been working very effectively for me till date. you can read more on this - http://techblog.tilllate.com/2008/07/20/ten-methods-to-obfuscate-e-mail-addresses-compared/\n']"
Designing a web crawler,"
I have come across an interview question ""If you were designing a web crawler, how would you avoid getting into infinite loops? "" and I am trying to answer it.
How does it all begin from the beginning.
Say Google started with some hub pages say hundreds of them (How these hub pages were found in the first place is a different sub-question).
As Google follows links from a page and so on, does it keep making  a hash table to make sure that it doesn't follow the earlier visited pages.
What if the same page has 2 names (URLs) say in these days when we have URL shorteners etc..
I have taken Google as an example. Though Google doesn't leak how its web crawler algorithms and page ranking etc work, but any guesses?
",46k,"
            74
        ","['\nIf you want to get a detailed answer take a look at section 3.8 this paper, which describes the URL-seen test of a modern scraper:\n\nIn the course of extracting links, any\nWeb crawler will encounter multiple\nlinks to the same document. To avoid\ndownloading and processing a document\nmultiple times, a URL-seen test must\nbe performed on each extracted link\nbefore adding it to the URL frontier.\n(An alternative design would be to\ninstead perform the URL-seen test when\nthe URL is removed from the frontier,\nbut this approach would result in a\nmuch larger frontier.)\nTo perform the\nURL-seen test, we store all of the\nURLs seen by Mercator in canonical\nform in a large table called the URL\nset. Again, there are too many entries\nfor them all to fit in memory, so like\nthe document fingerprint set, the URL\nset is stored mostly on disk.\nTo save\nspace, we do not store the textual\nrepresentation of each URL in the URL\nset, but rather a fixed-sized\nchecksum. Unlike the fingerprints\npresented to the content-seen test’s\ndocument fingerprint set, the stream\nof URLs tested against the URL set has\na non-trivial amount of locality. To\nreduce the number of operations on the\nbacking disk file, we therefore keep\nan in-memory cache of popular URLs.\nThe intuition for this cache is that\nlinks to some URLs are quite common,\nso caching the popular ones in memory\nwill lead to a high in-memory hit\nrate.\nIn fact, using an in-memory\ncache of 2^18 entries and the LRU-like\nclock replacement policy, we achieve\nan overall hit rate on the in-memory\ncache of 66.2%, and a hit rate of 9.5%\non the table of recently-added URLs,\nfor a net hit rate of 75.7%. Moreover,\nof the 24.3% of requests that miss in\nboth the cache of popular URLs and the\ntable of recently-added URLs, about\n1=3 produce hits on the buffer in our\nrandom access file implementation,\nwhich also resides in user-space. The\nnet result of all this buffering is\nthat each membership test we perform\non the URL set results in an average\nof 0.16 seek and 0.17 read kernel\ncalls (some fraction of which are\nserved out of the kernel’s file system\nbuffers). So, each URL set membership\ntest induces one-sixth as many kernel\ncalls as a membership test on the\ndocument fingerprint set. These\nsavings are purely due to the amount\nof URL locality (i.e., repetition of\npopular URLs) inherent in the stream\nof URLs encountered during a crawl.\n\nBasically they hash all of the URLs with a hashing function that guarantees unique hashes for each URL and due to the locality of URLs, it becomes very easy to find URLs. Google even open-sourced their hashing function: CityHash\nWARNING!\nThey might also be talking about bot traps!!! A bot trap is a section of a page that keeps generating new links with unique URLs and you will essentially get trapped in an ""infinite loop"" by following the links that are being served by that page. This is not exactly a loop, because a loop would be the result of visiting the same URL, but it\'s an infinite chain of URLs which you should avoid crawling.\nUpdate 12/13/2012- the day after the world was supposed to end :)\nPer Fr0zenFyr\'s comment: if one uses the AOPIC algorithm for selecting pages, then it\'s fairly easy to avoid bot-traps of the infinite loop kind. Here is a summary of how AOPIC works:\n\nGet a set of N seed pages.\nAllocate X amount of credit to each page, such that each page has X/N credit (i.e. equal amount of credit) before crawling has started.\nSelect a page P, where the P has the highest amount of credit (or if all pages have the same amount of credit, then crawl a random page).\nCrawl page P (let\'s say that P had 100 credits when it was crawled).\nExtract all the links from page P (let\'s say there are 10 of them).\nSet the credits of P to 0.\nTake a 10% ""tax"" and allocate it to a Lambda page.\nAllocate an equal amount of credits each link found on page P from P\'s original credit - the tax: so (100 (P credits) - 10 (10% tax))/10 (links) = 9 credits per each link.\nRepeat from step 3.\n\nSince the Lambda page continuously collects tax, eventually it will be the page with the largest amount of credit and we\'ll have to ""crawl"" it. I say ""crawl"" in quotes, because we don\'t actually make an HTTP request for the Lambda page, we just take its credits and distribute them equally to all of the pages in our database.\nSince bot traps only give internal links credits and they rarely get credit from the outside, they will continually leak credits (from taxation) to the Lambda page. The Lambda page will distribute that credits out to all of the pages in the database evenly and upon each cycle the bot trap page will lose more and more credits, until it has so little credits that it almost never gets crawled again. This will not happen with good pages, because they often get credits from back-links found on other pages. This also results in a dynamic page rank and what you will notice is that any time you take a snapshot of your database, order the pages by the amount of credits they have, then they will most likely be ordered roughly according to their true page rank.\nThis only avoid bot traps of the infinite-loop kind, but there are many other bot traps which you should watch out for and there are ways to get around them too.\n', ""\nWhile everybody here already suggested how to create your web crawler, here is how how Google ranks pages.\nGoogle gives each page a rank based on the number of callback links (how many links on other websites point to a specific website/page). This is called relevance score. This is based on the fact that if a page has many other pages link to it, it's probably an important page.\nEach site/page is viewed as a node in a graph. Links to other pages are directed edges. A degree of a vertex is defined as the number of incoming edges. Nodes with a higher number of incoming edges are ranked higher.\nHere's how the PageRank is determined. Suppose that page Pj has Lj links. If one of those links is to page Pi, then Pj will pass on 1/Lj of its importance to Pi. The importance ranking of Pi is then the sum of all the contributions made by pages linking to it. So if we denote the set of pages linking to Pi by Bi, then we have this formula:\nImportance(Pi)= sum( Importance(Pj)/Lj ) for all links from Pi to Bi\n\nThe ranks are placed in a matrix called hyperlink matrix: H[i,j]\nA row in this matrix is either 0, or 1/Lj if there is a link from Pi to Bi. Another property of this matrix is that if we sum all rows in a column we get 1. \nNow we need multiply this matrix by an Eigen vector, named I (with eigen value 1) such that:\nI = H*I\n\nNow we start iterating: IH, IIH, IIIH .... I^k *H until the solution converges. ie we get pretty much the same numbers in the matrix in step k and k+1.\nNow whatever is left in the I vector is the importance of each page.\nFor a simple class homework example see http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html\n\nAs for solving the duplicate issue in your interview question, do a checksum on the entire page and use either that or a bash of the checksum as your key in a map to keep track of visited pages.\n\n"", ""\nDepends on how deep their question was intended to be.  If they were just trying to avoid following the same links back and forth, then hashing the URL's would be sufficient.\nWhat about content that has literally thousands of URL's that lead to the same content?  Like a QueryString parameter that doesn't affect anything, but can have an infinite number of iterations.  I suppose you could hash the contents of the page as well and compare URL's to see if they are similar to catch content that is identified by multiple URL's.  See for example, Bot Traps mentioned in @Lirik's post.\n"", ""\nYou'd have to have some sort of hash table to store the results in, you'd just have to check it before each page load.   \n"", '\nThe problem here is not to crawl duplicated URLS, wich is resolved by a index using a hash obtained from urls.  The problem is to crawl DUPLICATED CONTENT.  Each url of a ""Crawler Trap"" is different (year, day, SessionID...).\nThere is not a ""perfect"" solution... but you can use some of this strategies:\n• Keep a field of wich level the url is inside the website. For each cicle of getting urls from a page, increase the level. It will be like a tree.  You can stop to crawl at certain level, like 10 (i think google use this). \n• You can try to create a kind of HASH wich can be compared to find similar documents, since you cant compare with each document in your database. There are SimHash from google, but i could not find any implementation to use. Then i´ve created my own. My hash count low and high frequency characters inside the html code and generate a 20bytes hash, wich is compared with a small cache of last crawled pages inside a AVLTree with an NearNeighbors search with some tolerance (about 2). You cant use any reference to characters locations in this hash. After ""recognize"" the trap, you can record the url pattern of the duplicate content and start to ignore pages with that too.\n• Like google, you can create a ranking to each website and ""trust"" more in one than others.\n', '\nThe web crawler is a computer program which used to collect/crawling following key values(HREF links, Image links, Meta Data .etc) from given website URL. It is designed like intelligent to follow different HREF links which are already fetched from the previous URL, so in this way, Crawler can jump from one website to other websites. Usually, it called as a Web spider or Web Bot. This mechanism always acts as a backbone of the Web search engine.\nPlease find the source code from my tech blog - http://www.algonuts.info/how-to-built-a-simple-web-crawler-in-php.html\n<?php\nclass webCrawler\n{\n    public $siteURL;\n    public $error;\n\n    function __construct()\n    {\n        $this->siteURL = """";\n        $this->error = """";\n    }\n\n    function parser()   \n    {\n        global $hrefTag,$hrefTagCountStart,$hrefTagCountFinal,$hrefTagLengthStart,$hrefTagLengthFinal,$hrefTagPointer;\n        global $imgTag,$imgTagCountStart,$imgTagCountFinal,$imgTagLengthStart,$imgTagLengthFinal,$imgTagPointer;\n        global $Url_Extensions,$Document_Extensions,$Image_Extensions,$crawlOptions;\n\n        $dotCount = 0;\n        $slashCount = 0;\n        $singleSlashCount = 0;\n        $doubleSlashCount = 0;\n        $parentDirectoryCount = 0;\n\n        $linkBuffer = array();\n\n        if(($url = trim($this->siteURL)) != """")\n        {\n            $crawlURL = rtrim($url,""/"");\n            if(($directoryURL = dirname($crawlURL)) == ""http:"")\n            {   $directoryURL = $crawlURL;  }\n            $urlParser = preg_split(""/\\//"",$crawlURL);\n\n            //-- Curl Start --\n            $curlObject = curl_init($crawlURL);\n            curl_setopt_array($curlObject,$crawlOptions);\n            $webPageContent = curl_exec($curlObject);\n            $errorNumber = curl_errno($curlObject);\n            curl_close($curlObject);\n            //-- Curl End --\n\n            if($errorNumber == 0)\n            {\n                $webPageCounter = 0;\n                $webPageLength = strlen($webPageContent);\n                while($webPageCounter < $webPageLength)\n                {\n                    $character = $webPageContent[$webPageCounter];\n                    if($character == """")\n                    {   \n                        $webPageCounter++;  \n                        continue;\n                    }\n                    $character = strtolower($character);\n                    //-- Href Filter Start --\n                    if($hrefTagPointer[$hrefTagLengthStart] == $character)\n                    {\n                        $hrefTagLengthStart++;\n                        if($hrefTagLengthStart == $hrefTagLengthFinal)\n                        {\n                            $hrefTagCountStart++;\n                            if($hrefTagCountStart == $hrefTagCountFinal)\n                            {\n                                if($hrefURL != """")\n                                {\n                                    if($parentDirectoryCount >= 1 || $singleSlashCount >= 1 || $doubleSlashCount >= 1)\n                                    {\n                                        if($doubleSlashCount >= 1)\n                                        {   $hrefURL = ""http://"".$hrefURL;  }\n                                        else if($parentDirectoryCount >= 1)\n                                        {\n                                            $tempData = 0;\n                                            $tempString = """";\n                                            $tempTotal = count($urlParser) - $parentDirectoryCount;\n                                            while($tempData < $tempTotal)\n                                            {\n                                                $tempString .= $urlParser[$tempData].""/"";\n                                                $tempData++;\n                                            }\n                                            $hrefURL = $tempString."""".$hrefURL;\n                                        }\n                                        else if($singleSlashCount >= 1)\n                                        {   $hrefURL = $urlParser[0].""/"".$urlParser[1].""/"".$urlParser[2].""/"".$hrefURL;  }\n                                    }\n                                    $host = """";\n                                    $hrefURL = urldecode($hrefURL);\n                                    $hrefURL = rtrim($hrefURL,""/"");\n                                    if(filter_var($hrefURL,FILTER_VALIDATE_URL) == true)\n                                    {   \n                                        $dump = parse_url($hrefURL);\n                                        if(isset($dump[""host""]))\n                                        {   $host = trim(strtolower($dump[""host""]));    }\n                                    }\n                                    else\n                                    {\n                                        $hrefURL = $directoryURL.""/"".$hrefURL;\n                                        if(filter_var($hrefURL,FILTER_VALIDATE_URL) == true)\n                                        {   \n                                            $dump = parse_url($hrefURL);    \n                                            if(isset($dump[""host""]))\n                                            {   $host = trim(strtolower($dump[""host""]));    }\n                                        }\n                                    }\n                                    if($host != """")\n                                    {\n                                        $extension = pathinfo($hrefURL,PATHINFO_EXTENSION);\n                                        if($extension != """")\n                                        {\n                                            $tempBuffer ="""";\n                                            $extensionlength = strlen($extension);\n                                            for($tempData = 0; $tempData < $extensionlength; $tempData++)\n                                            {\n                                                if($extension[$tempData] != ""?"")\n                                                {   \n                                                    $tempBuffer = $tempBuffer.$extension[$tempData];\n                                                    continue;\n                                                }\n                                                else\n                                                {\n                                                    $extension = trim($tempBuffer);\n                                                    break;\n                                                }\n                                            }\n                                            if(in_array($extension,$Url_Extensions))\n                                            {   $type = ""domain"";   }\n                                            else if(in_array($extension,$Image_Extensions))\n                                            {   $type = ""image"";    }\n                                            else if(in_array($extension,$Document_Extensions))\n                                            {   $type = ""document""; }\n                                            else\n                                            {   $type = ""unknown"";  }\n                                        }\n                                        else\n                                        {   $type = ""domain"";   }\n\n                                        if($hrefURL != """")\n                                        {\n                                            if($type == ""domain"" && !in_array($hrefURL,$this->linkBuffer[""domain""]))\n                                            {   $this->linkBuffer[""domain""][] = $hrefURL;   }\n                                            if($type == ""image"" && !in_array($hrefURL,$this->linkBuffer[""image""]))\n                                            {   $this->linkBuffer[""image""][] = $hrefURL;    }\n                                            if($type == ""document"" && !in_array($hrefURL,$this->linkBuffer[""document""]))\n                                            {   $this->linkBuffer[""document""][] = $hrefURL; }\n                                            if($type == ""unknown"" && !in_array($hrefURL,$this->linkBuffer[""unknown""]))\n                                            {   $this->linkBuffer[""unknown""][] = $hrefURL;  }\n                                        }\n                                    }\n                                }\n                                $hrefTagCountStart = 0;\n                            }\n                            if($hrefTagCountStart == 3)\n                            {\n                                $hrefURL = """";\n                                $dotCount = 0;\n                                $slashCount = 0;\n                                $singleSlashCount = 0;\n                                $doubleSlashCount = 0;\n                                $parentDirectoryCount = 0;\n                                $webPageCounter++;\n                                while($webPageCounter < $webPageLength)\n                                {\n                                    $character = $webPageContent[$webPageCounter];\n                                    if($character == """")\n                                    {   \n                                        $webPageCounter++;  \n                                        continue;\n                                    }\n                                    if($character == ""\\"""" || $character == ""\'"")\n                                    {\n                                        $webPageCounter++;\n                                        while($webPageCounter < $webPageLength)\n                                        {\n                                            $character = $webPageContent[$webPageCounter];\n                                            if($character == """")\n                                            {   \n                                                $webPageCounter++;  \n                                                continue;\n                                            }\n                                            if($character == ""\\"""" || $character == ""\'"" || $character == ""#"")\n                                            {   \n                                                $webPageCounter--;  \n                                                break;  \n                                            }\n                                            else if($hrefURL != """")\n                                            {   $hrefURL .= $character; }\n                                            else if($character == ""."" || $character == ""/"")\n                                            {\n                                                if($character == ""."")\n                                                {\n                                                    $dotCount++;\n                                                    $slashCount = 0;\n                                                }\n                                                else if($character == ""/"")\n                                                {\n                                                    $slashCount++;\n                                                    if($dotCount == 2 && $slashCount == 1)\n                                                    $parentDirectoryCount++;\n                                                    else if($dotCount == 0 && $slashCount == 1)\n                                                    $singleSlashCount++;\n                                                    else if($dotCount == 0 && $slashCount == 2)\n                                                    $doubleSlashCount++;\n                                                    $dotCount = 0;\n                                                }\n                                            }\n                                            else\n                                            {   $hrefURL .= $character; }\n                                            $webPageCounter++;\n                                        }\n                                        break;\n                                    }\n                                    $webPageCounter++;\n                                }\n                            }\n                            $hrefTagLengthStart = 0;\n                            $hrefTagLengthFinal = strlen($hrefTag[$hrefTagCountStart]);\n                            $hrefTagPointer =& $hrefTag[$hrefTagCountStart];\n                        }\n                    }\n                    else\n                    {   $hrefTagLengthStart = 0;    }\n                    //-- Href Filter End --\n                    //-- Image Filter Start --\n                    if($imgTagPointer[$imgTagLengthStart] == $character)\n                    {\n                        $imgTagLengthStart++;\n                        if($imgTagLengthStart == $imgTagLengthFinal)\n                        {\n                            $imgTagCountStart++;\n                            if($imgTagCountStart == $imgTagCountFinal)\n                            {\n                                if($imgURL != """")\n                                {\n                                    if($parentDirectoryCount >= 1 || $singleSlashCount >= 1 || $doubleSlashCount >= 1)\n                                    {\n                                        if($doubleSlashCount >= 1)\n                                        {   $imgURL = ""http://"".$imgURL;    }\n                                        else if($parentDirectoryCount >= 1)\n                                        {\n                                            $tempData = 0;\n                                            $tempString = """";\n                                            $tempTotal = count($urlParser) - $parentDirectoryCount;\n                                            while($tempData < $tempTotal)\n                                            {\n                                                $tempString .= $urlParser[$tempData].""/"";\n                                                $tempData++;\n                                            }\n                                            $imgURL = $tempString."""".$imgURL;\n                                        }\n                                        else if($singleSlashCount >= 1)\n                                        {   $imgURL = $urlParser[0].""/"".$urlParser[1].""/"".$urlParser[2].""/"".$imgURL;    }\n                                    }\n                                    $host = """";\n                                    $imgURL = urldecode($imgURL);\n                                    $imgURL = rtrim($imgURL,""/"");\n                                    if(filter_var($imgURL,FILTER_VALIDATE_URL) == true)\n                                    {   \n                                        $dump = parse_url($imgURL); \n                                        $host = trim(strtolower($dump[""host""]));\n                                    }\n                                    else\n                                    {\n                                        $imgURL = $directoryURL.""/"".$imgURL;\n                                        if(filter_var($imgURL,FILTER_VALIDATE_URL) == true)\n                                        {   \n                                            $dump = parse_url($imgURL); \n                                            $host = trim(strtolower($dump[""host""]));\n                                        }   \n                                    }\n                                    if($host != """")\n                                    {\n                                        $extension = pathinfo($imgURL,PATHINFO_EXTENSION);\n                                        if($extension != """")\n                                        {\n                                            $tempBuffer ="""";\n                                            $extensionlength = strlen($extension);\n                                            for($tempData = 0; $tempData < $extensionlength; $tempData++)\n                                            {\n                                                if($extension[$tempData] != ""?"")\n                                                {   \n                                                    $tempBuffer = $tempBuffer.$extension[$tempData];\n                                                    continue;\n                                                }\n                                                else\n                                                {\n                                                    $extension = trim($tempBuffer);\n                                                    break;\n                                                }\n                                            }\n                                            if(in_array($extension,$Url_Extensions))\n                                            {   $type = ""domain"";   }\n                                            else if(in_array($extension,$Image_Extensions))\n                                            {   $type = ""image"";    }\n                                            else if(in_array($extension,$Document_Extensions))\n                                            {   $type = ""document""; }\n                                            else\n                                            {   $type = ""unknown"";  }\n                                        }\n                                        else\n                                        {   $type = ""domain"";   }\n\n                                        if($imgURL != """")\n                                        {\n                                            if($type == ""domain"" && !in_array($imgURL,$this->linkBuffer[""domain""]))\n                                            {   $this->linkBuffer[""domain""][] = $imgURL;    }\n                                            if($type == ""image"" && !in_array($imgURL,$this->linkBuffer[""image""]))\n                                            {   $this->linkBuffer[""image""][] = $imgURL; }\n                                            if($type == ""document"" && !in_array($imgURL,$this->linkBuffer[""document""]))\n                                            {   $this->linkBuffer[""document""][] = $imgURL;  }\n                                            if($type == ""unknown"" && !in_array($imgURL,$this->linkBuffer[""unknown""]))\n                                            {   $this->linkBuffer[""unknown""][] = $imgURL;   }\n                                        }\n                                    }\n                                }\n                                $imgTagCountStart = 0;\n                            }\n                            if($imgTagCountStart == 3)\n                            {\n                                $imgURL = """";\n                                $dotCount = 0;\n                                $slashCount = 0;\n                                $singleSlashCount = 0;\n                                $doubleSlashCount = 0;\n                                $parentDirectoryCount = 0;\n                                $webPageCounter++;\n                                while($webPageCounter < $webPageLength)\n                                {\n                                    $character = $webPageContent[$webPageCounter];\n                                    if($character == """")\n                                    {   \n                                        $webPageCounter++;  \n                                        continue;\n                                    }\n                                    if($character == ""\\"""" || $character == ""\'"")\n                                    {\n                                        $webPageCounter++;\n                                        while($webPageCounter < $webPageLength)\n                                        {\n                                            $character = $webPageContent[$webPageCounter];\n                                            if($character == """")\n                                            {   \n                                                $webPageCounter++;  \n                                                continue;\n                                            }\n                                            if($character == ""\\"""" || $character == ""\'"" || $character == ""#"")\n                                            {   \n                                                $webPageCounter--;  \n                                                break;  \n                                            }\n                                            else if($imgURL != """")\n                                            {   $imgURL .= $character;  }\n                                            else if($character == ""."" || $character == ""/"")\n                                            {\n                                                if($character == ""."")\n                                                {\n                                                    $dotCount++;\n                                                    $slashCount = 0;\n                                                }\n                                                else if($character == ""/"")\n                                                {\n                                                    $slashCount++;\n                                                    if($dotCount == 2 && $slashCount == 1)\n                                                    $parentDirectoryCount++;\n                                                    else if($dotCount == 0 && $slashCount == 1)\n                                                    $singleSlashCount++;\n                                                    else if($dotCount == 0 && $slashCount == 2)\n                                                    $doubleSlashCount++;\n                                                    $dotCount = 0;\n                                                }\n                                            }\n                                            else\n                                            {   $imgURL .= $character;  }\n                                            $webPageCounter++;\n                                        }\n                                        break;\n                                    }\n                                    $webPageCounter++;\n                                }\n                            }\n                            $imgTagLengthStart = 0;\n                            $imgTagLengthFinal = strlen($imgTag[$imgTagCountStart]);\n                            $imgTagPointer =& $imgTag[$imgTagCountStart];\n                        }\n                    }\n                    else\n                    {   $imgTagLengthStart = 0; }\n                    //-- Image Filter End --\n                    $webPageCounter++;\n                }\n            }\n            else\n            {   $this->error = ""Unable to proceed, permission denied"";  }\n        }\n        else\n        {   $this->error = ""Please enter url"";  }\n\n        if($this->error != """")\n        {   $this->linkBuffer[""error""] = $this->error;  }\n\n        return $this->linkBuffer;\n    }   \n}\n?>\n\n', ""\nWell the web is basically a directed graph, so you can construct a graph out of the urls and then do a BFS or DFS traversal while marking the visited nodes so you don't visit the same page twice. \n"", '\nThis is a web crawler example. Which can be used to collect mac Addresses for mac spoofing. \n#!/usr/bin/env python\n\nimport sys\nimport os\nimport urlparse\nimport urllib\nfrom bs4 import BeautifulSoup\n\ndef mac_addr_str(f_data):\nglobal fptr\nglobal mac_list\nword_array = f_data.split("" "")\n\n    for word in word_array:\n        if len(word) == 17 and \':\' in word[2] and \':\' in word[5] and \':\' in word[8] and \':\' in word[11] and \':\' in word[14]:\n            if word not in mac_list:\n                mac_list.append(word)\n                fptr.writelines(word +""\\n"")\n                print word\n\n\n\nurl = ""http://stackoverflow.com/questions/tagged/mac-address""\n\nurl_list = [url]\nvisited = [url]\npwd = os.getcwd();\npwd = pwd + ""/internet_mac.txt"";\n\nfptr = open(pwd, ""a"")\nmac_list = []\n\nwhile len(url_list) > 0:\n    try:\n        htmltext = urllib.urlopen(url_list[0]).read()\n    except:\n        url_list[0]\n    mac_addr_str(htmltext)\n    soup = BeautifulSoup(htmltext)\n    url_list.pop(0)\n    for tag in soup.findAll(\'a\',href=True):\n        tag[\'href\'] = urlparse.urljoin(url,tag[\'href\'])\n        if url in tag[\'href\'] and tag[\'href\'] not in visited:\n            url_list.append(tag[\'href\'])\n            visited.append(tag[\'href\'])\n\nChange the url to crawl more sites......good luck\n']"
Scrapy Python Set up User Agent,"
I tried to override the user-agent of my crawlspider by adding an extra line to the project configuration file. Here is the code:
[settings]
default = myproject.settings
USER_AGENT = ""Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36""


[deploy]
#url = http://localhost:6800/
project = myproject

But when I run the crawler against my own web, I notice the spider did not pick up my customized user agent but the default one ""Scrapy/0.18.2 (+http://scrapy.org)"". 
Can any one explain what I have done wrong. 
Note:
(1). It works when I tried to override the user agent globally: 
scrapy crawl myproject.com -o output.csv -t csv -s USER_AGENT=""Mozilla....""

(2). When I remove the line ""default = myproject.setting"" from the configuration file, and run scrapy crawl myproject.com, it says ""cannot find spider.."", so I feel like the default setting should not be removed in this case.
Thanks a lot for the help in advance.                            
",53k,"
            40
        ","['\nMove your USER_AGENT line to the settings.py file, and not in your scrapy.cfg file. settings.py should be at same level as items.py if you use scrapy startproject command, in your case  it should be something like myproject/settings.py\n', ""\nJust in case anyone lands here that manually controls the scrapy crawl. i.e. you do not use the scrapy crawl process from the shell...\n$ scrapy crawl myproject\n\nBut insted you use CrawlerProcess() or CrawlerRunner()...\nprocess = CrawlerProcess()\n\nor \nprocess = CrawlerRunner()\n\nthen the user agent, along with other settings, can be passed to the crawler in a dictionary of configuration variables. \nLike this...\n    process = CrawlerProcess(\n            {\n                'USER_AGENT': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n            }\n    )\n\n""]"
Difference between find and filter in jquery,"
I'm working on fetching data from wiki pages. I'm using a combination of php and jquery to do this. First I am using curl in php to fetch page contents and echoing the content. The filename is content.php:
$url = $_GET['url'];
$url = trim($url,"" "");
$url = urldecode($url);
$url = str_replace("" "",""%20"",$url);

echo ""<a class='urlmax'>"".$_GET['title'].""</a>"";
echo crawl($url);

Then jQuery is used to find the matched elements. 
$.get(""content.php"",{url:""http://en.wikipedia.org/w/index.php?action=render&title=""+str_replace("" "",""_"",data[x]),title:str_replace("" "",""_"",data[x])},function(hdata){
                        var imgs = $(hdata).find('a.image img');
                        var ent = $(hdata).filter('a.urlmax');


                        ent = $(ent[0]).text();


});

I was able to successfully get images but for the variable ent when I use find instead of filter, it's returning an empty array. Only filter is working. Why is this?
Edit: I know the basic difference between find and filter. Here both the a.image img and a.urlmax are descendats of the hdata. Then why find does not work on a.urlmax. Not a.urlmax alone it's not working on any other class or id
",26k,"
            29
        ","[""\n.find()\nhttp://api.jquery.com/find/\n\nGet the descendants of each element in the current set of matched elements, filtered by a selector, jQuery object, or element.\n\nFilter, on the other hand, works on the currently matched elements. That's why filter worked but find did not (you needed to look at the current element).\n"", '\nfilter will select subset of element from the selected element\nfind will select descendent/children of selected element\nTo make it more clear filter will search through all element whereas find will search only in the descendent list\n', '\n.find()\nIt will returns descendant elements of the selected element.\nExemple (jsfiddle):\n    <style>\n      .Cell{\n         margin: 15px;\n         width: 400px;\n         border: 2px solid lightgrey;\n    \n      }\n      .Cell * {\n         display: block;\n         border: 2px solid lightgrey;\n         color: lightgrey;\n         padding: 5px;\n         margin: 10px;\n    }\n    </style>\n\n    <div class=\'Cell Plus\'>div (1)\n      <div class=\'Plus\'>Child</div>\n    </div>\n    \n    <div class=\'Cell Plus\'>div (2)\n      <div class=\'Plus\'>Child</div>\n    </div>\n\nJS:\n$("".Cell"").find("".Plus"").css({""color"": ""red"", ""border"": ""2px solid red""});\n\nResult:\n\n\n.filter()\nIt will returns elements that match a certain criteria.\nUsing the same html above:\n$("".Cell"").filter("".Plus"").css({""color"": ""red"", ""border"": ""2px solid red""});\n\nResult:\n\n']"
What are some good Ruby-based web crawlers? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



I am looking at writing my own, but I am wondering if there are any good web crawlers out there which are written in Ruby.
Short of a full-blown web crawler, any gems that might be helpful in building a web crawler would be useful. I know this part of the question is touched upon in a couple of places, but a list of gems applicable to building a web crawler would be a great resource as well.
",22k,"
            21
        ","[""\nI used to write spiders, page scrapers and site analyzers for my job, and still write them periodically to scratch some itch I get.\nRuby has some excellent gems to make it easy:\n\nNokogiri is my #1 choice for the HTML parser. I used to use Hpricot, but found some sites that made it explode in flames. I switched to Nokogiri afterwards and have been very happy with it. I regularly use it for parsing HTML, RDF/RSS/Atom and XML. Ox looks interesting too, so that might be another candidate, though I find searching the DOM a lot easier than trying to walk through a big hash, such as what is returned by Ox.\n\nOpenURI is good as a simple HTTP client, but it can get in the way when you want to do more complex things or need to have multiple requests firing at once. I'd recommend looking at HTTPClient or Typhoeus with Hydra for modest to heavyweight jobs. Curb is good too, because it uses the cURL library, but the interface isn't as intuitive to me. It's worth looking at though. HTTPclient is also worth looking at, but I lean toward the previously mentioned ones.\nNote: OpenURI has some flaws and vulnerabilities that can affect unsuspecting programmers so it's fallen out of favor somewhat. RestClient is a very worthy successor.\n\nYou'll need a backing database, and some way to talk to it. This isn't a task for Rails per se, but you could use ActiveRecord, detached from Rails, to talk to the database. I've done that a couple times and it works all right. Instead, I really like Sequel for my ORM. It's very flexible in how it lets you talk to the database, from using straight SQL to using Sequel's ability to programmatically build a query, to modeling the database and using migrations. Once you have the database built, you could use Rails to act as a front-end to the data though.\n\nIf you are going to navigate sites in any way beyond simply grabbing pages and following links, you'll want to look at Mechanize. It makes it easy to fill out forms and submit pages. As an added bonus, you can grab the content of a page as a Nokogiri HTML document and parse away using Nokogiri's multitude of tricks.\n\nFor massaging/mangling URLs I really like Addressable::URI. It's more full-featured than the built-in URI module. One thing that URI does that's nice is it has the URI#extract method to scan a string for URLs. If that string happened to be the body of a web page it would be an alternate way of locating links, but its downside is you'll also get links to images, videos, ads, etc., and you'll have to filter those out, probably resulting in more work than if you use a parser and look for <a> tags exclusively. For that matter, Mechanize also has the links method which returns all the links in a page, but you'll still have to filter them to determine whether you want to follow or ignore them.\n\nIf you think you'll need to deal with Javascript manipulated pages, or pages that get their content dynamically from AJAX, you should look into using one of the WATIR variants. There are flavors for the different browsers on different OSes, such as Firewatir, Safariwatir and Operawatir, so you'll have to figure out what works for you.\n\nYou do NOT want to rely on keeping your list of URLs to visit, or visited URLs, in memory. Design a database schema and store that information there. Spend some time up front designing the schema, thinking about what things you'll want to know as you collect links on a site. SQLite3, MySQL and Postgres are all excellent choices, depending on how big you think your database needs will be. One of my site analyzers was custom designed to help us recommend SEO changes for a Fortune 50 company. It ran for over three weeks covering about twenty different sites before we had enough data and stopped it. Imagine what would have happened if we had a power-outage and all that data went in the bit-bucket.\n\n\nAfter all that you'll want to also make your code be aware of proper spidering etiquette: What are the key considerations when creating a web crawler?\n"", '\nI am building wombat, a Ruby DSL to crawl web pages and extract content. Check it out on github https://github.com/felipecsl/wombat\nIt is still in an early stage but is already functional with basic functionality. More stuff will be added really soon.\n', '\nSo you want a good Ruby-based web crawler?\nTry spider or anemone. Both have solid usage according to RubyGems download counts.\nThe other answers, so far, are detailed and helpful but they don\'t have a laser-like focus on the question, which asks for ruby libraries for web crawlers. It would seem that this distinction can get muddled: see my answer to ""Crawling vs. Web-Scraping?""\n', '\nTin Man\'s comprehensive list  is good but partly outdated for me.\nMost websites my customers deal with are heavily AJAX/Javascript dependent.\nI\'ve been using Watir / watir-webdriver / selenium for a few years too, but the overhead of having to load up a hidden web browser on the backend to render that DOM stuff just isn\'t viable, let alone that all this time they still haven\'t implemented a useable ""browser session reuse"" to let new code execution reuse an old browser in memory for this purpose, shooting down tickets that might have worked their way up the API layers eventually. (refering to https://code.google.com/p/selenium/issues/detail?id=18 ) **\nhttps://rubygems.org/gems/phantomjs\nis what we\'re migrating new projects over to now, to let the necessary data get rendered without even any sort of invisible Xvfb memory & CPU heavy web browser.\n** Alternative approaches also failed to pan out:\n\nhow to serialize an object using TCPServer inside?\nCan a watir browser object be re-used in a later Ruby process?\n\n', '\nIf you don\'t want to write your own, then use any ordinary web crawler.  There are dozens out there.\nIf you do want to write your own, then write your own. A web crawler isn\'t exactly a complicated activity, it consists of:\n\nDownloading a website.\nLocating URLs in that website, filtered however you dang well please.\nFor each URL in that website, repeat step 1.\n\nOh, and this seems to be a duplicate of ""Web crawler in ruby"".\n']"
Scrapy - how to identify already scraped urls,"
Im using scrapy to crawl a news website on a daily basis. How do i  restrict scrapy from scraping already scraped URLs. Also is there any clear documentation or examples on SgmlLinkExtractor.
",10k,"
            15
        ","[""\nYou can actually do this quite easily with the scrapy snippet located here: http://snipplr.com/view/67018/middleware-to-avoid-revisiting-already-visited-items/\nTo use it, copy the code from the link and put it into some file in your scrapy project. \nTo reference it, add a line in your settings.py to reference it:\nSPIDER_MIDDLEWARES = { 'project.middlewares.ignore.IgnoreVisitedItems': 560 }\n\nThe specifics on WHY you pick the number that you do can be read up here: http://doc.scrapy.org/en/latest/topics/downloader-middleware.html\nFinally, you'll need to modify your items.py so that each item class has the following fields:\nvisit_id = Field()\nvisit_status = Field()\n\nAnd I think that's it.  The next time you run your spider it should automatically try to start avoiding the same sites.  \nGood luck!\n"", '\nThis is straight forward. Maintain all your previously crawled urls in python dict. So when you try to try them next time, see if that url is there in the dict. else crawl.\ndef load_urls(prev_urls):\n    prev = dict()\n    for url in prev_urls:\n        prev[url] = True\n    return prev\n\ndef fresh_crawl(prev_urls, new_urls):\n    for url in new_urls:\n        if url not in prev_urls:\n            crawl(url)\n    return\n\ndef main():\n    purls = load_urls(prev_urls)\n    fresh_crawl(purls, nurls)\n    return\n\nThe above code was typed in SO text editor aka browser. Might have syntax errors. You might also need to make a few changes. But the logic is there...\nNOTE: But beware that some websites constantly keep changing their content. So sometimes you might have to recrawl a particular webpage (i.e. same url) just to get the updated content.\n', ""\nI think jama22's answer is a little incomplete. \nIn the snippet if self.FILTER_VISITED in x.meta:, you can see that you require FILTER_VISITED in your Request instance in order for that request to be ignored. This is to ensure that you can differentiate between links that you want to traverse and move around and item links that well, you don't want to see again.\n"", '\nScrapy can auto-filter urls which are scraped, isn\'t it? Some different urls point to the same page will not be filtered, such as ""www.xxx.com/home/"" and ""www.xxx.com/home/index.html"".\n', ""\nFor today (2019), this post is the best answer for this problem. \nhttps://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016\nIt's a lib to handle MIDDLEWARES automatcally.\nHope to help someone. I've spent a lot of time seaching for this.\n""]"
Simple web crawler in C#,"
I have created a simple web crawler but I want to add the recursion function so that every page that is opened I can get the URLs in this page, but I have no idea how I can do that and I want also to include threads to make it faster.
Here is my code
namespace Crawler
{
    public partial class Form1 : Form
    {
        String Rstring;

        public Form1()
        {
            InitializeComponent();
        }

        private void button1_Click(object sender, EventArgs e)
        {
            
            WebRequest myWebRequest;
            WebResponse myWebResponse;
            String URL = textBox1.Text;

            myWebRequest =  WebRequest.Create(URL);
            myWebResponse = myWebRequest.GetResponse();//Returns a response from an Internet resource

            Stream streamResponse = myWebResponse.GetResponseStream();//return the data stream from the internet
                                                                       //and save it in the stream

            StreamReader sreader = new StreamReader(streamResponse);//reads the data stream
            Rstring = sreader.ReadToEnd();//reads it to the end
            String Links = GetContent(Rstring);//gets the links only
            
            textBox2.Text = Rstring;
            textBox3.Text = Links;
            streamResponse.Close();
            sreader.Close();
            myWebResponse.Close();




        }

        private String GetContent(String Rstring)
        {
            String sString="""";
            HTMLDocument d = new HTMLDocument();
            IHTMLDocument2 doc = (IHTMLDocument2)d;
            doc.write(Rstring);
            
            IHTMLElementCollection L = doc.links;
           
            foreach (IHTMLElement links in  L)
            {
                sString += links.getAttribute(""href"", 0);
                sString += ""/n"";
            }
            return sString;
        }

",69k,"
            13
        ","['\nI fixed your GetContent method as follow to get new links from crawled page:\npublic ISet<string> GetNewLinks(string content)\n{\n    Regex regexLink = new Regex(""(?<=<a\\\\s*?href=(?:\'|\\""))[^\'\\""]*?(?=(?:\'|\\""))"");\n\n    ISet<string> newLinks = new HashSet<string>();    \n    foreach (var match in regexLink.Matches(content))\n    {\n        if (!newLinks.Contains(match.ToString()))\n            newLinks.Add(match.ToString());\n    }\n\n    return newLinks;\n}\n\nUpdated\nFixed: regex should be regexLink. Thanks @shashlearner for pointing this out (my mistype).\n', '\ni have created something similar using Reactive Extension.\nhttps://github.com/Misterhex/WebCrawler\ni hope it can help you.\nCrawler crawler = new Crawler();\n\nIObservable observable = crawler.Crawl(new Uri(""http://www.codinghorror.com/""));\n\nobservable.Subscribe(onNext: Console.WriteLine, \nonCompleted: () => Console.WriteLine(""Crawling completed""));\n\n', '\nThe following includes an answer/recommendation.\nI believe you should use a dataGridView instead of a textBox as when you look at it in GUI it is easier to see the links (URLs) found.\nYou could change:\ntextBox3.Text = Links;\n\nto \n dataGridView.DataSource = Links;  \n\nNow for the question, you haven\'t included: \nusing System.  ""\'s""\n\nwhich ones were used, as it would be appreciated if I could get them as can\'t figure it out.\n', ""\nFrom a design standpoint, I've written a few webcrawlers.  Basically you want to implement a Depth First Search using a Stack data structure.   You can use Breadth First Search also, but you'll likely come into stack memory issues.  Good luck.\n""]"
Asp.net Request.Browser.Crawler - Dynamic Crawler List?,"
I learned Why Request.Browser.Crawler is Always False in C# (http://www.digcode.com/default.aspx?page=ed51cde3-d979-4daf-afae-fa6192562ea9&article=bc3a7a4f-f53e-4f88-8e9c-c9337f6c05a0).
Does anyone uses some method to dynamically update the Crawler's list, so Request.Browser.Crawler will be really useful?
",10k,"
            8
        ","[""\nI've been happy the the results supplied by Ocean's Browsercaps. It supports crawlers that Microsoft's config files has not bothered detecting. It will even parse out what version of the crawler is on your site, not that I really need that level of detail.\n"", '\nYou could check (regex) against Request.UserAgent.\nPeter Bromberg wrote a nice article about writing an ASP.NET Request Logger and Crawler Killer in ASP.NET.\nHere is the method he uses in his Logger class:\npublic static bool IsCrawler(HttpRequest request)\n{\n   // set next line to ""bool isCrawler = false; to use this to deny certain bots\n   bool isCrawler = request.Browser.Crawler;\n   // Microsoft doesn\'t properly detect several crawlers\n   if (!isCrawler)\n   {\n       // put any additional known crawlers in the Regex below\n       // you can also use this list to deny certain bots instead, if desired:\n       // just set bool isCrawler = false; for first line in method \n       // and only have the ones you want to deny in the following Regex list\n       Regex regEx = new Regex(""Slurp|slurp|ask|Ask|Teoma|teoma"");\n       isCrawler = regEx.Match(request.UserAgent).Success;\n   }\n   return isCrawler;\n}\n\n']"
How do I save the origin html file with Apache Nutch,"
I'm new to search engines and web crawlers. Now I want to store all the original pages in a particular web site as html files, but with Apache Nutch I can only get the binary database files. How do I get the original html files with Nutch? 
Does Nutch support it? If not, what other tools can I use to achieve my goal.(The tools that support distributed crawling are better.)
",6k,"
            5
        ","['\nWell, nutch will write the crawled data in binary form so if if you want that to be saved in html format, you will have to modify the code. (this will be painful if you are new to nutch).\nIf you want quick and easy solution for getting html pages:\n\nIf the list of pages/urls that you intend to have is quite low, then better get it done with a script which invokes wget for each url.\nOR use HTTrack tool. \n\nEDIT:\nWriting a your own nutch plugin will be great. Your problem will get solved plus you can contribute to nutch by submitting your work !!! If you are new to nutch (in terms of code & design), then you will have to invest lot of time building a new plugin ... else its easy to do. \nFew pointers for helping your initiative:\nHere is a page which talks about writing own nutch plugin.\nStart with Fetcher.java. See lines 647-648. That is the place where you can get the fetched content on per url basis (for those pages which got fetched successfully).\npstatus = output(fit.url, fit.datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS);\nupdateStatus(content.getContent().length);\n\nYou should add code right after this to invoke your plugin. Pass content object to it. By now, you would have guessed that content.getContent() is the content for url you want. Inside the plugin code, write it to some file. Filename should be based on the url name else it will be difficult to work with that. Url can be obtained by fit.url.\n', '\nYou must do modifications in run Nutch in Eclipse.\nWhen you are able to run, open Fetcher.java and add the lines between ""content saver"" command lines. \ncase ProtocolStatus.SUCCESS:        // got a page\n            pstatus = output(fit.url, fit.datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS, fit.outlinkDepth);\n            updateStatus(content.getContent().length);\'\n\n\n            //------------------------------------------- content saver ---------------------------------------------\\\\\n            String filename = ""savedsites//"" + content.getUrl().replace(\'/\', \'-\');  \n\n            File file = new File(filename);\n            file.getParentFile().mkdirs();\n            boolean exist = file.createNewFile();\n            if (!exist) {\n                System.out.println(""File exists."");\n            } else {\n                FileWriter fstream = new FileWriter(file);\n                BufferedWriter out = new BufferedWriter(fstream);\n                out.write(content.toString().substring(content.toString().indexOf(""<!DOCTYPE html"")));\n                out.close();\n                System.out.println(""File created successfully."");\n            }\n            //------------------------------------------- content saver ---------------------------------------------\\\\\n\n', '\nTo update this answer -\nIt is possible to post process the data from your crawldb segment folder, and read in the html (including other data nutch has stored) directly.\n    Configuration conf = NutchConfiguration.create();\n    FileSystem fs = FileSystem.get(conf);\n\n    Path file = new Path(segment, Content.DIR_NAME + ""/part-00000/data"");\n    SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);\n\n    try\n    {\n            Text key = new Text();\n            Content content = new Content();\n\n            while (reader.next(key, content)) \n            {\n                    System.out.println(new String(content.GetContent()));\n            }\n    }\n    catch (Exception e)\n    {\n\n    }\n\n', '\nThe answers here are obsolete. Now, it is simply possible to get the plain HTML-files with nutch dump. Please see this answer.\n', '\nIn apache Nutch 2.3.1\nYou can save the raw HTML by edit the Nutch code firstly run the nutch in eclipse by following https://wiki.apache.org/nutch/RunNutchInEclipse\nAfter you finish ruunning nutch in eclipse edit file FetcherReducer.java , add this code to the output method, run ant eclipse again to rebuild the class\nFinally the raw html will added to reportUrl column in your database\nif (content != null) {\nByteBuffer raw = fit.page.getContent();\nif (raw != null) {\n    ByteArrayInputStream arrayInputStream = new ByteArrayInputStream(raw.array(), raw.arrayOffset() + raw.position(), raw.remaining());\n    Scanner scanner = new Scanner(arrayInputStream);\n    scanner.useDelimiter(""\\\\Z"");//To read all scanner content in one String\n    String data = """";\n    if (scanner.hasNext()) {\n        data = scanner.next();\n    }\n    fit.page.setReprUrl(StringUtil.cleanField(data));\n    scanner.close();\n}\n\n']"
YouTube Data API to crawl all comments and replies,"
I have been desperately seeking a solution to crawl all comments and corresponding replies for my research. Am having a very hard time creating a data frame that includes comment data in correct and corresponding orders.
I am gonna share my code here so you professionals can take a look and give me some insights.
def get_video_comments(service, **kwargs):
    comments = []
    results = service.commentThreads().list(**kwargs).execute()

    while results:
        for item in results['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comment2 = item['snippet']['topLevelComment']['snippet']['publishedAt']
            comment3 = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
            comment4 = item['snippet']['topLevelComment']['snippet']['likeCount']
            if 'replies' in item.keys():
                for reply in item['replies']['comments']:
                    rauthor = reply['snippet']['authorDisplayName']
                    rtext = reply['snippet']['textDisplay']
                    rtime = reply['snippet']['publishedAt']
                    rlike = reply['snippet']['likeCount']
                    data = {'Reply ID': [rauthor], 'Reply Time': [rtime], 'Reply Comments': [rtext], 'Reply Likes': [rlike]}
                    print(rauthor)
                    print(rtext)
            data = {'Comment':[comment],'Date':[comment2],'ID':[comment3], 'Likes':[comment4]}
            result = pd.DataFrame(data)
            result.to_csv('youtube.csv', mode='a',header=False)
            print(comment)
            print(comment2)
            print(comment3)
            print(comment4)
            print('==============================')
            comments.append(comment)
                
        # Check if another page exists
        if 'nextPageToken' in results:
            kwargs['pageToken'] = results['nextPageToken']
            results = service.commentThreads().list(**kwargs).execute()
        else:
            break

    return comments

When I do this, my crawler collects comments but doesn't collect some of the replies that are under certain comments.
How can I make it collect comments and their corresponding replies and put them in a single data frame?
Update
So, somehow I managed to pull the information I wanted at the output section of Jupyter Notebook. All I have to do now is to append the result at the data frame.
Here is my updated code:
def get_video_comments(service, **kwargs):
    comments = []
    results = service.commentThreads().list(**kwargs).execute()

    while results:
        for item in results['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comment2 = item['snippet']['topLevelComment']['snippet']['publishedAt']
            comment3 = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
            comment4 = item['snippet']['topLevelComment']['snippet']['likeCount']
            if 'replies' in item.keys():
                for reply in item['replies']['comments']:
                    rauthor = reply['snippet']['authorDisplayName']
                    rtext = reply['snippet']['textDisplay']
                    rtime = reply['snippet']['publishedAt']
                    rlike = reply['snippet']['likeCount']
                    print(rtext)
                    print(rtime)
                    print(rauthor)
                    print('Likes: ', rlike)
                    
            print(comment)
            print(comment2)
            print(comment3)
            print(""Likes: "", comment4)

            print('==============================')
            comments.append(comment)
                
        # Check if another page exists
        if 'nextPageToken' in results:
            kwargs['pageToken'] = results['nextPageToken']
            results = service.commentThreads().list(**kwargs).execute()
        else:
            break

    return comments

The result is:

As you can see, the comments grouped under ======== lines are the comment and corresponding replies underneath.
What would be a good way to append the result into the data frame?
",2k,"
            4
        ","[""\nAccording to the official doc, the property replies.comments[] of CommentThreads resource has the following specification:\n\nreplies.comments[] (list)\nA list of one or more replies to the top-level comment. Each item in the list is a comment resource.\nThe list contains a limited number of replies, and unless the number of items in the list equals the value of the snippet.totalReplyCount property, the list of replies is only a subset of the total number of replies available for the top-level comment. To retrieve all of the replies for the top-level comment, you need to call the Comments.list method and use the parentId request parameter to identify the comment for which you want to retrieve replies.\n\nConsequently, if wanting to obtain all reply entries associated to a given top-level comment, you will have to use the Comments.list API endpoint queried appropriately.\nI recommend you to read my answer to a very much related question; there are three sections:\n\nTop-Level Comments and Associated Replies,\nThe property nextPageToken and the parameter pageToken, and\nAPI Limitations Imposed by Design.\n\nFrom the get go, you'll have to acknowledge that the API (as currently implemented) does not allow to obtain all top-level comments associated to a given video when the number of those comments exceeds a certain (unspecified) upper bound.\n\nFor what concerns a Python implementation, I would suggest that you do structure the code as follows:\ndef get_video_comments(service, video_id):\n    request = service.commentThreads().list(\n        videoId = video_id,\n        part = 'id,snippet,replies',\n        maxResults = 100\n    )\n    comments = []\n\n    while request:\n        response = request.execute()\n\n        for comment in response['items']:\n            reply_count = comment['snippet'] \\\n                ['totalReplyCount']\n            replies = comment.get('replies')\n            if replies is not None and \\\n               reply_count != len(replies['comments']):\n               replies['comments'] = get_comment_replies(\n                   service, comment['id'])\n\n            # 'comment' is a 'CommentThreads Resource' that has it's\n            # 'replies.comments' an array of 'Comments Resource'\n\n            # Do fill in the 'comments' data structure \n            # to be provided by this function:\n            ...\n\n        request = service.commentThreads().list_next(\n            request, response)\n\n    return comments\n\ndef get_comment_replies(service, comment_id):\n    request = service.comments().list(\n        parentId = comment_id,\n        part = 'id,snippet',\n        maxResults = 100\n    )\n    replies = []\n\n    while request:\n        response = request.execute()\n        replies.extend(response['items'])\n        request = service.comments().list_next(\n            request, response)\n\n    return replies\n\nNote that the ellipsis dots above -- ... -- would have to be replaced with actual code that fills in the array of structures to be returned by get_video_comments to its caller.\nThe simplest way (useful for quick testing) would be to have ... replaced with comments.append(comment) and then the caller of get_video_comments to simply pretty print (using json.dump) the object obtained from that function.\n"", '\nBased on stvar\' answer and the original publication here I built this code:\nimport os\nimport pickle\nimport csv\nimport json\nimport google.oauth2.credentials\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom google.auth.transport.requests import Request\n\nCLIENT_SECRETS_FILE = ""client_secret.json"" # for more information  to create your credentials json please visit https://python.gotrained.com/youtube-api-extracting-comments/\nSCOPES = [\'https://www.googleapis.com/auth/youtube.force-ssl\']\nAPI_SERVICE_NAME = \'youtube\'\nAPI_VERSION = \'v3\'\n\ndef get_authenticated_service():\n    credentials = None\n    if os.path.exists(\'token.pickle\'):\n        with open(\'token.pickle\', \'rb\') as token:\n            credentials = pickle.load(token)\n    #  Check if the credentials are invalid or do not exist\n    if not credentials or not credentials.valid:\n        # Check if the credentials have expired\n        if credentials and credentials.expired and credentials.refresh_token:\n            credentials.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                CLIENT_SECRETS_FILE, SCOPES)\n            credentials = flow.run_console()\n\n        # Save the credentials for the next run\n        with open(\'token.pickle\', \'wb\') as token:\n            pickle.dump(credentials, token)\n\n    return build(API_SERVICE_NAME, API_VERSION, credentials = credentials)\n\ndef get_video_comments(service, **kwargs):\n    request = service.commentThreads().list(**kwargs)\n    comments = []\n\n    while request:\n        response = request.execute()\n\n        for comment in response[\'items\']:\n            reply_count = comment[\'snippet\'] \\\n                [\'totalReplyCount\']\n            replies = comment.get(\'replies\')\n            if replies is not None and \\\n               reply_count != len(replies[\'comments\']):\n               replies[\'comments\'] = get_comment_replies(\n                   service, comment[\'id\'])\n\n            # \'comment\' is a \'CommentThreads Resource\' that has it\'s\n            # \'replies.comments\' an array of \'Comments Resource\'\n\n            # Do fill in the \'comments\' data structure \n            # to be provided by this function:\n            comments.append(comment)\n\n        request = service.commentThreads().list_next(\n            request, response)\n\n    return comments\ndef get_comment_replies(service, comment_id):\n    request = service.comments().list(\n        parentId = comment_id,\n        part = \'id,snippet\',\n        maxResults = 1000\n    )\n    replies = []\n\n    while request:\n        response = request.execute()\n        replies.extend(response[\'items\'])\n        request = service.comments().list_next(\n            request, response)\n\n    return replies\n\n\nif __name__ == \'__main__\':\n    # When running locally, disable OAuthlib\'s HTTPs verification. When\n    # running in production *do not* leave this option enabled.\n    os.environ[\'OAUTHLIB_INSECURE_TRANSPORT\'] = \'1\'\n    service = get_authenticated_service()\n    videoId = input(\'Enter Video id : \') # video id here (the video id of https://www.youtube.com/watch?v=vedLpKXzZqE -> is vedLpKXzZqE)\n    comments = get_video_comments(service, videoId=videoId, part=\'id,snippet,replies\', maxResults = 1000)\n\n\nwith open(\'youtube_comments\', \'w\', encoding=\'UTF8\') as f:\n    writer = csv.writer(f, delimiter=\',\', quotechar=\'""\', quoting=csv.QUOTE_MINIMAL)\n    for row in comments:\n            # convert the tuple to a list and write to the output file\n            writer.writerow([row])\n\n\nit returns a file called youtube_comments with this format:\n""{\'kind\': \'youtube#commentThread\', \'etag\': \'gvhv4hkH0H2OqQAHQKxzfA-K_tA\', \'id\': \'UgzSgI1YEvwcuF4cPwN4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'topLevelComment\': {\'kind\': \'youtube#comment\', \'etag\': \'qpuKZcuD4FKf6BHgRlMunersEeU\', \'id\': \'UgzSgI1YEvwcuF4cPwN4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'This is a comment\', \'textOriginal\': \'This is a comment\', \'authorDisplayName\': \'Gabriell Magana\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLRGBvo2ZncDP1xGjlX6anfUufNYi9b3w9kYZFDl=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UCKAa4FYftXsN7VKaPSlCivg\', \'authorChannelId\': {\'value\': \'UCKAa4FYftXsN7VKaPSlCivg\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 8, \'publishedAt\': \'2019-05-22T12:38:34Z\', \'updatedAt\': \'2019-05-22T12:38:34Z\'}}, \'canReply\': True, \'totalReplyCount\': 0, \'isPublic\': True}}""\n""{\'kind\': \'youtube#commentThread\', \'etag\': \'DsgDziMk7mB7xN4OoX7cmqlbDYE\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'topLevelComment\': {\'kind\': \'youtube#comment\', \'etag\': \'NYjvYM9W_umBafAfQkdg1P9apgg\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'This is another comment\', \'textOriginal\': \'This is another comment\', \'authorDisplayName\': \'Mary Montes\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLTg1b1yw8BX8Af0PoTR_t5OOwP9Cfl9_qL-o1iikw=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UC_GP_8HxDPsqJjJ3Fju_UeA\', \'authorChannelId\': {\'value\': \'UC_GP_8HxDPsqJjJ3Fju_UeA\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 9, \'publishedAt\': \'2019-05-15T05:10:49Z\', \'updatedAt\': \'2019-05-15T05:10:49Z\'}}, \'canReply\': True, \'totalReplyCount\': 3, \'isPublic\': True}, \'replies\': {\'comments\': [{\'kind\': \'youtube#comment\', \'etag\': \'Tu41ENCZYNJ2KBpYeYz4qgre0H8\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg.8uwduw6ppF79DbfJ9zMKxM\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'this is first reply\', \'parentId\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'authorDisplayName\': \'JULIO EMPRESARIO\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/eYP4MBcZ4bON_pHtdbtVsyWnsKbpNKye2wTPhgkffkMYk3ZbN0FL6Aa1o22YlFjn2RVUAkSQYw=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UCrpB9oZZZfmBv1aQsxrk66w\', \'authorChannelId\': {\'value\': \'UCrpB9oZZZfmBv1aQsxrk66w\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 2, \'publishedAt\': \'2020-09-15T04:06:50Z\', \'updatedAt\': \'2020-09-15T04:06:50Z\'}}, {\'kind\': \'youtube#comment\', \'etag\': \'OrpbnJddwzlzwGArCgtuuBsYr94\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg.8uwduw6ppF795E1w8RV1DJ\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'the second replay\', \'textOriginal\': \'the second replay\', \'parentId\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'authorDisplayName\': \'Anatolio27 Diaz\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLR1hOySIxEkvRCySExHjo3T6zGBNkvuKpPkqA=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UC04N8BM5aUwDJf-PNFxKI-g\', \'authorChannelId\': {\'value\': \'UC04N8BM5aUwDJf-PNFxKI-g\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 2, \'publishedAt\': \'2020-02-19T18:21:06Z\', \'updatedAt\': \'2020-02-19T18:21:06Z\'}}, {\'kind\': \'youtube#comment\', \'etag\': \'sPmIwerh3DTZshLiDVwOXn_fJx0\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg.8uwduw6ppF78wwH6Aabh4y\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'A third reply\', \'textOriginal\': \'A third reply\', \'parentId\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'authorDisplayName\': \'Voy detrás de mi pasión\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLTgzZ3ZFvkmmAlMzA77ApM-2uGFfvOBnzxegYEX=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UCvv6QMokO7KcJCDpK6qZg3Q\', \'authorChannelId\': {\'value\': \'UCvv6QMokO7KcJCDpK6qZg3Q\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 2, \'publishedAt\': \'2019-07-03T18:45:34Z\', \'updatedAt\': \'2019-07-03T18:45:34Z\'}}]}}""\n\nNow it is necessary a second step in order to information required. For this I a set of bash script toos like cut, awk and set:\ncut -d "":"" -f 10- youtube_comments | sed -e ""s/\', \'/\\n/g"" -e ""s/\'//g"" | awk \'/replies/{print ""------------------------****---------:::   Replies: ""$6""  :::---------******--------------------------------""}!/replies/{print}\' |sed \'/^textOriginal:/,/^authorDisplayName:/{/^authorDisplayName/!d}\' |sed \'/^authorProfileImageUrl:\\|^authorChannelUrl:\\|^authorChannelId:\\|^etag:\\|^updatedAt:\\|^parentId:\\|^id:/d\' |sed \'s/<[^>]*>//g\' | sed \'s/{textDisplay/{\\ntextDisplay/\' |sed \'/^snippet:/d\' | awk -F"":"" \'(NF==1){print ""========================================COMMENT===========================================""}(NF>1){a=0; print $0}\' | sed \'s/textDisplay: //g\' | sed \'s/authorDisplayName/User/g\' | sed \'s/T[0-9]\\{2\\}:[0-9]\\{2\\}:[0-9]\\{2\\}Z//g\' | sed \'s/likeCount: /Likes:/g\' | sed \'s/publishedAt: //g\' > output_file\n\nThe final result is a file called output_file with this format:\n========================================COMMENT===========================================\nThis is a comment\nUser: Robert Everest\nLikes:8, 2019-05-22\n========================================COMMENT===========================================\nThis is another comment\nUser: Anna Davis\nLikes:9, 2019-05-15\n------------------------****---------:::   Replies: 3,  :::---------******--------------------------------\nthis is first reply\nUser: John Doe\nLikes:2, 2020-09-15\nthe second replay\nUser: Caraqueno\nLikes:2, 2020-02-19\nA third reply\nUser: Rebeca\nLikes:2, 2019-07-03\n\nThe python script requires of the file token.pickle to work, it is generated the first time the python script run and when it expired, it have to be deleted and generated again.\n', '\nI had a similar issue that the OP does and managed to solve it, but someone in the community closed my question after I solved it and can\'t post there. I\'m posting it here for fidelity.\nThe YouTube API doesn\'t allow users to grab nested replies to comments. What it does allow is you to get the replies to the comments and all the comments i.e. Video --> Comments --> Comment Replies ---> Reply To Reply et al. Knowing this limitation we can write code to get all the top Comments, and then break into those comments to get the first-level replies.\nModuels\nimport os\nimport googleapiclient.discovery #required for using googleapi\nimport pandas as pd #require for data munging. We use pd.json_normalize to create the tables\nimport numpy as np #just good to have\nimport json # the requests are returned as json objects. \nfrom datetime import datetime #good to have for date modification\n\nGet All Comments Function\nFor a given vidId, this function will get the first 100 comments and place them into a df. It then use a while loop to check to see if the response api contains nextPageToken. While it does, it will continue to run to get all the comments until either all the comments are pulled or you run out of credits, whichever happens first.\ndef vidcomments(vidId):\n    # Disable OAuthlib\'s HTTPS verification when running locally.\n    # *DO NOT* leave this option enabled in production.\n    os.environ[""OAUTHLIB_INSECURE_TRANSPORT""] = ""1""\n\n    api_service_name = ""youtube""\n    api_version = ""v3""\n    DEVELOPER_KEY = ""yourapikey"" #<--- insert API key here\n\n    youtube = googleapiclient.discovery.build(\n        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n\n    request = youtube.commentThreads().list(\n        part=""snippet, replies"",\n        order=""time"",\n        maxResults=100,\n        textFormat=""plainText"",\n        videoId=vidId\n    )\n    \n    response = request.execute()\n    full = pd.json_normalize(response, record_path=[\'items\'])\n    while response:\n        \n        if \'nextPageToken\' in response:\n            response = youtube.commentThreads().list(\n                part=""snippet"",\n                maxResults=100,\n                textFormat=\'plainText\',\n                order=\'time\',\n                videoId=vidId,\n                pageToken=response[\'nextPageToken\']\n            ).execute()\n            \n            df2 = pd.json_normalize(response, record_path=[\'items\'])\n            full = full.append(df2)\n            \n        else:\n            break\n    return full\n\nGet All Replies To Comments Function\nFor a particular parentId, get all the first-level replies. Like the vidcomments() function noted above, it will run until all replies to all comments are pulled or you run out of credits, whichever happens first.\n    def repliesto(parentId):\n        # Disable OAuthlib\'s HTTPS verification when running locally.\n        # *DO NOT* leave this option enabled in production.\n        os.environ[""OAUTHLIB_INSECURE_TRANSPORT""] = ""1""\n\n        api_service_name = ""youtube""\n        api_version = ""v3""\n        DEVELOPER_KEY = DevKey #your dev key\n\n        youtube = googleapiclient.discovery.build(\n            api_service_name, api_version, developerKey = DEVELOPER_KEY)\n\n        request = youtube.comments().list(\n            part=""snippet"",\n            maxResults=100,\n            parentId=parentId,\n            textFormat=""plainText""\n        )\n        response = request.execute()\n\n        replies = pd.json_normalize(response, record_path=[\'items\'])\n        while response:\n\n            if \'nextPageToken\' in response:\n                response = youtube.comments().list(\n                    part=""snippet"",\n                    maxResults=100,\n                    parentId=parentId,\n                    textFormat=""plainText"",\n                    pageToken=response[\'nextPageToken\']                \n                ).execute()\n\n                df2 = pd.json_normalize(response, record_path=[\'items\'])\n                replies = pd.concat([replies, df2], sort=False)\n\n            else:\n                break\n        return replies\n\n\nPutting It Together\nFirst, run the vidcomments function to get all the comments information. Then use the code below to get all the reply information using a for loop to pull in each topLevelComment.id into a list, then use the list and another for loop to build the replies dataframe. This will create two separate Dataframes, one for Comments and another for Replies. After creating both of these Dataframes you can then join them in a way that makes sense for your purpose, either concat/union or a join/merge.\n    replyto = []\n    for reply in full[(full[\'snippet.totalReplyCount\']>0)] \n    [\'snippet.topLevelComment.id\']:\n        replyto.append(reply)\n\n    # create an empty DF to store all the replies\n    # use a for loop to place each item in our replyto list into the function defined above\n    \n    replies = pd.DataFrame()\n    for reply in replyto:\n        df = repliesto(reply)\n        replies = pd.concat([replies, df], ignore_index=True)\n\n']"
Wildcards in robots.txt,"
If in WordPress website I have categories in this order:
-Parent
--Child
---Subchild

I have permalinks set to:
%category%/%postname%
Let use an example.
I create post with post name ""Sport game"".
It's tag is sport-game.
It's full url is: domain.com/parent/child/subchild/sport-game
Why I use this kind of permalinks is exactly to block some content easier in robots.txt.
And now this is the part I have question for.
In robots.txt:
User-agent: Googlebot
Disallow: /parent/*
Disallow: /parent/*/*
Disallow: /parent/*/*/*

Disallow: /parent/* Is meaning of this rule that it's blocking domain.com/parent/child but not domain.com/parent/child/subchild and not domain.com/parent/?
Disallow: /parent/*/*/* Is meaning of this that it's blocking domain.com/parent/child/subchild/, that it's blocking only subchild, not child, not parent, and not posts under subchild?
",4k,"
            4
        ","['\nNote that the * wildcard in Disallow is not part of the original robots.txt specification. Some parsers support it, but as there is no specification, they might all handle it differently.\nAs you seem to be interested in Googlebot, have a look at Google’s robots.txt documentation.\nIn the examples it becomes clear that * means\n\nany string\n\n""Any string"" may, of course, also contain /.\nSo your first line Disallow: /parent/* should block every URL whose path starts with /parent/, including path segments separated by slashes.\nNote that this would be the same as Disallow: /parent/ in the original robots.txt specification, which also blocks any URL whose paths starts with /parent/, for example:\n\nhttp://example.com/parent/\nhttp://example.com/parent/foo\nhttp://example.com/parent/foo.html\nhttp://example.com/parent/foo/bar\nhttp://example.com/parent/foo/bar/\nhttp://example.com/parent/foo/bar/foo.html\n\n']"
How to request Google to re-crawl my website? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 This question does not appear to be about programming within the scope defined in the help center.


Closed 7 years ago.







                        Improve this question
                    



Does someone know a way to request Google to re-crawl a website? If possible, this shouldn't last months. My site is showing an old title in Google's search results. How can I show it with the correct title and description? 
",450k,"
            240
        ","['\nThere are two options. The first (and better) one is using the Fetch as Google option in Webmaster Tools that Mike Flynn commented  about. Here are detailed instructions:\n\nGo to: https://www.google.com/webmasters/tools/ and log in\nIf you haven\'t already, add and verify the site with the ""Add a Site"" button\nClick on the site name for the one you want to manage\nClick Crawl -> Fetch as Google\nOptional: if you want to do a specific page only, type in the URL\nClick Fetch\nClick Submit to Index\nSelect either ""URL"" or ""URL and its direct links""\nClick OK and you\'re done.\n\nWith the option above, as long as every page can be reached from some link on the initial page or a page that it links to, Google should recrawl the whole thing. If you want to explicitly tell it a list of pages to crawl on the domain, you can follow the directions to submit a sitemap.\nYour second (and generally slower) option is, as seanbreeden pointed out, submitting here: http://www.google.com/addurl/\nUpdate 2019:\n\nLogin to - Google Search Console\nAdd a site and verify it with the available methods.\nAfter verification from the console, click on URL Inspection.\nIn the Search bar on top, enter your website URL or custom URLs for inspection and enter.\nAfter Inspection, it\'ll show an option to Request Indexing\nClick on it and GoogleBot will add your website in a Queue for crawling.\n\n', '\nThe usual way is to either resubmit your site in your Google Webmaster Tools or submit it here:  http://www.google.com/addurl/\n', '\nGoogle says that it is unable to control when your site is re-crawled. Regardless, you could also check this post on ""forcing rewcrawls"", I haven\'t tried it myself but it\'s worth a shot if you\'re desperate. \nOn another note, I might add that you make sure you have a sitemap.xml up as this will also help with SEO.\n', '\nAs far I know, if you resubmit a sitemap it will trigger and crawler of your site. \n', ""\nNowadays, the revisiting of a website pretty much depends on its popularity, authority and how often its content changes. Having a sitemap.xml containing all URLs is always better. You can also set the lastmod tag of each URL entries. If you don't abuse it, crawlers will take it into account.\n""]"
What is the difference between web-crawling and web-scraping? [duplicate],"






This question already has answers here:
                        
                    



crawler vs scraper

                                (6 answers)
                            

Closed 4 years ago.



Is there a difference between Crawling and Web-scraping?
If there's a difference, what's the best method to use in order to collect some web data to supply a database for later use in a customised search engine?
",73k,"
            101
        ","[""\nCrawling would be essentially what Google, Yahoo, MSN, etc. do, looking for ANY information.  Scraping is generally targeted at certain websites, for specfic data, e.g. for price comparison, so are coded quite differently.\nUsually a scraper will be bespoke to the websites it is supposed to be scraping, and would be doing things a (good) crawler wouldn't do, i.e.:\n\nHave no regard for robots.txt\nIdentify itself as a browser\nSubmit forms with data \nExecute Javascript (if required to\nact like a user)\n\n"", ""\nYes, they are different. In practice, you may need to use both.\n(I have to jump in because, so far, the other answers don't get to the essence of it. They use examples but don't make the distinctions clear. Granted, they are from 2010!)\nWeb scraping, to use a minimal definition, is the process of processing a web document and extracting information out of it. You can do web scraping without doing web crawling. \nWeb crawling, to use a minimal definition, is the process of iteratively finding and fetching web links starting from a list of seed URL's. Strictly speaking, to do web crawling, you have to do some degree of web scraping (to extract the URL's.)\nTo clear up some concepts mentioned in the other answers:\n\nrobots.txt is intended to apply to any automated process that accesses a web page. So it applies to both crawlers and scrapers.\n'Proper' crawlers and scrapers, both, should identify themselves accurately.\n\nSome references:\n\nWikipedia on web scraping\nWikipedia on web crawlers\nWikipedia on robots.txt\n\n"", '\nAFAIK Web Crawling is what Google does - it goes around a website looking at links and building a database of the layout of that site and sites it links to\nWeb Scraping would be the progamatic analysis of a web page to load some data off of it, EG loading up BBC weather and ripping (scraping) the weather forcast off of it and placing it elsewhere or using it in another program.\n', ""\nThere's a fundamental difference between these two. \nFor those looking to dig deeper, I suggest you read this - \nWeb scraper, Web Crawler\nThis post goes into detail. A good summary is in this chart from the article:\n\n"", ""\nThere's definitely a difference between these two. One refers to visiting a site, the other to extracting.\n"", '\nWe crawl sites to have broad perspective how the site is structured, what are connections between pages, to estimate how much time we need to visit all pages we are interested in. Scraping is often harder to implement, but it’s an essence of data extraction. Let’s think of scraping as of covering website with sheet of paper with some rectangles cut out. We can now see only things we need, completely ignoring parts of website that are common for all pages (like navigation, footer, ads), or extraneous informations as comments or breadcrumbs.\nMore about differences between crawling and scrapping you find here: https://tarantoola.io/web-scraping-vs-web-crawling/\n']"
How to force scrapy to crawl duplicate url?,"
I am learning Scrapy a web crawling framework.
by default it does not crawl duplicate urls or urls which scrapy have already crawled.  
How to make Scrapy to crawl duplicate urls or urls which have already crawled?
I tried to find out on internet but could not find relevant help.  
I found DUPEFILTER_CLASS = RFPDupeFilter and SgmlLinkExtractor from Scrapy - Spider crawls duplicate urls but this question is opposite of what I am looking
",22k,"
            29
        ","[""\nYou're probably looking for the dont_filter=True argument on Request().\nSee http://doc.scrapy.org/en/latest/topics/request-response.html#request-objects\n"", ""\nA more elegant solution is to disable the duplicate filter altogether:\n# settings.py\nDUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'\n\nThis way you don't have to clutter all your Request creation code with dont_filter=True. Another side effect: this only disables duplicate filtering and not any other filters like offsite filtering.\nIf you want to use this setting selectively for only one or some of multiple spiders in your project, you can set it via custom_settings in the spider implementation:\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    custom_settings = {\n        'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n    }\n\n""]"
How to generate the start_urls dynamically in crawling?,"
I am crawling a site which may contain a lot of start_urls, like:
http://www.a.com/list_1_2_3.htm

I want to populate start_urls like [list_\d+_\d+_\d+\.htm],
and extract items from URLs like [node_\d+\.htm] during crawling. 
Can I use CrawlSpider to realize this function?
And how can I generate the start_urls dynamically in crawling?
",20k,"
            27
        ","[""\nThe best way to generate URLs dynamically is to override the start_requests method of the spider:  \n\nfrom scrapy.http.request import Request\n\ndef start_requests(self):\n      with open('urls.txt', 'rb') as urls:\n          for url in urls:\n              yield Request(url, self.parse)\n\n\n"", ""\nThere are two questions:\n1)yes you can realize this functionality by using Rules e.g ,\nrules =(Rule(SgmlLinkExtractor(allow = ('node_\\d+.htm')) ,callback = 'parse'))\n\nsuggested reading\n2) yes you can generate start_urls dynamically ,  start_urls is a \n\nlist\n\ne.g >>> start_urls = ['http://www.a.com/%d_%d_%d' %(n,n+1,n+2) for n in range(0, 26)]\n>>> start_urls\n\n['http://www.a.com/0_1_2', 'http://www.a.com/1_2_3', 'http://www.a.com/2_3_4', 'http://www.a.com/3_4_5', 'http://www.a.com/4_5_6', 'http://www.a.com/5_6_7',  'http://www.a.com/6_7_8', 'http://www.a.com/7_8_9', 'http://www.a.com/8_9_10','http://www.a.com/9_10_11', 'http://www.a.com/10_11_12', 'http://www.a.com/11_12_13', 'http://www.a.com/12_13_14', 'http://www.a.com/13_14_15', 'http://www.a.com/14_15_16', 'http://www.a.com/15_16_17', 'http://www.a.com/16_17_18', 'http://www.a.com/17_18_19', 'http://www.a.com/18_19_20', 'http://www.a.com/19_20_21', 'http://www.a.com/20_21_22', 'http://www.a.com/21_22_23', 'http://www.a.com/22_23_24', 'http://www.a.com/23_24_25', 'http://www.a.com/24_25_26', 'http://www.a.com/25_26_27']\n\n""]"
"Save complete web page (incl css, images) using python/selenium","
I am using Python/Selenium to submit genetic sequences to an online database, and want to save the full page of results I get back. Below is the code that gets me to the results I want:
from selenium import webdriver

URL = 'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome'
SEQUENCE = 'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA' #'GAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGA'
CHROME_WEBDRIVER_LOCATION = '/home/max/Downloads/chromedriver' # update this for your machine

# open page with selenium
# (first need to download Chrome webdriver, or a firefox webdriver, etc)
driver = webdriver.Chrome(executable_path=CHROME_WEBDRIVER_LOCATION)
driver.get(URL)
time.sleep(5)

# enter sequence into the query field and hit 'blast' button to search
seq_query_field = driver.find_element_by_id(""seq"")
seq_query_field.send_keys(SEQUENCE)

blast_button = driver.find_element_by_id(""b1"")
blast_button.click()
time.sleep(60)

At that point I have a page that I can manually click ""save as,"" and get a local file (with a corresponding folder of image/js assets) that lets me view the whole returned page locally (minus content which is generated dynamically from scrolling down the page, which is fine). I assumed there would be a simple way to mimic this 'save as' function in python/selenium but haven't found one. The code to save the page below just saves html, and does not leave me with a local file that looks like it does in the web browser, with images, etc.
content = driver.page_source
with open('webpage.html', 'w') as f:
    f.write(content)

I've also found this question/answer on SO, but the accepted answer just brings up the 'save as' box, and does not provide a way to click it (as two commenters point out)
Is there a simple way to 'save [full page] as' using python? Ideally I'd prefer an answer using selenium since selenium makes the crawling part so straightforward, but I'm open to using another library if there's a better tool for this job. Or maybe I just need to specify all of the images/tables I want to download in code, and there is no shortcut to emulating the right-click 'save as' functionality?
UPDATE - Follow up question for James' answer
So I ran James' code to generate a page.html (and associated files) and compared it to the html file I got from manually clicking save-as. The page.html saved via James' script is great and has everything I need, but when opened in a browser it also shows a lot of extra formatting text that's hidden in the manually save'd page. See attached screenshot (manually saved page on the left, script-saved page with extra formatting text shown on right). 

This is especially surprising to me because the raw html of the page saved by James' script seems to indicate those fields should still be hidden. See e.g. the html below, which appears the same in both files, but the text at issue only appears in the browser-rendered page on the one saved by James' script:
<p class=""helpbox ui-ncbitoggler-slave ui-ncbitoggler"" id=""hlp1"" aria-hidden=""true"">
These options control formatting of alignments in results pages. The
default is HTML, but other formats (including plain text) are available.
PSSM and PssmWithParameters are representations of Position Specific Scoring Matrices and are only available for PSI-BLAST. 
The Advanced view option allows the database descriptions to be sorted by various indices in a table.
</p>

Any idea why this is happening?
",19k,"
            25
        ","['\nAs you noted, Selenium cannot interact with the browser\'s context menu to use Save as..., so instead to do so, you could use an external automation library like pyautogui.\npyautogui.hotkey(\'ctrl\', \'s\')\ntime.sleep(1)\npyautogui.typewrite(SEQUENCE + \'.html\')\npyautogui.hotkey(\'enter\')\n\nThis code opens the Save as... window through its keyboard shortcut CTRL+S and then saves the webpage and its assets into the default downloads location by pressing enter. This code also names the file as the sequence in order to give it a unique name, though you could change this for your use case. If needed, you could additionally change the download location through some extra work with the tab and arrow keys.\nTested on Ubuntu 18.10; depending on your OS you may need to modify the key combination sent.\n\nFull code, in which I also added conditional waits to improve speed:\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.expected_conditions import visibility_of_element_located\nfrom selenium.webdriver.support.ui import WebDriverWait\nimport pyautogui\n\nURL = \'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome\'\nSEQUENCE = \'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA\' #\'GAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGA\'\n\n# open page with selenium\n# (first need to download Chrome webdriver, or a firefox webdriver, etc)\ndriver = webdriver.Chrome()\ndriver.get(URL)\n\n# enter sequence into the query field and hit \'blast\' button to search\nseq_query_field = driver.find_element_by_id(""seq"")\nseq_query_field.send_keys(SEQUENCE)\n\nblast_button = driver.find_element_by_id(""b1"")\nblast_button.click()\n\n# wait until results are loaded\nWebDriverWait(driver, 60).until(visibility_of_element_located((By.ID, \'grView\')))\n\n# open \'Save as...\' to save html and assets\npyautogui.hotkey(\'ctrl\', \'s\')\ntime.sleep(1)\npyautogui.typewrite(SEQUENCE + \'.html\')\npyautogui.hotkey(\'enter\')\n\n', '\nThis is not a perfect solution, but it will get you most of what you need.  You can replicate the behavior of ""save as full web page (complete)"" by parsing the html and downloading any loaded files (images, css, js, etc.) to their same relative path.  \nMost of the javascript won\'t work due to cross origin request blocking.  But the content will look (mostly) the same.\nThis uses requests to save the loaded files, lxml to parse the html, and os for the path legwork.\nfrom selenium import webdriver\nimport chromedriver_binary\nfrom lxml import html\nimport requests\nimport os\n\ndriver = webdriver.Chrome()\nURL = \'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome\'\nSEQUENCE = \'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA\' \nbase = \'https://blast.ncbi.nlm.nih.gov/\'\n\ndriver.get(URL)\nseq_query_field = driver.find_element_by_id(""seq"")\nseq_query_field.send_keys(SEQUENCE)\nblast_button = driver.find_element_by_id(""b1"")\nblast_button.click()\n\ncontent = driver.page_source\n# write the page content\nos.mkdir(\'page\')\nwith open(\'page/page.html\', \'w\') as fp:\n    fp.write(content)\n\n# download the referenced files to the same path as in the html\nsess = requests.Session()\nsess.get(base)            # sets cookies\n\n# parse html\nh = html.fromstring(content)\n# get css/js files loaded in the head\nfor hr in h.xpath(\'head//@href\'):\n    if not hr.startswith(\'http\'):\n        local_path = \'page/\' + hr\n        hr = base + hr\n    res = sess.get(hr)\n    if not os.path.exists(os.path.dirname(local_path)):\n        os.makedirs(os.path.dirname(local_path))\n    with open(local_path, \'wb\') as fp:\n        fp.write(res.content)\n\n# get image/js files from the body.  skip anything loaded from outside sources\nfor src in h.xpath(\'//@src\'):\n    if not src or src.startswith(\'http\'):\n        continue\n    local_path = \'page/\' + src\n    print(local_path)\n    src = base + src\n    res = sess.get(hr)\n    if not os.path.exists(os.path.dirname(local_path)):\n        os.makedirs(os.path.dirname(local_path))\n    with open(local_path, \'wb\') as fp:\n        fp.write(res.content)  \n\nYou should have a folder called page with a file called page.html in it with the content you are after.\n', '\nInspired by FThompson\'s answer above, I came up with the following tool that can download full/complete html for a given page url (see: https://github.com/markfront/SinglePageFullHtml)\nUPDATE - follow up with Max\'s suggestion, below are steps to use the tool:\n\nClone the project, then run maven to build:\n\n$> git clone https://github.com/markfront/SinglePageFullHtml.git\n\n$> cd ~/git/SinglePageFullHtml\n$> mvn clean compile package\n\n\nFind the generated jar file in target folder: SinglePageFullHtml-1.0-SNAPSHOT-jar-with-dependencies.jar\n\nRun the jar in command line like:\n\n\n$> java -jar .target/SinglePageFullHtml-1.0-SNAPSHOT-jar-with-dependencies.jar <page_url>\n\n\nThe result file name will have a prefix ""FP, followed by the hashcode of the page url, with file extension "".html"". It will be found in either folder ""/tmp"" (which you can get by System.getProperty(""java.io.tmp""). If not, try find it in your home dir or System.getProperty(""user.home"") in Java).\n\nThe result file will be a big fat self-contained html file that includes everything (css, javascript, images, etc.) referred to by the original html source.\n\n\n', '\nI\'ll advise u to have a try on sikulix which is an image based automation tool for operate any widgets within PC OS, it supports python grammar and run with command line and maybe the simplest way to solve ur problem.\nAll u need to do is just give it a screenshot, call sikulix script in ur python automation script(with OS.system(""xxxx"") or subprocess...).\n']"
Does solr do web crawling?,"
I am interested to do web crawling. I was looking at solr. 
Does solr do web crawling, or what are the steps to do web crawling?
",30k,"
            18
        ","[""\nSolr 5+ DOES in fact now do web crawling!\nhttp://lucene.apache.org/solr/\nOlder Solr versions do not do web crawling alone, as historically it's a search server that provides full text search capabilities.  It builds on top of Lucene.\nIf you need to crawl web pages using another Solr project then you have a number of options including:\n\nNutch - http://lucene.apache.org/nutch/\nWebsphinx - http://www.cs.cmu.edu/~rcm/websphinx/\nJSpider - http://j-spider.sourceforge.net/\nHeritrix - http://crawler.archive.org/\n\nIf you want to make use of the search facilities provided by Lucene or SOLR you'll need to build indexes from the web crawl results.\nSee this also: \nLucene crawler (it needs to build lucene index)\n"", '\nSolr does not in of itself have a web crawling feature.\nNutch is the ""de-facto"" crawler (and then some) for Solr.\n', '\nSolr 5 started supporting simple webcrawling (Java Doc). If want search, Solr is the tool, if you want to crawl, Nutch/Scrapy is better :) \nTo get it up and running, you can take a detail look at here. However, here is how to get it up and running in one line: \njava \n-classpath <pathtosolr>/dist/solr-core-5.4.1.jar \n-Dauto=yes \n-Dc=gettingstarted     -> collection: gettingstarted\n-Ddata=web             -> web crawling and indexing\n-Drecursive=3          -> go 3 levels deep\n-Ddelay=0              -> for the impatient use 10+ for production\norg.apache.solr.util.SimplePostTool   -> SimplePostTool\nhttp://datafireball.com/      -> a testing wordpress blog\n\nThe crawler here is very ""naive"" where you can find all the code from this Apache Solr\'s github repo.\nHere is how the response looks like: \nSimplePostTool version 5.0.0\nPosting web pages to Solr url http://localhost:8983/solr/gettingstarted/update/extract\nEntering auto mode. Indexing pages with content-types corresponding to file endings xml,json,csv,pdf,doc,docx,ppt,pptx,xls,xlsx,odt,odp,ods,ott,otp,ots,rtf,htm,html,txt,log\nSimplePostTool: WARNING: Never crawl an external web site faster than every 10 seconds, your IP will probably be blocked\nEntering recursive mode, depth=3, delay=0s\nEntering crawl at level 0 (1 links total, 1 new)\nPOSTed web resource http://datafireball.com (depth: 0)\nEntering crawl at level 1 (52 links total, 51 new)\nPOSTed web resource http://datafireball.com/2015/06 (depth: 1)\n...\nEntering crawl at level 2 (266 links total, 215 new)\n...\nPOSTed web resource http://datafireball.com/2015/08/18/a-few-functions-about-python-path (depth: 2)\n...\nEntering crawl at level 3 (846 links total, 656 new)\nPOSTed web resource http://datafireball.com/2014/09/06/node-js-web-scraping-using-cheerio (depth: 3)\nSimplePostTool: WARNING: The URL http://datafireball.com/2014/09/06/r-lattice-trellis-another-framework-for-data-visualization/?share=twitter returned a HTTP result status of 302\n423 web pages indexed.\nCOMMITting Solr index changes to http://localhost:8983/solr/gettingstarted/update/extract...\nTime spent: 0:05:55.059\n\nIn the end, you can see all the data are indexed properly. \n \n', '\nYou might also want to take a look at \nhttp://www.crawl-anywhere.com/\nVery powerful crawler that is compatible with Solr. \n', ""\nI have been using Nutch with Solr on my latest project and it seems to work quite nicely.\nIf you are using a Windows machine then I would strongly recommend following the 'No cygwin' instructions given by Jason Riffel too!\n"", '\nYes, I agree with the other posts here, use Apache Nutch\n\nbin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 3 -topN 5\n\nAlthough your solr version has the match the correct version of Nutch, because older versions of solr stores the indices in a different format\nIts tutorial:\nhttp://wiki.apache.org/nutch/NutchTutorial\n', ""\nI know it's been a while, but in case someone else is searching for a Solr crawler like me, there is a new open-source crawler called Norconex HTTP Collector \n"", ""\nI know this question is quite old, but I'll respond anyway for the newcomer that will wonder here.\nIn order to use Solr, you can use a web crawler that is capable of storing documents in Solr.\nFor instance, The Norconex HTTP Collector is a flexible and powerful open-source web crawler that is compatible with Solr.\nTo use Solr with the Norconex HTTP Collector you will need the Norconex HTTP Collector which is used to crawl the website that you want to collect data from, and you will need to install the Norconex Apache Solr Committer to store collected documents into Solr. When the committer is installed, you will need to configure the XML configuration file of the crawler. I would recommend that you follow this link to get started test how the crawler works and here to know how to configure the configuration file. Finally, you will need this link to configure the committer section of the configuration file with Solr.\nNote that if your goal is not to crawl web pages, Norconex also has a Filesystem Collector that can be used with the Sorl Committer as well.\n"", '\nDef Nutch !\nNutch also has a basic web front end which will let you query your search results. You might not even need to bother with SOLR depending on your requirements. If you do a Nutch/SOLR combination you should be able to take advantage of the recent work done to integrate SOLR and Nutch ...  http://issues.apache.org/jira/browse/NUTCH-442 \n']"
Submit data via web form and extract the results,"
My python level is Novice. I have never written a web scraper or crawler. I have written a python code to connect to an api and extract the data that I want. But for some the extracted data I want to get the gender of the author. I found this web site http://bookblog.net/gender/genie.php but downside is there isn't an api available. I was wondering how to write a python to submit data to the form in the page and extract the return data. It would be a great help if I could get some guidance on this.
This is the form dom: 
<form action=""analysis.php"" method=""POST"">
<textarea cols=""75"" rows=""13"" name=""text""></textarea>
<div class=""copyright"">(NOTE: The genie works best on texts of more than 500 words.)</div>
<p>
<b>Genre:</b>
<input type=""radio"" value=""fiction"" name=""genre"">
fiction&nbsp;&nbsp;
<input type=""radio"" value=""nonfiction"" name=""genre"">
nonfiction&nbsp;&nbsp;
<input type=""radio"" value=""blog"" name=""genre"">
blog entry
</p>
<p>
</form>

results page dom:
<p>
<b>The Gender Genie thinks the author of this passage is:</b>
male!
</p>

",49k,"
            17
        ","['\nNo need to use mechanize, just send the correct form data in a POST request.\nAlso, using regular expression to parse HTML is a bad idea. You would be better off using a HTML parser like lxml.html.\nimport requests\nimport lxml.html as lh\n\n\ndef gender_genie(text, genre):\n    url = \'http://bookblog.net/gender/analysis.php\'\n    caption = \'The Gender Genie thinks the author of this passage is:\'\n\n    form_data = {\n        \'text\': text,\n        \'genre\': genre,\n        \'submit\': \'submit\',\n    }\n\n    response = requests.post(url, data=form_data)\n\n    tree = lh.document_fromstring(response.content)\n\n    return tree.xpath(""//b[text()=$caption]"", caption=caption)[0].tail.strip()\n\n\nif __name__ == \'__main__\':\n    print gender_genie(\'I have a beard!\', \'blog\')\n\n', '\nYou can use mechanize to submit and retrieve content, and the re module for getting what you want. For example, the script below does it for the text of your own question:\nimport re\nfrom mechanize import Browser\n\ntext = """"""\nMy python level is Novice. I have never written a web scraper \nor crawler. I have written a python code to connect to an api and \nextract the data that I want. But for some the extracted data I want to \nget the gender of the author. I found this web site \nhttp://bookblog.net/gender/genie.php but downside is there isn\'t an api \navailable. I was wondering how to write a python to submit data to the \nform in the page and extract the return data. It would be a great help \nif I could get some guidance on this.""""""\n\nbrowser = Browser()\nbrowser.open(""http://bookblog.net/gender/genie.php"")\n\nbrowser.select_form(nr=0)\nbrowser[\'text\'] = text\nbrowser[\'genre\'] = [\'nonfiction\']\n\nresponse = browser.submit()\n\ncontent = response.read()\n\nresult = re.findall(\n    r\'<b>The Gender Genie thinks the author of this passage is:</b> (\\w*)!\', content)\n\nprint result[0]\n\nWhat does it do? It creates a mechanize.Browser and goes to the given URL:\nbrowser = Browser()\nbrowser.open(""http://bookblog.net/gender/genie.php"")\n\nThen it selects the form (since there is only one form to be filled, it will be the first):\nbrowser.select_form(nr=0)\n\nAlso, it sets the entries of the form...\nbrowser[\'text\'] = text\nbrowser[\'genre\'] = [\'nonfiction\']\n\n... and submit it:\nresponse = browser.submit()\n\nNow, we get the result:\ncontent = response.read()\n\nWe know that the result is in the form:\n<b>The Gender Genie thinks the author of this passage is:</b> male!\n\nSo we create a regex for matching and use re.findall():\nresult = re.findall(\n    r\'<b>The Gender Genie thinks the author of this passage is:</b> (\\w*)!\',\n    content)\n\nNow the result is available for your use:\nprint result[0]\n\n', '\nYou can use mechanize, see examples for details.\nfrom mechanize import ParseResponse, urlopen, urljoin\n\nuri = ""http://bookblog.net""\n\nresponse = urlopen(urljoin(uri, ""/gender/genie.php""))\nforms = ParseResponse(response, backwards_compat=False)\nform = forms[0]\n\n#print form\n\nform[\'text\'] = \'cheese\'\nform[\'genre\'] = [\'fiction\']\n\nprint urlopen(form.click()).read()\n\n']"
How to make a polygon radar (spider) chart in python,"
import matplotlib.pyplot as plt
import numpy as np

labels=['Siege', 'Initiation', 'Crowd_control', 'Wave_clear', 'Objective_damage']
markers = [0, 1, 2, 3, 4, 5]
str_markers = [""0"", ""1"", ""2"", ""3"", ""4"", ""5""]

def make_radar_chart(name, stats, attribute_labels=labels,
                     plot_markers=markers, plot_str_markers=str_markers):

    labels = np.array(attribute_labels)

    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)
    stats = np.concatenate((stats,[stats[0]]))
    angles = np.concatenate((angles,[angles[0]]))

    fig = plt.figure()
    ax = fig.add_subplot(111, polar=True)
    ax.plot(angles, stats, 'o-', linewidth=2)
    ax.fill(angles, stats, alpha=0.25)
    ax.set_thetagrids(angles * 180/np.pi, labels)
    plt.yticks(markers)
    ax.set_title(name)
    ax.grid(True)

    fig.savefig(""static/images/%s.png"" % name)

    return plt.show()

make_radar_chart(""Agni"", [2,3,4,4,5]) # example



Basically I want the chart to be a pentagon instead of circle. Can anyone help with this. I am using python matplotlib to save an image which will stored and displayed later. I want my chart to have the form of the second picture
EDIT:
    gridlines = ax.yaxis.get_gridlines()
    for gl in gridlines:
        gl.get_path()._interpolation_steps = 5

adding this section of code from answer below helped a lot. I am getting this chart. Still need to figure out how to get rid of the outer most ring: 
",27k,"
            17
        ","['\nThe radar chart demo shows how to make the a radar chart. The result looks like this:\n\nHere, the outer spine is polygon shaped as desired. However the inner grid lines are circular. \nSo the open question is how to make the gridlines the same shape as the spines.\nThis can be done by overriding the draw method and setting the gridlines\' path interpolation step variable to the number of variables of the RadarAxes class.\ngridlines = self.yaxis.get_gridlines()\nfor gl in gridlines:\n    gl.get_path()._interpolation_steps = num_vars\n\nComplete example:\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, RegularPolygon\nfrom matplotlib.path import Path\nfrom matplotlib.projections.polar import PolarAxes\nfrom matplotlib.projections import register_projection\nfrom matplotlib.spines import Spine\nfrom matplotlib.transforms import Affine2D\n\n\ndef radar_factory(num_vars, frame=\'circle\'):\n    """"""Create a radar chart with `num_vars` axes.\n\n    This function creates a RadarAxes projection and registers it.\n\n    Parameters\n    ----------\n    num_vars : int\n        Number of variables for radar chart.\n    frame : {\'circle\' | \'polygon\'}\n        Shape of frame surrounding axes.\n\n    """"""\n    # calculate evenly-spaced axis angles\n    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n\n    class RadarAxes(PolarAxes):\n\n        name = \'radar\'\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            # rotate plot such that the first axis is at the top\n            self.set_theta_zero_location(\'N\')\n\n        def fill(self, *args, closed=True, **kwargs):\n            """"""Override fill so that line is closed by default""""""\n            return super().fill(closed=closed, *args, **kwargs)\n\n        def plot(self, *args, **kwargs):\n            """"""Override plot so that line is closed by default""""""\n            lines = super().plot(*args, **kwargs)\n            for line in lines:\n                self._close_line(line)\n\n        def _close_line(self, line):\n            x, y = line.get_data()\n            # FIXME: markers at x[0], y[0] get doubled-up\n            if x[0] != x[-1]:\n                x = np.concatenate((x, [x[0]]))\n                y = np.concatenate((y, [y[0]]))\n                line.set_data(x, y)\n\n        def set_varlabels(self, labels):\n            self.set_thetagrids(np.degrees(theta), labels)\n\n        def _gen_axes_patch(self):\n            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n            # in axes coordinates.\n            if frame == \'circle\':\n                return Circle((0.5, 0.5), 0.5)\n            elif frame == \'polygon\':\n                return RegularPolygon((0.5, 0.5), num_vars,\n                                      radius=.5, edgecolor=""k"")\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n        def draw(self, renderer):\n            """""" Draw. If frame is polygon, make gridlines polygon-shaped """"""\n            if frame == \'polygon\':\n                gridlines = self.yaxis.get_gridlines()\n                for gl in gridlines:\n                    gl.get_path()._interpolation_steps = num_vars\n            super().draw(renderer)\n\n\n        def _gen_axes_spines(self):\n            if frame == \'circle\':\n                return super()._gen_axes_spines()\n            elif frame == \'polygon\':\n                # spine_type must be \'left\'/\'right\'/\'top\'/\'bottom\'/\'circle\'.\n                spine = Spine(axes=self,\n                              spine_type=\'circle\',\n                              path=Path.unit_regular_polygon(num_vars))\n                # unit_regular_polygon gives a polygon of radius 1 centered at\n                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n                # 0.5) in axes coordinates.\n                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n                                    + self.transAxes)\n\n\n                return {\'polar\': spine}\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n    register_projection(RadarAxes)\n    return theta\n\n\ndata = [[\'Sulfate\', \'Nitrate\', \'EC\', \'OC1\', \'OC2\', \'OC3\', \'OP\', \'CO\', \'O3\'],\n        (\'Basecase\', [\n            [0.88, 0.01, 0.03, 0.03, 0.00, 0.06, 0.01, 0.00, 0.00],\n            [0.07, 0.95, 0.04, 0.05, 0.00, 0.02, 0.01, 0.00, 0.00],\n            [0.01, 0.02, 0.85, 0.19, 0.05, 0.10, 0.00, 0.00, 0.00],\n            [0.02, 0.01, 0.07, 0.01, 0.21, 0.12, 0.98, 0.00, 0.00],\n            [0.01, 0.01, 0.02, 0.71, 0.74, 0.70, 0.00, 0.00, 0.00]])]\n\nN = len(data[0])\ntheta = radar_factory(N, frame=\'polygon\')\n\nspoke_labels = data.pop(0)\ntitle, case_data = data[0]\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection=\'radar\'))\nfig.subplots_adjust(top=0.85, bottom=0.05)\n\nax.set_rgrids([0.2, 0.4, 0.6, 0.8])\nax.set_title(title,  position=(0.5, 1.1), ha=\'center\')\n\nfor d in case_data:\n    line = ax.plot(theta, d)\n    ax.fill(theta, d,  alpha=0.25)\nax.set_varlabels(spoke_labels)\n\nplt.show()\n\n\n', '\nAs shown in this other post the answer from @ImportanceOfBeingErnest doesn\'t work in matplotlib>3.2.2 in that you get circular grids. As shown in this PR you can use the following\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, RegularPolygon\nfrom matplotlib.path import Path\nfrom matplotlib.projections.polar import PolarAxes\nfrom matplotlib.projections import register_projection\nfrom matplotlib.spines import Spine\nfrom matplotlib.transforms import Affine2D\n\n\ndef radar_factory(num_vars, frame=\'circle\'):\n    """"""Create a radar chart with `num_vars` axes.\n\n    This function creates a RadarAxes projection and registers it.\n\n    Parameters\n    ----------\n    num_vars : int\n        Number of variables for radar chart.\n    frame : {\'circle\' | \'polygon\'}\n        Shape of frame surrounding axes.\n\n    """"""\n    # calculate evenly-spaced axis angles\n    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n    \n    class RadarTransform(PolarAxes.PolarTransform):\n        def transform_path_non_affine(self, path):\n            # Paths with non-unit interpolation steps correspond to gridlines,\n            # in which case we force interpolation (to defeat PolarTransform\'s\n            # autoconversion to circular arcs).\n            if path._interpolation_steps > 1:\n                path = path.interpolated(num_vars)\n            return Path(self.transform(path.vertices), path.codes)\n\n    class RadarAxes(PolarAxes):\n\n        name = \'radar\'\n        \n        PolarTransform = RadarTransform\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            # rotate plot such that the first axis is at the top\n            self.set_theta_zero_location(\'N\')\n\n        def fill(self, *args, closed=True, **kwargs):\n            """"""Override fill so that line is closed by default""""""\n            return super().fill(closed=closed, *args, **kwargs)\n\n        def plot(self, *args, **kwargs):\n            """"""Override plot so that line is closed by default""""""\n            lines = super().plot(*args, **kwargs)\n            for line in lines:\n                self._close_line(line)\n\n        def _close_line(self, line):\n            x, y = line.get_data()\n            # FIXME: markers at x[0], y[0] get doubled-up\n            if x[0] != x[-1]:\n                x = np.concatenate((x, [x[0]]))\n                y = np.concatenate((y, [y[0]]))\n                line.set_data(x, y)\n\n        def set_varlabels(self, labels):\n            self.set_thetagrids(np.degrees(theta), labels)\n\n        def _gen_axes_patch(self):\n            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n            # in axes coordinates.\n            if frame == \'circle\':\n                return Circle((0.5, 0.5), 0.5)\n            elif frame == \'polygon\':\n                return RegularPolygon((0.5, 0.5), num_vars,\n                                      radius=.5, edgecolor=""k"")\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n        def draw(self, renderer):\n            """""" Draw. If frame is polygon, make gridlines polygon-shaped """"""\n            if frame == \'polygon\':\n                gridlines = self.yaxis.get_gridlines()\n                for gl in gridlines:\n                    gl.get_path()._interpolation_steps = num_vars\n            super().draw(renderer)\n\n\n        def _gen_axes_spines(self):\n            if frame == \'circle\':\n                return super()._gen_axes_spines()\n            elif frame == \'polygon\':\n                # spine_type must be \'left\'/\'right\'/\'top\'/\'bottom\'/\'circle\'.\n                spine = Spine(axes=self,\n                              spine_type=\'circle\',\n                              path=Path.unit_regular_polygon(num_vars))\n                # unit_regular_polygon gives a polygon of radius 1 centered at\n                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n                # 0.5) in axes coordinates.\n                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n                                    + self.transAxes)\n\n\n                return {\'polar\': spine}\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n    register_projection(RadarAxes)\n    return theta\n\n\ndata = [[\'Sulfate\', \'Nitrate\', \'EC\', \'OC1\', \'OC2\', \'OC3\', \'OP\', \'CO\', \'O3\'],\n        (\'Basecase\', [\n            [0.88, 0.01, 0.03, 0.03, 0.00, 0.06, 0.01, 0.00, 0.00],\n            [0.07, 0.95, 0.04, 0.05, 0.00, 0.02, 0.01, 0.00, 0.00],\n            [0.01, 0.02, 0.85, 0.19, 0.05, 0.10, 0.00, 0.00, 0.00],\n            [0.02, 0.01, 0.07, 0.01, 0.21, 0.12, 0.98, 0.00, 0.00],\n            [0.01, 0.01, 0.02, 0.71, 0.74, 0.70, 0.00, 0.00, 0.00]])]\n\nN = len(data[0])\ntheta = radar_factory(N, frame=\'polygon\')\n\nspoke_labels = data.pop(0)\ntitle, case_data = data[0]\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection=\'radar\'))\nfig.subplots_adjust(top=0.85, bottom=0.05)\n\nax.set_rgrids([0.2, 0.4, 0.6, 0.8])\nax.set_title(title,  position=(0.5, 1.1), ha=\'center\')\n\nfor d in case_data:\n    line = ax.plot(theta, d)\n    ax.fill(theta, d, alpha=0.25, label=\'_nolegend_\')\nax.set_varlabels(spoke_labels)\n\nplt.show()\n\nto get the desired:\n\n']"
Get Scrapy crawler output/results in script file function,"
I am using a script file to run a spider within scrapy project and spider is logging the crawler output/results. But i want to use spider output/results in that script file in some function .I did not want to save output/results in any file or DB.
Here is Script code get from https://doc.scrapy.org/en/latest/topics/practices.html#run-from-script
from twisted.internet import reactor
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging
from scrapy.utils.project import get_project_settings

configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})
runner = CrawlerRunner(get_project_settings())


d = runner.crawl('my_spider')
d.addBoth(lambda _: reactor.stop())
reactor.run()

def spider_output(output):
#     do something to that output

How can i get spider output in 'spider_output' method. It is possible to get output/results.
",9k,"
            15
        ","[""\nHere is the solution that get all output/results in a list\nfrom scrapy import signals\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\nfrom scrapy.signalmanager import dispatcher\n\n\ndef spider_results():\n    results = []\n\n    def crawler_results(signal, sender, item, response, spider):\n        results.append(item)\n\n    dispatcher.connect(crawler_results, signal=signals.item_scraped)\n\n    process = CrawlerProcess(get_project_settings())\n    process.crawl(MySpider)\n    process.start()  # the script will block here until the crawling is finished\n    return results\n\n\nif __name__ == '__main__':\n    print(spider_results())\n\n"", '\nThis is an old question, but for future reference. If you are working with python 3.6+ I recommend using scrapyscript that allows you to run your Spiders and get the results in a super simple way:\nfrom scrapyscript import Job, Processor\nfrom scrapy.spiders import Spider\nfrom scrapy import Request\nimport json\n\n# Define a Scrapy Spider, which can accept *args or **kwargs\n# https://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments\nclass PythonSpider(Spider):\n    name = \'myspider\'\n\n    def start_requests(self):\n        yield Request(self.url)\n\n    def parse(self, response):\n        title = response.xpath(\'//title/text()\').extract()\n        return {\'url\': response.request.url, \'title\': title}\n\n# Create jobs for each instance. *args and **kwargs supplied here will\n# be passed to the spider constructor at runtime\ngithubJob = Job(PythonSpider, url=\'http://www.github.com\')\npythonJob = Job(PythonSpider, url=\'http://www.python.org\')\n\n# Create a Processor, optionally passing in a Scrapy Settings object.\nprocessor = Processor(settings=None)\n\n# Start the reactor, and block until all spiders complete.\ndata = processor.run([githubJob, pythonJob])\n\n# Print the consolidated results\nprint(json.dumps(data, indent=4))\n\n[\n    {\n        ""title"": [\n            ""Welcome to Python.org""\n        ],\n        ""url"": ""https://www.python.org/""\n    },\n    {\n        ""title"": [\n            ""The world\'s leading software development platform \\u00b7 GitHub"",\n            ""1clr-code-hosting""\n        ],\n        ""url"": ""https://github.com/""\n    }\n]\n\n', ""\nAFAIK there is no way to do this, since crawl():\n\nReturns a deferred that is fired when the crawling is finished.\n\nAnd the crawler doesn't store results anywhere other than outputting them to logger.\nHowever returning ouput would conflict with the whole asynchronious nature and structure of scrapy, so saving to file then reading it is a prefered approach here.\nYou can simply devise pipeline that saves your items to file and simply read the file in your spider_output. You will receive your results since reactor.run() is blocking your script untill the output file is complete anyways.\n"", ""\nMy advice is to use the Python subprocess module to run spider from the script rather than using the method provided in the scrapy docs to run spider from python script. The reason for that is that with the subprocess module, you can capture the output/logs and even statements that you print from inside the spider.\nIn Python 3, execute the spider with the run method. Ex.\nimport subprocess\nprocess = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nif process.returncode == 0:\n    result = process.stdout.decode('utf-8')\nelse:\n    # code to check error using 'process.stderr'\n\nSetting the stdout/stderr to subprocess.PIPE will allow capture of output so it's very important to set this flag.\nHere command should be a sequence or a string (It it's a string, then call the run method with 1 more param: shell=True). For example:\ncommand = ['scrapy', 'crawl', 'website', '-a', 'customArg=blahblah']\n# or\ncommand = 'scrapy crawl website -a customArg=blahblah' # with shell=True\n#or\nimport shlex\ncommand = shlex.split('scrapy crawl website -a customArg=blahblah') # without shell=True\n\nAlso, process.stdout will contain the output from the script but it will be of type bytes. You need to convert it to str using decode('utf-8')\n"", ""\nIt's going to return all the results of a Spider within a list.\nfrom scrapyscript import Job, Processor\nfrom scrapy.utils.project import get_project_settings\n\n\ndef get_spider_output(spider, **kwargs):\n    job = Job(spider, **kwargs)\n    processor = Processor(settings=get_project_settings())\n    return processor.run([job])\n\n""]"
Using one Scrapy spider for several websites,"
I need to create a user configurable web spider/crawler, and I'm thinking about using Scrapy. But, I can't hard-code the domains and allowed URL regex:es -- this will instead be configurable in a GUI.
How do I (as simple as possible) create a spider or a set of spiders with Scrapy where the domains and allowed URL regex:es are dynamically configurable? E.g. I write the configuration to a file, and the spider reads it somehow.
",7k,"
            12
        ","[""\nWARNING: This answer was for Scrapy v0.7, spider manager api changed a lot since then.\nOverride default SpiderManager class, load your custom rules from a database or somewhere else and instanciate a custom spider with your own rules/regexes and domain_name\nin mybot/settings.py:\nSPIDER_MANAGER_CLASS = 'mybot.spidermanager.MySpiderManager'\n\nin mybot/spidermanager.py:\nfrom mybot.spider import MyParametrizedSpider\n\nclass MySpiderManager(object):\n    loaded = True\n\n    def fromdomain(self, name):\n        start_urls, extra_domain_names, regexes = self._get_spider_info(name)\n        return MyParametrizedSpider(name, start_urls, extra_domain_names, regexes)\n\n    def close_spider(self, spider):\n        # Put here code you want to run before spiders is closed\n        pass\n\n    def _get_spider_info(self, name):\n        # query your backend (maybe a sqldb) using `name` as primary key, \n        # and return start_urls, extra_domains and regexes\n        ...\n        return (start_urls, extra_domains, regexes)\n\nand now your custom spider class, in mybot/spider.py:\nfrom scrapy.spider import BaseSpider\n\nclass MyParametrizedSpider(BaseSpider):\n\n    def __init__(self, name, start_urls, extra_domain_names, regexes):\n        self.domain_name = name\n        self.start_urls = start_urls\n        self.extra_domain_names = extra_domain_names\n        self.regexes = regexes\n\n     def parse(self, response):\n         ...\n\nNotes:\n\nYou can extend CrawlSpider too if you want to take advantage of its Rules system\nTo run a spider use:  ./scrapy-ctl.py crawl <name>, where name is passed to SpiderManager.fromdomain and is the key to retreive more spider info from the backend system\nAs solution overrides default SpiderManager, coding a classic spider (a python module per SPIDER) doesn't works, but, I think this is not an issue for you. More info on default spiders manager TwistedPluginSpiderManager\n\n"", ""\nWhat you need is to dynamically create spider classes, subclassing your favorite generic spider class as supplied by scrapy (CrawlSpider subclasses with your rules added, or XmlFeedSpider, or whatever) and adding domain_name, start_urls, and possibly extra_domain_names (and/or start_requests(), etc), as you get or deduce them from your GUI (or config file, or whatever).\nPython makes it easy to perform such dynamic creation of class objects; a very simple example might be:\nfrom scrapy import spider\n\ndef makespider(domain_name, start_urls,\n               basecls=spider.BaseSpider):\n  return type(domain_name + 'Spider',\n              (basecls,),\n              {'domain_name': domain_name,\n               'start_urls': start_urls})\n\nallspiders = []\nfor domain, urls in listofdomainurlpairs:\n  allspiders.append(makespider(domain, urls))\n\nThis gives you a list of very bare-bone spider classes -- you'll probably want to add parse methods to them before you instantiate them.  Season to taste...;-).\n"", ""\nShameless self promotion on domo! you'll need to instantiate the crawler as given in the examples, for your project.\nAlso you'll need to make the crawler configurable on runtime, which is simply passing the configuration to crawler, and overriding the settings on runtime, when configuration changed. \n"", '\nNow it is extremely easy to configure scrapy for these purposes:\n\nAbout the first urls to visit, you can pass it as an attribute on the spider call with -a, and use the start_requests function to setup how to start the spider\nYou don\'t need to setup the allowed_domains variable for the spiders. If you don\'t include that class variable, the spider will be able to allow every domain.\n\nIt should end up to something like:\nclass MySpider(Spider):\n\n    name = ""myspider""\n\n    def start_requests(self):\n        yield Request(self.start_url, callback=self.parse)\n\n\n    def parse(self, response):\n        ...\n\nand you should call it with:\nscrapy crawl myspider -a start_url=""http://example.com""\n\n']"
Removing all spaces in text file with Python 3.x,"
So I have this crazy long text file made by my crawler and it for some reason added some spaces inbetween the links, like this:
https://example.com/asdf.html                                (note the spaces)
https://example.com/johndoe.php                              (again)

I want to get rid of that, but keep the new line. Keep in mind that the text file is 4.000+ lines long. I tried to do it myself but figured that I have no idea how to loop through new lines in files.
",51k,"
            9
        ","[""\nSeems like you can't directly edit a python file, so here is my suggestion:\n# first get all lines from file\nwith open('file.txt', 'r') as f:\n    lines = f.readlines()\n\n# remove spaces\nlines = [line.replace(' ', '') for line in lines]\n\n# finally, write lines in the file\nwith open('file.txt', 'w') as f:\n    f.writelines(lines)\n\n"", ""\nYou can open file and read line by line and remove white space - \nPython 3.x:\nwith open('filename') as f:\n    for line in f:\n        print(line.strip())\n\nPython 2.x:\nwith open('filename') as f:\n    for line in f:\n        print line.strip()\n\nIt will remove space from each line and print it.\nHope it helps!\n"", ""\nRead text from file, remove spaces, write text to file:\nwith open('file.txt', 'r') as f:\n    txt = f.read().replace(' ', '')\n\nwith open('file.txt', 'w') as f:\n    f.write(txt)\n\nIn @Leonardo Chirivì's solution it's unnecessary to create a list to store file contents when a string is sufficient and more memory efficient.  The .replace(' ', '') operation is only called once on the string, which is more efficient than iterating through a list performing replace for each line individually.\nTo avoid opening the file twice:\nwith open('file.txt', 'r+') as f:\n    txt = f.read().replace(' ', '')\n    f.seek(0)\n    f.write(txt)\n    f.truncate()\n\nIt would be more efficient to only open the file once.  This requires moving the file pointer back to the start of the file after reading, as well as truncating any possibly remaining content left over after you write back to the file.  A drawback to this solution however is that is not as easily readable.\n"", ""\nI had something similar that I'd been dealing with.\nThis is what worked for me (Note: This converts from 2+ spaces into a comma, but if you read below the code block, I explain how you can get rid of ALL whitespaces):\nimport re\n\n# read the file\nwith open('C:\\\\path\\\\to\\\\test_file.txt') as f:\n    read_file = f.read()\n    print(type(read_file)) # to confirm that it's a string\n\nread_file = re.sub(r'\\s{2,}', ',', read_file) # find/convert 2+ whitespace into ','\n\n# write the file\nwith open('C:\\\\path\\\\to\\\\test_file.txt', 'w') as f:\n    f.writelines('read_file')\n\nThis helped me then send the updated data to a CSV, which suited my need, but it can help for you as well, so instead of converting it to a comma (','), you can convert it to an empty string (''), and then [or] use a read_file.replace(' ', '') method if you don't need any whitespaces at all.\n"", ""\nLets not forget about adding back the \\n to go to the next row.\nThe complete function would be :\nwith open(str_path, 'r') as file :\n    str_lines = file.readlines()\n\n# remove spaces    \nif bl_right is True:    \n    str_lines = [line.rstrip() + '\\n' for line in str_lines]\nelif bl_left is True:   \n    str_lines = [line.lstrip() + '\\n' for line in str_lines]\nelse:                   \n    str_lines = [line.strip() + '\\n' for line in str_lines]\n\n# Write the file out again\nwith open(str_path, 'w') as file:\n    file.writelines(str_lines)\n\n""]"
Concurrent downloads - Python,"
the plan is this:
I download a webpage, collect a list of images parsed in the DOM and then download these. After this I would iterate through the images in order to evaluate which image is best suited to represent the webpage.
Problem is that images are downloaded 1 by 1 and this can take quite some time.

It would be great if someone could point me in some direction regarding the topic.
Help would be very much appreciated.
",7k,"
            9
        ","['\nSpeeding up crawling is basically Eventlet\'s main use case.  It\'s deeply fast -- we have an application that has to hit 2,000,000 urls in a few minutes.  It makes use of the fastest event interface on your system (epoll, generally), and uses greenthreads (which are built on top of coroutines and are very inexpensive) to make it easy to write.\nHere\'s an example from the docs:\nurls = [""http://www.google.com/intl/en_ALL/images/logo.gif"",\n     ""https://wiki.secondlife.com/w/images/secondlife.jpg"",\n     ""http://us.i1.yimg.com/us.yimg.com/i/ww/beta/y3.gif""]\n\nimport eventlet\nfrom eventlet.green import urllib2  \n\ndef fetch(url):\n  body = urllib2.urlopen(url).read()\n  return url, body\n\npool = eventlet.GreenPool()\nfor url, body in pool.imap(fetch, urls):\n  print ""got body from"", url, ""of length"", len(body)\n\nThis is a pretty good starting point for developing a more fully-featured crawler.  Feel free to pop in to #eventlet on Freenode to ask for help.\n[update: I added a more-complex recursive web crawler example to the docs.  I swear it was in the works before this question was asked, but the question did finally inspire me to finish it.  :)]\n', ""\nWhile threading is certainly a possibility, I would instead suggest asyncore -- there's an excellent example here which shows exactly the simultaneous fetching of two URLs (easy to generalize to any list of URLs!).\n"", '\nHere is an article on threading which uses url fetching as an example.\n', '\nNowadays there are excellent Python libs you might want to use - urllib3 and requests\n']"
Web Crawling (Ajax/JavaScript enabled pages) using java,"
I am very new to this web crawling. I am using crawler4j to crawl the websites. I am collecting the required information by crawling these sites. My problem here is I was unable to crawl the content for the following site. http://www.sciencedirect.com/science/article/pii/S1568494612005741. I want to crawl the following information from the aforementioned site (Please take a look at the attached screenshot).

If you observe the attached screenshot it has three names (Highlighted in red boxes). If you click one of the link you will see a popup and that popup contains the whole information about that author. I want to crawl the information which are there in that popup.
I am using the following code to crawl the content.
public class WebContentDownloader {

private Parser parser;
private PageFetcher pageFetcher;

public WebContentDownloader() {
    CrawlConfig config = new CrawlConfig();
    parser = new Parser(config);
    pageFetcher = new PageFetcher(config);
}

private Page download(String url) {
    WebURL curURL = new WebURL();
    curURL.setURL(url);
    PageFetchResult fetchResult = null;
    try {
        fetchResult = pageFetcher.fetchHeader(curURL);
        if (fetchResult.getStatusCode() == HttpStatus.SC_OK) {
            try {
                Page page = new Page(curURL);
                fetchResult.fetchContent(page);
                if (parser.parse(page, curURL.getURL())) {
                    return page;
                }
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    } finally {
        if (fetchResult != null) {
            fetchResult.discardContentIfNotConsumed();
        }
    }
    return null;
}

private String processUrl(String url) {
    System.out.println(""Processing: "" + url);
    Page page = download(url);
    if (page != null) {
        ParseData parseData = page.getParseData();
        if (parseData != null) {
            if (parseData instanceof HtmlParseData) {
                HtmlParseData htmlParseData = (HtmlParseData) parseData;
                return htmlParseData.getHtml();
            }
        } else {
            System.out.println(""Couldn't parse the content of the page."");
        }
    } else {
        System.out.println(""Couldn't fetch the content of the page."");
    }
    return null;
}

public String getHtmlContent(String argUrl) {
    return this.processUrl(argUrl);
}
}

I was able to crawl the content from the aforementioned link/site. But it doesn't have the information what I marked in the red boxes. I think those are the dynamic links.

My question is how can I crawl the content from the aforementioned link/website...???
How to crawl the content from Ajax/JavaScript based websites...???

Please can anyone help me on this.
Thanks & Regards,
Amar
",20k,"
            9
        ","['\nHi I found the workaround with the another library. I used \nSelinium WebDriver (org.openqa.selenium.WebDriver) library to extract the dynamic content. Here is the sample code.\npublic class CollectUrls {\n\nprivate WebDriver driver;\n\npublic CollectUrls() {\n    this.driver = new FirefoxDriver();\n    this.driver.manage().timeouts().implicitlyWait(30, TimeUnit.SECONDS);\n}\n\nprotected void next(String url, List<String> argUrlsList) {\n    this.driver.get(url);\n    String htmlContent = this.driver.getPageSource();\n}\n\nHere the ""htmlContent"" is the required one. Please let me know if you face any issues...???\nThanks,\nAmar\n', ""\nSimply said, Crawler4j is static crawler. Meaning that it can't parse the JavaScript on a page. So there is no way of getting the content you want by crawling that specific page you mentioned. Of course there are some workarounds to get it working.\nIf it is just this page you want to crawl, you could use a connection debugger. Check out this question for some tools. Find out which page the AJAX-request calls, and crawl that page.\nIf you have various websites which have dynamic content (JavaScript/ajax), you should consider using a dynamic-content-enabled crawler, like Crawljax (also written in Java).\n"", '\nI have find out the Solution of Dynamic Web page Crawling using Aperture and Selenium.Web Driver.\nAperture is Crawling Tools and Selenium is Testing Tools which can able to rendering Inspect Element. \n\n1. Extract the Aperture- core Jar file by Decompiler Tools and Create a Simple Web Crawling Java program. (https://svn.code.sf.net/p/aperture/code/aperture/trunk/)\n2. Download Selenium. WebDriver Jar Files and Added to Your Program.\n3. Go to CreatedDataObjec() method in org.semanticdesktop.aperture.accessor.http.HttpAccessor.(Aperture Decompiler).\nAdded Below Coding \n\n   WebDriver driver = new FirefoxDriver();\n   String baseurl=uri.toString();\n   driver.get(uri.toString());\n   String str = driver.getPageSource();\n        driver.close();\n stream= new ByteArrayInputStream(str.getBytes());\n\n']"
Apache HTTPClient throws java.net.SocketException: Connection reset for many domains,"
I'm creating a (well behaved) web spider and I notice that some servers are causing Apache HttpClient to give me a SocketException -- specifically:
java.net.SocketException: Connection reset

The code that causes this is:
// Execute the request
HttpResponse response; 
try {
    response = httpclient.execute(httpget); //httpclient is of type HttpClient
} catch (NullPointerException e) {
    return;//deep down in apache http sometimes throws a null pointer...  
}

For most servers it's just fine.  But for others, it immediately throws a SocketException.
Example of site that causes immediate SocketException: http://www.bhphotovideo.com/
Works great (as do most websites): http://www.google.com/
Now, as you can see, www.bhphotovideo.com loads fine in a web browser.  It also loads fine when I don't use Apache's HTTP Client.  (Code like this:)
 HttpURLConnection c = (HttpURLConnection)url.openConnection();  
 BufferedInputStream in = new BufferedInputStream(c.getInputStream());  
 Reader r = new InputStreamReader(in);     

 int i;  
 while ((i = r.read()) != -1) {  
      source.append((char) i);  
 }  

So, why don't I just use this code instead?  Well there are some key features in Apache's HTTP Client that I need to use.
Does anyone know what causes some servers to cause this exception?
Research so far:

Problem occurs on my local Mac dev machines AND an AWS EC2 Instance, so it's not a local firewall.
It seems the error isn't caused by the remote machine because the exception doesn't say ""by peer""
This stack overflow seems relavent java.net.SocketException: Connection reset but the answers don't show why this would happen only from Apache HTTP Client and not other approaches.

Bonus question: I'm doing a fair amount of crawling with this system.  Is there generally a better Java class for this other than Apache HTTP Client?  I've found a number of issues (such as the NullPointerException I have to catch in the code above).  It seems that HTTPClient is very picky about server communications -- more picky than I'd like for a crawler that can't just break when a server doesn't behave.
Thanks all! 
Solution
Honestly, I don't have a perfect solution, but it works, so that's good enough for me.
As pointed out by oleg below, Bixo has created a crawler that customizes HttpClient to be more forgiving to servers.  To ""get around"" the issue more than fix it, I just used SimpleHttpFetcher provided by Bixo here:
(linked removed - SO thinks I'm a spammer, so you'll have to google it yourself)
SimpleHttpFetcher fetch = new SimpleHttpFetcher(new UserAgent(""botname"",""contact@yourcompany.com"",""ENTER URL""));
try {
    FetchedResult result = fetch.fetch(""ENTER URL"");
    System.out.println(new String(result.getContent()));
} catch (BaseFetchException e) {
    e.printStackTrace();
}

The down side to this solution is that there are a lot of dependencies for Bixo -- so this may not be a good work around for everyone.  However, you can always just work through their use of DefaultHttpClient and see how they instantiated it to get it to work.  I decided to use the whole class because it handles some things for me, like automatic redirect following (and reporting the final destination url) that are helpful.
Thanks for the help all.
Edit: TinyBixo
Hi all.  So, I loved how Bixo worked, but didn't like that it had so many dependencies (including all of Hadoop).  So, I created a vastly simplified Bixo, without all the dependencies.  If you're running into the problems above, I would recommend using it (and feel free to make pull requests if you'd like to update it!)
It's available here: https://github.com/juliuss/TinyBixo 
",44k,"
            9
        ","[""\nFirst, to answer your question:\nThe connection reset was caused by a problem on the server side. Most likely the server failed to parse the request or was unable to process it and dropped the connection as a result without returning a valid response. There is likely something in the HTTP requests generated by HttpClient that causes server side logic to fail, probably due to a server side bug. Just because the error message does not say 'by peer' does not mean the connection reset took place on the client side. \nA few remarks:\n(1) Several popular web crawlers such as bixo http://openbixo.org/ use HttpClient without major issues but pretty much of them had to tweak HttpClient behavior to make it more lenient about common HTTP protocol violations. Per default HttpClient is rather strict about the HTTP protocol compliance.\n(2) Why did not you report the NPE problem or any other problem you have been experiencing to the HttpClient project?\n"", '\nThese two settings will sometimes help:\n client.getParams().setParameter(""http.socket.timeout"", new Integer(0));\n client.getParams().setParameter(""http.connection.stalecheck"", new  Boolean(true));\n\nThe first sets the socket timeout to be infinite.\n', '\nTry getting a network trace using wireshark, and augment that with log4j logging of the HTTPClient. That should show why the connection is being reset\n']"
Mass Downloading of Webpages C#,"
My application requires that I download a large amount of webpages into memory for further parsing and processing. What is the fastest way to do it? My current method (shown below) seems to be too slow and occasionally results in timeouts.
for (int i = 1; i<=pages; i++)
{
    string page_specific_link = baseurl + ""&page="" + i.ToString();

    try
    {    
        WebClient client = new WebClient();
        var pagesource = client.DownloadString(page_specific_link);
        client.Dispose();
        sourcelist.Add(pagesource);
    }
    catch (Exception)
    {
    }
}

",5k,"
            8
        ","['\nThe way you approach this problem is going to depend very much on how many pages you want to download, and how many sites you\'re referencing.\nI\'ll use a good round number like 1,000. If you want to download that many pages from a single site, it\'s going to take a lot longer than if you want to download 1,000 pages that are spread out across dozens or hundreds of sites. The reason is that if you hit a single site with a whole bunch of concurrent requests, you\'ll probably end up getting blocked.\nSo you have to implement a type of ""politeness policy,"" that issues a delay between multiple requests on a single site. The length of that delay depends on a number of things. If the site\'s robots.txt file has a crawl-delay entry, you should respect that. If they don\'t want you accessing more than one page per minute, then that\'s as fast as you should crawl. If there\'s no crawl-delay, you should base your delay on how long it takes a site to respond. For example, if you can download a page from the site in 500 milliseconds, you set your delay to X. If it takes a full second, set your delay to 2X. You can probably cap your delay to 60 seconds (unless crawl-delay is longer), and I would recommend that you set a minimum delay of 5 to 10 seconds.\nI wouldn\'t recommend using Parallel.ForEach for this. My testing has shown that it doesn\'t do a good job. Sometimes it over-taxes the connection and often it doesn\'t allow enough concurrent connections. I would instead create a queue of WebClient instances and then write something like:\n// Create queue of WebClient instances\nBlockingCollection<WebClient> ClientQueue = new BlockingCollection<WebClient>();\n// Initialize queue with some number of WebClient instances\n\n// now process urls\nforeach (var url in urls_to_download)\n{\n    var worker = ClientQueue.Take();\n    worker.DownloadStringAsync(url, ...);\n}\n\nWhen you initialize the WebClient instances that go into the queue, set their OnDownloadStringCompleted event handlers to point to a completed event handler. That handler should save the string to a file (or perhaps you should just use DownloadFileAsync), and then the client, adds itself back to the ClientQueue.\nIn my testing, I\'ve been able to support 10 to 15 concurrent connections with this method. Any more than that and I run into problems with DNS resolution (`DownloadStringAsync\'  doesn\'t do the DNS resolution asynchronously). You can get more connections, but doing so is a lot of work.\nThat\'s the approach I\'ve taken in the past, and it\'s worked very well for downloading thousands of pages quickly. It\'s definitely not the approach I took with my high performance Web crawler, though.\nI should also note that there is a huge difference in resource usage between these two blocks of code:\nWebClient MyWebClient = new WebClient();\nforeach (var url in urls_to_download)\n{\n    MyWebClient.DownloadString(url);\n}\n\n---------------\n\nforeach (var url in urls_to_download)\n{\n    WebClient MyWebClient = new WebClient();\n    MyWebClient.DownloadString(url);\n}\n\nThe first allocates a single WebClient instance that is used for all requests. The second allocates one WebClient for each request. The difference is huge. WebClient uses a lot of system resources, and allocating thousands of them in a relatively short time is going to impact performance. Believe me ... I\'ve run into this. You\'re better off allocating just 10 or 20 WebClients (as many as you need for concurrent processing), rather than allocating one per request.\n', '\nWhy not just use a web crawling framework. It can handle all the stuff for you like (multithreading, httprequests, parsing links, scheduling, politeness, etc..). \nAbot (https://code.google.com/p/abot/) handles all that stuff for you and is written in c#.\n', '\nIn addition to @Davids perfectly valid answer, I want to add a slightly cleaner ""version"" of his approach.\nvar pages = new List<string> { ""http://bing.com"", ""http://stackoverflow.com"" };\nvar sources = new BlockingCollection<string>();\n\nParallel.ForEach(pages, x =>\n{\n    using(var client = new WebClient())\n    {\n        var pagesource = client.DownloadString(x);\n        sources.Add(pagesource);\n    }\n});\n\n\nYet another approach, that uses async:\nstatic IEnumerable<string> GetSources(List<string> pages)\n{\n    var sources = new BlockingCollection<string>();\n    var latch = new CountdownEvent(pages.Count);\n\n    foreach (var p in pages)\n    {\n        using (var wc = new WebClient())\n        {\n            wc.DownloadStringCompleted += (x, e) =>\n            {\n                sources.Add(e.Result);\n                latch.Signal();\n            };\n\n            wc.DownloadStringAsync(new Uri(p));\n        }\n    }\n\n    latch.Wait();\n\n    return sources;\n}\n\n', '\nYou should use parallel programming for this purpose.\nThere are a lot of ways to achieve what u want; the easiest would be something like this:\nvar pageList = new List<string>();\n\nfor (int i = 1; i <= pages; i++)\n{\n  pageList.Add(baseurl + ""&page="" + i.ToString());\n}\n\n\n// pageList  is a list of urls\nParallel.ForEach<string>(pageList, (page) =>\n{\n  try\n    {\n      WebClient client = new WebClient();\n      var pagesource = client.DownloadString(page);\n      client.Dispose();\n      lock (sourcelist)\n      sourcelist.Add(pagesource);\n    }\n\n    catch (Exception) {}\n});\n\n', '\nI Had a similar Case ,and that\'s how i solved \nusing System;\n    using System.Threading;\n    using System.Collections.Generic;\n    using System.Net;\n    using System.IO;\n\nnamespace WebClientApp\n{\nclass MainClassApp\n{\n    private static int requests = 0;\n    private static object requests_lock = new object();\n\n    public static void Main() {\n\n        List<string> urls = new List<string> { ""http://www.google.com"", ""http://www.slashdot.org""};\n        foreach(var url in urls) {\n            ThreadPool.QueueUserWorkItem(GetUrl, url);\n        }\n\n        int cur_req = 0;\n\n        while(cur_req<urls.Count) {\n\n            lock(requests_lock) {\n                cur_req = requests; \n            }\n\n            Thread.Sleep(1000);\n        }\n\n        Console.WriteLine(""Done"");\n    }\n\nprivate static void GetUrl(Object the_url) {\n\n        string url = (string)the_url;\n        WebClient client = new WebClient();\n        Stream data = client.OpenRead (url);\n\n        StreamReader reader = new StreamReader(data);\n        string html = reader.ReadToEnd ();\n\n        /// Do something with html\n        Console.WriteLine(html);\n\n        lock(requests_lock) {\n            //Maybe you could add here the HTML to SourceList\n            requests++; \n        }\n    }\n}\n\nYou should think using Paralel\'s because the slow speed is because you\'re software is waiting for I/O and why not while a thread i waiting for I/O another one get started.\n', '\nWhile the other answers are perfectly valid, all of them (at the time of this writing) are neglecting something very important: calls to the web are IO bound, having a thread wait on an operation like this is going to strain system resources and have an impact on your system resources.\nWhat you really want to do is take advantage of the async methods on the WebClient class (as some have pointed out) as well as the Task Parallel Library\'s ability to handle the Event-Based Asynchronous Pattern.\nFirst, you would get the urls that you want to download:\nIEnumerable<Uri> urls = pages.Select(i => new Uri(baseurl + \n    ""&page="" + i.ToString(CultureInfo.InvariantCulture)));\n\nThen, you would create a new WebClient instance for each url, using the TaskCompletionSource<T> class to handle the calls asynchronously (this won\'t burn a thread):\nIEnumerable<Task<Tuple<Uri, string>> tasks = urls.Select(url => {\n    // Create the task completion source.\n    var tcs = new TaskCompletionSource<Tuple<Uri, string>>();\n\n    // The web client.\n    var wc = new WebClient();\n\n    // Attach to the DownloadStringCompleted event.\n    client.DownloadStringCompleted += (s, e) => {\n        // Dispose of the client when done.\n        using (wc)\n        {\n            // If there is an error, set it.\n            if (e.Error != null) \n            {\n                tcs.SetException(e.Error);\n            }\n            // Otherwise, set cancelled if cancelled.\n            else if (e.Cancelled) \n            {\n                tcs.SetCanceled();\n            }\n            else \n            {\n                // Set the result.\n                tcs.SetResult(new Tuple<string, string>(url, e.Result));\n            }\n        }\n    };\n\n    // Start the process asynchronously, don\'t burn a thread.\n    wc.DownloadStringAsync(url);\n\n    // Return the task.\n    return tcs.Task;\n});\n\nNow you have an IEnumerable<T> which you can convert to an array and wait on all of the results using Task.WaitAll:\n// Materialize the tasks.\nTask<Tuple<Uri, string>> materializedTasks = tasks.ToArray();\n\n// Wait for all to complete.\nTask.WaitAll(materializedTasks);\n\nThen, you can just use Result property on the Task<T> instances to get the pair of the url and the content:\n// Cycle through each of the results.\nforeach (Tuple<Uri, string> pair in materializedTasks.Select(t => t.Result))\n{\n    // pair.Item1 will contain the Uri.\n    // pair.Item2 will contain the content.\n}\n\nNote that the above code has the caveat of not having an error handling.\nIf you wanted to get even more throughput, instead of waiting for the entire list to be finished, you could process the content of a single page after it\'s done downloading; Task<T> is meant to be used like a pipeline, when you\'ve completed your unit of work, have it continue to the next one instead of waiting for all of the items to be done (if they can be done in an asynchronous manner).\n', '\nI am using an active Threads count and a arbitrary limit:\nprivate static volatile int activeThreads = 0;\n\npublic static void RecordData()\n{\n  var nbThreads = 10;\n  var source = db.ListOfUrls; // Thousands urls\n  var iterations = source.Length / groupSize; \n  for (int i = 0; i < iterations; i++)\n  {\n    var subList = source.Skip(groupSize* i).Take(groupSize);\n    Parallel.ForEach(subList, (item) => RecordUri(item)); \n    //I want to wait here until process further data to avoid overload\n    while (activeThreads > 30) Thread.Sleep(100);\n  }\n}\n\nprivate static async Task RecordUri(Uri uri)\n{\n   using (WebClient wc = new WebClient())\n   {\n      Interlocked.Increment(ref activeThreads);\n      wc.DownloadStringCompleted += (sender, e) => Interlocked.Decrement(ref iterationsCount);\n      var jsonData = """";\n      RootObject root;\n      jsonData = await wc.DownloadStringTaskAsync(uri);\n      var root = JsonConvert.DeserializeObject<RootObject>(jsonData);\n      RecordData(root)\n    }\n}\n\n']"
Running Multiple spiders in scrapy for 1 website in parallel?,"
I want to crawl a website with 2 parts and my script is not as fast as I need.
Is it possible to launch 2 spiders, one for scraping the first part and the second one for the second part? 
I tried to have 2 different classes, and run them
scrapy crawl firstSpider
scrapy crawl secondSpider

but i think that it is not smart.
I read the documentation of scrapyd but I don't know if it's good for my case.
",21k,"
            7
        ","['\nI think what you are looking for is something like this:\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider1(scrapy.Spider):\n    # Your first spider definition\n    ...\n\nclass MySpider2(scrapy.Spider):\n    # Your second spider definition\n    ...\n\nprocess = CrawlerProcess()\nprocess.crawl(MySpider1)\nprocess.crawl(MySpider2)\nprocess.start() # the script will block here until all crawling jobs are finished\n\nYou can read more at: running-multiple-spiders-in-the-same-process.\n', '\nOr you can run with like this, you need to save this code at the same directory with scrapy.cfg (My scrapy version is 1.3.3) :\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spiders.list():\n    print (""Running spider %s"" % (spider_name))\n    process.crawl(spider_name,query=""dvh"") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n\n', '\nBetter solution is (if you have multiple spiders) it dynamically get spiders and run them.\nfrom scrapy import spiderloader\nfrom scrapy.utils import project\nfrom twisted.internet.defer import inlineCallbacks\n\n\n@inlineCallbacks\ndef crawl():\n    settings = project.get_project_settings()\n    spider_loader = spiderloader.SpiderLoader.from_settings(settings)\n    spiders = spider_loader.list()\n    classes = [spider_loader.load(name) for name in spiders]\n    for my_spider in classes:\n        yield runner.crawl(my_spider)\n    reactor.stop()\n\ncrawl()\nreactor.run()\n\n(Second Solution):\nBecause spiders.list() is deprecated in Scrapy 1.4 Yuda solution should be converted to something like\nfrom scrapy import spiderloader    \nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsettings = get_project_settings()\nprocess = CrawlerProcess(settings)\nspider_loader = spiderloader.SpiderLoader.from_settings(settings)\n\nfor spider_name in spider_loader.list():\n    print(""Running spider %s"" % (spider_name))\n    process.crawl(spider_name)\nprocess.start()\n\n']"
Send parallel requests but only one per host with HttpClient and Polly to gracefully handle 429 responses,"
Intro:
I am building a single-node web crawler to simply validate URLs are 200 OK in a .NET Core console application. I have a collection of URLs at different hosts to which I am sending requests with HttpClient. I am fairly new to using Polly and TPL Dataflow.
Requirements:

I want to support sending multiple HTTP requests in parallel with a
configurable MaxDegreeOfParallelism.
I want to limit the number of parallel requests to any given host to 1 (or configurable). This is in order to gracefully handle per-host 429 TooManyRequests responses with a Polly policy. Alternatively, I could maybe use a Circuit Breaker to cancel concurrent requests to the same host on receipt of one 429 response and then proceed one-at-a-time to that specific host?
I am perfectly fine with not using TPL Dataflow at all in favor of maybe using a Polly Bulkhead or some other mechanism for throttled parallel requests, but I am not sure what that configuration would look like in order to implement requirement #2.

Current Implementation:
My current implementation works, except that I often see that I'll have x parallel requests to the same host return 429 at about the same time... Then, they all pause for the retry policy... Then, they all slam the same host again at the same time often still receiving 429s. Even if I distribute multiple instances of the same host evenly throughout the queue, my URL collection is overweighted with a few specific hosts that still start generating 429s eventually.
After receiving a 429, I think I only want to send one concurrent request to that host going forward to respect the remote host and pursue 200s. 
Validator Method:
public async Task<int> GetValidCount(IEnumerable<Uri> urls, CancellationToken cancellationToken)
{
    var validator = new TransformBlock<Uri, bool>(
        async u => (await _httpClient.GetAsync(u, HttpCompletionOption.ResponseHeadersRead, cancellationToken)).IsSuccessStatusCode,
        new ExecutionDataflowBlockOptions {MaxDegreeOfParallelism = MaxDegreeOfParallelism}
    );
    foreach (var url in urls)
        await validator.SendAsync(url, cancellationToken);
    validator.Complete();
    var validUrlCount = 0;
    while (await validator.OutputAvailableAsync(cancellationToken))
    {
        if(await validator.ReceiveAsync(cancellationToken))
            validUrlCount++;
    }
    await validator.Completion;
    return validUrlCount;
}

The Polly policy applied to the HttpClient instance used in GetValidCount() above.
IAsyncPolicy<HttpResponseMessage> waitAndRetryTooManyRequests = Policy
    .HandleResult<HttpResponseMessage>(r => r.StatusCode == HttpStatusCode.TooManyRequests)
    .WaitAndRetryAsync(3,
        (retryCount, response, context) =>
            response.Result?.Headers.RetryAfter.Delta ?? TimeSpan.FromMilliseconds(120),
        async (response, timespan, retryCount, context) =>
        {
            // log stuff
        });

Question:
How can I modify or replace this solution to add satisfaction of requirement #2?
",1k,"
            6
        ","['\nI\'d try to introduce some sort of a flag LimitedMode  to detect that this particular client is entered in limited mode. Below I declare two policies - one simple retry policy just to catch TooManyRequests and set the flag. The second policy is a out-of-the-box BulkHead policy.\n    public void ConfigureServices(IServiceCollection services)\n    {\n        /* other configuration */\n\n        var registry = services.AddPolicyRegistry();\n\n        var catchPolicy = Policy.HandleResult<HttpResponseMessage>(r =>\n            {\n                LimitedMode = r.StatusCode == HttpStatusCode.TooManyRequests;\n                return false;\n            })\n            .WaitAndRetryAsync(1, i => TimeSpan.FromSeconds(3)); \n\n        var bulkHead = Policy.BulkheadAsync<HttpResponseMessage>(1, 10, OnBulkheadRejectedAsync);\n\n        registry.Add(""catchPolicy"", catchPolicy);\n        registry.Add(""bulkHead"", bulkHead);\n\n        services.AddHttpClient<CrapyWeatherApiClient>((client) =>\n        {\n            client.BaseAddress = new Uri(""hosturl"");\n        }).AddPolicyHandlerFromRegistry(PolicySelector);\n    }\n\nThen you may want to dynamically decide on which policy to apply using the PolicySelector mechanism: in case the limited mode is active - wrap bulk head policy with catch 429 policy. If the success status code received - switch back to regular mode without a bulkhead.\n    private IAsyncPolicy<HttpResponseMessage> PolicySelector(IReadOnlyPolicyRegistry<string> registry, HttpRequestMessage request)\n    {\n        var catchPolicy = registry.Get<IAsyncPolicy<HttpResponseMessage>>(""catchPolicy"");\n        var bulkHead = registry.Get<IAsyncPolicy<HttpResponseMessage>>(""bulkHead"");\n        if (LimitedMode)\n        {\n            return catchPolicy.WrapAsync(bulkHead);\n        }\n\n        return catchPolicy;\n    }        \n\n', '\nHere is a method that creates a TransformBlock which prevents concurrent execution for messages with the same key. The key of each message is obtained by invoking the supplied keySelector function. Messages with the same key are processed sequentially to each other (not in parallel). The key is also passed as an argument to the transform function, because it can be useful in some cases.\npublic static TransformBlock<TInput, TOutput>\n    CreateExclusivePerKeyTransformBlock<TInput, TKey, TOutput>(\n    Func<TInput, TKey, Task<TOutput>> transform,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (transform == null) throw new ArgumentNullException(nameof(transform));\n    if (keySelector == null) throw new ArgumentNullException(nameof(keySelector));\n    if (dataflowBlockOptions == null)\n        throw new ArgumentNullException(nameof(dataflowBlockOptions));\n    keyComparer = keyComparer ?? EqualityComparer<TKey>.Default;\n\n    var internalCTS = CancellationTokenSource\n        .CreateLinkedTokenSource(dataflowBlockOptions.CancellationToken);\n\n    var maxDOP = dataflowBlockOptions.MaxDegreeOfParallelism;\n    var taskScheduler = dataflowBlockOptions.TaskScheduler;\n\n    var perKeySemaphores = new ConcurrentDictionary<TKey, SemaphoreSlim>(\n        keyComparer);\n\n    SemaphoreSlim maxDopSemaphore;\n    if (maxDOP == DataflowBlockOptions.Unbounded)\n    {\n        maxDopSemaphore = new SemaphoreSlim(Int32.MaxValue);\n    }\n    else\n    {\n        maxDopSemaphore = new SemaphoreSlim(maxDOP, maxDOP);\n\n        // The degree of parallelism is controlled by the semaphore\n        dataflowBlockOptions.MaxDegreeOfParallelism = DataflowBlockOptions.Unbounded;\n\n        // Use a limited-concurrency scheduler for preserving the processing order\n        dataflowBlockOptions.TaskScheduler = new ConcurrentExclusiveSchedulerPair(\n            taskScheduler, maxDOP).ConcurrentScheduler;\n    }\n\n    var block = new TransformBlock<TInput, TOutput>(async item =>\n    {\n        var key = keySelector(item);\n        var perKeySemaphore = perKeySemaphores\n            .GetOrAdd(key, _ => new SemaphoreSlim(1, 1));\n\n        // Continue on captured context before invoking the transform\n        await perKeySemaphore.WaitAsync(internalCTS.Token);\n        try\n        {\n            await maxDopSemaphore.WaitAsync(internalCTS.Token);\n            try\n            {\n                return await transform(item, key).ConfigureAwait(false);\n            }\n            catch (Exception ex) when (!(ex is OperationCanceledException))\n            {\n                internalCTS.Cancel(); // The block has failed\n                throw;\n            }\n            finally\n            {\n                maxDopSemaphore.Release();\n            }\n        }\n        finally\n        {\n            perKeySemaphore.Release();\n        }\n    }, dataflowBlockOptions);\n\n    dataflowBlockOptions.MaxDegreeOfParallelism = maxDOP; // Restore initial value\n    dataflowBlockOptions.TaskScheduler = taskScheduler; // Restore initial value\n    return block;\n}\n\nUsage example:\nvar validator = CreateExclusivePerKeyTransformBlock<Uri, string, bool>(\n    async (uri, host) =>\n    {\n        return (await _httpClient.GetAsync(uri, HttpCompletionOption\n            .ResponseHeadersRead, token)).IsSuccessStatusCode;\n    },\n    new ExecutionDataflowBlockOptions\n    {\n        MaxDegreeOfParallelism = 30,\n        CancellationToken = token,\n    },\n    keySelector: uri => uri.Host,\n    keyComparer: StringComparer.OrdinalIgnoreCase);\n\nAll execution options are supported (MaxDegreeOfParallelism, BoundedCapacity, CancellationToken, EnsureOrdered etc).\nBelow is an overload of the CreateExclusivePerKeyTransformBlock that accepts a synchronous delegate, and another method+overload that returns an ActionBlock instead of a TransformBlock, with the same behavior.\npublic static TransformBlock<TInput, TOutput>\n    CreateExclusivePerKeyTransformBlock<TInput, TKey, TOutput>(\n    Func<TInput, TKey, TOutput> transform,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (transform == null) throw new ArgumentNullException(nameof(transform));\n    return CreateExclusivePerKeyTransformBlock(\n        (item, key) => Task.FromResult(transform(item, key)),\n        dataflowBlockOptions, keySelector, keyComparer);\n}\n\n// An ITargetBlock is similar to an ActionBlock\npublic static ITargetBlock<TInput>\n    CreateExclusivePerKeyActionBlock<TInput, TKey>(\n    Func<TInput, TKey, Task> action,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (action == null) throw new ArgumentNullException(nameof(action));\n    var block = CreateExclusivePerKeyTransformBlock(async (item, key) =>\n        { await action(item, key).ConfigureAwait(false); return (object)null; },\n        dataflowBlockOptions, keySelector, keyComparer);\n    block.LinkTo(DataflowBlock.NullTarget<object>());\n    return block;\n}\n\npublic static ITargetBlock<TInput>\n    CreateExclusivePerKeyActionBlock<TInput, TKey>(\n    Action<TInput, TKey> action,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (action == null) throw new ArgumentNullException(nameof(action));\n    return CreateExclusivePerKeyActionBlock(\n        (item, key) => { action(item, key); return Task.CompletedTask; },\n        dataflowBlockOptions, keySelector, keyComparer);\n}\n\n\nCaution: This class allocates one SemaphoreSlim per key, and keeps a reference to it until the class instance is finally garbage collected. This could be an issue in case the number of different keys is huge. There is an implementation of a less allocatey async lock here, that stores internally only the SemaphoreSlims that are currently in use (plus a small pool of released SemaphoreSlims that can be reused), which could replace the ConcurrentDictionary<TKey, SemaphoreSlim> used by this implementation.\n']"
How to follow all links in CasperJS?,"
I'm having trouble clicking all JavaScript based links in a DOM and saving the
output. The links have the form 
<a id=""html"" href=""javascript:void(0);"" onclick=""goToHtml();"">HTML</a>

the following code works great:
var casper = require('casper').create();

var fs = require('fs');

var firstUrl = 'http://www.testurl.com/test.html';

var css_selector = '#jan_html';

casper.start(firstUrl);

casper.thenClick(css_selector, function(){
    console.log(""whoop"");
});

casper.waitFor(function check() {
    return this.getCurrentUrl() != firstUrl;
}, function then() {
    console.log(this.getCurrentUrl());
    var file_title = this.getTitle().split(' ').join('_') + '.html';
    fs.write(file_title, this.getPageContent());
});

casper.run();

However, how can I get this to work with a selector of ""a"", clicking all
available links and saving content? I'm not sure how to get the clickWhileSelector to remove nodes from the selector as is done here: Click on all links matching a selector
",9k,"
            5
        ","['\nI have this script that first will get all links from a page then save \'href\' attributes to an array, then will iterate over this array and then open each link one by one and echo the url :\nvar casper = require(\'casper\').create({\n    logLevel:""verbose"",\n    debug:true\n});\nvar links;\n\ncasper.start(\'http://localhost:8000\');\n\ncasper.then(function getLinks(){\n     links = this.evaluate(function(){\n        var links = document.getElementsByTagName(\'a\');\n        links = Array.prototype.map.call(links,function(link){\n            return link.getAttribute(\'href\');\n        });\n        return links;\n    });\n});\ncasper.then(function(){\n    this.each(links,function(self,link){\n        self.thenOpen(link,function(a){\n            this.echo(this.getCurrentUrl());\n        });\n    });\n});\ncasper.run(function(){\n    this.exit();\n});\n\n', '\nrusln\'s answer works great if all the links have a meaningful href attribute (actual URL). If you want to click every a that also triggers a javascript function, you may need to iterate some other way over the elements.\nI propose using the XPath generator from stijn de ryck for an element. \n\nYou can then sample all XPaths that are on the page. \nThen you open the page for every a that you have the XPath for and click it by XPath. \nWait a little if it is a single page application\nDo something\n\nvar startURL = \'http://localhost:8000\',\n    xPaths\n    x = require(\'casper\').selectXPath;\n\ncasper.start(startURL);\n\ncasper.then(function getLinks(){\n    xPaths = this.evaluate(function(){\n        // copied from https://stackoverflow.com/a/5178132/1816580\n        function createXPathFromElement(elm) {\n            var allNodes = document.getElementsByTagName(\'*\'); \n            for (var segs = []; elm && elm.nodeType == 1; elm = elm.parentNode) { \n                if (elm.hasAttribute(\'id\')) { \n                        var uniqueIdCount = 0; \n                        for (var n=0;n < allNodes.length;n++) { \n                            if (allNodes[n].hasAttribute(\'id\') && allNodes[n].id == elm.id) uniqueIdCount++; \n                            if (uniqueIdCount > 1) break; \n                        }; \n                        if ( uniqueIdCount == 1) { \n                            segs.unshift(\'id(""\' + elm.getAttribute(\'id\') + \'"")\'); \n                            return segs.join(\'/\'); \n                        } else { \n                            segs.unshift(elm.localName.toLowerCase() + \'[@id=""\' + elm.getAttribute(\'id\') + \'""]\'); \n                        } \n                } else if (elm.hasAttribute(\'class\')) { \n                    segs.unshift(elm.localName.toLowerCase() + \'[@class=""\' + elm.getAttribute(\'class\') + \'""]\'); \n                } else { \n                    for (i = 1, sib = elm.previousSibling; sib; sib = sib.previousSibling) { \n                        if (sib.localName == elm.localName)  i++; }; \n                        segs.unshift(elm.localName.toLowerCase() + \'[\' + i + \']\'); \n                }; \n            }; \n            return segs.length ? \'/\' + segs.join(\'/\') : null; \n        };\n        var links = document.getElementsByTagName(\'a\');\n        var xPaths = Array.prototype.map.call(links, createXPathFromElement);\n        return xPaths;\n    });\n});\ncasper.then(function(){\n    this.each(xPaths, function(self, xpath){\n        self.thenOpen(startURL);\n        self.thenClick(x(xpath));\n        // waiting some time may be necessary for single page applications\n        self.wait(1000);\n        self.then(function(a){\n            // do something meaningful here\n            this.echo(this.getCurrentUrl());\n        });\n\n        // Uncomment the following line in case each click opens a new page instead of staying at the same page\n        //self.back()\n    });\n});\ncasper.run(function(){\n    this.exit();\n});\n\n']"
Scrapy upload file,"
I am making a form request to a website using scrapy. The form requires to upload a pdf file, How can we do it in Scrapy. I am trying this like -
FormRequest(url,callback=self.parseSearchResponse,method=""POST"",formdata={'filename':'abc.xyz','file':'path to file/abc.xyz'})

",2k,"
            4
        ","['\nAt this very moment Scrapy has no built-in support for uploading files.\nFile uploading via forms in HTTP was specified in RFC1867. According to the spec, an HTTP request with Content-Type: multipart/form-data is required (in your code it would be application/x-www-form-urlencoded).\nTo achieve file uploading with Scrapy, you would need to:\n\nGet familiar with the basic concepts of HTTP file uploading.\nStart with scrapy.Request (instead of FormRequest).\nGive it a proper Content-Type header value.\nBuild the request body yourself.\n\nSee also: How does HTTP file upload work?\n', '\nI just spent an entire day trying to figure out how to implement this.\nFinally, I came upon a Scrapy pull request from 2016 that was never merged, with an implementation of a multipart form request:\nfrom scrapy import FormRequest\nfrom six.moves.urllib.parse import urljoin, urlencode\nimport lxml.html\nfrom parsel.selector import create_root_node\nimport six\nimport string\nimport random\nfrom scrapy.http.request import Request\nfrom scrapy.utils.python import to_bytes, is_listlike\nfrom scrapy.utils.response import get_base_url\n\n\nclass MultipartFormRequest(FormRequest):\n\n    def __init__(self, *args, **kwargs):\n        formdata = kwargs.pop(\'formdata\', None)\n\n        kwargs.setdefault(\'method\', \'POST\')\n\n        super(MultipartFormRequest, self).__init__(*args, **kwargs)\n\n        content_type = self.headers.setdefault(b\'Content-Type\', [b\'multipart/form-data\'])[0]\n        method = kwargs.get(\'method\').upper()\n        if formdata and method == \'POST\' and content_type == b\'multipart/form-data\':\n            items = formdata.items() if isinstance(formdata, dict) else formdata\n            self._boundary = \'\'\n\n            # encode the data using multipart spec\n            self._boundary = to_bytes(\'\'.join(\n                random.choice(string.digits + string.ascii_letters) for i in range(20)), self.encoding)\n            self.headers[b\'Content-Type\'] = b\'multipart/form-data; boundary=\' + self._boundary\n            request_data = _multpart_encode(items, self._boundary, self.encoding)\n            self._set_body(request_data)\n\n\nclass MultipartFile(object):\n\n    def __init__(self, name, content, mimetype=\'application/octet-stream\'):\n        self.name = name\n        self.content = content\n        self.mimetype = mimetype\n\n\ndef _get_form_url(form, url):\n    if url is None:\n        return urljoin(form.base_url, form.action)\n    return urljoin(form.base_url, url)\n\n\ndef _urlencode(seq, enc):\n    values = [(to_bytes(k, enc), to_bytes(v, enc))\n              for k, vs in seq\n              for v in (vs if is_listlike(vs) else [vs])]\n    return urlencode(values, doseq=1)\n\n\ndef _multpart_encode(items, boundary, enc):\n    body = []\n\n    for name, value in items:\n        body.append(b\'--\' + boundary)\n        if isinstance(value, MultipartFile):\n            file_name = value.name\n            content = value.content\n            content_type = value.mimetype\n\n            body.append(\n                b\'Content-Disposition: form-data; name=""\' + to_bytes(name, enc) + b\'""; filename=""\' + to_bytes(file_name,\n                                                                                                              enc) + b\'""\')\n            body.append(b\'Content-Type: \' + to_bytes(content_type, enc))\n            body.append(b\'\')\n            body.append(to_bytes(content, enc))\n        else:\n            body.append(b\'Content-Disposition: form-data; name=""\' + to_bytes(name, enc) + b\'""\')\n            body.append(b\'\')\n            body.append(to_bytes(value, enc))\n\n    body.append(b\'--\' + boundary + b\'--\')\n    return b\'\\r\\n\'.join(body)\n\n\ndef _get_form(response, formname, formid, formnumber, formxpath):\n    """"""Find the form element """"""\n    root = create_root_node(response.text, lxml.html.HTMLParser,\n                            base_url=get_base_url(response))\n    forms = root.xpath(\'//form\')\n    if not forms:\n        raise ValueError(""No <form> element found in %s"" % response)\n\n    if formname is not None:\n        f = root.xpath(\'//form[@name=""%s""]\' % formname)\n        if f:\n            return f[0]\n\n    if formid is not None:\n        f = root.xpath(\'//form[@id=""%s""]\' % formid)\n        if f:\n            return f[0]\n\n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == \'form\':\n                    return el\n                el = el.getparent()\n                if el is None:\n                    break\n        encoded = formxpath if six.PY3 else formxpath.encode(\'unicode_escape\')\n        raise ValueError(\'No <form> element found with %s\' % encoded)\n\n    # If we get here, it means that either formname was None\n    # or invalid\n    if formnumber is not None:\n        try:\n            form = forms[formnumber]\n        except IndexError:\n            raise IndexError(""Form number %d not found in %s"" %\n                             (formnumber, response))\n        else:\n            return form\n\n\ndef _get_inputs(form, formdata, dont_click, clickdata, response):\n    try:\n        formdata = dict(formdata or ())\n    except (ValueError, TypeError):\n        raise ValueError(\'formdata should be a dict or iterable of tuples\')\n\n    inputs = form.xpath(\'descendant::textarea\'\n                        \'|descendant::select\'\n                        \'|descendant::input[not(@type) or @type[\'\n                        \' not(re:test(., ""^(?:submit|image|reset)$"", ""i""))\'\n                        \' and (../@checked or\'\n                        \'  not(re:test(., ""^(?:checkbox|radio)$"", ""i"")))]]\',\n                        namespaces={\n                            ""re"": ""http://exslt.org/regular-expressions""})\n    values = [(k, u\'\' if v is None else v)\n              for k, v in (_value(e) for e in inputs)\n              if k and k not in formdata]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n            values.append(clickable)\n\n    values.extend(formdata.items())\n    return values\n\n\ndef _value(ele):\n    n = ele.name\n    v = ele.value\n    if ele.tag == \'select\':\n        return _select_value(ele, n, v)\n    return n, v\n\n\ndef _select_value(ele, n, v):\n    multiple = ele.multiple\n    if v is None and not multiple:\n        # Match browser behaviour on simple select tag without options selected\n        # And for select tags wihout options\n        o = ele.value_options\n        return (n, o[0]) if o else (None, None)\n    elif v is not None and multiple:\n        # This is a workround to bug in lxml fixed 2.3.1\n        # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n        selected_options = ele.xpath(\'.//option[@selected]\')\n        v = [(o.get(\'value\') or o.text or u\'\').strip() for o in selected_options]\n    return n, v\n\n\ndef _get_clickable(clickdata, form):\n    """"""\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    """"""\n    clickables = [\n        el for el in form.xpath(\n            \'descendant::*[(self::input or self::button)\'\n            \' and re:test(@type, ""^submit$"", ""i"")]\'\n            \'|descendant::button[not(@type)]\',\n            namespaces={""re"": ""http://exslt.org/regular-expressions""})\n    ]\n    if not clickables:\n        return\n\n    # If we don\'t have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.get(\'name\'), el.get(\'value\') or \'\')\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get(\'nr\', None)\n    if nr is not None:\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (el.get(\'name\'), el.get(\'value\') or \'\')\n\n    # We didn\'t find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = u\'.//*\' + \\\n            u\'\'.join(u\'[@%s=""%s""]\' % c for c in six.iteritems(clickdata))\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].get(\'name\'), el[0].get(\'value\') or \'\')\n    elif len(el) > 1:\n        raise ValueError(""Multiple elements found (%r) matching the criteria ""\n                         ""in clickdata: %r"" % (el, clickdata))\n    else:\n        raise ValueError(\'No clickable element matching clickdata: %r\' % (clickdata,))\n\nThis is the code I used to call the request (in my case I needed to upload an image):\nwith open(img_path, \'rb\') as file:\n    img = file.read()\n    file_name = os.path.basename(img_path)\n    multipart_file = MultipartFile(file_name, img, ""image/png"")\n    form_data = {\n        ""param"": ""value"", # this is an example of a text parameter\n        ""PicUpload"": multipart_file\n    }\n    yield MultipartFormRequest(url=upload_url, formdata=form_data,\n                               callback=self.my_callback)\n\nIt\'s a shame that so much time has passed and Scrapy still doesn\'t have a built in way to do this, especially since someone wrote a very simple implementation years ago.\n']"
Make a JavaScript-aware Crawler,"
I want to make a script that's crawling a website and it should return the locations of all the banners showed on that page.
The locations of banners are most of the time from known domains. But banners are not in the HTML as an easy image or swf-file. Most of the times a Javascript is used to show the banner.
So if a .swf-file or image-file is loaded from a banner-domain, it should return that url.
Is that possible to do? And how could I do that roughly?
Best would be if it can also returns the landing page of that ad. How to solve that?
",2k,"
            2
        ","[""\nYou could use selenium to open the pages in a real browser and then access the DOM.\nPhantomJS might also be worth a look - it's a headless version of WebKit (the engine behind Chrome, Safari, etc.).\nHowever, none of those solutions are pure php - if that's a requirement, you'll probably have to write your own JavaScript engine in PHP (which is nothing I'd ask my worst enemy to do ;))\n"", '\nIn order to get the output of the JavaScript you will need a JavaScript engine (such as Google\'s V8 Engine). The V8 engine is written in C++ but there are some resources that tell you embed the V8 engine into PHP.\nWith that said, you have to study the output ""by hand"" and determine exactly what can be scraped and how to identify it. Once you\'ve identified some common syntax for the advertisement banners, then you can write a script to extract the banner and the landing page which is referenced.\nNone of this is easy work, but if you have an example of an ad you\'d like to collect then I can give you more advice.\n']"
Are Robots.txt and metadata tags enough to stop search engines to index dynamic pages that are dependent of $_GET variables?,"
I created a php page that is only accessible by means of token/pass received through $_GET
Therefore if you go to the following url you'll get a generic or blank page

http://fakepage11.com/secret_page.php

However if you used the link with the token it shows you special content

http://fakepage11.com/secret_page.php?token=344ee833bde0d8fa008de206606769e4

Of course this is not as safe as a login page, but my only concern is to create a dynamic page that is not indexable and only accessed through the provided link.
Are dynamic pages that are dependent of $_GET variables indexed by google and other search engines?
If so, will include the following be enough to hide it?

Robots.txt User-agent: * Disallow: /

metadata: <META NAME=""ROBOTS"" CONTENT=""NOINDEX"">


Even if I type into google:

site:fakepage11.com/

Thank you!
",930,"
            2
        ","['\nIf a search engine bot finds the link with the token somehow¹, it may crawl and index it.\nIf you use robots.txt to disallow crawling the page, conforming search engine bots won’t crawl the page, but they may still index its URL (which then might appear in a site: search).\nIf you use meta-robots to disallow indexing the page, conforming search engine bots won’t index the page, but they may still crawl it.\nYou can’t have both: If you disallow crawling, conforming bots can never learn that you also disallow indexing, because they are not allowed to visit the page to see your meta-robots element. \n¹ There are countless ways how search engines might find a link. For example, a user that visits the page might use a browser toolbar that automatically sends all visited URLs to a search engine.\n', '\nIf your page isn\'t discoverable then it will not be indexed.\nby ""discoverable"" we mean:\n\nit is a standard web page, i.e. index.*\nit is referenced by another link either yours or from another site\n\nSo in your case by using the get parameter for access, you achieve 1 but not necessarily 2 since someone may reference that link and hence the ""hidden"" page.\nYou can use the robots.txt that you gave and in that case the page will not get indexed by a bot that respects that (not all will do). Not indexing your page doesn\'t mean of course that the ""hidden"" page URL will not be in the wild.\nFurthermore another issue - depending on your requirements - is that you use unencrypted HTTP, that means that your ""hidden"" URLs and content of pages are visible to every server between your server and the user.\nApart from search engines take care that certain services are caching/resolving content when URLs are exchanged for example in Skype or Facebook messenger. In that cases they will visit the URL and try to extract metadata and  maybe cache it if applicable. Of course this scenario does not expose your URL to the public but it is exposed to the systems of those services and with them the content that you have ""hidden"".\nUPDATE:\nAnother issue to consider is the exposing of a ""hidden"" page by linking to another page. In that case in the logs of the server that hosts the linked URL your page will be seen as a referral and thus be visible, that expands also to Google Analytics etc. Thus if you want to remain stealth do not link to another pages from the hidden page.\n']"
Puppeteer not giving accurate HTML code for page with shadow roots,"
I am trying to download the HTML code for the website intersight.com/help/. But puppeteer is not returning the HTML code with hrefs as we can see in the page (example https://intersight.com/help/getting_started is not present in the downloaded HTML). On inspecting the HTML in browser I came to know that all the missing HTML is present inside the <an-hulk></an-hulk> tags. I don't know what these tags mean.
const puppeteer = require('puppeteer');
const fs = require('fs');
(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  const data = await page.goto('https://intersight.com/help/', { waitUntil: 'domcontentloaded' });
  // Tried all the below lines, neither worked
  // await page.waitForSelector('.helplet-links')
  // document.querySelector(""#app > an-hulk"").shadowRoot.querySelector(""#content"").shadowRoot.querySelector(""#main > div > div > div > an-hulk-home"").shadowRoot.querySelector(""div > div > div:nth-child(1) > div:nth-child(1) > div.helplet-links > ul > li:nth-child(1) > a > span"")
  // await page.evaluateHandle(`document.querySelector(""#app > an-hulk"").shadowRoot.querySelector(""#content"").shadowRoot.querySelector(""#main > div > div > div > an-hulk-home"")`);
  await page.evaluateHandle(`document.querySelector(""an-hulk"").shadowRoot.querySelector(""#aside"").shadowRoot.querySelectorAll("".item"")`)
  const result = await page.content()
  fs.writeFile('./intersight.html', result, (err) => {
    if (err) console.log(err)
    else console.log('done!!')
  })
  // console.log(result)
  await browser.close();
})();

",910,"
            1
        ","['\nAs mentioned in the comments, you\'re dealing with a page that uses shadow roots. Traditional selectors that attempt to pierce shadow roots won\'t work through the console or Puppeteer without help. Short of using a library, the idea is to identify any shadow root elements by their .shadowRoot property, then dive into them recursively and repeat the process until you get the data you\'re after.\nThis code should grab all of the hrefs on the page (I didn\'t do a manual count) following this strategy:\nconst puppeteer = require(""puppeteer"");\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch({headless: true});\n  const [page] = await browser.pages();\n  const url = ""https://intersight.com/help/"";\n  const data = await page.goto(url, {\n    waitUntil: ""networkidle0""\n  });\n  await page.waitForSelector(""an-hulk"", {visible: true});\n  const hrefs = await page.evaluate(() => {\n    const walk = root => [\n      ...[...root.querySelectorAll(""a[href]"")]\n        .map(e => e.getAttribute(""href"")),\n      ...[...root.querySelectorAll(""*"")]\n        .filter(e => e.shadowRoot)\n        .flatMap(e => walk(e.shadowRoot))\n    ];\n    return walk(document);\n  });\n  console.log(hrefs);\n  console.log(hrefs.length); // => 44 at the time I ran this\n\n  // Bonus example of diving manually into shadow roots...\n  //const html = await page.evaluate(() =>\n  //  document\n  //    .querySelector(""#app > an-hulk"")\n  //    .shadowRoot\n  //    .querySelector(""#content"")\n  //    .shadowRoot\n  //    .querySelector(""#main an-hulk-home"")\n  //    .shadowRoot\n  //    .querySelector("".content"")\n  //    .innerHTML\n  //);\n  //console.log(html);\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close());\n;\n\nNote that the sidebar and other parts of the page use event listeners on spans and divs to implement links, so these don\'t count as hrefs as far as the above code is concerned. If you want to access these URLs, there are a variety of strategies you can try, including clicking them and extracting the URL after navigation. This is speculative since it\'s not clear that you want to do this.\n\nA few remarks about your code:\n\nPuppeteer wait until page is completely loaded is an important resource. { waitUntil: \'domcontentloaded\' } is a weaker condition than { waitUntil: \'networkidle0\' }. Using page.waitForSelector(selector, {visible: true}) and page.waitForFunction(predicate) are important to use to ensure the elements have been rendered before you begin manipulating them. Even without the shadow root, it\'s not clear to me that the top-level ""an-hulk"" is going to be available by the time you run evaluate.\nAdd console listeners to your page to help debug. Try your queries one step at a time and break them into multiple stages to see where they go wrong.\nfs.writeFile should be await fs.promises.writeFile since you\'re in an async function.\n\n\nAdditional resources and similar threads:\n\nWhat is shadow root\nPuppeteer: Query nodes within shadow roots #858\nHow to get text from shadow root element?\nSelect element within shadow root\npuppeteer: clicking button in shadowroot\nManipulate / set style shadowRoot using Puppeteer\nPuppeteer: get full HTML content of a webpage, like innerHTML, but including any shadow roots?\nPopup form visible, but html code missing in Puppeteer\nCan\'t locate and click on a terms of conditions button\n\n']"
redirect all bots using htaccess apache,"
What .htaccess rewriterule should i use to detect known bots, for example the big ones:
altavista, google, bing, yahoo
I know i can check for their ips, or hosts, but is there a better way?
",3k,"
            1
        ",['\nRewriteCond %{HTTP_USER_AGENT} AltaVista [OR]\nRewriteCond %{HTTP_USER_AGENT} Googlebot [OR]\nRewriteCond %{HTTP_USER_AGENT} msnbot [OR]\nRewriteCond %{HTTP_USER_AGENT} Slurp\nRewriteRule ^.*$ IHateBots.html [L]\n\n']
Difference between BeautifulSoup and Scrapy crawler?,"
I want to make a website that shows the comparison between amazon and e-bay product price.
Which of these will work better and why? I am somewhat familiar with BeautifulSoup but not so much with Scrapy crawler.
",89k,"
            159
        ","['\nScrapy is a Web-spider or web scraper framework, You give Scrapy a root URL to start crawling, then you can specify constraints on how many (number of) URLs you want to crawl and fetch,etc. It is a complete framework for web-scraping or crawling.\nWhile\nBeautifulSoup is a parsing library which also does a pretty good job of fetching contents from URL and allows you to parse certain parts of them without any hassle. It only fetches the contents of the URL that you give and then stops. It does not crawl unless you manually put it inside an infinite loop with certain criteria.\nIn simple words, with Beautiful Soup you can build something similar to Scrapy.\nBeautiful Soup is a library while Scrapy is a complete framework.\nSource\n', ""\nI think both are good... im doing a project right now that use both. First i scrap all the pages using scrapy and save that on a mongodb collection using their pipelines, also downloading the images that exists on the page.\nAfter that i use BeautifulSoup4 to make a pos-processing where i must change attributes values and get some special tags.\nIf you don't know which pages products you want, a good tool will be scrapy since you can use their crawlers to run all amazon/ebay website looking for the products without making a explicit for loop.\nTake a look at the scrapy documentation, it's very simple to use.\n"", ""\nScrapy\nIt is a web scraping framework which comes with tons of goodies which make scraping from easier so that we can focus on crawling logic only. Some of my favourite things scrapy takes care for us are below.\n\nFeed exports: It basically allows us to save data in various formats like CSV,JSON,jsonlines and XML. \nAsynchronous scraping: Scrapy uses twisted framework which gives us power to visit multiple urls at once where each request is processed in non blocking way(Basically we don't have to wait for a request to finish before sending another request).\nSelectors: This is where we can compare scrapy with beautiful soup. Selectors are what allow us to select particular data from the webpage like heading, certain div with a class name etc.). Scrapy uses lxml for parsing which is extremely fast than beautiful soup.\nSetting proxy,user agent ,headers etc: scrapy allows us to set and rotate proxy,and other headers dynamically.\nItem Pipelines: Pipelines enable us to process data after extraction. For example we can configure pipeline to push data to your mysql server.\nCookies: scrapy automatically handles cookies for us.\n\netc.\n\nTLDR: scrapy is a framework that provides everything that one might\n  need to build large scale crawls. It provides various features that\n  hide complexity of crawling the webs. one can simply start writing web\n  crawlers without worrying about the setup burden.\n\nBeautiful soup\nBeautiful Soup is a Python package for parsing HTML and XML documents. So with Beautiful soup you can parse a webpage that has been already downloaded. BS4 is very popular and old. Unlike scrapy,You cannot use beautiful soup only to make crawlers. You will need other libraries like requests,urllib etc to make crawlers with bs4. Again, this means you would need to manage the list of urls being crawled,to be crawled, handle cookies , manage proxy, handle errors, create your own functions to push data to CSV,JSON,XML etc. If you want to speed up than you will have to use other libraries like multiprocessing.\nTo sum up.\n\nScrapy is a rich framework that you can use to start writing crawlers\nwithout any hassale.\nBeautiful soup is a library that you can use to parse a webpage. It\ncannot be used alone to scrape web.\n\nYou should definitely use scrapy for your amazon and e-bay product price comparison website. You could build a database of urls and run the crawler every day(cron jobs,Celery for scheduling crawls) and update the price on your database.This way your website will always pull from the database and crawler and database will act as individual components.\n"", '\nBoth are using to parse data.\nScrapy:\n\nScrapy is a fast high-level web crawling and web scraping framework,\nused to crawl websites and extract structured data from their pages.\nBut it has some limitations when data comes from java script or\nloading dynamicaly, we can over come it by using packages like splash, \nselenium etc.\n\nBeautifulSoup:\n\nBeautiful Soup is a Python library for pulling data out of HTML and\nXML files.\nwe can use this package for getting data from java script or \ndynamically loading pages.\n\nScrapy with BeautifulSoup is one of the best combo we can work with for scraping static and dynamic contents \n', ""\nThe way I do it is to use the eBay/Amazon API's rather than scrapy, and then parse the results using BeautifulSoup.\nThe APIs gives you an official way of getting the same data that you would have got from scrapy crawler, with no need to worry about hiding your identity, mess about with proxies,etc.\n"", '\nBeautifulSoup is a library that lets you extract information from a web page.\nScrapy on the other hand is a framework, which does the above thing and many more things you probably need in your scraping project like pipelines for saving data.\nYou can check this blog to get started with Scrapy\nhttps://www.inkoop.io/blog/web-scraping-using-python-and-scrapy/\n', ""\nUsing scrapy you can save tons of code and start with structured programming, If you dont like any of the scapy's pre-written methods then BeautifulSoup can be used in the place of scrapy method.\nBig project takes both advantages.\n"", '\nBeautifulsoup is web scraping small library. it does your job but sometime it does not satisfy your needs.i mean if you scrape  websites in large amount of data  so here in this case beautifulsoup fails.\nIn this case  you should use Scrapy which is  a complete  scraping framework  which will do you job.\nAlso scrapy has support for databases(all kind of databases) so it is a huge\nof scrapy over other web  scraping libraries.\n', '\nThe differences are many and selection of any tool/technology depends on individual needs.\nFew major differences are:\n\nBeautifulSoup is comparatively is easy to learn than Scrapy. \nThe extensions, support, community is larger for Scrapy than for BeautifulSoup.\nScrapy should be considered as a Spider while BeautifulSoup is a Parser.\n\n', '\nLong story short:\n\nScrapy is a multitool. BS4 is a penknife.\n\nNow a list of peculiarities for each one from personal experience:\nScrapy:\n\nheavy\nissues with installing dependencies might occur\ntakes time to master\nwell supported and documented, always up to date with a large and active community\nfast to extract\ngood for large jobs\nhas a native cloud (can deploy code into the cloud and forget about it until done)\nhas a native API\nvariety of settings and add-ons (middlewares), allows you to fine-tune your code to the slightest details.\nhighly  structured code\neasily integrates with residential proxies, and offers middleware for IP rotation.\nhandy informative(cloud) interface, handy for debugging\n\nbs4:\n\nlightweight\nfast to install\nfast to learn\nfast and dirty to code\nis suitable for simple tasks\nis suitable for testing sites and hypothesis\ncan use curl from chrome dev tools and convert curl to requests and use the result directly in your code for cookie-dependent sites or complex post requests.\n\nSummary:\nUse bs4 If you are just starting or using scraping once in a while for small projects.\nUse Scrapy if you are a professional web scraper that has to deal with large-scale data collection, and have to run the scraper for a long time.\n']"
Get a list of URLs from a site [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 7 years ago.







                        Improve this question
                    



I'm deploying a replacement site for a client but they don't want all their old pages to end in 404s. Keeping the old URL structure wasn't possible because it was hideous.
So I'm writing a 404 handler that should look for an old page being requested and do a permanent redirect to the new page. Problem is, I need a list of all the old page URLs.
I could do this manually, but I'd be interested if there are any apps that would provide me a list of relative (eg: /page/path, not http:/.../page/path) URLs just given the home page. Like a spider but one that doesn't care about the content other than to find deeper pages.
",494k,"
            117
        ","[""\nI didn't mean to answer my own question but I just thought about running a sitemap generator. First one I found http://www.xml-sitemaps.com has a nice text output. Perfect for my needs.\n"", ""\ndo wget -r -l0 www.oldsite.com\nThen just find www.oldsite.com would reveal all urls, I believe.\nAlternatively, just serve that custom not-found page on every 404 request!\nI.e. if someone used the wrong link, he would get the page telling that page wasn't found, and making some hints about site's content.\n"", '\nHere is a list of sitemap generators (from which obviously you can get the list of URLs from a site): http://code.google.com/p/sitemap-generators/wiki/SitemapGenerators\n\nWeb Sitemap Generators\nThe following are links to tools that generate or maintain files in\n  the XML Sitemaps format, an open standard defined on sitemaps.org and\n  supported by the search engines such as Ask, Google, Microsoft Live\n  Search and Yahoo!. Sitemap files generally contain a collection of\n  URLs on a website along with some meta-data for these URLs. The\n  following tools generally generate ""web-type"" XML Sitemap and URL-list\n  files (some may also support other formats).\nPlease Note: Google has not tested or verified the features or\n  security of the third party software listed on this site. Please\n  direct any questions regarding the software to the software\'s author.\n  We hope you enjoy these tools!\nServer-side Programs\n\nEnarion phpSitemapsNG (PHP)\nGoogle Sitemap Generator (Linux/Windows, 32/64bit, open-source)\nOutil en PHP (French, PHP)\nPerl Sitemap Generator (Perl)\nPython Sitemap Generator (Python)\nSimple Sitemaps (PHP)\nSiteMap XML Dynamic Sitemap Generator (PHP) $\nSitemap generator for OS/2 (REXX-script)\nXML Sitemap Generator (PHP) $\n\nCMS and Other Plugins:\n\nASP.NET - Sitemaps.Net\nDotClear (Spanish)\nDotClear (2)\nDrupal\nECommerce Templates (PHP) $\nEcommerce Templates (PHP or ASP) $\nLifeType\nMediaWiki Sitemap generator\nmnoGoSearch\nOS Commerce\nphpWebSite\nPlone\nRapidWeaver\nTextpattern\nvBulletin\nWikka Wiki (PHP)\nWordPress\n\nDownloadable Tools\n\nGSiteCrawler (Windows)\nGWebCrawler & Sitemap Creator (Windows)\nG-Mapper (Windows)\nInspyder Sitemap Creator (Windows) $\nIntelliMapper (Windows) $\nMicrosys A1 Sitemap Generator (Windows) $\nRage Google Sitemap Automator $ (OS-X)\nScreaming Frog SEO Spider and Sitemap generator (Windows/Mac) $\nSite Map Pro (Windows) $\nSitemap Writer (Windows) $\nSitemap Generator by DevIntelligence (Windows)\nSorrowmans Sitemap Tools (Windows)\nTheSiteMapper (Windows) $\nVigos Gsitemap (Windows)\nVisual SEO Studio (Windows)\nWebDesignPros Sitemap Generator (Java Webstart Application)\nWeblight (Windows/Mac) $\nWonderWebWare Sitemap Generator (Windows)\n\nOnline Generators/Services\n\nAuditMyPc.com Sitemap Generator\nAutoMapIt\nAutositemap $\nEnarion phpSitemapsNG\nFree Sitemap Generator\nNeuroticweb.com Sitemap Generator\nROR Sitemap Generator\nScriptSocket Sitemap Generator\nSeoUtility Sitemap Generator (Italian)\nSitemapDoc\nSitemapspal\nSitemapSubmit\nSmart-IT-Consulting Google Sitemaps XML Validator\nXML Sitemap Generator\nXML-Sitemaps Generator\n\nCMS with integrated Sitemap generators\n\nConcrete5\n\nGoogle News Sitemap Generators   The following plugins allow\n  publishers to update Google News Sitemap files, a variant of the\n  sitemaps.org protocol that we describe in our Help Center. In addition\n  to the normal properties of Sitemap files, Google News Sitemaps allow\n  publishers to describe the types of content they publish, along with\n  specifying levels of access for individual articles. More information\n  about Google News can be found in our Help Center and Help Forums.\n\nWordPress Google News plugin\n\nCode Snippets / Libraries\n\nASP script\nEmacs Lisp script\nJava library\nPerl script\nPHP class\nPHP generator script\n\nIf you believe that a tool should be added or removed for a legitimate\n  reason, please leave a comment in the Webmaster Help Forum.\n\n', '\nThe best on I have found is http://www.auditmypc.com/xml-sitemap.asp which uses Java, and has no limit on pages, and even lets you export results as a raw URL list.\nIt also uses sessions, so if you are using a CMS, make sure you are logged out before you run the crawl.\n', '\nSo, in an ideal world you\'d have a spec for all pages in your site. You would also have a test infrastructure that could hit all your pages to test them.\nYou\'re presumably not in an ideal world. Why not do this...?\n\nCreate a mapping between the well\nknown old URLs and the new ones.\nRedirect when you see an old URL.\nI\'d possibly consider presenting a\n""this page has moved, it\'s new url\nis XXX, you\'ll be redirected\nshortly"".\nIf you have no mapping, present a\n    ""sorry - this page has moved. Here\'s\n    a link to the home page"" message and\n    redirect them if you like.\nLog all redirects - especially the\n    ones with no mapping. Over time, add\n    mappings for pages that are\n    important.\n\n', ""\nwget from a linux box might also be a good option as there are switches to spider and change it's output.\nEDIT: wget is also available on Windows: http://gnuwin32.sourceforge.net/packages/wget.htm\n"", '\nWrite a spider which reads in every html from disk and outputs every ""href"" attribute of an ""a"" element (can be done with a parser). Keep in mind which links belong to a certain page (this is common task for a MultiMap datastructre). After this you can produce a mapping file which acts as the input for the 404 handler.\n', '\nI would look into any number of online sitemap generation tools.  Personally, I\'ve used this one (java based)in the past, but if you do a google search for ""sitemap builder"" I\'m sure you\'ll find lots of different options.\n']"
crawler vs scraper,"
Can somebody distinguish between a crawler and scraper in terms of scope and functionality.
",40k,"
            78
        ","[""\nA crawler gets web pages -- i.e., given a starting address (or set of starting addresses) and some conditions (e.g., how many links deep to go, types of files to ignore) it downloads whatever is linked to from the starting point(s).\nA scraper takes pages that have been downloaded or, in a more general sense, data that's formatted for display, and (attempts to) extract data from those pages, so that it can (for example) be stored in a database and manipulated as desired.\nDepending on how you use the result, scraping may well violate the rights of the owner of the information and/or user agreements about use of web sites (crawling violates the latter in some cases as well). Many sites include a file named robots.txt in their root (i.e. having the URL http://server/robots.txt) to specify how (and if) crawlers should treat that site -- in particular, it can list (partial) URLs that a crawler should not attempt to visit. These can be specified separately per crawler (user-agent) if desired.\n"", ""\nCrawlers surf the web, following links.  An example would be the Google robot that gets pages to index.  Scrapers extract values from forms, but don't necessarily have anything to do with the web.\n"", '\nWeb crawler gets links (Urls - Pages) in a logic and scraper get values (extracting) from HTML.\nThere are so many web crawler tools. Visit page to see some. Any XML - HTML parser can used to extract (scrape) data from crawled pages. (I recommend Jsoup for parsing and extracting data)\n', ""\nGenerally, crawlers would follow the links to reach numerous pages while scrapers is, in some sense, just pulling the contents displayed online and would not reach the deeper links. \nThe most typical crawler is google bots, which would follow the links to reach all the web pages on your website and would index the contents if they found it useful(that's why you need robots.txt to tell which contents you do not want to be indexed). So we could search such kind of contents on its website. While the purpose of scrapers is just to pull the contents for personal uses and would not have much effects on others. \nHowever, there's no distinct difference about crawlers and scrapers now as some automated web scraping tools also allow you to crawl the website by following the links, like Octoparse and import.io. They are not the crawlers like google bots, but they are able to automatically crawl the websites to get numerous data without coding.\n"", '\nScrapers and crawlers do not always distinguish, I mean - you can find crawlers which scrape, in fact, Scraper Crawler is doing both and is named accordingly:\n\nit crawls to a URL i.e. indexes all the URL in that main URL\ndepth of crawling is how far the indexing goes in the URL tree\nthen it scrapes whatever you define in a regexp\n\n', ""\nI know this question is quite old, but I'll respond anyway for the newcomer that will wonder here.\nFrom what I can gather and understand it seems that those two terms are often confused with each other due to their similarity and people will often refer to them as the same thing.\nHowever, they are not quite the same. A crawler(or spider) will follow each link in the page it crawls from the starter page. This is why it is also referred to as a spider bot since it will create a kind of a spider web of pages.\nA scraper will extract the data from a page, usually from the pages downloaded with the crawler.\nIf you are interested in either of those, you can try the Norconex HTTP Collector.\n""]"
Detect Search Crawlers via JavaScript,"
I am wondering how would I go abouts in detecting search crawlers? The reason I ask is because I want to suppress certain JavaScript calls if the user agent is a bot.
I have found an example of how to to detect a certain browser, but am unable to find examples of how to detect a search crawler: 
/MSIE (\d+\.\d+);/.test(navigator.userAgent); //test for MSIE x.x
Example of search crawlers I want to block:
Google 
Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html) 
Googlebot/2.1 (+http://www.googlebot.com/bot.html) 
Googlebot/2.1 (+http://www.google.com/bot.html) 

Baidu 
Baiduspider+(+http://www.baidu.com/search/spider_jp.html) 
Baiduspider+(+http://www.baidu.com/search/spider.htm) 
BaiDuSpider 

",46k,"
            61
        ","['\nThis is the regex the ruby UA agent_orange library uses to test if a userAgent looks to be a bot. You can narrow it down for specific bots by referencing the bot userAgent list here:\n/bot|crawler|spider|crawling/i\n\nFor example you have some object, util.browser, you can store what type of device a user is on:\nutil.browser = {\n   bot: /bot|googlebot|crawler|spider|robot|crawling/i.test(navigator.userAgent),\n   mobile: ...,\n   desktop: ...\n}\n\n', '\nTry this. It\'s based on the crawlers list on available on https://github.com/monperrus/crawler-user-agents\nvar botPattern = ""(googlebot\\/|bot|Googlebot-Mobile|Googlebot-Image|Google favicon|Mediapartners-Google|bingbot|slurp|java|wget|curl|Commons-HttpClient|Python-urllib|libwww|httpunit|nutch|phpcrawl|msnbot|jyxobot|FAST-WebCrawler|FAST Enterprise Crawler|biglotron|teoma|convera|seekbot|gigablast|exabot|ngbot|ia_archiver|GingerCrawler|webmon |httrack|webcrawler|grub.org|UsineNouvelleCrawler|antibot|netresearchserver|speedy|fluffy|bibnum.bnf|findlink|msrbot|panscient|yacybot|AISearchBot|IOI|ips-agent|tagoobot|MJ12bot|dotbot|woriobot|yanga|buzzbot|mlbot|yandexbot|purebot|Linguee Bot|Voyager|CyberPatrol|voilabot|baiduspider|citeseerxbot|spbot|twengabot|postrank|turnitinbot|scribdbot|page2rss|sitebot|linkdex|Adidxbot|blekkobot|ezooms|dotbot|Mail.RU_Bot|discobot|heritrix|findthatfile|europarchive.org|NerdByNature.Bot|sistrix crawler|ahrefsbot|Aboundex|domaincrawler|wbsearchbot|summify|ccbot|edisterbot|seznambot|ec2linkfinder|gslfbot|aihitbot|intelium_bot|facebookexternalhit|yeti|RetrevoPageAnalyzer|lb-spider|sogou|lssbot|careerbot|wotbox|wocbot|ichiro|DuckDuckBot|lssrocketcrawler|drupact|webcompanycrawler|acoonbot|openindexspider|gnam gnam spider|web-archive-net.com.bot|backlinkcrawler|coccoc|integromedb|content crawler spider|toplistbot|seokicks-robot|it2media-domain-crawler|ip-web-crawler.com|siteexplorer.info|elisabot|proximic|changedetection|blexbot|arabot|WeSEE:Search|niki-bot|CrystalSemanticsBot|rogerbot|360Spider|psbot|InterfaxScanBot|Lipperhey SEO Service|CC Metadata Scaper|g00g1e.net|GrapeshotCrawler|urlappendbot|brainobot|fr-crawler|binlar|SimpleCrawler|Livelapbot|Twitterbot|cXensebot|smtbot|bnf.fr_bot|A6-Indexer|ADmantX|Facebot|Twitterbot|OrangeBot|memorybot|AdvBot|MegaIndex|SemanticScholarBot|ltx71|nerdybot|xovibot|BUbiNG|Qwantify|archive.org_bot|Applebot|TweetmemeBot|crawler4j|findxbot|SemrushBot|yoozBot|lipperhey|y!j-asr|Domain Re-Animator Bot|AddThis)"";\nvar re = new RegExp(botPattern, \'i\');\nvar userAgent = navigator.userAgent; \nif (re.test(userAgent)) {\n    console.log(\'the user agent is a crawler!\');\n}\n\n', ""\nThe following regex will match the biggest search engines according to this post.\n/bot|google|baidu|bing|msn|teoma|slurp|yandex/i\n    .test(navigator.userAgent)\n\nThe matches search engines are:\n\nBaidu\nBingbot/MSN\nDuckDuckGo (duckduckbot)\nGoogle\nTeoma\nYahoo!\nYandex\n\nAdditionally, I've added bot as a catchall for smaller crawlers/bots.\n"", '\nThis might help to detect the robots user agents while also keeping things more organized:\nJavascript\nconst detectRobot = (userAgent) => {\n  const robots = new RegExp([\n    /bot/,/spider/,/crawl/,                            // GENERAL TERMS\n    /APIs-Google/,/AdsBot/,/Googlebot/,                // GOOGLE ROBOTS\n    /mediapartners/,/Google Favicon/,\n    /FeedFetcher/,/Google-Read-Aloud/,\n    /DuplexWeb-Google/,/googleweblight/,\n    /bing/,/yandex/,/baidu/,/duckduck/,/yahoo/,        // OTHER ENGINES\n    /ecosia/,/ia_archiver/,\n    /facebook/,/instagram/,/pinterest/,/reddit/,       // SOCIAL MEDIA\n    /slack/,/twitter/,/whatsapp/,/youtube/,\n    /semrush/,                                         // OTHER\n  ].map((r) => r.source).join(""|""),""i"");               // BUILD REGEXP + ""i"" FLAG\n\n  return robots.test(userAgent);\n};\n\nTypescript\nconst detectRobot = (userAgent: string): boolean => {\n  const robots = new RegExp(([\n    /bot/,/spider/,/crawl/,                               // GENERAL TERMS\n    /APIs-Google/,/AdsBot/,/Googlebot/,                   // GOOGLE ROBOTS\n    /mediapartners/,/Google Favicon/,\n    /FeedFetcher/,/Google-Read-Aloud/,\n    /DuplexWeb-Google/,/googleweblight/,\n    /bing/,/yandex/,/baidu/,/duckduck/,/yahoo/,           // OTHER ENGINES\n    /ecosia/,/ia_archiver/,\n    /facebook/,/instagram/,/pinterest/,/reddit/,          // SOCIAL MEDIA\n    /slack/,/twitter/,/whatsapp/,/youtube/,\n    /semrush/,                                            // OTHER\n  ] as RegExp[]).map((r) => r.source).join(""|""),""i"");     // BUILD REGEXP + ""i"" FLAG\n\n  return robots.test(userAgent);\n};\n\n\nUse on server:\nconst userAgent = req.get(\'user-agent\');\nconst isRobot = detectRobot(userAgent);\n\nUse on ""client"" / some phantom browser a bot might be using:\nconst userAgent = navigator.userAgent;\nconst isRobot = detectRobot(userAgent);\n\nOverview of Google crawlers:\nhttps://developers.google.com/search/docs/advanced/crawling/overview-google-crawlers\n', ""\nisTrusted property could help you.\n\nThe isTrusted read-only property of the Event interface is a Boolean\nthat is true when the event was generated by a user action, and false\nwhen the event was created or modified by a script or dispatched via\nEventTarget.dispatchEvent().\n\neg:\nisCrawler() {\n  return event.isTrusted;\n}\n\n⚠ Note that IE isn't compatible.\nRead more from doc: https://developer.mozilla.org/en-US/docs/Web/API/Event/isTrusted\n"", '\nPeople might light to check out the new navigator.webdriver property, which allows bots to inform you that they are bots:\nhttps://developer.mozilla.org/en-US/docs/Web/API/Navigator/webdriver\n\nThe webdriver read-only property of the navigator interface indicates whether the user agent is controlled by automation.\n\n\nIt defines a standard way for co-operating user agents to inform the document that it is controlled by WebDriver, for example, so that alternate code paths can be triggered during automation.\n\nIt is supported by all major browsers and respected by major browser automation software like Puppeteer. Users of automation software can of course disable it, and so it should only be used to detect ""good"" bots.\n', '\nI combined some of the above and removed some redundancy. I use this in .htaccess on a semi-private site:\n(google|bot|crawl|spider|slurp|baidu|bing|msn|teoma|yandex|java|wget|curl|Commons-HttpClient|Python-urllib|libwww|httpunit|nutch|biglotron|convera|gigablast|archive|webmon|httrack|grub|netresearchserver|speedy|fluffy|bibnum|findlink|panscient|IOI|ips-agent|yanga|Voyager|CyberPatrol|postrank|page2rss|linkdex|ezooms|heritrix|findthatfile|Aboundex|summify|ec2linkfinder|facebook|slack|instagram|pinterest|reddit|twitter|whatsapp|yeti|RetrevoPageAnalyzer|sogou|wotbox|ichiro|drupact|coccoc|integromedb|siteexplorer|proximic|changedetection|WeSEE|scrape|scaper|g00g1e|binlar|indexer|MegaIndex|ltx71|BUbiNG|Qwantify|lipperhey|y!j-asr|AddThis)\n', '\nThe ""test for MSIE x.x"" example is just code for testing the userAgent against a Regular Expression. In your example the Regexp is the\n/MSIE (\\d+\\.\\d+);/\n\npart. Just replace it with your own Regexp you want to test the user agent against. It would be something like\n/Google|Baidu|Baiduspider/.test(navigator.userAgent)\n\nwhere the vertical bar is the ""or"" operator to match the user agent against all of your mentioned robots. For more information about Regular Expression you can refer to this site since javascript uses perl-style RegExp.\n', ""\nI found this isbot package that has the built-in isbot() function. It seams to me that the package is properly maintained and that they keep everything up-to-date.\nUSAGE:\nconst isBot = require('isbot');\n\n...\n\nisBot(req.get('user-agent'));\n\nPackage: https://www.npmjs.com/package/isbot\n""]"
Node.JS: How to pass variables to asynchronous callbacks? [duplicate],"






This question already has answers here:
                        
                    



JavaScript closure inside loops – simple practical example

                                (44 answers)
                            

Closed 6 years ago.



I'm sure my problem is based on a lack of understanding of asynch programming in node.js but here goes.
For example: I have a list of links I want to crawl. When each asynch request returns I want to know which URL it is for. But, presumably because of race conditions, each request returns with the URL set to the last value in the list.
var links = ['http://google.com', 'http://yahoo.com'];
for (link in links) {
    var url = links[link];
    require('request')(url, function() {
        console.log(url);
    });
}

Expected output:
http://google.com
http://yahoo.com

Actual output:
http://yahoo.com
http://yahoo.com

So my question is either:

How do I pass url (by value) to the call back function? OR
What is the proper way of chaining the HTTP requests so they run sequentially? OR
Something else I'm missing?

PS: For 1. I don't want a solution which examines the callback's parameters but a general way of a callback knowing about variables 'from above'.
",49k,"
            46
        ","[""\nYour url variable is not scoped to the for loop as JavaScript only supports global and function scoping.  So you need to create a function scope for your request call to capture the url value in each iteration of the loop by using an immediate function:\nvar links = ['http://google.com', 'http://yahoo.com'];\nfor (link in links) {\n    (function(url) {\n        require('request')(url, function() {\n            console.log(url);\n        });\n    })(links[link]);\n}\n\nBTW, embedding a require in the middle of loop isn't good practice.  It should probably be re-written as:\nvar request = require('request');\nvar links = ['http://google.com', 'http://yahoo.com'];\nfor (link in links) {\n    (function(url) {\n        request(url, function() {\n            console.log(url);\n        });\n    })(links[link]);\n}\n\n"", ""\nCheck this blog out. A variable can be passed by using .bind() method. In your case it would be like this:\nvar links = ['http://google.com', 'http://yahoo.com'];\nfor (link in links) {\nvar url = links[link];\n\nrequire('request')(url, function() {\n\n    console.log(this.urlAsy);\n\n}.bind({urlAsy:url}));\n}\n\n"", ""\nSee https://stackoverflow.com/a/11747331/243639 for a general discussion of this issue.\nI'd suggest something like\nvar links = ['http://google.com', 'http://yahoo.com'];\n\nfunction createCallback(_url) {\n    return function() {\n        console.log(_url);\n    }\n};\n\nfor (link in links) {\n    var url = links[link];\n    require('request')(url, createCallback(url));\n}\n\n""]"
how to totally ignore 'debugger' statement in chrome?,"
'never pause here' can not work

after I continue：

still paused
",28k,"
            38
        ","['\nTo totally ignore all breakpoints in Chrome, you must do as follows:\n\nOpen your page in the Chrome browser.\n\nPress F12 or right-click on the page and select Inspect.\n\nIn the Source panel, press Ctrl+F8 to deactivate all breakpoints. (or: At the top-right corner, select deactivate breakpoints.)\n\n\nAll breakpoints and debugger statements will be deactivated.\nI tested it in Chrome 79.0.3945.88 (64-bit) and I found that the debugger statement is ignored.\n\n', '\nTo stop hitting debugger statements, you must either set a ""never pause here"" breakpoint, OR you must pause stopping on exceptions.\nThis works because debugger breakpoints are considered exceptions by the browser.\n\n']"
How do I lock read/write to MySQL tables so that I can select and then insert without other programs reading/writing to the database?,"
I am running many instances of a webcrawler in parallel.
Each crawler selects a domain from a table, inserts that url and a start time into a log table, and then starts crawling the domain.
Other parallel crawlers check the log table to see what domains are already being crawled before selecting their own domain to crawl.
I need to prevent other crawlers from selecting a domain that has just been selected by another crawler but doesn't have a log entry yet.  My best guess at how to do this is to lock the database from all other read/writes while one crawler selects a domain and inserts a row in the log table (two queries).
How the heck does one do this?  I'm afraid this is terribly complex and relies on many other things.  Please help get me started.

This code seems like a good solution (see the error below, however):
INSERT INTO crawlLog (companyId, timeStartCrawling)
VALUES
(
    (
        SELECT companies.id FROM companies
        LEFT OUTER JOIN crawlLog
        ON companies.id = crawlLog.companyId
        WHERE crawlLog.companyId IS NULL
        LIMIT 1
    ),
    now()
)

but I keep getting the following mysql error:
You can't specify target table 'crawlLog' for update in FROM clause

Is there a way to accomplish the same thing without this problem?  I've tried a couple different ways.  Including this:
INSERT INTO crawlLog (companyId, timeStartCrawling)
VALUES
(
    (
        SELECT id
        FROM companies
        WHERE id NOT IN (SELECT companyId FROM crawlLog) LIMIT 1
    ),
    now()
)

",104k,"
            38
        ","['\nYou can lock tables using the MySQL LOCK TABLES command like this:\nLOCK TABLES tablename WRITE;\n\n# Do other queries here\n\nUNLOCK TABLES;\n\nSee:\nhttp://dev.mysql.com/doc/refman/5.5/en/lock-tables.html\n', '\nWell, table locks are one way to deal with that; but this makes parallel requests impossible. If the table is InnoDB you could force a row lock instead, using SELECT ... FOR UPDATE within a transaction. \nBEGIN;\n\nSELECT ... FROM your_table WHERE domainname = ... FOR UPDATE\n\n# do whatever you have to do\n\nCOMMIT;\n\nPlease note that you will need an index on domainname (or whatever column you use in the WHERE-clause) for this to work, but this makes sense in general and I assume you will have that anyway.\n', '\nYou probably don\'t want to lock the table.  If you do that you\'ll have to worry about trapping errors when the other crawlers try to write to the database - which is what you were thinking when you said ""...terribly complex and relies on many other things.""\nInstead you should probably wrap the group of queries in a MySQL transaction (see http://dev.mysql.com/doc/refman/5.0/en/commit.html) like this:\nSTART TRANSACTION;\nSELECT @URL:=url FROM tablewiththeurls WHERE uncrawled=1 ORDER BY somecriterion LIMIT 1;\nINSERT INTO loggingtable SET url=@URL;\nCOMMIT;\n\nOr something close to that.\n[edit]  I just realized - you could probably do everything you need in a single query and not even have to worry about transactions.  Something like this:\nINSERT INTO loggingtable (url) SELECT url FROM tablewithurls u LEFT JOIN loggingtable l ON l.url=t.url WHERE {some criterion used to pick the url to work on} AND l.url IS NULL.\n\n', ""\nI got some inspiration from @Eljakim's answer and started this new thread where I figured out a great trick.  It doesn't involve locking anything and is very simple.\nINSERT INTO crawlLog (companyId, timeStartCrawling)\nSELECT id, now()\nFROM companies\nWHERE id NOT IN\n(\n    SELECT companyId\n    FROM crawlLog AS crawlLogAlias\n)\nLIMIT 1\n\n"", ""\nI wouldn't use locking, or transactions.\nThe easiest way to go is to INSERT a record in the logging table if it's not yet present, and then check for that record.\nAssume you have tblcrawels (cra_id) that is filled with your crawlers and tblurl (url_id) that is filled with the URLs, and a table tbllogging (log_cra_id, log_url_id) for your logfile.\nYou would run the following query if crawler 1 wants to start crawling url 2:\nINSERT INTO tbllogging (log_cra_id, log_url_id) \nSELECT 1, url_id FROM tblurl LEFT JOIN tbllogging on url_id=log_url \nWHERE url_id=2 AND log_url_id IS NULL;\n\nThe next step is to check whether this record has been inserted.\nSELECT * FROM tbllogging WHERE log_url_id=2 AND log_cra_id=1\n\nIf you get any results then crawler 1 can crawl this url. If you don't get any results this means that another crawler has inserted in the same line and is already crawling.\n"", ""\nIt's better to use row lock or transactional based query so that other parallel request context can access the table.\n""]"
Passing arguments to process.crawl in Scrapy python,"
I would like to get the same result as this command line :
scrapy crawl linkedin_anonymous -a first=James -a last=Bond -o output.json
My script is as follows :
import scrapy
from linkedin_anonymous_spider import LinkedInAnonymousSpider
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

spider = LinkedInAnonymousSpider(None, ""James"", ""Bond"")
process = CrawlerProcess(get_project_settings())
process.crawl(spider) ## <-------------- (1)
process.start()

I found out that process.crawl() in (1) is creating another LinkedInAnonymousSpider where first and last are None (printed in (2)), if so, then there is no point of creating the object spider and how is it possible to pass the arguments first and last to process.crawl()?
linkedin_anonymous :
from logging import INFO

import scrapy

class LinkedInAnonymousSpider(scrapy.Spider):
    name = ""linkedin_anonymous""
    allowed_domains = [""linkedin.com""]
    start_urls = []

    base_url = ""https://www.linkedin.com/pub/dir/?first=%s&last=%s&search=Search""

    def __init__(self, input = None, first= None, last=None):
        self.input = input  # source file name
        self.first = first
        self.last = last

    def start_requests(self):
        print self.first ## <------------- (2)
        if self.first and self.last: # taking input from command line parameters
                url = self.base_url % (self.first, self.last)
                yield self.make_requests_from_url(url)

    def parse(self, response): . . .

",14k,"
            35
        ","[""\npass the spider arguments on the process.crawl method:\nprocess.crawl(spider, input='inputargument', first='James', last='Bond')\n\n"", '\nYou can do it the easy way:\nfrom scrapy import cmdline\n\ncmdline.execute(""scrapy crawl linkedin_anonymous -a first=James -a last=Bond -o output.json"".split())\n\n', ""\nif you have Scrapyd and you want to schedule the spider, do this\ncurl http://localhost:6800/schedule.json -d project=projectname -d spider=spidername -d first='James' -d last='Bond'\n""]"
How do I use the Python Scrapy module to list all the URLs from my website?,"
I want to use the Python Scrapy module to scrape all the URLs from my website and write the list to a file. I looked in the examples but didn't see any simple example to do this.
",22k,"
            24
        ","[""\nHere's the python program that worked for me:\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.spider import BaseSpider\nfrom scrapy.http import Request\n\nDOMAIN = 'example.com'\nURL = 'http://%s' % DOMAIN\n\nclass MySpider(BaseSpider):\n    name = DOMAIN\n    allowed_domains = [DOMAIN]\n    start_urls = [\n        URL\n    ]\n\n    def parse(self, response):\n        hxs = HtmlXPathSelector(response)\n        for url in hxs.select('//a/@href').extract():\n            if not ( url.startswith('http://') or url.startswith('https://') ):\n                url= URL + url \n            print url\n            yield Request(url, callback=self.parse)\n\nSave this in a file called spider.py.\nYou can then use a shell pipeline to post process this text:\nbash$ scrapy runspider spider.py > urls.out\nbash$ cat urls.out| grep 'example.com' |sort |uniq |grep -v '#' |grep -v 'mailto' > example.urls\n\nThis gives me a list of all the unique urls in my site.\n"", '\nsomething cleaner (and maybe more useful) would be using LinkExtractor\nfrom scrapy.linkextractors import LinkExtractor\n\n    def parse(self, response):\n        le = LinkExtractor() # empty for getting everything, check different options on documentation\n        for link in le.extract_links(response):\n            yield Request(link.url, callback=self.parse)\n\n']"
Java Web Crawler Libraries,"
I wanted to make a Java based web crawler for an experiment. I heard that making a Web Crawler in Java was the way to go if this is your first time. However, I have two important questions.

How will my program 'visit' or 'connect' to web pages? Please give a brief explanation. (I understand the basics of the layers of abstraction from the hardware up to the software, here I am interested in the Java abstractions)
What libraries should I use? I would assume I need a library for connecting to web pages, a library for HTTP/HTTPS protocol, and a library for HTML parsing.

",45k,"
            22
        ","['\nCrawler4j is the best solution for you,\nCrawler4j is an open source Java crawler which provides a simple interface for crawling the Web. You can setup a multi-threaded web crawler in 5 minutes!\nAlso visit. for more java based web crawler tools and brief explanation for each.\n', '\nThis is How your program \'visit\' or \'connect\' to web pages.  \n    URL url;\n    InputStream is = null;\n    DataInputStream dis;\n    String line;\n\n    try {\n        url = new URL(""http://stackoverflow.com/"");\n        is = url.openStream();  // throws an IOException\n        dis = new DataInputStream(new BufferedInputStream(is));\n\n        while ((line = dis.readLine()) != null) {\n            System.out.println(line);\n        }\n    } catch (MalformedURLException mue) {\n         mue.printStackTrace();\n    } catch (IOException ioe) {\n         ioe.printStackTrace();\n    } finally {\n        try {\n            is.close();\n        } catch (IOException ioe) {\n            // nothing to see here\n        }\n    }\n\nThis will download source of html page.\nFor HTML parsing see this\nAlso take a look at jSpider and jsoup\n', ""\nRight now there is a inclusion of many java based HTML parser that support visiting and parsing the HTML pages.\n\nJsoup\nJaunt API\nHtmlCleaner\nJTidy\nNekoHTML\nTagSoup\n\nHere's the complete list of HTML parser with basic comparison.\n"", '\nHave a look at these existing projects if you want to learn how it can be done:\n\nApache Nutch\ncrawler4j\ngecco\nNorconex HTTP Collector\nvidageek crawler\nwebmagic\nWebmuncher\n\nA typical crawler process is a loop consisting of fetching, parsing, link extraction, and processing of the output (storing, indexing). Though the devil is in the details, i.e. how to be ""polite"" and respect robots.txt, meta tags, redirects, rate limits, URL canonicalization, infinite depth, retries, revisits, etc.\n\nFlow diagram courtesy of Norconex HTTP Collector.\n', ""\nFor parsing content, I'm using Apache Tika.\n"", '\nI come up with another solution to propose that no one mention. There is a library called Selenum it is is an open-source automating testing tool used for automating web applications for testing purposes, but is certainly not limited to only this . You can write a web crawler and get benefited from this automation testing tool just as a human would do.\nAs an illustration, i will provide to you a quick tutorial to get a better look of how it works. if you are being bored to read this post take a look at this Video to understand what capabilities this library can offer in order to crawl web pages.\nSelenium Components\nTo begin with Selenium consist of various components that coexisted in a unique process and perform their action on the java program. This main component is called Webdriver and it must be included in your program in order to make it working properly.\nGo to the following site here and download the latest release for your computer OS (Windows, Linux, or MacOS). It is a ZIP archive containing chromedriver.exe. Save it on your computer and then extract it to a convenient location just as C:\\WebDrivers\\User\\chromedriver.exe We will use this location later in the java program.\nThe next step is to inlude the jar library. Assuming you are using maven project to build the java programm you need to add the follow dependency to your pom.xml\n<dependency>\n <groupId>org.seleniumhq.selenium</groupId>\n <artifactId>selenium-java</artifactId>\n <version>3.8.1</version>\n</dependency>\n\nSelenium Web driver Setup\nLet us get started with Selenium. The first step is to create a ChromeDriver instance:\nSystem.setProperty(""webdriver.chrome.driver"", ""C:\\WebDrivers\\User\\chromedriver.exe);\nWebDriver driver = new ChromeDriver();\n\nNow its time to get deeper in code.The following example shows a simple programma that open a web page and extract some useful Html components. It is easy to understand, as it has comments that explain the steps clearly. Please take a brief look to understand how to capture the objects\n//Launch website\n      driver.navigate().to(""http://www.calculator.net/"");\n\n      //Maximize the browser\n      driver.manage().window().maximize();\n\n      // Click on Math Calculators\n      driver.findElement(By.xpath("".//*[@id = \'menu\']/div[3]/a"")).click();\n\n      // Click on Percent Calculators\n      driver.findElement(By.xpath("".//*[@id = \'menu\']/div[4]/div[3]/a"")).click();\n\n      // Enter value 10 in the first number of the percent Calculator\n      driver.findElement(By.id(""cpar1"")).sendKeys(""10"");\n\n      // Enter value 50 in the second number of the percent Calculator\n      driver.findElement(By.id(""cpar2"")).sendKeys(""50"");\n\n      // Click Calculate Button\n      driver.findElement(By.xpath("".//*[@id = \'content\']/table/tbody/tr[2]/td/input[2]"")).click();\n\n\n      // Get the Result Text based on its xpath\n      String result =\n         driver.findElement(By.xpath("".//*[@id = \'content\']/p[2]/font/b"")).getText();\n\n\n      // Print a Log In message to the screen\n      System.out.println("" The Result is "" + result);\n\nOnce you are done with your work, the browser window can be closed with:\ndriver.quit();\n\nSelenium Browser Options\nThere too much functionality you can implement when you working with this library, For example, assuming you are using chrome you can add in your code\nChromeOptions options = new ChromeOptions();\n\nTake look at how we can use WebDriver to open Chrome extensions using ChromeOptions\noptions.addExtensions(new File(""src\\test\\resources\\extensions\\extension.crx""));\n\nThis is for using Incognito mode\noptions.addArguments(""--incognito"");\n\nthis one for disabling javascript and info bars\noptions.addArguments(""--disable-infobars"");\noptions.addArguments(""--disable-javascript"");\n\nthis one if you want to make the browser scraping silently and hide browser crawling in the background\noptions.addArguments(""--headless"");\n\nonce you have done with it then\nWebDriver driver = new ChromeDriver(options);\n\nTo sum up let\'s see what Selenium has to offer and make it a unique choice compared with the other solutions that proposed on this post thus far.\n\nLanguage and Framework Support\nOpen Source Availability\nMulti-Browser Support\nSupport Across Various Operating Systems\nEase Of Implementation\nReusability and Integrations\nParallel Test Execution and Faster Go-to-Market\nEasy to Learn and Use\nConstant Updates\n\n', '\nI recommend you to use the HttpClient library. You can found examples here.\n', '\nI think jsoup is better than others, jsoup runs on Java 1.5 and up, Scala, Android, OSGi, and Google App Engine.\n', '\nI would prefer crawler4j. Crawler4j is an open source Java crawler which provides a simple interface for crawling the Web. You can setup a multi-threaded web crawler in few hours. \n', '\nYou can explore.apache droid or apache nutch to  get the feel of java based crawler\n', '\nThough mainly used for Unit Testing web applications, HttpUnit traverses a website, clicks links, analyzes tables and form elements, and gives you meta data about all the pages.  I use it for Web Crawling, not just for Unit Testing. - http://httpunit.sourceforge.net/\n', '\nHere is a list of available crawler:\nhttps://java-source.net/open-source/crawlers\nBut I suggest using Apache Nutch\n']"
