Title,Description,Views,Votes,Answers,URL,Tag
WebDriverWait not working as expected,"
I am working with selenium to scrape some data.
There is button on the page that I am clicking say ""custom_cols"". This button opens up a window for me where I can select my columns. 
This new window sometimes takes some time to open (around 5 seconds). So to handle this I have used 
WebDriverWait 

with delay as 20 seconds. But some times it fails to select find elements on new window, even if the element is visible. This happens only once in ten times for rest of time it works properly.
I have used same function(WebDriverWait) on other places also and it is works as expected. I mean it waits till the elements gets visible and then clicks it at the moment it finds it.
My question is why elements on new window is not visible even though I am waiting for element to get visible. To add here I have tried to increase delay time but still I get that error once in a while.
My code is here 
def wait_for_elem_xpath(self, delay = None, xpath = """"):
    if delay is None:
        delay = self.delay

    try:
        myElem = WebDriverWait(self.browser, delay).until(EC.presence_of_element_located((By.XPATH , xpath)))
    except TimeoutException:
        print (""xpath: Loading took too much time!"")
    return myElem
select_all_performance = '//*[@id=""mks""]/body/div[7]/div[2]/div/div/div/div/div[2]/div/div[2]/div[2]/div/div[1]/div[1]/section/header/div'
self.wait_for_elem_xpath(xpath = select_all_performance).click()

",14k,"
            16
        ","['\nOnce you wait for the element and moving forward as you are trying to invoke click() method instead of using presence_of_element_located() method you need to use element_to_be_clickable() as follows :\ntry:\n    myElem = WebDriverWait(self.browser, delay).until(EC.element_to_be_clickable((By.XPATH , xpath)))\n\n\nUpdate\nAs per your counter question in the comments here are the details of the three methods :\npresence_of_element_located\npresence_of_element_located(locator) is defined as follows :\nclass selenium.webdriver.support.expected_conditions.presence_of_element_located(locator)\n\nParameter : locator - used to find the element returns the WebElement once it is located\n\nDescription : An expectation for checking that an element is present on the DOM of a page. This does not necessarily mean that the element is visible or interactable (i.e. clickable). \n\nvisibility_of_element_located\nvisibility_of_element_located(locator) is defined as follows :\nclass selenium.webdriver.support.expected_conditions.visibility_of_element_located(locator)\n\nParameter : locator -  used to find the element returns the WebElement once it is located and visible\n\nDescription : An expectation for checking that an element is present on the DOM of a page and visible. Visibility means that the element is not only displayed but also has a height and width that is greater than 0.\n\nelement_to_be_clickable\nelement_to_be_clickable(locator) is defined as follows :\nclass selenium.webdriver.support.expected_conditions.element_to_be_clickable(locator)\n\nParameter : locator - used to find the element returns the WebElement once it is visible, enabled and interactable (i.e. clickable).\n\nDescription : An Expectation for checking an element is visible, enabled and interactable such that you can click it. \n\n']",https://stackoverflow.com/questions/49775502/webdriverwait-not-working-as-expected,web-scraping
Web-scraping JavaScript page with Python,"
I'm trying to develop a simple web scraper. I want to extract text without the HTML code. It works on plain HTML, but not in some pages where JavaScript code adds text.
For example, if some JavaScript code adds some text, I can't see it, because when I call:
response = urllib2.urlopen(request)

I get the original text without the added one (because JavaScript is executed in the client).
So, I'm looking for some ideas to solve this problem.
",438k,"
            264
        ","['\nEDIT Sept 2021: phantomjs isn\'t maintained any more, either\nEDIT 30/Dec/2017: This answer appears in top results of Google searches, so I decided to update it. The old answer is still at the end.\ndryscape isn\'t maintained anymore and the library dryscape developers recommend is Python 2 only. I have found using Selenium\'s python library with Phantom JS as a web driver fast enough and easy to get the work done.\nOnce you have installed Phantom JS, make sure the phantomjs binary is available in the current path:\nphantomjs --version\n# result:\n2.1.1\n\n#Example\nTo give an example, I created a sample page with following HTML code. (link):\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=""utf-8"">\n  <title>Javascript scraping test</title>\n</head>\n<body>\n  <p id=\'intro-text\'>No javascript support</p>\n  <script>\n     document.getElementById(\'intro-text\').innerHTML = \'Yay! Supports javascript\';\n  </script> \n</body>\n</html>\n\nwithout javascript it says: No javascript support and with javascript: Yay! Supports javascript\n#Scraping without JS support:\nimport requests\nfrom bs4 import BeautifulSoup\nresponse = requests.get(my_url)\nsoup = BeautifulSoup(response.text)\nsoup.find(id=""intro-text"")\n# Result:\n<p id=""intro-text"">No javascript support</p>\n\n#Scraping with JS support:\nfrom selenium import webdriver\ndriver = webdriver.PhantomJS()\ndriver.get(my_url)\np_element = driver.find_element_by_id(id_=\'intro-text\')\nprint(p_element.text)\n# result:\n\'Yay! Supports javascript\'\n\n\nYou can also use Python library dryscrape to scrape javascript driven websites.\n#Scraping with JS support:\nimport dryscrape\nfrom bs4 import BeautifulSoup\nsession = dryscrape.Session()\nsession.visit(my_url)\nresponse = session.body()\nsoup = BeautifulSoup(response)\nsoup.find(id=""intro-text"")\n# Result:\n<p id=""intro-text"">Yay! Supports javascript</p>\n\n', '\nWe are not getting the correct results because any javascript generated content needs to be rendered on the DOM. When we fetch an HTML page, we fetch the initial, unmodified by javascript, DOM.\nTherefore we need to render the javascript content before we crawl the page.\nAs selenium is already mentioned many times in this thread (and how slow it gets sometimes was mentioned also), I will list two other possible solutions.\n\nSolution 1: This is a very nice tutorial on how to use Scrapy to crawl javascript generated content and we are going to follow just that.\nWhat we will need:\n\nDocker installed in our machine. This is a plus over other solutions until this point, as it utilizes an OS-independent platform.\nInstall Splash following the instruction listed for our corresponding OS.Quoting from splash documentation:\n\nSplash is a javascript rendering service. It’s a lightweight web browser with an HTTP API, implemented in Python 3 using Twisted and QT5. \n\nEssentially we are going to use Splash to render Javascript generated content.\nRun the splash server: sudo docker run -p 8050:8050 scrapinghub/splash.\nInstall the scrapy-splash plugin: pip install scrapy-splash\nAssuming that we already have a Scrapy project created (if not, let\'s make one), we will follow the guide and update the settings.py:\n\nThen go to your scrapy project’s settings.py and set these middlewares:\nDOWNLOADER_MIDDLEWARES = {\n      \'scrapy_splash.SplashCookiesMiddleware\': 723,\n      \'scrapy_splash.SplashMiddleware\': 725,\n      \'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\': 810,\n}\n\nThe URL of the Splash server(if you’re using Win or OSX this should be the URL of the docker machine: How to get a Docker container\'s IP address from the host?):\nSPLASH_URL = \'http://localhost:8050\'\n\nAnd finally you need to set these values too:\nDUPEFILTER_CLASS = \'scrapy_splash.SplashAwareDupeFilter\'\nHTTPCACHE_STORAGE = \'scrapy_splash.SplashAwareFSCacheStorage\'\n\n\nFinally, we can use a SplashRequest:\n\nIn a normal spider you have Request objects which you can use to open URLs. If the page you want to open contains JS generated data you have to use SplashRequest(or SplashFormRequest) to render the page. Here’s a simple example:\nclass MySpider(scrapy.Spider):\n    name = ""jsscraper""\n    start_urls = [""http://quotes.toscrape.com/js/""]\n\n    def start_requests(self):\n        for url in self.start_urls:\n        yield SplashRequest(\n            url=url, callback=self.parse, endpoint=\'render.html\'\n        )\n\n    def parse(self, response):\n        for q in response.css(""div.quote""):\n        quote = QuoteItem()\n        quote[""author""] = q.css("".author::text"").extract_first()\n        quote[""quote""] = q.css("".text::text"").extract_first()\n        yield quote\n\nSplashRequest renders the URL as html and returns the response which you can use in the callback(parse) method.\n\n\n\nSolution 2: Let\'s call this experimental at the moment (May 2018)...\nThis solution is for Python\'s version 3.6 only (at the moment).\nDo you know the requests module (well who doesn\'t)?\nNow it has a web crawling little sibling: requests-HTML:\n\nThis library intends to make parsing HTML (e.g. scraping the web) as simple and intuitive as possible.\n\n\nInstall requests-html: pipenv install requests-html\nMake a request to the page\'s url:\nfrom requests_html import HTMLSession\n\nsession = HTMLSession()\nr = session.get(a_page_url)\n\nRender the response to get the Javascript generated bits:\nr.html.render()\n\n\nFinally, the module seems to offer scraping capabilities.\nAlternatively, we can try the well-documented way of using BeautifulSoup with the r.html object we just rendered.\n', '\nMaybe selenium can do it.\nfrom selenium import webdriver\nimport time\n\ndriver = webdriver.Firefox()\ndriver.get(url)\ntime.sleep(5)\nhtmlSource = driver.page_source\n\n', ""\nIf you have ever used the Requests module for python before, I recently found out that the developer created a new module called Requests-HTML which now also has the ability to render JavaScript.\nYou can also visit https://html.python-requests.org/ to learn more about this module, or if your only interested about rendering JavaScript then you can visit https://html.python-requests.org/?#javascript-support to directly learn how to use the module to render JavaScript using Python.\nEssentially, Once you correctly install the Requests-HTML module, the following example, which is shown on the above link, shows how you can use this module to scrape a website and render JavaScript contained within the website:\nfrom requests_html import HTMLSession\nsession = HTMLSession()\n\nr = session.get('http://python-requests.org/')\n\nr.html.render()\n\nr.html.search('Python 2 will retire in only {months} months!')['months']\n\n'<time>25</time>' #This is the result.\n\nI recently learnt about this from a YouTube video. Click Here! to watch the YouTube video, which demonstrates how the module works.\n"", ""\nIt sounds like the data you're really looking for can be accessed via secondary URL called by some javascript on the primary page.\nWhile you could try running javascript on the server to handle this, a simpler approach  to might be to load up the page using Firefox and use a tool like Charles or Firebug to identify exactly what that secondary URL is. Then you can just query that URL directly for the data you are interested in.\n"", '\nThis seems to be a good solution also, taken from a great blog post\nimport sys  \nfrom PyQt4.QtGui import *  \nfrom PyQt4.QtCore import *  \nfrom PyQt4.QtWebKit import *  \nfrom lxml import html \n\n#Take this class for granted.Just use result of rendering.\nclass Render(QWebPage):  \n  def __init__(self, url):  \n    self.app = QApplication(sys.argv)  \n    QWebPage.__init__(self)  \n    self.loadFinished.connect(self._loadFinished)  \n    self.mainFrame().load(QUrl(url))  \n    self.app.exec_()  \n\n  def _loadFinished(self, result):  \n    self.frame = self.mainFrame()  \n    self.app.quit()  \n\nurl = \'http://pycoders.com/archive/\'  \nr = Render(url)  \nresult = r.frame.toHtml()\n# This step is important.Converting QString to Ascii for lxml to process\n\n# The following returns an lxml element tree\narchive_links = html.fromstring(str(result.toAscii()))\nprint archive_links\n\n# The following returns an array containing the URLs\nraw_links = archive_links.xpath(\'//div[@class=""campaign""]/a/@href\')\nprint raw_links\n\n', '\nSelenium is the best for scraping JS and Ajax content.\nCheck this article for extracting data from the web using Python\n$ pip install selenium\n\nThen download Chrome webdriver.\nfrom selenium import webdriver\n\nbrowser = webdriver.Chrome()\n\nbrowser.get(""https://www.python.org/"")\n\nnav = browser.find_element_by_id(""mainnav"")\n\nprint(nav.text)\n\nEasy, right?\n', ""\nYou can also execute javascript using webdriver.\nfrom selenium import webdriver\n\ndriver = webdriver.Firefox()\ndriver.get(url)\ndriver.execute_script('document.title')\n\nor store the value in a variable\nresult = driver.execute_script('var text = document.title ; return text')\n\n"", '\nI personally prefer using scrapy and selenium and dockerizing both in separate containers. This way you can install both with minimal hassle and crawl modern websites that almost all contain javascript in one form or another. Here\'s an example:\nUse the scrapy startproject to create your scraper and write your spider, the skeleton can be as simple as this:\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = \'my_spider\'\n    start_urls = [\'https://somewhere.com\']\n\n    def start_requests(self):\n        yield scrapy.Request(url=self.start_urls[0])\n\n\n    def parse(self, response):\n\n        # do stuff with results, scrape items etc.\n        # now were just checking everything worked\n\n        print(response.body)\n\nThe real magic happens in the middlewares.py. Overwrite two methods in the downloader middleware,  __init__ and  process_request, in the following way:\n# import some additional modules that we need\nimport os\nfrom copy import deepcopy\nfrom time import sleep\n\nfrom scrapy import signals\nfrom scrapy.http import HtmlResponse\nfrom selenium import webdriver\n\nclass SampleProjectDownloaderMiddleware(object):\n\ndef __init__(self):\n    SELENIUM_LOCATION = os.environ.get(\'SELENIUM_LOCATION\', \'NOT_HERE\')\n    SELENIUM_URL = f\'http://{SELENIUM_LOCATION}:4444/wd/hub\'\n    chrome_options = webdriver.ChromeOptions()\n\n    # chrome_options.add_experimental_option(""mobileEmulation"", mobile_emulation)\n    self.driver = webdriver.Remote(command_executor=SELENIUM_URL,\n                                   desired_capabilities=chrome_options.to_capabilities())\n\n\ndef process_request(self, request, spider):\n\n    self.driver.get(request.url)\n\n    # sleep a bit so the page has time to load\n    # or monitor items on page to continue as soon as page ready\n    sleep(4)\n\n    # if you need to manipulate the page content like clicking and scrolling, you do it here\n    # self.driver.find_element_by_css_selector(\'.my-class\').click()\n\n    # you only need the now properly and completely rendered html from your page to get results\n    body = deepcopy(self.driver.page_source)\n\n    # copy the current url in case of redirects\n    url = deepcopy(self.driver.current_url)\n\n    return HtmlResponse(url, body=body, encoding=\'utf-8\', request=request)\n\nDont forget to enable this middlware by uncommenting the next lines in the settings.py file:\nDOWNLOADER_MIDDLEWARES = {\n\'sample_project.middlewares.SampleProjectDownloaderMiddleware\': 543,}\n\nNext for dockerization. Create your Dockerfile from a lightweight image (I\'m using python Alpine here), copy your project directory to it, install requirements:\n# Use an official Python runtime as a parent image\nFROM python:3.6-alpine\n\n# install some packages necessary to scrapy and then curl because it\'s  handy for debugging\nRUN apk --update add linux-headers libffi-dev openssl-dev build-base libxslt-dev libxml2-dev curl python-dev\n\nWORKDIR /my_scraper\n\nADD requirements.txt /my_scraper/\n\nRUN pip install -r requirements.txt\n\nADD . /scrapers\n\nAnd finally bring it all together in docker-compose.yaml:\nversion: \'2\'\nservices:\n  selenium:\n    image: selenium/standalone-chrome\n    ports:\n      - ""4444:4444""\n    shm_size: 1G\n\n  my_scraper:\n    build: .\n    depends_on:\n      - ""selenium""\n    environment:\n      - SELENIUM_LOCATION=samplecrawler_selenium_1\n    volumes:\n      - .:/my_scraper\n    # use this command to keep the container running\n    command: tail -f /dev/null\n\nRun docker-compose up -d. If you\'re doing this the first time it will take a while for it to fetch the latest selenium/standalone-chrome and the build your scraper image as well. \nOnce it\'s done, you can check that your containers are running with docker ps and also check that the name of the selenium container matches that of the environment variable that we passed to our scraper container (here, it was SELENIUM_LOCATION=samplecrawler_selenium_1). \nEnter your scraper container with docker exec -ti YOUR_CONTAINER_NAME sh , the command for me was docker exec -ti samplecrawler_my_scraper_1 sh, cd into the right directory and run your scraper with scrapy crawl my_spider.\nThe entire thing is on my github page and you can get it from here\n', '\nA mix of BeautifulSoup and Selenium works very well for me.\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup as bs\n\ndriver = webdriver.Firefox()\ndriver.get(""http://somedomain/url_that_delays_loading"")\n    try:\n        element = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.ID, ""myDynamicElement""))) #waits 10 seconds until element is located. Can have other wait conditions  such as visibility_of_element_located or text_to_be_present_in_element\n\n        html = driver.page_source\n        soup = bs(html, ""lxml"")\n        dynamic_text = soup.find_all(""p"", {""class"":""class_name""}) #or other attributes, optional\n    else:\n        print(""Couldnt locate element"")\n\nP.S. You can find more wait conditions here\n', '\nUsing PyQt5\nfrom PyQt5.QtWidgets import QApplication\nfrom PyQt5.QtCore import QUrl\nfrom PyQt5.QtWebEngineWidgets import QWebEnginePage\nimport sys\nimport bs4 as bs\nimport urllib.request\n\n\nclass Client(QWebEnginePage):\n    def __init__(self,url):\n        global app\n        self.app = QApplication(sys.argv)\n        QWebEnginePage.__init__(self)\n        self.html = """"\n        self.loadFinished.connect(self.on_load_finished)\n        self.load(QUrl(url))\n        self.app.exec_()\n\n    def on_load_finished(self):\n        self.html = self.toHtml(self.Callable)\n        print(""Load Finished"")\n\n    def Callable(self,data):\n        self.html = data\n        self.app.quit()\n\n# url = """"\n# client_response = Client(url)\n# print(client_response.html)\n\n', ""\nYou'll want to use urllib, requests, beautifulSoup and selenium web driver in your script for different parts of the page, (to name a few).\nSometimes you'll get what you need with just one of these modules.\nSometimes you'll need two, three, or all of these modules.\nSometimes you'll need to switch off the js on your browser.\nSometimes you'll need header info in your script.\nNo websites can be scraped the same way and no website can be scraped in the same way forever without having to modify your crawler, usually after a few months. But they can all be scraped! Where there's a will there's a way for sure.\nIf you need scraped data continuously into the future just scrape everything you need and store it in .dat files with pickle.\nJust keep searching how to try what with these modules and copying and pasting your errors into the Google.\n"", '\nPyppeteer\nYou might consider Pyppeteer, a Python port of the Chrome/Chromium driver front-end Puppeteer.\nHere\'s a simple example to show how you can use Pyppeteer to access data that was injected into the page dynamically:\nimport asyncio\nfrom pyppeteer import launch\n\nasync def main():\n    browser = await launch({""headless"": True})\n    [page] = await browser.pages()\n\n    # normally, you go to a live site...\n    #await page.goto(""http://www.example.com"")\n    # but for this example, just set the HTML directly:\n    await page.setContent(""""""\n    <body>\n    <script>\n    // inject content dynamically with JS, not part of the static HTML!\n    document.body.innerHTML = `<p>hello world</p>`; \n    </script>\n    </body>\n    """""")\n    print(await page.content()) # shows that the `<p>` was inserted\n\n    # evaluate a JS expression in browser context and scrape the data\n    expr = ""document.querySelector(\'p\').textContent""\n    print(await page.evaluate(expr, force_expr=True)) # => hello world\n\n    await browser.close()\n\nasyncio.get_event_loop().run_until_complete(main())\n\nSee Pyppeteer\'s reference docs.\n', '\nTry accessing the API directly\nA common scenario you\'ll see in scraping is that the data is being requested asynchronously from an API endpoint by the webpage. A minimal example of this would be the following site:\n\n\n<body>\n<script>\nfetch(""https://jsonplaceholder.typicode.com/posts/1"")\n  .then(res => {\n    if (!res.ok) throw Error(res.status);\n    \n    return res.json();\n  })\n  .then(data => {\n    // inject data dynamically via JS after page load\n    document.body.innerText = data.title;\n  })\n  .catch(err => console.error(err))\n;\n</script>\n</body>\n\n\nIn many cases, the API will be protected by CORS or an access token or prohibitively rate limited, but in other cases it\'s publicly-accessible and you can bypass the website entirely. For CORS issues, you might try cors-anywhere.\nThe general procedure is to use your browser\'s developer tools\' network tab to search the requests made by the page for keywords/substrings of the data you want to scrape. Often, you\'ll see an unprotected API request endpoint with a JSON payload that you can access directly with urllib or requests modules. That\'s the case with the above runnable snippet which you can use to practice. After clicking ""run snippet"", here\'s how I found the endpoint in my network tab:\n\nThis example is contrived; the endpoint URL will likely be non-obvious from looking at the static markup because it could be dynamically assembled, minified and buried under dozens of other requests and endpoints. The network request will also show any relevant request payload details like access token you may need.\nAfter obtaining the endpoint URL and relevant details, build a request in Python using a standard HTTP library and request the data:\n>>> import requests\n>>> res = requests.get(""https://jsonplaceholder.typicode.com/posts/1"")\n>>> data = res.json()\n>>> data[""title""]\n\'sunt aut facere repellat provident occaecati excepturi optio reprehenderit\'\n\nWhen you can get away with it, this tends to be much easier, faster and more reliable than scraping the page with Selenium, Pyppeteer, Scrapy or whatever the popular scraping libraries are at the time you\'re reading this post.\nIf you\'re unlucky and the data hasn\'t arrived via an API request that returns the data in a nice format, it could be part of the original browser\'s payload in a <script> tag, either as a JSON string or (more likely) a JS object. For example:\n\n\n<body>\n<script>\n  var someHardcodedData = {\n    userId: 1,\n    id: 1,\n    title: \'sunt aut facere repellat provident occaecati excepturi optio reprehenderit\', \n    body: \'quia et suscipit\\nsuscipit recusandae con sequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\'\n  };\n  document.body.textContent = someHardcodedData.title;\n</script>\n</body>\n\n\nThere\'s no one-size-fits-all way to obtain this data. The basic technique is to use BeautifulSoup to access the <script> tag text, then apply a regex or a parse to extract the object structure, JSON string, or whatever format the data might be in. Here\'s a proof-of-concept on the sample structure shown above:\nimport json\nimport re\nfrom bs4 import BeautifulSoup\n\n# pretend we\'ve already used requests to retrieve the data, \n# so we hardcode it for the purposes of this example\ntext = """"""\n<body>\n<script>\n  var someHardcodedData = {\n    userId: 1,\n    id: 1,\n    title: \'sunt aut facere repellat provident occaecati excepturi optio reprehenderit\', \n    body: \'quia et suscipit\\nsuscipit recusandae con sequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\'\n  };\n  document.body.textContent = someHardcodedData.title;\n</script>\n</body>\n""""""\nsoup = BeautifulSoup(text, ""lxml"")\nscript_text = str(soup.select_one(""script""))\npattern = r""title: \'(.*?)\'""\nprint(re.search(pattern, script_text, re.S).group(1))\n\nCheck out these resources for parsing JS objects that aren\'t quite valid JSON:\n\nHow to convert raw javascript object to python dictionary?\nHow to Fix JSON Key Values without double-quotes?\n\nHere are some additional case studies/proofs-of-concept where scraping was bypassed using an API:\n\nHow can I scrape yelp reviews and star ratings into CSV using Python beautifulsoup\nBeautiful Soup returns None on existing element\nExtract data from  BeautifulSoup Python\nScraping Bandcamp fan collections via POST (uses a hybrid approach where an initial request was made to the website to extract a token from the markup using BeautifulSoup which was then used in a second request to a JSON endpoint)\n\nIf all else fails, try one of the many dynamic scraping libraries listed in this thread.\n', '\nPlaywright-Python\nYet another option is playwright-python, a port of Microsoft\'s Playwright (itself a Puppeteer-influenced browser automation library) to Python.\nHere\'s the minimal example of selecting an element and grabbing its text:\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch()\n    page = browser.new_page()\n    page.goto(""http://whatsmyuseragent.org/"")\n    ua = page.query_selector("".user-agent"");\n    print(ua.text_content())\n    browser.close()\n\n', '\nAs mentioned, Selenium is a good choice for rendering the results of the JavaScript:\nfrom selenium.webdriver import Firefox\nfrom selenium.webdriver.firefox.options import Options\n\noptions = Options()\noptions.headless = True\nbrowser = Firefox(executable_path=""/usr/local/bin/geckodriver"", options=options)\n\nurl = ""https://www.example.com""\nbrowser.get(url)\n\nAnd gazpacho is a really easy library to parse over the rendered html:\nfrom gazpacho import Soup\n\nsoup = Soup(browser.page_source)\nsoup.find(""a"").attrs[\'href\']\n\n', '\nI recently used requests_html library to solve this problem.\nTheir expanded documentation at readthedocs.io is pretty good (skip the annotated version at pypi.org). If your use case is basic, you are likely to have some success.\nfrom requests_html import HTMLSession\nsession = HTMLSession()\nresponse = session.request(method=""get"",url=""www.google.com/"")\nresponse.html.render()\n\nIf you are having trouble rendering the data you need with response.html.render(), you can pass some javascript to the render function to render the particular js object you need. This is copied from their docs, but it might be just what you need:\n\nIf script is specified, it will execute the provided JavaScript at\nruntime. Example:\n\nscript = """"""\n    () => {\n        return {\n            width: document.documentElement.clientWidth,\n            height: document.documentElement.clientHeight,\n            deviceScaleFactor: window.devicePixelRatio,\n        }\n    } \n""""""\n\n\nReturns the return value of the executed script, if any is provided:\n\n>>> response.html.render(script=script)\n{\'width\': 800, \'height\': 600, \'deviceScaleFactor\': 1}\n\nIn my case, the data I wanted were the arrays that populated a javascript plot but the data wasn\'t getting rendered as text anywhere in the html. Sometimes its not clear at all what the object names are of the data you want if the data is populated dynamically. If you can\'t track down the js objects directly from view source or inspect, you can type in ""window"" followed by ENTER in the debugger console in the browser (Chrome) to pull up a full list of objects rendered by the browser. If you make a few educated guesses about where the data is stored, you might have some luck finding it there. My graph data was under window.view.data in the console, so in the ""script"" variable passed to the .render() method quoted above, I used:\nreturn {\n    data: window.view.data\n}\n\n', '\nEasy and Quick Solution:\nI was dealing with same problem. I want to scrape some data which is build with JavaScript. If I scrape only text from this site with BeautifulSoup then I ended with  tags in text.\nI want to render this  tag and wills to grab information from this.\nAlso, I dont want to use heavy frameworks  like Scrapy and selenium.\nSo, I found that get method of requests module takes urls, and it actually renders the script tag.\nExample:\nimport requests\ncustom_User_agent = ""Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0""\nurl = ""https://www.abc.xyz/your/url""\nresponse = requests.get(url, headers={""User-Agent"": custom_User_agent})\nhtml_text = response.text\n\nThis will renders load site and renders  tags.\nHope this will help as quick and easy solution to render site which is loaded with script tags.\n']",https://stackoverflow.com/questions/8049520/web-scraping-javascript-page-with-python,web-scraping
Scraping data to Google Sheets from a website that uses JavaScript,"
I am trying to import data from the following website to Google Sheets. I want to import all the matches for the day.
https://www.tournamentsoftware.com/tournament/b731fdcd-a0c8-4558-9344-2a14c267ee8b/Matches
I have tried importxml and importhtml, but it seems this does not work as the website uses JavaScript. I have also tried to use Apipheny without any success.
When using Apipheny, the error message is

'Failed to fetch data - please verify your API Request: {DNS error'

",1k,"
            1
        ","['\nTl;Dr\nAdapted from my answer to How to know if Google Sheets IMPORTDATA, IMPORTFEED, IMPORTHTML or IMPORTXML functions are able to get data from a resource hosted on a website? (also posted by me)\nPlease spend some time learning how to use the browsers developers tools so you will be able to identify\n\nif the data is already included in source code of the webpage as JSON / literal JavaScript object or in another form\nif the webpage is doing a GET or POST requests to retrieve the data and when those requests are done (i.e. as some point of the page parsing, or on event)\nif the requests require data from cookies\n\n\nBrief guide about how to use the web browser to find useful details about the webpage / data to import\n\nOpen the source code and look if the required data is included. Sometimes the data is included as JSON and added to the DOM using JavaScript. In this case it might be possible to retrieve the data by using the Google Sheets functions or URL Fetch Service from Google Apps Script.\nLet say that you use Chrome. Open the Dev Tools, then look at the Elements tab. There you will see the DOM. It might be helpful to identify if the data that you want to import besides being on visible elements is included in hidden / not visible elements like <script> tags.\nLook at Source, there you might be able to see the JavaScript code. It might include the data that you want to import as JavaScript object (commonly referred as JSON).\n\n\nThere are a lot of questions about google-sheets +web-scraping that mentions problems using importhtml and/or importxml that already have answers and even many include code (JavaScript snippets, Google Apps Script functions, etc.) that might save you to have to use an specialized web-scraping tool that has a more stepped learning curve. At the bottom of this answer there is a list of questions about using Google Sheets built-in functions, including annotations of the workaround proposed.\nOn Is there a way to get a single response from a text/event-stream without using event listeners? ask about using EventSource. While this can\'t be used on server side code, the answer show how to use the HtmlService to use it on client-side code and retrieve the result to Google Sheets.\n\nAs you already realized, the Google Sheets built-in functions importhtml(), importxml(), importdata() and importfeed() only work with static pages that do not require signing in or other forms of authentication.\nWhen the content of a public page is created dynamically by using JavaScript, it cannot be accessed with those functions, by the other hand the website\'s webmaster may also purposefully have prevented web scraping.\n\nHow to identify if content is added dynamically\nTo check if the content is added dynamically, using Chrome,\n\nOpen the URL of the source data.\nPress F12 to open Chrome Developer Tools\nPress Control+Shift+P to open the Command Menu.\nStart typing javascript, select Disable JavaScript, and then press Enter to run the command. JavaScript is now disabled.\n\nJavaScript will remain disabled in this tab so long as you have DevTools open.\nReload the page to see if the content that you want to import is shown, if it\'s shown it could be imported by using Google Sheets built-in functions, otherwise it\'s not possible but might be possible by using other means for doing web scraping.\n\n According to Wikipedia,\n\n Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.\n\nUse of robots.txt to block Web crawlers\nThe webmasters could use robots.txt file to block access to website. In such case the result will be #N/A Could not fetch URL.\nUse of User agent\nThe webpage could be designed to return a special a custom message instead of the data.\n\nBelow there are more details about how Google Sheets built-in ""web-scraping"" functions works\nIMPORTDATA, IMPORTFEED, IMPORTHTML and IMPORTXML are able to get content from resources hosted on websites that are:\n\nPublicly available. This means that the resource doesn\'t require authorization / to be logged in into any service to access it.\nThe content is ""static"". This mean that if you open the resource using the view source code option of modern web browsers it will be displayed as plain text.\n\nNOTE: The Chrome\'s Inspect tool shows the parsed DOM; in other works the actual structure/content of the web page which could be dynamically modified by JavaScript code or browser extensions/plugins.\n\n\nThe content has the appropriated structure.\n\nIMPORTDATA works with structured content as csv or tsv doesn\'t matter of the file extension of the resource.\nIMPORTFEED works with marked up content as ATOM/RSS\nIMPORTHTML works with marked up content as HTML that includes properly markedup list or tables.\nIMPORTXML works with marked up content as XML or any of its variants like XHTML.\n\n\nThe content doesn\'t exceeds the maximum size. Google haven\'t disclosed this limit but the below error will be shown when the content exceeds the maximum size:\n\nResource at url contents exceeded maximum size.\n\n\nGoogle servers are not blocked by means of robots.txt or the user agent.\n\nOn W3C Markup Validator there are several tools to checkout is the resources had been properly marked up.\nRegarding CSV check out Are there known services to validate CSV files\nIt\'s worth to note that the spreadsheet\n\nshould have enough room for the imported content; Google Sheets has a 10 million cell limit by spreadsheet, according to this post a columns limit of 18278, and a 50 thousand characters as cell content even as a value or formula.\nit doesn\'t handle well large in-cell content; the ""limit"" depends on the user screen size and resolution as now it\'s possible to zoom in/out.\n\n\nReferences\n\nhttps://developers.google.com/web/tools/chrome-devtools/javascript/disable\nhttps://en.wikipedia.org/wiki/Web_scraping\n\nRelated\n\nUsing Google Apps Script to scrape Dynamic Web Pages\nScraping data from website using vba\nBlock Website Scraping by Google Docs\nIs there a way to get a single response from a text/event-stream without using event listeners?\n\nSoftware Recommendations\n\nWeb scraping tool/software available for free?\nRecommendations for web scraping tools that require minimal installation\n\nWeb Applications\nThe following question is about a different result, #N/A Could not fetch URL\n\nInability to use IMPORTHTML in Google sheets\n\n\nSimilar questions\nSome of this questions might be closed as duplicate of this one\n\nImporting javascript table into Google Docs spreadsheet\nImportxml Imported Content Empty\nscrape table using google app scripts\n\nOne answer includes Google Apps Script code using the URL Fetch Service\n\n\nCapture element using ImportXML with XPath\nHow to import Javascript tables into Google spreadsheet?\nScrape the current share price data from the ASX\n\nOne of the answers includes Google Apps Script code to get data from a JSON source\n\n\nGuidance on webscraping using Google Sheets\nHow to Scrape data from Indiegogo.com in google sheets via IMPORTXML formula\nWhy importxml and importhtml not working here?\nGoogle Sheet use Importxml error could not fetch url\n\n\n\nOne answer includes Google Apps Script code using the URL Fetch Service\n\n\n\n\nGoogle Sheets - Pull Data for investment portfolio\nExtracting value from API/Webpage\nIMPORTXML shows an error while scraping data from website\n\nOne answer shows the xhr request found using browser developer tools\n\n\nReplacing =ImportHTML with URLFetchApp\n\nOne answer includes Google Apps Script code using the URL Fetch Service\n\n\nHow to use IMPORTXML to import hidden div tag?\nGoogle Sheet Web-scraping ImportXml Xpath on Yahoo Finance doesn\'t works with french stock\n\nOne of the answers includes Google Apps Script code to get data from a JSON source. As of January 4th 2023, it\'s not longer working, very likely because Yahoo! Finance is now encrying the JSON. See the Tainake\'s answer to How to pull Yahoo Finance Historical Price Data from its Object with Google Apps Script? for script using Crypto.js to handle this.\n\n\nHow to fetch data which is loaded by the ajax (asynchronous) method after the web page has already been loaded using apps script?\n\nOne answer suggest to read the data from the server instead of scraping from a webpage.\n\n\nUsing ImportXML to pull data\nExtracting data from web page using Cheerio Library\n\nOne answer suggest the use of an API and Google Apps Script\n\n\n\n', '\nImportXML is good for basic tasks, but it won\'t get you too far if you are serious in scraping:\n\nThe approach only works with the most basic websites (no SPAs rendered in browsers can be scraped this way. Any basic web scraping protection or connectivity issue breaks the process, and there isn\'t any control over HTTP request geo location, or number of retries) - and Yahoo Finance is not a simple website\n\nIf the target website data requires some cleanup post-processing, it\'s getting very complicated since you are now ""programming with Excel formulas"", rather a painful process compared to regular code writing in conventional programming languages\n\nThere isn\'t any proper launch and cache control, so the function can be triggered occasionally and if the HTTP request fails, cells will be populated with ERR! values\n\n\nI recommend using proper tools (automation framework and scraping engine which can render JavaScript-powered websites) and use Google Sheets just for basic storage purposes:\nhttps://youtu.be/uBC752CWTew (Pipedream for automation and ScrapeNinja engine for scraping)\n']",https://stackoverflow.com/questions/74237688/scraping-data-to-google-sheets-from-a-website-that-uses-javascript,web-scraping
retrieve links from web page using python and BeautifulSoup [closed],"






Closed. This question needs details or clarity. It is not currently accepting answers.
                        
                    










Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



How can I retrieve the links of a webpage and copy the url address of the links using Python?
",319k,"
            178
        ","[""\nHere's a short snippet using the SoupStrainer class in BeautifulSoup:\nimport httplib2\nfrom bs4 import BeautifulSoup, SoupStrainer\n\nhttp = httplib2.Http()\nstatus, response = http.request('http://www.nytimes.com')\n\nfor link in BeautifulSoup(response, parse_only=SoupStrainer('a')):\n    if link.has_attr('href'):\n        print(link['href'])\n\nThe BeautifulSoup documentation is actually quite good, and covers a number of typical scenarios:\nhttps://www.crummy.com/software/BeautifulSoup/bs4/doc/\nEdit: Note that I used the SoupStrainer class because it's a bit more efficient (memory and speed wise), if you know what you're parsing in advance.\n"", '\nFor completeness sake, the BeautifulSoup 4 version, making use of the encoding supplied by the server as well:\nfrom bs4 import BeautifulSoup\nimport urllib.request\n\nparser = \'html.parser\'  # or \'lxml\' (preferred) or \'html5lib\', if installed\nresp = urllib.request.urlopen(""http://www.gpsbasecamp.com/national-parks"")\nsoup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param(\'charset\'))\n\nfor link in soup.find_all(\'a\', href=True):\n    print(link[\'href\'])\n\nor the Python 2 version:\nfrom bs4 import BeautifulSoup\nimport urllib2\n\nparser = \'html.parser\'  # or \'lxml\' (preferred) or \'html5lib\', if installed\nresp = urllib2.urlopen(""http://www.gpsbasecamp.com/national-parks"")\nsoup = BeautifulSoup(resp, parser, from_encoding=resp.info().getparam(\'charset\'))\n\nfor link in soup.find_all(\'a\', href=True):\n    print link[\'href\']\n\nand a version using the requests library, which as written will work in both Python 2 and 3:\nfrom bs4 import BeautifulSoup\nfrom bs4.dammit import EncodingDetector\nimport requests\n\nparser = \'html.parser\'  # or \'lxml\' (preferred) or \'html5lib\', if installed\nresp = requests.get(""http://www.gpsbasecamp.com/national-parks"")\nhttp_encoding = resp.encoding if \'charset\' in resp.headers.get(\'content-type\', \'\').lower() else None\nhtml_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\nencoding = html_encoding or http_encoding\nsoup = BeautifulSoup(resp.content, parser, from_encoding=encoding)\n\nfor link in soup.find_all(\'a\', href=True):\n    print(link[\'href\'])\n\nThe soup.find_all(\'a\', href=True) call finds all <a> elements that have an href attribute; elements without the attribute are skipped.\nBeautifulSoup 3 stopped development in March 2012; new projects really should use BeautifulSoup 4, always.\nNote that you should leave decoding the HTML from bytes to BeautifulSoup. You can inform BeautifulSoup of the characterset found in the HTTP response headers to assist in decoding, but this can be wrong and conflicting with a <meta> header info found in the HTML itself, which is why the above uses the BeautifulSoup internal class method EncodingDetector.find_declared_encoding() to make sure that such embedded encoding hints win over a misconfigured server.\nWith requests, the response.encoding attribute defaults to Latin-1 if the response has a text/* mimetype, even if no characterset was returned. This is consistent with the HTTP RFCs but painful when used with HTML parsing, so you should ignore that attribute when no charset is set in the Content-Type header.\n', '\nOthers have recommended BeautifulSoup, but it\'s much better to use lxml. Despite its name, it is also for parsing and scraping HTML. It\'s much, much faster than BeautifulSoup, and it even handles ""broken"" HTML better than BeautifulSoup (their claim to fame). It has a compatibility API for BeautifulSoup too if you don\'t want to learn the lxml API.\nIan Blicking agrees.\nThere\'s no reason to use BeautifulSoup anymore, unless you\'re on Google App Engine or something where anything not purely Python isn\'t allowed.\nlxml.html also supports CSS3 selectors so this sort of thing is trivial.\nAn example with lxml and xpath would look like this:\nimport urllib\nimport lxml.html\nconnection = urllib.urlopen(\'http://www.nytimes.com\')\n\ndom =  lxml.html.fromstring(connection.read())\n\nfor link in dom.xpath(\'//a/@href\'): # select the url in href for all a tags(links)\n    print link\n\n', '\nimport urllib2\nimport BeautifulSoup\n\nrequest = urllib2.Request(""http://www.gpsbasecamp.com/national-parks"")\nresponse = urllib2.urlopen(request)\nsoup = BeautifulSoup.BeautifulSoup(response)\nfor a in soup.findAll(\'a\'):\n  if \'national-park\' in a[\'href\']:\n    print \'found a url with national-park in the link\'\n\n', '\nThe following code is to retrieve all the links available in a webpage using urllib2 and BeautifulSoup4:\nimport urllib2\nfrom bs4 import BeautifulSoup\n\nurl = urllib2.urlopen(""http://www.espncricinfo.com/"").read()\nsoup = BeautifulSoup(url)\n\nfor line in soup.find_all(\'a\'):\n    print(line.get(\'href\'))\n\n', '\nLinks can be within a variety of attributes so you could pass a list of those attributes to select.\nFor example, with src and href attributes (here I am using the starts with ^ operator to specify that either of these attributes values starts with http):\nfrom bs4 import BeautifulSoup as bs\nimport requests\nr = requests.get(\'https://stackoverflow.com/\')\nsoup = bs(r.content, \'lxml\')\nlinks = [item[\'href\'] if item.get(\'href\') is not None else item[\'src\'] for item in soup.select(\'[href^=""http""], [src^=""http""]\') ]\nprint(links)\n\nAttribute = value selectors\n\n[attr^=value]\nRepresents elements with an attribute name of attr whose value is prefixed (preceded) by value.\n\nThere are also the commonly used $ (ends with) and * (contains) operators. For a full syntax list see the link above.\n', '\nUnder the hood BeautifulSoup now uses lxml. Requests, lxml & list comprehensions makes a killer combo.\nimport requests\nimport lxml.html\n\ndom = lxml.html.fromstring(requests.get(\'http://www.nytimes.com\').content)\n\n[x for x in dom.xpath(\'//a/@href\') if \'//\' in x and \'nytimes.com\' not in x]\n\nIn the list comp, the ""if \'//\' and \'url.com\' not in x"" is a simple method to scrub the url list of the sites \'internal\' navigation urls, etc.\n', '\njust for getting the links, without B.soup and regex:\nimport urllib2\nurl=""http://www.somewhere.com""\npage=urllib2.urlopen(url)\ndata=page.read().split(""</a>"")\ntag=""<a href=\\""""\nendtag=""\\"">""\nfor item in data:\n    if ""<a href"" in item:\n        try:\n            ind = item.index(tag)\n            item=item[ind+len(tag):]\n            end=item.index(endtag)\n        except: pass\n        else:\n            print item[:end]\n\nfor more complex operations, of course BSoup is still preferred.\n', ""\nThis script does what your looking for, But also resolves the relative links to absolute links.\nimport urllib\nimport lxml.html\nimport urlparse\n\ndef get_dom(url):\n    connection = urllib.urlopen(url)\n    return lxml.html.fromstring(connection.read())\n\ndef get_links(url):\n    return resolve_links((link for link in get_dom(url).xpath('//a/@href')))\n\ndef guess_root(links):\n    for link in links:\n        if link.startswith('http'):\n            parsed_link = urlparse.urlparse(link)\n            scheme = parsed_link.scheme + '://'\n            netloc = parsed_link.netloc\n            return scheme + netloc\n\ndef resolve_links(links):\n    root = guess_root(links)\n    for link in links:\n        if not link.startswith('http'):\n            link = urlparse.urljoin(root, link)\n        yield link  \n\nfor link in get_links('http://www.google.com'):\n    print link\n\n"", '\nTo find all the links, we will in this example use the urllib2 module together\nwith the re.module\n*One of the most powerful function in the re module is ""re.findall()"".\nWhile re.search() is used to find the first match for a pattern, re.findall() finds all\nthe matches and returns them as a list of strings, with each string representing one match*\nimport urllib2\n\nimport re\n#connect to a URL\nwebsite = urllib2.urlopen(url)\n\n#read html code\nhtml = website.read()\n\n#use re.findall to get all the links\nlinks = re.findall(\'""((http|ftp)s?://.*?)""\', html)\n\nprint links\n\n', '\nWhy not use regular expressions:\nimport urllib2\nimport re\nurl = ""http://www.somewhere.com""\npage = urllib2.urlopen(url)\npage = page.read()\nlinks = re.findall(r""<a.*?\\s*href=\\""(.*?)\\"".*?>(.*?)</a>"", page)\nfor link in links:\n    print(\'href: %s, HTML text: %s\' % (link[0], link[1]))\n\n', ""\nHere's an example using @ars accepted answer and the BeautifulSoup4, requests, and wget modules to handle the downloads.\nimport requests\nimport wget\nimport os\n\nfrom bs4 import BeautifulSoup, SoupStrainer\n\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/eeg_full/'\nfile_type = '.tar.gz'\n\nresponse = requests.get(url)\n\nfor link in BeautifulSoup(response.content, 'html.parser', parse_only=SoupStrainer('a')):\n    if link.has_attr('href'):\n        if file_type in link['href']:\n            full_path = url + link['href']\n            wget.download(full_path)\n\n"", ""\nI found the answer by @Blairg23 working , after the following correction (covering the scenario where it failed to work correctly):\nfor link in BeautifulSoup(response.content, 'html.parser', parse_only=SoupStrainer('a')):\n    if link.has_attr('href'):\n        if file_type in link['href']:\n            full_path =urlparse.urljoin(url , link['href']) #module urlparse need to be imported\n            wget.download(full_path)\n\nFor Python 3:\nurllib.parse.urljoin has to be used in order to obtain the full URL instead.\n"", '\nBeatifulSoup\'s own parser can be slow. It might be more feasible to use lxml which is capable of parsing directly from a URL (with some limitations mentioned below).\nimport lxml.html\n\ndoc = lxml.html.parse(url)\n\nlinks = doc.xpath(\'//a[@href]\')\n\nfor link in links:\n    print link.attrib[\'href\']\n\nThe code above will return the links as is, and in most cases they would be relative links or absolute from the site root. Since my use case was to only extract a certain type of links, below is a version that converts the links to full URLs and which optionally accepts a glob pattern like *.mp3. It won\'t handle single and double dots in the relative paths though, but so far I didn\'t have the need for it. If you need to parse URL fragments containing ../ or ./ then urlparse.urljoin might come in handy.\nNOTE: Direct lxml url parsing doesn\'t handle loading from https and doesn\'t do redirects, so for this reason the version below is using urllib2 + lxml.\n#!/usr/bin/env python\nimport sys\nimport urllib2\nimport urlparse\nimport lxml.html\nimport fnmatch\n\ntry:\n    import urltools as urltools\nexcept ImportError:\n    sys.stderr.write(\'To normalize URLs run: `pip install urltools --user`\')\n    urltools = None\n\n\ndef get_host(url):\n    p = urlparse.urlparse(url)\n    return ""{}://{}"".format(p.scheme, p.netloc)\n\n\nif __name__ == \'__main__\':\n    url = sys.argv[1]\n    host = get_host(url)\n    glob_patt = len(sys.argv) > 2 and sys.argv[2] or \'*\'\n\n    doc = lxml.html.parse(urllib2.urlopen(url))\n    links = doc.xpath(\'//a[@href]\')\n\n    for link in links:\n        href = link.attrib[\'href\']\n\n        if fnmatch.fnmatch(href, glob_patt):\n\n            if not href.startswith((\'http://\', \'https://\' \'ftp://\')):\n\n                if href.startswith(\'/\'):\n                    href = host + href\n                else:\n                    parent_url = url.rsplit(\'/\', 1)[0]\n                    href = urlparse.urljoin(parent_url, href)\n\n                    if urltools:\n                        href = urltools.normalize(href)\n\n            print href\n\nThe usage is as follows:\ngetlinks.py http://stackoverflow.com/a/37758066/191246\ngetlinks.py http://stackoverflow.com/a/37758066/191246 ""*users*""\ngetlinks.py http://fakedomain.mu/somepage.html ""*.mp3""\n\n', '\nThere can be many duplicate links together with both external and internal links.  To differentiate between the two and just get unique links using sets:\n# Python 3.\nimport urllib    \nfrom bs4 import BeautifulSoup\n\nurl = ""http://www.espncricinfo.com/""\nresp = urllib.request.urlopen(url)\n# Get server encoding per recommendation of Martijn Pieters.\nsoup = BeautifulSoup(resp, from_encoding=resp.info().get_param(\'charset\'))  \nexternal_links = set()\ninternal_links = set()\nfor line in soup.find_all(\'a\'):\n    link = line.get(\'href\')\n    if not link:\n        continue\n    if link.startswith(\'http\'):\n        external_links.add(link)\n    else:\n        internal_links.add(link)\n\n# Depending on usage, full internal links may be preferred.\nfull_internal_links = {\n    urllib.parse.urljoin(url, internal_link) \n    for internal_link in internal_links\n}\n\n# Print all unique external and full internal links.\nfor link in external_links.union(full_internal_links):\n    print(link)\n\n', '\nimport urllib2\nfrom bs4 import BeautifulSoup\na=urllib2.urlopen(\'http://dir.yahoo.com\')\ncode=a.read()\nsoup=BeautifulSoup(code)\nlinks=soup.findAll(""a"")\n#To get href part alone\nprint links[0].attrs[\'href\']\n\n']",https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup,web-scraping
Difference between text and innerHTML using Selenium,"
What’s the difference between getting text and innerHTML when using Selenium?
Even though we have text under a particular element, when we perform .text we get empty values. But doing .get_attribute(""innerHTML"") works fine.
What is the difference between two? When should someone use '.get_attribute(""innerHTML"")' over .text?
",16k,"
            14
        ","['\nTo start with, text is a property where as innerHTML is an attribute. Fundamentally there are some differences between a property and an attribute.\n\nget_attribute(""innerHTML"")\nget_attribute(innerHTML) gets the innerHTML of the element.\nThis method will first try to return the value of a property with the given name. If a property with that name doesn’t exist, it returns the value of the attribute with the same name. If there’s no attribute with that name, None is returned.\nValues which are considered truthy, that is equals true or false, are returned as booleans. All other non-None values are returned as strings. For attributes or properties which do not exist, None is returned.\n\nArguments:\ninnerHTML - Name of the attribute/property to retrieve.\n\n\nExample:\n# Extract the text of an element.\nmy_text = target_element.get_attribute(""innerHTML"")\n\n\n\n\ntext\ntext gets the text of the element.\n\nDefinition:\ndef text(self):\n    """"""The text of the element.""""""\n    return self._execute(Command.GET_ELEMENT_TEXT)[\'value\']\n\n\nExample:\n# Extract the text of an element.\nmy_text = target_element.text\n\n\n\nDoes it still sound similar? Read below...\n\nAttributes and properties\nWhen the browser loads the page, it parses the HTML and generates DOM objects from it. For element nodes, most standard HTML attributes automatically become properties of DOM objects.\nFor instance, if the tag is:\n<body id=""page"">\n\nthen the DOM object has body.id=""page"".\n\nNote: The attribute-property mapping is not one-to-one!\n\n\nHTML attributes\nIn HTML, tags may have attributes. When the browser parses the HTML to create DOM objects for tags, it recognizes standard attributes and creates DOM properties from them.\nSo when an element has id or another standard attribute, the corresponding property gets created. But that doesn’t happen if the attribute is non-standard.\n\nNote: A standard attribute for one element can be unknown for another one. For instance, type is standard attribute for <input> tag, but not for <body> tag. Standard attributes are described in the specification for the corresponding element class.\n\nSo, if an attribute is non-standard, there won’t be a DOM-property for it. In that case all attributes are accessible by using the following methods:\n\nelem.hasAttribute(name): checks for existence.\nelem.getAttribute(name): gets the value.\nelem.setAttribute(name, value): sets the value.\nelem.removeAttribute(name): removes the attribute.\n\nAn example of reading a non-standard property:\n<body something=""non-standard"">\n  <script>\n    alert(document.body.getAttribute(\'something\')); // non-standard\n  </script>\n</body>\n\n\nProperty-attribute synchronization\nWhen a standard attribute changes, the corresponding property is auto-updated, and (with some exceptions) vice versa. But there are exclusions, for instance input.value synchronizes only from attribute -> to property, but not back. This feature actually comes in handy, because the user may modify value, and then after it, if we want to recover the ""original"" value from HTML, it’s in the attribute.\n\nAs per Attributes and Properties in Python when we reference an attribute of an object with something like someObject.someAttr, Python uses several special methods to get the someAttr attribute of the object. In the simplest case, attributes are simply instance variables.\nPython Attributes\nIn a broader perspective:\n\nAn attribute is a name that appears after an object name. This is the syntactic construct. For example, someObj.name.\nAn instance variable is an item in the internal __dict__ of an object.\nThe default semantics of an attribute reference is to provide access to the instance variable. When we mention someObj.name, the default behavior is effectively someObj.__dict__[\'name\']\n\nPython Properties\nIn Python we can bind getter, setter (and deleter) functions with an attribute name, using the built-in property() function or @property decorator. When we do this, each reference to an attribute has the syntax of direct access to an instance variable, but it invokes the given method function.\n', '\n.text will retrieve an empty string of the text in not present in the view port, so you can scroll the object into the viewport and try .text. It should retrieve the value.\nOn the contrary, innerhtml can get the value, even if it is present outside the view port.\n', '\nFor instance, <div><span>Example Text</span></div>.\n.get_attribute(""innerHTML"") gives you the actual HTML inside the current element. So theDivElement.get_attribute(""innerHTML"") returns ""<span>Example Text</span>"".\n.text gives you only text, not including the HTML node. So theDivElement.text returns ""Example Text"".\nPlease note that the algorithm for .text depends on webdriver of each browser. In some cases, such as element is hidden, you might get different text when you use a different webdriver.\nI usually get text from .get_attribute(""innerText"") instead of .text, so I can handle the all the cases.\n', '\nChrome (I\'m not sure about other browsers) ignores the extra spaces within the HTML code and displays them as a single space.\n<div><span>Example  Text</span></div> <!-- Notice the two spaces -->\n\n.get_attribute(\'innerHTML\') will return the double-spaced text, which is what you would see when you inspect element), while .text will return the string with only 1 space.\n>>> print(element.get_attribute(\'innerHTML\'))\n\'Example  Text\'\n>>> print(element.text)\n\'Example Text\'\n\nThis difference is not trivial as the following will result in a NoSuchElementException.\n>>> arg = \'//div[contains(text(),""Example Text"")]\'\n>>> driver.find_element_by_xpath(arg)\n\nSimilarly, .get_attribute(\'innerHTML\') for the following returns Example&nbsp;Text, while .text returns Example Text.\n<div><span>Example&nbsp;Text</span></div>\n\n', '\nI have just selected the CSS selector and used the below code:\nfrom selenium import webdriver\n\ndriver = webdriver.Chrome()\ndriver.maximize_window()\ndriver.get(""http://www.costco.com/Weatherproof%C2%AE-Men\'s-Ultra-Tech-Jacket.product.100106552.html"")\nprint driver.find_element_by_css_selector("".product-h1-container.visible-xl-block>h1"").text\n\nand it prints:\nWeatherproof® Men\'s Ultra Tech Jacket\n\nThe problem is h1[itemprop=\'name\'] selector on Google Chrome or Chrome are returning two matching nodes while .product-h1-container.visible-xl-block>h1 is returning only one matching node. That’s why it\'s printing what is expected.\nTo prove my point, run the below code:\nfrom selenium import webdriver\n\ndriver = webdriver.Chrome()\ndriver.maximize_window()\ndriver.get(""http://www.costco.com/Weatherproof%C2%AE-Men\'s-Ultra-Tech-Jacket.product.100106552.html"")\nx= driver.find_elements_by_css_selector(""h1[itemprop=\'name\'] "")\n\nfor i in x:\n    print ""This is line "" , i.text\n\nIt will print\nThis is line\nThis is line  Weatherproof® Men\'s Ultra Tech Jacket\n\nBecause select_element_by_css_selector selects the first element with matching selector and that does not contain any text so it does not print. Hope you understand now\n']",https://stackoverflow.com/questions/40416048/difference-between-text-and-innerhtml-using-selenium,web-scraping
How can I pass variable into an evaluate function?,"
I'm trying to pass a variable into a page.evaluate() function in Puppeteer, but when I use the following very simplified example, the variable evalVar is undefined.
I can't find any examples to build on, so I need help passing that variable into the page.evaluate() function so I can use it inside.
const puppeteer = require('puppeteer');

(async() => {

  const browser = await puppeteer.launch({headless: false});
  const page = await browser.newPage();

  const evalVar = 'WHUT??';

  try {

    await page.goto('https://www.google.com.au');
    await page.waitForSelector('#fbar');
    const links = await page.evaluate((evalVar) => {

      console.log('evalVar:', evalVar); // appears undefined

      const urls = [];
      hrefs = document.querySelectorAll('#fbar #fsl a');
      hrefs.forEach(function(el) {
        urls.push(el.href);
      });
      return urls;
    })
    console.log('links:', links);

  } catch (err) {

    console.log('ERR:', err.message);

  } finally {

    // browser.close();

  }

})();

",134k,"
            248
        ","['\nYou have to pass the variable as an argument to the pageFunction like this:\nconst links = await page.evaluate((evalVar) => {\n\n  console.log(evalVar); // 2. should be defined now\n  …\n\n}, evalVar); // 1. pass variable as an argument\n\nYou can pass in multiple variables by passing more arguments to  page.evaluate():\nawait page.evaluate((a, b c) => { console.log(a, b, c) }, a, b, c)\n\nThe arguments must either be serializable as JSON or JSHandles of in-browser objects: https://pptr.dev/#?show=api-pageevaluatepagefunction-args\n', ""\nI encourage you to stick on this style, because it's more convenient and readable.\nlet name = 'jack';\nlet age  = 33;\nlet location = 'Berlin/Germany';\n\nawait page.evaluate(({name, age, location}) => {\n\n    console.log(name);\n    console.log(age);\n    console.log(location);\n\n},{name, age, location});\n\n"", '\nSingle Variable:\nYou can pass one variable to page.evaluate() using the following syntax:\nawait page.evaluate(example => { /* ... */ }, example);\n\n\nNote: You do not need to enclose the variable in (), unless you are going to be passing multiple variables.\n\nMultiple Variables:\nYou can pass multiple variables to page.evaluate() using the following syntax:\nawait page.evaluate((example_1, example_2) => { /* ... */ }, example_1, example_2);\n\n\nNote: Enclosing your variables within {} is not necessary.\n\n', ""\nIt took me quite a while to figure out that console.log() in evaluate() can't show in node console. \nRef: https://github.com/GoogleChrome/puppeteer/issues/1944\n\neverything that is run inside the page.evaluate function is done in the context of the browser page. The script is running in the browser not in node.js so if you log it will show in the browsers console which if you are running headless you will not see. You also can't set a node breakpoint inside the function.\n\nHope this can help.\n"", ""\nFor pass a function, there are two ways you can do it.\n// 1. Defined in evaluationContext\nawait page.evaluate(() => {\n  window.yourFunc = function() {...};\n});\nconst links = await page.evaluate(() => {\n  const func = window.yourFunc;\n  func();\n});\n\n\n// 2. Transform function to serializable(string). (Function can not be serialized)\nconst yourFunc = function() {...};\nconst obj = {\n  func: yourFunc.toString()\n};\nconst otherObj = {\n  foo: 'bar'\n};\nconst links = await page.evaluate((obj, aObj) => {\n   const funStr = obj.func;\n   const func = new Function(`return ${funStr}.apply(null, arguments)`)\n   func();\n\n   const foo = aObj.foo; // bar, for object\n   window.foo = foo;\n   debugger;\n}, obj, otherObj);\n\nYou can add devtools: true to the launch options for test\n"", '\nI have a typescript example that could help someone new in typescript.\nconst hyperlinks: string [] = await page.evaluate((url: string, regex: RegExp, querySelect: string) => {\n.........\n}, url, regex, querySelect);\n\n', ""\nSlightly different version from @wolf answer above. Make code much more reusable between different context.\n// util functions\nexport const pipe = (...fns) => initialVal => fns.reduce((acc, fn) => fn(acc), initialVal)\nexport const pluck = key => obj => obj[key] || null\nexport const map = fn => item => fn(item)\n// these variables will be cast to string, look below at fn.toString()\n\nconst updatedAt = await page.evaluate(\n  ([selector, util]) => {\n    let { pipe, map, pluck } = util\n    pipe = new Function(`return ${pipe}`)()\n    map = new Function(`return ${map}`)()\n    pluck = new Function(`return ${pluck}`)()\n\n    return pipe(\n      s => document.querySelector(s),\n      pluck('textContent'),\n      map(text => text.trim()),\n      map(date => Date.parse(date)),\n      map(timeStamp => Promise.resolve(timeStamp))\n    )(selector)\n  },\n  [\n    '#table-announcements tbody td:nth-child(2) .d-none',\n    { pipe: pipe.toString(), map: map.toString(), pluck: pluck.toString() },\n  ]\n)\n\nAlso not that functions inside pipe cant used something like this\n// incorrect, which is i don't know why\npipe(document.querySelector) \n\n// should be \npipe(s => document.querySelector(s))\n\n""]",https://stackoverflow.com/questions/46088351/how-can-i-pass-variable-into-an-evaluate-function,web-scraping
How to find elements by class,"
I'm having trouble parsing HTML elements with ""class"" attribute using Beautifulsoup. The code looks like this
soup = BeautifulSoup(sdata)
mydivs = soup.findAll('div')
for div in mydivs: 
    if (div[""class""] == ""stylelistrow""):
        print div

I get an error on the same line ""after"" the script finishes. 
File ""./beautifulcoding.py"", line 130, in getlanguage
  if (div[""class""] == ""stylelistrow""):
File ""/usr/local/lib/python2.6/dist-packages/BeautifulSoup.py"", line 599, in __getitem__
   return self._getAttrMap()[key]
KeyError: 'class'

How do I get rid of this error?
",1.0m,"
            609
        ","['\nYou can refine your search to only find those divs with a given class using BS3:\nmydivs = soup.find_all(""div"", {""class"": ""stylelistrow""})\n\n', '\nFrom the documentation:\nAs of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument class_:\nsoup.find_all(""a"", class_=""sister"")\n\nWhich in this case would be:\nsoup.find_all(""div"", class_=""stylelistrow"")\n\nIt would also work for:\nsoup.find_all(""div"", class_=""stylelistrowone stylelistrowtwo"")\n\n', '\nUpdate: 2016\nIn the latest version of beautifulsoup, the method \'findAll\' has been renamed to \n\'find_all\'. Link to official documentation\n\nHence the answer will be \nsoup.find_all(""html_element"", class_=""your_class_name"")\n\n', '\nCSS selectors\nsingle class first match\nsoup.select_one(\'.stylelistrow\')\n\nlist of matches\nsoup.select(\'.stylelistrow\')\n\ncompound class (i.e. AND another class)\nsoup.select_one(\'.stylelistrow.otherclassname\')\nsoup.select(\'.stylelistrow.otherclassname\')\n\nSpaces in compound class names e.g. class = stylelistrow otherclassname are replaced with ""."". You can continue to add classes.\nlist of classes (OR - match whichever present)\nsoup.select_one(\'.stylelistrow, .otherclassname\')\nsoup.select(\'.stylelistrow, .otherclassname\')\n\nClass attribute whose values contains a string e.g. with ""stylelistrow"":\nstarts with ""style"":\n[class^=style]\n\nends with ""row""\n[class$=row]\n\ncontains ""list"":\n[class*=list]\n\nThe ^, $ and * are operators. Read more here: https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors\nIf you wanted to exclude this class then, with anchor tag as an example, selecting anchor tags without this class:\na:not(.stylelistrow)\n\nYou can pass simple, compound and complex css selectors lists inside of :not() pseudo class. See https://facelessuser.github.io/soupsieve/selectors/pseudo-classes/#:not\n\nbs4 4.7.1 +\nSpecific class whose innerText contains a string\nsoup.select_one(\'.stylelistrow:contains(""some string"")\')\nsoup.select(\'.stylelistrow:contains(""some string"")\')\n\nN.B.\nsoupsieve 2.1.0 + Dec\'2020 onwards\n\nNEW: In order to avoid conflicts with future CSS specification\nchanges, non-standard pseudo classes will now start with the :-soup-\nprefix. As a consequence, :contains() will now be known as\n:-soup-contains(), though for a time the deprecated form of\n:contains() will still be allowed with a warning that users should\nmigrate over to :-soup-contains().\nNEW: Added new non-standard pseudo class :-soup-contains-own() which\noperates similar to :-soup-contains() except that it only looks at\ntext nodes directly associated with the currently scoped element and\nnot its descendants.\n\nSpecific class which has a certain child element e.g. a tag\nsoup.select_one(\'.stylelistrow:has(a)\')\nsoup.select(\'.stylelistrow:has(a)\')\n\n', '\nSpecific to BeautifulSoup 3:\nsoup.findAll(\'div\',\n             {\'class\': lambda x: x \n                       and \'stylelistrow\' in x.split()\n             }\n            )\n\nWill find all of these:\n<div class=""stylelistrow"">\n<div class=""stylelistrow button"">\n<div class=""button stylelistrow"">\n\n', ""\nA straight forward way would be :\nsoup = BeautifulSoup(sdata)\nfor each_div in soup.findAll('div',{'class':'stylelist'}):\n    print each_div\n\nMake sure you take of the casing of findAll, its not findall\n"", '\n\nHow to find elements by class\nI\'m having trouble parsing html elements with ""class"" attribute using Beautifulsoup.\n\nYou can easily find by one class, but if you want to find by the intersection of two classes, it\'s a little more difficult,\nFrom the documentation (emphasis added):\n\nIf you want to search for tags that match two or more CSS classes, you should use a CSS selector:\ncss_soup.select(""p.strikeout.body"")\n# [<p class=""body strikeout""></p>]\n\n\nTo be clear, this selects only the p tags that are both strikeout and body class.\nTo find for the intersection of any in a set of classes (not the intersection, but the union), you can give a list to the class_ keyword argument (as of 4.1.2):\nsoup = BeautifulSoup(sdata)\nclass_list = [""stylelistrow""] # can add any other classes to this list.\n# will find any divs with any names in class_list:\nmydivs = soup.find_all(\'div\', class_=class_list) \n\nAlso note that findAll has been renamed from the camelCase to the more Pythonic find_all.\n', ""\nUse class_= If you want to find element(s) without stating the HTML tag.\nFor single element:\nsoup.find(class_='my-class-name')\n\nFor multiple elements:\nsoup.find_all(class_='my-class-name')\n\n"", ""\nAs of BeautifulSoup 4+ ,\nIf you have a single class name , you can just pass the class name as parameter like :\nmydivs = soup.find_all('div', 'class_name')\n\nOr if you have more than one class names , just pass the list of class names as parameter like :\nmydivs = soup.find_all('div', ['class1', 'class2'])\n\n"", '\nthe following worked for me\na_tag = soup.find_all(""div"",class_=\'full tabpublist\')\n\n', ""\nThis works for me to access the class attribute (on beautifulsoup 4, contrary to what the documentation says). The KeyError comes a list being returned not a dictionary.\nfor hit in soup.findAll(name='span'):\n    print hit.contents[1]['class']\n\n"", '\nOther answers did not work for me.\nIn other answers the findAll is being used on the soup object itself, but I needed a way to do a find by class name on objects inside a specific element extracted from the object I obtained after doing findAll.\nIf you are trying to do a search inside nested HTML elements to get objects by class name, try below -\n# parse html\npage_soup = soup(web_page.read(), ""html.parser"")\n\n# filter out items matching class name\nall_songs = page_soup.findAll(""li"", ""song_item"")\n\n# traverse through all_songs\nfor song in all_songs:\n\n    # get text out of span element matching class \'song_name\'\n    # doing a \'find\' by class name within a specific song element taken out of \'all_songs\' collection\n    song.find(""span"", ""song_name"").text\n\nPoints to note:\n\nI\'m not explicitly defining the search to be on \'class\' attribute findAll(""li"", {""class"": ""song_item""}), since it\'s the only attribute I\'m searching on and it will by default search for class attribute if you don\'t exclusively tell which attribute you want to find on. \nWhen you do a findAll or find, the resulting object is of class bs4.element.ResultSet which is a subclass of list. You can utilize all methods of ResultSet, inside any number of nested elements (as long as they are of type ResultSet) to do a find or find all.\nMy BS4 version - 4.9.1, Python version - 3.8.1\n\n', '\nConcerning @Wernight\'s comment on the top answer about partial matching...\nYou can partially match:\n\n<div class=""stylelistrow""> and\n<div class=""stylelistrow button"">\n\nwith gazpacho:\nfrom gazpacho import Soup\n\nmy_divs = soup.find(""div"", {""class"": ""stylelistrow""}, partial=True)\n\nBoth will be captured and returned as a list of Soup objects.\n', '\nAlternatively we can use lxml, it support xpath and very fast!\nfrom lxml import html, etree \n\nattr = html.fromstring(html_text)#passing the raw html\nhandles = attr.xpath(\'//div[@class=""stylelistrow""]\')#xpath exresssion to find that specific class\n\nfor each in handles:\n    print(etree.tostring(each))#printing the html as string\n\n', '\nsingle\nsoup.find(""form"",{""class"":""c-login__form""})\n\nmultiple\nres=soup.find_all(""input"")\nfor each in res:\n    print(each)\n\n', '\nTry to check if the div has a class attribute first, like this:\nsoup = BeautifulSoup(sdata)\nmydivs = soup.findAll(\'div\')\nfor div in mydivs:\n    if ""class"" in div:\n        if (div[""class""]==""stylelistrow""):\n            print div\n\n', '\nThis worked for me:\nfor div in mydivs:\n    try:\n        clazz = div[""class""]\n    except KeyError:\n        clazz = """"\n    if (clazz == ""stylelistrow""):\n        print div\n\n', '\nThis should work:\nsoup = BeautifulSoup(sdata)\nmydivs = soup.findAll(\'div\')\nfor div in mydivs: \n    if (div.find(class_ == ""stylelistrow""):\n        print div\n\n', ""\nThe following should work\nsoup.find('span', attrs={'class':'totalcount'})\n\nreplace 'totalcount' with your class name and 'span' with tag you are looking for. Also, if your class contains multiple names with space, just choose one and use.\nP.S. This finds the first element with given criteria. If you want to find all elements then replace 'find' with 'find_all'.\n""]",https://stackoverflow.com/questions/5041008/how-to-find-elements-by-class,web-scraping
How can I efficiently parse HTML with Java?,"
I do a lot of HTML parsing in my line of work. Up until now, I was using the HtmlUnit headless browser for parsing and browser automation.
Now, I want to separate both the tasks.
I want to use a light HTML parser because it takes much time in HtmlUnit to first load a page, then get the source and then parse it.
I want to know which HTML parser can parse HTML efficiently. I need

Speed
Ease to locate any HtmlElement by its ""id"" or ""name"" or ""tag type"".

It would be ok for me if it doesn't clean the dirty HTML code. I don't need to clean any HTML source. I just need an easiest way to move across HtmlElements and harvest data from them.
",205k,"
            206
        ","['\nSelf plug: I have just released a new Java HTML parser: jsoup. I mention it here because I think it will do what you are after.\nIts party trick is a CSS selector syntax to find elements, e.g.:\nString html = ""<html><head><title>First parse</title></head>""\n  + ""<body><p>Parsed HTML into a doc.</p></body></html>"";\nDocument doc = Jsoup.parse(html);\nElements links = doc.select(""a"");\nElement head = doc.select(""head"").first();\n\nSee the Selector javadoc for more info.\nThis is a new project, so any ideas for improvement are very welcome!\n', ""\nThe best I've seen so far is HtmlCleaner:\n\nHtmlCleaner is open-source HTML parser written in Java. HTML found on Web is usually dirty, ill-formed and unsuitable for further processing. For any serious consumption of such documents, it is necessary to first clean up the mess and bring the order to tags, attributes and ordinary text. For the given HTML document, HtmlCleaner reorders individual elements and produces well-formed XML. By default, it follows similar rules that the most of web browsers use in order to create Document Object Model. However, user may provide custom tag and rule set for tag filtering and balancing.\n\nWith HtmlCleaner you can locate any element using XPath.\nFor other html parsers see this SO question.\n"", ""\nI suggest Validator.nu's parser, based on the HTML5 parsing algorithm. It is the parser used in Mozilla from 2010-05-03\n""]",https://stackoverflow.com/questions/2168610/how-can-i-efficiently-parse-html-with-java,web-scraping
selenium with scrapy for dynamic page,"
I'm trying to scrape product information from a webpage, using scrapy. My to-be-scraped webpage looks like this:

starts with a product_list page with 10 products
a click on ""next""  button loads the next 10 products (url doesn't change between the two pages)
i use LinkExtractor to follow each product link into the product page, and get all the information I need

I tried to replicate the next-button-ajax-call but can't get working, so I'm giving selenium a try. I can run selenium's webdriver in a separate script, but I don't know how to integrate with scrapy. Where shall I put the selenium part in my scrapy spider? 
My spider is pretty standard, like the following:
class ProductSpider(CrawlSpider):
    name = ""product_spider""
    allowed_domains = ['example.com']
    start_urls = ['http://example.com/shanghai']
    rules = [
        Rule(SgmlLinkExtractor(restrict_xpaths='//div[@id=""productList""]//dl[@class=""t2""]//dt'), callback='parse_product'),
        ]

    def parse_product(self, response):
        self.log(""parsing product %s"" %response.url, level=INFO)
        hxs = HtmlXPathSelector(response)
        # actual data follows

Any idea is appreciated. Thank you!
",109k,"
            100
        ","['\nIt really depends on how do you need to scrape the site and how and what data do you want to get. \nHere\'s an example how you can follow pagination on ebay using Scrapy+Selenium:\nimport scrapy\nfrom selenium import webdriver\n\nclass ProductSpider(scrapy.Spider):\n    name = ""product_spider""\n    allowed_domains = [\'ebay.com\']\n    start_urls = [\'http://www.ebay.com/sch/i.html?_odkw=books&_osacat=0&_trksid=p2045573.m570.l1313.TR0.TRC0.Xpython&_nkw=python&_sacat=0&_from=R40\']\n\n    def __init__(self):\n        self.driver = webdriver.Firefox()\n\n    def parse(self, response):\n        self.driver.get(response.url)\n\n        while True:\n            next = self.driver.find_element_by_xpath(\'//td[@class=""pagn-next""]/a\')\n\n            try:\n                next.click()\n\n                # get the data and write it to scrapy items\n            except:\n                break\n\n        self.driver.close()\n\nHere are some examples of ""selenium spiders"":\n\nExecuting Javascript Submit form functions using scrapy in python\nhttps://gist.github.com/cheekybastard/4944914\nhttps://gist.github.com/irfani/1045108\nhttp://snipplr.com/view/66998/\n\n\nThere is also an alternative to having to use Selenium with Scrapy. In some cases, using ScrapyJS middleware is enough to handle the dynamic parts of a page. Sample real-world usage:\n\nScraping dynamic content using python-Scrapy\n\n', '\nIf (url doesn\'t change between the two pages) then you should add dont_filter=True with your scrapy.Request() or scrapy will find this url as a duplicate after processing first page. \nIf you need to render pages with javascript you should use scrapy-splash, you can also check this scrapy middleware which can handle javascript pages using selenium or you can do that by launching any headless browser\nBut more effective and faster solution is inspect your browser and see what requests are made during submitting a form or triggering a certain event. Try to simulate the same requests as your browser sends. If you can replicate the request(s) correctly you will get the data you need.\nHere is an example :\nclass ScrollScraper(Spider):\n    name = ""scrollingscraper""\n\n    quote_url = ""http://quotes.toscrape.com/api/quotes?page=""\n    start_urls = [quote_url + ""1""]\n\n    def parse(self, response):\n        quote_item = QuoteItem()\n        print response.body\n        data = json.loads(response.body)\n        for item in data.get(\'quotes\', []):\n            quote_item[""author""] = item.get(\'author\', {}).get(\'name\')\n            quote_item[\'quote\'] = item.get(\'text\')\n            quote_item[\'tags\'] = item.get(\'tags\')\n            yield quote_item\n\n        if data[\'has_next\']:\n            next_page = data[\'page\'] + 1\n            yield Request(self.quote_url + str(next_page))\n\nWhen pagination url is same for every pages & uses POST request then you can use scrapy.FormRequest() instead of scrapy.Request(), both are same but FormRequest adds a new argument (formdata=) to the constructor. \nHere is another spider example form this post:\nclass SpiderClass(scrapy.Spider):\n    # spider name and all\n    name = \'ajax\'\n    page_incr = 1\n    start_urls = [\'http://www.pcguia.pt/category/reviews/#paginated=1\']\n    pagination_url = \'http://www.pcguia.pt/wp-content/themes/flavor/functions/ajax.php\'\n\n    def parse(self, response):\n\n        sel = Selector(response)\n\n        if self.page_incr > 1:\n            json_data = json.loads(response.body)\n            sel = Selector(text=json_data.get(\'content\', \'\'))\n\n        # your code here\n\n        # pagination code starts here\n        if sel.xpath(\'//div[@class=""panel-wrapper""]\'):\n            self.page_incr += 1\n            formdata = {\n                \'sorter\': \'recent\',\n                \'location\': \'main loop\',\n                \'loop\': \'main loop\',\n                \'action\': \'sort\',\n                \'view\': \'grid\',\n                \'columns\': \'3\',\n                \'paginated\': str(self.page_incr),\n                \'currentquery[category_name]\': \'reviews\'\n            }\n            yield FormRequest(url=self.pagination_url, formdata=formdata, callback=self.parse)\n        else:\n            return\n\n']",https://stackoverflow.com/questions/17975471/selenium-with-scrapy-for-dynamic-page,web-scraping
How to scrape only visible webpage text with BeautifulSoup?,"
Basically, I want to use BeautifulSoup to grab strictly the visible text on a webpage. For instance, this webpage is my test case. And I mainly want to just get the body text (article) and maybe even a few tab names here and there. I have tried the suggestion in this SO question that returns lots of <script> tags and html comments which I don't want. I can't figure out the arguments I need for the function findAll() in order to just get the visible texts on a webpage.
So, how should I find all visible text excluding scripts, comments, css etc.?
",156k,"
            145
        ","['\nTry this:\nfrom bs4 import BeautifulSoup\nfrom bs4.element import Comment\nimport urllib.request\n\n\ndef tag_visible(element):\n    if element.parent.name in [\'style\', \'script\', \'head\', \'title\', \'meta\', \'[document]\']:\n        return False\n    if isinstance(element, Comment):\n        return False\n    return True\n\n\ndef text_from_html(body):\n    soup = BeautifulSoup(body, \'html.parser\')\n    texts = soup.findAll(text=True)\n    visible_texts = filter(tag_visible, texts)  \n    return u"" "".join(t.strip() for t in visible_texts)\n\nhtml = urllib.request.urlopen(\'http://www.nytimes.com/2009/12/21/us/21storm.html\').read()\nprint(text_from_html(html))\n\n', ""\nThe approved answer from @jbochi does not work for me.  The str() function call raises an exception because it cannot encode the non-ascii characters in the BeautifulSoup element.  Here is a more succinct way to filter the example web page to visible text.\nhtml = open('21storm.html').read()\nsoup = BeautifulSoup(html)\n[s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\nvisible_text = soup.getText()\n\n"", '\nimport urllib\nfrom bs4 import BeautifulSoup\n\nurl = ""https://www.yahoo.com""\nhtml = urllib.urlopen(url).read()\nsoup = BeautifulSoup(html)\n\n# kill all script and style elements\nfor script in soup([""script"", ""style""]):\n    script.extract()    # rip it out\n\n# get text\ntext = soup.get_text()\n\n# break into lines and remove leading and trailing space on each\nlines = (line.strip() for line in text.splitlines())\n# break multi-headlines into a line each\nchunks = (phrase.strip() for line in lines for phrase in line.split(""  ""))\n# drop blank lines\ntext = \'\\n\'.join(chunk for chunk in chunks if chunk)\n\nprint(text.encode(\'utf-8\'))\n\n', '\nI completely respect using Beautiful Soup to get rendered content, but it may not be the ideal package for acquiring the rendered content on a page.\nI had a similar problem to get rendered content, or the visible content in a typical browser.  In particular I had many perhaps atypical cases to work with such a simple example below.  In this case the non displayable tag is nested in a style tag, and is not visible in many browsers that I have checked.  Other variations exist such as defining a class tag setting display to none.  Then using this class for the div. \n<html>\n  <title>  Title here</title>\n\n  <body>\n\n    lots of text here <p> <br>\n    <h1> even headings </h1>\n\n    <style type=""text/css""> \n        <div > this will not be visible </div> \n    </style>\n\n\n  </body>\n\n</html>\n\nOne solution posted above is: \nhtml = Utilities.ReadFile(\'simple.html\')\nsoup = BeautifulSoup.BeautifulSoup(html)\ntexts = soup.findAll(text=True)\nvisible_texts = filter(visible, texts)\nprint(visible_texts)\n\n\n[u\'\\n\', u\'\\n\', u\'\\n\\n        lots of text here \', u\' \', u\'\\n\', u\' even headings \', u\'\\n\', u\' this will not be visible \', u\'\\n\', u\'\\n\']\n\nThis solution certainly has applications in many cases and does the job quite well generally but in the html posted above it retains the text that is not rendered.  After searching SO a couple solutions came up here BeautifulSoup get_text does not strip all tags and JavaScript  and here Rendered HTML to plain text using Python\nI tried both these solutions: html2text and nltk.clean_html and was surprised by the timing results so thought they warranted an answer for posterity.  Of course, the speeds highly depend on the contents of the data...\nOne answer here from @Helge was about using nltk of all things.  \nimport nltk\n\n%timeit nltk.clean_html(html)\nwas returning 153 us per loop\n\nIt worked really well to return a string with rendered html.  This nltk module was faster than even html2text, though perhaps html2text is more robust. \nbetterHTML = html.decode(errors=\'ignore\')\n%timeit html2text.html2text(betterHTML)\n%3.09 ms per loop\n\n', ""\nUsing BeautifulSoup the easiest way with less code to just get the strings, without empty lines and crap.\ntag = <Parent_Tag_that_contains_the_data>\nsoup = BeautifulSoup(tag, 'html.parser')\n\nfor i in soup.stripped_strings:\n    print repr(i)\n\n"", '\nIf you care about performance, here\'s another more efficient way:\nimport re\n\nINVISIBLE_ELEMS = (\'style\', \'script\', \'head\', \'title\')\nRE_SPACES = re.compile(r\'\\s{3,}\')\n\ndef visible_texts(soup):\n    """""" get visible text from a document """"""\n    text = \' \'.join([\n        s for s in soup.strings\n        if s.parent.name not in INVISIBLE_ELEMS\n    ])\n    # collapse multiple spaces to two spaces.\n    return RE_SPACES.sub(\'  \', text)\n\nsoup.strings is an iterator, and it returns NavigableString so that you can check the parent\'s tag name directly, without going through multiple loops.\n', '\nWhile, i would completely suggest using beautiful-soup in general, if anyone is looking to display the visible parts of a malformed html (e.g. where you have just a segment or line of a web-page) for whatever-reason, the the following will remove content between < and > tags:\nimport re   ## only use with malformed html - this is not efficient\ndef display_visible_html_using_re(text):             \n    return(re.sub(""(\\<.*?\\>)"", """",text))\n\n', '\nThe title is inside an <nyt_headline> tag, which is nested inside an <h1> tag and a <div> tag with id ""article"".  \nsoup.findAll(\'nyt_headline\', limit=1)\n\nShould work.\nThe article body is inside an <nyt_text> tag, which is nested inside a <div> tag with id ""articleBody"".  Inside the <nyt_text>  element, the text itself is contained within <p>  tags.  Images are not within those <p> tags.  It\'s difficult for me to experiment with the syntax, but I expect a working scrape to look something like this.\ntext = soup.findAll(\'nyt_text\', limit=1)[0]\ntext.findAll(\'p\')\n\n', '\nfrom bs4 import BeautifulSoup\nfrom bs4.element import Comment\nimport urllib.request\nimport re\nimport ssl\n\ndef tag_visible(element):\n    if element.parent.name in [\'style\', \'script\', \'head\', \'title\', \'meta\', \'[document]\']:\n        return False\n    if isinstance(element, Comment):\n        return False\n    if re.match(r""[\\n]+"",str(element)): return False\n    return True\ndef text_from_html(url):\n    body = urllib.request.urlopen(url,context=ssl._create_unverified_context()).read()\n    soup = BeautifulSoup(body ,""lxml"")\n    texts = soup.findAll(text=True)\n    visible_texts = filter(tag_visible, texts)  \n    text = u"","".join(t.strip() for t in visible_texts)\n    text = text.lstrip().rstrip()\n    text = text.split(\',\')\n    clean_text = \'\'\n    for sen in text:\n        if sen:\n            sen = sen.rstrip().lstrip()\n            clean_text += sen+\',\'\n    return clean_text\nurl = \'http://www.nytimes.com/2009/12/21/us/21storm.html\'\nprint(text_from_html(url))\n\n', '\nThe simplest way to handle this case is by using getattr().  You can adapt this example to your needs:\nfrom bs4 import BeautifulSoup\n\nsource_html = """"""\n<span class=""ratingsDisplay"">\n    <a class=""ratingNumber"" href=""https://www.youtube.com/watch?v=oHg5SJYRHA0"" target=""_blank"" rel=""noopener"">\n        <span class=""ratingsContent"">3.7</span>\n    </a>\n</span>\n""""""\n\nsoup = BeautifulSoup(source_html, ""lxml"")\nmy_ratings = getattr(soup.find(\'span\', {""class"": ""ratingsContent""}), ""text"", None)\nprint(my_ratings)\n\nThis will find the text element,""3.7"", within the tag object <span class=""ratingsContent"">3.7</span> when it exists, however, default to NoneType when it does not.\n\ngetattr(object, name[, default])\nReturn the value of the named attribute of object. name must be a string. If the string is the name of one of the object’s attributes, the result is the value of that attribute. For example, getattr(x, \'foobar\') is equivalent to x.foobar. If the named attribute does not exist, default is returned if provided, otherwise, AttributeError is raised.\n\n', ""\nUPDATE\nFrom the docs: As of Beautiful Soup version 4.9.0, when lxml or html.parser are in use, the contents of <script>, <style>, and <template> tags are generally not considered to be ‘text’, since those tags are not part of the human-visible content of the page.\nTo get all the human readable text of the HTML <body> you can use .get_text(), to get rid of redundant whitespaces, etc. set strip parameter and join/separate all by a single whitespace:\nimport bs4, requests\n\nresponse = requests.get('https://www.nytimes.com/interactive/2022/09/13/us/politics/congress-stock-trading-investigation.html',headers={'User-Agent': 'Mozilla/5.0','cache-control': 'max-age=0'}, cookies={'cookies':''})\nsoup = bs4.BeautifulSoup(response.text)\n\nsoup.article.get_text(' ', strip=True)\n\nIn newer code avoid old syntax findAll() instead use find_all() or select() with css selectors - For more take a minute to check docs\n""]",https://stackoverflow.com/questions/1936466/how-to-scrape-only-visible-webpage-text-with-beautifulsoup,web-scraping
Scraping html tables into R data frames using the XML package,"
How do I scrape html tables using the XML package?
Take, for example, this wikipedia page on the Brazilian soccer team. I would like to read it in R and get the ""list of all matches Brazil have played against FIFA recognised teams"" table as a data.frame. How can I do this?
",127k,"
            161
        ","['\n…or a shorter try:\nlibrary(XML)\nlibrary(RCurl)\nlibrary(rlist)\ntheurl <- getURL(""https://en.wikipedia.org/wiki/Brazil_national_football_team"",.opts = list(ssl.verifypeer = FALSE) )\ntables <- readHTMLTable(theurl)\ntables <- list.clean(tables, fun = is.null, recursive = FALSE)\nn.rows <- unlist(lapply(tables, function(t) dim(t)[1]))\n\nthe picked table is the longest one on the page\ntables[[which.max(n.rows)]]\n\n', '\nlibrary(RCurl)\nlibrary(XML)\n\n# Download page using RCurl\n# You may need to set proxy details, etc.,  in the call to getURL\ntheurl <- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""\nwebpage <- getURL(theurl)\n# Process escape characters\nwebpage <- readLines(tc <- textConnection(webpage)); close(tc)\n\n# Parse the html tree, ignoring errors on the page\npagetree <- htmlTreeParse(webpage, error=function(...){})\n\n# Navigate your way through the tree. It may be possible to do this more efficiently using getNodeSet\nbody <- pagetree$children$html$children$body \ndivbodyContent <- body$children$div$children[[1]]$children$div$children[[4]]\ntables <- divbodyContent$children[names(divbodyContent)==""table""]\n\n#In this case, the required table is the only one with class ""wikitable sortable""  \ntableclasses <- sapply(tables, function(x) x$attributes[""class""])\nthetable  <- tables[which(tableclasses==""wikitable sortable"")]$table\n\n#Get columns headers\nheaders <- thetable$children[[1]]$children\ncolumnnames <- unname(sapply(headers, function(x) x$children$text$value))\n\n# Get rows from table\ncontent <- c()\nfor(i in 2:length(thetable$children))\n{\n   tablerow <- thetable$children[[i]]$children\n   opponent <- tablerow[[1]]$children[[2]]$children$text$value\n   others <- unname(sapply(tablerow[-1], function(x) x$children$text$value)) \n   content <- rbind(content, c(opponent, others))\n}\n\n# Convert to data frame\ncolnames(content) <- columnnames\nas.data.frame(content)\n\nEdited to add:\nSample output\n                     Opponent Played Won Drawn Lost Goals for Goals against \xa0% Won\n    1               Argentina     94  36    24   34       148           150  38.3%\n    2                Paraguay     72  44    17   11       160            61  61.1%\n    3                 Uruguay     72  33    19   20       127            93  45.8%\n    ...\n\n', '\nThe rvest along with xml2 is another popular package for parsing html web pages.\nlibrary(rvest)\ntheurl <- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""\nfile<-read_html(theurl)\ntables<-html_nodes(file, ""table"")\ntable1 <- html_table(tables[4], fill = TRUE)\n\nThe syntax is easier to use than the xml package and for most web pages the package provides all of the options ones needs.\n', '\nAnother option using Xpath.\nlibrary(RCurl)\nlibrary(XML)\n\ntheurl <- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""\nwebpage <- getURL(theurl)\nwebpage <- readLines(tc <- textConnection(webpage)); close(tc)\n\npagetree <- htmlTreeParse(webpage, error=function(...){}, useInternalNodes = TRUE)\n\n# Extract table header and contents\ntablehead <- xpathSApply(pagetree, ""//*/table[@class=\'wikitable sortable\']/tr/th"", xmlValue)\nresults <- xpathSApply(pagetree, ""//*/table[@class=\'wikitable sortable\']/tr/td"", xmlValue)\n\n# Convert character vector to dataframe\ncontent <- as.data.frame(matrix(results, ncol = 8, byrow = TRUE))\n\n# Clean up the results\ncontent[,1] <- gsub(""Â\xa0"", """", content[,1])\ntablehead <- gsub(""Â\xa0"", """", tablehead)\nnames(content) <- tablehead\n\nProduces this result\n> head(content)\n   Opponent Played Won Drawn Lost Goals for Goals against % Won\n1 Argentina     94  36    24   34       148           150 38.3%\n2  Paraguay     72  44    17   11       160            61 61.1%\n3   Uruguay     72  33    19   20       127            93 45.8%\n4     Chile     64  45    12    7       147            53 70.3%\n5      Peru     39  27     9    3        83            27 69.2%\n6    Mexico     36  21     6    9        69            34 58.3%\n\n']",https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package,web-scraping
How to use Python requests to fake a browser visit a.k.a and generate User Agent?,"
I want to get the content from this website.
If I use a browser like Firefox or Chrome I could get the real website page I want, but if I use the Python requests package (or wget command) to get it, it returns a totally different HTML page.
I thought the developer of the website had made some blocks for this.
Question
How do I fake a browser visit by using python requests or command wget?
",321k,"
            179
        ","[""\nProvide a User-Agent header:\nimport requests\n\nurl = 'http://www.ichangtou.com/#company:data_000008.html'\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n\nresponse = requests.get(url, headers=headers)\nprint(response.content)\n\nFYI, here is a list of User-Agent strings for different browsers:\n\nList of all Browsers\n\n\nAs a side note, there is a pretty useful third-party package called fake-useragent that provides a nice abstraction layer over user agents:\n\nfake-useragent\nUp to date simple useragent faker with real world database\n\nDemo:\n>>> from fake_useragent import UserAgent\n>>> ua = UserAgent()\n>>> ua.chrome\nu'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36'\n>>> ua.random\nu'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36'\n\n"", '\nI used fake UserAgent.\nHow to use:\nfrom fake_useragent import UserAgent\nimport requests\n   \n\nua = UserAgent()\nprint(ua.chrome)\nheader = {\'User-Agent\':str(ua.chrome)}\nprint(header)\nurl = ""https://www.hybrid-analysis.com/recent-submissions?filter=file&sort=^timestamp""\nhtmlContent = requests.get(url, headers=header)\nprint(htmlContent)\n\nOutput:\nMozilla/5.0 (Macintosh; Intel Mac OS X 10_8_2) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1309.0 Safari/537.17\n{\'User-Agent\': \'Mozilla/5.0 (X11; OpenBSD i386) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\'}\n<Response [200]>\n\n', '\nTry doing this, using firefox as fake user agent (moreover, it\'s a good startup script for web scraping with the use of cookies):\n#!/usr/bin/env python2\n# -*- coding: utf8 -*-\n# vim:ts=4:sw=4\n\n\nimport cookielib, urllib2, sys\n\ndef doIt(uri):\n    cj = cookielib.CookieJar()\n    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n    page = opener.open(uri)\n    page.addheaders = [(\'User-agent\', \'Mozilla/5.0\')]\n    print page.read()\n\nfor i in sys.argv[1:]:\n    doIt(i)\n\nUSAGE:\npython script.py ""http://www.ichangtou.com/#company:data_000008.html""\n\n', '\nThe root of the answer is that the person asking the question needs to have a JavaScript interpreter to get what they are after. What I have found is I am able to get all of the information I wanted on a website in json before it was interpreted by JavaScript. This has saved me a ton of time in what would be parsing html hoping each webpage is in the same format.\nSo when you get a response from a website using requests really look at the html/text because you might find the javascripts JSON in the footer ready to be parsed. \n', '\nI use pyuser_agent. this package use get user agnet\nimport pyuser_agent\nimport requests\n\nua = pyuser_agent.UA()\n\nheaders = {\n      ""User-Agent"" : ua.random\n}\nprint(headers)\n\nuri = ""https://github.com/THAVASIGTI/""\nres = requests.request(""GET"",uri,headers=headers)\nprint(res)\n\nconsole out\n{\'User-Agent\': \'Mozilla/5.0 (Windows; U; Windows NT 6.1; zh-CN) AppleWebKit/533+ (KHTML, like Gecko)\'}\n<Response [200]>\n\n', ""\nAnswer\nYou need to create a header with a proper formatted User agent String, it server to communicate client-server.\nYou can check your own user agent Here.\nExample\nMozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0\nMozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0\n\nThird party Package user_agent 0.1.9 \nI found this module very simple to use, in one line of code it randomly generates a User agent string.\nfrom user_agent import generate_user_agent, generate_navigator\nfrom pprint import pprint\n\nprint(generate_user_agent())\n# 'Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.3; Win64; x64)'\n\nprint(generate_user_agent(os=('mac', 'linux')))\n# 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:36.0) Gecko/20100101 Firefox/36.0'\n\npprint(generate_navigator())\n\n# {'app_code_name': 'Mozilla',\n#  'app_name': 'Netscape',\n#  'appversion': '5.0',\n#  'name': 'firefox',\n#  'os': 'linux',\n#  'oscpu': 'Linux i686 on x86_64',\n#  'platform': 'Linux i686 on x86_64',\n#  'user_agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686 on x86_64; rv:41.0) Gecko/20100101 Firefox/41.0',\n#  'version': '41.0'}\n\npprint(generate_navigator_js())\n\n# {'appCodeName': 'Mozilla',\n#  'appName': 'Netscape',\n#  'appVersion': '38.0',\n#  'platform': 'MacIntel',\n#  'userAgent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:38.0) Gecko/20100101 Firefox/38.0'}\n\n"", ""\nUser agent is ok but he wants to fetch a JavaScript site.we can use selenium but it is annoying to setup and maintain so the best way to fetch a JavaScript rendered page is requests_html module. Which is a superset of the well known request module. To install use pip\npip install requests-html\n\nAnd to fetch a JavaScript rendered page use\nfrom requests_html import HTMLSession\nsession = HTMLSession()\nr = session.get('https://python.org/')\n\nHope it will help. It uses puppter to render javascript and also it downloads chromium but you don't have to worry everything is happening under the hood.you will get the end result.\n"", ""\nI had a similar issue but I was unable to use the UserAgent class inside the fake_useragent module. I was running the code inside a docker container\nimport requests\nimport ujson\nimport random\n\nresponse = requests.get('https://fake-useragent.herokuapp.com/browsers/0.1.11')\nagents_dictionary = ujson.loads(response.text)\nrandom_browser_number = str(random.randint(0, len(agents_dictionary['randomize'])))\nrandom_browser = agents_dictionary['randomize'][random_browser_number]\nuser_agents_list = agents_dictionary['browsers'][random_browser]\nuser_agent = user_agents_list[random.randint(0, len(user_agents_list)-1)]\n\nI targeted the endpoint used in the module. This solution still gave me a random user agent however there is the possibility that the data structure at the endpoint could change.\n"", '\nThis is how, I have been using a random user agent from a list of nearlly 1000 fake user agents\nfrom random_user_agent.user_agent import UserAgent\nfrom random_user_agent.params import SoftwareName, OperatingSystem\nsoftware_names = [SoftwareName.ANDROID.value]\noperating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value, OperatingSystem.MAC.value]   \n\nuser_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=1000)\n\n# Get list of user agents.\nuser_agents = user_agent_rotator.get_user_agents()\n\nuser_agent_random = user_agent_rotator.get_random_user_agent()\n\nExample\nprint(user_agent_random)\n\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\nFor more details visit this link\n']",https://stackoverflow.com/questions/27652543/how-to-use-python-requests-to-fake-a-browser-visit-a-k-a-and-generate-user-agent,web-scraping
Is it ok to scrape data from Google results? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 5 years ago.







                        Improve this question
                    



I'd like to fetch results from Google using curl to detect potential duplicate content.
Is there a high risk of being banned by Google?
",135k,"
            77
        ","['\nGoogle disallows automated access in their TOS, so if you accept their terms you would break them.\nThat said, I know of no lawsuit from Google against a scraper.\nEven Microsoft scraped Google, they powered their search engine Bing with it. They got caught in 2011 red handed :)\nThere are two options to scrape Google results:\n1) Use their API\n\nUPDATE 2020: Google has reprecated previous APIs (again) and has new\nprices and new limits. Now\n(https://developers.google.com/custom-search/v1/overview) you can\nquery up to 10k results per day at 1,500 USD per month, more than that\nis not permitted and the results are not what they display in normal\nsearches.\n\n\nYou can issue around 40 requests per hour You are limited to what\nthey give you, it\'s not really useful if you want to track ranking\npositions or what a real user would see. That\'s something you are not\nallowed to gather.\n\nIf you want a higher amount of API requests you need to pay.\n\n60 requests per hour cost 2000 USD per year, more queries require a\ncustom deal.\n\n\n2) Scrape the normal result pages\n\nHere comes the tricky part. It is possible to scrape the normal result pages.\nGoogle does not allow it.\nIf you scrape at a rate higher than 8 (updated from 15) keyword requests per hour you risk detection, higher than 10/h (updated from 20) will get you blocked from my experience.\nBy using multiple IPs you can up the rate, so with 100 IP addresses you can scrape up to 1000 requests per hour. (24k a day) (updated)\nThere is an open source search engine scraper written in PHP at http://scraping.compunect.com\nIt allows to reliable scrape Google, parses the results properly and manages IP addresses, delays, etc.\nSo if you can use PHP it\'s a nice kickstart, otherwise the code will still be useful to learn how it is done.\n\n3) Alternatively use a scraping service (updated)\n\nRecently a customer of mine had a huge search engine scraping requirement but it was not \'ongoing\', it\'s more like one huge refresh per month.\nIn this case I could not find a self-made solution that\'s \'economic\'.\nI used the service at http://scraping.services instead.\nThey also provide open source code and so far it\'s running well (several thousand resultpages per hour during the refreshes)\nThe downside is that such a service means that your solution is ""bound"" to one professional supplier, the upside is that it was a lot cheaper than the other options I evaluated (and faster in our case)\nOne option to reduce the dependency on one company is to make two approaches at the same time. Using the scraping service as primary source of data and falling back to a proxy based solution like described at 2) when required.\n\n', '\nGoogle will eventually block your IP when you exceed a certain amount of requests. \n', '\nGoogle thrives on scraping websites of the world...so if it was ""so illegal"" then even Google won\'t survive ..of course other answers mention ways of mitigating IP blocks by Google. One more way to explore avoiding captcha could be scraping at random times (dint try) ..Moreover, I have a feeling, that if we provide novelty or some significant processing of data then it sounds fine at least to me...if we are simply copying a website.. or hampering its business/brand in some way...then it is bad and should be avoided..on top of it all...if you are a startup then no one will fight you as there is no benefit.. but if your entire premise is on scraping even when you are funded then you should think of more sophisticated ways...alternative APIs..eventually..Also Google keeps releasing (or depricating)  fields for its API so what you want to scrap now may be in roadmap of new Google API releases..\n']",https://stackoverflow.com/questions/22657548/is-it-ok-to-scrape-data-from-google-results,web-scraping
can we use XPath with BeautifulSoup?,"
I am using BeautifulSoup to scrape an URL and I had the following code, to find the td tag whose class is 'empformbody':
import urllib
import urllib2
from BeautifulSoup import BeautifulSoup

url =  ""http://www.example.com/servlet/av/ResultTemplate=AVResult.html""
req = urllib2.Request(url)
response = urllib2.urlopen(req)
the_page = response.read()
soup = BeautifulSoup(the_page)

soup.findAll('td',attrs={'class':'empformbody'})

Now in the above code we can use findAll to get tags and information related to them, but I want to use XPath. Is it possible to use XPath with BeautifulSoup? If possible, please provide me example code.
",277k,"
            153
        ","['\nNope, BeautifulSoup, by itself, does not support XPath expressions.\nAn alternative library, lxml, does support XPath 1.0. It has a BeautifulSoup compatible mode where it\'ll try and parse broken HTML the way Soup does. However, the default lxml HTML parser does just as good a job of parsing broken HTML, and I believe is faster.\nOnce you\'ve parsed your document into an lxml tree, you can use the .xpath() method to search for elements.\ntry:\n    # Python 2\n    from urllib2 import urlopen\nexcept ImportError:\n    from urllib.request import urlopen\nfrom lxml import etree\n\nurl =  ""http://www.example.com/servlet/av/ResultTemplate=AVResult.html""\nresponse = urlopen(url)\nhtmlparser = etree.HTMLParser()\ntree = etree.parse(response, htmlparser)\ntree.xpath(xpathselector)\n\nThere is also a dedicated lxml.html() module with additional functionality.\nNote that in the above example I passed the response object directly to lxml, as having the parser read directly from the stream is more efficient than reading the response into a large string first. To do the same with the requests library, you want to set stream=True and pass in the response.raw object after enabling transparent transport decompression:\nimport lxml.html\nimport requests\n\nurl =  ""http://www.example.com/servlet/av/ResultTemplate=AVResult.html""\nresponse = requests.get(url, stream=True)\nresponse.raw.decode_content = True\ntree = lxml.html.parse(response.raw)\n\nOf possible interest to you is the CSS Selector support; the CSSSelector class translates CSS statements into XPath expressions, making your search for td.empformbody that much easier:\nfrom lxml.cssselect import CSSSelector\n\ntd_empformbody = CSSSelector(\'td.empformbody\')\nfor elem in td_empformbody(tree):\n    # Do something with these table cells.\n\nComing full circle: BeautifulSoup itself does have very complete CSS selector support:\nfor cell in soup.select(\'table#foobar td.empformbody\'):\n    # Do something with these table cells.\n\n', '\nI can confirm that there is no XPath support within Beautiful Soup.\n', '\nAs others have said, BeautifulSoup doesn\'t have xpath support.  There are probably a number of ways to get something from an xpath, including using Selenium.  However, here\'s a solution that works in either Python 2 or 3:\nfrom lxml import html\nimport requests\n\npage = requests.get(\'http://econpy.pythonanywhere.com/ex/001.html\')\ntree = html.fromstring(page.content)\n#This will create a list of buyers:\nbuyers = tree.xpath(\'//div[@title=""buyer-name""]/text()\')\n#This will create a list of prices\nprices = tree.xpath(\'//span[@class=""item-price""]/text()\')\n\nprint(\'Buyers: \', buyers)\nprint(\'Prices: \', prices)\n\nI used this as a reference.\n', ""\nBeautifulSoup has a function named findNext from current element directed childern,so:\nfather.findNext('div',{'class':'class_value'}).findNext('div',{'id':'id_value'}).findAll('a') \n\nAbove code can imitate the following xpath:\ndiv[class=class_value]/div[id=id_value]\n\n"", '\nfrom lxml import etree\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(open(\'path of your localfile.html\'),\'html.parser\')\ndom = etree.HTML(str(soup))\nprint dom.xpath(\'//*[@id=""BGINP01_S1""]/section/div/font/text()\')\n\nAbove used the combination of Soup object with lxml and one can extract the value using xpath\n', '\nwhen you use lxml all simple:\ntree = lxml.html.fromstring(html)\ni_need_element = tree.xpath(\'//a[@class=""shared-components""]/@href\')\n\nbut when use BeautifulSoup BS4 all simple too:\n\nfirst remove ""//"" and ""@""  \nsecond - add star before ""=""\n\ntry this magic:\nsoup = BeautifulSoup(html, ""lxml"")\ni_need_element = soup.select (\'a[class*=""shared-components""]\')\n\nas you see, this does not support sub-tag, so i remove ""/@href"" part\n', ""\nI've searched through their docs and it seems there is no XPath option.\nAlso, as you can see here on a similar question on SO, the OP is asking for a translation from XPath to BeautifulSoup, so my conclusion would be - no, there is no XPath parsing available.\n"", '\nMaybe you can try the following without XPath\nfrom simplified_scrapy.simplified_doc import SimplifiedDoc \nhtml = \'\'\'\n<html>\n<body>\n<div>\n    <h1>Example Domain</h1>\n    <p>This domain is for use in illustrative examples in documents. You may use this\n    domain in literature without prior coordination or asking for permission.</p>\n    <p><a href=""https://www.iana.org/domains/example"">More information...</a></p>\n</div>\n</body>\n</html>\n\'\'\'\n# What XPath can do, so can it\ndoc = SimplifiedDoc(html)\n# The result is the same as doc.getElementByTag(\'body\').getElementByTag(\'div\').getElementByTag(\'h1\').text\nprint (doc.body.div.h1.text)\nprint (doc.div.h1.text)\nprint (doc.h1.text) # Shorter paths will be faster\nprint (doc.div.getChildren())\nprint (doc.div.getChildren(\'p\'))\n\n', '\nThis is a pretty old thread, but there is a work-around solution now, which may not have been in BeautifulSoup at the time. \nHere is an example of what I did. I use the ""requests"" module to read an RSS feed and get its text content in a variable called ""rss_text"". With that, I run it thru BeautifulSoup, search for the xpath /rss/channel/title, and retrieve its contents. It\'s not exactly XPath in all its glory (wildcards, multiple paths, etc.), but if you just have a basic path you want to locate, this works. \nfrom bs4 import BeautifulSoup\nrss_obj = BeautifulSoup(rss_text, \'xml\')\ncls.title = rss_obj.rss.channel.title.get_text()\n\n', ""\nuse   soup.find(class_='myclass')\n""]",https://stackoverflow.com/questions/11465555/can-we-use-xpath-with-beautifulsoup,web-scraping
Using python Requests with javascript pages,"
I am trying to use the Requests framework with python (http://docs.python-requests.org/en/latest/) but the page I am trying to get to uses javascript to fetch the info that I want. 
I have tried to search on the web for a solution but the fact that I am searching with the keyword javascript most of the stuff I am getting is how to scrape with the javascript language.
Is there anyway to use the requests framework with pages that use javascript?
",169k,"
            85
        ","[""\nGood news: there is now a requests module that supports javascript:  https://pypi.org/project/requests-html/\nfrom requests_html import HTMLSession\n\nsession = HTMLSession()\n\nr = session.get('http://www.yourjspage.com')\n\nr.html.render()  # this call executes the js in the page\n\nAs a bonus this wraps BeautifulSoup, I think, so you can do things like\nr.html.find('#myElementID').text\n\nwhich returns the content of the HTML element as you'd expect.\n"", '\nYou are going to have to make the same request (using the Requests library) that the javascript is making.  You can use any number of tools (including those built into Chrome and Firefox) to inspect the http request that is coming from javascript and simply make this request yourself from Python.\n', '\nWhile Selenium might seem tempting and useful, it has one main problem that can\'t be fixed: performance. By calculating every single thing a browser does, you will need a lot more power. Even PhantomJS does not compete with a simple request. I recommend that you will only use Selenium when you really need to click buttons. If you only need javascript, I recommend PyQt (check https://www.youtube.com/watch?v=FSH77vnOGqU to learn it).\nHowever, if you want to use Selenium, I recommend Chrome over PhantomJS. Many users have problems with PhantomJS where a website simply does not work in Phantom. Chrome can be headless (non-graphical) too!\nFirst, make sure you have installed ChromeDriver, which Selenium depends on for using Google Chrome.\nThen, make sure you have Google Chrome of version 60 or higher by checking it in the URL chrome://settings/help\nNow, all you need to do is the following code:\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium import webdriver\n\nchrome_options = Options()\nchrome_options.add_argument(""--headless"")\n\ndriver = webdriver.Chrome(chrome_options=chrome_options)\n\nIf you do not know how to use Selenium, here is a quick overview:\ndriver.get(""https://www.google.com"") #Browser goes to google.com\n\nFinding elements:\nUse either the ELEMENTS or ELEMENT method. Examples:\ndriver.find_element_by_css_selector(""div.logo-subtext"") #Find your country in Google. (singular)\n\n\ndriver.find_element(s)_by_css_selector(css_selector) # Every element that matches this CSS selector\ndriver.find_element(s)_by_class_name(class_name) # Every element with the following class\ndriver.find_element(s)_by_id(id) # Every element with the following ID\ndriver.find_element(s)_by_link_text(link_text) # Every  with the full link text\ndriver.find_element(s)_by_partial_link_text(partial_link_text) # Every  with partial link text.\ndriver.find_element(s)_by_name(name) # Every element where name=argument\ndriver.find_element(s)_by_tag_name(tag_name) # Every element with the tag name argument\n\nOk! I found an element (or elements list). But what do I do now?\nHere are the methods you can do on an element elem:\n\nelem.tag_name # Could return button in a .\nelem.get_attribute(""id"") # Returns the ID of an element.\nelem.text # The inner text of an element.\nelem.clear() # Clears a text input.\nelem.is_displayed() # True for visible elements, False for invisible elements.\nelem.is_enabled() # True for an enabled input, False otherwise.\nelem.is_selected() # Is this radio button or checkbox element selected?\nelem.location # A dictionary representing the X and Y location of an element on the screen.\nelem.click() # Click elem.\nelem.send_keys(""thelegend27"") # Type thelegend27 into elem (useful for text inputs)\nelem.submit() # Submit the form in which elem takes part.\n\nSpecial commands:\n\ndriver.back() # Click the Back button.\ndriver.forward() # Click the Forward button.\ndriver.refresh() # Refresh the page.\ndriver.quit() # Close the browser including all the tabs.\nfoo = driver.execute_script(""return \'hello\';"") # Execute javascript (COULD TAKE RETURN VALUES!)\n\n', '\nUsing Selenium or jQuery enabled requests are slow. It is more efficient to find out which cookie is generated after website checking for JavaScript on the browser and get that cookie and use it for each of your requests.\nIn one example it worked through following cookies:\nthe cookie generated after checking for javascript for this example is ""cf_clearance"".\nso  simply create a session.\nupdate cookie and headers as such:\ns = requests.Session()\ns.cookies[""cf_clearance""] = ""cb4c883efc59d0e990caf7508902591f4569e7bf-1617321078-0-150""\ns.headers.update({\n            ""user-agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) \n               AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36""\n        })\ns.get(url)\n\nand you are good to go no need for JavaScript solution such as Selenium. This is way faster and efficient. you just have to get cookie once after opening up the browser.\n', '\nSome way to do that is to invoke your request by using selenium.\nLet\'s install dependecies by using pip or pip3:\npip install selenium\netc.\nIf you run script by using python3\nuse instead:\npip3 install selenium\n(...)\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ndriver = webdriver.Chrome(ChromeDriverManager().install())\nurl = \'http://myurl.com\'\n\n# Please wait until the page will be ready:\nelement = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, ""div.some_placeholder"")))\nelement.text = \'Some text on the page :)\' # <-- Here it is! I got what I wanted :)\n\n', '\nits a wrapper around pyppeteer or smth? :( i thought its something different\n    @property\n    async def browser(self):\n        if not hasattr(self, ""_browser""):\n            self._browser = await pyppeteer.launch(ignoreHTTPSErrors=not(self.verify), headless=True, args=self.__browser_args)\n\n        return self._browser\n\n']",https://stackoverflow.com/questions/26393231/using-python-requests-with-javascript-pages,web-scraping
How does reCAPTCHA 3 know I'm using Selenium/chromedriver?,"
I'm curious how reCAPTCHA v3 works. Specifically the browser fingerprinting.
When I launch an instance of Chrome through Selenium/chromedriver and test against reCAPTCHA 3 (https://recaptcha-demo.appspot.com/recaptcha-v3-request-scores.php) I always get a score of 0.1 when using Selenium/chromedriver.
When using incognito with a normal instance, I get 0.3.
I've beaten other detection systems by injecting JavaScript and modifying the web driver object and recompiling webdriver from source and modifying the $cdc_ variables.
I can see what looks like some obfuscated POST back to the server, so I'm going to start digging there.
What might it be looking for to determine if I'm running Selenium/chromedriver?
",80k,"
            42
        ","['\nreCaptcha\nWebsites can easily detect the network traffic and identify your program as a BOT. Google have already released 5(five) reCAPTCHA to choose from when creating a new site. While four of them are active and reCAPTCHA v1 being shutdown.\n\nreCAPTCHA versions and types\n\nreCAPTCHA v3 (verify requests with a score): reCAPTCHA v3 allows you to verify if an interaction is legitimate without any user interaction. It is a pure JavaScript API returning a score, giving you the ability to take action in the context of your site: for instance requiring additional factors of authentication, sending a post to moderation, or throttling bots that may be scraping content.\nreCAPTCHA v2 - ""I\'m not a robot"" Checkbox: The ""I\'m not a robot"" Checkbox requires the user to click a checkbox indicating the user is not a robot. This will either pass the user immediately (with No CAPTCHA) or challenge them to validate whether or not they are human. This is the simplest option to integrate with and only requires two lines of HTML to render the checkbox.\n\n\n\nreCAPTCHA v2 - Invisible reCAPTCHA badge: The invisible reCAPTCHA badge does not require the user to click on a checkbox, instead it is invoked directly when the user clicks on an existing button on your site or can be invoked via a JavaScript API call. The integration requires a JavaScript callback when reCAPTCHA verification is complete. By default only the most suspicious traffic will be prompted to solve a captcha. To alter this behavior edit your site security preference under advanced settings.\n\n\n\nreCAPTCHA v2 - Android: The reCAPTCHA Android library is part of the Google Play services SafetyNet APIs. This library provides native Android APIs that you can integrate directly into an app. You should set up Google Play services in your app and connect to the GoogleApiClient before invoking the reCAPTCHA API. This will either pass the user through immediately (without a CAPTCHA prompt) or challenge them to validate whether they are human. \nreCAPTCHA v1: reCAPTCHA v1 has been shut down since March 2018.\n\n\nSolution\nHowever there are some generic approaches to avoid getting detected while web-scraping:\n\nThe first and foremost attribute a website can determine your script/program is through your monitor size. So it is recommended not to use the conventional Viewport.\nIf you need to send multiple requests to a website keep on changing the User Agent on each request. Here you can find a detailed discussion on Way to change Google Chrome user agent in Selenium?\nTo simulate human like behavior you may require to slow down the script execution even beyond WebDriverWait and expected_conditions inducing time.sleep(secs). Here you can find a detailed discussion on How to sleep webdriver in python for milliseconds\n\n\nOutro\nSome food for thought:\n\nSelenium webdriver: Modifying navigator.webdriver flag to prevent selenium detection\nUnable to use Selenium to automate Chase site login\nConfidence Score of the request using reCAPTCHA v3 API\n\n', '\nSelenium and Puppeteer have some browser configurations that is different from a non-automated browser. Also, since some JavaScript functions are injected into browser to manipulate elements, you need to create some override to avoid detections.\nThere are some good articles explaining some points about Selenium and Puppeteer detection while it runs on a site with detection mechanisms:\nDetecting Chrome headless, new techniques - You can use it to write defensive code for your bot.\nIt is not possible to detect and block Google Chrome headless - it explains in a clear and sound way the differences that JavaScript code can detect between a browser launched by automated software and a real one, and also how to fake it.\nGitHub - headless-cat-n-mouse - Example using Puppeteer + Python to avoid detection\n']",https://stackoverflow.com/questions/55501524/how-does-recaptcha-3-know-im-using-selenium-chromedriver,web-scraping
CasperJS/PhantomJS doesn't load https page,"
I know there are certain web pages PhantomJS/CasperJS can't open, and I was wondering if this one was one of them: https://maizepages.umich.edu. CasperJS gives an error: PhantomJS failed to open page status=fail.
I tried ignoring-ssl-errors and changing my user agent but I'm not sure how to determine which ones to use.
All I'm doing right now is the basic casper setup with casper.start(url, function () { ... }) where url=https://maizepages.umich.edu;
",19k,"
            24
        ","['\nThe problem may be related to the recent discovery of a SSLv3 vulnerability (POODLE). Website owners were forced to remove SSLv3 support from their websites. Since PhantomJS < v1.9.8 uses SSLv3 by default, you should use TLSv1:\ncasperjs --ssl-protocol=tlsv1 yourScript.js\n\nThe catchall solution would be to use any for when newer PhantomJS versions come along with other SSL protocols. But this would make the POODLE vulnerability exploitable on sites which haven\'t yet disabled SSLv3.\ncasperjs --ssl-protocol=any yourScript.js\n\nAlternative method: Update to PhantomJS 1.9.8 or higher. Note that updating to PhantomJS 1.9.8 leads to a new bug, which is especially annoying for CasperJS.\nHow to verify: Add a resource.error event handler like this at the beginning of your script:\ncasper.on(""resource.error"", function(resourceError){\n    console.log(\'Unable to load resource (#\' + resourceError.id + \'URL:\' + resourceError.url + \')\');\n    console.log(\'Error code: \' + resourceError.errorCode + \'. Description: \' + resourceError.errorString);\n});\n\nIf it is indeed a problem with SSLv3 the error will be something like:\n\nError code: 6. Description: SSL handshake failed\n\n\nAs an aside, you also might want to run with the --ignore-ssl-errors=true commandline option, when there is something wrong with the certificate.\n']",https://stackoverflow.com/questions/26415188/casperjs-phantomjs-doesnt-load-https-page,web-scraping
Scrape multiple urls using QWebPage,"
I'm using Qt's QWebPage to render a page that uses javascript to update its content dynamically - so a library that just downloads a static version of the page (such as urllib2) won't work.
My problem is, when I render a second page, about 99% of the time the program just crashes. At other times, it will work three times before crashing. I've also gotten a few segfaults, but it is all very random.
My guess is the object I'm using to render isn't getting deleted properly, so trying to reuse it is possibly causing some problems for myself. I've looked all over and no one really seems to be having this same issue.
Here's the code I'm using. The program downloads web pages from steam's community market so I can create a database of all the items. I need to call the getItemsFromPage function multiple times to get all of the items, as they are broken up into pages (showing results 1-10 out of X amount).
import csv
import re
import sys
from string import replace
from bs4 import BeautifulSoup
from PyQt4.QtGui import *
from PyQt4.QtCore import *
from PyQt4.QtWebKit import *

class Item:
    __slots__ = (""name"", ""count"", ""price"", ""game"")

    def __repr__(self):
        return self.name + ""("" + str(self.count) + "")""

    def __str__(self):
        return self.name + "", "" + str(self.count) + "", $"" + str(self.price)

class Render(QWebPage):  
    def __init__(self, url):
        self.app = QApplication(sys.argv)
        QWebPage.__init__(self)
        self.loadFinished.connect(self._loadFinished)
        self.mainFrame().load(QUrl(url))
        self.app.exec_()

    def _loadFinished(self, result):
        self.frame = self.mainFrame()
        self.app.quit()
        self.deleteLater()

def getItemsFromPage(appid, page=1):

    r = Render(""http://steamcommunity.com/market/search?q=appid:"" + str(appid) + ""#p"" + str(page))

    soup = BeautifulSoup(str(r.frame.toHtml().toUtf8()))

    itemLst = soup.find_all(""div"", ""market_listing_row market_recent_listing_row"")

    items = []

    for k in itemLst:
        i = Item()

        i.name = k.find(""span"", ""market_listing_item_name"").string
        i.count = int(replace(k.find(""span"", ""market_listing_num_listings_qty"").string, "","", """"))
        i.price = float(re.search(r'\$([0-9]+\.[0-9]+)', str(k)).group(1))
        i.game = appid

        items.append(i)

    return items

if __name__ == ""__main__"":

    print ""Updating market items to dota2.csv ...""

    i = 1

    with open(""dota2.csv"", ""w"") as f:
        writer = csv.writer(f)

        r = None

        while True:
            print ""Page "" + str(i)

            items = getItemsFromPage(570)

            if len(items) == 0:
                print ""No items found, stopping...""
                break

            for k in items:
                writer.writerow((k.name, k.count, k.price, k.game))

            i += 1

    print ""Done.""

Calling getItemsFromPage once works fine. Subsequent calls give me my problem. The output of the program is typically
Updating market items to dota2.csv ...
Page 1
Page 2

and then it crashes. It should go on for over 700 pages. 
",3k,"
            6
        ","[""\nThe problem with your program is that you are attempting to create a new QApplication with every url you fetch.\nInstead, only one QApplication and one WebPage should be created. The WebPage can use its loadFinished signal to create an internal loop by fetching a new url after each one has been processed. Custom html processing can be added by connecting a user-defined slot to a signal which emits the html text and the url when they become available. The scripts below (for PyQt5 and PyQt4) show how to implement this.\nHere are some examples which show how to use the WebPage class:\nUsage:\ndef my_html_processor(html, url):\n    print('loaded: [%d chars] %s' % (len(html), url))\n\nimport sys\napp = QApplication(sys.argv)\nwebpage = WebPage(verbose=False)\nwebpage.htmlReady.connect(my_html_processor)\n\n# example 1: process list of urls\n\nurls = ['https://en.wikipedia.org/wiki/Special:Random'] * 3\nprint('Processing list of urls...')\nwebpage.process(urls)\n\n# example 2: process one url continuously\n#\n# import signal, itertools\n# signal.signal(signal.SIGINT, signal.SIG_DFL)\n#\n# print('Processing url continuously...')\n# print('Press Ctrl+C to quit')\n#\n# url = 'https://en.wikipedia.org/wiki/Special:Random'\n# webpage.process(itertools.repeat(url))\n\nsys.exit(app.exec_())\n\nPyQt5 WebPage:\nfrom PyQt5.QtCore import pyqtSignal, QUrl\nfrom PyQt5.QtWidgets import QApplication\nfrom PyQt5.QtWebEngineWidgets import QWebEnginePage\n\nclass WebPage(QWebEnginePage):\n    htmlReady = pyqtSignal(str, str)\n\n    def __init__(self, verbose=False):\n        super().__init__()\n        self._verbose = verbose\n        self.loadFinished.connect(self.handleLoadFinished)\n\n    def process(self, urls):\n        self._urls = iter(urls)\n        self.fetchNext()\n\n    def fetchNext(self):\n        try:\n            url = next(self._urls)\n        except StopIteration:\n            return False\n        else:\n            self.load(QUrl(url))\n        return True\n\n    def processCurrentPage(self, html):\n        self.htmlReady.emit(html, self.url().toString())\n        if not self.fetchNext():\n            QApplication.instance().quit()\n\n    def handleLoadFinished(self):\n        self.toHtml(self.processCurrentPage)\n\n    def javaScriptConsoleMessage(self, *args, **kwargs):\n        if self._verbose:\n            super().javaScriptConsoleMessage(*args, **kwargs)\n\nPyQt4 WebPage:\nfrom PyQt4.QtCore import pyqtSignal, QUrl\nfrom PyQt4.QtGui import QApplication\nfrom PyQt4.QtWebKit import QWebPage\n\nclass WebPage(QWebPage):\n    htmlReady = pyqtSignal(str, str)\n\n    def __init__(self, verbose=False):\n        super(WebPage, self).__init__()\n        self._verbose = verbose\n        self.mainFrame().loadFinished.connect(self.handleLoadFinished)\n\n    def start(self, urls):\n        self._urls = iter(urls)\n        self.fetchNext()\n\n    def fetchNext(self):\n        try:\n            url = next(self._urls)\n        except StopIteration:\n            return False\n        else:\n            self.mainFrame().load(QUrl(url))\n        return True\n\n    def processCurrentPage(self):\n        self.htmlReady.emit(\n            self.mainFrame().toHtml(), self.mainFrame().url().toString())\n        print('loaded: [%d bytes] %s' % (self.bytesReceived(), url))\n\n    def handleLoadFinished(self):\n        self.processCurrentPage()\n        if not self.fetchNext():\n            QApplication.instance().quit()\n\n    def javaScriptConsoleMessage(self, *args, **kwargs):\n        if self._verbose:\n            super(WebPage, self).javaScriptConsoleMessage(*args, **kwargs)\n\n""]",https://stackoverflow.com/questions/21274865/scrape-multiple-urls-using-qwebpage,web-scraping
Web scraping with Python [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I'd like to grab daily sunrise/sunset times from a web site. Is it possible to scrape web content with Python? what are the modules used? Is there any tutorial available?
",214k,"
            196
        ","[""\nUse urllib2 in combination with the brilliant BeautifulSoup library:\nimport urllib2\nfrom BeautifulSoup import BeautifulSoup\n# or if you're using BeautifulSoup4:\n# from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(urllib2.urlopen('http://example.com').read())\n\nfor row in soup('table', {'class': 'spad'})[0].tbody('tr'):\n    tds = row('td')\n    print tds[0].string, tds[1].string\n    # will print date and sunrise\n\n"", ""\nI'd really recommend Scrapy.\nQuote from a deleted answer:\n\n\nScrapy crawling is fastest than mechanize because uses asynchronous operations (on top of Twisted).\nScrapy has better and fastest support for parsing (x)html on top of libxml2.\nScrapy is a mature framework with full unicode, handles redirections, gzipped responses, odd encodings, integrated http cache, etc.\nOnce you are into Scrapy, you can write a spider in less than 5 minutes that download images, creates thumbnails and export the extracted data directly to csv or json.\n\n\n"", '\nI collected together scripts from my web scraping work into this bit-bucket library.\nExample script for your case:\nfrom webscraping import download, xpath\nD = download.Download()\n\nhtml = D.get(\'http://example.com\')\nfor row in xpath.search(html, \'//table[@class=""spad""]/tbody/tr\'):\n    cols = xpath.search(row, \'/td\')\n    print \'Sunrise: %s, Sunset: %s\' % (cols[1], cols[2])\n\nOutput:\nSunrise: 08:39, Sunset: 16:08\nSunrise: 08:39, Sunset: 16:09\nSunrise: 08:39, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:11\nSunrise: 08:40, Sunset: 16:12\nSunrise: 08:40, Sunset: 16:13\n\n', ""\nI would strongly suggest checking out pyquery. It uses jquery-like (aka css-like) syntax which makes things really easy for those coming from that background.\nFor your case, it would be something like:\nfrom pyquery import *\n\nhtml = PyQuery(url='http://www.example.com/')\ntrs = html('table.spad tbody tr')\n\nfor tr in trs:\n  tds = tr.getchildren()\n  print tds[1].text, tds[2].text\n\nOutput:\n5:16 AM 9:28 PM\n5:15 AM 9:30 PM\n5:13 AM 9:31 PM\n5:12 AM 9:33 PM\n5:11 AM 9:34 PM\n5:10 AM 9:35 PM\n5:09 AM 9:37 PM\n\n"", ""\nYou can use urllib2 to make the HTTP requests, and then you'll have web content.\nYou can get it like this:\nimport urllib2\nresponse = urllib2.urlopen('http://example.com')\nhtml = response.read()\n\nBeautiful Soup is a python HTML parser that is supposed to be good for screen scraping.\nIn particular, here is their tutorial on parsing an HTML document.\nGood luck!\n"", '\nI use a combination of Scrapemark (finding urls - py2) and httlib2 (downloading images - py2+3). The scrapemark.py has 500 lines of code, but uses regular expressions, so it may be not so fast, did not test.\nExample for scraping your website:\n\nimport sys\nfrom pprint import pprint\nfrom scrapemark import scrape\n\npprint(scrape(""""""\n    <table class=""spad"">\n        <tbody>\n            {*\n                <tr>\n                    <td>{{[].day}}</td>\n                    <td>{{[].sunrise}}</td>\n                    <td>{{[].sunset}}</td>\n                    {# ... #}\n                </tr>\n            *}\n        </tbody>\n    </table>\n"""""", url=sys.argv[1] ))\n\nUsage:\npython2 sunscraper.py http://www.example.com/\n\nResult:\n[{\'day\': u\'1. Dez 2012\', \'sunrise\': u\'08:18\', \'sunset\': u\'16:10\'},\n {\'day\': u\'2. Dez 2012\', \'sunrise\': u\'08:19\', \'sunset\': u\'16:10\'},\n {\'day\': u\'3. Dez 2012\', \'sunrise\': u\'08:21\', \'sunset\': u\'16:09\'},\n {\'day\': u\'4. Dez 2012\', \'sunrise\': u\'08:22\', \'sunset\': u\'16:09\'},\n {\'day\': u\'5. Dez 2012\', \'sunrise\': u\'08:23\', \'sunset\': u\'16:08\'},\n {\'day\': u\'6. Dez 2012\', \'sunrise\': u\'08:25\', \'sunset\': u\'16:08\'},\n {\'day\': u\'7. Dez 2012\', \'sunrise\': u\'08:26\', \'sunset\': u\'16:07\'}]\n\n', '\nMake your life easier by using CSS Selectors\nI know I have come late to party but I have a nice suggestion for you.\nUsing BeautifulSoup is already been suggested I would rather prefer using CSS Selectors to scrape data inside HTML\nimport urllib2\nfrom bs4 import BeautifulSoup\n\nmain_url = ""http://www.example.com""\n\nmain_page_html  = tryAgain(main_url)\nmain_page_soup = BeautifulSoup(main_page_html)\n\n# Scrape all TDs from TRs inside Table\nfor tr in main_page_soup.select(""table.class_of_table""):\n   for td in tr.select(""td#id""):\n       print(td.text)\n       # For acnhors inside TD\n       print(td.select(""a"")[0].text)\n       # Value of Href attribute\n       print(td.select(""a"")[0][""href""])\n\n# This is method that scrape URL and if it doesnt get scraped, waits for 20 seconds and then tries again. (I use it because my internet connection sometimes get disconnects)\ndef tryAgain(passed_url):\n    try:\n        page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n        return page\n    except Exception:\n        while 1:\n            print(""Trying again the URL:"")\n            print(passed_url)\n            try:\n                page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n                print(""-------------------------------------"")\n                print(""---- URL was successfully scraped ---"")\n                print(""-------------------------------------"")\n                return page\n            except Exception:\n                time.sleep(20)\n                continue \n\n', '\nIf we think of getting name of items from any specific category then we can do that by specifying the class name of that category using css selector:\nimport requests ; from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(requests.get(\'https://www.flipkart.com/\').text, ""lxml"")\nfor link in soup.select(\'div._2kSfQ4\'):\n    print(link.text)\n\nThis is the partial search results:\nPuma, USPA, Adidas & moreUp to 70% OffMen\'s Shoes\nShirts, T-Shirts...Under ₹599For Men\nNike, UCB, Adidas & moreUnder ₹999Men\'s Sandals, Slippers\nPhilips & moreStarting ₹99LED Bulbs & Emergency Lights\n\n', '\nHere is a simple web crawler, i used BeautifulSoup and we will search for all the links(anchors) who\'s class name is _3NFO0d. I used Flipkar.com, it is an online retailing store.\nimport requests\nfrom bs4 import BeautifulSoup\ndef crawl_flipkart():\n    url = \'https://www.flipkart.com/\'\n    source_code = requests.get(url)\n    plain_text = source_code.text\n    soup = BeautifulSoup(plain_text, ""lxml"")\n    for link in soup.findAll(\'a\', {\'class\': \'_3NFO0d\'}):\n        href = link.get(\'href\')\n        print(href)\n\ncrawl_flipkart()\n\n', '\nPython has good options to scrape the web. The best one with a framework is scrapy. It can be a little tricky for beginners, so here is a little help. \n1. Install python above 3.5 (lower ones till 2.7 will work). \n2. Create a environment in conda ( I did this). \n3. Install scrapy at a location and run in from there. \n4. Scrapy shell will give you an interactive interface to test you code. \n5. Scrapy startproject projectname will create a framework.\n6. Scrapy genspider spidername will create a spider. You can create as many spiders as you want. While doing this make sure you are inside the project directory. \n\n\nThe easier one is to use requests and beautiful soup. Before starting give one hour of time to go through the documentation, it will solve most of your doubts. BS4 offer wide range of parsers that you can opt for. Use user-agent and sleep to make scraping easier. BS4 returns a bs.tag so use variable[0]. If there is js running, you wont be able to scrape using requests and bs4 directly. You  could get the api link then parse the JSON to get the information you need or try selenium.  \n']",https://stackoverflow.com/questions/2081586/web-scraping-with-python,web-scraping
Scraping: SSL: CERTIFICATE_VERIFY_FAILED error for http://en.wikipedia.org,"
I'm practicing the code from 'Web Scraping with Python', and I keep having this certificate problem:
from urllib.request import urlopen 
from bs4 import BeautifulSoup 
import re

pages = set()
def getLinks(pageUrl):
    global pages
    html = urlopen(""http://en.wikipedia.org""+pageUrl)
    bsObj = BeautifulSoup(html)
    for link in bsObj.findAll(""a"", href=re.compile(""^(/wiki/)"")):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                #We have encountered a new page
                newPage = link.attrs['href'] 
                print(newPage) 
                pages.add(newPage) 
                getLinks(newPage)
getLinks("""")

The error is:
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 1319, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1049)>

Btw,I was also practicing scrapy, but kept getting the problem: command not found: scrapy (I tried all sorts of solutions online but none works... really frustrating)
",338k,"
            252
        ","['\nOnce upon a time I stumbled  with this issue. If you\'re using macOS go to Macintosh HD > Applications > Python3.6 folder (or whatever version of python you\'re using) > double click on ""Install Certificates.command"" file. :D\n', '\nto use unverified ssl you can add this to your code:\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n', '\nThis terminal command:\nopen /Applications/Python\\ 3.7/Install\\ Certificates.command\nFound here:\nhttps://stackoverflow.com/a/57614113/6207266\nResolved it for me. \nWith my config\npip install --upgrade certifi\nhad no impact.\n', '\nTo solve this: \nAll you need to do is to install Python certificates! A common issue on macOS.  \nOpen these files: \nInstall Certificates.command\nUpdate Shell Profile.command\n\nSimply Run these two scripts and you wont have this issue any more.  \nHope this helps!\n', '\nFor novice users, you can go in the Applications folder and expand the Python 3.7 folder. Now first run (or double click) the Install Certificates.command and then Update Shell Profile.command\n\n', '\nFor anyone who is using anaconda, you would install the certifi package, see more at: \nhttps://anaconda.org/anaconda/certifi\nTo install, type this line in your terminal:\nconda install -c anaconda certifi\n\n', '\nopen /Applications/Python\\ 3.7/Install\\ Certificates.command\n\nTry this command in terminal\n', '\nTwo steps worked for me :\n- going Macintosh HD > Applications > Python3.7 folder \n- click on ""Install Certificates.command""\n', ""\nIf you're running on a Mac you could just search for Install Certificates.command on the spotlight and hit enter.\n"", '\nI could find this solution and is working fine:\ncd /Applications/Python\\ 3.7/\n./Install\\ Certificates.command\n\n', '\nTake a look at this post, it seems like for later versions of Python, certificates are not pre installed which seems to cause this error. You should be able to run the following command to install the certifi package: /Applications/Python\\ 3.6/Install\\ Certificates.command\nPost 1: urllib and ""SSL: CERTIFICATE_VERIFY_FAILED"" Error\nPost 2: Airbrake error: urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate\n', '\nI had the same error and solved the problem by running the program code below:\n# install_certifi.py\n#\n# sample script to install or update a set of default Root Certificates\n# for the ssl module.  Uses the certificates provided by the certifi package:\n#       https://pypi.python.org/pypi/certifi\n\nimport os\nimport os.path\nimport ssl\nimport stat\nimport subprocess\nimport sys\n\nSTAT_0o775 = ( stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR\n             | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP\n             | stat.S_IROTH |                stat.S_IXOTH )\n\n\ndef main():\n    openssl_dir, openssl_cafile = os.path.split(\n        ssl.get_default_verify_paths().openssl_cafile)\n\n    print("" -- pip install --upgrade certifi"")\n    subprocess.check_call([sys.executable,\n        ""-E"", ""-s"", ""-m"", ""pip"", ""install"", ""--upgrade"", ""certifi""])\n\n    import certifi\n\n    # change working directory to the default SSL directory\n    os.chdir(openssl_dir)\n    relpath_to_certifi_cafile = os.path.relpath(certifi.where())\n    print("" -- removing any existing file or link"")\n    try:\n        os.remove(openssl_cafile)\n    except FileNotFoundError:\n        pass\n    print("" -- creating symlink to certifi certificate bundle"")\n    os.symlink(relpath_to_certifi_cafile, openssl_cafile)\n    print("" -- setting permissions"")\n    os.chmod(openssl_cafile, STAT_0o775)\n    print("" -- update complete"")\n\nif __name__ == \'__main__\':\n    main()\n\n', '\ni didn\'t solve the problem, sadly.\nbut managed to make to codes work (almost all of my codes have this probelm btw)\nthe local issuer certificate problem happens under python3.7\nso i changed back to python2.7 QAQ\nand all that needed to change including ""from urllib2 import urlopen"" instead of ""from urllib.request import urlopen""\nso sad...\n', ""\nI'm a relative novice compared to all the experts on Stack Overflow.\nI have 2 versions of jupyter notebook running (one through a fresh Anaconda Navigator installation and one through ????). I think this is because Anaconda was installed as a local installation on my Mac (per Anaconda instructions). \nI already had python 3.7 installed. After that, I used my terminal to open jupyter notebook and I think that it put another version globally onto my Mac. \nHowever, I'm not sure because I'm just learning through trial and error!\nI did the terminal command: \nconda install -c anaconda certifi \n\n(as directed above, but it didn't work.) \nMy python 3.7 is installed on OS Catalina10.15.3 in:\n\n/Library/Python/3.7/site-packages AND\n~/Library/Python/3.7/lib/python/site-packages\n\nThe certificate is at:\n\n~/Library/Python/3.7/lib/python/site-packages/certifi-2019.11.28.dist-info\n\nI tried to find the Install Certificate.command ... but couldn't find it through looking through the file structures...not in Applications...not in links above.\nI finally installed it by finding it through Spotlight (as someone suggested above). And it double clicked automatically and installed ANOTHER certificate in the same folder as:\n\n~/Library/Python/3.7/lib/python/site-packages/\n\nNONE of the above solved anything for me...I still got the same error.  \nSo, I solved the problem by:\n\nclosing my jupyter notebook.\nopening Anaconda Navigator.\nopening jupyter notebook through the Navigator GUI (instead of\nthrough Terminal). \nopening my notebook and running the code.\n\nI can't tell you why this worked.  But it solved the problem for me.\nI just want to save someone the hassle next time. If someone can tell my why it worked, that would be terrific.\nI didn't try the other terminal commands because of the 2 versions of jupyter notebook that I knew were a problem. I just don't know how to fix that.\n"", '\nUse requests library.\nTry this solution, or just add https:// before the URL:\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\n\npages = set()\ndef getLinks(pageUrl):\n    global pages\n    html = requests.get(""http://en.wikipedia.org""+pageUrl, verify=False).text\n    bsObj = BeautifulSoup(html)\n    for link in bsObj.findAll(""a"", href=re.compile(""^(/wiki/)"")):\n        if \'href\' in link.attrs:\n            if link.attrs[\'href\'] not in pages:\n                #We have encountered a new page\n                newPage = link.attrs[\'href\']\n                print(newPage)\n                pages.add(newPage)\n                getLinks(newPage)\ngetLinks("""")\n\nCheck if this works for you\n', '\nFor me the problem was that I was setting REQUESTS_CA_BUNDLE in my .bash_profile\n/Users/westonagreene/.bash_profile:\n...\nexport REQUESTS_CA_BUNDLE=/usr/local/etc/openssl/cert.pem\n...\n\nOnce I set REQUESTS_CA_BUNDLE to blank (i.e. removed from .bash_profile), requests worked again.\nexport REQUESTS_CA_BUNDLE=""""\n\nThe problem only exhibited when executing python requests via a CLI (Command Line Interface). If I ran requests.get(URL, CERT) it resolved just fine.\nMac OS Catalina (10.15.6).\nPyenv of 3.6.11.\nError message I was getting: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)\nMy answer elsewhere: https://stackoverflow.com/a/64151964/4420657\n', ""\nI am using Debian 10 buster and try download a file with youtube-dl and get this error:\nsudo youtube-dl -k https://youtu.be/uscis0CnDjk\n\n[youtube] uscis0CnDjk: Downloading webpage\nERROR: Unable to download webpage: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)> (caused by URLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)')))\n\nCertificates with python2 and python3.8 are installed correctly, but i persistent receive the same error.\nfinally (which is not the best solution, but works for me was to eliminate the certificate check as it is given as an option in youtube-dl) whith this command\nsudo youtube-dl -k --no-check-certificate https://youtu.be/uscis0CnDjk \n"", '\nI am seeing this issue on a Ubuntu 20.04 system and none of the ""real fixes"" (like this one) helped.\nWhile Firefox was willing to open the site just fine neither GNOME Web (i.e. Epiphany) nor Python3 or wget were accepting the certificate. After some searching, I came across this answer on ServerFault which lists two common reasons:\n\n\nThe certificate is really signed by an unknown CA (for instance an internal CA).\nThe certificate is signed with an intermediate CA certificate from one of the well known CA\'s and the remote server is misconfigured in the regard that it doesn\'t include that intermediate CA certificate as a CA chain it\'s response.\n\n\nYou can use the Qualys SSL Labs website to check the site\'s certificates and if there are issues, contact the site\'s administrator to have it fixed.\nIf you really need to work around the issue right now, I\'d recommend a temporary solution like Rambod\'s confined to the site(s) you\'re trying to access.\n', '\nMake sure your websockets is >=10.0\nAdditional to:\nInstall Certificates.command\nUpdate Shell Profile.command\npip3 install websockets==10.0\n', '\nThis will work. Set the environment variable PYTHONHTTPSVERIFY to 0.\n\nBy typing linux command:\n\nexport PYTHONHTTPSVERIFY = 0\n\nOR\n\nUsing in python code:\n\nimport os\nos.environ[""PYTHONHTTPSVERIFY""] = ""0""\n\n', '\nBTW guys if you are getting the same error using aiohttp just put verify_ssl=False argument into your TCPConnector:\nimport aiohttp\n...\n\nasync with aiohttp.ClientSession(\n    connector=aiohttp.TCPConnector(verify_ssl=False)\n) as session:\n    async with session.get(url) as response:\n        body = await response.text()\n\n', ""\nI am using anaconda on windows. Was getting the same error until I tried the following;\nimport urllib.request\nlink = 'http://docs.python.org'\nwith urllib.request.urlopen(link) as response:\n    htmlSource = response.read()\n\nwhich I got from the stackoverflow thread on using urlopen:\nPython urllib urlopen not working\n""]",https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org,web-scraping
Scraping data from website using vba,"
Im trying to scrape data from website: http://uk.investing.com/rates-bonds/financial-futures via vba, like real-time price, i.e. German 5 YR Bobl, US 30Y T-Bond, i have tried excel web query but it only scrapes the whole website, but I would like to scrape the rate only, is there a way of doing this?
",157k,"
            17
        ","['\nThere are several ways of doing this. This is an answer that I write hoping that all the basics of Internet Explorer automation will be found when browsing for the keywords ""scraping data from website"", but remember that nothing\'s worth as your own research (if you don\'t want to stick to pre-written codes that you\'re not able to customize).\nPlease note that this is one way, that I don\'t prefer in terms of performance (since it depends on the browser speed) but that is good to understand the rationale behind Internet automation.\n1) If I need to browse the web, I need a browser! So I create an Internet Explorer browser:\nDim appIE As Object\nSet appIE = CreateObject(""internetexplorer.application"")\n\n2) I ask the browser to browse the target webpage. Through the use of the property "".Visible"", I decide if I want to see the browser doing its job or not. When building the code is nice to have Visible = True, but when the code is working for scraping data is nice not to see it everytime so Visible = False. \nWith appIE\n    .Navigate ""http://uk.investing.com/rates-bonds/financial-futures""\n    .Visible = True\nEnd With\n\n3) The webpage will need some time to load. So, I will wait meanwhile it\'s busy...\nDo While appIE.Busy\n    DoEvents\nLoop\n\n4) Well, now the page is loaded. Let\'s say that I want to scrape the change of the US30Y T-Bond:\nWhat I will do is just clicking F12 on Internet Explorer to see the webpage\'s code, and hence using the pointer (in red circle) I will click on the element that I want to scrape to see how can I reach my purpose. \n\n5) What I should do is straight-forward. First of all, I will get by the ID property the tr element which is containing the value:\nSet allRowOfData = appIE.document.getElementById(""pair_8907"")\n\nHere I will get a collection of td elements (specifically, tr is a row of data, and the td are its cells. We are looking for the 8th, so I will write:\nDim myValue As String: myValue = allRowOfData.Cells(7).innerHTML\n\nWhy did I write 7 instead of 8? Because the collections of cells starts from 0, so the index of the 8th element is 7 (8-1). Shortly analysing this line of code:\n\n.Cells() makes me access the td elements;\ninnerHTML is the property of the cell containing the value we look for. \n\nOnce we have our value, which is now stored into the myValue variable, we can just close the IE browser and releasing the memory by setting it to Nothing:\nappIE.Quit\nSet appIE = Nothing\n\nWell, now you have your value and you can do whatever you want with it: put it into a cell (Range(""A1"").Value = myValue), or into a label of a form (Me.label1.Text = myValue).\nI\'d just like to point you out that this is not how StackOverflow works: here you post questions about specific coding problems, but you should make your own search first. The reason why I\'m answering a question which is not showing too much research effort is just that I see it asked several times and, back to the time when I learned how to do this, I remember that I would have liked having some better support to get started with. So I hope that this answer, which is just a ""study input"" and not at all the best/most complete solution, can be a support for next user having your same problem. Because I have learned how to program thanks to this community, and I like to think that you and other beginners might use my input to discover the beautiful world of programming. \nEnjoy your practice ;) \n', '\nOther methods were mentioned so let us please acknowledge that, at the time of writing, we are in the 21st century. Let\'s park the local bus browser opening, and fly with an XMLHTTP GET request (XHR GET for short).\nWiki moment:\n\nXHR is an API in the form of an object whose methods transfer data\nbetween a web browser and a web server. The object is provided by the\nbrowser\'s JavaScript environment\n\nIt\'s a fast method for retrieving data that doesn\'t require opening a browser. The server response can be read into an HTMLDocument and the process of grabbing the table continued from there.\nNote that javascript rendered/dynamically added content will not be retrieved as there is no javascript engine running (which there is in a browser).\nIn the below code, the table is grabbed by its id cr1.\n\nIn the helper sub, WriteTable,  we loop the columns (td tags) and then the table rows (tr tags), and finally traverse the length of each table row, table cell by table cell. As we only want data from columns 1 and 8, a Select Case statement is used specify what is written out to the sheet.\n\nSample webpage view:\n\n\nSample code output:\n\n\nVBA:\nOption Explicit\nPublic Sub GetRates()\n    Dim html As HTMLDocument, hTable As HTMLTable \'<== Tools > References > Microsoft HTML Object Library\n    \n    Set html = New HTMLDocument\n      \n    With CreateObject(""MSXML2.XMLHTTP"")\n        .Open ""GET"", ""https://uk.investing.com/rates-bonds/financial-futures"", False\n        .setRequestHeader ""If-Modified-Since"", ""Sat, 1 Jan 2000 00:00:00 GMT"" \'to deal with potential caching\n        .send\n        html.body.innerHTML = .responseText\n    End With\n    \n    Application.ScreenUpdating = False\n    \n    Set hTable = html.getElementById(""cr1"")\n    WriteTable hTable, 1, ThisWorkbook.Worksheets(""Sheet1"")\n    \n    Application.ScreenUpdating = True\nEnd Sub\n\nPublic Sub WriteTable(ByVal hTable As HTMLTable, Optional ByVal startRow As Long = 1, Optional ByVal ws As Worksheet)\n    Dim tSection As Object, tRow As Object, tCell As Object, tr As Object, td As Object, r As Long, C As Long, tBody As Object\n    r = startRow: If ws Is Nothing Then Set ws = ActiveSheet\n    With ws\n        Dim headers As Object, header As Object, columnCounter As Long\n        Set headers = hTable.getElementsByTagName(""th"")\n        For Each header In headers\n            columnCounter = columnCounter + 1\n            Select Case columnCounter\n            Case 2\n                .Cells(startRow, 1) = header.innerText\n            Case 8\n                .Cells(startRow, 2) = header.innerText\n            End Select\n        Next header\n        startRow = startRow + 1\n        Set tBody = hTable.getElementsByTagName(""tbody"")\n        For Each tSection In tBody\n            Set tRow = tSection.getElementsByTagName(""tr"")\n            For Each tr In tRow\n                r = r + 1\n                Set tCell = tr.getElementsByTagName(""td"")\n                C = 1\n                For Each td In tCell\n                    Select Case C\n                    Case 2\n                        .Cells(r, 1).Value = td.innerText\n                    Case 8\n                        .Cells(r, 2).Value = td.innerText\n                    End Select\n                    C = C + 1\n                Next td\n            Next tr\n        Next tSection\n    End With\nEnd Sub\n\n', ""\nyou can use winhttprequest object instead of internet explorer as it's good to load data excluding pictures n advertisement instead of downloading full webpage including advertisement n pictures those make internet explorer object heavy compare to winhttpRequest object. \n"", '\nThis question asked long before. But I thought following information will useful for newbies. Actually you can easily get the values from class name like this.\nSub ExtractLastValue()\n\nSet objIE = CreateObject(""InternetExplorer.Application"")\n\nobjIE.Top = 0\nobjIE.Left = 0\nobjIE.Width = 800\nobjIE.Height = 600\n\nobjIE.Visible = True\n\nobjIE.Navigate (""https://uk.investing.com/rates-bonds/financial-futures/"")\n\nDo\nDoEvents\nLoop Until objIE.readystate = 4\n\nMsgBox objIE.document.getElementsByClassName(""pid-8907-last"")(0).innerText\n\nEnd Sub\n\nAnd if you are new to web scraping please read this blog post.\nWeb Scraping - Basics\nAnd also there are various techniques to extract data from web pages. This article explain few of them with examples.\nWeb Scraping - Collecting Data From a Webpage\n', '\nI modified some thing that were poping up error for me and end up with this which worked great to extract the data as I needed:\nSub get_data_web()\n\nDim appIE As Object\nSet appIE = CreateObject(""internetexplorer.application"")\n\nWith appIE\n    .navigate ""https://finance.yahoo.com/quote/NQ%3DF/futures?p=NQ%3DF""\n    .Visible = True\nEnd With\n\nDo While appIE.Busy\n    DoEvents\nLoop\n\nSet allRowofData = appIE.document.getElementsByClassName(""Ta(end) BdT Bdc($c-fuji-grey-c) H(36px)"")\n\nDim i As Long\nDim myValue As String\n\nCount = 1\n\n    For Each itm In allRowofData\n\n        For i = 0 To 4\n\n        myValue = itm.Cells(i).innerText\n        ActiveSheet.Cells(Count, i + 1).Value = myValue\n\n        Next\n\n        Count = Count + 1\n\n    Next\n\nappIE.Quit\nSet appIE = Nothing\n\n\nEnd Sub\n\n']",https://stackoverflow.com/questions/27066963/scraping-data-from-website-using-vba,web-scraping
"How to ""scan"" a website (or page) for info, and bring it into my program?","
Well, I'm pretty much trying to figure out how to pull information from a webpage, and bring it into my program (in Java). 
For example, if I know the exact page I want info from, for the sake of simplicity a Best Buy item page, how would I get the appropriate info I need off of that page? Like the title, price, description? 
What would this process even be called? I have no idea were to even begin researching this.
Edit:
Okay, I'm running a test for the JSoup(the one posted by BalusC), but I keep getting this error:
Exception in thread ""main"" java.lang.NoSuchMethodError: java.util.LinkedList.peekFirst()Ljava/lang/Object;
at org.jsoup.parser.TokenQueue.consumeWord(TokenQueue.java:209)
at org.jsoup.parser.Parser.parseStartTag(Parser.java:117)
at org.jsoup.parser.Parser.parse(Parser.java:76)
at org.jsoup.parser.Parser.parse(Parser.java:51)
at org.jsoup.Jsoup.parse(Jsoup.java:28)
at org.jsoup.Jsoup.parse(Jsoup.java:56)
at test.main(test.java:12)

I do have Apache Commons
",111k,"
            58
        ","['\nUse a HTML parser like Jsoup. This has my preference above the other HTML parsers available in Java since it supports jQuery like CSS selectors. Also, its class representing a list of nodes, Elements, implements Iterable so that you can iterate over it in an enhanced for loop (so there\'s no need to hassle with verbose Node and NodeList like classes in the average Java DOM parser).\nHere\'s a basic kickoff example (just put the latest Jsoup JAR file in classpath):\npackage com.stackoverflow.q2835505;\n\nimport org.jsoup.Jsoup;\nimport org.jsoup.nodes.Document;\nimport org.jsoup.nodes.Element;\nimport org.jsoup.select.Elements;\n\npublic class Test {\n\n    public static void main(String[] args) throws Exception {\n        String url = ""https://stackoverflow.com/questions/2835505"";\n        Document document = Jsoup.connect(url).get();\n\n        String question = document.select(""#question .post-text"").text();\n        System.out.println(""Question: "" + question);\n\n        Elements answerers = document.select(""#answers .user-details a"");\n        for (Element answerer : answerers) {\n            System.out.println(""Answerer: "" + answerer.text());\n        }\n    }\n\n}\n\nAs you might have guessed, this prints your own question and the names of all answerers.\n', ""\nThis is referred to as screen scraping, wikipedia has this article on the more specific web scraping. It can be a major challenge because there's some ugly, mess-up, broken-if-not-for-browser-cleverness HTML out there, so good luck. \n"", ""\nI would use JTidy - it is simlar to JSoup, but I don't know JSoup well. JTidy handles broken HTML and returns a w3c Document, so you can use this as a source to XSLT to extract the content you are really interested in. If you don't know XSLT, then you might as well go with JSoup, as the Document model is nicer to work with than w3c.\nEDIT: A quick look on the JSoup website shows that JSoup may indeed be the better choice. It seems to support CSS selectors out the box for extracting stuff from the document. This may be a lot easier to work with than getting into XSLT.\n"", ""\nYou may use an html parser (many useful links here: java html parser).\nThe process is called 'grabbing website content'. Search 'grab website content java' for further invertigation.\n"", '\njsoup supports java 1.5\nhttps://github.com/tburch/jsoup/commit/d8ea84f46e009a7f144ee414a9fa73ea187019a3\nlooks like that stack was a bug, and has been fixed\n', ""\nYou'd probably want to look at the HTML to see if you can find strings that are unique and near your text, then you can use line/char-offsets to get to the data.\nCould be awkward in Java, if there aren't any XML classes similar to the ones found in System.XML.Linq in C#.\n"", '\nYou could also try jARVEST.\nIt is based on a JRuby DSL over a pure-Java engine to spider-scrape-transform web sites.\nExample:\nFind all links inside a web page (wget and xpath are constructs of the jARVEST\'s language):\nwget | xpath(\'//a/@href\')\n\nInside a Java program:\nJarvest jarvest = new Jarvest();\n  String[] results = jarvest.exec(\n    ""wget | xpath(\'//a/@href\')"", //robot! \n    ""http://www.google.com"" //inputs\n  );\n  for (String s : results){\n    System.out.println(s);\n  }\n\n', '\nMy answer won\'t probably be useful to the writer of this question (I am 8 months late so not the right timing I guess) but I think it will probably be useful for many other developers that might come across this answer.\nToday, I just released (in the name of my company) an HTML to POJO complete framework that you can use to map HTML to any POJO class with simply some annotations. The library itself is quite handy and features many other things all the while being very pluggable. You can have a look to it right here : https://github.com/whimtrip/jwht-htmltopojo\nHow to use : Basics\nImagine we need to parse the following html page :\n<html>\n    <head>\n        <title>A Simple HTML Document</title>\n    </head>\n    <body>\n        <div class=""restaurant"">\n            <h1>A la bonne Franquette</h1>\n            <p>French cuisine restaurant for gourmet of fellow french people</p>\n            <div class=""location"">\n                <p>in <span>London</span></p>\n            </div>\n            <p>Restaurant n*18,190. Ranked 113 out of 1,550 restaurants</p>  \n            <div class=""meals"">\n                <div class=""meal"">\n                    <p>Veal Cutlet</p>\n                    <p rating-color=""green"">4.5/5 stars</p>\n                    <p>Chef Mr. Frenchie</p>\n                </div>\n\n                <div class=""meal"">\n                    <p>Ratatouille</p>\n                    <p rating-color=""orange"">3.6/5 stars</p>\n                    <p>Chef Mr. Frenchie and Mme. French-Cuisine</p>\n                </div>\n\n            </div> \n        </div>    \n    </body>\n</html>\n\nLet\'s create the POJOs we want to map it to :\npublic class Restaurant {\n\n    @Selector( value = ""div.restaurant > h1"")\n    private String name;\n\n    @Selector( value = ""div.restaurant > p:nth-child(2)"")\n    private String description;\n\n    @Selector( value = ""div.restaurant > div:nth-child(3) > p > span"")    \n    private String location;    \n\n    @Selector( \n        value = ""div.restaurant > p:nth-child(4)""\n        format = ""^Restaurant n\\*([0-9,]+). Ranked ([0-9,]+) out of ([0-9,]+) restaurants$"",\n        indexForRegexPattern = 1,\n        useDeserializer = true,\n        deserializer = ReplacerDeserializer.class,\n        preConvert = true,\n        postConvert = false\n    )\n    // so that the number becomes a valid number as they are shown in this format : 18,190\n    @ReplaceWith(value = "","", with = """")\n    private Long id;\n\n    @Selector( \n        value = ""div.restaurant > p:nth-child(4)""\n        format = ""^Restaurant n\\*([0-9,]+). Ranked ([0-9,]+) out of ([0-9,]+) restaurants$"",\n        // This time, we want the second regex group and not the first one anymore\n        indexForRegexPattern = 2,\n        useDeserializer = true,\n        deserializer = ReplacerDeserializer.class,\n        preConvert = true,\n        postConvert = false\n    )\n    // so that the number becomes a valid number as they are shown in this format : 18,190\n    @ReplaceWith(value = "","", with = """")\n    private Integer rank;\n\n    @Selector(value = "".meal"")    \n    private List<Meal> meals;\n\n    // getters and setters\n\n}\n\nAnd now the Meal class as well :\npublic class Meal {\n\n    @Selector(value = ""p:nth-child(1)"")\n    private String name;\n\n    @Selector(\n        value = ""p:nth-child(2)"",\n        format = ""^([0-9.]+)\\/5 stars$"",\n        indexForRegexPattern = 1\n    )\n    private Float stars;\n\n    @Selector(\n        value = ""p:nth-child(2)"",\n        // rating-color custom attribute can be used as well\n        attr = ""rating-color""\n    )\n    private String ratingColor;\n\n    @Selector(\n        value = ""p:nth-child(3)""\n    )\n    private String chefs;\n\n    // getters and setters.\n}\n\nWe provided some more explanations on the above code on our github page.\nFor the moment, let\'s see how to scrap this.\nprivate static final String MY_HTML_FILE = ""my-html-file.html"";\n\npublic static void main(String[] args) {\n\n\n    HtmlToPojoEngine htmlToPojoEngine = HtmlToPojoEngine.create();\n\n    HtmlAdapter<Restaurant> adapter = htmlToPojoEngine.adapter(Restaurant.class);\n\n    // If they were several restaurants in the same page, \n    // you would need to create a parent POJO containing\n    // a list of Restaurants as shown with the meals here\n    Restaurant restaurant = adapter.fromHtml(getHtmlBody());\n\n    // That\'s it, do some magic now!\n\n}\n\n\nprivate static String getHtmlBody() throws IOException {\n    byte[] encoded = Files.readAllBytes(Paths.get(MY_HTML_FILE));\n    return new String(encoded, Charset.forName(""UTF-8""));\n\n}\n\nAnother short example can be found here\nHope this will help someone out there!\n', '\nJSoup solution is great, but if you need to extract just something really simple it may be easier to use regex or String.indexOf\nAs others have already mentioned the process is called scraping\n', ""\nLook into the cURL library.  I've never used it in Java, but I'm sure there must be bindings for it.  Basically, what you'll do is send a cURL request to whatever page you want to 'scrape'.  The request will return a string with the source code to the page.  From there, you will use regex to parse whatever data you want from the source code.  That's generally how you are going to do it.\n""]",https://stackoverflow.com/questions/2835505/how-to-scan-a-website-or-page-for-info-and-bring-it-into-my-program,web-scraping
Java HTML Parsing [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 11 years ago.



I'm working on an app which scrapes data from a website and I was wondering how I should go about getting the data.  Specifically I need data contained in a number of div tags which use a specific CSS class - Currently (for testing purposes) I'm just checking for 
div class = ""classname""

in each line of HTML - This works, but I can't help but feel there is a better solution out there.  
Is there any nice way where I could give a class a line of HTML and have some nice methods like:
boolean usesClass(String CSSClassname);
String getText();
String getLink();

",110k,"
            52
        ","['\nAnother library that might be useful for HTML processing is jsoup.\nJsoup tries to clean malformed HTML and allows html parsing in Java using jQuery like tag selector syntax.\nhttp://jsoup.org/ \n', '\nThe main problem as stated by preceding coments is malformed HTML, so an html cleaner or HTML-XML converter is a must. Once you get the XML code (XHTML) there are plenty of tools to handle it. You could get it with a simple SAX handler that extracts only the data you need or any tree-based method (DOM, JDOM, etc.) that let you even modify original code.\nHere is a sample code that uses HTML cleaner to get all DIVs that use a certain class and print out all Text content inside it.\nimport java.io.IOException;\nimport java.net.URL;\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\n\nimport org.htmlcleaner.HtmlCleaner;\nimport org.htmlcleaner.TagNode;\n\n/**\n * @author Fernando Miguélez Palomo <fernandoDOTmiguelezATgmailDOTcom>\n */\npublic class TestHtmlParse\n{\n    static final String className = ""tags"";\n    static final String url = ""http://www.stackoverflow.com"";\n\n    TagNode rootNode;\n\n    public TestHtmlParse(URL htmlPage) throws IOException\n    {\n        HtmlCleaner cleaner = new HtmlCleaner();\n        rootNode = cleaner.clean(htmlPage);\n    }\n\n    List getDivsByClass(String CSSClassname)\n    {\n        List divList = new ArrayList();\n\n        TagNode divElements[] = rootNode.getElementsByName(""div"", true);\n        for (int i = 0; divElements != null && i < divElements.length; i++)\n        {\n            String classType = divElements[i].getAttributeByName(""class"");\n            if (classType != null && classType.equals(CSSClassname))\n            {\n                divList.add(divElements[i]);\n            }\n        }\n\n        return divList;\n    }\n\n    public static void main(String[] args)\n    {\n        try\n        {\n            TestHtmlParse thp = new TestHtmlParse(new URL(url));\n\n            List divs = thp.getDivsByClass(className);\n            System.out.println(""*** Text of DIVs with class \'""+className+""\' at \'""+url+""\' ***"");\n            for (Iterator iterator = divs.iterator(); iterator.hasNext();)\n            {\n                TagNode divElement = (TagNode) iterator.next();\n                System.out.println(""Text child nodes of DIV: "" + divElement.getText().toString());\n            }\n        }\n        catch(Exception e)\n        {\n            e.printStackTrace();\n        }\n    }\n}\n\n', '\nSeveral years ago I used JTidy for the same purpose:\nhttp://jtidy.sourceforge.net/\n""JTidy is a Java port of HTML Tidy, a HTML syntax checker and pretty printer. Like its non-Java cousin, JTidy can be used as a tool for cleaning up malformed and faulty HTML. In addition, JTidy provides a DOM interface to the document that is being processed, which effectively makes you able to use JTidy as a DOM parser for real-world HTML.\nJTidy was written by Andy Quick, who later stepped down from the maintainer position. Now JTidy is maintained by a group of volunteers.\nMore information on JTidy can be found on the JTidy SourceForge project page .""\n', '\nYou might be interested by TagSoup, a Java HTML parser able to handle malformed HTML. XML parsers would work only on well formed XHTML.\n', '\nThe HTMLParser project (http://htmlparser.sourceforge.net/) might be a possibility.  It seems to be pretty decent at handling malformed HTML.  The following snippet should do what you need:\nParser parser = new Parser(htmlInput);\nCssSelectorNodeFilter cssFilter = \n    new CssSelectorNodeFilter(""DIV.targetClassName"");\nNodeList nodes = parser.parse(cssFilter);\n\n', '\nJericho: http://jericho.htmlparser.net/docs/index.html\nEasy to use, supports not well formed HTML, a lot of examples.\n', '\nHTMLUnit might be of help. It does a lot more stuff too.\nhttp://htmlunit.sourceforge.net/1\n', '\nLet\'s not forget Jerry, its jQuery in java: a fast and concise Java Library that simplifies HTML document parsing, traversing and manipulating; includes usage of css3 selectors.\nExample:\nJerry doc = jerry(html);\ndoc.$(""div#jodd p.neat"").css(""color"", ""red"").addClass(""ohmy"");\n\nExample:\ndoc.form(""#myform"", new JerryFormHandler() {\n    public void onForm(Jerry form, Map<String, String[]> parameters) {\n        // process form and parameters\n    }\n});\n\nOf course, these are just some quick examples to get the feeling how it all looks like.\n', ""\nThe nu.validator project is an excellent, high performance HTML parser that doesn't cut corners correctness-wise.\n\nThe Validator.nu HTML Parser is an implementation of the HTML5 parsing algorithm in Java. The parser is designed to work as a drop-in replacement for the XML parser in applications that already support XHTML 1.x content with an XML parser and use SAX, DOM or XOM to interface with the parser. Low-level functionality is provided for applications that wish to perform their own IO and support document.write() with scripting. The parser core compiles on Google Web Toolkit and can be automatically translated into C++. (The C++ translation capability is currently used for porting the parser for use in Gecko.)\n\n"", '\nYou can also use XWiki HTML Cleaner:\nIt uses HTMLCleaner and extends it to generate valid XHTML 1.1 content.\n', ""\nIf your HTML is well-formed, you can easily employ an XML parser to do the job for you... If you're only reading, SAX would be ideal.\n""]",https://stackoverflow.com/questions/238036/java-html-parsing,web-scraping
Scraping dynamic content using python-Scrapy,"
Disclaimer: I've seen numerous other similar posts on StackOverflow and tried to do it the same way but was they don't seem to work on this website.
I'm using Python-Scrapy for getting data from koovs.com. 
However, I'm not able to get the product size, which is dynamically generated. Specifically, if someone could guide me a little on getting the 'Not available' size tag from the drop-down menu on this link, I'd be grateful. 
I am able to get the size list statically, but doing that I only get the list of sizes but not which of them are available.
",55k,"
            47
        ","['\nYou can also solve it with ScrapyJS (no need for selenium and a real browser):\n\nThis library provides Scrapy+JavaScript integration using Splash. \n\nFollow the installation instructions for Splash and ScrapyJS, start the splash docker container:\n$ docker run -p 8050:8050 scrapinghub/splash\n\nPut the following settings into settings.py:\nSPLASH_URL = \'http://192.168.59.103:8050\' \n\nDOWNLOADER_MIDDLEWARES = {\n    \'scrapyjs.SplashMiddleware\': 725,\n}\n\nDUPEFILTER_CLASS = \'scrapyjs.SplashAwareDupeFilter\'\n\nAnd here is your sample spider that is able to see the size availability information:\n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = ""example""\n    allowed_domains = [""koovs.com""]\n    start_urls = (\n        \'http://www.koovs.com/only-onlall-stripe-ls-shirt-59554.html?from=category-651&skuid=236376\',\n    )\n\n    def start_requests(self):\n        for url in self.start_urls:\n            yield scrapy.Request(url, self.parse, meta={\n                \'splash\': {\n                    \'endpoint\': \'render.html\',\n                    \'args\': {\'wait\': 0.5}\n                }\n            })\n\n    def parse(self, response):\n        for option in response.css(""div.select-size select.sizeOptions option"")[1:]:\n            print option.xpath(""text()"").extract()\n\nHere is what is printed on the console:\n[u\'S / 34 -- Not Available\']\n[u\'L / 40 -- Not Available\']\n[u\'L / 42\']\n\n', '\nFrom what I understand, the size availability is determined dynamically in javascript being executed in the browser. Scrapy is not a browser and cannot execute javascript.\nIf you are okay with switching to selenium browser automation tool, here is a sample code:\nfrom selenium import webdriver\nfrom selenium.webdriver.support.select import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\n\nbrowser = webdriver.Firefox()  # can be webdriver.PhantomJS()\nbrowser.get(\'http://www.koovs.com/only-onlall-stripe-ls-shirt-59554.html?from=category-651&skuid=236376\')\n\n# wait for the select element to become visible\nselect_element = WebDriverWait(browser, 10).until(EC.visibility_of_element_located((By.CSS_SELECTOR, ""div.select-size select.sizeOptions"")))\n\nselect = Select(select_element)\nfor option in select.options[1:]:\n    print option.text\n\nbrowser.quit()\n\nIt prints:\nS / 34 -- Not Available\nL / 40 -- Not Available\nL / 42\n\nNote that in place of Firefox you can use other webdrivers like Chrome or Safari. There is also an option to use a headless PhantomJS browser.\nYou can also combine Scrapy with Selenium if needed, see:\n\nselenium with scrapy for dynamic page\nscrapy-webdriver\nseleniumcrawler\n\n', ""\nI faced that problem and solved easily by following these steps\npip install splash \npip install scrapy-splash \npip install scrapyjs\ndownload and install docker-toolbox\nopen docker-quickterminal and enter \n$ docker run -p 8050:8050 scrapinghub/splash\n\nTo set the SPLASH_URL check the default ip configured in the docker machine by entering  $ docker-machine ip default (My IP was 192.168.99.100)\nSPLASH_URL = 'http://192.168.99.100:8050'\nDOWNLOADER_MIDDLEWARES = {\n    'scrapyjs.SplashMiddleware': 725,\n}\n\nDUPEFILTER_CLASS = 'scrapyjs.SplashAwareDupeFilter'\n\nThat's it!\n"", '\nYou have to interpret the json of the website, examples\nscrapy.readthedocs and \ntestingcan.github.io\nimport scrapy\nimport json\nclass QuoteSpider(scrapy.Spider):\n   name = \'quote\'\n   allowed_domains = [\'quotes.toscrape.com\']\n   page = 1\n   start_urls = [\'http://quotes.toscrape.com/api/quotes?page=1\']\n\n   def parse(self, response):\n      data = json.loads(response.text)\n      for quote in data[""quotes""]:\n        yield {""quote"": quote[""text""]}\n      if data[""has_next""]:\n          self.page += 1\n          url = ""http://quotes.toscrape.com/api/quotes?page={}"".format(self.page)\n          yield scrapy.Request(url=url, callback=self.parse)\n\n']",https://stackoverflow.com/questions/30345623/scraping-dynamic-content-using-python-scrapy,web-scraping
Headless Browser and scraping - solutions [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



I'm trying to put list of possible solutions for browser automatic tests suits and headless browser platforms capable of scraping.

BROWSER TESTING / SCRAPING:

Selenium - polyglot flagship in browser automation, bindings for Python, Ruby,  JavaScript, C#, Haskell and more, IDE for Firefox (as an extension) for faster test deployment. Can act as a Server and has tons of features.

JAVASCRIPT

PhantomJS - JavaScript, headless testing with screen capture and automation, uses Webkit. As of version 1.8 Selenium's WebDriver API is implemented, so you can use any WebDriver binding and tests will be compatible with Selenium
SlimerJS - similar to PhantomJS, uses Gecko (Firefox) instead of WebKit
CasperJS - JavaScript, build on both PhantomJS and SlimerJS, has extra features
Ghost Driver - JavaScript implementation of the WebDriver Wire Protocol for PhantomJS.
new PhantomCSS - CSS regression testing. A CasperJS module for automating visual regression testing with PhantomJS and Resemble.js.
new WebdriverCSS - plugin for Webdriver.io for automating visual regression testing
new PhantomFlow - Describe and visualize user flows through tests. An experimental approach to Web user interface testing.
new trifleJS - ports the PhantomJS API to use the Internet Explorer engine.
new CasperJS IDE (commercial)

NODE.JS

Node-phantom - bridges the gap between PhantomJS and node.js
WebDriverJs - Selenium WebDriver bindings for node.js by Selenium Team
WD.js - node module for WebDriver/Selenium 2
yiewd - WD.js wrapper using latest Harmony generators! Get rid of the callback pyramid with yield
ZombieJs - Insanely fast, headless full-stack testing using node.js
NightwatchJs - Node JS based testing solution using Selenium Webdriver
Chimera - Chimera: can do everything what phantomJS does, but in a full JS environment
Dalek.js - Automated cross browser testing with JavaScript through Selenium Webdriver
Webdriver.io - better implementation of WebDriver bindings with predefined 50+ actions
Nightmare - Electron bridge with a high-level API.
jsdom - Tailored towards web scraping. A very lightweight DOM implemented in Node.js, it supports pages with javascript.
new Puppeteer - Node library which provides a high-level API to control Chrome or Chromium. Puppeteer runs headless by default.

WEB SCRAPING / MINING

Scrapy - Python, mainly a scraper/miner - fast, well documented and, can be linked with Django Dynamic Scraper for nice mining deployments, or Scrapy Cloud for PaaS (server-less) deployment, works in terminal or an server stand-alone proces, can be used with Celery, built on top of Twisted
Snailer - node.js module, untested yet.
Node-Crawler - node.js module, untested yet.

ONLINE TOOLS

new Web Scraping Language - Simple syntax to crawl the web

new Online HTTP client - Dedicated SO answer

dead CasperBox - Run CasperJS scripts online


Android TOOLS for Automation

new Mechanica Browser App


RELATED LINKS & RESOURCES

Comparsion of Webscraping software
new Resemble.js : Image analysis and comparison

Questions:

Any pure Node.js solution or Nodejs to PhanthomJS/CasperJS module that actually works and is documented?

Answer: Chimera seems to go in that direction, checkout Chimera

Other solutions capable of easier JavaScript injection than Selenium?

Do you know any pure ruby solutions?


Answer: Checkout the list created by rjk with ruby based solutions

Do you know any related tech or solution?

Feel free to edit this question and add content as you wish! Thank you for your contributions!
",83k,"
            377
        ","['\nIf Ruby is your thing, you may also try:\n\nhttps://github.com/chriskite/anemone (dev stopped)\nhttps://github.com/sparklemotion/mechanize\nhttps://github.com/postmodern/spidr\nhttps://github.com/stewartmckee/cobweb\nhttp://watirwebdriver.com/ (Selenium)\n\nalso, Nokogiri gem can be used for scraping:\n\nhttp://nokogiri.org/\n\nthere is a dedicated book about how to utilise nokogiri for scraping by packt publishing\n', '\nhttp://triflejs.org/ is like phantomjs but based on IE\n', '\nA kind of JS-based Selenium is Dalek.js. It not only aims for automated frontend-tests, you can also do screenshots with it. It has webdrivers for all important browsers. Unfortunately those webdrivers seem to be worth improving (just not to say ""buggy"" to Firefox).\n']",https://stackoverflow.com/questions/18539491/headless-browser-and-scraping-solutions,web-scraping
Problem HTTP error 403 in Python 3 Web Scraping,"
I was trying to scrape a website for practice, but I kept on getting the HTTP Error 403 (does it think I'm a bot)?
Here is my code:
#import requests
import urllib.request
from bs4 import BeautifulSoup
#from urllib import urlopen
import re

webpage = urllib.request.urlopen('http://www.cmegroup.com/trading/products/#sortField=oi&sortAsc=false&venues=3&page=1&cleared=1&group=1').read
findrows = re.compile('<tr class=""- banding(?:On|Off)>(.*?)</tr>')
findlink = re.compile('<a href ="">(.*)</a>')

row_array = re.findall(findrows, webpage)
links = re.finall(findlink, webpate)

print(len(row_array))

iterator = []

The error I get is:
 File ""C:\Python33\lib\urllib\request.py"", line 160, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Python33\lib\urllib\request.py"", line 479, in open
    response = meth(req, response)
  File ""C:\Python33\lib\urllib\request.py"", line 591, in http_response
    'http', request, response, code, msg, hdrs)
  File ""C:\Python33\lib\urllib\request.py"", line 517, in error
    return self._call_chain(*args)
  File ""C:\Python33\lib\urllib\request.py"", line 451, in _call_chain
    result = func(*args)
  File ""C:\Python33\lib\urllib\request.py"", line 599, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden

",279k,"
            168
        ","[""\nThis is probably because of mod_security or some similar server security feature which blocks known spider/bot user agents (urllib uses something like python urllib/3.3.0, it's easily detected). Try setting a known browser user agent with:\nfrom urllib.request import Request, urlopen\n\nreq = Request(\n    url='http://www.cmegroup.com/trading/products/#sortField=oi&sortAsc=false&venues=3&page=1&cleared=1&group=1', \n    headers={'User-Agent': 'Mozilla/5.0'}\n)\nwebpage = urlopen(req).read()\n\nThis works for me.\nBy the way, in your code you are missing the () after .read in the urlopen line, but I think that it's a typo.\nTIP: since this is exercise, choose a different, non restrictive site. Maybe they are blocking urllib for some reason...\n"", '\nDefinitely it\'s blocking because of your use of urllib based on the user agent. This same thing is happening to me with OfferUp. You can create a new class called AppURLopener which overrides the user-agent with Mozilla. \nimport urllib.request\n\nclass AppURLopener(urllib.request.FancyURLopener):\n    version = ""Mozilla/5.0""\n\nopener = AppURLopener()\nresponse = opener.open(\'http://httpbin.org/user-agent\')\n\nSource\n', '\n""This is probably because of mod_security or some similar server security feature which blocks known\n\nspider/bot\n\nuser agents (urllib uses something like python urllib/3.3.0, it\'s easily detected)"" - as already mentioned by Stefano Sanfilippo\nfrom urllib.request import Request, urlopen\nurl=""https://stackoverflow.com/search?q=html+error+403""\nreq = Request(url, headers={\'User-Agent\': \'Mozilla/5.0\'})\n\nweb_byte = urlopen(req).read()\n\nwebpage = web_byte.decode(\'utf-8\')\n\nThe web_byte is a byte object returned by the server and the content type present in webpage is mostly utf-8.\nTherefore you need to decode web_byte using decode method.\nThis solves complete problem while I was having trying to scrape from a website using PyCharm\nP.S -> I use python 3.4\n', ""\nBased on previous answers this has worked for me with Python 3.7 by increasing the timeout to 10.\nfrom urllib.request import Request, urlopen\n\nreq = Request('Url_Link', headers={'User-Agent': 'XYZ/3.0'})\nwebpage = urlopen(req, timeout=10).read()\n\nprint(webpage)\n\n"", '\nAdding cookie to the request headers worked for me\nfrom urllib.request import Request, urlopen\n\n# Function to get the page content\ndef get_page_content(url, head):\n  """"""\n  Function to get the page content\n  """"""\n  req = Request(url, headers=head)\n  return urlopen(req)\n\nurl = \'https://example.com\'\nhead = {\n  \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36\',\n  \'Accept\': \'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\',\n  \'Accept-Charset\': \'ISO-8859-1,utf-8;q=0.7,*;q=0.3\',\n  \'Accept-Encoding\': \'none\',\n  \'Accept-Language\': \'en-US,en;q=0.8\',\n  \'Connection\': \'keep-alive\',\n  \'refere\': \'https://example.com\',\n  \'cookie\': """"""your cookie value ( you can get that from your web page) """"""\n}\n\ndata = get_page_content(url, head).read()\nprint(data)\n\n', ""\nSince the page works in browser and not when calling within python program, it seems that the web app that serves that url recognizes that you request the content not by the browser.\nDemonstration:\ncurl --dump-header r.txt http://www.cmegroup.com/trading/products/#sortField=oi&sortAsc=false&venues=3&page=1&cleared=1&group=1\n\n...\n<HTML><HEAD>\n<TITLE>Access Denied</TITLE>\n</HEAD><BODY>\n<H1>Access Denied</H1>\nYou don't have permission to access ...\n</HTML>\n\nand the content in r.txt has status line:\nHTTP/1.1 403 Forbidden\n\nTry posting header 'User-Agent' which fakes web client.\nNOTE: The page contains Ajax call that creates the table you probably want to parse. You'll need to check the javascript logic of the page or simply using browser debugger (like Firebug / Net tab) to see which url you need to call to get the table's content.\n"", ""\nIf you feel guilty about faking the user-agent as Mozilla (comment in the top answer from Stefano), it could work with a non-urllib User-Agent as well. This worked for the sites I reference:\n    req = urlrequest.Request(link, headers={'User-Agent': 'XYZ/3.0'})\n    urlrequest.urlopen(req, timeout=10).read()\n\nMy application is to test validity by scraping specific links that I refer to, in my articles. Not a generic scraper.\n"", ""\nYou can try in two ways. The detail is in this link. \n1) Via pip\n\npip install --upgrade certifi\n\n2) If it doesn't work, try to run a Cerificates.command that comes bundled with Python 3.* for Mac:(Go to your python installation location and double click the file)\n\nopen /Applications/Python\\ 3.*/Install\\ Certificates.command\n\n"", '\nI ran into this same problem and was not able to solve it using the answers above. I ended up getting around the issue by using requests.get() and then using the .text of the result instead of using read():\nfrom requests import get\n\nreq = get(link)\nresult = req.text\n\n', '\nyou can use urllib\'s build_opener like this:\nopener = urllib.request.build_opener()\nopener.addheaders = [(\'User-Agent\', \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\'), (\'Accept\',\'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\'), (\'Accept-Encoding\',\'gzip, deflate, br\'),\\\n    (\'Accept-Language\',\'en-US,en;q=0.5\' ), (""Connection"", ""keep-alive""), (""Upgrade-Insecure-Requests"",\'1\')]\nurllib.request.install_opener(opener)\nurllib.request.urlretrieve(url, ""test.xlsx"")\n\n', '\nI pulled my hair out with this for a while and the answer ended up being pretty simple. I checked the response text and I was getting ""URL signature expired"" which is a message you wouldn\'t normally see unless you checked the response text.\nThis means some URLs just expire, usually for security purposes. Try to get the URL again and update the URL in your script. If there isn\'t a new URL for the content you\'re trying to scrape, then unfortunately you can\'t scrape for it.\n', '\nOpen the developer tools and open the network tap. chose among the items u want yo scrap, the expanding details will have the user agent and add it there\n']",https://stackoverflow.com/questions/16627227/problem-http-error-403-in-python-3-web-scraping,web-scraping
How can I download a file on a click event using selenium?,"
I am working on python and selenium. I want to download file from clicking event using selenium. I wrote following code.  
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.common.keys import Keys

browser = webdriver.Firefox()
browser.get(""http://www.drugcite.com/?q=ACTIMMUNE"")

browser.close()

I want to download both files from links with name ""Export Data"" from given url. How can I achieve it as it works with click event only?
",123k,"
            58
        ","['\nFind the link using find_element(s)_by_*, then call click method.\nfrom selenium import webdriver\n\n# To prevent download dialog\nprofile = webdriver.FirefoxProfile()\nprofile.set_preference(\'browser.download.folderList\', 2) # custom location\nprofile.set_preference(\'browser.download.manager.showWhenStarting\', False)\nprofile.set_preference(\'browser.download.dir\', \'/tmp\')\nprofile.set_preference(\'browser.helperApps.neverAsk.saveToDisk\', \'text/csv\')\n\nbrowser = webdriver.Firefox(profile)\nbrowser.get(""http://www.drugcite.com/?q=ACTIMMUNE"")\n\nbrowser.find_element_by_id(\'exportpt\').click()\nbrowser.find_element_by_id(\'exporthlgt\').click()\n\nAdded profile manipulation code to prevent download dialog.\n', '\nI\'ll admit this solution is a little more ""hacky"" than the Firefox Profile saveToDisk alternative, but it works across both Chrome and Firefox, and doesn\'t rely on a browser-specific feature which could change at any time. And if nothing else, maybe this will give someone a little different perspective on how to solve future challenges.\nPrerequisites: Ensure you have selenium and pyvirtualdisplay installed...\n\nPython 2: sudo pip install selenium pyvirtualdisplay\nPython 3: sudo pip3 install selenium pyvirtualdisplay\n\nThe Magic\nimport pyvirtualdisplay\nimport selenium\nimport selenium.webdriver\nimport time\nimport base64\nimport json\n\nroot_url = \'https://www.google.com\'\ndownload_url = \'https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png\'\n\nprint(\'Opening virtual display\')\ndisplay = pyvirtualdisplay.Display(visible=0, size=(1280, 1024,))\ndisplay.start()\nprint(\'\\tDone\')\n\nprint(\'Opening web browser\')\ndriver = selenium.webdriver.Firefox()\n#driver = selenium.webdriver.Chrome() # Alternately, give Chrome a try\nprint(\'\\tDone\')\n\nprint(\'Retrieving initial web page\')\ndriver.get(root_url)\nprint(\'\\tDone\')\n\nprint(\'Injecting retrieval code into web page\')\ndriver.execute_script(""""""\n    window.file_contents = null;\n    var xhr = new XMLHttpRequest();\n    xhr.responseType = \'blob\';\n    xhr.onload = function() {\n        var reader  = new FileReader();\n        reader.onloadend = function() {\n            window.file_contents = reader.result;\n        };\n        reader.readAsDataURL(xhr.response);\n    };\n    xhr.open(\'GET\', %(download_url)s);\n    xhr.send();\n"""""".replace(\'\\r\\n\', \' \').replace(\'\\r\', \' \').replace(\'\\n\', \' \') % {\n    \'download_url\': json.dumps(download_url),\n})\n\nprint(\'Looping until file is retrieved\')\ndownloaded_file = None\nwhile downloaded_file is None:\n    # Returns the file retrieved base64 encoded (perfect for downloading binary)\n    downloaded_file = driver.execute_script(\'return (window.file_contents !== null ? window.file_contents.split(\\\',\\\')[1] : null);\')\n    print(downloaded_file)\n    if not downloaded_file:\n        print(\'\\tNot downloaded, waiting...\')\n        time.sleep(0.5)\nprint(\'\\tDone\')\n\nprint(\'Writing file to disk\')\nfp = open(\'google-logo.png\', \'wb\')\nfp.write(base64.b64decode(downloaded_file))\nfp.close()\nprint(\'\\tDone\')\ndriver.close() # close web browser, or it\'ll persist after python exits.\ndisplay.popen.kill() # close virtual display, or it\'ll persist after python exits.\n\nExplaination\nWe first load a URL on the domain we\'re targeting a file download from. This allows us to perform an AJAX request on that domain, without running into cross site scripting issues.\nNext, we\'re injecting some javascript into the DOM which fires off an AJAX request. Once the AJAX request returns a response, we take the response and load it into a FileReader object. From there we can extract the base64 encoded content of the file by calling readAsDataUrl(). We\'re then taking the base64 encoded content and appending it to window, a gobally accessible variable.\nFinally, because the AJAX request is asynchronous, we enter  a Python while loop waiting for the content to be appended to the window. Once it\'s appended, we decode the base64 content retrieved from the window and save it to a file.\nThis solution should work across all modern browsers supported by Selenium, and works whether text or binary, and across all mime types.\nAlternate Approach\nWhile I haven\'t tested this, Selenium does afford you the ability to wait until an element is present in the DOM. Rather than looping until a globally accessible variable is populated, you could create an element with a particular ID in the DOM and use the binding of that element as the trigger to retrieve the downloaded file.\n', ""\nIn chrome what I do is downloading the files by clicking on the links, then I open chrome://downloads page and then retrieve the downloaded files list from shadow DOM like this:\ndocs = document\n  .querySelector('downloads-manager')\n  .shadowRoot.querySelector('#downloads-list')\n  .getElementsByTagName('downloads-item')\n\nThis solution is restrained to chrome, the data also contains information like file path and download date. (note this code is from JS, may not be the correct python syntax)\n"", '\nHere is the full working code. You can use web scraping to enter the username password and other field. For getting the field names appearing on the webpage, use inspect element. Element name(Username,Password or Click Button) can be entered through class or name.\nfrom selenium import webdriver\n# Using Chrome to access web\noptions = webdriver.ChromeOptions() \noptions.add_argument(""download.default_directory=C:/Test"") # Set the download Path\ndriver = webdriver.Chrome(options=options)\n# Open the website\ntry:\n    driver.get(\'xxxx\') # Your Website Address\n    password_box = driver.find_element_by_name(\'password\')\n    password_box.send_keys(\'xxxx\') #Password\n    download_button = driver.find_element_by_class_name(\'link_w_pass\')\n    download_button.click()\n    driver.quit()\nexcept:\n    driver.quit()\n    print(""Faulty URL"")\n\n']",https://stackoverflow.com/questions/18439851/how-can-i-download-a-file-on-a-click-event-using-selenium,web-scraping
Using BeautifulSoup to extract text without tags,"
My webpage looks like this:
<p>
  <strong class=""offender"">YOB:</strong> 1987<br/>
  <strong class=""offender"">RACE:</strong> WHITE<br/>
  <strong class=""offender"">GENDER:</strong> FEMALE<br/>
  <strong class=""offender"">HEIGHT:</strong> 5'05''<br/>
  <strong class=""offender"">WEIGHT:</strong> 118<br/>
  <strong class=""offender"">EYE COLOR:</strong> GREEN<br/>
  <strong class=""offender"">HAIR COLOR:</strong> BROWN<br/>
</p>

I want to extract the info for each individual and get YOB:1987, RACE:WHITE, etc...
What I tried is:
subc = soup.find_all('p')
subc1 = subc[1]
subc2 = subc1.find_all('strong')

But this gives me only the values of YOB:, RACE:, etc...
Is there a way that I can get the data in YOB:1987, RACE:WHITE format?
",179k,"
            64
        ","['\nJust loop through all the <strong> tags and use next_sibling to get what you want. Like this:\nfor strong_tag in soup.find_all(\'strong\'):\n    print(strong_tag.text, strong_tag.next_sibling)\n\nDemo:\nfrom bs4 import BeautifulSoup\n\nhtml = \'\'\'\n<p>\n  <strong class=""offender"">YOB:</strong> 1987<br />\n  <strong class=""offender"">RACE:</strong> WHITE<br />\n  <strong class=""offender"">GENDER:</strong> FEMALE<br />\n  <strong class=""offender"">HEIGHT:</strong> 5\'05\'\'<br />\n  <strong class=""offender"">WEIGHT:</strong> 118<br />\n  <strong class=""offender"">EYE COLOR:</strong> GREEN<br />\n  <strong class=""offender"">HAIR COLOR:</strong> BROWN<br />\n</p>\n\'\'\'\n\nsoup = BeautifulSoup(html)\n\nfor strong_tag in soup.find_all(\'strong\'):\n    print(strong_tag.text, strong_tag.next_sibling)\n\nThis gives you:\nYOB:  1987\nRACE:  WHITE\nGENDER:  FEMALE\nHEIGHT:  5\'05\'\'\nWEIGHT:  118\nEYE COLOR:  GREEN\nHAIR COLOR:  BROWN\n\n', '\nI think you can get it using subc1.text.\n>>> html = """"""\n<p>\n    <strong class=""offender"">YOB:</strong> 1987<br />\n    <strong class=""offender"">RACE:</strong> WHITE<br />\n    <strong class=""offender"">GENDER:</strong> FEMALE<br />\n    <strong class=""offender"">HEIGHT:</strong> 5\'05\'\'<br />\n    <strong class=""offender"">WEIGHT:</strong> 118<br />\n    <strong class=""offender"">EYE COLOR:</strong> GREEN<br />\n    <strong class=""offender"">HAIR COLOR:</strong> BROWN<br />\n</p>\n""""""\n>>> from bs4 import BeautifulSoup\n>>> soup = BeautifulSoup(html)\n>>> print soup.text\n\n\nYOB: 1987\nRACE: WHITE\nGENDER: FEMALE\nHEIGHT: 5\'05\'\'\nWEIGHT: 118\nEYE COLOR: GREEN\nHAIR COLOR: BROWN\n\nOr if you want to explore it, you can use .contents :\n>>> p = soup.find(\'p\')\n>>> from pprint import pprint\n>>> pprint(p.contents)\n[u\'\\n\',\n <strong class=""offender"">YOB:</strong>,\n u\' 1987\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">RACE:</strong>,\n u\' WHITE\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">GENDER:</strong>,\n u\' FEMALE\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">HEIGHT:</strong>,\n u"" 5\'05\'\'"",\n <br/>,\n u\'\\n\',\n <strong class=""offender"">WEIGHT:</strong>,\n u\' 118\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">EYE COLOR:</strong>,\n u\' GREEN\',\n <br/>,\n u\'\\n\',\n <strong class=""offender"">HAIR COLOR:</strong>,\n u\' BROWN\',\n <br/>,\n u\'\\n\']\n\nand filter out the necessary items from the list:\n>>> data = dict(zip([x.text for x in p.contents[1::4]], [x.strip() for x in p.contents[2::4]]))\n>>> pprint(data)\n{u\'EYE COLOR:\': u\'GREEN\',\n u\'GENDER:\': u\'FEMALE\',\n u\'HAIR COLOR:\': u\'BROWN\',\n u\'HEIGHT:\': u""5\'05\'\'"",\n u\'RACE:\': u\'WHITE\',\n u\'WEIGHT:\': u\'118\',\n u\'YOB:\': u\'1987\'}\n\n', '\nyou can try this indside findall for loop:\nitem_price = item.find(\'span\', attrs={\'class\':\'s-item__price\'}).text\n\nit extracts only text and assigs it to ""item_pice""\n', '\nI think you could solve this with .strip() in gazpacho:\nInput:\nhtml = """"""\\\n<p>\n  <strong class=""offender"">YOB:</strong> 1987<br />\n  <strong class=""offender"">RACE:</strong> WHITE<br />\n  <strong class=""offender"">GENDER:</strong> FEMALE<br />\n  <strong class=""offender"">HEIGHT:</strong> 5\'05\'\'<br />\n  <strong class=""offender"">WEIGHT:</strong> 118<br />\n  <strong class=""offender"">EYE COLOR:</strong> GREEN<br />\n  <strong class=""offender"">HAIR COLOR:</strong> BROWN<br />\n</p>\n""""""\n\nCode:\nsoup = Soup(html)\ntext = soup.find(""p"").strip(whitespace=False) # to keep \\n characters intact\nlines = [\n    line.strip()\n    for line in text.split(""\\n"")\n    if line != """"\n]\ndata = dict([line.split("": "") for line in lines])\n\nOutput:\nprint(data)\n# {\'YOB\': \'1987\',\n#  \'RACE\': \'WHITE\',\n#  \'GENDER\': \'FEMALE\',\n#  \'HEIGHT\': ""5\'05\'\'"",\n#  \'WEIGHT\': \'118\',\n#  \'EYE COLOR\': \'GREEN\',\n#  \'HAIR COLOR\': \'BROWN\'}\n\n']",https://stackoverflow.com/questions/23380171/using-beautifulsoup-to-extract-text-without-tags,web-scraping
Headless browser for C# (.NET)? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



I am (was) a Python developer who is building a GUI web scraping application. Recently I've decided to migrate to .NET framework and write the same application in C# (this decision wasn't mine).
In Python, I've used the Mechanize library. However, I can't seem to find anything similar in .NET. What I need is a browser that will run in a headless mode, which has the ability to fill out forms, submit them, etc. JavaScript parser is not a must, but it would be quite useful. 
",57k,"
            40
        ","['\nThere are some options:\n\nWebKit.Net (free)\n\nAwesomium\nIt is based on Chrome/WebKit and works like a charm.\nThere is a free license available but also a commercial one and if need be you can buy the source code :-)\n\nHTML Agility Pack (free) (An HTML Parser library, NOT a headless browser)\nThis helps with extracting information from HTML etc. and might be useful in your case (possibly in combination with HttpWebRequest)\n\n\n', ""\nMore solutions:\n\nPhantomJS - full featured headless web\nbrowser. Often used in pair with Selenium which allows you to\naccess the browser from .NET application.\nOptimus (nuget package)- lightweight headless web browser. It's in beta but it is sufficient for some cases.\n\nI used to use both for web testing. But they are also suitable for web scraping.\n"", ""\nYou may be after TrifleJS (currently in beta), or something similar using the .NET WebBrowser class which communicates with IE via a windowless ActiveX/COM API.\nYou'll essentially be running a fully fledged browser (not a http request wrapper) using Internet Explorer's Trident engine, if you are not interested in the JavaScript API (a port of phantomjs) you may still be able to use some of the C# codebase to get around key concepts (custom headers, cookies, script execution, screenshot rendering etc). \nNote that this can also emulate different versions of IE depending on what you have installed.\n\n""]",https://stackoverflow.com/questions/10161413/headless-browser-for-c-sharp-net,web-scraping
How to convert raw javascript object to a dictionary?,"
When screen-scraping some website, I extract data from <script> tags.
The data I get is not in standard JSON format. I cannot use json.loads().
# from
js_obj = '{x:1, y:2, z:3}'

# to
py_obj = {'x':1, 'y':2, 'z':3}

Currently, I use regex to transform the raw data to JSON format.
But I feel pretty bad when I encounter complicated data structure.
Do you have some better solutions?
",30k,"
            32
        ","['\ndemjson.decode()\nimport demjson\n\n# from\njs_obj = \'{x:1, y:2, z:3}\'\n\n# to\npy_obj = demjson.decode(js_obj)\n\njsonnet.evaluate_snippet()\nimport json, _jsonnet\n\n# from\njs_obj = \'{x:1, y:2, z:3}\'\n\n# to\npy_obj = json.loads(_jsonnet.evaluate_snippet(\'snippet\', js_obj))\n\nast.literal_eval()\nimport ast\n\n# from\njs_obj = ""{\'x\':1, \'y\':2, \'z\':3}""\n\n# to\npy_obj = ast.literal_eval(js_obj)\n\n', ""\nUse json5\nimport json5\n\njs_obj = '{x:1, y:2, z:3}'\n\npy_obj = json5.loads(js_obj)\n\nprint(py_obj)\n\n# output\n# {'x': 1, 'y': 2, 'z': 3}\n\n"", ""\nI'm facing the same problem this afternoon, and I finally found a quite good solution. That is JSON5.\nThe syntax of JSON5 is more similar to native JavaScript, so it can help you parse non-standard JSON objects.\nYou might want to check pyjson5 out.\n"", '\nThis will likely not work everywhere, but as a start, here\'s a simple regex that should convert the keys into quoted strings so you can pass into json.loads.  Or is this what you\'re already doing?\nIn[70] : quote_keys_regex = r\'([\\{\\s,])(\\w+)(:)\'\n\nIn[71] : re.sub(quote_keys_regex, r\'\\1""\\2""\\3\', js_obj)\nOut[71]: \'{""x"":1, ""y"":2, ""z"":3}\'\n\nIn[72] : js_obj_2 = \'{x:1, y:2, z:{k:3,j:2}}\'\n\nInt[73]: re.sub(quote_keys_regex, r\'\\1""\\2""\\3\', js_obj_2)\nOut[73]: \'{""x"":1, ""y"":2, ""z"":{""k"":3,""j"":2}}\'\n\n', '\nIf you have node available on the system, you can ask it to evaluate the javascript expression for you, and print the stringified result. The resulting JSON can then be fed to json.loads:\ndef evaluate_javascript(s):\n    """"""Evaluate and stringify a javascript expression in node.js, and convert the\n    resulting JSON to a Python object""""""\n    node = Popen([\'node\', \'-\'], stdin=PIPE, stdout=PIPE)\n    stdout, _ = node.communicate(f\'console.log(JSON.stringify({s}))\'.encode(\'utf8\'))\n    return json.loads(stdout.decode(\'utf8\'))\n\n', '\nNot including objects \njson.loads()\n\njson.loads() doesn\'t accept undefined, you have to change to null\njson.loads() only accept double quotes\n\n\n{""foo"": 1, ""bar"": null}\n\n\nUse this if you are sure that your javascript code only have double quotes on key names.  \nimport json\n\njson_text = """"""{""foo"": 1, ""bar"": undefined}""""""\njson_text = re.sub(r\'(""\\s*:\\s*)undefined(\\s*[,}])\', \'\\\\1null\\\\2\', json_text)\n\npy_obj = json.loads(json_text)\n\nast.literal_eval()\n\nast.literal_eval() doesn\'t accept undefined, you have to change to None\nast.literal_eval() doesn\'t accept null, you have to change to None\nast.literal_eval() doesn\'t accept true, you have to change to True\nast.literal_eval() doesn\'t accept false, you have to change to False\nast.literal_eval() accept single and double quotes\n\n\n{""foo"": 1, ""bar"": None} or {\'foo\': 1, \'bar\': None}\n\n\nimport ast\n\njs_obj = """"""{\'foo\': 1, \'bar\': undefined}""""""\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)undefined(\\s*[,}])\', \'\\\\1None\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)null(\\s*[,}])\', \'\\\\1None\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)NaN(\\s*[,}])\', \'\\\\1None\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)true(\\s*[,}])\', \'\\\\1True\\\\2\', js_obj)\njs_obj = re.sub(r\'([\\\'\\""]\\s*:\\s*)false(\\s*[,}])\', \'\\\\1False\\\\2\', js_obj)\n\npy_obj = ast.literal_eval(js_obj) \n\n']",https://stackoverflow.com/questions/24027589/how-to-convert-raw-javascript-object-to-a-dictionary,web-scraping
How to save an image locally using Python whose URL address I already know?,"
I know the URL of an image on Internet.
e.g. http://www.digimouth.com/news/media/2011/09/google-logo.jpg, which contains the logo of Google.
Now, how can I download this image using Python without actually opening the URL in a browser and saving the file manually.
",336k,"
            199
        ","['\nPython 2\nHere is a more straightforward way if all you want to do is save it as a file:\nimport urllib\n\nurllib.urlretrieve(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"", ""local-filename.jpg"")\n\nThe second argument is the local path where the file should be saved.\nPython 3\nAs SergO suggested the  code below should work with Python 3.\nimport urllib.request\n\nurllib.request.urlretrieve(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"", ""local-filename.jpg"")\n\n', '\nimport urllib\nresource = urllib.urlopen(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"")\noutput = open(""file01.jpg"",""wb"")\noutput.write(resource.read())\noutput.close()\n\nfile01.jpg will contain your image. \n', '\nI wrote a script that does just this, and it is available on my github for your use. \nI utilized BeautifulSoup to allow me to parse any website for images. If you will be doing much web scraping (or intend to use my tool) I suggest you sudo pip install BeautifulSoup. Information on BeautifulSoup is available here.\nFor convenience here is my code:\nfrom bs4 import BeautifulSoup\nfrom urllib2 import urlopen\nimport urllib\n\n# use this image scraper from the location that \n#you want to save scraped images to\n\ndef make_soup(url):\n    html = urlopen(url).read()\n    return BeautifulSoup(html)\n\ndef get_images(url):\n    soup = make_soup(url)\n    #this makes a list of bs4 element tags\n    images = [img for img in soup.findAll(\'img\')]\n    print (str(len(images)) + ""images found."")\n    print \'Downloading images to current working directory.\'\n    #compile our unicode list of image links\n    image_links = [each.get(\'src\') for each in images]\n    for each in image_links:\n        filename=each.split(\'/\')[-1]\n        urllib.urlretrieve(each, filename)\n    return image_links\n\n#a standard call looks like this\n#get_images(\'http://www.wookmark.com\')\n\n', ""\nThis can be done with requests. Load the page and dump the binary content to a file.\nimport os\nimport requests\n\nurl = 'https://apod.nasa.gov/apod/image/1701/potw1636aN159_HST_2048.jpg'\npage = requests.get(url)\n\nf_ext = os.path.splitext(url)[-1]\nf_name = 'img{}'.format(f_ext)\nwith open(f_name, 'wb') as f:\n    f.write(page.content)\n\n"", '\nPython 3\nurllib.request — Extensible library for opening URLs\nfrom urllib.error import HTTPError\nfrom urllib.request import urlretrieve\n\ntry:\n    urlretrieve(image_url, image_local_path)\nexcept FileNotFoundError as err:\n    print(err)   # something wrong with local path\nexcept HTTPError as err:\n    print(err)  # something wrong with url\n\n', '\nI made a script expanding on Yup.\'s script. I fixed some things. It will now bypass 403:Forbidden problems. It wont crash when an image fails to be retrieved. It tries to avoid corrupted previews. It gets the right absolute urls. It gives out more information. It can be run with an argument from the command line. \n# getem.py\n# python2 script to download all images in a given url\n# use: python getem.py http://url.where.images.are\n\nfrom bs4 import BeautifulSoup\nimport urllib2\nimport shutil\nimport requests\nfrom urlparse import urljoin\nimport sys\nimport time\n\ndef make_soup(url):\n    req = urllib2.Request(url, headers={\'User-Agent\' : ""Magic Browser""}) \n    html = urllib2.urlopen(req)\n    return BeautifulSoup(html, \'html.parser\')\n\ndef get_images(url):\n    soup = make_soup(url)\n    images = [img for img in soup.findAll(\'img\')]\n    print (str(len(images)) + "" images found."")\n    print \'Downloading images to current working directory.\'\n    image_links = [each.get(\'src\') for each in images]\n    for each in image_links:\n        try:\n            filename = each.strip().split(\'/\')[-1].strip()\n            src = urljoin(url, each)\n            print \'Getting: \' + filename\n            response = requests.get(src, stream=True)\n            # delay to avoid corrupted previews\n            time.sleep(1)\n            with open(filename, \'wb\') as out_file:\n                shutil.copyfileobj(response.raw, out_file)\n        except:\n            print \'  An error occured. Continuing.\'\n    print \'Done.\'\n\nif __name__ == \'__main__\':\n    url = sys.argv[1]\n    get_images(url)\n\n', '\nA solution which works with Python 2 and Python 3:\ntry:\n    from urllib.request import urlretrieve  # Python 3\nexcept ImportError:\n    from urllib import urlretrieve  # Python 2\n\nurl = ""http://www.digimouth.com/news/media/2011/09/google-logo.jpg""\nurlretrieve(url, ""local-filename.jpg"")\n\nor, if the additional requirement of requests is acceptable and if it is a http(s) URL:\ndef load_requests(source_url, sink_path):\n    """"""\n    Load a file from an URL (e.g. http).\n\n    Parameters\n    ----------\n    source_url : str\n        Where to load the file from.\n    sink_path : str\n        Where the loaded file is stored.\n    """"""\n    import requests\n    r = requests.get(source_url, stream=True)\n    if r.status_code == 200:\n        with open(sink_path, \'wb\') as f:\n            for chunk in r:\n                f.write(chunk)\n\n', ""\nUsing requests library\nimport requests\nimport shutil,os\n\nheaders = {\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'\n}\ncurrentDir = os.getcwd()\npath = os.path.join(currentDir,'Images')#saving images to Images folder\n\ndef ImageDl(url):\n    attempts = 0\n    while attempts < 5:#retry 5 times\n        try:\n            filename = url.split('/')[-1]\n            r = requests.get(url,headers=headers,stream=True,timeout=5)\n            if r.status_code == 200:\n                with open(os.path.join(path,filename),'wb') as f:\n                    r.raw.decode_content = True\n                    shutil.copyfileobj(r.raw,f)\n            print(filename)\n            break\n        except Exception as e:\n            attempts+=1\n            print(e)\n\n\nImageDl(url)\n\n"", ""\nUse a simple python wget module to download the link. Usage below:\nimport wget\nwget.download('http://www.digimouth.com/news/media/2011/09/google-logo.jpg')\n\n"", '\nThis is very short answer.\nimport urllib\nurllib.urlretrieve(""http://photogallery.sandesh.com/Picture.aspx?AlubumId=422040"", ""Abc.jpg"")\n\n', '\nVersion for Python 3\nI adjusted the code of @madprops for Python 3\n# getem.py\n# python2 script to download all images in a given url\n# use: python getem.py http://url.where.images.are\n\nfrom bs4 import BeautifulSoup\nimport urllib.request\nimport shutil\nimport requests\nfrom urllib.parse import urljoin\nimport sys\nimport time\n\ndef make_soup(url):\n    req = urllib.request.Request(url, headers={\'User-Agent\' : ""Magic Browser""}) \n    html = urllib.request.urlopen(req)\n    return BeautifulSoup(html, \'html.parser\')\n\ndef get_images(url):\n    soup = make_soup(url)\n    images = [img for img in soup.findAll(\'img\')]\n    print (str(len(images)) + "" images found."")\n    print(\'Downloading images to current working directory.\')\n    image_links = [each.get(\'src\') for each in images]\n    for each in image_links:\n        try:\n            filename = each.strip().split(\'/\')[-1].strip()\n            src = urljoin(url, each)\n            print(\'Getting: \' + filename)\n            response = requests.get(src, stream=True)\n            # delay to avoid corrupted previews\n            time.sleep(1)\n            with open(filename, \'wb\') as out_file:\n                shutil.copyfileobj(response.raw, out_file)\n        except:\n            print(\'  An error occured. Continuing.\')\n    print(\'Done.\')\n\nif __name__ == \'__main__\':\n    get_images(\'http://www.wookmark.com\')\n\n', '\nLate answer, but for python>=3.6 you can use dload, i.e.:\nimport dload\ndload.save(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"")\n\nif you need the image as bytes, use:\nimg_bytes = dload.bytes(""http://www.digimouth.com/news/media/2011/09/google-logo.jpg"")\n\n\ninstall using pip3 install dload\n', '\nSomething fresh for Python 3 using Requests:\nComments in the code. Ready to use function.\n\nimport requests\nfrom os import path\n\ndef get_image(image_url):\n    """"""\n    Get image based on url.\n    :return: Image name if everything OK, False otherwise\n    """"""\n    image_name = path.split(image_url)[1]\n    try:\n        image = requests.get(image_url)\n    except OSError:  # Little too wide, but work OK, no additional imports needed. Catch all conection problems\n        return False\n    if image.status_code == 200:  # we could have retrieved error page\n        base_dir = path.join(path.dirname(path.realpath(__file__)), ""images"") # Use your own path or """" to use current working directory. Folder must exist.\n        with open(path.join(base_dir, image_name), ""wb"") as f:\n            f.write(image.content)\n        return image_name\n\nget_image(""https://apod.nasddfda.gov/apod/image/2003/S106_Mishra_1947.jpg"")\n\n\n', ""\nthis is the easiest method to download images.\nimport requests\nfrom slugify import slugify\n\nimg_url = 'https://apod.nasa.gov/apod/image/1701/potw1636aN159_HST_2048.jpg'\nimg = requests.get(img_url).content\nimg_file = open(slugify(img_url) + '.' + str(img_url).split('.')[-1], 'wb')\nimg_file.write(img)\nimg_file.close()\n\n"", '\nIf you don\'t already have the url for the image, you could scrape it with gazpacho:\nfrom gazpacho import Soup\nbase_url = ""http://books.toscrape.com""\n\nsoup = Soup.get(base_url)\nlinks = [img.attrs[""src""] for img in soup.find(""img"")]\n\nAnd then download the asset with urllib as mentioned:\nfrom pathlib import Path\nfrom urllib.request import urlretrieve as download\n\ndirectory = ""images""\nPath(directory).mkdir(exist_ok=True)\n\nlink = links[0]\nname = link.split(""/"")[-1]\n\ndownload(f""{base_url}/{link}"", f""{directory}/{name}"")\n\n', '\n# import the required libraries from Python\nimport pathlib,urllib.request \n\n# Using pathlib, specify where the image is to be saved\ndownloads_path = str(pathlib.Path.home() / ""Downloads"")\n\n# Form a full image path by joining the path to the \n# images\' new name\n\npicture_path  = os.path.join(downloads_path, ""new-image.png"")\n\n# ""/home/User/Downloads/new-image.png""\n\n# Using ""urlretrieve()"" from urllib.request save the image \nurllib.request.urlretrieve(""//example.com/image.png"", picture_path)\n\n# urlretrieve() takes in 2 arguments\n# 1. The URL of the image to be downloaded\n# 2. The image new name after download. By default, the image is saved\n#    inside your current working directory\n\n', '\nOk, so, this is my rudimentary attempt, and probably total overkill.\nUpdate if needed, as this doesn\'t handle any timeouts, but, I got this working for fun.\nCode listed here: https://github.com/JayRizzo/JayRizzoTools/blob/master/pyImageDownloader.py\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# =============================================================================\n# Created Syst: MAC OSX High Sierra 21.5.0 (17G65)\n# Created Plat: Python 3.9.5 (\'v3.9.5:0a7dcbdb13\', \'May  3 2021 13:17:02\')\n# Created By  : Jeromie Kirchoff\n# Created Date: Thu Jun 15 23:31:01 2022 CDT\n# Last ModDate: Thu Jun 16 01:41:01 2022 CDT\n# =============================================================================\n# NOTE: Doesn\'t work on SVG images at this time.\n# I will look into this further: https://stackoverflow.com/a/6599172/1896134\n# =============================================================================\nimport requests                                 # to get image from the web\nimport shutil                                   # to save it locally\nimport os                                       # needed\nfrom os.path import exists as filepathexist     # check if file paths exist\nfrom os.path import join                        # joins path for different os\nfrom os.path import expanduser                  # expands current home\nfrom pyuser_agent import UA                     # generates random UserAgent\n\nclass ImageDownloader(object):\n    """"""URL ImageDownloader.\n    Input : Full Image URL\n    Output: Image saved to your ~/Pictures/JayRizzoDL folder.\n    """"""\n    def __init__(self, URL: str):\n        self.url = URL\n        self.headers = {""User-Agent"" : UA().random}\n        self.currentHome = expanduser(\'~\')\n        self.desktop = join(self.currentHome + ""/Desktop/"")\n        self.download = join(self.currentHome + ""/Downloads/"")\n        self.pictures = join(self.currentHome + ""/Pictures/JayRizzoDL/"")\n        self.outfile = """"\n        self.filename = """"\n        self.response = """"\n        self.rawstream = """"\n        self.createdfilepath = """"\n        self.imgFileName = """"\n        # Check if the JayRizzoDL exists in the pictures folder.\n        # if it doesn\'t exist create it.\n        if not filepathexist(self.pictures):\n            os.mkdir(self.pictures)\n        self.main()\n\n    def getFileNameFromURL(self, URL: str):\n        """"""Parse the URL for the name after the last forward slash.""""""\n        NewFileName = self.url.strip().split(\'/\')[-1].strip()\n        return NewFileName\n\n    def getResponse(self, URL: str):\n        """"""Try streaming the URL for the raw data.""""""\n        self.response = requests.get(self.url, headers=self.headers, stream=True)\n        return self.response\n\n    def gocreateFile(self, name: str, response):\n        """"""Try creating the file with the raw data in a custom folder.""""""\n        self.outfile = join(self.pictures, name)\n        with open(self.outfile, \'wb\') as outFilePath:\n            shutil.copyfileobj(response.raw, outFilePath)\n        return self.outfile\n\n    def main(self):\n        """"""Combine Everything and use in for loops.""""""\n        self.filename = self.getFileNameFromURL(self.url)\n        self.rawstream = self.getResponse(self.url)\n        self.createdfilepath = self.gocreateFile(self.filename, self.rawstream)\n        print(f""File was created: {self.createdfilepath}"")\n        return\n\nif __name__ == \'__main__\':\n    # Example when calling the file directly.\n    ImageDownloader(""https://stackoverflow.design/assets/img/logos/so/logo-stackoverflow.png"")\n\n\n', '\nDownload Image file, with avoiding all possible error:\nimport requests\nimport validators\nfrom urllib.request import Request, urlopen\nfrom urllib.error import URLError, HTTPError\n\n\ndef is_downloadable(url):\n  valid=validators. url(url)\n  if valid==False:\n    return False\n  req = Request(url)\n  try:\n    response = urlopen(req)\n  except HTTPError as e:\n    return False\n  except URLError as e:\n    return False\n  else:\n    return True\n\n\n\nfor i in range(len(File_data)):   #File data Contain list of address for image \n                                                      #file\n  url = File_data[i][1]\n  try:\n    if (is_downloadable(url)):\n      try:\n        r = requests.get(url, allow_redirects=True)\n        if url.find(\'/\'):\n          fname = url.rsplit(\'/\', 1)[1]\n          fname = pth+File_data[i][0]+""$""+fname #Destination to save \n                                                   #image file\n          open(fname, \'wb\').write(r.content)\n      except Exception as e:\n        print(e)\n  except Exception as e:\n    print(e)\n\n']",https://stackoverflow.com/questions/8286352/how-to-save-an-image-locally-using-python-whose-url-address-i-already-know,web-scraping
How to scrape a website which requires login using python and beautifulsoup?,"
If I want to scrape a website that requires login with password first, how can I start scraping it with python using beautifulsoup4 library? Below is what I do for websites that do not require login. 
from bs4 import BeautifulSoup    
import urllib2 
url = urllib2.urlopen(""http://www.python.org"")    
content = url.read()    
soup = BeautifulSoup(content)

How should the code be changed to accommodate login? Assume that the website I want to scrape is a forum that requires login. An example is http://forum.arduino.cc/index.php
",137k,"
            94
        ","['\nYou can use mechanize:\nimport mechanize\nfrom bs4 import BeautifulSoup\nimport urllib2 \nimport cookielib ## http.cookiejar in python3\n\ncj = cookielib.CookieJar()\nbr = mechanize.Browser()\nbr.set_cookiejar(cj)\nbr.open(""https://id.arduino.cc/auth/login/"")\n\nbr.select_form(nr=0)\nbr.form[\'username\'] = \'username\'\nbr.form[\'password\'] = \'password.\'\nbr.submit()\n\nprint br.response().read()\n\nOr urllib - Login to website using urllib2\n', ""\nThere is a simpler way, from my pov, that gets you there without selenium or mechanize, or other 3rd party tools, albeit it is semi-automated.\nBasically, when you login into a site in a normal way, you identify yourself in a unique way using your credentials, and the same identity is used  thereafter for every other interaction, which is stored in cookies and headers, for a brief period of time.\nWhat you need to do is use the same cookies and headers when you make your http requests, and you'll be in.\nTo replicate that, follow these steps:\n\nIn your browser, open the developer tools\nGo to the site, and login\nAfter the login, go to the network tab, and then refresh the page\nAt this point, you should see a list of requests, the top one being the actual site - and that will be our focus, because it contains the data with the identity we can use for Python and BeautifulSoup to scrape it\nRight click the site request (the top one), hover over copy, and then copy as \ncURL\nLike this:\n\n\n\nThen go to this site which converts cURL into python requests: https://curl.trillworks.com/\nTake the python code and use the generated cookies and headers to proceed with the scraping\n\n"", '\nIf you go for selenium, then you can do something like below:\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\n\n# If you want to open Chrome\ndriver = webdriver.Chrome()\n# If you want to open Firefox\ndriver = webdriver.Firefox()\n\nusername = driver.find_element_by_id(""username"")\npassword = driver.find_element_by_id(""password"")\nusername.send_keys(""YourUsername"")\npassword.send_keys(""YourPassword"")\ndriver.find_element_by_id(""submit_btn"").click()\n\nHowever, if you\'re adamant that you\'re only going to use BeautifulSoup, you can do that with a library like requests or urllib. Basically all you have to do is POST the data as a payload with the URL.\nimport requests\nfrom bs4 import BeautifulSoup\n\nlogin_url = \'http://example.com/login\'\ndata = {\n    \'username\': \'your_username\',\n    \'password\': \'your_password\'\n}\n\nwith requests.Session() as s:\n    response = s.post(login_url , data)\n    print(response.text)\n    index_page= s.get(\'http://example.com\')\n    soup = BeautifulSoup(index_page.text, \'html.parser\')\n    print(soup.title)\n\n', '\nYou can use selenium to log in and retrieve the page source, which you can then pass to Beautiful Soup to extract the data you want.\n', '\nSince Python version wasn\'t specified, here is my take on it for Python 3, done without any external libraries (StackOverflow). After login use BeautifulSoup as usual, or any other kind of scraping.\nLikewise, script on my GitHub here\nWhole script replicated below as to StackOverflow guidelines:\n# Login to website using just Python 3 Standard Library\nimport urllib.parse\nimport urllib.request\nimport http.cookiejar\n\ndef scraper_login():\n    ####### change variables here, like URL, action URL, user, pass\n    # your base URL here, will be used for headers and such, with and without https://\n    base_url = \'www.example.com\'\n    https_base_url = \'https://\' + base_url\n\n    # here goes URL that\'s found inside form action=\'.....\'\n    #   adjust as needed, can be all kinds of weird stuff\n    authentication_url = https_base_url + \'/login\'\n\n    # username and password for login\n    username = \'yourusername\'\n    password = \'SoMePassw0rd!\'\n\n    # we will use this string to confirm a login at end\n    check_string = \'Logout\'\n\n    ####### rest of the script is logic\n    # but you will need to tweak couple things maybe regarding ""token"" logic\n    #   (can be _token or token or _token_ or secret ... etc)\n\n    # big thing! you need a referer for most pages! and correct headers are the key\n    headers={""Content-Type"":""application/x-www-form-urlencoded"",\n    ""User-agent"":""Mozilla/5.0 Chrome/81.0.4044.92"",    # Chrome 80+ as per web search\n    ""Host"":base_url,\n    ""Origin"":https_base_url,\n    ""Referer"":https_base_url}\n\n    # initiate the cookie jar (using : http.cookiejar and urllib.request)\n    cookie_jar = http.cookiejar.CookieJar()\n    opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar))\n    urllib.request.install_opener(opener)\n\n    # first a simple request, just to get login page and parse out the token\n    #       (using : urllib.request)\n    request = urllib.request.Request(https_base_url)\n    response = urllib.request.urlopen(request)\n    contents = response.read()\n\n    # parse the page, we look for token eg. on my page it was something like this:\n    #    <input type=""hidden"" name=""_token"" value=""random1234567890qwertzstring"">\n    #       this can probably be done better with regex and similar\n    #       but I\'m newb, so bear with me\n    html = contents.decode(""utf-8"")\n    # text just before start and just after end of your token string\n    mark_start = \'<input type=""hidden"" name=""_token"" value=""\'\n    mark_end = \'"">\'\n    # index of those two points\n    start_index = html.find(mark_start) + len(mark_start)\n    end_index = html.find(mark_end, start_index)\n    # and text between them is our token, store it for second step of actual login\n    token = html[start_index:end_index]\n\n    # here we craft our payload, it\'s all the form fields, including HIDDEN fields!\n    #   that includes token we scraped earler, as that\'s usually in hidden fields\n    #   make sure left side is from ""name"" attributes of the form,\n    #       and right side is what you want to post as ""value""\n    #   and for hidden fields make sure you replicate the expected answer,\n    #       eg. ""token"" or ""yes I agree"" checkboxes and such\n    payload = {\n        \'_token\':token,\n    #    \'name\':\'value\',    # make sure this is the format of all additional fields !\n        \'login\':username,\n        \'password\':password\n    }\n\n    # now we prepare all we need for login\n    #   data - with our payload (user/pass/token) urlencoded and encoded as bytes\n    data = urllib.parse.urlencode(payload)\n    binary_data = data.encode(\'UTF-8\')\n    # and put the URL + encoded data + correct headers into our POST request\n    #   btw, despite what I thought it is automatically treated as POST\n    #   I guess because of byte encoded data field you don\'t need to say it like this:\n    #       urllib.request.Request(authentication_url, binary_data, headers, method=\'POST\')\n    request = urllib.request.Request(authentication_url, binary_data, headers)\n    response = urllib.request.urlopen(request)\n    contents = response.read()\n\n    # just for kicks, we confirm some element in the page that\'s secure behind the login\n    #   we use a particular string we know only occurs after login,\n    #   like ""logout"" or ""welcome"" or ""member"", etc. I found ""Logout"" is pretty safe so far\n    contents = contents.decode(""utf-8"")\n    index = contents.find(check_string)\n    # if we find it\n    if index != -1:\n        print(f""We found \'{check_string}\' at index position : {index}"")\n    else:\n        print(f""String \'{check_string}\' was not found! Maybe we did not login ?!"")\n\nscraper_login()\n\n']",https://stackoverflow.com/questions/23102833/how-to-scrape-a-website-which-requires-login-using-python-and-beautifulsoup,web-scraping
Python: find_element_by_css_selector,"
I am trying to click the login button with webdriver
<a class=""login-btn"" href=""javascript:;"" data-bind=""click:loginSection.loginClick"">
    <span class=""btn-text"">Login</span>
</a>

My code:
submit=driver.find_element_by_css_selector('a.login-btn').click()

or try this code:
submit=driver.find_element_by_class_name('login-btn').click()

Neither of these is working, need some advice. Thanks in advance
Error:
NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""css selector"",""selector"":""a.login-btn""}

",23k,"
            5
        ","['\nTo click on the Login button you can use either of the the following line of code :\n\nLinkText :\ndriver.find_element_by_link_text(""Login"").click()\n\nCssSelector :\ndriver.find_element_by_css_selector(""a.login-btn > span.btn-text"").click()\n\nGetting more granular with the CssSelector you can also use the following line of code :\ndriver.find_element_by_css_selector(""a.login-btn[data-bind=\'click:loginSection.loginClick\'] > span.btn-text"").click()\n\n\nUpdate :\nAs you are seeing NoSuchElementException you can check this discussion\n']",https://stackoverflow.com/questions/48578336/python-find-element-by-css-selector,web-scraping
Can bs4 get the dynamic content of a webpage if requests can't?,"
So I've tried Selenium previously and now wanted to test out bs4. I tried running the following code but recieved None as an output.
res_pewdiepie = requests.get(
    'https://www.youtube.com/user/PewDiePie')
soup = bs4.BeautifulSoup(res_pewdiepie.content, ""lxml"")
subs = soup.find(id=""sub-count"")
print(subs)

After researching for a while, I found out that requests doesn't load dynamic content like the subcount on YouTube or Socialblade.  Is there a way to get this information with bs4 or if do I have to switch back to something like Selenium?
Thanks in advance!
",624,"
            2
        ","[""\nBeautifulSoup can only parse a text you give it, in this case the page source. If the information is not there it can't do anything about it. So, I believe you have to switch back to something that supports javascript.\nSome options:\npython-selenium\nrequests-html\n"", '\nI use splash for stuff like this. You can run it in a docker container. You can tweak how long it waits for rendering on a per-request basis. There\'s also a scrapy plugin if you\'re doing any serious crawling. Here\'s a snippet from one of my crawlers, running Splash locally using Docker. Good luck.\ntarget_url = ""https://somewhere.example.com/""\nsplash_url = ""http://localhost:8050/render.json""\nbody = json.dumps({""url"": target_url, ""har"": 0, ""html"": 1, ""wait"": 10,})\nheaders = {""Content-Type"": ""application/json""}\n\nresponse = requests.post(splash_url, data=body, headers=headers)\nresult = json.loads(response.text)\nhtml = result[""html""]\n\n']",https://stackoverflow.com/questions/65265321/can-bs4-get-the-dynamic-content-of-a-webpage-if-requests-cant,web-scraping
How do you scrape AJAX pages?,"
Please advise how to scrape AJAX pages.
",74k,"
            57
        ","['\nOverview:\nAll screen scraping first requires manual review of the page you want to extract resources from.  When dealing with AJAX you usually just need to analyze a bit more than just simply the HTML. \nWhen dealing with AJAX this just means that the value you want is not in the initial HTML document that you requested, but that javascript will be exectued which asks the server for the extra information you want. \nYou can therefore usually simply analyze the javascript and see which request the javascript makes and just call this URL instead from the start. \n\nExample:\nTake this as an example, assume the page you want to scrape from has the following script:\n<script type=""text/javascript"">\nfunction ajaxFunction()\n{\nvar xmlHttp;\ntry\n  {\n  // Firefox, Opera 8.0+, Safari\n  xmlHttp=new XMLHttpRequest();\n  }\ncatch (e)\n  {\n  // Internet Explorer\n  try\n    {\n    xmlHttp=new ActiveXObject(""Msxml2.XMLHTTP"");\n    }\n  catch (e)\n    {\n    try\n      {\n      xmlHttp=new ActiveXObject(""Microsoft.XMLHTTP"");\n      }\n    catch (e)\n      {\n      alert(""Your browser does not support AJAX!"");\n      return false;\n      }\n    }\n  }\n  xmlHttp.onreadystatechange=function()\n    {\n    if(xmlHttp.readyState==4)\n      {\n      document.myForm.time.value=xmlHttp.responseText;\n      }\n    }\n  xmlHttp.open(""GET"",""time.asp"",true);\n  xmlHttp.send(null);\n  }\n</script>\n\nThen all you need to do is instead do an HTTP request to time.asp of the same server instead.   Example from w3schools.\n\nAdvanced scraping with C++: \nFor complex usage, and if you\'re using C++ you could also consider using the firefox javascript engine SpiderMonkey to execute the javascript on a page. \nAdvanced scraping with Java:\nFor complex usage, and if you\'re using Java you could also consider using the firefox javascript engine for Java Rhino\nAdvanced scraping with .NET:\nFor complex usage, and if you\'re using .Net you could also consider using the Microsoft.vsa assembly.  Recently replaced with ICodeCompiler/CodeDOM.\n', ""\nIn my opinion the simpliest solution is to use Casperjs, a framework based on the WebKit headless browser phantomjs.\nThe whole page is loaded, and it's very easy to scrape any ajax-related data.\nYou can check this basic tutorial to learn Automating & Scraping with PhantomJS and CasperJS\nYou can also give a look at this example code, on how to scrape google suggests keywords :\n/*global casper:true*/\nvar casper = require('casper').create();\nvar suggestions = [];\nvar word = casper.cli.get(0);\n\nif (!word) {\n    casper.echo('please provide a word').exit(1);\n}\n\ncasper.start('http://www.google.com/', function() {\n    this.sendKeys('input[name=q]', word);\n});\n\ncasper.waitFor(function() {\n  return this.fetchText('.gsq_a table span').indexOf(word) === 0\n}, function() {\n  suggestions = this.evaluate(function() {\n      var nodes = document.querySelectorAll('.gsq_a table span');\n      return [].map.call(nodes, function(node){\n          return node.textContent;\n      });\n  });\n});\n\ncasper.run(function() {\n  this.echo(suggestions.join('\\n')).exit();\n});\n\n"", '\nIf you can get at it, try examining the DOM tree. Selenium does this as a part of testing a page. It also has functions to click buttons and follow links, which may be useful.\n', '\nThe best way to scrape web pages using Ajax or in general pages using Javascript is with a browser itself or a headless browser (a browser without GUI). Currently phantomjs is a well promoted headless browser using WebKit. An alternative that I used with success is HtmlUnit (in Java or .NET via IKVM, which is a simulated browser. Another known alternative is using a web automation tool like Selenium.\nI wrote many articles about this subject like web scraping Ajax and Javascript sites and automated browserless OAuth authentication for Twitter. At the end of the first article there are a lot of extra resources that I have been compiling since 2011.\n', ""\nI like PhearJS, but that might be partially because I built it.\nThat said, it's a service you run in the background that speaks HTTP(S) and renders pages as JSON for you, including any metadata you might need.\n"", ""\nDepends on the ajax page.  The first part of screen scraping is determining how the page works.  Is there some sort of variable you can iterate through to request all the data from the page?  Personally I've used Web Scraper Plus for a lot of screen scraping related tasks because it is cheap, not difficult to get started, non-programmers can get it working relatively quickly.\nSide Note: Terms of Use is probably somewhere you might want to check before doing this.  Depending on the site iterating through everything may raise some flags.  \n"", '\nI think Brian R. Bondy\'s answer is useful when the source code is easy to read. I prefer an easy way using tools like Wireshark or HttpAnalyzer to capture the packet and get the url from  the ""Host"" field and the ""GET"" field.\nFor example,I capture a packet like the following:\nGET /hqzx/quote.aspx?type=3&market=1&sorttype=3&updown=up&page=1&count=8&time=164330 \n HTTP/1.1\nAccept: */*\nReferer: http://quote.hexun.com/stock/default.aspx\nAccept-Language: zh-cn\nAccept-Encoding: gzip, deflate\nUser-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)\nHost: quote.tool.hexun.com\nConnection: Keep-Alive\n\nThen the URL is : \nhttp://quote.tool.hexun.com/hqzx/quote.aspx?type=3&market=1&sorttype=3&updown=up&page=1&count=8&time=164330\n\n', '\nAs a low cost solution you can also try SWExplorerAutomation (SWEA).  The program creates an automation API for any Web application developed with HTML, DHTML or AJAX. \n', '\nSelenium WebDriver is a good solution: you program a browser and you automate what needs to be done in the browser. Browsers (Chrome, Firefox, etc) provide their own drivers that work with Selenium. Since it works as an automated REAL browser, the pages (including javascript and Ajax) get loaded as they do with a human using that browser.\nThe downside is that it is slow (since you would most probably like to wait for all images and scripts to load before you do your scraping on that single page).\n', ""\nI have previously linked to MIT's solvent and EnvJS as my answers to scrape off Ajax pages. These projects seem no longer accessible.\nOut of sheer necessity, I have invented another way to actually scrape off Ajax pages, and it has worked for tough sites like findthecompany which have methods to find headless javascript engines and show no data.\nThe technique is to use chrome extensions to do scraping. Chrome extensions are the best place to scrape off Ajax pages because they actually allow us access to javascript modified DOM. The technique is as follows, I will certainly open source the code in sometime. Create a chrome extension ( assuming you know how to create one, and its architecture and capabilities. This is easy to learn and practice as there are lots of samples),\n\nUse content scripts to access the DOM, by using xpath. Pretty much get the entire list or table or dynamically rendered content using xpath into a variable as string HTML Nodes. ( Only content scripts can access DOM but they can't contact a URL using XMLHTTP )\nFrom content script, using message passing, message the entire stripped DOM as string, to a background script. ( Background scripts can talk to URLs but can't touch the DOM ). We use message passing to get these to talk.\nYou can use various events to loop through web pages and pass each stripped HTML Node content to the background script.\nNow use the background script, to talk to an external server (on localhost), a simple one created using Nodejs/python. Just send the entire HTML Nodes as string, to the server, where the server would just persist the content posted to it, into files, with appropriate variables to identify page numbers or URLs.\nNow you have scraped AJAX content ( HTML Nodes as string ), but these are partial html nodes. Now you can use your favorite XPATH library to load these into memory and use XPATH to scrape information into Tables or text.\n\nPlease comment if you cant understand and I can write it better. ( first attempt ). Also, I am trying to release sample code as soon as possible.\n""]",https://stackoverflow.com/questions/260540/how-do-you-scrape-ajax-pages,web-scraping
Scraping Google Finance (BeautifulSoup),"
I'm trying to scrape Google Finance, and get the ""Related Stocks"" table, which has id ""cc-table"" and class ""gf-table"" based on the webpage inspector in Chrome. (Sample Link: https://www.google.com/finance?q=tsla)
But when I run .find(""table"") or .findAll(""table""), this table does not come up. I can find JSON-looking objects with the table's contents in the HTML content in Python, but do not know how to get it. Any ideas?
",8k,"
            1
        ","['\nThe page is rendered with JavaScript. There are several ways to render and scrape it.\nI can scrape it with Selenium.\nFirst install Selenium:\nsudo pip3 install selenium\n\nThen get a driver https://sites.google.com/a/chromium.org/chromedriver/downloads\nimport bs4 as bs\nfrom selenium import webdriver  \nbrowser = webdriver.Chrome()\nurl = (""https://www.google.com/finance?q=tsla"")\nbrowser.get(url)\nhtml_source = browser.page_source\nbrowser.quit()\nsoup = bs.BeautifulSoup(html_source, ""lxml"")\nfor el in soup.find_all(""table"", {""id"": ""cc-table""}):\n    print(el.get_text())\n\nAlternatively  PyQt5\nfrom PyQt5.QtGui import *  \nfrom PyQt5.QtCore import *  \nfrom PyQt5.QtWebKit import *  \nfrom PyQt5.QtWebKitWidgets import QWebPage\nfrom PyQt5.QtWidgets import QApplication\nimport bs4 as bs\nimport sys\n\nclass Render(QWebPage):  \n    def __init__(self, url):  \n        self.app = QApplication(sys.argv)  \n        QWebPage.__init__(self)  \n        self.loadFinished.connect(self._loadFinished)  \n        self.mainFrame().load(QUrl(url))  \n        self.app.exec_()  \n\n    def _loadFinished(self, result):  \n        self.frame = self.mainFrame()  \n        self.app.quit()  \n\nurl = ""https://www.google.com/finance?q=tsla""\nr = Render(url)  \nresult = r.frame.toHtml()\nsoup = bs.BeautifulSoup(result,\'lxml\')\nfor el in soup.find_all(""table"", {""id"": ""cc-table""}):\n    print(el.get_text())\n\nAlternatively Dryscrape \nimport bs4 as bs\nimport dryscrape\n\nurl = ""https://www.google.com/finance?q=tsla""\nsession = dryscrape.Session()\nsession.visit(url)\ndsire_get = session.body()\nsoup = bs.BeautifulSoup(dsire_get,\'lxml\')\nfor el in soup.find_all(""table"", {""id"": ""cc-table""}):\n    print(el.get_text())\n\nall output:\nValuation▲▼Company name▲▼Price▲▼Change▲▼Chg %▲▼d | m | y▲▼Mkt Cap▲▼TSLATesla Inc328.40-1.52-0.46%53.69BDDAIFDaimler AG72.94-1.50-2.01%76.29BFFord Motor Company11.53-0.17-1.45%45.25BGMGeneral Motors Co...36.07-0.34-0.93%53.93BRNSDFRENAULT SA EUR3.8197.000.000.00%28.69BHMCHonda Motor Co Lt...27.52-0.18-0.65%49.47BAUDVFAUDI AG NPV840.400.000.00%36.14BTMToyota Motor Corp...109.31-0.53-0.48%177.79BBAMXFBAYER MOTOREN WER...94.57-2.41-2.48%56.93BNSANYNissan Motor Co L...20.400.000.00%42.85BMMTOFMITSUBISHI MOTOR ...6.86+0.091.26%10.22B\n\nEDIT\nQtWebKit got deprecated upstream in Qt 5.5 and removed in 5.6.\nYou can switch to PyQt5.QtWebEngineWidgets\n', '\nYou can scrape Google Finance using BeautifulSoup web scraping library without the need to use selenium as the data you want to extract doesn\'t render via Javascript. Plus it will be much faster than launching the whole browser.\nCheck code in online IDE.\n\nfrom bs4 import BeautifulSoup\nimport requests, lxml, json\n   \nparams = {\n        ""hl"": ""en"" \n        }\n\nheaders = {\n        ""User-Agent"": ""Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36"",\n        }\n\nhtml = requests.get(f""https://www.google.com/finance?q=tsla)"", params=params, headers=headers, timeout=30)\nsoup = BeautifulSoup(html.text, ""lxml"")\n\nticker_data = []\n\nfor ticker in soup.select(\'.tOzDHb\'):\n  title = ticker.select_one(\'.RwFyvf\').text\n  price = ticker.select_one(\'.YMlKec\').text\n  index = ticker.select_one(\'.COaKTb\').text\n  price_change = ticker.select_one(""[jsname=Fe7oBc]"")[""aria-label""]\n\n  ticker_data.append({\n    ""index"": index,\n  ""title"" : title,\n  ""price"" : price,\n  ""price_change"" : price_change\n  })  \nprint(json.dumps(ticker_data, indent=2))\n\nExample output\n[\n  {\n    ""index"": ""Index"",\n    ""title"": ""Dow Jones Industrial Average"",\n    ""price"": ""32,774.41"",\n    ""price_change"": ""Down by 0.18%""\n  },\n  {\n    ""index"": ""Index"",\n    ""title"": ""S&P 500"",\n    ""price"": ""4,122.47"",\n    ""price_change"": ""Down by 0.42%""\n  },\n  {\n    ""index"": ""TSLA"",\n    ""title"": ""Tesla Inc"",\n    ""price"": ""$850.00"",\n    ""price_change"": ""Down by 2.44%""\n  },\n  # ...\n]\n\n\nThere\'s a scrape Google Finance Ticker Quote Data in Python blog post if you need to scrape more data from Google Finance.\n', ""\nMost website owners don't like scrapers because they take data the company values, use up a whole bunch of their server time and bandwidth, and give nothing in return. Big companies like Google may have entire teams employing a whole host of methods to detect and block bots trying to scrape their data.\nThere are several ways around this:\n\nScrape from another less secured website.\nSee if Google or another company has an API for public use.\nUse a more advanced scraper like Selenium (and probably still be blocked by google).\n\n""]",https://stackoverflow.com/questions/45259232/scraping-google-finance-beautifulsoup,web-scraping
How to run Scrapy from within a Python script,"
I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:
http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/
http://snipplr.com/view/67006/using-scrapy-from-a-script/
I can't figure out where I should put my spider code and how to call it from the main function. Please help. This is the example code:
# This snippet can be used to run scrapy spiders independent of scrapyd or the scrapy command line tool and use it from a script. 
# 
# The multiprocessing library is used in order to work around a bug in Twisted, in which you cannot restart an already running reactor or in this case a scrapy instance.
# 
# [Here](http://groups.google.com/group/scrapy-users/browse_thread/thread/f332fc5b749d401a) is the mailing-list discussion for this snippet. 

#!/usr/bin/python
import os
os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'project.settings') #Must be at the top before other imports

from scrapy import log, signals, project
from scrapy.xlib.pydispatch import dispatcher
from scrapy.conf import settings
from scrapy.crawler import CrawlerProcess
from multiprocessing import Process, Queue

class CrawlerScript():

    def __init__(self):
        self.crawler = CrawlerProcess(settings)
        if not hasattr(project, 'crawler'):
            self.crawler.install()
        self.crawler.configure()
        self.items = []
        dispatcher.connect(self._item_passed, signals.item_passed)

    def _item_passed(self, item):
        self.items.append(item)

    def _crawl(self, queue, spider_name):
        spider = self.crawler.spiders.create(spider_name)
        if spider:
            self.crawler.queue.append_spider(spider)
        self.crawler.start()
        self.crawler.stop()
        queue.put(self.items)

    def crawl(self, spider):
        queue = Queue()
        p = Process(target=self._crawl, args=(queue, spider,))
        p.start()
        p.join()
        return queue.get(True)

# Usage
if __name__ == ""__main__"":
    log.start()

    """"""
    This example runs spider1 and then spider2 three times. 
    """"""
    items = list()
    crawler = CrawlerScript()
    items.append(crawler.crawl('spider1'))
    for i in range(3):
        items.append(crawler.crawl('spider2'))
    print items

# Snippet imported from snippets.scrapy.org (which no longer works)
# author: joehillen
# date  : Oct 24, 2010

Thank you.
",83k,"
            85
        ","[""\nAll other answers reference Scrapy v0.x. According to the updated docs, Scrapy 1.0 demands:\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider(scrapy.Spider):\n    # Your spider definition\n    ...\n\nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\n\nprocess.crawl(MySpider)\nprocess.start() # the script will block here until the crawling is finished\n\n"", '\nSimply we can use\nfrom scrapy.crawler import CrawlerProcess\nfrom project.spiders.test_spider import SpiderName\n\nprocess = CrawlerProcess()\nprocess.crawl(SpiderName, arg1=val1,arg2=val2)\nprocess.start()\n\nUse these arguments inside spider __init__ function with the global scope.\n', ""\nThough I haven't tried it I think the answer can be found within the scrapy documentation. To quote directly from it:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import Settings\nfrom scrapy import log\nfrom testspiders.spiders.followall import FollowAllSpider\n\nspider = FollowAllSpider(domain='scrapinghub.com')\ncrawler = Crawler(Settings())\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here\n\nFrom what I gather this is a new development in the library which renders some of the earlier approaches online (such as that in the question) obsolete.\n"", '\nIn scrapy 0.19.x you should do this:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy import log, signals\nfrom testspiders.spiders.followall import FollowAllSpider\nfrom scrapy.utils.project import get_project_settings\n\nspider = FollowAllSpider(domain=\'scrapinghub.com\')\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.signals.connect(reactor.stop, signal=signals.spider_closed)\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here until the spider_closed signal was sent\n\nNote these lines     \nsettings = get_project_settings()\ncrawler = Crawler(settings)\n\nWithout it your spider won\'t use your settings and will not save the items.\nTook me a while to figure out why the example in documentation wasn\'t saving my items. I sent a pull request to fix the doc example.\nOne more to do so is just call command directly from you script\nfrom scrapy import cmdline\ncmdline.execute(""scrapy crawl followall"".split())  #followall is the spider\'s name\n\nCopied this answer from my first answer in here:\nhttps://stackoverflow.com/a/19060485/1402286\n', '\nWhen there are multiple crawlers need to be run inside one python script, the reactor stop needs to be handled with caution as the reactor can only be stopped once and cannot be restarted. \nHowever, I found while doing my project that using \nos.system(""scrapy crawl yourspider"")\n\nis the easiest. This will save me from handling all sorts of signals especially when I have multiple spiders.\nIf Performance is a concern, you can use multiprocessing to run your spiders in parallel, something like:\ndef _crawl(spider_name=None):\n    if spider_name:\n        os.system(\'scrapy crawl %s\' % spider_name)\n    return None\n\ndef run_crawler():\n\n    spider_names = [\'spider1\', \'spider2\', \'spider2\']\n\n    pool = Pool(processes=len(spider_names))\n    pool.map(_crawl, spider_names)\n\n', '\nit  is an improvement of\nScrapy throws an error when run using crawlerprocess\nand https://github.com/scrapy/scrapy/issues/1904#issuecomment-205331087\nFirst create your usual spider for successful command line running. it is very very important that it should run and export data or image or file\nOnce it is over, do just like pasted in my program above spider class definition and below __name __ to invoke settings.\nit will get necessary settings which ""from scrapy.utils.project import get_project_settings"" failed to do which is recommended by many\nboth above and below portions should be there together. only one don\'t run.\nSpider will run in scrapy.cfg folder not any other folder\ntree  diagram may be displayed by the moderators for reference\n#Tree\n[enter image description here][1]\n\n#spider.py\nimport sys\nsys.path.append(r\'D:\\ivana\\flow\') #folder where scrapy.cfg is located\n\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.settings import Settings\nfrom flow import settings as my_settings\n\n#----------------Typical Spider Program starts here-----------------------------\n\n          spider class definition here\n\n#----------------Typical Spider Program ends here-------------------------------\n\nif __name__ == ""__main__"":\n\n    crawler_settings = Settings()\n    crawler_settings.setmodule(my_settings)\n\n    process = CrawlerProcess(settings=crawler_settings)\n    process.crawl(FlowSpider) # it is for class FlowSpider(scrapy.Spider):\n    process.start(stop_after_crawl=True)\n\n', ""\n# -*- coding: utf-8 -*-\nimport sys\nfrom scrapy.cmdline import execute\n\n\ndef gen_argv(s):\n    sys.argv = s.split()\n\n\nif __name__ == '__main__':\n    gen_argv('scrapy crawl abc_spider')\n    execute()\n\nPut this code to the path you can run scrapy crawl abc_spider from command line. (Tested with Scrapy==0.24.6)\n"", ""\nIf you want to run a simple crawling, It's easy by just running command: \nscrapy crawl . \nThere is another options to export your results to store in some formats like: \nJson, xml, csv. \nscrapy crawl  -o result.csv or result.json or result.xml. \nyou may want to try it\n""]",https://stackoverflow.com/questions/13437402/how-to-run-scrapy-from-within-a-python-script,web-scraping
How to connect via HTTPS using Jsoup?,"
It's working fine over HTTP, but when I try and use an HTTPS source it throws the following exception:
10-12 13:22:11.169: WARN/System.err(332): javax.net.ssl.SSLHandshakeException: java.security.cert.CertPathValidatorException: Trust anchor for certification path not found.
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.xnet.provider.jsse.OpenSSLSocketImpl.startHandshake(OpenSSLSocketImpl.java:477)
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.xnet.provider.jsse.OpenSSLSocketImpl.startHandshake(OpenSSLSocketImpl.java:328)
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.http.HttpConnection.setupSecureSocket(HttpConnection.java:185)
10-12 13:22:11.179: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.https.HttpsURLConnectionImpl$HttpsEngine.makeSslConnection(HttpsURLConnectionImpl.java:433)
10-12 13:22:11.189: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.https.HttpsURLConnectionImpl$HttpsEngine.makeConnection(HttpsURLConnectionImpl.java:378)
10-12 13:22:11.189: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.http.HttpURLConnectionImpl.connect(HttpURLConnectionImpl.java:205)
10-12 13:22:11.189: WARN/System.err(332):     at org.apache.harmony.luni.internal.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:152)
10-12 13:22:11.189: WARN/System.err(332):     at org.jsoup.helper.HttpConnection$Response.execute(HttpConnection.java:377)
10-12 13:22:11.189: WARN/System.err(332):     at org.jsoup.helper.HttpConnection$Response.execute(HttpConnection.java:364)
10-12 13:22:11.189: WARN/System.err(332):     at org.jsoup.helper.HttpConnection.execute(HttpConnection.java:143)

Here's the relevant code:
try {
    doc = Jsoup.connect(""https url here"").get();
} catch (IOException e) {
    Log.e(""sys"",""coudnt get the html"");
    e.printStackTrace();
}

",53k,"
            28
        ","['\nIf you want to do it the right way, and/or you need to deal with only one site, then you basically need to grab the SSL certificate of the website in question and import it in your Java key store. This will result in a JKS file which you in turn set as SSL trust store before using Jsoup (or java.net.URLConnection). \nYou can grab the certificate from your webbrowser\'s store. Let\'s assume that you\'re using Firefox.\n\nGo to the website in question using Firefox, which is in your case https://web2.uconn.edu/driver/old/timepoints.php?stopid=10\nLeft in the address bar you\'ll see ""uconn.edu"" in blue (this indicates a valid SSL certificate)\nClick on it for details and then click on the More information button.\nIn the security dialogue which appears, click the View Certificate button.\nIn the certificate panel which appears, go to the Details tab.\nClick the deepest item of the certificate hierarchy, which is in this case ""web2.uconn.edu"" and finally click the Export button.\n\nNow you\'ve a web2.uconn.edu.crt file.\nNext, open the command prompt and import it in the Java key store using the keytool command (it\'s part of the JRE):\nkeytool -import -v -file /path/to/web2.uconn.edu.crt -keystore /path/to/web2.uconn.edu.jks -storepass drowssap\n\nThe -file must point to the location of the .crt file which you just downloaded. The -keystore must point to the location of the generated .jks file (which you in turn want to set as SSL trust store). The -storepass is required, you can just enter whatever password you want as long as it\'s at least 6 characters.\nNow, you\'ve a web2.uconn.edu.jks file. You can finally set it as SSL trust store before connecting as follows:\nSystem.setProperty(""javax.net.ssl.trustStore"", ""/path/to/web2.uconn.edu.jks"");\nDocument document = Jsoup.connect(""https://web2.uconn.edu/driver/old/timepoints.php?stopid=10"").get();\n// ...\n\n\nAs a completely different alternative, particularly when you need to deal with multiple sites (i.e. you\'re creating a world wide web crawler), then you can also instruct Jsoup (basically, java.net.URLConnection) to blindly trust all SSL certificates. See also section ""Dealing with untrusted or misconfigured HTTPS sites"" at the very bottom of this answer: Using java.net.URLConnection to fire and handle HTTP requests\n', '\nIn my case, all I needed to do was to add the .validateTLSCertificates(false) in my connection\nDocument doc  = Jsoup.connect(httpsURLAsString)\n            .timeout(60000).validateTLSCertificates(false).get();\n\nI also had to increase the read timeout but I think this is irrelevant\n', '\nI stumbled over the answers here and in the linked question in my search and want to add two pieces of information, as the accepted answer doesn\'t fit my quite similar scenario, but there is an additional solution that fits even in that case (cert and hostname don\'t match for test systems).\n\nThere is a github request to add such a functionality. So perhaps soon the problem will be solved: https://github.com/jhy/jsoup/pull/343 \nedit: Github request was resolved and the method to disable certificate validation is: validateTLSCertificates(boolean validate)\nBased on http://www.nakov.com/blog/2009/07/16/disable-certificate-validation-in-java-ssl-connections/ I found a solution which seems to work (at least in my scenario where jsoup 1.7.3 is called as part of a maven task). I wrapped it in a method disableSSLCertCheck() that I call before the very first Jsoup.connect().\n\nBefore you use this method, you should be really sure that you understand what you do there - not checking SSL certificates is a really stupid thing. Always use correct SSL certificates for your servers which are signed by a commonly accepted CA. If you can\'t afford a commonly accepted CA use correct SSL certificates nevertheless with @BalusC accepted answer above. If you can\'t configure correct SSL certificates (which should never be the case in production environments) the following method could work:\n    private void disableSSLCertCheck() throws NoSuchAlgorithmException, KeyManagementException {\n    // Create a trust manager that does not validate certificate chains\n    TrustManager[] trustAllCerts = new TrustManager[] {new X509TrustManager() {\n            public java.security.cert.X509Certificate[] getAcceptedIssuers() {\n                return null;\n            }\n            public void checkClientTrusted(X509Certificate[] certs, String authType) {\n            }\n            public void checkServerTrusted(X509Certificate[] certs, String authType) {\n            }\n        }\n    };\n\n    // Install the all-trusting trust manager\n    SSLContext sc = SSLContext.getInstance(""SSL"");\n    sc.init(null, trustAllCerts, new java.security.SecureRandom());\n    HttpsURLConnection.setDefaultSSLSocketFactory(sc.getSocketFactory());\n\n    // Create all-trusting host name verifier\n    HostnameVerifier allHostsValid = new HostnameVerifier() {\n        public boolean verify(String hostname, SSLSession session) {\n            return true;\n        }\n    };\n\n    // Install the all-trusting host verifier\n    HttpsURLConnection.setDefaultHostnameVerifier(allHostsValid);\n    }\n\n', '\nTo suppress certificate warnings for specific JSoup connection can use following approach:\nKotlin\n\nval document = Jsoup.connect(""url"")\n        .sslSocketFactory(socketFactory())\n        .get()\n\n\nprivate fun socketFactory(): SSLSocketFactory {\n    val trustAllCerts = arrayOf<TrustManager>(object : X509TrustManager {\n        @Throws(CertificateException::class)\n        override fun checkClientTrusted(chain: Array<X509Certificate>, authType: String) {\n        }\n\n        @Throws(CertificateException::class)\n        override fun checkServerTrusted(chain: Array<X509Certificate>, authType: String) {\n        }\n\n        override fun getAcceptedIssuers(): Array<X509Certificate> {\n            return arrayOf()\n        }\n    })\n\n    try {\n        val sslContext = SSLContext.getInstance(""TLS"")\n        sslContext.init(null, trustAllCerts, java.security.SecureRandom())\n        return sslContext.socketFactory\n    } catch (e: Exception) {\n        when (e) {\n            is RuntimeException, is KeyManagementException -> {\n                throw RuntimeException(""Failed to create a SSL socket factory"", e)\n            }\n            else -> throw e\n        }\n    }\n}\n\n\nJava\n\n\n Document document = Jsoup.connect(""url"")\n        .sslSocketFactory(socketFactory())\n        .get();\n\n\n  private SSLSocketFactory socketFactory() {\n    TrustManager[] trustAllCerts = new TrustManager[]{new X509TrustManager() {\n      public java.security.cert.X509Certificate[] getAcceptedIssuers() {\n        return null;\n      }\n\n      public void checkClientTrusted(X509Certificate[] certs, String authType) {\n      }\n\n      public void checkServerTrusted(X509Certificate[] certs, String authType) {\n      }\n    }};\n\n    try {\n      SSLContext sslContext = SSLContext.getInstance(""TLS"");\n      sslContext.init(null, trustAllCerts, new java.security.SecureRandom());\n      return sslContext.getSocketFactory();\n    } catch (NoSuchAlgorithmException | KeyManagementException e) {\n      throw new RuntimeException(""Failed to create a SSL socket factory"", e);\n    }\n  }\n\n\nNB. As mentioned before ignoring certificates is not a good idea. \n', ""\nI've had the same problem but took the lazy route - tell your app to ignore the cert and carry on anyway.\nI got the code from here:  How do I use a local HTTPS URL in java?\nYou'll have to import these classes for it to work:\nimport javax.net.ssl.HostnameVerifier;\nimport javax.net.ssl.HttpsURLConnection;\nimport javax.net.ssl.SSLContext;\nimport javax.net.ssl.SSLSession;\nimport javax.net.ssl.TrustManager;\nimport javax.net.ssl.X509TrustManager;\n\nJust run that method somewhere before you try to make the connection and voila, it just trusts the cert no matter what.  Of course this isn't any help if you actually want to make sure the cert is real, but good for monitoring your own internal websites etc.\n"", ""\nI'm no expert in this field but I ran into a similar exception when trying to connect to a website over HTTPS using java.net APIs.  The browser does a lot of work for you regarding SSL certificates when you visit a site using HTTPS.  However, when you are manually connecting to sites (using HTTP requests manually), all that work still needs to be done.  Now I don't know what all this work is exactly, but it has to do with downloading certificates and putting them where Java can find them.  Here's a link that will hopefully point you in the right direction.\nhttp://confluence.atlassian.com/display/JIRA/Connecting+to+SSL+services\n"", '\nI was facing the same issue with Jsoup, I was not able to connect and get the document for https urls but when I changed my JDK version from 1.7 to 1.8, the issue got resolved.\nIt may help you :) \n', ""\nI've had that problem only in dev environment. The solution to solve it was just to add a few flags to ignore SSL to VM:\n-Ddeployment.security.TLSv1.1=false \n-Ddeployment.security.TLSv1.2=false\n\n"", '\nAfter testing the solutions here. It is strange that sslSocketFactory setting in Jsoup is completely useless and it never works. So there is no need to get and set SSLSocketFactory.\nActually the second half of Mori solution works. Just need the following before using Jsoup:\n// Create all-trusting host name verifier\nHostnameVerifier allHostsValid = new HostnameVerifier() {\n    public boolean verify(String hostname, SSLSession session) {\n        return true;\n    }\n};\n\n// Install the all-trusting host verifier\nHttpsURLConnection.setDefaultHostnameVerifier(allHostsValid);\n\nThis is tested with Jsoup 1.13.1.\n', '\nTry following (just put it before Jsoup.connect(""https://example.com""):\n    Authenticator.setDefault(new Authenticator() {\n        @Override\n        protected PasswordAuthentication getPasswordAuthentication() {\n            return new PasswordAuthentication(username, password.toCharArray());\n        }\n    });\n\n']",https://stackoverflow.com/questions/7744075/how-to-connect-via-https-using-jsoup,web-scraping
How to run Puppeteer code in any web browser?,"
I'm trying to do some web scraping with Puppeteer and I need to retrieve the value into a Website I'm building.
I have tried to load the Puppeteer file in the html file as if it was a JavaScript file but I keep getting an error. However, if I run it in a cmd window it works well.

Scraper.js:

getPrice();
function getPrice() {
    const puppeteer = require('puppeteer');
    void (async () => {
        try {
            const browser = await puppeteer.launch()
            const page = await browser.newPage()              
            await page.goto('http://example.com') 
            await page.setViewport({ width: 1920, height: 938 })        
            await page.waitForSelector('.m-hotel-info > .l-container > .l-header-section > .l-m-col-2 > .m-button')
            await page.click('.m-hotel-info > .l-container > .l-header-section > .l-m-col-2 > .m-button')
            await page.waitForSelector('.modal-content')
            await page.click('.tile-hsearch-hws > .m-search-tabs > #edit-search-panel > .l-em-reset > .m-field-wrap > .l-xs-col-4 > .analytics-click')
            await page.waitForNavigation();
            await page.waitForSelector('.tile-search-filter > .l-display-none')
            const innerText = await page.evaluate(() => document.querySelector('.tile-search-filter > .l-display-none').innerText);
            console.log(innerText)
        } catch (error) {
            console.log(error)
        }

    })()
}


index.html:

<html>
  <head></head>
  <body>
    <script src=""../js/scraper.js"" type=""text/javascript""></script>
  </body>
</html>

The expected result should be this one in the console of Chrome:

But I'm getting this error instead:


What am I doing wrong?
",20k,"
            11
        ","['\nEDIT: Since puppeteer removed support for puppeteer-web, I moved it out of the repo and tried to patch it a bit.\nIt does work with browser. The package is called puppeteer-web, specifically made for such cases.\nBut the main point is, there must be some instance of chrome running on some server. Only then you can connect to it.\nYou can use it later on in your web page to drive another browser instance through its WS Endpoint:\n<script src=""https://unpkg.com/puppeteer-web"">\n</script>\n\n<script>\n  const browser = await puppeteer.connect({\n    browserWSEndpoint: `ws://0.0.0.0:8080`, // <-- connect to a server running somewhere\n    ignoreHTTPSErrors: true\n  });\n\n  const pagesCount = (await browser.pages()).length;\n  const browserWSEndpoint = await browser.wsEndpoint();\n  console.log({ browserWSEndpoint, pagesCount });\n</script>\n\nI had some fun with puppeteer and webpack,\n\nplayground-react-puppeteer\nplayground-electron-react-puppeteer-example\n\nSee these answers for full understanding of creating the server and more,\n\nOfficial link to puppeteer-web\nPuppeteer with docker\nPuppeteer with chrome extension\nPuppeteer with local wsEndpoint\n\n', '\nInstead, use Puppeteer in the backend and make an API to interface your frontend with it if your main goal is to web scrape and get the data in the frontend.\n', ""\nPuppeteer runs on the server in Node.js. For the common case, rather than using puppeteer-web to allow the client to write Puppeteer code to control the browser, it's better to create an HTTP or websocket API that lets clients indirectly trigger Puppeteer code.\nReasons to prefer a REST API over puppeteer-connect:\n\nbetter support for arbitrary client codebases--clients that aren't written in JS (desktop, command line and mobile apps, for example) can use the API just as easily as the browser can\nno dependency on puppeteer-connect\nlower client-side complexity; for many use cases JS won't be required at all if HTML forms suffice\nbetter control of client behavior--running a browser on the server is a heavy load and has powerful capabilities that are easy to exploit\neasier to integrate with other backend code and resources like the file system\nprovides seamless integration with an existing API as just another set of routes\nhiding Puppeteer as an implementation detail lets you switch to, say, Playwright in the future without the client code being affected.\n\nSimilarly, rather than exposing a mock fs object to read and write files on the server, we expose REST API endpoints to accomplish these tasks. This is a useful layer of abstraction.\nSince there are many use cases for Puppeteer in the context of an API (usually Express), it's hard to offer a general example, but here are a few case studies you can use as starting points:\n\nPuppeteer unable to run on Heroku\nPuppeteer doesn't close browser\nParallelism of Puppeteer with Express Router Node JS. How to pass page between routes while maintaining concurrency\n\n""]",https://stackoverflow.com/questions/54647694/how-to-run-puppeteer-code-in-any-web-browser,web-scraping
Accessing object in iframe using VBA,"
To the point:
I have successfully used VBA to do the following:

Login to a website using getElementsByName
Select parameters for the report that will be generated (using getelementsby...)
generating the report after selecting parameters which renders the resulting dataset into an iframe on the same page

Important to note - The website is client-side
The above was the simple part, the difficult part is as below:

clicking on a gif image within the iframe that exports the dataset to a csv

I have tried the following:
Dim idoc As HTMLDocument
Dim iframe As HTMLFrameElement
Dim iframe2 As HTMLDocument

Set idoc = objIE.document
Set iframe = idoc.all(""iframename"")
Set iframe2 = iframe.contentDocument

    Do Until InStr(1, objIE.document.all(""iframename"").contentDocument.innerHTML, ""img.gif"", vbTextCompare) = 0
        DoEvents
    Loop

To give some context to the logic above -

I accessed the main frame
i accessed the iframe by its name element
i accessed the content within the iframe
I attempted to find the gif image that needs to be clicked to export to csv

It is at this line that it trips up saying ""Object doesn't support this property or method""
Also tried accessing the iframe gif by the a element and href attribute but this totally failed. I also tried grabbing the image from its source URL but all this does it take me to the page the image is from.
note: the iframe does not have an ID and strangely the gif image does not have an ""onclick"" element/event

Final consideration - attempted scraping the iframe using R

accessing the HTML node of the iframe was simple, however trying to access the attributes of the iframe and subsequently the nodes of the table proved unsuccessful. All it returned was ""Character(0)""
library(rvest)
library(magrittr)

Blah <-read_html(""web address redacted"") %>%
  html_nodes(""#iframe"")%>%
  html_nodes(""#img"")%>%
  html_attr(""#src"")%>%
  #read_html()%>%
  head()
Blah

As soon as a i include read_html the following error returns on the script:
Error in if (grepl(""<|>"", x)) { : argument is of length zero
I suspect this is referring to the Character(0) 
Appreciate any guidance here!
Many Thanks,

HTML

<div align=""center""> 
    <table id=""table1"" style=""border-collapse: collapse"" width=""700"" cellspacing=""0"" cellpadding=""0"" border=""0""> 
        <tbody>
            <tr>
                <td colspan=""6""> &nbsp;</td>
            </tr> 
            <tr> 
                <td colspan=""6""> 
                    <a href=""href redacted"">
                        <img src=""img.gif"" width=""38"" height=""38"" border=""0"" align=""right"">
                    </a>
                    <strong>x - </strong>
                </td>
            </tr> 
        </tbody>
    </table>
</div>

",19k,"
            10
        ","['\nIt is sometimes tricky with iframes. Based on html you provided I have created this example. Which works locally, but would it work for you as well?\nTo get to the IFrame the frames collection can be used. Hope you know the name of the IFrame?\nDim iframeDoc As MSHTML.HTMLDocument\nSet iframeDoc = doc.frames(""iframename"").document\n\nThen to go the the image we can use querySelector method e.g. like this:\nDim img As MSHTML.HTMLImg\nSet img = iframeDoc.querySelector(""div table[id=\'table1\'] tbody tr td a[href^=\'https://stackoverflow.com\'] img"")\n\nThe selector a[href^=\'https://stackoverflow.com\'] selects anchor which has an href attribute which starts with given text. The ^ denotes the beginning.\nThen when we have the image just a simple call to click on its parent which is the desired anchor. HTH\n\nComplete example:\nOption Explicit\n\n\' Add reference to Microsoft Internet Controls (SHDocVw)\n\' Add reference to Microsoft HTML Object Library\n\nSub Demo()\n\n    Dim ie As SHDocVw.InternetExplorer\n    Dim doc As MSHTML.HTMLDocument\n    Dim url As String\n    \n    url = ""file:///C:/Users/dusek/Documents/My Web Sites/mainpage.html""\n    Set ie = New SHDocVw.InternetExplorer\n    ie.Visible = True\n    ie.navigate url\n\n    While ie.Busy Or ie.readyState <> READYSTATE_COMPLETE\n        DoEvents\n    Wend\n    \n    Set doc = ie.document\n    \n    Dim iframeDoc As MSHTML.HTMLDocument\n    Set iframeDoc = doc.frames(""iframename"").document\n    If iframeDoc Is Nothing Then\n        MsgBox ""IFrame with name \'iframename\' was not found.""\n        ie.Quit\n        Exit Sub\n    End If\n    \n    Dim img As MSHTML.HTMLImg\n    Set img = iframeDoc.querySelector(""div table[id=\'table1\'] tbody tr td a[href^=\'https://stackoverflow.com\'] img"")\n    If img Is Nothing Then\n        MsgBox ""Image element within iframe was not found.""\n        ie.Quit\n        Exit Sub\n    Else\n        img.parentElement.Click\n    End If\n    \n    ie.Quit\nEnd Sub\n\n\nMain page HTML used\n\n<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">\n<html xmlns=""http://www.w3.org/1999/xhtml"">\n\n<head>\n<!-- saved from url=(0016)http://localhost -->\n<meta content=""text/html; charset=utf-8"" http-equiv=""Content-Type"" />\n<title>x -</title>\n</head>\n\n<body>\n<iframe name=""iframename"" src=""iframe1.html"">\n</iframe>\n</body>\n\n</html>\n\n\nIFrame HTML used (saved as file iframe1.html\n\n<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">\n<html xmlns=""http://www.w3.org/1999/xhtml"">\n\n<head>\n<!-- saved from url=(0016)http://localhost -->\n<meta content=""text/html; charset=utf-8"" http-equiv=""Content-Type"" />\n<title>Untitled 2</title>\n</head>\n\n<body>\n<div align=""center""> \n    <table id=""table1"" style=""border-collapse: collapse"" width=""700"" cellspacing=""0"" cellpadding=""0"" border=""0""> \n        <tbody>\n            <tr>\n                <td colspan=""6""> &nbsp;</td>\n            </tr> \n            <tr> \n                <td colspan=""6""> \n                    <a href=""https://stackoverflow.com/questions/44902558/accessing-object-in-iframe-using-vba"">\n                        <img src=""img.gif"" width=""38"" height=""38"" border=""0"" align=""right"">\n                    </a>\n                    <strong>x - </strong>\n                </td>\n            </tr> \n        </tbody>\n    </table>\n</div>\n\n</body>\n\n</html>\n\n\nBTW, The frame may be referenced by it\'s index also doc.frames(0).document. Thanks to Paulo Bueno.\n\n', '\nI thought I would expand on the answer already given.\nIn the case of Internet Explorer you may have one of two common situations to handle regarding iframes.\n\nsrc of iframe is subject to same origin policy restrictions:\n\n\nThe iframe src has a different origin to the landing page in which case, due to same origin policy, attempts to access it will yield access denied.\nResolution:\nConsider using selenium basic to automate a different browser such as Chrome where CORS is allowed/you can switch to the iframe and continue working with the iframe document\nExample:\nOption Explicit\n\'download selenium https://github.com/florentbr/SeleniumBasic/releases/tag/v2.0.9.0\n\'Ensure latest applicable driver e.g. ChromeDriver.exe in Selenium folder\n\'VBE > Tools > References > Add reference to selenium type library\nPublic Sub Example()\n    Dim d As WebDriver\n    Const URL As String = ""https://www.rosterresource.com/mlb-roster-grid/""\n    Set d = New ChromeDriver\n    With d\n        .Start ""Chrome""\n        .get URL\n        .SwitchToFrame .FindElementByCss(""iframe"") \'< pass the iframe element as the identifier argument\n        \' .SwitchToDefaultContent \'\'to go back to parent document.\n        Stop \'<== delete me later\n        .Quit\n    End With\nEnd Sub\n\n\n\nsrc of iframe is not subject to same origin policy restrictions:\n\n\nResolution:\nThe methods as detailed in answer already given. Additionally, you can extract the src of the iframe and .Navigate2 that to access\n.Navigate2 .document.querySelector(""iframe"").src\n\nIf you only want to work with the contents of the iframe then simply do your initial .Navigate2 the iframe src and don\'t even visit the initial landing page\nExample:\nOption Explicit\nPublic Sub NavigateUsingSrcOfIframe()\n    Dim IE As New InternetExplorer\n    With IE\n        .Visible = True\n        .Navigate2 ""http://www.bursamalaysia.com/market/listed-companies/company-announcements/5978065""\n\n        While .Busy Or .readyState < 4: DoEvents: Wend\n        \n        .Navigate2 .document.querySelector(""iframe"").src\n        \n        While .Busy Or .readyState < 4: DoEvents: Wend\n\n        Stop \'<== delete me later\n        .Quit\n    End With\nEnd Sub\n\n\n\niframe in ShadowRoot\n\n\nAn unlikely case might be an iframe in shadowroot. You should really have one or the other and not one within the other.\n\nResolution:\nIn that case you need an additional accessor of\nElement.shadowRoot.querySelector(""iframe"").contentDocument\n\nwhere Element is your parent element with shadowRoot attached. This method will only work if the shadowRoot mode is set to Open.\nSide note:\nA nice selenium based example, using ExecuteScript to return shadowRoot is given here: How Do I Access Elements in the Shadow DOM using Selenium in VBA?\n', ""\nAdding to the answers given:\nIf you're ok with using a DLL and rewrite your code, you can run Microsoft's Edge browser (a Chrome-based browser) with VBA. With that you can do almost anything you want. Note however, that access to the DOM is performed by javascript, not by an object like Dim IE As New InternetExplorer. Look at the VBA sample and you'll get the grasp.\nhttps://github.com/peakpeak-github/libEdge\nSidenote: Samples for C# and C++ are also included.\n""]",https://stackoverflow.com/questions/44902558/accessing-object-in-iframe-using-vba,web-scraping
Scrapy Very Basic Example,"
Hi I have Python Scrapy installed on my mac and I was trying to follow the very first example on their web. 
They were trying to run the command:
scrapy crawl mininova.org -o scraped_data.json -t json

I don't quite understand what does this mean? looks like scrapy turns out to be a separate program. And I don't think they have a command called crawl. In the example, they have a paragraph of code, which is the definition of the class MininovaSpider and the TorrentItem. I don't know where these two classes should go to, go to the same file and what is the name of this python file? 
",24k,"
            26
        ","['\nTL;DR: see Self-contained minimum example script to run scrapy.\nFirst of all, having a normal Scrapy project with a separate .cfg, settings.py, pipelines.py, items.py, spiders package etc is a recommended way to keep and handle your web-scraping logic. It provides a modularity, separation of concerns that keeps things organized, clear and testable. \nIf you are following the official Scrapy tutorial to create a project, you are running web-scraping via a special scrapy command-line tool:\nscrapy crawl myspider\n\n\nBut, Scrapy also provides an API to run crawling from a script.\nThere are several key concepts that should be mentioned:\n\nSettings class - basically a key-value ""container"" which is initialized with default built-in values\nCrawler class - the main class that acts like a glue for all the different components involved in web-scraping with Scrapy\nTwisted reactor - since Scrapy is built-in on top of twisted asynchronous networking library - to start a crawler, we need to put it inside the Twisted Reactor, which is in simple words, an event loop:\n\n\nThe reactor is the core of the event loop within Twisted – the loop which drives applications using Twisted. The event loop is a programming construct that waits for and\n  dispatches events or messages in a program. It works by calling some\n  internal or external “event provider”, which generally blocks until an\n  event has arrived, and then calls the relevant event handler\n  (“dispatches the event”). The reactor provides basic interfaces to a\n  number of services, including network communications, threading, and\n  event dispatching.\n\nHere is a basic and simplified process of running Scrapy from script:\n\ncreate a Settings instance (or use get_project_settings() to use existing settings):\nsettings = Settings()  # or settings = get_project_settings()\n\ninstantiate Crawler with settings instance passed in:\ncrawler = Crawler(settings)\n\ninstantiate a spider (this is what it is all about eventually, right?):\nspider = MySpider()\n\nconfigure signals. This is an important step if you want to have a post-processing logic, collect stats or, at least, to ever finish crawling since the twisted reactor needs to be stopped manually. Scrapy docs suggest to stop the reactor in the spider_closed signal handler:\n\n\nNote that you will also have to shutdown the Twisted reactor yourself\n  after the spider is finished. This can be achieved by connecting a\n  handler to the signals.spider_closed signal.\n\ndef callback(spider, reason):\n    stats = spider.crawler.stats.get_stats()\n    # stats here is a dictionary of crawling stats that you usually see on the console        \n\n    # here we need to stop the reactor\n    reactor.stop()\n\ncrawler.signals.connect(callback, signal=signals.spider_closed)\n\n\nconfigure and start crawler instance with a spider passed in:\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\n\noptionally start logging:\nlog.start()\n\nstart the reactor - this would block the script execution:\nreactor.run()\n\n\nHere is an example self-contained script that is using DmozSpider spider and involves item loaders with input and output processors and item pipelines:\nimport json\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.contrib.loader import ItemLoader\nfrom scrapy.contrib.loader.processor import Join, MapCompose, TakeFirst\nfrom scrapy import log, signals, Spider, Item, Field\nfrom scrapy.settings import Settings\nfrom twisted.internet import reactor\n\n\n# define an item class\nclass DmozItem(Item):\n    title = Field()\n    link = Field()\n    desc = Field()\n\n\n# define an item loader with input and output processors\nclass DmozItemLoader(ItemLoader):\n    default_input_processor = MapCompose(unicode.strip)\n    default_output_processor = TakeFirst()\n\n    desc_out = Join()\n\n\n# define a pipeline\nclass JsonWriterPipeline(object):\n    def __init__(self):\n        self.file = open(\'items.jl\', \'wb\')\n\n    def process_item(self, item, spider):\n        line = json.dumps(dict(item)) + ""\\n""\n        self.file.write(line)\n        return item\n\n\n# define a spider\nclass DmozSpider(Spider):\n    name = ""dmoz""\n    allowed_domains = [""dmoz.org""]\n    start_urls = [\n        ""http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"",\n        ""http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/""\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath(\'//ul/li\'):\n            loader = DmozItemLoader(DmozItem(), selector=sel, response=response)\n            loader.add_xpath(\'title\', \'a/text()\')\n            loader.add_xpath(\'link\', \'a/@href\')\n            loader.add_xpath(\'desc\', \'text()\')\n            yield loader.load_item()\n\n\n# callback fired when the spider is closed\ndef callback(spider, reason):\n    stats = spider.crawler.stats.get_stats()  # collect/log stats?\n\n    # stop the reactor\n    reactor.stop()\n\n\n# instantiate settings and provide a custom configuration\nsettings = Settings()\nsettings.set(\'ITEM_PIPELINES\', {\n    \'__main__.JsonWriterPipeline\': 100\n})\n\n# instantiate a crawler passing in settings\ncrawler = Crawler(settings)\n\n# instantiate a spider\nspider = DmozSpider()\n\n# configure signals\ncrawler.signals.connect(callback, signal=signals.spider_closed)\n\n# configure and start the crawler\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\n\n# start logging\nlog.start()\n\n# start the reactor (blocks execution)\nreactor.run()\n\nRun it in a usual way:\npython runner.py\n\nand observe items exported to items.jl with the help of the pipeline:\n{""desc"": """", ""link"": ""/"", ""title"": ""Top""}\n{""link"": ""/Computers/"", ""title"": ""Computers""}\n{""link"": ""/Computers/Programming/"", ""title"": ""Programming""}\n{""link"": ""/Computers/Programming/Languages/"", ""title"": ""Languages""}\n{""link"": ""/Computers/Programming/Languages/Python/"", ""title"": ""Python""}\n...\n\nGist is available here (feel free to improve): \n\nSelf-contained minimum example script to run scrapy\n\n\nNotes:\nIf you define settings by instantiating a Settings() object - you\'ll get all the defaults Scrapy settings. But, if you want to, for example, configure an existing pipeline, or configure a DEPTH_LIMIT or tweak any other setting, you need to either set it in the script via settings.set() (as demonstrated in the example):\npipelines = {\n    \'mypackage.pipelines.FilterPipeline\': 100,\n    \'mypackage.pipelines.MySQLPipeline\': 200\n}\nsettings.set(\'ITEM_PIPELINES\', pipelines, priority=\'cmdline\')\n\nor, use an existing settings.py with all the custom settings preconfigured:\nfrom scrapy.utils.project import get_project_settings\n\nsettings = get_project_settings()\n\n\nOther useful links on the subject:\n\nHow to run Scrapy from within a Python script\nConfused about running Scrapy from within a Python script\nscrapy run spider from script\n\n', '\nYou may have better luck looking through the tutorial first, as opposed to the ""Scrapy at a glance"" webpage.\nThe tutorial implies that Scrapy is, in fact, a separate program. \nRunning the command scrapy startproject tutorial will create a folder called tutorial several files already set up for you. \nFor example, in my case, the modules/packages items, pipelines, settings and spiders have been added to the root package tutorial .\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\nThe TorrentItem class would be placed inside items.py, and the MininovaSpider class would go inside the spiders folder. \nOnce the project is set up, the command-line parameters for Scrapy appear to be fairly straightforward. They take the form:\nscrapy crawl <website-name> -o <output-file> -t <output-type>\n\nAlternatively, if you want to run scrapy without the overhead of creating a project directory, you can use the runspider command:\nscrapy runspider my_spider.py\n\n']",https://stackoverflow.com/questions/18838494/scrapy-very-basic-example,web-scraping
Scrape web page contents,"
I am developing a project, for which I want to scrape the contents of a website in the background and get some limited content from that scraped website. For example, in my page I have ""userid"" and ""password"" fields, by using those I will access my mail and scrape my inbox contents and display it in my page.
I done the above by using javascript alone. But when I click the sign in button the URL of my page (http://localhost/web/Login.html) is changed to the URL (http://mail.in.com/mails/inbox.php?nomail=....) which I am scraped. But I scrap the details without changing my url.
",73k,"
            20
        ","[""\nDefinitely go with PHP Simple HTML DOM Parser. It's fast, easy and super flexible. It basically sticks an entire HTML page in an object then you can access any element from that object.\nLike the example of the official site, to get all links on the main Google page:\n// Create DOM from URL or file\n$html = file_get_html('http://www.google.com/');\n\n// Find all images \nforeach($html->find('img') as $element) \n       echo $element->src . '<br>';\n\n// Find all links \nforeach($html->find('a') as $element) \n       echo $element->href . '<br>';\n\n"", '\nThe HTTP Request\nFirst, you make an HTTP request to get the content of the page. There are several ways to do that.\nfopen\nThe most basic way to send an HTTP request, is to use fopen. A main advantage is that you can set how many characters are read at a time, which can be useful when reading very large files. It\'s not the easiest thing to do correctly, though, and it\'s not recommended to do this unless you\'re reading very large files and fear running into memory issues.\n$fp = fopen(""http://www.4wtech.com/csp/web/Employee/Login.csp"", ""rb"");\nif (FALSE === $fp) {\n    exit(""Failed to open stream to URL"");\n}\n\n$result = \'\';\n\nwhile (!feof($fp)) {\n    $result .= fread($fp, 8192);\n}\nfclose($fp);\necho $result;\n\nfile_get_contents\nThe easiest way, is just using file_get_contents. If does more or less the same as fopen, but you have less options to choose from. A main advantage here is that it requires but one line of code.\n$result = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\');\necho $result;\n\nsockets\nIf you need more control of what headers are sent to the server, you can use sockets, in combination with fopen.\n$fp = fsockopen(""www.4wtech.com/csp/web/Employee/Login.csp"", 80, $errno, $errstr, 30);\nif (!$fp) {\n    $result = ""$errstr ($errno)<br />\\n"";\n} else {\n    $result = \'\';\n    $out = ""GET / HTTP/1.1\\r\\n"";\n    $out .= ""Host: www.4wtech.com/csp/web/Employee/Login.csp\\r\\n"";\n    $out .= ""Connection: Close\\r\\n\\r\\n"";\n    fwrite($fp, $out);\n    while (!feof($fp)) {\n        $result .= fgets($fp, 128);\n    }\n    fclose($fp);\n}\necho $result;\n\nstreams\nAlternatively, you can also use streams. Streams are similar to sockets and can be used in combination with both fopen and file_get_contents.\n$opts = array(\n  \'http\'=>array(\n    \'method\'=>""GET"",\n    \'header\'=>""Accept-language: en\\r\\n"" .\n              ""Cookie: foo=bar\\r\\n""\n  )\n);\n\n$context = stream_context_create($opts);\n\n$result = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\', false, $context);\necho result;\n\ncURL\nIf your server supports cURL (it usually does), it is recommended to use cURL. A key advantage of using cURL, is that it relies on a popular C library commonly used in other programming languages. It also provides a convenient way for creating request headers, and auto-parses response headers, with a simple interface in case of errors.\n$defaults = array( \n    CURLOPT_URL, ""http://www.4wtech.com/csp/web/Employee/Login.csp""\n    CURLOPT_HEADER=> 0\n);\n\n$ch = curl_init(); \ncurl_setopt_array($ch, ($options + $defaults)); \nif( ! $result = curl_exec($ch)) { \n    trigger_error(curl_error($ch)); \n} \ncurl_close($ch); \necho $result; \n\nLibraries\nAlternatively, you can use one of many PHP libraries. I wouldn\'t recommend using a library, though, as it\'s likely to be overkill. In most cases, you\'re better off writing your own HTTP class using cURL under the hood.\n\nThe HTML parsing\nPHP has a convenient way to load any HTML into a DOMDocument.\n$pagecontent = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\');\n$doc = new DOMDocument();\n$doc->loadHTML($pagecontent);\necho $doc->saveHTML();\n\nUnfortunately, PHP support for HTML5 is limited. If you run into errors trying to parse your page content, consider using a third party library. For that, I can recommend Masterminds/html5-php. Parsing an HTML file with this library is very similar to parsing an HTML file with DOMDocument.\nuse Masterminds\\HTML5;\n\n$pagecontent = file_get_contents(\'http://www.4wtech.com/csp/web/Employee/Login.csp\');\n$html5 = new HTML5();\n$dom = $html5->loadHTML($html);\necho $html5->saveHTML($dom);\n\nAlternatively, you can use eg. my library PHPPowertools/DOM-Query. It uses customized version of Masterminds/html5-php under the hood parsing an HTML5 string into a DomDocument and symfony/DomCrawler for conversion of CSS selectors to XPath selectors. It always uses the same DomDocument, even when passing one object to another, to ensure decent performance.\nnamespace PowerTools;\n\n// Get file content\n$pagecontent = file_get_contents( \'http://www.4wtech.com/csp/web/Employee/Login.csp\' );\n\n// Define your DOMCrawler based on file string\n$H = new DOM_Query( $pagecontent );\n\n// Define your DOMCrawler based on an existing DOM_Query instance\n$H = new DOM_Query( $H->select(\'body\') );\n\n// Passing a string (CSS selector)\n$s = $H->select( \'div.foo\' );\n\n// Passing an element object (DOM Element)\n$s = $H->select( $documentBody );\n\n// Passing a DOM Query object\n$s = $H->select( $H->select(\'p + p\') );\n\n// Select the body tag\n$body = $H->select(\'body\');\n\n// Combine different classes as one selector to get all site blocks\n$siteblocks = $body->select(\'.site-header, .masthead, .site-body, .site-footer\');\n\n// Nest your methods just like you would with jQuery\n$siteblocks->select(\'button\')->add(\'span\')->addClass(\'icon icon-printer\');\n\n// Use a lambda function to set the text of all site blocks\n$siteblocks->text(function( $i, $val) {\n    return $i . "" - "" . $val->attr(\'class\');\n});\n\n// Append the following HTML to all site blocks\n$siteblocks->append(\'<div class=""site-center""></div>\');\n\n// Use a descendant selector to select the site\'s footer\n$sitefooter = $body->select(\'.site-footer > .site-center\');\n\n// Set some attributes for the site\'s footer\n$sitefooter->attr(array(\'id\' => \'aweeesome\', \'data-val\' => \'see\'));\n\n// Use a lambda function to set the attributes of all site blocks\n$siteblocks->attr(\'data-val\', function( $i, $val) {\n    return $i . "" - "" . $val->attr(\'class\') . "" - photo by Kelly Clark"";\n});\n\n// Select the parent of the site\'s footer\n$sitefooterparent = $sitefooter->parent();\n\n// Remove the class of all i-tags within the site\'s footer\'s parent\n$sitefooterparent->select(\'i\')->removeAttr(\'class\');\n\n// Wrap the site\'s footer within two nex selectors\n$sitefooter->wrap(\'<section><div class=""footer-wrapper""></div></section>\');\n\n', '\nYou can use the cURL extension of PHP to do HTTP requests to another web site from within your PHP page script. See the documentation here.\nOf course the downside here is that your site will respond slowly because you will have to scrape the external web site before you can present the full page/output to your user.\n', ""\nHave you tried OutWit Hub? It's a whole scraping environment. You can let it try to guess the structure or develop your own scrapers. I really suggest you have a look at it. It made my life much simpler.\nZR\n""]",https://stackoverflow.com/questions/584826/scrape-web-page-contents,web-scraping
How can I catch and process the data from the XHR responses using casperjs?,"
The data on the webpage is displayed dynamically and it seems that checking for every change in the html and extracting the data is a very daunting task and also needs me to use very unreliable XPaths. So I would want to be able to extract the data from the XHR packets. 
I hope to be able to extract information from XHR packets as well as generate 'XHR' packets to be sent to the server. 
The extracting information part is more important for me because the sending of information can be handled easily by automatically triggering html elements using casperjs.
I'm attaching a screenshot of what I mean.
The text in the response tab is the data I need to process afterwards. (This XHR response has been received from the server.)
",15k,"
            12
        ","['\nThis is not easily possible, because the resource.received event handler only provides meta data like url, headers or status, but not the actual data. The underlying phantomjs event handler acts the same way.\n\nStateless AJAX Request\nIf the ajax call is stateless, you may repeat the request\ncasper.on(""resource.received"", function(resource){\n    // somehow identify this request, here: if it contains "".json""\n    // it also also only does something when the stage is ""end"" otherwise this would be executed two times\n    if (resource.url.indexOf("".json"") != -1 && resource.stage == ""end"") {\n        var data = casper.evaluate(function(url){\n            // synchronous GET request\n            return __utils__.sendAJAX(url, ""GET"");\n        }, resource.url);\n        // do something with data, you might need to JSON.parse(data)\n    }\n});\ncasper.start(url); // your script\n\nYou may want to add the event listener to resource.requested. That way you don\'t need to way for the call to complete.\nYou can also do this right inside of the control flow like this (source: A: CasperJS waitForResource: how to get the resource i\'ve waited for):\ncasper.start(url);\n\nvar res, resData;\ncasper.waitForResource(function check(resource){\n    res = resource;\n    return resource.url.indexOf("".json"") != -1;\n}, function then(){\n    resData = casper.evaluate(function(url){\n        // synchronous GET request\n        return __utils__.sendAJAX(url, ""GET"");\n    }, res.url);\n    // do something with the data here or in a later step\n});\n\ncasper.run();\n\n\nStateful AJAX Request\nIf it is not stateless, you would need to replace the implementation of XMLHttpRequest. You will need to inject your own implementation of the onreadystatechange handler, collect the information in the page window object and later collect it in another evaluate call.\nYou may want to look at the XHR faker in sinon.js or use the following complete proxy for XMLHttpRequest (I modeled it after method 3 from How can I create a XMLHttpRequest wrapper/proxy?):\nfunction replaceXHR(){\n    (function(window, debug){\n        function args(a){\n            var s = """";\n            for(var i = 0; i < a.length; i++) {\n                s += ""\\t\\n["" + i + ""] => "" + a[i];\n            }\n            return s;\n        }\n        var _XMLHttpRequest = window.XMLHttpRequest;\n\n        window.XMLHttpRequest = function() {\n            this.xhr = new _XMLHttpRequest();\n        }\n\n        // proxy ALL methods/properties\n        var methods = [ \n            ""open"", \n            ""abort"", \n            ""setRequestHeader"", \n            ""send"", \n            ""addEventListener"", \n            ""removeEventListener"", \n            ""getResponseHeader"", \n            ""getAllResponseHeaders"", \n            ""dispatchEvent"", \n            ""overrideMimeType""\n        ];\n        methods.forEach(function(method){\n            window.XMLHttpRequest.prototype[method] = function() {\n                if (debug) console.log(""ARGUMENTS"", method, args(arguments));\n                if (method == ""open"") {\n                    this._url = arguments[1];\n                }\n                return this.xhr[method].apply(this.xhr, arguments);\n            }\n        });\n\n        // proxy change event handler\n        Object.defineProperty(window.XMLHttpRequest.prototype, ""onreadystatechange"", {\n            get: function(){\n                // this will probably never called\n                return this.xhr.onreadystatechange;\n            },\n            set: function(onreadystatechange){\n                var that = this.xhr;\n                var realThis = this;\n                that.onreadystatechange = function(){\n                    // request is fully loaded\n                    if (that.readyState == 4) {\n                        if (debug) console.log(""RESPONSE RECEIVED:"", typeof that.responseText == ""string"" ? that.responseText.length : ""none"");\n                        // there is a response and filter execution based on url\n                        if (that.responseText && realThis._url.indexOf(""whatever"") != -1) {\n                            window.myAwesomeResponse = that.responseText;\n                        }\n                    }\n                    onreadystatechange.call(that);\n                };\n            }\n        });\n\n        var otherscalars = [\n            ""onabort"",\n            ""onerror"",\n            ""onload"",\n            ""onloadstart"",\n            ""onloadend"",\n            ""onprogress"",\n            ""readyState"",\n            ""responseText"",\n            ""responseType"",\n            ""responseXML"",\n            ""status"",\n            ""statusText"",\n            ""upload"",\n            ""withCredentials"",\n            ""DONE"",\n            ""UNSENT"",\n            ""HEADERS_RECEIVED"",\n            ""LOADING"",\n            ""OPENED""\n        ];\n        otherscalars.forEach(function(scalar){\n            Object.defineProperty(window.XMLHttpRequest.prototype, scalar, {\n                get: function(){\n                    return this.xhr[scalar];\n                },\n                set: function(obj){\n                    this.xhr[scalar] = obj;\n                }\n            });\n        });\n    })(window, false);\n}\n\nIf you want to capture the AJAX calls from the very beginning, you need to add this to one of the first event handlers\ncasper.on(""page.initialized"", function(resource){\n    this.evaluate(replaceXHR);\n});\n\nor evaluate(replaceXHR) when you need it.\nThe control flow would look like this:\nfunction replaceXHR(){ /* from above*/ }\n\ncasper.start(yourUrl, function(){\n    this.evaluate(replaceXHR);\n});\n\nfunction getAwesomeResponse(){\n    return this.evaluate(function(){\n        return window.myAwesomeResponse;\n    });\n}\n\n// stops waiting if window.myAwesomeResponse is something that evaluates to true\ncasper.waitFor(getAwesomeResponse, function then(){\n    var data = JSON.parse(getAwesomeResponse());\n    // Do something with data\n});\n\ncasper.run();\n\nAs described above, I create a proxy for XMLHttpRequest so that every time it is used on the page, I can do something with it. The page that you scrape uses the xhr.onreadystatechange callback to receive data. The proxying is done by defining a specific setter function which writes the received data to window.myAwesomeResponse in the page context. The only thing you need to do is retrieving this text.\n\nJSONP Request\nWriting a proxy for JSONP is even easier, if you know the prefix (the function to call with the loaded JSON e.g. insert({""data"":[""Some"", ""JSON"", ""here""],""id"":""asdasda"")). You can overwrite insert in the page context\n\nafter the page is loaded\ncasper.start(url).then(function(){\n    this.evaluate(function(){\n        var oldInsert = insert;\n        insert = function(json){\n            window.myAwesomeResponse = json;\n            oldInsert.apply(window, arguments);\n        };\n    });\n}).waitFor(getAwesomeResponse, function then(){\n    var data = JSON.parse(getAwesomeResponse());\n    // Do something with data\n}).run();\n\nor before the request is received (if the function is registered just before the request is invoked)\ncasper.on(""resource.requested"", function(resource){\n    // filter on the correct call\n    if (resource.url.indexOf("".jsonp"") != -1) {\n        this.evaluate(function(){\n            var oldInsert = insert;\n            insert = function(json){\n                window.myAwesomeResponse = json;\n                oldInsert.apply(window, arguments);\n            };\n        });\n    }\n}).run();\n\ncasper.start(url).waitFor(getAwesomeResponse, function then(){\n    var data = JSON.parse(getAwesomeResponse());\n    // Do something with data\n}).run();\n\n\n', '\nI may be late into the party, but the answer may help someone like me who would fall into this problem later in future.\nI had to start with PhantomJS, then moved to CasperJS but finally settled with SlimerJS. Slimer is based on Phantom, is compatible with Casper, and can send you back the response body using the same onResponseReceived method, in ""response.body"" part.\nReference: https://docs.slimerjs.org/current/api/webpage.html#webpage-onresourcereceived\n', '\n@Artjom\'s answer\'s doesn\'t work for me in the recent Chrome and CasperJS versions.\nBased on @Artjom\'s answer and based on gilly3\'s answer on how to replace XMLHttpRequest, I have composed a new solution that should work in most/all versions of the different browsers. Works for me.\nSlimerJS cannot work on newer version of FireFox, therefore no good for me.\nHere is the the generic code to add a listner to load of XHR (not dependent on CasperJS):\nvar addXHRListener = function (XHROnStateChange) {\n\n    var XHROnLoad = function () {\n        if (this.readyState == 4) {\n            XHROnStateChange(this)\n        }\n    }\n\n    var open_original = XMLHttpRequest.prototype.open;\n\n    XMLHttpRequest.prototype.open = function (method, url, async, unk1, unk2) {\n        this.requestUrl = url\n        open_original.apply(this, arguments);\n    };\n\n    var xhrSend = XMLHttpRequest.prototype.send;\n    XMLHttpRequest.prototype.send = function () {\n\n        var xhr = this;\n        if (xhr.addEventListener) {\n            xhr.removeEventListener(""readystatechange"", XHROnLoad);\n            xhr.addEventListener(""readystatechange"", XHROnLoad, false);\n        } else {\n            function readyStateChange() {\n                if (handler) {\n                    if (handler.handleEvent) {\n                        handler.handleEvent.apply(xhr, arguments);\n                    } else {\n                        handler.apply(xhr, arguments);\n                    }\n                }\n                XHROnLoad.apply(xhr, arguments);\n                setReadyStateChange();\n            }\n\n            function setReadyStateChange() {\n                setTimeout(function () {\n                    if (xhr.onreadystatechange != readyStateChange) {\n                        handler = xhr.onreadystatechange;\n                        xhr.onreadystatechange = readyStateChange;\n                    }\n                }, 1);\n            }\n\n            var handler;\n            setReadyStateChange();\n        }\n        xhrSend.apply(xhr, arguments);\n    };\n\n}\n\nHere is CasperJS code to emit a custom event on load of XHR:\ncasper.on(""page.initialized"", function (resource) {\n    var emitXHRLoad = function (xhr) {\n        window.callPhantom({eventName: \'xhr.load\', eventData: xhr})\n    }\n    this.evaluate(addXHRListener, emitXHRLoad);\n});\n\ncasper.on(\'remote.callback\', function (data) {\n    casper.emit(data.eventName, data.eventData)\n});\n\nHere is a code to listen to ""xhr.load"" event and get the XHR response body:\ncasper.on(\'xhr.load\', function (xhr) {\n    console.log(\'xhr load\', xhr.requestUrl)\n    console.log(\'xhr load\', xhr.responseText)\n});\n\n', '\nAdditionally, you can also directly download the content and manipulate it later. \nHere is the example of the script I am using to retrieve a JSON and save it locally :\n\n\nvar casper = require(\'casper\').create({\r\n    pageSettings: {\r\n        webSecurityEnabled: false\r\n    }\r\n});\r\n\r\nvar url = \'https://twitter.com/users/username_available?username=whatever\';\r\n\r\ncasper.start(\'about:blank\', function() {\r\n   this.download(url, ""hop.json"");\r\n});\r\n\r\ncasper.run(function() {\r\n    this.echo(\'Done.\').exit();\r\n});\n\n\n']",https://stackoverflow.com/questions/24555370/how-can-i-catch-and-process-the-data-from-the-xhr-responses-using-casperjs,web-scraping
Scrape website with dynamic mouseover event,"
I am trying to scrape data which is generated dynamically from mouseover events. 
I want to capture the information from the Hash Rate Distribution chart from
https://slushpool.com/stats/?c=btc which is generated when you scroll over each circle.  
The code below gets the html data from the website, and returns the table which is filled once the mouse passes over a circle. However, I have not been able to figure out how to trigger the mouseover event for each circle to fill the table.
from lxml import etree
from xml.etree import ElementTree
from selenium import webdriver

driver_path = ""#Firefox web driver""
browser = webdriver.Firefox(executable_path=driver_path)
browser.get(""https://slushpool.com/stats/?c=btc"") 


page = browser.page_source #Get page html 
tree = etree.HTML(page) #create etree

table_Xpath = '/html/body/div[1]/div/div/div/div/div[5]/div[1]/div/div/div[2]/div[2]/div[2]/div/table'

table =tree.xpath(table_Xpath) #get table using Xpath

print(ElementTree.tostring(table[0])) #Returns empty table. 
#Should return data from each mouseover event

Is there a way to trigger the mouseover event for each circle, then extract the generated data.
Thank you in advance for the help!
",3k,"
            3
        ","['\nTo trigger the mouseover event for each circle you have to induce WebDriverWait for the visibility_of_all_elements_located() and you can use the following Locator Strategies:\n\nCode Block:\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.action_chains import ActionChains\n\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument(""start-maximized"")\nchrome_options.add_experimental_option(""excludeSwitches"", [""enable-automation""])\nchrome_options.add_experimental_option(\'useAutomationExtension\', False)\ndriver = webdriver.Chrome(options=chrome_options, executable_path=r\'C:\\Utility\\BrowserDrivers\\chromedriver.exe\')\ndriver.get(""https://slushpool.com/stats/?c=btc"")\ndriver.execute_script(""return arguments[0].scrollIntoView(true);"", WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, ""//h1//span[text()=\'Distribution\']""))))\nelements = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.XPATH, ""//h1//span[text()=\'Distribution\']//following::div[1]/*[name()=\'svg\']//*[name()=\'g\']//*[name()=\'g\' and @class=\'paper\']//*[name()=\'circle\']"")))\nfor element in elements:\n    ActionChains(driver).move_to_element(element).perform()\n\nBrowser Snapshot:\n\n\n', '\nThis is the circle locator you mean:\n.find_element_by_css_selector(\'._1p0PmxVw._3GzjmWLG\')\n\nBut it will change because mouseover effect, to be:\n.find_element_by_css_selector(\'._1p0PmxVw._3GzjmWLG._1suU9Mx1\')\n\n\nSo you need wait until the element to changed for each move.\nAnd the most important is how to inspect a hover element, then you can get the bellow:\n\nAnd causes the element for get data you mean to be appear:\nxpath: //div[@class=""_3jGHi0co _1zbokARu"" and contains(@style,""display: block"")]\n\nYou can use ActionChains to perform move the element.\nFinally you can try the bellow code:\nbrowser.get(\'https://slushpool.com/stats/?c=btc\')\nbrowser.maximize_window()\n\n#wait all circle\nelements = WebDriverWait(browser, 20).until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, \'._1p0PmxVw._3GzjmWLG\')))\ntable = browser.find_element_by_class_name(\'paper\')\n\n#move perform -> to table\nbrowser.execute_script(""arguments[0].scrollIntoView(true);"", table)\n\ndata = []\nfor circle in elements:\n    #move perform -> to each circle\n    ActionChains(browser).move_to_element(circle).perform()\n    # wait change mouseover effect\n    mouseover = WebDriverWait(browser, 5).until(EC.visibility_of_element_located((By.XPATH, \'//div[@class=""_3jGHi0co _1zbokARu"" and contains(@style,""display: block"")]\')))\n    data.append(mouseover.text)\n\nprint(data[0])\nprint(data)\n\nFollowing import:\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver import ActionChains\n\nConsole output:\n\nFirst data > data[0]\n536.9 Ph/s - 1.074 Eh/s\nUser Count 2\nAverage Hash Rate 546.1 Ph/s\nGroup Hash Rate 1.092 Eh/s\nAll data > data\n\n[u\'536.9 Ph/s - 1.074 Eh/s\\nUser Count 2\\nAverage Hash Rate 546.9 Ph/s\\nGroup Hash Rate 1.094 Eh/s\', u\'67.11 Ph/s - 134.2 Ph/s\\nUser Count 14\\nAverage Hash Rate 91.27 Ph/s\\nGroup Hash Rate 1.278 Eh/s\', u\'67.11 Ph/s - 134.2 Ph/s\\nUser Count 14\\nAverage Hash Rate 91.27 Ph/s\\nGroup Hash Rate 1.278 Eh/s\', u\'16.78 Ph/s - 33.55 Ph/s\\nUser Count 23\\nAverage Hash Rate 23.36 Ph/s\\nGroup Hash Rate 537.2 Ph/s\', u\'8.389 Ph/s - 16.78 Ph/s\\nUser Count 33\\nAverage Hash Rate 11.80 Ph/s\\nGroup Hash Rate 389.4 Ph/s\', u\'4.194 Ph/s - 8.389 Ph/s\\nUser Count 67\\nAverage Hash Rate 5.704 Ph/s\\nGroup Hash Rate 382.2 Ph/s\', u\'2.097 Ph/s - 4.194 Ph/s\\nUser Count 137\\nAverage Hash Rate 2.959 Ph/s\\nGroup Hash Rate 405.3 Ph/s\', u\'1.049 Ph/s - 2.097 Ph/s\\nUser Count 233\\nAverage Hash Rate 1.475 Ph/s\\nGroup Hash Rate 343.7 Ph/s\', u\'1.049 Ph/s - 2.097 Ph/s\\nUser Count 233\\nAverage Hash Rate 1.475 Ph/s\\nGroup Hash Rate 343.7 Ph/s\', u\'524.3 Th/s - 1.049 Ph/s\\nUser Count 397\\nAverage Hash Rate 731.4 Th/s\\nGroup Hash Rate 290.4 Ph/s\', u\'262.1 Th/s - 524.3 Th/s\\nUser Count 745\\nAverage Hash Rate 360.3 Th/s\\nGroup Hash Rate 268.4 Ph/s\', u\'131.1 Th/s - 262.1 Th/s\\nUser Count 1479\\nAverage Hash Rate 182.7 Th/s\\nGroup Hash Rate 270.1 Ph/s\', u\'65.54 Th/s - 131.1 Th/s\\nUser Count 2351\\nAverage Hash Rate 92.47 Th/s\\nGroup Hash Rate 217.4 Ph/s\', u\'32.77 Th/s - 65.54 Th/s\\nUser Count 3107\\nAverage Hash Rate 47.23 Th/s\\nGroup Hash Rate 146.8 Ph/s\', u\'16.38 Th/s - 32.77 Th/s\\nUser Count 3380\\nAverage Hash Rate 25.24 Th/s\\nGroup Hash Rate 85.30 Ph/s\', u\'8.192 Th/s - 16.38 Th/s\\nUser Count 4276\\nAverage Hash Rate 13.00 Th/s\\nGroup Hash Rate 55.57 Ph/s\', u\'4.096 Th/s - 8.192 Th/s\\nUser Count 540\\nAverage Hash Rate 5.953 Th/s\\nGroup Hash Rate 3.215 Ph/s\', u\'2.048 Th/s - 4.096 Th/s\\nUser Count 284\\nAverage Hash Rate 3.193 Th/s\\nGroup Hash Rate 906.8 Th/s\', u\'1.024 Th/s - 2.048 Th/s\\nUser Count 226\\nAverage Hash Rate 1.368 Th/s\\nGroup Hash Rate 309.1 Th/s\', u\'512.0 Gh/s - 1.024 Th/s\\nUser Count 136\\nAverage Hash Rate 774.4 Gh/s\\nGroup Hash Rate 105.3 Th/s\', u\'256.0 Gh/s - 512.0 Gh/s\\nUser Count 116\\nAverage Hash Rate 401.5 Gh/s\\nGroup Hash Rate 46.57 Th/s\', u\'128.0 Gh/s - 256.0 Gh/s\\nUser Count 75\\nAverage Hash Rate 186.4 Gh/s\\nGroup Hash Rate 13.98 Th/s\', u\'64.00 Gh/s - 128.0 Gh/s\\nUser Count 78\\nAverage Hash Rate 96.39 Gh/s\\nGroup Hash Rate 7.518 Th/s\', u\'32.00 Gh/s - 64.00 Gh/s\\nUser Count 70\\nAverage Hash Rate 45.68 Gh/s\\nGroup Hash Rate 3.198 Th/s\', u\'16.00 Gh/s - 32.00 Gh/s\\nUser Count 48\\nAverage Hash Rate 23.37 Gh/s\\nGroup Hash Rate 1.122 Th/s\', u\'8.000 Gh/s - 16.00 Gh/s\\nUser Count 62\\nAverage Hash Rate 11.91 Gh/s\\nGroup Hash Rate 738.5 Gh/s\', u\'4.000 Gh/s - 8.000 Gh/s\\nUser Count 153\\nAverage Hash Rate 3.078 Gh/s\\nGroup Hash Rate 471.0 Gh/s\']\n\n']",https://stackoverflow.com/questions/57901045/scrape-website-with-dynamic-mouseover-event,web-scraping
Why does headless need to be false for Puppeteer to work?,"
I'm creating a web api that scrapes a given url and sends that back. I am using Puppeteer to do this. I asked this question: Puppeteer not behaving like in Developer Console
and recieved an answer that suggested it would only work if headless was set to be false. I don't want to be constantly opening up a browser UI i don't need (I just the need the data!) so I'm looking for why headless has to be false and can I get a fix that lets headless = true.
Here's my code:
express()
  .get(""/*"", (req, res) => {
    global.notBaseURL = req.params[0];
    (async () => {
      const browser = await puppet.launch({ headless: false }); // Line of Interest
      const page = await browser.newPage();
      console.log(req.params[0]);
      await page.goto(req.params[0], { waitUntil: ""networkidle2"" }); //this is the url
      title = await page.$eval(""title"", (el) => el.innerText);

      browser.close();

      res.send({
        title: title,
      });
    })();
  })
  .listen(PORT, () => console.log(`Listening on ${PORT}`));

This is the page I'm trying to scrape: https://www.nordstrom.com/s/zella-high-waist-studio-pocket-7-8-leggings/5460106?origin=coordinating-5460106-0-1-FTR-recbot-recently_viewed_snowplow_mvp&recs_placement=FTR&recs_strategy=recently_viewed_snowplow_mvp&recs_source=recbot&recs_page_type=category&recs_seed=0&color=BLACK
",3k,"
            2
        ","['\nThe reason it might work in UI mode but not headless is that sites who aggressively fight scraping will detect that you are running in a headless browser.\nSome possible workarounds:\nUse puppeteer-extra\nFound here: https://github.com/berstend/puppeteer-extra\nCheck out their docs for how to use it. It has a couple plugins that might help in getting past headless-mode detection:\n\npuppeteer-extra-plugin-anonymize-ua -- anonymizes your User Agent. Note that this might help with getting past headless mode detection, but as you\'ll see if you visit https://amiunique.org/ it is unlikely to be enough to keep you from being identified as a repeat visitor.\npuppeteer-extra-plugin-stealth -- this might help win the cat-and-mouse game of not being detected as headless. There are many tricks that are employed to detect headless mode, and as many tricks to evade them.\n\nRun a ""real"" Chromium instance/UI\nIt\'s possible to run a single browser UI in a manner that let\'s you attach puppeteer to that running instance. Here\'s an article that explains it: https://medium.com/@jaredpotter1/connecting-puppeteer-to-existing-chrome-window-8a10828149e0\nEssentially you\'re starting Chrome or Chromium (or Edge?) from the command line with --remote-debugging-port=9222 (or any old port?) plus other command line switches depending on what environment you\'re running it in. Then you use puppeteer to connect to that running instance instead of having it do the default behavior of launching a headless Chromium instance: const browser = await puppeteer.connect({ browserURL: ENDPOINT_URL });.  Read the puppeteer docs here for more info: https://pptr.dev/#?product=Puppeteer&version=v5.2.1&show=api-puppeteerlaunchoptions\nThe ENDPOINT_URL is displayed in the terminal when you launch the browser from the command line with the --remote-debugging-port=9222 option.\nThis option is going to require some server/ops mojo, so be prepared to do a lot more Stack Overflow searches. :-)\nThere are other strategies I\'m sure but those are the two I\'m most familiar with. Good luck!\n', '\nTodd\'s answer is thorough, but worth trying before resorting to some of the recommendations there is to slap on the following user agent line pulled from the relevant Puppeteer GitHub issue Different behavior between { headless: false } and { headless: true }:\nawait page.setUserAgent(""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36"");\nawait page.goto(yourURL);\n\nNow, the Nordstorm site provided by OP seems to be able to detect robots even with headless: false, at least at the present moment. But other sites are less strict and I\'ve found the above line to be useful on some of them as shown in Puppeteer can\'t find elements when Headless TRUE and Puppeteer , bringing back blank array.\nVisit the GH issue thread above for other ideas and see useragents.me for a rotating list of current user agents.\n']",https://stackoverflow.com/questions/63818869/why-does-headless-need-to-be-false-for-puppeteer-to-work,web-scraping
"How to import a table from web page (with ""div class"") to excel?","
I'm trying to import to Excel a list of exhibitors and countries from this webpage and I'm not getting it.
Can Someone help me?
I have tried the methods listed in this forum and doesn't work.
Sub test()

    Dim objIE As Object
    Dim hmtl As HTMLDocument

    Dim elements As IHTMLElementCollection

    Set objIE = New InternetExplorer
    objIE.Visible = True

    objIE.navigate ""https://sps.mesago.com/events/en/exhibitors_products/exhibitor-list.html""

    Application.StatusBar = ""Loading, Please wait...""

    While objIE.Busy
        DoEvents
    Wend
    Do
    Loop Until objIE.readyState = READYSTATE_COMPLETE

    Application.StatusBar = ""Importing data...""

    Set html = objIE.document

    'I try differents types and name - ByClassName(""...""), ByTagName(""...""), ...
    Set elements = html.getElementsByClassName(""list"") 

    For i = 0 To elements.Length - 1
         Sheet1.Range(""A"" & (i + 1)) = elements(i).innerText
    Next i

    objIE.Quit
    Set objIE = Nothing

    Application.StatusBar = """"

End Sub

Sorry about my English.
",3k,"
            0
        ","['\nYou don\'t need a browser to be opened. You can do this with XHR. The url I am using can be found in the network tab via F12 (Dev tools)\nIf you search that tab after making your request you will find that url and the response has a layout such as:\n\nimage link: https://i.stack.imgur.com/C8oLj.png\nI loop the rows and the columns to populate a 2d array (table like format) which I write out to the sheet in one go at end.\n\nVBA:\nOption Explicit\nPublic Sub GetExhibitorsInfo()\n    Dim ws As Worksheet, results(), i As Long, html As HTMLDocument\n\n    Set ws = ThisWorkbook.Worksheets(""Sheet1"")\n    Set html = New HTMLDocument\n\n    With CreateObject(""MSXML2.XMLHTTP"")\n        .Open ""GET"", ""https://sps.mesago.com/events/en/exhibitors_products/exhibitor-list.html"", False\n        .setRequestHeader ""User-Agent"", ""Mozilla/5.0""\n        .send\n        html.body.innerHTML = .responseText\n    End With\n\n    Dim rows As Object, html2 As HTMLDocument, columnsInfo As Object\n    Dim r As Long, c As Long, j As Long, headers(), columnCount As Long\n\n    headers = Array(""name2_kat"", ""art"", ""std_nr_sort"", ""kfzkz_kat"", ""halle"", _\n    ""sortierung_katalog"", ""std_nr"", ""ort_info_kat"", ""name3_kat"", ""webseite"", _\n    ""land_kat"", ""standbez1"", ""name1_kat"")\n    Set rows = html.querySelectorAll(""[data-entry]"")\n    Set html2 = New HTMLDocument\n    html2.body.innerHTML = rows.item(0).innerHTML\n    columnCount = html2.querySelectorAll(""[data-entry-key]"").length\n\n    ReDim results(1 To rows.length, 1 To columnCount)\n\n    For i = 0 To rows.length - 1\n        r = r + 1: c = 1\n        html2.body.innerHTML = rows.item(i).innerHTML\n        Set columnsInfo = html2.querySelectorAll(""[data-entry-key]"")\n        For j = 0 To columnsInfo.length - 1\n            results(r, c) = columnsInfo.item(j).innerText \'columnsInfo.item(j).getAttribute(""data-entry-key"")\n            c = c + 1\n        Next\n    Next\n    With ws\n        .Cells(1, 1).Resize(1, columnCount) = headers\n        .Cells(2, 1).Resize(UBound(results, 1), UBound(results, 2)) = results\n    End With\nEnd Sub\n\n']",https://stackoverflow.com/questions/56277464/how-to-import-a-table-from-web-page-with-div-class-to-excel,web-scraping
How would I import YouTube Likes and Dislikes and a ratio from YouTube onto Google Sheets? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 3 years ago.


This post was edited and submitted for review 5 months ago and failed to reopen the post:

Original close reason(s) were not resolved






                        Improve this question
                    



What would be the correct xpath to get  YouTube likes and dislikes from a video?
",3k,"
            -3
        ","['\nTITLE:\n=IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""//*[@id=\'eow-title\']"")\n\nor:\n=REGEXEXTRACT(QUERY(ARRAY_CONSTRAIN(IMPORTDATA(A12), 500, 1),\n ""where Col1 contains \'/title\'"", 0), "">(.+)<"")\n\n\nVIEWS:\n=VALUE(REGEXREPLACE(TEXT(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",\n ""//*[contains(@class, \'watch-view-count\')]""),0),"" view(s)?"",""""))\n\nDURATION:\n=SUBSTITUTE(REGEXREPLACE(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""//*[@itemprop=\'duration\']/@content""),""PT|S"",""""),""M"","":"")\n\nLIKES:\n=IF(ISNA(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-like-button\')])[1]""))=TRUE,0,\n         IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-like-button\')])[1]""))\n\nDISLIKES:\n=IF(ISNA(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-dislike-button\')])[1]""))=TRUE,0,\n         IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",""(//*[contains(@class,\'like-button-renderer-dislike-button\')])[1]""))\n\nUPLOADED:\n=REGEXREPLACE(IMPORTXML(""https://www.youtube.com/watch?v=MkgR0SxmMKo"",\n ""//*[contains(@class, \'watch-time-text\')]""),""((Uploaded)|(Published)|(Streamed live)) on "","""")\n\nSUBSCRIPTIONS:\n=IFERROR(MID(QUERY(IMPORTXML(""https://www.youtube.com/channel/""&A1,\n ""//div[@class=\'primary-header-actions\']""), ""select Col1""), 31, 20), )\n\n\nCHANNEL NAME:\n=INDEX(IMPORTHTML(""https://www.youtube.com/channel/UC7_gcs09iThXybpVgjHZ_7g"",""list"",1),1,1)\n\nCHANNEL ID:\n=ARRAYFORMULA(REGEXREPLACE(QUERY(SUBSTITUTE(ARRAY_CONSTRAIN(\n IMPORTDATA(https://www.youtube.com/watch?v=rckrnYw5sOA), 3000, 1), """""""", """"),\n ""where Col1 contains \'<meta itemprop=channelId content=\'""),\n ""<meta itemprop=channelId content=|>"", """"))\n\n\n\n\nUPDATE:\nchannel name (07/07/2021):\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(A4)), \n ""where Col1 contains \'\\x22channelName\\x22:\\x22\'"", 0), "":\\\\x22(.+)\\\\x22$"")\n\nvideo title (08/08/2021)\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(A1)), \n ""where Col1 starts with \'title:""""\'"", 0), """"""(.*)"""""")\n\n\nduration (21/04/2022)\n=TEXT(1*REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(B1)), \n ""where Col1 contains \'approxDurationMs\' limit 1"", ), \n ""\\d+"")/3600000/24, ""mm:ss"")\n\n\nchannel views (07/06/2022)\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTXML(A1, ""//*"")), \n ""where Col1 contains \'""&CHAR(10)&""Creators\'"", ), \n "".x22text.x22:.x22(.+).x22,.x22bold.x22"")\n\n\nvideo views (01/09/2022)\n=REGEXEXTRACT(QUERY(FLATTEN(IMPORTDATA(A1)); \n ""where Col1 starts with \'viewCount\'""; ); ""\\d+"")*1\n\n\n']",https://stackoverflow.com/questions/55060363/how-would-i-import-youtube-likes-and-dislikes-and-a-ratio-from-youtube-onto-goog,web-scraping
How do I prevent site scraping? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



I have a fairly large music website with a large artist database.  I've been noticing other music sites scraping our site's data (I enter dummy Artist names here and there and then do google searches for them).  
How can I prevent screen scraping?  Is it even possible?
",131k,"
            329
        ","['\nNote: Since the complete version of this answer exceeds Stack Overflow\'s length limit, you\'ll need to head  to GitHub to read the extended version, with more tips and details.\n\nIn order to hinder scraping (also known as Webscraping, Screenscraping, Web data mining, Web harvesting, or Web data extraction), it helps to know how these scrapers work, and \n, by extension, what prevents them from working well.\nThere\'s various types of scraper, and each works differently:\n\nSpiders, such as Google\'s bot or website copiers like HTtrack, which recursively follow links to other pages in order to get data. These are sometimes used for targeted scraping to get specific data, often in combination with a HTML parser to extract the desired data from each page.\nShell scripts: Sometimes, common Unix tools are used for scraping: Wget or Curl to download pages, and Grep (Regex) to extract the data.\nHTML parsers, such as ones based on Jsoup, Scrapy, and others. Similar to shell-script regex based ones, these work by extracting data from pages based on patterns in HTML, usually ignoring everything else. \nFor example: If your website has a search feature, such a scraper might submit a request for a search, and then get all the result links and their titles from the results page HTML, in order to specifically get only search result links and their titles. These are the most common.\nScreenscrapers, based on eg. Selenium or PhantomJS, which open your website in a real browser, run JavaScript, AJAX, and so on, and then get the desired text from the webpage, usually by:\n\nGetting the HTML from the browser after your page has been loaded and JavaScript has run, and then using a HTML parser to extract the desired data. These are the most common, and so many of the methods for breaking HTML parsers / scrapers also work here.\nTaking a screenshot of the rendered pages, and then using OCR to extract the desired text from the screenshot. These are rare, and only dedicated scrapers who really want your data will set this up.\n\nWebscraping services such as ScrapingHub or Kimono. In fact, there\'s people whose job is to figure out how to scrape your site and pull out the content for others to use.\nUnsurprisingly, professional scraping services are the hardest to deter, but if you make it hard and time-consuming to figure out how to scrape your site, these (and people who pay them to do so) may not be bothered to scrape your website.\nEmbedding your website in other site\'s pages with frames, and embedding your site in mobile apps. \nWhile not technically scraping, mobile apps (Android and iOS) can embed websites, and inject custom CSS and JavaScript, thus completely changing the appearance of your pages.\nHuman copy - paste: People will copy and paste your content in order to use it elsewhere.\n\nThere is a lot overlap between these different kinds of scraper, and many scrapers will behave similarly, even if they use different technologies and methods.\nThese tips mostly my own ideas, various difficulties that I\'ve encountered while writing scrapers, as well as bits of information and ideas from around the interwebs. \nHow to stop scraping\nYou can\'t completely prevent it, since whatever you do, determined scrapers can still figure out how to scrape. However, you can stop a lot of scraping by doing a few things:\nMonitor your logs & traffic patterns; limit access if you see unusual activity:\nCheck your logs regularly, and in case of unusual activity indicative of automated access (scrapers), such as many similar actions from the same IP address, you can block or limit access.\nSpecifically, some ideas:\n\nRate limiting:\nOnly allow users (and scrapers) to perform a limited number of actions in a certain time - for example, only allow a few searches per second from any specific IP address or user. This will slow down scrapers, and make them ineffective. You could also show a captcha if actions are completed too fast or faster than a real user would.\nDetect unusual activity:\nIf you see unusual activity, such as many similar requests from a specific IP address, someone looking at an excessive number of pages or performing an unusual number of searches, you can prevent access, or show a captcha for subsequent requests.\nDon\'t just monitor & rate limit by IP address - use other indicators too:\nIf you do block or rate limit, don\'t just do it on a per-IP address basis; you can use other indicators and methods to identify specific users or scrapers. Some indicators which can help you identify specific users / scrapers include:\n\nHow fast users fill out forms, and where on a button they click;\nYou can gather a lot of information with JavaScript, such as screen size / resolution, timezone, installed fonts, etc; you can use this to identify users.\nHTTP headers and their order, especially User-Agent.\n\nAs an example, if you get many request from a single IP address, all using the same User Agent, screen size (determined with JavaScript), and the user (scraper in this case) always clicks on the button in the same way and at regular intervals, it\'s probably a screen scraper; and you can temporarily block similar requests (eg. block all requests with that user agent and screen size coming from that particular IP address), and this way you won\'t inconvenience real users on that IP address, eg. in case of a shared internet connection.\nYou can also take this further, as you can identify similar requests, even if they come from different IP addresses, indicative of distributed scraping (a scraper using a botnet or a network of proxies). If you get a lot of otherwise identical requests, but they come from different IP addresses, you can block. Again, be aware of not inadvertently blocking real users.\nThis can be effective against screenscrapers which run JavaScript, as you can get a lot of information from them.\nRelated questions on Security Stack Exchange:\n\nHow to uniquely identify users with the same external IP address? for more details, and \nWhy do people use IP address bans when IP addresses often change? for info on the limits of these methods.\n\nInstead of temporarily blocking access, use a Captcha:\nThe simple way to implement rate-limiting would be to temporarily block access for a certain amount of time, however using a Captcha may be better, see the section on Captchas further down.\n\nRequire registration & login\nRequire account creation in order to view your content, if this is feasible for your site. This is a good deterrent for scrapers, but is also a good deterrent for real users.\n\nIf you require account creation and login, you can accurately track user and scraper actions. This way, you can easily detect when a specific account is being used for scraping, and ban it. Things like rate limiting or detecting abuse (such as a huge number of searches in a short time) become easier, as you can identify specific scrapers instead of just IP addresses.\n\nIn order to avoid scripts creating many accounts, you should:\n\nRequire an email address for registration, and verify that email address by sending a link that must be opened in order to activate the account. Allow only one account per email address.\nRequire a captcha to be solved during registration / account creation.\n\nRequiring account creation to view content will drive users and search engines away; if you require account creation in order to view an article, users will go elsewhere.\nBlock access from cloud hosting and scraping service IP addresses\nSometimes, scrapers will be run from web hosting services, such as Amazon Web Services or GAE, or VPSes.  Limit access to your website (or show a captcha) for requests originating from the IP addresses used by such cloud hosting services.\nSimilarly, you can also limit access from IP addresses used by proxy or VPN providers, as scrapers may use such proxy servers to avoid many requests being detected.\nBeware that by blocking access from proxy servers and VPNs, you will negatively affect real users.\nMake your error message nondescript if you do block\nIf you do block / limit access, you should ensure that you don\'t tell the scraper what caused the block, thereby giving them clues as to how to fix their scraper. So a bad idea would be to show error pages with text like:\n\nToo many requests from your IP address, please try again later.\nError, User Agent header not present !\n\nInstead, show a friendly error message that doesn\'t tell the scraper what caused it. Something like this is much better:\n\nSorry, something went wrong. You can contact support via helpdesk@example.com, should the problem persist.\n\nThis is also a lot more user friendly for real users, should they ever see such an error page. You should also consider showing a captcha for subsequent requests instead of a hard block, in case a real user sees the error message, so that you don\'t block and thus cause legitimate users to contact you.\nUse Captchas if you suspect that your website is being accessed by a scraper.\nCaptchas (""Completely Automated Test to Tell Computers and Humans apart"") are very effective against stopping scrapers. Unfortunately, they are also very effective at irritating users. \nAs such, they are useful when you suspect a possible scraper, and want to stop the scraping, without also blocking access in case it isn\'t a scraper but a real user. You might want to consider showing a captcha before allowing access to the content if you suspect a scraper.\nThings to be aware of when using Captchas:\n\nDon\'t roll your own, use something like Google\'s reCaptcha : It\'s a lot easier than implementing a captcha yourself, it\'s more user-friendly than some blurry and warped text solution you might come up with yourself (users often only need to tick a box), and it\'s also a lot harder for a scripter to solve than a simple image served from your site\nDon\'t include the solution to the captcha in the HTML markup: I\'ve actually seen one website which had the solution for the captcha in the page itself, (although quite well hidden) thus making it pretty useless. Don\'t do something like this. Again, use a service like reCaptcha, and you won\'t have this kind of problem (if you use it properly).\nCaptchas can be solved in bulk: There are captcha-solving services where actual, low-paid, humans solve captchas in bulk. Again, using reCaptcha is a good idea here, as they have protections (such as the relatively short time the user has in order to solve the captcha). This kind of service is unlikely to be used unless your data is really valuable.\n\nServe your text content as an image\nYou can render text into an image server-side, and serve that to be displayed, which will hinder simple scrapers extracting text.\nHowever, this is bad for screen readers, search engines, performance, and pretty much everything else. It\'s also illegal in some places (due to accessibility, eg. the Americans with Disabilities Act), and it\'s also easy to circumvent with some OCR, so don\'t do it. \nYou can do something similar with CSS sprites, but that suffers from the same problems.\nDon\'t expose your complete dataset:\nIf feasible, don\'t provide a way for a script / bot to get all of your dataset. As an example: You have a news site, with lots of individual articles. You could make those articles be only accessible by searching for them via the on site search, and, if you don\'t have a list of all the articles on the site and their URLs anywhere, those articles will be only accessible by using the search feature. This means that a script wanting to get all the articles off your site will have to do searches for all possible phrases which may appear in your articles in order to find them all, which will be time-consuming, horribly inefficient, and will hopefully make the scraper give up.\nThis will be ineffective if:\n\nThe bot / script does not want / need the full dataset anyway.\nYour articles are served from a URL which looks something like example.com/article.php?articleId=12345. This (and similar things) which will allow scrapers to simply iterate over all the articleIds and request all the articles that way.\nThere are other ways to eventually find all the articles, such as by writing a script to follow links within articles which lead to other articles.\nSearching for something like ""and"" or ""the"" can reveal almost everything, so that is something to be aware of. (You can avoid this by only returning the top 10 or 20 results).\nYou need search engines to find your content.\n\nDon\'t expose your APIs, endpoints, and similar things:\nMake sure you don\'t expose any APIs, even unintentionally. For example, if you are using AJAX or network requests from within Adobe Flash or Java Applets (God forbid!) to load your data it is trivial to look at the network requests from the page and figure out where those requests are going to, and then reverse engineer and use those endpoints in a scraper program. Make sure you obfuscate your endpoints and make them hard for others to use, as described.\nTo deter HTML parsers and scrapers:\nSince HTML parsers work by extracting content from pages based on identifiable patterns in the HTML, we can intentionally change those patterns in oder to break these scrapers, or even screw with them. Most of these tips also apply to other scrapers like spiders and screenscrapers too.\nFrequently change your HTML\nScrapers which process HTML directly do so by extracting contents from specific, identifiable parts of your HTML page. For example: If all pages on your website have a div with an id of article-content, which contains the text of the article, then it is trivial to write a script to visit all the article pages on your site, and extract the content text of the article-content div on each article page, and voilà, the scraper has all the articles from your site in a format that can be reused elsewhere.\nIf you change the HTML and the structure of your pages frequently, such scrapers will no longer work.\n\nYou can frequently change the id\'s and classes of elements in your HTML, perhaps even automatically. So, if your div.article-content becomes something like div.a4c36dda13eaf0, and changes every week, the scraper will work fine initially, but will break after a week. Make sure to change the length of your ids / classes too, otherwise the scraper will use div.[any-14-characters] to find the desired div instead. Beware of other similar holes too..\nIf there is no way to find the desired content from the markup, the scraper will do so from the way the HTML is structured. So, if all your article pages are similar in that every div inside a div which comes after a h1 is the article content, scrapers will get the article content based on that. Again, to break this, you can add / remove extra markup to your HTML, periodically and randomly, eg. adding extra divs or spans. With modern server side HTML processing, this should not be too hard.\n\nThings to be aware of:\n\nIt will be tedious and difficult to implement, maintain, and debug.\nYou will hinder caching. Especially if you change ids or classes of your HTML elements, this will require corresponding changes in your CSS and JavaScript files, which means that every time you change them, they will have to be re-downloaded by the browser. This will result in longer page load times for repeat visitors, and increased server load. If you only change it once a week, it will not be a big problem.\nClever scrapers will still be able to get your content by inferring where the actual content is, eg. by knowing that a large single block of text on the page is likely to be the actual article. This makes it possible to still find & extract the desired data from the page. Boilerpipe does exactly this.\n\nEssentially, make sure that it is not easy for a script to find the actual, desired content for every similar page.\nSee also How to prevent crawlers depending on XPath from getting page contents for details on how this can be implemented in PHP.\nChange your HTML based on the user\'s location\nThis is sort of similar to the previous tip. If you serve different HTML based on your user\'s location / country (determined by IP address), this may break scrapers which are delivered to users. For example, if someone is writing a mobile app which scrapes data from your site, it will work fine initially, but break when it\'s actually distributed to users, as those users may be in a different country, and thus get different HTML, which the embedded scraper was not designed to consume.\nFrequently change your HTML, actively screw with the scrapers by doing so !\nAn example: You have a search feature on your website, located at example.com/search?query=somesearchquery, which returns the following HTML:\n<div class=""search-result"">\n  <h3 class=""search-result-title"">Stack Overflow has become the world\'s most popular programming Q & A website</h3>\n  <p class=""search-result-excerpt"">The website Stack Overflow has now become the most popular programming Q & A website, with 10 million questions and many users, which...</p>\n  <a class""search-result-link"" href=""/stories/story-link"">Read more</a>\n</div>\n(And so on, lots more identically structured divs with search results)\n\nAs you may have guessed this is easy to scrape: all a scraper needs to do is hit the search URL with a query, and extract the desired data from the returned HTML. In addition to periodically changing the HTML as described above, you could also leave the old markup with the old ids and classes in, hide it with CSS, and fill it with fake data, thereby poisoning the scraper. Here\'s how the search results page could be changed:\n<div class=""the-real-search-result"">\n  <h3 class=""the-real-search-result-title"">Stack Overflow has become the world\'s most popular programming Q & A website</h3>\n  <p class=""the-real-search-result-excerpt"">The website Stack Overflow has now become the most popular programming Q & A website, with 10 million questions and many users, which...</p>\n  <a class""the-real-search-result-link"" href=""/stories/story-link"">Read more</a>\n</div>\n\n<div class=""search-result"" style=""display:none"">\n  <h3 class=""search-result-title"">Visit Example.com now, for all the latest Stack Overflow related news !</h3>\n  <p class=""search-result-excerpt"">Example.com is so awesome, visit now !</p>\n  <a class""search-result-link"" href=""http://example.com/"">Visit Now !</a>\n</div>\n(More real search results follow)\n\nThis will mean that scrapers written to extract data from the HTML based on classes or IDs will continue to seemingly work, but they will get fake data or even ads, data which real users will never see, as they\'re hidden with CSS.\nScrew with the scraper: Insert fake, invisible honeypot data into your page\nAdding on to the previous example, you can add invisible honeypot items to your HTML to catch scrapers. An example which could be added to the previously described search results page:\n<div class=""search-result"" style=""display:none"">\n  <h3 class=""search-result-title"">This search result is here to prevent scraping</h3>\n  <p class=""search-result-excerpt"">If you\'re a human and see this, please ignore it. If you\'re a scraper, please click the link below :-)\n  Note that clicking the link below will block access to this site for 24 hours.</p>\n  <a class""search-result-link"" href=""/scrapertrap/scrapertrap.php"">I\'m a scraper !</a>\n</div>\n(The actual, real, search results follow.)\n\nA scraper written to get all the search results will pick this up, just like any of the other, real search results on the page, and visit the link, looking for the desired content. A real human will never even see it in the first place (due to it being hidden with CSS), and won\'t visit the link. A genuine and desirable spider such as Google\'s will not visit the link either because you disallowed /scrapertrap/ in your robots.txt.\nYou can make your scrapertrap.php do something like block access for the IP address that visited it or force a captcha for all subsequent requests from that IP.\n\nDon\'t forget to disallow your honeypot (/scrapertrap/) in your robots.txt file so that search engine bots don\'t fall into it.\nYou can / should combine this with the previous tip of changing your HTML frequently.\nChange this frequently too, as scrapers will eventually learn to avoid it. Change the honeypot URL and text. Also want to consider changing the inline CSS used for hiding, and use an ID attribute and external CSS instead, as scrapers will learn to avoid anything which has a style attribute with CSS used to hide the content. Also try only enabling it sometimes, so the scraper works initially, but breaks after a while. This also applies to the previous tip.\nMalicious people can prevent access for real users by sharing a link to your honeypot, or even embedding that link somewhere as an image (eg. on a forum). Change the URL frequently, and make any ban times relatively short.\n\nServe fake and useless data if you detect a scraper\nIf you detect what is obviously a scraper, you can serve up fake and useless data; this will corrupt the data the scraper gets from your website. You should also make it impossible to distinguish such fake data from real data, so that scrapers don\'t know that they\'re being screwed with.\nAs an example: you have a news website; if you detect a scraper, instead of blocking access,  serve up fake, randomly generated articles, and this will poison the data the scraper gets. If you make your fake data indistinguishable from the real thing, you\'ll make it hard for scrapers to get what they want, namely the actual, real data. \nDon\'t accept requests if the User Agent is empty / missing\nOften, lazily written scrapers will not send a User Agent header with their request, whereas all  browsers as well as search engine spiders will. \nIf you get a request where the User Agent header is not present, you can show a captcha, or simply block or limit access. (Or serve fake data as described above, or something else..)\nIt\'s trivial to spoof, but as a measure against poorly written scrapers it is worth implementing.\nDon\'t accept requests if the User Agent is a common scraper one; blacklist ones used by scrapers\nIn some cases, scrapers will use a User Agent which no real browser or search engine spider uses, such as:\n\n""Mozilla"" (Just that, nothing else. I\'ve seen a few questions about scraping here, using that. A real browser will never use only that)\n""Java 1.7.43_u43"" (By default, Java\'s HttpUrlConnection uses something like this.)\n""BIZCO EasyScraping Studio 2.0""\n""wget"", ""curl"", ""libcurl"",.. (Wget and cURL are sometimes used for basic scraping)\n\nIf you find that a specific User Agent string is used by scrapers on your site, and it is not used by real browsers or legitimate spiders, you can also add it to your blacklist.\nIf it doesn\'t request assets (CSS, images), it\'s not a real browser.\nA real browser will (almost always) request and download assets such as images and CSS. HTML parsers and scrapers won\'t as they are only interested in the actual pages and their content.\nYou could log requests to your assets, and if you see lots of requests for only the HTML, it may be a scraper.\nBeware that search engine bots, ancient mobile devices, screen readers and misconfigured devices may not request assets either.\nUse and require cookies; use them to track user and scraper actions.\nYou can require cookies to be enabled in order to view your website. This will deter inexperienced and newbie scraper writers, however it is easy to for a scraper to send cookies. If you do use and require them, you can track user and scraper actions with them, and thus implement rate-limiting, blocking, or showing captchas on a per-user instead of a per-IP basis.\nFor example: when the user performs search, set a unique identifying cookie. When the results pages are viewed, verify that cookie. If the user opens all the search results (you can tell from the cookie), then it\'s probably a scraper.\nUsing cookies may be ineffective, as scrapers can send the cookies with their requests too, and discard them as needed. You will also prevent access for real users who have cookies disabled, if your site only works with cookies.\nNote that if you use JavaScript to set and retrieve the cookie, you\'ll block scrapers which don\'t run JavaScript, since they can\'t retrieve and send the cookie with their request.\nUse JavaScript + Ajax to load your content\nYou could use JavaScript + AJAX to load your content after the page itself loads. This will make the content inaccessible to HTML parsers which do not run JavaScript. This is often an effective deterrent to newbie and inexperienced programmers writing scrapers.\nBe aware of:\n\nUsing JavaScript to load the actual content will degrade user experience and performance\nSearch engines may not run JavaScript either, thus preventing them from indexing your content. This may not be a problem for search results pages, but may be for other things, such as article pages.\n\nObfuscate your markup, network requests from scripts, and everything else.\nIf you use Ajax and JavaScript to load your data, obfuscate the data which is transferred. As an example, you could encode your data on the server (with something as simple as base64 or more complex), and then decode and display it on the client, after fetching via Ajax. This will mean that someone inspecting network traffic will not immediately see how your page works and loads data, and it will be tougher for someone to directly request request data from your endpoints, as they will have to reverse-engineer your descrambling algorithm.\n\nIf you do use Ajax for loading the data, you should make it hard to use the endpoints without loading the page first, eg by requiring some session key as a parameter, which you can embed in your JavaScript or your HTML.\nYou can also embed your obfuscated data directly in the initial HTML page and use JavaScript to deobfuscate and display it, which would avoid the extra network requests. Doing this will make it significantly harder to extract the data using a HTML-only parser which does not run JavaScript, as the one writing the scraper will have to reverse engineer your JavaScript (which you should obfuscate too).\nYou might want to change your obfuscation methods regularly, to break scrapers who have figured it out.\n\nThere are several disadvantages to doing something like this, though:\n\nIt will be tedious and difficult to implement, maintain, and debug.\nIt will be ineffective against scrapers and screenscrapers which actually run JavaScript and then extract the data. (Most simple HTML parsers don\'t run JavaScript though)\nIt will make your site nonfunctional for real users if they have JavaScript disabled.\nPerformance and page-load times will suffer.\n\nNon-Technical:\n\nTell people not to scrape, and some will respect it\nFind a lawyer\nMake your data available, provide an API:\nYou could make your data easily available and require attribution and a link back to your site. Perhaps charge $$$ for it.\n\nMiscellaneous:\n\nThere are also commercial scraping protection services, such as the anti-scraping by Cloudflare or Distill Networks (Details on how it works here), which do these things, and more for you.\nFind a balance between usability for real users and scraper-proofness: Everything you do will impact user experience negatively in one way or another,  find compromises.\nDon\'t forget your mobile site and apps. If you have a mobile app, that can be screenscraped too, and network traffic can be inspected to determine the REST endpoints it uses.\nScrapers can scrape other scrapers: If there\'s one website which has content scraped from yours, other scrapers can scrape from that scraper\'s website.\n\nFurther reading:\n\nWikipedia\'s article on Web scraping. Many details on the technologies involved and the different types of web scraper.\nStopping scripters from slamming your website hundreds of times a second. Q & A on a very similar problem - bots checking a website and buying things as soon as they go on sale. A lot of relevant info, esp. on Captchas and rate-limiting.\n\n', '\nI will presume that you have set up robots.txt.\nAs others have mentioned, scrapers can fake nearly every aspect of their activities, and it is probably very difficult to identify the requests that are coming from the bad guys.\nI would consider:\n\nSet up a page, /jail.html.\nDisallow access to the page in robots.txt (so the respectful spiders will never visit).\nPlace a link on one of your pages, hiding it with CSS (display: none).\nRecord IP addresses of visitors to /jail.html.\n\nThis might help you to quickly identify requests from scrapers that are flagrantly disregarding your robots.txt.\nYou might also want to make your /jail.html a whole entire website that has the same, exact markup as normal pages, but with fake data (/jail/album/63ajdka, /jail/track/3aads8, etc.). This way, the bad scrapers won\'t be alerted to ""unusual input"" until you have the chance to block them entirely.\n', ""\nSue 'em. \nSeriously: If you have some money, talk to a good, nice, young lawyer who knows their way around the Internets. You could really be able to do something here. Depending on where the sites are based, you could have a lawyer write up a cease & desist or its equivalent in your country. You may be able to at least scare the bastards.\nDocument the insertion of your dummy values. Insert dummy values that clearly (but obscurely) point to you. I think this is common practice with phone book companies, and here in Germany, I think there have been several instances when copycats got busted through fake entries they copied 1:1.\nIt would be a shame if this would drive you into messing up your HTML code, dragging down SEO, validity and other things (even though a templating system that uses a slightly different HTML structure on each request for identical pages might already help a lot against scrapers that always rely on HTML structures and class/ID names to get the content out.)  \nCases like this are what copyright laws are good for. Ripping off other people's honest work to make money with is something that you should be able to fight against.\n"", ""\nProvide an XML API to access your data; in a manner that is simple to use. If people want your data, they'll get it, you might as well go all out.\nThis way you can provide a subset of functionality in an effective manner, ensuring that, at the very least, the scrapers won't guzzle up HTTP requests and massive amounts of bandwidth.\nThen all you have to do is convince the people who want your data to use the API. ;)\n"", ""\nThere is really nothing you can do to completely prevent this. Scrapers can fake their user agent, use multiple IP addresses, etc. and appear as a normal user. The only thing you can do is make the text not available at the time the page is loaded - make it with image, flash, or load it with JavaScript. However, the first two are bad ideas, and the last one would be an accessibility issue if JavaScript is not enabled for some of your regular users.\nIf they are absolutely slamming your site and rifling through all of your pages, you could do some kind of rate limiting.\nThere is some hope though. Scrapers rely on your site's data being in a consistent format. If you could randomize it somehow it could break their scraper. Things like changing the ID or class names of page elements on each load, etc. But that is a lot of work to do and I'm not sure if it's worth it. And even then, they could probably get around it with enough dedication.\n"", ""\nSorry, it's really quite hard to do this...\nI would suggest that you politely ask them to not use your content (if your content is copyrighted).\nIf it is and they don't take it down, then you can take furthur action and send them a cease and desist letter.\nGenerally, whatever you do to prevent scraping will probably end up with a more negative effect, e.g. accessibility, bots/spiders, etc.\n"", '\nOkay, as all posts say, if you want to make it search engine-friendly then bots can scrape for sure.\nBut you can still do a few things, and it may be affective for 60-70 % scraping bots.\nMake a checker script like below.\nIf a particular IP address is visiting very fast then after a few visits (5-10) put its IP address + browser information in a file or database.\nThe next step\n(This would be a background process and running all time or scheduled after a few minutes.) Make one another script that will keep on checking those suspicious IP addresses.\nCase 1. If the user Agent is of a known search engine like Google, Bing, Yahoo (you can find more information on user agents by googling it). Then you must see http://www.iplists.com/. This list and try to match patterns. And if it seems like a faked user-agent then ask to fill in a CAPTCHA on the next visit. (You need to research a bit more on bots IP addresses. I know this is achievable and also try whois of the IP address. It can be helpful.)\nCase 2. No user agent of a search bot: Simply ask to fill in a CAPTCHA on the next visit.\n', '\nLate answer - and also this answer probably isn\'t the one you want to hear...\nMyself already wrote many (many tens) of different specialized data-mining scrapers. (just because I like the ""open data"" philosophy).\nHere are already many advices in other answers - now i will play the devil\'s advocate role and will extend and/or correct their effectiveness.\nFirst:\n\nif someone really wants your data\nyou can\'t effectively (technically) hide your data\nif the data should be publicly accessible to your ""regular users""\n\nTrying to use some technical barriers aren\'t worth the troubles, caused:\n\nto your regular users by worsening their user-experience\nto regular and welcomed bots (search engines)\netc...\n\nPlain HMTL - the easiest way is parse the plain HTML pages, with well defined structure and css classes. E.g. it is enough to inspect element with Firebug, and use the right Xpaths, and/or CSS path in my scraper.\nYou could generate the HTML structure dynamically and also, you can generate dynamically the CSS class-names (and the CSS itself too) (e.g. by using some random class names) - but\n\nyou want to present the informations to your regular users in consistent way\ne.g. again - it is enough to analyze the page structure once more to setup the scraper.\nand it can be done automatically by analyzing some ""already known content""\n\n\nonce someone already knows (by earlier scrape), e.g.:\nwhat contains the informations about ""phil collins""\nenough display the ""phil collins"" page and (automatically) analyze how the page is structured ""today"" :)\n\n\nYou can\'t change the structure for every response, because your regular users will hate you. Also, this will cause more troubles for you (maintenance) not for the scraper. The XPath or CSS path is determinable by the scraping script automatically from the known content.\nAjax - little bit harder in the start, but many times speeds up the scraping process :) - why?\nWhen analyzing the requests and responses, i just setup my own proxy server (written in perl) and my firefox is using it. Of course, because it is my own proxy - it is completely hidden - the target server see it as regular browser. (So, no X-Forwarded-for and such headers).\nBased on the proxy logs, mostly is possible to determine the ""logic"" of the ajax requests, e.g. i could skip most of the html scraping, and just use the well-structured ajax responses (mostly in JSON format).\nSo, the ajax doesn\'t helps much...\nSome more complicated are pages which uses much packed javascript functions.\nHere is possible to use two basic methods:\n\nunpack and understand the JS and create a scraper which follows the Javascript logic (the hard way)\nor (preferably using by myself) - just using Mozilla with Mozrepl for scrape. E.g. the real scraping is done in full featured javascript enabled browser, which is programmed to clicking to the right elements and just grabbing the ""decoded"" responses directly from the browser window.\n\nSuch scraping is slow (the scraping is done as in regular browser), but it is\n\nvery easy to setup and use\nand it is nearly impossible to counter it :)\nand the ""slowness"" is needed anyway to counter the ""blocking the rapid same IP based requests""\n\nThe User-Agent based filtering doesn\'t helps at all. Any serious data-miner will set it to some correct one in his scraper.\nRequire Login - doesn\'t helps. The simplest way beat it (without any analyze and/or scripting the login-protocol) is just logging into the site as regular user, using Mozilla and after just run the Mozrepl based scraper...\nRemember, the require login helps for anonymous bots, but doesn\'t helps against someone who want scrape your data. He just register himself to your site as regular user.\nUsing frames isn\'t very effective also. This is used by many live movie services and it not very hard to beat. The frames are simply another one HTML/Javascript pages what are needed to analyze... If the data worth the troubles - the data-miner will do the required analyze.\nIP-based limiting isn\'t effective at all - here are too many public proxy servers and also here is the TOR... :) It doesn\'t slows down the scraping (for someone who really wants your data).\nVery hard is scrape data hidden in images. (e.g. simply converting the data into images server-side). Employing ""tesseract"" (OCR) helps many times - but honestly - the data must worth the troubles for the scraper. (which many times doesn\'t worth).\nOn the other side, your users will hate you for this. Myself, (even when not scraping) hate websites which doesn\'t allows copy the page content into the clipboard (because the information are in the images, or (the silly ones) trying to bond to the right click some custom Javascript event. :)\nThe hardest are the sites which using java applets or flash, and the applet uses secure https requests itself internally. But think twice - how happy will be your iPhone users... ;). Therefore, currently very few sites using them. Myself, blocking all flash content in my browser (in regular browsing sessions) - and never using sites which depends on Flash.\nYour milestones could be..., so you can try this method - just remember - you will probably loose some of your users. Also remember, some SWF files are decompilable. ;)\nCaptcha (the good ones - like reCaptcha) helps a lot - but your users will hate you... - just imagine, how your users will love you when they need solve some captchas in all pages showing informations about the music artists.\nProbably don\'t need to continue - you already got into the picture.\nNow what you should do:\nRemember: It is nearly impossible to hide your data, if you on the other side want publish them (in friendly way) to your regular users.\nSo,\n\nmake your data easily accessible - by some API\n\n\nthis allows the easy data access\ne.g. offload your server from scraping - good for you\n\nsetup the right usage rights (e.g. for example must cite the source)\nremember, many data isn\'t copyright-able - and hard to protect them\nadd some fake data (as you already done) and use legal tools\n\n\nas others already said, send an ""cease and desist letter""\nother legal actions (sue and like) probably is too costly and hard to win (especially against non US sites)\n\n\nThink twice before you will try to use some technical barriers.\nRather as trying block the data-miners, just add more efforts to your website usability. Your user will love you. The time (&energy) invested into technical barriers usually aren\'t worth - better to spend the time to make even better website...\nAlso, data-thieves aren\'t like normal thieves.\nIf you buy an inexpensive home alarm and add an warning ""this house is connected to the police"" - many thieves will not even try to break into. Because one wrong move by him - and he going to jail...\nSo, you investing only few bucks, but the thief investing and risk much.\nBut the data-thief hasn\'t such risks. just the opposite - ff you make one wrong move (e.g. if you introduce some BUG as a result of technical barriers), you will loose your users. If the the scraping bot will not work for the first time, nothing happens - the data-miner just will try another approach and/or will debug the script.\nIn this case, you need invest much more - and the scraper investing much less.\nJust think where you want invest your time & energy...\nPs: english isn\'t my native - so forgive my broken english...\n', '\nThings that might work against beginner scrapers:\n\nIP blocking\nuse lots of ajax\ncheck referer request header\nrequire login\n\nThings that will help in general:\n\nchange your layout every week\nrobots.txt\n\nThings that will help but will make your users hate you:\n\ncaptcha\n\n', ""\nI have done a lot of web scraping and summarized some techniques to stop web scrapers  on my blog based on what I find annoying.\nIt is a tradeoff between your users and scrapers. If you limit IP's, use CAPTCHA's, require login, etc, you make like difficult for the scrapers. But this may also drive away your genuine users.\n"", ""\nFrom a tech perspective: \nJust model what Google does when you hit them with too many queries at once. That should put a halt to a lot of it.\nFrom a legal perspective:\nIt sounds like the data you're publishing is not proprietary. Meaning you're publishing names and stats and other information that cannot be copyrighted. \nIf this is the case, the scrapers are not violating copyright by redistributing your information about artist name etc. However, they may be violating copyright when they load your site into memory because your site contains elements that are copyrightable (like layout etc).\nI recommend reading about Facebook v. Power.com and seeing the arguments Facebook used to stop screen scraping. There are many legal ways you can go about trying to stop someone from scraping your website. They can be far reaching and imaginative. Sometimes the courts buy the arguments. Sometimes they don't. \nBut, assuming you're publishing public domain information that's not copyrightable like names and basic stats... you should just let it go in the name of free speech and open data. That is, what the web's all about.\n"", ""\nYour best option is unfortunately fairly manual: Look for traffic patterns that you believe are indicative of scraping and ban their IP addresses.\nSince you're talking about a public site then making the site search-engine friendly will also make the site scraping-friendly. If a search-engine can crawl and scrape your site then an malicious scraper can as well. It's a fine-line to walk.\n"", ""\nSure it's possible. For 100% success, take your site offline.\nIn reality you can do some things that make scraping a little more difficult. Google does browser checks to make sure you're not a robot scraping search results (although this, like most everything else, can be spoofed).\nYou can do things like require several seconds between the first connection to your site, and subsequent clicks. I'm not sure what the ideal time would be or exactly how to do it, but that's another idea.\nI'm sure there are several other people who have a lot more experience, but I hope those ideas are at least somewhat helpful.\n"", ""\n\nNo, it's not possible to stop (in any way)\nEmbrace it. Why not publish as RDFa and become super search engine friendly and encourage the re-use of data? People will thank you and provide credit where due (see musicbrainz as an example).\n\nIt is not the answer you probably want, but why hide what you're trying to make public?\n"", ""\nThere are a few things you can do to try and prevent screen scraping.  Some are not very effective, while others (a CAPTCHA) are, but hinder usability.  You have to keep in mind too that it may hinder legitimate site scrapers, such as search engine indexes.\nHowever, I assume that if you don't want it scraped that means you don't want search engines to index it either.\nHere are some things you can try:\n\nShow the text in an image.  This is quite reliable, and is less of a pain on the user than a CAPTCHA, but means they won't be able to cut and paste and it won't scale prettily or be accessible.\nUse a CAPTCHA and require it to be completed before returning the page.  This is a reliable method, but also the biggest pain to impose on a user.\nRequire the user to sign up for an account before viewing the pages, and confirm their email address.  This will be pretty effective, but not totally - a screen-scraper might set up an account and might cleverly program their script to log in for them.\nIf the client's user-agent string is empty, block access.  A site-scraping script will often be lazily programmed and won't set a user-agent string, whereas all web browsers will.\nYou can set up a black list of known screen scraper user-agent strings as you discover them.  Again, this will only help the lazily-coded ones; a programmer who knows what he's doing can set a user-agent string to impersonate a web browser.\nChange the URL path often.  When you change it, make sure the old one keeps working, but only for as long as one user is likely to have their browser open.  Make it hard to predict what the new URL path will be.  This will make it difficult for scripts to grab it if their URL is hard-coded.  It'd be best to do this with some kind of script.\n\nIf I had to do this, I'd probably use a combination of the last three, because they minimise the inconvenience to legitimate users.  However, you'd have to accept that you won't be able to block everyone this way and once someone figures out how to get around it, they'll be able to scrape it forever.  You could then just try to block their IP addresses as you discover them I guess.\n"", '\nMethod One (Small Sites Only):\nServe encrypted / encoded data.I Scape the web using python (urllib, requests, beautifulSoup etc...) and found many websites that serve encrypted / encoded data that is not decrypt-able in any programming language simply because the encryption method does not exist.\nI achieved this in a PHP website by encrypting and minimizing the output (WARNING: this is not a good idea for large sites) the response was always jumbled content.\nExample of minimizing output in PHP (How to minify php page html output?):\n<?php\n  function sanitize_output($buffer) {\n    $search = array(\n      \'/\\>[^\\S ]+/s\', // strip whitespaces after tags, except space\n      \'/[^\\S ]+\\</s\', // strip whitespaces before tags, except space\n      \'/(\\s)+/s\'      // shorten multiple whitespace sequences\n    );\n    $replace = array(\'>\', \'<\', \'\\\\1\');\n    $buffer = preg_replace($search, $replace, $buffer);\n    return $buffer;\n  }\n  ob_start(""sanitize_output"");\n?>\n\nMethod Two:\nIf you can\'t stop them screw them over serve fake / useless data as a response.\nMethod Three:\nblock common scraping user agents, you\'ll see this in major / large websites as it is impossible to scrape them with ""python3.4"" as you User-Agent.\nMethod Four:\nMake sure all the user headers are valid, I sometimes provide as many headers as possible to make my scraper seem like an authentic user, some of them are not even true or valid like en-FU :).\nHere is a list of some of the headers I commonly provide.\nheaders = {\n  ""Requested-URI"": ""/example"",\n  ""Request-Method"": ""GET"",\n  ""Remote-IP-Address"": ""656.787.909.121"",\n  ""Remote-IP-Port"": ""69696"",\n  ""Protocol-version"": ""HTTP/1.1"",\n  ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"",\n  ""Accept-Encoding"": ""gzip,deflate"",\n  ""Accept-Language"": ""en-FU,en;q=0.8"",\n  ""Cache-Control"": ""max-age=0"",\n  ""Connection"": ""keep-alive"",\n  ""Dnt"": ""1"",  \n  ""Host"": ""http://example.com"",\n  ""Referer"": ""http://example.com"",\n  ""Upgrade-Insecure-Requests"": ""1"",\n  ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36""\n}\n\n', '\nQuick approach to this would be to set a booby/bot trap.\n\nMake a page that if it\'s opened a certain amount of times or even opened at all, will collect certain information like the IP and whatnot (you can also consider irregularities or patterns but this page shouldn\'t have to be opened at all). \nMake a link to this in your page that is hidden with CSS display:none; or left:-9999px; positon:absolute; try to place it in places that are less unlikely to be ignored like where your content falls under and not your footer as sometimes bots can choose to forget about certain parts of a page. \nIn your robots.txt file set a whole bunch of disallow rules to pages you don\'t want friendly bots (LOL, like they have happy faces!) to gather information on and set this page as one of them. \nNow, If a friendly bot comes through it should ignore that page. Right but that still isn\'t good enough. Make a couple more of these pages or somehow re-route a page to accept differnt names. and then place more disallow rules to these trap pages in your robots.txt file alongside pages you want ignored.\nCollect the IP of these bots or anyone that enters into these pages, don\'t ban them but make a function to display noodled text in your content like random numbers, copyright notices, specific text strings, display scary pictures, basically anything to hinder your good content. You can also set links that point to a page which will take forever to load ie. in php you can use the sleep() function. This will fight the crawler back if it has some sort of detection to bypass pages that take way too long to load as some well written bots are set to process X amount of links at a time.  \nIf you have made specific text strings/sentences why not go to your favorite search engine and search for them, it might show you where your content is ending up.\n\nAnyway, if you think tactically and creatively this could be a good starting point. The best thing to do would be to learn how a bot works.\nI\'d also think about scambling some ID\'s or the way attributes on the page element are displayed: \n<a class=""someclass"" href=""../xyz/abc"" rel=""nofollow"" title=""sometitle""> \n\nthat changes its form every time as some bots might be set to be looking for specific patterns in your pages or targeted elements. \n<a title=""sometitle"" href=""../xyz/abc"" rel=""nofollow"" class=""someclass""> \n\nid=""p-12802"" > id=""p-00392""\n\n', ""\nYou can't stop normal screen scraping. For better or worse, it's the nature of the web.\nYou can make it so no one can access certain things (including music files) unless they're logged in as a registered user. It's not too difficult to do in Apache. I assume it wouldn't be too difficult to do in IIS as well.\n"", ""\nRather than blacklisting bots, maybe you should whitelist them.  If you don't want to kill your search results for the top few engines, you can whitelist their user-agent strings, which are generally well-publicized.  The less ethical bots tend to forge user-agent strings of popular web browsers.  The top few search engines should be driving upwards of 95% of your traffic.\nIdentifying the bots themselves should be fairly straightforward, using the techniques other posters have suggested.\n"", ""\nMost have been already said, but have you considered the CloudFlare protection? I mean this:\n\nOther companies probably do this too, CloudFlare is the only one I know.\nI'm pretty sure that would complicate their work. I also once got IP banned automatically for 4 months when I tried to scrap data of a site protected by CloudFlare due to rate limit (I used simple AJAX request loop).\n"", '\nOne way would be to serve the content as XML attributes, URL encoded strings, preformatted text with HTML encoded JSON, or data URIs, then transform it to HTML on the client. Here are a few sites which do this:\n\nSkechers: XML\n<document \n filename="""" \n height="""" \n width="""" \n title=""SKECHERS"" \n linkType="""" \n linkUrl="""" \n imageMap="""" \n href=&quot;http://www.bobsfromskechers.com&quot; \n alt=&quot;BOBS from Skechers&quot; \n title=&quot;BOBS from Skechers&quot; \n/>\n\nChrome Web Store: JSON\n<script type=""text/javascript"" src=""https://apis.google.com/js/plusone.js"">{""lang"": ""en"", ""parsetags"": ""explicit""}</script>\n\nBing News: data URL\n<script type=""text/javascript"">\n  //<![CDATA[\n  (function()\n    {\n    var x;x=_ge(\'emb7\');\n    if(x)\n      {\n      x.src=\'data:image/jpeg;base64,/*...*/\';\n      } \n    }() )\n\nProtopage: URL Encoded Strings\nunescape(\'Rolling%20Stone%20%3a%20Rock%20and%20Roll%20Daily\')\n\nTiddlyWiki : HTML Entities + preformatted JSON\n   <pre>\n   {&quot;tiddlers&quot;: \n    {\n    &quot;GettingStarted&quot;: \n      {\n      &quot;title&quot;: &quot;GettingStarted&quot;,\n      &quot;text&quot;: &quot;Welcome to TiddlyWiki,\n      }\n    }\n   }\n   </pre>\n\nAmazon: Lazy Loading\namzn.copilot.jQuery=i;amzn.copilot.jQuery(document).ready(function(){d(b);f(c,function() {amzn.copilot.setup({serviceEndPoint:h.vipUrl,isContinuedSession:true})})})},f=function(i,h){var j=document.createElement(""script"");j.type=""text/javascript"";j.src=i;j.async=true;j.onload=h;a.appendChild(j)},d=function(h){var i=document.createElement(""link"");i.type=""text/css"";i.rel=""stylesheet"";i.href=h;a.appendChild(i)}})();\namzn.copilot.checkCoPilotSession({jsUrl : \'http://z-ecx.images-amazon.com/images/G/01/browser-scripts/cs-copilot-customer-js/cs-copilot-customer-js-min-1875890922._V1_.js\', cssUrl : \'http://z-ecx.images-amazon.com/images/G/01/browser-scripts/cs-copilot-customer-css/cs-copilot-customer-css-min-2367001420._V1_.css\', vipUrl : \'https://copilot.amazon.com\'\n\nXMLCalabash: Namespaced XML + Custom MIME type + Custom File extension\n   <p:declare-step type=""pxp:zip"">\n        <p:input port=""source"" sequence=""true"" primary=""true""/>\n        <p:input port=""manifest""/>\n        <p:output port=""result""/>\n        <p:option name=""href"" required=""true"" cx:type=""xsd:anyURI""/>\n        <p:option name=""compression-method"" cx:type=""stored|deflated""/>\n        <p:option name=""compression-level"" cx:type=""smallest|fastest|default|huffman|none""/>\n        <p:option name=""command"" select=""\'update\'"" cx:type=""update|freshen|create|delete""/>\n   </p:declare-step>\n\n\nIf you view source on any of the above, you see that scraping will simply return metadata and navigation.\n', ""\nI agree with most of the posts above, and I'd like to add that the more search engine friendly your site is, the more scrape-able it would be. You could try do a couple of things that are very out there that make it harder for scrapers, but it might also affect your search-ability... It depends on how well you want your site to rank on search engines of course.\n"", '\nPutting your content behind a captcha would mean that robots would find it difficult to access your content.  However, humans would be inconvenienced so that may be undesirable.\n', '\nIf you want to see a great example, check out http://www.bkstr.com/.  They use a j/s algorithm to set a cookie, then reloads the page so it can use the cookie to validate that the request is being run within a browser.  A desktop app built to scrape could definitely get by this, but it would stop most cURL type scraping.\n', ""\nScreen scrapers work by processing HTML. And if they are determined to get your data there is not much you can do technically because the human eyeball processes anything. Legally it's already been pointed out you may have some recourse though and that would be my recommendation.\nHowever, you can hide the critical part of your data by using non-HTML-based presentation logic\n\nGenerate a Flash file for each artist/album, etc.\nGenerate an image for each artist content. Maybe just an image for the artist name, etc. would be enough. Do this by rendering the text onto a JPEG/PNG file on the server and linking to that image.\n\nBear in mind that this would probably affect your search rankings.\n"", '\nGenerate the HTML, CSS and JavaScript. It is easier to write generators than parsers, so you could generate each served page differently. You can no longer use a cache or static content then.\n']",https://stackoverflow.com/questions/3161548/how-do-i-prevent-site-scraping,web-scraping
BeautifulSoup webscraping find_all( ): finding exact match,"
I'm using Python and BeautifulSoup for web scraping.
Lets say I have the following html code to scrape:
<body>
    <div class=""product"">Product 1</div>
    <div class=""product"">Product 2</div>
    <div class=""product special"">Product 3</div>
    <div class=""product special"">Product 4</div>
</body>

Using BeautifulSoup, I want to find ONLY the products with the attribute class=""product""
(only Product 1 and 2), not the 'special' products
If I do the following:
result = soup.find_all('div', {'class': 'product'})

the result includes ALL the products (1,2,3, and 4).
What should I do to find products whose class EXACTLY matches 'product'??

The Code I ran:
from bs4 import BeautifulSoup
import re

text = """"""
<body>
    <div class=""product"">Product 1</div>
    <div class=""product"">Product 2</div>
    <div class=""product special"">Product 3</div>
    <div class=""product special"">Product 4</div>
</body>""""""

soup = BeautifulSoup(text)
result = soup.findAll(attrs={'class': re.compile(r""^product$"")})
print result

Output:
[<div class=""product"">Product 1</div>, <div class=""product"">Product 2</div>, <div class=""product special"">Product 3</div>, <div class=""product special"">Product 4</div>]

",111k,"
            40
        ","['\nIn BeautifulSoup 4, the class attribute (and several other attributes, such as accesskey and the headers attribute on table cell elements) is treated as a set; you match against individual elements listed in the attribute. This follows the HTML standard.\nAs such, you cannot limit the search to just one class.\nYou\'ll have to use a custom function here to match against the class instead:\nresult = soup.find_all(lambda tag: tag.name == \'div\' and \n                                   tag.get(\'class\') == [\'product\'])\n\nI used a lambda to create an anonymous function; each tag is matched on name (must be \'div\'), and the class attribute must be exactly equal to the list [\'product\']; e.g. have just the one value.\nDemo:\n>>> from bs4 import BeautifulSoup\n>>> text = """"""\n... <body>\n...     <div class=""product"">Product 1</div>\n...     <div class=""product"">Product 2</div>\n...     <div class=""product special"">Product 3</div>\n...     <div class=""product special"">Product 4</div>\n... </body>""""""\n>>> soup = BeautifulSoup(text)\n>>> soup.find_all(lambda tag: tag.name == \'div\' and tag.get(\'class\') == [\'product\'])\n[<div class=""product"">Product 1</div>, <div class=""product"">Product 2</div>]\n\nFor completeness sake, here are all such set attributes, from the BeautifulSoup source code:\n# The HTML standard defines these attributes as containing a\n# space-separated list of values, not a single value. That is,\n# class=""foo bar"" means that the \'class\' attribute has two values,\n# \'foo\' and \'bar\', not the single value \'foo bar\'.  When we\n# encounter one of these attributes, we will parse its value into\n# a list of values if possible. Upon output, the list will be\n# converted back into a string.\ncdata_list_attributes = {\n    ""*"" : [\'class\', \'accesskey\', \'dropzone\'],\n    ""a"" : [\'rel\', \'rev\'],\n    ""link"" :  [\'rel\', \'rev\'],\n    ""td"" : [""headers""],\n    ""th"" : [""headers""],\n    ""td"" : [""headers""],\n    ""form"" : [""accept-charset""],\n    ""object"" : [""archive""],\n\n    # These are HTML5 specific, as are *.accesskey and *.dropzone above.\n    ""area"" : [""rel""],\n    ""icon"" : [""sizes""],\n    ""iframe"" : [""sandbox""],\n    ""output"" : [""for""],\n    }\n\n', ""\nYou can use CSS selectors like so:\nresult = soup.select('div.product.special')\n\ncss-selectors\n"", '\nsoup.findAll(attrs={\'class\': re.compile(r""^product$"")})\n\nThis code matches anything that doesn\'t have the product at the end of its class.\n', '\nYou could solve this problem and capture just Product 1 and Product 2 with gazpacho by enforcing exact matching:\nfrom gazpacho import Soup\n\nhtml = """"""\\\n<body>\n    <div class=""product"">Product 1</div>\n    <div class=""product"">Product 2</div>\n    <div class=""product special"">Product 3</div>\n    <div class=""product special"">Product 4</div>\n</body>\n""""""\n\nsoup = Soup(html)\ndivs = soup.find(""div"", {""class"": ""product""}, partial=False)\n[div.text for div in divs]\n\nOutputs exactly:\n[\'Product 1\', \'Product 2\']\n\n', '\nchange your code from\nresult = soup.findAll(attrs={\'class\': re.compile(r""^product$"")})\n\nto\nresult = soup.find_all(attrs={\'class\': \'product\'})\n\nand the result is a list and access through index\n']",https://stackoverflow.com/questions/22726860/beautifulsoup-webscraping-find-all-finding-exact-match,web-scraping
Periodically refresh IMPORTXML() spreadsheet function,"
I have a large sheet with around 30 importxml functions that obtain data from a website that updates usually twice a day.
I would like to run the importxml function on a timely basis (every 8 hours) for my Google Spreadsheet to save the data in another sheet. The saving already works, however the updating does not!
I read in Google Spreadsheet row update that it might run every 2 hours, however I do not believe that this is true, because since I added it to my sheet nothing has changed or updated, when the spreadsheet is NOT opened.
How can I ""trigger"" the importxml function in my Google Spreadsheet in an easy way, as I have a lot of importxml functions in it?
",56k,"
            17
        ","['\nI made a couple of adjustments to Mogsdad\'s answer:\n\nFixed the releaseLock() call placement\nUpdates (or adds) a querystring parameter to the url in the import function (as opposed to storing, removing, waiting 5 seconds, and then restoring all relevant formulas)\nWorks on a specific sheet in your spreadsheet\nShows time of last update\n\n...\nfunction RefreshImports() {\n  var lock = LockService.getScriptLock();\n  if (!lock.tryLock(5000)) return;             // Wait up to 5s for previous refresh to end.\n\n  var id = ""[YOUR SPREADSHEET ID]"";\n  var ss = SpreadsheetApp.openById(id);\n  var sheet = ss.getSheetByName(""[SHEET NAME]"");\n  var dataRange = sheet.getDataRange();\n  var formulas = dataRange.getFormulas();\n  var content = """";\n  var now = new Date();\n  var time = now.getTime();\n  var re = /.*[^a-z0-9]import(?:xml|data|feed|html|range)\\(.*/gi;\n  var re2 = /((\\?|&)(update=[0-9]*))/gi;\n  var re3 = /("",)/gi;\n\n  for (var row=0; row<formulas.length; row++) {\n    for (var col=0; col<formulas[0].length; col++) {\n      content = formulas[row][col];\n      if (content != """") {\n        var match = content.search(re);\n        if (match !== -1 ) {\n          // import function is used in this cell\n          var updatedContent = content.toString().replace(re2,""$2update="" + time);\n          if (updatedContent == content) {\n            // No querystring exists yet in url\n            updatedContent = content.toString().replace(re3,""?update="" + time + ""$1"");\n          }\n          // Update url in formula with querystring param\n          sheet.getRange(row+1, col+1).setFormula(updatedContent);\n        }\n      }\n    }\n  }\n\n  // Done refresh; release the lock.\n  lock.releaseLock();\n\n  // Show last updated time on sheet somewhere\n  sheet.getRange(7,2).setValue(""Rates were last updated at "" + now.toLocaleTimeString())\n}\n\n', '\nThe Google Spreadsheet row update question and its answers refer to the ""Old Sheets"", which had different behaviour than the 2015 version of Google Sheets does. There is no automatic refresh of content with ""New Sheets""; changes are only evaluated now in response to edits.\nWhile Sheets no longer provides this capability natively, we can use a script to refresh the ""import"" formulas (IMPORTXML, IMPORTDATA, IMPORTHTML and IMPORTANGE).\nUtility script\nFor periodic refresh of IMPORT formulas, set this function up as a time-driven trigger.\nCaveats:\n\nImport function Formula changes made to the spreadsheet by other scripts or users  during the refresh period COULD BE OVERWRITTEN.\nOverlapping refreshes might make your spreadsheet unstable. To mitigate that, the utility script uses a ScriptLock. This may conflict with other uses of that lock in your script.\n\n\xa0\n/**\n * Go through all sheets in a spreadsheet, identify and remove all spreadsheet\n * import functions, then replace them a while later. This causes a ""refresh""\n * of the ""import"" functions. For periodic refresh of these formulas, set this\n * function up as a time-based trigger.\n *\n * Caution: Formula changes made to the spreadsheet by other scripts or users\n * during the refresh period COULD BE OVERWRITTEN.\n *\n * From: https://stackoverflow.com/a/33875957/1677912\n */\nfunction RefreshImports() {\n  var lock = LockService.getScriptLock();\n  if (!lock.tryLock(5000)) return;             // Wait up to 5s for previous refresh to end.\n  // At this point, we are holding the lock.\n\n  var id = ""YOUR-SHEET-ID"";\n  var ss = SpreadsheetApp.openById(id);\n  var sheets = ss.getSheets();\n\n  for (var sheetNum=0; sheetNum<sheets.length; sheetNum++) {\n    var sheet = sheets[sheetNum];\n    var dataRange = sheet.getDataRange();\n    var formulas = dataRange.getFormulas();\n    var tempFormulas = [];\n    for (var row=0; row<formulas.length; row++) {\n      for (col=0; col<formulas[0].length; col++) {\n        // Blank all formulas containing any ""import"" function\n        // See https://regex101.com/r/bE7fJ6/2\n        var re = /.*[^a-z0-9]import(?:xml|data|feed|html|range)\\(.*/gi;\n        if (formulas[row][col].search(re) !== -1 ) {\n          tempFormulas.push({row:row+1,\n                             col:col+1,\n                             formula:formulas[row][col]});\n          sheet.getRange(row+1, col+1).setFormula("""");\n        }\n      }\n    }\n\n    // After a pause, replace the import functions\n    Utilities.sleep(5000);\n    for (var i=0; i<tempFormulas.length; i++) {\n      var cell = tempFormulas[i];\n      sheet.getRange( cell.row, cell.col ).setFormula(cell.formula)\n    }\n\n    // Done refresh; release the lock.\n    lock.releaseLock();\n  }\n}\n\n', '\nTo answer your question for an easy ""trigger"" to force the function to reload:\nadd an additional not used parameter to the url you are loading, while referencing a cell for the value of that parameter.\nOnce you alter the content of that cell, the function reloads.\nexample:\nimportxml(""http://www.example.com/?noop="" & $A$1,""..."")\n\nunfortunately you cannot put a date calculating function into the referenced cell, that throws an error that this is not allowed.\n', '\nYou can also put each XML formula as a comment in the respective cells and record a macro to copy and paste it in the same cell. Later use the Scripts and then the Trigger functionality to schedule this macro.\n\n\n\n']",https://stackoverflow.com/questions/33872967/periodically-refresh-importxml-spreadsheet-function,web-scraping
'list' object has no attribute 'get_attribute' while iterating through WebElements,"
I'm trying to use Python and Selenium to scrape multiple links on a web page. I'm using find_elements_by_xpath and I'm able to locate a list of elements but I'm having trouble changing the list that is returned to the actual href links. I know find_element_by_xpath works, but that only works for one element.
Here is my code:
path_to_chromedriver = 'path to chromedriver location'
browser = webdriver.Chrome(executable_path = path_to_chromedriver)

browser.get(""file:///path to html file"")

all_trails = []

#finds all elements with the class 'text-truncate trail-name' then 
#retrieve the a element
#this seems to be just giving us the element location but not the 
#actual location

find_href = browser.find_elements_by_xpath('//div[@class=""text truncate trail-name""]/a[1]')
all_trails.append(find_href)

print all_trails

This code is returning:
<selenium.webdriver.remote.webelement.WebElement 
(session=""dd178d79c66b747696c5d3750ea8cb17"", 
element=""0.5700549730549636-1663"")>, 
<selenium.webdriver.remote.webelement.WebElement 
(session=""dd178d79c66b747696c5d3750ea8cb17"", 
element=""0.5700549730549636-1664"")>,

I expect the all_trails array to be a list of links like: www.google.com, www.yahoo.com, www.bing.com.
I've tried looping through the all_trails list and running the get_attribute('href') method on the list but I get the error:

Does anyone have any idea how to convert the selenium WebElement's to href links?
Any help would be greatly appreciated :)
",16k,"
            4
        ","['\nLet us see what\'s happening in your code :\nWithout any visibility to the concerned HTML it seems the following line returns two WebElements in to the List find_href which are inturn are appended to the all_trails List :\nfind_href = browser.find_elements_by_xpath(\'//div[@class=""text truncate trail-name""]/a[1]\')\n\nHence when we print the List all_trails both the WebElements are printed. Hence No Error.\nAs per the error snap shot you have provided, you are trying to invoke get_attribute(""href"") method over a List which is Not Supported. Hence you see the error :\n\'List\' Object has no attribute \'get_attribute\'\n\nSolution :\nTo get the href attribute, we have to iterate over the List as follows :\nfind_href = browser.find_elements_by_xpath(\'//your_xpath\')\nfor my_href in find_href:\n    print(my_href.get_attribute(""href""))\n\n', '\nIf you have the following HTML:\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 1</a>\n</div>\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 2</a>\n</div>\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 3</a>\n</div>\n<div class=""text-truncate trail-name"">\n<a href=""http://google.com"">Link 4</a>\n</div>\n\nYour code should look like:\nall_trails = []\n\nall_links = browser.find_elements_by_css_selector("".text-truncate.trail-name>a"")\n\nfor link in all_links:\n\n    all_trails.append(link.get_attribute(""href""))\n\nWhere all_trails -- is a list of links (Link 1, Link 2 and so on).\nHope it helps you!\n', '\nUse it in Singular form as find_element_by_css_selector instead of using find_elements_by_css_selector as it returns many webElements in List. So you need to loop through each webElement to use Attribute.\n', '\nfind_href = browser.find_elements_by_xpath(\'//div[@class=""text truncate trail-name""]/a[1]\')\nfor i in find_href:\n      all_trails.append(i.get_attribute(\'href\'))\n\nget_attribute works on elements of that list, not list itself.\n', '\nget_attribute works on elements of that list only, not list itself. For eg :-\ndef fetch_img_urls(search_query: str):\n    driver.get(\'https://images.google.com/\')\n    search = driver.find_element(By.CLASS_NAME, ""gLFyf.gsfi"")\n    search.send_keys(search_query)\n    search.send_keys(Keys.RETURN)\n    links=[]\n    try:\n        time.sleep(5)\n        urls = driver.find_elements(By.CSS_SELECTOR,\'a.VFACy.kGQAp.sMi44c.lNHeqe.WGvvNb\')\n        for url in urls:\n            #print(url.get_attribute(""href""))\n            links.append(url.get_attribute(""href""))\n            print(links)\n\n    except Exception as e:\n        print(f\'error{e}\')\n        driver.quit()\n\n']",https://stackoverflow.com/questions/47735375/list-object-has-no-attribute-get-attribute-while-iterating-through-webelemen,web-scraping
Web scraping program cannot find element which I can see in the browser,"
I am trying to get the titles of the streams on https://www.twitch.tv/directory/game/Dota%202, using Requests and BeautifulSoup. I know that my search criteria are correct, yet my program does not find the elements I need.
Here is a screenshot showing the relevant part of the source code in the browser:

The HTML source as text:


<div class=""tw-media-card-meta__title"">
  <div class=""tw-c-text-alt"">
    <a class=""tw-full-width tw-interactive tw-link tw-link--button tw-link--hover-underline-none tw-link--inherit"" data-a-target=""preview-card-title-link"" href=""/weplayesport_en"">
      <div class=""tw-align-items-start tw-flex"">
        <h3 class=""tw-ellipsis tw-font-size-5"" title=""NAVI vs HellRaisers | BO5 | ODPixel &amp; S4 | WeSave! Charity Play"">NAVI vs HellRaisers | BO5 | ODPixel &amp; S4 | WeSave! Charity Play</h3>
      </div>
    </a>
  </div>
</div>


Here is my code:
import requests
from bs4 import BeautifulSoup

req = requests.get(""https://www.twitch.tv/directory/game/Dota%202"")

soup = BeautifulSoup(req.content, ""lxml"")

title_elems = soup.find_all(""h3"", attrs={""title"": True})

print(title_elems)

When I run it, title_elems is just the empty list ([]).
Why is my program not finding the elements?
",1k,"
            -8
        ","[""\nThe element you're interested in is dynamically generated, after the initial page load, which means that your browser executed JavaScript, made other network requests, etc. in order to build the page. Requests is just an HTTP library, and as such will not do those things.\nYou could use a tool like Selenium, or perhaps even analyze the network traffic for the data you need and make the requests directly.\n"", '\nThis is something you can do to grab the title of different games from that site using requests module:\nimport re\nimport requests\n\nurl = ""https://www.twitch.tv/directory/game/Dota%202""\nlink = ""https://gql.twitch.tv/gql""\n\nformdata = [{""operationName"":""DirectoryRoot_Directory"",""variables"":{""name"":""dota 2""},""extensions"":{""persistedQuery"":{""version"":1,""sha256Hash"":""9f4f6ae67f21ee50b454fcf048691107a52bfe7907ead73b9427398e343ca319""}}},{""operationName"":""Directory_DirectoryBanner"",""variables"":{""name"":""Dota 2""},""extensions"":{""persistedQuery"":{""version"":1,""sha256Hash"":""a64b0348103e054cbdb20c58de5fc05160da3f86c37c80263d7e6282f2577f54""}}},{""operationName"":""DirectoryPage_Game"",""variables"":{""name"":""dota 2"",""options"":{""sort"":""RELEVANCE"",""recommendationsContext"":{""platform"":""web""},""requestID"":""JIRA-VXP-2397"",""tags"":[]},""sortTypeIsRecency"":False,""limit"":30},""extensions"":{""persistedQuery"":{""version"":1,""sha256Hash"":""f2ac02ded21558ad8b747a0b63c0bb02b0533b6df8080259be10d82af63d50b3""}}}]\n\nwith requests.Session() as s:\n    s.headers[\'User-Agent\'] = ""Mozilla/5.0 (Windows NT 6.1; ) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36""\n    res = s.get(url)\n    s.headers[\'Client-Id\'] = re.findall(r""Client-ID\\"":\\""(.*?)\\"""",res.text)[0]\n    res = s.post(link,json=formdata)\n    for item in res.json()[2][\'data\'][\'game\'][\'streams\'][\'edges\']:\n        print(item[\'node\'][\'title\'])\n\nThe type of results you may get:\nPUBS 7.27 POG VIBE COOL FUN\nPlaying more 7.27\n7.27 Pubs :) Climb to 9k\nВ новый сезон с новыми победами Custom Hero Chaos |  !discord \nБобрый вечер !розыгрыш \nRERUN: Adroit vs Execration Game 1 - BTS Pro Series 2: SEA - Group Stage w/ MLP & johnxfire\n(RU) Повтор | EGB.com Arena of Blood \nNo Matter What\nmid 5000-6000(5600) (!розыгрыш каждый день)\nchegou a cam, pena que to parecendo um neanderthal\n\n']",https://stackoverflow.com/questions/60904786/web-scraping-program-cannot-find-element-which-i-can-see-in-the-browser,web-scraping
Web scraping with Java,"
I'm not able to find any good web scraping Java based API. The site which I need to scrape does not provide any API as well; I want to iterate over all web pages using some pageID and extract the HTML titles / other stuff in their DOM trees.
Are there ways other than web scraping?
",142k,"
            75
        ","['\njsoup\nExtracting the title is not difficult, and you have many options, search here on Stack Overflow for ""Java HTML parsers"". One of them is Jsoup.\nYou can navigate the page using DOM if you know the page structure, see\nhttp://jsoup.org/cookbook/extracting-data/dom-navigation\nIt\'s a good library and I\'ve used it in my last projects.\n', ""\nYour best bet is to use Selenium Web Driver since it\n\nProvides visual feedback to the coder (see your scraping in action, see where it stops)\n\nAccurate and Consistent as it directly controls the browser you use.\n\nSlow. Doesn't hit web pages like HtmlUnit does but sometimes you don't want to hit too fast.\nHtmlunit is fast but is horrible at handling Javascript and AJAX.\n\n\n"", '\nHTMLUnit can be used to do web scraping, it supports invoking pages, filling & submitting forms. I have used this in my project. It is good java library for web scraping.\nread here for more\n', '\nmechanize for Java would be a good fit for this, and as Wadjy Essam mentioned it uses JSoup for the HMLT. mechanize is a stageful HTTP/HTML client that supports navigation, form submissions, and page scraping.\nhttp://gistlabs.com/software/mechanize-for-java/ (and the GitHub here https://github.com/GistLabs/mechanize)\n', '\nThere is also Jaunt Java Web Scraping & JSON Querying - http://jaunt-api.com\n', '\nYou might look into jwht-scraper!\nThis is a complete scraping framework that has all the features a developper could expect from a web scraper :\n\nProxy support\nWarning Sign Support to detect captchas and more\nComplex link following features\nMultithreading\nVarious scraping delays when required\nRotating User-Agent\nRequest auto retry and HTTP redirections supports\nHTTP headers, cookies and more support\nGET and POST support\nAnnotation Configuration\nDetailed Scraping Metrics\nAsync handling of the scraper client\njwht-htmltopojo fully featured framework to map HTML to POJO\nCustom Input Format handling and built in JSON -> POJO mapping\nFull Exception Handling Control\nDetailed Logging with log4j\nPOJO injection\nCustom processing hooks\nEasy to use and well documented API\n\nIt works with (jwht-htmltopojo)[https://github.com/whimtrip/jwht-htmltopojo) lib which itsef uses Jsoup mentionned by several other people here.\nTogether they will help you built awesome scrapers mapping directly HTML to POJOs and bypassing any classical scraping problems in only a matter of minutes!\nHope this might help some people here!\nDisclaimer, I am the one who developed it, feel free to let me know your remarks!\n', '\nLook at an HTML parser such as TagSoup, HTMLCleaner or NekoHTML.\n', '\nIf you wish to automate scraping of large amount pages or data, then you could try Gotz ETL. \nIt is completely model driven like a real ETL tool. Data structure, task workflow and pages to scrape are defined with a set of XML definition files and no coding is required. Query can be written either using Selectors with JSoup or XPath with HtmlUnit.\n', '\nFor tasks of this type I usually use Crawller4j + Jsoup.\nWith crawler4j I download the pages from a domain, you can specify which ULR with a regular expression.\nWith jsoup, I ""parsed"" the html data you have searched for and downloaded with crawler4j.\nNormally you can also download data with jsoup, but Crawler4J makes it easier to find links.\nAnother advantage of using crawler4j is that it is multithreaded and you can configure the number of concurrent threads\nhttps://github.com/yasserg/crawler4j/wiki\n', '\nNormally I use selenium, which is software for testing automation.\nYou can control a browser through a webdriver, so you will not have problems with javascripts and it is usually not very detected if you use the full version. Headless browsers can be more identified.\n', '\nI am an engineer at WebScrapingAPI and I recommend your our product since we offer many features such as rendering javascript, CSS extracting, ip rotations, proxies and many others which you can find here , in our docs. Furthermore, we provide support for Java and we have Java examples for our features in order to ease your implementation process.\nOur API is very easy to use and beginner friendly. Take the following example where we want to render the javascript of httpbin.org. It can be done as simple as that:\nHttpResponse<String> response = Unirest.get(""https://api.webscrapingapi.com/v1?api_key=%7B%7Bapi_key%7D%7D&url=https%3A%2F%2Fhttpbin.org&render_js=1"")\n  .asString(); \n\nOr an example with using residential proxy:\nHttpResponse<String> response = Unirest.get(""https://api.webscrapingapi.com/v1?api_key=%7B%7Bapi_key%7D%7D&url=https%3A%2F%2Fhttpbin.org%2Fget&proxy_type=residential"")\n  .asString();\n\nOn top of that, in case you struggle with implementation or you encounter any issue with our services we have a very effective customer support which is ready to jump in and help right away.\n']",https://stackoverflow.com/questions/3202305/web-scraping-with-java,web-scraping
Python - Download Images from google Image search?,"
I want to download all Images of google image search using python . The code I am using seems to have some problem some times .My code is 
import os
import sys
import time
from urllib import FancyURLopener
import urllib2
import simplejson

# Define search term
searchTerm = ""parrot""

# Replace spaces ' ' in search term for '%20' in order to comply with request
searchTerm = searchTerm.replace(' ','%20')


# Start FancyURLopener with defined version 
class MyOpener(FancyURLopener): 
    version = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127     Firefox/2.0.0.11'
    myopener = MyOpener()

    # Set count to 0
    count= 0

    for i in range(0,10):
    # Notice that the start changes for each iteration in order to request a new set of   images for each loop
    url = ('https://ajax.googleapis.com/ajax/services/search/images?' + 'v=1.0& q='+searchTerm+'&start='+str(i*10)+'&userip=MyIP')
    print url
    request = urllib2.Request(url, None, {'Referer': 'testing'})
    response = urllib2.urlopen(request)

# Get results using JSON
    results = simplejson.load(response)
    data = results['responseData']
    dataInfo = data['results']

# Iterate for each result and get unescaped url
    for myUrl in dataInfo:
        count = count + 1
        my_url = myUrl['unescapedUrl']
        myopener.retrieve(myUrl['unescapedUrl'],str(count)+'.jpg')        

After downloading few pages I am getting an error as follows:
Traceback (most recent call last):
  File ""C:\Python27\img_google3.py"", line 37, in <module>
    dataInfo = data['results']
TypeError: 'NoneType' object has no attribute '__getitem__'

What to do ??????   
",130k,"
            43
        ","['\nI have modified my code. Now the code can download 100 images for a given query, and images are full high resolution that is original images are being downloaded.\nI am downloading the images using urllib2 & Beautiful soup\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport urllib2\nimport os\nimport cookielib\nimport json\n\ndef get_soup(url,header):\n    return BeautifulSoup(urllib2.urlopen(urllib2.Request(url,headers=header)),\'html.parser\')\n\n\nquery = raw_input(""query image"")# you can change the query for the image  here\nimage_type=""ActiOn""\nquery= query.split()\nquery=\'+\'.join(query)\nurl=""https://www.google.co.in/search?q=""+query+""&source=lnms&tbm=isch""\nprint url\n#add the directory for your image here\nDIR=""Pictures""\nheader={\'User-Agent\':""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36""\n}\nsoup = get_soup(url,header)\n\n\nActualImages=[]# contains the link for Large original images, type of  image\nfor a in soup.find_all(""div"",{""class"":""rg_meta""}):\n    link , Type =json.loads(a.text)[""ou""]  ,json.loads(a.text)[""ity""]\n    ActualImages.append((link,Type))\n\nprint  ""there are total"" , len(ActualImages),""images""\n\nif not os.path.exists(DIR):\n            os.mkdir(DIR)\nDIR = os.path.join(DIR, query.split()[0])\n\nif not os.path.exists(DIR):\n            os.mkdir(DIR)\n###print images\nfor i , (img , Type) in enumerate( ActualImages):\n    try:\n        req = urllib2.Request(img, headers={\'User-Agent\' : header})\n        raw_img = urllib2.urlopen(req).read()\n\n        cntr = len([i for i in os.listdir(DIR) if image_type in i]) + 1\n        print cntr\n        if len(Type)==0:\n            f = open(os.path.join(DIR , image_type + ""_""+ str(cntr)+"".jpg""), \'wb\')\n        else :\n            f = open(os.path.join(DIR , image_type + ""_""+ str(cntr)+"".""+Type), \'wb\')\n\n\n        f.write(raw_img)\n        f.close()\n    except Exception as e:\n        print ""could not load : ""+img\n        print e\n\ni hope this helps you \n', '\nThe Google Image Search API is deprecated, you need to use the Google Custom Search for what you want to achieve. To fetch the images you need to do this:\nimport urllib2\nimport simplejson\nimport cStringIO\n\nfetcher = urllib2.build_opener()\nsearchTerm = \'parrot\'\nstartIndex = 0\nsearchUrl = ""http://ajax.googleapis.com/ajax/services/search/images?v=1.0&q="" + searchTerm + ""&start="" + startIndex\nf = fetcher.open(searchUrl)\ndeserialized_output = simplejson.load(f)\n\nThis will give you 4 results, as JSON, you need to iteratively get the results by incrementing the startIndex in the API request.\nTo get the images you need to use a library like cStringIO.\nFor example, to access the first image, you need to do this:\nimageUrl = deserialized_output[\'responseData\'][\'results\'][0][\'unescapedUrl\']\nfile = cStringIO.StringIO(urllib.urlopen(imageUrl).read())\nimg = Image.open(file)\n\n', ""\nGoogle deprecated their API, scraping Google is complicated, so I would suggest using Bing API instead to automatically download images. The pip package bing-image-downloader allows you to easily download an arbitrary number of images to a directory with a single line of code.\nfrom bing_image_downloader import downloader\n\ndownloader.download(query_string, limit=100, output_dir='dataset', adult_filter_off=True, force_replace=False, timeout=60, verbose=True)\n\nGoogle is not so good, and Microsoft is not so evil\n"", '\nHere\'s my latest google image snarfer, written in Python, using Selenium and headless Chrome.\nIt requires python-selenium, the chromium-driver, and a module called retry from pip.\nLink: http://sam.aiki.info/b/google-images.py\nExample Usage:\ngoogle-images.py tiger 10 --opts isz:lt,islt:svga,itp:photo > urls.txt\nparallel=5\nuser_agent=""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36""\n(i=0; while read url; do wget -e robots=off -T10 --tries 10 -U""$user_agent"" ""$url"" -O`printf %04d $i`.jpg & i=$(($i+1)) ; [ $(($i % $parallel)) = 0 ] && wait; done < urls.txt; wait)\n\nHelp Usage:\n\n$ google-images.py --help\nusage: google-images.py [-h] [--safe SAFE] [--opts OPTS] query n\n\nFetch image URLs from Google Image Search.\n\npositional arguments:\n  query        image search query\n  n            number of images (approx)\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --safe SAFE  safe search [off|active|images]\n  --opts OPTS  search options, e.g.\n               isz:lt,islt:svga,itp:photo,ic:color,ift:jpg\n\nCode:\n#!/usr/bin/env python3\n\n# requires: selenium, chromium-driver, retry\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nimport selenium.common.exceptions as sel_ex\nimport sys\nimport time\nimport urllib.parse\nfrom retry import retry\nimport argparse\nimport logging\n\nlogging.basicConfig(stream=sys.stderr, level=logging.INFO)\nlogger = logging.getLogger()\nretry_logger = None\n\ncss_thumbnail = ""img.Q4LuWd""\ncss_large = ""img.n3VNCb""\ncss_load_more = "".mye4qd""\nselenium_exceptions = (sel_ex.ElementClickInterceptedException, sel_ex.ElementNotInteractableException, sel_ex.StaleElementReferenceException)\n\ndef scroll_to_end(wd):\n    wd.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")\n\n@retry(exceptions=KeyError, tries=6, delay=0.1, backoff=2, logger=retry_logger)\ndef get_thumbnails(wd, want_more_than=0):\n    wd.execute_script(""document.querySelector(\'{}\').click();"".format(css_load_more))\n    thumbnails = wd.find_elements_by_css_selector(css_thumbnail)\n    n_results = len(thumbnails)\n    if n_results <= want_more_than:\n        raise KeyError(""no new thumbnails"")\n    return thumbnails\n\n@retry(exceptions=KeyError, tries=6, delay=0.1, backoff=2, logger=retry_logger)\ndef get_image_src(wd):\n    actual_images = wd.find_elements_by_css_selector(css_large)\n    sources = []\n    for img in actual_images:\n        src = img.get_attribute(""src"")\n        if src.startswith(""http"") and not src.startswith(""https://encrypted-tbn0.gstatic.com/""):\n            sources.append(src)\n    if not len(sources):\n        raise KeyError(""no large image"")\n    return sources\n\n@retry(exceptions=selenium_exceptions, tries=6, delay=0.1, backoff=2, logger=retry_logger)\ndef retry_click(el):\n    el.click()\n\ndef get_images(wd, start=0, n=20, out=None):\n    thumbnails = []\n    count = len(thumbnails)\n    while count < n:\n        scroll_to_end(wd)\n        try:\n            thumbnails = get_thumbnails(wd, want_more_than=count)\n        except KeyError as e:\n            logger.warning(""cannot load enough thumbnails"")\n            break\n        count = len(thumbnails)\n    sources = []\n    for tn in thumbnails:\n        try:\n            retry_click(tn)\n        except selenium_exceptions as e:\n            logger.warning(""main image click failed"")\n            continue\n        sources1 = []\n        try:\n            sources1 = get_image_src(wd)\n        except KeyError as e:\n            pass\n            # logger.warning(""main image not found"")\n        if not sources1:\n            tn_src = tn.get_attribute(""src"")\n            if not tn_src.startswith(""data""):\n                logger.warning(""no src found for main image, using thumbnail"")          \n                sources1 = [tn_src]\n            else:\n                logger.warning(""no src found for main image, thumbnail is a data URL"")\n        for src in sources1:\n            if not src in sources:\n                sources.append(src)\n                if out:\n                    print(src, file=out)\n                    out.flush()\n        if len(sources) >= n:\n            break\n    return sources\n\ndef google_image_search(wd, query, safe=""off"", n=20, opts=\'\', out=None):\n    search_url_t = ""https://www.google.com/search?safe={safe}&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img&tbs={opts}""\n    search_url = search_url_t.format(q=urllib.parse.quote(query), opts=urllib.parse.quote(opts), safe=safe)\n    wd.get(search_url)\n    sources = get_images(wd, n=n, out=out)\n    return sources\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Fetch image URLs from Google Image Search.\')\n    parser.add_argument(\'--safe\', type=str, default=""off"", help=\'safe search [off|active|images]\')\n    parser.add_argument(\'--opts\', type=str, default="""", help=\'search options, e.g. isz:lt,islt:svga,itp:photo,ic:color,ift:jpg\')\n    parser.add_argument(\'query\', type=str, help=\'image search query\')\n    parser.add_argument(\'n\', type=int, default=20, help=\'number of images (approx)\')\n    args = parser.parse_args()\n\n    opts = Options()\n    opts.add_argument(""--headless"")\n    # opts.add_argument(""--blink-settings=imagesEnabled=false"")\n    with webdriver.Chrome(options=opts) as wd:\n        sources = google_image_search(wd, args.query, safe=args.safe, n=args.n, opts=args.opts, out=sys.stdout)\n\nmain()\n\n', '\nHaven\'t looked into your code but this is an example solution made with selenium to try to get 400 pictures from the search term\n# -*- coding: utf-8 -*-\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nimport json\nimport os\nimport urllib2\n\nsearchterm = \'vannmelon\' # will also be the name of the folder\nurl = ""https://www.google.co.in/search?q=""+searchterm+""&source=lnms&tbm=isch""\nbrowser = webdriver.Firefox()\nbrowser.get(url)\nheader={\'User-Agent\':""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36""}\ncounter = 0\nsuccounter = 0\n\nif not os.path.exists(searchterm):\n    os.mkdir(searchterm)\n\nfor _ in range(500):\n    browser.execute_script(""window.scrollBy(0,10000)"")\n\nfor x in browser.find_elements_by_xpath(""//div[@class=\'rg_meta\']""):\n    counter = counter + 1\n    print ""Total Count:"", counter\n    print ""Succsessful Count:"", succounter\n    print ""URL:"",json.loads(x.get_attribute(\'innerHTML\'))[""ou""]\n\n    img = json.loads(x.get_attribute(\'innerHTML\'))[""ou""]\n    imgtype = json.loads(x.get_attribute(\'innerHTML\'))[""ity""]\n    try:\n        req = urllib2.Request(img, headers={\'User-Agent\': header})\n        raw_img = urllib2.urlopen(req).read()\n        File = open(os.path.join(searchterm , searchterm + ""_"" + str(counter) + ""."" + imgtype), ""wb"")\n        File.write(raw_img)\n        File.close()\n        succounter = succounter + 1\n    except:\n            print ""can\'t get img""\n\nprint succounter, ""pictures succesfully downloaded""\nbrowser.close()\n\n', '\nAdding to Piees\'s answer, for downloading any number of images from the search results, we need to simulate a click on \'Show more results\' button after first 400 results are loaded.\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nimport os\nimport json\nimport urllib2\nimport sys\nimport time\n\n# adding path to geckodriver to the OS environment variable\n# assuming that it is stored at the same path as this script\nos.environ[""PATH""] += os.pathsep + os.getcwd()\ndownload_path = ""dataset/""\n\ndef main():\n    searchtext = sys.argv[1] # the search query\n    num_requested = int(sys.argv[2]) # number of images to download\n    number_of_scrolls = num_requested / 400 + 1 \n    # number_of_scrolls * 400 images will be opened in the browser\n\n    if not os.path.exists(download_path + searchtext.replace("" "", ""_"")):\n        os.makedirs(download_path + searchtext.replace("" "", ""_""))\n\n    url = ""https://www.google.co.in/search?q=""+searchtext+""&source=lnms&tbm=isch""\n    driver = webdriver.Firefox()\n    driver.get(url)\n\n    headers = {}\n    headers[\'User-Agent\'] = ""Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36""\n    extensions = {""jpg"", ""jpeg"", ""png"", ""gif""}\n    img_count = 0\n    downloaded_img_count = 0\n\n    for _ in xrange(number_of_scrolls):\n        for __ in xrange(10):\n            # multiple scrolls needed to show all 400 images\n            driver.execute_script(""window.scrollBy(0, 1000000)"")\n            time.sleep(0.2)\n        # to load next 400 images\n        time.sleep(0.5)\n        try:\n            driver.find_element_by_xpath(""//input[@value=\'Show more results\']"").click()\n        except Exception as e:\n            print ""Less images found:"", e\n            break\n\n    # imges = driver.find_elements_by_xpath(\'//div[@class=""rg_meta""]\') # not working anymore\n    imges = driver.find_elements_by_xpath(\'//div[contains(@class,""rg_meta"")]\')\n    print ""Total images:"", len(imges), ""\\n""\n    for img in imges:\n        img_count += 1\n        img_url = json.loads(img.get_attribute(\'innerHTML\'))[""ou""]\n        img_type = json.loads(img.get_attribute(\'innerHTML\'))[""ity""]\n        print ""Downloading image"", img_count, "": "", img_url\n        try:\n            if img_type not in extensions:\n                img_type = ""jpg""\n            req = urllib2.Request(img_url, headers=headers)\n            raw_img = urllib2.urlopen(req).read()\n            f = open(download_path+searchtext.replace("" "", ""_"")+""/""+str(downloaded_img_count)+"".""+img_type, ""wb"")\n            f.write(raw_img)\n            f.close\n            downloaded_img_count += 1\n        except Exception as e:\n            print ""Download failed:"", e\n        finally:\n            print\n        if downloaded_img_count >= num_requested:\n            break\n\n    print ""Total downloaded: "", downloaded_img_count, ""/"", img_count\n    driver.quit()\n\nif __name__ == ""__main__"":\n    main()\n\nFull code is here.  \n', '\nYou can also use Selenium with Python. Here is how:\nfrom selenium import webdriver\nimport urllib\nfrom selenium.webdriver.common.keys import Keys\ndriver = webdriver.Chrome(\'C:/Python27/Scripts/chromedriver.exe\')\nword=""apple""\nurl=""http://images.google.com/search?q=""+word+""&tbm=isch&sout=1""\ndriver.get(url)\nimageXpathSelector=\'//*[@id=""ires""]/table/tbody/tr[1]/td[1]/a/img\'\nimg=driver.find_element_by_xpath(imageXpathSelector)\nsrc=(img.get_attribute(\'src\'))\nurllib.urlretrieve(src, word+"".jpg"")\ndriver.close()\n\n(This code works on Python 2.7)\nPlease be informed that you should install Selenium package with \'pip install selenium\' and you should download chromedriver.exe from here\nOn the contrary of the other web scraping techniques, Selenium opens the browser and download the items because Selenium\'s mission is testing rather than scraping.\n', ""\nThis worked for me in Windows 10, Python 3.9.7:\n\npip install bing-image-downloader\n\nBelow code downloads 10 images of India from Bing search Engine to desired output folder:\nfrom bing_image_downloader import downloader\ndownloader.download('India', limit=10,  output_dir='dataset', adult_filter_off=True, force_replace=False, timeout=60, verbose=True)\n\nDocumentation: https://pypi.org/project/bing-image-downloader/\n"", '\nThis one as other code snippets have grown old and no longer worked for me. Downloads 100 images for each keyword, inspired from one of the solutions above.\nfrom bs4 import BeautifulSoup\nimport urllib2\nimport os\n\n\nclass GoogleeImageDownloader(object):\n    _URL = ""https://www.google.co.in/search?q={}&source=lnms&tbm=isch""\n    _BASE_DIR = \'GoogleImages\'\n    _HEADERS = {\n        \'User-Agent\':""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36""\n    }\n\n    def __init__(self):\n        query = raw_input(""Enter keyword to search images\\n"")\n        self.dir_name = os.path.join(self._BASE_DIR, query.split()[0])\n        self.url = self._URL.format(urllib2.quote(query)) \n        self.make_dir_for_downloads()\n        self.initiate_downloads()\n\n    def make_dir_for_downloads(self):\n        print ""Creating necessary directories""\n        if not os.path.exists(self._BASE_DIR):\n            os.mkdir(self._BASE_DIR)\n\n        if not os.path.exists(self.dir_name):\n            os.mkdir(self.dir_name)\n\n    def initiate_downloads(self):\n        src_list = []\n        soup = BeautifulSoup(urllib2.urlopen(urllib2.Request(self.url,headers=self._HEADERS)),\'html.parser\')\n        for img in soup.find_all(\'img\'):\n            if img.has_attr(""data-src""):\n                src_list.append(img[\'data-src\'])\n        print ""{} of images collected for downloads"".format(len(src_list))\n        self.save_images(src_list)\n\n    def save_images(self, src_list):\n        print ""Saving Images...""\n        for i , src in enumerate(src_list):\n            try:\n                req = urllib2.Request(src, headers=self._HEADERS)\n                raw_img = urllib2.urlopen(req).read()\n                with open(os.path.join(self.dir_name , str(i)+"".jpg""), \'wb\') as f:\n                    f.write(raw_img)\n            except Exception as e:\n                print (""could not save image"")\n                raise e\n\n\nif __name__ == ""__main__"":\n    GoogleeImageDownloader()\n\n', '\nI know this question is old, but I ran across it recently and none of the previous answers work anymore. So I wrote this script to gather images from google. As of right now it can download as many images as are available.\nhere is a github link to it as well https://github.com/CumminUp07/imengine/blob/master/get_google_images.py\nDISCLAIMER: DUE TO COPYRIGHT ISSUES, IMAGES GATHERED SHOULD ONLY BE USED FOR RESEARCH AND EDUCATION PURPOSES ONLY\nfrom bs4 import BeautifulSoup as Soup\nimport urllib2\nimport json\nimport urllib\n\n#programtically go through google image ajax json return and save links to list#\n#num_images is more of a suggestion                                            #  \n#it will get the ceiling of the nearest 100 if available                       #\ndef get_links(query_string, num_images):\n    #initialize place for links\n    links = []\n    #step by 100 because each return gives up to 100 links\n    for i in range(0,num_images,100):\n        url = \'https://www.google.com/search?ei=1m7NWePfFYaGmQG51q7IBg&hl=en&q=\'+query_string+\'\\\n        &tbm=isch&ved=0ahUKEwjjovnD7sjWAhUGQyYKHTmrC2kQuT0I7gEoAQ&start=\'+str(i)+\'\\\n        &yv=2&vet=10ahUKEwjjovnD7sjWAhUGQyYKHTmrC2kQuT0I7gEoAQ.1m7NWePfFYaGmQG51q7IBg.i&ijn=1&asearch=ichunk&async=_id:rg_s,_pms:s\'\n\n        #set user agent to avoid 403 error\n        request = urllib2.Request(url, None, {\'User-Agent\': \'Mozilla/5.0\'}) \n\n        #returns json formatted string of the html\n        json_string = urllib2.urlopen(request).read() \n\n        #parse as json\n        page = json.loads(json_string) \n\n        #html found here\n        html = page[1][1] \n\n        #use BeautifulSoup to parse as html\n        new_soup = Soup(html,\'lxml\')\n\n        #all img tags, only returns results of search\n        imgs = new_soup.find_all(\'img\')\n\n        #loop through images and put src in links list\n        for j in range(len(imgs)):\n            links.append(imgs[j][""src""])\n\n    return links\n\n#download images                              #\n#takes list of links, directory to save to    # \n#and prefix for file names                    #\n#saves images in directory as a one up number #\n#with prefix added                            #\n#all images will be .jpg                      #\ndef get_images(links,directory,pre):\n    for i in range(len(links)):\n        urllib.urlretrieve(links[i], ""./""+directory+""/""+str(pre)+str(i)+"".jpg"")\n\n#main function to search images                 #\n#takes two lists, base term and secondary terms #\n#also takes number of images to download per    #\n#combination                                    #\n#it runs every combination of search terms      #\n#with base term first then secondary            #\ndef search_images(base,terms,num_images):\n    for y in range(len(base)):\n        for x in range(len(terms)):\n            all_links = get_links(base[y]+\'+\'+terms[x],num_images)\n            get_images(all_links,""images"",x)\n\nif __name__ == \'__main__\':\n    terms = [""cars"",""numbers"",""scenery"",""people"",""dogs"",""cats"",""animals""]\n    base = [""animated""]\n    search_images(base,terms,1000)\n\n', '\nInstead of google image search, try other image searches like ecosia or bing.\nHere is a sample code for retrieving images from ecosia search engine.\nfrom bs4 import BeautifulSoup\nimport requests\nimport urllib\n\nuser_agent = \'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7\'\nheaders = {\'User-Agent\':user_agent} \nurls = [""https://www.ecosia.org/images?q=india%20pan%20card%20example""]\n#The url\'s from which the image is to be extracted.\nindex = 0\n\nfor url in urls:\n    request = urllib.request.Request(url,None,headers) #The assembled request\n    response = urllib.request.urlopen(request)\n    data = response.read() # Read the html result page\n\n    soup = BeautifulSoup(data, \'html.parser\')\n    \n    for link in soup.find_all(\'img\'):   \n        #The images are enclosed in \'img\' tag and the \'src\' contains the url of the image.\n        img_url = link.get(\'src\')\n        dest = str(index) + "".jpg""  #Destination to store the image.\n        try:\n            urllib.request.urlretrieve(img_url)\n            index += 1\n        except:\n            continue\n\nThe code works with google image search but it fails to retrieve images because google stores the images in encrypted format which is difficult to retrieve from the image url.\nThe solutions works as on 1-Feb-2021.\n', '\nOkay, so instead of coding this from you I am going to tell you what you\'re doing wrong and it might lead you in the right direction. Usually most modern websites render html dynamically via javascript and so if you simply send a GET request(with urllib/CURL/fetch/axios) you wont get what you usually see in the browser going to the same URL/web address. What you need is something that renders the javascript code to create the same HTML/webpage you see on your browser, you can use something like selenium gecko driver for firefox to do this and there python modules out there that let you do this.\nI hope this helps, if you still feel lost here\'s a simple script i wrote a while back to extract something similar from your google photos\nfrom selenium import webdriver\nimport re\nurl=""https://photos.app.goo.gl/xxxxxxx""\ndriver = webdriver.Firefox()\ndriver.get(url)\nregPrms=""^background-image\\:url\\(.*\\)$""\nregPrms=""^The.*Spain$""\nhtml = driver.page_source\n\nurls=re.findall(""(?P<url>https?://[^\\s\\""$]+)"", html)\n\nfin=[]\nfor url in urls:\n        if ""video-downloads"" in url:\n            fin.append(url)\nprint(""The Following ZIP contains all your pictures"")\nfor url in fin:\n        print(""-------------------"")\n        print(url)\n\n\n', '\n\nYou can achieve this using selenium as others mentioned it above.\nAlternatively, you can try using Google Images API from SerpApi. Check out the playground.\n\nCode and example. Fuction to download images was taken from this answer:\nimport os, time, shutil, httpx, asyncio\nfrom urllib.parse import urlparse\nfrom serpapi import GoogleSearch\n\n# https://stackoverflow.com/a/39217788/1291371\nasync def download_file(url):\n    print(f\'Downloading {url}\')\n\n    # https://stackoverflow.com/a/18727481/1291371\n    parsed_url = urlparse(url)\n    local_filename = os.path.basename(parsed_url.path)\n\n    os.makedirs(\'images\', exist_ok=True)\n\n    async with httpx.AsyncClient() as client:\n        async with client.stream(\'GET\', url) as response:\n            async with open(f\'images/{local_filename}\', \'wb\') as f:\n                await asyncio.to_thread(shutil.copyfileobj, response.raw, f)\n\n    return local_filename\n\nasync def main():\n    start = time.perf_counter()\n\n    params = {\n        ""engine"": ""google"",\n        ""ijn"": ""0"",\n        ""q"": ""lasagna"",\n        ""tbm"": ""isch"",\n        ""api_key"": os.getenv(""API_KEY""),\n    }\n\n    search = GoogleSearch(params)\n    results = search.get_dict()\n\n    download_files_tasks = [\n        download_file(image[\'original\']) for image in results[\'images_results\']\n    ]\n\n    await asyncio.gather(*download_files_tasks, return_exceptions=True)\n\n    print(\n        f""Downloaded {len(download_files_tasks)} images in {time.perf_counter() - start:0.4f} seconds"")\n\nasyncio.run(main())\n\n\nDisclaimer, I work for SerpApi.\n\n', ""\nThe one I used is :\nhttps://github.com/hellock/icrawler\nThis package is a mini framework of web crawlers. With modularization design, it is easy to use and extend. It supports media data like images and videos very well, and can also be applied to texts and another type of files. Scrapy is heavy and powerful, while icrawler is tiny and flexible.\ndef main():\n    parser = ArgumentParser(description='Test built-in crawlers')\n    parser.add_argument(\n        '--crawler',\n        nargs='+',\n        default=['google', 'bing', 'baidu', 'flickr', 'greedy', 'urllist'],\n        help='which crawlers to test')\n    args = parser.parse_args()\n    for crawler in args.crawler:\n        eval('test_{}()'.format(crawler))\n        print('\\n')\n\n""]",https://stackoverflow.com/questions/20716842/python-download-images-from-google-image-search,web-scraping
Wait page to load before getting data with requests.get in python 3,"
I have a page that i need to get the source to use with BS4, but the middle of the page takes 1 second(maybe less) to load the content, and requests.get catches the source of the page before the section loads, how can I wait a second before getting the data?
r = requests.get(URL + self.search, headers=USER_AGENT, timeout=5 )
    soup = BeautifulSoup(r.content, 'html.parser')
    a = soup.find_all('section', 'wrapper')

The page
<section class=""wrapper"" id=""resultado_busca"">

",94k,"
            38
        ","['\nIt doesn\'t look like a problem of waiting, it looks like the element is being created by JavaScript, requests can\'t handle dynamically generated elements by JavaScript. A suggestion is to use selenium together with PhantomJS to get the page source, then you can use BeautifulSoup for your parsing, the code shown below will do exactly that:\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\n\nurl = ""http://legendas.tv/busca/walking%20dead%20s03e02""\nbrowser = webdriver.PhantomJS()\nbrowser.get(url)\nhtml = browser.page_source\nsoup = BeautifulSoup(html, \'lxml\')\na = soup.find(\'section\', \'wrapper\')\n\nAlso, there\'s no need to use .findAll if you are only looking for one element only.\n', '\nI had the same problem, and none of the submitted answers really worked for me.\nBut after long research, I found a solution:\nfrom requests_html import HTMLSession\ns = HTMLSession()\nresponse = s.get(url)\nresponse.html.render()\n\nprint(response)\n# prints out the content of the fully loaded page\n# response can be parsed with for example bs4\n\nThe requests_html package (docs) is an official package, distributed by the Python Software Foundation. It has some additional JavaScript capabilities, like for example the ability to wait until the JS of a page has finished loading.\nThe package only supports Python Version 3.6 and above at the moment, so it might not work with another version.\n', ""\nI found a way to that !!!\nr = requests.get('https://github.com', timeout=(3.05, 27))\n\nIn this, timeout has two values, first one is to set session timeout and the second one is what you need. The second one decides after how much seconds the response is sent. You can calculate the time it takes to populate and then print the data out. \n"", '\nSelenium is good way to solve that, but accepted answer is quite deprecated. As @Seth mentioned in comments headless mode of Firefox/Chrome (or possibly other browsers) should be used instead of PhantomJS.\nFirst of all you need to download specific driver:\nGeckodriver for Firefox\nChromeDriver for Chrome\nNext you can add path to downloaded driver to system your PATH variable. But that\'s not necessary, you can also specify in code where executable lies.\nFirefox:\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\n\noptions = webdriver.FirefoxOptions()\noptions.add_argument(\'--headless\')\n# executable_path param is not needed if you updated PATH\nbrowser = webdriver.Firefox(options=options, executable_path=\'YOUR_PATH/geckodriver.exe\')\nbrowser.get(""http://legendas.tv/busca/walking%20dead%20s03e02"")\nhtml = browser.page_source\nsoup = BeautifulSoup(html, features=""html.parser"")\nprint(soup)\nbrowser.quit()\n\nSimilarly for Chrome:\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver    \n\noptions = webdriver.ChromeOptions()\noptions.add_argument(\'--headless\')\n# executable_path param is not needed if you updated PATH\nbrowser = webdriver.Chrome(options=options, executable_path=\'YOUR_PATH/chromedriver.exe\')\nbrowser.get(""http://legendas.tv/busca/walking%20dead%20s03e02"")\nhtml = browser.page_source\nsoup = BeautifulSoup(html, features=""html.parser"")\nprint(soup)\nbrowser.quit()\n\nIt\'s good to remember about browser.quit() to avoid hanging processes after code execution. If you worry that your code may fail before browser is disposed you can wrap it in try...except block and put browser.quit() in finally part to ensure it will be called.\nAdditionally, if part of source is still not loaded using that method, you can ask selenium to wait till specific element is present:\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as ec\nfrom selenium.webdriver.common.by import By\nfrom selenium.common.exceptions import TimeoutException\n\noptions = webdriver.FirefoxOptions()\noptions.add_argument(\'--headless\')\nbrowser = webdriver.Firefox(options=options, executable_path=\'YOUR_PATH/geckodriver.exe\')\n\ntry:\n    browser.get(""http://legendas.tv/busca/walking%20dead%20s03e02"")\n    timeout_in_seconds = 10\n    WebDriverWait(browser, timeout_in_seconds).until(ec.presence_of_element_located((By.ID, \'resultado_busca\')))\n    html = browser.page_source\n    soup = BeautifulSoup(html, features=""html.parser"")\n    print(soup)\nexcept TimeoutException:\n    print(""I give up..."")\nfinally:\n    browser.quit()\n\nIf you\'re interested in other drivers than Firefox or Chrome check docs.\n', '\nIn Python 3, Using the module urllib in practice works better when loading dynamic webpages than the requests module. \ni.e\nimport urllib.request\ntry:\n    with urllib.request.urlopen(url) as response:\n\n        html = response.read().decode(\'utf-8\')#use whatever encoding as per the webpage\nexcept urllib.request.HTTPError as e:\n    if e.code==404:\n        print(f""{url} is not found"")\n    elif e.code==503:\n        print(f\'{url} base webservices are not available\')\n        ## can add authentication here \n    else:\n        print(\'http error\',e)\n\n', '\nJust to list my way of doing it, maybe it can be of value for someone:\nmax_retries = # some int\nretry_delay = # some int\nn = 1\nready = 0\nwhile n < max_retries:\n  try:\n     response = requests.get(\'https://github.com\')\n     if response.ok:\n        ready = 1\n        break\n  except requests.exceptions.RequestException:\n     print(""Website not availabe..."")\n  n += 1\n  time.sleep(retry_delay)\n\nif ready != 1:\n  print(""Problem"")\nelse:\n  print(""All good"")\n\n']",https://stackoverflow.com/questions/45448994/wait-page-to-load-before-getting-data-with-requests-get-in-python-3,web-scraping
"""SSL: certificate_verify_failed"" error when scraping https://www.thenewboston.com/","
So I started learning Python recently using ""The New Boston's"" videos on youtube, everything was going great until I got to his tutorial of making a simple web crawler. While I understood it with no problem, when I run the code I get errors all seemingly based around ""SSL: CERTIFICATE_VERIFY_FAILED."" I've been searching for an answer since last night trying to figure out how to fix it, it seems no one else in the comments on the video or on his website are having the same problem as me and even using someone elses code from his website I get the same results. I'll post the code from the one I got from the website as it's giving me the same error and the one I coded is a mess right now.
import requests
from bs4 import BeautifulSoup

def trade_spider(max_pages):
    page = 1
    while page <= max_pages:
        url = ""https://www.thenewboston.com/forum/category.php?id=15&orderby=recent&page="" + str(page) #this is page of popular posts
        source_code = requests.get(url)
        # just get the code, no headers or anything
        plain_text = source_code.text
        # BeautifulSoup objects can be sorted through easy
        for link in soup.findAll('a', {'class': 'index_singleListingTitles'}): #all links, which contains """" class='index_singleListingTitles' """" in it.
            href = ""https://www.thenewboston.com/"" + link.get('href')
            title = link.string # just the text, not the HTML
            print(href)
            print(title)
            # get_single_item_data(href)
    page += 1
trade_spider(1)

The full error is: ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:645)
I apologize if this is  a dumb question, I'm still new to programming but I seriously can't figure this out, I was thinking about just skipping this tutorial but it's bothering me not being able to fix this, thanks!
",95k,"
            28
        ","[""\nThe problem is not in your code but in the web site you are trying to access. When looking at the analysis by SSLLabs you will note:\n\nThis server's certificate chain is incomplete. Grade capped to B.\n\nThis means that the server configuration is wrong and that not only python but several others will have problems with this site. Some desktop browsers work around this configuration problem by trying to load the missing certificates from the internet or fill in with cached certificates. But other browsers or applications will fail too, similar to python.\nTo work around the broken server configuration you might explicitly extract the missing certificates and add them to you trust store. Or you might give the certificate as trust inside the verify argument. From the documentation:\n\nYou can pass verify the path to a CA_BUNDLE file or directory with\ncertificates of trusted CAs:\n>>> requests.get('https://github.com', verify='/path/to/certfile') \n\nThis list of trusted CAs can also be specified through the\nREQUESTS_CA_BUNDLE environment variable.\n\n"", '\nYou can tell requests not to verify the SSL certificate:\n>>> url = ""https://www.thenewboston.com/forum/category.php?id=15&orderby=recent&page=1""\n>>> response = requests.get(url, verify=False)\n>>> response.status_code\n200\n\nSee more in the requests doc\n', '\nYou are probably missing the stock certificates in your system. E.g. if running on Ubuntu, check that ca-certificates package is installed.\n', ""\nif you want to use the Python dmg installer, you also have to read Python 3's ReadMe and run the bash command to get new certificates.\nTry running \n/Applications/Python\\ 3.6/Install\\ Certificates.command\n\n"", '\nIt\'s worth shedding a bit more ""hands-on"" light about what happens here, adding upon @Steffen Ullrich\'s answer here and elsewhere:\n\nurllib and “SSL: CERTIFICATE_VERIFY_FAILED” Error\nPython Urllib2 SSL error (a very detailed answer)\n\nNotes:\n\nI\'ll use another website than the OP, because the OP\'s website currently has no issues.\nI used Ubunto to run the following commands (curl and openssl). I tried running curl on my Windows 10, but got different, unhelpful output.\n\nThe error experienced by the OP can be ""reproduced"" by using the following curl command:\ncurl -vvI https://www.vimmi.net\n\nWhich outputs (note the last line):\n* TCP_NODELAY set\n* Connected to www.vimmi.net (82.80.192.7) port 443 (#0)\n* ALPN, offering h2\n* ALPN, offering http/1.1\n* successfully set certificate verify locations:\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: /etc/ssl/certs\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\n* TLSv1.2 (IN), TLS handshake, Certificate (11):\n* TLSv1.2 (OUT), TLS alert, Server hello (2):\n* SSL certificate problem: unable to get local issuer certificate\n* stopped the pause stream!\n* Closing connection 0\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\n\nNow let\'s run it with the --insecure flag, which will display the problematic certificate:\ncurl --insecure -vvI https://www.vimmi.net\n\nOutputs (note the last two lines):\n* Rebuilt URL to: https://www.vimmi.net/\n*   Trying 82.80.192.7...\n* TCP_NODELAY set\n* Connected to www.vimmi.net (82.80.192.7) port 443 (#0)\n* ALPN, offering h2\n* ALPN, offering http/1.1\n* successfully set certificate verify locations:\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: /etc/ssl/certs\n* [...]\n* Server certificate:\n*  subject: OU=Domain Control Validated; CN=vimmi.net\n*  start date: Aug  5 15:43:45 2019 GMT\n*  expire date: Oct  4 16:16:12 2020 GMT\n*  issuer: C=US; ST=Arizona; L=Scottsdale; O=GoDaddy.com, Inc.; OU=http://certs.godaddy.com/repository/; CN=Go Daddy Secure Certificate Authority - G2\n*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.\n\nThe same result can be seen using openssl, which is worth mentioning because it\'s used internally by python:\necho | openssl s_client -connect vimmi.net:443\n\nOutputs:\nCONNECTED(00000005)\ndepth=0 OU = Domain Control Validated, CN = vimmi.net\nverify error:num=20:unable to get local issuer certificate\nverify return:1\ndepth=0 OU = Domain Control Validated, CN = vimmi.net\nverify error:num=21:unable to verify the first certificate\nverify return:1\n---\nCertificate chain\n 0 s:OU = Domain Control Validated, CN = vimmi.net\n   i:C = US, ST = Arizona, L = Scottsdale, O = ""GoDaddy.com, Inc."", OU = http://certs.godaddy.com/repository/, CN = Go Daddy Secure Certificate Authority - G2\n---\nServer certificate\n-----BEGIN CERTIFICATE-----\n[...]\n-----END CERTIFICATE-----\n[...]\n---\nDONE\n\nSo why both curl and openssl can\'t verify the certificate Go Daddy issued for that website?\nWell, to ""verify a certificate"" (to use openssl\'s error message terminology) means to verify that the certificate contains a trusted source signature (put differently: the certificate was signed by a trusted source), thus verifying vimmi.net identity (""identity"" here strictly means that ""the public key contained in the certificate belongs to the person, organization, server or other entity noted in the certificate"").\nA source is ""trusted"" if we can establish its ""chain of trust"", with the following properties:\n\n\nThe Issuer of each certificate (except the last one) matches the Subject of the next certificate in the list\nEach certificate (except the last one) is signed by the secret key corresponding to the next certificate in the chain (i.e. the signature\nof one certificate can be verified using the public key contained in\nthe following certificate)\nThe last certificate in the list is a trust anchor: a certificate that you trust because it was delivered to you by some trustworthy\nprocedure\n\n\nIn our case, the issuer is ""Go Daddy Secure Certificate Authority - G2"". That is, the entity named ""Go Daddy Secure Certificate Authority - G2"" signed the certificate, so it\'s supposed to be a trusted source.\nTo establish this entity\'s trustworthiness, we have 2 options:\n\nAssume that ""Go Daddy Secure Certificate Authority - G2"" is a ""trust anchor"" (see listing 3 above). Well, it turns out that curl and openssl try to act upon this assumption: they searched that entity\'s certificate on their default paths (called CA paths), which are:\n\nfor curl, it\'s /etc/ssl/certs.\nfor openssl, it\'s /use/lib/ssl (run openssl version -a to see that).\n\n\n\nBut that certificate wasn\'t found, leaving us with a second option:\n\nFollow steps 1 and 2 listed above; in order to do that, we need to get the certificate issued for that entity.\nThis can be achieved by downloading it from its source, or using the browser.\n\nfor example, go to vimmi.net using Chrome, click the padlock > ""Certificate"" > ""Certification Path"" tab, select the entity > ""View Certificate"", then in the opened window go to ""Details"" tab > ""Copy to File"" > Base-64 encoded > save the file)\n\n\n\nGreat! Now that we have that certificate (which can be in whatever file format: cer, pem, etc.; you can even save it as a txt file), let\'s tell curl to use it:\ncurl --cacert test.cer https://vimmi.net\n\nGoing back to Python\nOnce we have:\n\n""Go Daddy Secure Certificate Authority - G2"" certificate\n""Go Daddy Root Certificate Authority - G2"" certificate (wasn\'t mentioned above, but can be achieved in a similar way).\n\nWe need to copy their contents into a single file, let\'s call it combined.cer, and let\'s put it in the current directory. Then, simply:\nimport requests\n\nres = requests.get(""https://vimmi.net"", verify=""./combined.cer"")\nprint (res.status_code) # 200\n\n\nBTW, ""Go Daddy Root Certificate Authority - G2"" is listed as a trusted authority by browsers and various tools; that\'s why we didn\'t have to specify it for curl.\n\nFurther reading:\n\nhow are ssl certificates verified, especially @ychaouche image.\nThe First Few Milliseconds of an HTTPS Connection\nWikipedia: Public key certificate, Certificate authority\nNice video: Basics of Certificate Chain Validation.\nHelpful SE answers that focus on certificate signature terminology: 1, 2, 3.\nCertificates in relation to Man-In-The-Middle attack: 1, 2.\nThe most dangerous code in the world: validating SSL certificates in non-browser software\n\n', '\nI\'m posting this as an answer because I\'ve gotten past your issue thus far, but there\'s still issues in your code (which when fixed, I can update).\nSo long story short: you could be using an old version of requests or the ssl certificate should be invalid. There\'s more information in this SO question: Python requests ""certificate verify failed""\nI\'ve updated the code into my own bsoup.py file:\n#!/usr/bin/env python3\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef trade_spider(max_pages):\n    page = 1\n    while page <= max_pages:\n        url = ""https://www.thenewboston.com/forum/category.php?id=15&orderby=recent&page="" + str(page) #this is page of popular posts\n        source_code = requests.get(url, timeout=5, verify=False)\n        # just get the code, no headers or anything\n        plain_text = source_code.text\n        # BeautifulSoup objects can be sorted through easy\n        for link in BeautifulSoup.findAll(\'a\', {\'class\': \'index_singleListingTitles\'}): #all links, which contains """" class=\'index_singleListingTitles\' """" in it.\n            href = ""https://www.thenewboston.com/"" + link.get(\'href\')\n            title = link.string # just the text, not the HTML\n            print(href)\n            print(title)\n            # get_single_item_data(href)\n\n        page += 1\n\nif __name__ == ""__main__"":\n    trade_spider(1)\n\nWhen I run the script, it gives me this error:\nhttps://www.thenewboston.com/forum/category.php?id=15&orderby=recent&page=1\nTraceback (most recent call last):\n  File ""./bsoup.py"", line 26, in <module>\n    trade_spider(1)\n  File ""./bsoup.py"", line 16, in trade_spider\n    for link in BeautifulSoup.findAll(\'a\', {\'class\': \'index_singleListingTitles\'}): #all links, which contains """" class=\'index_singleListingTitles\' """" in it.\n  File ""/usr/local/lib/python3.4/dist-packages/bs4/element.py"", line 1256, in find_all\n    generator = self.descendants\nAttributeError: \'str\' object has no attribute \'descendants\'\n\nThere\'s an issue somewhere with your findAll method. I\'ve used both python3 and python2, wherein python2 reports this:\nTypeError: unbound method find_all() must be called with BeautifulSoup instance as first argument (got str instance instead)\n\nSo it looks like you\'ll need to fix up that method before you can continue\n', '\nI spent several hours trying to fix some Python and update certs on a VM.  In my case I was working against a server that someone else had set up.  It turned out that the wrong cert had been uploaded to the server.  I found this command on another SO answer.\nroot@ubuntu:~/cloud-tools# openssl s_client -connect abc.def.com:443\nCONNECTED(00000005)\ndepth=0 OU = Domain Control Validated, CN = abc.def.com\nverify error:num=20:unable to get local issuer certificate\nverify return:1\ndepth=0 OU = Domain Control Validated, CN = abc.def.com\nverify error:num=21:unable to verify the first certificate\nverify return:1\n---\nCertificate chain\n0 s:OU = Domain Control Validated, CN = abc.def.com\n   i:C = US, ST = Arizona, L = Scottsdale, O = ""GoDaddy.com, Inc."", OU = http://certs.godaddy.com/repository/, CN = Go Daddy Secure Certificate Authority - G2\n\n']",https://stackoverflow.com/questions/34503206/ssl-certificate-verify-failed-error-when-scraping-https-www-thenewboston-co,web-scraping
Reading dynamically generated web pages using python,"
I am trying to scrape a web site using python and beautiful soup. I encountered that in some sites, the image links although seen on the browser is cannot be seen in the source code. However on using Chrome Inspect or Fiddler, we can see the the corresponding codes. 
What I see in the source code is:
<div id=""cntnt""></div>

But on Chrome Inspect, I can see a whole bunch of HTML\CSS code generated within this div class. Is there a way to load the generated content also within python? I am using the regular urllib in python and I am able to get the source but without the generated part.
I am not a web developer hence I am not able to express the behaviour in better terms. Please feel free to clarify if my question seems vague !
",42k,"
            24
        ","['\nYou need JavaScript Engine to parse and run JavaScript code inside the page.\nThere are a bunch of headless browsers that can help you\nhttp://code.google.com/p/spynner/\nhttp://phantomjs.org/\nhttp://zombie.labnotes.org/\nhttp://github.com/ryanpetrello/python-zombie\nhttp://jeanphix.me/Ghost.py/\nhttp://webscraping.com/blog/Scraping-JavaScript-webpages-with-webkit/\n', '\nThe Content of the website may be generated after load via javascript, In order to obtain the generated script via python refer to this answer\n', '\nA regular scraper gets just the HTML document. To get any content generated by JavaScript logic, you rather need a Headless browser that would also generate the DOM, load and run the scripts like a regular browser would. The Wikipedia article and some other pages on the Net have lists of those and their capabilities.\nKeep in mind when choosing that some previously major products of those are abandoned now.\n', '\nTRY THIS FIRST!\nPerhaps the data technically could be in the javascript itself and all this javascript engine business is needed. (Some GREAT links here!)\nBut from experience, my first guess is that the JS is pulling the data in via an ajax request. If you can get your program simulate that, you\'ll probably get everything you need handed right to you without any tedious parsing/executing/scraping involved!\nIt will take a little detective work though. I suggest turning on your network traffic logger (such as ""Web Developer Toolbar"" in Firefox) and then visiting the site. Focus your attention attention on any/all XmlHTTPRequests.  The data you need should be found somewhere in one of these responses, probably in the middle of some JSON text.\nNow, see if you can re-create that request and get the data directly.  (NOTE: You may have to set the User-Agent of your request so the server thinks you\'re a ""real"" web browser.)\n']",https://stackoverflow.com/questions/13960567/reading-dynamically-generated-web-pages-using-python,web-scraping
Detect when a web page is loaded without using sleep,"
I am creating a VB script on windows which opens a site in IE. What I want: Detect when the web page is loaded and display a message. I achieved this by using sleep (WScript.Sleep) for approx. seconds when the site gets loaded. However, the site pops up user name, password in the midway. Only when the user enter credentials, it finishes loading the page. So I don't want to use ""sleep"" for approx seconds, instead an exact function or a way to detect that the page got loaded. I checked on line and tried using Do While loop, onload, onclick functions, but nothing works. To simplify, even if I write a script to open a site like yahoo and detect, display a message ""Hi"" when the page is loaded: It doesn't work without using sleep (WScript.Sleep).
",15k,"
            5
        ","['\nTry conventional method:\nSet objIE = CreateObject(""InternetExplorer.Application"")\nobjIE.Visible = True\nobjIE.Navigate ""https://www.yahoo.com/""\nDo While objIE.ReadyState <> 4\n    WScript.Sleep 10\nLoop\n\' your code here\n\' ...\n\nUPD: this one should check for errors:\nSet objIE = CreateObject(""InternetExplorer.Application"")\nobjIE.Visible = True\nobjIE.Navigate ""https://www.yahoo.com/""\nOn Error Resume Next\nDo \n    If objIE.ReadyState = 4 Then\n        If Err = 0 Then\n            Exit Do\n        Else\n            Err.Clear\n        End If\n    End If\n    WScript.Sleep 10\nLoop\nOn Error Goto 0\n\' your code here\n\' ...\n\nUPD2: You wrote that IE gets disconnected as the login pop-up comes in, hypothetically there is a way to catch disconnection, and then get IE instance again. Note this is ""abnormal programming"" :) I hope this helps:\nOption Explicit\nDim objIE, strSignature, strInitType\n\nSet objIE = CreateObject(""InternetExplorer.Application"") \' create IE instance\nobjIE.Visible = True\nstrSignature = Left(CreateObject(""Scriptlet.TypeLib"").GUID, 38) \' generate uid\nobjIE.putproperty ""marker"", strSignature \' tokenize the instance\nstrInitType = TypeName(objIE) \' get typename\nobjIE.Navigate ""https://www.yahoo.com/""\nMsgBox ""Initial type = "" & TypeName(objIE) \' for visualisation\n\nOn Error Resume Next\nDo While TypeName(objIE) = strInitType \' wait until typename changes (ActveX disconnection), may cause error 800A000E if not within OERN\n    WScript.Sleep 10\nLoop\nMsgBox ""Changed type = "" & TypeName(objIE) \' for visualisation\n\nSet objIE = Nothing \' excessive statement, just for clearance\nDo\n    For Each objIE In CreateObject(""Shell.Application"").Windows \' loop through all explorer windows to find tokenized instance\n        If objIE.getproperty(""marker"") = strSignature Then \' our instance found\n            If TypeName(objIE) = strInitType Then Exit Do \' may be excessive type check\n        End If\n    Next\n    WScript.Sleep 10\nLoop\nMsgBox ""Found type = "" & TypeName(objIE) \' for visualisation\nOn Error GoTo 0\n\nDo While objIE.ReadyState <> 4 \' conventional wait if instance not ready\n    WScript.Sleep 10\nLoop\n\nMsgBox ""Title = "" & objIE.Document.Title \' for visualisation\n\nYou can get all text nodes, links etc. from DOM, as follows:\nOption Explicit\nDim objIE, colTags, strResult, objTag, objChild, arrResult\n\nSet objIE = CreateObject(""InternetExplorer.Application"")\nobjIE.Visible = True\nobjIE.Navigate ""https://www.yahoo.com/""\n\nDo While objIE.ReadyState <> 4\n    WScript.Sleep 10\nLoop\n\nSet colTags = objIE.Document.GetElementsByTagName(""a"")\nstrResult = ""Total "" & colTags.Length & "" DOM Anchor Nodes:"" & vbCrLf\nFor Each objTag In colTags\n    strResult = strResult & objTag.GetAttribute(""href"") & vbCrLf\nNext\nShowInNotepad strResult\n\nSet colTags = objIE.Document.GetElementsByTagName(""*"")\narrResult = Array()\nFor Each objTag In colTags\n    For Each objChild In objTag.ChildNodes\n        If objChild.NodeType = 3 Then\n            ReDim Preserve arrResult(UBound(arrResult) + 1)\n            arrResult(UBound(arrResult)) = objChild.NodeValue\n        End If\n    Next\nNext\nstrResult = ""Total "" & colTags.Length & "" DOM object nodes + total "" & UBound(arrResult) + 1 & "" #text nodes:"" & vbCrLf\nstrResult = strResult & Join(arrResult, vbCrLf)\nShowInNotepad strResult\n\nobjIE.Quit\n\nSub ShowInNotepad(strToFile)\n    Dim strTempPath\n    With CreateObject(""Scripting.FileSystemObject"")\n        strTempPath = CreateObject(""WScript.Shell"").ExpandEnvironmentStrings(""%TEMP%"") & ""\\"" & .gettempname\n        With .CreateTextFile(strTempPath, True, True)\n            .WriteLine (strToFile)\n            .Close\n        End With\n        CreateObject(""WScript.Shell"").Run ""notepad.exe "" & strTempPath, 1, True\n        .DeleteFile (strTempPath)\n    End With\nEnd Sub\n\nAlso look get text data\nUPD3: I want to place here additional check if webpage loading and initialization are completed:\n\' ...\n\' Navigating to some url\nobjIE.Navigate strUrl\n\' Wait for IE ready\nDo While objIE.ReadyState <> 4 Or objIE.Busy\n    WScript.Sleep 10\nLoop\n\' Wait for document complete\nDo While objIE.Document.ReadyState <> ""complete""\n    WScript.Sleep 10\nLoop\n\' Processing loaded webpage code\n\' ...\n\nUPD4: There are some cases when you need to track if a target node have been created in the document (usually it\'s necessary if you get Object required error while attempting to access the node by .getElementById, etc.):\nIf the page uses AJAX (loaded page source HTML doesn\'t contain target node, active content like JavaScript creates it dynamically), there is the example in the below snippet of a page, showing how that could look like. The text node 5.99 might be created after the page was completely loaded, and some other requests to a server for extra data to be displayed have taken a place:\n...\n<td class=""price-label"">\n    <span id=""priceblock"" class=""price-big color"">\n        5.99\n    </span>\n</td>\n...\n\nOr if you are loading e. g. Google search result page and waiting for Next button is appeared (especially, if you invoked .click method on the previous page), or loading some page with login web form and waiting for username input field like <input name=""userID"" id=""userID"" type=""text"" maxlength=""24"" required="""" placeholder=""Username"" autofocus="""">.\nThe below code allows to make an additional check if the target node is accessible:\nWith objIE\n    \' Navigating to some url\n    .Navigate strUrl\n    \' Wait for IE ready\n    Do While .ReadyState <> 4 Or .Busy\n        WScript.Sleep 10\n    Loop\n    \' Wait for document complete\n    Do While .Document.ReadyState <> ""complete""\n        WScript.Sleep 10\n    Loop\n    \' Wait for target node created\n    Do While TypeName(.Document.getElementById(""userID"")) = ""Null""\n        WScript.Sleep 10\n    Loop\n    \' Processing target node\n    .Document.getElementById(""userID"").Value = ""myusername""\n    \' ...\n    \'\nEnd With\n\n', '\nThe Following Check by Element Solved for me : \nFunction waitLoadByElement(p_ElementName)\n\nDo While IE.ReadyState <> 4 Or IE.Busy\n        WScript.Sleep 1000\n    Loop\n\n    Do While IE.Document.ReadyState <> ""complete""\n        WScript.Sleep 1000\n    Loop\n\n       \' This is the interesting part\n\n    Do While (instr(IE.document.getElementsByTagName(""body"")(0).InnerHTML,p_ElementName) < 1 )\n    v_counter = v_counter + 1\n\n        WScript.Sleep 1000\n    Loop\n    On Error GoTo 0\n\n    if v_counter > 0 then\n        MyEcho ""[ Waited Object to Load ] : "" & v_counter & "" - Seconds""\n    end if\n\nEnd Function\n\n']",https://stackoverflow.com/questions/23232488/detect-when-a-web-page-is-loaded-without-using-sleep,web-scraping
Problems submitting a login form with Jsoup,"
For some reason this code will not let me into the website when I use the correct login information. The System.out.println posts the code of the login page, indicating my code did not work. Can someone tell me what I'm forgetting or what's wrong with it?
public void connect() {

    try {
        Connection.Response loginForm = Jsoup.connect(""https://www.capitaliq.com/CIQDotNet/Login.aspx/login.php"")
                .method(Connection.Method.GET)
                .execute();

        org.jsoup.nodes.Document document = Jsoup.connect(""https://www.capitaliq.com/CIQDotNet/Login.aspx/authentication.php"")
                .data(""cookieexists"", ""false"")
                .data(""username"", ""myUsername"")
                .data(""password"", ""myPassword"")
                .cookies(loginForm.cookies())
                .post();
        System.out.println(document);
    } catch (IOException ex) {
        Logger.getLogger(WebCrawler.class.getName()).log(Level.SEVERE, null, ex);
    }
}

",2k,"
            1
        ","['\nBesides the username, password and the cookies, the site requeires two additional values for the login - VIEWSTATE and EVENTVALIDATION.\nYou can get them from the response of the first Get request, like this -  \nDocument doc = loginForm.parse();\nElement e = doc.select(""input[id=__VIEWSTATE]"").first();\nString viewState = e.attr(""value"");\ne = doc.select(""input[id=__EVENTVALIDATION]"").first();\nString eventValidation = e.attr(""value"");\n\nAnd add it after the password (the order doesn\'t really matter) -  \norg.jsoup.nodes.Document document = (org.jsoup.nodes.Document) Jsoup.connect(""https://www.capitaliq.com/CIQDotNet/Login.aspx/authentication.php"").userAgent(""Mozilla/5.0"")               \n            .data(""myLogin$myUsername"", ""MyUsername"")\n            .data(""myLogin$myPassword, ""MyPassword"")\n            .data(""myLogin$myLoginButton.x"", ""22"")                   \n            .data(""myLogin$myLoginButton.y"", ""8"")\n            .data(""__VIEWSTATE"", viewState)\n            .data(""__EVENTVALIDATION"", eventValidation)\n            .cookies(loginForm.cookies())\n            .post();\n\nI would also add the userAgent field to both requests - some sites test it and send different pages to different clients, so if you would like to get the same response as you get with your browser, add to the requests .userAgent(""Mozilla/5.0"") (or whatever browser you\'re using).\nEdit\nThe userName\'s field name is myLogin$myUsername, the password is myLogin$myPassword and the Post request also contains data about the login button. Ican\'t test it, because I don\'t have user at that site, but I believe it will work. Hope this solves your problem.  \nEDIT 2\nTo enable the remember me field during login, add this line to the post request:  \n.data(""myLogin$myEnableAutoLogin"", ""on"")\n\n']",https://stackoverflow.com/questions/31871801/problems-submitting-a-login-form-with-jsoup,web-scraping
How can I get the CSS Selector in Chrome?,"
I want to be able to select/highlight an element on the page and find its selector like this:

div.firstRow
  div.priceAvail>div>div.PriceCompare>div.BodyS

I know you can see the selection on the bottom after doing an inspect element, but how can I copy this path to the clipboard? In Firebug I think you can do this, but don't see a way to do this using the Chrome Developer Tools and search for an extension did not turn-up anything.
This is what I am trying to do for more reference:
http://asciicasts.com/episodes/173-screen-scraping-with-scrapi
",125k,"
            54
        ","['\n\nIf Chrome Dev tools if you select the element in the source pane and right click, then you will see the ""Copy CSS Path"" option.\nIn newer versions of Chrome, this is (right-click) > Copy > Copy selector.\n You can also get the XPath with (right-click) > Copy > Copy XPath\n', '\nAlthough not an extension, I did find a bookmarklet called Selector Gadget that does exactly what I was looking for.\n', '\nThe workflow I currently follow to get CSS selectors from elements with the latest Chrome version (59) is as follows:\n\nOpen Chrome Dev tools (cmd/ctrl + alt + j):\n\n\n\nClick on the select element tool in page (cmd/ctrl + alt + c):\n\n\n\nClick on the element you want to get the selector from in order to view it in the dev tools panel:\n\n\n\nRight click on the dev tools element:\n\n\n\nClick on Copy -> Copy selector:\n\n\nWhich gives me the following:\n#question > table > tbody > tr:nth-child(1) > td.postcell > div > div.post-text > blockquote > p\n', '\nDo ""Inspect Element"" or Ctrl+Shift+I, it\'s at the VERY bottom of the screen. You can also type in the ""Search Elements"" box at the top-right of the dev tools if not sure about the selector. \n', ""\nIt's sometime necessary to do this if you have a very complex app structure that you inherited and are trying to track down a very tricky multi nested css depth problem. Jquery mobile pre 1.3 would be a good example of this. Bootstrap apps etc..\nI tried the above tool but could not get that to actually select the entire parent and children of a complex inheritance similar to the original posters question.\n""]",https://stackoverflow.com/questions/4500572/how-can-i-get-the-css-selector-in-chrome,web-scraping
How do I call a Javascript function from Python?,"
I am working on a web-scraping project. One of the websites I am working with has the data coming from Javascript.
There was a suggestion on one of my earlier questions that I can directly call the Javascript from Python, but I'm not sure how to accomplish this.
For example: If a JavaScript function is defined as: add_2(var,var2)
How would I call that JavaScript function from Python?
",115k,"
            42
        ","['\nFind a JavaScript interpreter that has Python bindings. (Try Rhino? V8? SeaMonkey?). When you have found one, it should come with examples of how to use it from python.\nPython itself, however, does not include a JavaScript interpreter.\n', '\nTo interact with JavaScript from Python I use webkit, which is the browser renderer behind Chrome and Safari. There are Python bindings to webkit through Qt. In particular there is a function for executing JavaScript called evaluateJavaScript().\nHere is a full example to execute JavaScript and extract the final HTML.\n', ""\nAn interesting alternative I discovered recently is the Python bond module, which can be used to communicate with a NodeJs process (v8 engine).\nUsage would be very similar to the pyv8 bindings, but you can directly use any NodeJs library without modification, which is a major selling point for me.\nYour python code would look like this:\nval = js.call('add2', var1, var2)\n\nor even:\nadd2 = js.callable('add2')\nval = add2(var1, var2)\n\nCalling functions though is definitely slower than pyv8, so it greatly depends on your needs. If you need to use an npm package that does a lot of heavy-lifting, bond is great. You can even have more nodejs processes running in parallel.\nBut if you just need to call a bunch of JS functions (for instance, to have the same validation functions between the browser/backend), pyv8 will definitely be a lot faster.\n"", '\nYou can eventually get the JavaScript from the page and execute it through some interpreter (such as v8 or Rhino). However, you can get a good result in a way easier way by using some functional testing tools, such as Selenium or Splinter. These solutions launch a browser and effectively load the page - it can be slow but assures that the expected browser displayed content will be available.\nFor example, consider the HTML document below:\n<html>\n    <head>\n        <title>Test</title>\n        <script type=""text/javascript"">\n            function addContent(divId) {\n                var div = document.getElementById(divId);\n                div.innerHTML = \'<em>My content!</em>\';\n            }\n        </script>\n    </head>\n    <body>\n        <p>The element below will receive content</p>\n        <div id=""mydiv"" />\n        <script type=""text/javascript"">addContent(\'mydiv\')</script>\n    </body>\n</html>\n\nThe script below will use Splinter. Splinter will launch Firefox and after the complete load of the page it will get the content added to a div by JavaScript:\nfrom splinter.browser import Browser\nimport os.path\n\nbrowser = Browser()\nbrowser.visit(\'file://\' + os.path.realpath(\'test.html\'))\nelements = browser.find_by_css(""#mydiv"")\ndiv = elements[0]\nprint div.value\n\nbrowser.quit()\n\nThe result will be the content printed in the stdout.\n', ""\nYou might call node through Popen.\nMy example how to do it\nprint execute('''function (args) {\n    var result = 0;\n    args.map(function (i) {\n        result += i;\n    });\n    return result;\n}''', args=[[1, 2, 3, 4, 5]])\n\n"", '\nHi so one possible solution would be to use ajax with flask to comunicate between javascript and python. You would run a server with flask and then open the website in a browser. This way you could run javascript functions when the website is created via pythoncode or with a button how it is done in this example.\nHTML code:\n\n\n<html>\n<script src=""//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js""></script>\n\n\n<script>\n    function pycall() {\n      $.getJSON(\'/pycall\', {content: ""content from js""},function(data) {\n          alert(data.result);\n      });\n    }\n</script>\n\n\n<button type=""button"" onclick=""pycall()"">click me</button>\n \n\n</html>\n\nPython Code:\nfrom flask import Flask, jsonify, render_template, request\n\napp = Flask(__name__)\n\n\ndef load_file(file_name):\n    data = None\n    with open(file_name, \'r\') as file:\n        data = file.read()\n    return data\n\n@app.route(\'/pycall\')\ndef pycall():\n    content = request.args.get(\'content\', 0, type=str)\n    \n    print(""call_received"",content)\n    return jsonify(result=""data from python"")\n\n@app.route(\'/\')\ndef index():\n    return load_file(""basic.html"")\n\n\n\nimport webbrowser\nprint(""opening localhost"")\nurl = ""http://127.0.0.1:5000/""\nwebbrowser.open(url)\napp.run()\n\noutput in python:\ncall_received content from js\nalert in browser:\ndata from python\n', '\nThis worked for me for simple js file, source:\nhttps://www.geeksforgeeks.org/how-to-run-javascript-from-python/\npip install js2py\npip install temp\n\nfile.py\nimport js2py\neval_res, tempfile = js2py.run_file(""scripts/dev/test.js"")\ntempfile.wish(""GeeksforGeeks"")\n\nscripts/dev/test.js\nfunction wish(name) {\n    console.log(""Hello, "" + name + ""!"")\n}\n\n', '\nDid a whole run-down of the different methods recently.  \nPyQt4\nnode.js/zombie.js\nphantomjs\nPhantomjs was the winner hands down, very straightforward with lots of examples.\n']",https://stackoverflow.com/questions/8284765/how-do-i-call-a-javascript-function-from-python,web-scraping
Web scraping in PHP,"
I'm looking for a way to make a small preview of another page from a URL given by the user in PHP.
I'd like to retrieve only the title of the page, an image (like the logo of the website) and a bit of text or a description if it's available. Is there any simple way to do this without any external libraries/classes? Thanks
So far I've tried using the DOCDocument class, loading the HTML and displaying it on the screen, but I don't think that's the proper way to do it
",117k,"
            40
        ","['\nI recommend you consider simple_html_dom for this. It will make it very easy. \nHere is a working example of how to pull the title, and first image.\n<?php\nrequire \'simple_html_dom.php\';\n\n$html = file_get_html(\'http://www.google.com/\');\n$title = $html->find(\'title\', 0);\n$image = $html->find(\'img\', 0);\n\necho $title->plaintext.""<br>\\n"";\necho $image->src;\n?>\n\nHere is a second example that will do the same without an external library. I should note that using regex on HTML is NOT a good idea.\n<?php\n$data = file_get_contents(\'http://www.google.com/\');\n\npreg_match(\'/<title>([^<]+)<\\/title>/i\', $data, $matches);\n$title = $matches[1];\n\npreg_match(\'/<img[^>]*src=[\\\'""]([^\\\'""]+)[\\\'""][^>]*>/i\', $data, $matches);\n$img = $matches[1];\n\necho $title.""<br>\\n"";\necho $img;\n?>\n\n', ""\nYou may use either of these libraries. As you know each one has pros & cons, so you may consult notes about each one or take time & try it on your own:\n\nGuzzle: An Independent HTTP client, so no need to depend on cURL, SOAP or REST.\nGoutte: Built on Guzzle & some of Symfony components by Symfony developer.\nhQuery: A fast scraper with caching capabilities. high performance on scraping large docs.\nRequests: Famous for its user-friendly usage.\nBuzz: A lightweight client, ideal for beginners.\nReactPHP: Async scraper, with comprehensive tutorials & examples.\n\nYou'd better check them all & use everyone in its best intended occasion.\n"", '\nThis question is fairly old but still ranks very highly on Google Search results for web scraping tools in PHP.  Web scraping in PHP has advanced considerably in the intervening years since the question was asked.  I actively maintain the Ultimate Web Scraper Toolkit, which hasn\'t been mentioned yet but predates many of the other tools listed here except for Simple HTML DOM.\nThe toolkit includes TagFilter, which I actually prefer over other parsing options because it uses a state engine to process HTML with a continuous streaming tokenizer for precise data extraction.\nTo answer the original question of, ""Is there any simple way to do this without any external libraries/classes?""  The answer is no.  HTML is rather complex and there\'s nothing built into PHP that\'s particularly suitable for the task.  You really need a reusable library to parse generic HTML correctly and consistently.  Plus you\'ll find plenty of uses for such a library.\nAlso, a really good web scraper toolkit will have three major, highly-polished components/capabilities:\n\nData retrieval.  This is making a HTTP(S) request to a server and pulling down data.  A good web scraping library will also allow for large binary data blobs to be written directly to disk as they come down off the network instead of loading the whole thing into RAM.  The ability to do dynamic form extraction and submission is also very handy.  A really good library will let you fine-tune every aspect of each request to each server as well as look at the raw data it sent and received on the wire.  Some web servers are extremely picky about input, so being able to accurately replicate a browser is handy.\nData extraction.  This is finding pieces of content inside retrieved HTML and pulling it out, usually to store it into a database for future lookups.  A good web scraping library will also be able to correctly parse any semi-valid HTML thrown at it, including Microsoft Word HTML and ASP.NET output where odd things show up like a single HTML tag that spans several lines.  The ability to easily extract all the data from poorly designed, complex, classless tags like ASP.NET HTML table elements that some overpaid government employees made is also very nice to have (i.e. the extraction tool has more than just a DOM or CSS3-style selection engine available).  Also, in your case, the ability to early-terminate both the data retrieval and data extraction after reading in 50KB or as soon as you find what you are looking for is a plus, which could be useful if someone submits a URL to a 500MB file.\nData manipulation.  This is the inverse of #2.  A really good library will be able to modify the input HTML document several times without negatively impacting performance.  When would you want to do this?  Sanitizing user-submitted HTML, transforming content for a newsletter or sending other email, downloading content for offline viewing, or preparing content for transport to another service that\'s finicky about input (e.g. sending to Apple News or Amazon Alexa).  The ability to create a custom HTML-style template language is also a nice bonus.\n\nObviously, Ultimate Web Scraper Toolkit does all of the above...and more:\nI also like my toolkit because it comes with a WebSocket client class, which makes scraping WebSocket content easier.  I\'ve had to do that a couple of times.\nIt was also relatively simple to turn the clients on their heads and make WebServer and WebSocketServer classes.  You know you\'ve got a good library when you can turn the client into a server....but then I went and made PHP App Server with those classes.  I think it\'s becoming a monster!\n', '\nYou can use SimpleHtmlDom for this. and then look for the title and img tags or what ever else you need to do.\n', ""\nI like the Dom Crawler library. Very easy to use, has lots of options like:\n$crawler = $crawler\n->filter('body > p')\n->reduce(function (Crawler $node, $i) {\n    // filters every other node\n    return ($i % 2) == 0;\n});\n\n""]",https://stackoverflow.com/questions/9813273/web-scraping-in-php,web-scraping
Python selenium multiprocessing,"
I've written a script in python in combination with selenium to scrape the links of different posts from its landing page and finally get the title of each post by tracking the url leading to its inner page. Although the content I parsed here are static ones, I used selenium to see how it works in multiprocessing. 
However, my intention is to do the scraping using multiprocessing. So far I know that selenium doesn't support multiprocessing but it seems I was wrong.
My question: how can I reduce the execution time using selenium when it is made to run using multiprocessing?
This is my try (it's a working one):
import requests
from urllib.parse import urljoin
from multiprocessing.pool import ThreadPool
from bs4 import BeautifulSoup
from selenium import webdriver

def get_links(link):
  res = requests.get(link)
  soup = BeautifulSoup(res.text,""lxml"")
  titles = [urljoin(url,items.get(""href"")) for items in soup.select("".summary .question-hyperlink"")]
  return titles

def get_title(url):
  chromeOptions = webdriver.ChromeOptions()
  chromeOptions.add_argument(""--headless"")
  driver = webdriver.Chrome(chrome_options=chromeOptions)
  driver.get(url)
  sauce = BeautifulSoup(driver.page_source,""lxml"")
  item = sauce.select_one(""h1 a"").text
  print(item)

if __name__ == '__main__':
  url = ""https://stackoverflow.com/questions/tagged/web-scraping""
  ThreadPool(5).map(get_title,get_links(url))

",22k,"
            31
        ","['\n\nhow can I reduce the execution time using selenium when it is made to run using multiprocessing\n\nA lot of time in your solution is spent on launching the webdriver for each URL. You can reduce this time by launching the driver only once per thread:\n(... skipped for brevity ...)\n\nthreadLocal = threading.local()\n\ndef get_driver():\n  driver = getattr(threadLocal, \'driver\', None)\n  if driver is None:\n    chromeOptions = webdriver.ChromeOptions()\n    chromeOptions.add_argument(""--headless"")\n    driver = webdriver.Chrome(chrome_options=chromeOptions)\n    setattr(threadLocal, \'driver\', driver)\n  return driver\n\n\ndef get_title(url):\n  driver = get_driver()\n  driver.get(url)\n  (...)\n\n(...)\n\nOn my system this reduces the time from 1m7s to just 24.895s, a ~35% improvement. To test yourself, download the full script.\nNote: ThreadPool uses threads, which are constrained by the Python GIL. That\'s ok if for the most part the task is I/O bound. Depending on the post-processing you do with the scraped results, you may want to use a multiprocessing.Pool instead. This launches parallel processes which as a group are not constrained by the GIL. The rest of the code stays the same.\n', '\nThe one potential problem I see with the clever one-driver-per-thread answer is that it omits any mechanism for ""quitting"" the drivers and thus leaving the possibility of processes hanging around. I would make the following changes:\n\nUse instead class Driver that will crate the driver instance and store it on the thread-local storage but also have a destructor that will quit the driver when the thread-local storage is deleted:\n\nclass Driver:\n    def __init__(self):\n        options = webdriver.ChromeOptions()\n        options.add_argument(""--headless"")\n        self.driver = webdriver.Chrome(options=options)\n\n    def __del__(self):\n        self.driver.quit() # clean up driver when we are cleaned up\n        #print(\'The driver has been ""quitted"".\')\n\n\ncreate_driver now becomes:\n\nthreadLocal = threading.local()\n\ndef create_driver():\n    the_driver = getattr(threadLocal, \'the_driver\', None)\n    if the_driver is None:\n        the_driver = Driver()\n        setattr(threadLocal, \'the_driver\', the_driver)\n    return the_driver.driver\n\n\nFinally, after you have no further use for the ThreadPool instance but before it is terminated, add the following lines to delete the thread-local storage and force the Driver instances\' destructors to be called (hopefully):\n\ndel threadLocal\nimport gc\ngc.collect() # a little extra insurance\n\n', ""\n\nMy question: how can I reduce the execution time?\n\nSelenium seems the wrong tool for web scraping - though I appreciate YMMV, in particular if you need to simulate user interaction with the web site or there is some JavaScript limitation/requirement.\nFor scraping tasks without much interaction, I have had good results using the opensource Scrapy Python package for large-scale scrapying tasks. It does multiprocessing out of the box, it is easy to write new scripts and store the data in files or a database -- and it is really fast. \nYour script would look something like this when implemented as a fully parallel Scrapy spider (note I did not test this, see documentation on selectors).\nimport scrapy\nclass BlogSpider(scrapy.Spider):\n    name = 'blogspider'\n    start_urls = ['https://stackoverflow.com/questions/tagged/web-scraping']\n\n    def parse(self, response):\n        for title in response.css('.summary .question-hyperlink'):\n            yield title.get('href')\n\nTo run put this into blogspider.py and run \n$ scrapy runspider blogspider.py\n\nSee the Scrapy website for a complete tutorial.\nNote that Scrapy also supports JavaScript through scrapy-splash, thanks to the pointer by @SIM. I didn't have any exposure with that so far so can't speak to this other than it looks well integrated with how Scrapy works.\n""]",https://stackoverflow.com/questions/53475578/python-selenium-multiprocessing,web-scraping
Android: Using WebView outside an Activity context,"
I am trying to achieve Web Scraping through a background IntentService that periodically scrape a website without a view displaying on the users phone.  

Since I have to do call some javascript on the loaded page I cannot use any HttpGet's etc.
I therefore have to use a WebView instance which can only run on an UI thread.
Any attempts to start an Activity that use a WebView results in a View coming into the phones foreground (as per Android's design of Activities)
Any attempts to use a WebView outside of an Activity context resulted in error pointing to the fact that you cannot use WebView on a non-UI thread.
For various complexity reasons I cannot consider using libraries such as Rhino for UI-less web scraping.

Is there any way of working around this problem?
",29k,"
            29
        ","['\nYou can display a webview from a service. Code below creates a window which your service has access to. The window isn\'t visible because the size is 0 by 0.\npublic class ServiceWithWebView extends Service {\n\n    @Override\n    public void onCreate() {\n        super.onCreate();\n\n        WindowManager windowManager = (WindowManager) getSystemService(WINDOW_SERVICE);\n        params = new WindowManager.LayoutParams(WindowManager.LayoutParams.WRAP_CONTENT, WindowManager.LayoutParams.WRAP_CONTENT, WindowManager.LayoutParams.TYPE_SYSTEM_OVERLAY, WindowManager.LayoutParams.FLAG_NOT_TOUCHABLE, PixelFormat.TRANSLUCENT);\n        params.gravity = Gravity.TOP | Gravity.LEFT;\n        params.x = 0;\n        params.y = 0;\n        params.width = 0;\n        params.height = 0;\n\n        LinearLayout view = new LinearLayout(this);\n        view.setLayoutParams(new RelativeLayout.LayoutParams(RelativeLayout.LayoutParams.MATCH_PARENT, RelativeLayout.LayoutParams.MATCH_PARENT));\n\n        WebView wv = new WebView(this);\n        wv.setLayoutParams(new LinearLayout.LayoutParams(LinearLayout.LayoutParams.MATCH_PARENT, LinearLayout.LayoutParams.MATCH_PARENT));\n        view.addView(wv);\n        wv.loadUrl(""http://google.com"");\n\n        windowManager.addView(view, params);\n    }\n}\n\nAlso this will require the android.permission.SYSTEM_ALERT_WINDOW permission.\n', '\nCorrect me if I am wrong but the correct answer to this question is that there is NO possible way to use a WebView in the background while the user is doing other things on the phone without interrupting the user by means of an Activity.\nI have applied both Randy and Code_Yoga\'s suggestions:  Using an activity with ""Theme.NoDisplay"" to launch a background service with a WebView to do some work.  However even though no view is visible the switching to that activity for that second to start the services interrupts the user (ex. like pausing a running game that was being played).\nTotally disastrous news for my app so I am still hoping someone will give me a way to use a WebView that does not need an Activity (or a substitute for a WebView that can accomplish the same)\n', '\nYou can use this to hide the Activity \n         <activity android:name=""MyActivity""\n          android:label=""@string/app_name""\n          android:theme=""@android:style/Theme.NoDisplay"">\n\nDoing this will prevent the app from showing any Activity. \nAnd then you can do your stuff in the Activity.\n', '\nthe solution was like this, but with Looper.getMainLooper() :\nhttps://github.com/JonasCz/save-for-offline/blob/master/app/src/main/java/jonas/tool/saveForOffline/ScreenshotService.java\n@Override\npublic void onCreate() {\n    super.onCreate();\n    //HandlerThread thread = new HandlerThread(""ScreenshotService"", Process.THREAD_PRIORITY_BACKGROUND);\n    //thread.start();\n    //mServiceHandler = new ServiceHandler(thread.getLooper()); // not working\n    mServiceHandler = new ServiceHandler(Looper.getMainLooper()); // working\n}\n\nwith help of @JonasCz : https://stackoverflow.com/a/28234761/466363\n', ""\nI used the following code to get round this problem:\nHandler handler = new Handler(Looper.getMainLooper());\ntry\n{\n    handler.post(\n        new Runnable()\n        {\n            @Override\n            public void run()\n            {\n                ProcessRequest(); // Where this method runs the code you're needing\n            }\n        }\n    );\n} catch (Exception e)\n{\n    e.printStackTrace();\n}\n\n"", ""\nA WebView cannot exist outside of an Activity or Fragment due to it being a UI.\nHowever, this means that an Activity is only needed to create the WebView, not handle all its requests.\nIf you create the invisible WebView in your main activity and have it accessible from a static context, you should be able to perform tasks in the view in the background from anywhere, since I believe all of WebView's IO is done asynchronously. \nTo take away the ick of that global access, you could always launch a Service with a reference to the WebView to do the work you need.\n"", '\nor a substitute for a WebView that can accomplish the same <=== if you do not wish to show the loaded info on UI, maybe you can try to use HTTP to call the url directly, and process on the returned response from HTTP\n', ""\nWhy don't you create a Backend Service that does the scraping for you?\nAnd then you just poll results from a RESTful Webservice or even use a messaging middleware (e.g. ZeroMQ).\nMaybe more elegant if it fits your use case: let the Scraping Service send your App Push Messages via GCM :)\n"", ""\nI am not sure if this is a silver bullet to the given problem.\nAs per @Pierre's accepted answer (sounds correct to me)\n\nthere is NO possible way to use a WebView in the background while the\nuser is doing other things on the phone without interrupting the user\nby means of an Activity.\n\nThus, I believe there must be some architectural/flow/strategy changes that must be done in order to solve this problem.\nProposed Solution #1: Instead of getting a push notification from the server and run a background job and followed by running some JS code or WebView. Instead, Whenever user launch the application one should query the backend server to know whether there is any need to perform any scraping or not. And on the basis of backend input android client can run JS code or WebView and pass the result back to the server.\nI haven't tried this solution. But hope it is feasible.\n\nThis will also solve the following problem stated in the comments:\nReason for this is because the backend will get detected as a bot scraping from the same IP and get blocked (in addition to backend resources needed to do a lot of scraping on different pages).\nData might be unavailable for some time (until some user scrape it for you). But surely we can provide a better user experience to the end users using this strategy.\n"", ""\nI know it'a been a year and a half, but I'm now facing the same issue. I solved it eventually by running my Javascript code inside a Node engine that is running inside my Android App. It's called JXCore. You can take a look. Also, take a look at this sample that runs Javascript without a WebView. I really would like to know what did you end up using?\n""]",https://stackoverflow.com/questions/18865035/android-using-webview-outside-an-activity-context,web-scraping
Scraping ajax pages using python,"
I've already seen this question about scraping ajax, but python isn't mentioned there. I considered using scrapy, i believe they have some docs on that subject, but as you can see the website is down. So i don't know what to do. I want to do the following:
I only have one url, example.com you go from page to page by clicking submit, the url doesn't change since they're using ajax to display the content. I want to scrape the content of each page, how to do it? 
Lets say that i want to scrape only the numbers, is there anything other than scrapy that would do it? If not, would you give me a snippet on how to do it, just because their website is down so i can't reach the docs.
",47k,"
            18
        ","['\nFirst of all, scrapy docs are available at https://scrapy.readthedocs.org/en/latest/.\nSpeaking about handling ajax while web scraping. Basically, the idea is rather simple: \n\nopen browser developer tools, network tab\ngo to the target site\nclick submit button and see what XHR request is going to the server\nsimulate this XHR request in your spider\n\nAlso see:\n\nCan scrapy be used to scrape dynamic content from websites that are using AJAX?\nPagination using scrapy\n\nHope that helps.\n', '\nI found the answer very useful but I would like to make it more simple.\nresponse = requests.post(request_url, data=payload, headers=request_headers)\n\nrequest.post takes three parameters url, data and headers. Values for these three attributes can be found in the XHR request.\nCopy the whole request header and form data to load into the above variables and you are good to go\n']",https://stackoverflow.com/questions/16390257/scraping-ajax-pages-using-python,web-scraping
Android - Parse JS generated urls with JSOUP,"
im trying to parse url generated by Bootstrap`s Bootpage.js which looks like
https://example.com/#page-2
but JSOUP cant parse it and showing main url.
how to get normal link from Bootpage or how to make JSOUP to parse it.
Parsing code:
Jsoup.connect(""https://example.com/#page-2"").followRedirects(true).get();

",6k,"
            6
        ","['\n(See UPDATE below, first/accepted solution didn\'t met the android requirement, but is left for reference.) \n\nDesktop Solution\nHtmlUnit doesn\'t seem able to handle this site (often the case, lately). So I don\'t have a plain java solution either, but you could use PhantomJS: download the binary for your os, create a script file, start the process from within your java code and parse the output with a dom parser like jsoup.\nScript file (here called simple.js): \nvar page = require(\'webpage\').create();\nvar fs = require(\'fs\');\nvar system = require(\'system\');\n\nvar url = """";\nvar fileName = ""output"";\n// first parameter: url\n// second parameter: filename for output\nconsole.log(""args length: "" + system.args.length);\n\nif (system.args.length > 1) {\n    url=system.args[1];\n}\nif (system.args.length > 2){\n    fileName=system.args[2];\n}\nif(url===""""){\n    phantom.exit();\n}\n\npage.settings.userAgent = \'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.120 Safari/537.36\';\npage.settings.loadImages = false; \n\npage.open(url, function(status) {\n    console.log(""Status: "" + status);\n    if(status === ""success"") {\n        var path = fileName+\'.html\';\n        fs.write(path, page.content, \'w\');\n    }\n    phantom.exit();\n});\n\nJava code (example to get title and cover-url):\ntry {\n    //change path to phantomjs binary and your script file\n    String outputFileName = ""srulad"";\n    String phantomJSPath = ""phantomjs"" + File.separator + ""bin"" + File.separator + ""phantomjs"";\n    String scriptFile = ""simple.js"";\n\n    String urlParameter = ""http://srulad.com/#page-2"";\n\n    new File(outputFileName+"".html"").delete();\n\n    Process process = Runtime.getRuntime().exec(phantomJSPath + "" "" + scriptFile + "" "" + urlParameter + "" "" + outputFileName);\n    process.waitFor();\n\n    Document doc = Jsoup.parse(new File(outputFileName + "".html""),""UTF-8""); // output.html is created by phantom.js, same path as page.js\n    Elements elements = doc.select(""#list_page-2 > div"");\n\n    for (Element element : elements) {\n        System.out.println(element.select(""div.l-description.float-left > div:nth-child(1) > a"").first().attr(""title""));\n        System.out.println(element.select(""div.l-image.float-left > a > img.lazy"").first().attr(""data-original""));\n    }\n} catch (IOException | InterruptedException e) {\n    e.printStackTrace();\n}\n\nOutput:\nსიყვარული და მოწყალება / Love & Mercy\nhttp://srulad.com/assets/uploads/42410_Love_and_Mercy.jpg\nმუზა / The Muse\nhttp://srulad.com/assets/uploads/43164_large_qRzsimNz0eDyFLFJcbVLIxlqii.jpg\n...\n\n\nUPDATE\nParsing of websites with javascript based dynamic content in Android is possible using WebView and jsoup. \nThe following example app uses a javascript enabled WebView to render a Javascript dependent website. With a JavascriptInterface the html source is returned, parsed with jsoup and as a proof of concept the titles and the urls to the cover-images are used to populate a ListView. The buttons decrement or increment the page number triggering an update of the ListView. Note: tested on an Android 5.1.1/API 22 device.\nadd internet permission to your AndroidManifest.xml\n<uses-permission android:name=""android.permission.INTERNET"" />\n\nactivity_main.xml\n<?xml version=""1.0"" encoding=""utf-8""?>\n<LinearLayout xmlns:android=""http://schemas.android.com/apk/res/android""\n    android:orientation=""vertical""\n    android:layout_width=""match_parent""\n    android:layout_height=""match_parent"">\n\n    <LinearLayout\n        android:orientation=""horizontal""\n        android:layout_width=""match_parent""\n        android:layout_height=""wrap_content"">\n\n        <Button\n            android:layout_width=""wrap_content""\n            android:layout_height=""wrap_content""\n            android:text=""@string/page_down""\n            android:id=""@+id/buttonDown""\n            android:layout_weight=""0.5"" />\n\n        <Button\n            android:layout_width=""wrap_content""\n            android:layout_height=""wrap_content""\n            android:text=""@string/page_up""\n            android:id=""@+id/buttonUp""\n            android:layout_weight=""0.5"" />\n    </LinearLayout>\n\n    <ListView\n        android:layout_width=""match_parent""\n        android:layout_height=""0dp""\n        android:id=""@+id/listView""\n        android:layout_gravity=""bottom""\n        android:layout_weight=""0.5"" />\n</LinearLayout>\n\nMainActivity.java\npublic class MainActivity extends AppCompatActivity {\n\n    private final Handler uiHandler = new Handler();\n    private ArrayAdapter<String> adapter;\n    private ArrayList<String> entries = new ArrayList<>();\n    private ProgressDialog progressDialog;\n\n    private class JSHtmlInterface {\n        @android.webkit.JavascriptInterface\n        public void showHTML(String html) {\n            final String htmlContent = html;\n\n            uiHandler.post(\n                new Runnable() {\n                    @Override\n                    public void run() {\n                        Document doc = Jsoup.parse(htmlContent);\n                        Elements elements = doc.select(""#online_movies > div > div"");\n                        entries.clear();\n                        for (Element element : elements) {\n                            String title = element.select(""div.l-description.float-left > div:nth-child(1) > a"").first().attr(""title"");\n                            String imgUrl = element.select(""div.l-image.float-left > a > img.lazy"").first().attr(""data-original"");\n                            entries.add(title + ""\\n"" + imgUrl);\n                        }\n                        adapter.notifyDataSetChanged();\n                    }\n                }\n            );\n        }\n    }\n\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        ListView listView = (ListView) findViewById(R.id.listView);\n        adapter = new ArrayAdapter<>(this, android.R.layout.simple_list_item_1, android.R.id.text1, entries);\n        listView.setAdapter(adapter);\n\n        progressDialog = ProgressDialog.show(this, ""Loading"",""Please wait..."", true);\n        progressDialog.setCancelable(false);\n\n        try {\n            final WebView browser = new WebView(this);\n            browser.setVisibility(View.INVISIBLE);\n            browser.setLayerType(View.LAYER_TYPE_NONE,null);\n            browser.getSettings().setJavaScriptEnabled(true);\n            browser.getSettings().setBlockNetworkImage(true);\n            browser.getSettings().setDomStorageEnabled(false);\n            browser.getSettings().setCacheMode(WebSettings.LOAD_NO_CACHE);\n            browser.getSettings().setLoadsImagesAutomatically(false);\n            browser.getSettings().setGeolocationEnabled(false);\n            browser.getSettings().setSupportZoom(false);\n\n            browser.addJavascriptInterface(new JSHtmlInterface(), ""JSBridge"");\n\n            browser.setWebViewClient(\n                new WebViewClient() {\n\n                    @Override\n                    public void onPageStarted(WebView view, String url, Bitmap favicon) {\n                        progressDialog.show();\n                        super.onPageStarted(view, url, favicon);\n                    }\n\n                    @Override\n                    public void onPageFinished(WebView view, String url) {\n                        browser.loadUrl(""javascript:window.JSBridge.showHTML(\'<html>\'+document.getElementsByTagName(\'html\')[0].innerHTML+\'</html>\');"");\n                        progressDialog.dismiss();\n                    }\n                }\n            );\n\n            findViewById(R.id.buttonDown).setOnClickListener(new View.OnClickListener() {\n                @Override\n                public void onClick(View view) {\n                    uiHandler.post(new Runnable() {\n                        @Override\n                        public void run() {\n                            int page = Integer.parseInt(browser.getUrl().split(""-"")[1]);\n                            int newPage = page > 1 ? page-1 : 1;\n                            browser.loadUrl(""http://srulad.com/#page-"" + newPage);\n                            browser.loadUrl(browser.getUrl()); // not sure why this is needed, but doesn\'t update without it on my device\n                            if(getSupportActionBar()!=null) getSupportActionBar().setTitle(browser.getUrl());\n                        }\n                    });\n                }\n            });\n\n            findViewById(R.id.buttonUp).setOnClickListener(new View.OnClickListener() {\n                @Override\n                public void onClick(View view) {\n                    uiHandler.post(new Runnable() {\n                        @Override\n                        public void run() {\n                            int page = Integer.parseInt(browser.getUrl().split(""-"")[1]);\n                            int newPage = page+1;\n                            browser.loadUrl(""http://srulad.com/#page-"" + newPage);\n                            browser.loadUrl(browser.getUrl()); // not sure why this is needed, but doesn\'t update without it on my device\n                            if(getSupportActionBar()!=null) getSupportActionBar().setTitle(browser.getUrl());\n                        }\n                    });\n                }\n            });\n\n            browser.loadUrl(""http://srulad.com/#page-1"");\n            if(getSupportActionBar()!=null) getSupportActionBar().setTitle(browser.getUrl());\n\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n\n']",https://stackoverflow.com/questions/39140121/android-parse-js-generated-urls-with-jsoup,web-scraping
UnicodeEncodeError: 'ascii' codec can't encode character '\xe9' - -when using urlib.request python3,"
I'm writing a script that goes to a list of links and parses the information.
It works for most sites but It's choking on some with 
""UnicodeEncodeError: 'ascii' codec can't encode character '\xe9' in position 13: ordinal not in range(128)""
It stops on client.py which is part of urlib on python3
the exact link is:
http://finance.yahoo.com/news/cafés-growing-faster-than-fast-food-peers-144512056.html
There are quite a few similar postings here but none of the answers seems to work for me.
my code is:
from urllib import request

def __request(link,debug=0):      

try:
    html = request.urlopen(link, timeout=35).read() #made this long as I was getting lots of timeouts
    unicode_html = html.decode('utf-8','ignore')

# NOTE the except HTTPError must come first, otherwise except URLError will also catch an HTTPError.
except HTTPError as e:
    if debug:
        print('The server couldn\'t fulfill the request for ' + link)
        print('Error code: ', e.code)
    return ''
except URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('timeout')
        return ''    
else:
    return unicode_html

this calls the request function
link = 'http://finance.yahoo.com/news/cafés-growing-faster-than-fast-food-peers-144512056.html'
page = __request(link)
And the traceback is:
Traceback (most recent call last):
  File ""<string>"", line 250, in run_nodebug
  File ""C:\reader\get_news.py"", line 276, in <module>
    main()
  File ""C:\reader\get_news.py"", line 255, in main
    body = get_article_body(item['link'],debug=0)
  File ""C:\reader\get_news.py"", line 155, in get_article_body
    page = __request('na',url)
  File ""C:\reader\get_news.py"", line 50, in __request
    html = request.urlopen(link, timeout=35).read()
  File ""C:\Python33\Lib\urllib\request.py"", line 156, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Python33\Lib\urllib\request.py"", line 469, in open
    response = self._open(req, data)
  File ""C:\Python33\Lib\urllib\request.py"", line 487, in _open
    '_open', req)
  File ""C:\Python33\Lib\urllib\request.py"", line 447, in _call_chain
    result = func(*args)
  File ""C:\Python33\Lib\urllib\request.py"", line 1268, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File ""C:\Python33\Lib\urllib\request.py"", line 1248, in do_open
    h.request(req.get_method(), req.selector, req.data, headers)
  File ""C:\Python33\Lib\http\client.py"", line 1061, in request
    self._send_request(method, url, body, headers)
  File ""C:\Python33\Lib\http\client.py"", line 1089, in _send_request
    self.putrequest(method, url, **skips)
  File ""C:\Python33\Lib\http\client.py"", line 953, in putrequest
    self._output(request.encode('ascii'))
UnicodeEncodeError: 'ascii' codec can't encode character '\xe9' in position 13: ordinal not in range(128)

Any help appreciated It's driving me crazy , I think I've tried all combinations of x.decode    and similar 
(I could ignore the offending characters if that is possible.)
",4k,"
            5
        ","[""\nUse a percent-encoded URL:\nlink = 'http://finance.yahoo.com/news/caf%C3%A9s-growing-faster-than-fast-food-peers-144512056.html'\n\n\nI found the above percent-encoded URL by pointing the browser at \nhttp://finance.yahoo.com/news/cafés-growing-faster-than-fast-food-peers-144512056.html\n\ngoing to the page, then copying-and-pasting the \nencoded url supplied by the browser back into the text editor. However, you can generate a percent-encoded URL programmatically using:\nfrom urllib import parse\n\nlink = 'http://finance.yahoo.com/news/cafés-growing-faster-than-fast-food-peers-144512056.html'\n\nscheme, netloc, path, query, fragment = parse.urlsplit(link)\npath = parse.quote(path)\nlink = parse.urlunsplit((scheme, netloc, path, query, fragment))\n\nwhich yields\nhttp://finance.yahoo.com/news/caf%C3%A9s-growing-faster-than-fast-food-peers-144512056.html\n\n"", ""\nYour URL contains characters that cannot be represented as ASCII characters.\nYou'll have to ensure that all characters have been properly URL encoded; use urllib.parse.quote_plus for example; it'll use UTF-8 URL-encoded escaping to represent any non-ASCII characters.\n""]",https://stackoverflow.com/questions/22734464/unicodeencodeerror-ascii-codec-cant-encode-character-xe9-when-using-ur,web-scraping
IMPORTXML and right XPath from Bloomberg price [duplicate],"






This question already has answers here:
                        
                    



Scraping data to Google Sheets from a website that uses JavaScript

                                (2 answers)
                            

Closed last month.



I am trying to get price from mutual-fund from the Bloomberg website.
I have tried to use the ImportXML function in Google sheets, put in the Bloomberg link and copy the Full XPath but it always return with the #N/A.
This is my function:
=IMPORTXML(""https://www.bloomberg.com/quote/KAUGVAA:LX"",""/html/body/div[6]/div/div/section/section[1]/div/div[2]/section[1]/section/section/section/div[1]/span[1]"")

This is the Bloomberg link:
https://www.bloomberg.com/quote/KAUGVAA:LX?leadSource=uverify%20wall
Does anyone know what I am doing wrong?
",194,"
            0
        ","['\nif all you are getting is #N/A error you have 3 options before turning to a script\n\ndisable JavaScript. google sheets\' IMPORT formulae do not support the reading of JS content/elements. after you disable JS on your URL and the element you wish to scrape is not present there is 99.9% certainty you can give up! if the stuff you seek is still there move to point 2...\n\n\nrun an XML debugging formula to test what can be scrapped:\n=IMPORTXML(""URL""; ""//*"")\n\nif the result is #N/A give up and move to point 3...\n\nrun a sourcecode debugging formula to test what else can be scrapped:\n=IMPORTDATA(""URL"")\n\nif the output is #N/A give up and move to the next point. if the output is any other kind of error try:\n=QUERY(FLATTEN(IMPORTDATA(""URL"")); ""where Col1 is not null""; )\n\n\nat this stage open a google and try to find a different website that hosts the same data you want to get. then repeat steps 1-3. still no luck and your requirements are not that high? move to the next point...\n\ngo to google and search the URL. if there is a match try to check if there is a Cache:\n\ntake the URL and repeat steps 2-3. if this is not your thing or if luck left your life for good, move to point 6...\n=IMPORTXML(""https://webcache.googleusercontent.com/search?q=cache:aQET6JV0DywJ:https://www.bloomberg.com/quote/KAUGVAA:LX&cd=1&hl=en&ct=clnk"", \n ""//div[@class=\'overviewRow__66339412a5\']"")\n\n\n\ngive up or use a script\n\n\n']",https://stackoverflow.com/questions/74014518/importxml-and-right-xpath-from-bloomberg-price,web-scraping
How to scroll down with Phantomjs to load dynamic content,"
I am trying to scrape links from a page that generates content dynamically as the user scroll down to the bottom (infinite scrolling). I have tried doing different things with Phantomjs but not able to gather links beyond first page. Let say the element at the bottom which loads content has class .has-more-items. It is available until final content is loaded while scrolling and then becomes unavailable in DOM (display:none). Here are the things I have tried-

Setting viewportSize to a large height right after var page = require('webpage').create();


page.viewportSize = {             width: 1600,            height: 10000,
          };


Using page.scrollPosition = { top: 10000, left: 0 } inside page.open but have no effect like-


page.open('http://example.com/?q=houston', function(status) {
   if (status == ""success"") {
      page.scrollPosition = { top: 10000, left: 0 };  
   }
});



Also tried putting it inside page.evaluate function but that gives 


Reference error: Can't find variable page


Tried using jQuery and JS code inside page.evaluate and page.open but to no avail-


$(""html, body"").animate({ scrollTop: $(document).height() }, 10,
  function() {
          //console.log('check for execution');
      });

as it is and also inside document.ready. Similarly for JS code-
window.scrollBy(0,10000)

as it is and also inside window.onload
I am really struck on it for 2 days now and not able to find a way. Any help or hint would be appreciated.
Update
I have found a helpful piece of code at https://groups.google.com/forum/?fromgroups=#!topic/phantomjs/8LrWRW8ZrA0
var hitRockBottom = false; while (!hitRockBottom) {
    // Scroll the page (not sure if this is the best way to do so...)
    page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };

    // Check if we've hit the bottom
    hitRockBottom = page.evaluate(function() {
        return document.querySelector("".has-more-items"") === null;
    }); }

Where .has-more-items is the element class I want to access which is available at the bottom of the page initially and as we scroll down, it moves further down until all data is loaded and then becomes unavailable.
However, when I tested it is clear that it is running into infinite loops without scrolling down (I render pictures to check). I have tried to replace page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 }; with codes from below as well (one at a time)
window.document.body.scrollTop = '1000';
location.href = "".has-more-items"";
page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };
document.location.href="".has-more-items"";

But nothing seems to work.
",36k,"
            47
        ","['\nFound a way to do it and tried to adapt to your situation. I didn\'t test the best way of finding the bottom of the page because I had a different context, but check the solution below. The thing here is that you have to wait a little for the page to load and javascript works asynchronously so you have to use setInterval or setTimeout (see) to achieve this.\npage.open(\'http://example.com/?q=houston\', function () {\n\n  // Check for the bottom div and scroll down from time to time\n  window.setInterval(function() {\n      // Check if there is a div with class="".has-more-items"" \n      // (not sure if there\'s a better way of doing this)\n      var count = page.content.match(/class="".has-more-items""/g);\n\n      if(count === null) { // Didn\'t find\n        page.evaluate(function() {\n          // Scroll to the bottom of page\n          window.document.body.scrollTop = document.body.scrollHeight;\n        });\n      }\n      else { // Found\n        // Do what you want\n        ...\n        phantom.exit();\n      }\n  }, 500); // Number of milliseconds to wait between scrolls\n\n});\n\n', '\nI know that it has been answered a long time ago, but I also found a solution to my specific scenario. The result is a piece of javascript that scrolls to the bottom of the page. It is optimized to reduce waiting time.\nIt is not written for PhantomJS by default, so that will have to be modified. However, for a beginner or someone who doesn\'t have root access, an Iframe with injected javascript (run Google Chrome with --disable-javascript parameter) is a good alternative method for scraping a smaller set of ajax pages. The main benefit is that it\'s easily debuggable, because you have a visual overview of what\'s going on with your scraper.\nfunction ScrollForAjax () {\n\n    scrollintervals = 50;\n    scrollmaxtime = 1000;\n\n    if(typeof(scrolltime)==""undefined""){\n        scrolltime = 0;\n    }\n\n    scrolldocheight1 = $(iframeselector).contents().find(""body"").height();\n\n    $(""body"").scrollTop(scrolldocheight1);\n    setTimeout(function(){\n\n        scrolldocheight2 = $(""body"").height();\n\n        if(scrolltime===scrollmaxtime || scrolltime>scrollmaxtime){\n            scrolltime = 0;\n            $(""body"").scrollTop(0);\n            ScrapeCurrentPage(iframeselector);\n        }\n\n        else if(scrolldocheight2>scrolldocheight1){\n            scrolltime = 0;\n            ScrollForAjax (iframeselector);\n        }\n\n        else if(scrolldocheight1>=scrolldocheight2){\n            ScrollForAjax (iframeselector);\n        }\n\n    },scrollintervals);\n\n    scrolltime += scrollintervals;\n}\n\nscrollmaxtime is a timeout variable. Hope this is useful to someone :)\n', '\nThe ""correct"" solution didn\'t work for me. And, from what I\'ve read CasperJS doesn\'t use window (but I may be wrong on that), which makes me doubt that window works.\nThe following works for me in the Firefox/Chrome console; but, doesn\'t work in CasperJS (within casper.evaluate function).\n$(document).scrollTop($(document).height());\n\nWhat did work for me in CasperJS was:\ncasper.scrollToBottom();\ncasper.wait(1000, function waitCb() {\n  casper.capture(""loadedContent.png"");\n});\n\nWhich, also worked when moving casper.capture into Casper\'s then function.\nHowever, the above solution won\'t work on some sites like Twitter; jQuery seems to break the casper.scrollToBottom() function, and I had to remove the clientScripts reference to jQuery when working within Twitter.\nvar casper = require(\'casper\').create({\n    clientScripts: [\n       // \'jquery.js\'\n    ]\n});\n\nSome websites (e.g. BoingBoing.net) seem to work fine with jQuery and CasperJS scrollToBottom(). Not sure why some sites work and others don\'t.\n', ""\nThe code snippet below work just fine for pinterest. I researched a lot to scrape pinterest without phantomjs but it is impossible to find the infinite scroll trigger link. I think the code below will help other infinite scroll web page to scrape.\npage.open(pageUrl).then(function (status) {\n    var count = 0;\n    // Scrolls to the bottom of page\n    function scroll2btm() {\n        if (count < 500) {\n            page.evaluate(function(limit) {\n                window.scrollTo(0, document.body.scrollHeight || document.documentElement.scrollHeight);\n                return document.getElementsByClassName('pinWrapper').length; // use desired contents (eg. pin) selector for count presence number\n            }).then(function(c) {\n                count = c;\n                console.log(count); // print no of content found to check\n            });\n            setTimeout(scroll2btm,3000);\n        } else {\n            // required number of item found\n        }\n    }\n    scroll2btm();\n});\n\n""]",https://stackoverflow.com/questions/16561582/how-to-scroll-down-with-phantomjs-to-load-dynamic-content,web-scraping
Scraping a dynamic ecommerce page with infinite scroll,"
I'm using rvest in R to do some scraping. I know some HTML and CSS.
I want to get the prices of every product of a URI:
http://www.linio.com.co/tecnologia/celulares-telefonia-gps/
The new items load as you go down on the page (as you do some scrolling).
What I've done so far:
Linio_Celulares <- html(""http://www.linio.com.co/celulares-telefonia-gps/"")

Linio_Celulares %>%
  html_nodes("".product-itm-price-new"") %>%
  html_text()

And i get what i need, but just for the 25 first elements (those load for default). 
 [1] ""$ 1.999.900"" ""$ 1.999.900"" ""$ 1.999.900"" ""$ 2.299.900"" ""$ 2.279.900""
 [6] ""$ 2.279.900"" ""$ 1.159.900"" ""$ 1.749.900"" ""$ 1.879.900"" ""$ 189.900""  
[11] ""$ 2.299.900"" ""$ 2.499.900"" ""$ 2.499.900"" ""$ 2.799.000"" ""$ 529.900""  
[16] ""$ 2.699.900"" ""$ 2.149.900"" ""$ 189.900""   ""$ 2.549.900"" ""$ 1.395.900""
[21] ""$ 249.900""   ""$ 41.900""    ""$ 319.900""   ""$ 149.900"" 

Question: How to get all the elements of this dynamic section? 
I guess, I could scroll the page until all elements are loaded and then use html(URL). But this seems like a lot of work (i'm planning of doing this on different sections). There should be a programmatic work around.
",13k,"
            22
        ","['\nAs @nrussell suggested, you can use RSelenium to programatically scroll down the page before getting the source code.\nYou could for example do:\nlibrary(RSelenium)\nlibrary(rvest)\n#start RSelenium\ncheckForServer()\nstartServer()\nremDr <- remoteDriver()\nremDr$open()\n\n#navigate to your page\nremDr$navigate(""http://www.linio.com.co/tecnologia/celulares-telefonia-gps/"")\n\n#scroll down 5 times, waiting for the page to load at each time\nfor(i in 1:5){      \nremDr$executeScript(paste(""scroll(0,"",i*10000,"");""))\nSys.sleep(3)    \n}\n\n#get the page html\npage_source<-remDr$getPageSource()\n\n#parse it\nhtml(page_source[[1]]) %>% html_nodes("".product-itm-price-new"") %>%\n  html_text()\n\n', '\nlibrary(rvest)\nurl<-""https://www.linio.com.co/c/celulares-y-tablets?page=1""\npage<-html_session(url)\n\nhtml_nodes(page,css="".price-secondary"") %>% html_text()\n\nLoop through the website https://www.linio.com.co/c/celulares-y-tablets?page=2 and 3 and so on and it will be easy for you to scrape the data\nEDIT dated 07/05/2019\nThe website elements changed. Hence new code\nlibrary(rvest)\nurl<-""https://www.linio.com.co/c/celulares-y-tablets?page=1""\npage<-html_session(url)\n\nhtml_nodes(page,css="".price-main"") %>% html_text()\n\n']",https://stackoverflow.com/questions/29861117/scraping-a-dynamic-ecommerce-page-with-infinite-scroll,web-scraping
Scrape password-protected website in R,"
I'm trying to scrape data from a password-protected website in R.  Reading around, it seems that the httr and RCurl packages are the best options for scraping with password authentication (I've also looked into the XML package).
The website I'm trying to scrape is below (you need a free account in order to access the full page):
http://subscribers.footballguys.com/myfbg/myviewprojections.php?projector=2
Here are my two attempts (replacing ""username"" with my username and ""password"" with my password):
#This returns ""Status: 200"" without the data from the page:
library(httr)
GET(""http://subscribers.footballguys.com/myfbg/myviewprojections.php?projector=2"", authenticate(""username"", ""password""))

#This returns the non-password protected preview (i.e., not the full page):
library(XML)
library(RCurl)
readHTMLTable(getURL(""http://subscribers.footballguys.com/myfbg/myviewprojections.php?projector=2"", userpwd = ""username:password""))

I have looked at other relevant posts (links below), but can't figure out how to apply their answers to my case.
How to use R to download a zipped file from a SSL page that requires cookies
How to webscrape secured pages in R (https links) (using readHTMLTable from XML package)?
Reading information from a password protected site
R - RCurl scrape data from a password-protected site
http://www.inside-r.org/questions/how-scrape-data-password-protected-https-website-using-r-hold
",27k,"
            18
        ","['\nYou can use RSelenium. I have used the dev version as you can run phantomjs without a Selenium Server. \n# Install RSelenium if required. You will need phantomjs in your path or follow instructions\n# in package vignettes\n# devtools::install_github(""ropensci/RSelenium"")\n# login first\nappURL <- \'http://subscribers.footballguys.com/amember/login.php\'\nlibrary(RSelenium)\npJS <- phantom() # start phantomjs\nremDr <- remoteDriver(browserName = ""phantomjs"")\nremDr$open()\nremDr$navigate(appURL)\nremDr$findElement(""id"", ""login"")$sendKeysToElement(list(""myusername""))\nremDr$findElement(""id"", ""pass"")$sendKeysToElement(list(""mypass""))\nremDr$findElement(""css"", "".am-login-form input[type=\'submit\']"")$clickElement()\n\nappURL <- \'http://subscribers.footballguys.com/myfbg/myviewprojections.php?projector=2\'\nremDr$navigate(appURL)\ntableElem<- remDr$findElement(""css"", ""table.datamedium"")\nres <- readHTMLTable(header = TRUE, tableElem$getElementAttribute(""outerHTML"")[[1]])\n> res[[1]][1:5, ]\nRank             Name Tm/Bye Age Exp Cmp Att  Cm%  PYd Y/Att PTD Int Rsh  Yd TD FantPt\n1    1   Peyton Manning  DEN/4  38  17 415 620 66.9 4929  7.95  43  12  24   7  0 407.15\n2    2       Drew Brees   NO/6  35  14 404 615 65.7 4859  7.90  37  16  22  44  1 385.35\n3    3    Aaron Rodgers   GB/9  31  10 364 560 65.0 4446  7.94  33  13  52 224  3 381.70\n4    4      Andrew Luck IND/10  25   3 366 610 60.0 4423  7.25  27  13  62 338  2 361.95\n5    5 Matthew Stafford  DET/9  26   6 377 643 58.6 4668  7.26  32  19  34 102  1 358.60\n\nFinally when you are finished close phantomjs\npJS$stop()\n\nIf you want to use a traditional browser like firefox for example (if you wanted to stick to the version on CRAN) you would use:\nRSelenium::startServer()\nremDr <- remoteDriver()\n........\n........\nremDr$closeServer()\n\nin place of the related phantomjs calls.\n', '\nI don\'t have an account to test with, but maybe this will work:\nlibrary(httr)\nlibrary(XML)\n\nhandle <- handle(""http://subscribers.footballguys.com"") \npath   <- ""amember/login.php""\n\n# fields found in the login form.\nlogin <- list(\n  amember_login = ""username""\n ,amember_pass  = ""password""\n ,amember_redirect_url = \n   ""http://subscribers.footballguys.com/myfbg/myviewprojections.php?projector=2""\n)\n\nresponse <- POST(handle = handle, path = path, body = login)\n\nNow, the response object might hold what you need (or maybe you can directly query the page of interest after the login request; I am not sure the redirect will work, but it is a field in the web form), and handle might be re-used for subsequent requests. Can\'t test it; but this works for me in many situations.\nYou can output the table using XML\n> readHTMLTable(content(response))[[1]][1:5,]\n  Rank             Name Tm/Bye Age Exp Cmp Att  Cm%  PYd Y/Att PTD Int Rsh  Yd TD FantPt\n1    1   Peyton Manning  DEN/4  38  17 415 620 66.9 4929  7.95  43  12  24   7  0 407.15\n2    2       Drew Brees   NO/6  35  14 404 615 65.7 4859  7.90  37  16  22  44  1 385.35\n3    3    Aaron Rodgers   GB/9  31  10 364 560 65.0 4446  7.94  33  13  52 224  3 381.70\n4    4      Andrew Luck IND/10  25   3 366 610 60.0 4423  7.25  27  13  62 338  2 361.95\n5    5 Matthew Stafford  DET/9  26   6 377 643 58.6 4668  7.26  32  19  34 102  1 358.60\n\n']",https://stackoverflow.com/questions/24723606/scrape-password-protected-website-in-r,web-scraping
pandas read_html ValueError: No tables found,"
I am trying to scrap the historical weather data from the ""https://www.wunderground.com/personal-weather-station/dashboard?ID=KMAHADLE7#history/tdata/s20170201/e20170201/mcustom.html"" weather underground page. I have the following code: 
import pandas as pd 

page_link = 'https://www.wunderground.com/personal-weather-station/dashboard?ID=KMAHADLE7#history/tdata/s20170201/e20170201/mcustom.html'
df = pd.read_html(page_link)
print(df)

I have the following response: 
Traceback (most recent call last):
 File ""weather_station_scrapping.py"", line 11, in <module>
  result = pd.read_html(page_link)
 File ""/anaconda3/lib/python3.6/site-packages/pandas/io/html.py"", line 987, in read_html
  displayed_only=displayed_only)
 File ""/anaconda3/lib/python3.6/site-packages/pandas/io/html.py"", line 815, in _parse raise_with_traceback(retained)
 File ""/anaconda3/lib/python3.6/site-packages/pandas/compat/__init__.py"", line 403, in raise_with_traceback
  raise exc.with_traceback(traceback)
ValueError: No tables found

Although, this page clearly has a table but it is not being picked by the read_html. I have tried using Selenium so that the page can be loaded before I read it. 
from selenium import webdriver
from selenium.webdriver.common.keys import Keys

driver = webdriver.Firefox()
driver.get(""https://www.wunderground.com/personal-weather-station/dashboard?ID=KMAHADLE7#history/tdata/s20170201/e20170201/mcustom.html"")
elem = driver.find_element_by_id(""history_table"")

head = elem.find_element_by_tag_name('thead')
body = elem.find_element_by_tag_name('tbody')

list_rows = []

for items in body.find_element_by_tag_name('tr'):
    list_cells = []
    for item in items.find_elements_by_tag_name('td'):
        list_cells.append(item.text)
    list_rows.append(list_cells)
driver.close()

Now, the problem is that it cannot find ""tr"". I would appreciate any suggestions. 
",42k,"
            12
        ","['\nHere\'s a solution using selenium for browser automation\nfrom selenium import webdriver\nimport pandas as pd\ndriver = webdriver.Chrome(chromedriver)\ndriver.implicitly_wait(30)\n\ndriver.get(\'https://www.wunderground.com/personal-weather-station/dashboard?ID=KMAHADLE7#history/tdata/s20170201/e20170201/mcustom.html\')\n    df=pd.read_html(driver.find_element_by_id(""history_table"").get_attribute(\'outerHTML\'))[0]\n\nTime    Temperature Dew Point   Humidity    Wind    Speed   Gust    Pressure  Precip. Rate. Precip. Accum.  UV  Solar\n0   12:02 AM    25.5 °C 18.7 °C 75 %    East    0 kph   0 kph   29.3 hPa    0 mm    0 mm    0   0 w/m²\n1   12:07 AM    25.5 °C 19 °C   76 %    East    0 kph   0 kph   29.31 hPa   0 mm    0 mm    0   0 w/m²\n2   12:12 AM    25.5 °C 19 °C   76 %    East    0 kph   0 kph   29.31 hPa   0 mm    0 mm    0   0 w/m²\n3   12:17 AM    25.5 °C 18.7 °C 75 %    East    0 kph   0 kph   29.3 hPa    0 mm    0 mm    0   0 w/m²\n4   12:22 AM    25.5 °C 18.7 °C 75 %    East    0 kph   0 kph   29.3 hPa    0 mm    0 mm    0   0 w/m²\n\nEditing with breakdown of exactly what\'s happening, since the above one-liner is actually not very good self-documenting code:\nAfter setting up the driver, we select the table with its ID value (Thankfully this site actually uses reasonable and descriptive IDs)\ntab=driver.find_element_by_id(""history_table"")\n\nThen, from that element, we get the HTML instead of the web driver element object\ntab_html=tab.get_attribute(\'outerHTML\')\n\nWe use pandas to parse the html\ntab_dfs=pd.read_html(tab_html)\n\nFrom the docs:\n\n""read_html returns a list of DataFrame objects, even if there is only\n  a single table contained in the HTML content""\n\nSo we index into that list with the only table we have, at index zero\ndf=tab_dfs[0]\n\n', '\nYou can use requests and avoid opening browser. \nYou can get current conditions by using:\nhttps://stationdata.wunderground.com/cgi-bin/stationlookup?station=KMAHADLE7&units=both&v=2.0&format=json&callback=jQuery1720724027235122559_1542743885014&_=15\nand strip of \'jQuery1720724027235122559_1542743885014(\' from the left and \')\' from the right. Then handle the json string.\nYou can get summary and history by calling the API with the following\nhttps://api-ak.wunderground.com/api/606f3f6977348613/history_20170201null/units:both/v:2.0/q/pws:KMAHADLE7.json?callback=jQuery1720724027235122559_1542743885015&_=1542743886276\nYou then need to strip \'jQuery1720724027235122559_1542743885015(\' from the front and \');\' from the right. You then have a JSON string you can parse. \nSample of JSON:\n\nYou can find these URLs by using F12 dev tools in browser and inspecting the network tab for the traffic created during page load.\nAn example for current, noting there seems to be a problem with nulls in the JSON so I am replacing with ""placeholder"":\nimport requests\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\nfrom bs4 import BeautifulSoup\n\nurl = \'https://stationdata.wunderground.com/cgi-bin/stationlookup?station=KMAHADLE7&units=both&v=2.0&format=json&callback=jQuery1720724027235122559_1542743885014&_=15\'\nres = requests.get(url)\nsoup = BeautifulSoup(res.content, ""lxml"")\ns = soup.select(\'html\')[0].text.strip(\'jQuery1720724027235122559_1542743885014(\').strip(\')\')\ns = s.replace(\'null\',\'""placeholder""\')\ndata= json.loads(s)\ndata = json_normalize(data)\ndf = pd.DataFrame(data)\nprint(df)\n\n']",https://stackoverflow.com/questions/53398785/pandas-read-html-valueerror-no-tables-found,web-scraping
Beautiful Soup 4 find_all don't find links that Beautiful Soup 3 finds,"
I noticed a really annoying bug: BeautifulSoup4 (package: bs4) often finds less tags than the previous version (package: BeautifulSoup).
Here's a reproductible instance of that issue:
import requests
import bs4
import BeautifulSoup

r = requests.get('http://wordpress.org/download/release-archive/')
s4 = bs4.BeautifulSoup(r.text)
s3 = BeautifulSoup.BeautifulSoup(r.text)

print 'With BeautifulSoup 4 : {}'.format(len(s4.findAll('a')))
print 'With BeautifulSoup 3 : {}'.format(len(s3.findAll('a')))

Output:
With BeautifulSoup 4 : 557
With BeautifulSoup 3 : 1701

The difference is not minor as you can see.
Here are the exact versions of the modules in case someone is wondering:
In [20]: bs4.__version__
Out[20]: '4.2.1'

In [21]: BeautifulSoup.__version__
Out[21]: '3.2.1'

",4k,"
            0
        ","[""\nYou have lxml installed, which means that BeautifulSoup 4 will use that parser over the standard-library html.parser option.\nYou can upgrade lxml to 3.2.1 (which for me returns 1701 results for your test page); lxml itself uses libxml2 and libxslt which may be to blame too here. You may have to upgrade those instead / as well. See the lxml requirements page; currently libxml2 2.7.8 or newer is recommended.\nOr explicitly specify the other parser when parsing the soup:\ns4 = bs4.BeautifulSoup(r.text, 'html.parser')\n\n""]",https://stackoverflow.com/questions/17698836/beautiful-soup-4-find-all-dont-find-links-that-beautiful-soup-3-finds,web-scraping
Android Web Scraping with a Headless Browser [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I have spent a day on researching a library that can be used to accomplish the following:

Retrieve the full contents of a webpage like in the background without rendering result to a view.
The lib should support pages that fires off ajax requests to load some additional result data after the initial HTML has loaded for example.
From the resulting html I need to grab elements in xpath or css selector form.
In future I also possibly need to navigate to a next page (fire off events, submitting buttons/links etc)

Here is what I have tried without success:

Jsoup: Works great but no support for javascript/ajax (so it does not load full page)
Android built in HttpEntity: same problem with javascript/ajax as jsoup
HtmlUnit: Looks exactly what I need but after hours cannot get it to work on Android (Other users failed by trying to load the 12MB+ worth of jar files.  I myself loaded the full source code and referenced it as a project library only to find that things such as Applets and java.awt (used by HtmlUnit) does not exist in Android).
Rhino - I find this very confusing and don't know how to get it working in Android and even if it is what I am looking for.
Selenium Driver: Looks like it can work but you don't have an straightforward way to implement it in a headless way so that you don't have the actual html displayed to a view.

I really want HtmlUnit to work as it seems the best suited for my solution.  Is there any way or at least another library I have missed that is suitable for my needs?
I am currently using Android Studio 0.1.7 and can move to Ellipse if needed.
Thanks in advance!
",21k,"
            35
        ","['\nOk after 2 weeks I admit defeat and are using a workaround which works great for me at the moment.\nThe problem: \nIt is too difficult to port HTMLUnit to Android (or at least with my level of expertise).  I am sure its a worthwhile project (and not that time consuming for experienced java programmer) . I emailed the guys at HTMLUnit and they commented that they are not looking into a port or what effort will be involved but suggested anyone who wants to start with such a project should send an message to their mailing list to get more developers involved (http://htmlunit.sourceforge.net/mail-lists.html).\nThe workaround: \nI used android\'s built in WebView and overrided the onPageFinished method of Webview class to inject Javascript that grabs all the html after the page has fully loaded. Webview can also be used to called futher javascript actions, clicking buttons, filling in forms etc.\nCode: \nwebView.getSettings().setJavaScriptEnabled(true);\nMyJavaScriptInterface jInterface = new MyJavaScriptInterface();\nwebView.addJavascriptInterface(jInterface, ""HtmlViewer"");\n\nwebView.setWebViewClient(new WebViewClient() {\n\n    @Override\n    public void onPageFinished(WebView view, String url) {\n       //Load HTML\n       webView.loadUrl(""javascript:window.HtmlViewer.showHTML(\'<html>\'+document.getElementsByTagName(\'html\')[0].innerHTML+\'</html>\');"");\n    }\n\n}\n\nwebView.loadUrl(StartURL);\nParseHtml(jInterface.html);   \n\npublic class MyJavaScriptInterface {\n\n    public String html;\n\n    @JavascriptInterface\n    public void showHTML(String _html) {\n        html = _html;\n    }\n}\n\n', ""\nI have taken the implementation mentioned above (injecting JavaScript) and that works for me. All I do is simply set the visibility of the webview to be hidden under other UI elements. I was also thinking of doing the same with selenium. I have used selenium with Chrome in Python and it's great but like you mentioned it is not easy to not show the browser window. But I think it might be possible to just not show the component in Android. I'll have to try.\n""]",https://stackoverflow.com/questions/17399055/android-web-scraping-with-a-headless-browser,web-scraping
Jsoup Cookies for HTTPS scraping,"
I am experimenting with this site to gather my username on the welcome page to learn Jsoup and Android.  Using the following code
Connection.Response res = Jsoup.connect(""http://www.mikeportnoy.com/forum/login.aspx"")
    .data(""ctl00$ContentPlaceHolder1$ctl00$Login1$UserName"", ""username"", ""ctl00$ContentPlaceHolder1$ctl00$Login1$Password"", ""password"")
    .method(Method.POST)
    .execute();
String sessionId = res.cookie("".ASPXAUTH"");

Document doc2 = Jsoup.connect(""http://www.mikeportnoy.com/forum/default.aspx"")
.cookie("".ASPXAUTH"", sessionId)
.get();

My cookie (.ASPXAUTH) always ends up NULL.  If I delete this cookie in a webbrowser, I lose my connection.  So I am sure it is the correct cookie.  In addition, if I change the code
.cookie("".ASPXAUTH"", ""jkaldfjjfasldjf"")  Using the correct values of course

I am able to scrape my login name from this page.  This also makes me think I have the correct cookie.  So, how come my cookie comes up Null?  Are my username and password name fields incorrect?  Something else?  
Thanks.
",36k,"
            28
        ","['\nI know I\'m kinda late by 10 months here. But a good option using Jsoup is to use this easy peasy piece of code:\n//This will get you the response.\nResponse res = Jsoup\n    .connect(""url"")\n    .data(""loginField"", ""login@login.com"", ""passField"", ""pass1234"")\n    .method(Method.POST)\n    .execute();\n\n//This will get you cookies\nMap<String, String> cookies = res.cookies();\n\n//And this is the easieste way I\'ve found to remain in session\nDocumente doc = Jsoup.connect(""url"").cookies(cookies).get();\n\nThough I\'m still having trouble connection to SOME websites, I connect to a whole lot of them with the same basic piece of code. Oh, and before I forget.. What I figured my problem is, is SSL certificates. You have to properly manage them in a way I still haven\'t quite figured out. \n', '\nI always do this in two steps (like normal human),\n\nRead login page (by GET, read cookies)\nSubmit form and cookies (by POST, without cookie manipulation)\n\nExample:\nConnection.Response response = Jsoup.connect(""http://www.mikeportnoy.com/forum/login.aspx"")\n        .method(Connection.Method.GET)\n        .execute();\n\nresponse = Jsoup.connect(""http://www.mikeportnoy.com/forum/login.aspx"")\n        .data(""ctl00$ContentPlaceHolder1$ctl00$Login1$UserName"", ""username"")\n        .data(""ctl00$ContentPlaceHolder1$ctl00$Login1$Password"", ""password"")\n        .cookies(response.cookies())\n        .method(Connection.Method.POST)\n        .execute();\n\nDocument homePage = Jsoup.connect(""http://www.mikeportnoy.com/forum/default.aspx"")\n        .cookies(response.cookies())\n        .get();\n\nAnd always set cookies from previuos request to next using\n         .cookies(response.cookies())\n\nSSL is not important here. If you have problem with certifcates then execute this method for ignore SSL.\npublic static void trustEveryone() {\n    try {\n        HttpsURLConnection.setDefaultHostnameVerifier(new HostnameVerifier() {\n            public boolean verify(String hostname, SSLSession session) {\n                return true;\n            }\n        });\n\n        SSLContext context = SSLContext.getInstance(""TLS"");\n        context.init(null, new X509TrustManager[]{new X509TrustManager() {\n            public void checkClientTrusted(X509Certificate[] chain, String authType) throws CertificateException { }\n\n            public void checkServerTrusted(X509Certificate[] chain, String authType) throws CertificateException { }\n\n            public X509Certificate[] getAcceptedIssuers() {\n                return new X509Certificate[0];\n            }\n        }}, new SecureRandom());\n        HttpsURLConnection.setDefaultSSLSocketFactory(context.getSocketFactory());\n    } catch (Exception e) { // should never happen\n        e.printStackTrace();\n    }\n}\n\n', '\nWhat if you try fetching and passing all cookies without assuming anything like this: Sending POST request with username and password and save session cookie\nIf you still have problems try looking in to this: Issues with passing cookies to GET request (after POST)\n']",https://stackoverflow.com/questions/7139178/jsoup-cookies-for-https-scraping,web-scraping
Scraping webpage generated by JavaScript with C#,"
I have a web browser, and a label in Visual Studio, and basically what I'm trying to do is grab a section from another webpage.
I tried using WebClient.DownloadString and WebClient.DownloadFile, and both of them give me the source code of the web page before the JavaScript loads the content.  My next idea was to use a web browser tool and just call webBrowser.DocumentText after the page loaded and that did not work, it still gives me the original source of the page.
Is there a way I can grab the page post JavaScript load?
",31k,"
            23
        ","['\nThe problem is the browser usually executes the javascript and it results with an updated DOM. Unless you can analyze the javascript or intercept the data it uses, you will need to execute the code as a browser would. In the past I ran into the same issue, I utilized selenium and PhantomJS to render the page. After it renders the page, I would use the WebDriver client to navigate the DOM and retrieve the content I needed, post AJAX. \nAt a high-level, these are the steps:\n\nInstalled selenium: http://docs.seleniumhq.org/\nStarted the selenium hub as a service\nDownloaded phantomjs (a headless browser, that can execute the javascript): http://phantomjs.org/\nStarted phantomjs in webdriver mode pointing to the selenium hub\nIn my scraping application installed the webdriver client nuget package: Install-Package Selenium.WebDriver\n\nHere is an example usage of the phantomjs webdriver:\nvar options = new PhantomJSOptions();\noptions.AddAdditionalCapability(""IsJavaScriptEnabled"",true);\n\nvar driver = new RemoteWebDriver( new URI(Configuration.SeleniumServerHub),\n                    options.ToCapabilities(),\n                    TimeSpan.FromSeconds(3)\n                  );\ndriver.Url = ""http://www.regulations.gov/#!documentDetail;D=APHIS-2013-0013-0083"";\ndriver.Navigate();\n//the driver can now provide you with what you need (it will execute the script)\n//get the source of the page\nvar source = driver.PageSource;\n//fully navigate the dom\nvar pathElement = driver.FindElementById(""some-id"");\n\nMore info on selenium, phantomjs and webdriver can be found at the following links:\nhttp://docs.seleniumhq.org/\nhttp://docs.seleniumhq.org/projects/webdriver/\nhttp://phantomjs.org/\nEDIT: Easier Method \nIt appears there is a nuget package for the phantomjs, such that you don\'t need the hub (I used a cluster to do massive scrapping in this manner):\nInstall web driver:\nInstall-Package Selenium.WebDriver\n\nInstall embedded exe:\nInstall-Package phantomjs.exe\n\nUpdated code:\nvar driver = new PhantomJSDriver();\ndriver.Url = ""http://www.regulations.gov/#!documentDetail;D=APHIS-2013-0013-0083"";\ndriver.Navigate();\n//the driver can now provide you with what you need (it will execute the script)\n//get the source of the page\nvar source = driver.PageSource;\n//fully navigate the dom\nvar pathElement = driver.FindElementById(""some-id"");\n\n', '\nThanks to wbennet, I discovered PhantomJSCloud.com. Enough free service to scrap pages through web API calls.\npublic static string GetPagePhantomJs(string url)\n{\n    using (var client = new System.Net.Http.HttpClient())\n    {\n        client.DefaultRequestHeaders.ExpectContinue = false;\n        var pageRequestJson = new System.Net.Http.StringContent\n            (@""{\'url\':\'"" + url + ""\',\'renderType\':\'html\',\'outputAsJson\':false }"");\n        var response = client.PostAsync\n            (""https://PhantomJsCloud.com/api/browser/v2/{YOUR_API_KEY}/"",\n            pageRequestJson).Result;\n        return response.Content.ReadAsStringAsync().Result;\n    }\n}\n\nYeah.\n', '\nok i will show you how to enable javascript using phantomjs and selenuim with c# \n\ncreate a new console project name it as you want \ngo to solution explorer in your right hand \na right click  on References click on Manage NuGet packages\na windows will shows click on browse than install Selenium.WebDriver \ndownold phantomjs from here Phantomjs\nin your main function type this code \n    var options = new PhantomJSOptions();\n    options.AddAdditionalCapability(""IsJavaScriptEnabled"", true);\n    IWebDriver driver = new PhantomJSDriver(""phantomjs Folder Path"", options);\n    driver.Navigate().GoToUrl(""https://www.yourwebsite.com/"");\n\n    try\n    {\n        string pagesource = driver.PageSource;\n        driver.FindElement(By.Id(""yourelement""));\n        Console.Write(""yourelement founded"");\n\n    }\n    catch (Exception e)\n    {\n        Console.WriteLine(e.Message);\n\n    }\n\n    Console.Read();\n\n\n\ndon\'t forget to put yourwebsite and the element that you loooking for and the phantomjs.exe path in you machine in this code below\n\nhave great time of coding and thanks wbennett\n']",https://stackoverflow.com/questions/24288726/scraping-webpage-generated-by-javascript-with-c-sharp,web-scraping
web scraping dynamic content with python,"
I'd like to use Python to scrape the contents of the ""Were you looking for these authors:"" box on web pages like this one: http://academic.research.microsoft.com/Search?query=lander
Unfortunately the contents of the box get loaded dynamically by JavaScript. Usually in this situation I can read the Javascript to figure out what's going on, or I can use an browser extension like Firebug to figure out where the dynamic content is coming from. No such luck this time...the Javascript is pretty convoluted and Firebug doesn't give many clues about how to get at the content.
Are there any tricks that will make this task easy? 
",21k,"
            6
        ","['\nInstead of trying to reverse engineer it, you can use ghost.py to directly interact with JavaScript on the page.\nIf you run the following query in a chrome console, you\'ll see it returns everything you want.\ndocument.getElementsByClassName(\'inline-text-org\');\n\nReturns\n[<div class=\u200b""inline-text-org"" title=\u200b""University of Manchester"">\u200bUniversity of Manchester\u200b</div>, \n <div class=\u200b""inline-text-org"" title=\u200b""University of California Irvine"">\u200bUniversity of California ...\u200b</div>\u200b\n  etc...\n\nYou can run JavaScript through python in a real life DOM using ghost.py.\nThis is really cool:\nfrom ghost import Ghost\nghost = Ghost()\npage, resources = ghost.open(\'http://academic.research.microsoft.com/Search?query=lander\')\nresult, resources = ghost.evaluate(\n    ""document.getElementsByClassName(\'inline-text-org\');"")\n\n', ""\nA very similar question was asked earlier here.\nQuoted is selenium, originally a testing environment for web-apps.\nI usually use Chrome's Developer Mode, which IMHO already gives even more details than Firefox.\n"", ""\nFor scraping dynamic content, you need not a simple scraper but a full-fledged headless browser.\ndhamaniasad/HeadlessBrowsers: A list of (almost) all headless web browsers in existence is the fullest list of these that I've seen; it lists which languages each has bindings for.\n(Note that more than a few of the listed projects are abandoned!)\n""]",https://stackoverflow.com/questions/17608572/web-scraping-dynamic-content-with-python,web-scraping
Dynamic dropdown doesn't populate with auto suggestions on https://www.nseindia.com/ when values are passed using Selenium and Python,"
driver = webdriver.Chrome('C:/Workspace/Development/chromedriver.exe')
driver.get('https://www.nseindia.com/companies-listing/corporate-filings-actions')
inputbox = driver.find_element_by_xpath('/html/body/div[7]/div[1]/div/section/div/div/div/div/div/div[1]/div[1]/div[1]/div/span/input[2]')
inputbox.send_keys(""Reliance"")

I'm trying to scrape the table from this website that would appear after you key in the company name in the textfield above it. The attached code block works well with such similar drop-downs of a normal google search and wolfram website, but when i run my script on the required website, that essentially just inputs the required text in the textfield - the dropdown shows 'No Records Found', whereas, when done manually it works well.
",2k,"
            3
        ","['\nI executed your test adding a few tweaks and ran the test as follows:\n\nCode Block:\nfrom selenium import webdriver        \nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\n\noptions = webdriver.ChromeOptions() \noptions.add_argument(""start-maximized"")\noptions.add_experimental_option(""excludeSwitches"", [""enable-automation""])\noptions.add_experimental_option(\'useAutomationExtension\', False)\ndriver = webdriver.Chrome(options=options, executable_path=r\'C:\\WebDrivers\\chromedriver.exe\')\ndriver.get(\'https://www.nseindia.com/companies-listing/corporate-filings-actions\')\nWebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, ""//div[@id=\'Corporate_Actions_equity\']//input[@placeholder=\'Company Name or Symbol\']""))).send_keys(""Reliance"")\n\nObservation: Similar to your observation, I have hit the same roadblock with no results as follows:\n\n\n\nDeep Dive\nIt seems the click() on the element with text as Get Data does happens. But while inspecting the DOM Tree of the webpage you will find that some of the <script> tag refers to JavaScripts having keyword akam. As an example:\n\n<script type=""text/javascript"" src=""https://www.nseindia.com/akam/11/3b383b75"" defer=""""></script>\n<noscript><img src=""https://www.nseindia.com/akam/11/pixel_3b383b75?a=dD02ZDMxODU2ODk2YTYwODA4M2JlOTlmOGNkZTY3Njg4ZWRmZjE4YmMwJmpzPW9mZg=="" style=""visibility: hidden; position: absolute; left: -999px; top: -999px;"" /></noscript>\n\nWhich is a clear indication that the website is protected by Bot Manager an advanced bot detection service provided by Akamai and the response gets blocked.\n\nBot Manager\nAs per the article Bot Manager - Foundations:\n\n\nConclusion\nSo it can be concluded that the request for the data is detected as being performed by Selenium driven WebDriver instance and the response is blocked.\n\nReferences\nA couple of documentations:\n\nBot Manager\nBot Manager : Foundations\n\n\ntl; dr\nA couple of relevant discussions:\n\nSelenium webdriver: Modifying navigator.webdriver flag to prevent selenium detection\nUnable to use Selenium to automate Chase site login\n\n']",https://stackoverflow.com/questions/62457093/dynamic-dropdown-doesnt-populate-with-auto-suggestions-on-https-www-nseindia,web-scraping
How to click a link by text with No Text in Python,"
I am trying to scrape a Wine data from vivino.com and using selenium to automate it and scrape as many data as possible. My code looks like this:
import time 
from selenium import webdriver

browser = webdriver.Chrome('C:\Program Files (x86)\chromedriver.exe')

browser.get('https://www.vivino.com/explore?e=eJwFwbEOQDAUBdC_uaNoMN7NZhQLEXmqmiZaUk3x987xkVXRwLtAVcLLy7qE_tiN0Bz6FhcV7M4s0ZkkB86VUZIL9l4kmyjW4ORmbo0nTTPVDxlkGvg%3D&cart_item_source=nav-explore') # Vivino Website with 5 wines for now (simple example). Plan to scrape around 10,000 wines 

lenOfPage = browser.execute_script(""window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;"")

match=False
while(match==False):
    lastCount = lenOfPage
    time.sleep(7)
    lenOfPage = browser.execute_script(""window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;"")
    if lastCount==lenOfPage:
        match=True

That opens a website with 5 wines and scrolls down. Now I want to click to hyperlink of the wine one by one to scrape information about its price, wine grapes sort, etc. So, basically my script will try scroll down which allows to have as many wines displayed on the page and then click to a first hyperlink, get additional information and go back. Then, the process will repeat. I don't think that's an efficient strategy but that's what I came up so far.
The problem I have is with hyperlink in the vivino website. There is no text near the href link which allows me to use find_element_by_link_text function:
<a class=""anchor__anchor--2QZvA"" href=""/weingut-r-a-pfaffl-austrian-cherry-zweigelt/w/1261542?year=2018&amp;price_id=23409078&amp;cart_item_source=direct-explore"" target=""_blank"">

Could you please suggest the way how click for a wine with Selenium that has not text after the hyperlink? I haven't found proper answer during my web search. Thanks in advance
",740,"
            1
        ","['\nYou\'re doing way more work than you have to - with Selenium I mean. When visiting the page, I logged my network traffic with Google Chrome\'s dev tools, and I saw that my browser made an HTTP GET request to a REST API, the response of which is JSON and contains all the wine/price information you could ever want. So, you don\'t need to do any scraping. Just imitate that GET request with the desired query-string parameters and the correct headers. It seems the REST API just cares about the user-agent header, which is trivial.\n\n\nFirst, visit the URL\nin Google Chrome.\nPress the F12 key to open the Google Chrome dev tools menu.\nClick on the Network tab.\nClick on the round Record button. It should turn red, and it will\nstart logging all network traffic in the log below. Click on the\nFilter button next to it, and then click on XHR. This will only\nshow XHR (XmlHttpRequest) requests in the log. We are interested in\nthese requests specifically, because it is via XHR that requests to\nAPIs are typically made. Here\'s what it should look like now:\n\n\n\nWith the Chrome dev tools menu still open, right-click (not\nleft-click) on the page refresh button to reveal a drop-down menu.\nThen, click on Empty Cache and Hard Reload.\n\n\nThis will empty your browser\'s cache for this page, and force your\nbrowser to refresh the page. As the page is being refreshed, you\nshould start to see some entries appearing in the traffic log.\n\nThere should now be some XHR request entries in the log. We don\'t know which one of these is the one we\'re actually interested in, so we just look at all of them until we find one that looks like it could be the right one (or, if it contains the information we\'re looking for, like information for individual wines, etc.). I happen to know that we are interested in the one that starts with explore?..., so let\'s click on that.\n\nOnce you click on the entry, a panel will open on the right of the\nlog. Click on the Headers tab.\n\n\nThis tab contains all the information regarding how this request was made. Under the General area, you can see the Request URL, which is the URL to the REST API endpoint that we made a request to. This URL might be quite long, because it will also typically contain the query-string parameters (those are the key-value pairs that come after explore?, like country_code=DE or currency_code=EUR. They are separated by &). The query-string parameters are important, because they contain information about certain filters that we want to apply to our query. In my code example, I\'ve removed them from the REST API endpoint URL, and instead moved them into the params dictionary. This step isn\'t required - you could also just leave them in the URL, but I find that it is easier to read and modify this way. The query-string parameters are also important because, sometimes, certains APIs will expect certain parameters to be present in the request, or they will expect them to have certain values - in other words, some APIs are very picky about their query-string parameters, and if you remove them or tamper with them in a way that the API doesn\'t expect, the API will say that your request isn\'t formulated correctly.\nIn the General area, you can also see Request Method, which in our case is GET. This tells us, that our browser made an HTTP GET request. Not all API endpoints work the same, some want HTTP POST, etc.\nStatus Code tells us what status code the server sent back. 200 means everything went OK. You can learn more about HTTP status codes here.\nLet\'s take a look at the Response Headers area. This area contains all the response headers that the server sent back after the request was made. These can be useful for a browser for things like setting cookies or knowing how to interpret the data the server has sent back.\nThe Request Headers area contains all the headers that your browser sent to the server when it made the request. Usually, it\'s a good idea to copy all of these key-value pairs and turn them into a Python dictionary headers, because that way you can be sure that your Python script will make the exact same request that your browser made. However, usually, I like to trim this down as much as I can. I know that many APIs desperately care about the user-agent field, so usually I\'ll keep that one, but sometimes they also care about the referer. As you work with different APIs, you\'ll have to just kind of figure out which request headers the API cares about through trial-and-error. This API happens to only care about the user-agent.\nThe last area Query String Parameters is just a cute way of showing the query-string parameters from the Request URL in a human-friendly list of key-value pairs. Sometimes it\'s helpful to copy them from here, rather than from the URL.\n\nNow, click on the Preview tab, next to the Headers tab.\n\n\nThe Preview tab contains a pretty-printed preview of the actual data that was sent back as a result of the browser\'s request. In our case, this contains the JSON data sent back by the server. You can click on the little gray triangles to expand or collapse certain parts of the JSON structure, to reveal different data.\n\nLooking at this, I can tell that the JSON response is one big dictionary, which has a key explore_vintage, whose value is another dictionary, which has a key records whose value is a list of dictionaries, where each dictionary in this list represents one wine object. Expanding the first record (the 0th one) reveals all information regarding the first wine in the list. You can explore these structures as much as you like to see what kinds of information are available to you.\n\n\ndef main():\n\n    import requests\n\n    url = ""https://www.vivino.com/api/explore/explore""\n\n    params = {\n        ""country_code"": ""DE"",\n        ""currency_code"": ""EUR"",\n        ""grape_filter"": ""varietal"",\n        ""min_rating"": ""3.5"",\n        ""order_by"": ""ratings_average"",\n        ""order"": ""desc"",\n        ""page"": ""1"",\n        ""price_range_max"": ""30"",\n        ""price_range_min"": ""7"",\n        ""wine_type_ids[]"": ""1""\n    }\n\n    headers = {\n        ""user-agent"": ""Mozilla/5.0""\n    }\n\n    response = requests.get(url, params=params, headers=headers)\n    response.raise_for_status()\n\n    records = response.json()[""explore_vintage""][""records""]\n\n    for record in records:\n        name = record[""vintage""][""name""]\n        price = record[""price""][""amount""]\n        currency = record[""price""][""currency""][""code""]\n        print(f""\\""{name}\\"" - Price: {price} {currency}"")\n\n    return 0\n\n\nif __name__ == ""__main__"":\n    import sys\n    sys.exit(main())\n\nOutput:\n""Varvaglione Cosimo Varvaglione Collezione Privata Primitivo di Manduria 2015"" - Price: 21.9 EUR\n""Masseria Borgo dei Trulli Mirea Primitivo di Manduria 2019"" - Price: 19.9 EUR\n""Vigneti del Salento Vigne Vecchie Primitivo di Manduria 2016"" - Price: 22.95 EUR\n""Vigneti del Salento Vigne Vecchie Leggenda Primitivo di Manduria 2016"" - Price: 17.87 EUR\n""Varvaglione Papale Linea Oro Primitivo di Manduria 2016"" - Price: 18.85 EUR\n""Caballo Loco Grand Cru Apalta 2014"" - Price: 27.9 EUR\n""Luccarelli Il Bacca Old Vine Primitivo di Manduria 2016"" - Price: 20.9 EUR\n""Mottura Stilio Primitivo di Manduria 2018"" - Price: 12.89 EUR\n""Caballo Loco Grand Cru Maipo 2015"" - Price: 24.81 EUR\n""Lorusso Michele Solone Primitivo 2017"" - Price: 21.39 EUR\n""Château Purcari Negru de Purcari 2017"" - Price: 29.8 EUR\n""San Marzano 60 Sessantanni Limited Edition Old Vines Primitivo di Manduria 2016"" - Price: 22.85 EUR\n""San Marzano 60 Sessantanni Old Vines Primitivo di Manduria 2016"" - Price: 20.9 EUR\n""San Marzano 60 Sessantanni Old Vines Primitivo di Manduria 2017"" - Price: 17.775 EUR\n""Lenotti Amarone della Valpolicella Classico 2015"" - Price: 27.95 EUR\n""Zeni Cruino Rosso Veronese 2015"" - Price: 22.9 EUR\n""Masseria Pietrosa Palmenti Primitivo di Manduria Vigne Vecchie 2016"" - Price: 25 EUR\n""Ravazzi Prezioso 2016"" - Price: 29.95 EUR\n""Nino Negri Sfursat Carlo Negri 2017"" - Price: 23.89 EUR\n""Quinta do Paral Reserva Tinto 2017"" - Price: 29.24 EUR\n""Wildekrans Barrel Select Reserve Pinotage 2016"" - Price: 29.9 EUR\n""Caballo Loco Grand Cru Limarí 2016"" - Price: 27.9 EUR\n""San Marzano F Negroamaro 2018"" - Price: 16.9 EUR\n""Atlan & Artisan 8 Vents Mallorca 2018"" - Price: 19 EUR\n""Schneider Rooi Olifant Red 2017"" - Price: 19.5 EUR\n>>> \n\nIt just seems to grab twenty-five records/wines per page, but changing the page key-value pair in the params query-string parameter dictionary will yield the records from whatever page you desire. I\'m currently located in Germany, that\'s why my country_code and currency_code are ""DE"" and ""EUR"", but you should be able to change those to suit your needs.\n\nEDIT - here are some more key-value pairs you may be interested in, though I would recommend you get familiar with how your browser\'s dev tools work so that you can discover these fields in the JSON yourself:\nrecord[""vintage""][""year""]\nrecord[""vintage""][""wine""][""region""][""name""]\nrecord[""vintage""][""wine""][""region""][""country""][""name""]\nrecord[""vintage""][""wine""][""taste""][""structure""][""acidity""]\nrecord[""vintage""][""wine""][""taste""][""structure""][""intensity""]\nrecord[""vintage""][""wine""][""taste""][""structure""][""sweetness""]\nrecord[""vintage""][""wine""][""taste""][""structure""][""tannin""]\nrecord[""vintage""][""wine""][""style""][""grapes""][0][""name""] # 0th grape information\nrecord[""vintage""][""wine""][""winery""][""name""]\n\nThe JSON unfortunately doesn\'t contain the alcohol content. This information is hardcoded in the HTML of a given wine\'s summary page. You would have to make a request to each wine summary page and pull the alcohol content value out of the page, maybe with a regex.\n', ""\nJust to answer your question, how to select the a tag with selenium to perform the .click():\nlinks = browser.find_elements_by_css_selector('div.cleanWineCard__cleanWineCard--tzKxV.cleanWineCard__row--CBPRR > a')\n\nUsing the css selector you can locate the div tag where the image of the wine is placed in surrounded by an a tag.\nNow you could loop the links perform the .click() and do what ever you wanna do:\nfor a in links:\n    a.click()\n    # grab information\n    # sleep a bit\n    #...\n\n"", '\n1-\nBy beautiful soup you can do the following. I change the code a little but you can still use the beautiful soup part.\nfrom bs4 import BeautifulSoup\nimport requests\nimport time, os\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\nchromedriver = ""C:\\\\Program Files\\Google\\Chrome\\Application\\chromedriver"" # path to the chromedriver executable\nos.environ[""webdriver.chrome.driver""] = chromedriver\ndriver = webdriver.Chrome(chromedriver)\ndriver.get(\'https://www.vivino.com/explore?e=eJwFwbEOQDAUBdC_uaNoMN7NZhQLEXmqmiZaUk3x987xkVXRwLtAVcLLy7qE_tiN0Bz6FhcV7M4s0ZkkB86VUZIL9l4kmyjW4ORmbo0nTTPVDxlkGvg%3D&cart_item_source=nav-explore\')\n\n#browser.get(\'https://www.vivino.com/explore?e=eJwFwbEOQDAUBdC_uaNoMN7NZhQLEXmqmiZaUk3x987xkVXRwLtAVcLLy7qE_tiN0Bz6FhcV7M4s0ZkkB86VUZIL9l4kmyjW4ORmbo0nTTPVDxlkGvg%3D&cart_item_source=nav-explore\') # Vivino Website with 5 wines for now (simple example). Plan to scrape around 10,000 wines \n\nlenOfPage = driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;"")\n\nlist= []\n\nmatch=False\nwhile(match==False):\n    lastCount = lenOfPage\n    time.sleep(7)\n    lenOfPage = driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;"")\n    match=True\n    soup = BeautifulSoup(driver.page_source, \'html.parser\')\n\n    for a in soup.find_all(\'a\', href=True):\n        print(  a[\'href\'] )\n\nand it will list you all the hrefs.\n2- For alternative: selenium also supports getting element by XPath. you can inspect the item and copy the XPath and then you can use it. Please check the guide below. It will help.\nhttps://selenium-python.readthedocs.io/locating-elements.html\nfor example the following works. I got this working code from at link https://stackoverflow.com/a/63828196/3756587. all credit goes to user mamal : https://stackoverflow.com/users/4941102/mamal\n    links = driver.find_elements_by_xpath(\'//*[@href]\')\n    for i in links:\n        print(i.get_attribute(\'href\'))\n\nNow you have all the links.  With a simple for loop, you can proceed.\nEnjoy!\n']",https://stackoverflow.com/questions/65585597/how-to-click-a-link-by-text-with-no-text-in-python,web-scraping
Scraping dynamic data selenium - Unable to locate element,"
I am very new to scraping and have a question. I am scraping worldometers covid data. As it is dynamic - I am doing it with selenium.
The code is the following:
from selenium import webdriver
import time

URL = ""https://www.worldometers.info/coronavirus/""

# Start the Driver
driver = webdriver.Chrome(executable_path = r""C:\Webdriver\chromedriver.exe"")
# Hit the url and wait for 10 seconds.
driver.get(URL)
time.sleep(10)
#find class element
data= driver.find_elements_by_class_name(""odd"" and ""even"")
#for loop
for d in data:
    country=d.find_element_by_xpath("".//*[@id='main_table_countries_today']"").text
    print(country)

current output:
NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""xpath"",""selector"":"".//*[@id='main_table_countries_today']""}
  (Session info: chrome=96.0.4664.45)

",124,"
            1
        ","['\nTo scrape table within worldometers covid data you need to induce WebDriverWait for the visibility_of_element_located() and using DataFrame from Pandas you can use the following Locator Strategy:\nCode Block:\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pandas as pd\n\noptions = Options()\noptions.add_argument(""start-maximized"")\ns = Service(\'C:\\\\BrowserDrivers\\\\chromedriver.exe\')\ndriver = webdriver.Chrome(service=s, options=options)\ndriver.get(""https://www.worldometers.info/coronavirus/"")\ndata = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.CSS_SELECTOR, ""table#main_table_countries_today""))).get_attribute(""outerHTML"")\ndf  = pd.read_html(data)\nprint(df)\ndriver.quit()\n\nConsole Output:\n[         # Country,Other  TotalCases  NewCases  ...  Deaths/1M pop   TotalTests  Tests/ 1M pop    Population\n0      NaN         World   264359298  632349.0  ...          673.3          NaN            NaN           NaN\n1      1.0           USA    49662381   89259.0  ...         2415.0  756671013.0      2267182.0  3.337495e+08\n2      2.0         India    34609741    3200.0  ...          336.0  643510926.0       459914.0  1.399198e+09\n3      3.0        Brazil    22118782   12910.0  ...         2865.0   63776166.0       297051.0  2.146975e+08\n4      4.0            UK    10329074   53945.0  ...         2124.0  364875273.0      5335159.0  6.839070e+07\n..     ...           ...         ...       ...  ...            ...          ...            ...           ...\n221  221.0         Samoa           3       NaN  ...            NaN          NaN            NaN  2.002800e+05\n222  222.0  Saint Helena           2       NaN  ...            NaN          NaN            NaN  6.103000e+03\n223  223.0    Micronesia           1       NaN  ...            NaN          NaN            NaN  1.167290e+05\n224  224.0         Tonga           1       NaN  ...            NaN          NaN            NaN  1.073890e+05\n225    NaN        Total:   264359298  632349.0  ...          673.3          NaN            NaN           NaN\n\n[226 rows x 15 columns]]\n\n']",https://stackoverflow.com/questions/70206678/scraping-dynamic-data-selenium-unable-to-locate-element,web-scraping
How to find tag with particular text with Beautiful Soup?,"
How to find text I am looking for in the following HTML (line breaks marked with \n)?
...
<tr>
  <td class=""pos"">\n
      ""Some text:""\n
      <br>\n
      <strong>some value</strong>\n
  </td>
</tr>
<tr>
  <td class=""pos"">\n
      ""Fixed text:""\n
      <br>\n
      <strong>text I am looking for</strong>\n
  </td>
</tr>
<tr>
  <td class=""pos"">\n
      ""Some other text:""\n
      <br>\n
      <strong>some other value</strong>\n
  </td>
</tr>
...

The code below returns first found value, so I need to filter by ""Fixed text:"" somehow.
result = soup.find('td', {'class' :'pos'}).find('strong').text

UPDATE: If I use the following code:
title = soup.find('td', text = re.compile(ur'Fixed text:(.*)', re.DOTALL), attrs = {'class': 'pos'})
self.response.out.write(str(title.string).decode('utf8'))

then it returns just Fixed text:, not the <strong>-highlighted text in that same element.
",145k,"
            45
        ","[""\nYou can pass a regular expression to the text parameter of findAll, like so:\nimport BeautifulSoup\nimport re\n\ncolumns = soup.findAll('td', text = re.compile('your regex here'), attrs = {'class' : 'pos'})\n\n"", '\nThis post got me to my answer even though the answer is missing from this post. I felt I should give back.\nThe challenge here is in the inconsistent behavior of BeautifulSoup.find when searching with and without text.\nNote:\nIf you have BeautifulSoup, you can test this locally via:\ncurl https://gist.githubusercontent.com/RichardBronosky/4060082/raw/test.py | python\n\nCode: https://gist.github.com/4060082\n# Taken from https://gist.github.com/4060082\nfrom BeautifulSoup import BeautifulSoup\nfrom urllib2 import urlopen\nfrom pprint import pprint\nimport re\n\nsoup = BeautifulSoup(urlopen(\'https://gist.githubusercontent.com/RichardBronosky/4060082/raw/test.html\').read())\n# I\'m going to assume that Peter knew that re.compile is meant to cache a computation result for a performance benefit. However, I\'m going to do that explicitly here to be very clear.\npattern = re.compile(\'Fixed text\')\n\n# Peter\'s suggestion here returns a list of what appear to be strings\ncolumns = soup.findAll(\'td\', text=pattern, attrs={\'class\' : \'pos\'})\n# ...but it is actually a BeautifulSoup.NavigableString\nprint type(columns[0])\n#>> <class \'BeautifulSoup.NavigableString\'>\n\n# you can reach the tag using one of the convenience attributes seen here\npprint(columns[0].__dict__)\n#>> {\'next\': <br />,\n#>>  \'nextSibling\': <br />,\n#>>  \'parent\': <td class=""pos"">\\n\n#>>       ""Fixed text:""\\n\n#>>       <br />\\n\n#>>       <strong>text I am looking for</strong>\\n\n#>>   </td>,\n#>>  \'previous\': <td class=""pos"">\\n\n#>>       ""Fixed text:""\\n\n#>>       <br />\\n\n#>>       <strong>text I am looking for</strong>\\n\n#>>   </td>,\n#>>  \'previousSibling\': None}\n\n# I feel that \'parent\' is safer to use than \'previous\' based on http://www.crummy.com/software/BeautifulSoup/bs4/doc/#method-names\n# So, if you want to find the \'text\' in the \'strong\' element...\npprint([t.parent.find(\'strong\').text for t in soup.findAll(\'td\', text=pattern, attrs={\'class\' : \'pos\'})])\n#>> [u\'text I am looking for\']\n\n# Here is what we have learned:\nprint soup.find(\'strong\')\n#>> <strong>some value</strong>\nprint soup.find(\'strong\', text=\'some value\')\n#>> u\'some value\'\nprint soup.find(\'strong\', text=\'some value\').parent\n#>> <strong>some value</strong>\nprint soup.find(\'strong\', text=\'some value\') == soup.find(\'strong\')\n#>> False\nprint soup.find(\'strong\', text=\'some value\') == soup.find(\'strong\').text\n#>> True\nprint soup.find(\'strong\', text=\'some value\').parent == soup.find(\'strong\')\n#>> True\n\nThough it is most certainly too late to help the OP, I hope they will make this as the answer since it does satisfy all quandaries around finding by text.\n']",https://stackoverflow.com/questions/9007653/how-to-find-tag-with-particular-text-with-beautiful-soup,web-scraping
How can I handle Javascript in a Perl web crawler?,"
I would like to crawl a website, the problem is, that its full of JavaScript things, such as buttons and such that when they are pressed, they do not change the URL, but the data on the page is changed. 
Usually I use LWP / Mechanize etc to crawl sites, but neither support JavaScript.
any idea?
",5k,"
            19
        ","['\nAnother option might be Selenium with WWW::Selenium module\n', ""\nThe WWW::Scripter module has a JavaScript plugin that may be useful. Can't say I've used it myself, however.\n"", '\nWWW::Mechanize::Firefox might be of use.  that way you can have Firefox handle the complex JavaScript issues and then extract the resultant html.\n', '\nI would suggest HtmlUnit and Perl wrapper: WWW::HtmlUnit.\n']",https://stackoverflow.com/questions/3769015/how-can-i-handle-javascript-in-a-perl-web-crawler,web-scraping
Python regular expression for HTML parsing,"
I want to grab the value of a hidden input field in HTML.
<input type=""hidden"" name=""fooId"" value=""12-3456789-1111111111"" />

I want to write a regular expression in Python that will return the value of fooId, given that I know the line in the HTML follows the format
<input type=""hidden"" name=""fooId"" value=""**[id is here]**"" />

Can someone provide an example in Python to parse the HTML for the value?
",27k,"
            11
        ","[""\nFor this particular case, BeautifulSoup is harder to write than a regex, but it is much more robust... I'm just contributing with the BeautifulSoup example, given that you already know which regexp to use :-)\nfrom BeautifulSoup import BeautifulSoup\n\n#Or retrieve it from the web, etc. \nhtml_data = open('/yourwebsite/page.html','r').read()\n\n#Create the soup object from the HTML data\nsoup = BeautifulSoup(html_data)\nfooId = soup.find('input',name='fooId',type='hidden') #Find the proper tag\nvalue = fooId.attrs[2][1] #The value of the third attribute of the desired tag \n                          #or index it directly via fooId['value']\n\n"", ""\nI agree with Vinko BeautifulSoup is the way to go. However I suggest using fooId['value'] to get the attribute rather than relying on value being the third attribute.\nfrom BeautifulSoup import BeautifulSoup\n#Or retrieve it from the web, etc.\nhtml_data = open('/yourwebsite/page.html','r').read()\n#Create the soup object from the HTML data\nsoup = BeautifulSoup(html_data)\nfooId = soup.find('input',name='fooId',type='hidden') #Find the proper tag\nvalue = fooId['value'] #The value attribute\n\n"", '\nimport re\nreg = re.compile(\'<input type=""hidden"" name=""([^""]*)"" value=""<id>"" />\')\nvalue = reg.search(inputHTML).group(1)\nprint \'Value is\', value\n\n', ""\nParsing is one of those areas where you really don't want to roll your own if you can avoid it, as you'll be chasing down the edge-cases and bugs for years go come\nI'd recommend using BeautifulSoup. It has a very good reputation and looks from the docs like it's pretty easy to use.\n"", '\nPyparsing is a good interim step between BeautifulSoup and regex.  It is more robust than just regexes, since its HTML tag parsing comprehends variations in case, whitespace, attribute presence/absence/order, but simpler to do this kind of basic tag extraction than using BS.\nYour example is especially simple, since everything you are looking for is in the attributes of the opening ""input"" tag.  Here is a pyparsing example showing several variations on your input tag that would give regexes fits, and also shows how NOT to match a tag if it is within a comment:\nhtml = """"""<html><body>\n<input type=""hidden"" name=""fooId"" value=""**[id is here]**"" />\n<blah>\n<input name=""fooId"" type=""hidden"" value=""**[id is here too]**"" />\n<input NAME=""fooId"" type=""hidden"" value=""**[id is HERE too]**"" />\n<INPUT NAME=""fooId"" type=""hidden"" value=""**[and id is even here TOO]**"" />\n<!--\n<input type=""hidden"" name=""fooId"" value=""**[don\'t report this id]**"" />\n-->\n<foo>\n</body></html>""""""\n\nfrom pyparsing import makeHTMLTags, withAttribute, htmlComment\n\n# use makeHTMLTags to create tag expression - makeHTMLTags returns expressions for\n# opening and closing tags, we\'re only interested in the opening tag\ninputTag = makeHTMLTags(""input"")[0]\n\n# only want input tags with special attributes\ninputTag.setParseAction(withAttribute(type=""hidden"", name=""fooId""))\n\n# don\'t report tags that are commented out\ninputTag.ignore(htmlComment)\n\n# use searchString to skip through the input \nfoundTags = inputTag.searchString(html)\n\n# dump out first result to show all returned tags and attributes\nprint foundTags[0].dump()\nprint\n\n# print out the value attribute for all matched tags\nfor inpTag in foundTags:\n    print inpTag.value\n\nPrints:\n[\'input\', [\'type\', \'hidden\'], [\'name\', \'fooId\'], [\'value\', \'**[id is here]**\'], True]\n- empty: True\n- name: fooId\n- startInput: [\'input\', [\'type\', \'hidden\'], [\'name\', \'fooId\'], [\'value\', \'**[id is here]**\'], True]\n  - empty: True\n  - name: fooId\n  - type: hidden\n  - value: **[id is here]**\n- type: hidden\n- value: **[id is here]**\n\n**[id is here]**\n**[id is here too]**\n**[id is HERE too]**\n**[and id is even here TOO]**\n\nYou can see that not only does pyparsing match these unpredictable variations, it returns the data in an object that makes it easy to read out the individual tag attributes and their values.\n', '\n/<input type=""hidden"" name=""fooId"" value=""([\\d-]+)"" \\/>/\n\n', '\n/<input\\s+type=""hidden""\\s+name=""([A-Za-z0-9_]+)""\\s+value=""([A-Za-z0-9_\\-]*)""\\s*/>/\n\n>>> import re\n>>> s = \'<input type=""hidden"" name=""fooId"" value=""12-3456789-1111111111"" />\'\n>>> re.match(\'<input\\s+type=""hidden""\\s+name=""([A-Za-z0-9_]+)""\\s+value=""([A-Za-z0-9_\\-]*)""\\s*/>\', s).groups()\n(\'fooId\', \'12-3456789-1111111111\')\n\n']",https://stackoverflow.com/questions/55391/python-regular-expression-for-html-parsing,web-scraping
Failproof Wait for IE to load,"
Is there a foolproof way for the script to wait till the Internet explorer is completely loaded?
Both oIE.Busy and / or oIE.ReadyState are not working the way they should: 
Set oIE = CreateObject(""InternetExplorer.application"")

    oIE.Visible = True
    oIE.navigate (""http://technopedia.com"")

    Do While oIE.Busy Or oIE.ReadyState <> 4: WScript.Sleep 100: Loop  

    ' <<<<< OR >>>>>>

    Do While oIE.ReadyState <> 4: WScript.Sleep 100: Loop

Any other suggestions?
",60k,"
            9
        ","['\nTry this one, it helped me to solve similar problem with IE once:\nSet oIE = CreateObject(""InternetExplorer.application"")\noIE.Visible = True\noIE.navigate (""http://technopedia.com"")\nDo While oIE.ReadyState = 4: WScript.Sleep 100: Loop\nDo While oIE.ReadyState <> 4: WScript.Sleep 100: Loop\n\' example ref to DOM\nMsgBox oIE.Document.GetElementsByTagName(""div"").Length\n\nUPD: Drilling down IE events I found that IE_DocumentComplete is the last event before the page is actually ready. So there is one more method to detect when a web page is loaded (note that you have to specify the exact destination URL which may differ from the target URL eg in case of redirection):\noption explicit\ndim ie, targurl, desturl, completed\n\nset ie = wscript.createobject(""internetexplorer.application"", ""ie_"")\nie.visible = true\n\ntargurl = ""http://technopedia.com/""\ndesturl = ""http://technopedia.com/""\n\n\' targurl = ""http://tumblr.com/""\n\' desturl = ""https://www.tumblr.com/"" \' redirection if you are not login\n\' desturl = ""https://www.tumblr.com/dashboard"" \' redirection if you are login\n\ncompleted = false\nie.navigate targurl\ndo until completed\n    wscript.sleep 100\nloop\n\' your code here\nmsgbox ie.document.getelementsbytagname(""*"").length\nie.quit\n\nsub ie_documentcomplete(byval pdisp, byval url)\n    if url = desturl then completed = true\nend sub\n\n', '\nI have for a very long time been successfully using:\nWhile IE.readyState <> 4 Or IE.Busy: DoEvents: Wend\n\nIt has been working perfectly until today, when I changed my PC and switched to Windows 10 and Office 16. Then it started working on some cases, but there were times when the loop was not completed. Neither one of the conditions in the loop was reached, so the loop was ENDLESS.\nAfter a lot of Googling, I have tried many suggestions until I found the solution in this post: Excel VBA Controlling IE local intranet\nThe solution is to add the URL to the trusted sites list in Internet Explorer Security tab. Finally!\n', '\nA few years later, it also hit me. I looked at the proposed solutions and tested a lot. The following combination of commands has been developed, which I will now use in my application.\nSet oIE = CreateObject(""InternetExplorer.application"")\n\noIE.Visible = True\noIE.navigate (""http://technopedia.com"")\n\nwscript.sleep 100\nDo While oIE.Busy or oIE.ReadyState <> 4: WScript.Sleep 100: Loop  \n\nwscript.sleep 100\nDo While oIE.Busy or oIE.ReadyState <> 4: WScript.Sleep 100: Loop  \n\nmsgbox oIE.ReadyState & "" / "" & oIE.Busy , vbInformation +  vbMsgBoxForeground , ""Information""\n\noIE.Quit\n\nset oIE = Nothing\n\nThe second identical loop I did install after it turned out that oIE.Busy = True was sometimes after the first loop.\nRegards,\nScriptMan\n', '\ntry to put this script on the top, this may solve Your problem.\n{ \n    $myWindowsID = [System.Security.Principal.WindowsIdentity]::GetCurrent();\n    $myWindowsPrincipal = New-Object System.Security.Principal.WindowsPrincipal($myWindowsID);\n    $adminRole = [System.Security.Principal.WindowsBuiltInRole]::Administrator;\n\n    if ($myWindowsPrincipal.IsInRole($adminRole)) {\n        $Host.UI.RawUI.WindowTitle = $myInvocation.MyCommand.Definition + ""(Elevated)"";\n        Clear-Host;\n    }\n    else {\n        $newProcess = New-Object System.Diagnostics.ProcessStartInfo ""PowerShell"";\n        $newProcess.Arguments = ""& \'"" + $script:MyInvocation.MyCommand.Path + ""\'""\n        $newProcess.Verb = ""runas"";\n        [System.Diagnostics.Process]::Start($newProcess);\n        Exit;\n    }\n}\n\nExplanation:\nPowershell is not having some rights when you are running script from the normal mode \nso it is not reading IE status properly \nand that is why DOM is not being loaded\nso, script doesn\'t found any parameter  \n', '\nSo through looking up this answer, I still had trouble with IE waiting for the page to completely load. In the hopes that this solution will help others, here is what I did:\nDim i As Integer\nDim j As Integer\nDim tagnames As Integer\n\nWhile ie.Busy\n    DoEvents\nWend\nWhile ie.ReadyState <> 4\n    DoEvents\nWend\n\ni = 0\nj = 0\nWhile (i <> 5)\n    i = 0\n    tagnames = ie.document.getelementsbytagname(""*"").Length\n    For j = 1 To 5\n        Sleep (50)\n        If tagnames = ie.document.getelementsbytagname(""*"").Length Then\n            b = b + 1\n        End If\n    Next j\nWend\n\nBasically, what this does is wait for 5 50ms intervals for the number of tagnames loaded to stop increasing.\nOf course, this is adjustable depending on what you want, but that worked for my application.\n', '\nIf your working with IE on a form submission, it\'s better to place it in a Sub so you can reference the same Sub repeatedly. \nDim IE\nSet IE = WScript.CreateObject(""InternetExplorer.Application"")\n    IE.Visible = True\n    IE.Navigate ""http://www.google.com""\n    Wait IE, 500\n\nSub Wait(IE, SleepInterval)\n    Do\n        WScript.Sleep SleepInterval\n    Loop While IE.ReadyState < 4 Or IE.Busy\nEnd Sub\n\nThe difference being, that your referencing the IE object AFTER the wscript.sleep. If you check them first and foremost before the object is loaded. It could cause script failure. \n']",https://stackoverflow.com/questions/23299134/failproof-wait-for-ie-to-load,web-scraping
How can I scrape website content in PHP from a website that requires a cookie login?,"
My problem is that it doesn't just require a basic cookie, but rather asks for a session cookie, and for randomly generated IDs. I think this means I need to use a web browser emulator with a cookie jar?
I have tried to use Snoopy, Goutte and a couple of other web browser emulators, but as of yet I have not been able to find tutorials on how to receive cookies. I am getting a little desperate!
Can anyone give me an example of how to accept cookies in Snoopy or Goutte?
Thanks in advance!
",16k,"
            6
        ","['\nYou can do that in cURL without needing external \'emulators\'.\nThe code below retrieves a page into a PHP variable to be parsed.\nScenario\nThere is a page (let\'s call it HOME) that opens the session. Server side, if it is in PHP, is the one (any one actually) calling session_start() for the first time. In other languages you need a specific page that will do all the session setup. From the client side it\'s the page supplying the session ID cookie. In PHP, all sessioned pages do; in other languages the landing page will do it, all the others will check if the cookie is there, and if there isn\'t, instead of creating the session, will drop you to HOME.\nThere is a page (LOGIN) that generates the login form and adds a critical information to the session - ""This user is logged in"". In the code below, this is the page asking for the session ID.\nAnd finally there are N pages where the goodies to be scrapes reside.\nSo we want to hit HOME, then LOGIN, then GOODIES one after another. In PHP (and other languages actually), again, HOME and LOGIN might well be the same page. Or all pages might share the same address, for example in Single Page Applications.\nThe Code\n    $url            = ""the url generating the session ID"";\n    $next_url       = ""the url asking for session"";\n\n    $ch             = curl_init();\n    curl_setopt($ch, CURLOPT_URL,    $url);\n    // We do not authenticate, only access page to get a session going.\n    // Change to False if it is not enough (you\'ll see that cookiefile\n    // remains empty).\n    curl_setopt($ch, CURLOPT_NOBODY, True);\n\n    // You may want to change User-Agent here, too\n    curl_setopt($ch, CURLOPT_COOKIEFILE, ""cookiefile"");\n    curl_setopt($ch, CURLOPT_COOKIEJAR,  ""cookiefile"");\n\n    // Just in case\n    curl_setopt($ch, CURLOPT_FOLLOWLOCATION, true);\n\n    $ret    = curl_exec($ch);\n\n    // This page we retrieve, and scrape, with GET method\n    foreach(array(\n            CURLOPT_POST            => False,       // We GET...\n            CURLOPT_NOBODY          => False,       // ...the body...\n            CURLOPT_URL             => $next_url,   // ...of $next_url...\n            CURLOPT_BINARYTRANSFER  => True,        // ...as binary...\n            CURLOPT_RETURNTRANSFER  => True,        // ...into $ret...\n            CURLOPT_FOLLOWLOCATION  => True,        // ...following redirections...\n            CURLOPT_MAXREDIRS       => 5,           // ...reasonably...\n            CURLOPT_REFERER         => $url,        // ...as if we came from $url...\n            //CURLOPT_COOKIEFILE      => \'cookiefile\', // Save these cookies\n            //CURLOPT_COOKIEJAR       => \'cookiefile\', // (already set above)\n            CURLOPT_CONNECTTIMEOUT  => 30,          // Seconds\n            CURLOPT_TIMEOUT         => 300,         // Seconds\n            CURLOPT_LOW_SPEED_LIMIT => 16384,       // 16 Kb/s\n            CURLOPT_LOW_SPEED_TIME  => 15,          // \n            ) as $option => $value)\n            if (!curl_setopt($ch, $option, $value))\n                    die(""could not set $option to "" . serialize($value));\n\n    $ret = curl_exec($ch);\n    // Done; cleanup.\n    curl_close($ch);\n\nImplementation\nFirst of all we have to get the login page.\nWe use a special User-Agent to introduce ourselves, in order both to be recognizable (we don\'t want to antagonize the webmaster) but also to fool the server into sending us a specific version of the site that is browser tailored. Ideally, we use the same User-Agent as any browser we\'re going to use to debug the page, plus a suffix to make it clear to whoever checks that it is an automated tool they\'re looking at (see comment by Halfer).\n    $ua = \'Mozilla/5.0 (Windows NT 5.1; rv:16.0) Gecko/20100101 Firefox/16.0 (ROBOT)\';\n    $cookiefile = ""cookiefile"";\n    $url1 = ""the login url generating the session ID"";\n\n    $ch             = curl_init();\n\n    curl_setopt($ch, CURLOPT_URL,            $url1);\n    curl_setopt($ch, CURLOPT_USERAGENT,      $ua);\n    curl_setopt($ch, CURLOPT_COOKIEFILE,     $cookiefile);\n    curl_setopt($ch, CURLOPT_COOKIEJAR,      $cookiefile);\n    curl_setopt($ch, CURLOPT_FOLLOWLOCATION, True);\n    curl_setopt($ch, CURLOPT_NOBODY,         False);\n    curl_setopt($ch, CURLOPT_RETURNTRANSFER, True);\n    curl_setopt($ch, CURLOPT_BINARYTRANSFER, True);\n    $ret    = curl_exec($ch);\n\nThis will retrieve the page asking for user/password. By inspecting the page, we find the needed fields (including hidden ones) and can populate them. The FORM tag tells us whether we need to go on with POST or GET.\nWe might want to inspect the form code to adjust the following operations, so we ask cURL to return the page content as-is into $ret, and to do return the page body. Sometimes, CURLOPT_NOBODY set to True is still enough to trigger session creation and cookie submission, and if so, it\'s faster. But CURLOPT_NOBODY (""no body"") works by issuing a HEAD request, instead of a GET; and sometimes the HEAD request doesn\'t work because the server will only react to a full GET.\nInstead of retrieving the body this way, it is also possible to login using a real Firefox and sniff the form content being posted with Firebug (or Chrome with Chrome Tools); some sites will try and populate/modify hidden fields with Javascript, so that the form being submitted will not be the one you see in the HTML code.\nA webmaster who wanted his site not scraped might send a hidden field with the timestamp. A human being (not aided by a too-clever browser - there are ways to tell browsers not to be clever; at worst, every time you change the name of user and pass fields) takes at least three seconds to fill a form. A cURL script takes zero. Of course, a delay can be simulated. It\'s all shadowboxing...\nWe may also want to be on the lookout for form appearance. A webmaster could for example build a form asking name, email, and password; and then, through use of CSS, move the ""email"" field where you would expect to find the name, and vice versa. So the real form being submitted will have a ""@"" in a field called username, none in the field called email. The server, that expects this, merely inverts again the two fields. A ""scraper"" built by hand (or a spambot) would do what seems natural, and send an email in the email field. And by so doing, it betrays itself. By working through the form once with a real CSS and JS aware browser, sending meaningful data, and sniffing what actually gets sent, we might be able to overcome this particular obstacle. Might, because there are ways of making life difficult. As I said, shadowboxing.\nBack to the case at hand, in this case the form contains three fields and has no Javascript overlay. We have cPASS, cUSR, and checkLOGIN with a value of \'Check login\'.\nSo we prepare the form with the proper fields. Note that the form is to be sent as application/x-www-form-urlencoded, which in PHP cURL means two things:\n\nwe are to use CURLOPT_POST\nthe option CURLOPT_POSTFIELDS must be a string (an array would signal cURL to submit as multipart/form-data, which might work... or might not).\n\nThe form fields are, as it says, urlencoded; there\'s a function for that.\nWe read the action field of the form; that\'s the URL we are to use to submit our authentication (which we must have).\nSo everything being ready...\n    $fields = array(\n        \'checkLOGIN\' => \'Check Login\',\n        \'cUSR\'       => \'jb007\',\n        \'cPASS\'      => \'astonmartin\',\n    );\n    $coded = array();\n    foreach($fields as $field => $value)\n        $coded[] = $field . \'=\' . urlencode($value);\n    $string = implode(\'&\', $coded);\n\n    curl_setopt($ch, CURLOPT_URL,         $url1); //same URL as before, the login url generating the session ID\n    curl_setopt($ch, CURLOPT_POST,        True);\n    curl_setopt($ch, CURLOPT_POSTFIELDS,  $string);\n    $ret    = curl_exec($ch);\n\nWe expect now a ""Hello, James - how about a nice game of chess?"" page. But more than that, we expect that the session associated with the cookie saved in the $cookiefile has been supplied with the critical information -- ""user is authenticated"".\nSo all following page requests made using $ch and the same cookie jar will be granted access, allowing us to \'scrape\' pages quite easily - just remember to set request mode back to GET:\n    curl_setopt($ch, CURLOPT_POST,        False);\n\n    // Start spidering\n    foreach($urls as $url)\n    {\n        curl_setopt($ch, CURLOPT_URL, $url);\n        $HTML = curl_exec($ch);\n        if (False === $HTML)\n        {\n            // Something went wrong, check curl_error() and curl_errno().\n        }\n    }\n    curl_close($ch);\n\nIn the loop, you have access to $HTML -- the HTML code of every single page.\nGreat the temptation of using regexps is. Resist it you must. To better cope with ever-changing HTML, as well as being sure not to turn up false positives or false negatives when the layout stays the same but the content changes (e.g. you discover that you have the weather forecasts of Nice, Tourrette-Levens, Castagniers, but never Asprémont or Gattières, and isn\'t that cürious?), the best option is to use DOM:\nGrabbing the href attribute of an A element\n', '\nObject-Oriented answer\nWe implement as much as possible of the previous answer in one class called Browser that should supply the normal navigation features.\nThen we should be able to put the site-specific code, in very simple form, in a new derived class that we call, say, FooBrowser, that performs scraping of the site Foo.\nThe class deriving Browser must supply some site-specific function such as a path() function allowing to store site-specific information, for example\nfunction path($basename) {\n    return \'/var/tmp/www.foo.bar/\' . $basename;\n}\n\nabstract class Browser\n{\n    private $options = [];\n    private $state   = [];\n    protected $cookies;\n\n    abstract protected function path($basename);\n\n    public function __construct($site, $options = []) {\n        $this->cookies   = $this->path(\'cookies\');\n        $this->options  = array_merge(\n            [\n                \'site\'      => $site,\n                \'userAgent\' => \'Mozilla/5.0 (Windows NT 5.1; rv:16.0) Gecko/20100101 Firefox/16.0 - LeoScraper\',\n                \'waitTime\'  => 250000,\n            ],\n            $options\n        );\n        $this->state = [\n            \'referer\' => \'/\',\n            \'url\'     => \'\',\n            \'curl\'    => \'\',\n        ];\n        $this->__wakeup();\n    }\n\n    /**\n     * Reactivates after sleep (e.g. in session) or creation\n     */\n    public function __wakeup() {\n        $this->state[\'curl\'] = curl_init();\n        $this->config([\n            CURLOPT_USERAGENT       => $this->options[\'userAgent\'],\n            CURLOPT_ENCODING        => \'\',\n            CURLOPT_NOBODY          => false,\n            // ...retrieving the body...\n            CURLOPT_BINARYTRANSFER  => true,\n            // ...as binary...\n            CURLOPT_RETURNTRANSFER  => true,\n            // ...into $ret...\n            CURLOPT_FOLLOWLOCATION  => true,\n            // ...following redirections...\n            CURLOPT_MAXREDIRS       => 5,\n            // ...reasonably...\n            CURLOPT_COOKIEFILE      => $this->cookies,\n            // Save these cookies\n            CURLOPT_COOKIEJAR       => $this->cookies,\n            // (already set above)\n            CURLOPT_CONNECTTIMEOUT  => 30,\n            // Seconds\n            CURLOPT_TIMEOUT         => 300,\n            // Seconds\n            CURLOPT_LOW_SPEED_LIMIT => 16384,\n            // 16 Kb/s\n            CURLOPT_LOW_SPEED_TIME  => 15,\n        ]);\n    }\n\n    /**\n     * Imports an options array.\n     *\n     * @param array $opts\n     * @throws DetailedError\n     */\n    private function config(array $opts = []) {\n        foreach ($opts as $key => $value) {\n            if (true !== curl_setopt($this->state[\'curl\'], $key, $value)) {\n                throw new \\Exception(\'Could not set cURL option\');\n            }\n        }\n    }\n\n    private function perform($url) {\n        $this->state[\'referer\'] = $this->state[\'url\'];\n        $this->state[\'url\'] = $url;\n        $this->config([\n            CURLOPT_URL     => $this->options[\'site\'] . $this->state[\'url\'],\n            CURLOPT_REFERER => $this->options[\'site\'] . $this->state[\'referer\'],\n        ]);\n        $response = curl_exec($this->state[\'curl\']);\n        // Should we ever want to randomize waitTime, do so here.\n        usleep($this->options[\'waitTime\']);\n\n        return $response;\n    }\n\n    /**\n     * Returns a configuration option.\n     * @param string $key       configuration key name\n     * @param string $value     value to set\n     * @return mixed\n     */\n    protected function option($key, $value = \'__DEFAULT__\') {\n        $curr   = $this->options[$key];\n        if (\'__DEFAULT__\' !== $value) {\n            $this->options[$key]    = $value;\n        }\n        return $curr;\n    }\n\n    /**\n     * Performs a POST.\n     *\n     * @param $url\n     * @param $fields\n     * @return mixed\n     */\n    public function post($url, array $fields) {\n        $this->config([\n            CURLOPT_POST       => true,\n            CURLOPT_POSTFIELDS => http_build_query($fields),\n        ]);\n        return $this->perform($url);\n    }\n\n    /**\n     * Performs a GET.\n     *\n     * @param       $url\n     * @param array $fields\n     * @return mixed\n     */\n    public function get($url, array $fields = []) {\n        $this->config([ CURLOPT_POST => false ]);\n        if (empty($fields)) {\n            $query = \'\';\n        } else {\n            $query = \'?\' . http_build_query($fields);\n        }\n        return $this->perform($url . $query);\n    }\n}\n\nNow to scrape FooSite:\n/* WWW_FOO_COM requires username and password to construct */\n\nclass WWW_FOO_COM_Browser extends Browser\n{\n    private $loggedIn   = false;\n\n    public function __construct($username, $password) {\n        parent::__construct(\'http://www.foo.bar.baz\', [\n            \'username\'  => $username,\n            \'password\'  => $password,\n            \'waitTime\'  => 250000,\n            \'userAgent\' => \'FooScraper\',\n            \'cache\'     => true\n        ]);\n        // Open the session\n        $this->get(\'/\');\n        // Navigate to the login page\n        $this->get(\'/login.do\');\n    }\n\n    /**\n     * Perform login.\n     */\n    public function login() {\n        $response = $this->post(\n            \'/ajax/loginPerform\',\n            [\n                \'j_un\'    => $this->option(\'username\'),\n                \'j_pw\'    => $this->option(\'password\'),\n            ]\n        );\n        // TODO: verify that response is OK.\n        // if (!strstr($response, ""Welcome "" . $this->option(\'username\'))\n        //     throw new \\Exception(""Bad username or password"")\n        $this->loggedIn = true;\n        return true;\n    }\n\n    public function scrape($entry) {\n        // We could implement caching to avoid scraping the same entry\n        // too often. Save $data into path(""entry-"" . md5($entry))\n        // and verify the filemtime of said file, is it newer than time()\n        // minus, say, 86400 seconds? If yes, return file_get_content and\n        // leave remote site alone.\n        $data = $this->get(\n            \'/foobars/baz.do\',\n            [\n                \'ticker\' => $entry\n            ]\n        );\n        return $data;\n    }\n\nNow the actual scraping code would be:\n    $scraper = new WWW_FOO_COM_Browser(\'lserni\', \'mypassword\');\n    if (!$scraper->login()) {\n        throw new \\Exception(""bad user or pass"");\n    }\n    // www.foo.com is a ticker site, we need little info for each\n    // Other examples might be much more complex.\n    $entries = [\n        \'APPL\', \'MSFT\', \'XKCD\'\n    ];\n    foreach ($entries as $entry) {\n        $html = $scraper->scrape($entry);\n        // Parse HTML\n    }\n\nMandatory notice: use a suitable parser to get data from raw HTML.\n']",https://stackoverflow.com/questions/13210140/how-can-i-scrape-website-content-in-php-from-a-website-that-requires-a-cookie-lo,web-scraping
How to scrape a public tableau dashboard? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



Every day I need to downlaod the data available on a public Tableau dashboard. After defining the parameters of interest (time series frequency, time series interval, etc) the dashboard allows you to download the series. 
My life would be reasonably easier if I could automate the download of these series to a database using Python or R. I've already tried to analyze the requests made on the page but I couldn't get much further. Is there any way to automate this process?
The dashboard: https://tableau.ons.org.br/t/ONS_Publico/views/DemandaMxima/HistricoDemandaMxima?:embed=y&:showAppBanner=false&:showShareOptions=true&:display_count=no&:showVizHome=no
",7k,"
            4
        ","['\nEdit\nI\'ve made a tableau scraper library to extract the data from Tableau worksheets\nYou can get the data from worksheets in a pandas dataframe directly. Also, the parametered values are supported.\nThe following example get the data from worksheet Simples Demanda Máxima Ano, then switch to daily mode, shows the worksheet Simples Demanda Máxima Semana Dia data and then set start date to 01/01/2017 :\nfrom tableauscraper import TableauScraper as TS\n\nurl = ""https://tableau.ons.org.br/t/ONS_Publico/views/DemandaMxima/HistricoDemandaMxima""\n\nts = TS()\nts.loads(url)\nwb = ts.getWorkbook()\n\n# show dataframe with yearly data\nws = wb.getWorksheet(""Simples Demanda Máxima Ano"")\nprint(ws.data)\n\n# switch to daily\nwb = wb.setParameter(""Escala de Tempo DM Simp 4"", ""Dia"")\n\n# show dataframe with daily data\nws = wb.getWorksheet(""Simples Demanda Máxima Semana Dia"")\nprint(ws.data)\n\n# switch to daily\nwb = wb.setParameter(\n    ""Início Primeiro Período DM Simp 4"", ""01/01/2017"")\n\n# show dataframe with daily data from 01/01/2017\nws = wb.getWorksheet(""Simples Demanda Máxima Semana Dia"")\nprint(ws.data)\n\n\nTry this on repl.it\n\nOriginal post\nThis answer is similar to this one but the initial URL page and tableau base URL differ. The process/algo remains the same essentially but I will details the steps :\nThe graphic is generated in JS from the result of an API :\nPOST https://tableau.ons.org.br/ROOT_PATH/bootstrapSession/sessions/SESSION_ID\n\nThe SESSION_ID parameter is located (among other things) in tsConfigContainer textarea in the URL used to build the iframe.\nStarting from https://tableau.ons.org.br/t/ONS_Publico/views/DemandaMxima/HistricoDemandaMxima?:embed=y&:showAppBanner=false&:showShareOptions=true&:display_count=no&:showVizHome=no :\n\nthere is a textarea with id tsConfigContainer with a bunch of json values\nextract the session_id and root path (vizql_root)\nmake a POST on https://tableau.ons.org.br/ROOT_PATH/bootstrapSession/sessions/SESSION_ID with the sheetId as form data\nextract the json from the result (result is not json)\n\nCode :\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport re\n\nurl = ""https://tableau.ons.org.br/t/ONS_Publico/views/DemandaMxima/HistricoDemandaMxima""\n\nr = requests.get(\n    url,\n    params= {\n        "":embed"":""y"",\n        "":showAppBanner"":""false"",\n        "":showShareOptions"":""true"",\n        "":display_count"":""no"",\n        ""showVizHome"": ""no""\n    }\n)\nsoup = BeautifulSoup(r.text, ""html.parser"")\n\ntableauData = json.loads(soup.find(""textarea"",{""id"": ""tsConfigContainer""}).text)\n\ndataUrl = f\'https://tableau.ons.org.br{tableauData[""vizql_root""]}/bootstrapSession/sessions/{tableauData[""sessionid""]}\'\n\nr = requests.post(dataUrl, data= {\n    ""sheet_id"": tableauData[""sheetId""],\n})\n\ndataReg = re.search(\'\\d+;({.*})\\d+;({.*})\', r.text, re.MULTILINE)\ninfo = json.loads(dataReg.group(1))\ndata = json.loads(dataReg.group(2))\n\nprint(data[""secondaryInfo""][""presModelMap""][""dataDictionary""][""presModelHolder""][""genDataDictionaryPresModel""][""dataSegments""][""0""][""dataColumns""])\n\n']",https://stackoverflow.com/questions/62095206/how-to-scrape-a-public-tableau-dashboard,web-scraping
Selenium Python: How to web scrape the element text,"
I am trying to webscrap data from roulette game.
While trying to
find element by class name (roulette_round_result-position__text)

I am getting this output:
<selenium.webdriver.remote.webelement.WebElement (session=""d4f20fd17bf4037ed8cf50b00e844a7f"", element=""f12cf837-6c77-4c90-9da2-7b5fb9da9e5d"")>

Any idea how to scrap this value? (In this case number 2)
My code:
number_1=0
    while number_1 == 0:
        try:
            number_1 = self.driver.find_element_by_class_name('roulette-round-result-position__text')
        except:
            pass

Screen shot from DevTools:

",581,"
            -1
        ","['\nYou are printing the WebElement. Hence you see the output as:\n<selenium.webdriver.remote.webelement.WebElement (session=""d4f20fd17bf4037ed8cf50b00e844a7f"", element=""f12cf837-6c77-4c90-9da2-7b5fb9da9e5d"")>\n\nInstead you may like to print the text within the element as:\nnumber_1 = self.driver.find_element_by_class_name(\'roulette-round-result-position__text\')\nprint(number_1.text)\n\nor\nprint(self.driver.find_element_by_class_name(\'roulette-round-result-position__text\').text)\n\n']",https://stackoverflow.com/questions/70385960/selenium-python-how-to-web-scrape-the-element-text,web-scraping
"Selenium-Debugging: Element is not clickable at point (X,Y)","
I try to scrape this site by Selenium.
I want to click in ""Next Page"" buttom, for this I do:
 driver.find_element_by_class_name('pagination-r').click()

it works for many pages but not for all, I got this error
WebDriverException: Message: Element is not clickable at point (918, 13). Other element would receive the click: <div class=""linkAuchan""></div>

always for this page 
I read this question 
and I tried this 
driver.implicitly_wait(10)
el = driver.find_element_by_class_name('pagination-r')
action = webdriver.common.action_chains.ActionChains(driver)
action.move_to_element_with_offset(el, 918, 13)
action.click()
action.perform()

but I got the same error
",80k,"
            70
        ","['\nAnother element is covering the element you are trying to click. You could use execute_script() to click on this.\nelement = driver.find_element_by_class_name(\'pagination-r\')\ndriver.execute_script(""arguments[0].click();"", element)\n\n', '\nI had a similar issue where using ActionChains was not solving my error:\nWebDriverException: Message: unknown error: Element is not clickable at point (5\n74, 892)\nI found a nice solution if you dont want to use execute_script:\n    from selenium.webdriver.common.keys import Keys #need to send keystrokes\n\n    inputElement = self.driver.find_element_by_name(\'checkout\')\n\n    inputElement.send_keys(""\\n"") #send enter for links, buttons\n\nor\n    inputElement.send_keys(Keys.SPACE) #for checkbox etc\n\n', '\nBecause element is not visible on the browser, first you need to scroll down to the element\nthis can be performed by executing javascript.\nelement = driver.find_element_by_class_name(\'pagination-r\')\ndriver.execute_script(""arguments[0].scrollIntoView();"", element)\ndriver.execute_script(""arguments[0].click();"", element)\n\n', '\nI have written logic to handle these type of exception . \n   def find_element_click(self, by, expression, search_window=None, timeout=32, ignore_exception=None,\n                       poll_frequency=4):\n    """"""It find the element and click then  handle all type of exception during click\n\n    :param poll_frequency:\n    :param by:\n    :param expression:\n    :param timeout:\n    :param ignore_exception:list It is a list of exception which is need to ignore.\n    :return:\n    """"""\n    if ignore_exception is None:\n        ignore_exception = []\n\n    ignore_exception.append(NoSuchElementException)\n    if search_window is None:\n        search_window = self.driver\n\n    end_time = time.time() + timeout\n    while True:\n        try:\n            web_element = search_window.find_element(by=by, value=expression)\n            web_element.click()\n            return True\n        except tuple(ignore_exception) as e:\n            self.logger.debug(str(e))\n            if time.time() > end_time:\n                self.logger.exception(e)\n                time.sleep(poll_frequency)\n                break\n        except Exception as e:\n            raise\n    return False\n\n', '\nIf you are receiving an element not clickable error, even after using wait on the element, try one of these workarounds:\n\nUse Action to move to the location of element and then run perform on action\n\nWebElement element = driver.findElement(By(""element_path""));\nActions actions = new Actions(driver);\nactions.moveToElement(element).click().perform();`\n\n\nCheck for an overlay or spinner on the element and wait for its invisibility\n\nBy spinnerimg = By.id(""spinner ID"");\nWebDriverWait wait = new WebDriverWait(driver, timeOutInSeconds);\nwait.until(ExpectedConditions.invisibilityOfElementLocated(spinnerimg ));\n\nHope this helps\n', '\nUse explicit wait instead of implicit.\n new WebDriverWait(TestingSession.Browser.WebDriver, TimeSpan.FromSeconds(10)).Until(ExpectedConditions.ElementExists((By.ClassName(""pagination-r\'"")))); \n\n', ""\nI had the similar issue with Chrome driver, changing the PageLoadStrategy of chromeOptions from 'Eager' to Normal fixed my problem.\nchromeOptions.PageLoadStrategy = PageLoadStrategy.Normal;\n\n""]",https://stackoverflow.com/questions/37879010/selenium-debugging-element-is-not-clickable-at-point-x-y,web-scraping
Click a Button in Scrapy,"
I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking).
I found out that Scrapy can handle forms (like logins) as shown here. But the problem is that there is no form to fill out, so it's not exactly what I need.
How can I simply click a button, which then shows the information I need?
Do I have to use an external library like mechanize or lxml?
",76k,"
            67
        ","[""\nScrapy cannot interpret javascript.\nIf you absolutely must interact with the javascript on the page, you want to be using Selenium.\nIf using Scrapy, the solution to the problem depends on what the button is doing.\nIf it's just showing content that was previously hidden, you can scrape the data without a problem, it doesn't matter that it wouldn't appear in the browser, the HTML is still there.\nIf it's fetching the content dynamically via AJAX when the button is pressed, the best thing to do is to view the HTTP request that goes out when you press the button using a tool like Firebug. You can then just request the data directly from that URL.\n\nDo I have to use an external library like mechanize or lxml?\n\nIf you want to interpret javascript, yes you need to use a different library, although neither of those two fit the bill. Neither of them know anything about javascript. Selenium is the way to go.\nIf you can give the URL of the page you're working on scraping I can take a look.\n"", '\nSelenium browser provide very nice solution. Here is an example (pip install -U selenium):\nfrom selenium import webdriver\n\nclass northshoreSpider(Spider):\n    name = \'xxx\'\n    allowed_domains = [\'www.example.org\']\n    start_urls = [\'https://www.example.org\']\n\n    def __init__(self):\n        self.driver = webdriver.Firefox()\n\n    def parse(self,response):\n            self.driver.get(\'https://www.example.org/abc\')\n\n            while True:\n                try:\n                    next = self.driver.find_element_by_xpath(\'//*[@id=""BTN_NEXT""]\')\n                    url = \'http://www.example.org/abcd\'\n                    yield Request(url,callback=self.parse2)\n                    next.click()\n                except:\n                    break\n\n            self.driver.close()\n\n    def parse2(self,response):\n        print \'you are here!\'\n\n', '\nTo properly and fully use JavaScript you need a full browser engine and this is possible only with Watir/WatiN/Selenium etc.\n', ""\nAlthough it's an old thread I've found quite useful to use Helium (built on top of Selenium) for this purpose and far more easier/simpler than using Selenium. It will be something like the following:\nfrom helium import *\n\nstart_firefox('your_url')\ns = S('path_to_your_button')\nclick(s)\n...\n\n\n""]",https://stackoverflow.com/questions/6682503/click-a-button-in-scrapy,web-scraping
Crawling multiple URLs in a loop using Puppeteer,"
I have an array of URLs to scrape data from:
urls = ['url','url','url'...]

This is what I'm doing:
urls.map(async (url)=>{
  await page.goto(url);
  await page.waitForNavigation({ waitUntil: 'networkidle' });
})

This seems to not wait for page load and visits all the URLs quite rapidly (I even tried using page.waitFor).
I wanted to know if am I doing something fundamentally wrong or this type of functionality is not advised/supported.
",27k,"
            27
        ","[""\nmap, forEach, reduce, etc, does not wait for the asynchronous operation within them, before they proceed to the next element of the iterator they are iterating over.\nThere are multiple ways of going through each item of an iterator synchronously while performing an asynchronous operation, but the easiest in this case I think would be to simply use a normal for operator, which does wait for the operation to finish.\nconst urls = [...]\n\nfor (let i = 0; i < urls.length; i++) {\n    const url = urls[i];\n    await page.goto(`${url}`);\n    await page.waitForNavigation({ waitUntil: 'networkidle2' });\n}\n\nThis would visit one url after another, as you are expecting. If you are curious about iterating serially using await/async, you can have a peek at this answer: https://stackoverflow.com/a/24586168/791691\n"", '\nThe accepted answer shows how to serially visit each page one at a time. However, you may want to visit multiple pages simultaneously when the task is embarrassingly parallel, that is, scraping a particular page isn\'t dependent on data extracted from other pages.\nA tool that can help achieve this is Promise.allSettled which lets us fire off a bunch of promises at once, determine which were successful and harvest results.\nFor a basic example, let\'s say we want to scrape usernames for Stack Overflow users given a series of ids.\nSerial code:\nconst puppeteer = require(""puppeteer""); // ^14.3.0\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch({dumpio: false});\n  const [page] = await browser.pages();\n  const baseURL = ""https://stackoverflow.com/users"";\n  const startId = 6243352;\n  const qty = 5;\n  const usernames = [];\n\n  for (let i = startId; i < startId + qty; i++) {\n    await page.goto(`${baseURL}/${i}`, {\n      waitUntil: ""domcontentloaded""\n    });\n    const sel = "".flex--item.mb12.fs-headline2.lh-xs"";\n    const el = await page.waitForSelector(sel);\n    usernames.push(await el.evaluate(el => el.textContent.trim()));\n  }\n\n  console.log(usernames);\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser.close())\n;\n\nParallel code:\nconst puppeteer = require(""puppeteer"");\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch();\n  const [page] = await browser.pages();\n  const baseURL = ""https://stackoverflow.com/users"";\n  const startId = 6243352;\n  const qty = 5;\n\n  const usernames = (await Promise.allSettled(\n    [...Array(qty)].map(async (_, i) => {\n      const page = await browser.newPage();\n      await page.goto(`${baseURL}/${i + startId}`, {\n        waitUntil: ""domcontentloaded""\n      });\n      const sel = "".flex--item.mb12.fs-headline2.lh-xs"";\n      const el = await page.waitForSelector(sel);\n      const text = await el.evaluate(el => el.textContent.trim());\n      await page.close();\n      return text;\n    })))\n    .filter(e => e.status === ""fulfilled"")\n    .map(e => e.value)\n  ;\n  console.log(usernames);\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser.close())\n;\n\nRemember that this is a technique, not a silver bullet that guarantees a speed increase on all workloads. It will take some experimentation to find the optimal balance between the cost of creating more pages versus the parallelization of network requests on a given particular task and system.\nThe example here is contrived since it\'s not interacting with the page dynamically, so there\'s not as much room for gain as in a typical Puppeteer use case that involves network requests and blocking waits per page.\nOf course, beware of rate limiting and any other restrictions imposed by sites (running the code above may anger Stack Overflow\'s rate limiter).\nFor tasks where creating a page per task is prohibitively expensive or you\'d like to set a cap on parallel request dispatches, consider using a task queue or combining serial and parallel code shown above to send requests in chunks. This answer shows a generic pattern for this agnostic of Puppeteer.\nThese patterns can be extended to handle the case when certain pages depend on data from other pages, forming a dependency graph.\nSee also Using async/await with a forEach loop which explains why the original attempt in this thread using map fails to wait for each promise.\n', ""\nIf you find that you are waiting on your promise indefinitely, the proposed solution is to use the following:\nconst urls = [...]\n\nfor (let i = 0; i < urls.length; i++) {\n    const url = urls[i];\n    const promise = page.waitForNavigation({ waitUntil: 'networkidle' });\n    await page.goto(`${url}`);\n    await promise;\n}\n\nAs referenced from this github issue\n"", ""\nBest way I found to achieve this. \n const puppeteer = require('puppeteer');\n(async () => {\n    const urls = ['https://www.google.com/', 'https://www.google.com/']\n    for (let i = 0; i < urls.length; i++) {\n\n        const url = urls[i];\n        const browser = await puppeteer.launch({ headless: false });\n        const page = await browser.newPage();\n        await page.goto(`${url}`, { waitUntil: 'networkidle2' });\n        await browser.close();\n\n    }\n})();\n\n"", '\nSomething no one else mentions is that if you are fetching multiple pages using the same page object it is crucial that you set its timeout to 0. Otherwise, once it has fetched the default 30 seconds worth of pages, it will timeout.\n  const browser = await puppeteer.launch();\n  const page = await browser.newPage();\n  page.setDefaultNavigationTimeout(0);\n\n']",https://stackoverflow.com/questions/46293216/crawling-multiple-urls-in-a-loop-using-puppeteer,web-scraping
How to get text from span tag in BeautifulSoup,"
I have links looks like this
<div class=""systemRequirementsMainBox"">
<div class=""systemRequirementsRamContent"">
<span title=""000 Plus Minimum RAM Requirement"">1 GB</span> </div>

I'm trying to get 1 GB from there. I tried
tt  = [a['title'] for a in soup.select("".systemRequirementsRamContent span"")]
for ram in tt:
    if ""RAM"" in ram.split():
        print (soup.string)

It outputs None.
I tried a['text'] but it gives me KeyError. How can I fix this and what is my mistake?
",91k,"
            25
        ","['\nYou can use a css selector, pulling the span you want using the title text :\nsoup = BeautifulSoup(""""""<div class=""systemRequirementsMainBox"">\n<div class=""systemRequirementsRamContent"">\n<span title=""000 Plus Minimum RAM Requirement"">1 GB</span> </div>"""""", ""xml"")\n\nprint(soup.select_one(""span[title*=RAM]"").text)\n\nThat finds the span with a title attribute that contains RAM, it is equivalent to saying in python, if ""RAM"" in span[""title""].\nOr using find with re.compile\nimport re\nprint(soup.find(""span"", title=re.compile(""RAM"")).text)\n\nTo get all the data:\nfrom bs4 import BeautifulSoup \nr  = requests.get(""http://www.game-debate.com/games/index.php?g_id=21580&game=000%20Plus"").content\n\nsoup = BeautifulSoup(r,""lxml"")\ncont = soup.select_one(""div.systemRequirementsRamContent"")\nram = cont.select_one(""span"")\nprint(ram[""title""], ram.text)\nfor span in soup.select(""div.systemRequirementsSmallerBox.sysReqGameSmallBox span""):\n        print(span[""title""],span.text)\n\nWhich will give you:\n000 Plus Minimum RAM Requirement 1 GB\n000 Plus Minimum Operating System Requirement Win Xp 32\n000 Plus Minimum Direct X Requirement DX 9\n000 Plus Minimum Hard Disk Drive Space Requirement 500 MB\n000 Plus GD Adjusted Operating System Requirement Win Xp 32\n000 Plus GD Adjusted Direct X Requirement DX 9\n000 Plus GD Adjusted Hard Disk Drive Space Requirement 500 MB\n000 Plus Recommended Operating System Requirement Win Xp 32\n000 Plus Recommended Hard Disk Drive Space Requirement 500 MB\n\n', '\nI tried to extract the text inside all the span tags inside the HTML document using find_all() function from bs4 (BeautifulSoup):\nfrom bs4 import BeautifulSoup\nimport requests\nurl=""YOUR_URL_HERE""\nresponse=requests.get(url)\nsoup=BeautifulSoup(response.content,html5lib)\nspans=soup.find_all(\'span\',""ENTER_Css_CLASS_HERE"")\nfor span in spans:\n  print(span.text)\n\n', '\nYou can simply use span tag in BeautifulSoup or you can include other attributes like class, title along with the span tag.\nfrom BeautifulSoup import BeautifulSoup as BSHTML\n\nhtmlText = """"""<div class=""systemRequirementsMainBox"">\n<div class=""systemRequirementsRamContent"">\n<span title=""000 Plus Minimum RAM Requirement"">1 GB</span> </div>""""""\n\nsoup = BSHTML(htmlText)\nspans = soup.findAll(\'span\')\n# spans = soup.findAll(\'span\', attrs = {\'class\' : \'your-class-name\'}) # or span by class name\n# spans = soup.findAll(\'span\', attrs = {\'title\' : \'000 Plus Minimum RAM Requirement\'}) # or span with a title\nfor span in spans:\n    print span.text\n\n', '\nYou could solve this with just a couple lines of gazpacho:\nfrom gazpacho import Soup\n\nhtml = """"""\\\n<div class=""systemRequirementsMainBox"">\n<div class=""systemRequirementsRamContent"">\n<span title=""000 Plus Minimum RAM Requirement"">1 GB</span> </div>\n""""""\n\nsoup = Soup(html)\nsoup.find(""span"", {""title"": ""Minimum RAM Requirement""}).text\n# \'1 GB\'\n\n']",https://stackoverflow.com/questions/38133759/how-to-get-text-from-span-tag-in-beautifulsoup,web-scraping
How to use Beautiful Soup to extract string in <script> tag?,"
In a given .html page, I have a script tag like so:
     <script>jQuery(window).load(function () {
  setTimeout(function(){
    jQuery(""input[name=Email]"").val(""name@email.com"");
  }, 1000);
});</script>

How can I use Beautiful Soup to extract the email address?
",43k,"
            22
        ","['\nTo add a bit more to the @Bob\'s answer and assuming you need to also locate the script tag in the HTML which may have other script tags.\nThe idea is to define a regular expression that would be used for both locating the element with BeautifulSoup and extracting the email value:\nimport re\n\nfrom bs4 import BeautifulSoup\n\n\ndata = """"""\n<body>\n    <script>jQuery(window).load(function () {\n      setTimeout(function(){\n        jQuery(""input[name=Email]"").val(""name@email.com"");\n      }, 1000);\n    });</script>\n</body>\n""""""\npattern = re.compile(r\'\\.val\\(""([^@]+@[^@]+\\.[^@]+)""\\);\', re.MULTILINE | re.DOTALL)\nsoup = BeautifulSoup(data, ""html.parser"")\n\nscript = soup.find(""script"", text=pattern)\nif script:\n    match = pattern.search(script.text)\n    if match:\n        email = match.group(1)\n        print(email)\n\nPrints: name@email.com.\nHere we are using a simple regular expression for the email address, but we can go further and be more strict about it but I doubt that would be practically necessary for this problem.\n', '\nI ran into a similar problem and the issue seems to be that calling script_tag.text returns an empty string. Instead, you have to call script_tag.string. Maybe this changed in some version of BeautifulSoup?\nAnyway, @alecxe\'s answer didn\'t work for me, so I modified their solution:\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndata = """"""\n<body>\n    <script>jQuery(window).load(function () {\n      setTimeout(function(){\n        jQuery(""input[name=Email]"").val(""name@email.com"");\n      }, 1000);\n    });</script>\n</body>\n""""""\nsoup = BeautifulSoup(data, ""html.parser"")\n\nscript_tag = soup.find(""script"")\nif script_tag:\n  # contains all of the script tag, e.g. ""jQuery(window)...""\n  script_tag_contents = script_tag.string\n\n  # from there you can search the string using a regex, etc.\n  email = re.search(r\'\\.+val\\(""(.+)""\\);\', script_tag_contents).group(1)\n  print(email)\n\nThis prints name@email.com.\n', '\nnot possible using only BeautifulSoup, but you can do it for example with BS + regular expressions\nimport re\nfrom bs4 import BeautifulSoup as BS\n\nhtml = """"""<script> ... </script>""""""\n\nbs = BS(html)\n\ntxt = bs.script.get_text()\n\nemail = re.match(r\'.+val\\(""(.+?)""\\);\', txt).group(1)\n\nor like this:\n...\n\nemail = txt.split(\'.val(""\')[1].split(\'"");\')[0]\n\n', '\nIn order to get the string inside the <script> tag, you can use .contents or .string.\ndata = """"""\n   <body>\n<script>jQuery(window).load(function () {\n  setTimeout(function(){\n    jQuery(""input[name=Email]"").val(""name@email.com"");\n  }, 1000);\n});</script>\n </body>\n    """"""\nsoup = BeautifulSoup(data, ""html.parser"")\n\nscript = soup.find(""script"")\ninner_text_with_string = script.string\ninner_text_with_content = script.contents[0]\n\nprint(\'inner_text_with_string\', inner_text_with_string)\nprint(\'inner_text_with_content\', inner_text_with_content)\n\n', '\nYou could solve this with just a couple of lines of gazpacho and .split, no regex required!\nfrom gazpacho import Soup\n\nhtml = """"""\\\n<script>jQuery(window).load(function () {\n  setTimeout(function(){\n    jQuery(""input[name=Email]"").val(""name@email.com"");\n  }, 1000);\n});</script>\n""""""\n\nsoup = Soup(html)\nstring = soup.find(""script"").text\nstring.split("".val(\\"""")[-1].split(""\\"");"")[0]\n\nWhich would output:\n\'name@email.com\'\n\n']",https://stackoverflow.com/questions/38547569/how-to-use-beautiful-soup-to-extract-string-in-script-tag,web-scraping
How to deal with the captcha when doing Web Scraping in Puppeteer?,"
I'm using Puppeteer for Web Scraping and I have just noticed that sometimes, the website I'm trying to scrape asks for a captcha due to the amount of visits I'm doing from my computer. The captcha form looks like this one:

So, I would need help about how to handle this. I have been thinking about sending the captcha form to the client-side since I use Express and EJS in order to send the values to my index website, but I don't know if Puppeteer can send something like that.
Any ideas?
",42k,"
            20
        ","['\nThis is a reCAPTCHA (version 2, check out demos here), which is shown to you as the owner of the page does not want you to automatically crawl the page.\nYour options are the following:\nOption 1: Stop crawling or try to use an official API\nAs the owner of the page does not want you to crawl that page, you could simply respect that decision and stop crawling. Maybe there is a documented API that you can use.\nOption 2: Automate/Outsource the captcha solving\nThere is an entire industry which has people (often in developing countries) filling out captchas for other people\'s bots. I will not link to any particular site, but you can check out the other answer from Md. Abu Taher for more information on the topic or search for captcha solver.\nOption 3: Solve the captcha yourself\nFor this, let me explain how reCAPTCHA works and what happens when you visit a page using it.\n\nHow reCAPTCHA (v2) works\nEach page has an ID, which you can check by looking at the source code, example:\n<div class=""g-recaptcha form-field"" data-sitekey=""ID_OF_THE_WEBSITE_LONG_RANDOM_STRING""></div>\n\nWhen the reCAPTCHA code is loaded it will add a response textarea to the form with no value. It will look like this:\n<textarea id=""g-recaptcha-response"" name=""g-recaptcha-response"" class=""g-recaptcha-response"" style=""... display: none;""></textarea>\n\nAfter you solved the challenge, reCAPTCHA will add a very long string to this text field (which can then later be checked by the server/reCAPTCHA service in the backend) when the form is submitted.\n\nHow to solve the captcha yourself\nBy copying the value of the textarea field you can transfer the ""solved challenge"" from one browser to another (this is also what the solving services to for you). The full process looks like this:\n\nDetect if the page uses reCAPTCHA (e.g. check for .g-recaptcha) in the ""crawling"" browser\nOpen a second browser in non-headless mode with the same URL\nSolve the captcha yourself\nRead the value from: document.querySelector(\'#g-recaptcha-response\').value\nPut that value into the first browser: document.querySelector(\'#g-recaptcha-response\').value = \'...\'\nSubmit the form\n\nFurther information/reading\nThere is not much public information from Google how exactly reCAPTCHA works as this is a cat-and-mouse game between bot creators and Google detection algorithms, but there are some resources online with more information:\n\nOfficial docs from Google: Obviously, they just explain the basics and not how it works ""in the back""\nInsideReCaptcha: This is a project from 2014 which tries to ""reverse-engineer"" reCAPTCHA. Although this is quite old, there is still a lot of useful information on the page.\nAnother question on stackoverflow: This question contains some useful information about reCAPTCHA, but also many speculative (and very likely) outdated approaches on how to fool a reCAPTCHA.\n\n', ""\nYou should use combination of following:\n\nUse an API if the target website provides that. It's the most legal way.\nIncrease wait time between scraping request, do not send mass request to the server.\nChange/rotate IP frequently.\nChange user agent, browser viewport size and fingerprint.\nUse third party solutions for captcha.\nResolve the captcha by yourself, check the answer by Thomas Dondorf. Basically you need to wait for the captcha to appear on another browser, solve it from there. Third party solutions does this for you. \n\n\nDisclaimer: Do not use anti-captcha plugins/services to misuse resources. Resources are expensive.\n\nBasically the idea is to use anti-captcha services like (2captcha) to deal with persisting recaptcha. \nYou can use this plugin called puppeteer-extra-plugin-recaptcha by berstend. \n// puppeteer-extra is a drop-in replacement for puppeteer,\n// it augments the installed puppeteer with plugin functionality\nconst puppeteer = require('puppeteer-extra')\n\n// add recaptcha plugin and provide it your 2captcha token\n// 2captcha is the builtin solution provider but others work as well.\nconst RecaptchaPlugin = require('puppeteer-extra-plugin-recaptcha')\npuppeteer.use(\n  RecaptchaPlugin({\n    provider: { id: '2captcha', token: 'XXXXXXX' },\n    visualFeedback: true // colorize reCAPTCHAs (violet = detected, green = solved)\n  })\n)\n\nAfterwards you can run the browser as usual. It will pick up any captcha on the page and attempt to resolve it. You have to find the submit button which varies from site to site if it exists.\n// puppeteer usage as normal\npuppeteer.launch({ headless: true }).then(async browser =>""]",https://stackoverflow.com/questions/55493536/how-to-deal-with-the-captcha-when-doing-web-scraping-in-puppeteer,web-scraping
CasperJS passing data back to PHP,"
CasperJS is being called by PHP using an exec() command. After CasperJS does its work such as retrieving parts of a webpage, how can the retrieved data be returned back to PHP?
",11k,"
            9
        ","['\nI think the best way to transfer data from CasperJS to another language such as PHP is running CasperJS script as a service. Because CasperJS has been written over PhantomJS, CasperJS can use an embedded web server module of PhantomJS called Mongoose.\nFor information about how works the embedded web server see here\nHere an example about how a CasperJS script can start a web server.\n//define ip and port to web service\nvar ip_server = \'127.0.0.1:8585\';\n\n//includes web server modules\nvar server = require(\'webserver\').create();\n\n//start web server\nvar service = server.listen(ip_server, function(request, response) {\n\n    var links = [];\n    var casper = require(\'casper\').create();\n\n    function getLinks() {\n        var links = document.querySelectorAll(\'h3.r a\');\n        return Array.prototype.map.call(links, function(e) {\n            return e.getAttribute(\'href\')\n        });\n    }\n\n    casper.start(\'http://google.fr/\', function() {\n        // search for \'casperjs\' from google form\n        this.fill(\'form[action=""/search""]\', { q: \'casperjs\' }, true);\n    });\n\n    casper.then(function() {\n        // aggregate results for the \'casperjs\' search\n        links = this.evaluate(getLinks);\n        // now search for \'phantomjs\' by filling the form again\n        this.fill(\'form[action=""/search""]\', { q: \'phantomjs\' }, true);\n    });\n\n    casper.then(function() {\n        // aggregate results for the \'phantomjs\' search\n        links = links.concat(this.evaluate(getLinks));\n    });\n\n    //\n    casper.run(function() {\n            response.statusCode = 200;\n            //sends results as JSON object\n            response.write(JSON.stringify(links, null, null));\n            response.close();              \n    });\n\n});\nconsole.log(\'Server running at http://\' + ip_server+\'/\');\n\n', ""\nYou can redirect output from stdout to an array.\nOn this page it says you can do:  \nstring exec ( string $command [, array &$output [, int &$return_var ]] )\n\nIt goes on to say: \n\nIf the output argument is present, then the specified array will be filled with every line of output from the command. \n\nSo basically you can do exec('casperjs command here, $array_here);\n""]",https://stackoverflow.com/questions/15852987/casperjs-passing-data-back-to-php,web-scraping
R web scraping across multiple pages,"
I am working on a web scraping program to search for specific wines and return a list of local wines of that variety. The problem I am having is multiple page results. The code below is a basic example of what I am working with 
url2 <- ""http://www.winemag.com/?s=washington+merlot&search_type=reviews""
htmlpage2 <- read_html(url2)
names2 <- html_nodes(htmlpage2, "".review-listing .title"")
Wines2 <- html_text(names2)

For this specific search there are 39 pages of results. I know the url changes to http://www.winemag.com/?s=washington%20merlot&drink_type=wine&page=2, but is there an easy way to make the code loop through all the returned pages and compile the results from all 39 pages into a single list? I know I can manually do all the urls, but that seems like overkill. 
",16k,"
            8
        ","['\nYou can do something similar with purrr::map_df() as well if you want all the info as a data.frame:\nlibrary(rvest)\nlibrary(purrr)\n\nurl_base <- ""http://www.winemag.com/?s=washington merlot&drink_type=wine&page=%d""\n\nmap_df(1:39, function(i) {\n\n  # simple but effective progress indicator\n  cat(""."")\n\n  pg <- read_html(sprintf(url_base, i))\n\n  data.frame(wine=html_text(html_nodes(pg, "".review-listing .title"")),\n             excerpt=html_text(html_nodes(pg, ""div.excerpt"")),\n             rating=gsub("" Points"", """", html_text(html_nodes(pg, ""span.rating""))),\n             appellation=html_text(html_nodes(pg, ""span.appellation"")),\n             price=gsub(""\\\\$"", """", html_text(html_nodes(pg, ""span.price""))),\n             stringsAsFactors=FALSE)\n\n}) -> wines\n\ndplyr::glimpse(wines)\n## Observations: 1,170\n## Variables: 5\n## $ wine        (chr) ""Charles Smith 2012 Royal City Syrah (Columbia Valley (WA)...\n## $ excerpt     (chr) ""Green olive, green stem and fresh herb aromas are at the ...\n## $ rating      (chr) ""96"", ""95"", ""94"", ""93"", ""93"", ""93"", ""93"", ""93"", ""93"", ""93""...\n## $ appellation (chr) ""Columbia Valley"", ""Columbia Valley"", ""Columbia Valley"", ""...\n## $ price       (chr) ""140"", ""70"", ""70"", ""20"", ""70"", ""40"", ""135"", ""50"", ""60"", ""3...\n\n', '\nYou can lapply across a vector of the URLs, which you can make by pasting the base URL to a sequence:\nlibrary(rvest)\n\nwines <- lapply(paste0(\'http://www.winemag.com/?s=washington%20merlot&drink_type=wine&page=\', 1:39),\n                function(url){\n                    url %>% read_html() %>% \n                        html_nodes("".review-listing .title"") %>% \n                        html_text()\n                })\n\nThe result will be returned in a list with an element for each page.\n']",https://stackoverflow.com/questions/36683510/r-web-scraping-across-multiple-pages,web-scraping
"Excel VBA ""Method 'Document' of object 'IWebBrowser2' failed""","
I'm trying to automate a form submission in Excel for work, and In have trouble with the basics. I keep getting the error message:

""Method 'Document' of object 'IWebBrowser2' failed""

With the code as is, and if I include the Or part in the waiting check, I get the error

""Automation Error The object invoked has disconnected from its clients.""

I'm not sure what to do here, I've searched all over for solutions. This code is intended to eventually do more than this, but it keeps failing on the first try to getElementsByTagName. 
Sub GoToWebsiteTest()
Dim appIE As Object 'Internet Explorer
Set appIE = Nothing
Dim objElement As Object
Dim objCollection As Object

If appIE Is Nothing Then Set appIE = CreateObject(""InternetExplorer.Application"")
sURL = *link*
With appIE
    .Visible = True
    .Navigate sURL
End With

Do While appIE.Busy ' Or appIE.ReadyState <> 4
    DoEvents
Loop

Set objCollection = appIE.Document.getElementsByTagName(""input"")

Set appIE = Nothing
End Sub

",42k,"
            5
        ","['\nI ran into this same issue a while back. Use internet explorer at a medium integrity level. InternetExplorer defaults to a low integrity level which, if you are doing this over a local intranet at work, sometimes will give the second error message you show above. \nClick here for more reading on this. I\'ve modified your code below. Please let me know if that helps.\nSub GoToWebsiteTest()\nDim appIE As InternetExplorerMedium\n\'Set appIE = Nothing\nDim objElement As Object\nDim objCollection As Object\n\nSet appIE = New InternetExplorerMedium\nsURL = ""http://example.com""\nWith appIE\n    .Navigate sURL\n    .Visible = True\nEnd With\n\nDo While appIE.Busy Or appIE.ReadyState <> 4\n    DoEvents\nLoop\n\nSet objCollection = appIE.Document.getElementsByTagName(""input"")\n\nSet appIE = Nothing\nEnd Sub\n\nRemember references for Microsoft Internet Controls, and depending on what you plan on doing further, Microsoft HTML Object Library\n', '\nNot exactly same as above code but somehow similar , the following code solved my problem:\nDo\nLoop Until ie.readystate = 3\nDo\nLoop Until ie.readystate = 4\n\nJust put it before the line you want to start working with the contents.\nTo get more information about how does it work you can check here\n', ""\nThe below method solved my problem for this error:\nClose all the explorer instances through 'Task manager' and try to run the code it will work.\n""]",https://stackoverflow.com/questions/30086425/excel-vba-method-document-of-object-iwebbrowser2-failed,web-scraping
Scraping dynamic content in a website [duplicate],"






This question already has answers here:
                        
                    



Scrape a dynamic website

                                (8 answers)
                            

Closed 5 months ago.



I need to scrape news announcements from this website, Link.
The announcements seem to be generated dynamically. They dont appear in the source. I usually use mechanize but I assume it wouldnt work. What can I do for this? I'm ok with python or perl.
",19k,"
            2
        ","[""\nIf the content is generated dynamically, you can use Windmill or Seleninum to drive the browser and get the data once it's been rendered.\nYou can find an example here.\n"", ""\nThe polite option would be to ask the owners of the site if they have an API which allows you access to their news stories.\nThe less polite option would be to trace the HTTP transactions that take place while the page is loading and work out which one is the AJAX call which pulls in the data.\nLooks like it's this one. But it looks like it might contain session data, so I don't know how long it will continue to work for.\n"", '\nThere\'s also WWW::Scripter ""For scripting web sites that have scripts"" . Never used it.\n', '\nIn python you can use urllib and urllib2 to connect to a website and collect data.  For example:\nfrom urllib2 import urlopen\nmyUrl = ""http://www.marketvectorsindices.com/#!News/List""\ninStream = urlopen(myUrl)\ninstream.read(1024) # etc, in a while loop\n# all your fun page parsing code (perhaps: import from xml.dom.minidom import parse)\n\n']",https://stackoverflow.com/questions/8323728/scraping-dynamic-content-in-a-website,web-scraping
Save and render a webpage with PhantomJS and node.js,"
I'm looking for an example of requesting a webpage, waiting for the JavaScript to render (JavaScript modifies the DOM), and then grabbing the HTML of the page.
This should be a simple example with an obvious use-case for PhantomJS. I can't find a decent example, the documentation seems to be all about command line use.
",63k,"
            62
        ","[""\nFrom your comments, I'd guess you have 2 options\n\nTry to find a phantomjs node module - https://github.com/amir20/phantomjs-node \nRun phantomjs as a child process inside node - http://nodejs.org/api/child_process.html\n\nEdit: \nIt seems the child process is suggested by phantomjs as a way of interacting with node, see faq - http://code.google.com/p/phantomjs/wiki/FAQ\nEdit:\nExample Phantomjs script for getting the pages HTML markup:\nvar page = require('webpage').create();  \npage.open('http://www.google.com', function (status) {\n    if (status !== 'success') {\n        console.log('Unable to access network');\n    } else {\n        var p = page.evaluate(function () {\n            return document.getElementsByTagName('html')[0].innerHTML\n        });\n        console.log(p);\n    }\n    phantom.exit();\n});\n\n"", ""\nWith v2 of phantomjs-node it's pretty easy to print the HTML after it has been processed. \nvar phantom = require('phantom');\n\nphantom.create().then(function(ph) {\n  ph.createPage().then(function(page) {\n    page.open('https://stackoverflow.com/').then(function(status) {\n      console.log(status);\n      page.property('content').then(function(content) {\n        console.log(content);\n        page.close();\n        ph.exit();\n      });\n    });\n  });\n});\n\nThis will show the output as it would have been rendered with the browser. \nEdit 2019: \nYou can use async/await:\nconst phantom = require('phantom');\n\n(async function() {\n  const instance = await phantom.create();\n  const page = await instance.createPage();\n  await page.on('onResourceRequested', function(requestData) {\n    console.info('Requesting', requestData.url);\n  });\n\n  const status = await page.open('https://stackoverflow.com/');\n  const content = await page.property('content');\n  console.log(content);\n\n  await instance.exit();\n})();\n\nOr if you just want to test, you can use npx\nnpx phantom@latest https://stackoverflow.com/\n\n"", ""\nI've used two different ways in the past, including the page.evaluate() method that queries the DOM that Declan mentioned. The other way I've passed info from the web page is to spit it out to console.log() from there, and in the phantomjs script use:\npage.onConsoleMessage = function (msg, line, source) {\n  console.log('console [' +source +':' +line +']> ' +msg);\n}\n\nI might also trap the variable msg in the onConsoleMessage and search for some encapsulate data. Depends on how you want to use the output.\nThen in the Nodejs script, you would have to scan the output of the Phantomjs script:\nvar yourfunc = function(...params...) {\n  var phantom = spawn('phantomjs', [...args]);\n  phantom.stdout.setEncoding('utf8');\n  phantom.stdout.on('data', function(data) {\n    //parse or echo data\n    var str_phantom_output = data.toString();\n    // The above will get triggered one or more times, so you'll need to\n    // add code to parse for whatever info you're expecting from the browser\n  });\n  phantom.stderr.on('data', function(data) {\n    // do something with error data\n  });\n  phantom.on('exit', function(code) {\n    if (code !== 0) {\n      // console.log('phantomjs exited with code ' +code);\n    } else {\n      // clean exit: do something else such as a passed-in callback\n    }\n  });\n}\n\nHope that helps some.\n"", '\nWhy not just use this ? \nvar page = require(\'webpage\').create();\npage.open(""http://example.com"", function (status)\n{\n    if (status !== \'success\') \n    {\n        console.log(\'FAIL to load the address\');            \n    } \n    else \n    {\n        console.log(\'Success in fetching the page\');\n        console.log(page.content);\n    }\n    phantom.exit();\n});\n\n', '\nLate update in case anyone stumbles on this question:\nA project on GitHub developed by a colleague of mine exactly aims at helping you do that: https://github.com/vmeurisse/phantomCrawl.\nIt still a bit young, it certainly is missing some documentation, but the example provided should help doing basic crawling.\n', ""\nHere's an old version that I use running node, express and phantomjs which saves out the page as a .png. You could tweak it fairly quickly to get the html.\nhttps://github.com/wehrhaus/sitescrape.git\n""]",https://stackoverflow.com/questions/9966826/save-and-render-a-webpage-with-phantomjs-and-node-js,web-scraping
Puppeteer - Protocol error (Page.navigate): Target closed,"
As you can see with the sample code below, I'm using Puppeteer with a cluster of workers in Node to run multiple requests of websites screenshots by a given URL:
const cluster = require('cluster');
const express = require('express');
const bodyParser = require('body-parser');
const puppeteer = require('puppeteer');

async function getScreenshot(domain) {
    let screenshot;
    const browser = await puppeteer.launch({ args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage'] });
    const page = await browser.newPage();

    try {
        await page.goto('http://' + domain + '/', { timeout: 60000, waitUntil: 'networkidle2' });
    } catch (error) {
        try {
            await page.goto('http://' + domain + '/', { timeout: 120000, waitUntil: 'networkidle2' });
            screenshot = await page.screenshot({ type: 'png', encoding: 'base64' });
        } catch (error) {
            console.error('Connecting to: ' + domain + ' failed due to: ' + error);
        }

    await page.close();
    await browser.close();

    return screenshot;
}

if (cluster.isMaster) {
    const numOfWorkers = require('os').cpus().length;
    for (let worker = 0; worker < numOfWorkers; worker++) {
        cluster.fork();
    }

    cluster.on('exit', function (worker, code, signal) {
        console.debug('Worker ' + worker.process.pid + ' died with code: ' + code + ', and signal: ' + signal);
        Cluster.fork();
    });

    cluster.on('message', function (handler, msg) {
        console.debug('Worker: ' + handler.process.pid + ' has finished working on ' + msg.domain + '. Exiting...');
        if (Cluster.workers[handler.id]) {
            Cluster.workers[handler.id].kill('SIGTERM');
        }
    });
} else {
    const app = express();
    app.use(bodyParser.json());
    app.listen(80, function() {
        console.debug('Worker ' + process.pid + ' is listening to incoming messages');
    });

    app.post('/screenshot', (req, res) => {
        const domain = req.body.domain;

        getScreenshot(domain)
            .then((screenshot) =>
                try {
                    process.send({ domain: domain });
                } catch (error) {
                    console.error('Error while exiting worker ' + process.pid + ' due to: ' + error);
                }

                res.status(200).json({ screenshot: screenshot });
            })
            .catch((error) => {
                try {
                    process.send({ domain: domain });
                } catch (error) {
                    console.error('Error while exiting worker ' + process.pid + ' due to: ' + error);
                }

                res.status(500).json({ error: error });
            });
    });
}

Some explanation:

Each time a request arrives a worker will process it and kill itself at the end
Each worker creates a new browser instance with a single page, and if a page took more than 60sec to load, it will retry reloading it (in the same page because maybe some resources has already been loaded) with timeout of 120sec
Once finished both the page and the browser will be closed

My problem is that some legitimate domains get errors that I can't explain:
Error: Protocol error (Page.navigate): Target closed.

Error: Protocol error (Runtime.callFunctionOn): Session closed. Most likely the page has been closed.

I read at some git issue (that I can't find now) that it can happen when the page redirects and adds 'www' at the start, but I'm hoping it's false...
Is there something I'm missing?
",76k,"
            51
        ","['\nWhat ""Target closed"" means\nWhen you launch a browser via puppeteer.launch it will start a browser and connect to it. From there on any function you execute on your opened browser (like page.goto) will be send via the Chrome DevTools Protocol to the browser. A target means a tab in this context.\nThe Target closed exception is thrown when you are trying to run a function, but the target (tab) was already closed.\nSimilar error messages\nThe error message was recently changed to give more meaningful information. It now gives the following message:\n\nError: Protocol error (Target.activateTarget): Session closed. Most likely the page has been closed.\n\n\nWhy does it happen\nThere are multiple reasons why this could happen.\n\nYou used a resource that was already closed\nMost likely, you are seeing this message because you closed the tab/browser and are still trying to use the resource. To give an simple example:\nconst browser = await puppeteer.launch();\nconst page = await browser.newPage();\n\nawait browser.close();\nawait page.goto(\'http://www.google.com\');\n\nIn this case the browser was closed and after that, a page.goto was called resulting in the error message. Most of the time, it will not be that obvious. Maybe an error handler already closed the page during a cleanup task, while your script is still crawling.\nThe browser crashed or was unable to initialize\nI also experience this every few hundred requests. There is an issue about this on the puppeteer repository as well. It seems to be the case, when you are using a lot of memory or CPU power. Maybe you are spawning a lot of browser? In these cases the browser might crash or disconnect.\nI found no ""silver bullet"" solution to this problem. But you might want to check out the library puppeteer-cluster (disclaimer: I\'m the author) which handles these kind of error cases and let\'s you retry the URL when the error happens. It can also manage a pool of browser instances and would also simplify your code.\n\n', ""\nFor me removing '--single-process' from args fixed the issue.\npuppeteerOptions: {\n    headless: true,\n    args: [\n        '--disable-gpu',\n        '--disable-dev-shm-usage',\n        '--disable-setuid-sandbox',\n        '--no-first-run',\n        '--no-sandbox',\n        '--no-zygote',\n        '--deterministic-fetch',\n        '--disable-features=IsolateOrigins',\n        '--disable-site-isolation-trials',\n        // '--single-process',\n    ],\n}\n\n"", ""\nI was just experiencing the same issue every time I tried running my puppeteer script*. The above did not resolve this issue for me.\nI got it to work by removing and reinstalling the puppeteer package:\nnpm remove puppeteer\nnpm i puppeteer\n\n*I only experienced this issue when setting the headless option to 'false`\n"", '\nI\'ve wound up at this thread a few times, and the typical culprit is that I forgot to await a Puppeteer page call that returned a promise, causing a race condition.\nHere\'s a minimal example of what this can look like:\nconst puppeteer = require(""puppeteer"");\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch({headless: true});\n  const [page] = await browser.pages();\n  page.goto(""https://www.stackoverflow.com""); // whoops, forgot await!\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close())\n;\n\nOutput is:\nC:\\Users\\foo\\Desktop\\puppeteer-playground\\node_modules\\puppeteer\\lib\\cjs\\puppeteer\\common\\Connection.js:217\n            this._callbacks.set(id, { resolve, reject, error: new Error(), method });\n                                                              ^\n\nError: Protocol error (Page.navigate): Target closed.\n    at C:\\Users\\foo\\Desktop\\puppeteer-playground\\node_modules\\puppeteer\\lib\\cjs\\puppeteer\\common\\Connection.js:217:63\n\nIn this case, it seems like an unmissable error, but in a larger chunk of code and the promise is nested or in a condition, it\'s easy to overlook.\nYou\'ll get a similar error for forgetting to await a page.click() or other promise call, for example, Error: Protocol error (Runtime.callFunctionOn): Target closed., which can be seen in the question UnhandledPromiseRejectionWarning: Error: Protocol error (Runtime.callFunctionOn): Target closed. (Puppeteer)\nThis is a contribution to the thread as a canonical resource for the error and may not be the solution to OP\'s problem, although the fundamental race condition seems to be a likely cause.\n', ""\nIn 2021 I'm receiving the very similar following error Error: Error pdf creationError: Protocol error (Target.setDiscoverTargets): Target closed., I solved it by playing with different args, so if your production server has a pipe:true flag in puppeteer.launch obj it will produce errors.\nAlso --disable-dev-shm-usage flag do the trick\nThe solution below works for me:\nconst browser = await puppeteer.launch({\n  headless: true,\n  // pipe: true, <-- delete this property\n  args: [\n    '--no-sandbox',\n    '--disable-dev-shm-usage', // <-- add this one\n    ],\n});\n\n"", '\nCheck your jest-puppeteer.config.js file.\nI made the below mistake\nmodule.exports = {\n    launch: {\n        headless: false,\n        browserContext: ""default"",\n    },\n};\n\nand after correcting it as below\nmodule.exports = {\n    launch: {\n        headless: false\n    },\n    browserContext: ""default"",\n};\n\neverything worked just fine!!!\n', '\nAfter hours of frustrations I realized that this happens when it goes to a new page and I need to be using await page.waitForNavigation() before I do anything and after I press a button or do any action that will cause it to redirect.\n']",https://stackoverflow.com/questions/51629151/puppeteer-protocol-error-page-navigate-target-closed,web-scraping
How to get the scrapy failure URLs?,"
I'm a newbie of scrapy and it's amazing crawler framework i have known! 
In my project, I sent more than 90, 000 requests, but there are some of them failed. 
I set the log level to be INFO, and i just can see some statistics but no details. 
2012-12-05 21:03:04+0800 [pd_spider] INFO: Dumping spider stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.ConnectionDone': 1,
 'downloader/request_bytes': 46282582,
 'downloader/request_count': 92383,
 'downloader/request_method_count/GET': 92383,
 'downloader/response_bytes': 123766459,
 'downloader/response_count': 92382,
 'downloader/response_status_count/200': 92382,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2012, 12, 5, 13, 3, 4, 836000),
 'item_scraped_count': 46191,
 'request_depth_max': 1,
 'scheduler/memory_enqueued': 92383,
 'start_time': datetime.datetime(2012, 12, 5, 12, 23, 25, 427000)}

Is there any way to get more detail report? For example, show those failed URLs. Thanks!
",39k,"
            51
        ","['\nYes, this is possible. \n\nThe code below adds a failed_urls list to a basic spider class and appends urls to it if the response status of the url is 404 (this would need to be extended to cover other error statuses as required). \nNext I added a handle that joins the list into a single string and adds it to the spider\'s stats when the spider is closed.\nBased on your comments, it\'s possible to track Twisted errors, and some of the answers below give examples on how to handle that particular use case\nThe code has been updated to work with Scrapy 1.8. All thanks to this should go to Juliano Mendieta, since all I did was simply to add his suggested edits and confirm that the spider worked as intended.\n\n\nfrom scrapy import Spider, signals\n\nclass MySpider(Spider):\n    handle_httpstatus_list = [404] \n    name = ""myspider""\n    allowed_domains = [""example.com""]\n    start_urls = [\n        \'http://www.example.com/thisurlexists.html\',\n        \'http://www.example.com/thisurldoesnotexist.html\',\n        \'http://www.example.com/neitherdoesthisone.html\'\n    ]\n\n    def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.failed_urls = []\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super(MySpider, cls).from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.handle_spider_closed, signals.spider_closed)\n        return spider\n\n    def parse(self, response):\n        if response.status == 404:\n            self.crawler.stats.inc_value(\'failed_url_count\')\n            self.failed_urls.append(response.url)\n\n    def handle_spider_closed(self, reason):\n        self.crawler.stats.set_value(\'failed_urls\', \', \'.join(self.failed_urls))\n\n    def process_exception(self, response, exception, spider):\n        ex_class = ""%s.%s"" % (exception.__class__.__module__, exception.__class__.__name__)\n        self.crawler.stats.inc_value(\'downloader/exception_count\', spider=spider)\n        self.crawler.stats.inc_value(\'downloader/exception_type_count/%s\' % ex_class, spider=spider)\n\n\nExample output (note that the downloader/exception_count* stats will only appear if exceptions are actually thrown - I simulated them by trying to run the spider after I\'d turned off my wireless adapter):\n2012-12-10 11:15:26+0000 [myspider] INFO: Dumping Scrapy stats:\n    {\'downloader/exception_count\': 15,\n     \'downloader/exception_type_count/twisted.internet.error.DNSLookupError\': 15,\n     \'downloader/request_bytes\': 717,\n     \'downloader/request_count\': 3,\n     \'downloader/request_method_count/GET\': 3,\n     \'downloader/response_bytes\': 15209,\n     \'downloader/response_count\': 3,\n     \'downloader/response_status_count/200\': 1,\n     \'downloader/response_status_count/404\': 2,\n     \'failed_url_count\': 2,\n     \'failed_urls\': \'http://www.example.com/thisurldoesnotexist.html, http://www.example.com/neitherdoesthisone.html\'\n     \'finish_reason\': \'finished\',\n     \'finish_time\': datetime.datetime(2012, 12, 10, 11, 15, 26, 874000),\n     \'log_count/DEBUG\': 9,\n     \'log_count/ERROR\': 2,\n     \'log_count/INFO\': 4,\n     \'response_received_count\': 3,\n     \'scheduler/dequeued\': 3,\n     \'scheduler/dequeued/memory\': 3,\n     \'scheduler/enqueued\': 3,\n     \'scheduler/enqueued/memory\': 3,\n     \'spider_exceptions/NameError\': 2,\n     \'start_time\': datetime.datetime(2012, 12, 10, 11, 15, 26, 560000)}\n\n', '\nHere\'s another example how to handle and collect 404 errors (checking github help pages):\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.item import Item, Field\n\n\nclass GitHubLinkItem(Item):\n    url = Field()\n    referer = Field()\n    status = Field()\n\n\nclass GithubHelpSpider(CrawlSpider):\n    name = ""github_help""\n    allowed_domains = [""help.github.com""]\n    start_urls = [""https://help.github.com"", ]\n    handle_httpstatus_list = [404]\n    rules = (Rule(SgmlLinkExtractor(), callback=\'parse_item\', follow=True),)\n\n    def parse_item(self, response):\n        if response.status == 404:\n            item = GitHubLinkItem()\n            item[\'url\'] = response.url\n            item[\'referer\'] = response.request.headers.get(\'Referer\')\n            item[\'status\'] = response.status\n\n            return item\n\nJust run scrapy runspider with -o output.json and see list of items in the output.json file.\n', '\nScrapy ignores 404 by default and does not parse it. If you are getting an error code 404 in response, you can handle this with a very easy way.\nIn settings.py, write:\nHTTPERROR_ALLOWED_CODES = [404,403]\n\nAnd then handle the response status code in your parse function:\ndef parse(self,response):\n    if response.status == 404:\n        #your action on error\n\n', ""\nThe answers from @Talvalin and @alecxe helped me a great deal, but they do not seem to capture downloader events that do not generate a response object (for instance, twisted.internet.error.TimeoutError and twisted.web.http.PotentialDataLoss). These errors show up in the stats dump at the end of the run, but without any meta info. \nAs I found out here, the errors are tracked by the stats.py middleware, captured in the DownloaderStats class' process_exception method, and specifically in the ex_class variable, which increments each error type as necessary, and then dumps the counts at the end of the run.  \nTo match such errors with information from the corresponding request object, you can add a unique id to each request (via request.meta), then pull it into the process_exception method of stats.py:\nself.stats.set_value('downloader/my_errs/{0}'.format(request.meta), ex_class)\n\nThat will generate a unique string for each downloader-based error not accompanied by a response. You can then save the altered stats.py as something else (e.g. my_stats.py), add it to the downloadermiddlewares (with the right precedence), and disable the stock stats.py:\nDOWNLOADER_MIDDLEWARES = {\n    'myproject.my_stats.MyDownloaderStats': 850,\n    'scrapy.downloadermiddleware.stats.DownloaderStats': None,\n    }\n\nThe output at the end of the run looks like this (here using meta info where each request url is mapped to a group_id and member_id separated by a slash, like '0/14'):\n{'downloader/exception_count': 3,\n 'downloader/exception_type_count/twisted.web.http.PotentialDataLoss': 3,\n 'downloader/my_errs/0/1': 'twisted.web.http.PotentialDataLoss',\n 'downloader/my_errs/0/38': 'twisted.web.http.PotentialDataLoss',\n 'downloader/my_errs/0/86': 'twisted.web.http.PotentialDataLoss',\n 'downloader/request_bytes': 47583,\n 'downloader/request_count': 133,\n 'downloader/request_method_count/GET': 133,\n 'downloader/response_bytes': 3416996,\n 'downloader/response_count': 130,\n 'downloader/response_status_count/200': 95,\n 'downloader/response_status_count/301': 24,\n 'downloader/response_status_count/302': 8,\n 'downloader/response_status_count/500': 3,\n 'finish_reason': 'finished'....}\n\nThis answer deals with non-downloader-based errors.\n"", '\nAs of scrapy 0.24.6, the method suggested by alecxe won\'t catch errors with the start URLs. To record errors with the start URLs you need to override parse_start_urls. Adapting alexce\'s answer for this purpose, you\'d get:\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.item import Item, Field\n\nclass GitHubLinkItem(Item):\n    url = Field()\n    referer = Field()\n    status = Field()\n\nclass GithubHelpSpider(CrawlSpider):\n    name = ""github_help""\n    allowed_domains = [""help.github.com""]\n    start_urls = [""https://help.github.com"", ]\n    handle_httpstatus_list = [404]\n    rules = (Rule(SgmlLinkExtractor(), callback=\'parse_item\', follow=True),)\n\n    def parse_start_url(self, response):\n        return self.handle_response(response)\n\n    def parse_item(self, response):\n        return self.handle_response(response)\n\n    def handle_response(self, response):\n        if response.status == 404:\n            item = GitHubLinkItem()\n            item[\'url\'] = response.url\n            item[\'referer\'] = response.request.headers.get(\'Referer\')\n            item[\'status\'] = response.status\n\n            return item\n\n', '\nThis is an update on this question. I ran in to a similar problem and needed to use the scrapy signals to call a function in my pipeline. I have edited @Talvalin\'s code, but wanted to make an answer just for some more clarity. \nBasically, you should add in self as an argument for handle_spider_closed.\nYou should also call the dispatcher in init so that you can pass the spider instance (self) to the handleing method.  \nfrom scrapy.spider import Spider\nfrom scrapy.xlib.pydispatch import dispatcher\nfrom scrapy import signals\n\nclass MySpider(Spider):\n    handle_httpstatus_list = [404] \n    name = ""myspider""\n    allowed_domains = [""example.com""]\n    start_urls = [\n        \'http://www.example.com/thisurlexists.html\',\n        \'http://www.example.com/thisurldoesnotexist.html\',\n        \'http://www.example.com/neitherdoesthisone.html\'\n    ]\n\n    def __init__(self, category=None):\n        self.failed_urls = []\n        # the dispatcher is now called in init\n        dispatcher.connect(self.handle_spider_closed,signals.spider_closed) \n\n\n    def parse(self, response):\n        if response.status == 404:\n            self.crawler.stats.inc_value(\'failed_url_count\')\n            self.failed_urls.append(response.url)\n\n    def handle_spider_closed(self, spider, reason): # added self \n        self.crawler.stats.set_value(\'failed_urls\',\',\'.join(spider.failed_urls))\n\n    def process_exception(self, response, exception, spider):\n        ex_class = ""%s.%s"" % (exception.__class__.__module__,  exception.__class__.__name__)\n        self.crawler.stats.inc_value(\'downloader/exception_count\', spider=spider)\n        self.crawler.stats.inc_value(\'downloader/exception_type_count/%s\' % ex_class, spider=spider)\n\nI hope this helps anyone with the same problem in the future.\n', ""\nIn addition to some of these answers, if you want to track Twisted errors, I would take a look at using the Request object's errback parameter, on which you can set a callback function to be called with the Twisted Failure on a request failure.\nIn addition to the url, this method can allow you to track the type of failure.\nYou can then log the urls by using: failure.request.url (where failure is the Twisted Failure object passed into errback).\n# these would be in a Spider\ndef start_requests(self):\n    for url in self.start_urls:\n        yield scrapy.Request(url, callback=self.parse,\n                                  errback=self.handle_error)\n\ndef handle_error(self, failure):\n    url = failure.request.url\n    logging.error('Failure type: %s, URL: %s', failure.type,\n                                               url)\n\nThe Scrapy docs give a full example of how this can be done, except that the calls to the Scrapy logger are now depreciated, so I've adapted my example to use Python's built in logging):\nhttps://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-errbacks\n"", ""\nYou can capture failed urls in two ways.\n\nDefine scrapy request with errback\nclass TestSpider(scrapy.Spider):\n    def start_requests(self):\n        yield scrapy.Request(url, callback=self.parse, errback=self.errback)\n\n    def errback(self, failure):\n        '''handle failed url (failure.request.url)'''\n        pass\n\nUse signals.item_dropped\nclass TestSpider(scrapy.Spider):\n    def __init__(self):\n        crawler.signals.connect(self.request_dropped, signal=signals.request_dropped)\n\n    def request_dropped(self, request, spider):\n        '''handle failed url (request.url)'''\n        pass\n\n\n[!Notice] Scrapy request with errback can not catch some auto retry failure, like connection error, RETRY_HTTP_CODES in settings.\n"", '\nBasically Scrapy Ignores 404 Error by Default, It was defined in httperror middleware.\nSo, Add HTTPERROR_ALLOW_ALL = True to your settings file.\nAfter this you can access response.status through your parse function.\nYou can handle it like this.\ndef parse(self,response):\n    if response.status==404:\n        print(response.status)\n    else:\n        do something\n\n']",https://stackoverflow.com/questions/13724730/how-to-get-the-scrapy-failure-urls,web-scraping
automatically execute an Excel macro on a cell change,"
How can I automatically execute an Excel macro each time a value in a particular cell changes?
Right now, my working code is:
Private Sub Worksheet_Change(ByVal Target As Range)
    If Not Intersect(Target, Range(""H5"")) Is Nothing Then Macro
End Sub

where ""H5"" is the particular cell being monitored and Macro is the name of the macro.
Is there a better way?
",526k,"
            98
        ","['\nYour code looks pretty good.\nBe careful, however, for your call to Range(""H5"") is a shortcut command to Application.Range(""H5""), which is equivalent to Application.ActiveSheet.Range(""H5""). This could be fine, if the only changes are user-changes -- which is the most typical -- but it is possible for the worksheet\'s cell values to change when it is not the active sheet via programmatic changes, e.g. VBA.\nWith this in mind, I would utilize Target.Worksheet.Range(""H5""):\nPrivate Sub Worksheet_Change(ByVal Target As Range)\n    If Not Intersect(Target, Target.Worksheet.Range(""H5"")) Is Nothing Then Macro\nEnd Sub\n\nOr you can use Me.Range(""H5""), if the event handler is on the code page for the worksheet in question (it usually is):\nPrivate Sub Worksheet_Change(ByVal Target As Range)\n    If Not Intersect(Target, Me.Range(""H5"")) Is Nothing Then Macro\nEnd Sub\n\n', '\nI spent a lot of time researching this and learning how it all works, after really messing up the event triggers. Since there was so much scattered info I decided to share what I have found to work all in one place, step by step as follows:\n1) Open VBA Editor, under VBA Project (YourWorkBookName.xlsm) open Microsoft Excel Object and select the Sheet to which the change event will pertain.\n2) The default code view is ""General."" From the drop-down list at the top middle, select ""Worksheet.""\n3) Private Sub Worksheet_SelectionChange is already there as it should be, leave it alone. Copy/Paste Mike Rosenblum\'s code from above and change the .Range reference to the cell for which you are watching for a change (B3, in my case). Do not place your Macro yet, however (I removed the word ""Macro"" after ""Then""):\nPrivate Sub Worksheet_Change(ByVal Target As Range)\n    If Not Intersect(Target, Me.Range(""H5"")) Is Nothing Then\nEnd Sub\n\nor from the drop-down list at the top left, select ""Change"" and in the space between Private Sub and End Sub, paste If Not Intersect(Target, Me.Range(""H5"")) Is Nothing Then\n4) On the line after ""Then"" turn off events so that when you call your macro, it does not trigger events and try to run this Worksheet_Change again in a never ending cycle that crashes Excel and/or otherwise messes everything up:\nApplication.EnableEvents = False\n\n5) Call your macro\nCall YourMacroName\n\n6) Turn events back on so the next change (and any/all other events) trigger:\nApplication.EnableEvents = True\n\n7) End the If block and the Sub:\n    End If\nEnd Sub\n\nThe entire code:\nPrivate Sub Worksheet_Change(ByVal Target As Range)\n    If Not Intersect(Target, Me.Range(""B3"")) Is Nothing Then\n        Application.EnableEvents = False\n        Call UpdateAndViewOnly\n        Application.EnableEvents = True\n    End If\nEnd Sub\n\nThis takes turning events on/off out of the Modules which creates problems and simply lets the change trigger, turns off events, runs your macro and turns events back on.\n', '\nHandle the Worksheet_Change event or the Workbook_SheetChange event.\nThe event handlers take an argument ""Target As Range"", so you can check if the range that\'s changing includes the cell you\'re interested in.\n', '\nI prefer this way, not using a cell but a range\n    Dim cell_to_test As Range, cells_changed As Range\n\n    Set cells_changed = Target(1, 1)\n    Set cell_to_test = Range( RANGE_OF_CELLS_TO_DETECT )\n\n    If Not Intersect(cells_changed, cell_to_test) Is Nothing Then \n       Macro\n    End If\n\n', '\nI have a cell which is linked to online stock database and updated frequently. I want to trigger a macro whenever the cell value is updated.\nI believe this is similar to cell value change by a program or any external data update but above examples somehow do not work for me. I think the problem is because excel internal events are not triggered, but thats my guess.\nI did the following,\nPrivate Sub Worksheet_Change(ByVal Target As Range) \n  If Not Intersect(Target, Target.Worksheets(""Symbols"").Range(""$C$3"")) Is Nothing Then\n   \'Run Macro\nEnd Sub\n\n']",https://stackoverflow.com/questions/409434/automatically-execute-an-excel-macro-on-a-cell-change,automation
Switch tabs using Selenium WebDriver with Java,"
Using Selenium WebDriver with Java.
I am trying to automate a functionality where I have to open a new tab do some operations there and come back to previous tab (Parent).
I used switch handle but it's not working.
And one strange thing the two tabs are having same window handle due to which I am not able to switch between tabs.
However when I am trying with different Firefox windows it works, but for tab it's not working.
How can I switch tabs?
Or, how can I switch tabs without using window handle as window handle is same of both tabs in my case?
(I have observed that when you open different tabs in same window, window handle remains same)
",385k,"
            75
        ","['\n    psdbComponent.clickDocumentLink();\n    ArrayList<String> tabs2 = new ArrayList<String> (driver.getWindowHandles());\n    driver.switchTo().window(tabs2.get(1));\n    driver.close();\n    driver.switchTo().window(tabs2.get(0));\n\nThis code perfectly worked for me. Try it out. You always need to switch your driver to new tab, before you want to do something on new tab.\n', '\nThis is a simple solution for opening a new tab, changing focus to it, closing the tab and return focus to the old/original tab:\n@Test\npublic void testTabs() {\n    driver.get(""https://business.twitter.com/start-advertising"");\n    assertStartAdvertising();\n\n    // considering that there is only one tab opened in that point.\n    String oldTab = driver.getWindowHandle();\n    driver.findElement(By.linkText(""Twitter Advertising Blog"")).click();\n    ArrayList<String> newTab = new ArrayList<String>(driver.getWindowHandles());\n    newTab.remove(oldTab);\n    // change focus to new tab\n    driver.switchTo().window(newTab.get(0));\n    assertAdvertisingBlog();\n\n    // Do what you want here, you are in the new tab\n\n    driver.close();\n    // change focus back to old tab\n    driver.switchTo().window(oldTab);\n    assertStartAdvertising();\n\n    // Do what you want here, you are in the old tab\n}\n\nprivate void assertStartAdvertising() {\n    assertEquals(""Start Advertising | Twitter for Business"", driver.getTitle());\n}\n\nprivate void assertAdvertisingBlog() {\n    assertEquals(""Twitter Advertising"", driver.getTitle());\n}\n\n', '\nThere is a difference how web driver handles different windows and how it handles different tabs.\nCase 1:\nIn case there are multiple windows, then the following code can help:\n//Get the current window handle\nString windowHandle = driver.getWindowHandle();\n\n//Get the list of window handles\nArrayList tabs = new ArrayList (driver.getWindowHandles());\nSystem.out.println(tabs.size());\n//Use the list of window handles to switch between windows\ndriver.switchTo().window(tabs.get(0));\n\n//Switch back to original window\ndriver.switchTo().window(mainWindowHandle);\n\n\nCase 2:\nIn case there are multiple tabs in the same window, then there is only one window handle. Hence switching between window handles keeps the control in the same tab. In this case using Ctrl + \\t (Ctrl + Tab) to switch between tabs is more useful.\n//Open a new tab using Ctrl + t\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL +""t"");\n//Switch between tabs using Ctrl + \\t\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL +""\\t"");\n\nDetailed sample code can be found here:\nhttp://design-interviews.blogspot.com/2014/11/switching-between-tabs-in-same-browser-window.html\n', '\nWork around\nAssumption : By Clicking something on your web page leads to open a new tab.\nUse below logic to switch to second tab.\nnew Actions(driver).sendKeys(driver.findElement(By.tagName(""html"")), Keys.CONTROL).sendKeys(driver.findElement(By.tagName(""html"")),Keys.NUMPAD2).build().perform();\n\nIn the same manner you can switch back to first tab again.\nnew Actions(driver).sendKeys(driver.findElement(By.tagName(""html"")), Keys.CONTROL).sendKeys(driver.findElement(By.tagName(""html"")),Keys.NUMPAD1).build().perform();\n\n', '\nSince the driver.window_handles is not in order , a better solution is this.\n\nfirst switch to the first tab using the shortcut Control + X to switch to the \'x\' th tab in the browser window .\n\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + ""1"");\n# goes to 1st tab\n\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + ""4"");\n# goes to 4th tab if its exists or goes to last tab.\n\n', '\nString selectLinkOpeninNewTab = Keys.chord(Keys.CONTROL, Keys.RETURN);\n    WebElement e = driver.findElement(By\n            .xpath(""html/body/header/div/div[1]/nav/a""));\ne.sendKeys(selectLinkOpeninNewTab);//to open the link in a current page in to the browsers new tab\n\n    e.sendKeys(Keys.CONTROL + ""\\t"");//to move focus to next tab in same browser\n    try {\n        Thread.sleep(8000);\n    } catch (InterruptedException e1) {\n        // TODO Auto-generated catch block\n        e1.printStackTrace();\n    }\n    //to wait some time in that tab\n    e.sendKeys(Keys.CONTROL + ""\\t"");//to switch the focus to old tab again\n\nHope it helps to you..\n', '\nThe first thing you need to do is opening a new tab and save it\'s handle name. It will be best to do it using javascript and not keys(ctrl+t) since keys aren\'t always available on automation servers. example:\npublic static String openNewTab(String url) {\n    executeJavaScript(""window.parent = window.open(\'parent\');"");\n    ArrayList<String> tabs = new ArrayList<String>(bot.driver.getWindowHandles());\n    String handleName = tabs.get(1);\n    bot.driver.switchTo().window(handleName);\n    System.setProperty(""current.window.handle"", handleName);\n    bot.driver.get(url);\n    return handleName;\n}\n\nThe second thing you need to do is switching between the tabs. Doing it by switch window handles only, will not always work since the tab you\'ll work on, won\'t always be in focus and Selenium will fail from time to time.\nAs I said, it\'s a bit problematic to use keys, and javascript doesn\'t really support switching tabs, so I used alerts to switch tabs and it worked like a charm:\npublic static void switchTab(int tabNumber, String handleName) {\n        driver.switchTo().window(handleName);\n        System.setProperty(""current.window.handle"", handleName);\n        if (tabNumber==1)\n            executeJavaScript(""alert(\\""alert\\"");"");\n        else\n            executeJavaScript(""parent.alert(\\""alert\\"");"");\n        bot.wait(1000);\n        driver.switchTo().alert().accept();\n    }\n\n', '\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL,Keys.SHIFT,Keys.TAB);\n\nThis method helps in switching between multiple windows. The restricting problem with this method is that it can only be used so many times until the required window is reached. Hope it helps.\n', '\nWith Selenium 2.53.1 using firefox 47.0.1 as the WebDriver in Java: no matter how many tabs I opened, ""driver.getWindowHandles()"" would only return one handle so it was impossible to switch between tabs.\nOnce I started using Chrome 51.0, I could get all handles.  The following code show how to access multiple drivers and multiple tabs within each driver.\n// INITIALIZE TWO DRIVERS (THESE REPRESENT SEPARATE CHROME WINDOWS)\ndriver1 = new ChromeDriver();\ndriver2 = new ChromeDriver();\n\n// LOOP TO OPEN AS MANY TABS AS YOU WISH\nfor(int i = 0; i < TAB_NUMBER; i++) {\n   driver1.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + ""t"");\n   // SLEEP FOR SPLIT SECOND TO ALLOW DRIVER TIME TO OPEN TAB\n   Thread.sleep(100);\n\n// STORE TAB HANDLES IN ARRAY LIST FOR EASY ACCESS\nArrayList tabs1 = new ArrayList<String> (driver1.getWindowHandles());\n\n// REPEAT FOR THE SECOND DRIVER (SECOND CHROME BROWSER WINDOW)\n\n// LOOP TO OPEN AS MANY TABS AS YOU WISH\nfor(int i = 0; i < TAB_NUMBER; i++) {\n   driver2.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + ""t"");\n   // SLEEP FOR SPLIT SECOND TO ALLOW DRIVER TIME TO OPEN TAB\n   Thread.sleep(100);\n\n// STORE TAB HANDLES IN ARRAY LIST FOR EASY ACCESS\nArrayList tabs2 = new ArrayList<String> (driver1.getWindowHandles());\n\n// NOW PERFORM DESIRED TASKS WITH FIRST BROWSER IN ANY TAB\nfor(int ii = 0; ii <= TAB_NUMBER; ii++) {\n   driver1.switchTo().window(tabs1.get(ii));\n   // LOGIC FOR THAT DRIVER\'S CURRENT TAB\n}\n\n// PERFORM DESIRED TASKS WITH SECOND BROWSER IN ANY TAB\nfor(int ii = 0; ii <= TAB_NUMBER; ii++) {\n   drvier2.switchTo().window(tabs2.get(ii));\n   // LOGIC FOR THAT DRIVER\'S CURRENT TAB\n}\n\nHopefully that gives you a good idea of how to manipulate multiple tabs in multiple browser windows.\n', '\nSimple Answer which worked for me:\nfor (String handle1 : driver1.getWindowHandles()) {\n        System.out.println(handle1); \n        driver1.switchTo().window(handle1);     \n}\n\n', '\nSet<String> tabs = driver.getWindowHandles();\nIterator<String> it = tabs.iterator();\ntab1 = it.next();\ntab2 = it.next();\ndriver.switchTo().window(tab1);\ndriver.close();\ndriver.switchTo().window(tab2);\n\nTry this. It should work\n', ""\nI had a problem recently, the link was opened in a new tab, but selenium focused still on the initial tab.\nI'm using Chromedriver and the only way to focus on a tab was for me to use switch_to_window().\nHere's the Python code:\ndriver.switch_to_window(driver.window_handles[-1])\n\nSo the tip is to find out the name of the window handle you need, they are stored as list in\ndriver.window_handles\n\n"", '\nPlease see below:\nWebDriver driver = new FirefoxDriver();\n\ndriver.manage().window().maximize();\ndriver.get(""https://www.irctc.co.in/"");\nString oldTab = driver.getWindowHandle();\n\n//For opening window in New Tab\nString selectLinkOpeninNewTab = Keys.chord(Keys.CONTROL,Keys.RETURN); \ndriver.findElement(By.linkText(""Hotels & Lounge"")).sendKeys(selectLinkOpeninNewTab);\n\n// Perform Ctrl + Tab to focus on new Tab window\nnew Actions(driver).sendKeys(Keys.chord(Keys.CONTROL, Keys.TAB)).perform();\n\n// Switch driver control to focused tab window\ndriver.switchTo().window(oldTab);\n\ndriver.findElement(By.id(""textfield"")).sendKeys(""bangalore"");\n\nHope this is helpful!\n', '\nIt is A very simple process: assume you have two tabs so you need to first close the current tab by using client.window(callback) because the switch command ""switches to the first available one"". Then you can easily switch tab using client.switchTab.\n', '\nA brief example of how to switch between tabs in a browser (in case with one window):\n// open the first tab\ndriver.get(""https://www.google.com"");\nThread.sleep(2000);\n\n// open the second tab\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + ""t"");\ndriver.get(""https://www.google.com"");\nThread.sleep(2000);\n\n// switch to the previous tab\ndriver.findElement(By.cssSelector(""body"")).sendKeys(Keys.CONTROL + """" + Keys.SHIFT + """" + Keys.TAB);\nThread.sleep(2000);\n\nI write Thread.sleep(2000) just to have a timeout to see switching between the tabs.\nYou can use CTRL+TAB for switching to the next tab and CTRL+SHIFT+TAB for switching to the previous tab.\n', '\nThis will work for the MacOS for Firefox and Chrome:\n// opens the default browser tab with the first webpage\ndriver.get(""the url 1"");\nthread.sleep(2000);\n\n// opens the second tab\ndriver.findElement(By.cssSelector(""Body"")).sendKeys(Keys.COMMAND + ""t"");\ndriver.get(""the url 2"");\nThread.sleep(2000);\n\n// comes back to the first tab\ndriver.findElement(By.cssSelector(""Body"")).sendKeys(Keys.COMMAND, Keys.SHIFT, ""{"");\n\n', '\nTo get parent window handles.   \nString parentHandle = driverObj.getWindowHandle();\npublic String switchTab(String parentHandle){\n    String currentHandle ="""";\n    Set<String> win  = ts.getDriver().getWindowHandles();   \n\n    Iterator<String> it =  win.iterator();\n    if(win.size() > 1){\n        while(it.hasNext()){\n            String handle = it.next();\n            if (!handle.equalsIgnoreCase(parentHandle)){\n                ts.getDriver().switchTo().window(handle);\n                currentHandle = handle;\n            }\n        }\n    }\n    else{\n        System.out.println(""Unable to switch"");\n    }\n    return currentHandle;\n}\n\n', '\nThe flaw with the selected answer is that it unnecessarily assumes order in webDriver.getWindowHandles().  The getWindowHandles() method returns a Set, which does not guarantee order. \nI used the following code to change tabs, which does not assume any ordering. \nString currentTabHandle = driver.getWindowHandle();\nString newTabHandle = driver.getWindowHandles()\n       .stream()\n       .filter(handle -> !handle.equals(currentTabHandle ))\n       .findFirst()\n       .get();\ndriver.switchTo().window(newTabHandle);\n\n', '\nprotected void switchTabsUsingPartOfUrl(String platform) {\n    String currentHandle = null;\n    try {\n        final Set<String> handles = driver.getWindowHandles();\n        if (handles.size() > 1) {\n            currentHandle = driver.getWindowHandle();\n        }\n        if (currentHandle != null) {\n            for (final String handle : handles) {\n                driver.switchTo().window(handle);\n                if (currentUrl().contains(platform) && !currentHandle.equals(handle)) {\n                    break;\n                }\n            }\n        } else {\n            for (final String handle : handles) {\n                driver.switchTo().window(handle);\n                if (currentUrl().contains(platform)) {\n                    break;\n                }\n            }\n        }\n    } catch (Exception e) {\n        System.out.println(""Switching tabs failed"");\n    }\n}\n\nCall this method and pass parameter a substring of url of the tab you want to switch to\n', '\npublic class TabBrowserDemo {\npublic static void main(String[] args) throws InterruptedException {\n    System.out.println(""Main Started"");\n    System.setProperty(""webdriver.gecko.driver"", ""driver//geckodriver.exe"");\n    WebDriver driver = new FirefoxDriver();\n    driver.get(""https://www.irctc.co.in/eticketing/userSignUp.jsf"");\n    driver.manage().timeouts().implicitlyWait(30, TimeUnit.SECONDS);\n\n    driver.findElement(By.xpath(""//a[text()=\'Flights\']"")).click();\n    waitForLoad(driver);\n    Set<String> ids = driver.getWindowHandles();\n    Iterator<String> iterator = ids.iterator();\n    String parentID = iterator.next();\n    System.out.println(""Parent WIn id "" + parentID);\n    String childID = iterator.next();\n    System.out.println(""child win id "" + childID);\n\n    driver.switchTo().window(childID);\n    List<WebElement> hyperlinks = driver.findElements(By.xpath(""//a""));\n\n    System.out.println(""Total links in tabbed browser "" + hyperlinks.size());\n\n    Thread.sleep(3000);\n//  driver.close();\n    driver.switchTo().window(parentID);\n    List<WebElement> hyperlinksOfParent = driver.findElements(By.xpath(""//a""));\n\n    System.out.println(""Total links "" + hyperlinksOfParent.size());\n\n}\n\npublic static void waitForLoad(WebDriver driver) {\n    ExpectedCondition<Boolean> pageLoadCondition = new\n            ExpectedCondition<Boolean>() {\n                public Boolean apply(WebDriver driver) {\n                    return ((JavascriptExecutor)driver).executeScript(""return document.readyState"").equals(""complete"");\n                }\n            };\n    WebDriverWait wait = new WebDriverWait(driver, 30);\n    wait.until(pageLoadCondition);\n}\n\n', '\n    public void switchToNextTab() {\n        ArrayList<String> tab = new ArrayList<>(driver.getWindowHandles());\n        driver.switchTo().window(tab.get(1));\n    }\n    \n    public void closeAndSwitchToNextTab() {\n        driver.close();\n        ArrayList<String> tab = new ArrayList<>(driver.getWindowHandles());\n        driver.switchTo().window(tab.get(1));\n    }\n\n    public void switchToPreviousTab() {\n        ArrayList<String> tab = new ArrayList<>(driver.getWindowHandles());\n        driver.switchTo().window(tab.get(0));\n    }\n\n    public void closeTabAndReturn() {\n        driver.close();\n        ArrayList<String> tab = new ArrayList<>(driver.getWindowHandles());\n        driver.switchTo().window(tab.get(0));\n    }\n\n    public void switchToPreviousTabAndClose() {\n        ArrayList<String> tab = new ArrayList<>(driver.getWindowHandles());\n        driver.switchTo().window(tab.get(1));\n        driver.close();\n    }\n\n', '\nWebDriver driver = new FirefoxDriver();\n\ndriver.switchTo().window(driver.getWindowHandles().toArray()[numPage].toString());\n\nnumPage - int (0,1..)\n\n', '\nString mainWindow = driver.getWindowHandle();\nseleniumHelper.switchToChildWindow();\n..\n..//your assertion steps\nseleniumHelper.switchToWindow(mainWindow);\n', '\nwith Java I used this for switching the selenium focus to the new tab.\n//Before the action that redirect to the new tab:\nString windHandleCurrent = driver.getWindowHandle();\n// code that click in a btn/link in order to open a new tab goes here\n// now to make selenium move to the new tab \nArrayList<String> windows = new ArrayList<String>(driver.getWindowHandles());\n    for(int i =0;i<windows.size();i++ ) {\n        String aWindow = windows.get(i);\n        if(aWindow != windHandleCurrent) {\n            driver.switchTo().window(aWindow);\n        }\n    }\n// now you can code your AssertJUnit for the new tab.\n\n', '\nSelenium 4 has new features:\n// Opens a new tab and switches to new tab\ndriver.switchTo().newWindow(WindowType.TAB);\n// Opens a new window and switches to new window\ndriver.switchTo().newWindow(WindowType.WINDOW);\n', '\ndriver.getWindowHandles() is a Set.I converted it to array of objects  by\nObject[] a=driver.getWindowHandles().toArray;\n\nSay you want to switch to 2nd tab then (after conversion) use\ndriver.switchTo().windows(a[1].toString());\n\n']",https://stackoverflow.com/questions/12729265/switch-tabs-using-selenium-webdriver-with-java,automation
headless internet browser? [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



I would like to do the following. Log into a website, click a couple of specific links, then click a download link. I'd like to run this as either a scheduled task on windows or cron job on Linux. I'm not picky about the language I use, but I'd like this to run with out putting a browser window up on the screen if possible.
",70k,"
            71
        ","['\nHere are a list of headless browsers that I know about:\n\nHtmlUnit - Java. Custom browser engine. Limited JavaScript support/DOM emulated. Open source.\nGhost - Python only. WebKit-based. Full JavaScript support. Open source.\nTwill - Python/command line. Custom browser engine. No JavaScript. Open source.\nPhantomJS - Command line/all platforms. WebKit-based. Full JavaScript support. Open source.\nAwesomium - C++/.NET/all platforms. Chromium-based. Full JavaScript support. Commercial/free.\nSimpleBrowser - .NET 4/C#. Custom browser engine. No JavaScript support. Open source.\nZombieJS - Node.js. Custom browser engine. JavaScript support/emulated DOM. Open source. Based on jsdom.\nEnvJS - JavaScript via Java/Rhino. Custom browser engine. JavaScript support/emulated DOM. Open source.\nWatir-webdriver with headless gem - Ruby via WebDriver.  Full JS Support via Browsers (Firefox/Chrome/Safari/IE). \nSpynner - Python only.  PyQT and WebKit. \njsdom - Node.js. Custom browser engine. Supports JS via emulated DOM. Open source.\nTrifleJS - port of PhantomJS using MSIE (Trident) and V8. Open source.\nui4j - Pure Java 8 solution. A wrapper library around the JavaFx WebKit Engine incl. headless modes.\nChromium Embedded Framework - Full up-to-date embedded version of Chromium with off-screen rendering as needed. C/C++, with .NET wrappers (and other languages). As it is Chromium, it has support for everything. BSD licensed.\nSelenium WebDriver - Full support for JavaScript via browsers (Firefox, IE, Chrome, Safari, Opera). Officially supported bindings are C#, Java, JavaScript, Haskell, Perl, Ruby, PHP, Python, Objective-C, and R. Unofficial bindings are available for Qt and Go. Open source.\n\nHeadless browsers that have JavaScript support via an emulated DOM generally have issues with some sites that use more advanced/obscure browser features, or have functionality that has visual dependencies (e.g. via CSS positions and so forth), so whilst the pure JavaScript support in these browsers is generally complete, the actual supported browser functionality should be considered as partial only.\n(Note: Original version of this post only mentioned HtmlUnit, hence the comments. If you know of other headless browser implementations and have edit rights, feel free to edit this post and add them.)\n', ""\nCheck out twill, a very convenient scripting language for precisely what you're looking for. From the examples:\nsetlocal username <your username>\nsetlocal password <your password>\n\ngo http://www.slashdot.org/\nformvalue 1 unickname $username\nformvalue 1 upasswd $password\nsubmit\n\ncode 200     # make sure form submission is correct!\n\nThere's also a Python API if you're looking for more flexibility.\n"", '\nHave a look at PhantomJS, a JavaScript based automation framework available for Windows, Mac OS X, Linux, other *ix systems.\nUsing PhantomJS, you can do things like this:\nconsole.log(\'Loading a web page\');\n\nvar page = new WebPage();\nvar url = ""http://www.phantomjs.org/"";\n\npage.open(url, function (status) {\n    // perform your task once the page is ready ...\n    phantom.exit();\n});\n\nOr evaluate a page\'s title:\nvar page = require(\'webpage\').create();\npage.open(url, function (status) {\n    var title = page.evaluate(function () {\n        return document.title;\n    });\n    console.log(\'Page title is \' + title);\n});\n\nExamples from PhantomJS\' Quickstart page. You can even render a page to a PNG, JPEG or PDF using the render() method.\n', '\nI once did that using the Internet Explorer ActiveX control (WebBrowser, MSHTML). You can instantiate it without making it visible.\nThis can be done with any language which supports COM (Delphi, VB6, VB.net, C#, C++, ...)\nOf course this is a quick-and-dirty solution and might not be appropriate in your situation.\n', '\nPhantomJS is a headless WebKit-based browser that you can script with JavaScript.\n', '\nExcept for the auto-download of the file (as that is a dialog box) a win form with the embedded webcontrol will do this.\nYou could look at Watin and Watin Recorder. They may help with C# code that can login to your website, navigate to a URL and possibly even help automate the file download.\nYMMV though.\n', ""\nIf the links are known (e.g, you don't have to search the page for them), then you can probably use wget. I believe that it will do the state management across multiple fetches.\nIf you are a little more enterprising, then I would delve into the new goodies in Python 3.0. They redid the interface to their HTTP stack and, IMHO, have a very nice interface that is susceptible to this type of scripting.\n"", '\nNode.js with YUI on the server. Check out this video: http://www.yuiblog.com/blog/2010/09/29/video-glass-node/\nThe guy in this video Dav Glass shows an example of how he uses node to fetch a page from Digg. He then attached YUI to the DOM he grabbed and can completely manipulate it.\n', '\nIf you use PHP - try http://mink.behat.org/\n', '\nYou can use Watir with Ruby or Watin with mono.\n', ""\nAlso you can use Live Http Headers (Firefox extension) to record headers which are sent to site (Login -> Links -> Download Link) and then replicate them with php using fsockopen. Only thing which you'll probably need to variate is the cookie's value which you receive from login page. \n"", '\nlibCURL could be used to create something like this.\n', '\nCan you not just use a download manager?\nThere\'s better ones, but FlashGet has browser-integration, and supports authentication. You can login, click a bunch of links and queue them up and schedule the download.\nYou could write something that, say, acts as a proxy which catches specific links and queues them for later download, or a Javascript bookmarklet that modifies links to go to ""http://localhost:1234/download_queuer?url="" + $link.href and have that queue the downloads - but you\'d be reinventing the download-manager-wheel, and with authentication it can be more complicated..\nOr, if you want the ""login, click links"" bit to be automated also - look into screen-scraping.. Basically you load the page via a HTTP library, find the download links and download them..\nSlightly simplified example, using Python:\nimport urllib\nfrom BeautifulSoup import BeautifulSoup\nsrc = urllib.urlopen(""http://%s:%s@example.com"" % (""username"", ""password""))\nsoup = BeautifulSoup(src)\n\nfor link_tag in soup.findAll(""a""):\n    link = link_tag[""href""]\n    filename = link.split(""/"")[-1] # get everything after last /\n    urllib.urlretrieve(link, filename)\n\nThat would download every link on example.com, after authenticating with the username/password of ""username"" and ""password"". You could, of course, find more specific links using BeautifulSoup\'s HTML selector\'s (for example, you could find all links with the class ""download"", or URL\'s that start with http://cdn.example.com).\nYou could do the same in pretty much any language..\n', ""\n.NET contains System.Windows.Forms.WebBrowser.  You can create an instance of this, send it to a URL, and then easily parse the html on that page.  You could then follow any links you found, etc.  \nI have worked with this object only minimally, so I'm no expert, but if you're already familiar with .NET then it would probably be worth looking into.\n""]",https://stackoverflow.com/questions/814757/headless-internet-browser,automation
Typing the Enter/Return key in Selenium,"
I'm looking for a quick way to type the Enter or Return key in Selenium.
Unfortunately, the form I'm trying to test (not my own code, so I can't modify) doesn't have a Submit button. When working with it manually, I just type Enter or Return. How can I do that with the Selenium type command as there is no button to click?
",913k,"
            330
        ","['\nimport org.openqa.selenium.Keys\n\nWebElement.sendKeys(Keys.RETURN);\n\n\nThe import statement is for Java. For other languages, it is maybe different. For example, in Python it is from selenium.webdriver.common.keys import Keys\n', '\nJava\ndriver.findElement(By.id(""Value"")).sendKeys(Keys.RETURN);\n\nOR,\ndriver.findElement(By.id(""Value"")).sendKeys(Keys.ENTER);\n\n\nPython\nfrom selenium.webdriver.common.keys import Keys\ndriver.find_element_by_name(""Value"").send_keys(Keys.RETURN)\n\nOR,\ndriver.find_element_by_name(""Value"").send_keys(Keys.ENTER)\n\nOR,\nelement = driver.find_element_by_id(""Value"")\nelement.send_keys(""keysToSend"")\nelement.submit()\n\n\nRuby\nelement = @driver.find_element(:name, ""value"")\nelement.send_keys ""keysToSend""\nelement.submit\n\nOR,\nelement = @driver.find_element(:name, ""value"")\nelement.send_keys ""keysToSend""\nelement.send_keys:return\n\nOR,\n@driver.action.send_keys(:enter).perform\n@driver.action.send_keys(:return).perform\n\n\nC#\ndriver.FindElement(By.Id(""Value"")).SendKeys(Keys.Return);\n\nOR,\ndriver.FindElement(By.Id(""Value"")).SendKeys(Keys.Enter);\n\n', '\nYou can use either of  Keys.ENTER or Keys.RETURN. Here are the details:\nUsage:\n\nJava:\n\nUsing Keys.ENTER:\nimport org.openqa.selenium.Keys;\ndriver.findElement(By.id(""element_id"")).sendKeys(Keys.ENTER);\n\n\nUsing Keys.RETURN:\nimport org.openqa.selenium.Keys;\ndriver.findElement(By.id(""element_id"")).sendKeys(Keys.RETURN);\n\n\n\n\nPython:\n\nUsing Keys.ENTER:\nfrom selenium.webdriver.common.keys import Keys\ndriver.find_element_by_id(""element_id"").send_keys(Keys.ENTER)\n\n\nUsing Keys.RETURN:\nfrom selenium.webdriver.common.keys import Keys\ndriver.find_element_by_id(""element_id"").send_keys(Keys.RETURN)\n\n\n\n\n\nKeys.ENTER and Keys.RETURN both are from org.openqa.selenium.Keys, which extends java.lang.Enum<Keys> and implements java.lang.CharSequence.\n\nEnum Keys\nEnum Keys is the representations of pressable keys that aren\'t text. These are stored in the Unicode PUA (Private Use Area) code points, 0xE000-0xF8FF.\nKey Codes:\nThe special keys codes for them are as follows:\n\nRETURN = u\'\\ue006\'\nENTER = u\'\\ue007\'\n\nThe implementation of all the Enum Keys are handled the same way.\nHence these is No Functional or Operational difference while working with either sendKeys(Keys.ENTER); or WebElement.sendKeys(Keys.RETURN); through Selenium.\n\nEnter Key and Return Key\nOn computer keyboards, the Enter (or the Return on Mac\xa0OS\xa0X) in most cases causes a command line, window form, or dialog box to operate its default function. This is typically to finish an ""entry"" and begin the desired process and is usually an alternative to pressing an OK button.\nThe Return is often also referred as the Enter and they usually perform identical functions; however in some particular applications (mainly page layout) Return operates specifically like the Carriage Return key from which it originates. In contrast, the Enter is commonly labelled with its name in plain text on generic PC keyboards.\n\nReferences\n\nEnter Key\nCarriage Return\n\n', '\nNow that Selenium 2 has been released, it\'s a bit easier to send an Enter key, since you can do it with the send_keys method of the selenium.webdriver.remote.webelement.WebElement class (this example code is in Python, but the same method exists in Java):\n>>> from selenium import webdriver\n>>> wd = webdriver.Firefox()\n>>> wd.get(""http://localhost/example/page"")\n>>> textbox = wd.find_element_by_css_selector(""input"")\n>>> textbox.send_keys(""Hello World\\n"")\n\n', '\nIn Python\nStep 1. from selenium.webdriver.common.keys import Keys\nStep 2. driver.find_element_by_name("""").send_keys(Keys.ENTER)\nNote: you have to write Keys.ENTER\n', '\nWhen writing HTML tests, the ENTER key is available as ${KEY_ENTER}.\nYou can use it with sendKeys, here is an example:\nsendKeys | id=search | ${KEY_ENTER}\n\n', '\nselenium.keyPress(""css=input.tagit-input.ui-autocomplete-input"", ""13"");\n\n', '\nYou just do this:\nfinal private WebElement input = driver.findElement(By.id(""myId""));\ninput.clear();\ninput.sendKeys(value); // The value we want to set to input\ninput.sendKeys(Keys.RETURN);\n\n', ""\nFor those folks who are using WebDriverJS Keys.RETURN would be referenced as \nwebdriver.Key.RETURN\n\nA more complete example as a reference might be helpful too:\nvar pressEnterToSend = function () {\n    var deferred = webdriver.promise.defer();\n    webdriver.findElement(webdriver.By.id('id-of-input-element')).then(function (element) {\n        element.sendKeys(webdriver.Key.RETURN);\n        deferred.resolve();\n    });\n\n    return deferred.promise;\n};\n\n"", '\ndriver.findElement(By.id(""Value"")).sendKeys(Keys.RETURN); or driver.findElement(By.id(""Value"")).sendKeys(Keys.ENTER);\n', '\nFor Selenium Remote Control with Java:\nselenium.keyPress(""elementID"", ""\\13"");\n\nFor Selenium WebDriver (a.k.a. Selenium 2) with Java:\ndriver.findElement(By.id(""elementID"")).sendKeys(Keys.ENTER);\n\nOr,\ndriver.findElement(By.id(""elementID"")).sendKeys(Keys.RETURN);\n\nAnother way to press Enter in WebDriver is by using the Actions class:\nActions action = new Actions(driver);\naction.sendKeys(driver.findElement(By.id(""elementID"")), Keys.ENTER).build().perform();\n\n', '\nsearch = browser.find_element_by_xpath(""//*[@type=\'text\']"")\nsearch.send_keys(u\'\\ue007\')\n\n#ENTER = u\'\\ue007\'\nRefer to Selenium\'s documentation \'Special Keys\'.\n', '\nI just like to note that I needed this for my Cucumber tests and found out that if you like to simulate pressing the enter/return key, you need to send the :return value and not the :enter value (see the values described here)\n', '\nTry to use an XPath expression for searching the element and then, the following code works:\ndriver.findElement(By.xpath("".//*[@id=\'txtFilterContentUnit\']"")).sendKeys(Keys.ENTER);\n\n', ""\nYou can call submit() on the element object in which you entered your text.\nAlternatively, you can specifically send the Enter key to it as shown in this Python snippet:\nfrom selenium.webdriver.common.keys import Keys\nelement.send_keys(Keys.ENTER) # 'element' is the WebElement object corresponding to the input field on the page\n\n"", '\nIf you are looking for ""how to press the Enter key from the keyboard in Selenium WebDriver (Java)"",then below code will definitely help you.\n// Assign a keyboard object\nKeyboard keyboard = ((HasInputDevices) driver).getKeyboard();\n\n// Enter a key\nkeyboard.pressKey(Keys.ENTER);\n\n', '\nTo enter keys using Selenium, first you need to import the following library:\nimport org.openqa.selenium.Keys\n\nthen add this code where you want to enter the key\nWebElement.sendKeys(Keys.RETURN);\n\nYou can replace RETURN with any key from the list according to your requirement.\n', '\nThere are the following ways of pressing keys - C#:\nDriver.FindElement(By.Id(""Value"")).SendKeys(Keys.Return);\n\nOR\nOpenQA.Selenium.Interactions.Actions action = new OpenQA.Selenium.Interactions.Actions(Driver);\naction.SendKeys(OpenQA.Selenium.Keys.Escape);\n\nOR\nIWebElement body = GlobalDriver.FindElement(By.TagName(""body""));\nbody.SendKeys(Keys.Escape);\n\n', '\nobject.sendKeys(""your message"", Keys.ENTER);\n\nIt works.\n', ""\nWhen you don't want to search any locator, you can use the Robot class. For example,\nRobot robot = new Robot();\nrobot.keyPress(KeyEvent.VK_ENTER);\nrobot.keyRelease(KeyEvent.VK_ENTER);\n\n"", '\nIf you just want to press the Enter key (python):\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.keys import Keys\n\naction = ActionChains(driver)\naction.send_keys(Keys.ENTER)\naction.perform()\n\n', '\nActions action = new Actions(driver);\naction.sendKeys(Keys.RETURN);\n\n', '\nFor Ruby:\ndriver.find_element(:id, ""XYZ"").send_keys:return\n\n', '\nFor Selenium WebDriver using XPath (if the key is visible):\ndriver.findElement(By.xpath(""xpath of text field"")).sendKeys(Keys.ENTER);\n\nor,\ndriver.findElement(By.xpath(""xpath of text field"")).sendKeys(Keys.RETURN);\n\n', '\nI had to send the Enter key in the middle of a text. So I passed the following text to send keys function to achieve 1\\n2\\n3:\n1\\N{U+E007}2\\N{U+E007}3\n\n', '\nJava/JavaScript:\nYou could probably do it this way also, non-natively:\npublic void triggerButtonOnEnterKeyInTextField(String textFieldId, String clickableButId)\n{\n    ((JavascriptExecutor) driver).executeScript(\n        ""   elementId = arguments[0];\n            buttonId = arguments[1];\n            document.getElementById(elementId)\n                .addEventListener(""keyup"", function(event) {\n                    event.preventDefault();\n                    if (event.keyCode == 13) {\n                        document.getElementById(buttonId).click();\n                    }\n                });"",\n\n        textFieldId,\n        clickableButId);\n}\n\n', '\nIt could be achieved using Action interface as well. In case of WebDriver -\nWebElement username = driver.findElement(By.name(""q""));\nusername.sendKeys(searchKey);\nActions action = new Actions(driver);\naction.sendKeys(Keys.RETURN);\naction.perform();\n\n', '\nYou can try:\nselenium.keyPress(""id="""", ""\\\\13"");\n\n', '\nIf you are in this specific situation:\na) want to just press the key, but you not have a specific webElement to click on\nb) you are using Selenium 2 (WebDriver)\nThen the solution is:\n    Actions builder = new Actions(webDriverInstance);\n    builder.sendKeys(Keys.RETURN).perform();\n\n', ""\nFor everyone using JavaScript / Node.js, this worked for me:\ndriver.findElement(By.xpath('xpath')).sendKeys('ENTER');\n\n""]",https://stackoverflow.com/questions/1629053/typing-the-enter-return-key-in-selenium,automation
.doc to pdf using python,"
I'am tasked with converting tons of .doc files to .pdf. And the only way my supervisor wants me to do this is through MSWord 2010. I know I should be able to automate this with python COM automation. Only problem is I dont know how and where to start. I tried searching for some tutorials but was not able to find any (May be I might have, but I don't know what I'm looking for). 
Right now I'm reading through this. Dont know how useful this is going to be.
",172k,"
            75
        ","[""\nA simple example using comtypes, converting a single file, input and output filenames given as commandline arguments:\nimport sys\nimport os\nimport comtypes.client\n\nwdFormatPDF = 17\n\nin_file = os.path.abspath(sys.argv[1])\nout_file = os.path.abspath(sys.argv[2])\n\nword = comtypes.client.CreateObject('Word.Application')\ndoc = word.Documents.Open(in_file)\ndoc.SaveAs(out_file, FileFormat=wdFormatPDF)\ndoc.Close()\nword.Quit()\n\nYou could also use pywin32, which would be the same except for:\nimport win32com.client\n\nand then:\nword = win32com.client.Dispatch('Word.Application')\n\n"", '\nYou can use the docx2pdf python package to bulk convert docx to pdf. It can be used as both a CLI and a python library. It requires Microsoft Office to be installed and uses COM on Windows and AppleScript (JXA) on macOS.\nfrom docx2pdf import convert\n\nconvert(""input.docx"")\nconvert(""input.docx"", ""output.pdf"")\nconvert(""my_docx_folder/"")\n\npip install docx2pdf\ndocx2pdf input.docx output.pdf\n\nDisclaimer: I wrote the docx2pdf package. https://github.com/AlJohri/docx2pdf\n', ""\nI have tested many solutions but no one of them works efficiently on Linux distribution.\nI recommend this solution :\nimport sys\nimport subprocess\nimport re\n\n\ndef convert_to(folder, source, timeout=None):\n    args = [libreoffice_exec(), '--headless', '--convert-to', 'pdf', '--outdir', folder, source]\n\n    process = subprocess.run(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n    filename = re.search('-> (.*?) using filter', process.stdout.decode())\n\n    return filename.group(1)\n\n\ndef libreoffice_exec():\n    # TODO: Provide support for more platforms\n    if sys.platform == 'darwin':\n        return '/Applications/LibreOffice.app/Contents/MacOS/soffice'\n    return 'libreoffice'\n\nand you call your function:\nresult = convert_to('TEMP Directory',  'Your File', timeout=15)\n\nAll resources:\n\nhttps://michalzalecki.com/converting-docx-to-pdf-using-python/\n\n"", '\nI have worked on this problem for half a day, so I think I should share some of my experience on this matter. Steven\'s answer is right, but it will fail on my computer. There are two key points to fix it here:\n(1). The first time when I created the \'Word.Application\' object, I should make it (the word app) visible before open any documents. (Actually, even I myself cannot explain why this works. If I do not do this on my computer, the program will crash when I try to open a document in the invisible model, then the \'Word.Application\' object will be deleted by OS. )\n(2). After doing (1), the program will work well sometimes but may fail often. The crash error ""COMError: (-2147418111, \'Call was rejected by callee.\', (None, None, None, 0, None))"" means that the COM Server may not be able to response so quickly. So I add a delay before I tried to open a document.\nAfter doing these two steps, the program will work perfectly with no failure anymore. The demo code is as below. If you have encountered the same problems, try to follow these two steps. Hope it helps.\n    import os\n    import comtypes.client\n    import time\n\n\n    wdFormatPDF = 17\n\n\n    # absolute path is needed\n    # be careful about the slash \'\\\', use \'\\\\\' or \'/\' or raw string r""...""\n    in_file=r\'absolute path of input docx file 1\'\n    out_file=r\'absolute path of output pdf file 1\'\n\n    in_file2=r\'absolute path of input docx file 2\'\n    out_file2=r\'absolute path of outputpdf file 2\'\n\n    # print out filenames\n    print in_file\n    print out_file\n    print in_file2\n    print out_file2\n\n\n    # create COM object\n    word = comtypes.client.CreateObject(\'Word.Application\')\n    # key point 1: make word visible before open a new document\n    word.Visible = True\n    # key point 2: wait for the COM Server to prepare well.\n    time.sleep(3)\n\n    # convert docx file 1 to pdf file 1\n    doc=word.Documents.Open(in_file) # open docx file 1\n    doc.SaveAs(out_file, FileFormat=wdFormatPDF) # conversion\n    doc.Close() # close docx file 1\n    word.Visible = False\n    # convert docx file 2 to pdf file 2\n    doc = word.Documents.Open(in_file2) # open docx file 2\n    doc.SaveAs(out_file2, FileFormat=wdFormatPDF) # conversion\n    doc.Close() # close docx file 2   \n    word.Quit() # close Word Application \n\n', '\nunoconv (writen in Python) and OpenOffice running as a headless daemon.\nhttps://github.com/unoconv/unoconv\nhttp://dag.wiee.rs/home-made/unoconv/\nWorks very nicely for doc, docx, ppt, pptx, xls, xlsx.\nVery useful if you need to convert docs or save/convert to certain formats on a server.\n', ""\nAs an alternative to the SaveAs function, you could also use ExportAsFixedFormat which gives you access to the PDF options dialog you would normally see in Word. With this you can specify bookmarks and other document properties.\ndoc.ExportAsFixedFormat(OutputFileName=pdf_file,\n    ExportFormat=17, #17 = PDF output, 18=XPS output\n    OpenAfterExport=False,\n    OptimizeFor=0,  #0=Print (higher res), 1=Screen (lower res)\n    CreateBookmarks=1, #0=No bookmarks, 1=Heading bookmarks only, 2=bookmarks match word bookmarks\n    DocStructureTags=True\n    );\n\nThe full list of function arguments is: 'OutputFileName', 'ExportFormat', 'OpenAfterExport', 'OptimizeFor', 'Range', 'From', 'To', 'Item', 'IncludeDocProps', 'KeepIRM', 'CreateBookmarks', 'DocStructureTags', 'BitmapMissingFonts', 'UseISO19005_1', 'FixedFormatExtClassPtr'\n"", ""\nIt's worth noting that Stevens answer works, but make sure if using a for loop to export multiple files to place the ClientObject or Dispatch statements before the loop - it only needs to be created once - see my problem: Python win32com.client.Dispatch looping through Word documents and export to PDF; fails when next loop occurs\n"", ""\nIf you don't mind using PowerShell have a look at this Hey, Scripting Guy! article. The code presented could be adopted to use the wdFormatPDF enumeration value of WdSaveFormat (see here).\nThis blog article presents a different implementation of the same idea.\n"", '\nI have modified it for ppt support as well. My solution support all the below-specified extensions.\nword_extensions = ["".doc"", "".odt"", "".rtf"", "".docx"", "".dotm"", "".docm""]\nppt_extensions = ["".ppt"", "".pptx""]\n\nMy Solution: Github Link\nI have modified code from Docx2PDF\n', '\nI tried the accepted answer but wasn\'t particularly keen on the bloated PDFs Word was producing which was usually an order of magnitude bigger than expected. After looking how to disable the dialogs when using a virtual PDF printer I came across Bullzip PDF Printer and I\'ve been rather impressed with its features. It\'s now replaced the other virtual printers I used previously. You\'ll find a ""free community edition"" on their download page.\nThe COM API can be found here and a list of the usable settings can be found here. The settings are written to a ""runonce"" file which is used for one print job only and then removed automatically. When printing multiple PDFs we need to make sure one print job completes before starting another to ensure the settings are used correctly for each file.\nimport os, re, time, datetime, win32com.client\n\ndef print_to_Bullzip(file):\n    util = win32com.client.Dispatch(""Bullzip.PDFUtil"")\n    settings = win32com.client.Dispatch(""Bullzip.PDFSettings"")\n    settings.PrinterName = util.DefaultPrinterName      # make sure we\'re controlling the right PDF printer\n\n    outputFile = re.sub(""\\.[^.]+$"", "".pdf"", file)\n    statusFile = re.sub(""\\.[^.]+$"", "".status"", file)\n\n    settings.SetValue(""Output"", outputFile)\n    settings.SetValue(""ConfirmOverwrite"", ""no"")\n    settings.SetValue(""ShowSaveAS"", ""never"")\n    settings.SetValue(""ShowSettings"", ""never"")\n    settings.SetValue(""ShowPDF"", ""no"")\n    settings.SetValue(""ShowProgress"", ""no"")\n    settings.SetValue(""ShowProgressFinished"", ""no"")     # disable balloon tip\n    settings.SetValue(""StatusFile"", statusFile)         # created after print job\n    settings.WriteSettings(True)                        # write settings to the runonce.ini\n    util.PrintFile(file, util.DefaultPrinterName)       # send to Bullzip virtual printer\n\n    # wait until print job completes before continuing\n    # otherwise settings for the next job may not be used\n    timestamp = datetime.datetime.now()\n    while( (datetime.datetime.now() - timestamp).seconds < 10):\n        if os.path.exists(statusFile) and os.path.isfile(statusFile):\n            error = util.ReadIniString(statusFile, ""Status"", ""Errors"", \'\')\n            if error != ""0"":\n                raise IOError(""PDF was created with errors"")\n            os.remove(statusFile)\n            return\n        time.sleep(0.1)\n    raise IOError(""PDF creation timed out"")\n\n', '\nI was working with this solution but I needed to search all .docx, .dotm, .docm, .odt, .doc or .rtf and then turn them all to .pdf (python 3.7.5). Hope it works...\nimport os\nimport win32com.client\n\nwdFormatPDF = 17\n\nfor root, dirs, files in os.walk(r\'your directory here\'):\n    for f in files:\n\n        if  f.endswith("".doc"")  or f.endswith("".odt"") or f.endswith("".rtf""):\n            try:\n                print(f)\n                in_file=os.path.join(root,f)\n                word = win32com.client.Dispatch(\'Word.Application\')\n                word.Visible = False\n                doc = word.Documents.Open(in_file)\n                doc.SaveAs(os.path.join(root,f[:-4]), FileFormat=wdFormatPDF)\n                doc.Close()\n                word.Quit()\n                word.Visible = True\n                print (\'done\')\n                os.remove(os.path.join(root,f))\n                pass\n            except:\n                print(\'could not open\')\n                # os.remove(os.path.join(root,f))\n        elif f.endswith("".docx"") or f.endswith("".dotm"") or f.endswith("".docm""):\n            try:\n                print(f)\n                in_file=os.path.join(root,f)\n                word = win32com.client.Dispatch(\'Word.Application\')\n                word.Visible = False\n                doc = word.Documents.Open(in_file)\n                doc.SaveAs(os.path.join(root,f[:-5]), FileFormat=wdFormatPDF)\n                doc.Close()\n                word.Quit()\n                word.Visible = True\n                print (\'done\')\n                os.remove(os.path.join(root,f))\n                pass\n            except:\n                print(\'could not open\')\n                # os.remove(os.path.join(root,f))\n        else:\n            pass\n\nThe try and except was for those documents I couldn\'t read and won\'t exit the code until the last document.\n', '\nYou should start from investigating so called virtual PDF print drivers.\nAs soon as you will find one you should be able to write batch file that prints your DOC files into PDF files. You probably can do this in Python too (setup printer driver output and issue document/print command in MSWord, later can be done using command line AFAIR).\n', '\nimport docx2txt\nfrom win32com import client\n\nimport os\n\nfiles_from_folder = r""c:\\\\doc""\n\ndirectory = os.fsencode(files_from_folder)\n\namount = 1\n\nword = client.DispatchEx(""Word.Application"")\nword.Visible = True\n\nfor file in os.listdir(directory):\n    filename = os.fsdecode(file)\n    print(filename)\n\n    if filename.endswith(\'docx\'):\n        text = docx2txt.process(os.path.join(files_from_folder, filename))\n\n        print(f\'{filename} transfered ({amount})\')\n        amount += 1\n        new_filename = filename.split(\'.\')[0] + \'.txt\'\n\n        try:\n            with open(os.path.join(files_from_folder + r\'\\txt_files\', new_filename), \'w\', encoding=\'utf-8\') as t:\n                t.write(text)\n        except:\n            os.mkdir(files_from_folder + r\'\\txt_files\')\n            with open(os.path.join(files_from_folder + r\'\\txt_files\', new_filename), \'w\', encoding=\'utf-8\') as t:\n                t.write(text)\n    elif filename.endswith(\'doc\'):\n        doc = word.Documents.Open(os.path.join(files_from_folder, filename))\n        text = doc.Range().Text\n        doc.Close()\n\n        print(f\'{filename} transfered ({amount})\')\n        amount += 1\n        new_filename = filename.split(\'.\')[0] + \'.txt\'\n\n        try:\n            with open(os.path.join(files_from_folder + r\'\\txt_files\', new_filename), \'w\', encoding=\'utf-8\') as t:\n                t.write(text)\n        except:\n            os.mkdir(files_from_folder + r\'\\txt_files\')\n            with open(os.path.join(files_from_folder + r\'\\txt_files\', new_filename), \'w\', encoding=\'utf-8\') as t:\n                t.write(text)\nword.Quit()\n\nThe Source Code, see here:\nhttps://neculaifantanaru.com/en/python-full-code-how-to-convert-doc-and-docx-files-to-pdf-from-the-folder.html\n', ""\nI would suggest ignoring your supervisor and use OpenOffice which has a Python api.  OpenOffice has built in support for Python and someone created a library specific for this purpose (PyODConverter).\nIf he isn't happy with the output, tell him it could take you weeks to do it with word.\n""]",https://stackoverflow.com/questions/6011115/doc-to-pdf-using-python,automation
Best practice - Git + Build automation - Keeping configs separate,"
Searching for the best approach to keep my config files separate, yet not introduce extra steps for new developers setting up their environments.
I am guessing a submodule would suffice to do the job, but then how would I switch configs seamlessly depending on the task at hand, aka pull in DEV config regularly, pull PROD branch of config repo during build?
Needs to be:

Easy and painless for new devs.  
PROD config files should only be accessible to select users + build user.

Thank you in advance.
",2k,"
            6
        ","['\nThat is called content filter driver, and it allows you to declare, in a .gitattributes file (and only for your config files type) a smudge script which will automatically on checkout:\n\ncombine a config file template file (config.tpl)\nwith the right config file value (config.dev, config.prod, ...)\nin order to produced a non-versioned config file (private file)\n\n\nSee ""Customizing Git - Git Attributes"":\necho \'*.cfg.tpl filter=config\' >> .gitattributes\ngit config --global filter.config.smudge yourScript\n\nWith that approach, you don\'t need submodules, but you can generate as many config file you need depending on your environment, like for instance your branch:\nA bit like in ""Find Git branch name in post-update hook"", your smudge script can find out in which branch it is currently executing with:\n#!/bin/sh\nbranch=$(git rev-parse --symbolic --abbrev-ref HEAD)\n\n']",https://stackoverflow.com/questions/20822073/best-practice-git-build-automation-keeping-configs-separate,automation
Stored procedure that Automatically delete rows older than 7 days in MYSQL,"
I would like to know if is possible to create a stored procedure that automatically, every day at 00:00, deletes every row of every table that is over 7 days.
I have seen few solutions but not sure if its what I am looking for, and would be nice if someone has any good example. I know this could be done with simple scripts in python and php, but I would like something more automated by MySQL.
Any help would be really appreciate.
Thanks!
",17k,"
            2
        ","[""\nMysql has its EVENT functionality for avoiding complicated cron interactions when much of what you are scheduling is sql related, and less file related. See the Manual page here. Hopefully the below reads as a quick overview of the important steps and things to consider, and verifiable testing too.\nshow variables where variable_name='event_scheduler';\n+-----------------+-------+\n| Variable_name   | Value |\n+-----------------+-------+\n| event_scheduler | OFF   |\n+-----------------+-------+\n\nooops, the event scheduler is not turned on. Nothing will trigger.\nSET GLOBAL event_scheduler = ON;  -- turn her on and confirm below\nshow variables where variable_name='event_scheduler';\n+-----------------+-------+\n| Variable_name   | Value |\n+-----------------+-------+\n| event_scheduler | ON    |\n+-----------------+-------+\n\nSchema for testing\ncreate table theMessages\n(   id int auto_increment primary key,\n    userId int not null,\n    message varchar(255) not null,\n    updateDt datetime not null,\n    key(updateDt)\n    -- FK's not shown\n);\n-- it is currently 2015-09-10 13:12:00\n-- truncate table theMessages;\ninsert theMessages(userId,message,updateDt) values (1,'I need to go now, no followup questions','2015-08-24 11:10:09');\ninsert theMessages(userId,message,updateDt) values (7,'You always say that ... just hiding','2015-08-29');\ninsert theMessages(userId,message,updateDt) values (1,'7 day test1','2015-09-03 12:00:00');\ninsert theMessages(userId,message,updateDt) values (1,'7 day test2','2015-09-03 14:00:00');\n\nCreate 2 events, 1st runs daily, 2nd runs every 10 minutes\nIgnore what they are actually doing (playing against one another). The point is on time difference approaches and scheduling.\nDELIMITER $$\nCREATE EVENT `delete7DayOldMessages`\n  ON SCHEDULE EVERY 1 DAY STARTS '2015-09-01 00:00:00'\n  ON COMPLETION PRESERVE\nDO BEGIN\n   delete from theMessages \n   where datediff(now(),updateDt)>6; -- not terribly exact, yesterday but <24hrs is still 1 day\n   -- etc etc all your stuff in here\nEND;$$\nDELIMITER ;\n\n...\nDELIMITER $$\nCREATE EVENT `Every_10_Minutes_Cleanup`\n  ON SCHEDULE EVERY 10 MINUTE STARTS '2015-09-01 00:00:00'\n  ON COMPLETION PRESERVE\nDO BEGIN\n   delete from theMessages \n   where TIMESTAMPDIFF(HOUR, updateDt, now())>168; -- messages over 1 week old (168 hours)\n   -- etc etc all your stuff in here\nEND;$$\nDELIMITER ;\n\nShow event statuses (different approaches)\nshow events from so_gibberish; -- list all events by schema name (db name)\nshow events; -- <--------- from workbench / sqlyog\nshow events\\G;` -- <--------- I like this one from mysql> prompt\n\n*************************** 1. row ***************************\n                  Db: so_gibberish\n                Name: delete7DayOldMessages\n             Definer: root@localhost\n           Time zone: SYSTEM\n                Type: RECURRING\n          Execute at: NULL\n      Interval value: 1\n      Interval field: DAY\n              Starts: 2015-09-01 00:00:00\n                Ends: NULL\n              Status: ENABLED\n          Originator: 1\ncharacter_set_client: utf8\ncollation_connection: utf8_general_ci\n  Database Collation: utf8_general_ci\n*************************** 2. row ***************************\n                  Db: so_gibberish\n                Name: Every_10_Minutes_Cleanup\n             Definer: root@localhost\n           Time zone: SYSTEM\n                Type: RECURRING\n          Execute at: NULL\n      Interval value: 10\n      Interval field: MINUTE\n              Starts: 2015-09-01 00:00:00\n                Ends: NULL\n              Status: ENABLED\n          Originator: 1\ncharacter_set_client: utf8\ncollation_connection: utf8_general_ci\n  Database Collation: utf8_general_ci\n2 rows in set (0.06 sec)\n\nRandom stuff to consider\ndrop event someEventName; -- <----- a good thing to know about\ncan't alias datediff and use in where clause in 1 line, so\nselect id,DATEDIFF(now(),updateDt) from theMessages where datediff(now(),updateDt)>6;\n\nget more exact, 168 hours for 1 week old\nselect id,TIMESTAMPDIFF(HOUR, updateDt, now()) as `difference` FROM theMessages;\n+----+------------+\n| id | difference |\n+----+------------+\n|  1 |        410 |\n|  2 |        301 |\n|  3 |        169 |\n|  4 |        167 |\n+----+------------+\n\nThe link to the Manual Page shows quite a bit of flexibilty with interval choices, shown below:\n\ninterval:\nquantity {YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE |\n          WEEK | SECOND | YEAR_MONTH | DAY_HOUR | DAY_MINUTE |\n          DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND}\n\n\nConcurrency\nEmbed any concurrency measures necessary that multiple events (or multiple firings of the same event) don't cause data to run amok.\nSet and Forget\nRemember, for now, because you are going to forget it, that these events just keep firing. So build in solid code that will just keep running, even when you forget. Which you most likely will.\nYour particular requirements\nYou need to determine which rows need to be deleted first by table, such that it honors Primary Key constraints. Just lump them all in proper order inside of the obvious area via the CREATE EVENT statement, which can be massive.\n"", '\nYou can use below stored procedure and either schedule it by crontab or through events.\nNote: Just change mydb with your Database, which database tables data you want to delete and test first in testing environment.\nDELIMITER $$\n\nUSE `mydb`$$\n\nDROP PROCEDURE IF EXISTS `sp_delete`$$\n\nCREATE DEFINER=`root`@`localhost` PROCEDURE `sp_delete`()\nBEGIN\nDECLARE done INT(1) DEFAULT 0;\nDECLARE _tbl VARCHAR(100) DEFAULT \'\';\nDECLARE CONTINUE HANDLER FOR SQLSTATE \'02000\' SET done=done;\n\nDECLARE cur1 CURSOR FOR SELECT table_name FROM information_schema.tables WHERE table_schema=\'mydb\' AND table_type=\'base table\';\nDECLARE CONTINUE HANDLER FOR SQLSTATE \'02000\' SET done=done;\nOPEN cur1;\n\nREPEAT\nFETCH cur1 INTO _tbl;\nIF _db = \'\' THEN\n    SET done = 1;\nEND IF;\n            IF (done<>1) THEN\n\n                             SET @str=CONCAT(""delete from "",_tbl,"" where updateon < SUBDATE(CURDATE(),INTERVAL 7 DAY)"");\n                 PREPARE stmt FROM @str;\n                 EXECUTE stmt;\n                 DEALLOCATE PREPARE stmt;\n\n            END IF;\n\nUNTIL done\nEND REPEAT;\n     CLOSE cur1;\n     SELECT \'done\';\n\n\nEND$$\n\nDELIMITER ;\n\n']",https://stackoverflow.com/questions/32507258/stored-procedure-that-automatically-delete-rows-older-than-7-days-in-mysql,automation
Selenium waitForElement,"
How do I write the function for Selenium to wait for a table with just a class identifier in Python? I'm having a devil of a time learning to use Selenium's Python webdriver functions.
",101k,"
            48
        ","[""\nFrom the Selenium Documentation PDF :\nimport contextlib\nimport selenium.webdriver as webdriver\nimport selenium.webdriver.support.ui as ui\n\nwith contextlib.closing(webdriver.Firefox()) as driver:\n    driver.get('http://www.google.com')\n    wait = ui.WebDriverWait(driver,10)\n    # Do not call `implicitly_wait` if using `WebDriverWait`.\n    #     It magnifies the timeout.\n    # driver.implicitly_wait(10)  \n    inputElement=driver.find_element_by_name('q')\n    inputElement.send_keys('Cheese!')\n    inputElement.submit()\n    print(driver.title)\n\n    wait.until(lambda driver: driver.title.lower().startswith('cheese!'))\n    print(driver.title)\n\n    # This raises\n    #     selenium.common.exceptions.TimeoutException: Message: None\n    #     after 10 seconds\n    wait.until(lambda driver: driver.find_element_by_id('someId'))\n    print(driver.title)\n\n"", ""\nSelenium 2's Python bindings have a new support class called expected_conditions.py for doing all sorts of things like testing if an element is visible. It's available here.\nNOTE: the above file is in the trunk as of Oct 12, 2012, but not yet in the latest download which is still 2.25. For the time being until a new Selenium version is released, you can just save this file locally for now and include it in your imports like I've done below.\nTo make life a little simpler, you can combine some of these expected condition methods with the Selenium wait until logic to make some very handy functions similar to what was available in Selenium 1. For example, I put this into my base class called SeleniumTest which all of my Selenium test classes extend:\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nimport selenium.webdriver.support.expected_conditions as EC\nimport selenium.webdriver.support.ui as ui\n\n@classmethod\ndef setUpClass(cls):\n    cls.selenium = WebDriver()\n    super(SeleniumTest, cls).setUpClass()\n\n@classmethod\ndef tearDownClass(cls):\n    cls.selenium.quit()\n    super(SeleniumTest, cls).tearDownClass()\n\n# return True if element is visible within 2 seconds, otherwise False\ndef is_visible(self, locator, timeout=2):\n    try:\n        ui.WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.CSS_SELECTOR, locator)))\n        return True\n    except TimeoutException:\n        return False\n\n# return True if element is not visible within 2 seconds, otherwise False\ndef is_not_visible(self, locator, timeout=2):\n    try:\n        ui.WebDriverWait(driver, timeout).until_not(EC.visibility_of_element_located((By.CSS_SELECTOR, locator)))\n        return True\n    except TimeoutException:\n        return False\n\nYou can then use these easily in your tests like so:\ndef test_search_no_city_entered_then_city_selected(self):\n    sel = self.selenium\n    sel.get('%s%s' % (self.live_server_url, '/'))\n    self.is_not_visible('#search-error')\n\n"", '\nI have made good experiences using:\n\ntime.sleep(seconds)\nwebdriver.Firefox.implicitly_wait(seconds)\n\nThe first one is pretty obvious - just wait a few seconds for some stuff.\nFor all my Selenium Scripts the sleep() with a few seconds (range from 1 to 3) works when I run them on my laptop, but on my Server the time to wait has a wider range, so I use implicitly_wait() too. I usually use implicitly_wait(30), which is really enough.\n\nAn implicit wait is to tell WebDriver to poll the DOM for a certain amount of time when trying to find an element or elements if they are not immediately available. The default setting is 0. Once set, the implicit wait is set for the life of the WebDriver object instance.\n\n', '\nI implemented the following for python for wait_for_condition since the python selenium driver does not support this function.\ndef wait_for_condition(c):\nfor x in range(1,10):\n    print ""Waiting for ajax: "" + c\n    x = browser.execute_script(""return "" + c)\n    if(x):\n        return\n    time.sleep(1)\n\nto be used as\nWait that an ExtJS Ajax call is not pending:\nwait_for_condition(""!Ext.Ajax.isLoading()"")\n\nA Javascript variable is set\nwait_for_condition(""CG.discovery != undefined;"")\n\netc.\n', '\nYou could always use a short sleep in a loop and pass it your element id:\ndef wait_for_element(element):\n     count = 1\n     if(self.is_element_present(element)):\n          if(self.is_visible(element)):\n              return\n          else:\n              time.sleep(.1)\n              count = count + 1\n     else:\n         time.sleep(.1)\n         count = count + 1\n         if(count > 300):\n             print(""Element %s not found"" % element)\n             self.stop\n             #prevents infinite loop\n\n', '\nUse Wait Until Page Contains Element with the proper XPath locator. For example, given the following HTML:\n<body>\n  <div id=""myDiv"">\n    <table class=""myTable"">\n      <!-- implementation -->\n    </table>\n  </div>\n</body>\n\n... you can enter the following keyword:\nWait Until Page Contains Element  //table[@class=\'myTable\']  5 seconds\n\nUnless I missed something, there is no need to create a new function for this.\n', '\nIn case this helps ...\nIn the Selenium IDE, I added ...\n  Command: waitForElementPresent\n  Target: //table[@class=\'pln\']\nThen I did File>Export TestCase As Python2(Web Driver), and it gave me this ...\ndef test_sel(self):\n    driver = self.driver\n    for i in range(60):\n        try:\n            if self.is_element_present(By.XPATH, ""//table[@class=\'pln\']""): break\n        except: pass\n        time.sleep(1)\n    else: self.fail(""time out"")\n\n', ""\neasier solution: \n    from selenium.webdriver.common.by import By    \n    import time\n\n    while len(driver.find_elements(By.ID, 'cs-paginate-next'))==0:\n        time.sleep(100)\n\n"", ""\nHopefully this helps\nfrom selenium import webdriver\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.wait import WebDriverWait\nfrom selenium.webdriver.common.by import By   \n\n\ndriver = webdriver.Firefox()\ndriver.get('www.url.com')\n\ntry:\n    wait = driver.WebDriverWait(driver,10).until(EC.presence_of_element_located(By.CLASS_NAME,'x'))\nexcept:\n    pass\n\n"", ""\nIf I don't know something about selenium command, I use selenium web idea RC with firefox. You can choose and add command in the combobox and when finish your test case after you can export the test code different language. like java, ruby, phyton, C#, etc..\n"", '\nYou can modify this function to all type of elements. The one below is just for the class element:\nWhere ""driver"" is the driver, ""element_name"" is the class name you are looking for, and ""sec"" is the maximum amount of seconds you are willing to wait.\ndef wait_for_class_element(driver,element_name,sec):\n\n    for i in range(sec):        \n        try:\n            driver.find_element_by_class_name(element_name)\n            break\n        except:        \n            print(""not yet"")\n            time.sleep(1)\n\n', ""\nI found an easier way to build this using a custom function, which is recursive in nature\nfrom selenium import webdriver\nimport time\n\ndef wait_element_by_id(id_value):\n    try:\n        elem = driver.find_element_by_id(id_value)\n    except:\n        time.sleep(2)\n        print 'Waiting for id '+id_value\n        wait_element_by_id(id_value)\n\nYou can replace find_element_by_id with find_element_by_name or find_element_by_tag_name based on your requirement\n"", '\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\n\n\n# wait until present\nWebDriverWait(driver, waittime).until(\n    EC.presence_of_element_located((By.CSS_SELECTOR, css_selector))\n)\n\n# wait until visible\nWebDriverWait(driver, waittime).until(\n    EC.visibility_of_element_located((By.CSS_SELECTOR, css_selector))\n)\n\n', ""\nI hope that's might help:\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as ec\n\ndriver = webdriver.Chrome()\ndriver.get(myURL)\nwait = WebDriverWait(driver, 10) \n\nwait.until(ec.presence_of_element_located((By.XPATH, myXPATH)))\n\nI recommend you to read this article to make it more clear.\n""]",https://stackoverflow.com/questions/7781792/selenium-waitforelement,automation
How to Automatically Start a Download in PHP?,"
What code do you need to add in PHP to automatically have the browser download a file to the local machine when a link is visited?
I am specifically thinking of functionality similar to that of download sites that prompt the user to save a file to disk once you click on the name of the software?
",114k,"
            55
        ","['\nSend the following headers before outputting the file:\nheader(""Content-Disposition: attachment; filename=\\"""" . basename($File) . ""\\"""");\nheader(""Content-Type: application/octet-stream"");\nheader(""Content-Length: "" . filesize($File));\nheader(""Connection: close"");\n\n@grom: Interesting about the \'application/octet-stream\' MIME type. I wasn\'t aware of that, have always just used \'application/force-download\' :)\n', '\nHere is an example of sending back a pdf.\nheader(\'Content-type: application/pdf\');\nheader(\'Content-Disposition: attachment; filename=""\' . basename($filename) . \'""\');\nheader(\'Content-Transfer-Encoding: binary\');\nreadfile($filename);\n\n@Swish I didn\'t find application/force-download content type to do anything different (tested in IE and Firefox). Is there a reason for not sending back the actual MIME type?\nAlso in the PHP manual Hayley Watson posted:\n\nIf you wish to force a file to be downloaded and saved, instead of being rendered, remember that there is no such MIME type as ""application/force-download"". The correct type to use in this situation is ""application/octet-stream"", and using anything else is merely relying on the fact that clients are supposed to ignore unrecognised MIME types and use ""application/octet-stream"" instead (reference: Sections 4.1.4 and 4.5.1 of RFC 2046).\n\nAlso according IANA there is no registered application/force-download type.\n', '\nA clean example.\n<?php\n    header(\'Content-Type: application/download\');\n    header(\'Content-Disposition: attachment; filename=""example.txt""\');\n    header(""Content-Length: "" . filesize(""example.txt""));\n\n    $fp = fopen(""example.txt"", ""r"");\n    fpassthru($fp);\n    fclose($fp);\n?>\n\n', '\nNone of above worked for me!\nWorking on 2021 for WordPress and PHP:\n<?php\n$file = ABSPATH . \'pdf.pdf\'; // Where ABSPATH is the absolute server path, not url\n//echo $file; //Be sure you are echoing the absolute path and file name\n$filename = \'Custom file name for the.pdf\'; /* Note: Always use .pdf at the end. */\n\nheader(\'Content-type: application/pdf\');\nheader(\'Content-Disposition: inline; filename=""\' . $filename . \'""\');\nheader(\'Content-Transfer-Encoding: binary\');\nheader(\'Content-Length: \' . filesize($file));\nheader(\'Accept-Ranges: bytes\');\n@readfile($file);\n\nThanks to: https://qastack.mx/programming/4679756/show-a-pdf-files-in-users-browser-via-php-perl\n', '\nmy code works for txt,doc,docx,pdf,ppt,pptx,jpg,png,zip extensions and I think its better to use the actual MIME types explicitly.\n$file_name = ""a.txt"";\n\n// extracting the extension:\n$ext = substr($file_name, strpos($file_name,\'.\')+1);\n\nheader(\'Content-disposition: attachment; filename=\'.$file_name);\n\nif(strtolower($ext) == ""txt"")\n{\n    header(\'Content-type: text/plain\'); // works for txt only\n}\nelse\n{\n    header(\'Content-type: application/\'.$ext); // works for all extensions except txt\n}\nreadfile($decrypted_file_path);\n\n']",https://stackoverflow.com/questions/40943/how-to-automatically-start-a-download-in-php,automation
Headless browser for C# (.NET)? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



I am (was) a Python developer who is building a GUI web scraping application. Recently I've decided to migrate to .NET framework and write the same application in C# (this decision wasn't mine).
In Python, I've used the Mechanize library. However, I can't seem to find anything similar in .NET. What I need is a browser that will run in a headless mode, which has the ability to fill out forms, submit them, etc. JavaScript parser is not a must, but it would be quite useful. 
",57k,"
            40
        ","['\nThere are some options:\n\nWebKit.Net (free)\n\nAwesomium\nIt is based on Chrome/WebKit and works like a charm.\nThere is a free license available but also a commercial one and if need be you can buy the source code :-)\n\nHTML Agility Pack (free) (An HTML Parser library, NOT a headless browser)\nThis helps with extracting information from HTML etc. and might be useful in your case (possibly in combination with HttpWebRequest)\n\n\n', ""\nMore solutions:\n\nPhantomJS - full featured headless web\nbrowser. Often used in pair with Selenium which allows you to\naccess the browser from .NET application.\nOptimus (nuget package)- lightweight headless web browser. It's in beta but it is sufficient for some cases.\n\nI used to use both for web testing. But they are also suitable for web scraping.\n"", ""\nYou may be after TrifleJS (currently in beta), or something similar using the .NET WebBrowser class which communicates with IE via a windowless ActiveX/COM API.\nYou'll essentially be running a fully fledged browser (not a http request wrapper) using Internet Explorer's Trident engine, if you are not interested in the JavaScript API (a port of phantomjs) you may still be able to use some of the C# codebase to get around key concepts (custom headers, cookies, script execution, screenshot rendering etc). \nNote that this can also emulate different versions of IE depending on what you have installed.\n\n""]",https://stackoverflow.com/questions/10161413/headless-browser-for-c-sharp-net,automation
Karate UI drag and drop [duplicate],"






This question already has an answer here:
                        
                    



KarateDSL UI Testing - Friendly Locators not working

                                (1 answer)
                            

Closed 2 years ago.



I am studying KarateUI possibilities. And I tried to use drag and drop functionality of framework.
I used a page with draggable elements https://www.seleniumeasy.com/test/drag-and-drop-demo.html and my script does not work on it. What is wrong with my script? Here it is:
mouse().move('{div/span}Draggable 1').down().move('#mydropzone').up()

And i also see in console of IDE next log
16:11:40.196 [ForkJoinPool-1-worker-1] DEBUG c.intuit.karate.driver.DriverOptions - >> {""method"":""Input.dispatchMouseEvent"",""params"":{""type"":""mouseMoved"",""x"":31,""y"":820},""id"":16}
16:11:40.200 [nioEventLoopGroup-2-1] DEBUG c.intuit.karate.driver.DriverOptions - << {""id"":16,""result"":{}}
16:11:40.203 [ForkJoinPool-1-worker-1] DEBUG c.intuit.karate.driver.DriverOptions - >> {""method"":""Input.dispatchMouseEvent"",""params"":{""type"":""mousePressed"",""x"":31,""y"":820,""button"":""left"",""clickCount"":1},""id"":17}
16:11:40.234 [nioEventLoopGroup-2-1] DEBUG c.intuit.karate.driver.DriverOptions - << {""id"":17,""result"":{}}
16:11:40.234 [ForkJoinPool-1-worker-1] DEBUG c.intuit.karate.driver.DriverOptions - >> {""method"":""Input.dispatchMouseEvent"",""params"":{""type"":""mouseMoved"",""x"":231,""y"":827},""id"":18}
16:11:40.242 [nioEventLoopGroup-2-1] DEBUG c.intuit.karate.driver.DriverOptions - << {""id"":18,""result"":{}}
16:11:40.242 [ForkJoinPool-1-worker-1] DEBUG c.intuit.karate.driver.DriverOptions - >> {""method"":""Input.dispatchMouseEvent"",""params"":{""type"":""mouseReleased"",""x"":231,""y"":827,""button"":""left"",""clickCount"":1},""id"":19}
16:11:40.250 [nioEventLoopGroup-2-1] DEBUG c.intuit.karate.driver.DriverOptions - << {""id"":19,""result"":{}}

",4k,"
            1
        ","['\nDrag and drop is actually quite hard to get right, so I recommend doing this via JavaScript. Executing JS is actually quite easy using Karate:\n* driver \'https://www.seleniumeasy.com/test/drag-and-drop-demo.html\'\n* script(""var myDragEvent = new Event(\'dragstart\'); myDragEvent.dataTransfer = new DataTransfer()"")\n* waitFor(\'{}Draggable 1\').script(""_.dispatchEvent(myDragEvent)"")\n* script(""var myDropEvent = new Event(\'drop\'); myDropEvent.dataTransfer = myDragEvent.dataTransfer"")\n* script(\'#mydropzone\', ""_.dispatchEvent(myDropEvent)"")\n* screenshot()\n\nSo with a little bit of awareness of some of the internals - e.g. the HTML5 DataTransfer API - you can do pretty much anything. I think ""bending the rules"" in cases like this is fine when it comes to automating complex E2E user-interactions in a browser.\nYou can of course wrap the drag-and-drop into a re-usable function in Karate, just keep in mind that ""DOM JS"" is sent to the browser as plain-text.\nRefer the docs: https://github.com/intuit/karate/tree/master/karate-core#function-composition\nEDIT: for those looking for other examples of using JS on the DOM:\n\nhttps://stackoverflow.com/a/60618233/143475\nhttps://stackoverflow.com/a/61478834/143475\nhttps://stackoverflow.com/a/66677401/143475\nhttps://stackoverflow.com/a/67701399/143475\nhttps://stackoverflow.com/a/67629911/143475\n\n']",https://stackoverflow.com/questions/60637144/karate-ui-drag-and-drop,automation
How can I use powershell to run through an installer?,"
I am trying to install a piece of software that when done manually has configuration options you can choose from when going through the process. I am trying to figure out a way to automate this using powershell but am stuck as to how I can set those configuration options. I believe I would need to run the start-process command on the installer .exe but I don't know where to go from there. Can I use the parameters on the start-process command to pass in the configurations I want? 
",11k,"
            0
        ","['\nUPDATE: Several links towards the bottom with information on how to handle installation, configuration and file extraction for setup.exe files.\nUPDATE: See Windows Installer PowerShell Module on github.com (scroll down for description, use releases tab for download). I haven\'t really tested it much, but it is from Heath Stewart - Microsoft Senior Software Engineer (github).\n\nI had a quick look for that installer, but didn\'t find it easily. Essentially the installer is either a Windows Installer database (MSI) or something else  - generally a setup.exe of some kind. An MSI database can also be wrapped in a setup.exe.\nYou should be aware that for legacy style installers a common practice for large scale deployment is to capture the legacy install with an application repackager tool, and then compile an MSI file to use for installation (effectively converting an installer from an old format to modern MSI format). This is a specialist task requiring good understanding of Windows and setups. It is generally done in large corporations for very large software distributions. If you are in a large company there might be a team dedicated to packaging software like the one you mention. Maybe check with your management. If the setup is an MSI the same team can also modify that for you according to your specifications.\n\nWith regards to your installer EXE. Try to run setup.exe /a from the command line and see if you get an option to extract files to a ""network install point"" (administrative install). Then you are dealing with an MSI file wrapped in a setup.exe. If that doesn\'t work you can try setup.exe /x or setup.exe /extract as well. \nWindows Installer has built-in features to allow you to customize the install via PUBLIC properties (uppercase) set at the command line or applied via a transform (Windows Installer\'s mechanism to apply substantial changes to the vendor file - it is a partial database that gets applied to the installation database from the vendor at runtime).\nNon-MSI, legacy installer technologies generally have fewer reliable ways to customize the installation settings, and they tend to be rather ad hoc when they are there. In particular the silent running and uninstall may be features that are missing or poorly executed. These installs are generally all wrapped in EXE format, and there are many tools used to generate them - each with their own quirks and features.\nIn other words, it all depends on what the installer is implemented as. Give that setup.exe /a a go, and update your answer with new information for us (don\'t add too many comments - we will check back).\n\nWith regards to using PowerShell. I haven\'t used PowerShell for deployment so far to be perfectly honest. Here is a basic description of how to install using PowerShell: https://kevinmarquette.github.io/2016-10-21-powershell-installing-msi-files/\nYou can also invoke automation for MSI files from PowerShell, I don\'t think this is relevant for what you asked, but here is a quick link for modifying a transform file: http://www.itninja.com/question/ps-how-to-edit-a-mst-file.\nThe normal way to install MSI files is via Window\'s built-in msiexec.exe command line. The basic msiexec.exe command line to install software is:\nmsiexec.exe /I ""C:\\Your.msi"" /QN /L*V ""C:\\msilog.log"" TRANSFORMS=""C:\\1031.mst;C:\\My.mst""\n\nQuick Parameter Explanation:\n/I = run install sequence\n/QN = run completely silently\n/L*V ""C:\\msilog.log"" = verbose logging\nTRANSFORMS=""C:\\1031.mst;C:\\My.mst"" = Apply transforms 1031.mst and My.mst (see below).\n\nWhat is a transform? Explained here: How to make better use of MSI files.\nAdvanced Installer has a general page on msiexec.exe command lines. And here is Microsoft\'s msiexec.exe documentation on MSDN.\n\nSome links:\n\nPerhaps see Michael Urman\'s answer here: Programmatically extract contents of InstallShield setup.exe. This is for Installshield packaged EXE files only.\nInstallshield setup.exe commands (general reference with some sample command lines - towards the end of the document it looks like the command lines are not correct, but the first ones look ok. The later ones are pretty obscure anyway - just thought I\'d let you know since I link to it). Here is the official Installshield help documentation.\nWise setup.exe commands - Wise is no longer available, but if the setup is older it can still be packaged with Wise.\nAdvanced Installer standard command line. For this tool setups can apparently be extracted with setup.exe /x or setup.exe /extract. See the link for full list. \nThere was also a ""silent switch finder"" tool used to find hidden switches in exe files (for deployment), but it failed a virustotal.com scan so I won\'t link to it. Maybe it is using something exotic, such as scanning a file\'s header at a bit level or something weird that is flagged as malware by mistake? Either way, not a tool I would use.\nAnd finally: http://unattended.sourceforge.net/installers.php. This link isn\'t bad, it presents some of the tools above and a few others - and the most common switches used. Untested by me, but looks ok.\nAnd there are other deployment tools that have their own way of packaging and delivering EXE files - it can be a jungle. I can provide a list of such tools with more links, but maybe that\'s just confusing. Please try what is provided above first.\nHere is a generic answer that might be helpful as well: Extract MSI from EXE\n\n']",https://stackoverflow.com/questions/46221983/how-can-i-use-powershell-to-run-through-an-installer,automation
Running script upon login in mac OS X [closed],"






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.


Closed 10 years ago.


This post was edited and submitted for review 9 months ago and failed to reopen the post:

Original close reason(s) were not resolved






                        Improve this question
                    



I am wondering if anyone is able to help me out with getting a shell (.sh) program to automatically run whenever I log in to my account on my computer. I am running Mac OS X 10.6.7.
I have a file ""Example.sh"" that I want to run when I log onto my computer. I do not have a problem running it when I am already logged in, but I want this to run automatically.
",330k,"
            338
        ","['\nFollow this:\n\nstart Automator.app\n\nselect Application\n\nclick Show library in the toolbar (if hidden)\n\nadd Run shell script (from the Actions/Utilities)\n\ncopy & paste your script into the window\n\ntest it\n\nsave somewhere (for example you can make an Applications folder in your HOME, you will get an your_name.app)\n\ngo to System Preferences -> Users & Groups -> Login items (or System Preferences -> Accounts -> Login items / depending of your MacOS version)\n\nadd this app\n\ntest & done ;)\n\n\nEDIT:\nI\'ve recently earned a ""Good answer"" badge for this answer. While my solution is simple and working, the cleanest way to run any program or shell script at login time is described in @trisweb\'s answer, unless, you want interactivity.\nWith automator solution you can do things like next:\n\nso, asking to run a script or quit the app, asking passwords, running other automator workflows at login time, conditionally run applications at login time and so on...\n', '\ntl;dr: use OSX\'s native process launcher and manager, launchd.\nTo do so, make a launchctl daemon. You\'ll have full control over all aspects of the script. You can run once or keep alive as a daemon. In most cases, this is the way to go.\n\nCreate a .plist file according to the instructions in the Apple Dev docs here or more detail below.\nPlace in ~/Library/LaunchAgents\nLog in (or run manually via launchctl load [filename.plist])\n\nFor more on launchd, the wikipedia article is quite good and describes the system and its advantages over other older systems.\n\nHere\'s the specific plist file to run a script at login.\n\nUpdated 2017/09/25 for OSX El Capitan and newer (credit to José Messias Jr):\n\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<!DOCTYPE plist PUBLIC ""-//Apple Computer//DTD PLIST 1.0//EN"" ""http://www.apple.com/DTDs/PropertyList-1.0.dtd"">\n<plist version=""1.0"">\n<dict>\n   <key>Label</key>\n   <string>com.user.loginscript</string>\n   <key>ProgramArguments</key>\n   <array><string>/path/to/executable/script.sh</string></array>\n   <key>RunAtLoad</key>\n   <true/>\n</dict>\n</plist>\n\nReplace the <string> after the Program key with your desired command (note that any script referenced by that command must be executable: chmod a+x /path/to/executable/script.sh to ensure it is for all users).\nSave as ~/Library/LaunchAgents/com.user.loginscript.plist\nRun launchctl load ~/Library/LaunchAgents/com.user.loginscript.plist and log out/in to test (or to test directly, run launchctl start com.user.loginscript)\nTail /var/log/system.log for error messages.\nThe key is that this is a User-specific launchd entry, so it will be run on login for the given user. System-specific launch daemons (placed in /Library/LaunchDaemons) are run on boot.\nIf you want a script to run on login for all users, I believe LoginHook is your only option, and that\'s probably the reason it exists.\n', '\n\nCreate a shell script named as login.sh in your $HOME folder.\n\nPaste the following one-line script into Script Editor: do shell script ""$HOME/login.sh""\n\nThen save it as an application.\n\nFinally add the application to your login items.\n\n\nIf you want to make the script output visual, you can swap step 2 for this:\ntell application ""Terminal""\n  activate\n  do script ""$HOME/login.sh""\nend tell\n\nIf multiple commands are needed something like this can be used:\ntell application ""Terminal""\n  activate\n  do script ""cd $HOME""\n  do script ""./login.sh"" in window 1\nend tell\n\n']",https://stackoverflow.com/questions/6442364/running-script-upon-login-in-mac-os-x,automation
How to copy a file to a remote server in Python using SCP or SSH?,"
I have a text file on my local machine that is generated by a daily Python script run in cron. 
I would like to add a bit of code to have that file sent securely to my server over SSH.
",359k,"
            134
        ","['\nTo do this in Python (i.e. not wrapping scp through subprocess.Popen or similar) with the Paramiko library, you would do something like this:\nimport os\nimport paramiko\n\nssh = paramiko.SSHClient() \nssh.load_host_keys(os.path.expanduser(os.path.join(""~"", "".ssh"", ""known_hosts"")))\nssh.connect(server, username=username, password=password)\nsftp = ssh.open_sftp()\nsftp.put(localpath, remotepath)\nsftp.close()\nssh.close()\n\n(You would probably want to deal with unknown hosts, errors, creating any directories necessary, and so on).\n', '\nYou can call the scp bash command (it copies files over SSH) with subprocess.run:\nimport subprocess\nsubprocess.run([""scp"", FILE, ""USER@SERVER:PATH""])\n#e.g. subprocess.run([""scp"", ""foo.bar"", ""joe@srvr.net:/path/to/foo.bar""])\n\nIf you\'re creating the file that you want to send in the same Python program, you\'ll want to call subprocess.run command outside the with block you\'re using to open the file (or call .close() on the file first if you\'re not using a with block), so you know it\'s flushed to disk from Python.\nYou need to generate (on the source machine) and install (on the destination machine) an ssh key beforehand so that the scp automatically gets authenticated with your public ssh key (in other words, so your script doesn\'t ask for a password).\n', '\nYou\'d probably use the subprocess module. Something like this:\nimport subprocess\np = subprocess.Popen([""scp"", myfile, destination])\nsts = os.waitpid(p.pid, 0)\n\nWhere destination is probably of the form user@remotehost:remotepath. Thanks to\n@Charles Duffy for pointing out the weakness in my original answer, which used a single string argument to specify the scp operation shell=True - that wouldn\'t handle whitespace in paths.\nThe module documentation has examples of error checking that you may want to perform in conjunction with this operation.\nEnsure that you\'ve set up proper credentials so that you can perform an unattended, passwordless scp between the machines. There is a stackoverflow question for this already.\n', '\nThere are a couple of different ways to approach the problem:\n\nWrap command-line programs\nuse a Python library that provides SSH capabilities (eg - Paramiko or Twisted Conch)\n\nEach approach has its own quirks. You will need to setup SSH keys to enable password-less logins if you are wrapping system commands like ""ssh"", ""scp"" or ""rsync."" You can embed a password in a script using Paramiko or some other library, but you might find the lack of documentation frustrating, especially if you are not familiar with the basics of the SSH connection (eg - key exchanges, agents, etc). It probably goes without saying that SSH keys are almost always a better idea than passwords for this sort of stuff.\nNOTE: its hard to beat rsync if you plan on transferring files via SSH, especially if the alternative is plain old scp.\nI\'ve used Paramiko with an eye towards replacing system calls but found myself drawn back to the wrapped commands due to their ease of use and immediate familiarity. You might be different. I gave Conch the once-over some time ago but it didn\'t appeal to me.\nIf opting for the system-call path, Python offers an array of options such as os.system or the commands/subprocess modules. I\'d go with the subprocess module if using version 2.4+.\n', '\nReached the same problem, but instead of ""hacking"" or emulating command line:\nFound this answer here.\nfrom paramiko import SSHClient\nfrom scp import SCPClient\n\nssh = SSHClient()\nssh.load_system_host_keys()\nssh.connect(\'example.com\')\n\nwith SCPClient(ssh.get_transport()) as scp:\n    scp.put(\'test.txt\', \'test2.txt\')\n    scp.get(\'test2.txt\')\n\n', '\nYou can do something like this, to handle the host key checking as well\nimport os\nos.system(""sshpass -p password scp -o StrictHostKeyChecking=no local_file_path username@hostname:remote_path"")\n\n', '\nfabric could be used to upload files vis ssh:\n#!/usr/bin/env python\nfrom fabric.api import execute, put\nfrom fabric.network import disconnect_all\n\nif __name__==""__main__"":\n    import sys\n    # specify hostname to connect to and the remote/local paths\n    srcdir, remote_dirname, hostname = sys.argv[1:]\n    try:\n        s = execute(put, srcdir, remote_dirname, host=hostname)\n        print(repr(s))\n    finally:\n        disconnect_all()\n\n', '\nYou can use the vassal package, which is exactly designed for this.\nAll you need is to install vassal and do\nfrom vassal.terminal import Terminal\nshell = Terminal([""scp username@host:/home/foo.txt foo_local.txt""])\nshell.run()\n\nAlso, it will save you authenticate credential and don\'t need to type them again and again.\n', '\nUsing the external resource paramiko;\n    from paramiko import SSHClient\n    from scp import SCPClient\n    import os\n\n    ssh = SSHClient() \n    ssh.load_host_keys(os.path.expanduser(os.path.join(""~"", "".ssh"", ""known_hosts"")))\n    ssh.connect(server, username=\'username\', password=\'password\')\n    with SCPClient(ssh.get_transport()) as scp:\n            scp.put(\'test.txt\', \'test2.txt\')\n\n', ""\nI used sshfs to mount the remote directory via ssh, and shutil to copy the files:\n$ mkdir ~/sshmount\n$ sshfs user@remotehost:/path/to/remote/dst ~/sshmount\n\nThen in python:\nimport shutil\nshutil.copy('a.txt', '~/sshmount')\n\nThis method has the advantage that you can stream data over if you are generating data rather than caching locally and sending a single large file.\n"", ""\nTry this if you wan't to use SSL certificates:\nimport subprocess\n\ntry:\n    # Set scp and ssh data.\n    connUser = 'john'\n    connHost = 'my.host.com'\n    connPath = '/home/john/'\n    connPrivateKey = '/home/user/myKey.pem'\n\n    # Use scp to send file from local to host.\n    scp = subprocess.Popen(['scp', '-i', connPrivateKey, 'myFile.txt', '{}@{}:{}'.format(connUser, connHost, connPath)])\n\nexcept CalledProcessError:\n    print('ERROR: Connection to host failed!')\n\n"", '\nA very simple approach is the following: \nimport os\nos.system(\'sshpass -p ""password"" scp user@host:/path/to/file ./\')\n\nNo python library are required (only os), and it works, however using this method relies on another ssh client to be installed. This could result in undesired behavior if ran on another system.\n', '\nCalling scp command via subprocess doesn\'t allow to receive the progress report inside the script. pexpect could be used to extract that info:\nimport pipes\nimport re\nimport pexpect # $ pip install pexpect\n\ndef progress(locals):\n    # extract percents\n    print(int(re.search(br\'(\\d+)%$\', locals[\'child\'].after).group(1)))\n\ncommand = ""scp %s %s"" % tuple(map(pipes.quote, [srcfile, destination]))\npexpect.run(command, events={r\'\\d+%\': progress})\n\nSee python copy file in local network (linux -> linux)\n', '\nKind of hacky, but the following should work :)\nimport os\nfilePath = ""/foo/bar/baz.py""\nserverPath = ""/blah/boo/boom.py""\nos.system(""scp ""+filePath+"" user@myserver.com:""+serverPath)\n\n']",https://stackoverflow.com/questions/68335/how-to-copy-a-file-to-a-remote-server-in-python-using-scp-or-ssh,automation
"Exception in thread ""main"" org.openqa.selenium.NoSuchElementException: Unable to locate element: //*[@id='login-email']","
I had to re-test the xpath, Previously it was working fine, But now it gives me an error. 
I tried with different locators as well, Like id, name. but still get the same error.
package staging;

import org.openqa.selenium.By;
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.firefox.FirefoxDriver;

public class login {

    public static void main (String[]args){
        System.setProperty(""webdriver.gecko.driver"",""C:\\Program Files\\geckodriver.exe"");
        WebDriver driver = new FirefoxDriver();

        //opening the browser
        driver.get(""https://staging.keela.co/login"");

        //logging
        driver.findElement(By.xpath(""//*[@id='login-email']"")).sendKeys(""bandanakeela@yopmail.com"");
        driver.findElement(By.xpath(""//*[@id='login-password']"")).sendKeys(""keela"");
        driver.findElement(By.xpath(""//*[@id='login-form']/div[3]/div/button"")).click();       
 }
}

",12k,"
            5
        ","['\nAs you access the url https://staging.keela.co/login there is a Ajax loader which blocks the UI, so we have to wait for the Ajax loader to complete loading the all the WebElements and the email and password field becomes visible. To achieve that we will introduce ExplicitWait i.e. WebDriverWait with ExpectedConditions set to elementToBeClickable for the email field.Here is the working code block:   \nSystem.setProperty(""webdriver.gecko.driver"",""C:\\\\Utility\\\\BrowserDrivers\\\\geckodriver.exe"");\nWebDriver driver = new FirefoxDriver();\ndriver.get(""https://staging.keela.co/login"");\nWebDriverWait wait = new WebDriverWait (driver, 15);\nWebElement element = wait.until(ExpectedConditions.elementToBeClickable(By.xpath(""//input[@id=\'login-email\']"")));\nelement.sendKeys(""bandanakeela@yopmail.com"");\ndriver.findElement(By.xpath(""//input[@id=\'login-password\']"")).sendKeys(""keela"");\ndriver.findElement(By.xpath(""//button[@class=\'btn btn-sm btn-block btn-primary\']"")).click(); \n\n', '\nTry this below code.\nNote: If id attribute is available then you should use id and for xpath try to use relative xpath.\nI have used explicit wait method, so your driver may able to find the next webelement, after page is fully loaded. \ndriver.get(""https://staging.keela.co/login"");\ndriver.manage().window().maximize();\n\n//Explicit wait for 60 seconds, to find the webelement. You can increase or decrease the time as per your specification.        \nnew WebDriverWait(driver, 60).until(ExpectedConditions.elementToBeClickable(driver.findElement(By.id(""login-email""))));\ndriver.findElement(By.id(""login-email"")).sendKeys(""bandanakeela@yopmail.com"");\ndriver.findElement(By.id(""login-password"")).sendKeys(""keela"");\ndriver.findElement(By.xpath(""//button[@type=\'submit\'][text()=\'Log in\']"")).click();\n\n', '\nThe page is using js to construct elements. So I would suggest you to use phantomjs driver. \nThen you have to wait until element exist. You see the gear icon when page is loading. wait until the element loads. and also you can use id instead of xpath since you know your element id .\nYou can choose which wait type you want to use. Explicit Waits or Implicit Waits.\nHere is the selenium documentation.\nand example code for wait:\nWebElement myDynamicElement = (new WebDriverWait(driver, 10))\n  .until(ExpectedConditions.presenceOfElementLocated(By.id(""login-email"")));\n\nor you can wait until page load:\nnew WebDriverWait(firefoxDriver, pageLoadTimeout).until(\n          webDriver -> ((JavascriptExecutor) webDriver).executeScript(""return document.readyState"").equals(""complete""));\n\n', '\nYou are opening the URL and at the very next moment entering email-id. Before entering email-id, you need to check if the page is fully loaded. In this case, explicit wait will help you out-\n//opening the browser\ndriver.get(""https://staging.keela.co/login"");\n\n//Explicit wait\n\nWebDriverWait wait = new WebDriverWait(WebDriverRefrence,20);\nWebElement email;\nemail = wait.until(ExpectedConditions.visibilityOfElementLocated(By.id(""login-email"")));\n\n//logging\ndriver.findElement(By.xpath(""//*[@id=\'login-email\']"")).sendKeys(""bandanakeela@yopmail.com"");\ndriver.findElement(By.xpath(""//*[@id=\'login-password\']"")).sendKeys(""keela"");\ndriver.findElement(By.xpath(""//*[@id=\'login-form\']/div[3]/div/button"")).click();\n\n']",https://stackoverflow.com/questions/46202283/exception-in-thread-main-org-openqa-selenium-nosuchelementexception-unable-to,automation
Have bash script answer interactive prompts [duplicate],"






This question already has answers here:
                        
                    



Passing arguments to an interactive program non-interactively

                                (5 answers)
                            

Closed 2 years ago.
The community reviewed whether to reopen this question 1 year ago and left it closed:

Original close reason(s) were not resolved




Is it possible to have a bash script automatically handle prompts that would normally be presented to the user with default actions?  Currently I am using a bash script to call an in-house tool that will display prompts to the user (prompting for Y/N) to complete actions, however the script I'm writing needs to be completely ""hands-off"", so I need a way to send Y|N to the prompt to allow the program to continue execution.  Is this possible?
",216k,"
            157
        ","['\nA simple\necho ""Y Y N N Y N Y Y N"" | ./your_script\n\nThis allow you to pass any sequence of ""Y"" or ""N"" to your script.\n', '\nThis is not ""auto-completion"", this is automation. One common tool for these things is called Expect.\nYou might also get away with just piping input from yes.\n', '\nIf you only have Y to send : \n$> yes Y |./your_script\n\nIf you only have N to send : \n$> yes N |./your_script\n\n', '\nI found the best way to send input is to use cat and a text file to pass along whatever input you need.\ncat ""input.txt"" | ./Script.sh\n\n', ""\nIn my situation I needed to answer some questions without Y or N but with text or blank.  I found the best way to do this in my situation was to create a shellscript file.  In my case I called it autocomplete.sh\nI was needing to answer some questions for a doctrine schema exporter so my file looked like this.\n-- This is an example only --\nphp vendor/bin/mysql-workbench-schema-export mysqlworkbenchfile.mwb ./doctrine << EOF\n`#Export to Doctrine Annotation Format`                                     1\n`#Would you like to change the setup configuration before exporting`        y\n`#Log to console`                                                           y\n`#Log file`                                                                 testing.log\n`#Filename [%entity%.%extension%]`\n`#Indentation [4]`\n`#Use tabs [no]`\n`#Eol delimeter (win, unix) [win]`\n`#Backup existing file [yes]`\n`#Add generator info as comment [yes]`\n`#Skip plural name checking [no]`\n`#Use logged storage [no]`\n`#Sort tables and views [yes]`\n`#Export only table categorized []`\n`#Enhance many to many detection [yes]`\n`#Skip many to many tables [yes]`\n`#Bundle namespace []`\n`#Entity namespace []`\n`#Repository namespace []`\n`#Use automatic repository [yes]`\n`#Skip column with relation [no]`\n`#Related var name format [%name%%related%]`\n`#Nullable attribute (auto, always) [auto]`\n`#Generated value strategy (auto, identity, sequence, table, none) [auto]`\n`#Default cascade (persist, remove, detach, merge, all, refresh, ) [no]`\n`#Use annotation prefix [ORM\\]`\n`#Skip getter and setter [no]`\n`#Generate entity serialization [yes]`\n`#Generate extendable entity [no]`                                          y\n`#Quote identifier strategy (auto, always, none) [auto]`\n`#Extends class []`\n`#Property typehint [no]`\nEOF\n\nThe thing I like about this strategy is you can comment what your answers are and using EOF a blank line is just that (the default answer).  Turns out by the way this exporter tool has its own JSON counterpart for answering these questions, but I figured that out after I did this =).\nto run the script simply be in the directory you want and run 'sh autocomplete.sh' in terminal.\nIn short by using << EOL & EOF in combination with Return Lines you can answer each question of the prompt as necessary.  Each new line is a new answer.\nMy example just shows how this can be done with comments also using the ` character so you remember what each step is.\nNote the other advantage of this method is you can answer with more then just Y or N ... in fact you can answer with blanks!\nHope this helps someone out.\n"", ""\nThere is a special build-in util for this - 'yes'.\nTo answer all questions with the same answer, you can run \nyes [answer] |./your_script\n\nOr you can put it inside your script have specific answer to each question\n""]",https://stackoverflow.com/questions/3804577/have-bash-script-answer-interactive-prompts,automation
Is there a PHP equivalent of Perl's WWW::Mechanize?,"
I'm looking for a library that has functionality similar to Perl's WWW::Mechanize, but for PHP. Basically, it should allow me to submit HTTP GET and POST requests with a simple syntax, and then parse the resulting page and return in a simple format all forms and their fields, along with all links on the page.
I know about CURL, but it's a little too barebones, and the syntax is pretty ugly (tons of curl_foo($curl_handle, ...) statements
Clarification:
I want something more high-level than the answers so far. For example, in Perl, you could do something like:
# navigate to the main page
$mech->get( 'http://www.somesite.com/' ); 

# follow a link that contains the text 'download this'
$mech->follow_link( text_regex => qr/download this/i );

# submit a POST form, to log into the site
$mech->submit_form(
    with_fields      => {
        username    => 'mungo',
        password    => 'lost-and-alone',
    }
);

# save the results as a file
$mech->save_content('somefile.zip');

To do the same thing using HTTP_Client or wget or CURL would be a lot of work, I'd have to manually parse the pages to find the links, find the form URL, extract all the hidden fields, and so on. The reason I'm asking for a PHP solution is that I have no experience with Perl, and I could probably build what I need with a lot of work, but it would be much quicker if I could do the above in PHP.
",15k,"
            26
        ","[""\nSimpleTest's ScriptableBrowser can be used independendly from the testing framework. I've used it for numerous automation-jobs.\n"", '\nI feel compelled to answer this, even though its an old post... I\'ve been working with PHP curl a lot and it is not as good anywhere near comparable to something like WWW:Mechanize, which I am switching to (I think I am going to go with the Ruby language implementation).. Curl is outdated as it requires too much ""grunt work"" to automate anything, the simpletest scriptable browser looked promising to me but in testing it, it won\'t work on most web forms I try it on... honestly, I think PHP is lacking in this category of scraping, web automation so its best to look at a different language, just wanted to post this since I have spent countless hours on this topic and maybe it will save someone else some time in the future.\n', '\nIt\'s 2016 now and there\'s Mink. It even supports different engines from headless pure-PHP ""browser"" (without JavaScript), over Selenium (which needs a browser like Firefox or Chrome) to a headless ""browser.js"" in NPM, which DOES support JavaScript.\n', '\nTry looking in the PEAR library. If all else fails, create an object wrapper for curl.\nYou can so something simple like this:\nclass curl {\n    private $resource;\n\n    public function __construct($url) {\n        $this->resource = curl_init($url);\n    }\n\n    public function __call($function, array $params) {\n        array_unshift($params, $this->resource);\n        return call_user_func_array(""curl_$function"", $params);\n    }\n}\n\n', ""\nTry one of the following:\n\nPEAR's HTTP_Request\nZend_Http_Client\n\n(Yes, it's ZendFramework code, but it doesn't make your class slower using it since it just loads the required libs.)\n"", '\nLook into Snoopy:\nhttp://sourceforge.net/projects/snoopy/\n', '\nCurl is the way to go for simple requests. It runs cross platform, has a PHP extension and is widely adopted and tested.\nI created a nice class that can GET and POST an array of data (INCLUDING FILES!) to a url by just calling CurlHandler::Get($url, $data) || CurlHandler::Post($url, $data). There\'s an optional HTTP User authentication option too :)\n/**\n * CURLHandler handles simple HTTP GETs and POSTs via Curl \n * \n * @package Pork\n * @author SchizoDuckie\n * @copyright SchizoDuckie 2008\n * @version 1.0\n * @access public\n */\nclass CURLHandler\n{\n\n    /**\n     * CURLHandler::Get()\n     * \n     * Executes a standard GET request via Curl.\n     * Static function, so that you can use: CurlHandler::Get(\'http://www.google.com\');\n     * \n     * @param string $url url to get\n     * @return string HTML output\n     */\n    public static function Get($url)\n    {\n       return self::doRequest(\'GET\', $url);\n    }\n\n    /**\n     * CURLHandler::Post()\n     * \n     * Executes a standard POST request via Curl.\n     * Static function, so you can use CurlHandler::Post(\'http://www.google.com\', array(\'q\'=>\'StackOverFlow\'));\n     * If you want to send a File via post (to e.g. PHP\'s $_FILES), prefix the value of an item with an @ ! \n     * @param string $url url to post data to\n     * @param Array $vars Array with key=>value pairs to post.\n     * @return string HTML output\n     */\n    public static function Post($url, $vars, $auth = false) \n    {\n       return self::doRequest(\'POST\', $url, $vars, $auth);\n    }\n\n    /**\n     * CURLHandler::doRequest()\n     * This is what actually does the request\n     * <pre>\n     * - Create Curl handle with curl_init\n     * - Set options like CURLOPT_URL, CURLOPT_RETURNTRANSFER and CURLOPT_HEADER\n     * - Set eventual optional options (like CURLOPT_POST and CURLOPT_POSTFIELDS)\n     * - Call curl_exec on the interface\n     * - Close the connection\n     * - Return the result or throw an exception.\n     * </pre>\n     * @param mixed $method Request Method (Get/ Post)\n     * @param mixed $url URI to get or post to\n     * @param mixed $vars Array of variables (only mandatory in POST requests)\n     * @return string HTML output\n     */\n    public static function doRequest($method, $url, $vars=array(), $auth = false)\n    {\n        $curlInterface = curl_init();\n\n        curl_setopt_array ($curlInterface, array( \n            CURLOPT_URL => $url,\n            CURLOPT_RETURNTRANSFER => 1,\n            CURLOPT_FOLLOWLOCATION =>1,\n            CURLOPT_HEADER => 0));\n        if (strtoupper($method) == \'POST\')\n        {\n            curl_setopt_array($curlInterface, array(\n                CURLOPT_POST => 1,\n                CURLOPT_POSTFIELDS => http_build_query($vars))\n            );  \n        }\n        if($auth !== false)\n        {\n              curl_setopt($curlInterface, CURLOPT_USERPWD, $auth[\'username\'] . "":"" . $auth[\'password\']);\n        }\n        $result = curl_exec ($curlInterface);\n        curl_close ($curlInterface);\n\n        if($result === NULL)\n        {\n            throw new Exception(\'Curl Request Error: \'.curl_errno($curlInterface) . "" - "" . curl_error($curlInterface));\n        }\n        else\n        {\n            return($result);\n        }\n    }\n\n}\n\n?>\n\n[edit] Read the clarification only now... You probably want to go with one of the tools mentioned above that automates stuff. You could also decide to use a clientside firefox extension like ChickenFoot for more flexibility. I\'ll leave the example class above here for future searches.\n', '\nIf you\'re using CakePHP in your project, or if you\'re inclined to extract the relevant library you can use their curl wrapper HttpSocket. It has the simple page-fetching syntax you describe, e.g., \n# This is the sugar for importing the library within CakePHP       \nApp::import(\'Core\', \'HttpSocket\');\n$HttpSocket = new HttpSocket();\n\n$result = $HttpSocket->post($login_url,\narray(\n  ""username"" => ""username"",\n  ""password"" => ""password""\n)\n);\n\n...although it doesn\'t have a way to parse the response page. For that I\'m going to use simplehtmldom: http://net.tutsplus.com/tutorials/php/html-parsing-and-screen-scraping-with-the-simple-html-dom-library/ which describes itself as having a jQuery-like syntax.\nI tend to agree that the bottom line is that PHP doesn\'t have the awesome scraping/automation libraries that Perl/Ruby have.\n', ""\nIf you're on a *nix system you could use shell_exec() with wget, which has a lot of nice options.\n""]",https://stackoverflow.com/questions/199045/is-there-a-php-equivalent-of-perls-wwwmechanize,automation
CRON job to run on the last day of the month,"
I need to create a CRON job that will run on the last day of every month.
I will create it using cPanel.
Any help is appreciated. 
Thanks
",237k,"
            138
        ","['\nPossibly the easiest way is to simply do three separate jobs:\n55 23 30 4,6,9,11        * myjob.sh\n55 23 31 1,3,5,7,8,10,12 * myjob.sh\n55 23 28 2               * myjob.sh\n\nThat will run on the 28th of February though, even on leap years so, if that\'s a problem, you\'ll need to find another way.\n\nHowever, it\'s usually both substantially easier and correct to run the job as soon as possible on the first day of each month, with something like:\n0 0 1 * * myjob.sh\n\nand modify the script to process the previous month\'s data.\nThis removes any hassles you may encounter with figuring out which day is the last of the month, and also ensures that all data for that month is available, assuming you\'re processing data. Running at five minutes to midnight on the last day of the month may see you missing anything that happens between then and midnight.\nThis is the usual way to do it anyway, for most end-of-month jobs.\n\nIf you still really want to run it on the last day of the month, one option is to simply detect if tomorrow is the first (either as part of your script, or in the crontab itself).\nSo, something like:\n55 23 28-31 * * [[ ""$(date --date=tomorrow +\\%d)"" == ""01"" ]] && myjob.sh\n\nshould be a good start, assuming you have a relatively intelligent date program.\nIf your date program isn\'t quite advanced enough to give you relative dates, you can just put together a very simple program to give you tomorrow\'s day of the month (you don\'t need the full power of date), such as:\n#include <stdio.h>\n#include <time.h>\n\nint main (void) {\n    // Get today, somewhere around midday (no DST issues).\n\n    time_t noonish = time (0);\n    struct tm *localtm = localtime (&noonish);\n    localtm->tm_hour = 12;\n\n    // Add one day (86,400 seconds).\n\n    noonish = mktime (localtm) + 86400;\n    localtm = localtime (&noonish);\n\n    // Output just day of month.\n\n    printf (""%d\\n"", localtm->tm_mday);\n\n    return 0;\n}\n\nand then use (assuming you\'ve called it tomdom for ""tomorrow\'s day of month""):\n55 23 28-31 * * [[ ""$(tomdom)"" == ""1"" ]] && myjob.sh\n\nThough you may want to consider adding error checking since both time() and mktime() can return -1 if something goes wrong. The code above, for reasons of simplicity, does not take that into account.\n', '\nThere\'s a slightly shorter method that can be used similar to one of the ones above. That is:\n[ $(date -d +1day +%d) -eq 1 ] && echo ""last day of month""\n\nAlso, the crontab entry could be update to only check on the 28th to 31st as it\'s pointless running it the other days of the month. Which would give you:\n0 23 28-31 * * [ $(date -d +1day +%d) -eq 1 ] && myscript.sh\n\n', '\nWhat about this one, after Wikipedia?\n55 23 L * * /full/path/to/command\n\n', '\nFor AWS Cloudwatch cron implementation (Scheduling Lambdas, etc..) this works:\n55 23 L * ? *\n\nRunning at 11:55pm on the last day of each month.\n', ""\nAdapting paxdiablo's solution, I run on the 28th and 29th of February.  The data from the 29th overwrites the 28th.\n# min  hr  date     month          dow\n  55   23  31     1,3,5,7,8,10,12   * /path/monthly_copy_data.sh\n  55   23  30     4,6,9,11          * /path/monthly_copy_data.sh\n  55   23  28,29  2                 * /path/monthly_copy_data.sh\n\n"", '\nYou could set up a cron job to run on every day of the month, and have it run a shell script like the following.  This script works out whether tomorrow\'s day number is less than today\'s (i.e. if tomorrow is a new month), and then does whatever you want.\nTODAY=`date +%d`\nTOMORROW=`date +%d -d ""1 day""`\n\n# See if tomorrow\'s day is less than today\'s\nif [ $TOMORROW -lt $TODAY ]; then\necho ""This is the last day of the month""\n# Do stuff...\nfi\n\n', '\nFor a safer method in a crontab based on @Indie solution (use absolute path to date + $() does not works on all crontab systems):\n0 23 28-31 * * [ `/bin/date -d +1day +\\%d` -eq 1 ] && myscript.sh\n\n', '\nSome cron implementations support the ""L"" flag to represent the last day of the month.\nIf you\'re lucky to be using one of those implementations, it\'s as simple as:\n0 55 23 L * ?\n\nThat will run at 11:55 pm on the last day of every month.\nhttp://www.quartz-scheduler.org/documentation/quartz-1.x/tutorials/crontrigger\n', '\n#########################################################\n# Memory Aid \n# environment    HOME=$HOME SHELL=$SHELL LOGNAME=$LOGNAME PATH=$PATH\n#########################################################\n#\n# string         meaning\n# ------         -------\n# @reboot        Run once, at startup.\n# @yearly        Run once a year, ""0 0 1 1 *"".\n# @annually      (same as @yearly)\n# @monthly       Run once a month, ""0 0 1 * *"".\n# @weekly        Run once a week, ""0 0 * * 0"".\n# @daily         Run once a day, ""0 0 * * *"".\n# @midnight      (same as @daily)\n# @hourly        Run once an hour, ""0 * * * *"".\n#mm     hh      Mday    Mon     Dow     CMD # minute, hour, month-day month DayofW CMD\n#........................................Minute of the hour\n#|      .................................Hour in the day (0..23)\n#|      |       .........................Day of month, 1..31 (mon,tue,wed)\n#|      |       |       .................Month (1.12) Jan, Feb.. Dec\n#|      |       |       |        ........day of the week 0-6  7==0\n#|      |       |       |        |      |command to be executed\n#V      V       V       V        V      V\n*       *       28-31   *       *       [ `date -d +\'1 day\' +\\%d` -eq 1 ] && echo ""Tomorrow is the first today now is  `date`"" >> ~/message\n1       0       1       *       *       rm -f ~/message\n*       *       28-31   *       *       [ `date -d +\'1 day\' +\\%d` -eq 1 ] && echo ""HOME=$HOME LOGNAME=$LOGNAME SHELL = $SHELL PATH=$PATH"" \n\n', ""\nSet up a cron job to run on the first day of the month. Then change the system's clock to be one day ahead.\n"", ""\n00 23 * * * [[ $(date +'%d') -eq $(cal | awk '!/^$/{ print $NF }' | tail -1) ]] && job\n\nCheck out a related question on the unix.com forum.\n"", '\nI found out solution (On the last day of the month) like below from this site.\n 0 0 0 L * ? *\n\nCRON details:\nSeconds Minutes Hours   Day Of Month    Month   Day Of Week  Year\n0       0       0       L               *       ?            *\n\nTo cross verify above expression,  click here which gives output like below.\n2021-12-31 Fri 00:00:00\n2022-01-31 Mon 00:00:00\n2022-02-28 Mon 00:00:00\n2022-03-31 Thu 00:00:00\n2022-04-30 Sat 00:00:00\n\n', ""\nYou can just connect all answers in one cron line and use only date command.\nJust check the difference between day of the month which is today and will be tomorrow:\n0 23 * * * root [ $(expr $(date +\\%d -d '1 days') - $(date +\\%d)  ) -le 0 ]  && echo true\n\nIf these difference is below 0 it means that we change the month and there is last day of the month.\n"", '\n55 23 28-31 * * echo ""[ $(date -d +1day +%d) -eq 1 ] && my.sh"" | /bin/bash \n\n', ""\nWhat about this? \nedit user's .bashprofile adding: \nexport LAST_DAY_OF_MONTH=$(cal | awk '!/^$/{ print $NF }' | tail -1)\n\nThen add this entry to crontab: \nmm hh * * 1-7 [[ $(date +'%d') -eq $LAST_DAY_OF_MONTH ]] && /absolutepath/myscript.sh\n\n"", ""\nIn tools like Jenkins, where usually there is no support for L nor tools similar to date, a cool trick might be setting up the timezone correctly. E.g. Pacific/Kiritimati is GMT+14:00, so if you're in Europe or in the US, this might do the trick.\nTZ=Pacific/Kiritimati \\n H 0 1 * * \n\nResult: Would last have run at Saturday, April 30, 2022 10:54:53 AM GMT; would next run at Tuesday, May 31, 2022 10:54:53 AM GMT.\n"", ""\nUse the below code to run cron on the last day of the month in PHP\n$commands = '30 23 '.date('t').' '.date('n').' *';\n\n"", '\nThe last day of month can be 28-31 depending on what month it is (Feb, March etc). However in either of these cases, the next day is always 1st of next month. So we can use that to make sure we run some job always on the last day of a month using the code below:\n0 8 28-31 * * [ ""$(date +%d -d tomorrow)"" = ""01"" ] && /your/script.sh\n\n', '\nNot sure of other languages but in javascript it is possible.\nIf you need your job to be completed before first day of month node-cron will allow you to set timezone - you have to set UTC+12:00 and if job is not too long most of the world will have results before start of their month.\n', '\nIf the day-of-the-month field could accept day zero that would very simply solve this problem. Eg. astronomers use day zero to express the last day of the previous month. So\n00 08 00 * * /usr/local/sbin/backup\n\nwould do the job in simple and easy way.\n', '\nBetter way to schedule cron on every next month of 1st day\nThis will run the command foo at 12:00AM.\n0 0 1 * * /usr/bin/foo\n', '\nBe cautious with ""yesterday"", ""today"", ""1day"" in the \'date\' program if running between midnight and 1am, because often those really mean ""24 hours"" which will be two days when daylight saving time change causes a 23 hour day.  I use ""date -d \'1am -12 hour\' ""\n']",https://stackoverflow.com/questions/6139189/cron-job-to-run-on-the-last-day-of-the-month,automation
How can I login to a website with Python?,"
How can I do it? 
I was trying to enter some specified link (with urllib), but to do it, I need to log in.
I have this source from the site:
<form id=""login-form"" action=""auth/login"" method=""post"">
    <div>
    <!--label for=""rememberme"">Remember me</label><input type=""checkbox"" class=""remember"" checked=""checked"" name=""remember me"" /-->
    <label for=""email"" id=""email-label"" class=""no-js"">Email</label>
    <input id=""email-email"" type=""text"" name=""handle"" value="""" autocomplete=""off"" />
    <label for=""combination"" id=""combo-label"" class=""no-js"">Combination</label>
    <input id=""password-clear"" type=""text"" value=""Combination"" autocomplete=""off"" />
    <input id=""password-password"" type=""password"" name=""password"" value="""" autocomplete=""off"" />
    <input id=""sumbitLogin"" class=""signin"" type=""submit"" value=""Sign In"" />

Is this possible?
",374k,"
            105
        ","['\nMaybe you want to use twill. It\'s quite easy to use and should be able to do what you want.\nIt will look like the following:\nfrom twill.commands import *\ngo(\'http://example.org\')\n\nfv(""1"", ""email-email"", ""blabla.com"")\nfv(""1"", ""password-clear"", ""testpass"")\n\nsubmit(\'0\')\n\nYou can use showforms() to list all forms once you used go… to browse to the site you want to login. Just try it from the python interpreter.\n', '\nLet me try to make it simple, suppose URL of the site is www.example.com and you need to sign up by filling username and password, so we go to the login page say http://www.example.com/login.php now and view it\'s source code and search for the action URL it will be in form tag something like \n <form name=""loginform"" method=""post"" action=""userinfo.php"">\n\nnow take userinfo.php to make absolute URL which will be \'http://example.com/userinfo.php\', now run a simple python script \nimport requests\nurl = \'http://example.com/userinfo.php\'\nvalues = {\'username\': \'user\',\n          \'password\': \'pass\'}\n\nr = requests.post(url, data=values)\nprint r.content\n\nI Hope that this helps someone somewhere someday.\n', '\nTypically you\'ll need cookies to log into a site, which means cookielib, urllib and urllib2. Here\'s a class which I wrote back when I was playing Facebook web games:\nimport cookielib\nimport urllib\nimport urllib2\n\n# set these to whatever your fb account is\nfb_username = ""your@facebook.login""\nfb_password = ""secretpassword""\n\nclass WebGamePlayer(object):\n\n    def __init__(self, login, password):\n        """""" Start up... """"""\n        self.login = login\n        self.password = password\n\n        self.cj = cookielib.CookieJar()\n        self.opener = urllib2.build_opener(\n            urllib2.HTTPRedirectHandler(),\n            urllib2.HTTPHandler(debuglevel=0),\n            urllib2.HTTPSHandler(debuglevel=0),\n            urllib2.HTTPCookieProcessor(self.cj)\n        )\n        self.opener.addheaders = [\n            (\'User-agent\', (\'Mozilla/4.0 (compatible; MSIE 6.0; \'\n                           \'Windows NT 5.2; .NET CLR 1.1.4322)\'))\n        ]\n\n        # need this twice - once to set cookies, once to log in...\n        self.loginToFacebook()\n        self.loginToFacebook()\n\n    def loginToFacebook(self):\n        """"""\n        Handle login. This should populate our cookie jar.\n        """"""\n        login_data = urllib.urlencode({\n            \'email\' : self.login,\n            \'pass\' : self.password,\n        })\n        response = self.opener.open(""https://login.facebook.com/login.php"", login_data)\n        return \'\'.join(response.readlines())\n\nYou won\'t necessarily need the HTTPS or Redirect handlers, but they don\'t hurt, and it makes the opener much more robust. You also might not need cookies, but it\'s hard to tell just from the form that you\'ve posted. I suspect that you might, purely from the \'Remember me\' input that\'s been commented out.\n', '\nWeb page automation ? Definitely ""webbot""\nwebbot even works web pages which have dynamically changing id and classnames and has more methods and features than selenium or mechanize.\n\nHere\'s a snippet :)\n\nfrom webbot import Browser \nweb = Browser()\nweb.go_to(\'google.com\') \nweb.click(\'Sign in\')\nweb.type(\'mymail@gmail.com\' , into=\'Email\')\nweb.click(\'NEXT\' , tag=\'span\')\nweb.type(\'mypassword\' , into=\'Password\' , id=\'passwordFieldId\') # specific selection\nweb.click(\'NEXT\' , tag=\'span\') # you are logged in ^_^\n\nThe docs are also pretty straight forward and simple to  use : https://webbot.readthedocs.io\n', ""\nimport cookielib\nimport urllib\nimport urllib2\n\nurl = 'http://www.someserver.com/auth/login'\nvalues = {'email-email' : 'john@example.com',\n          'password-clear' : 'Combination',\n          'password-password' : 'mypassword' }\n\ndata = urllib.urlencode(values)\ncookies = cookielib.CookieJar()\n\nopener = urllib2.build_opener(\n    urllib2.HTTPRedirectHandler(),\n    urllib2.HTTPHandler(debuglevel=0),\n    urllib2.HTTPSHandler(debuglevel=0),\n    urllib2.HTTPCookieProcessor(cookies))\n\nresponse = opener.open(url, data)\nthe_page = response.read()\nhttp_headers = response.info()\n# The login cookies should be contained in the cookies variable\n\nFor more information visit: https://docs.python.org/2/library/urllib2.html\n"", '\nWebsites in general can check authorization in many different ways, but the one you\'re targeting seems to make it reasonably easy for you.\nAll you need is to POST to the auth/login URL a form-encoded blob with the various fields you see there (forget the labels for, they\'re decoration for human visitors).  handle=whatever&password-clear=pwd and so on, as long as you know the values for the handle (AKA email) and password you should be fine.\nPresumably that POST will redirect you to some ""you\'ve successfully logged in"" page with a Set-Cookie header validating your session (be sure to save that cookie and send it back on further interaction along the session!).\n', '\nFor HTTP things, the current choice should be: Requests- HTTP for Humans\n']",https://stackoverflow.com/questions/2910221/how-can-i-login-to-a-website-with-python,automation
How to use Selenium with Python?,"
How do I set up Selenium to work with Python? I just want to write/export scripts in Python, and then run them. Are there any resources for that? I tried googling, but the stuff I found was either referring to an outdated version of Selenium (RC), or an outdated version of Python.
",116k,"
            51
        ","['\nYou mean Selenium WebDriver? \nHuh....\nPrerequisite: Install Python based on your OS\nInstall with following command \npip install -U selenium\n\nAnd use this module in your code \nfrom selenium import webdriver\n\nYou can also use many of the following as required \nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.common.exceptions import NoSuchElementException\n\nHere is an updated answer\nI would recommend you to run script without IDE... Here is my approach\n\nUSE IDE to find xpath of object / element\nAnd use find_element_by_xpath().click() \n\nAn example below shows login page automation \n#ScriptName : Login.py\n#---------------------\nfrom selenium import webdriver\n\n#Following are optional required\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.common.exceptions import NoSuchElementException\n\nbaseurl = ""http://www.mywebsite.com/login.php""\nusername = ""admin""\npassword = ""admin""\n\nxpaths = { \'usernameTxtBox\' : ""//input[@name=\'username\']"",\n           \'passwordTxtBox\' : ""//input[@name=\'password\']"",\n           \'submitButton\' :   ""//input[@name=\'login\']""\n         }\n\nmydriver = webdriver.Firefox()\nmydriver.get(baseurl)\nmydriver.maximize_window()\n\n#Clear Username TextBox if already allowed ""Remember Me"" \nmydriver.find_element_by_xpath(xpaths[\'usernameTxtBox\']).clear()\n\n#Write Username in Username TextBox\nmydriver.find_element_by_xpath(xpaths[\'usernameTxtBox\']).send_keys(username)\n\n#Clear Password TextBox if already allowed ""Remember Me"" \nmydriver.find_element_by_xpath(xpaths[\'passwordTxtBox\']).clear()\n\n#Write Password in password TextBox\nmydriver.find_element_by_xpath(xpaths[\'passwordTxtBox\']).send_keys(password)\n\n#Click Login button\nmydriver.find_element_by_xpath(xpaths[\'submitButton\']).click()\n\nThere is an another way that you can find xpath of any object -\n\nInstall Firebug and Firepath addons in firefox\nOpen URL in Firefox\nPress F12 to open Firepath developer instance \nSelect Firepath in below browser pane and chose select by ""xpath"" \nMove cursor of the mouse to element on webpage\nin the xpath textbox you will get xpath of an object/element.\nCopy Paste xpath to the script.\n\nRun script -\npython Login.py\n\nYou can also use a CSS selector instead of xpath. CSS selectors are slightly faster than xpath in most cases, and are usually preferred over xpath (if there isn\'t an ID attribute on the elements you\'re interacting with).\nFirepath can also capture the object\'s locator as a CSS selector if you move your cursor to the object. You\'ll have to update your code to use the equivalent find by CSS selector method instead -\nfind_element_by_css_selector(css_selector) \n\n', ""\nThere are a lot of sources for selenium - here is good one for simple use Selenium, and here is a example snippet too Selenium Examples\nYou can find a lot of good sources to use selenium, it's not too hard to get it set up and start using it.\n"", '\nYou just need to get selenium package imported, that you can do from command prompt using the command\npip install selenium\n\nWhen you have to use it in any IDE just import this package, no other documentation required to be imported\nFor Eg :\nimport selenium \nprint(selenium.__filepath__)\n\nThis is just a general command you may use in starting to check the filepath of selenium\n']",https://stackoverflow.com/questions/17540971/how-to-use-selenium-with-python,automation
How to put the WebBrowser control into IE9 into standards?,"
i am using automation (i.e. COM automation) to display some HTML in Internet Explorer (9):
ie = CoInternetExplorer.Create;
ie.Navigate2(""about:blank"");
webDocument = ie.Document;
webDocument.Write(szSourceHTML);
webDocument.Close();
ie.Visible = True;

Internet Explorer appears, showing my html, which starts off as:
<!DOCTYPE html>
<HTML>
<HEAD>
   ...


Note: the html5 standards-mode opt-in doctype html

Except that the document is not in ie9 standards mode; it's in ie8 standards mode:


If i save the html to my computer first:

and then view that html document, IE is put into standards mode:

My question is how update my SpawnIEWithSource(String html) function to throw the browser into standards mode?
void SpawnIEWithSource(String html)
{
   Variant ie = CoInternetExplorer.Create();
   ie.Navigate2(""about:blank"");
   webDocument = ie.Document;
   webDocument.Write(html);
   webDocument.Close();
   ie.Visible = true;
}


Edit: A more verbose, less understandable or readable code sample, that doesn't help further the question might be:
IWebBrowser2 ie;
CoCreateInstance(CLASS_InternetExplorer, null, CLSCTX_INPROC_SERVER | CLSCTX_LOCAL_SERVER, IID_WebBrowser2, ie);
ie.AddRef();
ie.Navigate2(""about:blank"");

IHtmlDocument doc;
dispDoc = ie.Document;
dispDoc.AddRef();
dispDoc.QueryInterface(IHTMLDocument2, doc);
dispDoc.Release()
doc.Write(html); 
doc.Close();
doc.Release();
ie.Visible = true;
ie.Release();


Update
Commenter asked on the ieblog entry Testing sites with Browser Mode vs. Doc Mode:

Can we get a description of how the document mode is determined when the HTML content is within an embedded webcontrol? Seems to be that the document mode is choosen differently - maybe for compatibility reasons?

MarkSil [MSFT] responded:

@Thomas: Thanks for raising that question. The WebBrowser Control determines the doc mode the same way that IE does because it contains the same web platform (e.g. there is one shared mshtml.dll across IE and WebBrowser Control hosts). The WebBrowser Control does default to the Compatibility View browser mode, which means that the default doc mode is IE7. Here is a blog post with more detail on this: blogs.msdn.com/.../more-ie8-extensibility-improvements.aspx.

To which Thomas responded:

@MarcSil (re: WebBrowser Control)
The problem with using registry entries to select document mode for WebControl is that it applies to the application as a whole. I write plugins for Google SketchUp where you have WebDialog windows to create UIs - it's just a WebBrowser control in a window. But that leads to problems as I want to force a document mode for my instance of the WebBrowser control, not for all of SU's WebBrowser controls as a whole.
So, my question is: how do you control the document mode per instance for a WebBrowser control?

",29k,"
            27
        ","['\nHave you tried setting in your html the\n<meta http-equiv=""X-UA-Compatible"" content=""IE=9"" />\n\nor\n<meta http-equiv=""X-UA-Compatible"" content=""IE=edge"" />\n\nwhich means latest version\n', '\nThe IE9 ""version"" of the WebBrowser control, like the IE8 version, is actually several browsers in one. Unlike the IE8 version, you do have a little more control over the rendering mode inside the page by changing the doctype. Of course, to change the browser mode you have to set your registry like the earlier answer. Here is the location of FEATURE_BROWSER_EMULATION:\nHKEY_LOCAL_MACHINE (or HKEY_CURRENT_USER)\n     SOFTWARE\n          Microsoft\n               Internet Explorer\n                    Main\n                         FeatureControl\n                              FEATURE_BROWSER_EMULATION\n                                   contoso.exe = (DWORD) 000090000\n\nHere is the complete set of codes:\n\n9999 (0x270F) - Internet Explorer 9.\nWebpages are displayed in IE9\nStandards mode, regardless of the\n!DOCTYPE directive. \n9000 (0x2328) - Internet Explorer 9. Webpages containing standards-based !DOCTYPE\ndirectives are displayed in IE9 mode.   \n8888 (0x22B8) -Webpages are\ndisplayed in IE8 Standards mode,\nregardless of the !DOCTYPE directive.\n8000 (0x1F40) - Webpages containing\nstandards-based !DOCTYPE directives\nare displayed in IE8 mode.\n7000 (0x1B58) - Webpages containing\nstandards-based !DOCTYPE directives\nare displayed in IE7 Standards mode.\n\nThe full docs:\nhttp://msdn.microsoft.com/en-us/library/ee330730%28VS.85%29.aspx#browser_emulation\n', '\nFEATURE_BROWSER_EMULATION does not works with CoInternetSetFeatureEnabled. The documentation of INTERNETFEATURELIST is not updated since IE7.\nSince the feature setting is under HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Internet Explorer\\Main\\FeatureControl you may be able to override the value in your process via a registry API hook. \n']",https://stackoverflow.com/questions/4097593/how-to-put-the-webbrowser-control-into-ie9-into-standards,automation
"C# WebBrowser Control - Form Submit Not Working using InvokeMember(""Click"")","
I am working on automated testing script and am using the WebBrowser control. I am trying to submit the following HTML and testing when the user accepts the terms of service:
    <form action=""http://post.dev.dealerconnextion/k/6hRbDTwn4xGVl2MHITQsBw/hrshq"" method=""post"">
        <input name=""StepCheck"" value=""U2FsdGVkX18zMTk5MzE5OUgFyFgD3V5yf5Rwbtfhf3gjdH4KSx4hqj4vkrw7K6e-"" type=""hidden"">
        <button type=""submit"" name=""continue"" value=""y"">ACCEPT the terms of use</button>
        <button type=""submit"" name=""continue"" value=""n"">DECLINE the terms of use</button>
    </form>

    // Terms of Use Information

    <form action=""http://post.dev.dealerconnextion/k/6hRbDTwn4xGVl2MHITQsBw/hrshq"" method=""post"">
        <input name=""StepCheck"" value=""U2FsdGVkX18zMTk5MzE5OUgFyFgD3V5yf5Rwbtfhf3gjdH4KSx4hqj4vkrw7K6e-"" type=""hidden"">
        <button type=""submit"" name=""continue"" value=""y"">ACCEPT the terms of use</button>
        <button type=""submit"" name=""continue"" value=""n"">DECLINE the terms of use</button>
    </form>

Here is the code in C#, but does not submit the form.
            HtmlElementCollection el = webBrowser.Document.GetElementsByTagName(""button"");
            foreach (HtmlElement btn in el)
            {
                if (btn.InnerText == ""ACCEPT the terms of use"")
                {
                    btn.InvokeMember(""Click"");
                }
            }

Any help would be much appreciated. Thanks.
",16k,"
            5
        ","['\nThe following code works for me, using the live form action URL from the question comments, tested with IE10. Try it as is. If it works for you as well, feel free to use it as a template for your web automation tasks. A couple of points:\n\nFEATURE_BROWSER_EMULATION is used to make sure the WebBrowser behaves in the same way as standalone IE browser (or as close as possible). This is a must for almost any WebBrowser-based project. I believe that\'s what should help to solve the original problem on your side.  \nAsynchronous code is used to improve the automation logic reliability, add support timeouts and cancellation and promote natural linear code flow (using async/await).\n\nC#:\nusing Microsoft.Win32;\nusing System;\nusing System.Diagnostics;\nusing System.Linq;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing System.Windows.Forms;\n\nnamespace WebAutomation\n{\n    // http://stackoverflow.com/q/19044659/1768303\n\n    public partial class MainForm : Form\n    {\n        WebBrowser webBrowser;\n\n        // non-deterministic delay to let AJAX code run\n        const int AJAX_DELAY = 1000;\n\n        // keep track of the main automation task\n        CancellationTokenSource mainCts;\n        Task mainTask = null;\n\n        public MainForm()\n        {\n            SetBrowserFeatureControl(); // set FEATURE_BROWSER_EMULATION first\n\n            InitializeComponent();\n\n            InitBrowser();\n\n            this.Load += (s, e) =>\n            {\n                // start the automation when form is loaded\n                // timeout the whole automation task in 30s\n                mainCts = new CancellationTokenSource(30000);\n                mainTask = DoAutomationAsync(mainCts.Token).ContinueWith((completedTask) =>\n                {\n                    Trace.WriteLine(String.Format(""Automation task status: {0}"", completedTask.Status.ToString()));\n                }, TaskScheduler.FromCurrentSynchronizationContext());\n            };\n\n            this.FormClosing += (s, e) =>\n            {\n                // cancel the automation if form closes\n                if (this.mainTask != null && !this.mainTask.IsCompleted)\n                    mainCts.Cancel();\n            };\n        }\n\n        // create a WebBrowser instance (could use an existing one)\n        void InitBrowser()\n        {\n            this.webBrowser = new WebBrowser();\n            this.webBrowser.Dock = DockStyle.Fill;\n            this.Controls.Add(this.webBrowser);\n            this.webBrowser.Visible = true;\n        }\n\n        // the main automation logic\n        async Task DoAutomationAsync(CancellationToken ct)\n        {\n            await NavigateAsync(ct, () => this.webBrowser.Navigate(""http://localhost:81/test.html""), 10000); // timeout in 10s\n            // page loaded, log the page\'s HTML\n            Trace.WriteLine(GetBrowserDocumentHtml());\n\n            // do the DOM automation\n            HtmlElementCollection all = webBrowser.Document.GetElementsByTagName(""button"");\n            // throw if none or more than one element found\n            HtmlElement btn = all.Cast<HtmlElement>().Single(\n                el => el.InnerHtml == ""ACCEPT the terms of use"");\n\n            ct.ThrowIfCancellationRequested();\n\n            // simulate a click which causes navigation\n            await NavigateAsync(ct, () => btn.InvokeMember(""click""), 10000); // timeout in 10s\n\n            // form submitted and new page loaded, log the page\'s HTML\n            Trace.WriteLine(GetBrowserDocumentHtml());\n\n            // could continue with another NavigateAsync\n            // othrwise, the automation session completed\n        }\n\n        // Get the full HTML content of the document\n        string GetBrowserDocumentHtml()\n        {\n            return this.webBrowser.Document.GetElementsByTagName(""html"")[0].OuterHtml;\n        }\n\n        // Async navigation\n        async Task NavigateAsync(CancellationToken ct, Action startNavigation, int timeout = Timeout.Infinite)\n        {\n            var onloadTcs = new TaskCompletionSource<bool>();\n            EventHandler onloadEventHandler = null;\n\n            WebBrowserDocumentCompletedEventHandler documentCompletedHandler = delegate\n            {\n                // DocumentCompleted may be called several time for the same page,\n                // beacuse of frames\n                if (onloadEventHandler != null || onloadTcs == null || onloadTcs.Task.IsCompleted)\n                    return;\n\n                // handle DOM onload event to make sure the document is fully loaded\n                onloadEventHandler = (s, e) =>\n                    onloadTcs.TrySetResult(true);\n                this.webBrowser.Document.Window.AttachEventHandler(""onload"", onloadEventHandler);\n            };\n\n            using (var cts = CancellationTokenSource.CreateLinkedTokenSource(ct))\n            {\n                if (timeout != Timeout.Infinite)\n                    cts.CancelAfter(Timeout.Infinite);\n\n                using (cts.Token.Register(() => onloadTcs.TrySetCanceled(), useSynchronizationContext: true)) \n                {\n                    this.webBrowser.DocumentCompleted += documentCompletedHandler;\n                    try \n                    {\n                        startNavigation();\n                        // wait for DOM onload, throw if cancelled\n                        await onloadTcs.Task;\n                        ct.ThrowIfCancellationRequested();\n                        // let AJAX code run, throw if cancelled\n                        await Task.Delay(AJAX_DELAY, ct);\n                    }\n                    finally \n                    {\n                        this.webBrowser.DocumentCompleted -= documentCompletedHandler;\n                        if (onloadEventHandler != null)\n                            this.webBrowser.Document.Window.DetachEventHandler(""onload"", onloadEventHandler);\n                    }\n                }\n            }\n        }\n\n        // Browser feature conntrol\n        void SetBrowserFeatureControl()\n        {\n            // http://msdn.microsoft.com/en-us/library/ee330720(v=vs.85).aspx\n\n            // FeatureControl settings are per-process\n            var fileName = System.IO.Path.GetFileName(Process.GetCurrentProcess().MainModule.FileName);\n\n            // make the control is not running inside Visual Studio Designer\n            if (String.Compare(fileName, ""devenv.exe"", true) == 0 || String.Compare(fileName, ""XDesProc.exe"", true) == 0)\n                return;\n\n            SetBrowserFeatureControlKey(""FEATURE_BROWSER_EMULATION"", fileName, GetBrowserEmulationMode()); // Webpages containing standards-based !DOCTYPE directives are displayed in IE10 Standards mode.\n        }\n\n        void SetBrowserFeatureControlKey(string feature, string appName, uint value)\n        {\n            using (var key = Registry.CurrentUser.CreateSubKey(\n                String.Concat(@""Software\\Microsoft\\Internet Explorer\\Main\\FeatureControl\\"", feature),\n                RegistryKeyPermissionCheck.ReadWriteSubTree))\n            {\n                key.SetValue(appName, (UInt32)value, RegistryValueKind.DWord);\n            }\n        }\n\n        UInt32 GetBrowserEmulationMode()\n        {\n            int browserVersion = 7;\n            using (var ieKey = Registry.LocalMachine.OpenSubKey(@""SOFTWARE\\Microsoft\\Internet Explorer"",\n                RegistryKeyPermissionCheck.ReadSubTree,\n                System.Security.AccessControl.RegistryRights.QueryValues))\n            {\n                var version = ieKey.GetValue(""svcVersion"");\n                if (null == version)\n                {\n                    version = ieKey.GetValue(""Version"");\n                    if (null == version)\n                        throw new ApplicationException(""Microsoft Internet Explorer is required!"");\n                }\n                int.TryParse(version.ToString().Split(\'.\')[0], out browserVersion);\n            }\n\n            UInt32 mode = 10000; // Internet Explorer 10. Webpages containing standards-based !DOCTYPE directives are displayed in IE10 Standards mode. Default value for Internet Explorer 10.\n            switch (browserVersion)\n            {\n                case 7:\n                    mode = 7000; // Webpages containing standards-based !DOCTYPE directives are displayed in IE7 Standards mode. Default value for applications hosting the WebBrowser Control.\n                    break;\n                case 8:\n                    mode = 8000; // Webpages containing standards-based !DOCTYPE directives are displayed in IE8 mode. Default value for Internet Explorer 8\n                    break;\n                case 9:\n                    mode = 9000; // Internet Explorer 9. Webpages containing standards-based !DOCTYPE directives are displayed in IE9 mode. Default value for Internet Explorer 9.\n                    break;\n                default:\n                    // use IE10 mode by default\n                    break;\n            }\n\n            return mode;\n        }\n    }\n}\n\nThe content of http://localhost:81/test.html:\n<!DOCTYPE html>\n<head>\n<meta http-equiv=""X-UA-Compatible"" content=""IE=edge""/>\n</head>\n<body>\n    <form action=""<the URL from OP\'s comments>"" method=""post"">\n        <input name=""StepCheck"" value=""U2FsdGVkX18zMTk5MzE5OUgFyFgD3V5yf5Rwbtfhf3gjdH4KSx4hqj4vkrw7K6e-"" type=""hidden"">\n        <button type=""submit"" name=""continue"" value=""y"">ACCEPT the terms of use</button>\n        <button type=""submit"" name=""continue"" value=""n"">DECLINE the terms of use</button>\n    </form>\n</body>\n\n', '\nThis works for me as follow. may that would be useful for someone.\nFirst I create an event handler for the button element when got focus. Once all the other form element are filled up with the appropriate values, You should give the focus to the button as follow:\nHtmlElement xUsername = xDoc.GetElementById(""username_txt"");\nHtmlElement xPassword = xDoc.GetElementById(""password_txt"");\nHtmlElement btnSubmit = xDoc.GetElementById(""btnSubmit"");\nif (xUsername != null && xPassword != null && btnSubmit != null)\n{\n    xUsername.SetAttribute(""value"", ""testUserName"");\n    xPassword.SetAttribute(""value"", ""123456789"");\n    btnSubmit.GotFocus += BtnSubmit_GotFocus;\n    btnSubmit.Focus();\n}\n\nThen event handler implementation would be like this:\nprivate void BtnSubmit_GotFocus(object sender, HtmlElementEventArgs e)\n{\n    var btnSubmit = sender as HtmlElement;\n    btnSubmit.RaiseEvent(""onclick"");\n    btnSubmit.InvokeMember(""click"");\n}\n\n', '\nIn my case I also couldn\'t get element clicked by simply invoking Click method of the found element.\nWhat worked is similar solution that Ali Tabandeh listed in his answer above:\n\nfind the needed html element\ndefine proper GotFocus event handler\nthen invoke Focus method of the found element.\n\nEvent handler for GotFocus should\n\nRaiseEvent ""onclick""\nInvoke ""click"" method\n\nThe problem was that this worked in my case from 3rd time (3 times needed to call htmlelement.Focus()).\n']",https://stackoverflow.com/questions/19044659/c-sharp-webbrowser-control-form-submit-not-working-using-invokememberclick,automation
webdriver.FirefoxProfile(): Is it possible to use a profile without making a copy of it?,"
As the documentation states, you can call webdriver.FirefoxProfile() with the optional argument of profile_directory to point to the directory of a specific profile you want the browser to use. I noticed it was taking a long time to run this command, so when I looked into the code, it looked like it was copying the specified profile Problem is, it takes an extremely long time for the profile to copy (something like >30 minutes, didn't have the patience to wait for it to finish.)
I'm using a hybrid of userscripts and selenium to do some automation for me, so to setup a new profile every single time I want to test out my code would be burdensome.
Is the only way to change this behaviour to edit the firefox_profile.py itself (if so, what would be the best way to go about it?)?
",10k,"
            6
        ","['\nAs per the current implementation of GeckoDriver with Firefox using the FirefoxProfile() works as follows :\n\nIf case of initiating a Browsing Session through a new Firefox Profile as follows :\nfrom selenium import webdriver\n\nmyprofile = webdriver.FirefoxProfile()\ndriver = webdriver.Firefox(firefox_profile=myprofile, executable_path=r\'C:\\Utility\\BrowserDrivers\\geckodriver.exe\')\ndriver.get(\'https://www.google.co.in\')\nprint(""Page Title is : %s"" %driver.title)\ndriver.quit()\n\nA new rust_mozprofile gets created on the run as follows :\n1521446301607   mozrunner::runner   INFO    Running command: ""C:\\\\Program Files\\\\Mozilla Firefox\\\\firefox.exe"" ""-marionette"" ""-profile"" ""C:\\\\Users\\\\ATECHM~1\\\\AppData\\\\Local\\\\Temp\\\\rust_mozprofile.xFayqKkZrOB8""\n\nOf-coarse on a successful closure (i.e. successful invocation of driver.quit()) the temporary rust_mozprofile.xFayqKkZrOB8 gets deleted/destroyed completely.\nAgain in case of initiating a Browsing Session through an existing Firefox Profile() as follows :\nfrom selenium import webdriver\n\nmyprofile = webdriver.FirefoxProfile(r\'C:\\Users\\AtechM_03\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\moskcpdq.SeleniumTest\')\ndriver = webdriver.Firefox(firefox_profile=myprofile, executable_path=r\'C:\\Utility\\BrowserDrivers\\geckodriver.exe\')\ndriver.get(\'https://www.google.co.in\')\nprint(""Page Title is : %s"" %driver.title)\ndriver.quit()\n\nSimilarly a new rust_mozprofile gets created on the run as follows :\n1521447102321   mozrunner::runner   INFO    Running command: ""C:\\\\Program Files\\\\Mozilla Firefox\\\\firefox.exe"" ""-marionette"" ""-profile"" ""C:\\\\Users\\\\ATECHM~1\\\\AppData\\\\Local\\\\Temp\\\\rust_mozprofile.2oSwrQwQoby9""\n\nSimilarly in this case as well on a successful closure (i.e. successful invocation of driver.quit()) the temporary rust_mozprofile.2oSwrQwQoby9 gets deleted/destroyed completely.\nSo the timespan you are observing is the time needed for a FirefoxProfile() to scoop out a new rust_mozprofile.\n\nPerhaps as per your question timespan for profile to copy (something like >30 minutes) is a pure overhead. So it won\'t be possible to use a Firefox Profile without making a copy of rust_mozprofile.\n\nSolution\n\nUpgrade Selenium Client to  current levels Version 3.11.0.\nUpgrade GeckoDriver to  current GeckoDriver v0.20.0 level.\nUpgrade Firefox version to Firefox Quantum v59.0.1 levels.\nClean your Project Workspace through your IDE and Rebuild your project with required dependencies only.\nUse CCleaner tool to wipe off all the OS chores before and after the execution of your test Suite.\nIf your base Firefox base version is too old, then uninstall it through Revo Uninstaller and install a recent GA and released version of Firefox Quantum.\nExecute your @Test.\n\n']",https://stackoverflow.com/questions/49356081/webdriver-firefoxprofile-is-it-possible-to-use-a-profile-without-making-a-cop,automation
Matlab: Running an m-file from command-line,"
Suppose that;
I have an m-file at location:
C:\M1\M2\M3\mfile.m
And exe file of the matlab is at this location:
C:\E1\E2\E3\matlab.exe
I want to run this m-file with Matlab, from command-line, for example inside a .bat file. How can I do this, is there a way to do it?
",214k,"
            124
        ","['\nA command like this runs the m-file successfully:\n""C:\\<a long path here>\\matlab.exe"" -nodisplay -nosplash -nodesktop -r ""run(\'C:\\<a long path here>\\mfile.m\'); exit;""\n', '\nI think that one important point that was not mentioned in the previous answers is that, if not explicitly indicated, the matlab interpreter will remain open.\nTherefore, to the answer of @hkBattousai I will add the exit command:\n""C:\\<a long path here>\\matlab.exe"" -nodisplay -nosplash -nodesktop -r ""run(\'C:\\<a long path here>\\mfile.m\');exit;""\n', '\nHere is what I would use instead, to gracefully handle errors from the script:\n""C:\\<a long path here>\\matlab.exe"" -nodisplay -nosplash -nodesktop -r ""try, run(\'C:\\<a long path here>\\mfile.m\'), catch, exit, end, exit""\n\nIf you want more verbosity:\n""C:\\<a long path here>\\matlab.exe"" -nodisplay -nosplash -nodesktop -r ""try, run(\'C:\\<a long path here>\\mfile.m\'), catch me, fprintf(\'%s / %s\\n\',me.identifier,me.message), end, exit""\n\nI found the original reference here. Since original link is now gone, here is the link to an alternate newreader still alive today:\n\nexit matlab when running batch m file\n\n', '\nOn Linux you can do the same and you can actually send back to the shell a custom error code, like the following:\n#!/bin/bash\nmatlab -nodisplay -nojvm -nosplash -nodesktop -r \\ \n      ""try, run(\'/foo/bar/my_script.m\'), catch, exit(1), end, exit(0);""\necho ""matlab exit code: $?""\n\nit prints matlab exit code: 1 if the script throws an exception, matlab exit code: 0 otherwise.\n', '\nHere are the steps:\n\nStart the command line.\nEnter the folder containing the .m file with cd C:\\M1\\M2\\M3\nRun the following: C:\\E1\\E2\\E3\\matlab.exe -r mfile\n\nWindows systems will use your current folder as the location for MATLAB to search for .m files, and the -r option tries to start the given .m file as soon as startup occurs.\n', '\nSince R2019b, there is a new command line option, -batch. It replaces -r, which is no longer recommended. It also unifies the syntax across platforms. See for example the documentation for Windows, for the other platforms the description is identical.\nmatlab -batch ""statement to run""\n\nThis starts MATLAB without the desktop or splash screen, logs all output to stdout and stderr, exits automatically when the statement completes, and provides an exit code reporting success or error.\nIt is thus no longer necessary to use try/catch around the code to run, and it is no longer necessary to add an exit statement.\n', '\ncat 1.m | matlab -nodesktop -nosplash\n\nAnd I use Ubuntu\n', ""\nThanks to malat. Your comment helped me. \nBut I want to add my try-catch block, as I found the MExeption method getReport() that returns the whole error message and prints it to the matlab console.\nAdditionally I printed the filename as this compilation is part of a batch script that calls matlab.\ntry\n    some_code\n    ...\ncatch message\n    display(['ERROR in file: ' message.stack.file])\n    display(['ERROR: ' getReport(message)])\nend;\n\nFor a false model name passed to legacy code generation method, the output would look like:\nERROR in file: C:\\..\\..\\..\nERROR: Undefined function or variable 'modelname'.\n\nError in sub-m-file (line 63)\nlegacy_code( 'slblock_generate', specs, modelname);\n\nError in m-file (line 11)\nsub-m-file\n\nError in run (line 63)\nevalin('caller', [script ';']);\n\nFinally, to display the output at the windows command prompt window, just log the matlab console to a file with -logfile logfile.txt (use additionally -wait) and call the batch command type logfile.txt\n"", '\nI run this command within a bash script, in particular to submit SGE jobs and batch process things:\n/Path_to_matlab -nodisplay -nosplash -nodesktop < m_file.m\n\n', ""\nSince none of the answers has information about feeding input argument, it is important to \nadd it here. After some research, I found this link\nFeeding the arguments is very similar to how we run a Matlab function. \nmatlab -r 'try myfunction(argument1,argument2); catch; end; quit'\n\nIf you are somehow getting an argument from bash/terminal, you simply need to insert that into the bash command as: \nmatlab -r 'try myfunction($MY_BASH_ARG,argument2); catch; end; quit'\n\n(This is after a couple of trial and error)\n""]",https://stackoverflow.com/questions/6657005/matlab-running-an-m-file-from-command-line,automation
Handling Browser Authentication using Selenium,"
Does anyone know about handling Browser Authentication using Selenium or any other tool during automation? 
",87k,"
            19
        ","[""\nEDIT in 2015:\nThis answer is outdated. WebDriver nowadays supports authentication! See How to handle authentication popup with Selenium WebDriver using Java\n\nOriginal answer:\nThis is not handled very well by Selenium.\n\nYou can try using http://username:password@example.com/yourpage\ninstead of just http://example.com/yourpage\nHowever, as far as I know, Firefox will still pop up a browser dialog requesting a confirmation.\n\nYou can try Robot if you're using Java (or any similar tool like AutoIt).\n\nYou could use driver.manage().addCookie() if you're using WebDriver.\n\nOr a custom FirefoxProfile that has already passed the authentication once.\n"", ""\nI spent days on this - literally.\nTrying to get past browser level authentication within my company network to hit an application.\nThe solution was to use the 'unsername:password@' component within the URL, BUT to add a forward slash at the end of the login URL.\nSo total login URL looks like this (note the '/' after yourpage):\nhttp://username:password@example.com/yourpage/\nWorks with Watir, Capybara and Selenium Webdriver.\n"", '\nEverything I have read on the Web didn\'t help me. So before making a request, like this:\ndriver.get(url);\n\nyou have to run a new thread like this:\nRunScript runScript = new RunScript();\nrunScript.start();\n\nIn this case you are free to input login and password on another thread of follwing class\npublic class RunScript extends Thread {\n\n@Override\npublic void run() {\n    try {\n        File file = new File(""D:\\\\jacob-1.18-x86.dll"");\n        System.setProperty(LibraryLoader.JACOB_DLL_PATH, file.getAbsolutePath());\n        AutoItX autoIt = new AutoItX();\n        Thread.sleep(2000);\n        autoIt.winActivate(""yourWindowName"", """");\n        autoIt.winWaitActive(""yourWindowName"");\n        if (autoIt.winExists(""yourWindowName"")) {\n            autoIt.send(""username{TAB}"", false);\n            autoIt.send(""password{Enter}"", false);\n            }\n        }\n    } catch (InterruptedException ex) {\n        //\n    }\n}\n}\n\n', '\nAll the hacks via auto-it, sikuli, etc. just wasting your time when you\'ll run it in your CI solution, using several browser types / OS / Version / Resolutions etc.\nThe way to do it correctly is to identify the authentication actual method and perform a login using Rest protocol for instance.\nI used it to get the JSESIONID cookie and insert it to the selenium driver.\nhint on that: go to a non-exiting url of the domian first, then set the cookie, then go to the required url - you are logged-in.\nuse: rest client authentication to get the JSESSION ID \nand With this information:\nbrowser().navigate(foo.getUrl()+""non-exiting-url"");\n\n//the information got from the rest client login:\nCookie cookie = new Cookie(name, value, domain, path, expiry, isSecure, isHttpOnly);\n\ntry {\n    driver.manage().addCookie(cookie);\n} catch (Exception e) {\n    System.out.println(e.toString());\n}\n\nbrowser().navigate(foo.getUrl());\n\n', '\nyou can use auto IT script to handle this problem\nWinWaitActive(""[CLASS:Chrome_WidgetWin_1]"", """", time)\nSend(""user"")\nSend(""{TAB}"")\nSend(""pass"")\nSend(""{ENTER}"")\n\n', '\nwith Chrome 70 and other versions :\nhttp://username:password@example.com/yourpage\n\n', '\nYou can use Java Robot class with Selenium 2 /Selenium WebDriver using Firefox\nWebDriver driver = new FirefoxDriver();\n    driver.get(""http://localhost:9990"");\n\n    WebElement myDynamicElement = driver.findElement(By.id(""app""));\n\n    Alert alert = driver.switchTo().alert();\n\n\n    try {\n        Robot robot = new Robot();\n        alert.sendKeys(""username"");\n\n        robot.keyPress(KeyEvent.VK_TAB);//go to password feild\n\n        robot.keyPress(KeyEvent.VK_P);\n        robot.keyPress(KeyEvent.VK_A);\n        robot.keyPress(KeyEvent.VK_S);\n        robot.keyPress(KeyEvent.VK_S);\n\n        robot.keyPress(KeyEvent.VK_ENTER);\n\n\n        } catch (AWTException e) {\n        e.printStackTrace();\n        }\n\n    }\n\nUsing Selenium with Robot\nhttp://docs.oracle.com/javase/1.5.0/docs/api/java/awt/Robot.html\n']",https://stackoverflow.com/questions/10395462/handling-browser-authentication-using-selenium,automation
How to get the range of occupied cells in excel sheet,"
I use C# to automate an excel file. I was able to get the workbook and the sheets it contains.
If for example I have in sheet1 two cols and 5 rows. I wanted o get the range for the occupied cells as A1:B5. I tried the following code but it did not give the correct result.
the columns # and row # were much bigger and the cells were empty as well. 
     Excel.Range xlRange = excelWorksheet.UsedRange;
     int col = xlRange.Columns.Count;
     int row = xlRange.Rows.Count;

Is there another way I can use to get that range?
",117k,"
            35
        ","['\nI had a very similar issue as you had. What actually worked is this:\niTotalColumns = xlWorkSheet.UsedRange.Columns.Count;\niTotalRows = xlWorkSheet.UsedRange.Rows.Count;\n\n//These two lines do the magic.\nxlWorkSheet.Columns.ClearFormats();\nxlWorkSheet.Rows.ClearFormats();\n\niTotalColumns = xlWorkSheet.UsedRange.Columns.Count;\niTotalRows = xlWorkSheet.UsedRange.Rows.Count;\n\nIMHO what happens is that when you delete data from Excel, it keeps on thinking that there is data in those cells, though they are blank. When I cleared the formats, it removes the blank cells and hence returns actual counts.\n', '\nExcel.Range last = sheet.Cells.SpecialCells(Excel.XlCellType.xlCellTypeLastCell, Type.Missing);\nExcel.Range range = sheet.get_Range(""A1"", last);\n\n""range"" will now be the occupied cell range\n', '\nSee the Range.SpecialCells method. For example, to get cells with constant values or formulas use:\n_xlWorksheet.UsedRange.SpecialCells(\n        Microsoft.Office.Interop.Excel.XlCellType.xlCellTypeConstants |\n        Microsoft.Office.Interop.Excel.XlCellType.xlCellTypeFormulas)\n\n', '\nThe only way I could get it to work in ALL scenarios (except Protected sheets) (based on Farham\'s Answer):\nIt supports:\n\nScanning Hidden Row / Columns\nIgnores formatted cells with no data / formula\n\nCode:\n// Unhide All Cells and clear formats\nsheet.Columns.ClearFormats();\nsheet.Rows.ClearFormats();\n\n// Detect Last used Row - Ignore cells that contains formulas that result in blank values\nint lastRowIgnoreFormulas = sheet.Cells.Find(\n                ""*"",\n                System.Reflection.Missing.Value,\n                InteropExcel.XlFindLookIn.xlValues,\n                InteropExcel.XlLookAt.xlWhole,\n                InteropExcel.XlSearchOrder.xlByRows,\n                InteropExcel.XlSearchDirection.xlPrevious,\n                false,\n                System.Reflection.Missing.Value,\n                System.Reflection.Missing.Value).Row;\n// Detect Last Used Column  - Ignore cells that contains formulas that result in blank values\nint lastColIgnoreFormulas = sheet.Cells.Find(\n                ""*"",\n                System.Reflection.Missing.Value,\n                System.Reflection.Missing.Value,\n                System.Reflection.Missing.Value,\n                InteropExcel.XlSearchOrder.xlByColumns,\n                InteropExcel.XlSearchDirection.xlPrevious,\n                false,\n                System.Reflection.Missing.Value,\n                System.Reflection.Missing.Value).Column;\n\n// Detect Last used Row / Column - Including cells that contains formulas that result in blank values\nint lastColIncludeFormulas = sheet.UsedRange.Columns.Count;\nint lastColIncludeFormulas = sheet.UsedRange.Rows.Count;\n\n', '\ndim lastRow as long   \'in VBA it\'s a long \nlastrow = wks.range(""A65000"").end(xlup).row\n\n', '\nBit old question now, but if somebody is looking for solution this works for me.\nusing Excel = Microsoft.Office.Interop.Excel;\n\nExcel.ApplicationClass excel = new Excel.ApplicationClass();\nExcel.Application app = excel.Application;\nExcel.Range all = app.get_Range(""A1:H10"", Type.Missing);\n\n', ""\nThese two lines on their own wasnt working for me:\nxlWorkSheet.Columns.ClearFormats();\nxlWorkSheet.Rows.ClearFormats();\n\nYou can test by hitting ctrl+end in the sheet and seeing which cell is selected. \nI found that adding this line after the first two solved the problem in all instances I've encountered:\nExcel.Range xlActiveRange = WorkSheet.UsedRange;\n\n"", '\nYou should try the currentRegion property, if you know from where you are to find the range. This will give you the boundaries of your used range.\n', '\nThis is tailored to finding formulas but you should be able to expand it to general content by altering how you test the starting cells.  You\'ll have to handle single cell ranges outside of this.\n    public static Range GetUsedPartOfRange(this Range range)\n    {\n        Excel.Range beginCell = range.Cells[1, 1];\n        Excel.Range endCell = range.Cells[range.Rows.Count, range.Columns.Count];\n\n        if (!beginCell.HasFormula)\n        {\n            var beginCellRow = range.Find(\n                ""*"",\n                beginCell,\n                XlFindLookIn.xlFormulas,\n                XlLookAt.xlPart,\n                XlSearchOrder.xlByRows,\n                XlSearchDirection.xlNext,\n                false);\n\n            var beginCellCol = range.Find(\n                ""*"",\n                beginCell,\n                XlFindLookIn.xlFormulas,\n                XlLookAt.xlPart,\n                XlSearchOrder.xlByColumns,\n                XlSearchDirection.xlNext,\n                false);\n\n            if (null == beginCellRow || null == beginCellCol)\n                return null;\n\n            beginCell = range.Worksheet.Cells[beginCellRow.Row, beginCellCol.Column];\n        }\n\n        if (!endCell.HasFormula)\n        {\n            var endCellRow = range.Find(\n            ""*"",\n            endCell,\n            XlFindLookIn.xlFormulas,\n            XlLookAt.xlPart,\n            XlSearchOrder.xlByRows,         \n            XlSearchDirection.xlPrevious,\n            false);\n\n            var endCellCol = range.Find(\n                ""*"",\n                endCell,\n                XlFindLookIn.xlFormulas,\n                XlLookAt.xlPart,\n                XlSearchOrder.xlByColumns,\n                XlSearchDirection.xlPrevious,\n                false);\n\n            if (null == endCellRow || null == endCellCol)\n                return null;\n\n            endCell = range.Worksheet.Cells[endCellRow.Row, endCellCol.Column];\n        }\n\n        if (null == endCell || null == beginCell)\n            return null;\n\n        Excel.Range finalRng = range.Worksheet.Range[beginCell, endCell];\n\n        return finalRng;\n    }\n}\n\n', '\nYou should not delete the data in box by pressing ""delete"", i think thats the problem , because excel will still detected the box as """" <- still have value, u should delete by right click the box and click delete.\n']",https://stackoverflow.com/questions/1284388/how-to-get-the-range-of-occupied-cells-in-excel-sheet,automation
Selenium Webdriver + Java - Eclipse: java.lang.NoClassDefFoundError,"
I installed JDK from here: http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
(This version for windows x64: Java SE Development Kit 8u151)
I downloaded eclipse from here:
http://www.eclipse.org/downloads/packages/eclipse-ide-java-developers/oxygenr
(Windows 64-bit)
I opened a new project in eclipse: File->New->Java Project
Then I downloaded Selenium Java Jars from here:
http://www.seleniumhq.org/download/ ---> java language
Then in eclipse I click on my project -> properties ->Java Build Path -> Libraries tab -> Add External JARs... -> I go to ""SeleniumDrivers\Java"" library (there I saved all the JARS that I downloaded) -> I checked all the files there:
these files
I clicked on ""ok"" and created a new class in eclipse
Then I downloaded chromedriver from here: http://www.seleniumhq.org/download
I unzipped it and saved it here: C:\Selenium\Drivers
This is my script:
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.chrome.ChromeDriver;

public class MainClass {

    public static void main(String[] args) {
        // TODO Auto-generated method stub
        System.out.println(""hi there\n"");

        System.setProperty(""webdriver.chrome.driver"", 
        ""C:/Selenium/Drivers/chromedriver.exe"");
        WebDriver driver = new ChromeDriver();
        driver.get(""https://www.facebook.com"");
    }

}

As you can see, this is a very basic script which opens chrome browser and navigate to facebook.
I run this script and got this error:
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/http/config/RegistryBuilder
    at org.openqa.selenium.remote.internal.HttpClientFactory.getClientConnectionManager(HttpClientFactory.java:69)
    at org.openqa.selenium.remote.internal.HttpClientFactory.<init>(HttpClientFactory.java:57)
    at org.openqa.selenium.remote.internal.HttpClientFactory.<init>(HttpClientFactory.java:60)
    at org.openqa.selenium.remote.internal.ApacheHttpClient$Factory.getDefaultHttpClientFactory(ApacheHttpClient.java:242)
    at org.openqa.selenium.remote.internal.ApacheHttpClient$Factory.<init>(ApacheHttpClient.java:219)
    at org.openqa.selenium.remote.HttpCommandExecutor.getDefaultClientFactory(HttpCommandExecutor.java:93)
    at org.openqa.selenium.remote.HttpCommandExecutor.<init>(HttpCommandExecutor.java:72)
    at org.openqa.selenium.remote.service.DriverCommandExecutor.<init>(DriverCommandExecutor.java:63)
    at org.openqa.selenium.chrome.ChromeDriverCommandExecutor.<init>(ChromeDriverCommandExecutor.java:36)
    at org.openqa.selenium.chrome.ChromeDriver.<init>(ChromeDriver.java:181)
    at org.openqa.selenium.chrome.ChromeDriver.<init>(ChromeDriver.java:168)
    at org.openqa.selenium.chrome.ChromeDriver.<init>(ChromeDriver.java:123)
    at MainClass.main(MainClass.java:11)
Caused by: java.lang.ClassNotFoundException: org.apache.http.config.RegistryBuilder
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    ... 13 more

I don't know how to resolve this issue, can you please help to solve it so that I will be able to run my basic script?
",18k,"
            0
        ","[""\njava.lang.NoClassDefFoundError is observed when the JRE can't find a Class.\nIn simple words the required imports or jar files are not available. From the snapshot you have shared its pretty much evident that you have tried to add the Java Client related jars.\nIn this case you need to follow the following steps:\n\nRemove all the jars referring to previous versions of Selenium standalone server & Selenium Java client\nImport only the selenium-server-standalone-3.7.0.\nIn your IDE within Project menu, select the option Build Automatically and execute the Clean option for all of your Projects.\nExecute your Test.\n\n"", ""\nSeems like the latest (v3.7) Selenium-Java zip file contains lesser jars in the lib folder. v3.6 contained 10 lib jars but v3.7 contains only 7 jars. \nThe missing jar which is causing all the issue for the op is 'httpcore-4.4.6.jar'. I am not sure whether the removal of jar is intentional or not. Maybe chromedriver has catch up with Selenium java 3.7seeing that .\nI the mean time use Selenium Java 3.6. Don't forget to add the /lib folder as well.\nhttp://selenium-release.storage.googleapis.com/3.6/selenium-java-3.6.0.zip\n"", ""\ni've added the three missing jars from version 3.6 and fixed everything.\nhttp://selenium-release.storage.googleapis.com/3.6/selenium-java-3.6.0.zip\n"", '\nupdate the the appium java-client to 7.3.0 and selenium-java to 3.141.59 this resolved my issue hope it helps.\n', ""\nI faced the same issue. For me, it didn't found the WebDriver. It seemed to happen as I imported the libraries the location other than classpath. Then I opened up a new project, went to the Properties>Java Build Path>Libraries. This time I imported the libraries under classpath. Now it works fine.\n"", ""\nFor those who are using Appium java client with Selenium, don't try to import Java client and selenium dependencies together in your pom.xml, you have already selenium dependencies imported with the java client dependency, you only have to import Java client dependency in your pom.xml and it should work.\nReference: https://mvnrepository.com/artifact/io.appium/java-client/7.6.0\n"", ""\nFor me, the issue was solved by adding the external jars (selenium-java jar files) to 'Classpath', instead of to 'Modulepath'\n\n""]",https://stackoverflow.com/questions/47104058/selenium-webdriver-java-eclipse-java-lang-noclassdeffounderror,automation
How to use ADB to send touch events to device using sendevent command?,"
I am trying to send touch events to a device using AndroidDebugBridge, so that I can do some basic automation for UI tests. I have followed the discussion in LINK. I am able to use sendevent to simulate touch on emulators, but unable to do the same on a device. 
Like in above link the emulator seems to send out 6 events for each touch ( xcoord, ycoord, 2 for press,2 for release) and it was easy to use this information to sendevents, but a getevent for the touchscreen for a device seems to generate far too many events.
Has somebody managed to send touch from ADB to a device? Could you please share the solution.
",202k,"
            104
        ","[""\nAndroid comes with an input command-line tool that can simulate miscellaneous input events. To simulate tapping, it's:\ninput tap x y\n\nYou can use the adb shell ( > 2.3.5) to run the command remotely:\nadb shell input tap x y\n\n"", '\nIn order to do a particular action (for example to open the web browser), you need to first figure out where to tap.  To do that, you can first run:\nadb shell getevent -l\n\nOnce you press on the device, at the location that you want, you will see this output:\n<...>\n/dev/input/event3: EV_KEY       BTN_TOUCH            DOWN\n/dev/input/event3: EV_ABS       ABS_MT_POSITION_X    000002f5\n/dev/input/event3: EV_ABS       ABS_MT_POSITION_Y    0000069e\n\nadb is telling you that a key was pressed (button down) at position 2f5, 69e in hex which is 757 and 1694 in decimal.\nIf you now want to generate the same event, you can use the input tap command at the same position:\nadb shell input tap 757 1694\n\nMore info can be found at:\nhttps://source.android.com/devices/input/touch-devices.html\nhttp://source.android.com/devices/input/getevent.html\n', '\n2.3.5 did not have input tap, just input keyevent and input text\nYou can use the monkeyrunner for it: (this is a copy of the answer at https://stackoverflow.com/a/18959385/1587329):\n\nYou might want to use monkeyrunner like this:\n\n$ monkeyrunner\n>>> from com.android.monkeyrunner import MonkeyRunner, MonkeyDevice\n>>> device = MonkeyRunner.waitForConnection()\n>>> device.touch(200, 400, MonkeyDevice.DOWN_AND_UP)\n\n\nYou can also do a drag, start activies etc.\n  Have a look at the api for MonkeyDevice.\n\n', ""\nYou don't need to use \n\nadb shell getevent -l\n\ncommand, you just need to enable in Developer Options on the device [Show Touch data] to get X and Y. \nSome more information can be found in my article here: https://mobileqablog.wordpress.com/2016/08/20/android-automatic-touchscreen-taps-adb-shell-input-touchscreen-tap/\n"", '\nBuilding on top of Tomas\'s answer, this is the best approach of finding the location tap position as an integer I found:\nadb shell getevent -l | grep ABS_MT_POSITION --line-buffered | awk \'{a = substr($0,54,8); sub(/^0+/, """", a); b = sprintf(""0x%s"",a); printf(""%d\\n"",strtonum(b))}\'\n\nUse adb shell getevent -l to get a list of events, the using grep for ABS_MT_POSITION (gets the line with touch events in hex) and finally use awk to get the relevant hex values, strip them of zeros and convert hex to integer. This continuously prints the x and y coordinates in the terminal only when you press on the device.\nYou can then use this adb shell command to send the command:\nadb shell input tap x y\n\n', ""\nConsider using Android's uiautomator, with adb shell uiautomator [...] or directly using the .jar that comes with the SDK.\n""]",https://stackoverflow.com/questions/3437686/how-to-use-adb-to-send-touch-events-to-device-using-sendevent-command,automation
Automating running command on Linux from Windows using PuTTY,"
I have a scenario where I need to run a linux shell command frequently (with different filenames) from windows. I am using PuTTY and WinSCP to do that (requires login name and password).  The file is copied to a predefined folder in the linux machine through WinSCP and then the command is run from PuTTY. Is there a way by which I can automate this through a program. Ideally I would like to right click the file from windows and issue the command which would copy the file to remote machine and run the predefined command (in PuTTy) with the filename as argument.
",267k,"
            71
        ","['\nPutty usually comes with the ""plink"" utility.\nThis is essentially the ""ssh"" command line command implemented as a windows .exe.\nIt pretty well documented in the putty manual under ""Using the command line tool plink"".\nYou just need to wrap a command like:\nplink root@myserver /etc/backups/do-backup.sh\n\nin a .bat script.\nYou can also use common shell constructs, like semicolons to execute multiple commands. e.g:\nplink read@myhost ls -lrt /home/read/files;/etc/backups/do-backup.sh\n\n', '\nThere could be security issues with common methods for auto-login. \nOne of the most easiest ways is documented below:\n\nRunning Putty from the Windows Command Line\n\nAnd as for the part the executes the command\nIn putty UI, Connection>SSH>  there\'s a field for remote command.\n\n4.17 The SSH panel\nThe SSH panel allows you to configure\n  options that only apply to SSH\n  sessions.\n4.17.1 Executing a specific command on the server\nIn SSH, you don\'t have to run a\n  general shell session on the server.\n  Instead, you can choose to run a\n  single specific command (such as a\n  mail user agent, for example). If you\n  want to do this, enter the command in\n  the ""Remote command"" box.\n  http://the.earth.li/~sgtatham/putty/0.53/htmldoc/Chapter4.html\n\nin short, your answers might just as well be similar to the text below:  \n\nlet Putty run command in remote server\n\n', '\nYou can write a TCL script and establish SSH session to that Linux machine and issue commands automatically. Check http://wiki.tcl.tk/11542 for a short tutorial.\n', '\nYou can create a putty session, and auto load the script on the server, when starting the session:\nputty -load ""sessionName"" \n\nAt remote command, point to the remote script.\n', '\nYou can do both tasks (the upload and the command execution) using WinSCP. Use WinSCP script like:\noption batch abort\noption confirm off\nopen your_session\nput %1%\ncall script.sh\nexit\n\nReference for the call command:\nhttps://winscp.net/eng/docs/scriptcommand_call\nReference for the %1% syntax:\nhttps://winscp.net/eng/docs/scripting#syntax\nYou can then run the script like:\nwinscp.exe /console /script=script_path\\upload.txt /parameter file_to_upload.dat\n\nActually, you can put a shortcut to the above command to the Windows Explorer\'s Send To menu, so that you can then just right-click any file and go to the Send To > Upload using WinSCP and Execute Remote Command (=name of the shortcut).\nFor that, go to the folder %USERPROFILE%\\SendTo and create a shortcut with the following target:\nwinscp_path\\winscp.exe /console /script=script_path\\upload.txt /parameter %1\n\nSee Creating entry in Explorer\'s ""Send To"" menu.\n', '\nHere is a totally out of the box solution.\n\nInstall AutoHotKey (ahk)\nMap the script to a key (e.g. F9)\nIn the ahk script,\na) Ftp the commands (.ksh) file to the linux machine\nb) Use plink like below. Plink should be installed if you have putty.\n\n\nplink sessionname -l username -pw password test.ksh\n\nor\n\nplink -ssh example.com -l username -pw password test.ksh\n\nAll the steps will be performed in sequence whenever you press F9 in windows.\n', '\nCode:\nusing System;\nusing System.Diagnostics;\nnamespace playSound\n{\n    class Program\n    {\n        public static void Main(string[] args)\n        {\n            Console.WriteLine(args[0]);\n\n            Process amixerMediaProcess = new Process();\n            amixerMediaProcess.StartInfo.CreateNoWindow = false;\n            amixerMediaProcess.StartInfo.UseShellExecute = false;\n            amixerMediaProcess.StartInfo.ErrorDialog = false;\n            amixerMediaProcess.StartInfo.RedirectStandardOutput = false;\n            amixerMediaProcess.StartInfo.RedirectStandardInput = false;\n            amixerMediaProcess.StartInfo.RedirectStandardError = false;\n            amixerMediaProcess.EnableRaisingEvents = true;\n\n            amixerMediaProcess.StartInfo.Arguments = string.Format(""{0}"",""-ssh username@""+args[0]+"" -pw password -m commands.txt"");\n            amixerMediaProcess.StartInfo.FileName = ""plink.exe"";\n            amixerMediaProcess.Start();\n\n\n            Console.Write(""Presskey to continue . . . "");\n            Console.ReadKey(true);\n    }\n}\n\n}\nSample commands.txt:\nps\nLink: https://huseyincakir.wordpress.com/2015/08/27/send-commands-to-a-remote-device-over-puttyssh-putty-send-command-from-command-line/\n', '\nTry MtPutty,\nyou can automate the ssh login in it. Its a great tool especially if you need to login to multiple servers many times. Try it here\nAnother tool worth trying is TeraTerm. Its really easy to use for the ssh automation stuff. You can get it here. But my favorite one is always MtPutty.\n', '\nIn case you are using Key based authentication, using saved Putty session seems to work great, for example to run a shell script on a remote server(In my case an ec2).Saved configuration will take care of authentication.\nC:\\Users> plink saved_putty_session_name path_to_shell_file/filename.sh\nPlease remember if you save your session with name like(user@hostname), this command would not work as it will be treated as part of the remote command.\n']",https://stackoverflow.com/questions/6147203/automating-running-command-on-linux-from-windows-using-putty,automation
Disposing of Microsoft.Office.Interop.Word.Application,"
(Somewhat of a follow on from the post (which remains unanswered): https://stackoverflow.com/q/6197829/314661)
Using the following code
Application app = new Application();
_Document doc = app.Documents.Open(""myDocPath.docx"", false, false, false);
doc.PrintOut(false);
doc.Close();

I am attempting to open and print a file programmatically.
The problem is each time I run the above code a new WINWORD.exe process is started and obviously this quickly eats up all the memory.
The application class doesn't seem to contain a dispose/close or similar method.
After a bit of research I (realized) and changed the code to the following.
 Application app = new Application();
 _Document doc = app.Documents.Open(fullFilePath + "".doc"", false, false, false);
 doc.PrintOut(false);
 doc.Close();
 int res = System.Runtime.InteropServices.Marshal.ReleaseComObject(doc);
 int res1 = System.Runtime.InteropServices.Marshal.ReleaseComObject(app);

And I can see the remaining reference count is zero but the processes remain? 
PS: I'm using Version 14 of the Microsoft.Office.Interop library.
",77k,"
            42
        ","['\nDo you not need to call Quit?\napp.Quit();\n\n', '\nPerhaps try setting doc = null and calling GC.Collect()\nEdit, not really my own code I forget where I got it but this is what I use to dispose of Excel, and it does the job maybe you can glean something from this:\npublic void DisposeExcelInstance()\n{\n    app.DisplayAlerts = false;\n    workBook.Close(null, null, null);\n    app.Workbooks.Close();\n    app.Quit();\n    if (workSheet != null)\n        System.Runtime.InteropServices.Marshal.ReleaseComObject(workSheet);\n    if (workBook != null)\n        System.Runtime.InteropServices.Marshal.ReleaseComObject(workBook);\n    if (app != null)\n        System.Runtime.InteropServices.Marshal.ReleaseComObject(app);\n    workSheet = null;\n    workBook = null;\n    app = null;\n    GC.Collect(); // force final cleanup!\n}\n\n', '\nI think the main issue, which nobody seems to have picked up on, is that you shouldn\'t be creating a new Application object in the first place if Word is already open.\nThose of us who have been coding since the days of COM and/or VB6 will remember GetActiveObject. Fortunately .Net only requires a ProgID.\nThe recommended way of doing this is as follows:\ntry\n{\n    wordApp = (word.Application) Marshal.GetActiveObject(""Word.Application"");\n}\ncatch(COMException ex) when (ex.HResult == -2147221021)\n{\n    wordApp = new word.Application();\n}\n\n', '\nThe best solution.. last:\ntry {\n\n    Microsoft.Office.Interop.Word.Application appWord = new Microsoft.Office.Interop.Word.Application();\n    appWord.Visible = false;\n    Microsoft.Office.Interop.Word.Document doc = null;\n    wordDocument = appWord.Documents.Open((INP), ReadOnly: true);\n\n    wordDocument.ExportAsFixedFormat(OUTP, Microsoft.Office.Interop.Word.WdExportFormat.wdExportFormatPDF);\n\n    // doc.Close(false); // Close the Word Document.\n    appWord.Quit(false); // Close Word Application.\n} catch (Exception ex) {\n    Console.WriteLine(ex.Message + ""     "" + ex.InnerException);\n}\n\n', '\nYou need to calls app.Quit() to close the application. I used below code & it worked like a charm for me - \ntry\n{\n   Microsoft.Office.Interop.Word.Application wordApp = new Microsoft.Office.Interop.Word.Application();\n   wordApp.Visible = false;\n   Microsoft.Office.Interop.Word.Document doc = null;\n\n   //Your code here...\n\n   doc.Close(false); // Close the Word Document.\n   wordApp.Quit(false); // Close Word Application.\n}\ncatch (Exception ex)\n{\n   MessageBox.Show(ex.Message + ""     "" + ex.InnerException);\n}\nfinally\n{\n   // Release all Interop objects.\n   if (doc != null)\n      System.Runtime.InteropServices.Marshal.ReleaseComObject(doc);\n   if (wordApp != null)\n      System.Runtime.InteropServices.Marshal.ReleaseComObject(wordApp);\n   doc = null;\n   wordApp = null;\n   GC.Collect();\n}\n\n', ""\nAgreed with other posters that GC.Collect() and Marshal.ReleaseComObject() is not needed. If the process still exists after running app.Quit(false), it might be because you're running the app invisible, and there is a prompt that is preventing the application from closing, such as a Document Recovery dialog. If that's the case, you need to add this when creating your application.\napp.DisplayAlerts = false;\n\n"", '\nI close the document, then the application, that works for me, then force garbage collection.\n// Document\nobject saveOptionsObject = saveDocument ? Word.WdSaveOptions.wdSaveChanges : Word.WdSaveOptions.wdDoNotSaveChanges;\nthis.WordDocument.Close(ref saveOptionsObject, ref Missing.Value, ref Missing.Value);\n\n// Application\nobject saveOptionsObject = Word.WdSaveOptions.wdDoNotSaveChanges;\nthis.WordApplication.Quit(ref saveOptionsObject, ref Missing.Value, ref Missing.Value); \n\nGC.Collect();\nGC.WaitForPendingFinalizers();\n\n', '\nTry this..\ndoc.Close(false);\napp.Quit(false);\nif (doc != null)\n    System.Runtime.InteropServices.Marshal.ReleaseComObject(doc);\nif (app != null)\n    System.Runtime.InteropServices.Marshal.ReleaseComObject(app);\ndoc = null;\napp = null;\nGC.Collect();\n\n']",https://stackoverflow.com/questions/6777422/disposing-of-microsoft-office-interop-word-application,automation
Selenium Webdriver: How to Download a PDF File with Python?,"
I am using selenium webdriver to automate downloading several PDF files. I get the PDF preview window (see below), and now I would like to download the file. How can I accomplish this using Google Chrome as the browser?  

",53k,"
            17
        ","['\nTry this code, it worked for me.\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option(\'prefs\', {\n""download.default_directory"": ""C:/Users/XXXX/Desktop"", #Change default directory for downloads\n""download.prompt_for_download"": False, #To auto download the file\n""download.directory_upgrade"": True,\n""plugins.always_open_pdf_externally"": True #It will not show PDF directly in chrome\n})\nself.driver = webdriver.Chrome(options=options)\n\n', '\nI found this piece of code somewhere on Stackoverflow itself and it serves the purpose for me without having to use selenium at all.\nimport urllib.request\n\nresponse = urllib.request.urlopen(URL)    \nfile = open(""FILENAME.pdf"", \'wb\')\nfile.write(response.read())\nfile.close()\n\n', '\nYou can download the pdf (Embeded pdf & Normal pdf) from web using selenium.\nfrom selenium import webdriver\n\ndownload_dir = ""C:\\\\Users\\\\omprakashpk\\\\Documents"" # for linux/*nix, download_dir=""/usr/Public""\noptions = webdriver.ChromeOptions()\n\nprofile = {""plugins.plugins_list"": [{""enabled"": False, ""name"": ""Chrome PDF Viewer""}], # Disable Chrome\'s PDF Viewer\n               ""download.default_directory"": download_dir , ""download.extensions_to_open"": ""applications/pdf""}\noptions.add_experimental_option(""prefs"", profile)\ndriver = webdriver.Chrome(\'C:\\\\chromedriver\\\\chromedriver_2_32.exe\', chrome_options=options)  # Optional argument, if not specified will search path.\n\ndriver.get(`pdf_url`)\n\nIt will download and save the pdf in directory specified. Change the download_dir location and chrome driver location as per your convenience. \nYou can download chrome driver from here.\nHope it helps!\n', '\nI did it and it worked, don\'t ask me how :)\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option(\'prefs\', {\n#""download.default_directory"": ""C:/Users/517/Download"", #Change default directory for downloads\n#""download.prompt_for_download"": False, #To auto download the file\n#""download.directory_upgrade"": True,\n""plugins.always_open_pdf_externally"": True #It will not show PDF directly in chrome \n})\ndriver = webdriver.Chrome(options=options)\n\n', '\nIn My case it worked without any code modification,Just need to disabled the Chrome pdf viewer\nHere are the steps to disable it\n\nGo into Chrome Settings\nScroll to the bottom click on Advanced\nUnder Privacy And Security - Click on ""Site Settings""\nScroll to PDF Documents\nEnable ""Download PDF files instead of automatically opening them in Chrome""\n\n']",https://stackoverflow.com/questions/43149534/selenium-webdriver-how-to-download-a-pdf-file-with-python,automation
How can I pass an argument to a PowerShell script?,"
There's a PowerShell script named itunesForward.ps1 that makes iTunes fast forward 30 seconds:
$iTunes = New-Object -ComObject iTunes.Application

if ($iTunes.playerstate -eq 1)
{
  $iTunes.PlayerPosition = $iTunes.PlayerPosition + 30
}

It is executed with a prompt line command:
powershell.exe itunesForward.ps1

Is it possible to pass an argument from the command line and have it applied in the script instead of the hardcoded 30 seconds value?
",848k,"
            551
        ","['\nTested as working:\n#Must be the first statement in your script (not counting comments)\nparam([Int32]$step=30) \n\n$iTunes = New-Object -ComObject iTunes.Application\n\nif ($iTunes.playerstate -eq 1)\n{\n  $iTunes.PlayerPosition = $iTunes.PlayerPosition + $step\n}\n\nCall it with\npowershell.exe -file itunesForward.ps1 -step 15\n\nMultiple parameters syntax (comments are optional, but allowed):\n<#\n    Script description.\n\n    Some notes.\n#>\nparam (\n    # height of largest column without top bar\n    [int]$h = 4000,\n    \n    # name of the output image\n    [string]$image = \'out.png\'\n)\n\nAnd some example for advanced parameters, e.g. Mandatory:\n<#\n    Script description.\n\n    Some notes.\n#>\nparam (\n    # height of largest column without top bar\n    [Parameter(Mandatory=$true)]\n    [int]$h,\n    \n    # name of the output image\n    [string]$image = \'out.png\'\n)\n\nWrite-Host ""$image $h""\n\nA default value will not work with a mandatory parameter. You can omit the =$true for advanced parameters of type boolean [Parameter(Mandatory)].\n', ""\nYou can use also the $args variable (that's like position parameters):\n$step = $args[0]\n\n$iTunes = New-Object -ComObject iTunes.Application\n\nif ($iTunes.playerstate -eq 1)\n{\n  $iTunes.PlayerPosition = $iTunes.PlayerPosition + $step\n}\n\nThen it can be called like:\npowershell.exe -file itunersforward.ps1 15\n\n"", '\nCall the script from a batch file (*.bat) or CMD\nPowerShell Core\npwsh.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 -Param1 Hello -Param2 World""\n\npwsh.exe -NoLogo -ExecutionPolicy Bypass -Command ""path-to-script/Script.ps1 -Param1 Hello -Param2 World""\n\npwsh.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 Hello -Param2 World""\n\npwsh.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 Hello World""\n\npwsh.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 -Param2 World Hello""\n\nPowerShell\npowershell.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 -Param1 Hello -Param2 World""\n\npowershell.exe -NoLogo -ExecutionPolicy Bypass -Command ""path-to-script/Script.ps1 -Param1 Hello -Param2 World""\n\npowershell.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 Hello -Param2 World""\n\npowershell.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 Hello World""\n\npowershell.exe -NoLogo -ExecutionPolicy Bypass -Command ""./Script.ps1 -Param2 World Hello""\n\n\nCall from PowerShell\nPowerShell Core or Windows PowerShell\n& path-to-script/Script.ps1 -Param1 Hello -Param2 World\n& ./Script.ps1 -Param1 Hello -Param2 World\n\n\nScript.ps1 - Script Code\nparam(\n    [Parameter(Mandatory=$True, Position=0, ValueFromPipeline=$false)]\n    [System.String]\n    $Param1,\n\n    [Parameter(Mandatory=$True, Position=1, ValueFromPipeline=$false)]\n    [System.String]\n    $Param2\n)\n\nWrite-Host $Param1\nWrite-Host $Param2\n\n', ""\nLet PowerShell analyze and decide the data type. It internally uses a 'Variant' for this.\nAnd generally it does a good job...\nparam($x)\n$iTunes = New-Object -ComObject iTunes.Application\nif ($iTunes.playerstate -eq 1)\n{\n    $iTunes.PlayerPosition = $iTunes.PlayerPosition + $x\n}\n\nOr if you need to pass multiple parameters:\nparam($x1, $x2)\n$iTunes = New-Object -ComObject iTunes.Application\nif ($iTunes.playerstate -eq 1)\n{\n    $iTunes.PlayerPosition = $iTunes.PlayerPosition + $x1\n    $iTunes.<AnyProperty>  = $x2\n}\n\n"", '\n# ENTRY POINT MAIN()\nParam(\n    [Parameter(Mandatory=$True)]\n    [String] $site,\n    [Parameter(Mandatory=$True)]\n    [String] $application,\n    [Parameter(Mandatory=$True)]\n    [String] $dir,\n    [Parameter(Mandatory=$True)]\n    [String] $applicationPool\n)\n\n# Create Web IIS Application\nfunction ValidateWebSite ([String] $webSiteName)\n{\n    $iisWebSite = Get-Website -Name $webSiteName\n    if($Null -eq $iisWebSite)\n    {\n        Write-Error -Message ""Error: Web Site Name: $($webSiteName) not exists.""  -Category ObjectNotFound\n    }\n    else\n    {\n        return 1\n    }\n}\n\n# Get full path from IIS WebSite\nfunction GetWebSiteDir ([String] $webSiteName)\n{\n    $iisWebSite = Get-Website -Name $webSiteName\n    if($Null -eq $iisWebSite)\n    {\n        Write-Error -Message ""Error: Web Site Name: $($webSiteName) not exists.""  -Category ObjectNotFound\n    }\n    else\n    {\n        return $iisWebSite.PhysicalPath\n    }\n}\n\n# Create Directory\nfunction CreateDirectory([string]$fullPath)\n{\n    $existEvaluation = Test-Path $fullPath -PathType Any\n    if($existEvaluation -eq $false)\n    {\n        new-item $fullPath -itemtype directory\n    }\n    return 1\n}\n\nfunction CreateApplicationWeb\n{\n    Param(\n        [String] $WebSite,\n        [String] $WebSitePath,\n        [String] $application,\n        [String] $applicationPath,\n        [String] $applicationPool\n        )\n    $fullDir = ""$($WebSitePath)\\$($applicationPath)""\n    CreateDirectory($fullDir)\n    New-WebApplication -Site $WebSite -Name $application -PhysicalPath $fullDir -ApplicationPool $applicationPool -Force\n}\n\n$fullWebSiteDir = GetWebSiteDir($Site)f($null -ne $fullWebSiteDir)\n{\n    CreateApplicationWeb -WebSite $Site -WebSitePath $fullWebSiteDir -application $application  -applicationPath $dir -applicationPool $applicationPool\n}\n\n', ""\nCreate a PowerShell script with the following code in the file.\nparam([string]$path)\nGet-ChildItem $path | Where-Object {$_.LinkType -eq 'SymbolicLink'} | select name, target\n\nThis creates a script with a path parameter. It will list all symbolic links within the path provided as well as the specified target of the symbolic link.\n"", ""\nYou can also define a variable directly in the PowerShell command line and then execute the script. The variable will be defined there, too. This helped me in a case where I couldn't modify a signed script.\nExample:\n PS C:\\temp> $stepsize = 30\n PS C:\\temp> .\\itunesForward.ps1\n\nwith iTunesForward.ps1 being\n$iTunes = New-Object -ComObject iTunes.Application\n\nif ($iTunes.playerstate -eq 1)\n{\n  $iTunes.PlayerPosition = $iTunes.PlayerPosition + $stepsize\n}\n\n""]",https://stackoverflow.com/questions/5592531/how-can-i-pass-an-argument-to-a-powershell-script,automation
"How can I automate the ""generate scripts"" task in SQL Server Management Studio 2008?","
I'd like to automate the script generation in SQL Server Management Studio 2008.
Right now what I do is :

Right click on my database, Tasks, ""Generate Scripts...""
manually select all the export options I need, and hit select all on the ""select object"" tab
Select the export folder
Eventually hit the ""Finish"" button

Is there a way to automate this task?
Edit : I want to generate creation scripts, not change scripts.
",97k,"
            112
        ","['\nSqlPubwiz has very limited options compared to the script generation in SSMS. By contrast the options available with SMO almost exactly match those in SSMS, suggesting it is probably even the same code. (I would hope MS didn\'t write it twice!) There are several examples on MSDN like this one that show scripting tables as individual objects. However if you want everything to script correctly with a \'full\' schema that includes \'DRI\' (Declarative Referential Integrity) objects like foreign keys then scripting tables individually doesn\'t work the dependencies out correctly. I found it is neccessary to collect all the URNs and hand them to the scripter as an array. This code, modified from the example, works for me (though I daresay you could tidy it up and comment it a bit more):\n    using Microsoft.SqlServer.Management.Smo;\n    using Microsoft.SqlServer.Management.Sdk.Sfc;\n    // etc...\n\n    // Connect to the local, default instance of SQL Server. \n    Server srv = new Server();\n\n    // Reference the database.  \n    Database db = srv.Databases[""YOURDBHERE""];\n\n    Scripter scrp = new Scripter(srv);\n    scrp.Options.ScriptDrops = false;\n    scrp.Options.WithDependencies = true;\n    scrp.Options.Indexes = true;   // To include indexes\n    scrp.Options.DriAllConstraints = true;   // to include referential constraints in the script\n    scrp.Options.Triggers = true;\n    scrp.Options.FullTextIndexes = true;\n    scrp.Options.NoCollation = false;\n    scrp.Options.Bindings = true;\n    scrp.Options.IncludeIfNotExists = false;\n    scrp.Options.ScriptBatchTerminator = true;\n    scrp.Options.ExtendedProperties = true;\n\n    scrp.PrefetchObjects = true; // some sources suggest this may speed things up\n\n    var urns = new List<Urn>();\n\n    // Iterate through the tables in database and script each one   \n    foreach (Table tb in db.Tables)\n    {\n        // check if the table is not a system table\n        if (tb.IsSystemObject == false)\n        {\n            urns.Add(tb.Urn);\n        }\n    }\n\n    // Iterate through the views in database and script each one. Display the script.   \n    foreach (View view in db.Views)\n    {\n        // check if the view is not a system object\n        if (view.IsSystemObject == false)\n        {\n            urns.Add(view.Urn);\n        }\n    }\n\n    // Iterate through the stored procedures in database and script each one. Display the script.   \n    foreach (StoredProcedure sp in db.StoredProcedures)\n    {\n        // check if the procedure is not a system object\n        if (sp.IsSystemObject == false)\n        {\n            urns.Add(sp.Urn);\n        }\n    }\n\n    StringBuilder builder = new StringBuilder();\n    System.Collections.Specialized.StringCollection sc = scrp.Script(urns.ToArray());\n    foreach (string st in sc)\n    {\n        // It seems each string is a sensible batch, and putting GO after it makes it work in tools like SSMS.\n        // Wrapping each string in an \'exec\' statement would work better if using SqlCommand to run the script.\n        builder.AppendLine(st);\n        builder.AppendLine(""GO"");\n    }\n\n    return builder.ToString();\n\n', ""\nWhat Brann is mentioning from the Visual Studio 2008 SP1 Team Suite is version 1.4 of the Database Publishing Wizard. It's installed with sql server 2008 (maybe only professional?) to \\Program Files\\Microsoft SQL Server\\90\\Tools\\Publishing\\1.4. The VS call from server explorer is simply calling this. You can achieve the same functionality via the command line like:\nsqlpubwiz help script\n\nI don't know if v1.4 has the same troubles that v1.1 did (users are converted to roles, constraints are not created in the right order), but it is not a solution for me because it doesn't script objects to different files like the Tasks->Generate Scripts option in SSMS does. I'm currently using a modified version of Scriptio (uses the MS SMO API) to act as an improved replacement for the database publishing wizard (sqlpubwiz.exe). It's not currently scriptable from the command line, I might add that contribution in the future.\nScriptio was originally posted on Bill Graziano's blog, but has subsequently been released to CodePlex by Bill and updated by others. Read the discussion to see how to compile for use with SQL Server 2008.\nhttp://scriptio.codeplex.com/\nEDIT: I've since started using RedGate's SQL Compare product to do this. It's a very nice replacement for all that sql publishing wizard should have been. You choose a database, backup, or snapshot as the source, and a folder as the output location and it dumps everything nicely into a folder structure. It happens to be the same format that their other product, SQL Source Control, uses.\n"", ""\nI wrote an open source command line utility named SchemaZen that does this. It's much faster than scripting from management studio and it's output is more version control friendly. It supports scripting both schema and data. \nTo generate scripts run:\nschemazen.exe script --server localhost --database db --scriptDir c:\\somedir\nThen to recreate the database from scripts run:\nschemazen.exe create --server localhost --database db --scriptDir c:\\somedir\n"", '\nYou can use SQL Server Management Object (SMO) to automate SQL Server 2005 management tasks including generating scripts: http://msdn.microsoft.com/en-us/library/ms162169.aspx.\n', ""\nIf you're a developer, definitely go with SMO.  Here's a link to the Scripter class, which is your starting point:\nScripter Class\n"", ""\nI don't see powershell with SQLPSX mentioned in any of these answers... I personally haven't played with it but it looks beautifully simple to use and ideally suited to this type of automation tasks, with tasks like:\nGet-SqlDatabase -dbname test -sqlserver server | Get-SqlTable | Get-SqlScripter | Set-Content -Path C:\\script.sql\nGet-SqlDatabase -dbname test -sqlserver server | Get-SqlStoredProcedure | Get-SqlScripter\nGet-SqlDatabase -dbname test -sqlserver server | Get-SqlView | Get-SqlScripter\n\n(ref: http://www.sqlservercentral.com/Forums/Topic1167710-1550-1.aspx#bm1168100)\nProject page: http://sqlpsx.codeplex.com/\nThe main advantage of this approach is that it combines the configurablity / customizability of using SMO directly, with the convenience and maintainability of using a simple existing tool like the Database Publishing Wizard.\n"", ""\nIn Tools > Options > Designers > Table and Database Designers there's an option for 'Auto generate change scripts' that will generate one for every change you make at the time you save it.\n"", '\nYou can do it with T-SQL code using the INFORMATION_SCHEMA tables.\nThere are also third-party tools - I like Apex SQL Script for precisely the use you are talking about.  I run it completely from the command-line.\n', '\nIf you want to a Microsoft solution you can try: Microsoft SQL Server Database Publishing Wizard 1.1\nhttp://www.microsoft.com/downloads/details.aspx?FamilyId=56E5B1C5-BF17-42E0-A410-371A838E570A&displaylang=en\nIt create a batch process you can run anytime you need to rebuild the scripts.\n', '\nTry new SQL Server command line tools to generate T-SQL scripts and monitor Dynamic Management Views.\nWorked for me like charm. It is a new python based tool from Microsoft that runs from command line.\nEverything works like described on the Microsoft page (see link below)\nWorked for me with SQL 2012 server.\nYou install it with pip:\n\n$pip install mssql-scripter\n\nCommand parameter overview as usual with h for help:\n\nmssql-scripter -h\n\nHint:\nIf you log in to SQL-Server via Windows authentication, just leave away Username and password.\nhttps://cloudblogs.microsoft.com/sqlserver/2017/05/17/try-new-sql-server-command-line-tools-to-generate-t-sql-scripts-and-monitor-dynamic-management-views/\n', ""\nI've been using DB Comparer - Its free and no fuss script entire DB and can compare to another DB and also produce a Diff script . Excellent for Development to Production change scripts. \nhttp://www.dbcomparer.com/\n"", '\nFrom Visual Studio 2008 SP1 TeamSuite :\nIn the Server Explorer / Data Connections tab, there\'s a publish to provider tool which does the same as ""Microsoft SQL Server Database Publishing Wizard"", but which is compatible with MS Sql Server 2008.\n', '\nThere is also this simple command line tool I build for my needs.\nhttp://mycodepad.wordpress.com/2013/11/18/export-ms-sql-database-schema-with-c/\nIt can export an entire db, and it tries to export encrypted objects. Everything is stored in folders and separate sql files for easy file comparison. \nCode is also available on github.\n', '\nI am using VS 2012(for DBs on MSSQL Server 2008) compare database has an option to save it, the comparison and options. This is essentially  what are your settings for delivery. After that you can do update or generate script.\nI just find it it a little bit awkward to load it from file later(drag and drop from windows explorer) as I do not see the file in solution explorer.\n']",https://stackoverflow.com/questions/483568/how-can-i-automate-the-generate-scripts-task-in-sql-server-management-studio-2,automation
Fastest way to interface between live (unsaved) Excel data and C# objects,"
I want to know what the fastest way is of reading and writing data to and from an open Excel workbook to c# objects.   The background is that I want to develop a c# application that is used from Excel and uses data held in excel.  
The business logic will reside in the c# application but the data will reside in an Excel workbook.  The user will be using Excel and will click a button (or do something similar) on the excel workbook to initiate the c# application.  The c# application will then read data off the excel workbook, process the data, and then write data back to the excel workbook.
There may be numerous blocks of data that are required to be read off and written back to the excel workbook but they will normally be of a relatively small size, say 10 rows and 20 columns.  Occasionally a large list of data may need to be processed, of the order of 50,000 rows and 40 columns.
I know that this is relatively easy to do say using VSTO but I want to know what the fastest (but still robust and elegant) solution is and get an idea of the speed.   I don't mind if the solution recommends using third party products or uses C++.
The obvious solution is using VSTO or interop but I don't know what the performance is like versus VBA which I'm currently using to read in the data, or if there are any other solutions.
This was posted on experts exchange saying that VSTO was dramatically slower than VBA but that was a couple of years ago and I don't know if the performance has improved.
http://www.experts-exchange.com/Microsoft/Development/VSTO/Q_23635459.html
Thanks.
",22k,"
            32
        ","['\nI\'ll take this as a challenge, and will bet the fastest way to shuffle your data between Excel and C# is to use Excel-DNA - http://excel-dna.net.\n(Disclaimer: I develop Excel-DNA. But it\'s still true...)\nBecause it uses the native .xll interface it skips all the COM integration overhead that you\'d have with VSTO or another COM-based add-in approach. With Excel-DNA you could make a macro that is hooked up to a menu or ribbon button which reads a range, processes it, and writes it back to a range in Excel. All using the native Excel interface from C# - not a COM object in sight.\nI\'ve made a small test function that takes the current selection into an array, squares every number in the array, and writes the result into Sheet 2 starting from cell A1. You just need to add the (free) Excel-DNA runtime which you can download from http://excel-dna.net.\nI read into C#, process and write back to Excel a million-cell range in under a second. Is this fast enough for you?\nMy function looks like this:\nusing ExcelDna.Integration;\npublic static class RangeTools {\n\n[ExcelCommand(MenuName=""Range Tools"", MenuText=""Square Selection"")]\npublic static void SquareRange()\n{\n    object[,] result;\n    \n    // Get a reference to the current selection\n    ExcelReference selection = (ExcelReference)XlCall.Excel(XlCall.xlfSelection);\n    // Get the value of the selection\n    object selectionContent = selection.GetValue();\n    if (selectionContent is object[,])\n    {\n        object[,] values = (object[,])selectionContent;\n        int rows = values.GetLength(0);\n        int cols = values.GetLength(1);\n        result = new object[rows,cols];\n        \n        // Process the values\n        for (int i = 0; i < rows; i++)\n        {\n            for (int j = 0; j < cols; j++)\n            {\n                if (values[i,j] is double)\n                {\n                    double val = (double)values[i,j];\n                    result[i,j] = val * val;\n                }\n                else\n                {\n                    result[i,j] = values[i,j];\n                }\n            }\n        }\n    }\n    else if (selectionContent is double)\n    {\n        double value = (double)selectionContent;\n        result = new object[,] {{value * value}}; \n    }\n    else\n    {\n        result = new object[,] {{""Selection was not a range or a number, but "" + selectionContent.ToString()}};\n    }\n    \n    // Now create the target reference that will refer to Sheet 2, getting a reference that contains the SheetId first\n    ExcelReference sheet2 = (ExcelReference)XlCall.Excel(XlCall.xlSheetId, ""Sheet2""); // Throws exception if no Sheet2 exists\n    // ... then creating the reference with the right size as new ExcelReference(RowFirst, RowLast, ColFirst, ColLast, SheetId)\n    int resultRows = result.GetLength(0);\n    int resultCols = result.GetLength(1);\n    ExcelReference target = new ExcelReference(0, resultRows-1, 0, resultCols-1, sheet2.SheetId);\n    // Finally setting the result into the target range.\n    target.SetValue(result);\n}\n}\n\n', '\nIf the C# application is a stand-alone application, then you will always have cross-process marshaling involved that will overwhelm any optimizations you can do by switching languages from, say, C# to C++. Stick to your most preferred language in this situation, which sounds like is C#. \nIf you are willing to make an add-in that runs within Excel, however, then your operations will avoid cross-process calls and run about 50x faster.\nIf you run within Excel as an add-in, then VBA is among the fastest options, but it does still involve COM and so C++ calls using an XLL add-in would be fastest. But VBA is still quite fast in terms of calls to the Excel object model. As for actual calculation speed, however, VBA runs as pcode, not as fully compiled code, and so executes about 2-3x slower than native code. This sounds very bad, but it isn\'t because the vast majority of the execution time taken with a typical Excel add-in or application involves calls to the Excel object model, so VBA vs. a fully compiled COM add-in, say using natively compiled VB 6.0, would only be about 5-15% slower, which is not noticeable.\nVB 6.0 is a compiled COM approach, and runs 2-3x faster than VBA for non-Excel related calls, but VB 6.0 is about 12 years old at this point and won\'t run in 64 bit mode, say if installing Office 2010, which can be installed to run 32 bit or 64 bit. Usage of 64 bit Excel is tiny at the moment, but will grow in usage, and so I would avoid VB 6.0 for this reason.\nC#, if running in-process as an Excel add-in would execute calls to the Excel object model as fast as VBA, and execute non-Excel calls 2-3x faster than VBA -- if running unshimmed. The approach recommended by Microsoft, however, is to run fully shimmed, for example, by making use of the COM Shim Wizard. By being shimmed, Excel is protected from your code (if it\'s faulty) and your code is fully protected from other 3rd party add-ins that could otherwise potentially cause problems. The down-side to this, however, is that a shimmed solution runs within a separate AppDomain, which requires cross-AppDomain marshaling that incurrs an execution speed penalty of about 40x -- which is very noticeable in many contexts.\nAdd-ins using Visual Studio Tools for Office (VSTO) are automatically loaded within a shim and executes within a separate AppDomain. There is no avoiding this if using VSTO. Therefore, calls to the Excel object model would also incur an approximately 40x execution speed degradation. VSTO is a gorgeous system for making very rich Excel add-ins, but execution speed is its weakness for applications such as yours.\nExcelDna is a free, open source project that allows you to use C# code, which is then converted for you to an XLL add-in that uses C++ code. That is, ExcelDna parses your C# code and creates the required C++ code for you. I\'ve not used it myself, but I am familiar with the process and it\'s very impressive. ExcelDna gets very good reviews from those that use it. [Edit: Note the following correction as per Govert\'s comments below: ""Hi Mike - I want add a small correction to clarify the Excel-Dna implementation: all the managed-to-Excel glue works at runtime from your managed assembly using reflection - there is no extra pre-compilation step or C++ code generation. Also, even though Excel-Dna uses .NET, there need not be any COM interop involved when talking to Excel - as an .xll the native interface can be used directly from .NET (though you can also use COM if you want). This makes high-performance UDFs and macros possible."" – Govert]\nYou also might want to look at Add-in Express. It\'s not free, but it would allow you to code in C# and although it shims your solution into a separate AppDomain, I believe that it\'s execution speed is outstanding. If I am understanding its execution speed correctly, then I\'m not sure how Add-in Express doing this, but it might be taking advantage of something called FastPath AppDomain marshaling. Don\'t quote me on any of this, however, as I\'m not very familiar with Add-in Express. You should check it out though and do your own research. [Edit: Reading Charles Williams\' answer, it looks like Add-in Express enables both COM and C API access. And Govert states that Excel DNA also enables both COM and the fastrer C API access. So you\'d probably want to check out both and compare them to ExcelDna.]\nMy advice would be to research Add-in Express and ExcelDna. Both approaches would allow you to code using C#, which you seem most familiar with.\nThe other main issue is how you make your calls. For example, Excel is very fast when handling an entire range of data passed back-and-forth as an array. This is vastly more efficient than looping through the cells individually. For example, the following code makes use of the Excel.Range.set_Value accessor method to assign a 10 x 10 array of values to a 10 x 10 range of cells in one shot:\nvoid AssignArrayToRange()\n{\n    // Create the array.\n    object[,] myArray = new object[10, 10];\n\n    // Initialize the array.\n    for (int i = 0; i < myArray.GetLength(0); i++)\n    {\n        for (int j = 0; j < myArray.GetLength(1); j++)\n        {\n            myArray[i, j] = i + j;\n        }\n    }\n\n    // Create a Range of the correct size:\n    int rows = myArray.GetLength(0);\n    int columns = myArray.GetLength(1);\n    Excel.Range range = myWorksheet.get_Range(""A1"", Type.Missing);\n    range = range.get_Resize(rows, columns);\n\n    // Assign the Array to the Range in one shot:\n    range.set_Value(Type.Missing, myArray);\n}\n\nOne can similarly make use of the Excel.Range.get_Value accessor method to read an array of values from a range in one step. Doing this and then looping through the values within the array is vastly faster than looping trough the values within the cells of the range individually.\n', ""\nFurther to Mike Rosenblum's comments on the use of arrays, I'd like to add that I've been using the very approach (VSTO + arrays) and when I measured it, the actual read speed itself was within milliseconds. Just remember to disable event handling and screen updating prior to the read/write, and remember to re-enable after the operation is complete.\nUsing C#, you can create 1-based arrays exactly the same as Excel VBA itself does.  This is pretty useful, especially because even in VSTO, when you extract the array from an Excel.Range object, the array is 1-based, so keeping the Excel-oriented arrays 1-based helps you avoid needing to always check for whether the array is one-based or zero-based. \n  (If the column position in the array has significance to you, having to deal with 0-based and 1-based arrays can be a real pain).\nGenerally reading the Excel.Range into an array would look something like this:\nvar myArray = (object[,])range.Value2;\n\n\nMy variation of Mike Rosenblum's array-write uses a 1-based array like this:\nint[] lowerBounds = new int[]{ 1, 1 };\nint[] lengths = new int[] { rowCount, columnCount };  \nvar myArray = \n    (object[,])Array.CreateInstance(typeof(object), lengths, lowerBounds);\n\nvar dataRange = GetRangeFromMySources();\n\n// this example is a bit too atomic; you probably want to disable \n// screen updates and events a bit higher up in the call stack...\ndataRange.Application.ScreenUpdating = false;\ndataRange.Application.EnableEvents = false;\n\ndataRange = dataRange.get_Resize(rowCount, columnCount);\ndataRange.set_Value(Excel.XlRangeValueDataType.xlRangeValueDefault, myArray);\n\ndataRange.Application.ScreenUpdating = true;\ndataRange.Application.EnableEvents = true;\n\n"", '\nThe fastest interface to Excel data is the C API. There are a number of products out there that link .NET to Excel using this interface.\n2 products I like that do this are Excel DNA (which is free and open source) and Addin Express (which is a commercial product and has both the C API and COM interface available).\n', '\nFirst off, your solution cannot be an Excel UDF (user-defined function). In our manuals, we give the following definition: ""Excel UDFs are used to build custom functions in Excel for the end user to use them in formulas."" I wouldn\'t mind if you suggest a better definition :) \nThat definition shows that a UDF cannot add a button to the UI (I know that XLLs can modify the CommandBar UI) or intercept keyboard shortcuts as well as Excel events. \nThat is, ExcelDNA is out of scope because it is purposed for developing XLL add-ins. The same applies to Excel-targeted functionality of Add-in Express since it allows developing XLL add-ins and Excel Automation add-ins. \nBecause you need to handle Excel events, your solution can be a standalone application but there are obvious limitations of such approach. The only real way is to create a COM add-in; it allows handling Excel events and adding custom things to the Excel UI. You have three possibilities:\n\nVSTO\nAdd-in Express (COM add-in functionality)\nShared Add-in (see the corresponding item in the New Project dialog in VS)\n\nIf talking about developing an Excel COM add-in, the 3 tools above provide different features: visual designers, shimming, etc. But I don\'t think they differ in the speed of accessing the Excel Object Model. Say, I don\'t know (and cannot imagine) why getting a COM object from the Default AppDomain should differ from getting the same COM object from another AppDomain. BTW, you can check if shimming influences the speed of operation by creating a shared add-in and then using the COM Shim Wizard to shim it. \nSpeed II. As I wrote to you yesterday: ""The best way to speed up reading and writing to a range of cells is to create a variable of the Excel.Range type referring to that range and then read/write an array from/to the Value property of the variable."" But contrary to what Francesco says, I don\'t attribute this to VSTO; this is a feature of the Excel object model.\nSpeed III. The fastest Excel UDFs are written in native C++, not in any .NET language. I haven\'t compared the speed of an XLL add-in produced by ExcelDNA and Add-in Express; I don\'t think you\'ll find any substantial difference here.\nTo sum up. I am convinced you are on a wrong way: COM add-ins based on Add-in Express, VSTO or Shared Add-in should read and write Excel cells at the same speed. I will be glad (sincerely) if someone disproves this statement.\nNow on your other questions. VSTO doesn\'t allow developing a COM add-in supporting Office 2000-2010. It requires three different codebases and at least two versions of Visual Studio to completely support Office 2003-2010; you need to have strong nerves and a portion of good luck to deploy a VSTO-based add-in for Excel 2003. With Add-in Express, you create a COM add-in for all Office versions with a single codebase; Add-in Express provides you with a setup project, which is ready to install your add-in in Excel 2000-2010 (32-bit and 64-bit); ClickOnce deployment is on board too. \nVSTO beats Add-in Express in one area: it allows creating so-called document-level add-ins. Imagine a workbook or template with some .NET code behind it; I wouldn\'t be surprised, however, if deployment of such things is a nightmare.\nOn Excel events. All Excel events are listed in MSDN, for instance, see Excel 2007 events\nRegards from Belarus (GMT+2),\nAndrei Smolin\nAdd-in Express Team Leader\n', ""\nI've used VBA code (macro) to gather & compact the data, and get this data in one call to C#, and vice versa. This will probably be the most performant approach.\nUsing C#, you will always need to use some marshalling. Using VSTO or COM Interop, the underlaying communication layer (marshalling overhead) is the same.\nIn VBA (Visual Basic For Application) you work directly on the objects in Excel. So the access to this data will always be faster.\nBut.... Once you have the data in C#, the manipulation of this data can be a lot faster.\nIf you are using VB6 or C++, you also go through a COM interface, and you will also be facing cross process marshalling.\nSo you are looking for a method to minimize cross process calls and marshalling.\n""]",https://stackoverflow.com/questions/3840270/fastest-way-to-interface-between-live-unsaved-excel-data-and-c-sharp-objects,automation
R command for setting working directory to source file location in Rstudio,"
I am working out some tutorials in R. Each R code is contained in a specific folder. There are data files and other files in there. I want to open the .r file and source it such that I do not have to change the working directory in Rstudio as shown below:

Is there a way to specify my working directory automatically in R.
",183k,"
            177
        ","['\nTo get the location of a script being sourced, you can use utils::getSrcDirectory or utils::getSrcFilename. These require a function as an input. Create a script with the following lines, and source it to see their usage:\nprint(utils::getSrcDirectory(function(){}))\nprint(utils::getSrcFilename(function(){}, full.names = TRUE))\n\nChanging the working directory to that of the current file can be done with:\nsetwd(getSrcDirectory(function(){})[1])\n\nThis does not work in RStudio if you Run the code rather than Sourceing it.  For that, you need to use rstudioapi::getActiveDocumentContext.\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\n\nThis second solution requires that you are using RStudio as your IDE, of course.\n', ""\nI know this question is outdated, but I was searching for a solution for that as well and Google lists this at the very top:\nthis.dir <- dirname(parent.frame(2)$ofile)\nsetwd(this.dir)\n\nput that somewhere into the file (best would be the beginning, though), so that the wd is changed according to that file.\nAccording to the comments, this might not necessarily work on every platform (Windows seems to work, Linux/Mac for some).\nKeep in mind that this solution is for 'sourcing' the files, not necessarily for running chunks in that file.\nsee also get filename and path of `source`d file\n"", ""\nFor rstudio, you can automatically set your working directory to the script directory using rstudioapi like that:\nlibrary(rstudioapi)\n\n# Getting the path of your current open file\ncurrent_path = rstudioapi::getActiveDocumentContext()$path \nsetwd(dirname(current_path ))\nprint( getwd() )\n\nThis works when Running or Sourceing your file.\nYou need to install the package rstudioapi first.\nNotice I print the path to be 100% sure I'm at the right place, but this is optional.\n"", '\ndirname(rstudioapi::getActiveDocumentContext()$path)\n\nworks for me but if you don\'t want to use rstudioapi and you are not in a proyect, you can use the symbol ~ in your path. The symbol ~ refers to the default RStudio working directory (at least on Windows).\n\nIf your RStudio working directory is ""D:/Documents"", setwd(""~/proyect1"") is the same as setwd(""D:/Documents/proyect1"").\nOnce you set that, you can navigate to a subdirectory: read.csv(""DATA/mydata.csv""). Is the same as read.csv(""D:/Documents/proyect1/DATA/mydata.csv"").\nIf you want to navigate to a parent folder, you can use ""../"". \nFor example: read.csv(""../olddata/DATA/mydata.csv"") which is the same as read.csv(""D:/Documents/oldata/DATA/mydata.csv"")\nThis is the best way for me to code scripts, no matter what computer you are using.\n', '\nI realize that this is an old thread, but I had a similar problem with needing to set the working directory and couldn\'t get any of the solutions to work for me. Here\'s what did work, in case anyone else stumbles across this later on:\n# SET WORKING DIRECTORY TO CURRENT DIRECTORY:\nsystem(""pwd=`pwd`; $pwd 2> dummyfile.txt"")\ndir <- fread(""dummyfile.txt"")\nn<- colnames(dir)[2]\nn2 <- substr(n, 1, nchar(n)-1)\nsetwd(n2)\n\nIt\'s a bit convoluted, but basically this uses system commands to get the working directory and save it to dummyfile.txt, then R reads that file using data.table::fread. The rest is just cleaning up what got printed to the file so that I\'m left with just the directory path.\nI needed to run R on a cluster, so there was no way to know what directory I\'d end up in (jobs get assigned a number and a compute node). This did the trick for me.\n', '\nThis answer can help:\nscript.dir <- dirname(sys.frame(1)$ofile)\n\n\nNote: script must be sourced in order to return correct path\n\nI found it in: https://support.rstudio.com/hc/communities/public/questions/200895567-can-user-obtain-the-path-of-current-Project-s-directory-\nThe BumbleBee´s answer (with parent.frame instead sys.frame) didn´t work to me, I always get an error.\n', '\nThe solution \ndirname(parent.frame(2)$ofile)\n\nnot working for me.\nI\'m using a brute force algorithm, but works:\nFile <- ""filename""\nFiles <- list.files(path=file.path(""~""),recursive=T,include.dirs=T)\nPath.file <- names(unlist(sapply(Files,grep,pattern=File))[1])\nDir.wd <- dirname(Path.file)\n\nMore easy when searching a directory:\nDirname <- ""subdir_name""\nDirs <- list.dirs(path=file.path(""~""),recursive=T)\ndir_wd <- names(unlist(sapply(Dirs,grep,pattern=Dirname))[1])\n\n', '\nIf you work on Linux you can try this:\nsetwd(system(""pwd"", intern = T) )\nIt works for me.\n', ""\nThe here package provides the here() function, which returns your project root directory based on some heuristics. \nNot the perfect solution, since it doesn't find the location of the script, but it suffices for some purposes so I thought I'd put it here.\n"", '\nI understand this is outdated, but I couldn\'t get the former answers to work very satisfactorily, so I wanted to contribute my method in case any one else encounters the same error mentioned in the comments to BumbleBee\'s answer.\nMine is based on a simple system command. All you feed the function is the name of your script:\nextractRootDir <- function(x) {\n    abs <- suppressWarnings(system(paste(""find ./ -name"",x), wait=T, intern=T, ignore.stderr=T))[1];\n    path <- paste(""~"",substr(abs, 3, length(strsplit(abs,"""")[[1]])),sep="""");\n    ret <- gsub(x, """", path);\n    return(ret);\n}\n\nsetwd(extractRootDir(""myScript.R""));\n\nThe output from the function would look like ""/Users/you/Path/To/Script"". Hope this helps anyone else who may have gotten stuck.\n', '\nsetwd(this.path::here())\n\nworks both for sourced and ""active"" scripts.\n', '\nI was just looking for a solution to this problem, came to this page. I know its dated but the previous solutions where unsatisfying or didn\'t work for me. Here is my work around if interested.\nfilename = ""your_file.R""\nfilepath = file.choose()  # browse and select your_file.R in the window\ndir = substr(filepath, 1, nchar(filepath)-nchar(filename))\nsetwd(dir)\n\n', '\nI feel like a mocking bird, but I\'m going to say it: I know this post is old, but...\nI just recently learned you cannot call the api when running the script from Task Scheduler and a .bat file. I learned this the hard way. Thought those of you who were using any of the rstudioapi:: methods might like to know that. We run lots of script this way overnight. Just recently changed our path to include the api called so we could ""dynamically"" set the working directory. Then, when the first one we tried with failed when triggered from Task Scheduler, investigation brought that information about.\nThis is the actual code that brought this issue to light: setwd(dirname(rstudioapi::getActiveDocumentContext()$path))\nWorks beautifully if you\'re running the script though!\nJust adding my two cents as I know people still pull these threads up, thought it might be helpful.\n', '\nIn case you use UTF-8 encoding:\npath <- rstudioapi::getActiveDocumentContext()$path\nEncoding(path) <- ""UTF-8""\nsetwd(dirname(path))\n\nYou need to install the package rstudioapi if you haven\'t done it yet.\n', '\nMost GUIs assume that if you are in a directory and ""open"", double-click, or otherwise attempt to execute an .R file, that the directory in which it resides will be the working directory unless otherwise specified. The Mac GUI provides a method to change that default behavior which is changeable in the Startup panel of Preferences that you set in a running session and become effective at the next ""startup"". You should be also looking at:\n?Startup\n\nThe RStudio documentation says:\n""When launched through a file association, RStudio automatically sets the working directory to the directory of the opened file.""  The default setup is for RStudio to be register as a handler for .R files, although there is also mention of ability to set a default ""association"" with RStudio for .Rdata and .R extensions. Whether having \'handler\' status and \'association\' status are the same on Linux, I cannot tell.\nhttp://www.rstudio.com/ide/docs/using/workspaces\n', ""\ndirname(parent.frame(2)$ofile)  \n\ndoesn't work for me either, but the following (as suggested in https://stackoverflow.com/a/35842176/992088) works for me in ubuntu 14.04\ndirname(rstudioapi::getActiveDocumentContext()$path)\n\n"", ""\nHere is another way to do it: \nset2 <- function(name=NULL) {\n  wd <- rstudioapi::getSourceEditorContext()$path\n  if (!is.null(name)) {\n    if (substr(name, nchar(name) - 1, nchar(name)) != '.R') \n      name <- paste0(name, '.R')\n  }\n  else {\n    name <- stringr::word(wd, -1, sep='/')\n  }\n  wd <- gsub(wd, pattern=paste0('/', name), replacement = '')\n  no_print <- eval(expr=setwd(wd), envir = .GlobalEnv)\n}\nset2()\n\n""]",https://stackoverflow.com/questions/13672720/r-command-for-setting-working-directory-to-source-file-location-in-rstudio,automation
Karate- Need help to assert a single dimension array for date range,"
I am trying to assert the values inside a single dimensional array. I have tried using match but it looks like the date ranges cannot be asserted. 
Below is the object array:
[
""2019-04-24T17:41:28"",
""2019-04-24T17:41:27.975"",
""2019-04-24T17:41:27.954"",
""2019-04-24T17:41:27.93"",
""2019-04-24T17:41:27.907"",
""2019-04-24T17:41:27.886"",
""2019-04-24T17:41:27.862"",
""2019-04-24T17:41:27.84"",
""2019-04-24T17:41:27.816"",
""2019-04-24T17:41:27.792""
]

I am trying to assert each values between the following date ranges:
MinDate:2019-04-24T17:25:00.000000+00:00
MaxDate:2019-04-24T17:50:00.000000+00:00

I have tried the following but none works:
* match dateCreated == '#[]? _.value >= fromDate'
 * eval for(var i = 0; i < responseJson.response.data.TotalItemCount; i++) dateCreated.add(responseJson.response.data.Items[i].DateCreated)  karate.assert(dateCreated[i] >= fromDate)

Any hint/tip on how to go about it.
",1k,"
            2
        ","['\nHere you go:\n* def dateToLong =\n""""""\nfunction(s) {\n  var SimpleDateFormat = Java.type(\'java.text.SimpleDateFormat\');\n  var sdf = new SimpleDateFormat(""yyyy-MM-dd\'T\'HH:mm:ss.SSS"");\n  return sdf.parse(s).time;\n} \n""""""\n* def min = dateToLong(\'2019-04-24T17:25:00.000\')\n* def max = dateToLong(\'2019-04-24T17:50:00.000\')\n* def isValid = function(x){ var temp = dateToLong(x); return temp >= min && temp <= max }\n\n* def response =\n""""""\n[\n""2019-04-24T17:41:27.975"",\n""2019-04-24T17:41:27.954"",\n""2019-04-24T17:41:27.93"",\n""2019-04-24T17:41:27.907"",\n""2019-04-24T17:41:27.886"",\n""2019-04-24T17:41:27.862"",\n""2019-04-24T17:41:27.84"",\n""2019-04-24T17:41:27.816"",\n""2019-04-24T17:41:27.792""\n]\n""""""\n* match each response == \'#? isValid(_)\'\n\nPlease refer the docs if you have doubts about any of the keywords. I removed the first date in the list because it was not consistent, but you have enough info to handle it if needed - you may need some conditional logic somewhere.\nAlso see:\nhttps://stackoverflow.com/a/54114432/143475\nhttps://stackoverflow.com/a/52892797/143475\n']",https://stackoverflow.com/questions/55938266/karate-need-help-to-assert-a-single-dimension-array-for-date-range,automation
Automating telnet session using Bash scripts,"
I am working on automating some telnet related tasks, using Bash scripts.
Once automated, there will be no interaction of the user with telnet (that is, the script will be totally automated).
The scripts looks something like this:
# execute some commands on the local system
# access a remote system with an IP address: 10.1.1.1 (for example)

telnet 10.1.1.1

# execute some commands on the remote system
# log all the activity (in a file) on the local system
# exit telnet
# continue with executing the rest of the script

There are two problems I am facing here:

How to execute the commands on the remote system from the script (without human interaction)?
From my experience with some test code, I was able to deduce that when telnet 10.1.1.1 is executed, telnet goes into an interactive session and the subsequent lines of code in the script are executed on the local system. How can I run the lines of code on the remote system rather than on the local one?

I am unable to get a log file for the activity in the telnet session on the local system. The stdout redirect I used makes a copy on the remote system (I do not want to perform a copy operation to copy the log to the local system). How can I achieve this functionality?


",327k,"
            89
        ","['\nWhile I\'d suggest using expect, too, for non-interactive use the normal shell commands might suffice. telnet accepts its command on stdin, so you just need to pipe or write the commands into it through heredoc:\ntelnet 10.1.1.1 <<EOF\nremotecommand 1\nremotecommand 2\nEOF\n\n(Edit: Judging from the comments, the remote command needs some time to process the inputs or the early SIGHUP is not taken gracefully by telnet. In these cases, you might try a short sleep on the input:)\n{ echo ""remotecommand 1""; echo ""remotecommand 2""; sleep 1; } | telnet 10.1.1.1\n\nIn any case, if it\'s getting interactive or anything, use expect.\n', '\nWrite an expect script.\nHere is an example:\n#!/usr/bin/expect\n\n#If it all goes pear shaped the script will timeout after 20 seconds.\nset timeout 20\n#First argument is assigned to the variable name\nset name [lindex $argv 0]\n#Second argument is assigned to the variable user\nset user [lindex $argv 1]\n#Third argument is assigned to the variable password\nset password [lindex $argv 2]\n#This spawns the telnet program and connects it to the variable name\nspawn telnet $name \n#The script expects login\nexpect ""login:"" \n#The script sends the user variable\nsend ""$user ""\n#The script expects Password\nexpect ""Password:""\n#The script sends the password variable\nsend ""$password ""\n#This hands control of the keyboard over to you (Nice expect feature!)\ninteract\n\nTo run:\n./myscript.expect name user password\n\n', '\nTelnet is often used when you learn the HTTP protocol. I used to use that script as a part of my web scraper:\necho ""open www.example.com 80""\nsleep 2\necho ""GET /index.html HTTP/1.1""\necho ""Host: www.example.com""\necho\necho\nsleep 2\n\nLet\'s say the name of the script is get-page.sh, then this will give you an HTML document:\nget-page.sh | telnet\n\nI hope this will be helpful to someone ;)\n', '\nThis worked for me.. \nI was trying to automate multiple telnet logins which require a username and password. The telnet session needs to run in the background indefinitely since I am saving logs from different servers to my machine.\ntelnet.sh automates telnet login using the \'expect\' command. More info can be found here: http://osix.net/modules/article/?id=30\ntelnet.sh\n#!/usr/bin/expect\nset timeout 20\nset hostName [lindex $argv 0]\nset userName [lindex $argv 1]\nset password [lindex $argv 2]\n\nspawn telnet $hostName\n\nexpect ""User Access Verification""\nexpect ""Username:""\nsend ""$userName\\r""\nexpect ""Password:""\nsend ""$password\\r"";\ninteract\n\nsample_script.sh is used to create a background process for each of the telnet sessions by running telnet.sh. More information can be found in the comments section of the code.\nsample_script.sh\n#!/bin/bash\n#start screen in detached mode with session-name \'default_session\' \nscreen -dmS default_session -t screen_name \n#save the generated logs in a log file \'abc.log\' \nscreen -S default_session -p screen_name -X stuff ""script -f /tmp/abc.log $(printf \\\\r)""\n#start the telnet session and generate logs\nscreen -S default_session -p screen_name -X stuff ""expect telnet.sh hostname username password $(printf \\\\r)""\n\n\nMake sure there is no screen running in the backgroud by using the\ncommand \'screen -ls\'. \nRead\nhttp://www.gnu.org/software/screen/manual/screen.html#Stuff to read\nmore about screen and its options. \n\'-p\' option in sample_script.sh\npreselects and reattaches to a specific window to send a command via\nthe ‘-X’ option otherwise you get a \'No screen session found\' error.\n\n', '\nYou can use expect scripts instaed of bash.\nBelow example show how to telnex into an embedded board having no password\n#!/usr/bin/expect\n\nset ip ""<ip>""\n\nspawn ""/bin/bash""\nsend ""telnet $ip\\r""\nexpect ""\'^]\'.""\nsend ""\\r""\nexpect ""#""\nsleep 2\n\nsend ""ls\\r""\nexpect ""#""\n\nsleep 2\nsend -- ""^]\\r""\nexpect ""telnet>""\nsend  ""quit\\r""\nexpect eof\n\n', '\nFollowing is working for me...\nput all of your IPs you want to telnet in IP_sheet.txt\nwhile true\nread a\ndo\n{\n    sleep 3\n    echo df -kh\n    sleep 3\n    echo exit\n} | telnet $a\ndone<IP_sheet.txt\n\n', ""\nThe answer by @thiton was helpful but I wanted to avoid the sleep command. Also telnet didn't exit the interactive mode, so my script got stuck.\nI solved that by sending telnet command with curl (which seems to wait for the response) and by explicitly telling telnet to quit like this:\ncurl telnet://10.1.1.1:23 <<EOF\nremotecommand 1\nremotecommand 2\nquit\nEOF\n\n"", '\n#!/bin/bash\nping_count=""4""\navg_max_limit=""1500""\nrouter=""sagemcom-fast-2804-v2""\nadress=""192.168.1.1""\nuser=""admin""\npass=""admin""\n\nVAR=$(\nexpect -c "" \n        set timeout 3\n        spawn telnet ""$adress""\n        expect \\""Login:\\"" \n        send \\""$user\\n\\""\n        expect \\""Password:\\""\n        send \\""$pass\\n\\""\n        expect \\""commands.\\""\n        send \\""ping ya.ru -c $ping_count\\n\\""\n        set timeout 9\n        expect \\""transmitted\\""\n        send \\""exit\\""\n        "")\n\ncount_ping=$(echo ""$VAR"" | grep packets | cut -c 1)\navg_ms=$(echo ""$VAR"" | grep round-trip | cut -d \'/\' -f 4 | cut -d \'.\' -f 1)\n\necho ""1_____ping___$count_ping|||____$avg_ms""\necho ""$VAR""\n\n', '\nUse ssh for that purpose. Generate keys without using a password and place it to .authorized_keys at the remote machine. Create the script to be run remotely, copy it to the other machine and then just run it remotely using ssh.\nI used this approach many times with a big success. Also note that it is much more secure than telnet.\n', '\nHere is how to use telnet in bash shell/expect  \n#!/usr/bin/expect\n# just do a chmod 755 one the script\n# ./YOUR_SCRIPT_NAME.sh $YOUHOST $PORT\n# if you get ""Escape character is \'^]\'"" as the output it means got connected otherwise it has failed\n\nset ip [lindex $argv 0]\nset port [lindex $argv 1]\n\nset timeout 5\nspawn telnet $ip $port\nexpect ""\'^]\'.""\n\n', '\nScript for obtain version of CISCO-servers:\n#!/bin/sh\n\nservers=\'\n192.168.34.1\n192.168.34.3\n192.168.34.2\n192.168.34.3\n\'\nuser=\'cisco_login\'\npass=\'cisco_password\'\n\nshow_version() {\nhost=$1\nexpect << EOF\nset timeout 20\nset host $host\nset user $user\nset pass $pass\nspawn telnet $host\nexpect ""Username:""\nsend ""$user\\r""\nexpect ""Password:""\nsend ""$pass\\r""\nexpect -re "".*#""\nsend ""show version\\r""\nexpect -re "".*-More-.*""\nsend "" ""\nexpect -re "".*#""\nsend ""exit\\r""\nEOF\n}\n\nfor ip in $servers; do\n  echo \'---------------------------------------------\'\n  echo ""$ip""\n  show_version $ip | grep -A3 \'SW Version\'\ndone\n\n', '\nHere is a solution that will work with a list of extenders.  This only requires bash - some of the answers above require expect and you may not be able to count on expect being installed.\n#!/bin/bash\n\ndeclare -a  Extenders=(""192.168.1.48"" ""192.168.1.50"" ""192.168.1.51"") \n# ""192.168.1.52"" ""192.168.1.56"" ""192.168.1.58"" ""192.168.1.59"" ""192.168.1.143"")\nsleep 5\n# Iterate the string array using for loop\nfor val in ${Extenders[@]}; do\n   { sleep 0.2; echo ""root""; sleep 0.2; echo ""ls""; sleep 0.2; }  | telnet $val\ndone\n\n', '\nPlay with tcpdump or wireshark and see what commands are sent to the server itself\nTry this\nprintf (printf ""$username\\r\\n$password\\r\\nwhoami\\r\\nexit\\r\\n"") | ncat $target 23\n\nSome servers require a delay with the password as it does not hold lines on the stack\nprintf (printf ""$username\\r\\n"";sleep 1;printf ""$password\\r\\nwhoami\\r\\nexit\\r\\n"") | ncat $target 23**\n\n']",https://stackoverflow.com/questions/7013137/automating-telnet-session-using-bash-scripts,automation
What are the best ways to automate a GDB debugging session?,"
Does GDB have a built in scripting mechanism, should I code up an expect script, or is there an even better solution out there?  
I'll be sending the same sequence of commands every time and I'll be saving the output of each command to a file (most likely using GDB's built-in logging mechanism, unless someone has a better idea).
",106k,"
            87
        ","['\nBasically, in this example I wanted to get some variable values in particular places of the code; and have them output until the program crashes. So here is first a little program which is guaranteed to crash in a few steps, test.c:\n#include <stdio.h>\n#include <stdlib.h>\n\nint icount = 1; // default value\n\nmain(int argc, char *argv[])\n{\n  int i;\n\n  if (argc == 2) {\n    icount = atoi(argv[1]);\n  }\n\n  i = icount;\n  while (i > -1) {\n    int b = 5 / i;\n    printf("" 5 / %d = %d \\n"", i, b );\n    i = i - 1;\n  }\n\n  printf(""Finished\\n"");\n  return 0;\n}\n\nThe only reason the program accepts command-line arguments is to be able to choose the number of steps before crashing - and to show that gdb ignores --args in batch mode. This I compile with:\ngcc -g test.c -o test.exe\n\nThen, I prepare the following script - the main trick here is to assign a command to each breakpoint, which will eventually continue (see also Automate gdb: show backtrace at every call to function puts). This script I call test.gdb:\n# http://sourceware.org/gdb/wiki/FAQ: to disable the\n# ""---Type <return> to continue, or q <return> to quit---""\n# in batch mode:\nset width 0\nset height 0\nset verbose off\n\n# at entry point - cmd1\nb main\ncommands 1\n  print argc\n  continue\nend\n\n# printf line - cmd2\nb test.c:17\ncommands 2\n  p i\n  p b\n  continue\nend\n\n# int b = line - cmd3\nb test.c:16\ncommands 3\n  p i\n  p b\n  continue\nend\n\n# show arguments for program\nshow args\nprintf ""Note, however: in batch mode, arguments will be ignored!\\n""\n\n# note: even if arguments are shown;\n# must specify cmdline arg for ""run""\n# when running in batch mode! (then they are ignored)\n# below, we specify command line argument ""2"":\nrun 2     # run\n\n#start # alternative to run: runs to main, and stops\n#continue\n\nNote that, if you intend to use it in batch mode, you have to ""start up"" the script at the end, with run or start or something similar.\nWith this script in place, I can call gdb in batch mode - which will generate the following output in the terminal:\n$ gdb --batch --command=test.gdb --args ./test.exe 5\nBreakpoint 1 at 0x804844d: file test.c, line 10.\nBreakpoint 2 at 0x8048485: file test.c, line 17.\nBreakpoint 3 at 0x8048473: file test.c, line 16.\nArgument list to give program being debugged when it is started is ""5"".\nNote, however: in batch mode, arguments will be ignored!\n\nBreakpoint 1, main (argc=2, argv=0xbffff424) at test.c:10\n10    if (argc == 2) {\n$1 = 2\n\nBreakpoint 3, main (argc=2, argv=0xbffff424) at test.c:16\n16      int b = 5 / i;\n$2 = 2\n$3 = 134513899\n\nBreakpoint 2, main (argc=2, argv=0xbffff424) at test.c:17\n17      printf("" 5 / %d = %d \\n"", i, b );\n$4 = 2\n$5 = 2\n 5 / 2 = 2 \n\nBreakpoint 3, main (argc=2, argv=0xbffff424) at test.c:16\n16      int b = 5 / i;\n$6 = 1\n$7 = 2\n\nBreakpoint 2, main (argc=2, argv=0xbffff424) at test.c:17\n17      printf("" 5 / %d = %d \\n"", i, b );\n$8 = 1\n$9 = 5\n 5 / 1 = 5 \n\nBreakpoint 3, main (argc=2, argv=0xbffff424) at test.c:16\n16      int b = 5 / i;\n$10 = 0\n$11 = 5\n\nProgram received signal SIGFPE, Arithmetic exception.\n0x0804847d in main (argc=2, argv=0xbffff424) at test.c:16\n16      int b = 5 / i;\n\nNote that while we specify command line argument 5, the loop still spins only two times (as is the specification of run in the gdb script); if run didn\'t have any arguments, it spins only once (the default value of the program) confirming that --args ./test.exe 5 is ignored.\nHowever, since now this is output in a single call, and without any user interaction, the command line output can easily be captured in a text file using bash redirection, say:\ngdb --batch --command=test.gdb --args ./test.exe 5 > out.txt\n\nThere is also an example of using python for automating gdb in c - GDB auto stepping - automatic printout of lines, while free running?\n', '\ngdb executes file .gdbinit after running.\nSo you can add your commands to this file and see if it is OK for you.\nThis is an example of .gdbinit in order to print backtrace for all f() calls:\nset pagination off\nset logging file gdb.txt\nset logging on\nfile a.out\nb f\ncommands\nbt\ncontinue\nend\ninfo breakpoints\nr\nset logging off\nquit\n\n', '\nIf a -x with a file is too much for you, just use multiple -ex\'s.\nThis is an example to track a running program showing (and saving) the backtrace on crashes\nsudo gdb -p ""$(pidof my-app)"" -batch \\\n  -ex ""set logging on"" \\\n  -ex continue \\\n  -ex ""bt full"" \\\n  -ex quit\n\n']",https://stackoverflow.com/questions/10748501/what-are-the-best-ways-to-automate-a-gdb-debugging-session,automation
How to find the UpgradeCode and ProductCode of an installed application in Windows 7,"
I have an application installed on my machine. I also have its source code but somehow the ProductCode and UpgradeCode of this application were changed.
Now I want to get the UpgradeCode and ProductCode of this installed application.  I feel there must be some tool for this. 
Can anyone kindly let me know how to get the UpgradeCode and ProductCode of an installed application?
",188k,"
            44
        ","['\n\nIMPORTANT: It\'s been a while since this answer was originally posted, and smart people came up with wiser answers. Check How can I find the Upgrade Code for an installed MSI file? from @ Stein Åsmul if you need a solid and comprehensive approach.\n\nHere\'s another way (you don\'t need any tools):\n\nopen system registry and search for HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall key (if it\'s a 32-bit installer on a 64-bit machine, it might be under HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall instead).\nthe GUIDs listed under that key are the products installed on this machine\nfind the one you\'re talking about - just step one by one until you see its name on the right pane\n\nThis GUID you stopped on is the ProductCode.\nNow, if you\'re sure that reinstallation of this application will go fine, you can run the following command line:\n\nmsiexec /i {PRODUCT-CODE-GUID-HERE}\n  REINSTALL=ALL REINSTALLMODE=omus /l*v\n  log.txt\n\nThis will ""repair"" your application. Now look at the log file and search for ""UpgradeCode"". This value is dumped there.\nNOTE: you should only do this if you are sure that reinstall flow is implemented correctly and this won\'t break your installed application. \n', '\nIt takes some time to return results, easily many tens of seconds, but wmic works well and can be scripted:\nwmic product where ""Name like \'%Word%\'"" get Name,Version,IdentifyingNumber\n\nresult:\nIdentifyingNumber                       Name                                      Version\n{90140000-001B-0409-0000-0000000FF1CE}  Microsoft Office Word MUI (English) 2010  14.0.6029.1000\n\nThe IdentifingNumber is the ProductCode. I didn\'t see a property for UpgradeCode, but perhaps it might be buried under something else. See http://quux.wiki.zoho.com/WMIC-Snippets.html for many other examples, including uninstall:\nwmic path win32_product where ""name = \'HP Software Update\'"" call Uninstall\n\nFor UpgradeCode see the excellent and detailed answer to How can I find the Upgrade Code for an installed MSI file?\n', '\nTo everyone using:\nGet-WMIObject win32_product\n\nYou should be aware that this will run a self-heal on every single MSI application installed on the PC. If you were to check eventvwr it will say it has finished reconfiguring each product.\nIn this case i use the following (a mixture of Yan Sklyarenko\'s method):\n$Reg = @( ""HKLM:\\Software\\Wow6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*"", ""HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*"" )\n$InstalledApps = Get-ItemProperty $Reg -EA 0\n$WantedApp = $InstalledApps | Where { $_.DisplayName -like ""*<part of product>*"" }\n\nNow if you were to type:\n$WantedApp.PSChildName\n\nYou would be given the following:\nPS D:\\SCCM> $WantedApp.PSChildName\n{047904BA-C065-40D5-969A-C7D91CA93D62}\n\nIf your organization uses loads of MST\'s whilst installing applications you would want to avoid running self-heals encase they revert some crucial settings.\n\nNote - This will find your product code, then the upgrade can be found as Yan mentioned. I usually, though, just use either \'InstEd It!\' or \'Orca\' then go to the Property table of the MSI and it lists them right at the top.\n\n', ""\nIf you have msi installer open it with Orca (tool from Microsoft), table Property (rows UpgradeCode, ProductCode, Product version etc) or table Upgrade column Upgrade Code.\nTry to find instller via registry: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall find required subkey and watch value InstallSource. Maybe along the way you'll be able to find the MSI file.\n"", '\nPowershell handles tasks like this fairly handily:\n$productCode = (gwmi win32_product | `\n                ? { $_.Name -Like ""<PRODUCT NAME HERE>*"" } | `\n                % { $_.IdentifyingNumber } | `\n                Select-Object -First 1)\n\nYou can then use it to get the uninstall information as well:\n$wow = """"\n$is32BitInstaller = $True # or $False\n\nif($is32BitInstaller -and [System.Environment]::Is64BitOperatingSystem) \n{\n    $wow = ""\\Wow6432Node"" \n}\n\n$regPath = ""HKEY_LOCAL_MACHINE\\SOFTWARE$wow\\Microsoft\\Windows\\CurrentVersion\\Uninstall""\n\ndir ""HKLM:\\SOFTWARE$wow\\Microsoft\\Windows\\CurrentVersion\\Uninstall"" | `\n? { $_.Name -Like ""$regPath\\$productCode""  }\n\n', '\nYou can use the MsiEnumProductsEx and MsiGetProductInfoEx methods to enumerate all the installed applications on your system and match the data to your application\n', '\nIn Windows 10 preview build with PowerShell 5, I can see that you can do:\n$info = Get-Package -Name YourInstalledProduct\n$info.Metadata[""ProductCode""]\n\nNot familiar with even not sure if all products has UpgradeCode, but  according to this post you need to search UpgradeCode from this registry path:\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Installer\\UpgradeCodes\n\nUnfortunately, the registry key values are the ProductCode and the registry keys are the UpgradeCode.\n', '\nIf anyone wants to get installed application package code, just execute below command with your application name in the command prompt. You will be getting product code along with package code.\nwmic product where ""Name like \'%YOUR_APPLICATION_NAME%\'"" get IdentifyingNumber, PackageCode\n', '\nAnother way-too-complicated workaround, with the benefit of not having to re-install the application as the previous workaround required. This requires that you have access to the msi (or a setup.exe with the msi embedded).\nIf you have Visual Studio 2012 (or possibly other editions) and install the free ""InstallShield LE"", then you can create a new setup project using InstallShield.\nOne of the configuration options in the ""Organize your Setup"" step is called ""Upgrade Paths"".  Open the properties for Upgrade Paths, and in the left pane right click ""Upgrade Paths"" and select ""New Upgrade Path"" ... now browse to the msi (or setup.exe containing the msi) and click ""open"".  The upgrade code will be populated for you in the settings page in the right pane which you should now see.\n', '\nHadn\'t found any way of finding out the UpgradeCode from an installed application, before seeing Yan Sklyarenko\'s workaround (currently) above. But if you/anyone else would find a way of finding out (at least) both UpgradeCode and ProductCode from a MSI, read on.\nFrom http://www.dwarfsoft.com/blog/2010/06/22/msi-package-code-fun/, modified to allow (when launched with wscript.exe) one popup box of info per MSI (Trunicated at 1023 chars, due to wscript.echo limitation); able to input MSI(s) from the GUI as well as the CLI; some basic human input validation; removed debug code (\' Set oDatabase) and 1 bug fix (DB.OpenView).\n\'Created by:   Chris Bennett\n\'Created Date: 22/06/2010\n\'Description:\n\'   Opens up MSI file(s) Passed as Arguments & returns ProductName, ProductCode,\n\'   The HKCR key created from ProductCode (a Packed GUID of ProductCode), the \n\'   PackageCode and the UpgradeCode of the MSI. Much quicker than getting these\n\'   out of the MSI\'s the Manual Way.\n\nReferences:\nhttp://msdn.microsoft.com/en-us/library/aa369794%28VS.85%29.aspx\nhttp://www.eggheadcafe.com/forumarchives/platformsdkmsi/Jan2006/post25948124.asp\nif wscript.arguments.count = 0 then\n  MSIs = inputbox(""Enter in * delimited list of MSI\'s to query (Max 254 characters)"", ""MSI Product Details"")\n  MSIs = split(MSIs,""*"")\nelse\n  set MSIs = wscript.arguments\nend if\n\nset objFS = createobject(""scripting.filesystemobject"")\nFor Each MSIPath in MSIs\n  if objFS.fileexists(MSIPath) then\n    Set MSIDetails = EvaluateMSI(MSIPath)\n    MSIDetails = MSIPath & "": "" & vbcrlf & vbcrlf & ""Product Name: "" &_\n    MSIDetails(""ProductName"") & vbcrlf & ""Product Code: "" &_\n    MSIDetails(""ProductCode"") & vbcrlf & ""Product Key : "" &_\n    ""HKCR\\Installer\\Products\\"" & PackGUID(MSIDetails(""ProductCode"")) &_\n    vbcrlf & ""Package Code: "" & MSIDetails(""PackageCode"") & vbcrlf &_\n    ""Upgrade Code: "" & MSIDetails(""UpgradeCode"") & vbcrlf\n    WScript.Echo MSIDetails\n  else\n    wscript.echo ""Inaccessible; Non-existant; or Error in Path for:"" & vbcrlf & MSIPath & vbcrlf & ""... skipping""\n  end if\nNext\n\nFunction EvaluateMSI(MSIPath)\n  On Error Resume Next\n  \' create installer object\n  Set oInstaller = CreateObject(""WindowsInstaller.Installer"")\n  \' open msi in read-only mode\n  Set oDatabase = oInstaller.OpenDatabase(MSIPath, 0)\n  Set objDictionary = CreateObject(""Scripting.Dictionary"")\n  \' Get Package Code from Summary Information Stream   \n  Set streamobj = oDatabase.SummaryInformation(0) \'0 = read only\n  objDictionary(""PackageCode"") = streamobj.Property(9)\n  \' Get Product Name from MSI Database\n  Set View = oDatabase.OpenView(""Select `Value` From Property WHERE `Property`=\'ProductName\'"")\n  View.Execute\n  Set ProductName = View.Fetch\n  objDictionary(""ProductName"") = ProductName.StringData(1)\n\n  \' Get Product Code from MSI Database\n  Set View = oDatabase.OpenView(""Select `Value` From Property WHERE `Property`=\'ProductCode\'"")\n  View.Execute\n  Set ProductCode = View.Fetch\n  objDictionary(""ProductCode"") = ProductCode.StringData(1)\n\n  \' Get Upgrade Code from MSI Database\n  Set View = oDatabase.OpenView(""Select `Value` From Property WHERE `Property`=\'UpgradeCode\'"")\n  View.Execute\n  Set UpgradeCode = View.Fetch\n  objDictionary(""UpgradeCode"") = UpgradeCode.StringData(1)\n\n  Set EvaluateMSI = objDictionary\n  On Error Goto 0\nEnd Function\n\nFunction PackGUID(guid)  \n  PackGUID = """"  \n  \'*  \n  Dim temp  \n  temp = Mid(guid,2,Len(guid)-2)  \n  Dim part  \n  part = Split(temp,""-"")  \n  Dim pack  \n  pack = """"  \n  Dim i, j  \n  For i = LBound(part) To UBound(part)\n    Select Case i\n      Case LBound(part), LBound(part)+1, LBound(part)+2\n        For j = Len(part(i)) To 1 Step -1  \n          pack = pack & Mid(part(i),j,1)  \n        Next  \n      Case Else\n        For j = 1 To Len(part(i)) Step 2  \n          pack = pack & Mid(part(i),j+1,1) & Mid(part(i),j,1)  \n      Next  \n    End Select\n  Next  \n  \'*  \n  PackGUID = pack  \nEnd Function\n\nIf one needs to copy&paste any of the GUID\'s in the popup, I tend to find it easiest to use a subsequent inputbox, like inputbox """","""",MSIDetails\n', ""\nIf you don't have the msi and you need the upgrade code, rather than the product code then the answer is here: How can I find the upgrade code for an installed application in C#?\n""]",https://stackoverflow.com/questions/5063129/how-to-find-the-upgradecode-and-productcode-of-an-installed-application-in-windo,automation
How to print every executed line in GDB automatically until a given breakpoint is reached?,"
I would like to be able to set a breakpoint in GDB, and have it run to that point - and in the process, print out lines it has ""stepped through"".
Here is an example, based on this simple file with a main and a function, and two breakpoints for each: 
$ cat > test.c <<EOF
#include ""stdio.h""

int count=0;

void doFunction(void) {
  // two steps forward
  count += 2;
  // one step back
  count--;
}

int main(void) {
  // some pointless init commands;
  count = 1;
  count += 2;
  count = 0;
  //main loop
  while(1) {
    doFunction();
    printf(""%d\n"", count);
  }
}
EOF

$ gcc -g -Wall test.c -o test.exe
$ chmod +x test.exe
$ gdb -se test.exe
...
Reading symbols from /path/to/test.exe...done.
(gdb) b main
Breakpoint 1 at 0x80483ec: file test.c, line 14.
(gdb) b doFunction
Breakpoint 2 at 0x80483c7: file test.c, line 7.

To start the session, I need to run (r) the program, which will then stop at first breakpoint (main):
(gdb) r
Starting program: /path/to/test.exe 

Breakpoint 1, main () at test.c:14
14    count = 1;
(gdb) 

At this point - I can, for instance, hit continue (c); and the process will run through, not outputing anything, and break at the requested line: 
(gdb) c
Continuing.

Breakpoint 2, doFunction () at test.c:7
7     count += 2;
(gdb)

On the other hand, instead of continue - I can go line by line, either by using step (s) or next (n); for instance:
14    count = 1;
(gdb) n
15    count += 2;
(gdb) s
16    count = 0;
(gdb) s
19      doFunction();
(gdb) s

Breakpoint 2, doFunction () at test.c:7
7     count += 2;
(gdb) s
9     count--;
(gdb) s
10  }
(gdb) s
main () at test.c:20
20      printf(""%d\n"", count);
(gdb) s
...
(gdb) s
_IO_vfprintf_internal (s=Cannot access memory at address 0xe5853361
) at vfprintf.c:210
210 vfprintf.c: No such file or directory.
    in vfprintf.c
(gdb) s
245 in vfprintf.c
(gdb) s
210 in vfprintf.c
(gdb) n
245 in vfprintf.c
...
(gdb) n
2006    in vfprintf.c
(gdb) n
__printf (format=0x80484f0 ""%d\n"") at printf.c:39
39  printf.c: No such file or directory.
    in printf.c
(gdb) n
main () at test.c:21
21    }
(gdb) n
19      doFunction();
(gdb) n

Breakpoint 2, doFunction () at test.c:7
7     count += 2;
(gdb) 

Anyways, I am aware that I can keep Enter pressed, and the last entered command (step or next) will repeat (left a bit longer session in the second case, to show that 'next' remains on same level, 'step' steps inside the functions being called). However, as it can be seen, depending on whether step or next runs, it may take a while until a result is reached - and so, I don't want to sit for 10 minutes with my hand stuck on the Enter button :) 
So, my question is - can I somehow instruct gdb to run to 'breakpoint 2' without further user intervention - while printing out the lines it goes through, as if step (or next) was pressed?
",22k,"
            34
        ","['\nWell, this wasn\'t easy - but I think I somewhat got it :) I went through a bunch of failed attempts (posted here); relevant code is below.\nBasically, the problem in a ""next/step until breakpoint"" is how to determine whether you\'re ""on"" a breakpoint or not, if the debugger is stopped (at a step). Note also I use GDB 7.2-1ubuntu11 (current for Ubuntu 11.04). So, it went like this:\n\nI first found about Convenience Variables, and thought - given there are program counters and such available, there must be some GDB convenience variable that gives the ""breakpoint"" status, and can be used directly in a GDB script. After looking through GDB reference Index for a while, however, I simply cannot find any such variables (my attempts are in nub.gdb)\nIn lack of such a ""breakpoint status"" internal variable - the only thing left to do, is to capture the (\'stdout\') command line output of GDB (in response to commands) as a string, and parse it (looking for ""Breakpoint"")\nThen, I found out about Python API to GDB, and the gdb.execute(""CMDSTR"", toString=True) command - which is seemingly exactly what is needed to capture the output: ""By default, any output produced by command is sent to gdb\'s standard output. If the to_string parameter is True, then output will be collected by gdb.execute and returned as a string[1]""!\n\n\nSo, first I tried to make a script (pygdb-nub.py,gdbwrap) that would utilize gdb.execute in the recommended manner; failed here - because of this:\n\n\nBug 627506 – python: gdb.execute([...], to_string=True) partly prints to stdout/stderr\nBug 10808 – Allow GDB/Python API to capture and store GDB output\n\nThen, I thought I\'d use a python script to subprocess.Popen the GDB program, while replacing its stdin and stdout; and then proceed controlling GDB from there (pygdb-sub.py) - that failed too... (apparently, because I didn\'t redirect stdin/out right)\nThen, I thought I\'d use python scripts to be called from GDB (via source) which would internally fork into a pty whenever gdb.execute should be called, so as to capture its output (pygdb-fork.gdb,pygdb-fork.py)... This almost worked - as there are strings returned; however GDB notices something ain\'t right: ""[tcsetpgrp failed in terminal_inferior: Operation not permitted]"", and the subsequent return strings don\'t seem to change.\n\n\nAnd finally, the approach that worked is: temporarily redirecting the GDB output from a gdb.execute to a logfile in RAM (Linux: /dev/shm); and then reading it back, parsing it and printing it from python - python also handles a simple while loop that steps until a breakpoint is reached.\nThe irony is - most of these bugs, that caused this solution via redirecting the logfile, are actually recently fixed in SVN; meaning those will propagate to the distros in the near future, and one will be able to use gdb.execute(""CMDSTR"", toString=True) directly :/ Yet, as I cannot risk building GDB from source right now (and possibly bumping into possible new incompatibilites), this is good enough for me also :)\n\xa0\nHere are the relevant files (partially also in pygdb-fork.gdb,pygdb-fork.py):\npygdb-logg.gdb is:\n# gdb script: pygdb-logg.gdb\n# easier interface for pygdb-logg.py stuff\n# from within gdb: (gdb) source -v pygdb-logg.gdb\n# from cdmline: gdb -x pygdb-logg.gdb -se test.exe\n\n# first, ""include"" the python file:\nsource -v pygdb-logg.py\n\n# define shorthand for nextUntilBreakpoint():\ndefine nub\n  python nextUntilBreakpoint()\nend\n\n# set up breakpoints for test.exe:\nb main\nb doFunction\n\n# go to main breakpoint\nrun\n\npygdb-logg.py is:\n# gdb will \'recognize\' this as python\n#  upon \'source pygdb-logg.py\'\n# however, from gdb functions still have\n#  to be called like:\n#  (gdb) python print logExecCapture(""bt"")\n\nimport sys\nimport gdb\nimport os\n\ndef logExecCapture(instr):\n  # /dev/shm - save file in RAM\n  ltxname=""/dev/shm/c.log""\n\n  gdb.execute(""set logging file ""+ltxname) # lpfname\n  gdb.execute(""set logging redirect on"")\n  gdb.execute(""set logging overwrite on"")\n  gdb.execute(""set logging on"")\n  gdb.execute(instr)\n  gdb.execute(""set logging off"")\n\n  replyContents = open(ltxname, \'r\').read() # read entire file\n  return replyContents\n\n# next until breakpoint\ndef nextUntilBreakpoint():\n  isInBreakpoint = -1;\n  # as long as we don\'t find ""Breakpoint"" in report:\n  while isInBreakpoint == -1:\n    REP=logExecCapture(""n"")\n    isInBreakpoint = REP.find(""Breakpoint"")\n    print ""LOOP:: "", isInBreakpoint, ""\\n"", REP\n\n\xa0\nBasically, pygdb-logg.gdb loads the pygdb-logg.py python script, sets up the alias nub for nextUntilBreakpoint, and initializes the session - everything else is handled by the python script. And here is a sample session - in respect to the test source in OP:\n$ gdb -x pygdb-logg.gdb -se test.exe\n...\nReading symbols from /path/to/test.exe...done.\nBreakpoint 1 at 0x80483ec: file test.c, line 14.\nBreakpoint 2 at 0x80483c7: file test.c, line 7.\n\nBreakpoint 1, main () at test.c:14\n14    count = 1;\n(gdb) nub\nLOOP::  -1\n15    count += 2;\n\nLOOP::  -1\n16    count = 0;\n\nLOOP::  -1\n19      doFunction();\n\nLOOP::  1\n\nBreakpoint 2, doFunction () at test.c:7\n7     count += 2;\n\n(gdb) nub\nLOOP::  -1\n9     count--;\n\nLOOP::  -1\n10  }\n\nLOOP::  -1\nmain () at test.c:20\n20      printf(""%d\\n"", count);\n\n1\nLOOP::  -1\n21    }\n\nLOOP::  -1\n19      doFunction();\n\nLOOP::  1\n\nBreakpoint 2, doFunction () at test.c:7\n7     count += 2;\n\n(gdb)\n\n... just as I wanted it :P Just don\'t know how reliable it is (and whether it will be possible to use in avr-gdb, which is what I need this for :) EDIT: version of avr-gdb in Ubuntu 11.04 is currently 6.4, which doesn\'t recognize the python command :()\n\xa0\nWell, hope this helps someone,\nCheers!\n\xa0\nHere some references:\n\nGDB: error detected on stdin\nGDB has problems with getting commands piped to STDIN\nRe: [Gdb] How do i use GDB other input?\ngdb doesn\'t accept input on stdin\nUsing gdb in an IDE - comp.os.linux.development.apps | Google Groups\nrmathew: Terminal Sickness\n[TUTORIAL] Calling an external program in C (Linux) - GIDForums\nshell - how to use multiple arguments with a shebang (i.e. #!)? - Stack Overflow\nRedirecting/storing output of shell into GDB variable? - Stack Overflow\nCorey Goldberg: Python - Redirect or Turn Off STDOUT and STDERR\nThe Cliffs of Inanity › 9. Scripting gdb\ngdb python scripting: where has parse_and_eval gone? - Stack Overflow\nshell - Invoke gdb to automatically pass arguments to the program being debugged - Stack Overflow\nStoring Files/Directories In Memory With tmpfs | HowtoForge - Linux Howtos and Tutorials\nsimple way to touch a file if it does not exist | Python | Python\nos.fork() different in cgi-script? - Python\njava - Writing tests that use GDB - how to capture output? - Stack Overflow\nDebugging with GDB: How to create GDB Commands in Python - Wiki\nGDB reference card\n\n', '\nWhat about doing it like this in gdb, using a command file. Change file argument, and loop count as required.\ngdb -x run.gdb\n\nrun.gdb:\nset pagination off\nset logging file gdb.log\nset logging on\nset $i = 0\nfile main\nbreak main\nbreak WriteData\n# sadly, commands not getting executed on reaching breakpoint 2\ncommands 2\n  set $i=1000\n  print ""commands 2 : %d"",$i\nend\nrun\nwhile ( $i < 1000 )\n  step\n  # next\n  # continue\n  set $i = $i + 1\nend\n\n', ""\nBased on the link in @sdaau's answer (http://www.mail-archive.com/gdb@gnu.org/msg00031.html), I created my own script to simply keep sending 's' and reading the output of gdb continuously, while printing output to textfile and terminal, of course, my script can be modified to fit anyone else's needs, however, I hope that the modification I made should fit most people needs.\nhttp://www.codeground.net/coding/gdb-step-into-all-lines-to-get-full-application-flow/\nwget http://www.codeground.net/downloads/gdbwalkthrough.c\ngcc gdbwalkthrough.c -o gdbwalkthrough\n./gdbwalkthrough <application full path> [application arguments]\n\n"", '\nAs a new answer, since the previous is already hogged :) Basically, if the point is to observe execution of source (and/or assembly) code lines as the program as running - as the motivation is often for me when looking into ""automatic printout"" -- then, basically, a very quick way is to use GDB TUI mode; I quote:\nc - gdb behavior : value optimized out - Stack Overflow #1354762\n\nUse the GDB TUI mode. My copy of GDB enables it when I type the minus and Enter. Then type C-x 2 (that is hold down Control and press X, release both and then press 2). That will put it into split source and disassembly display. Then use stepi and nexti to move one machine instruction at a time. Use C-x o to switch between the TUI windows.\n\nThe trick here is that, even if you hit continue - this time source will be shown and indicated on the TUI; and followed as the program runs: \n \n... and this for me avoids many situations where I\'d have to script the breakpoints in ""auto-stepping context"" (although there are still such situations).. Docs about TUI: TUI - Debugging with GDB\nCheers!\n', '\nActually, I have a Github repo with a Python-GDB extension, which does exactly the same thing as You have described, but with some more functionality.\nYou can just clone the repo:\ngit clone https://github.com/Viaceslavus/gdb-debug-until.git\n\nand feed the python script to GDB with the following command inside GDB:\nsource gdb-debug-until/debug_until.py\n\n(Change python script path if necessary)\nAnd now you can use the following command to run through each line of your code until a breakpoint:\ndebug-until somefile.c:100 --args="""" --end=""somefile.c:200""\n\n""somefile.c:100"" here is a starting breakpoint, and ""somefile.c:200"" is the final breakpoint.\n""--args"" specifies a set of arguments to your program (you can omit it if there are no arguments).\nWith this extension you can also run few times through the code (with \'-r\' option) and even specify some events that should be handled while debugging. For more info see:\nhttps://github.com/Viaceslavus/gdb-debug-until\n', '\nThe currently accepted answer includes a lot of file io and does only stop on breakpoints, but watchpoints, signals and possibly even the program end is ignored.\nUsing the python api this can be handled nicely:\n\ndefine a user command (with additional argument to say how fast to auto-step)\noptional: define a parameter for the default (both variants below)\ndo the while loop within python, handle the ""expected"" keyboard interrupt of CTRL-C\nregister a stop event handler that checks for the stop reason and store the kind of step there\nadjust the while loop to stop for a ""not simple"" stop (breakpoint/watchpoint/signal/...)\n\nThe following code may be placed in a gdb-auto-step.py which can be made active with source gdb-auto-step.py whenever you want that (or include in the .gdbinit file to make it always available):\nimport gdb\nimport time\nimport traceback\n\nclass CmdAutoStep (gdb.Command):\n    """"""Auto-Step through the code until something happens or manually interrupted.\nAn argument says how fast auto stepping is done (1-19, default 5).""""""\n    def __init__(self):\n        print(\'Registering command auto-step\')\n        super(CmdAutoStep, self).__init__(""auto-step"", gdb.COMMAND_RUNNING)\n        gdb.events.stop.connect(stop_handler_auto_step)\n    def invoke(self, argument, from_tty):\n        # sanity check - are we even active, prevents a spurious ""no registers"" exception\n        try:\n            gdb.newest_frame()\n        except gdb.error:\n            raise gdb.GdbError(""The program is not being run."")\n\n        # calculate sleep time\n        if argument:\n            if not argument.isdigit():\n                raise gdb.GdbError(""argument must be a digit, not "" + argument)\n            number = int(argument)\n            if number == 0 or number > 19:\n                raise gdb.GdbError(""argument must be a digit between 1 and 19"")   \n        sleep_time = 3.0 / (1.4 ** number)\n\n        # activate GDB scrolling, otherwise we\'d auto-step only one page\n        pagination = gdb.parameter(""pagination"")\n        if pagination:\n            gdb.execute(""set pagination off"", False, False)\n\n        # recognize the kind of stop via stop_handler_auto_step \n        global last_stop_was_simple\n        last_stop_was_simple = True\n\n        # actual auto-stepping\n        try:\n            while last_stop_was_simple:\n                gdb.execute(""step"")\n                time.sleep(sleep_time)\n        # we just quit the loop as requested\n        # pass keyboard and user errors unchanged\n        except (KeyboardInterrupt, gdb.GdbError):\n            raise\n        # that exception is unexpected, but we never know...\n        except Exception:\n            traceback.print_exc()\n        # never leave without cleanup...\n        finally:\n            if pagination:\n                gdb.execute(""set pagination on"", False, False)\n\ndef stop_handler_auto_step(event):\n    # check the type of stop, the following is the common one after step/next,\n    # a more complex one would be a subclass (for example breakpoint or signal)\n    global last_stop_was_simple\n    last_stop_was_simple = type(event) is gdb.StopEvent\n\nCmdAutoStep()\n\n\nTo specify the default with a parameter (aka ""the gdb way"") add another parameter and use it (comes also with 0 = unlimited)\nimport gdb\nimport time\nimport traceback\n\nclass ParameterAutoStep (gdb.Parameter):\n    """"""auto-step default speed (0-19, default 5)""""""\n    def __init__(self):\n        self.set_doc = """"""Set speed for ""auto-step"", internally used to calculate sleep time between ""step""s.\nset ""auto-step 0"" causes there to be no sleeping.""""""\n        self.show_doc = ""Speed value for auto-step.""\n        super(ParameterAutoStep, self).__init__(""auto-step"", gdb.COMMAND_RUNNING, gdb.PARAM_UINTEGER)\n        self.value = 5\n        self.backup = self.value\n\n    def get_set_string (self):\n        try:\n            self.value = int(ParameterAutoStep.validate(self.value))\n        except gdb.GdbError:\n            self.value = int (self.backup)\n            raise\n        self.backup = self.value\n        return """"\n\n    @staticmethod\n    def validate (argument):\n        """"""validation for auto-step speed""""""\n        try:\n            speed = int(argument)\n            if speed < 0 or speed > 19:\n                raise ValueError()\n        except (TypeError, ValueError):\n            raise gdb.GdbError(""speed-argument must be an integer between 1 and 19, or 0"")\n        return speed\n\nclass CmdAutoStep (gdb.Command):\n    """"""Auto-Step through the code until something happens or manually interrupted.\nAn argument says how fast auto stepping is done (see parameter ""auto-step"").""""""\n    def __init__(self):\n        print(\'Registering command and parameter auto-step\')\n        super(CmdAutoStep, self).__init__(""auto-step"", gdb.COMMAND_RUNNING)\n        self.defaultSpeed = ParameterAutoStep()\n        gdb.events.stop.connect(stop_handler_auto_step)\n\n    def invoke(self, argument, from_tty):\n        # sanity check - are we even active, prevents a spurious ""no registers"" exception\n        try:\n            gdb.newest_frame()\n        except gdb.error:\n            raise gdb.GdbError(""The program is not being run."")\n\n        # calculate sleep time\n        if argument:\n            number = ParameterAutoStep.validate(argument) # raises an error if not valid\n        else:\n            number = self.defaultSpeed.value\n        if number:\n            sleep_time = 3.0 / (1.4 ** number)\n        else:\n            sleep_time = 0\n\n        # activate GDB scrolling, otherwise we\'d auto-step only one page\n        pagination = gdb.parameter(""pagination"")\n        if pagination:\n            gdb.execute(""set pagination off"", False, False)\n\n        # recognize the kind of stop via stop_handler_auto_step \n        global last_stop_was_simple\n        last_stop_was_simple = True\n\n        # actual auto-stepping\n        try:\n            while last_stop_was_simple:\n                gdb.execute(""step"")\n                time.sleep(sleep_time)\n        # we just quit the loop as requested\n        # pass keyboard and user errors unchanged\n        except (KeyboardInterrupt, gdb.GdbError):\n            raise\n        # that exception is unexpected, but we never know...\n        except Exception:\n            traceback.print_exc()\n        # never leave without cleanup...\n        finally:\n            if pagination:\n                gdb.execute(""set pagination on"", False, False)\n\ndef stop_handler_auto_step(event):\n    # check the type of stop, the following is the common one after step/next,\n    # a more complex one would be a subclass (for example breakpoint or signal)\n    global last_stop_was_simple\n    last_stop_was_simple = type(event) is gdb.StopEvent\n\nCmdAutoStep()\n\n\n']",https://stackoverflow.com/questions/6947389/how-to-print-every-executed-line-in-gdb-automatically-until-a-given-breakpoint-i,automation
How to use WebBrowser control DocumentCompleted event in C#?,"
Before starting writing this question, i was trying to solve following
// 1. navigate to page
// 2. wait until page is downloaded
// 3. read and write some data from/to iframe 
// 4. submit (post) form

The problem was, that if a iframe exists on a web page, DocumentCompleted event would get fired more then once (after each document has been completed). It was highly likely that program would have tried to read data from DOM that was not completed and naturally - fail.
But suddenly while writing this question 'What if' monster inspired me, and i fix'ed the problem, that i was trying to solve. As i failed Google'ing this, i thought it would be nice to post it here.
    private int iframe_counter = 1; // needs to be 1, to pass DCF test
    public bool isLazyMan = default(bool);

    /// <summary>
    /// LOCK to stop inspecting DOM before DCF
    /// </summary>
    public void waitPolice() {
        while (isLazyMan) Application.DoEvents();
    }

    private void webBrowser1_Navigating(object sender, WebBrowserNavigatingEventArgs e) {
        if(!e.TargetFrameName.Equals(""""))
            iframe_counter --;
        isLazyMan = true;
    }

    private void webBrowser1_DocumentCompleted(object sender, WebBrowserDocumentCompletedEventArgs e) {
        if (!((WebBrowser)sender).Document.Url.Equals(e.Url))
            iframe_counter++;
        if (((WebBrowser)sender).Document.Window.Frames.Count <= iframe_counter) {//DCF test
            DocumentCompletedFully((WebBrowser)sender,e);
            isLazyMan = false; 
        }
    }

    private void DocumentCompletedFully(WebBrowser sender, WebBrowserDocumentCompletedEventArgs e){
        //code here
    }

For now at least, my 5m hack seems to be working fine. 
Maybe i am really failing at querying google or MSDN, but i can not find:
""How to use webbrowser control DocumentCompleted event in C# ?""
Remark: After learning a lot about webcontrol, I found that it does FuNKY stuff. 
Even if you detect that the document has completed, in most cases it wont stay like that forever. Page update can be done in several ways - frame refresh, ajax like request or server side push (you need to have some control that supports asynchronous communication and has html or JavaScript interop). Also some iframes will never load, so it's not best idea to wait for them forever. 
I ended up using:
if (e.Url != wb.Url)

",52k,"
            14
        ","['\nYou might want to know the AJAX calls as well.\nConsider using this:\nprivate void webBrowser_DocumentCompleted(object sender, WebBrowserDocumentCompletedEventArgs e)\n{\n    string url = e.Url.ToString();\n    if (!(url.StartsWith(""http://"") || url.StartsWith(""https://"")))\n    {\n            // in AJAX\n    }\n\n    if (e.Url.AbsolutePath != this.webBrowser.Url.AbsolutePath)\n    {\n            // IFRAME \n    }\n    else\n    {\n            // REAL DOCUMENT COMPLETE\n    }\n}\n\n', '\nI have yet to find a working solution to this problem online. Hopefully this will make it to the top and save everyone the months of tweaking I spent trying to solve it, and the edge cases associated with it. I have fought over this issue over the years as Microsoft has changed the implementation/reliability of isBusy and document.readystate. With IE8, I had to resort to the following solution. It\'s similar to the question/answer from Margus with a few exceptions. My code will handle nested frames, javascript/ajax requests and meta-redirects. I have simplified the code for clarity sake, but I also use a timeout function (not included) to reset the webpage after if 5 minutes domAccess still equals false.\nprivate void m_WebBrowser_BeforeNavigate(object pDisp, ref object URL, ref object Flags, ref object TargetFrameName, ref object PostData, ref object Headers, ref bool Cancel)\n{\n    //Javascript Events Trigger a Before Navigate Twice, but the first event \n    //will contain javascript: in the URL so we can ignore it.\n    if (!URL.ToString().ToUpper().StartsWith(""JAVASCRIPT:""))\n    {\n        //indicate the dom is not available\n        this.domAccess = false;\n        this.activeRequests.Add(URL);\n    }\n}\n\nprivate void m_WebBrowser_DocumentComplete(object pDisp, ref object URL) \n{\n\n    this.activeRequests.RemoveAt(0);\n\n    //if pDisp Matches the main activex instance then we are done.\n    if (pDisp.Equals((SHDocVw.WebBrowser)m_WebBrowser.ActiveXInstance)) \n    {\n        //Top Window has finished rendering \n        //Since it will always render last, clear the active requests.\n        //This solves Meta Redirects causing out of sync request counts\n        this.activeRequests.Clear();\n    }\n    else if (m_WebBrowser.Document != null)\n    {\n        //Some iframe completed dom render\n    }\n\n    //Record the final complete URL for reference\n    if (this.activeRequests.Count == 0)\n    {\n        //Finished downloading page - dom access ready\n        this.domAccess = true;\n    }\n}\n\n', '\nUnlike Thorsten I didn\'t have to use ShDocVw, but what did make the difference for me was adding the loop checking ReadyState and using Application.DoEvents() while not ready.  Here is my code:\n        this.webBrowser.DocumentCompleted += new WebBrowserDocumentCompletedEventHandler(WebBrowser_DocumentCompleted);\n        foreach (var item in this.urlList) // This is a Dictionary<string, string>\n        {\n            this.webBrowser.Navigate(item.Value);\n            while (this.webBrowser1.ReadyState != WebBrowserReadyState.Complete)\n            {\n                Application.DoEvents();\n            }\n        }\n\nAnd I used Yuki\'s solution for checking the results of WebBrowser_DocumentCompleted, though with the last if/else swapped per user\'s comment:\n     private void WebBrowser_DocumentCompleted(object sender, WebBrowserDocumentCompletedEventArgs e)\n    {\n        string url = e.Url.ToString();\n        var browser = (WebBrowser)sender;\n\n        if (!(url.StartsWith(""http://"") || url.StartsWith(""https://"")))     \n        {             \n            // in AJAX     \n        }\n        if (e.Url.AbsolutePath != this.webBrowser.Url.AbsolutePath)     \n        {\n            // IFRAME           \n        }     \n        else     \n        {             \n            // REAL DOCUMENT COMPLETE\n            // Put my code here\n        }\n    }\n\nWorked like a charm :)\n', '\nI had to do something similar. What I do is use ShDocVw directly (adding a reference to all the necessary interop assemblies to my project). Then, I do not add the WebBrowser control to my form, but the AXShDocVw.AxWebBrowser control.\nTo navigate and wait I use to following method:\nprivate void GotoUrlAndWait(AxWebBrowser wb, string url)\n{\n    object dummy = null;\n    wb.Navigate(url, ref dummy, ref dummy, ref dummy, ref dummy);\n\n    // Wait for the control the be initialized and ready.\n    while (wb.ReadyState != SHDocVw.tagREADYSTATE.READYSTATE_COMPLETE)\n        Application.DoEvents();\n}\n\n', '\nJust thought to drop a line or two here about a small improvement which works in conjunction with the code of FeiBao. The idea is to inject a landmark (javascript) variable in the webpage and use that to detect which of the subsequent DocumentComplete events is the real deal. I doubt it\'s bulletproof but it has worked more reliably in general than the approach that lacks it. Any comments welcome. Here is the boilerplate code:\n void WebBrowser_DocumentCompleted(object sender, WebBrowserDocumentCompletedEventArgs e)\n    {\n        string url = e.Url.ToString();\n        var browser = (WebBrowser)sender;\n\n        if (!(url.StartsWith(""http://"") || url.StartsWith(""https://"")))\n        {\n            // in AJAX     \n        }\n        if (e.Url.AbsolutePath != this.webBrowser.Url.AbsolutePath)\n        {\n            // IFRAME           \n        }\n        else if (browser.Document != null && (bool)browser.Document.InvokeScript(""eval"", new object[] { @""typeof window.YourLandMarkJavascriptVariableHere === \'undefined\'"" }))\n        {\n            ((IHTMLWindow2)browser.Document.Window.DomWindow).execScript(""var window.YourLandMarkJavascriptVariableHere = true;"");\n\n            // REAL DOCUMENT COMPLETE\n            // Put my code here\n        }\n    }\n\n']",https://stackoverflow.com/questions/840813/how-to-use-webbrowser-control-documentcompleted-event-in-c,automation
Automate saveas dialogue for IE9 (vba),"
I am trying to download an excel sheet from a website. I have thus far achieved until clicking the download button automatically (web scraping). Now ie9 is popping a save as screen. How do i automate that?
",15k,"
            9
        ","['\nYou may try this as it is worked for me on IE9:\n\n\nCopy file C:\\Windows\\System32\\UIAutomationCore.dll file to users Documents i.e C:\\Users\\admin\\Documents then add reference UIAutomationClient to your macro file. \nPaste below code in your module: \n    Option Explicit\n    Dim ie As InternetExplorer\n    Dim h As LongPtr\n    Private Declare PtrSafe Function FindWindowEx Lib ""user32"" Alias ""FindWindowExA"" (ByVal hWnd1 As LongPtr, ByVal hWnd2 As LongPtr, ByVal lpsz1 As String, ByVal lpsz2 As String) As LongPtr\n\nSub Download()\n    Dim o As IUIAutomation\n    Dim e As IUIAutomationElement\n    Set o = New CUIAutomation\n    h = ie.Hwnd\n    h = FindWindowEx(h, 0, ""Frame Notification Bar"", vbNullString)\n    If h = 0 Then Exit Sub\n\n    Set e = o.ElementFromHandle(ByVal h)\n    Dim iCnd As IUIAutomationCondition\n    Set iCnd = o.CreatePropertyCondition(UIA_NamePropertyId, ""Save"")\n\n    Dim Button As IUIAutomationElement\n    Set Button = e.FindFirst(TreeScope_Subtree, iCnd)\n    Dim InvokePattern As IUIAutomationInvokePattern\n    Set InvokePattern = Button.GetCurrentPattern(UIA_InvokePatternId)\n    InvokePattern.Invoke\nEnd Sub   \n\n\nTry at your end.\n', '\n\'This is a working code for vba in excel 2007 to open a file\n\'But you need to add the ""UIAutomationCore.dll"" to be copied \n\'from ""C:\\Windows\\System32\\UIAutomationCore.dll"" into the \n\'path ""C:\\Users\\admin\\Documents""    \n\'The path where to copy may be different and you can find it when you check on \n\'the box for UIAutomationClient - the location is given under it.\n\'Tools-references\n\nOption Explicit\n  Dim ie As InternetExplorer\n  Dim h As LONG_PTR\n  Private Declare Function FindWindowEx Lib ""user32"" Alias ""FindWindowExA"" (ByVal hWnd1 As LONG_PTR, ByVal hWnd2 As LONG_PTR, ByVal lpsz1 As String, ByVal lpsz2 As String) As LONG_PTR\n\n\nSub click_open()\n  Dim o As IUIAutomation\n  Dim e As IUIAutomationElement\n  Dim sh\n  Dim eachIE\n\nDo\n\n    Set sh = New Shell32.Shell\n     For Each eachIE In sh.Windows\n         \' Check if this is the desired URL\n\n    \' Here you can use your condition except .html\n    \' If you want to use your URL , then put the URL below in the code for condition check.\n    \' This code will reassign your IE object with the same reference for navigation and your issue will resolve.\n         If InStr(1, eachIE.LocationURL, ""<enter your page url>"") Then\n         Set ie = eachIE\n         Exit Do\n         End If\n     Next eachIE\n Loop\n\nSet o = New CUIAutomation\nh = ie.Hwnd\nh = FindWindowEx(h, 0, ""Frame Notification Bar"", vbNullString)\nIf h = 0 Then Exit Sub\n\nSet e = o.ElementFromHandle(ByVal h)\nDim iCnd As IUIAutomationCondition\nSet iCnd = o.CreatePropertyCondition(UIA_NamePropertyId, ""Open"")\n\n\nDim Button As IUIAutomationElement\nSet Button = e.FindFirst(TreeScope_Subtree, iCnd)\nDim InvokePattern As IUIAutomationInvokePattern\nSet InvokePattern = Button.GetCurrentPattern(UIA_InvokePatternId)\nInvokePattern.Invoke\n\nEnd Sub\n\n', '\nI sent the shortcut keys to IE11.\nNote: the code will not run as you expect if IE is not the active window on your machine so it won\'t work while in debug mode. The shortcut keys and how to send them are below.\n\nShortcut key:Alt+S\nVBA: Application.SendKeys ""%{S}""\n\n']",https://stackoverflow.com/questions/26038165/automate-saveas-dialogue-for-ie9-vba,automation
"NAnt or MSBuild, which one to choose and when?","
I am aware there are other NAnt and MSBuild related questions on Stack Overflow, but I could not find a direct comparison between the two and so here is the question.
When should one choose NAnt over MSBuild? Which one is better for what? Is NAnt more suitable for home/open source projects and MSBuild for work projects? What is the experience with any of the two?
",32k,"
            162
        ","['\nI\'ve done a similar investigation this week. Here\'s what I\'ve been able to determine:\nNAnt:\n\nCross-platform (supports Linux/Mono). It may be handy for installing a web site to multiple targets (that is,  Linux Apache and Windows IIS), for example.\n95% similar in syntax to Ant (easy for current Ant users or Java builders to pick up)\nIntegration with NUnit for running unit tests as part of the build, and with NDoc for producting documentation.\n\nMSBuild:\n\nBuilt-in to .NET.\nIntegrated with Visual Studio\nEasy to get started with MSBuild in Visual Studio - it\'s all behind the scenes. If you want to get deeper, you can hand edit the files.\n\nSubjective Differences: (YMMV)\n\nNAnt documentation is a little more straightforward. For example, the MSBuild Task Reference lists ""Csc Task - Describes the Csc task and its parameters. "" (thanks for the ""help""?), vs the NAnt Task Reference ""csc - Compiles C# programs."" UPDATE: I\'ve noticed the MSBuild documentation has been improved and is much better now (probably on par with NAnt).\nNot easy to figure out how to edit the build script source (*.*proj file) directly from within Visual Studio. With NAnt I just have Visual Studio treat the .build script as an XML file.\nApparently, in Visual Studio, Web Application Projects don\'t get a *.*proj file by default, so I had great difficulty figuring out how to even get MSBuild to run on mine to create a deployment script.\nNAnt is not built-in to Visual Studio and has to be added, either with an Add-In, or as an ""External Tool"". This is a bit of a pain to set up.\n(Edit:) One of my coworkers brought this up--if you want to set up a build machine using CruiseControl for continuous integration, CruiseControl integrates with NAnt nicely out of the box. UPDATE: CruiseControl also has an MSBuild task.\nPlease see comments below for full and up-to-date discussion of subjective differences.\n\n', '\nOne of the major draws of MSBuild for me (on Windows platforms) is that it comes as part of .NET itself. That means that any Windows machine that is up-to-date with Windows Update will have MSBuild available. Add to this the fact that C# compiler is also part of .NET itself and you have a platform that can build projects on clean machines. No need to install Visual Studio behemoth. NAnt, on the other hand, has to be explicitly installed  before a build can be triggered.\nJust for the record, I\'ve used NMake, Make, Ant, Rake, NAnt and MSBuild on non-trivial builds in the past (in that order). My favourite is MSBuild, hands down (and I do not favour it because ""that\'s what Visual Studio uses""). IMHO, it is a very under-appreciated build tool.\nI would compare NAnt vs. MSBuild to the difference between procedural and functional programming. NAnt is quite straightforward and you-get-what-you-see. MSBuild on the other hand requires a bit more thought. The learning curve is steeper. But once you ""get it"", you can do some amazing things with it.\nSo I would recommend looking at MSBuild if you also gravitate towards functional or logical style programming - if you are willing to invest a bit of time and effort before seeing tangible results (of course, I also strongly believe that the investment eventually pays off and you can do more powerful things more efficiently).\n', '\nPersonally, I use both - for the same project.\nMSBuild is great at building Visual Studio solutions and projects - that\'s what it\'s made for.\nNAnt is more easily hand-editable, in my opinion - particularly if you already know Ant. NAnt can call MSBuild very easily with NAntContrib. So, I hand-craft a NAnt script to do things like copying built files, cleaning up etc - and call MSBuild to do the actual ""turn my C# source code into assemblies"" part.\nIf you want an example of that, look at my Protocol Buffers build file. (I wouldn\'t claim it\'s a fabulous NAnt script, but it does the job.)\n', ""\nNAnt has more features out of the box, but MSBuild has a much better fundamental structure (item metadata rocks) which makes it much easier to build reusable MSBuild scripts.\nMSBuild takes a while to understand, but once you do it's very nice.\nLearning materials:\n\nInside the Microsoft Build Engine: Using MSBuild and Team Foundation Build\nby Sayed Ibrahim Hashimi (Jan, 2009)\nDeploying .NET Applications: Learning MSBuild and ClickOnce by Sayed\nby Y. Hashimi (Sep, 2008)\n\n"", ""\nKISS = Use MSBuild.\nWhy add something else into the mix when you have something that will do a reasonable job out of the box? When MSBuild arrived, NAnt died. And Mono will have an MSBuild implementation, (xbuild).\nDRY = Use MSBuild.\nAsk yourself what do you want out of a build system? I want a build system that is also used by my IDE rather than the maintaining two different configurations.\nPersonally, I'd love to hear some real arguments for NAnt, because I just can't think of any that really hold water.\n"", ""\nOne thing I noticed several posters mention was having to hand edit the .csproj (or .vbproj, etc.) files. \nMSBuild allows customization of these .*proj files through the use of .user files. If I have a project named MyCreativelyNamedProject.csproj and want to customize the MSBuild tasks inside of it, I can create a file named MyCreativelyNamedProject.csproj.user and use the CustomBeforeMicrosoftCommonTargets and CustomAfterMicrosoftCommonTargets to customize those files.\nAlso, both NAnt and MSBuild can be customized to the heart's content through custom MSBuild tasks and through NantContrib extensions.\nSo, using NAnt or MSBuild really comes down to familiarity:\n\nIf you are already familiar with Ant, use NAnt. The learning curve will be very easy.\nIf you are not familiar with either tool, MSBuild is integrated with Visual Studio already and requires no additional tools.\n\nIt is also worth adding that MSBuild is pretty much guaranteed to work with all new versions of .NET and Visual Studio as soon as they are released, whereas NAnt may have some lag.\n"", ""\nI use both in that my NAnt scripts call MSBuild. My main reason for staying with NAnt is isolation. Let me explain why I feel this is important:\n\nAdding dependencies to your project.\nThe NAnt build file is alien to Visual Studio (in my case I consider this a pro) so Visual Studio does not attempt to do anything with it. MSBuild tasks are embedded so part of the solution and can refer to other MSBuild tasks. I've received source code from someone else only to find out I could not build, because the MSBuild community tasks were not installed. What I find particularly frustrating is that Visual Studio just would not build and threw a bunch of errors that made me loose time debugging. This, despite the fact that the build being requested could have gone ahead (as a debug build for instance) without some of the extras of the MSBuild task. In short: I don't like adding dependencies to my project if I can avoid it.\nI don't trust Visual Studio as far as I could throw its development team. This stems back to the early days of Visual Studio when it would massacre my HTML. I still do not use the designer for instance (at a conference recently I found colleagues did the same). I have found that Visual Studio can screw up dependencies and version numbers in the DLL file (I cannot replicate this, but it did happen in a project consistently and caused a lot of grief and lost time). I have resorted to a build procedure that uses Visual Studio to build in debug mode only. For production, I use NAnt so that I control everything externally. Visual Studio just cannot interfere any longer if I build using NAnt.\n\nPS: I'm a web developer and do not do Windows Forms development.\n"", ""\nWhile I'm not very familiar with MsBuild, I'm under the impression that some of key differences on both sides can be supplemented by additions:\n\nMsBuildTasks\nNantContrib\n\nI recently had to build a Silverlight project in Nant. I discovered that life would be easier if I just did this with MsBuild - I ended up calling a MsBuild task from within a Nant script so I suppose it's not too out of the ordinary to mix and match the two.\nBeyond that, I suppose it's going to be a question of personal preference - obviously you can manage some/most of MsBuild's functionality from within Visual Studio, if that's your thing. Nant seems more flexible and better suited if you prefer to write scripts by hand, and if you come from the Java world you'll likely be right at home with it.\n"", ""\nI ended up using both. When redesigning our build system, I was facing a tricky problem. Namely, I couldn't get rid of .vcproj (and family) because we everybody was using VS to update the project files, settings, and configurations. So without a huge duplication and error prone process, we couldn't base our build system on a new set of files.\nFor this reason, I decided to keep the 'proj' files of VS and use MSBuild (they are MSBuild files, at least VS2005 and VS2008 use MSBuild project files). For everything else (custom configuration, unit testing, packaging, preparing documentation...) I used NAnt.\nFor continuous integration, I used CruiseControl. So we had CC scripts that triggered NAnt jobs, which for building used MSBuild.\nOne final note: MSBuild does NOT support Setup projects! So you're stuck with calling DevEnv.com or using Visual Studio directly. That's what I ended up doing, but I disabled the setup project by default from all solution configurations, since developers wouldn't normally need to build them, and if they do, they can manually select to build them.\n"", '\nI have switched from NAnt to MSBuild recently because of its ability to build VS solutions. I still use NAnt occasionally, though.\nYou may also want to check out MSBuild Community Tasks which is like NAntContrib.\n', ""\nThe documentation and tutorials available for NAnt make it easier to begin learning build scripts with NAnt. Once I got the hang of NAnt and creating build scripts I started translating that knowledge to MSBuild (I did X in NAnt, how do I do X in MSBuild?). Microsoft's documentation usually assumes a pretty high level of knowledge before it is useful.\nThe reason for switching from NAnt to MSBuild is because MSBuild is more current. Unfortunately the last release of NAnt was in December 8 2007, while MSBuild 4.0 (.NET 4.0) isn't far off. It looks like the NAnt project has died.\nIf you find good documentation for someone just beginning to learn creating build scripts using MSBuild, then skip NAnt and go straight for MSBuild. If NAnt ever comes out with a new release then I'd consider sticking with NAnt, but they're lagging behind now.\n"", '\nWe use both. NAnt is responsible for all ""scripting"" stuff, like copying, deploying on IIS, creating packages and MSBuild is responsible for building the solution. Then we can avoid problems with non-supported .NET 4.0 by a new version of NAnt.\nNAnt is also more scalable. If we want to migrate the deployment scripts to production servers, we only copy the build file and install a proper version of .NET - no Visual Studio problems with csproj files:)\n', '\nYDeliver by Manoj is a build framework built on top of PSake. It has a rich set of library functions, ability to define workflows, and we have used it to deliver over six enterprise projects to production.\nUse it in conjunction with TeamCity, CruiseControl, or anything that can run PowerShell.\n', '\nWe use FlubuCore. It\'s an open source C# library for building projects and executing deployment scripts using C# code.\nSimple example of how flubu is used:\nprotected override void ConfigureTargets(ITaskContext session)\n{           \n\n    var compile = session.CreateTarget(""compile"")\n        .SetDescription(""Compiles the solution."")\n        .AddTask(x => x.CompileSolutionTask())\n        .DependsOn(""generate.commonassinfo"");\n}\n\nYou can find more information about flubu and how to get started here:\nchoice-for-build-tool-msbuild-nant-or-something-else\n']",https://stackoverflow.com/questions/476163/nant-or-msbuild-which-one-to-choose-and-when,automation
Dead code detection in legacy C/C++ project [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



How would you go about dead code detection in C/C++ code? I have a pretty large code base to work with and at least 10-15% is dead code. Is there any Unix based tool to identify this areas? Some pieces of code still use a lot of preprocessor, can automated process handle that?
",38k,"
            68
        ","[""\nYou could use a code coverage analysis tool for this and look for unused spots in your code.\nA popular tool for the gcc toolchain is gcov, together with the graphical frontend lcov (http://ltp.sourceforge.net/coverage/lcov.php).\nIf you use gcc, you can compile with gcov support, which is enabled by the '--coverage' flag. Next, run your application or run your test suite with this gcov enabled build.\nBasically gcc will emit some extra files during compilation and the application will also emit some coverage data while running. You have to collect all of these (.gcdo and .gcda files). I'm not going in full detail here, but you probably need to set two environment variables to collect the coverage data in a sane way: GCOV_PREFIX and GCOV_PREFIX_STRIP... \nAfter the run, you can put all the coverage data together and run it through the lcov toolsuite. Merging of all the coverage files from different test runs is also possible, albeit a bit involved.\nAnyhow, you end up with a nice set of webpages showing some coverage information, pointing out the pieces of code that have no coverage and hence, were not used.\nOff course, you need to double check if the portions of code are not used in any situation and a lot depends on how good your tests exercise the codebase.  But at least, this will give an idea about possible dead-code candidates...\n"", '\nCompile it under gcc with -Wunreachable-code. \nI think that the more recent the version, the better results you\'ll get, but I may be wrong in my impression that it\'s something they\'ve been actively working on. Note that this does flow analysis, but I don\'t believe it tells you about ""code"" which is already dead by the time it leaves the preprocessor, because that\'s never parsed by the compiler. It also won\'t detect e.g. exported functions which are never called, or special case handling code which just so happen to be impossible because nothing ever calls the function with that parameter - you need code coverage for that (and run the functional tests, not the unit tests. Unit tests are supposed to have 100% code coverage, and hence execute code paths which are \'dead\' as far as the application is concerned). Still, with these limitations in mind it\'s an easy way to get started finding the most completely bollixed routines in the code base.\nThis CERT advisory lists some other tools for static dead code detection\n', '\nFor C code only and assuming that the source code of the whole project\nis available, launch an analysis with the Open Source tool Frama-C.\nAny statement of the program that displays red in the GUI is\ndead code.\nIf you have ""dead code"" problems, you may also be interested in\nremoving ""spare code"", code that is executed but does not\ncontribute to the end result. This requires you to provide\nan accurate modelization of I/O functions (you wouldn\'t want\nto remove a computation that appears to be ""spare"" but\nthat is used as an argument to printf). Frama-C has an option for pointing out spare code.\n', ""\nYour approach depends on the availability (automated) tests. If you have a test suite that you trust to cover a sufficient amount of functionality, you can use a coverage analysis, as previous answers already suggested. \nIf you are not so fortunate, you might want to look into source code analysis tools like SciTools' Understand that can help you analyse your code using a lot of built in analysis reports. My experience with that tool dates from 2 years ago, so I can't give you much detail, but what I do remember is that they had an impressive support with very fast turnaround times of bug fixes and answers to questions.\nI found a page on static source code analysis that lists many other tools as well.\nIf that doesn't help you sufficiently either, and you're specifically interested in finding out the preprocessor-related dead code, I would recommend you post some more details about the code. For example, if it is mostly related to various combinations of #ifdef settings you could write scripts to determine the (combinations of) settings and find out which combinations are never actually built, etc.\n"", '\nBoth Mozilla and Open Office have home-grown solutions.\n', '\ng++ 4.01 -Wunreachable-code warns about code that is unreachable within a function, but does not warn about unused functions.\nint foo() { \n    return 21; // point a\n}\n\nint bar() {\n  int a = 7;\n  return a;\n  a += 9;  // point b\n  return a;\n}\n\nint main(int, char **) {\n    return bar();\n}\n\ng++ 4.01 will issue a warning about point b, but say nothing about foo() (point a) even though it is unreachable in this file.  This behavior is correct although disappointing, because a compiler cannot know that function foo() is not declared extern in some other compilation unit and invoked from there; only a linker can be sure.\n', ""\nDead code analysis like this requires a global analysis of your entire project. You can't get this information by analyzing translation units individually (well, you can detect dead entities if they are entirely within a single translation unit, but I don't think that's what you are really looking for).\nWe've used our DMS Software Reengineering Toolkit to implement exactly this for Java code, by parsing all the compilation-units involved at once, building symbol tables for everything and chasing down all the references. A top level definition with no references and no claim of being an external API item is dead. This tool also automatically strips out the dead code, and at the end you can choose what you want: the report of dead entities, or the code stripped of those entities.\nDMS also parses C++ in a variety of dialects (EDIT Feb 2014: including MS and GCC versions of C++14 [EDIT Nov 2017: now C++17]) and builds all the necessary symbol tables. Tracking down the dead references would be straightforward from that point. DMS could also be used to strip them out. See http://www.semanticdesigns.com/Products/DMS/DMSToolkit.html\n"", '\nBullseye coverage tool would help. It is not free though.\n']",https://stackoverflow.com/questions/229069/dead-code-detection-in-legacy-c-c-project,automation
Check if element is clickable in Selenium Java,"
I'm new to Selenium and need to check if element is clickable in Selenium Java, since element.click() passes both on link and label.
I tried using the following code, but it is not working:
WebDriverWait wait = new WebDriverWait(Scenario1Test.driver, 10);

if(wait.until(ExpectedConditions.elementToBeClickable(By.xpath(""(//div[@id='brandSlider']/div[1]/div/div/div/img)[50]"")))==null)

",216k,"
            29
        ","['\nelementToBeClickable is used for checking an element is visible and enabled such that you can click it. \nExpectedConditions.elementToBeClickable returns WebElement if expected condition is true otherwise it will throw TimeoutException, It never returns null.\nSo if your using ExpectedConditions.elementToBeClickable to find an element which will always gives you the clickable element, so no need to check for null condition, you should try as below :-\nWebDriverWait wait = new WebDriverWait(Scenario1Test.driver, 10); \nWebElement element = wait.until(ExpectedConditions.elementToBeClickable(By.xpath(""(//div[@id=\'brandSlider\']/div[1]/div/div/div/img)[50]"")));\nelement.click();\n\nAs you are saying element.click() passes both on link and label that\'s doesn\'t mean element is not clickable, it means returned element clicked but may be there is no event performs on element by click action.\nNote:- I\'m suggesting you always try first to find elements by id, name, className and other locator. if you faced some difficulty to find then use cssSelector and always give last priority to xpath locator because it is slower than other locator to locate an element.\nHope it helps you..:)\n', '\nThere are instances when element.isDisplayed() && element.isEnabled() will return true but still element will not be clickable, because it is hidden/overlapped by some other element. \nIn such case, Exception caught is: \n\norg.openqa.selenium.WebDriverException: unknown error: Element is not\n  clickable at point (781, 704). Other element would receive the click:\n  <div class=""footer"">...</div> \n\nUse this code instead:\nWebElement  element=driver.findElement(By.xpath"""");  \nJavascriptExecutor ex=(JavascriptExecutor)driver;\nex.executeScript(""arguments[0].click()"", element);\n\nIt will work.\n', ""\nwait.until(ExpectedConditions) won't return null, it will either meet the condition or throw TimeoutException.\nYou can check if the element is displayed and enabled\nWebElement element = driver.findElement(By.xpath);\nif (element.isDisplayed() && element.isEnabled()) {\n    element.click();\n}\n\n"", '\nThere are certain things you have to take care:\n\nWebDriverWait inconjunction with ExpectedConditions as elementToBeClickable() returns the WebElement once it is located and clickable i.e. visible and enabled.\nIn this process, WebDriverWait will ignore instances of NotFoundException that are encountered by default in the until condition.\nOnce the duration of the wait expires on the desired element not being located and clickable, will throw a timeout exception.\nThe different approach to address this issue are:\n\nTo invoke click() as soon as the element is returned, you can use:\nnew WebDriverWait(driver, 10).until(ExpectedConditions.elementToBeClickable(By.xpath(""(//div[@id=\'brandSlider\']/div[1]/div/div/div/img)[50]""))).click();\n\n\nTo simply validate if the element is located and clickable, wrap up the WebDriverWait in a try-catch{} block as follows:\ntry {\n       new WebDriverWait(driver, 10).until(ExpectedConditions.elementToBeClickable(By.xpath(""(//div[@id=\'brandSlider\']/div[1]/div/div/div/img)[50]"")));\n       System.out.println(""Element is clickable"");\n     }\ncatch(TimeoutException e) {\n       System.out.println(""Element isn\'t clickable"");\n    }\n\n\nIf WebDriverWait returns the located and clickable element but the element is still not clickable, you need to invoke executeScript() method as follows:\nWebElement element = new WebDriverWait(driver, 10).until(ExpectedConditions.elementToBeClickable(By.xpath(""(//div[@id=\'brandSlider\']/div[1]/div/div/div/img)[50]""))); \n((JavascriptExecutor)driver).executeScript(""arguments[0].click();"", element);\n\n\n\n\n\n', '\nFrom the source code you will be able to view that, ExpectedConditions.elementToBeClickable(), it will judge the element visible and enabled, so you can use isEnabled() together with isDisplayed(). Following is the source code.\n\n\npublic static ExpectedCondition<WebElement> elementToBeClickable(final WebElement element) {\r\n\t\treturn new ExpectedCondition() {\r\n\t\t\tpublic WebElement apply(WebDriver driver) {\r\n\t\t\t\tWebElement visibleElement = (WebElement) ExpectedConditions.visibilityOf(element).apply(driver);\r\n\r\n\t\t\t\ttry {\r\n\t\t\t\t\treturn visibleElement != null && visibleElement.isEnabled() ? visibleElement : null;\r\n\t\t\t\t} catch (StaleElementReferenceException arg3) {\r\n\t\t\t\t\treturn null;\r\n\t\t\t\t}\r\n\t\t\t}\r\n\r\n\t\t\tpublic String toString() {\r\n\t\t\t\treturn ""element to be clickable: "" + element;\r\n\t\t\t}\r\n\t\t};\r\n\t}\n\n\n', '\nthe class attribute contains disabled when the element is not clickable.\nWebElement webElement = driver.findElement(By.id(""elementId""));\nif(!webElement.getAttribute(""class"").contains(""disabled"")){\n    webElement.click();\n}\n\n', '\nList<WebElement> wb=driver.findElements(By.xpath(newXpath));\n        for(WebElement we: wb){\n            if(we.isDisplayed() && we.isEnabled())\n            {\n                we.click();\n                break;\n            }\n        }\n    }\n\n']",https://stackoverflow.com/questions/38327049/check-if-element-is-clickable-in-selenium-java,automation
Interact with other programs using Python,"
I'm having the idea of writing a program using Python which shall find a lyric of a song whose name I provided. I think the whole process should boil down to couple of things below. These are what I want the program to do when I run it:

prompt me to enter a name of a song
copy that name
open a web browser (google chrome for example)
paste that name in the address bar and find information about the song
open a page that contains the lyrics
copy that lyrics
run a text editor (like Microsoft Word for instance)
paste the lyrics
save the new text file with the name of the song

I am not asking for code, of course. I just want to know the concepts or ideas about how to use python to interact with other programs
To be more specific, I think I want to know, fox example, just how we point out where is the address bar in Google Chrome and tell python to paste the name there. Or how we tell python how to copy the lyrics as well as paste it into the Microsof Word's sheet then save it.
I've been reading (I'm still reading) several books on Python: Byte of python, Learn python the hard way, Python for dummies, Beginning Game Development with Python and Pygame. However, I found out that it seems like I only (or almost only) learn to creat programs that work on itself (I can't tell my program to do things I want with other programs that are already installed on my computer)
I know that my question somehow sounds rather silly, but I really want to know how it works, the way we tell Python to regconize that this part of the Google chrome browser is the address bar and that it should paste the name of the song in it. The whole idea of making python interact with another program is really really vague to me and I just 
extremely want to grasp that.
Thank you everyone, whoever spend their time reading my so-long question.
ttriet204
",97k,"
            28
        ","['\nIf what you\'re really looking into is a good excuse to teach yourself how to interact with other apps, this may not be the best one. Web browsers are messy, the timing is going to be unpredictable, etc. So, you\'ve taken on a very hard task—and one that would be very easy if you did it the usual way (talk to the server directly, create the text file directly, etc., all without touching any other programs).\nBut if you do want to interact with other apps, there are a variety of different approaches, and which is appropriate depends on the kinds of apps you need to deal with.\n\nSome apps are designed to be automated from the outside. On Windows, this nearly always means they a COM interface, usually with an IDispatch interface, for which you can use pywin32\'s COM wrappers; on Mac, it means an AppleEvent interface, for which you use ScriptingBridge or appscript; on other platforms there is no universal standard. IE (but probably not Chrome) and Word both have such interfaces.\nSome apps have a non-GUI interface—whether that\'s a command line you can drive with popen, or a DLL/SO/DYLIB you can load up through ctypes. Or, ideally, someone else has already written Python bindings for you.\nSome apps have nothing but the GUI, and there\'s no way around doing GUI automation. You can do this at a low level, by crafting WM_ messages to send via pywin32 on Windows, using the accessibility APIs on Mac, etc., or at a somewhat higher level with libraries like pywinauto, or possibly at the very high level of selenium or similar tools built to automate specific apps.\n\nSo, you could do this with anything from selenium for Chrome and COM automation for Word, to crafting all the WM_ messages yourself. If this is meant to be a learning exercise, the question is which of those things you want to learn today.\n\nLet\'s start with COM automation. Using pywin32, you directly access the application\'s own scripting interfaces, without having to take control of the GUI from the user, figure out how to navigate menus and dialog boxes, etc. This is the modern version of writing ""Word macros""—the macros can be external scripts instead of inside Word, and they don\'t have to be written in VB, but they look pretty similar. The last part of your script would look something like this:\nword = win32com.client.dispatch(\'Word.Application\')\nword.Visible = True\ndoc = word.Documents.Add()\ndoc.Selection.TypeText(my_string)\ndoc.SaveAs(r\'C:\\TestFiles\\TestDoc.doc\')\n\nIf you look at Microsoft Word Scripts, you can see a bunch of examples. However, you may notice they\'re written in VBScript. And if you look around for tutorials, they\'re all written for VBScript (or older VB). And the documentation for most apps is written for VBScript (or VB, .NET, or even low-level COM). And all of the tutorials I know of for using COM automation from Python, like Quick Start to Client Side COM and Python, are written for people who already know about COM automation, and just want to know how to do it from Python. The fact that Microsoft keeps changing the name of everything makes it even harder to search for—how would you guess that googling for OLE automation, ActiveX scripting, Windows Scripting House, etc. would have anything to do with learning about COM automation? So, I\'m not sure what to recommend for getting started. I can promise that it\'s all as simple as it looks from that example above, once you do learn all the nonsense, but I don\'t know how to get past that initial hurdle. \nAnyway, not every application is automatable. And sometimes, even if it is, describing the GUI actions (what a user would click on the screen) is simpler than thinking in terms of the app\'s object model. ""Select the third paragraph"" is hard to describe in GUI terms, but ""select the whole document"" is easy—just hit control-A, or go to the Edit menu and Select All. GUI automation is much harder than COM automation, because you either have to send the app the same messages that Windows itself sends to represent your user actions (e.g., see ""Menu Notifications"") or, worse, craft mouse messages like ""go (32, 4) pixels from the top-left corner, click, mouse down 16 pixels, click again"" to say ""open the File menu, then click New"".\nFortunately, there are tools like pywinauto that wrap up both kinds of GUI automation stuff up to make it a lot simpler. And there are tools like swapy that can help you figure out what commands you want to send. If you\'re not wedded to Python, there are also tools like AutoIt and Actions that are even easier than using swapy and pywinauto, at least when you\'re getting started. Going this way, the last part of your script might look like:\nword.Activate()\nword.MenuSelect(\'File->New\')\nword.KeyStrokes(my_string)\nword.MenuSelect(\'File->Save As\')\nword.Dialogs[-1].FindTextField(\'Filename\').Select()\nword.KeyStrokes(r\'C:\\TestFiles\\TestDoc.doc\')\nword.Dialogs[-1].FindButton(\'OK\').Click()\n\nFinally, even with all of these tools, web browsers are very hard to automate, because each web page has its own menus, buttons, etc. that aren\'t Windows controls, but HTML. Unless you want to go all the way down to the level of ""move the mouse 12 pixels"", it\'s very hard to deal with these. That\'s where selenium comes in—it scripts web GUIs the same way that pywinauto scripts Windows GUIs.\n', '\nThe following script uses Automa to do exactly what you want (tested on Word 2010):\ndef find_lyrics():\n    print \'Please minimize all other open windows, then enter the song:\'\n    song = raw_input()\n    start(""Google Chrome"")\n    # Disable Google\'s autocompletion and set the language to English:\n    google_address = \'google.com/webhp?complete=0&hl=en\'\n    write(google_address, into=""Address"")\n    press(ENTER)\n    write(song + \' lyrics filetype:txt\')\n    click(""I\'m Feeling Lucky"")\n    press(CTRL + \'a\', CTRL + \'c\')\n    press(ALT + F4)\n    start(""Microsoft Word"")\n    press(CTRL + \'v\')\n    press(CTRL + \'s\')\n    click(""Desktop"")\n    write(song + \' lyrics\', into=""File name"")\n    click(""Save"")\n    press(ALT + F4)\n    print(""\\nThe lyrics have been saved in file \'%s lyrics\' ""\n          ""on your desktop."" % song)\n\nTo try it out for yourself, download Automa.zip from its Download page and unzip into, say, c:\\Program Files. You\'ll get a folder called Automa 1.1.2. Run Automa.exe in that folder. Copy the code above and paste it into Automa by right-clicking into the console window. Press Enter twice to get rid of the last ... in the window and arrive back at the prompt >>>. Close all other open windows and type\n>>> find_lyrics()\n\nThis performs the required steps. \nAutoma is a Python library: To use it as such, you have to add the line\nfrom automa.api import *\n\nto the top of your scripts and the file library.zip from Automa\'s installation directory to your environment variable PYTHONPATH. \nIf you have any other questions, just let me know :-)\n', '\nHere\'s an implementation in Python of @Matteo Italia\'s comment:\n\nYou are approaching the problem from a ""user perspective"" when you\n  should approach it from a ""programmer perspective""; you don\'t need to\n  open a browser, copy the text, open Word or whatever, you need to\n  perform the appropriate HTTP requests, parse the relevant HTML,\n  extract the text and write it to a file from inside your Python\n  script. All the tools to do this are available in Python (in\n  particular you\'ll need urllib2 and BeautifulSoup).\n\n#!/usr/bin/env python\nimport codecs\nimport json\nimport sys\nimport urllib\nimport urllib2\n\nimport bs4  # pip install beautifulsoup4\n\ndef extract_lyrics(page):\n    """"""Extract lyrics text from given lyrics.wikia.com html page.""""""\n    soup = bs4.BeautifulSoup(page)\n    result = []\n    for tag in soup.find(\'div\', \'lyricbox\'):\n        if isinstance(tag, bs4.NavigableString):\n            if not isinstance(tag, bs4.element.Comment):\n                result.append(tag)\n        elif tag.name == \'br\':\n            result.append(\'\\n\')\n    return """".join(result)\n\n# get artist, song to search\nartist = raw_input(""Enter artist:"")\nsong = raw_input(""Enter song:"")\n\n# make request\nquery = urllib.urlencode(dict(artist=artist, song=song, fmt=""realjson""))\nresponse = urllib2.urlopen(""http://lyrics.wikia.com/api.php?"" + query)\ndata = json.load(response)\n\nif data[\'lyrics\'] != \'Not found\':\n    # print short lyrics\n    print(data[\'lyrics\'])\n    # get full lyrics\n    lyrics = extract_lyrics(urllib2.urlopen(data[\'url\']))\n    # save to file\n    filename = ""[%s] [%s] lyrics.txt"" % (data[\'artist\'], data[\'song\'])\n    with codecs.open(filename, \'w\', encoding=\'utf-8\') as output_file:\n        output_file.write(lyrics)\n    print(""written \'%s\'"" % filename)\nelse:\n    sys.exit(\'not found\')\n\nExample\n$ printf ""Queen\\nWe are the Champions"" | python get-lyrics.py \n\nOutput\n\nI\'ve paid my dues\nTime after time\nI\'ve done my sentence\nBut committed no crime\n\nAnd bad mistakes\nI\'ve made a few\nI\'ve had my share of sand kicked [...]\nwritten \'[Queen] [We are the Champions] lyrics.txt\'\n\n', ""\nIf you really want to open a browser, etc, look at selenium. But that's overkill for your purposes. Selenium is used to simulate button clicks, etc for testing the appearance of websites on various browsers, etc. Mechanize is less of an overkill for this\nWhat you really want to do is understand how a browser (or any other program) works under the hood i.e. when you click on the mouse or type on the keyboard or hit Save, what does the program do behind the scenes? It is this behind-the-scenes work that you want your python code to do.\nSo, use urllib, urllib2 or requests (or heck, even scrapy) to request a web page (learn how to put together the url to a google search or the php GET request of a lyrics website). Google also has a search API that you can take advantage of, to perform a google search.\nOnce you have your results from your page request, parse it with xml, beautifulsoup, lxlml, etc and find the section of the request result that has the information you're after.\nNow that you have your lyrics, the simplest thing to do is open a text file and dump the lyrics in there and write to disk. But if you really want to do it with MS Word, then open a doc file in notepad or notepad++ and look at its structure. Now, use python to build a document with similar structure, wherein the content will be the downloaded lyrics.\nIf this method fails, you could look into pywinauto or such to automate the pasting of text into an MS Word doc and clicking on Save\nCitation: Matteo Italia, g.d.d.c from the comments on the OP\n"", '\nYou should look into a package called selenium for interacting with web browsers\n']",https://stackoverflow.com/questions/14288177/interact-with-other-programs-using-python,automation
JMeter : How to record HTTPS traffic?,"
I'm using Apache JMeter 2.3, which now supports ""attempt HTTPS spoofing"" under the Proxy Server element. 
I've tried this on several different servers, and have had no success. 
Has anyone been able to successfully record from an HTTPS source with this setting?
Or barring successfully recording, can anyone share a work-around? When available, I simply have HTTPS turned off at the server level, but this is not always feasible. Thoughts?
",34k,"
            13
        ","['\n\nStarting from JMeter 3.0 default port for the HTTP(S) Test Script Recorder is 8888\nThe easiest way to configure recording is using JMeter Templates feature. From JMeter\'s main menu select:\nFile -> Templates -> Recording -> Create\n\n\n\n\nDon\'t forget to start the recorder :\n\nIn JMeter < 4.0, Expand ""Workbench"", if >= 4.0, ignore this step\nSelect ""HTTP(S) Test Script Recorder""\nClick ""Start"" button\n\n\n\n\nYou will see a message regarding Root CA Certificate. Click OK:\n\n\n\nit is OK, it informs you JMeter has created a Root Certificate Authority that you need to import in your browser to be able to record correctly HTTPS traffic.\n\n\nTo Import this Root CA certificate in Firefox (it is located in jmeter/bin folder) for example:\n\n\n\n\n\n\n\n\nConfigure browser to use JMeter as proxy:\n\n\nIt is now Ok.\n\nYou can navigate to your application, samplers will be created under ""Recording Controller"" which is under ""Thread Group"" element\n\n', '\nWhile the JMeter proxy already has the ability to record HTTPS requests, a Chrome Extension that creates JMeter script came out recently:\nhttps://chrome.google.com/webstore/detail/blazemeter-the-load-testi/mbopgmdnpcbohhpnfglgohlbhfongabi?hl=en\nIt uses a BlazeMeter as the middleman (a commercial JMeter in the cloud service) but you can use their free service forever and still use the plugin to record a JMX script and download it locally to your own machine even if you never use any of the paid plans.\n', ""\nWhat I do is:\n\nGo to my website using my web server's IP-address (i.e. http://2.2.2.2/login.html)\nStart the recorder and run through my test case\nStop recording\nReplace all values of the IP address with the domain name (i.e. replace 2.2.2.2 with yoursite.com) from the HTTP Request Samplers\nSet the protocol to https in the HTTP Request Samplers\n\nIf you have more than a few pages, it's easiest to create an HTTP Request Defaults item, and set your domain name and protocol there.\nFYI, I'm using the latest stable build as of 2010-05-24: Jmeter 2.3.4 r785646.\n"", '\nThe newest version of Jmeter (2.4) now supports HTTPS recording.  Rejoice!\nMore details:\nhttp://wiki.apache.org/jmeter/JMeterFAQ#Can_JMeter_record_HTTPS_requests_using_the_recording_proxy.3F\n', '\nIs there any other way to record HTTPS than Bad boy and Https spoofing?\nYes--use a nightly build of JMeter, e.g. version r922204.\n', '\nHttps recording is successfully working in new version of Jmeter 2.9 as of today. I had to import proxy certificate and play around with Firefox to get this working.\nRefer this link for more information\nHttps recording using Jmeter\n', '\nYes, I have used it with ""attempt HTTPS spoofing"" on. Things are simple enough:\n\nTurn HTTPS Spoofing on (of course).\nMake sure that the browser sends Http request to Jmeter, so that Jmeter can record it and then send the encrypted request back to the server. So, the URL in the browser should start with http:// (and not with https://). The details could be found in my blog. \n\nPlease let me know if it works for you.\n', '\nI am using Webscarab to record https and ajax conversations.\nIt workd fine.  I extended the Webscarab with export function for Jmeter.\nBugzilla 48898.\n']",https://stackoverflow.com/questions/299529/jmeter-how-to-record-https-traffic,automation
Python - Control window with pywinauto while the window is minimized or hidden,"
What I'm trying to do:
I'm trying to create a script in python with pywinauto to automatically install notepad++ in the background (hidden or minimized), notepad++ is just an example since I will edit it to work with other software.
Problem:
The problem is that I want to do it while the installer is hidden or minimized, but if I move my mouse the script will stop working.
Question:
How can I execute this script and make it work, while the notepad++ installer is hidden or minimized.
This is my code so far:
import sys, os, pywinauto

pwa_app = pywinauto.application.Application()

app = pywinauto.Application().Start(r'npp.6.8.3.Installer.exe')

Wizard = app['Installer Language']

Wizard.NextButton.Click()

Wizard = app['Notepad++ v6.8.3 Setup']

Wizard.Wait('visible')

Wizard['Welcome to the Notepad++ v6.8.3 Setup'].Wait('ready')
Wizard.NextButton.Click()

Wizard['License Agreement'].Wait('ready')
Wizard['I &Agree'].Click()

Wizard['Choose Install Location'].Wait('ready')
Wizard.Button2.Click()

Wizard['Choose Components'].Wait('ready')
Wizard.Button2.Click()

Wizard['Create Shortcut on Desktop'].Wait('enabled').CheckByClick()
Wizard.Install.Click()

Wizard['Completing the Notepad++ v6.8.3 Setup'].Wait('ready', timeout=30)
Wizard['CheckBox'].Wait('enabled').Click()
Wizard.Finish.Click()
Wizard.WaitNot('visible')

",22k,"
            9
        ","['\nThe problem is here:\nWizard[\'Create Shortcut on Desktop\'].wait(\'enabled\').check_by_click()\n\ncheck_by_click() uses click_input() method that moves real mouse cursor and performs a realistic click.\nUse check() method instead.\n[EDIT] If the installer doesn\'t handle BM_SETCHECK properly the workaround may look so:\ncheckbox = Wizard[\'Create Shortcut on Desktop\'].wait(\'enabled\')\nif checkbox.get_check_state() != pywinauto.win32defines.BST_CHECKED:\n    checkbox.click()\n\nI will fix it in the next pywinauto release by creating methods check_by_click and check_by_click_input respectively.\n\n[EDIT 2]\nI tried your script with my fix and it works perfectly (and very fast) with and without mouse moves. Win7 x64, 32-bit Python 2.7, pywinauto 0.6.x, run as administrator.\nimport sys\nimport os\nfrom pywinauto import Application\n\napp = Application(backend=""win32"").start(r\'npp.6.8.3.Installer.exe\')\n\nWizard = app[\'Installer Language\']\n\nWizard.minimize()\nWizard.NextButton.click()\n\nWizard = app[\'Notepad++ v6.8.3 Setup\']\n\nWizard.wait(\'visible\')\nWizard.minimize()\n\nWizard[\'Welcome to the Notepad++ v6.8.3 Setup\'].wait(\'ready\')\nWizard.NextButton.click()\n\nWizard.minimize()\nWizard[\'License Agreement\'].wait(\'ready\')\nWizard[\'I &Agree\'].click()\n\nWizard.minimize()\nWizard[\'Choose Install Location\'].wait(\'ready\')\nWizard.Button2.click()\n\nWizard.minimize()\nWizard[\'Choose Components\'].wait(\'ready\')\nWizard.Button2.click()\n\nWizard.minimize()\ncheckbox = Wizard[\'Create Shortcut on Desktop\'].wait(\'enabled\')\nif checkbox.get_check_state() != pywinauto.win32defines.BST_CHECKED:\n    checkbox.click()\nWizard.Install.click()\n\nWizard[\'Completing the Notepad++ v6.8.3 Setup\'].wait(\'ready\', timeout=30)\nWizard.minimize()\nWizard[\'CheckBox\'].wait(\'enabled\').click()\nWizard.Finish.click()\nWizard.wait_not(\'visible\')\n\n']",https://stackoverflow.com/questions/32846550/python-control-window-with-pywinauto-while-the-window-is-minimized-or-hidden,automation
Schedule automatic daily upload with FileZilla [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.


Closed 7 years ago.


The community reviewed whether to reopen this question 1 year ago and left it closed:

Original close reason(s) were not resolved






                        Improve this question
                    



I would like to use FileZilla to automatically upload PDFs to my GoDaddy hosted site daily, replacing the previous day's sheets. Is there any way to do this? I read online that batch files might work, could someone post a sample version of a batch file that would do the trick?
",137k,"
            19
        ","['\nFileZilla does not have any command line arguments (nor any other way) that allow an automatic transfer.\nSome references:\n\nFileZilla Client command-line arguments\nhttps://trac.filezilla-project.org/ticket/2317\nHow do I send a file with FileZilla from the command line?\n\n\nThough you can use any other client that allows automation.\nYou have not specified, what protocol you are using. FTP or SFTP? You will definitely be able to use WinSCP, as it supports all protocols that the free version of FileZilla does (and more).\nCombine WinSCP scripting capabilities with Windows Scheduler:\n\nAutomate file transfers to FTP server or SFTP server;\nSchedule file transfers to FTP/SFTP server\n\nA typical WinSCP script for upload (with SFTP) looks like:\nopen sftp://user:password@example.com/ -hostkey=""ssh-rsa 2048 xxxxxxxxxxx...=""\nput c:\\mypdfs\\*.pdf /home/user/\nclose\n\nWith FTP, just replace the sftp:// with the ftp:// and remove the -hostkey=""..."" switch.\n\nSimilarly for download: How to schedule an automatic FTP download on Windows?\n\nWinSCP can even generate a script from an imported FileZilla session.\nFor details, see the guide to FileZilla automation.\n(I\'m the author of WinSCP)\n\nAnother option, if you are using SFTP, is the psftp.exe client from PuTTY suite.\n']",https://stackoverflow.com/questions/24945709/schedule-automatic-daily-upload-with-filezilla,automation
Executing a PHP script with a CRON Job [closed],"









                        It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and  cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened,  visit the help center.
                        
                    


Closed 9 years ago.



I would like to run a PHP script every day at midnight. After research on how to do this, it appears that the best way to achieve this is to use a CRON job.
If my php script was located at http://example.com/scripts/scriptExample.php, can somebody be able to show the most simple example of what this CRON command would look like?
I have looked through numerous posts but I cannot find a simple enough example for me to learn and build upon.
",94k,"
            24
        ","['\nCrontab needs the full path on your server.\n0 0 * * * php /var/www/vhosts/domain.com/httpdocs/scripts/example.php\n\nThis will execute every day at midnight.\n', '\nSo something like this:\n00 * * * * /usr/local/bin/php /home/john/myscript.php\n\nThe 00 * * * * means hourly \n/usr/local/bin/php - where php main engine is in\n/home/john/myscript.php - the script to run (physical path)\nYou can use also @hourly special key:\n@hourly /usr/local/bin/php /home/john/myscript.php\n\n', '\nIf You have a sudo access to your linux server :-\nThen do the following\nsudo crontab -e\n\nThis will open the cron tab for you on your server.\nNext thing is you have to do a cron entry for the file which you want to execute\n00 00 * * * /usr/local/bin/php ""path of the php file which you want to execute""\n\n00 00 * * * this will run your cron at midnight daily, means at 0hrs and 0mins\n', '\nAre you using a company to host your website? \nAs you should have a icon  in your c panel called cron jobs from there you can tell it what script to execute and when.\n']",https://stackoverflow.com/questions/16144350/executing-a-php-script-with-a-cron-job,automation
"Creating, opening and printing a word file from C++","
I have three related questions. 
I want to create a word file with a name from C++. I want to be able to sent the printing command to this file, so that the file is being printed without the user having to open the document and do it manually and I want to be able to open the document. Opening the document should just open word which then opens the file.
",47k,"
            16
        ","['\nYou can use Office Automation for this task. You can find answers to frequently asked questions about Office Automation with C++ at http://support.microsoft.com/kb/196776 and http://support.microsoft.com/kb/238972 .\nKeep in mind that to do Office Automation with C++, you need to understand how to use COM.\nHere are some examples of how to perform various tasks in word usign C++:\n\nhttp://support.microsoft.com/kb/220911/en-us\nhttp://support.microsoft.com/kb/238393/en-us\nhttp://support.microsoft.com/kb/238611/en-us\n\nMost of these samples show how to do it using MFC, but the concepts of using COM to manipulate Word are the same, even if you use ATL or COM directly.\n', '\nAs posted as an answer to a similar question, I advise you to look at this page where the author explains what solution he took to generate Word documents on a server, without MsWord being available, without automation or thirdparty libraries.\n', '\nWhen you have the file and just want to print it, then look at this entry at Raymond Chen\'s blog. You can use the verb ""print"" for printing.\nSee the shellexecute msdn entry for details.\n', '\nYou can use automation to open MS Word (in background or foreground) and then send the needed commands.\nA good starting place is the knowledge base article Office Automation Using Visual C++\nSome C source code is available in How To Use Visual C++ to Access DocumentProperties with Automation (the title says C++, but it is plain C)\n', '\nI have no experience from integrating with Microsoft Office, but I guess there are some APIs around that you can use for this.\nHowever, if what you want to accomplish is a rudimentary way of printing formatted output and exporting it to a file that can be handled in Word, you might want to look into the RTF format. The format is quite simple to learn, and is supported by the RtfTextBox (or is it RichTextBox?), which also has some printing capabilities. The rtf format is the same format as is used by Windows Wordpad (write.exe).\nThis also has the benefit of not depending on MS Office in order to work.\n', '\nMy solution to this is to use the following command:\nstart /min winword <filename> /q /n /f /mFilePrint /mFileExit\n\nThis allows the user to specify a printer, no. of copies, etc.\nReplace <filename> with the filename. It must be enclosed in double-quotation marks if it contains spaces. (e.g. file.rtf, ""A File.docx"")\nIt can be placed within a system call as in:\nsystem(""start /min winword <filename> /q /n /f /mFilePrint /mFileExit"");\n\nHere is a C++ header file with functions that handle this so you don\'t have to remember all of the switches if you use it frequently:\n/*winword.h\n *Includes functions to print Word files more easily\n */\n\n#ifndef WINWORD_H_\n#define WINWORD_H_\n\n#include <string.h>\n#include <stdlib.h>\n\n//Opens Word minimized, shows the user a dialog box to allow them to\n//select the printer, number of copies, etc., and then closes Word\nvoid wordprint(char* filename){\n   char* command = new char[64 + strlen(filename)];\n   strcpy(command, ""start /min winword \\"""");\n   strcat(command, filename);\n   strcat(command, ""\\"" /q /n /f /mFilePrint /mFileExit"");\n   system(command);\n   delete command;\n}\n\n//Opens the document in Word\nvoid wordopen(char* filename){\n   char* command = new char[64 + strlen(filename)];\n   strcpy(command, ""start /max winword \\"""");\n   strcat(command, filename);\n   strcat(command, ""\\"" /q /n"");\n   system(command);\n   delete command;\n}\n\n//Opens a copy of the document in Word so the user can save a copy\n//without seeing or modifying the original\nvoid wordduplicate(char* filename){\n   char* command = new char[64 + strlen(filename)];\n   strcpy(command, ""start /max winword \\"""");\n   strcat(command, filename);\n   strcat(command, ""\\"" /q /n /f"");\n   system(command);\n   delete command;\n}\n\n#endif\n\n']",https://stackoverflow.com/questions/145573/creating-opening-and-printing-a-word-file-from-c,automation
Programmatically add trusted sites to Internet Explorer,"
I'm doing an IE automation project using WatiN. 
When a file to be downloaded is clicked, I get the following in the Internet Explorer Information bar:

To help protect your security,
  Internet Explorer has blocked this
  site from downloading files to you
  computer.

In order to download the report, I can manually add the site to Internet Explorer's list of trusted sites, but I would prefer to check programmatically in .NET to see if the site is trusted and add it to the list if it is not. 
FYI, I'm currently using IE7.
",36k,"
            14
        ","['\nHave a look at this\nBasically it looks as if all you have to do is create registry key in \nHKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ZoneMap\\Domains\\DOMAINNAME\n\nthen a REG_DWORD value named ""http"" with value==2\n', '\nHere\'s the implementation that I came up with for writing the registry keys in .NET.\nThanks for setting me in the right direction, Ben.\nusing System;\nusing System.Collections.Generic;\nusing Microsoft.Win32;\n\n\nnamespace ReportManagement\n{\n    class ReportDownloader\n    {\n        [STAThread]\n        static void Main(string[] args)\n        {\n\n            const string domainsKeyLocation = @""Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ZoneMap\\Domains"";\n            const string domain = @""newsite.com"";\n            const int trustedSiteZone = 0x2;\n\n            var subdomains = new Dictionary<string, string>\n                                 {\n                                     {""www"", ""https""},\n                                     {""www"", ""http""},\n                                     {""blog"", ""https""},\n                                     {""blog"", ""http""}\n                                 };\n\n            RegistryKey currentUserKey = Registry.CurrentUser;\n\n            currentUserKey.GetOrCreateSubKey(domainsKeyLocation, domain, false);\n\n            foreach (var subdomain in subdomains)\n            {\n                CreateSubdomainKeyAndValue(currentUserKey, domainsKeyLocation, domain, subdomain, trustedSiteZone);\n            }\n\n            //automation code\n        }\n\n        private static void CreateSubdomainKeyAndValue(RegistryKey currentUserKey, string domainsKeyLocation, \n            string domain, KeyValuePair<string, string> subdomain, int zone)\n        {\n            RegistryKey subdomainRegistryKey = currentUserKey.GetOrCreateSubKey(\n                string.Format(@""{0}\\{1}"", domainsKeyLocation, domain), \n                subdomain.Key, true);\n\n            object objSubDomainValue = subdomainRegistryKey.GetValue(subdomain.Value);\n\n            if (objSubDomainValue == null || Convert.ToInt32(objSubDomainValue) != zone)\n            {\n                subdomainRegistryKey.SetValue(subdomain.Value, zone, RegistryValueKind.DWord);\n            }\n        }\n    }\n\n    public static class RegistryKeyExtensionMethods\n    {\n        public static RegistryKey GetOrCreateSubKey(this RegistryKey registryKey, string parentKeyLocation, \n            string key, bool writable)\n        {\n            string keyLocation = string.Format(@""{0}\\{1}"", parentKeyLocation, key);\n\n            RegistryKey foundRegistryKey = registryKey.OpenSubKey(keyLocation, writable);\n\n            return foundRegistryKey ?? registryKey.CreateSubKey(parentKeyLocation, key);\n        }\n\n        public static RegistryKey CreateSubKey(this RegistryKey registryKey, string parentKeyLocation, string key)\n        {\n            RegistryKey parentKey = registryKey.OpenSubKey(parentKeyLocation, true); //must be writable == true\n            if (parentKey == null) { throw new NullReferenceException(string.Format(""Missing parent key: {0}"", parentKeyLocation)); }\n\n            RegistryKey createdKey = parentKey.CreateSubKey(key);\n            if (createdKey == null) { throw new Exception(string.Format(""Key not created: {0}"", key)); }\n\n            return createdKey;\n        }\n    }\n}\n\n', '\nGlad I came across your postings. The only thing I can add to the excellent contributions already is that a different registry key is used whenever the URI contains an IP address i.e. the address isn\'t a fully qualified domain name.\nIn this instance you have to use an alternative approach:\nImagine I wish to add an IP address to the trusted sites: say 10.0.1.13 and I don\'t care what protocol.\n\nUnder HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ZoneMap\\Ranges, I create a key e.g. ""Range1"" and the inside that create the following values:\nA DWORD with name ""*"" and value 0x2  (for all protocols(*) and trusted site(2))\n    A string with name "":Range"" with value ""10.0.1.13""\n\n', '\nUsing powershell it is quite easy.\n#Setting IExplorer settings\nWrite-Verbose ""Now configuring IE""\n#Add http://website.com as a trusted Site/Domain\n#Navigate to the domains folder in the registry\nset-location ""HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings""\nset-location ZoneMap\\Domains\n\n#Create a new folder with the website name\nnew-item website/ -Force\nset-location website/\nnew-itemproperty . -Name * -Value 2 -Type DWORD -Force\nnew-itemproperty . -Name http -Value 2 -Type DWORD -Force\nnew-itemproperty . -Name https -Value 2 -Type DWORD -Force\n\n', '\nIn addition to adding the domain to the Trusted Sites list, you may also need to change the setting ""Automatically prompt for file downloads"" for the Trusted Sites zone. To do so programatically, you modify the key/value:\n\nHKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet\n  Settings\\Zones\\2@2200\n\nChange the value from 3 (Disable) to 0 (Enable). Here\'s some C# code to do that:\npublic void DisableForTrustedSitesZone()\n{\n    const string ZonesLocation = @""Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\Zones"";\n    const int TrustedSiteZone = 2;\n\n    const string AutoPromptForFileDownloadsValueName = @""2200"";\n    const int AutoPromptForFileDownloadsValueEnable = 0x00;     // Bypass security bar prompt\n\n    using (RegistryKey currentUserKey = Registry.CurrentUser)\n    {\n        RegistryKey trustedSiteZoneKey = currentUserKey.OpenSubKey(string.Format(@""{0}\\{1:d}"", ZonesLocation, TrustedSiteZone), true);\n        trustedSiteZoneKey.SetValue(AutoPromptForFileDownloadsValueName, AutoPromptForFileDownloadsValueEnable, RegistryValueKind.DWord);\n    }\n}\n\n', '\nHere is the implementation of adding trusted sites programmatically to IE - based on Even Mien\'s code. It supports domain name and IP address as well. The limitation is no specific protocol could be defined, instead it simply uses ""*"" for all protocols.\n//  Source : http://support.microsoft.com/kb/182569\nstatic class IeTrustedSite\n{\n    const string DOMAINS_KEY = @""Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ZoneMap\\Domains"";\n    const string RANGES_KEY = @""Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ZoneMap\\Ranges"";\n\n    const int TRUSTED_SITE_CODE = 0x2;\n    const string ALL_PROTOCOL = ""*"";\n    const string RANGE_ADDRESS = "":Range"";\n\n    public static void AddSite(string address)\n    {\n        string[] segmentList = address.Split(new string[] {"".""}, StringSplitOptions.None);\n        if (segmentList.Length == 4)\n            AddIpAddress(segmentList);\n        else\n            AddDomainName(segmentList);\n    }\n\n    static void AddIpAddress(string[] segmentList)\n    {\n        string ipAddress = segmentList[0] + ""."" + segmentList[1] + ""."" + segmentList[2] + ""."" + segmentList[3];\n        RegistryKey rangeKey = GetRangeKey(ipAddress);\n\n        rangeKey.SetValue(ALL_PROTOCOL, TRUSTED_SITE_CODE, RegistryValueKind.DWord);\n        rangeKey.SetValue(RANGE_ADDRESS, ipAddress, RegistryValueKind.String);\n    }\n\n    static RegistryKey GetRangeKey(string ipAddress)\n    {\n        RegistryKey currentUserKey = Registry.CurrentUser;\n        for (int i = 1; i < int.MaxValue; i++)\n        {\n            RegistryKey rangeKey = currentUserKey.GetOrCreateSubKey(RANGES_KEY, ""Range"" + i.ToString());\n\n            object addressValue = rangeKey.GetValue(RANGE_ADDRESS);\n            if (addressValue == null)\n            {\n                return rangeKey;\n            }\n            else\n            {\n                if (Convert.ToString(addressValue) == ipAddress)\n                    return rangeKey;\n            }\n        }\n        throw new Exception(""No range slot can be used."");\n    }\n\n    static void AddDomainName(string[] segmentList)\n    {\n        if (segmentList.Length == 2)\n        {\n            AddTwoSegmentDomainName(segmentList);\n        }\n        else if (segmentList.Length == 3)\n        {\n            AddThreeSegmentDomainName(segmentList);\n        }\n        else\n        {\n            throw new Exception(""Un-supported server address."");\n        }\n    }\n\n    static void AddTwoSegmentDomainName(string[] segmentList)\n    {\n        RegistryKey currentUserKey = Registry.CurrentUser;\n\n        string domain = segmentList[0] + ""."" + segmentList[1];\n        RegistryKey trustedSiteKey = currentUserKey.GetOrCreateSubKey(DOMAINS_KEY, domain);\n\n        SetDomainNameValue(trustedSiteKey);\n    }\n\n    static void AddThreeSegmentDomainName(string[] segmentList)\n    {\n        RegistryKey currentUserKey = Registry.CurrentUser;\n\n        string domain = segmentList[1] + ""."" + segmentList[2];\n        currentUserKey.GetOrCreateSubKey(DOMAINS_KEY, domain);\n\n        string serviceName = segmentList[0];\n        RegistryKey trustedSiteKey = currentUserKey.GetOrCreateSubKey(DOMAINS_KEY + @""\\"" + domain, serviceName);\n\n        SetDomainNameValue(trustedSiteKey);\n    }\n\n    static void SetDomainNameValue(RegistryKey subDomainRegistryKey)\n    {\n        object securityValue = subDomainRegistryKey.GetValue(ALL_PROTOCOL);\n        if (securityValue == null || Convert.ToInt32(securityValue) != TRUSTED_SITE_CODE)\n        {\n            subDomainRegistryKey.SetValue(ALL_PROTOCOL, TRUSTED_SITE_CODE, RegistryValueKind.DWord);\n        }\n    }\n}\n\nstatic class RegistryKeyExtension\n{\n    public static RegistryKey GetOrCreateSubKey(this RegistryKey registryKey, string parentString, string subString)\n    {\n        RegistryKey subKey = registryKey.OpenSubKey(parentString + @""\\"" + subString, true);\n        if (subKey == null)\n            subKey = registryKey.CreateSubKey(parentString, subString);\n\n        return subKey;\n    }\n\n    public static RegistryKey CreateSubKey(this RegistryKey registryKey, string parentString, string subString)\n    {\n        RegistryKey parentKey = registryKey.OpenSubKey(parentString, true);\n        if (parentKey == null)\n            throw new Exception(""BUG : parent key "" + parentString + "" is not exist.""); \n\n        return parentKey.CreateSubKey(subString);\n    }\n}\n\n', ""\n\nIf a website could add itself to the trusted sites, now that would be bad. \n\nI don't quite agree- as long as the browser asks the user for permission, the ability of a site to add itself to trusted sites can greatly simplify the user experience, where the user trusts the domain and wants correct page display. \nThe alternative is the user must manually go into internet options to add the domain, which is, for my users, not viable. \ni'm looking for a php or javascript method for the site to add itself, either through some IE api, or through the registry as you've so helpfully explained above!\nhave found these possible solutions so far:\n\nphp via shell\nothers i'm not allowed to list here because i don't have enough points\n\n""]",https://stackoverflow.com/questions/972345/programmatically-add-trusted-sites-to-internet-explorer,automation
How to delete mysql row after time passes?,"
I have no idea where to start with this one:
I have a database that stores postID and Date.
What I want to do is have my website auto delete all rows where Date is less than today. This script can't have any user input at all.  No button clicks, nothing. The script must run every day at midnight.
I've been looking all over the place for something that does this and I've found absolutely nothing.
",31k,"
            14
        ","['\nYou can use PHP script and use cron job on your cpanel. \nExample:\ncronjobcommand.php\n<?php \n include \'your_db_connection\';\n mysql_query(""DELETE FROM your_table_name WHERE Date < NOW()"");\n?>\n\nI have attached a screenshot below for your more reference.\n\n', ""\nFor those out there who are on a shared hosting, like 1and1's, and can't use cron, here are 2 alternatives :\n\nmysql events enable you to place a time trigger on mysql, which will execute when you'll want, without having to be fired by any kind of user input\nif you cannot create mysql events because you're on 1and1 :(, an alternative is to use webcron\n\nYou just need to tell webcron the url of the php script you'd like to be run, and they'll trigger it for you at the intervals you want\n"", ""\nWhy using cronjobs everyday?? Why not filter data on output. For example in your select check if post date equals today with adding a simple where:\nSELECT * FROM `posts`\nWHERE (DATE(`post_date`) = DATE(NOW()));\n\nThis way you're not required to do your database managements/cronjobs on any special time and it will be used just for database managements. Afterwards you can delete unnecessary data at any time using by mysql command like:\nDELETE FROM `posts` WHERE (\n    DATE(`post_date`) < DATE(NOW())\n)\n\n"", ""\nMost hosts provide a cron(8) service that can execute commands at specific times. You use the crontab(1) program to manage the crontab(5) file the describes when to run which commands.\nThere's a lot of functionality available to you, but if you write a program (shell script, php script, C program, whatever) that runs the appropriate MySQL commands, you can call the program via cron(8) in an entirely hands-off fashion.\nRun crontab -e to edit your current crontab(5) file. If none exists, hopefully you'll get one with a helpful header. If not, copy this:\n# m h  dom mon dow   command\n\nThe columns indicate the minute, hour, day of month, month, and day of week to execute commands. All the numbers in the columns are essentially ANDed together to decide when to run commands.\nThus, midnight every night would look like this:\n0 0 * * * /path/to/executable\n\nIt's remarkably flexible, so put some time into the documentation, and you'll find many uses for it.\n"", '\nYou should set cron job (scheduled tack.) for it. \nA cron job is an automated program setup for Linux and Unix operating systems. It allows the user to execute several commands or functions at a specific time and date.\nyou have cron Job in your cpanel setup. first you need to make a php script with your logic for delete record after each date. take date from server and write script for delete.\nthen go to cron tab in your cpanel and do settings for time interval to run cron and give path of your php script file.\n', ""\nMySQL doesn't have a task scheduler. So you have to use the task scheduler of your Operating System (CRON under Linux), or to lunch a basic task checker sub-script during the script of the main page (on another page that is supposed to display the changing data).\n""]",https://stackoverflow.com/questions/9865393/how-to-delete-mysql-row-after-time-passes,automation
How to make the Java.awt.Robot type unicode characters? (Is it possible?),"
We have a user provided string that may contain unicode characters, and we want the robot to type that string.
How do you convert a string into keyCodes that the robot will use?
How do you do it so it is also java version independant (1.3 -> 1.6)?
What we have working for ""ascii"" chars is
//char c = nextChar();
//char c = 'a'; // this works, and so does 'A'
char c = 'á'; // this doesn't, and neither does 'Ă'
Robot robot = new Robot();
KeyStroke key = KeyStroke.getKeyStroke(""pressed "" + Character.toUpperCase(c) );
if( null != key ) {
  // should only have to worry about case with standard characters
  if (Character.isUpperCase(c))
  {
    robot.keyPress(KeyEvent.VK_SHIFT);
  }

  robot.keyPress(key.getKeyCode());
  robot.keyRelease(key.getKeyCode());

  if (Character.isUpperCase(c))
  {
    robot.keyRelease(KeyEvent.VK_SHIFT);
  }
}

",16k,"
            12
        ","[""\nBased on javamonkey79's code I've created the following snippet which should work for all Unicode values...\npublic static void pressUnicode(Robot r, int key_code)\n{\n    r.keyPress(KeyEvent.VK_ALT);\n\n    for(int i = 3; i >= 0; --i)\n    {\n        // extracts a single decade of the key-code and adds\n        // an offset to get the required VK_NUMPAD key-code\n        int numpad_kc = key_code / (int) (Math.pow(10, i)) % 10 + KeyEvent.VK_NUMPAD0;\n\n        r.keyPress(numpad_kc);\n        r.keyRelease(numpad_kc);\n    }\n\n    r.keyRelease(KeyEvent.VK_ALT);\n}\n\nThis automatically goes through each decade of the unicode key-code, maps it to the corresponding VK_NUMPAD equivalent and presses/releases the keys accordingly.\n"", ""\nThe KeyEvent Class does not have direct mappings for many unicode classes in JRE 1.5. If you are running this on a Windows box what you may have to do is write a custom handler that does something like this:\nRobot robot = new Robot();\nchar curChar = 'Ã';\n\n// -- isUnicode( char ) should be pretty easy to figure out\nif ( isUnicode( curChar ) ) {\n   // -- this is an example, exact key combinations will vary\n   robot.keyPress( KeyEvent.VK_ALT );\n\n   robot.keyPress( KeyEvent.VK_NUMBER_SIGN );\n   robot.keyRelease( KeyEvent.VK_NUMBER_SIGN );\n\n   // -- have to apply some logic to know what sequence\n   robot.keyPress( KeyEvent.VK_0 );\n   robot.keyRelease( KeyEvent.VK_0 );\n   robot.keyPress( KeyEvent.VK_1 );\n   robot.keyRelease( KeyEvent.VK_1 );\n   robot.keyPress( KeyEvent.VK_9 );\n   robot.keyRelease( KeyEvent.VK_9 );\n   robot.keyPress( KeyEvent.VK_5 );\n   robot.keyRelease( KeyEvent.VK_5 );\n\n   robot.keyRelease( KeyEvent.VK_ALT );\n}\n\ne.g. Figure out what they key combinations are, and then map them to some sort of Object (maybe a HashMap?) for later lookup and execution.\nHope this helps :)\n"", '\ni think this is a bit late but... \nRobot robot = new Robot();\n\n   robot.keyPress( KeyEvent.VK_DEAD_ACUTE);\n\n   robot.keyPress( KeyEvent.VK_A );\n   robot.keyRelease( KeyEvent.VK_A );\n\n   robot.keyRelease( KeyEvent.VK_DEAD_ACUTE );\n\nthat just type an ""á""\n', '\nThe best way that i find when solve simulare problem\nimport java.awt.AWTException;\nimport java.awt.Robot;\n\npublic class MyRobot {\n\n    public static void typeString(String s)\n        {\n            try {\n            Robot robik = new Robot();\n            byte[] bytes = s.getBytes();\n            for (byte b : bytes)\n            {\n                int code = b;\n                // keycode only handles [A-Z] (which is ASCII decimal [65-90])\n                if (code > 96 && code < 123) code = code - 32;\n                robik.delay(40);\n                robik.keyPress(code);\n                robik.keyRelease(code);\n            }\n        } catch (AWTException e){\n\n    }\n  }\n\n}\n\nhttp://www.devdaily.com/java/java-robot-class-example-mouse-keystroke\\\n']",https://stackoverflow.com/questions/397113/how-to-make-the-java-awt-robot-type-unicode-characters-is-it-possible,automation
Is there a Python equivalent to Java's AWT Robot class? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 Questions asking us to recommend or find a tool, library or favorite off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, describe the problem and what has been done so far to solve it.


Closed 9 years ago.







                        Improve this question
                    



Does anyone know of a Python class similar to Java Robot? 
Specifically I would like to perform a screen grab in Ubuntu, and eventually track mouse clicks and keyboard presses (although that's a slightly different question).
",12k,"
            12
        ","['\nIf you have GTK, then you can use the gtk.gdk.Display class to do most of the work. It controls the keyboard/mouse pointer grabs a set of gtk.gdk.Screen objects.\n', '\nCheck out GNU LDTP:\n\nGNU/Linux Desktop Testing Project (GNU\n  LDTP) is aimed at producing high\n  quality test automation framework\n  [...]\n\nEspecially Writing LDTP test scripts in Python scripting language\n', '\nAs far as the screen grab, see this answer.  That worked for me.  Other answers to the same question might be of interest as well.\n', '\nCheck out the RobotFramework. I do not know if it will do the same things as JavaRobot, or if it will do more. But it is easy and very flexible to use.\n']",https://stackoverflow.com/questions/860013/is-there-a-python-equivalent-to-javas-awt-robot-class,automation
Why should I ever use CSS selectors as opposed to XPath for automated testing?,"
Please help me understand why using CSS selectors are even an option for automated testing.  I've been using the tool Ghost Inspector some in my workplace for creating lots of automated tests for our stuff.  This tool gives you the option of using CSS selectors intead of XPath. Why?
XPath is SO much more durable than CSS.  The CSS on any given UI is subject to change almost weekly on some projects/features.  This make the tests extremely brittle and prone to being broken regularly.
Is it because most new test writers don't want to learn about anything XPath and wish to stick to the basics? CSS selectors look prettier than XPath syntax? Please convince me. thanks.
",5k,"
            8
        ","['\nJeffC\'s answer here does plenty to sum up the pros and cons of each locator strategy. But I\'ll address your points specifically.\nFirst off, there is no need for anyone to convince you that selectors are better, because from a purely functional standpoint, they simply aren\'t (and I\'m saying this as someone with a gold css-selectors tag badge and almost 1000 answers to questions with that tag, so you know I\'m not biased). If you\'re more comfortable with XPath, use it — in terms of features and what you can do, XPath is vastly superior, there really is no contest there. And, as you correctly state, performance is no longer an issue (if it ever was).\nSelectors are there for quick and simple use cases and for users coming from HTML and CSS codebases, such as web developers, who want to get started with automated tests without having to learn another DSL. If you\'re responsible for the CSS of your own site you can also easily copy selectors from your stylesheet into your tests depending on what exactly you\'re testing.\nIf on the other hand you\'re coming from an XML/XSLT/XPath background, wonderful, you get to keep using the XPath you know and love1!\n\nYes, Xpath is way more durable than CSS because it can invoke specific content contains functionality.\n\nHaving a content contains feature doesn\'t make XPath more ""durable"" — it makes it more versatile. If you rely solely on an element\'s content and that content can potentially change or move around, your XPath becomes no less brittle than a selector that relies solely on an element\'s attributes or its position in the DOM tree.\nYou can do any of a number of things to make your XPath or selector more or less brittle, but that\'s an indicator of how versatile the DSL is, not how brittle it inherently is.\n\n1 Depending on what version of XPath you\'re used to.\n', ""\nOne of the most common conversation in the Selenium Community is which Locator Strategy is better among the two - Css or XPath with respect to performance. Supporters of CSS say that it is more readable and faster while those in favor of XPath says it's ability to transverse the HTML DOM (while CSS cannot). With such a divide based on different perspective it is hard to determine the best performing approach for you and your tests as a beginner. Here are some excerts from the industry experts :\n\nDave Haeffner who maintains Elemental Selenium carried out a test on a page with two HTML data tables, one table is written without helpful attributes (ID and Class), and the other with them. \n\n\nResults with Finding Elements By ID and Class :\nBrowser                 | CSS           | XPath\n----------------------------------------------------\nInternet Explorer 8     | 23 seconds    | 22 seconds\nChrome 31               | 17 seconds    | 16 seconds\nFirefox 26              | 22 seconds    | 22 seconds\nOpera 12                | 17 seconds    | 20 seconds\nSafari 5                | 18 seconds    | 18 seconds\n\nFinding Elements By Traversing :\nBrowser                 | CSS           | XPath\n----------------------------------------------------\nInternet Explorer 8     | not supported | 29 seconds\nChrome 31               | 24 seconds    | 26 seconds\nFirefox 26              | 27 seconds    | 27 seconds\nOpera 12                | 25 seconds    | 25 seconds\nSafari 5                | 23 seconds    | 22 seconds\n\nThe following were the takeaways :\n\nFor starters there is no dramatic difference in performance between XPath and CSS.\nTraversing the DOM in older browsers like IE8 does not work with CSS but is fine with XPath. And XPath can walk up the DOM (e.g. from child to parent), whereas CSS can only traverse down the DOM (e.g. from parent to child). However not being able to traverse the DOM with CSS in older browsers isn't necessarily a bad thing as it is more of an indicator that your page has poor design and could benefit from some helpful markup.\nAn argument in favor of CSS is that they are more readable, brief, and concise while it is a subjective call.\n\nBen Burton mentions you should use CSS because that's how applications are built. This makes the tests easier to write, talk about, and have others help maintain.\nAdam Goucher says to adopt a more hybrid approach -- focusing first on IDs, then CSS, and leveraging XPath only when you need it (e.g. walking up the DOM) and that XPath will always be more powerful for advanced locators.\n\n\nConclusion\nSo it appears to be a tough call to make. Especially now that we are aware with the knowledge that the choice is not as reliant on performance as it once was. The choice is not as permanent as choosing a programming language, and if you are using helpful abstraction (e.g. Page Objects) then leveraging a hybrid approach is simple to implement.\nTrivia\n\nCss Vs. X Path\nCss Vs. X Path, Under a Microscope\nCss Vs. X Path, Under a Microscope (Part 2)\n\n""]",https://stackoverflow.com/questions/51936193/why-should-i-ever-use-css-selectors-as-opposed-to-xpath-for-automated-testing,automation
python-pptx - How to replace keyword across multiple runs?,"
I have two PPTs (File1.pptx and File2.pptx) in which I have the below 2 lines
XX NOV 2021, Time: xx:xx – xx:xx hrs (90mins)
FY21/22 / FY22/23

I wish to replace like below
a) NOV 2021 as NOV 2022.
b) FY21/22 / FY22/23 as FY21/22 or FY22/23.
But the problem is my replacement works in File1.pptx but it doesn't work in File2.pptx.
When I printed the run text, I was able to see that they are represented differently in two slides.
def replace_text(replacements:dict,shapes:list):
    for shape in shapes:
        for match, replacement in replacements.items():
            if shape.has_text_frame:
                if (shape.text.find(match)) != -1:
                    text_frame = shape.text_frame
                    for paragraph in text_frame.paragraphs:
                        for run in paragraph.runs:
                            cur_text = run.text
                            print(cur_text)
                            print(""---"")
                            new_text = cur_text.replace(str(match), str(replacement))
                            run.text = new_text

In File1.pptx, the cur_text looks like below (for 1st keyword). So, my replace works (as it contains the keyword that I am looking for)

But in File2.pptx, the cur_text looks like below (for 1st keyword). So, replace doesn't work (because the cur_text doesn't match with my search term)

The same issue happens for 2nd keyword as well which is FY21/22 / FY22/23.
The problem is the split keyword could be in previous or next run from current run (with no pattern). So, we should be able to compare a search term with previous run term (along with current term as well). Then a match can be found (like Nov 2021) and be replaced.
This issue happens for only 10% of the search terms (and not for all of my search terms) but scary to live with this issue because if the % increases, we may have to do a lot of manual work. How do we avoid this and code correctly?
How do we get/extract/find/identify the word that we are looking for across multiple runs (when they are indeed present) like CTRL+F and replace it with desired keyword?
Any help please?
UPDATE - Incorrect replacements based on matching
Before replacement

After replacement

My replacement keywords can be found below
replacements = { 'How are you?': ""I'm fine!"",
                'FY21/22':'FY22/23',
                'FY_2021':'FY21/22',
                'FY20/21':'FY21/22',
                'GB2021':'GB2022',
                'GB2020':'GB2022',
                'SEP-2022':'SEP-2023',
                'SEP-2021':'SEP-2022',
                'OCT-2021':'OCT-2022',
                'OCT-2020':'OCT-2021',
                'OCT 2021':'OCT 2022',
                'NOV 2021':'NOV 2022',
                'FY2122':'FY22/23',
                'FY2021':'FY21/22',
                'FY1920':'FY20/21',
                'FY_2122':'FY22/23',
                'FY21/22 / FY22/23':'FY21/22 or FY22/23',
                'F21Y22':'FY22/23',
                'your FY20 POS FCST':'your FY22/23 POS FCST',
                'your FY21/22 POS FCST':'your FY22/23 POS FCST',
                'Q2/FY22/23':'Q2-FY22/23',
                'JAN-22':'JAN-23',
                'solution for FY21/22':'solution for FY22/23',
                'achievement in FY20/21':'achievement in FY21/22',
                'FY19/20':'FY20/21'}

",525,"
            4
        ","['\nAs one can find in python-pptx\'s documentation at https://python-pptx.readthedocs.io/en/latest/api/text.html\n\na text frame is made up of paragraphs and\na paragraph is made up of runs and specifies a font configuration that is used as the default for it\'s runs.\nruns specify part of the paragraph\'s text with a certain font configuration - possibly different from the default font configuration in the paragraph\n\nAll three have a field called text:\n\nThe text frame\'s text contains all the text from all it\'s paragraphs concatenated together with the appropriate line-feeds in between the paragraphs.\nThe paragraphs\'s text contains all the texts from all of it\'s runs concatenated to a long string with a vertical tab character (\\v) put wherever there was a so-called soft-break in any of the run\'s text (a soft break is like a line-feed but without terminating the paragraph).\nThe run\'s text contains text that is to be rendered with a certain font configuration (font family, font size, italic/bold/underlined, color etc. pp). It is the lowest level of the font configuration for any text.\n\nNow if you specify a line of text in a text-frame in a PowerPoint presentation, this text-frame will very likely only have one paragraph and that paragraph will have just one run.\nLet\'s say that line says: Hi there! How are you? What is your name? and is all normal (neither italic nor bold) and in size 10.\nNow if you go ahead in PowerPoint and make the questions How are you? What is your name? stand out by making them italic, you will end up with 2 runs in our paragraph:\n\nHello there!  with the default font configuration from the paragraph\nHow are you? What is you name? with the font configuration specifying the additional italic attribute.\n\nNow imagine, we want the How are you? stand out even more by making it bold and italic. We end up with 3 runs:\n\nHello there!  with the default font configuration from the paragraph.\nHow are you? with the font configuration specifying the BOLD and ITALIC attribute\n What is your name? with the font configuration specifying the ITALIC attribute.\n\nOne step further, making the are in How are you? bigger. We get 5 runs:\n\nHello there!  with the default font configuration from the paragraph.\nHow  with the font configuration specifying the BOLD and ITALIC attribute\nare with the font configuration specifying the BOLD and ITALIC attribute and font size 16\n you? with the font configuration specifying the BOLD and ITALIC attribute\n What is your name? with the font configuration specifying the ITALIC attribute.\n\nSo if you try to replace the How are you? with I\'m fine! with the code from your question, you won\'t succeed, because the text How are you? is actually distributed across 3 runs.\nYou can go one level higher and look at the paragraph\'s text, that still says Hello there! How are you? What is your name? since it is the concatenation of all its run\'s texts.\nBut if you go ahead and do the replacement of the paragraph\'s text, it will erase all runs and create one new run with the text Hello there! I\'m fine! What is your name? all the while deleting all the formatting that we put on the What is your name?.\nTherefore, changing text in a paragraph without affecting formatting of the other text in the paragraph is pretty involved. And even if the text you are looking for has all the same formatting, that is no guarantee for it to be within one run. Because if you - in our example above - make the are smaller again, the 5 runs will very likely remain, the runs 2 to 4 just having the same font configuration now.\nHere is the code to produce a test presentation with a text box containing the exact paragraph runs as given in my example above:\nfrom pptx import Presentation\nfrom pptx.chart.data import CategoryChartData\nfrom pptx.enum.chart import XL_CHART_TYPE,XL_LABEL_POSITION\nfrom pptx.util import Inches, Pt\nfrom pptx.dml.color import RGBColor\nfrom pptx.enum.dml import MSO_THEME_COLOR\n\n# create presentation with 1 slide ------\nprs = Presentation()\nslide = prs.slides.add_slide(prs.slide_layouts[5])\ntextbox_shape = slide.shapes.add_textbox(Pt(200),Pt(200),Pt(30),Pt(240))\ntext_frame = textbox_shape.text_frame\np = text_frame.paragraphs[0]\nfont = p.font\nfont.name = \'Arial\'\nfont.size = Pt(10)\nfont.bold = False\nfont.italic = False\nfont.color.rgb = RGBColor(0,0,0)\n\nrun = p.add_run()\nrun.text = \'Hello there! \'\n\nrun = p.add_run()\nrun.text = \'How \'\nfont = run.font\nfont.italic = True\nfont.bold = True\n\nrun = p.add_run()\nrun.text = \'are\'\nfont = run.font\nfont.italic = True\nfont.bold = True\nfont.size = Pt(16)\n\nrun = p.add_run()\nrun.text = \' you?\'\nfont = run.font\nfont.italic = True\nfont.bold = True\n\nrun = p.add_run()\nrun.text = \' What is your name?\'\nrun.font.italic = True\n\nprs.save(\'text-01.pptx\')\n\nAnd this is what it looks like, if you open it in PowerPoint:\n\nNow if you install the python code from my GitHub repository at https://github.com/fschaeck/python-pptx-text-replacer by running the command\npython -m pip install python-pptx-text-replacer\n\nand after successful installation run the command\npython-pptx-text-replacer -m ""How are you?"" -r ""I\'m fine!"" -i text-01.pptx -o text-02.pptx\n\nthe resulting presentation text-02.pptx will look like this:\n\nAs you can see, it mapped the replacement string exactly onto the existing font-configurations, thus if your match and it\'s replacement have the same length, the replacement string will retain the exact format of the match.\nBut - as an important side-note - if the text-frame has auto-size or fit-frame switched on, even all that work won\'t save you from screwing up the formatting, if the text after the replacement needs more or less space!\nIf you got issues with this code, please use the possibly improved version from GitHub first. If your problem remains, use the GitHub issue tracker to report it. The discussion of this question and answer is already getting out of hand. ;-)\n']",https://stackoverflow.com/questions/73219378/python-pptx-how-to-replace-keyword-across-multiple-runs,automation
Gluing (Imposition) PDF documents,"
I have several A4 PDF documents which I would like (two into one) ""glue"" together into A3 format PDF document. So I will get from 2PDFs A4 a single one sided PDF A3.
I have found the excellent utility PDFToolkit and some others but none of them can be used to ""glue"" side by side two documents. 
",24k,"
            20
        ","['\nI just came across a nice tool on superuser.com called PDFjam that can do all of the above in a single command:\npdfjam --nup 2x1 file1.pdf file2.pdf --outfile DONESKI.pdf\n\nIt has other standard features like page size plus a nice syntax for more sophisticated collations of pages (the tricky page re-ordering necessary for true booklet-style page imposition).\nIt\'s built on top of TeX which is, whatever it is. Installing is a breeze on Ubuntu: you can just apt-get install pdfjam. On Mac OS, I recommend getting BasicTeX (google ""mactex basictex""; SO thinks I\'m a spammer and won\'t let me post the link).\nThis is a lot easier and more maintanable than installing both pdftk and Multivalent (on both Mac OS for dev and Ubuntu for deploy), which wasn\'t going so well for me anyway...!\n', '\nFound the following (free and open-source) tool for doing Imposition called Impose (thanks danio for the tip). This solved my problem perfectly. \nEDIT:\nHere is how it\'s done:\nUse PDF Toolkit to joint two PDF files into one (two A4)\npdftk File1.pdf File2.pdf cat output OutputFile.pdf\n\nCreate from this a single page (one A3):\njava -cp Multivalent.jar tool.pdf.Impose -dim 2x1 -verbose -paper-size ""42.2x29.9cm"" -layout ""1,2"" OutputFile.pdf\n\n', ""\nI would like to advertise my pdftools\nIt's written in Python so should run on any platform. It's a wrapper to Latex (the pdfpages packages)  but can do lot of things with a single command line: merge pdf files, nup them (multiple input pages per output page) and number the pages of the output file (you specify the location and the format of the number)\nIt still needs some work but I think it's quite stable to be usable right now :)\n"", '\nThis puts two landscape letter pages onto a single portrait letter sheet, to be ""bound"" (i.e., folded) along the top.\npdftops $1 - | \npsbook | \npstops -w11in -h8.5in \'4:1@.65(.5in,0in)+0@.65(.5in,5.5in),2U@.65(8in,5.5in)+3@.65U(8in,11in)\' | \nps2pdf - $(basename $1 .pdf).psbook.pdf\n\nBy the way, I do this often, so I\'ll probably submit more ""answers"" to this question just to keep track of successful pstops pagespecs. Let me know if this is an inappropriate use of SO.\n', ""\nA nice, powerful, open-source imposition tool is included\nin the PoDoFo package:\n  http://podofo.sourceforge.net/\nIt works for me.  Some imposition plans can be found at:\n  http://www.av8n.com/computer/prepress/\nPoDoFo can do lots of other stuff, not just imposition.\nAnother useful imposition tool is Bookbinder (on the\nquantumelephant site).  It has a GUI that appeals to non-experts.\nIt is not as flexible or powerful as PoDoFo, but it can do\nimposition.\npdftk is more-or-less essential to have, but it will not\ndo imposition.\npdfjam is useless to me, because there are a wide range of\nvalid pdf files that it cannot handle.\nI've never been able to get multivalent to work, either.\n"", '\nWhat you want to do is imposition.  There are commercial tools to impose PDFs such as ARTS crackerjack and Quite imposing but they are pretty expensive (US$500), require a copy of acrobat professional and are overkill for imposing 2 A4 pages to an A3 sheet.\n', ""\nOn the Postscript side, a tool named pstops is able to rearrange pages of a Postscript file in any way you could imagine. I've not heard of such a tool for PDF. But pdf2ps and ps2pdf exist. So a not-so-ideal solution may be a combination of pdf2ps, pstops and ps2pdf.\n"", '\nI would combine the two A4 pages into one 2-page PDF using pdftk.  Then Print to PDF using something like PrimoPDF, and tell it to print to A3 format, two pages per side.\nI just tested this printing some slides from PowerPoint.  It worked great.  I selected A3 as my paper size in PowerPoint, and then chose to print 2 pages per side.  Printed to Primo and voila, I have two A4 slides per A3.\n', '\nYou can put multiple input pages on one output page using BookletImposer.\nAnd you can change page orders and combine multiple pdf files using PDF Mod.\nWith these two tools, you can do almost everything you want with pdf files (except editing their content).\n', '\nI had a similar problem. I tried Impose but it was giving me an\nException in thread ""main"" java.lang.NoClassDefFoundError: tool/pdf/Impose\nCaused by: java.lang.ClassNotFoundException: tool.pdf.Impose\n(...)\nCould not find the main class: tool.pdf.Impose.  Program will exit.\n\nI then tried PDF Snake which isn\'t free or open source, but has a completely unrestricted 30-day trial version. It worked perfectly, after tweaking the parameters to achieve what I wanted. It\'s a great tool. I would definitely buy it if it wasn\'t so expensive! Anyway, I thought I\'d leave my 2 cents in case anyone had the same problem I had with Impose.\n', '\nlook at this\nhttp://sourceforge.net/projects/proposition/\nIt needs laTex to run,\nbut when it does, works really fine\nRegards\n']",https://stackoverflow.com/questions/465271/gluing-imposition-pdf-documents,automation
How to get Sikuli working in headless mode,"
If we have a headless test server running sikuli (both ubuntu and windows configurations needed), how to get it work without a physical monitor and preferably for as many screen resolutions as possible.
",24k,"
            19
        ","['\nI successfully got sikuli running in headless mode (no physical monitor connected)\nUbuntu: check Xvfb.\nWindows: install display driver on the machine (to be headless) from virtualbox guest additions display drivers and use TightVNC to remotely set resolution from another machine.\nDetailed steps for windows 7\nAssume that:\n\nMachine A: to be headless machine, windows 7, with vnc server ready (e.g. TightVNC server installed and waiting for connections).\nMachine B: will be used to remotely setup the virtual display driver on machine A.\n\nsteps:\n\nDownload virtualbox guest additions iso file on Machine A from here (for latest version check latest version here and download VBoxGuestAdditions_x.y.z.iso)\n\nExtract iso file (possibly with winrar) into a directory (let us call it folder D)\n\nusing command prompt cd to D folder\nDriver extraction \n-To extract the 32-bit drivers to ""C:\\Drivers"", do the following:\n\nVBoxWindowsAdditions-x86 /extract /D=C:\\Drivers\n\n-For the 64-bit drivers:\n\nVBoxWindowsAdditions-amd64 /extract /D=C:\\Drivers\n\n\nGoto device manager\n\nadd hardware\n\n\n\n\n\n\n\n\n\nRestart and connect with VNC viewer, now you should be able to change screen resolution \n\n\nother valuable info on launchpad.\n', '\nI got SikuliX working in a true headless mode in GCE with a Windows 2016 client system. It takes some duct tape and other Rube Goldberg contraptions to work, but it can be done.\nThe issue is that, for GCE (and probably AWS and other cloud environment Windows clients), you don\'t have a virtual video adapter and display, so, unless there\'s an open RDP connection to the client, it doesn\'t have a screen, and SikuliX/OpenCV will get a 1024x768 black desktop, and fail. \nSo, the question is, how to create an RDP connection without having an actual screen anywhere. I did this using Xvfb (X Windows virtual frame buffer). This does require a second VM, though. Xvfb runs on Linux. The other piece of the puzzle is xfreerdp 2.0. The 2.x version is required for compatibility with recent versions of Windows. 1.x is included with some Linux distros; 2.x may need to be built from sources, depending on what flavor Linux you\'re using. I\'m using CentOS, which did require me to build my own.\nThe commands to establish the headless RDP session, once the pieces are in place, look something like this:\n/usr/bin/Xvfb :0 -screen 0 1920x1080x24 &\nexport DISPLAY=:0.0\n/usr/local/bin/xfreerdp /size:1920x1080 /u:[WindowsUser] /p:""[WindowsPassword]"" /v:[WindowsTarget]\n\nIn our environment we automated this as part of the build job kicked off by Jenkins. For this to work under the Jenkins slave, it was also necessary to run the Jenkins slave as a user process, rather than a service... this can be accomplished by enabling auto admin login and setting the slave launch script as a run (on logon) command.\n', '\nFor those looking to automate on ec2 windows machines, this worked for me: http://www.allianceglobalservices.com/blog/executing-automation-suite-on-disconnectedlocked-machines\nIn summary, I used RDC to connect, put the following code in a batch file on remote desktop, double clicked it, and sikulix  started working remotely (kicking me out of RDC at the same time).  Note that ec2 windows machines default to 1024x768 when tscon takes over which may be too small so TightVnc can be used to increase the resolution to 1280x1024 before running. \ntscon.exe 0 /dest:console\ntscon.exe 1 /dest:console\ntscon.exe 2 /dest:console\ntscon.exe 3 /dest:console\nSTART /DC:\\Sikulix /WAIT /B C:\\Sikulix\\runsikulix.cmd -d 3 -r C:\\test.sikuli -f C:\\Sikulix\\log.txt -d C:\\Sikulix\\userlog.txt\n\n', '\nI just figure out a way to resolve similar issue.\nMy env:\nlocal: windows pc\nremote (for running sikulix + app I would like to test): windows ec2 instance\nMy way:\n1.create a .bat file, its contents:\nping 127.0.0.1 -n 15 > nul\n\nfor /f ""skip=1 tokens=3"" %%s in (\'query user %USERNAME%\') do (\n  %windir%\\System32\\tscon.exe %%s /dest:console\n)\n\ncd ""\\path\\to\\sikulix""\njava -jar sikulixide-2.0.5.jar -r /path/to/sikulix -c > logfile.log\n\n\nprepare your app\nrun the bat (right click > run as administrator)\nping will give your 10s, so that you can bring your app back to front\nyou will be disconnnected from rdp connection\n\nExplanation:\n\nping is like ""sleep""\nfor loop: kick out current user & keep session alive\n\n']",https://stackoverflow.com/questions/26032706/how-to-get-sikuli-working-in-headless-mode,automation
"VBA Internet Explorer Automation - How to Select ""Open"" When Downloading a File","
This is my first question ever here on stackoverflow!
I've been searching for a solution to this problem for a while and haven't found any help. I may just be using the wrong keywords in my searches, but so far I've had no luck. Here's the question:
In VBA, how can I select the ""Open"" option from the file download dialog in Internet Explorer?
Just for extra clarification, I'm talking about the yellow-orange bar that pops up across the bottom of the screen in IE9 when a file is downloaded.
I'm doing some VBA automation to download hundreds of PDFs from the web using Internet Explorer, but there is an intermediate step where a .fdf file has to be opened before I get to the actual PDF. So I first need to select the ""Open"" option so that I can move on to the next step of the automation. Like I said earlier, I've done a lot of searching and had no luck so far.
I've tried using SendKeys in hopes that hitting Enter would work, and that was a last ditch effort that didn't work.
Thanks in advance for the help!
",48k,"
            14
        ","[""\nI have covered this extensively here.\nTopic: VBA/VB.Net/VB6–Click Open/Save/Cancel Button on IE Download window – PART I\nLink: http://www.siddharthrout.com/2011/10/23/vbavb-netvb6click-opensavecancel-button-on-ie-download-window/\nand\n\nEDIT (IMP) If you are using IE 9 Do not forget to read PART 2 as it includes and covers the window structure of IE 9 download window\n\nTopic: VBA/VB.Net/VB6–Click Open/Save/Cancel Button on IE Download window – PART II\nLink: http://www.siddharthrout.com/2012/02/02/vbavb-netvb6click-opensavecancel-button-on-ie-download-window-part-ii/\nThe above links discuss on how to use use the API's to achieve what you want.\nFrom the 1st link...\n\nLike you and me, we both have names, similarly windows have “handles” (hWnd), Class etc. Once you know what that hWnd is, it is easier to interact with that window.\nFindwindow API finds the hWnd of a particular window by using the class name and the caption of the window (“File Download”) in this case. The “Open“, “Save” and “Cancel” buttons are windows in itself but they are child windows of the main window which is “File Download“. That means each one of those will also have a hWnd :) To find the child windows, we don’t use FindWindow but use FindWindowEx. All the three buttons “Open“, “Save” and “Cancel” have the same class which is “ Button”.\n\n"", '\nSimilar post: link\n    Option Explicit\n    Dim ie As InternetExplorer\n    Dim h As LongPtr\n    Private Declare PtrSafe Function FindWindowEx Lib ""user32"" Alias ""FindWindowExA"" (ByVal hWnd1 As LongPtr, ByVal hWnd2 As LongPtr, ByVal lpsz1 As String, ByVal lpsz2 As String) As LongPtr\n\nSub Download()\n    Dim o As IUIAutomation\n    Dim e As IUIAutomationElement\n    Set o = New CUIAutomation\n    h = ie.Hwnd\n    h = FindWindowEx(h, 0, ""Frame Notification Bar"", vbNullString)\n    If h = 0 Then Exit Sub\n\n    Set e = o.ElementFromHandle(ByVal h)\n    Dim iCnd As IUIAutomationCondition\n    Set iCnd = o.CreatePropertyCondition(UIA_NamePropertyId, ""Open"")\n\n    Dim Button As IUIAutomationElement\n    Set Button = e.FindFirst(TreeScope_Subtree, iCnd)\n    Dim InvokePattern As IUIAutomationInvokePattern\n    Set InvokePattern = Button.GetCurrentPattern(UIA_InvokePatternId)\n    InvokePattern.Invoke\nEnd Sub \n\n', '\nI sent the shortcut keys to the application. Here they are for IE11. Sorry I could not test in IE9. If you hold down Alt, it may show you the other key to the combo as IE11 does. \nNote: the code will not run as you expect if IE is not the active window on your machine so it won\'t work while in debug mode.\n\nShortcut key:Alt+O\nVBA: Application.SendKeys ""%{O}""\n\n']",https://stackoverflow.com/questions/10400795/vba-internet-explorer-automation-how-to-select-open-when-downloading-a-file,automation
Accessing Excel Custom Document Properties programmatically,"
I'm trying to add custom properties to a workbook I have created programmatically.  I have a method in place for getting and setting properties, but the problem is the workbook is returning null for the CustomDocumentProperties property.  I cannot figure out how to initialize this property so that I can add and retrieve properties from the workbook.  Microsoft.Office.Core.DocumentProperties is an interface, so I cant go and do the following
if(workbook.CustomDocumentProperties == null)
    workbook.CustomDocumentProperties = new DocumentProperties;

Here is the code I have to get and set the properties:
     private object GetDocumentProperty(string propertyName, MsoDocProperties type)
    {
        object returnVal = null;

        Microsoft.Office.Core.DocumentProperties properties;
        properties = (Microsoft.Office.Core.DocumentProperties)workBk.CustomDocumentProperties;

        foreach (Microsoft.Office.Core.DocumentProperty property in properties)
        {
            if (property.Name == propertyName && property.Type == type)
            {
                returnVal = property.Value;
            }
            DisposeComObject(property);
        }

        DisposeComObject(properties);

        return returnVal;
    }

    protected void SetDocumentProperty(string propertyName, string propertyValue)
    {
        DocumentProperties properties;
        properties = workBk.CustomDocumentProperties as DocumentProperties;

        bool propertyExists = false;
        foreach (DocumentProperty prop in properties)
        {
            if (prop.Name == propertyName)
            {
                prop.Value = propertyValue;
                propertyExists = true;
            }
            DisposeComObject(prop);

            if(propertyExists) break;
        }

        if (!propertyExists)
        {
            properties.Add(propertyName, false, MsoDocProperties.msoPropertyTypeString, propertyValue, Type.Missing);
        }

        DisposeComObject(propertyExists);

    }

The line
    properties = workBk.CustomDocumentProperties as DocumentProperties;
always set properties to null.
This is using Microsoft.Office.Core v12.0.0.0 and Microsoft.Office.Interop.Excell v12.0.0.0 (Office 2007)
",21k,"
            13
        ","['\nIf you are targetting .NET 4.0, you can use the dynamic key word for late binding\n Document doc = GetActiveDocument();\n if ( doc != null )\n {\n     dynamic properties = doc.CustomDocumentProperties;\n     foreach (dynamic p in properties)\n     {\n         Console.WriteLine( p.Name + "" "" + p.Value);\n     }\n }\n\n', '\nI looked at my own code and can see that I access the properties using late binding. I can\'t remember why, but I\'ll post some code in case it helps.\nobject properties = workBk.GetType().InvokeMember(""CustomDocumentProperties"", BindingFlags.Default | BindingFlags.GetProperty, null, workBk, null);\n\nobject property = properties.GetType().InvokeMember(""Item"", BindingFlags.Default | BindingFlags.GetProperty, null, properties, new object[] { propertyIndex });\n\nobject propertyValue = property.GetType().InvokeMember(""Value"", BindingFlags.Default | BindingFlags.GetProperty, null, propertyWrapper.Object, null);\n\nEDIT: ah, now I remember why. :-)\nEDIT 2: Jimbojones\' answer - to use the dynamic keyword - is a better solution (if you value ease-of-use over the performance overhead of using dynamic).\n', '\nI found the solution here.\nHere is the code I ended up with:\n    public void SetDocumentProperty(string propertyName, string propertyValue)\n    {\n        object oDocCustomProps = workBk.CustomDocumentProperties;\n        Type typeDocCustomProps = oDocCustomProps.GetType();\n\n        object[] oArgs = {propertyName,false,\n                 MsoDocProperties.msoPropertyTypeString,\n                 propertyValue};\n\n        typeDocCustomProps.InvokeMember(""Add"", BindingFlags.Default |\n                                   BindingFlags.InvokeMethod, null,\n                                   oDocCustomProps, oArgs);\n\n    }\n\n    private object GetDocumentProperty(string propertyName, MsoDocProperties type)\n    {\n        object returnVal = null;\n\n        object oDocCustomProps = workBk.CustomDocumentProperties;\n        Type typeDocCustomProps = oDocCustomProps.GetType();\n\n\n        object returned = typeDocCustomProps.InvokeMember(""Item"", \n                                    BindingFlags.Default |\n                                   BindingFlags.GetProperty, null,\n                                   oDocCustomProps, new object[] { propertyName });\n\n        Type typeDocAuthorProp = returned.GetType();\n        returnVal = typeDocAuthorProp.InvokeMember(""Value"",\n                                   BindingFlags.Default |\n                                   BindingFlags.GetProperty,\n                                   null, returned,\n                                   new object[] { }).ToString();\n\n        return returnVal;\n    }\n\nSome exception handling is necessary to hand if the property doesnt exist when retrieved\n', '\nLate answer to this question, but I worked out a simpler method for adding custom DocumentProperties that might be of use to someone in the future.\nMy problem was that calling the Add() method with the System type supplied by System.String.GetType() triggered a COMException: Type mismatch. Referring to the link in the previous answers it\'s clear that this method expects an Office-specific type, so the code that ended up working for me was:\nvar custProps = (Office.DocumentProperties)this.CustomDocumentProperties;\ncustProps.Add( ""AProperty"", false, MsoDocProperties.msoPropertyTypeString, ""AStringProperty"" );\n\nBecause it\'s a CustomDocumentProperty Office will add the custom property without difficulty, but if you need to check for existence or validate the value when the CustomDocumentProperty might not exist you\'ll have to catch a System.ArgumentException.\nEDIT\nAs pointed out in Oliver Bock\'s comment, this is an Office 2007 and up only solution, as far as I know.\n']",https://stackoverflow.com/questions/1137763/accessing-excel-custom-document-properties-programmatically,automation
How to access Microsoft Word existing instance using late binding,"
i am developing some code in c# where i will be interacting with Microsoft Word. I want to be able to have the option of re-using an existing instance or as an alternative creating a new instance.
Keeping in mind i want to do all of this using LATE BINDING... it is safe to say i have figured out how to get things working when creating a new instance.. i just call Activator.CreateInstance etc..
The problem i am having is how do i reuse an existing instance, for example, Word is already open and i want to use that instance.
Is there an Activator.UseExistingInstance? or something similar??
Thanks!
",11k,"
            5
        ","['\nYou\'re looking for Marshal.GetActiveObject.\nobject word;\ntry\n{\n    word = System.Runtime.InteropServices.Marshal.GetActiveObject(""Word.Application"");\n}\ncatch (COMException)\n{\n    Type type = Type.GetTypeFromProgID(""Word.Application"");\n    word = System.Activator.CreateInstance(type);\n}\n\n', '\nYou might want to have a look at the AccessibleObjectFromWindow api function defined in Oleacc.dll. Andrew Whitechapel has some articles on how to use it. Based on his articles I wrote an answer to a very similar question (about Excel, not Word), which you can find here:\n\nHow to use use late binding to get Excel instance?\n\nThere you will find an example how to connect to an already running Excel instance and then automating this instance using late binding. \nUpdate: \nHere is a short sample adapted to Word:\nusing System;\nusing System.Reflection;\nusing System.Runtime.InteropServices;\nusing System.Text;\n\nnamespace WordLateBindingSample\n{\n    [ComImport, InterfaceType(ComInterfaceType.InterfaceIsIUnknown), Guid(""00020400-0000-0000-C000-000000000046"")]\n    public interface IDispatch\n    {\n    }\n\n    class Program\n    {\n        [DllImport(""user32.dll"", SetLastError = true)]\n        static extern IntPtr FindWindow(string lpClassName, string lpWindowName);\n\n        [DllImport(""Oleacc.dll"")]\n        static extern int AccessibleObjectFromWindow(int hwnd, uint dwObjectID, byte[] riid, out IDispatch ptr);\n\n        public delegate bool EnumChildCallback(int hwnd, ref int lParam);\n\n        [DllImport(""User32.dll"")]\n        public static extern bool EnumChildWindows(int hWndParent, EnumChildCallback lpEnumFunc, ref int lParam);\n\n        [DllImport(""User32.dll"")]\n        public static extern int GetClassName(int hWnd, StringBuilder lpClassName, int nMaxCount);\n\n        public static bool EnumChildProc(int hwndChild, ref int lParam)\n        {\n            StringBuilder buf = new StringBuilder(128);\n            GetClassName(hwndChild, buf, 128);\n            if (buf.ToString() == ""_WwG"")\n            {\n                lParam = hwndChild;\n                return false;\n            }\n            return true;\n        }\n\n        static void Main(string[] args)\n        {\n            // Use the window class name (""OpusApp"") to retrieve a handle to Word\'s main window.\n            // Alternatively you can get the window handle via the process id:\n            // int hwnd = (int)Process.GetProcessById(wordPid).MainWindowHandle;\n            //\n            int hwnd = (int)FindWindow(""OpusApp"", null);\n\n            if (hwnd != 0)\n            {\n                int hwndChild = 0;\n\n                // Search the accessible child window (it has class name ""_WwG"") \n                // as described in http://msdn.microsoft.com/en-us/library/dd317978%28VS.85%29.aspx\n                //\n                EnumChildCallback cb = new EnumChildCallback(EnumChildProc);\n                EnumChildWindows(hwnd, cb, ref hwndChild);\n\n                if (hwndChild != 0)\n                {\n                    // We call AccessibleObjectFromWindow, passing the constant OBJID_NATIVEOM (defined in winuser.h) \n                    // and IID_IDispatch - we want an IDispatch pointer into the native object model.\n                    //\n                    const uint OBJID_NATIVEOM = 0xFFFFFFF0;\n                    Guid IID_IDispatch = new Guid(""{00020400-0000-0000-C000-000000000046}"");\n                    IDispatch ptr;\n\n                    int hr = AccessibleObjectFromWindow(hwndChild, OBJID_NATIVEOM, IID_IDispatch.ToByteArray(), out ptr);\n\n                    if (hr >= 0)\n                    {\n                        object wordApp = ptr.GetType().InvokeMember(""Application"", BindingFlags.GetProperty, null, ptr, null);\n\n                        object version = wordApp.GetType().InvokeMember(""Version"", BindingFlags.GetField | BindingFlags.InvokeMethod | BindingFlags.GetProperty, null, wordApp, null);\n                        Console.WriteLine(string.Format(""Word version is: {0}"", version));\n                    }\n                }\n            }\n        }\n    }\n}\n\n']",https://stackoverflow.com/questions/2203968/how-to-access-microsoft-word-existing-instance-using-late-binding,automation
Selenium can't open a second page,"
I am using Selenium to open different pages of a site. Have tried multiple times but the browser does not open a second webpage after the initial GET call. Have tried on both Chrome and Safari. Here is my code:
driver = webdriver.Chrome()
driver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-2"")
driver.set_page_load_timeout(30)
driver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-3"")

Here is the error I get for the second call:

The info from Network logs is Error 504, but I have verified that it works perfectly when done on another window of the browser, without automation
",3k,"
            1
        ","['\noptions.add_experimental_option(\n    ""excludeSwitches"", [\'enable-automation\'])\n\noptions.add_argument(\n    ""user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"")\noptions.add_argument(""--remote-debugging-port=9222"")\n\ndriver = webdriver.Chrome(options=options)\ndriver.get(\n    ""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-2"")\ndriver.set_page_load_timeout(30)\ndriver.get(\n    ""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-3"")\n\nthe website is detecting automation use above code :)\nYou can also do this in single line\nJust add below argument:\noptions.add_argument(\'--disable-blink-features=AutomationControlled\')\n\ndisabling enable-automation , or disabling automation controller disables webdriver.navigator which few websites uses to detect automation scripts\n', '\nA bit of more information about your usecase would have helped to construct a more canonical answer. However I was able to access the Page 2 of justdial.com/Chennai/Hr-Consultancy-Services with a minimized code block as follows:\n\nCode Block:\nfrom selenium import webdriver\n\noptions = webdriver.ChromeOptions() \noptions.add_argument(""start-maximized"")\ndriver = webdriver.Chrome(options=options, executable_path=r\'C:\\WebDrivers\\chromedriver.exe\')\ndriver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-2"")\n\n\nBrowser Snapshot:\n\n\n\nBut while sending multiple get() one after another:\ndriver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-2"")\ndriver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-3"")\n\nIt seems ChromeDriver initiated Chrome Browser gets detected and the following error is shown:\nAn error occurred while processing your request.\nReference #97.e5732c31.1612205693.6fd2708\n\n\nSolution\nTo avoid the detection you can add the following option:\n--disable-blink-features=AutomationControlled\n\n\nExample\nfrom selenium import webdriver\n\noptions = webdriver.ChromeOptions() \noptions.add_argument(""start-maximized"")\noptions.add_argument(\'--disable-blink-features=AutomationControlled\')\ndriver = webdriver.Chrome(options=options, executable_path=r\'C:\\WebDrivers\\chromedriver.exe\')\ndriver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-2"")\ndriver.get(""https://www.justdial.com/Chennai/Hr-Consultancy-Services/nct-10258625/page-3"")\n\n']",https://stackoverflow.com/questions/65994908/selenium-cant-open-a-second-page,automation
Puppeteer wait for all images to load then take screenshot,"
I am using Puppeteer to try to take a screenshot of a website after all images have loaded but can't get it to work.
Here is the code I've got so far, I am using https://www.digg.com as the example website:
const puppeteer = require('puppeteer');

(async () => {
    const browser = await puppeteer.launch();
    const page = await browser.newPage();
    await page.goto('https://www.digg.com/');

    await page.setViewport({width: 1640, height: 800});

    await page.evaluate(() => {
        return Promise.resolve(window.scrollTo(0,document.body.scrollHeight));
    });

    await page.waitFor(1000);

    await page.evaluate(() => {
        var images = document.querySelectorAll('img');

        function preLoad() {

            var promises = [];

            function loadImage(img) {
                return new Promise(function(resolve,reject) {
                    if (img.complete) {
                        resolve(img)
                    }
                    img.onload = function() {
                        resolve(img);
                    };
                    img.onerror = function(e) {
                        resolve(img);
                    };
                })
            }

            for (var i = 0; i < images.length; i++)
            {
                promises.push(loadImage(images[i]));
            }

            return Promise.all(promises);
        }

        return preLoad();
    });

    await page.screenshot({path: 'digg.png', fullPage: true});

    browser.close();
})();

",56k,"
            58
        ","['\nThere is a built-in option for that:\nawait page.goto(\'https://www.digg.com/\', {""waitUntil"" : ""networkidle0""});\n\n\nnetworkidle0 - consider navigation to be finished when there are no more than 0 network connections for at least 500 ms\n\n\nnetworkidle2 - consider navigation to be finished when there are no more than 2 network connections for at least 500 ms.\n\nOf course it won\'t work if you\'re working with endless-scrolling-single-page-applications like Twitter.\nPuppeteer GitHub issue #1552 provides explanation for the motivation behind networkidle2.\n', '\nAnother option, actually evaluate to get callback when all images were loaded\nThis option will also work with setContent that doesn\'t support the wait networkidle0 option\nawait page.evaluate(async () => {\n  const selectors = Array.from(document.querySelectorAll(""img""));\n  await Promise.all(selectors.map(img => {\n    if (img.complete) return;\n    return new Promise((resolve, reject) => {\n      img.addEventListener(\'load\', resolve);\n      img.addEventListener(\'error\', reject);\n    });\n  }));\n})\n\n', ""\nWait for Lazy Loading Images\nYou may want to consider scrolling down first using a method such as Element.scrollIntoView() to account for lazy loading images:\nawait page.goto('https://www.digg.com/', {\n  waitUntil: 'networkidle0', // Wait for all non-lazy loaded images to load\n});\n\nawait page.evaluate(async () => {\n  // Scroll down to bottom of page to activate lazy loading images\n  document.body.scrollIntoView(false);\n\n  // Wait for all remaining lazy loading images to load\n  await Promise.all(Array.from(document.getElementsByTagName('img'), image => {\n    if (image.complete) {\n      return;\n    }\n\n    return new Promise((resolve, reject) => {\n      image.addEventListener('load', resolve);\n      image.addEventListener('error', reject);\n    });\n  }));\n});\n\n"", ""\nI'm facing the exact same issue.\nI have a feeling the solution will involve using:\nawait page.setRequestInterceptionEnabled(true);\n\npage.on('request', interceptedRequest => {\n    //some code here that adds this request to ...\n    //a list and checks whether all list items have ...\n    //been successfully completed!\n});\n\nhttps://github.com/GoogleChrome/puppeteer/blob/master/docs/api.md#pagesetrequestinterceptionenabledvalue\n"", ""\nI found a solution which is applicable to multiple sites using the page.setViewPort(...) method as given below:\nconst puppeteer = require('puppeteer');\n\nasync(() => {\n    const browser = await puppeteer.launch({\n        headless: true, // Set to false while development\n        defaultViewport: null,\n        args: [\n            '--no-sandbox',\n            '--start-maximized', // Start in maximized state\n        ],\n    });\n\n    const page = await = browser.newPage();\n    await page.goto('https://www.digg.com/', {\n        waitUntil: 'networkidle0', timeout: 0\n    });\n\n    // Get scroll width and height of the rendered page and set viewport\n    const bodyWidth = await page.evaluate(() => document.body.scrollWidth);\n    const bodyHeight = await page.evaluate(() => document.body.scrollHeight);\n    await page.setViewport({ width: bodyWidth, height: bodyHeight });\n\n    await page.waitFor(1000);\n    await page.screenshot({path: 'digg-example.png' });\n})();\n\n""]",https://stackoverflow.com/questions/46160929/puppeteer-wait-for-all-images-to-load-then-take-screenshot,automation
Android – multiple custom versions of the same app,"
Whats the best way to deploy several customized versions of a Android application? 
Currently I have a script to exchange the resource folder for getting a customized version of my app. It works great, but all custom versions still have the same package name in the AndroidManifest.xml. Therefore it is not possible to install two customized versions of the app at the same time.
This is one solution for this problem, but that has to be done by hand
Can you think of a more easy solution, or how this could be built into a skript?
(btw: it is not for a porn/spam/whatever app, not even a paid one)
",28k,"
            47
        ","['\nPerhaps the built-in Android ""library"" concept was not fully baked at the time of the original post, but it may be the preferred method as of 2011.  Follow these steps for an ant build:\nStarting from a working app (let\'s call it directory ""myOrigApp"", package com.foo.myapp), just add this line to ""default.properties"" to make it a library:\nandroid.library=true\n\nNow create a new app in a sibling directory in any way you prefer (let\'s call it directory ""sibling"", package com.foo.myVariant).  Using Intellij Idea, for example, create a project \'from scratch\' with directory \'sibling\' and it will create all the files/directories you would normally need.\nIn that new, sibling directory edit ""default.properties"" to add the dependency:\nandroid.library.reference.1=../myOrigApp\n\nCopy over the Manifest from the original dir:\ncd sibling\ncp ../myOrigApp/AndroidManifest.xml  ../myOrigApp/local.properties ../myOrigApp/build.properties  .\n\nEdit that copied Manifest file to change its package name to your new variant, ""com.foo.myVarient""; that\'s the only change.\nIf you just run the ant build scripts, you may be done. (I had to just set up signing keys.)\nIf you want to set up an IDE like Idea to have the library project as a dependent of the variant project, follow these steps to add a library project to the variant project (assumes you already have a project set up for both):\n\nOpen the original project, bring up Project Settings, select your Facet and check ""Is Library Project"" and save.\nOpen the variant project, bring up Project Settings, select Modules\nAdd a module\nSelect “Import existing module”\nBrowse to the Original directory (myOrigApp) and select its .iml file (IntelliJ project source file)\nClick ""Finish."" (The library project is added as a module within the variant project.)\nIn the modules list click over the Variant project to select it.\nOn the right hand side select the ""Dependencies"" tab.\nClick ""Add…""\nChoose ""Module dependency…"" (A list should appear that includes the name of the module/library you previously added to the project--perhaps the only entry in the list). \nSelect the library project you added and press OK. (It will be added to the list of dependencies of your project.)\nPress OK to finish configuring the project. (You should see 2 modules, with the library\'s resources and classes available and recognized in the Variant project.)\n\n', '\nWhat I did for something similar to this is to just use an antlib task and then go through all java and xml files to replace my old package string to the new package string. It didn\'t matter if the files were not in the correct src paths according to the package. Just doing a regex replace for all the files was enough for me to get this working...\nFor example to replace it in all your java files under the src directory:\n <replaceregexp flags=""g"" byline=""false"">\n    <regexp pattern=""old.package.string"" /> \n    <substitution expression=""new.package.string"" />\n    <fileset dir=""src"" includes=""**/*.java"" /> \n </replaceregexp>\n\n', ""\nYou definitely want to use Gradle flavors that comes natively, encouraged even, on Android Studio.\nIt seems to explain all the basics really well. I just finished converting to Gradle today, and it works great. Custom app icons, names, and strings, etc.\nAs the website explains, part of the purpose behind this design was to make it more dynamic and more easily allow multiple APKs to be created with essentially the same code, which sounds similar what you're doing.\nI probably didn't explain it the best, but that website does a pretty good job.\n"", '\nThe linked-to solution does not have to be done by hand. Bear in mind that the package attribute in the <manifest> element does not have to be where the code resides, so long as you spell out the fully-qualified classes elsewhere in the manifest (e.g., activity android:name=""com.commonsware.android.MyActivity"" rather than activity android:name="".MyActivity""). Script your manifest change and use Ant to build a new APK. AFAIK, that should work.\n', '\nSupport Multiple Partners \nPrepare config.xml\n\n\n     Build project for different partner \n<!--partner.dir, pkg.name, ver.code, ver.name are input from command line when execute \'ant\' -->\n\n<!-- set global properties for this build -->\n<property name=""build.bin"" location=""bin""/>\n<property name=""build.gen"" location=""gen""/>\n<property name=""src"" location=""src""/>\n<property name=""res"" location=""res""/>\n\n<target name=""preparefiles"" description=""Prepare files for different partner"" >\n    <delete dir=""${build.bin}"" />\n    <delete dir=""${build.gen}"" />\n\n    <copy todir=""${res}"" overwrite=""true"" />\n        <fileset dir=""${partner.dir}/res"" /> \n    </copy>\n\n    <!-- change the import in all Java source files -->\n    <replaceregexp file=""AndroidManifest.xml""\n        match=\'android.versionCode=""(.*)""\'\n        replace=\'android.versionCode=""${ver.code}""\'\n        byline=""false"">\n\n    <replaceregexp file=""AndroidManifest.xml""\n        match=\'android.versionName=""(.*)""\'\n        replace=\'android.versionName=""${ver.name}""\'\n        byline=""false"">\n\n    <replaceregexp file=""AndroidManifest.xml""\n        match=\'package=""(.*)""\'\n        replace=\'package=""${pkg.name}""\'\n        byline=""false"">\n\n    <!-- change the package name in AndroidManifest -->\n    <replaceregexp flags=""g"" byline=""false"">\n        <regexp pattern=""import(.*)com.myproject.com.R;"" /> \n        <substitution expression=""import com.${pkg.name}.R;"" />\n        <fileset dir=""${src}"" includes=""**/*.java"" /> \n    </replaceregexp>\n\n    <replaceregexp flags=""g"" byline=""false"">\n        <regexp pattern=""(package com.myproject.com;)"" /> \n        <substitution expression=""\\1&#10;import com.${pkg.name}.R;"" />\n        <fileset dir=""${src}"" includes=""**/*.java"" /> \n    </replaceregexp>\n</target>\n\n\nPrepare Files\n$ ant -f config.xml -Dpartner.dir=""xxx"" -Dpkg.name=""xxx"" -Dver.code=""xxx"" -Dver.name=""xxx"" preparefiles\nCreate build.xml\nBuild\n$ ant debug\nor\n$ ant release\n', ""\nI'm using the maven-android-plugin to achieve this. Specify one AndroidManifest.xml for the generated-sources goal and another AndroidManifest.xml for the final apk goal. That way the source code project retains the actual source code package name during generation of the R class and the build phase, while the market adapted manifest package name is in the second AndroidManifest.xml which is included in the final apk file.\n"", '\nI wound up with a script that patches the sources; patching the source sounds risky, but in presence of version control the risk is acceptable.\nSo I made one version, committed the source, made the other version, committed the source, and looking at diffs wrote a patching script in Python.\nI am not sure if it is the best solution. (And the code misses some os.path.joins)\nThe heart of the script is the following function:\n# In the file \'fname\',\n# find the text matching ""before oldtext after"" (all occurrences) and\n# replace \'oldtext\' with \'newtext\' (all occurrences).\n# If \'mandatory\' is true, raise an exception if no replacements were made.\ndef fileReplace(fname,before,newtext,after,mandatory=True):\n    with open(fname, \'r+\') as f:\n    read_data = f.read()\n    pattern = r""(""+re.escape(before)+r"")\\w+(""+re.escape(after)+r"")""\n    replacement = r""\\1""+newtext+r""\\2""\n    new_data,replacements_made = re.subn(pattern,replacement,read_data,flags=re.MULTILINE)\n    if replacements_made and really:\n        f.seek(0)\n        f.truncate()\n        f.write(new_data)\n        if verbose:\n            print ""patching "",fname,"" ("",replacements_made,"" occurrence"", ""s"" if 1!=replacements_made else """","")""\n    elif replacements_made:\n        print fname,"":""\n        print new_data\n    elif mandatory:\n        raise Exception(""cannot patch the file: ""+fname)\n\nAnd you may find the following one of use:\n# Change the application resource package name everywhere in the src/ tree.\n# Yes, this changes the java files. We hope that if something goes wrong,\n# the version control will save us.\ndef patchResourcePackageNameInSrc(pname):\n    for root, dirs, files in os.walk(\'src\'):\n    if \'.svn\' in dirs:\n        dirs.remove(\'.svn\')\n    for fname in files:\n        fileReplace(os.path.join(root,fname),""import com.xyz."",pname,"".R;"",mandatory=False)\n\nThere is also a function that copies assets from x-assets-cfgname to assets (earlier it turned out that for me it is more convenient to have a subdirectory in assets).\ndef copyAssets(vname,force=False):\n    assets_source = ""x-assets-""+vname+""/xxx""\n    assets_target = ""assets/xxx""\n    if not os.path.exists(assets_source):\n        raise Exception(""Invalid variant name: ""+vname+"" (the assets directory ""+assets_source+"" does not exist)"")\n    if os.path.exists(assets_target+""/.svn""):\n        raise Exception(""The assets directory must not be under version control! ""+assets_target+""/.svn exists!"")\n    if os.path.exists(assets_target):\n        shutil.rmtree(assets_target)\n    shutil.copytree(assets_source, assets_target, ignore=shutil.ignore_patterns(\'.svn\'))\n\nWell, you get the idea. Now you can write your own script.\n', ""\nI think the best way is to create a new project and copy the stuff. steps,\n- create new android project without a class\n- create package (package name should be corresponding to the one in the manifest file), or just copy the package name in the 'gen' folder\n- copy the java files\n- copy the drawable folders\n- copy the layout files\n- copy any other file(s) used in ur project\n- copy manifest file's data\nthis has been simpler for me for the task\n""]",https://stackoverflow.com/questions/1222302/android-multiple-custom-versions-of-the-same-app,automation
Upload file to SFTP using PowerShell,"
We were asked to set up an automated upload from one of our servers to an SFTP site. There will be a file that is exported from a database to a filer every Monday morning and they want the file to be uploaded to SFTP on Tuesday. The current authentication method we are using is username and password (I believe there was an option to have key file as well but username/password option was chosen).
The way I am envisioning this is to have a script sitting on a server that will be triggered by Windows Task scheduler to run at a specific time (Tuesday) that will grab the file in question upload it to the SFTP and then move it to a different location for backup purposes.
For example:

Local Directory: C:\FileDump
SFTP Directory: /Outbox/
Backup Directory: C:\Backup

I tried few things at this point WinSCP being one of them as well as SFTP PowerShell Snap-In but nothing has worked for me so far. 
This will be running on Windows Server 2012R2.
When I run Get-Host my console host version is 4.0.
Thanks.
",170k,"
            44
        ","['\nYou didn\'t tell us what particular problem do you have with the WinSCP, so I can really only repeat what\'s in WinSCP documentation.\n\nDownload WinSCP .NET assembly.\nThe latest package as of now is WinSCP-5.21.7-Automation.zip;\n\nExtract the .zip archive along your script;\n\nUse a code like this (based on the official PowerShell upload example):\n# Load WinSCP .NET assembly\nAdd-Type -Path ""WinSCPnet.dll""\n\n# Setup session options\n$sessionOptions = New-Object WinSCP.SessionOptions -Property @{\n    Protocol = [WinSCP.Protocol]::Sftp\n    HostName = ""example.com""\n    UserName = ""user""\n    Password = ""mypassword""\n    SshHostKeyFingerprint = ""ssh-rsa 2048 xxxxxxxxxxx...=""\n}\n\n$session = New-Object WinSCP.Session\n\ntry\n{\n    # Connect\n    $session.Open($sessionOptions)\n\n    # Upload\n    $session.PutFiles(""C:\\FileDump\\export.txt"", ""/Outbox/"").Check()\n}\nfinally\n{\n    # Disconnect, clean up\n    $session.Dispose()\n}\n\n\n\n\nYou can have WinSCP generate the PowerShell script for the upload for you:\n\nLogin to your server with WinSCP GUI;\nNavigate to the target directory in the remote file panel;\nSelect the file for upload in the local file panel;\nInvoke the Upload command;\nOn the Transfer options dialog, go to Transfer Settings > Generate Code;\nOn the Generate transfer code dialog, select the .NET assembly code tab;\nChoose PowerShell language.\n\nYou will get a code like above with all session and transfer settings filled in.\n\n(I\'m the author of WinSCP)\n', '\nThere isn\'t currently a built-in PowerShell method for doing the SFTP part. You\'ll have to use something like psftp.exe or a PowerShell module like Posh-SSH.\nHere is an example using Posh-SSH:\n# Set the credentials\n$Password = ConvertTo-SecureString \'Password1\' -AsPlainText -Force\n$Credential = New-Object System.Management.Automation.PSCredential (\'root\', $Password)\n\n# Set local file path, SFTP path, and the backup location path which I assume is an SMB path\n$FilePath = ""C:\\FileDump\\test.txt""\n$SftpPath = \'/Outbox\'\n$SmbPath = \'\\\\filer01\\Backup\'\n\n# Set the IP of the SFTP server\n$SftpIp = \'10.209.26.105\'\n\n# Load the Posh-SSH module\nImport-Module C:\\Temp\\Posh-SSH\n\n# Establish the SFTP connection\n$ThisSession = New-SFTPSession -ComputerName $SftpIp -Credential $Credential\n\n# Upload the file to the SFTP path\nSet-SFTPFile -SessionId ($ThisSession).SessionId -LocalFile $FilePath -RemotePath $SftpPath\n\n#Disconnect all SFTP Sessions\nGet-SFTPSession | % { Remove-SFTPSession -SessionId ($_.SessionId) }\n\n# Copy the file to the SMB location\nCopy-Item -Path $FilePath -Destination $SmbPath\n\nSome additional notes:\n\nYou\'ll have to download the Posh-SSH module which you can install to your user module directory (e.g. C:\\Users\\jon_dechiro\\Documents\\WindowsPowerShell\\Modules) and just load using the name or put it anywhere and load it like I have in the code above.\nIf having the credentials in the script is not acceptable you\'ll have to use a credential file. If you need help with that I can update with some details or point you to some links.\nChange the paths, IPs, etc. as needed.\n\nThat should give you a decent starting point.\n', ""\nI am able to sftp using PowerShell as below:\nPS C:\\Users\\user\\Desktop> sftp user@aa.bb.cc.dd                                                     \nuser@aa.bb.cc.dd's password:\nConnected to user@aa.bb.cc.dd.\nsftp> ls\ntestFolder\nsftp> cd testFolder\nsftp> ls\ntaj_mahal.jpeg\nsftp> put taj_mahal_1.jpeg\nUploading taj_mahal_1.jpeg to /home/user/testFolder/taj_mahal_1.jpeg\ntaj_mahal_1.jpeg                                                                      100%   11KB  35.6KB/s   00:00\nsftp> ls\ntaj_mahal.jpeg      taj_mahal_1.jpeg\nsftp>\n\nI do not have installed Posh-SSH or anything like that. I am using Windows 10 Pro PowerShell. No additional modules installed.\n"", ""\nUsing PuTTY's pscp.exe (which I have in an $env:path directory):\npscp -sftp -pw passwd c:\\filedump\\* user@host:/Outbox/\nmv c:\\filedump\\* c:\\backup\\*\n\n"", '\n$FilePath = ""C:\\Backup\\xxx.zip""\n$SftpPath = \'/Cloud_Deployment/Backup\'\n$SftpIp = \'mercury.xxx.xx.uk\' #Or IP\n$Password = \'password\'\n$userroot = \'username\'\n$Password = ConvertTo-SecureString $Password -AsPlainText -Force\n$Credential = New-Object System.Management.Automation.PSCredential ($userroot, $Password)\nInstall-Module -Name Posh-SSH  #rus as Admin \n$SFTPSession = New-SFTPSession -ComputerName $SftpIp -Credential $Credential\n#Download file\n#Get-SFTPItem -SessionId $SFTPSession.SessionId -Path $SftpPath/test.txt -Destination c:\\temp\n#Upload file\nSet-SFTPItem -SessionId $SFTPSession.SessionId -Path $FilePath -Destination $SftpPath\n#Disconnect all SFTP Sessions\nRemove-SFTPSession -SFTPSession $SFTPSession\n#or \nGet-SFTPSession | % { Remove-SFTPSession -SessionId ($_.SessionId) }\nRef : powershell-sftp\nIf any how you face error ""PackageManagement\\Install-Package : No match was found for the specified search criteria and module name \'Posh-SSH\'""\nThen please visit Here\n', '\nWell, while using powershell 7, we can simply upload files using sftp with following command\necho ""put localpath/file.txt destinationpath/file.txt"" | sftp username@server\n\nmake sure to add these double quotes.\n']",https://stackoverflow.com/questions/38732025/upload-file-to-sftp-using-powershell,automation
How to programmatically fill input elements built with React?,"
I'm tasked with crawling website built with React. I'm trying to fill in input fields and submitting the form using javascript injects to the page (either selenium or webview in mobile). This works like a charm on every other site + technology but React seems to be a real pain.
so here is a sample code 
var email = document.getElementById( 'email' );
email.value = 'example@mail.com';

I the value changes on the DOM input element, but the React does not trigger the change event.
I've been trying plethora of different ways to get the React to update the state.
var event = new Event('change', { bubbles: true });
email.dispatchEvent( event );

no avail
var event = new Event('input', { bubbles: true });
email.dispatchEvent( event );

not working
email.onChange( event );

not working
I cannot believe interacting with React has been made so difficult. I would greatly appreciate any help. 
Thank you
",24k,"
            37
        ","[""\nThis accepted solution appears not to work in React > 15.6 (including React 16) as a result of changes to de-dupe input and change events.\nYou can see the React discussion here: https://github.com/facebook/react/issues/10135\nAnd the suggested workaround here:\nhttps://github.com/facebook/react/issues/10135#issuecomment-314441175\nReproduced here for convenience:\nInstead of\ninput.value = 'foo';\ninput.dispatchEvent(new Event('input', {bubbles: true}));\n\nYou would use\nfunction setNativeValue(element, value) {\n  const valueSetter = Object.getOwnPropertyDescriptor(element, 'value').set;\n  const prototype = Object.getPrototypeOf(element);\n  const prototypeValueSetter = Object.getOwnPropertyDescriptor(prototype, 'value').set;\n\n  if (valueSetter && valueSetter !== prototypeValueSetter) {\n    prototypeValueSetter.call(element, value);\n  } else {\n    valueSetter.call(element, value);\n  }\n}\n\nand then\nsetNativeValue(input, 'foo');\ninput.dispatchEvent(new Event('input', { bubbles: true }));\n\n"", '\nReact is listening for the input event of text fields.\nYou can change the value and manually trigger an input event, and react\'s onChange handler will trigger:\n\n\nclass Form extends React.Component {\r\n  constructor(props) {\r\n    super(props)\r\n    this.state = {value: \'\'}\r\n  }\r\n  \r\n  handleChange(e) {\r\n    this.setState({value: e.target.value})\r\n    console.log(\'State updated to \', e.target.value);\r\n  }\r\n  \r\n  render() {\r\n    return (\r\n      <div>\r\n        <input\r\n          id=\'textfield\'\r\n          value={this.state.value}\r\n          onChange={this.handleChange.bind(this)}\r\n        />\r\n        <p>{this.state.value}</p>\r\n      </div>      \r\n    )\r\n  }\r\n}\r\n\r\nReactDOM.render(\r\n  <Form />,\r\n  document.getElementById(\'app\')\r\n)\r\n\r\ndocument.getElementById(\'textfield\').value = \'foo\'\r\nconst event = new Event(\'input\', { bubbles: true })\r\ndocument.getElementById(\'textfield\').dispatchEvent(event)\n<script src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react.min.js""></script>\r\n<script src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react-dom.min.js""></script>\r\n\r\n<div id=\'app\'></div>\n\n\n', ""\nHere is the cleanest possible solution for inputs, selects, checkboxes, etc. (works not only for react inputs)\n/**\n * See [Modify React Component's State using jQuery/Plain Javascript from Chrome Extension](https://stackoverflow.com/q/41166005)\n * See https://github.com/facebook/react/issues/11488#issuecomment-347775628\n * See [How to programmatically fill input elements built with React?](https://stackoverflow.com/q/40894637)\n * See https://github.com/facebook/react/issues/10135#issuecomment-401496776\n *\n * @param {HTMLInputElement | HTMLSelectElement} el\n * @param {string} value\n */\nfunction setNativeValue(el, value) {\n  const previousValue = el.value;\n\n  if (el.type === 'checkbox' || el.type === 'radio') {\n    if ((!!value && !el.checked) || (!!!value && el.checked)) {\n      el.click();\n    }\n  } else el.value = value;\n\n  const tracker = el._valueTracker;\n  if (tracker) {\n    tracker.setValue(previousValue);\n  }\n\n  // 'change' instead of 'input', see https://github.com/facebook/react/issues/11488#issuecomment-381590324\n  el.dispatchEvent(new Event('change', { bubbles: true }));\n}\n\nUsage:\nsetNativeValue(document.getElementById('name'), 'Your name');\ndocument.getElementById('radio').click(); // or setNativeValue(document.getElementById('radio'), true)\ndocument.getElementById('checkbox').click(); // or setNativeValue(document.getElementById('checkbox'), true)\n\n"", '\nI noticed the input element had some property with a name along the lines of __reactEventHandlers$..., which had some functions including an onChange.\nThis worked for finding that function and triggering it\nlet getReactEventHandlers = (element) => {\n    // the name of the attribute changes, so we find it using a match.\n    // It\'s something like `element.__reactEventHandlers$...`\n    let reactEventHandlersName = Object.keys(element)\n       .filter(key => key.match(\'reactEventHandler\'));\n    return element[reactEventHandlersName];\n}\n\nlet triggerReactOnChangeEvent = (element) => {\n    let ev = new Event(\'change\');\n    // workaround to set the event target, because `ev.target = element` doesn\'t work\n    Object.defineProperty(ev, \'target\', {writable: false, value: element});\n    getReactEventHandlers(element).onChange(ev);\n}\n\ninput.value = ""some value"";\ntriggerReactOnChangeEvent(input);\n\n', '\nWithout element ids:\nexport default function SomeComponent() {\n    const inputRef = useRef();\n    const [address, setAddress] = useState("""");\n    const onAddressChange = (e) => {\n        setAddress(e.target.value);\n    }\n    const setAddressProgrammatically = (newValue) => {\n        const event = new Event(\'change\', { bubbles: true });\n        const input = inputRef.current;\n        if (input) {\n            setAddress(newValue);\n            input.value = newValue;\n            input.dispatchEvent(event);\n        }\n    }\n    return (\n        ...\n        <input ref={inputRef} type=""text"" value={address} onChange={onAddressChange}/>\n        ...\n    );\n}\n\n', '\nReact 17 works with fibers:\nfunction findReact(dom) {\n    let key = Object.keys(dom).find(key => key.startsWith(""__reactFiber$""));\n    let internalInstance = dom[key];\n    if (internalInstance == null) return ""internalInstance is null: "" + key;\n\n    if (internalInstance.return) { // react 16+\n        return internalInstance._debugOwner\n            ? internalInstance._debugOwner.stateNode\n           : internalInstance.return.stateNode;\n    } else { // react <16\n        return internalInstance._currentElement._owner._instance;\n   }\n}\n\nthen:\nfindReact(domElement).onChangeWrapper(""New value"");\n\nthe domElement in this is the tr with the data-param-name of the field you are trying to change:\nvar domElement = ?.querySelectorAll(\'tr[data-param-name=""<my field name>""]\')\n\n']",https://stackoverflow.com/questions/40894637/how-to-programmatically-fill-input-elements-built-with-react,automation
Using conditional statements inside 'expect',"
I need to automate logging into a TELNET session using expect, but I need to take care of multiple passwords for the same username.
Here's the flow I need to create:

Open TELNET session to an IP
Send user-name
Send password
Wrong password? Send the same user-name again, then a different password
Should have successfully logged-in at this point...

For what it's worth, here's what I've got so far:
#!/usr/bin/expect
spawn telnet 192.168.40.100
expect ""login:""
send ""spongebob\r""
expect ""password:""
send ""squarepants\r""
expect ""login incorrect"" {
  expect ""login:""
  send ""spongebob\r""
  expect ""password:""
  send ""rhombuspants\r""
}
expect ""prompt\>"" {
  send_user ""success!\r""
}
send ""blah...blah...blah\r""

Needless to say this doesn't work, and nor does it look very pretty. From my adventures with Google expect seems to be something of a dark-art. Thanks in advance to anyone for assistance in the matter!
",77k,"
            33
        ","['\nHave to recomment the Exploring Expect book for all expect programmers -- invaluable.\nI\'ve rewritten your code: (untested)\nproc login {user pass} {\n    expect ""login:""\n    send ""$user\\r""\n    expect ""password:""\n    send ""$pass\\r""\n}\n\nset username spongebob \nset passwords {squarepants rhombuspants}\nset index 0\n\nspawn telnet 192.168.40.100\nlogin $username [lindex $passwords $index]\nexpect {\n    ""login incorrect"" {\n        send_user ""failed with $username:[lindex $passwords $index]\\n""\n        incr index\n        if {$index == [llength $passwords]} {\n            error ""ran out of possible passwords""\n        }\n        login $username [lindex $passwords $index]\n        exp_continue\n    }\n    ""prompt>"" \n}\nsend_user ""success!\\n""\n# ...\n\nexp_continue loops back to the beginning of the expect block -- it\'s like a ""redo"" statement.\nNote that send_user ends with \\n not \\r\nYou don\'t have to escape the > character in your prompt: it\'s not special for Tcl.\n', '\nWith a bit of bashing I found a solution. Turns out that expect uses a TCL syntax that I\'m not at all familiar with:\n#!/usr/bin/expect\nset pass(0) ""squarepants""\nset pass(1) ""rhombuspants""\nset pass(2) ""trapezoidpants""\nset count 0\nset prompt ""> ""\nspawn telnet 192.168.40.100\nexpect {\n  ""$prompt"" {\n    send_user ""successfully logged in!\\r""\n  }\n  ""password:"" {\n    send ""$pass($count)\\r""\n    exp_continue\n  }\n  ""login incorrect"" {\n    incr count\n    exp_continue\n  }\n  ""username:"" {\n    send ""spongebob\\r""\n    exp_continue\n  }\n}\nsend ""command1\\r""\nexpect ""$prompt""\nsend ""command2\\r""\nexpect ""$prompt""\nsend ""exit\\r""\nexpect eof\nexit\n\nHopefully this will be useful to others.\n', ""\nIf you know the user ids and passwords, then you ought also to know which userid/password pairs are aligned with which systems.  I think you'd be better off maintaining a map of which userid/password pair goes with which system then extracting that information and simply use the correct one.\nSo -- since you obviously don't like my advice, then I suggest you look at the wikipedia page and implement a procedure that returns 0 if successful and 1 if the expectation times out.  That will allow you to detect when the password supplied failed -- the prompt expectation times out -- and retry.  If this is helpful, you can remove your downvote now that I've edited it.\nIn retrospect, you'd probably want to do this in conjunction with the map anyway since you'd want to detect a failed login if the password was changed.\n""]",https://stackoverflow.com/questions/1538444/using-conditional-statements-inside-expect,automation
How can I use persisted cookies from a file using phantomjs,"
I have some authentication requried to hit a particular url. In browser I need to login only once, as for other related urls which can use the session id from the cookie need not required to go to the login page. 
Similarly, can I use the cookie generated in the cookie file using --cookies-file=cookies.txt in the commandline in phantomjs to open other page which requires the same cookie detail.
Please suggest.
",23k,"
            25
        ","['\nPhantom JS and cookies\n--cookies-file=cookies.txt will only store non-session cookies in the cookie jar. Login and authentication is more commonly based on session cookies.\nWhat about session cookies?\nTo save these is quite simple, but you should consider that they will likely expire quickly.\nYou need to write your program logic to consider this. For example\n\nLoad cookies from the cookiejar\nHit a URL to check if the user is logged in\nIf not logged in\nLog in, Save cookies to cookiejar\ncontinue with processing\n\nExample\nvar fs = require(\'fs\');\nvar CookieJar = ""cookiejar.json"";\nvar pageResponses = {};\npage.onResourceReceived = function(response) {\n    pageResponses[response.url] = response.status;\n    fs.write(CookieJar, JSON.stringify(phantom.cookies), ""w"");\n};\nif(fs.isFile(CookieJar))\n    Array.prototype.forEach.call(JSON.parse(fs.read(CookieJar)), function(x){\n        phantom.addCookie(x);\n    });\n\npage.open(LoginCheckURL, function(status){\n // this assumes that when you are not logged in, the server replies with a 303\n if(pageResponses[LoginCheckURL] == 303)\n {  \n    //attempt login\n    //assuming a resourceRequested event is fired the cookies will be written to the jar and on your next load of the script they will be found and used\n }\n\n\n});\n\n', '\nThe file created by the option --cookies-file=cookies.txt is serialized from CookieJar: there are extra characters and it\'s sometimes difficult to parse.\nIt may looks like:\n[General]\ncookies=""@Variant(\\0\\0\\0\\x7f\\0\\0\\0\\x16QList<QNetworkCookie>\\0\\0\\0\\0\\x1\\0\\0\\0\\v\\0\\0\\0{__cfduid=da7fda1ef6dd8b38450c6ad5632...\n\nI used in the past phantom.cookies. This array will be pre-populated by any existing Cookie data stored in the cookie file specified in the PhantomJS startup config/command-line options, if any. But you can also add dynamic cookie by using phantom.addCookie.\nA basic example is \nphantom.addCookie({\n    \'name\':     \'Valid-Cookie-Name\',   /* required property */\n    \'value\':    \'Valid-Cookie-Value\',  /* required property */\n    \'domain\':   \'localhost\',           /* required property */\n    \'path\':     \'/foo\',\n    \'httponly\': true,\n    \'secure\':   false,\n    \'expires\':  (new Date()).getTime() + (1000 * 60 * 60)   /* <-- expires in 1 hour */\n});\n\nWith these methods, it\'s not so difficult to implement your own cookie management logic.\n']",https://stackoverflow.com/questions/18739354/how-can-i-use-persisted-cookies-from-a-file-using-phantomjs,automation
c# and excel automation - ending the running instance,"
I'm attempting Excel automation through C#. I have followed all the instructions from Microsoft on how to go about this, but I'm still struggling to discard the final reference(s) to Excel for it to close and to enable the GC to collect it.
A code sample follows. When I comment out the code block that contains lines similar to:
Sheet.Cells[iRowCount, 1] = data[""fullname""].ToString();

then the file saves and Excel quits. Otherwise the file saves but Excel is left running as a process. The next time this code runs it creates a new instance and they eventually build up.
Any help is appreciated. Thanks.
This is the barebones of my code:
        Excel.Application xl = null;
        Excel._Workbook wBook = null;
        Excel._Worksheet wSheet = null;
        Excel.Range range = null;

        object m_objOpt = System.Reflection.Missing.Value;

        try
        {
            // open the template
            xl = new Excel.Application();
            wBook = (Excel._Workbook)xl.Workbooks.Open(excelTemplatePath + _report.ExcelTemplate, false, false, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt);
            wSheet = (Excel._Worksheet)wBook.ActiveSheet;

            int iRowCount = 2;

            // enumerate and drop the values straight into the Excel file
            while (data.Read())
            {

                wSheet.Cells[iRowCount, 1] = data[""fullname""].ToString();
                wSheet.Cells[iRowCount, 2] = data[""brand""].ToString();
                wSheet.Cells[iRowCount, 3] = data[""agency""].ToString();
                wSheet.Cells[iRowCount, 4] = data[""advertiser""].ToString();
                wSheet.Cells[iRowCount, 5] = data[""product""].ToString();
                wSheet.Cells[iRowCount, 6] = data[""comment""].ToString();
                wSheet.Cells[iRowCount, 7] = data[""brief""].ToString();
                wSheet.Cells[iRowCount, 8] = data[""responseDate""].ToString();
                wSheet.Cells[iRowCount, 9] = data[""share""].ToString();
                wSheet.Cells[iRowCount, 10] = data[""status""].ToString();
                wSheet.Cells[iRowCount, 11] = data[""startDate""].ToString();
                wSheet.Cells[iRowCount, 12] = data[""value""].ToString();

                iRowCount++;
            }

            DirectoryInfo saveTo = Directory.CreateDirectory(excelTemplatePath + _report.FolderGuid.ToString() + ""\\"");
            _report.ReportLocation = saveTo.FullName + _report.ExcelTemplate;
            wBook.Close(true, _report.ReportLocation, m_objOpt);
            wBook = null;

        }
        catch (Exception ex)
        {
            LogException.HandleException(ex);
        }
        finally
        {
            NAR(wSheet);
            if (wBook != null)
                wBook.Close(false, m_objOpt, m_objOpt);
            NAR(wBook);
            xl.Quit();
            NAR(xl);
            GC.Collect();
        }

private void NAR(object o)
{
    try
    {
        System.Runtime.InteropServices.Marshal.ReleaseComObject(o);
    }
    catch { }
    finally
    {
        o = null;
    }
}


Update
No matter what I try, the 'clean' method or the 'ugly' method (see answers below), the excel instance still hangs around as soon as this line is hit:
wSheet.Cells[iRowCount, 1] = data[""fullname""].ToString();

If I comment that line out (and the other similar ones below it, obviously) the Excel app exits gracefully. As soon as one line per above is uncommented, Excel sticks around.
I think I'm going to have to check if there's a running instance prior to assigning the xl variable and hook into that instead. I forgot to mention that this is a windows service, but that shouldn't matter, should it?

",22k,"
            14
        ","['\nUPDATE (November 2016)\nI\'ve just read a convincing argument by Hans Passant that using GC.Collect is actually the right way to go. I no longer work with Office (thank goodness), but if I did I\'d probably want to give this another try - it would certainly simplify a lot of the (thousands of lines) of code I wrote trying to do things the ""right"" way (as I saw it then).\nI\'ll leave my original answer for posterity...\n\nAs Mike says in his answer, there is an easy way and a hard way to deal with this. Mike suggests using the easy way because... it\'s easier. I don\'t personally believe that\'s a good enough reason, and I don\'t believe it\'s the right way. It smacks of ""turn it off and on again"" to me.\nI have several years experience of developing an Office automation application in .NET, and these COM interop problems plagued me for the first few weeks & months when I first ran into the issue, not least because Microsoft are very coy about admitting there\'s a problem in the first place, and at the time good advice was hard to find on the web.\nI have a way of working that I now use virtually without thinking about it, and it\'s years since I had a problem. It\'s still important to be alive to all the hidden objects that you might be creating - and yes, if you miss one, you might have a leak that only becomes apparent much later. But it\'s no worse than things used to be in the bad old days of malloc/free.\nI do think there\'s something to be said for cleaning up after yourself as you go, rather than at the end. If you\'re only starting Excel to fill in a few cells, then maybe it doesn\'t matter - but if you\'re going to be doing some heavy lifting, then that\'s a different matter.\nAnyway, the technique I use is to use a wrapper class that implements IDisposable, and which in its Dispose method calls ReleaseComObject. That way I can use using statements to ensure that the object is disposed (and the COM object released) as soon as I\'m finished with it.\nCrucially, it\'ll get disposed/released even if my function returns early, or there\'s an Exception, etc. Also, it\'ll only get disposed/released if it was actually created in the first place - call me a pedant but the suggested code that attempts to release objects that may not actually have been created looks to me like sloppy code. I have a similar objection to using FinalReleaseComObject - you should know how many times you caused the creation of a COM reference, and should therefore be able to release it the same number of times.\nA typical snippet of my code might look like this (or it would, if I was using C# v2 and could use generics :-)):\nusing (ComWrapper<Excel.Application> application = new ComWrapper<Excel.Application>(new Excel.Application()))\n{\n  try\n  {\n    using (ComWrapper<Excel.Workbooks> workbooks = new ComWrapper<Excel.Workbooks>(application.ComObject.Workbooks))\n    {\n      using (ComWrapper<Excel.Workbook> workbook = new ComWrapper<Excel.Workbook>(workbooks.ComObject.Open(...)))\n      {\n        using (ComWrapper<Excel.Worksheet> worksheet = new ComWrapper<Excel.Worksheet>(workbook.ComObject.ActiveSheet))\n        {\n          FillTheWorksheet(worksheet);\n        }\n        // Close the workbook here (see edit 2 below)\n      }\n    }\n  }\n  finally\n  {\n    application.ComObject.Quit();\n  }\n}\n\nNow, I\'m not about to pretend that that isn\'t wordy, and the indentation caused by object creation can get out of hand if you don\'t divide stuff into smaller methods. This example is something of a worst case, since all we\'re doing is creating objects. Normally there\'s a lot more going on between the braces and the overhead is much less.\nNote that as per the example above I would always pass the \'wrapped\' objects between methods, never a naked COM object, and it would be the responsibility of the caller to dispose of it (usually with a using statement). Similarly, I would always return a wrapped object, never a naked one, and again it would be the responsibility of the caller to release it. You could use a different protocol, but it\'s important to have clear rules, just as it was when we used to have to do our own memory management.\nThe ComWrapper<T> class used here hopefully requires little explanation. It simply stores a reference to the wrapped COM object, and releases it explicitly (using ReleaseComObject) in its Dispose method. The ComObject method simply returns a typed reference to the wrapped COM object.\nHope this helps!\nEDIT: I\'ve only now followed the link over to Mike\'s answer to another question, and I see that another answer to that question there has a link to a wrapper class, much as I suggest above.\nAlso, with regard to Mike\'s answer to that other question, I have to say I was very nearly seduced by the ""just use GC.Collect"" argument. However, I was mainly drawn to that on a false premise; it looked at first glance like there would be no need to worry about the COM references at all. However, as Mike says you do still need to explicitly release the COM objects associated with all your in-scope variables - and so all you\'ve done is reduce rather than remove the need for COM-object management. Personally, I\'d rather go the whole hog. \nI also note a tendency in lots of answers to write code where everything gets released at the end of a method, in a big block of ReleaseComObject calls. That\'s all very well if everything works as planned, but I would urge anyone writing serious code to consider what would happen if an exception were thrown, or if the method had several exit points (the code would not be executed, and thus the COM objects would not be released). This is why I favor the use of ""wrappers"" and usings. It\'s wordy, but it does make for bulletproof code.\nEDIT2: I\'ve updated the code above to indicate where the workbook should be closed with or without saving changes. Here\'s the code to save changes:\nobject saveChanges = Excel.XlSaveAction.xlSaveChanges;\n\nworkbook.ComObject.Close(saveChanges, Type.Missing, Type.Missing);\n\n...and to not save changes, simply change xlSaveChanges to xlDoNotSaveChanges.\n', '\nWhat is happening is that your call to:\nSheet.Cells[iRowCount, 1] = data[""fullname""].ToString();\n\nIs essentially the same as:\nExcel.Range cell = Sheet.Cells[iRowCount, 1];\ncell.Value = data[""fullname""].ToString();\n\nBy doing it this way, you can see that you are creating an Excel.Range object, and then assigning a value to it. This way also gives us a named reference to our range variable, the cell variable, that allows us to release it directly if we wanted. So you could clean up your objects one of two ways:\n(1) The difficult and ugly way:\nwhile (data.Read())\n{\n    Excel.Range cell = Sheet.Cells[iRowCount, 1];\n    cell.Value = data[""fullname""].ToString();\n    Marshal.FinalReleaseComObject(cell);\n\n    cell = Sheet.Cells[iRowCount, 2];\n    cell.Value = data[""brand""].ToString();\n    Marshal.FinalReleaseComObject(cell);\n\n    cell = Sheet.Cells[iRowCount, 3];\n    cell.Value = data[""agency""].ToString();\n    Marshal.FinalReleaseComObject(cell);\n\n    // etc...\n}\n\nIn the above, we are releasing each range object via a call to Marshal.FinalReleaseComObject(cell) as we go along.\n(2) The easy and clean way: \nLeave your code exactly as you currently have it, and then at the end you can clean up as follows:\nGC.Collect();\nGC.WaitForPendingFinalizers();\n\nif (wSheet != null)\n{\n    Marshal.FinalReleaseComObject(wSheet)\n}\nif (wBook != null)\n{\n    wBook.Close(false, m_objOpt, m_objOpt);\n    Marshal.FinalReleaseComObject(wBook);\n}\nxl.Quit();\nMarshal.FinalReleaseComObject(xl);\n\nIn short, your existing code is extremely close. If you just add calls to GC.Collect() and GC.WaitForPendingFinalizers() before your \'NAR\' calls, I think it should work for you. (In short, both Jamie\'s code and Ahmad\'s code are correct. Jamie\'s is cleaner, but Ahmad\'s code is an easier ""quick fix"" for you because you would only have to add the calls to calls to GC.Collect() and GC.WaitForPendingFinalizers() to your existing code.)\nJamie and Amhad also listed links to the .NET Automation Forum that I participate on (thanks guys!) Here are a couple of related posts that I\'ve made here on StackOverflow :\n(1) How to properly clean up Excel interop objects in C#\n(2) C# Automate PowerPoint Excel -- PowerPoint does not quit\nI hope this helps, Sean...\nMike\n', ""\nAdd the following before your call to xl.Quit():\nGC.Collect(); \nGC.WaitForPendingFinalizers(); \n\nYou can also use Marshal.FinalReleaseComObject() in your NAR method instead of ReleaseComObject. ReleaseComObject decrements the reference count by 1 while FinalReleaseComObject releases all references so the count is 0.\nSo your finally block would look like:\nfinally\n{\n    GC.Collect(); \n    GC.WaitForPendingFinalizers(); \n\n    NAR(wSheet);\n    if (wBook != null)\n        wBook.Close(false, m_objOpt, m_objOpt);\n    NAR(wBook);\n    xl.Quit();\n    NAR(xl);\n}\n\nUpdated NAR method:\nprivate void NAR(object o)\n{\n    try\n    {\n        System.Runtime.InteropServices.Marshal.FinalReleaseComObject(o);\n    }\n    catch { }\n    finally\n    {\n        o = null;\n    }\n}\n\nI had researched this awhile ago and in examples I found usually the GC related calls were at the end after closing the app. However, there's an MVP (Mike Rosenblum) that mentions that it ought to be called in the beginning. I've tried both ways and they've worked. I also tried it without the WaitForPendingFinalizers and it worked although it shouldn't hurt anything. YMMV.\nHere are the relevant links by the MVP I mentioned (they're in VB but it's not that different):\n\nhttp://www.xtremevbtalk.com/showthread.php?p=1157109#post1157109\nhttp://www.xtremevbtalk.com/showthread.php?s=bcdea222412c5cbfa7f02cfaf8f7b33f&p=1156479#post1156479\n\n"", '\nAs others have already covered InterOp i would suggest that if you deal with Excel files with XLSX extension you should use EPPlus which will make your Excel nightmares go away.\n', '\nI have just answered this question here:\nKilling excel process by its main window hWnd\n', '\nIts over 4 years since this was posted but I came across the same problem and was able to solve it.  Apparently just accessing the Cells array creates a COM object. So if you were to do:\n    wSheet = (Excel._Worksheet)wBook.ActiveSheet;\n    Microsoft.Office.Interop.Excel.Range cells = wSheet.Cells;\n\n    int iRowCount = 2;\n\n    // enumerate and drop the values straight into the Excel file\n    while (data.Read())\n    {\n        Microsoft.Office.Interop.Excel.Range cell = cells[iRowCount, 1];\n        cell  = data[""fullname""].ToString();\n        Marshal.FinalReleaseComObject(cell);\n    }\n    Marshal.FinalReleaseComObject(cells);\n\nand then the rest of your cleanup it should fix the problem.\n', '\nWhat I ended up doing to solve a similar problem was get the process Id and kill that as a last resort...\n[DllImport(""user32.dll"", SetLastError = true)]\n   static extern IntPtr GetWindowThreadProcessId(int hWnd, out IntPtr lpdwProcessId);\n\n...\n\nobjApp = new Excel.Application();\n\nIntPtr processID;\nGetWindowThreadProcessId(objApp.Hwnd, out processID);\nexcel = Process.GetProcessById(processID.ToInt32());\n\n...\n\nobjApp.Application.Quit();\nMarshal.FinalReleaseComObject(objApp);\n_excel.Kill();\n\n', ""\nHere's the contents of my hasn't-failed-yet finally block for cleaning up Excel automation. My application leaves Excel open so there's no Quit call. The reference in the comment was my source.\nfinally\n{\n    // Cleanup -- See http://www.xtremevbtalk.com/showthread.php?t=160433\n    GC.Collect();\n    GC.WaitForPendingFinalizers();\n    GC.Collect();\n    GC.WaitForPendingFinalizers();\n    // Calls are needed to avoid memory leak\n    Marshal.FinalReleaseComObject(sheet);\n    Marshal.FinalReleaseComObject(book);\n    Marshal.FinalReleaseComObject(excel);\n}\n\n"", '\nHave you considered using a pure .NET solution such as SpreadsheetGear for .NET? Here is what your code might like like using SpreadsheetGear:\n// open the template            \nusing (IWorkbookSet workbookSet = SpreadsheetGear.Factory.GetWorkbookSet())\n{\n    IWorkbook wBook = workbookSet.Workbooks.Open(excelTemplatePath + _report.ExcelTemplate);\n    IWorksheet wSheet = wBook.ActiveWorksheet;\n    int iRowCount = 2;\n    // enumerate and drop the values straight into the Excel file            \n    while (data.Read())\n    {\n        wSheet.Cells[iRowCount, 1].Value = data[""fullname""].ToString();\n        wSheet.Cells[iRowCount, 2].Value = data[""brand""].ToString();\n        wSheet.Cells[iRowCount, 3].Value = data[""agency""].ToString();\n        wSheet.Cells[iRowCount, 4].Value = data[""advertiser""].ToString();\n        wSheet.Cells[iRowCount, 5].Value = data[""product""].ToString();\n        wSheet.Cells[iRowCount, 6].Value = data[""comment""].ToString();\n        wSheet.Cells[iRowCount, 7].Value = data[""brief""].ToString();\n        wSheet.Cells[iRowCount, 8].Value = data[""responseDate""].ToString();\n        wSheet.Cells[iRowCount, 9].Value = data[""share""].ToString();\n        wSheet.Cells[iRowCount, 10].Value = data[""status""].ToString();\n        wSheet.Cells[iRowCount, 11].Value = data[""startDate""].ToString();\n        wSheet.Cells[iRowCount, 12].Value = data[""value""].ToString();\n        iRowCount++;\n    }\n    DirectoryInfo saveTo = Directory.CreateDirectory(excelTemplatePath + _report.FolderGuid.ToString() + ""\\\\"");\n    _report.ReportLocation = saveTo.FullName + _report.ExcelTemplate;\n    wBook.SaveAs(_report.ReportLocation, FileFormat.OpenXMLWorkbook);\n}\n\nIf you have more than a few rows, you might be shocked at how much faster it runs. And you will never have to worry about a hanging instance of Excel.\nYou can download the free trial here and try it for yourself.\nDisclaimer: I own SpreadsheetGear LLC\n', '\nThe easiest way to paste code is through a question - this doesn\'t mean that I have answered my own question (unfortunately).\nApologies to those trying to help me - I was not able to get back to this until now. It still has me stumped...\nI have completely isolated the Excel code into one function as per\nprivate bool GenerateDailyProposalsReport(ScheduledReport report)\n{\n    // start of test\n\n    Excel.Application xl = null;\n    Excel._Workbook wBook = null;\n    Excel._Worksheet wSheet = null;\n    Excel.Range xlrange = null;\n    object m_objOpt = System.Reflection.Missing.Value;\n\n    xl = new Excel.Application();\n    wBook = (Excel._Workbook)xl.Workbooks.Open(@""E:\\Development\\Romain\\APN\\SalesLinkReportManager\\ExcelTemplates\\DailyProposalReport.xls"", false, false, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt);\n    wSheet = (Excel._Worksheet)wBook.ActiveSheet;\n    xlrange = wSheet.Cells[2, 1] as Excel.Range;\n\n    // PROBLEM LINE ************\n    xlrange.Value2 = ""fullname"";\n    //**************************\n\n    wBook.Close(true, @""c:\\temp\\DailyProposalReport.xls"", m_objOpt);\n    xl.Quit();\n\n    GC.Collect();\n    GC.WaitForPendingFinalizers();\n    GC.Collect();\n    GC.WaitForPendingFinalizers();\n\n    Marshal.FinalReleaseComObject(xlrange);\n    Marshal.FinalReleaseComObject(wSheet);\n    Marshal.FinalReleaseComObject(wBook);\n    Marshal.FinalReleaseComObject(xl);\n\n    xlrange = null;\n    wSheet = null;\n    wBook = null;\n    xl = null;\n\n    // end of test\n    return true;\n\n}\n\nIf I comment out the PROBLEM LINE above, the instance of Excel is released from memory. As it stands, it does not.\nI\'d appreciate any further help on this as time is fleeting and a deadline looms (don\'t they all). \nPlease ask if you need more information.\nThanks in anticipation.\nAddendum\nA bit more information that may or may not shed more light on this. I have resorted to killing the process (stopgap measure) after a certain time lapse (5-10 seconds to give Excel time to finish it\'s processes). I have two reports scheduled - the first report is created and saved to disk and the Excel process is killed, then emailed. The second is created, saved to disk, the process is killed but suddenly there is an error when attempting the email. The error is:\n The process cannot access the file\'....\' etc\nSo even when the Excel app has been killed, the actual Excel file is still being held by the windows service. I have to kill the service to delete the file...\n', '\nI\'m afraid I am running out of ideas here Sean. :-(\nGary could have some thoughts, but although his wrapper approach is very solid, it won\'t actually help you in this case because you are already doing everything pretty much correctly.\nI\'ll list a few thoughts here. I don\'t see how any of them will actually work because your mystery line \nxlrange.Value2 = ""fullname"";\n\nwould not seem to be impacted by any of these ideas, but here goes:\n(1) Don\'t make use of the _Workbook and _Worksheet interfaces. Use Workbook and Worksheet instead. (For more on this see: Excel interop: _Worksheet or Worksheet?.)\n(2) Any time you have two dots (""."") on the same line when accessing an Excel object, break it up into two lines, assigning each object to a named variable. Then, within the cleanup section of your code, explicitly release each variable using Marshal.FinalReleaseComObject().\nFor example, your code here:\n wBook = (Excel._Workbook)xl.Workbooks.Open(@""E:\\Development\\Romain\\APN\\SalesLinkReportManager\\ExcelTemplates\\DailyProposalReport.xls"", false, false, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt, m_objOpt);\n\ncould be broken up to:\nExcel.Workbooks wBooks = xl.Workbooks;\nwBook = wBooks.Open(""@""E:\\Development\\...\\DailyProposalReport.xls"", etc...);\n\nAnd then later, within the cleanup section, you would have:\nMarshal.FinalReleaseComObject(xlrange);\nMarshal.FinalReleaseComObject(wSheet);\nMarshal.FinalReleaseComObject(wBook);\nMarshal.FinalReleaseComObject(wBooks); // <-- Added\nMarshal.FinalReleaseComObject(xl);\n\n(3) I am not sure what is going on with your Process.Kill approach. If you call wBook.Close() and then xl.Quit() before calling Process.Kill(), you should have no troubles. Workbook.Close() does not return execution to you until the workbook is closed, and Excel.Quit() will not return execution until Excel has finished shutting down (although it might still be hanging).  \nAfter calling Process.Kill(), you can check the Process.HasExited property in a loop, or, better, call the Process.WaitForExit() method which will pause until it has exited for you. I would guess that this will generally take well under a second to occur. It is better to wait less time and be certain than to wait 5 - 10 seconds and only be guessing.\n(4) You should try these cleanup ideas that I\'ve listed above, but I am starting to suspect that you might have an issue with other processes that might be working with Excel, such as an add-in or anti-virus program. These add-ins can cause Excel to hang if they are not done correctly. If this occurs, it can be very difficult or impossible to get Excel to release. You would need to figure out the offending program and then disable it. Another possibility is that operating as a Windows Service somehow is an issue. I don\'t see why it would be, but I do not have experience automating Excel via a Windows Service, so I can\'t say. If your problems are related to this, then using Process.Kill will likely be your only resort here.\nThis is all I can think of off-hand, Sean. I hope this helps. Let us know how it goes...\n-- Mike\n', '\nSean,\nI\'m going to re-post your code again with my changes (below). I\'ve avoided changing your code too much, so I haven\'t added any exception handling, etc. This code is not robust.\nprivate bool GenerateDailyProposalsReport(ScheduledReport report)\n{\n    Excel.Application xl = null;\n    Excel.Workbooks wBooks = null;\n    Excel.Workbook wBook = null;\n    Excel.Worksheet wSheet = null;\n    Excel.Range xlrange = null;\n    Excel.Range xlcell = null;\n\n    xl = new Excel.Application();\n\n    wBooks = xl.Workbooks;\n\n    wBook = wBooks.Open(@""DailyProposalReport.xls"", false, false, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing, Type.Missing);\n\n    wSheet = wBook.ActiveSheet;\n\n    xlrange = wSheet.Cells;\n\n    xlcell = xlrange[2, 1] as Excel.Range;\n\n    xlcell.Value2 = ""fullname"";\n\n    Marshal.ReleaseComObject(xlcell);\n    Marshal.ReleaseComObject(xlrange);\n    Marshal.ReleaseComObject(wSheet);\n\n    wBook.Close(true, @""c:\\temp\\DailyProposalReport.xls"", Type.Missing);\n\n    Marshal.ReleaseComObject(wBook);\n    Marshal.ReleaseComObject(wBooks);\n\n    xl.Quit();\n\n    Marshal.ReleaseComObject(xl);\n\n    return true;\n}\n\nPoints to note:\n\nThe Workbooks method of the\nApplication class creates a\nWorkbooks object which holds a\nreference to the corresponding COM\nobject, so we need to ensure we\nsubsequently release that reference,\nwhich is why I added the variable\nwBooks and the corresponding call to ReleaseComObject.\nSimilarly, the Cells method of the\nWorksheet object returns a Range\nobject with another COM reference,\nso we need to clean that up too.\nHence the need for 2 separate Range variables.\nI\'ve released the COM references\n(using ReleaseComObject) as soon\nas they\'re no longer needed, which I\nthink is good practice even if it\nisn\'t strictly necessary. Also, (and\nthis may be superstition) I\'ve\nreleased all the objects owned by\nthe workbook before closing the\nworkbook, and released the workbook\nbefore closing Excel.\nI\'m not calling GC.Collect etc.\nbecause it shouldn\'t be necessary.\nReally!\nI\'m using ReleaseComObject rather\nthan FinalReleaseComObject, because it should be perfectly sufficient.\nI\'m not null-ing the variables after\nuse; once again, it\'s not doing\nanything worthwhile.\nNot relevant\nhere, but I\'m using Type.Missing\ninstead of\nSystem.Reflection.Missing.Value for\nconvenience. Roll on C#v4 where\noptional parameters will be\nsupported by the compiler!\n\nI\'ve not been able to compile or run this code, but I\'m pretty confident it\'ll work. Good luck!\n', ""\nThere's no need to use the excel com objects from C#. You can use OleDb to modify the sheets.\nhttp://www.codeproject.com/KB/office/excel_using_oledb.aspx\n"", '\nI was having a similar problem.\nI removed _worksheet and _workbook and all was well.\n']",https://stackoverflow.com/questions/1041266/c-sharp-and-excel-automation-ending-the-running-instance,automation
"PsExec Throws Error Messages, but works without any problems","
So we are using PsExec a lot in our automations to install virtual machines, as we can't use ps remote sessions with our windows 2003 machines. Everything works great and there are no Problems, but PsExec keeps throwing errors, even every command is being carried out without correctly. 
For example:
D:\tools\pstools\psexec.exe $guestIP -u $global:default_user -p $global:default_pwd -d -i C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -command ""Enable-PSRemoting -Force""

Enables the PsRemoting on the guest, but also throws this error message:
psexec.exe : 
Bei D:\Scripts\VMware\VMware_Module5.ps1:489 Zeichen:29
+     D:\tools\pstools\psexec.exe <<<<  $guestIP -u $global:default_user -p $global:default_pwd -d -i C:\Windows\System32\WindowsPowerShell\
v1.0\powershell.exe -command ""Enable-PSRemoting -Force""
+ CategoryInfo          : NotSpecified: (:String) [], RemoteException
+ FullyQualifiedErrorId : NativeCommandError

PsExec v1.98 - Execute processes remotely
Copyright (C) 2001-2010 Mark Russinovich
Sysinternals - www.sysinternals.com


Connecting to 172.17.23.95...Starting PsExec service on 172.17.23.95...Connecting with PsExec service on 172.17.23.95...Starting C:\Windows\
System32\WindowsPowerShell\v1.0\powershell.exe on 172.17.23.95...
C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe started on 172.17.23.95 with process ID 2600.

These kinds of error messages apear ALWAYS no matter how i use psexec, like with quotes, with vriables/fixed values, other flags, etc. Does anybody has an idea how i could fix this? It is not a real problem, but it makes finding errors a pain in the ass, because the ""errors"" are everywhere. Disabling the error messages of psexec at all would also help...
",21k,"
            11
        ","['\nThis is because PowerShell sometimes reports a NativeCommandError when a process writes to STDERR. PsExec writes the infoline\nPsExec v1.98 - Execute processes remotely\nCopyright (C) 2001-2010 Mark Russinovich\nSysinternals - www.sysinternals.com\n\nto STDERR which means it can cause this.\nFor more information, see these questions / answers:\n\nhttps://stackoverflow.com/a/1416933/478656\nhttps://stackoverflow.com/a/11826589/478656\nhttps://stackoverflow.com/a/10666208/478656\n\n', ""\nredirect stderr to null worked best for me. see below link\nError when calling 3rd party executable from Powershell when using an IDE\nHere's the relevant section from that link:\nTo avoid this you can redirect stderr to null e.g.:\ndu 2> $null\nEssentially the console host and ISE (as well as remoting) treat the stderr stream differently. On the console host it was important for PowerShell to support applications like edit.com to work along with other applications that write colored output and errors to the screen. If the I/O stream is not redirected on console host, PowerShell gives the native EXE a console handle to write to directly. This bypasses PowerShell so PowerShell can't see that there are errors written so it can't report the error via $error or by writing to PowerShell's stderr stream.\nISE and remoting don't need to support this scenario so they do see the errors on stderr and subsequently write the error and update $error.\n.\\PsExec.exe \\$hostname -u $script:userName -p $script:password /accepteula -h cmd /c $powerShellArgs 2> $null\n"", '\nI have created a psexec wrapper for powershell, which may be helpful to people browsing this question:\nfunction Return-CommandResultsUsingPsexec {\n    param(\n        [Parameter(Mandatory=$true)] [string] $command_str,\n        [Parameter(Mandatory=$true)] [string] $remote_computer,\n        [Parameter(Mandatory=$true)] [string] $psexec_path,\n        [switch] $include_blank_lines\n    )\n\n    begin {\n        $remote_computer_regex_escaped = [regex]::Escape($remote_computer)\n\n        # $ps_exec_header = ""`r`nPsExec v2.2 - Execute processes remotely`r`nCopyright (C) 2001-2016 Mark Russinovich`r`nSysinternals - www.sysinternals.com`r`n""\n\n        $ps_exec_regex_headers_array = @(\n            \'^\\s*PsExec v\\d+(?:\\.\\d+)? - Execute processes remotely\\s*$\',\n            \'^\\s*Copyright \\(C\\) \\d{4}(?:-\\d{4})? Mark Russinovich\\s*$\',\n            \'^\\s*Sysinternals - www\\.sysinternals\\.com\\s*$\'\n        )\n\n        $ps_exec_regex_info_array = @(\n            (\'^\\s*Connecting to \' + $remote_computer_regex_escaped + \'\\.{3}\\s*$\'),\n            (\'^\\s*Starting PSEXESVC service on \' + $remote_computer_regex_escaped + \'\\.{3}\\s*$\'),\n            (\'^\\s*Connecting with PsExec service on \' + $remote_computer_regex_escaped + \'\\.{3}\\s*$\'),\n            (\'^\\s*Starting .+ on \' + $remote_computer_regex_escaped + \'\\.{3}\\s*$\')\n        )\n\n        $bypass_regex_array = $ps_exec_regex_headers_array + $ps_exec_regex_info_array\n\n        $exit_code_regex_str = (\'^.+ exited on \' + $remote_computer_regex_escaped + \' with error code (\\d+)\\.\\s*$\')\n\n        $ps_exec_args_str = (\'""\\\\\' + $remote_computer + \'"" \' + $command_str)\n    }\n\n    process {\n        $return_dict = @{\n            \'std_out\' = (New-Object \'system.collections.generic.list[string]\');\n            \'std_err\' = (New-Object \'system.collections.generic.list[string]\');\n            \'exit_code\' = $null;\n            \'bypassed_std\' = (New-Object \'system.collections.generic.list[string]\');\n        }\n\n        $process_info = New-Object System.Diagnostics.ProcessStartInfo\n        $process_info.RedirectStandardError = $true\n        $process_info.RedirectStandardOutput = $true\n        $process_info.UseShellExecute = $false\n        $process_info.FileName = $psexec_path\n        $process_info.Arguments = $ps_exec_args_str\n\n        $process = New-Object System.Diagnostics.Process\n        $process.StartInfo = $process_info\n        $process.Start() | Out-Null\n\n        $std_dict = [ordered] @{\n            \'std_out\' = New-Object \'system.collections.generic.list[string]\';\n            \'std_err\' = New-Object \'system.collections.generic.list[string]\';\n        }\n\n        # $stdout_str = $process.StandardOutput.ReadToEnd()\n        while ($true) {\n            $line = $process.StandardOutput.ReadLine()\n            if ($line -eq $null) {\n                break\n            }\n            $std_dict[\'std_out\'].Add($line)\n        }\n\n        # $stderr_str = $process.StandardError.ReadToEnd()\n        while ($true) {\n            $line = $process.StandardError.ReadLine()\n            if ($line -eq $null) {\n                break\n            }\n            $std_dict[\'std_err\'].Add($line)\n        }\n\n        $process.WaitForExit()\n\n        ForEach ($std_type in $std_dict.Keys) {\n            ForEach ($line in $std_dict[$std_type]) {\n                if ((-not $include_blank_lines) -and ($line -match \'^\\s*$\')) {\n                    continue\n                }\n\n                $do_continue = $false\n                ForEach ($regex_str in $bypass_regex_array) {\n                    if ($line -match $regex_str) {\n                        $return_dict[\'bypassed_std\'].Add($line)\n                        $do_continue = $true\n                        break\n                    }\n                }\n                if ($do_continue) {\n                    continue\n                }\n\n                $exit_code_regex_match = [regex]::Match($line, $exit_code_regex_str)\n\n                if ($exit_code_regex_match.Success) {\n                    $return_dict[\'exit_code\'] = [int] $exit_code_regex_match.Groups[1].Value\n                } elseif ($std_type -eq \'std_out\') {\n                    $return_dict[\'std_out\'].Add($line)\n                } elseif ($std_type -eq \'std_err\') {\n                    $return_dict[\'std_err\'].Add($line)\n                } else {\n                    throw \'this conditional should never be true; if so, something was coded incorrectly\'\n                }\n            }\n        }\n\n        return $return_dict\n    }\n}\n\n']",https://stackoverflow.com/questions/18380227/psexec-throws-error-messages-but-works-without-any-problems,automation
"IE9, Automation server can't create object error while using CertEnroll.dll","
In my web page, a JS block like this:
var classFactory = new ActiveXObject(""X509Enrollment.CX509EnrollmentWebClassFactory"");

// Other initialize CertEnroll Objects

It works fine in windows7(32bit or 64bit) with IE8(32bit), as long as I change the IE8 secure setting, enable Initializing and Script ActiveX controls not marked as safe.
But when use IE9(32bit), I try anything I can find on web, it reports error ""Automation server can't create object.""
I even created a static html file, save it in my hard disk, and then open it with IE9(32bit), it worked fine. Then I put the html file on my web site, visit the html file with url, then it came up with the error message again.
I have worked on this problem for 4 days, any suggestion would be appreciated.
3Q. I hope you can read my words as I'm not an native English speaker.
",99k,"
            11
        ","['\na) Go to Tools-->Internet Options\nb) Select security tab\nc) Click on Trusted Sites (or Local Intranet depending on whether your site is trusted or not)\nd) Click on Custom Level\ne) Ensure that ""Initialize and script active x controls is not marked safe for scripting""  is enabled - this comes under Activex controls and plug-ins section towards 1/4th of the scroll bar.\nClick OK, OK.\nOnce this is completed, clear the browser cookies and cache. Close all your browser sessions. Reopen the IE to launch your site. \nTry to disable the setting in step (e) to see if the problem comes back - that should give more insight to the problem.\n']",https://stackoverflow.com/questions/15686040/ie9-automation-server-cant-create-object-error-while-using-certenroll-dll,automation
How to send input to the console as if the user is typing?,"
This is my problem. I have a program that has to run in a TTY, cygwin provides this TTY. When I redirect stdIn the program fails because it does not have a TTY. I cannot modify this program, and need some way of automating it. 
How can I grab the cmd.exe window and send it data and make it think the user is typing it? 
I'm using C#, I believe there is a way to do it with java.awt.Robot but I have to use C# for other reasons.
",13k,"
            7
        ","['\nI have figured out how to send the input to the console.  I used what Jon Skeet said.  I am not 100% sure this is the correct way to implement this.\nIf there are any comments on to make this better I would love to here.  I did this just to see if I could figure it out.\nHere is the program I stared that waited for input form the user\nclass Program\n{\n    static void Main(string[] args)\n    {\n        // This is needed to wait for the other process to wire up.\n        System.Threading.Thread.Sleep(2000);\n\n        Console.WriteLine(""Enter Pharse: "");\n\n        string pharse = Console.ReadLine();\n\n        Console.WriteLine(""The password is \'{0}\'"", pharse);\n\n\n        Console.WriteLine(""Press any key to exit. . ."");\n        string lastLine = Console.ReadLine();\n\n        Console.WriteLine(""Last Line is: \'{0}\'"", lastLine);\n    }\n}\n\nThis is the console app writing to the other one\nclass Program\n{\n    static void Main(string[] args)\n    {\n        // Find the path of the Console to start\n        string readFilePath = System.IO.Path.GetFullPath(@""..\\..\\..\\ReadingConsole\\bin\\Debug\\ReadingConsole.exe"");\n\n        ProcessStartInfo startInfo = new ProcessStartInfo(readFilePath);\n\n        startInfo.RedirectStandardOutput = true;\n        startInfo.RedirectStandardInput = true;\n        startInfo.WindowStyle = ProcessWindowStyle.Hidden;\n        startInfo.CreateNoWindow = true;\n        startInfo.UseShellExecute = false;\n\n        Process readProcess = new Process();\n        readProcess.StartInfo = startInfo;\n\n        // This is the key to send data to the server that I found\n        readProcess.OutputDataReceived += new DataReceivedEventHandler(readProcess_OutputDataReceived);\n\n        // Start the process\n        readProcess.Start();\n\n        readProcess.BeginOutputReadLine();\n\n        // Wait for other process to spin up\n        System.Threading.Thread.Sleep(5000);\n\n        // Send Hello World\n        readProcess.StandardInput.WriteLine(""Hello World"");\n\n        readProcess.StandardInput.WriteLine(""Exit"");\n\n        readProcess.WaitForExit();\n    }\n\n    static void readProcess_OutputDataReceived(object sender, DataReceivedEventArgs e)\n    {\n        // Write what was sent in the event\n        Console.WriteLine(""Data Recieved at {1}: {0}"", e.Data, DateTime.UtcNow.Ticks);\n    }\n}\n\n', '\nThis sounds like a task for SendKeys(). It\'s not C#, but VBScript, but nontheless - you asked for some way of automating it:\nSet Shell = CreateObject(""WScript.Shell"")\n\nShell.Run ""cmd.exe /k title RemoteControlShell""\nWScript.Sleep 250\n\nShell.AppActivate ""RemoteControlShell""\nWScript.Sleep 250\n\nShell.SendKeys ""dir{ENTER}""\n\n', '\nCan you start the program (or cygwin) within your code, using ProcessStartInfo.RedirectStandardInput (and output/error) to control the data flow?\n', '\nI had similar problem some time ago, cygwin should write some useful information (exact cygwin function, error text and WINAPI error code) to error stream, you should redirect it somewhere and read what it writes.\n']",https://stackoverflow.com/questions/451228/how-to-send-input-to-the-console-as-if-the-user-is-typing,automation
IE11 Frame Notification Bar Save button,"
On a 64-bit system with MS Excel 2010 and IE11 I'm using this code to automate download process from a website:  
hWnd = FindWindowEx(IE.hWnd, 0, ""Frame Notification Bar"", vbNullString)

If hWnd Then
    hWnd = FindWindowEx(hWnd, 0&, ""Button"", ""Save"")
End If

If hWnd Then
    SetForegroundWindow (hWnd)
    Sleep 600
    SendMessage hWnd, BM_CLICK, 0, 0
End If

Everything goes OK until the Frame Notification Bar appears. I'm getting the HWND of this window, but can't get the HWND of the ""Save"" button, so that I can send click to it.
",10k,"
            4
        ","['\nIf somebody is still trying to find the solution, for IE11 it\'s here.\nOn the very first line of the code of Vahagn Sargsyan above, instead of ""Frame Notification Bar"" get the exact title of the dialog box, which might be in English ""View Downloads - Internet Explorer"". This allows you to grab the right hWnd.\nBecause in IE11 there no more button accelerator to Save files, follow the solution posted here by pmr.\nFrom pmr code, just get the following lines:\nSet e = o.ElementFromHandle(ByVal h)\nDim iCnd As IUIAutomationCondition\nSet iCnd = o.CreatePropertyCondition(UIA_NamePropertyId, ""Save"")\n\nDim Button As IUIAutomationElement\nSet Button = e.FindFirst(TreeScope_Subtree, iCnd)\nDim InvokePattern As IUIAutomationInvokePattern\nSet InvokePattern = Button.GetCurrentPattern(UIA_InvokePatternId)\nInvokePattern.Invoke\n\nThis should solve your issue. This unlocked the situation for me with French localisation.\n', ""\nI'm assuming you're talking about that little frame that pops up at the bottom of IE giving you options to Open, Save or Cancel. If so, you might wanna check out another answer to a similar question asked here.\nA secondary solution would be a more complicated one (here), but works nonetheless. You'll have to import the modules from the workbook provided in this forum (you'll need to signup for membership though, but its free so just do it.) and that'll do basically what you need, albeit in a way that allows you more flexibility (choose filepath, filename, etc) and also a little more convoluted. \nEither way, hope I helped. \n""]",https://stackoverflow.com/questions/31489801/ie11-frame-notification-bar-save-button,automation
VBA:IE-How to assign pathname to file input tag without popup file upload form?,"
I am currently doing automaiton for file uploading
Below is HTML tag for input file tag:
 <input name=""file"" title=""Type the path of the file or click the Browse button to find the file."" id=""file"" type=""file"" size=""20"">

And below is button HTML Tag:
<input name=""Attach"" title=""Attach File (New Window)"" class=""btn"" id=""Attach"" onclick=""javascript:setLastMousePosition(event); window.openPopup('/widg/uploadwaiting.jsp', 'uploadWaiting', 400, 130, 'width=400,height=130,resizable=no,toolbar=no,status=no,scrollbars=no,menubar=no,directories=no,location=no,dependant=no', true);"" type=""submit"" value=""Attach File"">

My VBA coding is:
Dim filee As Object
Set filee = mydoc.getElementById(""file"")
filee.Value = filenamepath

Set attach = mydoc.getElementsByName(""Attach"")
attach(0).Click

When I am running this coding, input filepath box not assign path name so i am getting chose file path.
Find attach screenshot.
 
Finally i have tried below code but that send key not executing  
Dim filee As Object
    Set filee = mydoc.getElementById(""file"")
    filee.Click

obj.SetText filename
obj.PutInClipboard
SendKeys ""^v""
SendKeys ""{ENTER}""

Set attach = mydoc.getElementsByName(""Attach"")
    attach(0).Click

Set finall = mydoc.getElementsByName(""cancel"")
    finall(0).Click

Kindly tell me the windows API program to assign my file name directory in fine name: input box on opened Choose File to Open explorer and click the open button. 
",7k,"
            2
        ","['\nI fixed this issue by running external VBScript contain file path to set it on \'Choose File to Upload\' pop up window using SendKeys method after send Enter Key to close this pop up, and this run successfully because the extranl VBScript run on another process so it will not stuck on VBA code.\nNotes:\n1- I dynamically create the external VBScript from VBA code and save it on Temp folder after that I run this script using WScript.Shell.Run to excute it on another thread\n1- At the begining of the external VBScript I set 1 sec delay to be sure the \'Choose File to Upload\' pop up window already opened from VBA.\nAnd here is the complete code:\n....\n....\n\nSet filee = mydoc.getElementById(""file"")\n\n    CompleteUploadThread MyFilePath\n    filee.Foucs\n    filee.Click\n\n....\n....\n\nPrivate Sub CompleteUploadThread(ByVal fName As String)\n    Dim strScript As String, sFileName As String, wsh As Object\n    Set wsh = VBA.CreateObject(""WScript.Shell"")\n    \'---Create VBscript String---\n    strScript = ""WScript.Sleep 1000"" & vbCrLf & _\n                ""Dim wsh"" & vbCrLf & _\n                ""Set wsh = CreateObject(""""WScript.Shell"""")"" & vbCrLf & _\n                ""wsh.SendKeys """""" & fName & """""""" & vbCrLf & _\n                ""wsh.SendKeys """"{ENTER}"""""" & vbCrLf & _\n                ""Set wsh = Nothing""\n    \'---Save the VBscript String to file---\n    sFileName = wsh.ExpandEnvironmentStrings(""%Temp%"") & ""\\zz_automation.vbs""\n    Open sFileName For Output As #1\n    Print #1, strScript\n    Close #1\n    \'---Execute the VBscript file asynchronously---\n    wsh.Run """""""" & sFileName & """"""""\n    Set wsh = Nothing\nEnd Sub\n\n', '\nAs setting the value of a file input element is disabled due to security reasons, the ""send keys"" method seems to be the only option for automating file uploads using the IE API.\nI just stumbled over the same problem that the code after the Click does not seem to be executed - that is, unless the dialog is closed. This indicates that the Click method is blocking, making it impossible to interact with the dialog from within the macro.\nI could solve that by using a different method to open the dialog: by setting the focus to the file element with Focus, and sending the space key with SendKeys.\nIn your case, replace\nfilee.Click\n\nwith\nfilee.Focus\nSendKeys "" ""\n\n', '\nleemes\'s method(Sending  key to the file selection button on IE) is an easy way to automate the file selection procedure.\nIn addition, if IEObject.Visible sometimes fails to give focus to the IE Window,\n we\'d better send the IE window to the top-most position using Windows API before using \'SendKeys\' like following:\n#If VBA7 Then\n    Declare PtrSafe Function SetForegroundWindow Lib ""user32"" (ByVal hwnd As LongPtr) As LongPtr\n    Declare PtrSafe Function FindWindow Lib ""user32"" Alias ""FindWindowA"" (ByVal lpClassName As Any, ByVal lpWindowName As Any) As LongPtr\n#Else\n    Declare Function SetForegroundWindow Lib ""user32"" (ByVal hwnd As Long) As Long\n    Declare Function FindWindow Lib ""user32"" Alias ""FindWindowA"" (ByVal lpClassName As Any, ByVal lpWindowName As Any) As Long\n#End If\n\nSub Test()\n\n   \'first create or get IE object\n   Set IE= ...\n   ...\n\n   \'second, send IE window to the foreground\n   Dim TargetWnd\n   TargetWnd = FindWindow(""IEFrame"", vbNullString)  \'find IE window\n   If TargetWnd = 0 Then Debug.Print ""Window not found."" \'Else Debug.Print TargetWnd\n   SetForegroundWindow (TargetWnd)\n\n   \'sendkeys\n   set filee = getElement....\n   filee.Focus\n   SendKeys "" ""     \'send Space key instead of .Click method\n   SendKeys ""filePath""     \' ""C:\\path\\filename""  \' Type-in the filename \n   SendKeys ""{Enter}""    \'closes the file dialog\n\n   \'finally submit       \n   ...\n   ...\n\nend Sub\n\n']",https://stackoverflow.com/questions/34803863/vbaie-how-to-assign-pathname-to-file-input-tag-without-popup-file-upload-form,automation
How can you automatically remove trailing whitespace in vim,"
I am getting 'trailing whitespace' errors trying to commit some files in Git.
I want to remove these trailing whitespace characters automatically right before I save Python files.
Can you configure Vim to do this? If so, how?
",75k,"
            219
        ","[""\nI found the answer here.\nAdding the following to my .vimrc file did the trick:\nautocmd BufWritePre *.py :%s/\\s\\+$//e\n\nThe e flag at the end means that the command doesn't issue an error message if the search pattern fails. See :h :s_flags for more.\n"", ""\nCompilation of above plus saving cursor position:\nfunction! <SID>StripTrailingWhitespaces()\n  if !&binary && &filetype != 'diff'\n    let l:save = winsaveview()\n    keeppatterns %s/\\s\\+$//e\n    call winrestview(l:save)\n  endif\nendfun\n\nautocmd FileType c,cpp,java,php,ruby,python autocmd BufWritePre <buffer> :call <SID>StripTrailingWhitespaces()\n\nIf you want to apply this on save to any file, leave out the second autocmd and use a wildcard *:\nautocmd BufWritePre,FileWritePre,FileAppendPre,FilterWritePre *\n  \\ :call <SID>StripTrailingWhitespaces()\n\n"", ""\nI also usually have a :\n\nmatch Todo /\\s\\+$/\n\nin my .vimrc file, so that end of line whitespace are hilighted.\nTodo being a syntax hilighting group-name that is used for hilighting keywords like TODO, FIXME or XXX. It has an annoyingly ugly yellowish background color, and I find it's the best to hilight things you don't want in your code :-)\n"", ""\nI both highlight existing trailing whitespace and also strip trailing whitespace.\nI configure my editor (vim) to show white space at the end, e.g.\n\nwith this at the bottom of my .vimrc:\n\nhighlight ExtraWhitespace ctermbg=red guibg=red\nmatch ExtraWhitespace /\\s\\+$/\nautocmd BufWinEnter * match ExtraWhitespace /\\s\\+$/\nautocmd InsertEnter * match ExtraWhitespace /\\s\\+\\%#\\@<!$/\nautocmd InsertLeave * match ExtraWhitespace /\\s\\+$/\nautocmd BufWinLeave * call clearmatches()\n\nand I 'auto-strip' it from files when saving them, in my case *.rb for ruby files, again in my ~/.vimrc\nfunction! TrimWhiteSpace()\n    %s/\\s\\+$//e\nendfunction\nautocmd BufWritePre     *.rb :call TrimWhiteSpace()\n\n"", ""\nHere's a way to filter by more than one FileType.\n\nautocmd FileType c,cpp,python,ruby,java autocmd BufWritePre <buffer> :%s/\\s\\+$//e\n\n"", '\nI saw this solution in a comment at \nVIM Wikia - Remove unwanted spaces\nI really liked it. Adds a . on the unwanted white spaces.\n\nPut this in your .vimrc\n"" Removes trailing spaces\nfunction TrimWhiteSpace()\n  %s/\\s*$//\n  \'\'\nendfunction\n\nset list listchars=trail:.,extends:>\nautocmd FileWritePre * call TrimWhiteSpace()\nautocmd FileAppendPre * call TrimWhiteSpace()\nautocmd FilterWritePre * call TrimWhiteSpace()\nautocmd BufWritePre * call TrimWhiteSpace()\n\n', '\nCopied and pasted from http://blog.kamil.dworakowski.name/2009/09/unobtrusive-highlighting-of-trailing.html (the link no longer works, but the bit you need is below)\n""This has the advantage of not highlighting each space you type at the end of the line, only when you open a file or leave insert mode. Very neat.""\n\nhighlight ExtraWhitespace ctermbg=red guibg=red\nau ColorScheme * highlight ExtraWhitespace guibg=red\nau BufEnter * match ExtraWhitespace /\\s\\+$/\nau InsertEnter * match ExtraWhitespace /\\s\\+\\%#\\@<!$/\nau InsertLeave * match ExtraWhiteSpace /\\s\\+$/\n\n', '\nThis is how I\'m doing it. I can\'t remember where I stole it from tbh.\n\nautocmd BufWritePre * :call <SID>StripWhite()\nfun! <SID>StripWhite()\n    %s/[ \\t]\\+$//ge\n    %s!^\\( \\+\\)\\t!\\=StrRepeat(""\\t"", 1 + strlen(submatch(1)) / 8)!ge\nendfun\n\n', ""\nA solution which simply strips trailing whitespace from the file is not acceptable in all circumstances. It will work in a project which has had this policy from the start, and so there are no such whitespace that you did not just add yourself in your upcoming commit.\nSuppose you wish merely not to add new instances of trailing whitespace, without affecting existing whitespace in lines that you didn't edit, in order to keep your commit free of changes which are irrelevant to your work.\nIn that case, with git, you can can use a script like this:\n#!/bin/sh\n\nset -e # bail on errors\n\ngit stash save commit-cleanup\ngit stash show -p | sed '/^\\+/s/ *$//' | git apply\ngit stash drop\n\nThat is to say, we stash the changes, and then filter all the + lines in the diff to remove their trailing whitespace as we re-apply the change to the working directory. If this command pipe is successful, we drop the stash.\n"", ""\nThe other approaches here somehow didn't work for me in MacVim when used in the .vimrc file. So here's one that does and highlights trailing spaces:\nset encoding=utf-8\nset listchars=trail:·\nset list\n\n"", '\nFor people who want to run it for specific file types (FileTypes are not always reliable):\nautocmd BufWritePre *.c,*.cpp,*.cc,*.h,*.hpp,*.py,*.m,*.mm :%s/\\s\\+$//e\n\nOr with vim7:\nautocmd BufWritePre *.{c,cpp,cc,h,hpp,py,m,mm} :%s/\\s\\+$//e\n\n', '\nIf you trim whitespace, you should only do it on files that are already clean.  ""When in Rome..."".  This is good etiquette when working on codebases where spurious changes are unwelcome.  \nThis function detects trailing whitespace and turns on trimming only if it was already clean.\nThe credit for this idea goes to a gem of a comment here:  https://github.com/atom/whitespace/issues/10  (longest bug ticket comment stream ever)\nautocmd BufNewFile,BufRead *.test call KarlDetectWhitespace()\n\nfun! KarlDetectWhitespace()\npython << endpython\nimport vim\nnr_unclean = 0\nfor line in vim.current.buffer:\n    if line.rstrip() != line:\n        nr_unclean += 1\n\nprint ""Unclean Lines: %d"" % nr_unclean\nprint ""Name: %s"" % vim.current.buffer.name\ncmd = ""autocmd BufWritePre <buffer> call KarlStripTrailingWhitespace()""\nif nr_unclean == 0:\n    print ""Enabling Whitespace Trimming on Save""\n    vim.command(cmd)\nelse:\n    print ""Whitespace Trimming Disabled""\nendpython\nendfun\n\nfun! KarlStripTrailingWhitespace()\n    let l = line(""."")\n    let c = col(""."")\n    %s/\\s\\+$//e\n    call cursor(l, c)\nendfun\n\n', ""\nautocmd BufWritePre *.py execute 'norm m`' | %s/\\s\\+$//e | norm g``\n\nThis will keep the cursor in the same position as it was just before saving\n"", ""\nautocmd BufWritePre * :%s/\\s\\+$//<CR>:let @/=''<CR>\n""]",https://stackoverflow.com/questions/356126/how-can-you-automatically-remove-trailing-whitespace-in-vim,automation
WatiN or Selenium? [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



I'm going to start coding some automated tests of our presentation soon. It seems that everyone recommends WatiN and Selenium. Which do you prefer for automated testing of ASP.NET web forms? Which of these products work better for you?
As a side note, I noticed that WatiN 2.0 has been in CTP since March 2008, is that something to be concerned about?
",48k,"
            149
        ","[""\nI'm currently working hard on a beta release of WatiN 2.0 somewhere in Q1 of 2009. It will be a major upgrade to the current CTP 2.0 versions and will basically give you the same functionality to automate FireFox and IE as version 1.3.0 offers for automating IE.\nSo no concerns there.\nJeroen van Menen\nLead dev WatiN\n"", '\nIf you\'re looking to make a serious long-term investment in a framework that will continue to be improved and supported by the community, Selenium is probably your best bet. For example, I just came across this info on Matt Raible\'s blog:\n\nAs of Friday, Google has over 50 teams\n  running over 51K tests per day on\n  internal Selenium Farm. 96% of these\n  tests are handled by Selenium RC and\n  the Farm machines correctly. The other\n  4% are partly due to RC bugs, partly\n  to test errors, but isolating the\n  cause can be difficult. Selenium has\n  been adopted as the primary technology\n  for functional testing of web\n  applications within Google. That\'s the\n  good news.\n\nI also went to one of the Selenium meetups recently and learned that Google is putting serious resources into improving Selenium and integrating it with WebDriver, which is an automated testing tool developed by Simon Stewart. One of the major advantages of WebDriver is that it controls the browser itself rather than running inside the browser as a Javascript application, which means that major stumbling blocks like the ""same origin"" problem will no longer be an issue. \n', ""\nWe've tested both and decided to go with WaTiN.  As others have pointed out, Selenium does have some nice features not found in WaTiN, but we ran into issues getting Selenium working and once we did it was definitely slower when running tests than WaTiN. If I remember correctly, the setup issues we ran into stemmed from the fact that Selenium had a separate app to control the actual browser where WaTiN did everything in process.\n"", '\nI\'ve been trying \'em both out and here are my initial thoughts...\n\nWatiN\nThe Good\n\nFast execution.\nScript creation tools are independent projects; there are 2 that I know of: Wax (Excel based, hosted on CodePlex) and WatiN Test Record (hosted on SourceForge). Neither is as robust as Selenium IDE.\nVery good IE support.  Can attach and detach to/from running instances. Can access native window handles etc. (See script example below).\nNuGet packaged, easy to get running in .NET, Visual Studio style environments and keep updated.\n\nThe Bad\n\nGoogling WatiN (watin xyz) often causes Google to recommend ""watir xyz"" instead. Not that much documentation out there. \nWhat little there is (documentation), it is confusing; for example: at first blush it would appear that there is no native support for CSS selectors. Especially since there are extensions libraries like \'WatiNCssSelectorExtensions\' and many blog articles about alternative techniques (such as injecting jQuery/sizzle into the page). On Stack Overflow, I found a comment by Jeroen van Menen which suggests that there is native support. At least the lead-developer spends time on Stack Overflow :)\nNo native XPath support.\nNo out-of-the-box remote execution/grid based execution.\n\nScript Example (C#). You can\'t do this with Selenium (not that I know off, at least):\nclass IEManager\n{\n    IE _ie = null;\n    object _lock = new object();\n\n    IE GetInstance(string UrlFragment)\n    {\n        lock (_lock)\n        {\n            if (_ie == null)\n            {\n                var instances = new IECollection(true);  //Find all existing IE instances\n                var match = instances.FirstOrDefault(ie=>ie.Url.Contains(UrlFragment));\n                _ie = match ?? new IE();\n                if (match==null)  //we created a new instance, so we should clean it up when done!\n                    _ie.AutoClose = true;\n            }\n        }\n\n        return _ie;\n    }\n}\n\n\nSelenium\n\nSlower than WatiN (especially since a new process has to be created).\nBuilt-in CSS selectors/XPath support.\nSelenium IDE is good (can\'t say great, but it’s the best in class!).\nFeels more Java-ish than .NET-ish...but really, it\'s programming language agnostic; all commands are sent to an out-of-process \'Driver\'. The driver is really a \'host\' process for the browser instance. All communication must be serialised in/out across process boundaries, which might explain the speed issues relative to WatiN.\nDecoupled processes - ""Driver"" and ""Control""  mean more robustness, more complexity, etc., but also easier to create grids/distributed test environments. Would have really liked it if the ""distribution"" mechanism (i.e. the communication between Driver & Control) were across WebSphere or other existing, robust, message queue manager.\nSupport chrome and other browsers out of the box.\n\nDespite everything, I went with WatiN in the end; I mainly intend to write small screen-scraping applications and want to use LINQPad for development. Attaching to a remote IE instance (one that I did not spawn myself) is a big plus. I can fiddle around in an existing instance...then run a bit of script...then fiddle again etc. This is harder to do with Selenium, though I suppose ""pauses"" could be embedded in the script during which time I could fiddle directly with the browser.\n', ""\nThe biggest difference is that Selenium has support for different browsers (not just IE or FF, see http://seleniumhq.org/about/platforms.html#browsers.\nAlso, Selenium has a remote control server (http://seleniumhq.org/projects/remote-control/), which means that you don't need to run the browser on the same machine the test code is running. You can therefore test your Web app. on different OS platforms.\nIn general I would recommend using Selenium. I've used WatiN few years ago, but I wasn't satisfied with its stability (it has probably improved by now). The biggest plus for Selenium for me is the fact that you can test the Web app. on different browsers.\n"", '\nNeither. Use Coypu. It wraps Selenium. Much more durable. https://github.com/featurist/coypu\nUpdate\nYe Oliver you\'re right. Ok why\'s it better?\nPersonally I\'ve found the Selenium driver for IE in particular to be very fragile - there\'s a number of \'standard\' driver exceptions that I\'ve time again found when driving Selenium for Unit Tests on ajax heavy websites.\nDid I mention I want to write my scripts in c# as a Test Project ? Yes Acceptance Tests within a continous build deployment.\nWell Coypu deals with the above. It\'s a wrapper for Selenium that allows test fixtures such as,\nbrowser.Visit(""file:///C:/users/adiel/localstuff.htm"")\nbrowser.Select(""toyota"").From(""make"");\nbrowser.ClickButton(""Search"");\n\n... which will spin up a (configurable brand of) browser and run the script. It works great with scoped regions and is VERY extendable.\nThere\'s more examples at GitHub and as Olvier below mentions, Adrian\'s video is excellent. I think it\'s the best way to drive browser based tests in the .Net world and tries to follow it\'s Ruby namesake capybara\n', ""\nI've used both, they both seem to work ok.  My nod is for Selenium as it seemed to have better Ajax support.  I believe WaTiN has matured though since last I used it so it should have the same thing.\nThe biggest thing would be which development environment do you like to be in?  Selenium and Watin have recorders but Selenium is in the browser and watin is in visual studio. + and -'s to both of those.\n"", '\nUntil now we are a pure Microsoft Shop for delivering solutions for the enterprise and went with WatiN. This may change in the future.\nAs a more recent source:\nMicrosoft printed in MSDN Magazine 12/2010 a BDD-Primer with the combination of SpecFlow with WatiN (cool BDD-Behavior Driven Development). Its author Brandon Satrom (msft Developer Evangelist) has also posted in December 2010 a Video Webcast teaching in detail 1:1 his above findings.\nThere is a Whitepaper from 04/2011 on Supporting ATDD/BDD with SpecLog, SpecFlow and Team Foundation Server (Acceptance Test Driven Development/Behavior Driven Development) from Christian Hassa, whose team built SpecFlow.\n', ""\nI use Watin, but haven't used Selenium.  I can say I got up and running quickly on Watin and have had few to no problems.  I can't think of anything I have wanted to do that I couldn't figure out with it.  HTH\n"", '\nI generally use Selenium, mainly because I like the Selenium IDE plugin for FireFox for recording starting points for my tests.\n', ""\nI recommend WebAii since that's what I've had any success with and when using it my gripes were few. I never tried Selenium and I don't remember using WaTiN much, at least not to the point where I could get it to succesfully work. I don't know of any framework that deals with Windows dialogs gracefully, although WebAii has an interface for implementing your own dialog handlers.\n"", ""\nI considered using both. I used the recorder for Selenium to build some tests in FF. I tried to do the same in Watin and found that the Watin Recorder (2.0.9.1228) is completely worthless for our sites. It appeared to be rendering the site in IE6 -- making our site effectively unusable for recording. We don't support IE6. I couldn't find any way to change the browser it is using. I only found one Watin Recorder out there. If there's more than one, or one that is kept up to date, please comment. \nThe Selenium Recorder IDE for Firefox is simple to use and ports tests to C#. It isn't great at this. I couldn't get porting test suites to work, despite reading a blog post or two that had workarounds. So there's a bit of manipulation of the generated code. Still, it works 90% and that's better than the alternative. \nFor my money/time, Selenium is superior just for the ease of building new tests. IE doesn't have any good developer toolbars that are anywhere near as good as Firebug, so I'm doing my development in Firefox to begin with, so having a good working recorder in Firefox is a huge bonus. \nMy conclusion here was a lot like that democracy quote by Churchill: Selenium is the worst form of automated UI testing. Except for all the other ones. \n"", ""\nAt the risk of going off on a tangent, I'd recommend Axe/WatiN.  Axe allows tests to be written in Excel by 'Manual' Testers with no knowledge of the underlying test 'language'.  It does need a 'Technician' to write the bespoke actions (IE. Today I had to do a slightly complex Table lookup & cross-reference) but once written the actions can be used in tests by the non-techy testers.\nI also heard that the UK Government Gateway project (which I believe has 6K+ tests automated tests) recently ported all their tests from Axe/Winrunner to Axe/Watin within a week!! And many of the tests are pretty complex - I know as I worked on it a few years ago...\nI'm looking at Selenium at the moment, as a potential Client uses it.  But i do suggest a wee look at Axe as a layer above the 'work horse' tool.\n"", ""\nIf you have to access iframes, modal dialogs and cross domain iframes WatiN is a way to go.  Selenium couldn't handle the iframes it was throwing commandtimeout exceptions.  WatiN you could do lot more things especially if the website uses IE specific stuff like ShowModalDialog etc.. WatiN handles all of them very well.  I could even do cross domain iframe access.\n"", '\nYou will have to do both if you need to do IE and FF testing, but they are only going to work so well for presentation testing. They cant detect if one element is slightly off, just that the elements are present. I dont know of anything that can replace the human eye for UI / presentation testing, though you could do a few things to assist it (take screenshots of the pages at each step for users to review). \n']",https://stackoverflow.com/questions/417380/watin-or-selenium,automation
find and delete file or folder older than x days,"
I want to delete file and folder older than 7 days so I tried 
[17:07:14 root@client01.abc.com:~]# find /tmp/ -mindepth 1 -maxdepth 1 -ctime +7 -exec ls -l {} \;

So when I run find /tmp/ -mindepth 1 -maxdepth 1 -ctime +7 -exec ls -l {} \; it doesnt show any dir, but for find /tmp/ -mindepth 1 -maxdepth 2 -ctime +7 -exec ls -l {} \; it does show few files in subdir.
Whats is the right way to delete files/folders older than 7 days in one specific dir ?
",127k,"
            37
        ","['\nYou can make use of this piece of code\nfind /tmp/* -mtime +7 -exec rm {} \\;\n\nExplanation\n\nThe first argument is the path to the files. This can be a path, a directory, or a wildcard as in the example above. I would recommend using the full path, and make sure that you run the command without the exec rm to make sure you are getting the right results.\n\nThe second argument, -mtime, is used to specify the number of days old that the file is. If you enter +7, it will find files older than 7 days.\n\nThe third argument, -exec, allows you to pass in a command such as rm. The {} \\; at the end is required to end the command.\n\n\nSource : http://www.howtogeek.com/howto/ubuntu/delete-files-older-than-x-days-on-linux/\nFor deleting folders, after emptying inside of them you can rmdirinstad of rm in the piece of code, also if you only want to see directories you can add\n-type d\n\nto piece of code such as below:\nfind /tmp/*/* -mtime +7 -type d -exec rmdir {} \\;\n\n', '\nEasier to just do \nfind /tmp/* -mtime +7 -exec rm -rf {} \\; \n\nWhich will del files and dirs\n', '\nmy easy way:\nfind /tmp/* -daystart -mtime +7 -delete\n\nthe daystart option measure times from the beginning of today rather than from 24 hours ago\nref: official_doc\n', '\nfind /tmp/* -mtime +7 -type f -exec rm {} \\;\n\nRemove files.\nfind /tmp/ -empty -type d -delete\n\nRemove empty directories.\n']",https://stackoverflow.com/questions/31389483/find-and-delete-file-or-folder-older-than-x-days,automation
What is the best automated website UI testing framework [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 9 years ago.



What are the good automated web UI testing tools?
I want to be able to use it in the .Net world - but it doesn't have to written in .net.
Features such as a record mode, integration into build process\ continuous integration would be nice.
Im going to look at:

Watir
Selenium

Are there any others I should look at?
",22k,"
            34
        ","[""\nI definitively recommend Selenium, you can use it from .NET, supports different browsers, works in automatic builds and CI processes (we use it from CCNet). The code is stable. It has a few quirks, but after all they all do.\nWhichever tool you choose, I recommend making your own test facade class(es) around it. The facade should be designed to suite your concrete testing needs, without exposing too much the details of the testing tool's API. This will make the test code easier to write and maintain.\nUPDATE: if you use ASP.NET view state in your app, you could have problems using a pure HTTP test tool. This is where browser-controllers (like Selenium) are much better.\n"", '\nWatiN\nAutomates FF and IE\n[Test] \npublic void SearchForWatiNOnGoogle()\n{\n using (IE ie = new IE(""http://www.google.com""))\n {\n  ie.TextField(Find.ByName(""q"")).TypeText(""WatiN"");\n  ie.Button(Find.ByName(""btnG"")).Click();\n\n  Assert.IsTrue(ie.ContainsText(""WatiN""));\n }\n}\n\nhttp://watin.sourceforge.net/\n', '\nWatin is pretty unstable to use it in serious projects. It often fails with unexpected reasons like ""EI is busy"" or something like ""Error with COM object"". \nSelenium is much more stable and it already has a lot of supporting tools. For example Selenium GRID is a solution which allows significantly decrease run time of tests. (Our smoke tests on Watin takes 6 hours to run).\n', '\nCurrently in my job i use QTP and it so far atleast can handle pretty much anything we throw at it both on the UI and it has a special mode for testing non gui services allowing us to check both and help us narrow down where some problems occur when we change the system. It is in my opinion very configurable and the inclusion of vbscript as its language allows integration with lots and lots of things on windows to allow you  to do pretty much anything you want! For instance we use it to control the excel com object to make custom excel reports of success and failure so the format of the results is the same wether a test was run manually and also on another project used the adodb object to check that when a page submits information to the database that the database contains the correct data for that record!\nAs for integration into the build process i have not tried this myself but it is possible to launch qtp and  a test from a vbs file so i would assume this should be fairly trvial as ms tools tend to allow you to run vbs files pretty easily  from most tools.\nI would reccomend it to anyone assuming you can get someone to buy the license!\n', '\nYou can also try VSTT - http://blogs.msdn.com/b/slumley/archive/2009/05/28/vsts-2010-feature-enhancements-for-web-test-playback-ui.aspx\nTelerik Test Tools - http://www.telerik.com/automated-testing-tools.aspx\nVisual Studio UI Test Extensibility–Scenarios & Guiding Principles - http://blogs.msdn.com/b/mathew_aniyan/archive/2011/03/28/visual-studio-ui-test-extensibility-scenarios-amp-guiding-principles.aspx\nVSTS Web Test Step-by-Step Primer - http://blogs.msdn.com/b/jimmymay/archive/2009/02/23/vsts-web-test-step-by-step-primer-7-minute-video-by-microsoft-a-c-e-performance-engineer-chris-lundquist-with-copious-notes-screen-shots-from-your-humble-correspondent.aspx\n', ""\nyou might also be interested in taking a look at what the ASP.NET team cooked up itself: Lightweight Test Automation Framework.\nThere's also a dedicated forum for it.\n"", ""\nHaving used several different automated testing solutions (TestComplete, QTP, etc), I have to put a vote in for Telerik + Visual Studio.  Telerik has great support forums, and is very compatible with whatever testing framework you come up with.  Our Devs put unique IDs into their HTML code so that our scripts don't need to be rewritten even with pretty drastic UI refactors.  It's definitely more challenging than record and playback, but once you have your unique IDs in place, the automation code requires little or no maintenance.\n"", '\nTry httpunit\n', ""\nDepend on what you would like to achieve. \nYou can use web test built in the Visual Studio Tester Edition. It's quite good and easy to automate. You can use external data as a test data source and it integrates nicely with VS.\nThere is also test tool by Automated QA (forgot the name) which looks good but expensive.\nAnd there is Selenium. That's the one we are using in Symantec. The biggest advantage is that it actually uses a browser you want to test. VS mimic a browser by changing http request parameters only so you may not be able to test your site for cross-browser compatibility. Selenium on the other hand uses browser and automates it so you can actually test your site in IE, Firefox etc. It can be also integrated with VS unit tests so you can see test results in VS.\nSo I would recommend Selenium or VS.\n"", ""\nI've used Selenium. The features were good, and it was usable but it was buggy. \nThe IDE would often record events incorrectly (so tests would need to be manually changed), and test files occasionally became completely unusable for no apparent reason, which meant they would have to be recreated all over again. Also development on Selenium IDE seems to have stopped; there hasn't been any bug fixes and patches for a while, and bug reports seem to go unnoticed. \nMolybdenum is an alternative, built on Selenium that's worth looking into.\nhttp://www.molyb.org/\n"", ""\nJust to throw out another option (of which I haven't tried but I do like Telerik) is Telerik's new WebUI Testing Studio.  I will also echo Selenium up votes.\n"", '\nI forget one nice tools and can find link on it but find this ... http://weblogs.asp.net/bsimser/archive/2008/02/21/automated-ui-testing-with-project-white.aspx maybe can help.\n', '\nIf you are looking for simple, cross-browser tool with record and playback, multithreaded playback, intergration with build processes, powerful scripting, good reporting and excellent support, go for Sahi. It will be much easier for your testers/devs to learn and maintain.\n', '\nyou might want to take in consideration near Selenium also Rational Functional Tester ! whether you are familiar with coding in .Net or Java and want to just play around with record & replay or want to create more sophisticated programmatic testing I would recommend it.\n', '\nWebDriver is another possibility: http://code.google.com/p/webdriver\nThey are working on a .NET wrapper that may be interesting for you.\n', '\nTry QEngine. It has all the features of QTP.\n', '\nYou may want to look at RIATest for cross-platform cross-browser testing of web applications. \nIt works on Windows and Mac, supported browsers are Firefox, IE and Chrome. Automated testing scripts written on one platform/browser can be run against all other supported platforms/browsers.\nIt has the features that you want: user interaction recording mode and integration with CI servers (outputs results in JUnit format which can be consumed by CI servers such as Hudson).\n(Disclaimer: I am a RIATest team member).\n']",https://stackoverflow.com/questions/805910/what-is-the-best-automated-website-ui-testing-framework,automation
What are the shortcut to Auto-generating toString Method in Eclipse?,"
Is it good or bad practice auto-generating toString methods for some simple classes?
I was thinking of generating something like below where it takes the variable names and produces a toString method that prints the name followed by its value.
private String name;
private int age;
private double height;

public String toString(){
   return String.format(""Name: %s Age: %d Height %f"", name, age, height);
}

",43k,"
            30
        ","[""\nEclipse 3.5.2 (and possibly earlier versions) already provides this feature. If you right-click within the editor, you'll find it under Source -> Generate toString()...\nTo answer your question about whether it's a bad practice to autogenerate toString(), my opinion is that it is not. If the generated code is very similar to the code you would have written yourself, then why bother typing it out?\n"", '\nI personally like to implement a toString method for all objects, as it helps in debugging.\nI would look into using the Apache Commons ToStringBuilder.\nYou can implement a simple toString method using reflection as follows:\npublic String toString() {\n   return ToStringBuilder.reflectionToString(this);\n}\n\nUsing this method, you will not have to update your toString method if/when fields are added.\n', ""\nIf you use lombok they have a @ToString annotation which will generate the toString for you.\nThe reason why this is much better to use instead of generating toString with eclipse for instance is that if you later add,remove or change attributes of the class, you will also have to regenerate the toString. If you use lombok you don't have to do that.\n"", '\nTo add to Steve\'s and Don\'s answers (+1 for them) :\nMake your toString() method simple, make sure it nevers triggers expections (especially be aware of fields that could be null). \nIf possible, don\'t call other methods of your class. At least, be sure that your toString() method doesn\'t modify your object. \nAnd be aware of silly exception-toString loops:\npublic class MyClass { \n       ... \n       public String toString() { \n          // BAD PRACTICE 1: this can throw NPE - just use field1\n            return "" field1="" + field1.toString() \n                + "" extraData="" + getExtraData();\n          // BAD PRACTICE 2: potential exception-toString loop\n       }\n\n       public MyExtraData getExtraData() {\n           try { \n           .... do something\n           } catch(Exception e) {\n              throw new RuntimeException(""error getting extradata - "" + this.toString(),e);\n           }\n\n       }\n\n}\n\n', ""\nIn IntelliJ Idea you can press alt+insert, the Generate popup will open; now select the fields and click the OK button; that's it.\n\n\n\nFurther tip: In the Generate toString dialog, it gives you a choice to select the template by clicking the drop down on the template combo box. Here you can select StringBuffer if you need to or any other template as required. Play with it to get accustomed. I like it :)\n"", '\n\nShortcut to generate toString() method\n\n\nPress Alt + Shift + S + S (double)\nRight click -> Source -> Generate toString() ...\nGo to Source menu -> Generate toString() ...\nGo to Windows menu -> Preferences -> General -> Keys (Write Generate toString on text field)\n\n', ""\nBe clear when adding toString() as to the audience of the generated text.  Some frameworks use the toString() method to generate end user visible text (e.g. certain web frameworks), whereas many people use toString() methods to generate debugging / developer information.  Either way, make sure that you have enough uniqueness in the toString implementation to satisfy your requirements.\nThe default JDK implementation of toString() generates developer info, so that's usually the pattern I recommend if possible, but if you are working on a project with a different idea / expectation you could wind up confused...\n"", '\nJust noticed -In NetBeans IDE you can generate toString() method by selecting fields you want to generate it for right click->insert code or use shortcut ALT+INSERT and then select toString().\nWay it looks is :\n@Override\npublic String toString() {\n    return ""ClassName{""+""fieldName=""+fieldName+\'}\';\n}\n\nIts great way to debug and no need for additional libs.\n', '\nConsidering some old answers including @Steve\'s, I\'d like to add answer as per latest library. \nAdd dependency\n        <dependency>\n            <groupId>org.apache.commons</groupId>\n            <artifactId>commons-lang3</artifactId>\n            <version>3.10</version>\n        </dependency>\n\nIn your class \nimport org.apache.commons.lang3.builder.ReflectionToStringBuilder;\n\npublic class User {\n     ... \n\n     @Override\n     public String toString() {\n          return ReflectionToStringBuilder.toString(this);\n     }\n}\n\nYou can exclude certain fields as below \n    ... \n    @Override\n    public String toString() {\n        return ReflectionToStringBuilder.toStringExclude(this, ""name""); // Name will be excluded from toString output \n    }\n\n']",https://stackoverflow.com/questions/2653268/what-are-the-shortcut-to-auto-generating-tostring-method-in-eclipse,automation
How to add application to Azure AD programmatically?,"
I want to automate the creation of my application in Azure AD and get back the client id generated by Azure AD.
Are there PowerShell commandlets to do this? Is there some other means, like an API of doing this besides the management console?
Can you point me to an example?
Thanks!
",22k,"
            29
        ","['\n', '\n', '\n', '\n']",https://stackoverflow.com/questions/31684821/how-to-add-application-to-azure-ad-programmatically,automation
"Automation Google login with python and selenium shows """"This browser or app may be not secure""""","
I've tried login with Gmail or any Google services but it shows the following ""This browser or app may not be secure"" message:

I also tried to do options like enable less secure app in my acc but it didn't work.
then I made a new google account and it worked with me. but not with my old acc.

how can i solve this ? 
How can i open selenium in the normal chrome browser not the one controlled by automated software
?

This is my code

    from selenium.webdriver import Chrome
    from selenium.webdriver.chrome.options import Options


    browser = webdriver.Chrome()
    browser.get('https://accounts.google.com/servicelogin')
    search_form = browser.find_element_by_id(""identifierId"")
    search_form.send_keys('mygmail')
    nextButton = browser.find_elements_by_xpath('//*[@id =""identifierNext""]') 
    search_form.send_keys('password')
    nextButton[0].click() 

",26k,"
            23
        ","['\nFirst of all don\'t use chrome and chromedriver. You need to use Firefox.(if not installed) Download and install Firefox. Login to Google with normal Firefox.\nYou need to show the Google site that you are not a robot. You can use code like this:\nfrom selenium import webdriver\nimport geckodriver_autoinstaller\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n\ngeckodriver_autoinstaller.install()\n\nprofile = webdriver.FirefoxProfile(\n    \'/Users/<user name>/Library/Application Support/Firefox/Profiles/xxxxx.default-release\')\n\nprofile.set_preference(""dom.webdriver.enabled"", False)\nprofile.set_preference(\'useAutomationExtension\', False)\nprofile.update_preferences()\ndesired = DesiredCapabilities.FIREFOX\n\ndriver = webdriver.Firefox(firefox_profile=profile,\n                           desired_capabilities=desired)\n\nThis can help you find your profile location.\nBut, why Firefox?\nActually there is only one reason, chromedriver was coded by Google.\nThey can easily understand if it is a bot or not. But when we add user data with Firefox, they cannot understand if there is a bot or not.\nYou can fool Google like this. It worked for me too. I tried very hard to do this. Hope it will be resolved in you too.\n', '\nThis is working for me. I found the solution from GitHub.\n   from selenium import webdriver\n   from selenium_stealth import stealth\n\n   options = webdriver.ChromeOptions()\n   options.add_argument(""user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"")\n   options.add_experimental_option(""excludeSwitches"", [""enable-automation""])\n   options.add_experimental_option(\'useAutomationExtension\', False)\n   options.add_argument(\'--disable-blink-features=AutomationControlled\')\n   driver = webdriver.Chrome(options=options)\n   stealth(driver,\n        languages=[""en-US"", ""en""],\n        vendor=""Google Inc."",\n        platform=""Win32"",\n        webgl_vendor=""Intel Inc."",\n        renderer=""Intel Iris OpenGL Engine"",\n        fix_hairline=True,\n        )\n   driver.get(""https://www.google.com"")\n\n', '\nYou can easily bypass the google bot detection with the undetected_chromedriver:\npip install undetected-chromedriver\npip install selenium\n\nCredits: https://github.com/xtekky/google-login-bypass/blob/main/login.py\nimport undetected_chromedriver as uc\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nclass Main:\n  def __init_(self) -> None:\n    self.url    = \'https://accounts.google.com/ServiceLogin\'\n    self.driver = driver = uc.Chrome(use_subprocess=True)\n    self.time   = 10\n    \n  def login(self, email, password):\n    WebDriverWait(self.driver, 20).until(EC.visibility_of_element_located((By.NAME, \'identifier\'))).send_keys(f\'{email}\\n)\n    WebDriverWait(self.driver, 20).until(EC.visibility_of_element_located((By.NAME, \'password\'))).send_keys(f\'{password}\\n)\n                                                                                \n    self.code()\n                                                                                  \n  def code(self):\n    # [ ---------- paste your code here ---------- ]\n    time.sleep(self.time)                                                                                  \n                                                                                  \nif __name__ == ""__main__"":\n  #  ---------- EDIT ----------\n  email = \'email\' # replace email\n  password = \'password\' # replace password\n  #  ---------- EDIT ----------                                                                                                                                                         \n \n  driver = Main()\n  driver.login(email, password) \n\n', '\nFrom terminal pip install undetected-chromedriver\nthen do the following steps, as shown below.\nNOTE: indent your code inside if name == main, as i have done, only then the program will work\nimport undetected_chromedriver as uc\nfrom time import sleep\nfrom selenium.webdriver.common.by import By\n\n\nif __name__ == \'__main__\':\n    \n    driver = uc.Chrome()\n    driver.get(\'https://accounts.google.com/\')\n\n    # add email\n    driver.find_element(By.XPATH, \'//*[@id=""identifierId""]\').send_keys(YOUR EMAIL)\n    driver.find_element(By.XPATH, \'//*[@id=""identifierNext""]/div/button/span\').click()\n    sleep(3)\n    driver.find_element(By.XPATH, \'//*[@id=""password""]/div[1]/div/div[1]/input\').send_keys(YOUR PASSWORD)\n    driver.find_element(By.XPATH, \'//*[@id=""passwordNext""]/div/button/span\').click()\n    sleep(10)\n\n\n\n', '\nIf you\'d prefer Chrome over Firefox, the way to go around Googles automation detection is by using the undetected_chromedriver library.\nYou can install the package using pip install undetected-chromedriver.\nOnce your driver object is initiated you can simply use selenium to operate the driver afterwards.\n# initiate the driver with undetetcted_chromedriver\nimport undetected_chromedriver.v2 as uc\ndriver = uc.Chrome()\n\n# operate the driver as you would with selenium\ndriver.get(\'https://my-url.com\') \n\n# Example use of selenium imports to be used with the driver\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.webdriver.common.by import By\n\ntry:\n    driver.find_element(By.XPATH, \'//*[@id=""my-id""]\').click()\nexcept NoSuchElementException:\n    print(""No Such Element Exception"")\n\nNote: the login of Google is a popup, so don\'t forget to swap window handles to the popup to login and then afterwards to switch back to the main window.\n', '\nYou can use undetected-chromedriver from github\nimport undetected_chromedriver as uc\nfrom selenium import webdriver\n\noptions = uc.ChromeOptions()\noptions.add_argument(""--ignore-certificate-error"")\noptions.add_argument(""--ignore-ssl-errors"")\n# e.g. Chrome path in Mac =/Users/x/Library/xx/Chrome/Default/\noptions.add_argument( ""--user-data-dir=<Your chrome profile>"")\ndriver = uc.Chrome(options=options)\nurl=\'https://accounts.google.com/servicelogin\'\ndriver.get(url)\n\nYour first import need to be undetected Chrome driver.\n', '\nFirstly, I would ask you to use the undetected_chromedriver module\nI have found some issues with the original repo so I have forked a repo that supports user_data_dir and also allows you to provide custom chromedriver path as well.\nUninstall the old module and install this by using git clone and then go the folder and run python setup.py install\nForked repo link : https://github.com/anilabhadatta/undetected-chromedriver\nImport the latest undetected_chromdriver module:\nimport undetected_chromedriver.v2 as ucdriver\nFor using user_data_dir feature write:\noptions.user_data_dir = ""path_to _user-data-dir""\ninstead of using this\noptions.add_argument(f""user_data_dir={path_to _user-data-dir}"")\nprofile_directory name is the same as how we write in selenium\noptions.add_argument(f""--profile-directory=Default"")\nFor using custom chrome path,\noptions.binary_location = chrome_path\nFor using custom chromedriver path,\ndriver = ucdriver.Chrome(executable_path=f\'{path_to_chromedriver}\', options=options) \nRecently Google made some changes for which the authentication part didn\'t work.\nI have tested this in Python 3.9.0, there are reports that it may not work correctly in 3.10.0\nAnd this is tested in both Windows and Linux.\nFinal Code:\ndef load_chrome_driver(headless):\n    chrome_loc = ""/home/ubuntu/Downloads/chromium-browser/""\n    chrome_path = chrome_loc + ""chrome""\n    chromedriver_path = chrome_loc + ""chromedriver""\n    user_data_dir = ""/home/ubuntu/.config/chromium/custom_user""\n    options = webdriver.ChromeOptions()\n    if headless:\n        options.add_argument(\'headless\')\n    options.add_argument(\'--profile-directory=Default\')\n    options.add_argument(""--start-maximized"")\n    options.add_argument(\'--disable-gpu\')\n    options.add_argument(\'--no-sandbox\')\n    options.add_argument(""--disable-dev-shm-usage"")\n    options.add_argument(\'--log-level=3\')\n    options.binary_location = chrome_path\n    options.user_data_dir = user_data_dir\n    driver = ucdriver.Chrome(\n        executable_path=chromedriver_path, options=options)\n    driver.set_window_size(1920, 1080)\n    driver.set_window_position(0, 0)\n    return driver\n\n', '\nGoogle is detecting that you\'re using a bot. You must first of all add this snippet of config (convert it to python since i wrote it in java):\n    options.addArguments(""--no-sandbox"");\n            options.addArguments(""--disable-dev-shm-usage"");\n            options.addArguments(""--disable-blink-features"");\n            options.setExperimentalOption(""excludeSwitches"", Arrays.asList(""enable-automation""));\n            options.addArguments(""--disable-blink-features=AutomationControlled"");\n            options.addArguments(""--disable-infobars"");\n\n        options.addArguments(""--remote-debugging-port=9222"");\n\noptions.setCapability(CapabilityType.UNEXPECTED_ALERT_BEHAVIOUR, UnexpectedAlertBehaviour.IGNORE);\n\ndriver.executeScript(""Object.defineProperty(navigator, \'webdriver\', {get: () => undefined})"");\n\nThe last instruction is the key one, when you launch Selenium, the navigator var of Chrome is set to \'true\', that means that the browser is controlled by a bot, setting it with JS to undefined renders it a ""normal browser"" in view of Google.\nAnother important thing you should do is to change the HEX of the chromedriver binary, i can PM you my already modified one (ubuntu environment), if you trust me obv.\n']",https://stackoverflow.com/questions/66209119/automation-google-login-with-python-and-selenium-shows-this-browser-or-app-may,automation
Programmatically building htpasswd,"
Is there a programmatic way to build htpasswd files, without depending on OS specific functions (i.e. exec(), passthru())?
",21k,"
            22
        ","[""\n.httpasswd files are just text files with a specific format depending on the hash function specified. If you are using MD5 they look like this:\nfoo:$apr1$y1cXxW5l$3vapv2yyCXaYz8zGoXj241\n\nThat's the login, a colon, ,$apr1$, the salt and 1000 times md5 encoded as base64. If you select SHA1 they look like this:\nfoo:{SHA}BW6v589SIg3i3zaEW47RcMZ+I+M=\n\nThat's the login, a colon, the string {SHA} and the SHA1 hash encoded with base64.\nIf your language has an implementation of either MD5 or SHA1 and base64 you can just create the file like this:\n<?php\n\n$login = 'foo';\n$pass = 'pass';\n$hash = base64_encode(sha1($pass, true));\n\n$contents = $login . ':{SHA}' . $hash;\n\nfile_put_contents('.htpasswd', $contents);\n\n?>\n\nHere's more information on the format:\nhttp://httpd.apache.org/docs/2.2/misc/password_encryptions.html\n"", ""\nFrom what it says on the PHP website, you can use crypt() in the following method:\n<?php\n\n// Set the password & username\n$username = 'user';\n$password = 'mypassword';\n\n// Get the hash, letting the salt be automatically generated\n$hash = crypt($password);\n\n// write to a file\nfile_set_contents('.htpasswd', $username ':' . $contents);\n\n?>\n\nPart of this example can be found: http://ca3.php.net/crypt\nThis will of course overwrite the entire existing file, so you'll want to do some kind of concatination.\nI'm not 100% sure this will work, but I'm pretty sure.\n"", ""\nTrac ships with a Python replacement for htpasswd, which I'm sure you could port to your language of choice: htpasswd.py.\n""]",https://stackoverflow.com/questions/39916/programmatically-building-htpasswd,automation
Learning and Understanding the Xcode Build System,"
Alright, I'm curious about the build process with Xcode. Setting up multiple Targets, how to automate versioning and generally understanding the system so I can manipulate it to do what I want. 
Does anyone have any books or can point me to some documentation somewhere so that I can figure all of this out? 
Thanks a ton.
Another thing, if anyone actually sees this since changing it.
But any books anyone is aware of that will focus on Xcode 4? There's Xcode 3 Unleashed but I'd be real curious if there are any books that focus heavily on Xcode 4. 
",10k,"
            20
        ","['\nOne thing that is really essential for consistent, reproducible, automatable builds is knowledge of the xcodebuild command.  Sadly I can\'t find any official docs on it apart from the manpage (type man xcodebuild).  There\'s a useful guide to automating iphone builds here that includes building with xcodebuild and versioning with agvtool.  This is just as relevant to general building of Mac apps.\nGenerally building with xcodebuild is very simple: \ncd project_dir\nxcodebuild -project myproject.xcodeproj -configuration Release ARCHS=""x86_64 i386"" build\n\nOnce you can build from a script like this it\'s very easy to slot into an automated build system. \n', '\nXcode build process\n[LLVM], Clang LLVM, Swift LLVM\nXcode uses xcodebuild internally[Example]\n\nObjective-C language\n1. Preprocessing\n[Driver][Preprocessing][Parsing and Semantic Analysis] Parser `.m -> AST`\n    2.Compiling by compiler(Clang)\n    [Code Generation and Optimization](GCC_OPTIMIZATION_LEVEL) LLVM IR Generation `AST -> LLVM IR`. \n    3. Assembling\n    [LLVM backend] LLVM `LLVM IR -> .o`\n4. Static Linking(ld)\n5. Output binary\n\nXcode and Objective-C exposes some of these steps:\n\nPreprocessing:\n\n\nReplace macros\n\nSplits .h and .m.\nIn Xcode, you can look at the preprocessor output of .m file by selecting\n  select .m file -> Product -> Perform Action -> Preprocess\n\n\n\n\nCompiling - translates a low-level intermediate code.\nOften you can see this file when debug a code that you are not owned. Xcode allows you to review the output.\n select .m file -> Product -> Perform Action -> Assemble\n\n\nAssembling(produce .o) - translates code into object file (.o file)[Mach-O] In Xcode, you’ll find these object files inside the <product_name>.build/Objects-normal folder inside the derived data directory.\n\nStatic Linking(produce .app, .a, .framework ...)[About] - It is a part of static linker that has to resolve symbols between object files and libraries/frameworks. This process produce a merged executable file which can contain additional resources and dynamic binary\n\nOutput binary\n\n\nSwift language\nLVVM Frontend\n\n1. Preprocessing\n[Parse module] Parser `.m -> AST`\n[Sema module] Type checking > type-checks AST and annotates it with type information\n    2.Compiling by compiler(Swiftc)\n    [SILGen module] SIL Generator `AST -> raw SIL` > optimizations\n        `Guaranteed Optimization Passes`, `Diagnostic Passes` `raw SIL -> canonical SIL. This optimization is applied in any case \n        `Optimization Passes`(SWIFT_OPTIMIZATION_LEVEL) \n    [IRGen] LLVM IR Generation\n\nLVVM Backend\n    3. Assembling\n    [LLVM backend] LLVM `LLVM IR -> .o`\n4. Static Linking(ld)\n5. Output binary\n\nSwift Example\nimport Foundation\n\nclass ClassA {\n    func foo(param: String) -> Int {\n        return 1\n    }\n}\n\nAST\n-dump-parse            Parse input file(s) and dump AST(s)\n-dump-ast              Parse and type-check input file(s) and dump AST(s)\n-print-ast             Parse and type-check input file(s) and pretty print AST(s)\n\n# xcrun swiftc -dump-parse ""ClassA.swift"" \n\n(source_file ""ClassA.swift""\n  (import_decl range=[ClassA.swift:8:1 - line:8:8] \'Foundation\')\n  (class_decl range=[ClassA.swift:10:1 - line:14:1] ""ClassA""\n    (func_decl range=[ClassA.swift:11:5 - line:13:5] ""foo(param:)""\n      (parameter ""self"")\n      (parameter_list range=[ClassA.swift:11:13 - line:11:27]\n\n...\n\n#xcrun swiftc -dump-ast ""ClassA.swift"" \n\n(source_file ""ClassA.swift""\n  (import_decl range=[ClassA.swift:8:1 - line:8:8] \'Foundation\')\n  (class_decl range=[ClassA.swift:10:1 - line:14:1] ""ClassA"" interface type=\'ClassA.Type\' access=internal non-resilient\n    (func_decl range=[ClassA.swift:11:5 - line:13:5] ""foo(param:)"" interface type=\'(ClassA) -> (String) -> Int\' access=internal\n      (parameter ""self"")\n\n...      \n\n# xcrun swiftc -print-ast ""ClassA.swift""\nimport Foundation\n\ninternal class ClassA {\n  internal func foo(param: String) -> Int\n  @objc deinit\n  internal init()\n}\n\n#xcrun swiftc -frontend -emit-syntax ""ClassA.swift"" | python -m json.tool\n\n{\n    ""kind"": ""SourceFile"",\n    ""layout"": [\n        {\n            ""kind"": ""CodeBlockItemList"",\n            ""layout"": [\n                {\n                    ""kind"": ""CodeBlockItem"",\n                    ""layout"": [\n                        {\n                            ""kind"": ""ImportDecl"",\n                            ""layout"": [\n                                {\n                                   \n...\n\nSIL\n-emit-silgen           Emit raw SIL file(s)\n-emit-sil              Emit canonical SIL file(s)\n\n# xcrun swiftc -emit-silgen  ""ClassA.swift""                      \n\nsil_stage raw\n\nimport Builtin\nimport Swift\nimport SwiftShims\n\nimport Foundation\n\nclass ClassA {\n  func foo(param: String) -> Int\n  @objc deinit\n  init()\n}\n\n// ClassA.foo(param:)\nsil hidden [ossa] @$s6ClassAAAC3foo5paramSiSS_tF : $@convention(method) (@guaranteed String, @guaranteed ClassA) -> Int {\n// %0 ""param""                                     // user: %2\n\n\nsil_vtable ClassA {\n  #ClassA.foo: (ClassA) -> (String) -> Int : @$s6ClassAAAC3foo5paramSiSS_tF // ClassA.foo(param:)\n  #ClassA.init!allocator: (ClassA.Type) -> () -> ClassA : @$s6ClassAAACABycfC // ClassA.__allocating_init()\n  #ClassA.deinit!deallocator: @$s6ClassAAACfD // ClassA.__deallocating_deinit\n}\n\n\n\n// Mappings from \'#fileID\' to \'#filePath\':\n//   \'ClassA/ClassA.swift\' => \'ClassA.swift\'\n\n...\n\nLLVM IR\n-emit-ir               Emit LLVM IR file(s)\n\nxcrun swiftc -emit-ir  ""ClassA.swift""\n; ModuleID = \'<swift-imported-modules>\'\nsource_filename = ""<swift-imported-modules>""\ntarget datalayout = ""e-m:o-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128""\ntarget triple = ""x86_64-apple-macosx12.0.0""\n\n%objc_class = type { %objc_class*, %objc_class*, %swift.opaque*, %swift.opaque*, %swift.opaque* }\n\ndefine i32 @main(i32 %0, i8** %1) #0 {\nentry:\n  %2 = bitcast i8** %1 to i8*\n  ret i32 0\n}\n\n...\n\n-emit-object           Emit object file(s) (-c)\n-emit-executable       Emit a linked executable\n\n[Build With Timing Summary]\nAlso you can use Xcode Report Navigator to learn more about build process. Moreover Xcode v14 includes a great feature to visualisation and analyzation of build process\nShow the Report navigator -> <select build> -> <right_click> -> Show in Timeline\n\n//or\n\nShow the Report navigator -> <select build> -> Editor -> Open Timeline\n\n\n']",https://stackoverflow.com/questions/5490048/learning-and-understanding-the-xcode-build-system,automation
Create Outlook email draft using PowerShell,"
I'm creating a PowerShell script to automate a process at work.  This process requires an email to be filled in and sent to someone else.  The email will always roughly follow the same sort of template however it will probably never be the same every time so I want to create an email draft in Outlook and open the email window so the extra details can be filled in before sending.
I've done a bit of searching online but all I can find is some code to send email silently. The code is as follows:
$ol = New-Object -comObject Outlook.Application  
$mail = $ol.CreateItem(0)  
$Mail.Recipients.Add(""XXX@YYY.ZZZ"")  
$Mail.Subject = ""PS1 Script TestMail""  
$Mail.Body = ""  
Test Mail  
""  
$Mail.Send() 

In short, does anyone have any idea how to create and save a new Outlook email draft and immediately open that draft for editing?
",41k,"
            19
        ","['\nBased on the other answers, I have trimmed down the code a bit and use\n$ol = New-Object -comObject Outlook.Application\n\n$mail = $ol.CreateItem(0)\n$mail.Subject = ""<subject>""\n$mail.Body = ""<body>""\n$mail.save()\n\n$inspector = $mail.GetInspector\n$inspector.Display()\n\nThis removes the unnecessary step of retrieving the mail from the drafts folder.  Incidentally, it also removes an error that occurred in Shay Levy\'s code when two draft emails had the same subject.\n', '\n$olFolderDrafts = 16\n$ol = New-Object -comObject Outlook.Application \n$ns = $ol.GetNameSpace(""MAPI"")\n\n# call the save method yo dave the email in the drafts folder\n$mail = $ol.CreateItem(0)\n$null = $Mail.Recipients.Add(""XXX@YYY.ZZZ"")  \n$Mail.Subject = ""PS1 Script TestMail""  \n$Mail.Body = ""  Test Mail  ""\n$Mail.save()\n\n# get it back from drafts and update the body\n$drafts = $ns.GetDefaultFolder($olFolderDrafts)\n$draft = $drafts.Items | where {$_.subject -eq \'PS1 Script TestMail\'}\n$draft.body += ""`n foo bar""\n$draft.save()\n\n# send the message\n#$draft.Send()\n\n', ""\nI think Shay Levy's answer is almost there: the only bit missing is the display of the item.\nTo do this, all you need is to get the relevant inspector object and tell it to display itself, thus:\n$inspector = $draft.GetInspector  \n$inspector.Display()\n\nSee the MSDN help on GetInspector for fancier behaviour.\n"", '\nif you want to use HTML template please use HTMLbody instead of Body , please find sample code below:\n$ol = New-Object -comObject Outlook.Application\n$mail = $ol.CreateItem(0)\n$mail.Subject = ""Top demand apps-SOURCE CLARIFICATION""\n$mail.HTMLBody=""<html><head></head><body><b>Joseph</b></body></Html>""\n$mail.save()\n\n$inspector = $mail.GetInspector\n$inspector.Display()\n\n', '\nThought I would add in to this as well. There are a few steps you can save yourself if you know a lot of the basics (subject, recipients, or other aspects). First create the template of the email and save that, e.g. somewhere maybe with the code? \nAs to the code itself, it follows much the same that others have posted.  \nBorrowing from Jason:\n$ol = New-Object -comObject Outlook.Application\n$msg = $ol.CreateItemFromTemplate(<<Path to template file>>)\n\nModify as needed. Append fields or modify body. The message can still be viewed prior to sending the same way $msg.GetInspector.Display(). Then call $msg.send() to send away!   \n']",https://stackoverflow.com/questions/1453723/create-outlook-email-draft-using-powershell,automation
GMail is blocking login via Automation (Selenium),"
I am using selenium to automate a mail verification process in a web application. I have a script already in place to login to gmail and read an activation mail received on the account. The script was perfectly working till yesterday but today I am facing a problem.

Additional Screenshot of issue

Gmail is not allowing sign in if the browser is launched with selenium. Says, 

You're using a browser that Google doesn't recognize or that's setup in a way that we don't support.


I have tried upgrading chromedriver version to 76.0.0 as I am using
chrome version 76.0.3809.100(64 bit). (Previously used chromedriver
2.45) Still, the problem persists.
Verified that this issue occurs even if I use Firefox instead of Chrome for automation.
Verified that Javascript is enabled in the browser
Gmail is not asking for any OTP or recovery mail. It is simply
blocking my attempt to login via automation. However I am able to
login to the same account manually.


Software used:  ""webdriverio"": ""^4.14.1"", ""wdio-cucumber-framework"":
  ""^2.2.8""

Any help is appreciated.
",38k,"
            18
        ","['\nAfter some trial and error, found out that this issue happens only in a scenario when multiple gmail accounts have already been created from the same App/IP/Device. Google somehow is marking those accounts and blocks them if they are launched by automation frameworks/extensions. \nTemporary Solutions:\n\nCreate a fresh GMail account using a\ndifferent mobile number from another device (Not recommended).\nWe should be using workarounds like nodemailer\nzeolearn.com/magazine/sending-and-receiving-emails-using-nodejs\n(as mentioned by Rahul L as a suggestion)\nAutomate temporary mail providers like Guerilla Mail or 10 Minute Mail if you are\nworried about only receiving mails\n\nMy humble opinion is to entirely avoid automating the UI of third party Mail applications as you cannot predict how their UI and elements will change. They might block you from launching for security purposes and they have every right to do so!\n', ""\nI just tried something out that worked for me after several hours of trial and error.\nAdding args: ['--disable-web-security', '--user-data-dir', '--allow-running-insecure-content' ] to my config resolved the issue.\nI realized later that this was not what helped me out as I tried with a different email and it didn't work. After some observations, I figured something else out and this has been tried and tested.\nUsing automation:\nGo to https://stackoverflow.com/users/login\nSelect Log in with Google Strategy\nEnter Google username and password\nLogin to Stackoverflow\nGo to https://gmail.com (or whatever Google app you want to access)\nAfter doing this consistently for like a whole day (about 24 hours), try automating your login directly to gmail (or whatever Google app you want to access) directly... I've had at least two other people do this with success.\nPS - You might want to continue with the stackoverflow login until you at least get a captcha request as we all went through that phase as well.\n"", '\nGo to Gmail --> Settings --> Search for ""Less secure app access"" --> Enable\nThis is not a suggested method because it allows access to less secure apps and it\'s easier to get into your account but if it\'s just a testing account and no important data is being transferred to or from, you can give this a try.\n', '\nYou need to open your editor and copy this code and paste them and save with this name as email.py and then open your terminal/cmd/powershell in that directory and type this python\xa0.\\email.py\nNote:\nMake sure your chrome driver in that directory where you save python file\nYou need to copy this code and paste in your editor\nHere is that script:\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport time\nimport pyautogui as pg\n\n\nusername = input(""Enter Your Username: "")\n\npassword = input(""Enter Your Password: "")\n\ndriver = webdriver.Chrome()\ndriver.maximize_window()\ndriver.get(""https://accounts.google.com/ServiceLogin?service=mail&passive=true&rm=false&continue=https://mail.google.com/mail/&ss=1&scc=1&ltmpl=default&ltmplcache=2&emr=1&osid=1#identifier"")\ndriver.maximize_window()\n\n\nmail = WebDriverWait(driver, 100).until(EC.element_to_be_clickable((By.XPATH, ""//*[@id=\'identifierId\']""))).send_keys(username)\n\nlogin = WebDriverWait(driver, 100).until(EC.element_to_be_clickable((By.XPATH, ""//*[@id=\'identifierNext\']/span""))).click()\n\npassw = WebDriverWait(driver, 100).until(EC.element_to_be_clickable((By.XPATH, ""//*[@id=\'password\']/div[1]/div/div[1]/input""))).send_keys(password)\n\nnext = WebDriverWait(driver, 100).until(EC.element_to_be_clickable((By.XPATH, ""//*[@id=\'passwordNext\']/span/span""))).click()\n\n', '\nSteps to login to gmail through stackoverflow :\n\nOpen a browser window and open stackoverflow\nClick on log in\nLogin with google\nEnter email and password\nStackoverflow is logged in with gmail credentials\nNow open mail.google.com (gmail.com)\nYou are now logged into gmail using selenium\n\nYou can copy this code and paste in your editor, you have to enter the path of your chrome driver, email address and password for your gmail where it is asked in the code.\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nimport time\n\ndriver=webdriver.Chrome(\'Enter the path of the chrome driver here\')\ndriver.get(""https://stackoverflow.com/"")\n\ndriver.maximize_window()\n\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'/html/body/header/div/ol[2]/li[2]/a[1]\').click()#Log in button in stackoverflow\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'//*[@id=""openid-buttons""]/button[1]\').click()# Log in with Google button\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'//*[@id=""identifierId""]\').send_keys(""Enter email address"")# Enter email address\n\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'//*[@id=""identifierNext""]/div/button/div[2]\').click() # Click next button after entering email address\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'//*[@id=""password""]/div[1]/div/div[1]/input\').send_keys(""Enter password"")#Enter password\n\ntime.sleep(5)\n\ndriver.find_element(By.XPATH, \'//*[@id=""passwordNext""]/div/button/div[2]\').click()# Click on next button after entering the password\n\ntime.sleep(5)\n\ndriver.get(""https://mail.google.com"")\n\ntime.sleep(5)\n\ndriver.close()\n\n', '\nFound solution, add these arguements:\noptions.AddArguments(""--disable-web-security"", ""--user-data-dir=true"", ""--allow-running-insecure-content"");\n^^  C# btw\n', '\nI think this is not a problem with the automation framework nor with the automated Chrome application, it is Google server that blocks automated login requests. I don\'t have any clear evidence/really understand how they can do it, but several clues point me to this direction:\n\nFirst, on the login page, after entering your user name and clicking next, a user lookup request is fired, something likes https://accounts.google.com/_/lookup/accountlookup?hl=vi&_reqid=xxxxx&rt=j, the response of this request includes something likes https://accounts.google.com/signin/rejected?rrk=46&hl=vi. In subsequent requests, the browser just tries to resolve & display what the error means to the user.\nSecond, it was stated here that ""Google doesn’t let you sign in from some browsers. Google might stop sign-ins from browsers that are being controlled through software automation rather than a human"". This means Googles has implemented measures to detect requests coming automated browser, especially Chrome-family browsers in debug mode, which are used by many automation frameworks.\nA thread about similar issue with Taiko also mentioned that Google blocks requests from Chrome running in debug mode.\n\nRecently I tried with Cypress and Taiko, but none works, same ""rejected"" issue, which ruined my initial plan of doing an e2e test for my app (which uses GG OIDC login).\n', '\nBelow code worked for me by using selenium-stealth module and editing chromedriver exe\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium import webdriver \nimport time\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\nfrom selenium_stealth import stealth\nchrome_options = Options()\nchrome_options.add_experimental_option(""useAutomationExtension"", False)\nchrome_options.add_experimental_option(""excludeSwitches"",[""enable-automation""])\nchrome_options.add_argument(""--start-maximized"")\nchrome_options.add_argument(\'--disable-logging\')\nchrome_options.add_argument(\'--no-sandbox\')\nchrome_options.add_argument(\'--disable-dev-shm-usage\')\nchrome_options.add_argument(\'--disable-blink-features=AutomationControlled\')\ncaps = DesiredCapabilities.CHROME\ncaps[\'goog:loggingPrefs\'] = {\'performance\': \'ALL\'}\n\n# for editing chromedriver exe so that its not detected(run only once)\nwith open(""chromedriver.exe"",""rb"") as d:\n    file_data=d.read()\nfile_data=file_data.replace(b""cdc_"",b""tch_"")\nwith open(""chromedriver.exe"",""wb"") as d:\n    d.write(file_data)\ndriver = webdriver.Chrome(\'chromedriver.exe\',chrome_options=chrome_options,desired_capabilities=caps)\n\n# for injecting js to that tab so that browser not detected\nstealth(driver,languages=[""en-US"", ""en""],vendor=""Google Inc."",platform=""Win32"",webgl_vendor=""Intel Inc."",renderer=""Intel Iris OpenGL Engine"",fix_hairline=True,)\ndriver.get(""https://accounts.google.com"")\ntime.sleep(3)\ndriver.switch_to.active_element.send_keys(""myemail@gmail.com\\n"")\ntime.sleep(3)\ndriver.switch_to.active_element.send_keys(""mypassword\\n"")\n\n']",https://stackoverflow.com/questions/57602974/gmail-is-blocking-login-via-automation-selenium,automation
Can a Perl script install its own CPAN dependencies?,"
I have a Perl script that has two dependencies that exist in CPAN. What I'd like to do is have the script itself prompt the user to install the necessary dependencies so the script will run properly. If the user needs to enter in some kind of authentication to install the dependencies that's fine: what I'm trying to avoid is the following workflow:

Run script -> Watch it fail -> Scour CPAN aimlessly -> Lynch the script writer

Instead I'm hoping for something like:

Run script -> Auto-download script dependencies (authenticating as necessary) -> Script succeeds -> Buy the script writer a beer

Can this be done?
",3k,"
            17
        ","[""\nEach of the standard build paradigms has their own way of specifying dependencies. In all of these cases, the build process will attempt to install your dependencies, automatically in some contexts.\nIn ExtUtils::MakeMaker, you pass a hash reference in the PREREQ_PM field to WriteMakefile:\n# Makefile.PL for My::Module\nuse ExtUtils::MakeMaker;\n\nWriteMakefile (\n    NAME => 'My::Module',\n    AUTHOR => ...,\n    ...,\n    PREREQ_PM => {\n        'Some::Dependency' => 0,             # any version\n        'Some::Other::Dependency' => 0.42,   # at least version 0.42\n        ...\n    },\n    ...\n );\n\nIn Module::Build, you pass a hashref to the build_requires field:\n# Build.PL\nuse Module::Build;\n...\nmy $builderclass = Module::Build->subclass( ... customizations ... );\nmy $builder = $builderclass->new(\n    module_name => 'My::Module',\n    ...,\n    build_requires => {\n        'Some::Dependency' => 0,\n        'Some::Other::Dependency' => 0.42,\n    },\n    ...\n);\n$builderclass->create_build_script();\n\nIn Module::Install, you execute one or more requires commands before calling the command to write the Makefile:\n# Makefile.PL\nuse inc::Module::Install;\n...\nrequires 'Some::Dependency' => 0;\nrequires 'Some::Other::Dependency' => 0.42;\ntest_requires 'Test::More' => 0.89;\n...\nWriteAll;\n\n"", ""\nYou can probably just execute this from inside your script.\nperl -MCPAN -e 'install MyModule::MyDepends'\n""]",https://stackoverflow.com/questions/7664829/can-a-perl-script-install-its-own-cpan-dependencies,automation
win32: simulate a click without simulating mouse movement?,"
I'm trying to simulate a mouse click on a window. I currently have success doing this as follows (I'm using Python, but it should apply to general win32):
win32api.SetCursorPos((x,y))
win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN,0,0)
win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP,0,0)

This works fine. However, if the click happens while I'm moving the mouse manually, the cursor position gets thrown off. Is there any way to send a click directly to a given (x,y) coordinate without moving the mouse there? I've tried something like the following with not much luck:
nx = x*65535/win32api.GetSystemMetrics(0)
ny = y*65535/win32api.GetSystemMetrics(1)
win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN | \
                     win32con.MOUSEEVENTF_ABSOLUTE,nx,ny)
win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP | \
                     win32con.MOUSEEVENTF_ABSOLUTE,nx,ny)

",21k,"
            10
        ","['\nTry WindowFromPoint() function:\nPOINT pt;\n    pt.x = 30; // This is your click coordinates\n    pt.y = 30;\n\nHWND hWnd = WindowFromPoint(pt);\nLPARAM lParam = MAKELPARAM(pt.x, pt.y);\nPostMessage(hWnd, WM_RBUTTONDOWN, MK_RBUTTON, lParam);\nPostMessage(hWnd, WM_RBUTTONUP, MK_RBUTTON, lParam);\n\n', ""\nThis doesn't answer the question, but it does solve my problem:\nwin32api.ClipCursor((x-1,y-1,x+1,y+1))\nwin32api.SetCursorPos((x,y))\nwin32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN| \\\n                     win32con.MOUSEEVENTF_ABSOLUTE,0,0)\nwin32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP| \\\n                     win32con.MOUSEEVENTF_ABSOLUTE,0,0)\nwin32api.ClipCursor((0,0,0,0))\n\nThe result is that any movements I'm making won't interfere with the click. The downside is that my actual movement will be messed up, so I'm still open to suggestions.\n""]",https://stackoverflow.com/questions/3720968/win32-simulate-a-click-without-simulating-mouse-movement,automation
"UI Automation ""Selected text""","
Anyone knows how to get selected text from other application using UI Automation and .Net?
http://msdn.microsoft.com/en-us/library/ms745158.aspx
",5k,"
            9
        ","['\nprivate void button1_Click(object sender, EventArgs e) {\n        Process[] plist = Process.GetProcesses();\n\n        foreach (Process p in plist) {\n            if (p.ProcessName == ""notepad"") {\n\n                AutomationElement ae = AutomationElement.FromHandle(p.MainWindowHandle);\n\n                AutomationElement npEdit = ae.FindFirst(TreeScope.Descendants, new PropertyCondition(AutomationElement.ClassNameProperty, ""Edit""));\n\n                TextPattern tp = npEdit.GetCurrentPattern(TextPattern.Pattern) as TextPattern;\n\n                TextPatternRange[] trs;\n\n                if (tp.SupportedTextSelection == SupportedTextSelection.None) {\n                    return;\n                }\n                else {\n                    trs = tp.GetSelection();\n                    lblSelectedText.Text = trs[0].GetText(-1);\n                }\n            }\n        }\n    }\n\n', '\nHere is another solution using only UI Automation.\nIt gets the selected text from Notepad and Wordpad.\n// Get only top level windows\nPropertyCondition condition = new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Window);\nAutomationElementCollection windows = AutomationElement.RootElement.FindAll(TreeScope.Children, condition);\nList<AutomationElement> allDocuments = new List<AutomationElement>();\n\nforeach (AutomationElement window in windows)\n{\n    string className = window.Current.ClassName;\n    if (className == ""Notepad"" || className == ""WordPadClass"")\n    {\n        condition = new PropertyCondition(AutomationElement.ControlTypeProperty, ControlType.Document);\n        AutomationElementCollection documents = window.FindAll(TreeScope.Children, condition);\n\n        if (documents.Count > 0)\n        {\n            allDocuments.Add(documents[0]);\n        }\n    }\n}\n\n// store all pieces of selected text here\nList<string> selectedText = new List<string>();\n\n// iterate through all documents found\nforeach (AutomationElement document in allDocuments)\n{\n    object patternObj = null;\n    if (document.TryGetCurrentPattern(TextPattern.Pattern, out patternObj) == true)\n    {\n        TextPattern textPattern = patternObj as TextPattern;\n        TextPatternRange[] ranges = textPattern.GetSelection();\n\n        foreach (TextPatternRange range in ranges)\n        {\n            string text = range.GetText(-1);\n            if (text.Length > 0)\n            {\n                selectedText.Add(text);\n            }\n        }\n    }\n}\n\n']",https://stackoverflow.com/questions/517694/ui-automation-selected-text,automation
VBA Internet Explorer wait for web page to load,"
I know questions like this have been asked before, but mine is a bit different and has been fairly troubling.  What I'm dealing with is a web page with a form with a few events that load more of the page when certain items in input boxes are filled out.  When these events fire the page loads again, but remains at the same URL with the same nameprop.  I've been using the following types of methods both seperately and strung together to handle waiting for the page to load, but sometimes the VBA still manages to continue executing and set the HTMLDocument variable to a page without the appropriate information on it causing the macro to debug.  Here are the kinds of things I've been trying so far:
While IE.Busy
    DoEvents
Wend

Do Until IE.statusText = ""Done""
    DoEvents
Loop

Do Until IE.readyState = 4
    DoEvents
Loop

I've even attempted to place these events into a loop like the following, but it didn't quite work because the lastModified property only returns a value down to the second and the macro spins through the fields fast enough that it is returning a new page in the same second:
Do Until IE.statusText = ""Done"" And IE.Busy = False And IE.ReadyState = 4 _
And IE.document.lastModified > LastModified ----or---- IE.document.nameprop = _
""some known and expected name prop here""
    While IE.Busy
        DoEvents
    Wend
    Do Until IE.statusText = ""Done""
        DoEvents
    Loop
    Do Until IE.readyState = 4
        DoEvents
    Loop
Loop

Even that fails to wait long enough to set the HTMLDocument object leading to a debug.  I've contemplated setting the next input element and checking that for nothing to further the code, but even that wouldn't be successful 100% of the time because generally the Input elements exist in the HTML but are hidden until the appropriate event is fired, which wouldn't be a problem but they don't load their possible selections until after the event is fired.  It might be an odd page.
Anyway... not sure what else to add.  If there is something else that might be helpful to see just ask.  I guess what I'm looking for is a way to get VBA to wait until IE knows another page isn't on it's way.  It seems to load a few times before it is completely done.
So... Anyone have any ideas?
EDIT:  Found a few new things to try.  Still, no dice.  It was suggested that I add these attempts.  Here is the code, for some reason the VBE and excel instance become non-responsive when using this approach after firing an event that should populate the options on the select element... thinking about trying xml... here is the code:
intCounter = 0
Do until intCounter > 2
    Do Until IE.Busy = False: DoEvents: Loop
    Do Until IE.ReadyState = 4: DoEvents: Loop
    Set HTMLDoc = IE.Document
    Do Until HTMLDoc.ReadyState = ""complete""
    Set HTMLSelect = HTMLDoc.getElementById(""ctl00$ctl00$MainContent$ChildMainContent$ddlEmployeeBranchCodes"")
    intCounter = 0
    For each Opt in HTMLSelect
        intCounter = intCounter + 1
    Next Opt
Loop

Based on what I can see happening on the web page, I know that it is somewhere in this loop that the VBE and Excel become non-responsive.  
Hope that helps... I know it didn't help me... Drats.
EDIT:  Just thought I'd add this.  When it comes to automating a web page, for the most part, I no longer use IE.  I've found it's much better, and sidesteps this issue of async stuff entirely, to simply perform the posts and gets yourself.  May not be the best solution depending on what you're trying to do, but it works pretty reliably if you look at the traffic closely and parameterize things well.
",50k,"
            7
        ","[""\nAfter an exhaustive search, I've determined that the AJAX request, javascript code that runs asynchronously in the background isn't something I can get a signal from in any way.  It does seem to trigger some event when it finishes with loading the page, which is an option I'd like to explore in the future.  However, for my purposes I simply used the same code I was already using to fill out the form on my page and I have it loop through each field again to check to see if the values are still correct before clicking the submit button.  It isn't an ideal solution, but it is a workaround that appears to have taken care of my issue in this case. \nI'm not sure if my solution would be applicable to someone else dealing with this issue, but there it is if it helps.  I think workarounds for this issue are going to have to be based on the application and web page in question.\n"", '\nOld question I know, but I think this is a good answer that I haven\'t seen about much...\n\nI had a similar problem; waiting for all the images on a google image search to load (which is a tricky thing since image loads are prompted by AJAX and the user scrolling the page). As has been suggested in the comments; my solution was to wait for a certain element to appear in the viewport (this is an area a little larger than the monitor screen, and is treated as what you can actually ""see"").\nThis is achieved with the getBoundingClientRect method\nDim myDiv As HTMLDivElement: Set myDiv = currPage.getElementById(""fbar"") \n\'myDiv should some element in the page which will only be visible once everything else is loaded\nDim elemRect As IHTMLRect: Set elemRect = myDiv.getBoundingClientRect\nDo Until elemRect.bottom > 0 \'If the element is not in the viewport, then this returns 0\n    DoEvents\n    \'Now run the code that triggers the Ajax requests\n    \'For me that was simply scrolling down by a big number\n    Set elemRect = myDiv.getBoundingClientRect\nLoop\nmyDiv.ScrollIntoView\n\nI explain in detail in the linked answer how this works, but essentially the BoundingClientRect.bottom is equal to 0 until the element is in the vieport.\nThe element is something which is loaded straight away (like a frame/template for the page). But you don\'t actually see it until all the content has been loaded, because it\'s right at the bottom. \nIf that element is indeed the last thing to be loaded (for my google search it was the Show More Results button), then as long as you get it into the viewport when it\'s loaded, you should be able to detect when it appears on the page. .bottom then returns a non-zero value (something to do with the actual position of the element on the page  - I didn\'t really care though for my purposes). I finish off with a .ScrollIntoView, but that\'s not essential.\n', '\nI had the same problem with My Webpage..\nWhat is did is...\nthe fist option is\nWhile Ie.**document**.readystate=""complete""\nDoEvents\nWend\n\nthere were few boxes in which options were loaded after a button click/even fire in another box...I jsst placed code like this..\nDo Until IE.document.getelementbyid(""Next box"").Lenght>0\nDoEvents\nLoop\nApplication.wait Now+Timevalue(""00:00:02)\n\n']",https://stackoverflow.com/questions/19933313/vba-internet-explorer-wait-for-web-page-to-load,automation
How to connect to existing instance of Excel from PowerShell?,"
All examples that automate Excel through PowerShell start with this line:
PS> $Excel = New-Object -Com Excel.Application

This seems to be handling a new instance of Excel, e.g. running $Excel.Visible = $true will show an empty, blank Excel window, not switch to the existing workbook.
If there is already an instance of Excel running, is there a way to connect to it?
",7k,"
            7
        ","['\nInstead of the usual New-Object -ComObject excel.application us this\n$excel = [Runtime.Interopservices.Marshal]::GetActiveObject(\'Excel.Application\')\nRest stays the same. \nOne downside. You will only get the excel ""instances"" started by the same user that will initiate the ps1.  \n', '\nYes, you can access the COM object via HWND [Window handle] using this WIN32 API (AccessibleObjectFromWindow). \n\n(See a SO post sample here of using this api via C#)\n\n.\nYou may have to write an assembly in C# and/or manipulate P/Invoke calls via Powershell.\nYou may give a shot at it & see how it goes.\n']",https://stackoverflow.com/questions/11081317/how-to-connect-to-existing-instance-of-excel-from-powershell,automation
Clicking a button on a page using a Greasemonkey/userscript in Chrome,"
I'm going to be as absolutely verbose here as possible as I've run into a few solutions that didn't end up panning out. Please keep in mind that I don't know Javascript. I know basic HTML and CSS. I don't have any actual programming background but I'm trying to learn bit by bit by researching basic tasks like this. Please talk to me like I'm an idiot. Any lingo I throw around in this post I learned while researching this specific issue. I'm writing this userscript as a personal project and to share with some friends.
What I'm trying to do.
I'm trying to write a userscript for Chrome/Greasemonkey (Chrome is my target browser) that will click the Refresh button on the Battlefield 3 server browser. For those of you that don't know, Battlefield 3 uses a web site paired with a browser plugin for VOIP and actually launching the game via a server browser. The bulk of it seems to be fairly straight forward HTML arranged in tables.
The idea is that when viewing the main page for a server that is full, the script will click the Refresh button every three seconds or so until the page reports an open spot on the server, then stop the refresh loop and click the join server button. I've already got the part of the script running that polls the server current and maximum players then assigns them to their own variables.
At this point I'm trying to get a click to work in the console so I can actually put it to some use in my script and am having zero luck.
The code I'm trying to manipulate.
This is the div for the button that I'm trying to click pulled from the Chrome dev tools:
<div class=""serverguide-header-refresh-button alternate show""> 
<div type=""reset"" class=""common-button-medium-grey"">
<p style=""position: relative;"">
<a href=""/bf3/servers/show/c7088bdc-2806-4758-bf93-2106792b34d8/"">Refresh </a>
</p>
</div>
</div>

(That link is not static. It's a link to a specific server page)
What I've tried.
To actually find the button I'm using getElementsByClassName. It doesn't have a unique ID but the class is unique to that element on this particular page so getElementsByClassName(""serverguide-header-refresh-button"")[0] is pulling the proper div each time. It's getting the script to perform any actual action on the button that's the problem.
document.getElementsByClassName(""serverguide-header-refresh-button"")[0].click();

I now realize this didn't work because it's not a conventional submit button. I don't understand the specifics of the standard here but I get that it doesn't support the .click() method.
function addJQuery(callback) {
  var script = document.createElement(""script"");
  script.setAttribute(""src"", ""http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js"");
  script.addEventListener('load', function() {
    var script = document.createElement(""script"");
    script.textContent = ""("" + callback.toString() + "")();"";
    document.body.appendChild(script);
  }, false);
  document.body.appendChild(script);
}

// the guts of this userscript
function main() {
  unsafeWindow.jQuery('.serverguide-header-refresh-button')[0].click();
}

// load jQuery and execute the main function
addJQuery(main);

This is simply unsafeWindow.jQuery('.serverguide-header-refresh-button').click(); wrapped in some code to load jQuery for userscripts. It was a bit I picked up elsewhere but was told it would only work if jQuery was loaded on the page. I figured it was worth a try. This is one of those I have no idea what I'm doing shots in the dark and it didn't work. I tried the same thing below with another snippet of jQuery code I picked up:
function addJQuery(callback) {
  var script = document.createElement(""script"");
  script.setAttribute(""src"", ""http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js"");
  script.addEventListener('load', function() {
    var script = document.createElement(""script"");
    script.textContent = ""("" + callback.toString() + "")();"";
    document.body.appendChild(script);
  }, false);
  document.body.appendChild(script);
}

// the guts of this userscript
function main() {
   var evObj = document.createEvent('Events');
      evObj.initEvent(""click"", true, false);
      document.getElementsByClassName('serverguide-header-refresh-button')[0].dispatchEvent(evObj);
}

    // load jQuery and execute the main function
addJQuery(main);

Both of these return Undefined in the Chrome and Firebug consoles.
So, would anyone be so kind as to help me create a bit of code for this script to press the Refresh button on this page?
",11k,"
            6
        ","['\nNote:\n\njQuery .click() does not work reliably on click events that were not set via jQuery in the first place.\nYou need to create the right kind of event; createEvent(\'Events\') is not the way.\nAs Jim Deville pointed out, the link pseudo-button was not being selected.\nYou do not need jQuery for this (so far).\nMake sure that the ""Refresh"" control is loaded statically for the test code as shown in the question.  If it\'s AJAXed in, the click attempt may fire too soon.\n\nPutting that all together, the following code should work.  But beware that some sites (like certain Google apps) use funky, state-sensitive designs -- which require a sequence of events.\nvar refreshBtn = document.querySelector (\n    ""div.serverguide-header-refresh-button div[type=\'reset\'] a""\n);\nvar clickEvent = document.createEvent (\'MouseEvents\');\nclickEvent.initEvent (\'click\', true, true);\nrefreshBtn.dispatchEvent (clickEvent);\n\n', '\nso, you are on the right track, but you keep grabbing the DIV not the A tag. To click on the ""button"" (link in this case), you have to click on the actual link because it won\'t tunnel down to the elements contained in the DIV.\ndocument.querySelector(\'serverguide-header-refresh-button a\')\n\nShould get you the A element to click on. From jQuery $(\'serverguide-header-refresh-button a\').click(); should work.\n']",https://stackoverflow.com/questions/8192126/clicking-a-button-on-a-page-using-a-greasemonkey-userscript-in-chrome,automation
reusing Internet Explorer COM Automation Object,"
I am using VBScript macros to utilize the InternetExplorer.Application COM automation object and I am struggling with reusing an existing instance of this object.
From what I have read, I should be able to use the GetObject() method in vbscript to grab a hold of an existing instance of this object.
When I execute the following code I get an ""Object creation failed - moniker syntax error"".
Is my issue really syntax? 
Is my issue how I am trying to use this object? 
or can what I am trying to accomplish just not be done?
Code:
Dim IEObject as object

Sub Main  
  Set IEObject =  GetObject( ""InternetExplorer.Application"" )

  'Set the window visable
  IEObject.Visible = True

  'Navigate to www.google.com
  IEObject.Navigate( ""www.google.com"" )
End Sub

Also, I have no problem running the CreateObject() which opens up a new internet explorer window and navigates where i want to, but i would rather not have the macro open up multiple instances of Internet Explorer.
",11k,"
            6
        ","['\nTry This:\n\nSet IEObject =  GetObject( ,""InternetExplorer.Application"" )\n\n*Notice the comma before ""InternetExplorer.Application""\nEDIT:\nTry this:\n\nDim IE As SHDocVw.InternetExplorer\n\nSet IE = GetObject(,""InternetExplorer.Application"")\n\nYou can also try this:\n\nDim ShellApp\nSet ShellApp = CreateObject(""Shell.Application"")\nDim ShellWindows\nSet ShellWindows = ShellApp.Windows()\nDim i\nFor i = 0 To ShellWindows.Count - 1\n    If InStr(ShellWindows.Item(i).FullName, ""iexplore.exe"") <> 0 Then\n        Set IEObject = ShellWindows.Item(i) \n    End If\nNext\nIEObject.Navigate2(""http://www.google.com"")\n\nEDIT: What you are trying may not be possible, take a look at this. http://support.microsoft.com/kb/239470\n']",https://stackoverflow.com/questions/941767/reusing-internet-explorer-com-automation-object,automation
simulate backspace key with java.awt.Robot,"
There seems to be an issue simulating the backspace key with java.awt.Robot.
This thread seems to confirm this but it does not propose a solution.
This works:
Robot rob = new Robot();
rob.keyPress(KeyEvent.VK_A);
rob.keyRelease(KeyEvent.VK_A);

This doesn't:
Robot rob = new Robot();
rob.keyPress(KeyEvent.VK_BACK_SPACE);
rob.keyRelease(KeyEvent.VK_BACK_SPACE);

Any ideas?
",10k,"
            5
        ","['\nIt seems to work in this test.\nAddendum: Regarding the cited article, ""Aside from those keys that are defined by the Java language (VK_ENTER, VK_BACK_SPACE, and VK_TAB), do not rely on the values of the VK_ constants. Sun reserves the right to change these values as needed to accomodate a wider range of keyboards in the future.""—java.awt.event.KeyEvent\npublic class RobotTest {\n\n    public static void main(String[] args) {\n        EventQueue.invokeLater(new Runnable() {\n            public void run() {\n                new RobotTest().create();\n            }\n        });\n    }\n\n    private void create() {\n        JFrame f = new JFrame();\n        f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n        f.setLocationRelativeTo(null);\n        f.setLayout(new FlowLayout());\n        f.add(new JTextField(8));\n        final JButton b = new JButton();\n        f.getRootPane().setDefaultButton(b);\n        b.addActionListener(new ActionListener() {\n            public void actionPerformed(ActionEvent e) {\n                b.setText(""@"" + e.getWhen());\n            }\n        });\n        f.add(b);\n        f.setSize(256, 128);\n        f.setVisible(true);\n        doTest();\n    }\n\n    private void doTest() {\n        try {\n            Robot r = new Robot();\n            int[] keys = {\n                KeyEvent.VK_T, KeyEvent.VK_E,\n                KeyEvent.VK_S, KeyEvent.VK_T,\n                KeyEvent.VK_Z, KeyEvent.VK_BACK_SPACE,\n                KeyEvent.VK_ENTER\n            };\n            for (int code : keys) {\n                r.keyPress(code);\n                r.keyRelease(code);\n            }\n        } catch (AWTException ex) {\n            ex.printStackTrace(System.err);\n        }\n    }\n}\n\n', '\nThe Backspace functionality does not work as expected. I added a Shift key with the Backspace and it works fine for me, here is the pseudo-code for it.\nrobot.keyPress(KeyEvent.VK_SHIFT);\nrobot.keyPress(KeyEvent.VK_BACK_SPACE);\nrobot.keyRelease(KeyEvent.VK_BACK_SPACE);\nrobot.keyRelease(KeyEvent.VK_SHIFT);\n\nThis does not seem to work for the Delete key though.\n']",https://stackoverflow.com/questions/2596641/simulate-backspace-key-with-java-awt-robot,automation
How to call python file in a feature file of karate automation?,"
I need to call a .py file and pass an argument to one of the functions in it and have to match the result returned.
For instance:
xyz.py
KarateXyz.feature

match result == call('xyz.py')# how to specify the method name and pass an argument in it?

",763,"
            1
        ","[""\nYes, you can use karate.exec() to call any OS process. Whether python is installed is up to you. Refer https://github.com/intuit/karate#karate-exec\n* def result = karate.exec('python foo.py bar')\n\nFor more details, refer: https://stackoverflow.com/a/64352676/143475\n""]",https://stackoverflow.com/questions/66546381/how-to-call-python-file-in-a-feature-file-of-karate-automation,automation
"Auto-reload browser when I save changes to html file, in Chrome?","
I'm editing an HTML file in Vim and I want the browser to refresh whenever the file underneath changes. 
Is there a plugin for Google Chrome that will listen for changes to the file and auto refresh the page every time I save a change to the file? I know there's XRefresh for Firefox but I could not get XRefresh to run at all.
How hard would it be to write a script to do this myself?
",123k,"
            147
        ","['\nPure JavaScript solution!\nLive.js\nJust add the following to your <head>:\n<script type=""text/javascript"" src=""https://livejs.com/live.js""></script>\n\n\nHow?\nJust include Live.js and it will monitor the current page including local CSS and Javascript by sending consecutive HEAD requests to the server. Changes to CSS will be applied dynamically and HTML or Javascript changes will reload the page. Try it!\n\n\nWhere?\nLive.js works in Firefox, Chrome, Safari, Opera and IE6+ until proven otherwise. Live.js is independent of the development framework or language you use, whether it be Ruby, Handcraft, Python, Django, NET, Java, Php, Drupal, Joomla or what-have-you.\n\nI copied this answer almost verbatim from here, because I think it\'s easier and more general than the currently accepted answer here.\n', '\nWith the addition of a single meta tag into your document, you can instruct the browser to automatically reload at a provided interval:\n<meta http-equiv=""refresh"" content=""3"" >\n\nPlaced within the head tag of your document, this meta tag will instruct the browser to refresh every three seconds.\n', ""\nI know this is an old question but in case it helps someone, there is a reload npm package that solves it.\nIn case that you are not running it on a server or have received the error Live.js doesn't support the file protocol. It needs http.\nJust install it:\nnpm install reload -g\n\nand then at your index.html directory, run:\nreload -b\n\nIt will start a server that will refresh every time you save your changes.\nThere are many other options in case you're running it on the server or anything else. Check the reference for more details!\n"", '\nHandy Bash one-liner for OS X, assuming that you have installed fswatch (brew install fswatch). It watches an arbitrary path/file and refreshes the active Chrome tab when there are changes:\nfswatch -o ~/path/to/watch | xargs -n1 -I {} osascript -e \'tell application ""Google Chrome"" to tell the active tab of its first window to reload\'\n\nSee more about fswatch here: https://stackoverflow.com/a/13807906/3510611\n', '\nI assume you\'re not on OSX?  Otherwise you could do something like this with applescript:\nhttp://brettterpstra.com/watch-for-file-changes-and-refresh-your-browser-automatically/\nThere is also a plugin for chrome called ""auto refresh plus"" where you can specify a reload every x seconds:\nhttps://chrome.google.com/webstore/detail/auto-refresh-plus-page-mo/hgeljhfekpckiiplhkigfehkdpldcggm?hl=en\n', '\nUpdate: Tincr is dead.\nTincr is a Chrome extension that will refresh the page whenever the file underneath changes.\n', '\nUse Gulp to watch the files and Browsersync to reload the browser.\nThe steps are:\nIn the command line execute\n\nnpm install --save-dev gulp browser-sync\n\nCreate gulpfile.js with the following contents:\nvar gulp = require(\'gulp\');\nvar browserSync = require(\'browser-sync\').create();\nvar reload = browserSync.reload;\n\ngulp.task(\'serve\', function() {\n  browserSync.init({\n    server: {\n      baseDir: ""./""\n    }\n  });\n\n  gulp.watch(""*.html"").on(""change"", reload);\n});\n\nRun\n\ngulp serve\n\nEdit HTML, save and see your browser reload. The magic is done through on-the-fly injection of special  tag into your HTML files.\n', '\nhttp://livereload.com/ - native app for OS X, Alpha version for Windows. Open sourced at https://github.com/livereload/LiveReload2\n', ""\nIf you are on GNU/Linux, you can use a pretty cool browser called Falkon. It's based on the Qt WebEngine. It's just like Firefox or Chromium - except, it auto refreshes the page when a file is updated. The auto refresh doesn't matter much whether you use vim, nano, or atom, vscode, brackets, geany, mousepad etc.\nOn Arch Linux, you can install Falkon pretty easily:\nsudo pacman -S falkon\n\nHere's the snap package.\n"", ""\nFollowing couple of lines can do the trick:\nvar bfr = '';\nsetInterval(function () {\n    fetch(window.location).then((response) => {\n        return response.text();\n    }).then(r => {\n        if (bfr != '' && bfr != r) {\n            window.location.reload();\n        }\n        else {\n            bfr = r;\n        }\n    });\n}, 1000);\n\nThis compares current response text with previously buffered response text after every second and will reload the page if there are any change in source code. You don't need any heavy duty plugins if you are just developing light weight pages.\n"", '\nThere is a java app for os x and Chrome called Refreschro. It will monitor a given set of files on the local file system and reload Chrome when a change is detected:\nhttp://neromi.com/refreschro/\n', '\nThis works for me (in Ubuntu):\n#!/bin/bash\n#\n# Watches the folder or files passed as arguments to the script and when it\n# detects a change it automatically refreshes the current selected Chrome tab or\n# window.\n#\n# Usage:\n# ./chrome-refresher.sh /folder/to/watch\n\nTIME_FORMAT=\'%F %H:%M\'\nOUTPUT_FORMAT=\'%T Event(s): %e fired for file: %w. Refreshing.\'\n\nwhile inotifywait --exclude \'.+\\.swp$\' -e modify -q \\\n    -r --timefmt ""${TIME_FORMAT}"" --format ""${OUTPUT_FORMAT}"" ""$@""; do\n    xdotool search --onlyvisible --class chromium windowactivate --sync key F5 \\\n    search --onlyvisible --class gnome-terminal windowactivate\ndone\n\nYou may need to install inotify and xdotool packages (sudo apt-get install inotify-tools xdotool in Ubuntu) and to change args of --class to the actual names of your preferred browser and terminal.\nStart the script as described and just open index.html in a browser. After each save in vim the script will focus your browser\'s window, refresh it, and then return to the terminal.\n', '\nA quick solution that I sometimes use is to divide the screen into\ntwo, and each time a change is made, click on the document xD .\n<script>\ndocument.addEventListener(""click"", function(){\n    window.location.reload();\n})\n</script>\n\n', '\nIf you are you are using visual studio code (which I highly recommend for Web Development), there is an extension by the name Live Server by Ritwick Dey with more than 9 million downloads. Just install it (recommended to restart vs code after that), and then just right-click on your main HTML file, there will be an option ""open with Live Server"", click it and your Website will be automatically open in a browser on a local server.\n', '\nIn node.js, you can wire-up primus.js (websockets) with gulp.js + gulp-watch (a task runner and change listener, respectively), so that gulp lets your browser window know it should refresh whenever html, js, etc, change. This is OS agnostic and I have it working in a local project. \nHere, the page is served by your web server, not loaded as a file from disk, which is actually more like the real thing.\n', ""\nThe most flexible solution I've found is the chrome LiveReload extension paired with a guard server. \nWatch all files in a project, or only the ones you specify. Here is a sample Guardfile config:\nguard 'livereload' do\n  watch(%r{.*\\.(css|js|html|markdown|md|yml)})\nend\n\nThe downside is that you have to set this up per project and it helps if you're familiar with ruby.\nI have also used the Tincr chrome extension - but it appears to be tightly coupled to frameworks and file structures. (I tried wiring up tincr for a jekyll project but it only allowed me to watch a single file for changes, not accounting for includes, partial or layout changes). Tincr however, works great out of the box with projects like rails that have consistent and predefined file structures. \nTincr would be a great solution if it allowed all inclusive match patterns for reloading, but the project is still limited in its feature set. \n"", '\nThis can be done using a simple python script.\n\nUse pyinotify to monitor a particular folder.\nUse Chrome with debugging enabled. Refresh can be done via a websocket connection.\n\nFull details can be referred here.\n', '\nBased on attekei\'s answer for OSX:\n$ brew install fswatch\n\nChuck all this into reload.scpt:\nfunction run(argv) {\n    if (argv.length < 1) {\n        console.log(""Please supply a (partial) URL to reload"");\n        return;\n    }\n    console.log(""Trying to reload: "" + argv[0]);\n    let a = Application(""Google Chrome"");\n    for (let i = 0; i < a.windows.length; i++) {\n        let win = a.windows[i];\n        for (let j = 0; j < win.tabs.length; j++) {\n            let tab = win.tabs[j];\n            if (tab.url().startsWith(""file://"") && tab.url().endsWith(argv[0])) {\n                console.log(""Reloading URL: "" + tab.url());\n                tab.reload();\n                return;\n            }\n        }\n    }\n    console.log(""Tab not found."");\n}\n\nThat will reload the first tab that it finds that starts with file:// and ends with the first command line argument. You can tweak it as desired.\nFinally, do something like this.\nfswatch -o ~/path/to/watch | xargs -n1 osascript -l JavaScript reload.scpt myindex.html \n\nfswatch -o outputs the number of files that have changed in each change event, one per line. Usually it will just print 1. xargs reads those 1s in and -n1 means it passes each one as an argument to a new execution of osascript (where it will be ignored).\n', ""\nAdd this to your HTML\n<script> window.addEventListener('focus', ()=>{document.location = document.location})</script>\n\nWhile you are editing, your browser page is blurred, by switching back to look at it, the focus event is fired and the page reloads.\n"", '\nInstall and set up chromix\nNow add this to your .vimrc\nautocmd BufWritePost *.html,*.js,*.css :silent ! chromix with http://localhost:4500/ reload\n\nchange the port to what you use\n', ""\nOk, here is my crude Auto Hotkey solution (On Linux, try Auto Key). When the save keyboard shortcut gets pressed, activate the browser, click the reload button, then switch back to the editor. Im just tired of getting other solutions running. Wont work if your editor does autosave.\n^s::   ; '^' means ctrl key. '!' is alt, '+' is shift. Can be combined.\n    MouseGetPos x,y\n    Send ^s\n\n    ; if your IDE is not that fast with saving, increase.\n    Sleep 100\n\n    ; Activate the browser. This may differ on your system. Can be found with AHK Window Spy.\n    WinActivate ahk_class Chrome_WidgetWin_1\n    WinWaitActive ahk_class Chrome_WidgetWin_1\n    Sleep 100   ; better safe than sorry.\n\n    ;~ Send ^F5   ; I dont know why this doesnt work ...\n    Click 92,62   ; ... so lets click the reload button instead.\n\n    ; Switch back to Editor. Can be found with AHK Window Spy.\n    WinActivate ahk_class zc-frame\n    WinWaitActive ahk_class zc-frame\n    Sleep 100   ; better safe than sorry.\n\n    MouseMove x,y\n    return\n\n"", '\nOffline solution using R\nThis code will:\n\nsetup a local server using the given .html-file\nreturn the server adress, so that you can watch it in a browser.\nmake the browser refresh everytime a change is saved to the .html-file.\n\ninstall.packages(""servr"")\nservr::httw(dir = ""c:/users/username/desktop/"")\n\n\n\nSimilar solutions exist for python etc. \n', '\nIf you have Node installed on your computer, then you can use light-server.\nSetp 1: Install light-server using command npm install -g light-server\nStep 2: While current working directory is the folder containing the static HTML page, start light-server using command npx light-server -s . -p 5000 -w ""*.css # # reloadcss"" -w ""*.html # # reloadhtml"" -w ""*.js # # reloadhtml""\nStep 3: Open the web page in browser at http://localhost:5000\n\n\nIn Step 2,\nPort can be changed using -p switch\nFiles that are being watched can be changed using -w switch\nServer directory can be changed using -s switch\n\nDocumentation for light-server is at https://www.npmjs.com/package/light-server\n', '\nAdd this in your ""head"" section.\nchange the time from 3000ms to any value which you prefer.\nA small hack to reload html file every 3secs. This is useful to me when I use vim + browser setup for JS development.\n    <script>\n        function autoreload() {\n            location.reload();\n        }\n        setInterval(autoreload, 3000);\n    </script>\n\n', '\nLive reload works fine on js and css changes. However it was not working for laravel\'s blade templates. So I wrote a small script that checks for page changes and reloads if there is change.\nAm not sure if it is the best way, but it works.\nI used the script together with Live.js.\nHere it is.\n    window.addEventListener(""load"", function(){\n        var current_url = window.location.href;\n        var current_data = httpGet(current_url);\n        var compone = current_data.length;\n        setInterval(function(){\n        var current_data_twos = httpGet(current_url);\n        var componecurrent_data_twos = current_data_twos.length;\n        if(compone !=componecurrent_data_twos ){\n          location.reload();\n        }\n      }, 1000);\n    });\n    function httpGet(theUrl)\n    {\n        let xmlhttp;\n\n        if (window.XMLHttpRequest) { // code for IE7+, Firefox, Chrome, Opera, Safari\n            xmlhttp=new XMLHttpRequest();\n        } else { // code for IE6, IE5\n            xmlhttp=new ActiveXObject(""Microsoft.XMLHTTP"");\n        }\n        xmlhttp.onreadystatechange=function() {\n            if (xmlhttp.readyState==4 && xmlhttp.status==200) {\n                return xmlhttp.responseText;\n            }\n        }\n        xmlhttp.open(""GET"", theUrl, false);\n        xmlhttp.send();\n        return xmlhttp.response;\n    }\n\n', '\nJust use the Live server extension and open the file with live server. extension namelive server\n', '\npip install https://github.com/joh/when-changed/archive/master.zip\n\nalias watch_refresh_chrome="" when-changed -v -r -1 -s ./ osascript -e \'tell application \\""Google Chrome\\"" to tell the active tab of its first window to reload\' ""\n\nthen just enter the directory you want to monitor execute ""watch_refresh_chrome""\n', '\n(function() {\n    setTimeout(function(){\n        window.location.reload(true);\n    }, 100);\n})();\n\nSave this code into a file livereload.js and include it at the bottom of the HTML script like so:\n<script type=""text/javascript"" src=""livereload.js""></script>\n\nWhat will this do is refresh the page every 100 mili-seconds. Any changes you make in code are instantly visible to the eyes.\n', '\nLive Reload Browser Page\n\nLive Reload Browser Page - tool 2022, for auto refreshing the browser page in real time for Google Chrome.\nAdditional features\n\nauto refreshing the browser page in real time\n\nauto HTML validation of the browser page\n\nreal-time alert on the browser page during web development\n\n\n']",https://stackoverflow.com/questions/5588658/auto-reload-browser-when-i-save-changes-to-html-file-in-chrome,automation
Is it possible to modify a registry entry via a .bat/.cmd script?,"
Is it possible to modify a registry value (whether string or DWORD) via a .bat/.cmd script?
",187k,"
            51
        ","['\n@Franci Penov - modify is possible in the sense of overwrite with /f, eg  \nreg add ""HKCU\\Software\\etc\\etc"" /f /v ""value"" /t REG_SZ /d ""Yes""\n\n', '\nYou can use the REG command. From http://www.ss64.com/nt/reg.html:\nSyntax:\n\n   REG QUERY [ROOT\\]RegKey /v ValueName [/s]\n   REG QUERY [ROOT\\]RegKey /ve  --This returns the (default) value\n\n   REG ADD [ROOT\\]RegKey /v ValueName [/t DataType] [/S Separator] [/d Data] [/f]\n   REG ADD [ROOT\\]RegKey /ve [/d Data] [/f]  -- Set the (default) value\n\n   REG DELETE [ROOT\\]RegKey /v ValueName [/f]\n   REG DELETE [ROOT\\]RegKey /ve [/f]  -- Remove the (default) value\n   REG DELETE [ROOT\\]RegKey /va [/f]  -- Delete all values under this key\n\n   REG COPY  [\\\\SourceMachine\\][ROOT\\]RegKey [\\\\DestMachine\\][ROOT\\]RegKey\n\n   REG EXPORT [ROOT\\]RegKey FileName.reg\n   REG IMPORT FileName.reg\n   REG SAVE [ROOT\\]RegKey FileName.hiv\n   REG RESTORE \\\\MachineName\\[ROOT]\\KeyName FileName.hiv\n\n   REG LOAD FileName KeyName\n   REG UNLOAD KeyName\n\n   REG COMPARE [ROOT\\]RegKey [ROOT\\]RegKey [/v ValueName] [Output] [/s]\n   REG COMPARE [ROOT\\]RegKey [ROOT\\]RegKey [/ve] [Output] [/s]\n\nKey:\n   ROOT :\n         HKLM = HKey_Local_machine (default)\n         HKCU = HKey_current_user\n         HKU  = HKey_users\n         HKCR = HKey_classes_root\n\n   ValueName : The value, under the selected RegKey, to edit.\n               (default is all keys and values)\n\n   /d Data   : The actual data to store as a ""String"", integer etc\n\n   /f        : Force an update without prompting ""Value exists, overwrite Y/N""\n\n   \\\\Machine : Name of remote machine - omitting defaults to current machine.\n                Only HKLM and HKU are available on remote machines.\n\n   FileName  : The filename to save or restore a registry hive.\n\n   KeyName   : A key name to load a hive file into. (Creating a new key)\n\n   /S        : Query all subkeys and values.\n\n   /S Separator : Character to use as the separator in REG_MULTI_SZ values\n                  the default is ""\\0"" \n\n   /t DataType  : REG_SZ (default) | REG_DWORD | REG_EXPAND_SZ | REG_MULTI_SZ\n\n   Output    : /od (only differences) /os (only matches) /oa (all) /on (no output)\n\n', '\nYes, you can script using the reg command.\nExample:\nreg add HKCU\\Software\\SomeProduct\nreg add HKCU\\Software\\SomeProduct /v Version /t REG_SZ /d v2.4.6\n\nThis would create key HKEY_CURRENT_USER\\Software\\SomeProduct, and add a String value ""v2.4.6"" named ""Version"" to that key.\nreg /? has the details.\n', '\nThis is how you can modify registry, without yes or no prompt and don\'t forget to run as administrator\nreg add HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\Shell\\etc\\etc   /v Valuename /t REG_SZ /d valuedata  /f \n\nBelow is a real example to set internet explorer as my default browser\nreg add HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\Shell\\Associations\\UrlAssociations\\https\\UserChoice   /v ProgId /t REG_SZ /d IE.HTTPS  /f \n\n\n/f Force: Force an update without prompting ""Value exists, overwrite\n  Y/N""\n/d Data   : The actual data to store as a ""String"", integer etc\n/v Value   : The value name eg ProgId\n/t DataType  : REG_SZ (default) | REG_DWORD | REG_EXPAND_SZ |\n  REG_MULTI_SZ\n\nLearn more about  Read, Set or Delete registry keys and values, save and restore from a .REG file. from here \n', '\nYou can make a .reg file and call start on it.  You can export any part of the registry as a .reg file to see what the format is.  \nFormat here:\nhttp://support.microsoft.com/kb/310516\nThis can be run on any Windows machine without installing other software.\n', '\nYes. You can use reg.exe which comes with the OS to add, delete or query registry values. Reg.exe does not have an explicit modify command, but you can do it by doing delete and then add.\n', '\nIn addition to reg.exe, I highly recommend that you also check out powershell, its vastly more capable in its registry handling.\n', '\nSee http://www.chaminade.org/MIS/Articles/RegistryEdit.htm\n']",https://stackoverflow.com/questions/130193/is-it-possible-to-modify-a-registry-entry-via-a-bat-cmd-script,automation
Register Variables in Loop in an Ansible Playbook,"
I have two ansible tasks as follows 
  tasks:
 - shell: ifconfig -a | sed 's/[ \t].*//;/^\(lo\|\)$/d'
   register: var1
 - debug: var=var1

 - shell: ethtool -i {{ item }} | grep bus-info | cut -b 16-22
   with_items: var1.stdout_lines
   register: var2
 - debug: var=var2

which is used to get a list of interfaces in a machine (linux) and get the bus address for each. I have one more task as follows in tha same playbook 
 - name: Binding the interfaces
   shell: echo {{ item.item }}
   with_flattened: var2.results
   register: var3

which I expect to iterate over value from var2 and then print the bus numbers. 
var2.results is as follows 
""var2"": {
    ""changed"": true,
    ""msg"": ""All items completed"",
    ""results"": [
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i br0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005778"",
            ""end"": ""2015-04-14 20:29:47.122203"",
            ""invocation"": {
                ""module_args"": ""ethtool -i br0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""br0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:47.116425"",
            ""stderr"": """",
            ""stdout"": """",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp13s0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005862"",
            ""end"": ""2015-04-14 20:29:47.359749"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp13s0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp13s0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:47.353887"",
            ""stderr"": """",
            ""stdout"": ""0d:00.0"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp14s0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005805"",
            ""end"": ""2015-04-14 20:29:47.576674"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp14s0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp14s0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:47.570869"",
            ""stderr"": """",
            ""stdout"": ""0e:00.0"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp15s0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005873"",
            ""end"": ""2015-04-14 20:29:47.875058"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp15s0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp15s0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:47.869185"",
            ""stderr"": """",
            ""stdout"": ""0f:00.0"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp5s0f1: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005870"",
            ""end"": ""2015-04-14 20:29:48.112027"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp5s0f1: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp5s0f1:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:48.106157"",
            ""stderr"": """",
            ""stdout"": ""05:00.1"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp5s0f2: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005863"",
            ""end"": ""2015-04-14 20:29:48.355733"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp5s0f2: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp5s0f2:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:48.349870"",
            ""stderr"": """",
            ""stdout"": ""05:00.2"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp5s0f3: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005829"",
            ""end"": ""2015-04-14 20:29:48.591244"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp5s0f3: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp5s0f3:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:48.585415"",
            ""stderr"": """",
            ""stdout"": ""05:00.3"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp9s0f0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005943"",
            ""end"": ""2015-04-14 20:29:48.910992"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp9s0f0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp9s0f0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:48.905049"",
            ""stderr"": """",
            ""stdout"": ""09:00.0"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i enp9s0f1: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005863"",
            ""end"": ""2015-04-14 20:29:49.143706"",
            ""invocation"": {
                ""module_args"": ""ethtool -i enp9s0f1: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""enp9s0f1:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:49.137843"",
            ""stderr"": """",
            ""stdout"": ""09:00.1"",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i lo: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005856"",
            ""end"": ""2015-04-14 20:29:49.386044"",
            ""invocation"": {
                ""module_args"": ""ethtool -i lo: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""lo:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:49.380188"",
            ""stderr"": ""Cannot get driver information: Operation not supported"",
            ""stdout"": """",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i virbr0: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.005859"",
            ""end"": ""2015-04-14 20:29:49.632356"",
            ""invocation"": {
                ""module_args"": ""ethtool -i virbr0: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""virbr0:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:49.626497"",
            ""stderr"": """",
            ""stdout"": """",
            ""warnings"": []
        },
        {
            ""changed"": true,
            ""cmd"": ""ethtool -i virbr0-nic: | grep bus-info | cut -b 16-22"",
            ""delta"": ""0:00:00.024850"",
            ""end"": ""2015-04-14 20:29:49.901539"",
            ""invocation"": {
                ""module_args"": ""ethtool -i virbr0-nic: | grep bus-info | cut -b 16-22"",
                ""module_name"": ""shell""
            },
            ""item"": ""virbr0-nic:"",
            ""rc"": 0,
            ""start"": ""2015-04-14 20:29:49.876689"",
            ""stderr"": """",
            ""stdout"": """",
            ""warnings"": []
        }
    ]

My objective is to get the value of stdout in each item above for example (""stdout"": ""09:00.0"") . I tried giving something like 
     - name: Binding the interfaces
       shell: echo {{ item.item.stdout}}
       with_flattened: var2.results
#       with_indexed_items: var2.results
       register: var3

But this is not giving the bus values in stdout correctly. Appreciate help in listing the variable of variable value in task as given below when the second variable is and indexed list. I am trying to avoid direct index numbering such as item[0] because the number of interfaces are dynamic and direct indexing may result in unexpected outcomes. 
Thanks 
",110k,"
            39
        ","['\nIs this what you\'re looking for:\n\nVariables registered for a task that has with_items have different format, they contain results for all items.\n\n- hosts: localhost\n  tags: s21\n  gather_facts: no\n  vars:\n    images:\n      - foo\n      - bar\n  tasks:\n    - shell: ""echo result-{{item}}""\n      register: ""r""\n      with_items: ""{{images}}""\n\n    - debug: var=r\n\n    - debug: msg=""item.item={{item.item}}, item.stdout={{item.stdout}}, item.changed={{item.changed}}""\n      with_items: ""{{r.results}}""\n\n    - debug: msg=""Gets printed only if this item changed - {{item}}""\n      when: ""{{item.changed == true}}""\n      with_items: ""{{r.results}}""\n\n\nSource: Register variables in with_items loop in Ansible playbook\n']",https://stackoverflow.com/questions/29635627/register-variables-in-loop-in-an-ansible-playbook,automation
Windows Console Application Getting Stuck (Needs Key Press) [duplicate],"






This question already has answers here:
                        
                    



How and why does QuickEdit mode in Command Prompt freeze applications?

                                (2 answers)
                            

Closed 6 years ago.



I have a console program that has different components that run like this:  
void start() {
while(true){
     DoSomething();
     Thread.Sleep(1000*5);
}
}

My main entry point looks like [pseudo-ish code]
Thread.Start(Componenet1.Start);
Thread.Start(Componenet2.Start);

while(true){
     Console.Writeline(""running"");
     Thread.Sleep(1000*5);
}

There are no Console.Reads anywhere. My problem is SOMETIMES the application will be running great but then stop and if I press any key on the window it will start working again. This happens fairly infrequently but I have this program deployed on 100+ VM's running 24/7 in an automated environment.
Also on the computer I have some AHK scripts and other stuff that manipulate the mouse but not sure if that has anything to do with it.
Also note that sometimes the CPU can really be running at 100% on the machines so maybe thread priority is an issue?
SOLUTION: You need to disable quick edit mode. Here is working C# code to do this:
 // http://msdn.microsoft.com/en-us/library/ms686033(VS.85).aspx
    [DllImport(""kernel32.dll"")]
    public static extern bool SetConsoleMode(IntPtr hConsoleHandle, uint dwMode);

    private const uint ENABLE_EXTENDED_FLAGS = 0x0080;

    static void Main(string[] args)
    {
         IntPtr handle = Process.GetCurrentProcess().MainWindowHandle;
         SetConsoleMode(handle, ENABLE_EXTENDED_FLAGS);

",10k,"
            38
        ","['\nIf the user accidentally clicks into the black console window, the cursor changes to a filled white rectangle, and the app hangs at the next Console.Write statement, until another clic is made.\nIt is a generic feature of the Console window when its ""QuickEdit Mode"" is enabled.\nIn order to disable that feature, you should uncheck the ""QuickEdit Mode"" option of your app\'s console window at run-time.\n']",https://stackoverflow.com/questions/4453692/windows-console-application-getting-stuck-needs-key-press,automation
element not interactable exception in selenium web automation,"
In the below code i cannot send password keys in the password field, i tried clicking the field, clearing the field and sending the keys. But now working in any of the method. But its working if i debug and test
  public class TestMail {
   protected static WebDriver driver;

   protected static String result;

   @BeforeClass

   public static void setup()  {
              System.setProperty(""webdriver.gecko.driver"",""D:\\geckodriver.exe"");

   driver = new FirefoxDriver();

   driver.manage().timeouts().implicitlyWait(60, TimeUnit.SECONDS);

  }

   @Test

 void Testcase1() {

   driver.get(""http://mail.google.com"");

   WebElement loginfield = driver.findElement(By.name(""Email""));
   if(loginfield.isDisplayed()){
       loginfield.sendKeys(""ragesh@gmail.in"");
   }
   else{
  WebElement newloginfield = driver.findElemnt(By.cssSelector(""#identifierId""));                                      
       newloginfield.sendKeys(""ragesh@gmail.in"");
      // System.out.println(""This is new login"");
   }


    driver.findElement(By.name(""signIn"")).click();

  // driver.findElement(By.cssSelector("".RveJvd"")).click();

   driver.manage().timeouts().implicitlyWait(15, TimeUnit.SECONDS);
 // WebElement pwd = driver.findElement(By.name(""Passwd""));
  WebElement pwd = driver.findElement(By.cssSelector(""#Passwd""));

  pwd.click();
  pwd.clear();
 // pwd.sendKeys(""123"");
 if(pwd.isEnabled()){
     pwd.sendKeys(""123"");
 }
 else{
     System.out.println(""Not Enabled"");
 }

",318k,"
            33
        ","['\nTry setting an implicit wait of maybe 10 seconds.\ngmail.manage().timeouts().implicitlyWait(10, TimeUnit.SECONDS);\n\nOr set an explicit wait. An explicit waits is code you define to wait for a certain condition to occur before proceeding further in the code. In your case, it is the visibility of the password input field. (Thanks to ainlolcat\'s comment)\nWebDriver gmail= new ChromeDriver();\ngmail.get(""https://www.gmail.co.in""); \ngmail.findElement(By.id(""Email"")).sendKeys(""abcd"");\ngmail.findElement(By.id(""next"")).click();\nWebDriverWait wait = new WebDriverWait(gmail, 10);\nWebElement element = wait.until(\nExpectedConditions.visibilityOfElementLocated(By.id(""Passwd"")));\ngmail.findElement(By.id(""Passwd"")).sendKeys(""xyz"");\n\nExplanation: The reason selenium can\'t find the element is because the id of the password input field is initially Passwd-hidden. After you click on the ""Next"" button, Google first verifies the email address entered and then shows the password input field (by changing the id from Passwd-hidden to Passwd). So, when the password field is still hidden (i.e. Google is still verifying the email id), your webdriver starts searching for the password input field with id Passwd which is still hidden. And hence, an exception is thrown.\n', '\n""element not interactable"" error can mean two things :\na.  Element has not properly rendered:\nSolution for this is just to use implicit /explicit wait\n\nImplicit wait :\ndriver.manage().timeouts().implicitlyWait(50, TimeUnit.SECONDS);\n\nExplicit wait :\nWebDriverWait wait=new WebDriverWait(driver, 20);\nelement1 = wait.until(ExpectedConditions.elementToBeClickable(By.className(""fa-stack-1x"")));\n\n\nb. Element has rendered but it is not in the visible part of the screen:\nSolution is just to scroll till the element. Based on the version of Selenium it can be handled in different ways but I will provide a solution that works in all versions :\n    JavascriptExecutor executor = (JavascriptExecutor) driver;\n    executor.executeScript(""arguments[0].scrollIntoView(true);"", element1);\n\n\nSuppose all this fails then another way is to again make use of Javascript executor as following :\nexecutor.executeScript(""arguments[0].click();"", element1);\n\nIf you still can\'t click , then it could again mean two things :\n\n\n1. Iframe\nCheck the DOM to see if the element you are inspecting lives in any frame. If that is true then you would need to switch to this frame before attempting any operation.\n    driver.switchTo().frame(""a077aa5e""); //switching the frame by ID\n    System.out.println(""********We are switching to the iframe*******"");\n    driver.findElement(By.xpath(""html/body/a/img"")).click();\n\n2. New tab\nIf a new tab has opened up and the element exists on it then you again need to code something like below to switch to it before attempting operation.\nString parent = driver.getWindowHandle();\ndriver.findElement(By.partialLinkText(""Continue"")).click();\nSet<String> s = driver.getWindowHandles();\n// Now iterate using Iterator\nIterator<String> I1 = s.iterator();\nwhile (I1.hasNext()) {\nString child_window = I1.next();\nif (!parent.equals(child_window)) {\n    driver.switchTo().window(child_window);\n    element1.click() \n}\n\n', '\nPlease try selecting the password field like this.\n    WebDriverWait wait = new WebDriverWait(driver, 10);\n    WebElement passwordElement = wait.until(ExpectedConditions.elementToBeClickable(By.cssSelector(""#Passwd"")));\n    passwordElement.click();\n  passwordElement.clear();\n     passwordElement.sendKeys(""123"");\n\n', '\nyou may also try full xpath, I had a similar issue where I had to click on an element which has a property javascript onclick function. the full xpath method worked and no interactable exception was thrown.\n', '\nIn my case the element that generated the Exception was a button belonging to a form. I replaced\nWebElement btnLogin = driver.findElement(By.cssSelector(""button""));\nbtnLogin.click();\n\nwith\nbtnLogin.submit();\n\nMy environment was chromedriver windows 10\n', ""\nIn my case, I'm using python-selenium.\nI have two instructions. The second instruction wasn't able to execute.\nI put a time.sleep(1) between two instructions and I'm done.\nIf you want you can change the sleep amount according to your need.\n"", '\nI had the same problem and then figured out the cause. I was trying to type in a span tag instead of an input tag. My XPath was written with a span tag, which was a wrong thing to do. I reviewed the Html for the element and found the problem. All I then did was to find the input tag which happens to be a child element. You can only type in an input field if your XPath is created with an input tagname\n', ""\nI'm going to hedge this answer with this: I know it's crap.. and there's got to be a better way. (See above answers) But I tried all the suggestions here and still got nill. Ended up chasing errors, ripping the code to bits. Then I tried this:\nimport keyboard    \nkeyboard.press_and_release('tab')\nkeyboard.press_and_release('tab')\nkeyboard.press_and_release('tab') #repeat as needed\nkeyboard.press_and_release('space') \n\nIt's pretty insufferable and you've got to make sure that you don't lose focus otherwise you'll just be tabbing and spacing on the wrong thing.\nMy assumption on why the other methods didn't work for me is that I'm trying to click on something the developers didn't want a bot clicking on. So I'm not clicking on it!\n"", '\nI got this error because I was using a wrong CSS selector with the Selenium WebDriver Node.js function By.css().\nYou can check if your selector is correct by using it in the web console of your web browser (Ctrl+Shift+K shortcut), with the JavaScript function document.querySelectorAll().\n', '\nIf it\'s working in the debug, then wait must be the proper solution.\nI will suggest to use the explicit wait, as given below:\nWebDriverWait wait = new WebDriverWait(new ChromeDriver(), 5);\nwait.until(ExpectedConditions.presenceOfElementLocated(By.cssSelector(""#Passwd"")));\n\n', '\nI came across this error too.  I thought it might have been because the field was not visible.  I tried the scroll solution above and although the field became visible in the controlled browser session I still got the exception.  The solution I am committing looks similar to below.  It looks like the event can bubble to the contained input field and the end result is the Selected property becomes true.\nThe field appears in my page something like this.\n<label>\n  <input name=""generic"" type=""checkbox"" ... >\n<label>\n\nThe generic working code looks more or less like this:\nvar checkbox = driver.FindElement(By.Name(""generic""), mustBeVisible: false);\ncheckbox.Selected.Should().BeFalse();\nvar label = checkbox.FindElement(By.XPath(""..""));\nlabel.Click();\ncheckbox.Selected.Should().BeTrue();\n\nYou\'ll need to translate this to your specific language.  I\'m using C# and FluentAssertions.  This solution worked for me with Chrome 94 and Selenium 3.141.0.\n', ""\nI had to hover over the element first for the sub-elements to appear. I didn't take that into account at first.\n    WebElement boardMenu = this.driver.findElement(By.linkText(boardTitle));\n    Actions action = new Actions(this.driver);\n\n    action.moveToElement(boardMenu).perform();\n\nAnother tip is to check that you are having one element of that DOM. Try using Ctrl+F when inspecting the web page and check your xpath there; it should return one element if you are going with the findElement method.\n""]",https://stackoverflow.com/questions/45183797/element-not-interactable-exception-in-selenium-web-automation,automation
