Title,Description,Views,Votes,Answers
Can scrapy be used to scrape dynamic content from websites that are using AJAX?,"
I have recently been learning Python and am dipping my hand into building a web-scraper.  It's nothing fancy at all; its only purpose is to get the data off of a betting website and have this data put into Excel.
Most of the issues are solvable and I'm having a good little mess around. However I'm hitting a massive hurdle over one issue. If a site loads a table of horses and lists current betting prices this information is not in any source file. The clue is that this data is live sometimes, with the numbers being updated obviously from some remote server. The HTML on my PC simply has a hole where their servers are pushing through all the interesting data that I need.
Now my experience with dynamic web content is low, so this thing is something I'm having trouble getting my head around. 
I think Java or Javascript is a key, this pops up often. 
The scraper is simply a odds comparison engine.  Some sites have APIs but I need this for those that don't. I'm using the scrapy library with Python 2.7
I do apologize if this question is too open-ended. In short, my question is: how can scrapy be used to scrape this dynamic data so that I can use it?  So that I can scrape this betting odds data in real-time?
",146k,"
            164
        ","['\nHere is a simple example of  scrapy with an AJAX request. Let see the site rubin-kazan.ru.\nAll messages are loaded with an AJAX request. My goal is to fetch these messages with all their attributes (author, date, ...):\n\nWhen I analyze the source code of the page I can\'t see all these messages because the web page uses AJAX technology. But I can with Firebug from Mozilla Firefox (or an equivalent tool in other browsers) to analyze the HTTP request that generate the messages on the web page:\n\nIt doesn\'t reload the whole page but only the parts of the page that contain messages. For this purpose I click an arbitrary number of page on the bottom:\n\nAnd I observe the HTTP request that is responsible for message body:\n\nAfter finish, I analyze the headers of the request (I must quote that this URL I\'ll extract from source page from var section, see the code below):\n\nAnd the form data content of the request (the HTTP method is ""Post""):\n\nAnd the content of response, which is a JSON file:\n\nWhich presents all the information I\'m looking for.\nFrom now, I must implement all this knowledge in scrapy. Let\'s define the spider for this purpose:\nclass spider(BaseSpider):\n    name = \'RubiGuesst\'\n    start_urls = [\'http://www.rubin-kazan.ru/guestbook.html\']\n\n    def parse(self, response):\n        url_list_gb_messages = re.search(r\'url_list_gb_messages=""(.*)""\', response.body).group(1)\n        yield FormRequest(\'http://www.rubin-kazan.ru\' + url_list_gb_messages, callback=self.RubiGuessItem,\n                          formdata={\'page\': str(page + 1), \'uid\': \'\'})\n\n    def RubiGuessItem(self, response):\n        json_file = response.body\n\nIn parse function I have the response for first request.\nIn RubiGuessItem I have the JSON file with all information. \n', ""\nWebkit based browsers (like Google Chrome or Safari) has built-in developer tools. In Chrome you can open it Menu->Tools->Developer Tools. The Network tab allows you to see all information about every request and response:\n\nIn the bottom of the picture you can see that I've filtered request down to XHR - these are requests made by javascript code.\nTip: log is cleared every time you load a page, at the bottom of the picture, the black dot button will preserve log.\nAfter analyzing requests and responses you can simulate these requests from your web-crawler and extract valuable data. In many cases it will be easier to get your data than parsing HTML, because that data does not contain presentation logic and is formatted to be accessed by javascript code.\nFirefox has similar extension, it is called firebug. Some will argue that firebug is even more powerful but I like the simplicity of webkit.\n"", '\nMany times when crawling we run into problems where content that is rendered on the page is generated with Javascript and therefore scrapy is unable to crawl for it (eg. ajax requests, jQuery craziness).\nHowever, if you use Scrapy along with the web testing framework Selenium then we are able to crawl anything displayed in a normal web browser.\nSome things to note:\n\nYou must have the Python version of Selenium RC installed for this to work, and you must have set up Selenium properly.  Also this is just a template crawler.  You could get much crazier and more advanced with things but I just wanted to show the basic idea.  As the code stands now you will be doing two requests for any given url.  One request is made by Scrapy and the other is made by Selenium.  I am sure there are ways around this so that you could possibly just make Selenium do the one and only request but I did not bother to implement that and by doing two requests you get to crawl the page with Scrapy too.\nThis is quite powerful because now you have the entire rendered DOM available for you to crawl and you can still use all the nice crawling features in Scrapy.  This will make for slower crawling of course but depending on how much you need the rendered DOM it might be worth the wait.\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.http import Request\n\nfrom selenium import selenium\n\nclass SeleniumSpider(CrawlSpider):\n    name = ""SeleniumSpider""\n    start_urls = [""http://www.domain.com""]\n\n    rules = (\n        Rule(SgmlLinkExtractor(allow=(\'\\.html\', )), callback=\'parse_page\',follow=True),\n    )\n\n    def __init__(self):\n        CrawlSpider.__init__(self)\n        self.verificationErrors = []\n        self.selenium = selenium(""localhost"", 4444, ""*chrome"", ""http://www.domain.com"")\n        self.selenium.start()\n\n    def __del__(self):\n        self.selenium.stop()\n        print self.verificationErrors\n        CrawlSpider.__del__(self)\n\n    def parse_page(self, response):\n        item = Item()\n\n        hxs = HtmlXPathSelector(response)\n        #Do some XPath selection with Scrapy\n        hxs.select(\'//div\').extract()\n\n        sel = self.selenium\n        sel.open(response.url)\n\n        #Wait for javscript to load in Selenium\n        time.sleep(2.5)\n\n        #Do some crawling of javascript created content with Selenium\n        sel.get_text(""//div"")\n        yield item\n\n# Snippet imported from snippets.scrapy.org (which no longer works)\n# author: wynbennett\n# date  : Jun 21, 2011\n\n\nReference: http://snipplr.com/view/66998/\n', '\nAnother solution would be to implement a download handler or download handler middleware. (see scrapy docs for more information on downloader middleware) The following is an example class using selenium with headless phantomjs webdriver: \n1) Define class within the middlewares.py script.\nfrom selenium import webdriver\nfrom scrapy.http import HtmlResponse\n\nclass JsDownload(object):\n\n    @check_spider_middleware\n    def process_request(self, request, spider):\n        driver = webdriver.PhantomJS(executable_path=\'D:\\phantomjs.exe\')\n        driver.get(request.url)\n        return HtmlResponse(request.url, encoding=\'utf-8\', body=driver.page_source.encode(\'utf-8\'))\n\n2) Add JsDownload() class to variable DOWNLOADER_MIDDLEWARE within settings.py:\nDOWNLOADER_MIDDLEWARES = {\'MyProj.middleware.MiddleWareModule.MiddleWareClass\': 500}\n\n3) Integrate the HTMLResponse within your_spider.py. Decoding the response body will get you the desired output.\nclass Spider(CrawlSpider):\n    # define unique name of spider\n    name = ""spider""\n\n    start_urls = [""https://www.url.de""] \n\n    def parse(self, response):\n        # initialize items\n        item = CrawlerItem()\n\n        # store data as items\n        item[""js_enabled""] = response.body.decode(""utf-8"") \n\nOptional Addon: \nI wanted the ability to tell different spiders which middleware to use so I implemented this wrapper:\ndef check_spider_middleware(method):\n@functools.wraps(method)\ndef wrapper(self, request, spider):\n    msg = \'%%s %s middleware step\' % (self.__class__.__name__,)\n    if self.__class__ in spider.middleware:\n        spider.log(msg % \'executing\', level=log.DEBUG)\n        return method(self, request, spider)\n    else:\n        spider.log(msg % \'skipping\', level=log.DEBUG)\n        return None\n\nreturn wrapper\n\nfor wrapper to work all spiders must have at minimum:\nmiddleware = set([])\n\nto include a middleware:\nmiddleware = set([MyProj.middleware.ModuleName.ClassName])\n\nAdvantage: \nThe main advantage to implementing it this way rather than in the spider is that you only end up making one request. In A T\'s solution for example: The download handler processes the request and then hands off the response to the spider. The spider then makes a brand new request in it\'s parse_page function -- That\'s two requests for the same content.\n', '\nI was using a custom downloader middleware, but wasn\'t very happy with it, as I didn\'t manage to make the cache work with it.\nA better approach was to implement a custom download handler.\nThere is a working example here. It looks like this:\n# encoding: utf-8\nfrom __future__ import unicode_literals\n\nfrom scrapy import signals\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.xlib.pydispatch import dispatcher\nfrom selenium import webdriver\nfrom six.moves import queue\nfrom twisted.internet import defer, threads\nfrom twisted.python.failure import Failure\n\n\nclass PhantomJSDownloadHandler(object):\n\n    def __init__(self, settings):\n        self.options = settings.get(\'PHANTOMJS_OPTIONS\', {})\n\n        max_run = settings.get(\'PHANTOMJS_MAXRUN\', 10)\n        self.sem = defer.DeferredSemaphore(max_run)\n        self.queue = queue.LifoQueue(max_run)\n\n        SignalManager(dispatcher.Any).connect(self._close, signal=signals.spider_closed)\n\n    def download_request(self, request, spider):\n        """"""use semaphore to guard a phantomjs pool""""""\n        return self.sem.run(self._wait_request, request, spider)\n\n    def _wait_request(self, request, spider):\n        try:\n            driver = self.queue.get_nowait()\n        except queue.Empty:\n            driver = webdriver.PhantomJS(**self.options)\n\n        driver.get(request.url)\n        # ghostdriver won\'t response when switch window until page is loaded\n        dfd = threads.deferToThread(lambda: driver.switch_to.window(driver.current_window_handle))\n        dfd.addCallback(self._response, driver, spider)\n        return dfd\n\n    def _response(self, _, driver, spider):\n        body = driver.execute_script(""return document.documentElement.innerHTML"")\n        if body.startswith(""<head></head>""):  # cannot access response header in Selenium\n            body = driver.execute_script(""return document.documentElement.textContent"")\n        url = driver.current_url\n        respcls = responsetypes.from_args(url=url, body=body[:100].encode(\'utf8\'))\n        resp = respcls(url=url, body=body, encoding=""utf-8"")\n\n        response_failed = getattr(spider, ""response_failed"", None)\n        if response_failed and callable(response_failed) and response_failed(resp, driver):\n            driver.close()\n            return defer.fail(Failure())\n        else:\n            self.queue.put(driver)\n            return defer.succeed(resp)\n\n    def _close(self):\n        while not self.queue.empty():\n            driver = self.queue.get_nowait()\n            driver.close()\n\nSuppose your scraper is called ""scraper"". If you put the mentioned code inside a file called handlers.py on the root of the ""scraper"" folder, then you could add to your settings.py:\nDOWNLOAD_HANDLERS = {\n    \'http\': \'scraper.handlers.PhantomJSDownloadHandler\',\n    \'https\': \'scraper.handlers.PhantomJSDownloadHandler\',\n}\n\nAnd voilà, the JS parsed DOM, with scrapy cache, retries, etc.\n', ""\n\nhow can scrapy be used to scrape this dynamic data so that I can use\n  it?\n\nI wonder why no one has posted the solution using Scrapy only. \nCheck out the blog post from Scrapy team SCRAPING INFINITE SCROLLING PAGES\n. The example scraps http://spidyquotes.herokuapp.com/scroll website which uses infinite scrolling. \nThe idea is to use Developer Tools of your browser and notice the AJAX requests, then based on that information create the requests for Scrapy.\nimport json\nimport scrapy\n\n\nclass SpidyQuotesSpider(scrapy.Spider):\n    name = 'spidyquotes'\n    quotes_base_url = 'http://spidyquotes.herokuapp.com/api/quotes?page=%s'\n    start_urls = [quotes_base_url % 1]\n    download_delay = 1.5\n\n    def parse(self, response):\n        data = json.loads(response.body)\n        for item in data.get('quotes', []):\n            yield {\n                'text': item.get('text'),\n                'author': item.get('author', {}).get('name'),\n                'tags': item.get('tags'),\n            }\n        if data['has_next']:\n            next_page = data['page'] + 1\n            yield scrapy.Request(self.quotes_base_url % next_page)\n\n"", '\nData that generated from external url which is API calls HTML response as POST method.\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass TestSpider(scrapy.Spider):\n    name = \'test\'  \n    def start_requests(self):\n        url = \'https://howlongtobeat.com/search_results?page=1\'\n        payload = ""queryString=&t=games&sorthead=popular&sortd=0&plat=&length_type=main&length_min=&length_max=&v=&f=&g=&detail=&randomize=0""\n        headers = {\n            ""content-type"":""application/x-www-form-urlencoded"",\n            ""user-agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36""\n        }\n\n        yield scrapy.Request(url,method=\'POST\', body=payload,headers=headers,callback=self.parse)\n\n    def parse(self, response):\n        cards = response.css(\'div[class=""search_list_details""]\')\n\n        for card in cards: \n            game_name = card.css(\'a[class=text_white]::attr(title)\').get()\n            yield {\n                ""game_name"":game_name\n            }\n           \n\nif __name__ == ""__main__"":\n    process =CrawlerProcess()\n    process.crawl(TestSpider)\n    process.start()\n\n', '\nYes, Scrapy can scrape dynamic websites, website that are rendered through JavaScript.\nThere are Two approaches to scrapy these kind of websites.\n\nyou can use splash to render Javascript code and then parse the rendered HTML.\nyou can find the doc and project here Scrapy splash, git\n\nas previously stated, by monitoring the network calls, yes, you can find the API call that fetch the data and mock that call in your scrapy spider might help you to get desired data.\n\n\n', ""\nThere are a few more modern alternatives in 2022 that I think should be mentioned, and I would like to list some pros and cons for the methods discussed in the more popular answers to this question.\n\nThe top answer and several others discuss using the browsers dev tools or packet capturing software to try to identify patterns in response url's, and try to re-construct them to use as scrapy.Requests.\n\nPros: This is still the best option in my opinion, and when it is available it is quick and often times simpler than even the traditional approach i.e. extracting content from the HTML using xpath and css selectors.\n\nCons: Unfortunately this is only available on a fraction of dynamic sites and frequently websites have security measures in place that make using this strategy difficult.\n\n\n\nUsing Selenium Webdriver is the other approach mentioned a lot in previous answers.\n\nPros: It's easy to implement, and integrate into the scrapy workflow. Additionally there are a ton of examples, and requires very little configuration if you use 3rd-party extensions like scrapy-selenium\n\nCons: It's slow!  One of scrapy's key features is it's asynchronous workflow that makes it easy to crawl dozens or even hundreds of pages in seconds. Using selenium cuts this down significantly.\n\n\n\n\nThere are two new methods that defenitely worth consideration, scrapy-splash and scrapy-playwright.\nscrapy-splash:\n\nA scrapy plugin that integrates splash, a javascript rendering service created  and maintained by the developers of scrapy, into the scrapy workflow. The plugin can be installed from pypi with pip3 install scrapy-splash, while splash needs to run in it's own process, and is easiest to run from a docker container.\n\nscrapy-playwright:\n\nPlaywright is a browser automation tool kind of like selenium, but without the crippling decrease in speed that comes with using selenium. Playwright has no issues fitting into the asynchronous scrapy workflow making sending requests just as quick as using scrapy alone. It is also much easier to install and integrate than selenium. The scrapy-playwright plugin is maintained by the developers of scrapy as well, and after installing via pypi with pip3 install scrapy-playwright is as easy as running playwright install in the terminal.\n\nMore details and many examples can be found at each of the plugin's github pages https://github.com/scrapy-plugins/scrapy-playwright and https://github.com/scrapy-plugins/scrapy-splash.\np.s.  Both projects tend to work better in a linux environment in my experience. for windows users i recommend using it with The Windows Subsystem for Linux(wsl).\n"", '\nI handle the ajax request by using Selenium and the Firefox web driver. It is not that fast if you need the crawler as a daemon, but much better than any manual solution.\n']"
PhantomJS failing to open HTTPS site,"
I'm using the following code based on loadspeed.js example to open up a https:// site which requires http server authentication as well.
var page = require('webpage').create(), system = require('system'), t, address;

page.settings.userName = 'myusername';
page.settings.password = 'mypassword';

if (system.args.length === 1) {
    console.log('Usage: scrape.js <some URL>');
    phantom.exit();
} else {
    t = Date.now();
    address = system.args[1];
    page.open(address, function (status) {
        if (status !== 'success') {
            console.log('FAIL to load the address');
        } else {
            t = Date.now() - t;
            console.log('Page title is ' + page.evaluate(function () {
                return document.title;
            }));
            console.log('Loading time ' + t + ' msec');
        }
        phantom.exit();
    });
}  

Its failing to load the page all the time. What could be wrong here? Are secured sites to be handled any differently? The site can be accessed successfully from browser though.
I'm just starting with Phantom right now and find it too good to stop playing around even though i'm not moving forward with this issue.
",77k,"
            104
        ","[""\nI tried Fred's and Cameron Tinker's answers, but only --ssl-protocol=any option seem to help me:\nphantomjs --ssl-protocol=any test.js\n\nAlso I think it should be way safer to use --ssl-protocol=any as you still are using encryption, but --ignore-ssl-errors=true will ignore (duh) all ssl errors, including malicious ones.\n"", ""\nThe problem is most likely due to SSL certificate errors. If you start phantomjs with the --ignore-ssl-errors=yes option, it should proceed to load the page as it would if there were no SSL errors:\nphantomjs --ignore-ssl-errors=yes [phantomOptions] script.js [scriptOptions]\n\nI've seen a few websites having problems with incorrectly implementing their SSL certificates or they've expired, etc. A complete list of command line options for phantomjs is available here: http://phantomjs.org/api/command-line.html.\n"", '\nNote that as of 2014-10-16, PhantomJS defaults to using SSLv3 to open HTTPS connections. With the POODLE vulnerability recently announced, many servers are disabling SSLv3 support.\nTo get around that, you should be able to run PhantomJS with:\nphantomjs --ssl-protocol=tlsv1\n\nHopefully, PhantomJS will be updated soon to make TLSv1 the default instead of SSLv3.\n', '\nexperienced same issue...\n--ignore-ssl-errors=yes was not enough to fix it for me,\nhad to do two more things:\n\n1) change user-agent\n\n2) tried all ssl-protocols, the only one that worked was tlsv1 for the page in question\n\nHope this helps...\n', '\nI experienced the same problem (casperjs 1.1.0-beta3/phantomjs 1.9.7). Using --ignore-ssl-errors=yes and --ssl-protocol=tlsv1 solved it. Using only one of the options did not solve it for me.\n', '\nI was receiving \n\nError creating SSL context"" from phantomJS (running on CentOS 6.6)\n\nBuilding from source fixed it for me. Don\'t forget to use the phantomjs that you built. (instead of the /usr/local/bin/phantomjs if you have it)\nsudo yum -y install gcc gcc-c++ make flex bison gperf ruby openssl-devel freetype-devel fontconfig-devel libicu-devel sqlite-devel libpng-devel libjpeg-devel\ngit clone git://github.com/ariya/phantomjs.git\ncd phantomjs\ngit checkout 2.0\n./build.sh\ncd bin/\n./phantomjs <your JS file>\n\n', '\nIf someone is using Phantomjs with Sahi the --ignore-ssl-errors option needs to go in your browser_types.xml file. It worked for me.\n<browserType>\n    <name>phantomjs</name>\n    <displayName>PhantomJS</displayName>\n    <icon>safari.png</icon>\n    <path>/usr/local/Cellar/phantomjs/1.9.2/bin/phantomjs</path>\n    <options>--ignore-ssl-errors=yes --debug=yes --proxy=localhost:9999 /usr/local/Cellar/phantomjs/phantom-sahi.js</options>\n    <processName>""PhantomJS""</processName>\n    <capacity>100</capacity>\n    <force>true</force>\n</browserType>\n\n', ""\nWhat about shebang?\nIf you're using shebang to execute phantomjs scripts, use the following shebang line\n#!/usr/bin/phantomjs --ignore-ssl-errors=yes\n    \nvar system = require('system');\nvar webpage = require('webpage');\n\n// ... rest of your script\n\nUse any of the above answers. i personally like --ignore-ssl-errors=yes since it's irrelevant to validate my loopback web servers' self-signed certificate.\n"", '\nNone of the other answers here helped me; it may be that the specific site(s) I was working with were too picky with their HTTP headers. This is what worked:\nvar page = webpage.create();\npage.customHeaders = {\n    ""Connection"": ""keep-alive""\n};\n\nI found out that PhantomJS was using ""Keep-Alive"" (capitalized), and the connection was not being kept alive. :)\n', '\nI was getting SSL Handshake Failed yesterday. I tried many combinations of phantomJS options (--ignore-ssl-errors=yes etc.), but none of them worked.\nUpgrading to phantomJS 2.1.1 fixed it. \nI used the phantomJS installation instructions at https://gist.github.com/julionc/7476620, changing the phantomJS version to 2.1.1.\n', '\nOn the machine you are trying to run phantomjs on to connect to a remote server, run ""openssl ciphers."" Copy and paste the ciphers listed into the --ssl-ciphers="""" command line option. This tells the connecting web server which ciphers are available to use to communicate with your client. If you don\'t set the ones available on your own machine, it can use any cipher your machine does not understand that the default modern browsers do that are used for the default setting.\n', '\nphantomjs --web-security=false --ignore-ssl-errors=true scripts.js\n', '\nThe only thing that worked for me was upping phantomjs from 1.9x to 2.x ;)\n']"
Web scraping with Python [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I'd like to grab daily sunrise/sunset times from a web site. Is it possible to scrape web content with Python? what are the modules used? Is there any tutorial available?
",213k,"
            196
        ","[""\nUse urllib2 in combination with the brilliant BeautifulSoup library:\nimport urllib2\nfrom BeautifulSoup import BeautifulSoup\n# or if you're using BeautifulSoup4:\n# from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(urllib2.urlopen('http://example.com').read())\n\nfor row in soup('table', {'class': 'spad'})[0].tbody('tr'):\n    tds = row('td')\n    print tds[0].string, tds[1].string\n    # will print date and sunrise\n\n"", ""\nI'd really recommend Scrapy.\nQuote from a deleted answer:\n\n\nScrapy crawling is fastest than mechanize because uses asynchronous operations (on top of Twisted).\nScrapy has better and fastest support for parsing (x)html on top of libxml2.\nScrapy is a mature framework with full unicode, handles redirections, gzipped responses, odd encodings, integrated http cache, etc.\nOnce you are into Scrapy, you can write a spider in less than 5 minutes that download images, creates thumbnails and export the extracted data directly to csv or json.\n\n\n"", '\nI collected together scripts from my web scraping work into this bit-bucket library.\nExample script for your case:\nfrom webscraping import download, xpath\nD = download.Download()\n\nhtml = D.get(\'http://example.com\')\nfor row in xpath.search(html, \'//table[@class=""spad""]/tbody/tr\'):\n    cols = xpath.search(row, \'/td\')\n    print \'Sunrise: %s, Sunset: %s\' % (cols[1], cols[2])\n\nOutput:\nSunrise: 08:39, Sunset: 16:08\nSunrise: 08:39, Sunset: 16:09\nSunrise: 08:39, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:11\nSunrise: 08:40, Sunset: 16:12\nSunrise: 08:40, Sunset: 16:13\n\n', ""\nI would strongly suggest checking out pyquery. It uses jquery-like (aka css-like) syntax which makes things really easy for those coming from that background.\nFor your case, it would be something like:\nfrom pyquery import *\n\nhtml = PyQuery(url='http://www.example.com/')\ntrs = html('table.spad tbody tr')\n\nfor tr in trs:\n  tds = tr.getchildren()\n  print tds[1].text, tds[2].text\n\nOutput:\n5:16 AM 9:28 PM\n5:15 AM 9:30 PM\n5:13 AM 9:31 PM\n5:12 AM 9:33 PM\n5:11 AM 9:34 PM\n5:10 AM 9:35 PM\n5:09 AM 9:37 PM\n\n"", ""\nYou can use urllib2 to make the HTTP requests, and then you'll have web content.\nYou can get it like this:\nimport urllib2\nresponse = urllib2.urlopen('http://example.com')\nhtml = response.read()\n\nBeautiful Soup is a python HTML parser that is supposed to be good for screen scraping.\nIn particular, here is their tutorial on parsing an HTML document.\nGood luck!\n"", '\nI use a combination of Scrapemark (finding urls - py2) and httlib2 (downloading images - py2+3). The scrapemark.py has 500 lines of code, but uses regular expressions, so it may be not so fast, did not test.\nExample for scraping your website:\n\nimport sys\nfrom pprint import pprint\nfrom scrapemark import scrape\n\npprint(scrape(""""""\n    <table class=""spad"">\n        <tbody>\n            {*\n                <tr>\n                    <td>{{[].day}}</td>\n                    <td>{{[].sunrise}}</td>\n                    <td>{{[].sunset}}</td>\n                    {# ... #}\n                </tr>\n            *}\n        </tbody>\n    </table>\n"""""", url=sys.argv[1] ))\n\nUsage:\npython2 sunscraper.py http://www.example.com/\n\nResult:\n[{\'day\': u\'1. Dez 2012\', \'sunrise\': u\'08:18\', \'sunset\': u\'16:10\'},\n {\'day\': u\'2. Dez 2012\', \'sunrise\': u\'08:19\', \'sunset\': u\'16:10\'},\n {\'day\': u\'3. Dez 2012\', \'sunrise\': u\'08:21\', \'sunset\': u\'16:09\'},\n {\'day\': u\'4. Dez 2012\', \'sunrise\': u\'08:22\', \'sunset\': u\'16:09\'},\n {\'day\': u\'5. Dez 2012\', \'sunrise\': u\'08:23\', \'sunset\': u\'16:08\'},\n {\'day\': u\'6. Dez 2012\', \'sunrise\': u\'08:25\', \'sunset\': u\'16:08\'},\n {\'day\': u\'7. Dez 2012\', \'sunrise\': u\'08:26\', \'sunset\': u\'16:07\'}]\n\n', '\nMake your life easier by using CSS Selectors\nI know I have come late to party but I have a nice suggestion for you.\nUsing BeautifulSoup is already been suggested I would rather prefer using CSS Selectors to scrape data inside HTML\nimport urllib2\nfrom bs4 import BeautifulSoup\n\nmain_url = ""http://www.example.com""\n\nmain_page_html  = tryAgain(main_url)\nmain_page_soup = BeautifulSoup(main_page_html)\n\n# Scrape all TDs from TRs inside Table\nfor tr in main_page_soup.select(""table.class_of_table""):\n   for td in tr.select(""td#id""):\n       print(td.text)\n       # For acnhors inside TD\n       print(td.select(""a"")[0].text)\n       # Value of Href attribute\n       print(td.select(""a"")[0][""href""])\n\n# This is method that scrape URL and if it doesnt get scraped, waits for 20 seconds and then tries again. (I use it because my internet connection sometimes get disconnects)\ndef tryAgain(passed_url):\n    try:\n        page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n        return page\n    except Exception:\n        while 1:\n            print(""Trying again the URL:"")\n            print(passed_url)\n            try:\n                page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n                print(""-------------------------------------"")\n                print(""---- URL was successfully scraped ---"")\n                print(""-------------------------------------"")\n                return page\n            except Exception:\n                time.sleep(20)\n                continue \n\n', '\nIf we think of getting name of items from any specific category then we can do that by specifying the class name of that category using css selector:\nimport requests ; from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(requests.get(\'https://www.flipkart.com/\').text, ""lxml"")\nfor link in soup.select(\'div._2kSfQ4\'):\n    print(link.text)\n\nThis is the partial search results:\nPuma, USPA, Adidas & moreUp to 70% OffMen\'s Shoes\nShirts, T-Shirts...Under ₹599For Men\nNike, UCB, Adidas & moreUnder ₹999Men\'s Sandals, Slippers\nPhilips & moreStarting ₹99LED Bulbs & Emergency Lights\n\n', '\nHere is a simple web crawler, i used BeautifulSoup and we will search for all the links(anchors) who\'s class name is _3NFO0d. I used Flipkar.com, it is an online retailing store.\nimport requests\nfrom bs4 import BeautifulSoup\ndef crawl_flipkart():\n    url = \'https://www.flipkart.com/\'\n    source_code = requests.get(url)\n    plain_text = source_code.text\n    soup = BeautifulSoup(plain_text, ""lxml"")\n    for link in soup.findAll(\'a\', {\'class\': \'_3NFO0d\'}):\n        href = link.get(\'href\')\n        print(href)\n\ncrawl_flipkart()\n\n', '\nPython has good options to scrape the web. The best one with a framework is scrapy. It can be a little tricky for beginners, so here is a little help. \n1. Install python above 3.5 (lower ones till 2.7 will work). \n2. Create a environment in conda ( I did this). \n3. Install scrapy at a location and run in from there. \n4. Scrapy shell will give you an interactive interface to test you code. \n5. Scrapy startproject projectname will create a framework.\n6. Scrapy genspider spidername will create a spider. You can create as many spiders as you want. While doing this make sure you are inside the project directory. \n\n\nThe easier one is to use requests and beautiful soup. Before starting give one hour of time to go through the documentation, it will solve most of your doubts. BS4 offer wide range of parsers that you can opt for. Use user-agent and sleep to make scraping easier. BS4 returns a bs.tag so use variable[0]. If there is js running, you wont be able to scrape using requests and bs4 directly. You  could get the api link then parse the JSON to get the information you need or try selenium.  \n']"
scrape html generated by javascript with python,"
I need to scrape a site with python. I obtain the source html code with the urlib module, but I need to scrape also some html code that is generated by a javascript function (which is included in the html source). What this functions does ""in"" the site is that when you press a button it outputs some html code. How can I ""press"" this button with python code? Can scrapy help me? I captured the POST request with firebug but when I try to pass it on the url I get a 403 error. Any suggestions?
",18k,"
            18
        ","['\nIn Python, I think Selenium 1.0 is the way to go. It’s a library that allows you to control a real web browser from your language of choice.\nYou need to have the web browser in question installed on the machine your script runs on, but it looks like the most reliable way to programmatically interrogate websites that use a lot of JavaScript.\n', ""\nSince there is no comprehensive answer here, I'll go ahead and write one.\nTo scrape off JS rendered pages, we will need a browser that has a JavaScript engine (e.i, support JavaScript rendering)\nOptions like Mechanize, url2lib will not work since they DO NOT support JavaScript. \nSo here's what you do:\nSetup PhantomJS to run with Selenium. After installing the dependencies for both of them (refer this), you can use the following code as an example to fetch the fully rendered website.\nfrom selenium import webdriver\n\ndriver = webdriver.PhantomJS()\ndriver.get('http://jokes.cc.com/')\nsoupFromJokesCC = BeautifulSoup(driver.page_source) #page_source fetches page after rendering is complete\ndriver.save_screenshot('screen.png') # save a screenshot to disk\n\ndriver.quit()\n\n"", '\nI have had to do this before (in .NET) and you are basically going to have to host a browser, get it to click the button, and then interrogate the DOM (document object model) of the browser to get at the generated HTML.\nThis is definitely one of the downsides to web apps moving towards an Ajax/Javascript approach to generating HTML client-side.\n', '\nI use webkit, which is the browser renderer behind Chrome and Safari. There are Python bindings to webkit through Qt. And here is a full example to execute JavaScript and extract the final HTML.\n', ""\nFor Scrapy (great python scraping framework) there is scrapyjs: an additional downloader handler / middleware handler able to scraping javascript generated content.\nIt's based on webkit engine by pygtk, python-webkit, and python-jswebkit and it's quite simple.\n""]"
jsoup posting and cookie,"
I'm trying to use jsoup to login to a site and then scrape information, I am running into in a problem, I can login successfully and create a Document from index.php but I cannot get other pages on the site. I know I need to set a cookie after I post and then load it when I'm trying to open another page on the site. But how do I do this? The following code lets me login and get index.php
Document doc = Jsoup.connect(""http://www.example.com/login.php"")
               .data(""username"", ""myUsername"", 
                     ""password"", ""myPassword"")
               .post();

I know I can use apache httpclient to do this but I don't want to. 
",64k,"
            52
        ","['\nWhen you login to the site, it is probably setting an authorised session cookie that needs to be sent on subsequent requests to maintain the session.\nYou can get the cookie like this:\nConnection.Response res = Jsoup.connect(""http://www.example.com/login.php"")\n    .data(""username"", ""myUsername"", ""password"", ""myPassword"")\n    .method(Method.POST)\n    .execute();\n\nDocument doc = res.parse();\nString sessionId = res.cookie(""SESSIONID""); // you will need to check what the right cookie name is\n\nAnd then send it on the next request like:\nDocument doc2 = Jsoup.connect(""http://www.example.com/otherPage"")\n    .cookie(""SESSIONID"", sessionId)\n    .get();\n\n', '\n//This will get you the response.\nResponse res = Jsoup\n    .connect(""loginPageUrl"")\n    .data(""loginField"", ""login@login.com"", ""passField"", ""pass1234"")\n    .method(Method.POST)\n    .execute();\n\n//This will get you cookies\nMap<String, String> loginCookies = res.cookies();\n\n//And this is the easiest way I\'ve found to remain in session\nDocument doc = Jsoup.connect(""urlYouNeedToBeLoggedInToAccess"")\n      .cookies(loginCookies)\n      .get();\n\n', '\nWhere the code was:\nDocument doc = Jsoup.connect(""urlYouNeedToBeLoggedInToAccess"").cookies().get(); \n\nI was having difficulties until I changed it to:\nDocument doc = Jsoup.connect(""urlYouNeedToBeLoggedInToAccess"").cookies(cookies).get();\n\nNow it is working flawlessly.\n', '\nHere is what you can try...\nimport org.jsoup.Connection;\n\n\nConnection.Response res = null;\n    try {\n        res = Jsoup\n                .connect(""http://www.example.com/login.php"")\n                .data(""username"", ""your login id"", ""password"", ""your password"")\n                .method(Connection.Method.POST)\n                .execute();\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n\nNow save all your cookies and make request to the other page you want.\n//Store Cookies\ncookies = res.cookies();\n\nMaking request to another page.\ntry {\n    Document doc = Jsoup.connect(""your-second-page-link"").cookies(cookies).get();\n}\ncatch(Exception e){\n    e.printStackTrace();\n}\n\nAsk if further help needed.\n', '\nConnection.Response res = Jsoup.connect(""http://www.example.com/login.php"")\n    .data(""username"", ""myUsername"")\n    .data(""password"", ""myPassword"")\n    .method(Connection.Method.POST)\n    .execute();\n//Connecting to the server with login details\nDocument doc = res.parse();\n//This will give the redirected file\nMap<String,String> cooki=res.cookies();\n//This gives the cookies stored into cooki\nDocument docs= Jsoup.connect(""http://www.example.com/otherPage"")\n    .cookies(cooki)\n    .get();\n//This gives the data of the required website\n\n', '\nWhy reconnect?\nif there are any cookies to avoid 403 Status i do so.\n                Document doc = null;\n                int statusCode = -1;\n                String statusMessage = null;\n                String strHTML = null;\n        \n                try {\n    // connect one time.                \n                    Connection con = Jsoup.connect(urlString);\n    // get response.\n                    Connection.Response res = con.execute();        \n    // get cookies\n                    Map<String, String> loginCookies = res.cookies();\n\n    // print cookie content and status message\n                    if (loginCookies != null) {\n                        for (Map.Entry<String, String> entry : loginCookies.entrySet()) {\n                            System.out.println(entry.getKey() + "":"" + entry.getValue().toString() + ""\\n"");\n                        }\n                    }\n        \n                    statusCode = res.statusCode();\n                    statusMessage = res.statusMessage();\n                    System.out.print(""Status CODE\\n"" + statusCode + ""\\n\\n"");\n                    System.out.print(""Status Message\\n"" + statusMessage + ""\\n\\n"");\n        \n    // set login cookies to connection here\n                    con.cookies(loginCookies).userAgent(""Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0"");\n        \n    // now do whatever you want, get document for example\n                    doc = con.get();\n    // get HTML\n                    strHTML = doc.head().html();\n\n                } catch (org.jsoup.HttpStatusException hse) {\n                    hse.printStackTrace();\n                } catch (IOException ioe) {\n                    ioe.printStackTrace();\n                }\n\n']"
How to implement a web scraper in PHP? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 7 years ago.







                        Improve this question
                    



What built-in PHP functions are useful for web scraping?  What are some good resources (web or print) for getting up to speed on web scraping with PHP?
",81k,"
            61
        ","['\nScraping generally encompasses 3 steps: \n\nfirst you GET or POST your request\nto a specified URL \nnext you receive\n    the html that is returned as the\n    response\nfinally you parse out of\n    that html the text you\'d like to\n    scrape.\n\nTo accomplish steps 1 and 2, below is a simple php class which uses Curl to fetch webpages using either GET or POST.  After you get the HTML back, you just use Regular Expressions to accomplish step 3 by parsing out the text you\'d like to scrape.\nFor regular expressions, my favorite tutorial site is the following:\nRegular Expressions Tutorial\nMy Favorite program for working with RegExs is Regex Buddy.  I would advise you to try the demo of that product even if you have no intention of buying it.  It is an invaluable tool and will even generate code for your regexs you make in your language of choice (including php).\nUsage:\n\n\n$curl = new Curl();\n$html = $curl->get(""http://www.google.com"");\n\n// now, do your regex work against $html\n\nPHP Class:\n\n\n<?php\n\nclass Curl\n{       \n\n    public $cookieJar = """";\n\n    public function __construct($cookieJarFile = \'cookies.txt\') {\n        $this->cookieJar = $cookieJarFile;\n    }\n\n    function setup()\n    {\n\n\n        $header = array();\n        $header[0] = ""Accept: text/xml,application/xml,application/xhtml+xml,"";\n        $header[0] .= ""text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5"";\n        $header[] =  ""Cache-Control: max-age=0"";\n        $header[] =  ""Connection: keep-alive"";\n        $header[] = ""Keep-Alive: 300"";\n        $header[] = ""Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7"";\n        $header[] = ""Accept-Language: en-us,en;q=0.5"";\n        $header[] = ""Pragma: ""; // browsers keep this blank.\n\n\n        curl_setopt($this->curl, CURLOPT_USERAGENT, \'Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US; rv:1.8.1.7) Gecko/20070914 Firefox/2.0.0.7\');\n        curl_setopt($this->curl, CURLOPT_HTTPHEADER, $header);\n        curl_setopt($this->curl,CURLOPT_COOKIEJAR, $this->cookieJar); \n        curl_setopt($this->curl,CURLOPT_COOKIEFILE, $this->cookieJar);\n        curl_setopt($this->curl,CURLOPT_AUTOREFERER, true);\n        curl_setopt($this->curl,CURLOPT_FOLLOWLOCATION, true);\n        curl_setopt($this->curl,CURLOPT_RETURNTRANSFER, true);  \n    }\n\n\n    function get($url)\n    { \n        $this->curl = curl_init($url);\n        $this->setup();\n\n        return $this->request();\n    }\n\n    function getAll($reg,$str)\n    {\n        preg_match_all($reg,$str,$matches);\n        return $matches[1];\n    }\n\n    function postForm($url, $fields, $referer=\'\')\n    {\n        $this->curl = curl_init($url);\n        $this->setup();\n        curl_setopt($this->curl, CURLOPT_URL, $url);\n        curl_setopt($this->curl, CURLOPT_POST, 1);\n        curl_setopt($this->curl, CURLOPT_REFERER, $referer);\n        curl_setopt($this->curl, CURLOPT_POSTFIELDS, $fields);\n        return $this->request();\n    }\n\n    function getInfo($info)\n    {\n        $info = ($info == \'lasturl\') ? curl_getinfo($this->curl, CURLINFO_EFFECTIVE_URL) : curl_getinfo($this->curl, $info);\n        return $info;\n    }\n\n    function request()\n    {\n        return curl_exec($this->curl);\n    }\n}\n\n?>\n\n\n', '\nI recommend Goutte, a simple PHP Web Scraper.\nExample Usage:-\nCreate a Goutte Client instance (which extends\nSymfony\\Component\\BrowserKit\\Client):\nuse Goutte\\Client;\n\n$client = new Client();\n\nMake requests with the request() method:\n$crawler = $client->request(\'GET\', \'http://www.symfony-project.org/\');\n\nThe request method returns a Crawler object\n(Symfony\\Component\\DomCrawler\\Crawler).\nClick on links:\n$link = $crawler->selectLink(\'Plugins\')->link();\n$crawler = $client->click($link);\n\nSubmit forms:\n$form = $crawler->selectButton(\'sign in\')->form();\n$crawler = $client->submit($form, array(\'signin[username]\' => \'fabien\', \'signin[password]\' => \'xxxxxx\'));\n\nExtract data:\n$nodes = $crawler->filter(\'.error_list\');\n\nif ($nodes->count())\n{\n  die(sprintf(""Authentification error: %s\\n"", $nodes->text()));\n}\n\nprintf(""Nb tasks: %d\\n"", $crawler->filter(\'#nb_tasks\')->text());\n\n', '\nScraperWiki is a pretty interesting project.\nHelps you build scrapers online in Python, Ruby or PHP - i was able to get a simple attempt up in a few minutes.\n', ""\nIf you need something that is easy to maintain, rather than fast to execute, it could help to use a scriptable browser, such as SimpleTest's.\n"", ""\nScraping can be pretty complex, depending on what you want to do. Have a read of this tutorial series on The Basics Of Writing A Scraper In PHP and see if you can get to grips with it.\nYou can use similar methods to automate form sign ups, logins, even fake clicking on Ads! The main limitations with using CURL though are that it doesn't support using javascript, so if you are trying to scrape a site that uses AJAX for pagination for example it can become a little tricky...but again there are ways around that!\n"", '\nhere is another one: a simple PHP Scraper without Regex.\n', '\nfile_get_contents() can take a remote URL and give you the source. You can then use regular expressions (with the Perl-compatible functions) to grab what you need.\nOut of curiosity, what are you trying to scrape?\n', ""\nI'd either use libcurl or Perl's LWP (libwww for perl). Is there a libwww for php?\n"", '\nScraper class from my framework:\n<?php\n\n/*\n    Example:\n\n    $site = $this->load->cls(\'scraper\', \'http://www.anysite.com\');\n    $excss = $site->getExternalCSS();\n    $incss = $site->getInternalCSS();\n    $ids = $site->getIds();\n    $classes = $site->getClasses();\n    $spans = $site->getSpans(); \n\n    print \'<pre>\';\n    print_r($excss);\n    print_r($incss);\n    print_r($ids);\n    print_r($classes);\n    print_r($spans);        \n\n*/\n\nclass scraper\n{\n    private $url = \'\';\n\n    public function __construct($url)\n    {\n        $this->url = file_get_contents(""$url"");\n    }\n\n    public function getInternalCSS()\n    {\n        $tmp = preg_match_all(\'/(style="")(.*?)("")/is\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getExternalCSS()\n    {\n        $tmp = preg_match_all(\'/(href="")(\\w.*\\.css)""/i\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getIds()\n    {\n        $tmp = preg_match_all(\'/(id=""(\\w*)"")/is\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getClasses()\n    {\n        $tmp = preg_match_all(\'/(class=""(\\w*)"")/is\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getSpans(){\n        $tmp = preg_match_all(\'/(<span>)(.*)(<\\/span>)/\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n}\n?>\n\n', '\nThe curl library allows you to download web pages. You should look into regular expressions for doing the scraping.\n']"
Headless Browser for Python (Javascript support REQUIRED!) [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 Questions asking us to recommend or find a tool, library or favorite off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, describe the problem and what has been done so far to solve it.


Closed 8 years ago.







                        Improve this question
                    



I need a headless browser which is fairly easy to use (I am still fairly new to Python and programming in general) which will allow me to navigate to a page, log into a form that requires Javascript, and then scrape the resulting web page by searching for results matching certain criteria, clicking check boxes, and clicking to download files. All of this requires Javascript.
I hear a headless browser is what I want - requirements/preferences are that I be able to run it from Python, and preferably that the resultant script will be compilable by py2exe (I am writing this program for other users).
So far Windmill looks like it MIGHT be what I want, but I am not sure.
Any ideas appreciated!
",43k,"
            57
        ","['\nI use webkit as a headless browser in Python via pyqt / pyside:\nhttp://www.riverbankcomputing.co.uk/software/pyqt/download\nhttp://developer.qt.nokia.com/wiki/Category:LanguageBindings::PySide::Downloads\nI particularly like webkit because it is simple to setup. For Ubuntu you just use: sudo apt-get install python-qt4\nHere is an example script:\nhttp://webscraping.com/blog/Scraping-JavaScript-webpages-with-webkit/\n', '\nThe answer to this question was Spynner\n', '\nI\'m in the midst of writing a Python driver for Zombie.js, ""a lightweight framework for testing client-side JavaScript code in a simulated environment"".\nI\'m currently at a standstill on a resolution to a bug in Node.js (before I write more tests and more code), but feel free to keep an eye on my project as it progresses:\nhttps://github.com/ryanpetrello/python-zombie\n', '\nThere are not too many headless browsers yet that support Javascript.\nYou could try Zombie.js or Phantomjs. Those are not Python, but plain Javascript and those really can do the job.\n', '\nTry using phantomjs, it has great javascript support.  Then you could run it as a subprocess of a python script \nhttp://docs.python.org/library/subprocess.html\nthat could boss it around.\n', '\nYou can use HTQL in combination with IRobotSoft webscraper.  Check here for examples: http://htql.net/\n']"
Is there a PHP equivalent of Perl's WWW::Mechanize?,"
I'm looking for a library that has functionality similar to Perl's WWW::Mechanize, but for PHP. Basically, it should allow me to submit HTTP GET and POST requests with a simple syntax, and then parse the resulting page and return in a simple format all forms and their fields, along with all links on the page.
I know about CURL, but it's a little too barebones, and the syntax is pretty ugly (tons of curl_foo($curl_handle, ...) statements
Clarification:
I want something more high-level than the answers so far. For example, in Perl, you could do something like:
# navigate to the main page
$mech->get( 'http://www.somesite.com/' ); 

# follow a link that contains the text 'download this'
$mech->follow_link( text_regex => qr/download this/i );

# submit a POST form, to log into the site
$mech->submit_form(
    with_fields      => {
        username    => 'mungo',
        password    => 'lost-and-alone',
    }
);

# save the results as a file
$mech->save_content('somefile.zip');

To do the same thing using HTTP_Client or wget or CURL would be a lot of work, I'd have to manually parse the pages to find the links, find the form URL, extract all the hidden fields, and so on. The reason I'm asking for a PHP solution is that I have no experience with Perl, and I could probably build what I need with a lot of work, but it would be much quicker if I could do the above in PHP.
",15k,"
            26
        ","[""\nSimpleTest's ScriptableBrowser can be used independendly from the testing framework. I've used it for numerous automation-jobs.\n"", '\nI feel compelled to answer this, even though its an old post... I\'ve been working with PHP curl a lot and it is not as good anywhere near comparable to something like WWW:Mechanize, which I am switching to (I think I am going to go with the Ruby language implementation).. Curl is outdated as it requires too much ""grunt work"" to automate anything, the simpletest scriptable browser looked promising to me but in testing it, it won\'t work on most web forms I try it on... honestly, I think PHP is lacking in this category of scraping, web automation so its best to look at a different language, just wanted to post this since I have spent countless hours on this topic and maybe it will save someone else some time in the future.\n', '\nIt\'s 2016 now and there\'s Mink. It even supports different engines from headless pure-PHP ""browser"" (without JavaScript), over Selenium (which needs a browser like Firefox or Chrome) to a headless ""browser.js"" in NPM, which DOES support JavaScript.\n', '\nTry looking in the PEAR library. If all else fails, create an object wrapper for curl.\nYou can so something simple like this:\nclass curl {\n    private $resource;\n\n    public function __construct($url) {\n        $this->resource = curl_init($url);\n    }\n\n    public function __call($function, array $params) {\n        array_unshift($params, $this->resource);\n        return call_user_func_array(""curl_$function"", $params);\n    }\n}\n\n', ""\nTry one of the following:\n\nPEAR's HTTP_Request\nZend_Http_Client\n\n(Yes, it's ZendFramework code, but it doesn't make your class slower using it since it just loads the required libs.)\n"", '\nLook into Snoopy:\nhttp://sourceforge.net/projects/snoopy/\n', '\nCurl is the way to go for simple requests. It runs cross platform, has a PHP extension and is widely adopted and tested.\nI created a nice class that can GET and POST an array of data (INCLUDING FILES!) to a url by just calling CurlHandler::Get($url, $data) || CurlHandler::Post($url, $data). There\'s an optional HTTP User authentication option too :)\n/**\n * CURLHandler handles simple HTTP GETs and POSTs via Curl \n * \n * @package Pork\n * @author SchizoDuckie\n * @copyright SchizoDuckie 2008\n * @version 1.0\n * @access public\n */\nclass CURLHandler\n{\n\n    /**\n     * CURLHandler::Get()\n     * \n     * Executes a standard GET request via Curl.\n     * Static function, so that you can use: CurlHandler::Get(\'http://www.google.com\');\n     * \n     * @param string $url url to get\n     * @return string HTML output\n     */\n    public static function Get($url)\n    {\n       return self::doRequest(\'GET\', $url);\n    }\n\n    /**\n     * CURLHandler::Post()\n     * \n     * Executes a standard POST request via Curl.\n     * Static function, so you can use CurlHandler::Post(\'http://www.google.com\', array(\'q\'=>\'StackOverFlow\'));\n     * If you want to send a File via post (to e.g. PHP\'s $_FILES), prefix the value of an item with an @ ! \n     * @param string $url url to post data to\n     * @param Array $vars Array with key=>value pairs to post.\n     * @return string HTML output\n     */\n    public static function Post($url, $vars, $auth = false) \n    {\n       return self::doRequest(\'POST\', $url, $vars, $auth);\n    }\n\n    /**\n     * CURLHandler::doRequest()\n     * This is what actually does the request\n     * <pre>\n     * - Create Curl handle with curl_init\n     * - Set options like CURLOPT_URL, CURLOPT_RETURNTRANSFER and CURLOPT_HEADER\n     * - Set eventual optional options (like CURLOPT_POST and CURLOPT_POSTFIELDS)\n     * - Call curl_exec on the interface\n     * - Close the connection\n     * - Return the result or throw an exception.\n     * </pre>\n     * @param mixed $method Request Method (Get/ Post)\n     * @param mixed $url URI to get or post to\n     * @param mixed $vars Array of variables (only mandatory in POST requests)\n     * @return string HTML output\n     */\n    public static function doRequest($method, $url, $vars=array(), $auth = false)\n    {\n        $curlInterface = curl_init();\n\n        curl_setopt_array ($curlInterface, array( \n            CURLOPT_URL => $url,\n            CURLOPT_RETURNTRANSFER => 1,\n            CURLOPT_FOLLOWLOCATION =>1,\n            CURLOPT_HEADER => 0));\n        if (strtoupper($method) == \'POST\')\n        {\n            curl_setopt_array($curlInterface, array(\n                CURLOPT_POST => 1,\n                CURLOPT_POSTFIELDS => http_build_query($vars))\n            );  \n        }\n        if($auth !== false)\n        {\n              curl_setopt($curlInterface, CURLOPT_USERPWD, $auth[\'username\'] . "":"" . $auth[\'password\']);\n        }\n        $result = curl_exec ($curlInterface);\n        curl_close ($curlInterface);\n\n        if($result === NULL)\n        {\n            throw new Exception(\'Curl Request Error: \'.curl_errno($curlInterface) . "" - "" . curl_error($curlInterface));\n        }\n        else\n        {\n            return($result);\n        }\n    }\n\n}\n\n?>\n\n[edit] Read the clarification only now... You probably want to go with one of the tools mentioned above that automates stuff. You could also decide to use a clientside firefox extension like ChickenFoot for more flexibility. I\'ll leave the example class above here for future searches.\n', '\nIf you\'re using CakePHP in your project, or if you\'re inclined to extract the relevant library you can use their curl wrapper HttpSocket. It has the simple page-fetching syntax you describe, e.g., \n# This is the sugar for importing the library within CakePHP       \nApp::import(\'Core\', \'HttpSocket\');\n$HttpSocket = new HttpSocket();\n\n$result = $HttpSocket->post($login_url,\narray(\n  ""username"" => ""username"",\n  ""password"" => ""password""\n)\n);\n\n...although it doesn\'t have a way to parse the response page. For that I\'m going to use simplehtmldom: http://net.tutsplus.com/tutorials/php/html-parsing-and-screen-scraping-with-the-simple-html-dom-library/ which describes itself as having a jQuery-like syntax.\nI tend to agree that the bottom line is that PHP doesn\'t have the awesome scraping/automation libraries that Perl/Ruby have.\n', ""\nIf you're on a *nix system you could use shell_exec() with wget, which has a lot of nice options.\n""]"
Executing Javascript from Python,"
I have HTML webpages that I am crawling using xpath. The etree.tostring of a certain node gives me this string:
<script>
<!--
function escramble_758(){
  var a,b,c
  a='+1 '
  b='84-'
  a+='425-'
  b+='7450'
  c='9'
  document.write(a+c+b)
}
escramble_758()
//-->
</script>

I just need the output of escramble_758(). I can write a regex to figure out the whole thing, but I want my code to remain tidy. What is the best alternative?
I am zipping through the following libraries, but I didnt see an exact solution. Most of them are trying to emulate browser, making things snail slow.

http://code.google.com/p/python-spidermonkey/ (clearly says it's not yet possible to call a function defined in Javascript)
http://code.google.com/p/webscraping/ (don't see anything for Javascript, I may be wrong)
http://pypi.python.org/pypi/selenium (Emulating browser)

Edit: An example will be great.. (barebones will do)
",175k,"
            67
        ","['\nYou can also use Js2Py which is written in pure python and is able to both execute and translate javascript to python. Supports virtually whole JavaScript even labels, getters, setters and other rarely used features. \nimport js2py\n\njs = """"""\nfunction escramble_758(){\nvar a,b,c\na=\'+1 \'\nb=\'84-\'\na+=\'425-\'\nb+=\'7450\'\nc=\'9\'\ndocument.write(a+c+b)\n}\nescramble_758()\n"""""".replace(""document.write"", ""return "")\n\nresult = js2py.eval_js(js)  # executing JavaScript and converting the result to python string \n\nAdvantages of Js2Py include portability and extremely easy integration with python (since basically JavaScript is being translated to python). \nTo install:\npip install js2py\n\n', '\nUsing PyV8, I can do this. However, I have to replace document.write with return because there\'s no DOM and therefore no document.\nimport PyV8\nctx = PyV8.JSContext()\nctx.enter()\n\njs = """"""\nfunction escramble_758(){\nvar a,b,c\na=\'+1 \'\nb=\'84-\'\na+=\'425-\'\nb+=\'7450\'\nc=\'9\'\ndocument.write(a+c+b)\n}\nescramble_758()\n""""""\n\nprint ctx.eval(js.replace(""document.write"", ""return ""))\n\nOr you could create a mock document object\nclass MockDocument(object):\n\n    def __init__(self):\n        self.value = \'\'\n\n    def write(self, *args):\n        self.value += \'\'.join(str(i) for i in args)\n\n\nclass Global(PyV8.JSClass):\n    def __init__(self):\n        self.document = MockDocument()\n\nscope = Global()\nctx = PyV8.JSContext(scope)\nctx.enter()\nctx.eval(js)\nprint scope.document.value\n\n', '\nOne more solution as PyV8 seems to be unmaintained and dependent on the old version of libv8. \nPyMiniRacer It\'s a wrapper around the v8 engine and it works with the new version and is actively maintained.\npip install py-mini-racer\nfrom py_mini_racer import py_mini_racer\nctx = py_mini_racer.MiniRacer()\nctx.eval(""""""\nfunction escramble_758(){\n    var a,b,c\n    a=\'+1 \'\n    b=\'84-\'\n    a+=\'425-\'\n    b+=\'7450\'\n    c=\'9\'\n    return a+c+b;\n}\n"""""")\nctx.call(""escramble_758"")\n\nAnd yes, you have to replace document.write with return as others suggested\n', '\nYou can use js2py context to execute your js code and get output from document.write with mock document object:\nimport js2py\n\njs = """"""\nvar output;\ndocument = {\n    write: function(value){\n        output = value;\n    }\n}\n"""""" + your_script\n\ncontext = js2py.EvalJs()\ncontext.execute(js)\nprint(context.output)\n\n', '\nYou can use requests-html which will download and use chromium underneath.\nfrom requests_html import HTML\n\nhtml = HTML(html=""<a href=\'http://www.example.com/\'>"")\n\nscript = """"""\nfunction escramble_758(){\n    var a,b,c\n    a=\'+1 \'\n    b=\'84-\'\n    a+=\'425-\'\n    b+=\'7450\'\n    c=\'9\'\n    return a+c+b;\n}\n""""""\n\nval = html.render(script=script, reload=False)\nprint(val)\n# +1 425-984-7450\n\nMore on this read here\n', '\nquickjs should be the best option after quickjs come out.  Just pip install quickjs and you are ready to go.\nmodify based on the example on README.\nfrom quickjs import Function\n\njs = """"""\nfunction escramble_758(){\nvar a,b,c\na=\'+1 \'\nb=\'84-\'\na+=\'425-\'\nb+=\'7450\'\nc=\'9\'\ndocument.write(a+c+b)\nescramble_758()\n}\n""""""\n\nescramble_758 = Function(\'escramble_758\', js.replace(""document.write"", ""return ""))\n\nprint(escramble_758())\n\nhttps://github.com/PetterS/quickjs\n', '\nReally late to the party but you can use a successor of pyv8 which is regularly maintained by a reputable organization (Subjective) named CloudFlare. Here is the repository URL:\nhttps://github.com/cloudflare/stpyv8\n']"
HTML Scraping in Php [duplicate],"






This question already has answers here:
                        
                    



How do you parse and process HTML/XML in PHP?

                                (31 answers)
                            

Closed 9 years ago.



I've been doing some HTML scraping in PHP using regular expressions.  This works, but the result is finicky and fragile.  Has anyone used any packages that provide a more robust solution?  A config driven solution would be ideal, but I'm not picky.
",46k,"
            39
        ","['\nI would recomend PHP Simple HTML DOM Parser after you have scraped the HTML from the page. It supports invalid HTML, and provides a very easy way to handle HTML elements. \n', ""\nIf the page you're scraping is valid X(HT)ML, then any of PHP's built-in XML parsers will do. \nI haven't had much success with PHP libraries for scraping. If you're adventurous though, you can try simplehtmldom. I'd recommend Hpricot for Ruby or Beautiful Soup for Python, which are both excellent parsers for HTML.\n"", ""\nI would also recommend 'Simple HTML DOM Parser.' It is a good option particularly if your familiar with jQuery or JavaScript selectors then you will find yourself at home.\nI have even blogged about it in the past.\n"", '\nI had some fun working with htmlSQL, which is not so much a high end solution, but really simple to work with.\n', ""\nUsing PHP for HTML scraping, I'd recommend cURL + regexp or cURL + some DOM parsers though I personally use cURL + regexp. If you have a profound taste of regexp, it's actually more accurate sometimes.\n"", ""\nI've had very good with results with the Simple Html DOM Parser mentioned above as well. And then there's the \xa0tidy Extension for PHP as well which works really well too.\n"", '\nI had to use curl on my host 1and1.\nhttp://www.quickscrape.com/ is what I came up with using the Simple DOM class!\n']"
What's a good tool to screen-scrape with Javascript support? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



Is there a good test suite or tool set that can automate website navigation -- with Javascript support -- and collect the HTML from the pages?
Of course I can scrape straight HTML with BeautifulSoup.  But this does me no good for sites that require Javascript. :)
",26k,"
            28
        ","[""\nYou could use Selenium or Watir to drive a real browser.\nTher are also some JavaScript-based headless browsers:\n\nPhantomJS is a headless Webkit browser.\n\npjscrape is a scraping framework based on PhantomJS and jQuery.\nCasperJS is a navigation scripting & testing utility bsaed on PhantomJS, if you need to do a little more than point at URLs to be scraped.\n\n\nZombie for Node.js\n\nPersonally, I'm most familiar with Selenium, which has support for writing automation scripts in a good number of languagues and has more mature tooling, such as the excellent Selenium IDE extension for Firefox, which can be used to write and run testcases, and can export test scripts to many languages.\n"", '\nUsing HtmlUnit is also a possibility.\n\nHtmlUnit is a ""GUI-Less browser for\n  Java programs"". It models HTML\n  documents and provides an API that\n  allows you to invoke pages, fill out\n  forms, click links, etc... just like\n  you do in your ""normal"" browser.\nIt has fairly good JavaScript support\n  (which is constantly improving) and is\n  able to work even with quite complex\n  AJAX libraries, simulating either\n  Firefox or Internet Explorer depending\n  on the configuration you want to use.\nIt is typically used for testing\n  purposes or to retrieve information\n  from web sites.\n\n', '\nSelenium now wraps htmlunit so you don´t need start a browser anymore. The new WebDriver api is very easy to use too. The first example use htmlunit driver \n', ""\nIt would be very difficult to code a solution that would work with any arbitrary site out there.  Each navigation menu implementation can be quite unique.  I've worked a great deal with scrapers, and, provided you know the site you wish to target, here is how I'd approach it.\nUsually, if you analyze the particular javascript used in a nav menu, it is fairly easy to use regular expressions to pull out the entire set of variables that are used to build the navmenu.  I have never used Beautiful Soup, but from your description it sounds like it may only work on HTML elements and not be able to work inside the script tags.\nIf you're still having problems, or need to emulate some form POSTs or ajax, get Firefox and install the LiveHttpHeaders plugin.  This plugin will allow you to manually browse the site and capture the urls being navigated along with any cookies that are being passed during your manual browsing.  That is what you need your scraperbot to send in a request to get a valid response from the target webserver(s).   This will also capture any ajax calls being made, and in many cases the same ajax calls must be implementated in your scraper to get your desired responses.\n"", '\nMozenda is a great tool to use as well.\n', ""\nKeep in mind that and javascript fanciness is messing with the brower's internal DOM model of the page, and does nothing to the raw HTML.\n"", ""\nI've been using Selenium for this and it find that it works great.\nSelenium runs in Browser and will work with Firefox, Webkit and IE.\nhttp://selenium.openqa.org/\n"", '\n@insin Watir is not IE only.\nhttps://stackoverflow.com/questions/81566#83387\n']"
Scraping ajax pages using python,"
I've already seen this question about scraping ajax, but python isn't mentioned there. I considered using scrapy, i believe they have some docs on that subject, but as you can see the website is down. So i don't know what to do. I want to do the following:
I only have one url, example.com you go from page to page by clicking submit, the url doesn't change since they're using ajax to display the content. I want to scrape the content of each page, how to do it? 
Lets say that i want to scrape only the numbers, is there anything other than scrapy that would do it? If not, would you give me a snippet on how to do it, just because their website is down so i can't reach the docs.
",47k,"
            18
        ","['\nFirst of all, scrapy docs are available at https://scrapy.readthedocs.org/en/latest/.\nSpeaking about handling ajax while web scraping. Basically, the idea is rather simple: \n\nopen browser developer tools, network tab\ngo to the target site\nclick submit button and see what XHR request is going to the server\nsimulate this XHR request in your spider\n\nAlso see:\n\nCan scrapy be used to scrape dynamic content from websites that are using AJAX?\nPagination using scrapy\n\nHope that helps.\n', '\nI found the answer very useful but I would like to make it more simple.\nresponse = requests.post(request_url, data=payload, headers=request_headers)\n\nrequest.post takes three parameters url, data and headers. Values for these three attributes can be found in the XHR request.\nCopy the whole request header and form data to load into the above variables and you are good to go\n']"
Download image file from the HTML page source,"
I am writing a scraper that downloads all the image files from a HTML page and saves them to a specific folder. All the images are part of the HTML page.
",100k,"
            48
        ","['\nHere is some code to download all the images from the supplied URL, and save them in the specified output folder. You can modify it to your own needs.\n""""""\ndumpimages.py\n    Downloads all the images on the supplied URL, and saves them to the\n    specified output file (""/test/"" by default)\n\nUsage:\n    python dumpimages.py http://example.com/ [output]\n""""""\nfrom bs4 import BeautifulSoup as bs\nfrom urllib.request import (\n    urlopen, urlparse, urlunparse, urlretrieve)\nimport os\nimport sys\n\ndef main(url, out_folder=""/test/""):\n    """"""Downloads all the images at \'url\' to /test/""""""\n    soup = bs(urlopen(url))\n    parsed = list(urlparse(url))\n\n    for image in soup.findAll(""img""):\n        print(""Image: %(src)s"" % image)\n        filename = image[""src""].split(""/"")[-1]\n        parsed[2] = image[""src""]\n        outpath = os.path.join(out_folder, filename)\n        if image[""src""].lower().startswith(""http""):\n            urlretrieve(image[""src""], outpath)\n        else:\n            urlretrieve(urlunparse(parsed), outpath)\n\ndef _usage():\n    print(""usage: python dumpimages.py http://example.com [outpath]"")\n\nif __name__ == ""__main__"":\n    url = sys.argv[-1]\n    out_folder = ""/test/""\n    if not url.lower().startswith(""http""):\n        out_folder = sys.argv[-1]\n        url = sys.argv[-2]\n        if not url.lower().startswith(""http""):\n            _usage()\n            sys.exit(-1)\n    main(url, out_folder)\n\nEdit: You can specify the output folder now.\n', '\nRyan\'s solution is good, but fails if the image source URLs are absolute URLs or anything that doesn\'t give a good result when simply concatenated to the main page URL.  urljoin recognizes absolute vs. relative URLs, so replace the loop in the middle with:\nfor image in soup.findAll(""img""):\n    print ""Image: %(src)s"" % image\n    image_url = urlparse.urljoin(url, image[\'src\'])\n    filename = image[""src""].split(""/"")[-1]\n    outpath = os.path.join(out_folder, filename)\n    urlretrieve(image_url, outpath)\n\n', '\nYou have to download the page and parse html document, find your image with regex and download it.. You can use urllib2 for downloading and Beautiful Soup for parsing html file.\n', '\nAnd this is function for download one image:\ndef download_photo(self, img_url, filename):\n    file_path = ""%s%s"" % (DOWNLOADED_IMAGE_PATH, filename)\n    downloaded_image = file(file_path, ""wb"")\n\n    image_on_web = urllib.urlopen(img_url)\n    while True:\n        buf = image_on_web.read(65536)\n        if len(buf) == 0:\n            break\n        downloaded_image.write(buf)\n    downloaded_image.close()\n    image_on_web.close()\n\n    return file_path\n\n', '\nUse htmllib to extract all img tags (override do_img), then use urllib2 to download all the images.\n', ""\nIf the request need an authorization refer to this one:\nr_img = requests.get(img_url, auth=(username, password)) \nf = open('000000.jpg','wb') \nf.write(r_img.content) \nf.close()\n\n"", '\nBased on code here\nRemoving some lines of code, you\'ll get only the images img tags.\nUses Python 3+ Requests, BeautifulSoup and other standard libraries.\nimport os, sys\nimport requests\nfrom urllib import parse\nfrom bs4 import BeautifulSoup\nimport re\n\ndef savePageImages(url, imagespath=\'images\'):\n    def soupfindnSave(pagefolder, tag2find=\'img\', inner=\'src\'):\n        if not os.path.exists(pagefolder): # create only once\n            os.mkdir(pagefolder)\n        for res in soup.findAll(tag2find):  \n            if res.has_attr(inner): # check inner tag (file object) MUST exists\n                try:\n                    filename, ext = os.path.splitext(os.path.basename(res[inner])) # get name and extension\n                    filename = re.sub(\'\\W+\', \'\', filename) + ext # clean special chars from name\n                    fileurl = parse.urljoin(url, res.get(inner))\n                    filepath = os.path.join(pagefolder, filename)\n                    if not os.path.isfile(filepath): # was not downloaded\n                        with open(filepath, \'wb\') as file:\n                            filebin = session.get(fileurl)\n                            file.write(filebin.content)\n                except Exception as exc:\n                    print(exc, file=sys.stderr)   \n    session = requests.Session()\n    #... whatever other requests config you need here\n    response = session.get(url)\n    soup = BeautifulSoup(response.text, ""html.parser"")\n    soupfindnSave(imagespath, \'img\', \'src\')\n\nUse like this bellow to save the google.com page images in a folder google_images:\nsavePageImages(\'https://www.google.com\', \'google_images\')\n\n', '\nimport urllib.request as req\n\nwith req.urlopen(image_link) as d, open(image_location, ""wb"") as image_object:\n    data = d.read()\n    image_object.write(data)\n\n']"
How to scroll down with Phantomjs to load dynamic content,"
I am trying to scrape links from a page that generates content dynamically as the user scroll down to the bottom (infinite scrolling). I have tried doing different things with Phantomjs but not able to gather links beyond first page. Let say the element at the bottom which loads content has class .has-more-items. It is available until final content is loaded while scrolling and then becomes unavailable in DOM (display:none). Here are the things I have tried-

Setting viewportSize to a large height right after var page = require('webpage').create();


page.viewportSize = {             width: 1600,            height: 10000,
          };


Using page.scrollPosition = { top: 10000, left: 0 } inside page.open but have no effect like-


page.open('http://example.com/?q=houston', function(status) {
   if (status == ""success"") {
      page.scrollPosition = { top: 10000, left: 0 };  
   }
});



Also tried putting it inside page.evaluate function but that gives 


Reference error: Can't find variable page


Tried using jQuery and JS code inside page.evaluate and page.open but to no avail-


$(""html, body"").animate({ scrollTop: $(document).height() }, 10,
  function() {
          //console.log('check for execution');
      });

as it is and also inside document.ready. Similarly for JS code-
window.scrollBy(0,10000)

as it is and also inside window.onload
I am really struck on it for 2 days now and not able to find a way. Any help or hint would be appreciated.
Update
I have found a helpful piece of code at https://groups.google.com/forum/?fromgroups=#!topic/phantomjs/8LrWRW8ZrA0
var hitRockBottom = false; while (!hitRockBottom) {
    // Scroll the page (not sure if this is the best way to do so...)
    page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };

    // Check if we've hit the bottom
    hitRockBottom = page.evaluate(function() {
        return document.querySelector("".has-more-items"") === null;
    }); }

Where .has-more-items is the element class I want to access which is available at the bottom of the page initially and as we scroll down, it moves further down until all data is loaded and then becomes unavailable.
However, when I tested it is clear that it is running into infinite loops without scrolling down (I render pictures to check). I have tried to replace page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 }; with codes from below as well (one at a time)
window.document.body.scrollTop = '1000';
location.href = "".has-more-items"";
page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };
document.location.href="".has-more-items"";

But nothing seems to work.
",36k,"
            47
        ","['\nFound a way to do it and tried to adapt to your situation. I didn\'t test the best way of finding the bottom of the page because I had a different context, but check the solution below. The thing here is that you have to wait a little for the page to load and javascript works asynchronously so you have to use setInterval or setTimeout (see) to achieve this.\npage.open(\'http://example.com/?q=houston\', function () {\n\n  // Check for the bottom div and scroll down from time to time\n  window.setInterval(function() {\n      // Check if there is a div with class="".has-more-items"" \n      // (not sure if there\'s a better way of doing this)\n      var count = page.content.match(/class="".has-more-items""/g);\n\n      if(count === null) { // Didn\'t find\n        page.evaluate(function() {\n          // Scroll to the bottom of page\n          window.document.body.scrollTop = document.body.scrollHeight;\n        });\n      }\n      else { // Found\n        // Do what you want\n        ...\n        phantom.exit();\n      }\n  }, 500); // Number of milliseconds to wait between scrolls\n\n});\n\n', '\nI know that it has been answered a long time ago, but I also found a solution to my specific scenario. The result is a piece of javascript that scrolls to the bottom of the page. It is optimized to reduce waiting time.\nIt is not written for PhantomJS by default, so that will have to be modified. However, for a beginner or someone who doesn\'t have root access, an Iframe with injected javascript (run Google Chrome with --disable-javascript parameter) is a good alternative method for scraping a smaller set of ajax pages. The main benefit is that it\'s easily debuggable, because you have a visual overview of what\'s going on with your scraper.\nfunction ScrollForAjax () {\n\n    scrollintervals = 50;\n    scrollmaxtime = 1000;\n\n    if(typeof(scrolltime)==""undefined""){\n        scrolltime = 0;\n    }\n\n    scrolldocheight1 = $(iframeselector).contents().find(""body"").height();\n\n    $(""body"").scrollTop(scrolldocheight1);\n    setTimeout(function(){\n\n        scrolldocheight2 = $(""body"").height();\n\n        if(scrolltime===scrollmaxtime || scrolltime>scrollmaxtime){\n            scrolltime = 0;\n            $(""body"").scrollTop(0);\n            ScrapeCurrentPage(iframeselector);\n        }\n\n        else if(scrolldocheight2>scrolldocheight1){\n            scrolltime = 0;\n            ScrollForAjax (iframeselector);\n        }\n\n        else if(scrolldocheight1>=scrolldocheight2){\n            ScrollForAjax (iframeselector);\n        }\n\n    },scrollintervals);\n\n    scrolltime += scrollintervals;\n}\n\nscrollmaxtime is a timeout variable. Hope this is useful to someone :)\n', '\nThe ""correct"" solution didn\'t work for me. And, from what I\'ve read CasperJS doesn\'t use window (but I may be wrong on that), which makes me doubt that window works.\nThe following works for me in the Firefox/Chrome console; but, doesn\'t work in CasperJS (within casper.evaluate function).\n$(document).scrollTop($(document).height());\n\nWhat did work for me in CasperJS was:\ncasper.scrollToBottom();\ncasper.wait(1000, function waitCb() {\n  casper.capture(""loadedContent.png"");\n});\n\nWhich, also worked when moving casper.capture into Casper\'s then function.\nHowever, the above solution won\'t work on some sites like Twitter; jQuery seems to break the casper.scrollToBottom() function, and I had to remove the clientScripts reference to jQuery when working within Twitter.\nvar casper = require(\'casper\').create({\n    clientScripts: [\n       // \'jquery.js\'\n    ]\n});\n\nSome websites (e.g. BoingBoing.net) seem to work fine with jQuery and CasperJS scrollToBottom(). Not sure why some sites work and others don\'t.\n', ""\nThe code snippet below work just fine for pinterest. I researched a lot to scrape pinterest without phantomjs but it is impossible to find the infinite scroll trigger link. I think the code below will help other infinite scroll web page to scrape.\npage.open(pageUrl).then(function (status) {\n    var count = 0;\n    // Scrolls to the bottom of page\n    function scroll2btm() {\n        if (count < 500) {\n            page.evaluate(function(limit) {\n                window.scrollTo(0, document.body.scrollHeight || document.documentElement.scrollHeight);\n                return document.getElementsByClassName('pinWrapper').length; // use desired contents (eg. pin) selector for count presence number\n            }).then(function(c) {\n                count = c;\n                console.log(count); // print no of content found to check\n            });\n            setTimeout(scroll2btm,3000);\n        } else {\n            // required number of item found\n        }\n    }\n    scroll2btm();\n});\n\n""]"
Simple Screen Scraping using jQuery,"
I have been playing with the idea of using a simple screen-scraper using jQuery and I am wondering if the following is possible.
I have simple HTML page and am making an attempt (if this is possible) to grab the contents of all of the list items from another page, like so:
Main Page:
<!-- jQuery -->
<script type='text/javascript'>
$(document).ready(function(){
$.getJSON(""[URL to other page]"",
  function(data){

    //Iterate through the <li> inside of the URL's data
    $.each(data.items, function(item){
      $(""<li/>"").value().appendTo(""#data"");
    });

  });
});
</script>

<!-- HTML -->
<html>
    <body>
       <div id='data'></div>
    </body>
</html>

Other Page:
//Html
<body>
    <p><b>Items to Scrape</b></p>   
    <ul>
        <li>I want to scrape what is here</li>
        <li>and what is here</li>
        <li>and here as well</li>
        <li>and append it in the main page</li>
    </ul>
</body>

So, is it possible using jQuery to pull all of the list item contents from an external page and append them inside of a div?
",101k,"
            46
        ","['\nUse $.ajax to load the other page into a variable, then create a temporary element and use .html() to set the contents to the value returned. Loop through the  element\'s children of nodeType 1 and keep their first children\'s nodeValues. If the external page is not on your web server you will need to proxy the file with your own web server.\nSomething like this:\n$.ajax({\n     url: ""/thePageToScrape.html"",\n     dataType: \'text\',\n     success: function(data) {\n          var elements = $(""<div>"").html(data)[0].getElementsByTagName(""ul"")[0].getElementsByTagName(""li"");\n          for(var i = 0; i < elements.length; i++) {\n               var theText = elements[i].firstChild.nodeValue;\n               // Do something here\n          }\n     }\n});\n\n', '\nSimple scraping with jQuery...\n// Get HTML from page\n$.get( \'http://example.com/\', function( html ) {\n\n    // Loop through elements you want to scrape content from\n    $(html).find(""ul"").find(""li"").each( function(){\n\n        var text = $(this).text();\n        // Do something with content\n\n    } )\n\n} );\n\n', '\n$.get(""/path/to/other/page"",function(data){\n  $(\'#data\').append($(\'li\',data));\n}\n\n', ""\nIf this is for the same domain then no problem - the jQuery solution is good.\nBut otherwise you can't access content from an arbitrary website because this is considered a security risk. See same origin policy. \nThere are of course server side workarounds such as a web proxy or CORS headers.\nOf if you're lucky they will support jsonp.\nBut if you want a client side solution to work with an arbitrary website and web browser then you are out of luck. There is a proposal to relax this policy, but this won't effect current web browsers.\n"", ""\nYou may want to consider pjscrape:\nhttp://nrabinowitz.github.io/pjscrape/\nIt allows you to do this from the command-line, using javascript and jQuery. It does this by using PhantomJS, which is a headless webkit browser (it has no window, and it exists only for your script's usage, so you can load complex websites that use AJAX and it will work just as if it were a real browser).\nThe examples are self-explanatory and I believe this works on all platforms (including Windows).\n"", '\nUse YQL or Yahoo pipes to make the cross domain request for the raw page html content. The yahoo pipe or YQL query will spit this back as a JSON that can be processed by jquery to extract and display the required data.\nOn the downside: YQL and Yahoo pipes OBEY the robots.txt file for the target domain\nand if the page is to long the Yahoo Pipes regex commands will not run.\n', '\nI am sure you will hit the CORS issue with requests in many cases.\nFrom here try to resolve CORS issue.\nvar name = ""kk"";\nvar url = ""http://anyorigin.com/go?url="" + encodeURIComponent(""https://www.yoursite.xyz/"") + name + ""&callback=?"";\n$.get(url, function(response) {\n  console.log(response);\n});\n\n']"
web scraping dynamic content with python,"
I'd like to use Python to scrape the contents of the ""Were you looking for these authors:"" box on web pages like this one: http://academic.research.microsoft.com/Search?query=lander
Unfortunately the contents of the box get loaded dynamically by JavaScript. Usually in this situation I can read the Javascript to figure out what's going on, or I can use an browser extension like Firebug to figure out where the dynamic content is coming from. No such luck this time...the Javascript is pretty convoluted and Firebug doesn't give many clues about how to get at the content.
Are there any tricks that will make this task easy? 
",21k,"
            6
        ","['\nInstead of trying to reverse engineer it, you can use ghost.py to directly interact with JavaScript on the page.\nIf you run the following query in a chrome console, you\'ll see it returns everything you want.\ndocument.getElementsByClassName(\'inline-text-org\');\n\nReturns\n[<div class=\u200b""inline-text-org"" title=\u200b""University of Manchester"">\u200bUniversity of Manchester\u200b</div>, \n <div class=\u200b""inline-text-org"" title=\u200b""University of California Irvine"">\u200bUniversity of California ...\u200b</div>\u200b\n  etc...\n\nYou can run JavaScript through python in a real life DOM using ghost.py.\nThis is really cool:\nfrom ghost import Ghost\nghost = Ghost()\npage, resources = ghost.open(\'http://academic.research.microsoft.com/Search?query=lander\')\nresult, resources = ghost.evaluate(\n    ""document.getElementsByClassName(\'inline-text-org\');"")\n\n', ""\nA very similar question was asked earlier here.\nQuoted is selenium, originally a testing environment for web-apps.\nI usually use Chrome's Developer Mode, which IMHO already gives even more details than Firefox.\n"", ""\nFor scraping dynamic content, you need not a simple scraper but a full-fledged headless browser.\ndhamaniasad/HeadlessBrowsers: A list of (almost) all headless web browsers in existence is the fullest list of these that I've seen; it lists which languages each has bindings for.\n(Note that more than a few of the listed projects are abandoned!)\n""]"
How I can get web page's content and save it into the string variable,"
How I can get the content of the web page using ASP.NET?  I need to write a program to get the HTML of a webpage and store it into a string variable.
",182k,"
            78
        ","['\nYou can use the WebClient\nUsing System.Net;\n\nusing(WebClient client = new WebClient()) {\n    string downloadString = client.DownloadString(""http://www.gooogle.com"");\n}\n\n', '\nI\'ve run into issues with Webclient.Downloadstring before. If you do, you can try this:\nWebRequest request = WebRequest.Create(""http://www.google.com"");\nWebResponse response = request.GetResponse();\nStream data = response.GetResponseStream();\nstring html = String.Empty;\nusing (StreamReader sr = new StreamReader(data))\n{\n    html = sr.ReadToEnd();\n}\n\n', '\nI recommend not using WebClient.DownloadString. This is because (at least in .NET 3.5) DownloadString is not smart enough to use/remove the BOM, should it be present. This can result in the BOM (Ã¯Â»Â¿) incorrectly appearing as part of the string when UTF-8 data is returned (at least without a charset) - ick!\nInstead, this slight variation will work correctly with BOMs:\nstring ReadTextFromUrl(string url) {\n    // WebClient is still convenient\n    // Assume UTF8, but detect BOM - could also honor response charset I suppose\n    using (var client = new WebClient())\n    using (var stream = client.OpenRead(url))\n    using (var textReader = new StreamReader(stream, Encoding.UTF8, true)) {\n        return textReader.ReadToEnd();\n    }\n}\n\n', '\nWebclient client = new Webclient();\nstring content = client.DownloadString(url);\n\nPass the URL of page who you want to get. You can parse the result using htmlagilitypack.\n']"
"Beautiful Soup cannot find a CSS class if the object has other classes, too","
if a page has <div class=""class1""> and <p class=""class1"">, then soup.findAll(True, 'class1') will find them both.
If it has <p class=""class1 class2"">, though, it will not be found.  How do I find all objects with a certain class, regardless of whether they have other classes, too?
",16k,"
            40
        ","[""\nUnfortunately, BeautifulSoup treats this as a class with a space in it 'class1 class2' rather than two classes ['class1','class2'].  A workaround is to use a regular expression to search for the class instead of a string.\nThis works: \nsoup.findAll(True, {'class': re.compile(r'\\bclass1\\b')})\n\n"", '\nJust in case anybody comes across this question. BeautifulSoup now supports this:\nPython 2.7.5 (default, May 15 2013, 22:43:36) [MSC v.1500 32 bit (Intel)]\nType ""copyright"", ""credits"" or ""license"" for more information.\n\nIn [1]: import bs4\n\nIn [2]: soup = bs4.BeautifulSoup(\'<div class=""foo bar""></div>\')\n\nIn [3]: soup(attrs={\'class\': \'bar\'})\nOut[3]: [<div class=""foo bar""></div>]\n\nAlso, you don\'t have to type findAll anymore.\n', '\nYou should use lxml. It works with multiple class values separated by spaces (\'class1 class2\').\nDespite its name, lxml is also for parsing and scraping HTML. It\'s much, much faster than BeautifulSoup, and it even handles ""broken"" HTML better than BeautifulSoup (their claim to fame). It has a compatibility API for BeautifulSoup too if you don\'t want to learn the lxml API.\nIan Bicking agrees and prefers lxml over BeautifulSoup.\nThere\'s no reason to use BeautifulSoup anymore, unless you\'re on Google App Engine or something where anything not purely Python isn\'t allowed.\nYou can even use CSS selectors with lxml, so it\'s far easier to use than BeautifulSoup. Try playing around with it in an interactive Python console.\n', '\nIt’s very useful to search for a tag that has a certain CSS class, but the name of the CSS attribute, “class”, is a reserved word in Python. Using class as a keyword argument will give you a syntax error. As of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument class_:\nLike:\nsoup.find_all(""a"", class_=""class1"")\n\n']"
file_get_contents() give me 403 Forbidden,"
I have a partner that has created some content for me to scrape.
I can access the page with my browser, but when trying to user file_get_contents, I get a 403 forbidden.
I've tried using stream_context_create, but that's not helping - it might be because I don't know what should go in there.
1) Is there any way for me to scrape the data?
2) If no, and if partner is not allowed to configure server to allow me access, what can I do then?
The code I've tried using:
$opts = array(
  'http'=>array(
    'user_agent' => 'My company name',
    'method'=>""GET"",
    'header'=> implode(""\r\n"", array(
      'Content-type: text/plain;'
    ))
  )
);

$context = stream_context_create($opts);

//Get header content
$_header = file_get_contents($partner_url,false, $context);

",35k,"
            26
        ","['\nThis is not a problem in your script, its a feature in you partners web server security.\nIt\'s hard to say exactly whats blocking you, most likely its some sort of block against scraping. If your partner has access to his web servers setup it might help pinpoint.\nWhat you could do is to ""fake a web browser"" by setting the user-agent headers so that it imitates a standard web browser.\nI would recommend cURL to do this, and it will be easy to find good documentation for doing this.\n    // create curl resource\n    $ch = curl_init();\n\n    // set url\n    curl_setopt($ch, CURLOPT_URL, ""example.com"");\n\n    //return the transfer as a string\n    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\n    curl_setopt($ch,CURLOPT_USERAGENT,\'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.13) Gecko/20080311 Firefox/2.0.0.13\');\n\n    // $output contains the output string\n    $output = curl_exec($ch);\n\n    // close curl resource to free up system resources\n    curl_close($ch); \n\n', ""\n//set User Agent first\nini_set('user_agent','Mozilla/4.0 (compatible; MSIE 6.0)'); \n\n"", ""\nAlso if for some reason you're requesting a http resource but that resource lives on your server you can save yourself some trouble if you just include the file as an absolute path.\nLike: /home/sally/statusReport/myhtmlfile.html \ninstead of \nhttps://example.org/myhtmlfile.html\n"", ""\nI have two things in my mind, If you're opening a URI with special characters, such as spaces, you need to encode the URI with urlencode() and A URL can be used as a filename with this function if the fopen wrappers have been enabled.\n""]"
Scrape a dynamic website,"
What is the best method to scrape a dynamic website where most of the content is generated by what appears to be ajax requests?  I have previous experience with a Mechanize, BeautifulSoup, and python combo, but I am up for something new.
--Edit--
For more detail: I'm trying to scrape the CNN primary database.  There is a wealth of information there, but there doesn't appear to be an api.
",8k,"
            12
        ","['\nThe best solution that I found was to use Firebug to monitor XmlHttpRequests, and then to use a script to resend them.\n', ""\nThis is a difficult problem because you either have to reverse engineer the JavaScript on a per-site basis, or implement a JavaScript engine and run the scripts (which has its own difficulties and pitfalls).\nIt's a heavy weight solution, but I've seen people doing this with GreaseMonkey scripts - allow Firefox to render everything and run the JavaScript, and then scrape the elements. You can even initiate user actions on the page if needed.\n"", '\nSelenium IDE, a tool for testing, is something I\'ve used for a lot of screen-scraping. There are a few things it doesn\'t handle well (Javascript window.alert() and popup windows in general), but it does its work on a page by actually triggering the click events and typing into the text boxes. Because the IDE portion runs in Firefox, you don\'t have to do all of the management of sessions, etc. as Firefox takes care of it. The IDE records and plays tests back.\nIt also exports C#, PHP, Java, etc. code to build compiled tests/scrapers that are executed on the Selenium server. I\'ve done that for more than a few of my Selenium scripts, which makes things like storing the scraped data in a database much easier.\nScripts are fairly simple to write and alter, being made up of things like (""clickAndWait"",""submitButton""). Worth a look given what you\'re describing.\n', '\nAdam Davis\'s advice is solid.\nI would additionally suggest that you try to ""reverse-engineer"" what the JavaScript is doing, and instead of trying to scrape the page, you issue the HTTP requests that the JavaScript is issuing and interpret the results yourself (most likely in JSON format, nice and easy to parse).  This strategy could be anything from trivial to a total nightmare, depending on the complexity of the JavaScript.\nThe best possibility, of course, would be to convince the website\'s maintainers to implement a developer-friendly API.  All the cool kids are doing it these days 8-)  Of course, they might not  want their data scraped in an automated fashion... in which case you can expect a cat-and-mouse game of making their page increasingly difficult to scrape :-(\n', ""\nThere is a bit of a learning curve, but tools like Pamie (Python) or Watir (Ruby) will let you latch into the IE web browser and get at the elements. This turns out to be easier than Mechanize and other HTTP level tools since you don't have to emulate the browser, you just ask the browser for the html elements. And it's going to be way easier than reverse engineering the Javascript/Ajax calls. If needed you can also use tools like beatiful soup in conjunction with Pamie.\n"", '\nProbably the easiest way is to use IE webbrowser control in C# (or any other language). You have access to all the stuff inside browser out of the box + you dont need to care about cookies, SSL and so on.\n', '\ni found the IE Webbrowser control have all kinds of quirks and workarounds that would justify some high quality software to take care of all those inconsistencies, layered around the shvwdoc.dll api and mshtml and provide a framework. \n', ""\nThis seems like it's a pretty common problem.  I wonder why someone hasn't anyone developed a programmatic browser?  I'm envisioning a Firefox you can call from the command line with a URL as an argument and it will load the page, run all of the initial page load JS events and save the resulting file.\nI mean Firefox, and other browsers already do this, why can't we simply strip off the UI stuff?  \n""]"
Scraping javascript website in R,"
I want to scrape the match time and date from this url:
http://www.scoreboard.com/game/rosol-l-goffin-d-2014/8drhX07d/#game-summary
By using the chrome dev tools, I can see this appears to be generated using the following code:
<td colspan=""3"" id=""utime"" class=""mstat-date"">01:20 AM, October 29, 2014</td>

But this is not in the source html.
I think this is because its java (correct me if Im wrong). How can I scrape this information using R?
",8k,"
            9
        ","['\nSo, RSelenium is not the only answer (anymore). If you can install the PhantomJS binary (grab phantomjs binaries from here: http://phantomjs.org/) then you can use it to render the HTML and scrape it with rvest (similar to the RSelenium approach but doesn\'t require java):\nlibrary(rvest)\n\n# render HTML from the site with phantomjs\n\nurl <- ""http://www.scoreboard.com/game/rosol-l-goffin-d-2014/8drhX07d/#game-summary""\n\nwriteLines(sprintf(""var page = require(\'webpage\').create();\npage.open(\'%s\', function () {\n    console.log(page.content); //page source\n    phantom.exit();\n});"", url), con=""scrape.js"")\n\nsystem(""phantomjs scrape.js > scrape.html"", intern = T)\n\n# extract the content you need\npg <- html(""scrape.html"")\npg %>% html_nodes(""#utime"") %>% html_text()\n\n## [1] ""10:20 AM, October 28, 2014""\n\n', '\nYou could also use docker as the web driver (in place of selenium)\nYou will still need to install phantomjs, and docker too. Then run:\nlibrary(RSelenium)\n\nurl <- ""http://www.scoreboard.com/game/rosol-l-goffin-d-2014/8drhX07d/#game-summary""\n\nsystem(\'docker run -d -p 4445:4444 selenium/standalone-chrome\') \nremDr <- remoteDriver(remoteServerAddr = ""localhost"", port = 4445L, browserName = ""chrome"")\nremDr$open()\nremDr$navigate(url)\n\nwriteLines(sprintf(""var page = require(\'webpage\').create();\npage.open(\'%s\', function () {\n    console.log(page.content); //page source\n    phantom.exit();\n});"", url), con=""scrape.js"")\n\nsystem(""phantomjs scrape.js > scrape.html"", intern = T)\n\n# extract the content you need\npg <- read_html(""scrape.html"")\npg %>% html_nodes(""#utime"") %>% html_text()\n\n# [1] ""10:20 AM, October 28, 2014""\n\n']"
Protection from screen scraping [closed],"






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.


Closed 10 years ago.







                        Improve this question
                    



Following on from my question on the Legalities of screen scraping, even if it's illegal people will still try, so:
What technical mechanisms can be employed to prevent or at least disincentivise screen scraping?
Oh and just for grins and to make life difficult, it may well be nice to retain access for search engines. I may well be playing devil's advocate here but there is a serious underlying point.
",24k,"
            31
        ","['\nYou can’t prevent it.\n', ""\nI've written a blog post about this here:  http://blog.screen-scraper.com/2009/08/17/further-thoughts-on-hindering-screen-scraping/\nTo paraphrase:\nIf you post information on the internet someone can get it, it's just a matter of how many resources they want to invest.  Some means to make the required resources higher are:\nTuring tests\nThe most common implementation of the Turning Test is the old CAPTCHA that tries to ensure a human reads the text in an image, and feeds it into a form.\nWe have found a large number of sites that implement a very weak CAPTCHA that takes only a few minutes to get around. On the other hand, there are some very good implementations of Turing Tests that we would opt not to deal with given the choice, but a sophisticated OCR can sometimes overcome those, or many bulletin board spammers have some clever tricks to get past these.\nData as images\nSometimes you know which parts of your data are valuable. In that case it becomes reasonable to replace such text with an image. As with the Turing Test, there is OCR software that can read it, and there’s no reason we can’t save the image and have someone read it later.\nOften times, however, listing data as an image without a text alternate is in violation of the Americans with Disabilities Act (ADA), and can be overcome with a couple of phone calls to a company’s legal department.\nCode obfuscation\nUsing something like a JavaScript function to show data on the page though it’s not anywhere in the HTML source is a good trick. Other examples include putting prolific, extraneous comments through the page or having an interactive page that orders things in an unpredictable way (and the example I think of used CSS to make the display the same no matter the arrangement of the code.)\nCSS Sprites\nRecently we’ve encountered some instances where a page has one images containing numbers and letters, and used CSS to display only the characters they desired.  This is in effect a combination of the previous 2 methods.  First we have to get that master-image and read what characters are there, then we’d need to read the CSS in the site and determine to what character each tag was pointing.\nWhile this is very clever, I suspect this too would run afoul the ADA, though I’ve not tested that yet.\nLimit search results\nMost of the data we want to get at is behind some sort of form. Some are easy, and submitting a blank form will yield all of the results. Some need an asterisk or percent put in the form. The hardest ones are those that will give you only so many results per query. Sometimes we just make a loop that will submit the letters of the alphabet to the form, but if that’s too general, we must make a loop to submit all combination of 2 or 3 letters–that’s 17,576 page requests.\nIP Filtering\nOn occasion, a diligent webmaster will notice a large number of page requests coming from a particular IP address, and block requests from that domain.  There are a number of methods to pass requests through alternate domains, however, so this method isn’t generally very effective.\nSite Tinkering\nScraping always keys off of certain things in the HTML.  Some sites have the resources to constantly tweak their HTML so that any scrapes are constantly out of date.  Therefore it becomes cost ineffective to continually update the scrape for the constantly changing conditions.\n"", '\nSo, one approach would be to obfuscate the code (rot13, or something), and then have some javascript in the page that do something like document.write(unobfuscate(obfuscated_page)). But this totally blows away search engines (probably!).\nOf course this doesn’t actually stop someone who wants to steal your data either, but it does make it harder.\nOnce the client has the data it is pretty much game over, so you need to look at something on the server side.\nGiven that search engines are basically screen scrapers things are difficult. You need to look at what the difference between the good screen scrapers and the bad screen scrapers are. And of course, you have just the normal human users as well. So this comes down to a problem of how can you on the server effectively classify as request as coming from a human, a good screen scraper, or a bad screen scraper.\nSo, the place to start would be looking at your log-files and seeing if there is some pattern that allows you to effectively classify requests, and then on determining the pattern see if there is some way that a bad screen scraper, upon knowing this classification, could cloak itself to appear like a human or good screen scraper.\nSome ideas:\n\nYou may be able to determine the good screen scrapers by IP address(es)..\nYou could potentially determine scraper vs. human by number of concurrent connections, total number of connections per time-period, access pattern, etc.\n\nObviously these aren’t ideal or fool-proof. Another tactic is to determine what measures can you take that are unobtrusive to humans, but (may be) annoying for scrapers. An example might be slowing down the number of requests. (Depends on the time criticality of the request. If they are scraping in real-time, this would effect their end users).\nThe other aspect is to look at serving these users better. Clearly they are scraping because they want the data. If you provide them an easy way in which to directly obtain the data in a useful format then that will be easier for them to do instead of screen scraping. If there is an easy way then access to the data can be regulated. E.g: give requesters a unique key, and then limit the number of requests per key to avoid overload on the server, or charge per 1000 requests, etc.\nOf course there are still people who will want to rip you off, and then there are probably other ways to disincentivise, bu they probably start being non-technical, and require legal avenues to be persued.\n', ""\nIt's pretty hard to prevent screen scraping but if you really, really wanted to you could\nchange your HTML frequently or change the HTML tag names frequently. Most screen scrapers work by using string comparisons with tag names, or regular expressions searching for particular strings etc. If you are changing the underlying HTML it will make them need to change their software.\n"", '\nIt would be very difficult to prevent.  The problem is that Web pages are meant to be parsed by a program (your browser), so they are exceptionally easy to scrape.  The best you can do is be vigilant, and if you find that your site is being scraped, block the IP of the offending program.\n', ""\nDon't prevent it, detect it and retaliate those who try.\nFor example, leave your site open to download but disseminate some links that no sane user would follow. If someone follows that link, is clicking too fast for a human or other suspicious behaviour, react promptly to stop the user from trying. If there is a login system, block the user and contact him regarding unacceptable behaviour. That should make sure they don't try again. If there is no login system, instead of actual pages, return a big warning with fake links to the same warning.\nThis really applies for things like Safari Bookshelf where a user copy-pasting a piece of code or a chapter to mail a colleague is fine while a full download of book is not acceptable. I'm quite sure that they detect when some tries to download their books, block the account and show the culprit that he might get in REAL trouble should he try that again.\nTo make a non-IT analogy, if airport security only made it hard to bring weapons on board of planes, terrorists would try many ways to sneak one past security. But the fact that just trying will get you in deep trouble make it so that nobody is going to try and find the ways to sneak one. The risk of getting caught and punished is too high. Just do the same. If possible.\n"", ""\nSearch engines ARE screen scrapers by definition.  So most things you do to make it harder to screen scrape will also make it harder to index your content.\nWell behaved robots will honour your robots.txt file.\nYou could also block the IP of known offenders or add obfuscating HTML tags into your content when it's not sent to a known good robot.  It's a losing battle though.  I recommend the litigation route for known offenders.\nYou could also hide identifying data in the content to make it easier to track down offenders. Encyclopaedias have been known to to add Fictitious entries to help detect and prosecute copyright infringers.\n"", ""\nPrevent? -- impossible, but you can make it harder.\nDisincentivise? -- possible, but you won't like the answer: provide bulk data exports for interested parties. \nOn the long run, all your competitors will have the same data if you publish it, so you need other means of diversifying your website (e.g. update it more frequently, make it faster or easier to use). Nowdays even Google is using scraped information like user reviews, what do you think you can do about it? Sue them and get booted from their index?\n"", ""\nThe best return on investment is probably to add random newlines and multiple spaces, since most screen scrapers work from the HTML as text rather than as a XML (since most pages don't parse as valid XML).\nThe browser ignores whitespace, so your user's don't notice that \n  Price : 1\n  Price :    2\n  Price\\n:\\n3\n\nare different.  (this comes from my experience scraping government sites with AWK).\nNext step is adding  tags around random elements to mess up the DOM.\n"", '\nOne way is to create an function that takes text and position and then Serverside generate x, y pos for every character in the text, generate divs in random order containing the characters. Generate a javascript that then posision every div on right place on screen. Looks good on screen but in code behind there is no real order to fetch the text if you dont go throuh the trouble to scrape via your javascript (that can be changed dynamically every request)\nToo much work and have possibly many quirks, it depends on how much text and how complicate UI you have on the site and other things.\n', '\nVery few I think given the intention of any site is to publish (i.e. to make public) information. \n\nYou can hide your data behind logins of course, but that\'s a very situational solution. \nI\'ve seen apps which would only serve up content where the request headers indicated a web browser (rather than say anonymous or ""jakarta"") but that\'s easy to spoof and you\'ll lose some genuine humans.\nThen there\'s the possibility that you accept some scrapage but make life insurmountably hard for them by not serving content if requests are coming from the same IP at too high a rate. This suffers from not being full coverage but more importantly there is the ""AOL problem"" that an IP can cover many many unique human users.\n\nBoth of the last two techniques also depend heavily on having traffic intercepting technology which is an inevitable performance and/or financial outlay.\n', ""\nGiven that most sites want a good search engine ranking, and search engines are scraper bots, there's not much you can do that won't harm your SEO. \nYou could make an entirely ajax loaded site or flash based site, which would make it harder for bots, or hide everything behind a login, which would make it harder still, but either of these approaches is going to hurt your search rankings and possibly annoy your users, and if someone really wants it, they'll find a way.  \nThe only guaranteed way of having content that can't be scraped is to not publish it on the web. The nature of the web is such that when you put it out there, it's out there.\n"", '\nIf its not much information you want to protect you can convert it to a picture on the fly. Then they must use OCR wich makes it easier to scrape another site instead of yours..\n', ""\nYou could check the user agent of clients coming to your site. Some third party screen scraping programs have their own user agent so you could block that. Good screen scrapers however spoof their user agent so you won't be able to detect it. Be careful if you do try to block anyone because you don't want to block a legitimate user :)\nThe best you can hope for is to block people using screen scrapers that aren't smart enough to change their user agent.\n"", '\nI tried to ""screen scrape"" some PDF files once, only to find that they\'d actually put the characters in the PDF in semi-random order.  I guess the PDF format allows you to specify a location for each block of text, and they\'d used very small blocks (smaller than a word).  I suspect that the PDFs in question weren\'t trying to prevent screen scraping so much as they were doing something weird with their render engine.\nI wonder if you could do something like that.\n', '\nYou could put everything in flash, but in most cases that would annoy many legitimate users, myself included. It can work for some information such as stock prices or graphs.\n', '\nI suspect there is no good way to do this.\nI suppose you could run all your content through a mechanism to convert text to images rendered using a CAPTCHA-style font and layout, but that would break SEO and annoy your users.\n', '\nWell, before you push the content from the server to the client, remove all the \\r\\n, \\n, \\t and replace everything with nothing but a single space. Now you have 1 long line in your html page. Google does this. This will make it hard for others to read your html or JavaScript.\nThen you can create empty tags and randomly insert them here and there. The will have no effect.\nThen you can log all the IPs and how often they hit your site. If you see one that comes in on time everytime, you mark it as robot and block it.\nMake sure you leave the search engines alone if you want them to come in.\n\nHope this helps\n', ""\nWhat about using the iText library to create PDFs out of your database information? As with Flash, it won't make scraping impossible, but might make it a little more difficult.\nNels\n"", ""\nOld question, but- adding interactivity makes screen scraping much more difficult. If the data isn't in the original response- say, you made an AJAX request to populate a div after page load- most scrapers won't see it.\nFor example- I use the mechanize library to do my scraping. Mechanize doesn't execute Javascript- it isn't a modern browser- it just parses HTML, let's me follow links and extract text, etc. Whenever I run into a page that makes heavy use of Javascript, I choke- without a fully scripted browser (that supports the full gamut of Javascript) I'm stuck.\nThis is the same issue that makes automated testing of highly interactive web applications so difficult.\n"", '\nI never thought that preventing print screen would be possible... well what do you know, checkout the new tech - sivizion.com. With their video buffer technology there is no way to do a print screen, cool, really cool, though hard to use ... I think they license the tech also, check it out. (If I am wrong please post here how it can be hacked.)\nFound it here: How do I prevent print screen\n']"
CasperJS passing data back to PHP,"
CasperJS is being called by PHP using an exec() command. After CasperJS does its work such as retrieving parts of a webpage, how can the retrieved data be returned back to PHP?
",11k,"
            9
        ","['\nI think the best way to transfer data from CasperJS to another language such as PHP is running CasperJS script as a service. Because CasperJS has been written over PhantomJS, CasperJS can use an embedded web server module of PhantomJS called Mongoose.\nFor information about how works the embedded web server see here\nHere an example about how a CasperJS script can start a web server.\n//define ip and port to web service\nvar ip_server = \'127.0.0.1:8585\';\n\n//includes web server modules\nvar server = require(\'webserver\').create();\n\n//start web server\nvar service = server.listen(ip_server, function(request, response) {\n\n    var links = [];\n    var casper = require(\'casper\').create();\n\n    function getLinks() {\n        var links = document.querySelectorAll(\'h3.r a\');\n        return Array.prototype.map.call(links, function(e) {\n            return e.getAttribute(\'href\')\n        });\n    }\n\n    casper.start(\'http://google.fr/\', function() {\n        // search for \'casperjs\' from google form\n        this.fill(\'form[action=""/search""]\', { q: \'casperjs\' }, true);\n    });\n\n    casper.then(function() {\n        // aggregate results for the \'casperjs\' search\n        links = this.evaluate(getLinks);\n        // now search for \'phantomjs\' by filling the form again\n        this.fill(\'form[action=""/search""]\', { q: \'phantomjs\' }, true);\n    });\n\n    casper.then(function() {\n        // aggregate results for the \'phantomjs\' search\n        links = links.concat(this.evaluate(getLinks));\n    });\n\n    //\n    casper.run(function() {\n            response.statusCode = 200;\n            //sends results as JSON object\n            response.write(JSON.stringify(links, null, null));\n            response.close();              \n    });\n\n});\nconsole.log(\'Server running at http://\' + ip_server+\'/\');\n\n', ""\nYou can redirect output from stdout to an array.\nOn this page it says you can do:  \nstring exec ( string $command [, array &$output [, int &$return_var ]] )\n\nIt goes on to say: \n\nIf the output argument is present, then the specified array will be filled with every line of output from the command. \n\nSo basically you can do exec('casperjs command here, $array_here);\n""]"
unable to call firefox from selenium in python on AWS machine,"
I am trying to use selenium from python to scrape some dynamics pages with javascript. However, I cannot call firefox after I followed the instruction of selenium on the pypi page(http://pypi.python.org/pypi/selenium). I installed firefox on AWS ubuntu 12.04. The error message I got is:
In [1]: from selenium import webdriver

In [2]: br = webdriver.Firefox()
---------------------------------------------------------------------------
WebDriverException                        Traceback (most recent call last)
/home/ubuntu/<ipython-input-2-d6a5d754ea44> in <module>()
----> 1 br = webdriver.Firefox()

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/webdriver.pyc in __init__(self, firefox_profile, firefox_binary, timeout)
     49         RemoteWebDriver.__init__(self,
     50             command_executor=ExtensionConnection(""127.0.0.1"", self.profile,
---> 51             self.binary, timeout),
     52             desired_capabilities=DesiredCapabilities.FIREFOX)
     53

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/extension_connection.pyc in __init__(self, host, firefox_profile, firefox_binary, timeout)
     45         self.profile.add_extension()
     46
---> 47         self.binary.launch_browser(self.profile)
     48         _URL = ""http://%s:%d/hub"" % (HOST, PORT)
     49         RemoteConnection.__init__(

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/firefox_binary.pyc in launch_browser(self, profile)
     42
     43         self._start_from_profile_path(self.profile.path)
---> 44         self._wait_until_connectable()
     45
     46     def kill(self):

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/firefox_binary.pyc in _wait_until_connectable(self)
     79                 raise WebDriverException(""The browser appears to have exited ""
     80                       ""before we could connect. The output was: %s"" %
---> 81                       self._get_firefox_output())
     82             if count == 30:
     83                 self.kill()

WebDriverException: Message: 'The browser appears to have exited before we could connect. The output was: Error: no display specified\n'

I did search on the web and found that this problem happened with other people (https://groups.google.com/forum/?fromgroups=#!topic/selenium-users/21sJrOJULZY). But I don't understand the solution, if it is. 
Can anyone help me please? Thanks!
",24k,"
            33
        ","['\n', '\n', '\n', '\n']"
scrape websites with infinite scrolling,"
I have written many scrapers but I am not really sure how to handle infinite scrollers. These days most website etc, Facebook, Pinterest has infinite scrollers.
",31k,"
            31
        ","['\nYou can use selenium to scrap the infinite scrolling website like twitter or facebook. \nStep 1 : Install Selenium using pip \npip install selenium \n\nStep 2 : use the code below to automate infinite scroll and extract the source code\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import NoAlertPresentException\nimport sys\n\nimport unittest, time, re\n\nclass Sel(unittest.TestCase):\n    def setUp(self):\n        self.driver = webdriver.Firefox()\n        self.driver.implicitly_wait(30)\n        self.base_url = ""https://twitter.com""\n        self.verificationErrors = []\n        self.accept_next_alert = True\n    def test_sel(self):\n        driver = self.driver\n        delay = 3\n        driver.get(self.base_url + ""/search?q=stckoverflow&src=typd"")\n        driver.find_element_by_link_text(""All"").click()\n        for i in range(1,100):\n            self.driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")\n            time.sleep(4)\n        html_source = driver.page_source\n        data = html_source.encode(\'utf-8\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n\nStep 3 : Print the data if required.\n', ""\nMost sites that have infinite scrolling do (as Lattyware notes) have a proper API as well, and you will likely be better served by using this rather than scraping.\nBut if you must scrape...\nSuch sites are using JavaScript to request additional content from the site when you reach the bottom of the page. All you need to do is figure out the URL of that additional content and you can retrieve it. Figuring out the required URL can be done by inspecting the script, by using the Firefox Web console, or by using a debug proxy.\nFor example, open the Firefox Web Console, turn off all the filter buttons except Net, and load the site you wish to scrape. You'll see all the files as they are loaded. Scroll the page while watching the Web Console and you'll see the URLs being used for the additional requests. Then you can request that URL yourself and see what format the data is in (probably JSON) and get it into your Python script.\n"", '\nFinding the url of the ajax source will be the best option but it can be cumbersome for certain sites. Alternatively you could use a headless browser like QWebKit from PyQt and send keyboard events while reading the data from the DOM tree. QWebKit has a nice and simple api.\n']"
Screen Scraping from a web page with a lot of Javascript [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



I have been asked to write an app which screen scrapes info from an intranet web page and presents the certain info from it in a nice easy to view format. The web page is a real mess and requires the user to click on half a dozen icons to discover if an ordered item has arrived or has been receipted. As you can imagine users find this irritating to say the least and it would be nice to have an app anyone can use that lists the state of their orders in a single screen.
Yes I know a better solution would be to re-write the web app but that would involve calling in the vendor and would cost us as small fortune.
Anyway while looking into this I discovered the web page I want to scrape is mostly Javascript (although it doesn't use any AJAX techniques). Does anyone know if a library or program exists which I could feed with the Javascript and which would then spit out the DOM for my app to parse ? 
I can pretty much write the app in any language but my preference would be JavaFX just so I could have a play with it.
Thanks for your time.
Ian
",18k,"
            17
        ","[""\nYou may consider using HTMLunit\nIt's a java class library made to automate browsing without having to control a browser, and it integrates the Mozilla Rhino Javascript engine to process javascript on the pages it loads. There's also a JRuby wrapper for that, named Celerity. Its javascript support is not really perfect right now, but if your pages don't use many hacks things should work fine the performance should be way better than controlling a browser. Furthermore, you don't have to worry about cookies being persisted after your scraping is over and all the other nasty things connected to controlling a browser (history, autocomplete, temp files etc).\n"", ""\nSince you say that no AJAX is used, then all the info is present at the HTML source. The javascript just renders it based on user clicks. So you need to reverse engineer the way the application works, parse the html and the javascript code and extract the useful information. It is strictly business of text parsing - you shouldn't deal with running javascript and producing a new DOM. This would be much more difficult to do.\nIf AJAX was used, your job would be easier. You could easily find out how the AJAX services work (probably by receiving JSON and XML) and extract the information.\n"", '\nYou could consider using a greasemonkey JS. greasemonkey is a very powerful Firefox add on that allows you to run your own script alongside that of specific web sites. This allows you to modify how the web site is displayed, add or remove content. You can even use it to  do AJAX style lookups and add dynamic content. \nIf your tool is for in house use, and users are all happy to use Firefox then this could be a winner.\nRegards\n', '\nI suggest IRobotSoft web scraper.  It is a dedicated free software for screen scraping with the best javascript support.  You can create and test a robot with its visual interface.  You can also embed it into your own application using its ActiveX control and hide the browser window. \n', ""\nI'd go with Perl's Win32::IE::Mechanize which lets you automate Internet Explorer. You should be able to click on icons and extract text while letting MSIE do the annoying tasks of processing all the JS.\n"", ""\nI agree with kgiannakakis' answer. I'd be suprised if you couldn't reverse engineer the javascript to identify where the information comes from and then write some simple Python scripts using Urllib2 and the Beautiful Soup library to scrape the same information.\nIf Python and scraping are a new idea, there's some excellent tutorials available on how to get going.\n[Edit] Looks like there's a Python version of mechanize too. Time to re-write some scrapers I developed a while back! :-)\n"", '\nI created a project site2archive that uses phantomJs to render including JS stuff and wget to scrape. phantomJs is based on Webkit, that delivers a similar browsing environment as Safari and Google Chrome.\n']"
Perform screen-scape of Webbrowser control in thread,"
I am using the technique shown in 

WebBrowser Control in a new thread

Trying to get a screen-scrape of a webpage I have been able to get the following code to successfully work when the WebBrowser control is placed on a WinForm. However it fails by providing an arbitrary image of the desktop when run inside a thread.
Thread browserThread = new Thread(() =>
{
    WebBrowser br = new WebBrowser();
    br.DocumentCompleted += webBrowser1_DocumentCompleted;
    br.ProgressChanged += webBrowser1_ProgressChanged;
    br.ScriptErrorsSuppressed = true;
    br.Navigate(url);
    Application.Run();
});
browserThread.SetApartmentState(ApartmentState.STA);
browserThread.Start();

private Image TakeSnapShot(WebBrowser browser)
{
    int width;
    int height;

    width = browser.ClientRectangle.Width;
    height = browser.ClientRectangle.Height;

    Bitmap image = new Bitmap(width, height);

    using (Graphics graphics = Graphics.FromImage(image))
    {
        Point p = new Point(0, 0);
        Point upperLeftSource = browser.PointToScreen(p);
        Point upperLeftDestination = new Point(0, 0);

        Size blockRegionSize = browser.ClientRectangle.Size;
        blockRegionSize.Width = blockRegionSize.Width - 15;
        blockRegionSize.Height = blockRegionSize.Height - 15;
        graphics.CopyFromScreen(upperLeftSource, upperLeftDestination, blockRegionSize);
    }

    return image;
}

This obviously happens because of the method Graphics.CopyFromScreen() but I am unaware of any other approach. Is there a way to resolve this issue that anyone could suggest? or is my only option to create a form, add the control, make it visible and then screen-scrape? For obvious reasons I'm hoping to avoid such an approach.
",4k,"
            3
        ","['\nYou can write \nprivate Image TakeSnapShot(WebBrowser browser)\n{\n     browser.Width = browser.Document.Body.ScrollRectangle.Width;\n     browser.Height= browser.Document.Body.ScrollRectangle.Height;\n\n     Bitmap bitmap = new Bitmap(browser.Width - System.Windows.Forms.SystemInformation.VerticalScrollBarWidth, browser.Height);\n\n     browser.DrawToBitmap(bitmap, new Rectangle(0, 0, bitmap.Width, bitmap.Height));\n\n     return bitmap;\n}\n\nA full working code\nvar image = await WebUtils.GetPageAsImageAsync(""http://www.stackoverflow.com"");\nimage.Save(fname , System.Drawing.Imaging.ImageFormat.Bmp);\n\n\npublic class WebUtils\n{\n    public static Task<Image> GetPageAsImageAsync(string url)\n    {\n        var tcs = new TaskCompletionSource<Image>();\n\n        var thread = new Thread(() =>\n        {\n            WebBrowser browser = new WebBrowser();\n            browser.Size = new Size(1280, 768);\n\n            WebBrowserDocumentCompletedEventHandler documentCompleted = null;\n            documentCompleted = async (o, s) =>\n            {\n                browser.DocumentCompleted -= documentCompleted;\n                await Task.Delay(2000); //Run JS a few seconds more\n\n                Bitmap bitmap = TakeSnapshot(browser);\n\n                tcs.TrySetResult(bitmap);\n                browser.Dispose();\n                Application.ExitThread();\n            };\n\n            browser.ScriptErrorsSuppressed = true;\n            browser.DocumentCompleted += documentCompleted;\n            browser.Navigate(url);\n            Application.Run();\n        });\n\n        thread.SetApartmentState(ApartmentState.STA);\n        thread.Start();\n\n        return tcs.Task;\n    }\n\n    private static Bitmap TakeSnapshot(WebBrowser browser)\n    {\n         browser.Width = browser.Document.Body.ScrollRectangle.Width;\n         browser.Height= browser.Document.Body.ScrollRectangle.Height;\n\n         Bitmap bitmap = new Bitmap(browser.Width - System.Windows.Forms.SystemInformation.VerticalScrollBarWidth, browser.Height);\n\n         browser.DrawToBitmap(bitmap, new Rectangle(0, 0, bitmap.Width, bitmap.Height));\n\n         return bitmap;\n    }\n}\n\n', '\nusing (Graphics graphics = Graphics.FromImage(image))\n    {\n        Point p = new Point(0, 0);\n        Point upperLeftSource = browser.PointToScreen(p);\n        Point upperLeftDestination = new Point(0, 0);\n\n        Size blockRegionSize = browser.ClientRectangle.Size;\n        blockRegionSize.Width = blockRegionSize.Width - 15;\n        blockRegionSize.Height = blockRegionSize.Height - 15;\n        graphics.CopyFromScreen(upperLeftSource, upperLeftDestination, blockRegionSize);\n    }\n\nDo you really need using statement ? \nYou also return image but how is copied image assigned to it? \n']"
Looping over urls to do the same thing,"
I am tring to scrape a few sites. Here is my code:
for (var i = 0; i < urls.length; i++) {
    url = urls[i];
    console.log(""Start scraping: "" + url);

    page.open(url, function () {
        waitFor(function() {
            return page.evaluate(function() {
                return document.getElementById(""progressWrapper"").childNodes.length == 1;
            });

        }, function() {
            var price = page.evaluate(function() {
                // do something
                return price;
            });

            console.log(price);
            result = url + "" ; "" + price;
            output = output + ""\r\n"" + result;
        });
    });

}
fs.write('test.txt', output);
phantom.exit();

I want to scrape all sites in the array urls, extract some information and then write this information to a text file.
But there seems to be a problem with the for loop. When scraping only one site without using a loop, all works as I want. But with the loop, first nothing happens, then the line 
console.log(""Start scraping: "" + url);

is shown, but one time too much.
If url = {a,b,c}, then phantomjs does:
Start scraping: a 
Start scraping: b 
Start scraping: c 
Start scraping:

It seems that page.open isn't called at all.
I am newbie to JS so I am sorry for this stupid question.
",4k,"
            1
        ","['\nPhantomJS is asynchronous. By calling page.open() multiple times using a loop, you essentially rush the execution of the callback. You\'re overwriting the current request before it is finished with a new request which is then again overwritten. You need to execute them one after the other, for example like this:\npage.open(url, function () {\n    waitFor(function() {\n       // something\n    }, function() {\n        page.open(url, function () {\n            waitFor(function() {\n               // something\n            }, function() {\n                // and so on\n            });\n        });\n    });\n});\n\nBut this is tedious. There are utilities that can help you with writing nicer code like async.js. You can install it in the directory of the phantomjs script through npm.\nvar async = require(""async""); // install async through npm\nvar tests = urls.map(function(url){\n    return function(callback){\n        page.open(url, function () {\n            waitFor(function() {\n               // something\n            }, function() {\n                callback();\n            });\n        });\n    };\n});\nasync.series(tests, function finish(){\n    fs.write(\'test.txt\', output);\n    phantom.exit();\n});\n\nIf you don\'t want any dependencies, then it is also easy to define your own recursive function (from here):\nvar urls = [/*....*/];\n\nfunction handle_page(url){\n    page.open(url, function(){\n        waitFor(function() {\n           // something\n        }, function() {\n            next_page();\n        });\n    });\n}\n\nfunction next_page(){\n    var url = urls.shift();\n    if(!urls){\n        phantom.exit(0);\n    }\n    handle_page(url);\n}\n\nnext_page();\n\n']"
Scraping contents of multi web pages of a website using BeautifulSoup and Selenium,"
The website I want to scrap is :
http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061
I want to get the last page number of the above the link for proceeding, which is 499 while taking the screenshot.

My code :
   from bs4 import BeautifulSoup 
   from urllib.request import urlopen as uReq
   from selenium import webdriver;import time
   from selenium.webdriver.common.by import By
   from selenium.webdriver.support.ui import WebDriverWait
   from selenium.webdriver.support import expected_conditions as EC
   from selenium.webdriver.common.desired_capabilities import         DesiredCapabilities

   firefox_capabilities = DesiredCapabilities.FIREFOX
   firefox_capabilities['marionette'] = True
   firefox_capabilities['binary'] = '/etc/firefox'

   driver = webdriver.Firefox(capabilities=firefox_capabilities)
   url = ""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061""

   driver.get(url)
   wait = WebDriverWait(driver, 10)
   soup=BeautifulSoup(driver.page_source,""lxml"")
   containers = soup.findAll(""ul"",{""class"":""pages table""})
   containers[0] = soup.findAll(""li"")
   li_len = len(containers[0])
   for item in soup.find(""ul"",{""class"":""pages table""}) : 
   li_text = item.select(""li"")[li_len].text
   print(""li_text : {}\n"".format(li_text))
   driver.quit()

I need help to figure out the error in my code for getting the last page number. Also, I would be grateful if someone give the alternate solution for the same and suggest ways to achieve my intention.
",1k,"
            0
        ","['\nIf you want to get the last page number of the above the link for proceeding, which is 499 you can use either Selenium or Beautifulsoup as follows :\n\nSelenium :\nfrom selenium import webdriver\n\ndriver = webdriver.Firefox(executable_path=r\'C:\\Utility\\BrowserDrivers\\geckodriver.exe\')\nurl = ""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061""\ndriver.get(url)\nelement = driver.find_element_by_xpath(""//div[@class=\'row pagination\']//p/span[contains(.,\'Reviews on Reliance Jio\')]"")\ndriver.execute_script(""return arguments[0].scrollIntoView(true);"", element)\nprint(driver.find_element_by_xpath(""//ul[@class=\'pagination table\']/li/ul[@class=\'pages table\']//li[last()]/a"").get_attribute(""innerHTML""))\ndriver.quit()\n\nConsole Output :\n499\n\n\nBeautifulsoup :\nimport bs4\nfrom bs4 import BeautifulSoup as soup\nfrom urllib.request import urlopen as uReq\n\nurl = ""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061""\nuClient = uReq(url)\npage_html = uClient.read()\nuClient.close()\npage_soup = soup(page_html, ""html.parser"")\ncontainer = page_soup.find(""ul"",{""class"":""pages table""})\nall_li = container.findAll(""li"")\nlast_div = None\nfor last_div in all_li:pass\nif last_div:\n    content = last_div.getText()\n    print(content)\n\nConsole Output :\n499\n\n']"
Scrape web pages in real time with Node.js,"
What's a good was to scrape website content using Node.js. I'd like to build something very, very fast that can execute searches in the style of kayak.com, where one query is dispatched to several different sites, the results scraped, and returned to the client as they become available.
Let's assume that this script should just provide the results in JSON format, and we can process them either directly in the browser or in another web application.
A few starting points:
Using node.js and jquery to scrape websites
Anybody have any ideas?
",46k,"
            66
        ","['\nNode.io seems to take the cake :-)\n', '\nAll aforementioned solutions presume running the scraper locally. This means you will be severely limited in performance (due to running them in sequence or in a limited set of threads). A better approach, imho, is to rely on an existing, albeit commercial, scraping grid.\nHere is an example:\nvar bobik = new Bobik(""YOUR_AUTH_TOKEN"");\nbobik.scrape({\n  urls: [\'amazon.com\', \'zynga.com\', \'http://finance.google.com/\', \'http://shopping.yahoo.com\'],\n  queries:  [""//th"", ""//img/@src"", ""return document.title"", ""return $(\'script\').length"", ""#logo"", "".logo""]\n}, function (scraped_data) {\n  if (!scraped_data) {\n    console.log(""Data is unavailable"");\n    return;\n  }\n  var scraped_urls = Object.keys(scraped_data);\n  for (var url in scraped_urls)\n    console.log(""Results from "" + url + "": "" + scraped_data[scraped_urls[url]]);\n});\n\nHere, scraping is performed remotely and a callback is issued to your code only when results are ready (there is also an option to collect results as they become available).\nYou can download Bobik client proxy SDK at https://github.com/emirkin/bobik_javascript_sdk\n', ""\nI've been doing research myself, and https://npmjs.org/package/wscraper boasts itself as a\n\na web scraper agent based on cheerio.js a fast, flexible, and lean\n  implementation of core jQuery; built on top of request.js; inspired by\n  http-agent.js\n\nVery low usage (according to npmjs.org) but worth a look for any interested parties.\n"", ""\nYou don't always need to jQuery. If you play with the DOM returned from jsdom for example you can easily take what you need yourself (also considering you dont have to worry about xbrowser issues.) See: https://gist.github.com/1335009 that's not taking away from node.io at all, just saying you might be able to do it yourself depending...\n"", ""\nThe new way using ES7/promises\nUsually when you're scraping you want to use some method to\n\nGet the resource on the webserver (html document usually)\nRead that resource and work with it as\n\n\nA DOM/tree structure and make it navigable\nparse it as token-document with something like SAS.\n\n\nBoth tree, and token-parsing have advantages, but tree is usually substantially simpler. We'll do that. Check out request-promise, here is how it works:\nconst rp = require('request-promise');\nconst cheerio = require('cheerio'); // Basically jQuery for node.js \n\nconst options = {\n    uri: 'http://www.google.com',\n    transform: function (body) {\n        return cheerio.load(body);\n    }\n};\n\nrp(options)\n    .then(function ($) {\n        // Process html like you would with jQuery... \n    })\n    .catch(function (err) {\n        // Crawling failed or Cheerio \n\nThis is using cheerio which is essentially a lightweight server-side jQuery-esque library (that doesn't need a window object, or jsdom).\nBecause you're using promises, you can also write this in an asychronous function. It'll look synchronous, but it'll be asynchronous with ES7:\nasync function parseDocument() {\n    let $;\n    try {\n      $ = await rp(options);\n    } catch (err) { console.error(err); }\n\n    console.log( $('title').text() ); // prints just the text in the <title>\n}\n\n"", '\ncheck out https://github.com/rc0x03/node-promise-parser\nFast: uses libxml C bindings\nLightweight: no dependencies like jQuery, cheerio, or jsdom\nClean: promise based interface- no more nested callbacks\nFlexible: supports both CSS and XPath selectors\n\n', ""\nI see most answers the right path with cheerio and so forth, however once you get to the point where you need to parse and execute JavaScript (ala SPA's and more), then I'd check out https://github.com/joelgriffith/navalia (I'm the author). Navalia is built to support scraping in a headless-browser context, and it's pretty quick. Thanks!\n"", '\nIt is my easy to use but badly spelled general purpose scraper https://github.com/harish2704/html-scraper written for Node.JS\nIt can extract information based on predefined schemas.\nA schema defnition includes a css selector and a data extraction function.\nIt currently using cheerio for dom parsing..\n']"
How can I scrape an HTML table to CSV?,"
The Problem
I use a tool at work that lets me do queries and get back HTML tables of info. I do not have any kind of back-end access to it.
A lot of this info would be much more useful if I could put it into a spreadsheet for sorting, averaging, etc. How can I screen-scrape this data to a CSV file?
My First Idea
Since I know jQuery, I thought I might use it to strip out the table formatting onscreen, insert commas and line breaks, and just copy the whole mess into notepad and save as a CSV. Any better ideas?
The Solution
Yes, folks, it really was as easy as copying and pasting. Don't I feel silly.
Specifically, when I pasted into the spreadsheet, I had to select ""Paste Special"" and choose the format ""text."" Otherwise it tried to paste everything into a single cell, even if I highlighted the whole spreadsheet.
",83k,"
            44
        ","[""\n\nSelect the HTML table in your tools's UI and copy it into the clipboard (if that's possible\nPaste it into Excel.\nSave as CSV file\n\nHowever, this is a manual solution not an automated one.\n"", '\nusing python: \nfor example imagine you want to scrape forex quotes in csv form from some site like:fxquotes\nthen...\nfrom BeautifulSoup import BeautifulSoup\nimport urllib,string,csv,sys,os\nfrom string import replace\n\ndate_s = \'&date1=01/01/08\'\ndate_f = \'&date=11/10/08\'\nfx_url = \'http://www.oanda.com/convert/fxhistory?date_fmt=us\'\nfx_url_end = \'&lang=en&margin_fixed=0&format=CSV&redirected=1\'\ncur1,cur2 = \'USD\',\'AUD\'\nfx_url = fx_url + date_f + date_s + \'&exch=\' + cur1 +\'&exch2=\' + cur1\nfx_url = fx_url +\'&expr=\' + cur2 +  \'&expr2=\' + cur2 + fx_url_end\ndata = urllib.urlopen(fx_url).read()\nsoup = BeautifulSoup(data)\ndata = str(soup.findAll(\'pre\', limit=1))\ndata = replace(data,\'[<pre>\',\'\')\ndata = replace(data,\'</pre>]\',\'\')\nfile_location = \'/Users/location_edit_this\'\nfile_name = file_location + \'usd_aus.csv\'\nfile = open(file_name,""w"")\nfile.write(data)\nfile.close()\n\n\nedit: to get values from a table:\nexample from: palewire\nfrom mechanize import Browser\nfrom BeautifulSoup import BeautifulSoup\n\nmech = Browser()\n\nurl = ""http://www.palewire.com/scrape/albums/2007.html""\npage = mech.open(url)\n\nhtml = page.read()\nsoup = BeautifulSoup(html)\n\ntable = soup.find(""table"", border=1)\n\nfor row in table.findAll(\'tr\')[1:]:\n    col = row.findAll(\'td\')\n\n    rank = col[0].string\n    artist = col[1].string\n    album = col[2].string\n    cover_link = col[3].img[\'src\']\n\n    record = (rank, artist, album, cover_link)\n    print ""|"".join(record)\n\n', '\nThis is my python version using the (currently) latest version of BeautifulSoup which can be obtained using, e.g.,\n$ sudo easy_install beautifulsoup4\n\nThe script reads HTML from the standard input, and outputs the text found in all tables in proper CSV format.\n#!/usr/bin/python\nfrom bs4 import BeautifulSoup\nimport sys\nimport re\nimport csv\n\ndef cell_text(cell):\n    return "" "".join(cell.stripped_strings)\n\nsoup = BeautifulSoup(sys.stdin.read())\noutput = csv.writer(sys.stdout)\n\nfor table in soup.find_all(\'table\'):\n    for row in table.find_all(\'tr\'):\n        col = map(cell_text, row.find_all(re.compile(\'t[dh]\')))\n        output.writerow(col)\n    output.writerow([])\n\n', '\nEven easier (because it saves it for you for next time) ...\nIn Excel\nData/Import External Data/New Web Query\nwill take you to a url prompt. Enter your url, and it will delimit available tables on the page to import. Voila.\n', '\nTwo ways come to mind (especially for those of us that don\'t have Excel):\n\nGoogle Spreadsheets has an excellent importHTML function: \n\n=importHTML(""http://example.com/page/with/table"", ""table"", index\nIndex starts at 1\nI recommend a copy and paste values shortly after import\nFile -> Download as -> CSV\n\nPython\'s superb Pandas library has handy read_html and to_csv functions\n\nHere\'s a basic Python3 script that prompts for the URL, which table at that URL, and a filename for the CSV.\n\n\n', '\nQuick and dirty:\nCopy out of browser into Excel, save as CSV.\nBetter solution (for long term use):\nWrite a bit of code in the language of your choice that will pull the html contents down, and scrape out the bits that you want.  You could probably throw in all of the data operations (sorting, averaging, etc) on top of the data retrieval.  That way, you just have to run your code and you get the actual report that you want.\nIt all depends on how often you will be performing this particular task.\n', '\nExcel can open a http page.\nEg:\n\nClick File, Open\nUnder filename, paste the URL  ie: How can I scrape an HTML table to CSV?\nClick ok\n\nExcel does its best to convert the html to a table.\nIts not the most elegant solution, but does work!\n', '\nBasic Python implementation using BeautifulSoup, also considering both rowspan and colspan:\nfrom BeautifulSoup import BeautifulSoup\n\ndef table2csv(html_txt):\n   csvs = []\n   soup = BeautifulSoup(html_txt)\n   tables = soup.findAll(\'table\')\n\n   for table in tables:\n       csv = \'\'\n       rows = table.findAll(\'tr\')\n       row_spans = []\n       do_ident = False\n\n       for tr in rows:\n           cols = tr.findAll([\'th\',\'td\'])\n\n           for cell in cols:\n               colspan = int(cell.get(\'colspan\',1))\n               rowspan = int(cell.get(\'rowspan\',1))\n\n               if do_ident:\n                   do_ident = False\n                   csv += \',\'*(len(row_spans))\n\n               if rowspan > 1: row_spans.append(rowspan)\n\n               csv += \'""{text}""\'.format(text=cell.text) + \',\'*(colspan)\n\n           if row_spans:\n               for i in xrange(len(row_spans)-1,-1,-1):\n                   row_spans[i] -= 1\n                   if row_spans[i] < 1: row_spans.pop()\n\n           do_ident = True if row_spans else False\n\n           csv += \'\\n\'\n\n       csvs.append(csv)\n       #print csv\n\n   return \'\\n\\n\'.join(csvs)\n\n', '\nHere is a tested example that combines grequest and soup to download large quantities of pages from a structured website:\n#!/usr/bin/python\n\nfrom bs4 import BeautifulSoup\nimport sys\nimport re\nimport csv\nimport grequests\nimport time\n\ndef cell_text(cell):\n    return "" "".join(cell.stripped_strings)\n\ndef parse_table(body_html):\n    soup = BeautifulSoup(body_html)\n    for table in soup.find_all(\'table\'):\n        for row in table.find_all(\'tr\'):\n            col = map(cell_text, row.find_all(re.compile(\'t[dh]\')))\n            print(col)\n\ndef process_a_page(response, *args, **kwargs): \n    parse_table(response.content)\n\ndef download_a_chunk(k):\n    chunk_size = 10 #number of html pages\n    x = ""http://www.blahblah....com/inclusiones.php?p=""\n    x2 = ""&name=...""\n    URLS = [x+str(i)+x2 for i in range(k*chunk_size, k*(chunk_size+1)) ]\n    reqs = [grequests.get(url, hooks={\'response\': process_a_page}) for url in URLS]\n    resp = grequests.map(reqs, size=10)\n\n# download slowly so the server does not block you\nfor k in range(0,500):\n    print(""downloading chunk "",str(k))\n    download_a_chunk(k)\n    time.sleep(11)\n\n', ""\nHave you tried opening it with excel?\nIf you save a spreadsheet in excel as html you'll see the format excel uses.\nFrom a web app I wrote I spit out this html format so the user can export to excel.\n"", ""\nIf you're screen scraping and the table you're trying to convert has a given ID, you could always do a regex parse of the html along with some scripting to generate a CSV.\n""]"
How to download any(!) webpage with correct charset in python?,"
Problem
When screen-scraping a webpage using python one has to know the character encoding of the page. If you get the character encoding wrong than your output will be messed up.
People usually use some rudimentary technique to detect the encoding. They either use the charset from the header or the charset defined in the meta tag or they use an encoding detector (which does not care about meta tags or headers).
By using only one these techniques, sometimes you will not get the same result as you would in a browser.
Browsers do it this way:

Meta tags always takes precedence (or xml definition)
Encoding defined in the header is used when there is no charset defined in a meta tag
If the encoding is not defined at all, than it is time for encoding detection.

(Well... at least that is the way I believe most browsers do it. Documentation is really scarce.)
What I'm looking for is a library that can decide the character set of a page the way a browser would. I'm sure I'm not the first who needs a proper solution to this problem.
Solution (I have not tried it yet...)
According to Beautiful Soup's documentation.
Beautiful Soup tries the following encodings, in order of priority, to turn your document into Unicode:

An encoding you pass in as the
fromEncoding argument to the soup
constructor.
An encoding discovered  in the document itself: for instance,   in an XML declaration or (for HTML   documents) an http-equiv META tag. If   Beautiful Soup finds this kind of   encoding within the document, it   parses the document again from the   beginning and gives the new encoding   a try. The only exception is if you   explicitly specified an encoding, and   that encoding actually worked: then   it will ignore any encoding it finds   in the document.
An encoding sniffed   by looking at the first few bytes of   the file. If an encoding is detected
at this stage, it will be one of the
UTF-* encodings, EBCDIC, or ASCII.
An
encoding sniffed by the chardet
library, if you have it installed.
UTF-8
Windows-1252

",16k,"
            35
        ","[""\nWhen you download a file with urllib or urllib2, you can find out whether a charset header was transmitted:\nfp = urllib2.urlopen(request)\ncharset = fp.headers.getparam('charset')\n\nYou can use BeautifulSoup to locate a meta element in the HTML:\nsoup = BeatifulSoup.BeautifulSoup(data)\nmeta = soup.findAll('meta', {'http-equiv':lambda v:v.lower()=='content-type'})\n\nIf neither is available, browsers typically fall back to user configuration, combined with auto-detection. As rajax proposes, you could use the chardet module. If you have user configuration available telling you that the page should be Chinese (say), you may be able to do better.\n"", '\nUse the Universal Encoding Detector:\n>>> import chardet\n>>> chardet.detect(urlread(""http://google.cn/""))\n{\'encoding\': \'GB2312\', \'confidence\': 0.99}\n\nThe other option would be to just use wget:\n  import os\n  h = os.popen(\'wget -q -O foo1.txt http://foo.html\')\n  h.close()\n  s = open(\'foo1.txt\').read()\n\n', ""\nIt seems like you need a hybrid of the answers presented:\n\nFetch the page using urllib\nFind <meta> tags using beautiful soup or other method\nIf no meta tags exist, check the headers returned by urllib\nIf that still doesn't give you an answer, use the universal encoding detector.\n\nI honestly don't believe you're going to find anything better than that.  \nIn fact if you read further into the FAQ you linked to in the comments on the other answer, that's what the author of detector library advocates.\nIf you believe the FAQ, this is what the browsers do (as requested in your original question) as the detector is a port of the firefox sniffing code.\n"", '\nI would use html5lib for this.\n', ""\nScrapy downloads a page and detects a correct encoding for it, unlike requests.get(url).text or urlopen. To do so it tries to follow browser-like rules - this is the best one can do, because website owners have incentive to make their websites work in a browser. Scrapy needs to take HTTP headers, <meta> tags, BOM marks and differences in encoding names in account. \nContent-based guessing (chardet, UnicodeDammit) on its own is not a correct solution, as it may fail; it should be only used as a last resort when headers or <meta> or BOM marks are not available or provide no information.\nYou don't have to use Scrapy to get its encoding detection functions; they are released (among with some other stuff) in a separate library called w3lib: https://github.com/scrapy/w3lib. \nTo get page encoding and unicode body use w3lib.encoding.html_to_unicode function, with a content-based guessing fallback:\nimport chardet\nfrom w3lib.encoding import html_to_unicode\n\ndef _guess_encoding(data):\n    return chardet.detect(data).get('encoding')\n\ndetected_encoding, html_content_unicode = html_to_unicode(\n    content_type_header,\n    html_content_bytes,\n    default_encoding='utf8', \n    auto_detect_fun=_guess_encoding,\n)\n\n"", '\ninstead of trying to get a page then figuring out the charset the browser would use, why not just use a browser to fetch the page and check what charset it uses.. \nfrom win32com.client import DispatchWithEvents\nimport threading\n\n\nstopEvent=threading.Event()\n\nclass EventHandler(object):\n    def OnDownloadBegin(self):\n        pass\n\ndef waitUntilReady(ie):\n    """"""\n    copypasted from\n    http://mail.python.org/pipermail/python-win32/2004-June/002040.html\n    """"""\n    if ie.ReadyState!=4:\n        while 1:\n            print ""waiting""\n            pythoncom.PumpWaitingMessages()\n            stopEvent.wait(.2)\n            if stopEvent.isSet() or ie.ReadyState==4:\n                stopEvent.clear()\n                break;\n\nie = DispatchWithEvents(""InternetExplorer.Application"", EventHandler)\nie.Visible = 0\nie.Navigate(\'http://kskky.info\')\nwaitUntilReady(ie)\nd = ie.Document\nprint d.CharSet\n\n', '\nBeautifulSoup dose this with UnicodeDammit : Unicode, Dammit\n']"
"Nokogiri, open-uri, and Unicode Characters","
I'm using Nokogiri and open-uri to grab the contents of the title tag on a webpage, but am having trouble with accented characters.  What's the best way to deal with these?  Here's what I'm doing:
require 'open-uri'
require 'nokogiri'

doc = Nokogiri::HTML(open(link))
title = doc.at_css(""title"")

At this point, the title looks like this:

Rag\303\271

Instead of:

Ragù

How can I have nokogiri return the proper character (e.g. ù in this case)?
Here's an example URL:
http://www.epicurious.com/recipes/food/views/Tagliatelle-with-Duck-Ragu-242037
",23k,"
            27
        ","['\nSummary: When feeding UTF-8 to Nokogiri through open-uri, use open(...).read and pass the resulting string to Nokogiri.\nAnalysis:\nIf I fetch the page using curl, the headers properly show Content-Type: text/html; charset=UTF-8 and the file content includes valid UTF-8, e.g. ""Genealogía de Jesucristo"". But even with a magic comment on the Ruby file and setting the doc encoding, it\'s no good:\n# encoding: UTF-8\nrequire \'nokogiri\'\nrequire \'open-uri\'\n\ndoc = Nokogiri::HTML(open(\'http://www.biblegateway.com/passage/?search=Mateo1-2&version=NVI\'))\ndoc.encoding = \'utf-8\'\nh52 = doc.css(\'h5\')[1]\nputs h52.text, h52.text.encoding\n#=> GenealogÃ a de Jesucristo\n#=> UTF-8\n\nWe can see that this is not the fault of open-uri:\nhtml = open(\'http://www.biblegateway.com/passage/?search=Mateo1-2&version=NVI\')\ngene = html.read[/Gene\\S+/]\nputs gene, gene.encoding\n#=> Genealogía\n#=> UTF-8\n\nThis is a Nokogiri issue when dealing with open-uri, it seems. This can be worked around by passing the HTML as a raw string to Nokogiri:\n# encoding: UTF-8\nrequire \'nokogiri\'\nrequire \'open-uri\'\n\nhtml = open(\'http://www.biblegateway.com/passage/?search=Mateo1-2&version=NVI\')\ndoc = Nokogiri::HTML(html.read)\ndoc.encoding = \'utf-8\'\nh52 = doc.css(\'h5\')[1].text\nputs h52, h52.encoding, h52 == ""Genealogía de Jesucristo""\n#=> Genealogía de Jesucristo\n#=> UTF-8\n#=> true\n\n', ""\nI was having the same problem and the Iconv approach wasn't working. Nokogiri::HTML is an alias to Nokogiri::HTML.parse(thing, url, encoding, options).\nSo, you just need to do:\ndoc = Nokogiri::HTML(open(link).read, nil, 'utf-8')\nand it'll convert the page encoding properly to utf-8. You'll see Ragù instead of Rag\\303\\271.\n"", '\nWhen you say ""looks like this,"" are you viewing this value IRB? It\'s going to escape non-ASCII range characters with C-style escaping of the byte sequences that represent the characters.\nIf you print them with puts, you\'ll get them back as you expect, presuming your shell console is using the same encoding as the string in question (Apparently UTF-8 in this case, based on the two bytes returned for that character). If you are storing the values in a text file, printing to a handle should also result in UTF-8 sequences.\nIf you need to translate between UTF-8 and other encodings, the specifics depend on whether you\'re in Ruby 1.9 or 1.8.6.\nFor 1.9: http://blog.grayproductions.net/articles/ruby_19s_string\nfor 1.8, you probably need to look at Iconv.\nAlso, if you need to interact with COM components in Windows, you\'ll need to tell ruby to use the correct encoding with something like the following:\nrequire \'win32ole\'\n\nWIN32OLE.codepage = WIN32OLE::CP_UTF8\n\nIf you\'re interacting with mysql, you\'ll need to set the collation on the table to one that supports the encoding that you\'re working with. In general, it\'s best to set the collation to UTF-8, even if some of your content is coming back in other encodings; you\'ll just need to convert as necessary.\nNokogiri has some features for dealing with different encodings (probably through Iconv), but I\'m a little out of practice with that, so I\'ll leave explanation of that to someone else.\n', '\nTry setting the encoding option of Nokogiri, like so:\nrequire \'open-uri\'\nrequire \'nokogiri\'\ndoc = Nokogiri::HTML(open(link))\ndoc.encoding = \'utf-8\'\ntitle = doc.at_css(""title"")\n\n', '\nYou need to convert the response from the website being scraped (here epicurious.com) into utf-8 encoding.\nas per the html content from the page being scraped, its ""ISO-8859-1"" for now. So, you need to do something like this:\nrequire \'iconv\'\ndoc = Nokogiri::HTML(Iconv.conv(\'utf-8//IGNORE\', \'ISO-8859-1\', open(link).read))\n\nRead more about it here: http://www.quarkruby.com/2009/9/22/rails-utf-8-and-html-screen-scraping\n', '\nChanging Nokogiri::HTML(...) to Nokogiri::HTML5(...) fixed issues I was having with parsing certain special character, specifically em-dashes.\n(The accented characters in your link came through fine in both, so don\'t know if this would help you with that.)\nEXAMPLE:\nurl = \'https://www.youtube.com/watch?v=4r6gr7uytQA\'\n\ndoc = Nokogiri::HTML(open(url))\ndoc.title\n=> ""Josh Waitzkin â\\u0080\\u0094 How to Cram 2 Months of Learning into 1 Day | The Tim Ferriss Show - YouTube""\n\ndoc = Nokogiri::HTML5(open(url))\ndoc.title\n=> ""Josh Waitzkin — How to Cram 2 Months of Learning into 1 Day | The Tim Ferriss Show - YouTube""\n\n', '\nJust to add a cross-reference, this SO page gives some related information:\nHow to make Nokogiri transparently return un/encoded Html entities untouched?\n', ""\nTip: you could also use the Scrapifier gem to get metadata, as the page title, from URIs in a very simple way. The data are all encoded in UTF-8.\nCheck it out: https://github.com/tiagopog/scrapifier\nHope it's useful for you.\n""]"
Using Python and Mechanize to submit form data and authenticate,"
I want to submit login to the website Reddit.com, navigate to a particular area of the page, and submit a comment.  I don't see what's wrong with this code, but it is not working in that no change is reflected on the Reddit site.
import mechanize
import cookielib


def main():

#Browser
br = mechanize.Browser()


# Cookie Jar
cj = cookielib.LWPCookieJar()
br.set_cookiejar(cj)

# Browser options
br.set_handle_equiv(True)
br.set_handle_gzip(True)
br.set_handle_redirect(True)
br.set_handle_referer(True)
br.set_handle_robots(False)

# Follows refresh 0 but not hangs on refresh > 0
br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)

#Opens the site to be navigated
r= br.open('http://www.reddit.com')
html = r.read()

# Select the second (index one) form
br.select_form(nr=1)

# User credentials
br.form['user'] = 'DUMMYUSERNAME'
br.form['passwd'] = 'DUMMYPASSWORD'

# Login
br.submit()

#Open up comment page
r= br.open('http://www.reddit.com/r/PoopSandwiches/comments/f47f8/testing/')
html = r.read()

#Text box is the 8th form on the page (which, I believe, is the text area)
br.select_form(nr=7)

#Change 'text' value to a testing string
br.form['text']= ""this is an automated test""

#Submit the information  
br.submit()

What's wrong with this?
",26k,"
            15
        ","['\nI would definitely suggest trying to use the API if possible, but this works for me (not for your example post, which has been deleted, but for any active one):\n#!/usr/bin/env python\n\nimport mechanize\nimport cookielib\nimport urllib\nimport logging\nimport sys\n\ndef main():\n\n    br = mechanize.Browser()\n    cj = cookielib.LWPCookieJar()\n    br.set_cookiejar(cj)\n\n    br.set_handle_equiv(True)\n    br.set_handle_gzip(True)\n    br.set_handle_redirect(True)\n    br.set_handle_referer(True)\n    br.set_handle_robots(False)\n\n    br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)\n\n    r= br.open(\'http://www.reddit.com\')\n\n    # Select the second (index one) form\n    br.select_form(nr=1)\n\n    # User credentials\n    br.form[\'user\'] = \'user\'\n    br.form[\'passwd\'] = \'passwd\'\n\n    # Login\n    br.submit()\n\n    # Open up comment page\n    posting = \'http://www.reddit.com/r/PoopSandwiches/comments/f47f8/testing/\'\n    rval = \'PoopSandwiches\'\n    # you can get the rval in other ways, but this will work for testing\n\n    r = br.open(posting)\n\n    # You need the \'uh\' value from the first form\n    br.select_form(nr=0)\n    uh = br.form[\'uh\']\n\n    br.select_form(nr=7)\n    thing_id = br.form[\'thing_id\']\n    id = \'#\' + br.form.attrs[\'id\']\n    # The id that gets posted is the form id with a \'#\' prepended.\n\n    data = {\'uh\':uh, \'thing_id\':thing_id, \'id\':id, \'renderstyle\':\'html\', \'r\':rval, \'text\':""Your text here!""}\n    new_data_dict = dict((k, urllib.quote(v).replace(\'%20\', \'+\')) for k, v in data.iteritems())\n\n    # not sure if the replace needs to happen, I did it anyway\n    new_data = \'thing_id=%(thing_id)s&text=%(text)s&id=%(id)s&r=%(r)s&uh=%(uh)s&renderstyle=%(renderstyle)s\' %(new_data_dict)\n\n    # not sure which of these headers are really needed, but it works with all\n    # of them, so why not just include them.\n    req = mechanize.Request(\'http://www.reddit.com/api/comment\', new_data)\n    req.add_header(\'Referer\', posting)\n    req.add_header(\'Accept\', \' application/json, text/javascript, */*\')\n    req.add_header(\'Content-Type\', \'application/x-www-form-urlencoded; charset=UTF-8\')\n    req.add_header(\'X-Requested-With\', \'XMLHttpRequest\')\n    cj.add_cookie_header(req)\n    res = mechanize.urlopen(req)\n\nmain()\n\nIt would be interesting to turn javascript off and see how the reddit comments are handled then.  Right now there is a bunch of magic that happens in an onsubmit function called when making your post.  This is where the uh and id value get added.\n']"
View Generated Source (After AJAX/JavaScript) in C#,"
Is there a way to view the generated source of a web page (the code after all AJAX calls and JavaScript DOM manipulations have taken place) from a C# application without opening up a browser from the code?
Viewing the initial page using a WebRequest or WebClient object works ok, but if the page makes extensive use of JavaScript to alter the DOM on page load, then these don't provide an accurate picture of the page.
I have tried using Selenium and Watin UI testing frameworks and they work perfectly, supplying the generated source as it appears after all JavaScript manipulations are completed.  Unfortunately, they do this by opening up an actual web browser, which is very slow.  I've implemented a selenium server which offloads this work to another machine, but there is still a substantial delay.
Is there a .Net library that will load and parse a page (like a browser) and spit out the generated code?  Clearly, Google and Yahoo aren't opening up browsers for every page they want to spider (of course they may have more resources than me...).  
Is there such a library or am I out of luck unless I'm willing to dissect the source code of an open source browser?
SOLUTION
Well, thank you everyone for you're help.  I have a working solution that is about 10X faster then Selenium. Woo!
Thanks to this old article from beansoftware I was able to use the System.Windows.Forms.WebBrowser control to download the page and parse it, then give em the generated source.  Even though the control is in Windows.Forms, you can still run it from Asp.Net (which is what I'm doing), just remember to add System.Window.Forms to your project references.
There are two notable things about the code.  First, the WebBrowser control is called in a new thread.  This is because it must run on a single threaded apartment.
Second, the GeneratedSource variable is set in two places.  This is not due to an intelligent design decision :)  I'm still working on it and will update this answer when I'm done.  wb_DocumentCompleted() is called multiple times.  First when the initial HTML is downloaded, then again when the first round of JavaScript completes.  Unfortunately, the site I'm scraping has 3 different loading stages.  1) Load initial HTML 2) Do first round of JavaScript DOM manipulation 3) pause for half a second then do a second round of JS DOM manipulation.
For some reason, the second round isn't cause by the wb_DocumentCompleted() function, but it is always caught when wb.ReadyState == Complete.  So why not remove it from wb_DocumentCompleted()? I'm still not sure why it isn't caught there and that's where the beadsoftware article recommended putting it.  I'm going to keep looking into it.  I just wanted to publish this code so anyone who's interested can use it.  Enjoy!
using System.Threading;
using System.Windows.Forms;

public class WebProcessor
{
    private string GeneratedSource{ get; set; }
    private string URL { get; set; }

    public string GetGeneratedHTML(string url)
    {
        URL = url;

        Thread t = new Thread(new ThreadStart(WebBrowserThread));
        t.SetApartmentState(ApartmentState.STA);
        t.Start();
        t.Join();

        return GeneratedSource;
    }

    private void WebBrowserThread()
    {
        WebBrowser wb = new WebBrowser();
        wb.Navigate(URL);

        wb.DocumentCompleted += 
            new WebBrowserDocumentCompletedEventHandler(
                wb_DocumentCompleted);

        while (wb.ReadyState != WebBrowserReadyState.Complete)
            Application.DoEvents();

        //Added this line, because the final HTML takes a while to show up
        GeneratedSource= wb.Document.Body.InnerHtml;

        wb.Dispose();
    }

    private void wb_DocumentCompleted(object sender, 
        WebBrowserDocumentCompletedEventArgs e)
    {
        WebBrowser wb = (WebBrowser)sender;
        GeneratedSource= wb.Document.Body.InnerHtml;
    }
}

",14k,"
            27
        ","['\nit is possibly using an instance of a browser (in you case: the ie control). you can easily use in your app and open a page. the control will then load it and process any javascript. once this is done you can access the controls dom object and get the ""interpreted"" code.\n', '\nBest way is using PhantomJs. That\'s Great. (sample of that is Article).\nMy solution is look like this:\nvar page = require(\'webpage\').create();\n\npage.open(""https://sample.com"", function(){\n    page.evaluate(function(){\n        var i = 0,\n        oJson = jsonData,\n        sKey;\n        localStorage.clear();\n\n        for (; sKey = Object.keys(oJson)[i]; i++) {\n            localStorage.setItem(sKey,oJson[sKey])\n        }\n    });\n\n    page.open(""https://sample.com"", function(){\n        setTimeout(function(){\n         page.render(""screenshoot.png"") \n            // Where you want to save it    \n           console.log(page.content); //page source\n            // You can access its content using jQuery\n            var fbcomments = page.evaluate(function(){\n                return $(""body"").contents().find("".content"") \n            }) \n            phantom.exit();\n        },10000)\n    });     \n});\n\n', ""\nTheoretically yes, but, at present, no.\nI don't think there is currently a product or OSS project that does this.  Such a product would need to have it's own javascript interpreter and be able to accurately emulate the run-time environment and quirks of every browser it supports.\nGiven that you need something that accurately emulates the server + browser environment in order to produce the final page code, in the long run, I think that using a browser instance is the best way to accurately generate the page in its final state. This is especially true, when you consider that, after the page load completes, the page sources can still change over time in the browser from AJAX/javascript.\n""]"
How to use the WebClient.DownloadDataAsync() method in this context?,"
My plan is to have a user write down a movie title in my program and my program will pull the appropiate information asynchronously so the UI doesn't freeze up.
Here's the code:
public class IMDB
    {
        WebClient WebClientX = new WebClient();
        byte[] Buffer = null;


        public string[] SearchForMovie(string SearchParameter)
        {
            //Format the search parameter so it forms a valid IMDB *SEARCH* url.
            //From within the search website we're going to pull the actual movie
            //link.
            string sitesearchURL = FindURL(SearchParameter);

            //Have a method download asynchronously the ENTIRE source code of the
            //IMDB *search* website.
            Buffer = WebClientX.DownloadDataAsync(sitesearchURL);


            //Pass the IMDB source code to method findInformation().

            //string [] lol = findInformation();

            //????

            //Profit.

            string[] lol = null;
            return lol;
        }

My actual problem lies in the WebClientX.DownloadDataAsync() method. I can't use a string URL for it. How can I use that built in function to download the bytes of the site (for later use I will convert this to string, I know how to do this) and without freezing up my GUI?
Perhaps a clear cut example of the DownloadDataAsync so I can learn how to use it?
Thanks SO, you're always such a great resource.
",52k,"
            21
        ","[""\nThere is a newer DownloadDataTaskAsync method that allows you to await the result. It is simpler to read and easier to wire up by far. I'd use that...\nvar client = new WebClient();\n\nvar data = await client.DownloadDataTaskAsync(new Uri(imageUrl));\n\nawait outstream.WriteAsync(data, 0, data.Length);\n\n"", '\nYou need to handle the DownloadDataCompleted event:\nstatic void Main()\n{\n    string url = ""http://google.com"";\n    WebClient client = new WebClient();\n    client.DownloadDataCompleted += DownloadDataCompleted;\n    client.DownloadDataAsync(new Uri(url));\n    Console.ReadLine();\n}\n\nstatic void DownloadDataCompleted(object sender,\n    DownloadDataCompletedEventArgs e)\n{\n    byte[] raw = e.Result;\n    Console.WriteLine(raw.Length + "" bytes received"");\n}\n\nThe args contains other bits of information relating to error conditions etc - check those too.\nAlso note that you\'ll be coming into DownloadDataCompleted on a different thread; if you are in a UI (winform, wpf, etc) you\'ll need to get to the UI thread before updating the UI. From winforms, use this.Invoke. For WPF, look at the Dispatcher.\n', '\nstatic void Main(string[] args)\n{\n    byte[] data = null;\n    WebClient client = new WebClient();\n    client.DownloadDataCompleted += \n       delegate(object sender, DownloadDataCompletedEventArgs e)\n       {\n            data = e.Result;\n       };\n    Console.WriteLine(""starting..."");\n    client.DownloadDataAsync(new Uri(""http://stackoverflow.com/questions/""));\n    while (client.IsBusy)\n    {\n         Console.WriteLine(""\\twaiting..."");\n         Thread.Sleep(100);\n    }\n    Console.WriteLine(""done. {0} bytes received;"", data.Length);\n}\n\n', '\nIf anyone using above in web application or websites please set Async = ""true"" in the page directive declaration in aspx file. \n', '\nThreadPool.QueueUserWorkItem(state => WebClientX.DownloadDataAsync(sitesearchURL));\n\nhttp://workblog.pilin.name/2009/02/system.html\n', '\n//using ManualResetEvent class\nstatic ManualResetEvent evnts = new ManualResetEvent(false);\nstatic void Main(string[] args)\n{\n    byte[] data = null;\n    WebClient client = new WebClient();\n    client.DownloadDataCompleted += \n        delegate(object sender, DownloadDataCompletedEventArgs e)\n        {\n             data = e.Result;\n             evnts.Set();\n        };\n    Console.WriteLine(""starting..."");\n    evnts.Reset();\n    client.DownloadDataAsync(new Uri(""http://stackoverflow.com/questions/""));\n    evnts.WaitOne(); // wait to download complete\n\n    Console.WriteLine(""done. {0} bytes received;"", data.Length);\n}\n\n']"
Text Extraction from HTML Java,"
I'm working on a program that downloads HTML pages and then selects some of the information and write it to another file.
I want to extract the information which is intbetween the paragraph tags, but i can only get one line of the paragraph. My code is as follows;
FileReader fileReader = new FileReader(file);
BufferedReader buffRd = new BufferedReader(fileReader);
BufferedWriter out = new BufferedWriter(new FileWriter(newFile.txt));
String s;

while ((s = br.readLine()) !=null) {
    if(s.contains(""<p>"")) {
        try {
            out.write(s);
        } catch (IOException e) {
        }
    }
}

i was trying to add another while loop, which would tell the program to keep writing to file until the line contains the </p> tag, by saying;
while ((s = br.readLine()) !=null) {
    if(s.contains(""<p>"")) {
        while(!s.contains(""</p>"") {
            try {
                out.write(s);
            } catch (IOException e) {
            }
        }
    }
}

But this doesn't work. Could someone please help.
",52k,"
            19
        ","['\njsoup\nAnother html parser I really liked using was jsoup. You could get all the <p> elements in 2 lines of code.\nDocument doc = Jsoup.connect(""http://en.wikipedia.org/"").get();\nElements ps = doc.select(""p"");\n\nThen write it out to a file in one more line\nout.write(ps.text());  //it will append all of the p elements together in one long string\n\nor if you want them on separate lines you can iterate through the elements and write them out separately. \n', '\njericho is one of several posible html parsers that could make this task both easy and safe.\n', '\nJTidy can represent an HTML document (even a malformed one) as a document model, making the process of extracting the contents of a <p> tag a rather more elegant process than manually thunking through the raw text.\n', '\nTry (if you don\'t want to use a HTML parser library):\n\n        FileReader fileReader = new FileReader(file);\n        BufferedReader buffRd = new BufferedReader(fileReader);\n        BufferedWriter out = new BufferedWriter(new FileWriter(newFile.txt));\n        String s;\n        int writeTo = 0;\n        while ((s = br.readLine()) !=null) \n        {\n                if(s.contains(""<p>""))\n                {\n                        writeTo = 1;\n\n                        try \n                        {\n                            out.write(s);\n                    } \n                        catch (IOException e) \n                        {\n\n                    }\n                }\n                if(s.contains(""</p>""))\n                {\n                        writeTo = 0;\n\n                        try \n                        {\n                            out.write(s);\n                    } \n                        catch (IOException e) \n                        {\n\n                    }\n                }\n                else if(writeTo==1)\n                {\n                        try \n                        {\n                            out.write(s);\n                    } \n                        catch (IOException e) \n                        {\n\n                    }\n                }\n}\n\n', ""\nI've had success using TagSoup & XPath to parse HTML.\nhttp://home.ccil.org/~cowan/XML/tagsoup/\n"", '\nUse a ParserCallback. Its a simple class thats included with the JDK. It notifies you every time a new tag is found and then you can extract the text of the tag. Simple example:\nimport java.io.*;\nimport java.net.*;\nimport javax.swing.text.*;\nimport javax.swing.text.html.*;\nimport javax.swing.text.html.parser.*;\n\npublic class ParserCallbackTest extends HTMLEditorKit.ParserCallback\n{\n    private int tabLevel = 1;\n    private int line = 1;\n\n    public void handleComment(char[] data, int pos)\n    {\n        displayData(new String(data));\n    }\n\n    public void handleEndOfLineString(String eol)\n    {\n        System.out.println( line++ );\n    }\n\n    public void handleEndTag(HTML.Tag tag, int pos)\n    {\n        tabLevel--;\n        displayData(""/"" + tag);\n    }\n\n    public void handleError(String errorMsg, int pos)\n    {\n        displayData(pos + "":"" + errorMsg);\n    }\n\n    public void handleMutableTag(HTML.Tag tag, MutableAttributeSet a, int pos)\n    {\n        displayData(""mutable:"" + tag + "": "" + pos + "": "" + a);\n    }\n\n    public void handleSimpleTag(HTML.Tag tag, MutableAttributeSet a, int pos)\n    {\n        displayData( tag + ""::"" + a );\n//      tabLevel++;\n    }\n\n    public void handleStartTag(HTML.Tag tag, MutableAttributeSet a, int pos)\n    {\n        displayData( tag + "":"" + a );\n        tabLevel++;\n    }\n\n    public void handleText(char[] data, int pos)\n    {\n        displayData( new String(data) );\n    }\n\n    private void displayData(String text)\n    {\n        for (int i = 0; i < tabLevel; i++)\n            System.out.print(""\\t"");\n\n        System.out.println(text);\n    }\n\n    public static void main(String[] args)\n    throws IOException\n    {\n        ParserCallbackTest parser = new ParserCallbackTest();\n\n        // args[0] is the file to parse\n\n        Reader reader = new FileReader(args[0]);\n//      URLConnection conn = new URL(args[0]).openConnection();\n//      Reader reader = new InputStreamReader(conn.getInputStream());\n\n        try\n        {\n            new ParserDelegator().parse(reader, parser, true);\n        }\n        catch (IOException e)\n        {\n            System.out.println(e);\n        }\n    }\n}\n\nSo all you need to do is set a boolean flag when the paragraph tag is found. Then in the handleText() method you extract the text.\n', '\nTry this.\n public static void main( String[] args )\n{\n    String url = ""http://en.wikipedia.org/wiki/Big_data"";\n\n    Document document;\n    try {\n        document = Jsoup.connect(url).get();\n        Elements paragraphs = document.select(""p"");\n\n        Element firstParagraph = paragraphs.first();\n        Element lastParagraph = paragraphs.last();\n        Element p;\n        int i=1;\n        p=firstParagraph;\n        System.out.println(""*  "" +p.text());\n        while (p!=lastParagraph){\n            p=paragraphs.get(i);\n            System.out.println(""*  "" +p.text());\n            i++;\n        } \n} catch (IOException e) {\n    // TODO Auto-generated catch block\n    e.printStackTrace();\n}\n}\n\n', '\nYou may just be using the wrong tool for the job:\nperl -ne ""print if m|<p>| .. m|</p>|"" infile.txt >outfile.txt\n\n']"
Screen Scraping a Javascript based webpage in Python,"
I am working on a screen scraping tool in Python. But, as I look through the source of the webpage, I noticed that most of the data is coming through Javascript. 
Any idea, how to scrape javascript based webpage ? Any tool  in Python ?
Thanks
",7k,"
            4
        ","['\nScraping javascript-based webpages is possible with selenium. In particular, try the Selenium WebDriver.\n', '\nI use webkit, which is the browser renderer behind Chrome and Safari. There are Python bindings to webkit through Qt. \nAnd here is a full Python example to execute JavaScript and extract the final HTML.\n', '\nYou can use the QtWebKit module of the PyQt4 library\n']"
Click on a javascript link within python?,"
I am navigating a site using python's mechanize module and having trouble clicking on a javascript link for next page.  I did a bit of reading and people suggested I need python-spidermonkey and DOMforms.  I managed to get them installed by I am not sure of the syntax to actually click on the link.
I can identify the code on the page as: 
<a href=""javascript:__doPostBack('ctl00$MainContent$gvSearchResults','Page$2')"">2</a>

Does anyone know how to click on it? or if perhaps there's another tool.
Thanks
",12k,"
            15
        ","['\nI mainly use HtmlUnit under jython for these use cases. Also I published a simple article on the subject: Web Scraping Ajax and Javascript sites.\n', ""\ninstead of struggling with python-spidermonkey try webkit's qt python bindings.\nHere is a full example to execute JavaScript and extract the final HTML.\n"", ""\nhow about calling __doPostBack('ctl00$MainContent$gvSearchResults','Page$'+pageid); (javascript method, via python-spidermonkey)\n""]"
How can I use Perl to grab text from a web page that is dynamically generated with JavaScript?,"
There is a website I am trying to pull information from in Perl, however the section of the page I need is being generated using javascript so all you see in the source is:
<div id=""results""></div>

I need to somehow pull out the contents of that div and save it to a file using Perl/proxies/whatever. e.g. the information I want to save would be
document.getElementById('results').innerHTML;

I am not sure if this is possible or if anyone had any ideas or a way to do this.
I was using a lynx source dump for other pages but since I cant straight forward screen scrape this page I came here to ask about it!
If anyone is interested, the page is http://downloadcenter.trendmicro.com/index.php?clk=left_nav&clkval=pattern_file&regs=NABU and the info I am trying to get is the row about the ConsumerOPR
",6k,"
            12
        ","[""\nYou'll need to reverse-engineer what the Javascript is doing. Does it fire off an AJAX request to populate the <div>? If so, it should be pretty easy to sniff the request using Firebug and then duplicate it with LWP::UserAgent or WWW::Mechanize to get the information.\nIf the Javascript is just doing pure DOM manipulation, then that means the data must exist somewhere else in the page or the Javascript already. So figure out where it's coming from and grab it.\nFinally, if none of those options are adequate, you may need to just use a real browser to do it. There are a few options for automating browser behavior, like WWW::Mechanize::Firefox or Win32::IE::Mechanize.\n"", '\nBringing the Browser to the Server by John Resig might be useful.\n', ""\nAs the content of your page is generated by some Javascript, you need the ability to :\n\nExecute some Javascript code\n\n\nEven, possibly, some complex JS code, doing Ajax requests and all that ?\n\nAnd do it with an engine that supports the functions/methods that are present in a browser (like DOM manipulations)\n\n\nA solution could be to actually really start a browser to navigate to that page, and, then, parse the page that's loaded by it, to extract the information ?\nI've never used this for grabbing, but the Selenium suite might help, here : using Selenium RC, you can start a real browser, and pilot it -- then, you have functions to get data from it.\nIt's not quite fast, and it's pretty heavy (it has to start a browser !), but it works quite well : you'll be using Firefox, for example, to navigate to your page -- which means a real Javascript engine, that's used every day by a lot of people ;-)\n"", ""\nThis might be what your looking for (in PHP):\n$url = 'http://downloadcenter.trendmicro.com/ajx/pattern_result.php';\n\n$ch = curl_init();\ncurl_setopt ($ch, CURLOPT_SSL_VERIFYPEER, FALSE);\ncurl_setopt ($ch, CURLOPT_URL, $url);\ncurl_setopt ($ch, CURLOPT_POST, 1);\ncurl_setopt ($ch, CURLOPT_POSTFIELDS, 'q=patresult_page&reg=NABU');\ncurl_setopt ($ch, CURLOPT_RETURNTRANSFER, 1);\n$content = curl_exec($ch);\ncurl_close($ch);\n\necho $content;\nexit;\n\nonce you get the content you can use something like: http://code.google.com/p/phpquery/ to parse the results you need or a similar perl equivalent??? \nAnd/or do the parsing yourself.\nFYI: all I did was use firebug to inspect the requests and recreated it with PHP/CURL...\n"", '\nto work with the dynamically created HTML you can use the FireFox Chickenfoot plugin.\nOr if you need something that works from a command line script use bindings to Perl. I have done this with Python before.\n']"
How can i grab CData out of BeautifulSoup,"
I have a website that I'm scraping that has a similar structure the following. I'd like to be able to grab the info out of the CData block. 
I'm using BeautifulSoup to pull other info off the page, so if the solution can work with that, it would help keep my learning curve down as I'm a python novice.
Specifically, I want to get at the two different types of data hidden in the CData statement. the first which is just text I'm pretty sure I can throw a regex at it and get what I need. For the second type, if i could drop the data that has html elements into it's own beautifulsoup, I can parse that. 
I'm just learning python and beautifulsoup, so I'm struggling to find the magical incantation that will give me just the CData by itself.
<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN""   ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">
<html xmlns=""http://www.w3.org/1999/xhtml"">  
<head>  
<title>
   Cows and Sheep
  </title>
</head>
<body>
 <div id=""main"">
  <div id=""main-precontents"">
   <div id=""main-contents"" class=""main-contents"">
    <script type=""text/javascript"">
       //<![CDATA[var _ = g_cow;_[7654]={cowname_enus:'cows rule!',leather_quality:99,icon:'cow_level_23'};_[37357]={sheepname_enus:'baa breath',wool_quality:75,icon:'sheep_level_23'};_[39654].cowmeat_enus = '<table><tr><td><b class=""q4"">cows rule!</b><br></br>
       <!--ts-->
       get it now<table width=""100%""><tr><td>NOW</td><th>NOW</th></tr></table><span>244 Cows</span><br></br>67 leather<br></br>68 Brains
       <!--yy-->
       <span class=""q0"">Cow Bonus: +9 Cow Power</span><br></br>Sheep Power 60 / 60<br></br>Sheep 88<br></br>Cow Level 555</td></tr></table>
       <!--?5695:5:40:45-->
       ';
        //]]>
      </script>
     </div>
     </div>
    </div>
 </body>
</html>

",15k,"
            12
        ","['\nOne thing you need to be careful of BeautifulSoup grabbing CData is not to use a lxml parser.\nBy default, the lxml parser will strip CDATA sections from the tree and replace them by their plain text content, Learn more here\n#Trying it with html.parser\n\n\n>>> from bs4 import BeautifulSoup\n>>> import bs4\n>>> s=\'\'\'<?xml version=""1.0"" ?>\n<foo>\n    <bar><![CDATA[\n        aaaaaaaaaaaaa\n    ]]></bar>\n</foo>\'\'\'\n>>> soup = BeautifulSoup(s, ""html.parser"")\n>>> soup.find(text=lambda tag: isinstance(tag, bs4.CData)).string.strip()\n\'aaaaaaaaaaaaa\'\n>>> \n\n', '\nBeautifulSoup sees CData as a special case (subclass) of ""navigable strings"". So for example:\nimport BeautifulSoup\n\ntxt = \'\'\'<foobar>We have\n       <![CDATA[some data here]]>\n       and more.\n       </foobar>\'\'\'\n\nsoup = BeautifulSoup.BeautifulSoup(txt)\nfor cd in soup.findAll(text=True):\n  if isinstance(cd, BeautifulSoup.CData):\n    print \'CData contents: %r\' % cd\n\nIn your case of course you could look in the subtree starting at the div with the \'main-contents\' ID, rather than all over the document tree.\n', '\nYou could try this:\nfrom BeautifulSoup import BeautifulSoup\n\n// source.html contains your html above\nf = open(\'source.html\')\nsoup = BeautifulSoup(\'\'.join(f.readlines()))\ns = soup.findAll(\'script\')\ncdata = s[0].contents[0]\n\nThat should give you the contents of cdata.\nUpdate\nThis may be a little cleaner:\nfrom BeautifulSoup import BeautifulSoup\nimport re\n\n// source.html contains your html above\nf = open(\'source.html\')\nsoup = BeautifulSoup(\'\'.join(f.readlines()))\ncdata = soup.find(text=re.compile(""CDATA""))\n\nJust personal preference, but I like the bottom one a little better.\n', ""\nimport re\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(content)\nfor x in soup.find_all('item'):\n    print re.sub('[\\[CDATA\\]]', '', x.string)\n\n"", ""\nFor anyone using BeautifulSoup4, Alex Martelli's solution works but do this:\nfrom bs4 import BeautifulSoup, CData\n\nsoup = BeautifulSoup(txt)\nfor cd in soup.findAll(text=True):\n  if isinstance(cd, Cdata):\n    print 'CData contents: %r' % cd\n\n""]"
php - Fastest way to check presence of text in many domains (above 1000),"
I have a php script running and using cURL to retrieve the content of webpages on which I would like to check for the presence of some text.
Right now it looks like this:
for( $i = 0; $i < $num_target; $i++ ) {
    $ch = curl_init();
    $timeout = 10;
    curl_setopt ($ch, CURLOPT_URL,$target[$i]);
    curl_setopt ($ch, CURLOPT_RETURNTRANSFER, true);
    curl_setopt ($ch, CURLOPT_FORBID_REUSE, true);
    curl_setopt ($ch, CURLOPT_CONNECTTIMEOUT, $timeout);
    $url = curl_exec ($ch);
    curl_close($ch);

    if (preg_match($text,$url,$match)) {
        $match[$i] = $match;
        echo ""text"" . $text . "" found in URL: "" . $url . "": "" . $match .;

        } else {
        $match[$i] = $match;
        echo ""text"" . $text . "" not found in URL: "" . $url . "": no match"";
        }
}

I was wondering if I could use a special cURL setup that makes it faster ( I looked in the php manual chose the options that seemed the best to me but I may have neglected some that could increase the speed and performance of the script).
I was then wondering if using cgi, Perl or python (or another solution) could be faster than php.
Thank you in advance for any help / advice / suggestion.
",952,"
            -2
        ","['\nYou can use curl_multi_init .... which Allows the processing of multiple cURL handles in parallel.\nExample \n$url = array();\n$url[] = \'http://www.huffingtonpost.com\';\n$url[] = \'http://www.yahoo.com\';\n$url[] = \'http://www.google.com\';\n$url[] = \'http://technet.microsoft.com/en-us/\';\n\n$start = microtime(true);\necho ""<pre>"";\nprint_r(checkLinks($url, ""Azure""));\necho ""<h1>"", microtime(true) - $start, ""</h1>"";\n\nOutput\nArray\n(\n    [0] => http://technet.microsoft.com/en-us/\n)\n\n1.2735739707947 <-- Faster\n\nFunction Used\nfunction checkLinks($nodes, $text) {\n    $mh = curl_multi_init();\n    $curl_array = array();\n    foreach ( $nodes as $i => $url ) {\n        $curl_array[$i] = curl_init($url);\n        curl_setopt($curl_array[$i], CURLOPT_RETURNTRANSFER, true);\n        curl_setopt($curl_array[$i], CURLOPT_USERAGENT, \'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.1.2) Gecko/20090729 Firefox/3.5.2 (.NET CLR 3.5.30729)\');\n        curl_setopt($curl_array[$i], CURLOPT_CONNECTTIMEOUT, 5);\n        curl_setopt($curl_array[$i], CURLOPT_TIMEOUT, 15);\n        curl_multi_add_handle($mh, $curl_array[$i]);\n    }\n    $running = NULL;\n    do {\n        usleep(10000);\n        curl_multi_exec($mh, $running);\n    } while ( $running > 0 );\n    $res = array();\n    foreach ( $nodes as $i => $url ) {\n        $curlErrorCode = curl_errno($curl_array[$i]);\n        if ($curlErrorCode === 0) {\n            $info = curl_getinfo($curl_array[$i]);\n            if ($info[\'http_code\'] == 200) {\n                if (stripos(curl_multi_getcontent($curl_array[$i]), $text) !== false) {\n                    $res[] = $info[\'url\'];\n                }\n            }\n        }\n        curl_multi_remove_handle($mh, $curl_array[$i]);\n        curl_close($curl_array[$i]);\n    }\n    curl_multi_close($mh);\n    return $res;\n}\n\n']"
What's the best way of scraping data from a website? [closed],"






Closed. This question is opinion-based. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 8 years ago.







                        Improve this question
                    



I need to extract contents from a website, but the application doesn’t provide any application programming interface or another mechanism to access that data programmatically.
I found a useful third-party tool called Import.io that provides click and go functionality for scraping web pages and building data sets, the only thing is I want to keep my data locally and I don't want to subscribe to any subscription plans.
What kind of technique does this company use for scraping the web pages and building their datasets? I found some web scraping frameworks pjscrape & Scrapy could they provide such a feature
",167k,"
            114
        ","['\nYou will definitely want to start with a good web scraping framework. Later on you may decide that they are too limiting and you can put together your own stack of libraries but without a lot of scraping experience your design will be much worse than pjscrape or scrapy.\nNote: I use the terms crawling and scraping basically interchangeable here. This is a copy of my answer to your Quora question, it\'s pretty long.\nTools\nGet very familiar with either Firebug or Chrome dev tools depending on your preferred browser. This will be absolutely necessary as you browse the site you are pulling data from and map out which urls contain the data you are looking for and what data formats make up the responses.\nYou will need a good working knowledge of HTTP as well as HTML and will probably want to find a decent piece of man in the middle proxy software. You will need to be able to inspect HTTP requests and responses and understand how the cookies and session information and query parameters are being passed around. Fiddler (http://www.telerik.com/fiddler) and Charles Proxy (http://www.charlesproxy.com/) are popular tools. I use mitmproxy (http://mitmproxy.org/) a lot as I\'m more of a keyboard guy than a mouse guy.\nSome kind of console/shell/REPL type environment where you can try out various pieces of code with instant feedback will be invaluable. Reverse engineering tasks like this are a lot of trial and error so you will want a workflow that makes this easy.\nLanguage\nPHP is basically out, it\'s not well suited for this task and the library/framework support is poor in this area. Python (Scrapy is a great starting point) and Clojure/Clojurescript (incredibly powerful and productive but a big learning curve) are great languages for this problem. Since you would rather not learn a new language and you already know Javascript I would definitely suggest sticking with JS. I have not used pjscrape but it looks quite good from a quick read of their docs. It\'s well suited and implements an excellent solution to the problem I describe below.\nA note on Regular expressions:\nDO NOT USE REGULAR EXPRESSIONS TO PARSE HTML.\nA lot of beginners do this because they are already familiar with regexes. It\'s a huge mistake, use xpath or css selectors to navigate html and only use regular expressions to extract data from actual text inside an html node. This might already be obvious to you, it becomes obvious quickly if you try it but a lot of people waste a lot of time going down this road for some reason. Don\'t be scared of xpath or css selectors, they are WAY easier to learn than regexes and they were designed to solve this exact problem.\nJavascript-heavy sites\nIn the old days you just had to make an http request and parse the HTML reponse. Now you will almost certainly have to deal with sites that are a mix of standard HTML HTTP request/responses and asynchronous HTTP calls made by the javascript portion of the target site. This is where your proxy software and the network tab of firebug/devtools comes in very handy. The responses to these might be html or they might be json, in rare cases they will be xml or something else.\nThere are two approaches to this problem:\nThe low level approach:\nYou can figure out what ajax urls the site javascript is calling and what those responses look like and make those same requests yourself. So you might pull the html from http://example.com/foobar and extract one piece of data and then have to pull the json response from http://example.com/api/baz?foo=b... to get the other piece of data. You\'ll need to be aware of passing the correct cookies or session parameters. It\'s very rare, but occasionally some required parameters for an ajax call will be the result of some crazy calculation done in the site\'s javascript, reverse engineering this can be annoying.\nThe embedded browser approach:\nWhy do you need to work out what data is in html and what data comes in from an ajax call? Managing all that session and cookie data? You don\'t have to when you browse a site, the browser and the site javascript do that. That\'s the whole point.\nIf you just load the page into a headless browser engine like phantomjs it will load the page, run the javascript and tell you when all the ajax calls have completed. You can inject your own javascript if necessary to trigger the appropriate clicks or whatever is necessary to trigger the site javascript to load the appropriate data.\nYou now have two options, get it to spit out the finished html and parse it or inject some javascript into the page that does your parsing and data formatting and spits the data out (probably in json format). You can freely mix these two options as well.\nWhich approach is best?\nThat depends, you will need to be familiar and comfortable with the low level approach for sure. The embedded browser approach works for anything, it will be much easier to implement and will make some of the trickiest problems in scraping disappear. It\'s also quite a complex piece of machinery that you will need to understand. It\'s not just HTTP requests and responses, it\'s requests, embedded browser rendering, site javascript, injected javascript, your own code and 2-way interaction with the embedded browser process.\nThe embedded browser is also much slower at scale because of the rendering overhead but that will almost certainly not matter unless you are scraping a lot of different domains. Your need to rate limit your requests will make the rendering time completely negligible in the case of a single domain.\nRate Limiting/Bot behaviour\nYou need to be very aware of this. You need to make requests to your target domains at a reasonable rate. You need to write a well behaved bot when crawling websites, and that means respecting robots.txt and not hammering the server with requests. Mistakes or negligence here is very unethical since this can be considered a denial of service attack. The acceptable rate varies depending on who you ask, 1req/s is the max that the Google crawler runs at but you are not Google and you probably aren\'t as welcome as Google. Keep it as slow as reasonable. I would suggest 2-5 seconds between each page request.\nIdentify your requests with a user agent string that identifies your bot and have a webpage for your bot explaining it\'s purpose. This url goes in the agent string.\nYou will be easy to block if the site wants to block you. A smart engineer on their end can easily identify bots and a few minutes of work on their end can cause weeks of work changing your scraping code on your end or just make it impossible. If the relationship is antagonistic then a smart engineer at the target site can completely stymie a genius engineer writing a crawler. Scraping code is inherently fragile and this is easily exploited. Something that would provoke this response is almost certainly unethical anyway, so write a well behaved bot and don\'t worry about this.\nTesting\nNot a unit/integration test person? Too bad. You will now have to become one. Sites change frequently and you will be changing your code frequently. This is a large part of the challenge.\nThere are a lot of moving parts involved in scraping a modern website, good test practices will help a lot. Many of the bugs you will encounter while writing this type of code will be the type that just return corrupted data silently. Without good tests to check for regressions you will find out that you\'ve been saving useless corrupted data to your database for a while without noticing. This project will make you very familiar with data validation (find some good libraries to use) and testing. There are not many other problems that combine requiring comprehensive tests and being very difficult to test.\nThe second part of your tests involve caching and change detection. While writing your code you don\'t want to be hammering the server for the same page over and over again for no reason. While running your unit tests you want to know if your tests are failing because you broke your code or because the website has been redesigned. Run your unit tests against a cached copy of the urls involved. A caching proxy is very useful here but tricky to configure and use properly.\nYou also do want to know if the site has changed. If they redesigned the site and your crawler is broken your unit tests will still pass because they are running against a cached copy! You will need either another, smaller set of integration tests that are run infrequently against the live site or good logging and error detection in your crawling code that logs the exact issues, alerts you to the problem and stops crawling. Now you can update your cache, run your unit tests and see what you need to change.\nLegal Issues\nThe law here can be slightly dangerous if you do stupid things. If the law gets involved you are dealing with people who regularly refer to wget and curl as ""hacking tools"". You don\'t want this.\nThe ethical reality of the situation is that there is no difference between using browser software to request a url and look at some data and using your own software to request a url and look at some data. Google is the largest scraping company in the world and they are loved for it. Identifying your bots name in the user agent and being open about the goals and intentions of your web crawler will help here as the law understands what Google is. If you are doing anything shady, like creating fake user accounts or accessing areas of the site that you shouldn\'t (either ""blocked"" by robots.txt or because of some kind of authorization exploit) then be aware that you are doing something unethical and the law\'s ignorance of technology will be extraordinarily dangerous here. It\'s a ridiculous situation but it\'s a real one.\nIt\'s literally possible to try and build a new search engine on the up and up as an upstanding citizen, make a mistake or have a bug in your software and be seen as a hacker. Not something you want considering the current political reality.\nWho am I to write this giant wall of text anyway?\nI\'ve written a lot of web crawling related code in my life. I\'ve been doing web related software development for more than a decade as a consultant, employee and startup founder. The early days were writing perl crawlers/scrapers and php websites. When we were embedding hidden iframes loading csv data into webpages to do ajax before Jesse James Garrett named it ajax, before XMLHTTPRequest was an idea. Before jQuery, before json. I\'m in my mid-30\'s, that\'s apparently considered ancient for this business.\nI\'ve written large scale crawling/scraping systems twice, once for a large team at a media company (in Perl) and recently for a small team as the CTO of a search engine startup (in Python/Javascript). I currently work as a consultant, mostly coding in Clojure/Clojurescript (a wonderful expert language in general and has libraries that make crawler/scraper problems a delight)\nI\'ve written successful anti-crawling software systems as well. It\'s remarkably easy to write nigh-unscrapable sites if you want to or to identify and sabotage bots you don\'t like.\nI like writing crawlers, scrapers and parsers more than any other type of software. It\'s challenging, fun and can be used to create amazing things.\n', '\nYes you can do it yourself. It is just a matter of grabbing the sources of the page and parsing them the way you want. \nThere are various possibilities. A good combo is using python-requests (built on top of urllib2, it is urllib.request in Python3) and BeautifulSoup4, which has its methods to select elements and also permits CSS selectors:\nimport requests\nfrom BeautifulSoup4 import BeautifulSoup as bs\nrequest = requests.get(""http://foo.bar"")\nsoup = bs(request.text) \nsome_elements = soup.find_all(""div"", class_=""myCssClass"")\n\nSome will prefer xpath parsing or jquery-like pyquery, lxml or something else.\nWhen the data you want is produced by some JavaScript, the above won\'t work. You either need python-ghost or Selenium. I prefer the latter combined with PhantomJS, much lighter and simpler to install, and easy to use:\nfrom selenium import webdriver\nclient = webdriver.PhantomJS()\nclient.get(""http://foo"")\nsoup = bs(client.page_source)\n\nI would advice to start your own solution. You\'ll understand Scrapy\'s benefits doing so.\nps: take a look at scrapely: https://github.com/scrapy/scrapely\npps: take a look at Portia, to start extracting information visually, without programming knowledge: https://github.com/scrapinghub/portia \n']"
"Headless, scriptable Firefox/Webkit on linux? [closed]","






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I'm looking to automate some web interactions, namely periodic download of files from a secure website. This basically involves entering my username/password and navigating to the appropriate URL.
I tried simple scripting in Python, followed by more sophisticated scripting, only to discover this particular website is using some obnoxious javascript and flash based mechanism for login, rendering my methods useless. 
I then tried HTMLUnit, but that doesn't seem to want to work either. I suspect use of Flash is the issue.
I don't really want to think about it any more, so I'm leaning towards scripting an actual browser to log in and grab the file I need. 
Requirements are:

Run on linux server (ie. no X running). If I really need to have X I can make that happen, but I won't be happy.
Be reliable. I want to start this thing and never think about it again.
Be scriptable. Nothing too sophisticated, but I should be able to tell the browser the various steps to take and pages to visit.

Are there any good toolkits for a headless, X-less scriptable browser? Have you tried something like this and if so do you have any words of wisdom?
",22k,"
            46
        ","['\nWhat about phantomjs?  \n', '\nI did related task with IE embedded browser (although it was gui application with hidden browser component panel). Actually you can take any layout engine and cut output logic. Navigation is should be done via firing script-like events.\nYou can use Crowbar. It is headless version of firefox (Gecko engine). It turns browser into RESTful server that can accept requests (""fetch  url""). So it parse html, represent it as DOM, wait defined delay for all script performed. \nIt works on linux. I suppose you can easily extend it for your goal using JS and rich XULrunner abilities.\n', '\nHave you tried Selenium? It will allow you to record a usage scenario, using an extension for Firefox, which can later be played back using a number of different methods.\nEdit: I just realized this was a very late response. :)\n', '\nHave a look at WebKitDriver. The project includes headless implementation of WebKit.\n', ""\nI don't know how to do flash interactions (and am also interested), but for html/javascript you can use Chickenfoot. \nAnd to get a headless + scriptable browser working on Linux you can use the Qt webkit library. Here is an example use.\n"", ""\nTo accomplish this, I just write Chrome extensions that post to CouchDBs (example and its Futon). Add the Couch to the permissions in the manifest to allow cross-domain XHRs.\n(I arrived at this thread in search of a headless alternative to what I've been doing; having found this thread, I'm going to try Crowbar at some point.)\nAlso, considering the bizarre characteristics of this website, I can't help wondering whether you can exploit some security hole to get around the Flash and Javascript.\n""]"
Scrapy Python Set up User Agent,"
I tried to override the user-agent of my crawlspider by adding an extra line to the project configuration file. Here is the code:
[settings]
default = myproject.settings
USER_AGENT = ""Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36""


[deploy]
#url = http://localhost:6800/
project = myproject

But when I run the crawler against my own web, I notice the spider did not pick up my customized user agent but the default one ""Scrapy/0.18.2 (+http://scrapy.org)"". 
Can any one explain what I have done wrong. 
Note:
(1). It works when I tried to override the user agent globally: 
scrapy crawl myproject.com -o output.csv -t csv -s USER_AGENT=""Mozilla....""

(2). When I remove the line ""default = myproject.setting"" from the configuration file, and run scrapy crawl myproject.com, it says ""cannot find spider.."", so I feel like the default setting should not be removed in this case.
Thanks a lot for the help in advance.                            
",53k,"
            40
        ","['\nMove your USER_AGENT line to the settings.py file, and not in your scrapy.cfg file. settings.py should be at same level as items.py if you use scrapy startproject command, in your case  it should be something like myproject/settings.py\n', ""\nJust in case anyone lands here that manually controls the scrapy crawl. i.e. you do not use the scrapy crawl process from the shell...\n$ scrapy crawl myproject\n\nBut insted you use CrawlerProcess() or CrawlerRunner()...\nprocess = CrawlerProcess()\n\nor \nprocess = CrawlerRunner()\n\nthen the user agent, along with other settings, can be passed to the crawler in a dictionary of configuration variables. \nLike this...\n    process = CrawlerProcess(\n            {\n                'USER_AGENT': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n            }\n    )\n\n""]"
How to fetch HTML in Java,"
Without the use of any external library, what is the simplest way to fetch a website's HTML content into a String?
",72k,"
            33
        ","['\nI\'m currently using this:\nString content = null;\nURLConnection connection = null;\ntry {\n  connection =  new URL(""http://www.google.com"").openConnection();\n  Scanner scanner = new Scanner(connection.getInputStream());\n  scanner.useDelimiter(""\\\\Z"");\n  content = scanner.next();\n  scanner.close();\n}catch ( Exception ex ) {\n    ex.printStackTrace();\n}\nSystem.out.println(content);\n\nBut not sure if there\'s a better way.\n', '\nThis has worked well for me:\nURL url = new URL(theURL);\nInputStream is = url.openStream();\nint ptr = 0;\nStringBuffer buffer = new StringBuffer();\nwhile ((ptr = is.read()) != -1) {\n    buffer.append((char)ptr);\n}\n\nNot sure at to whether the other solution(s) provided are any more efficient or not.\n', ""\nI just left this post in your other thread, though what you have above might work as well.  I don't think either would be any easier than the other.  The Apache packages can be accessed by just using import org.apache.commons.HttpClient at the top of your code.\nEdit: Forgot the link ;)\n"", '\nWhilst not vanilla-Java, I\'ll offer up a simpler solution. Use Groovy ;-)\nString siteContent = new URL(""http://www.google.com"").text\n\n', '\nIts not library but a tool named curl generally installed in most of the servers or you can easily install in ubuntu by \nsudo apt install curl\n\nThen fetch any html page and store it to your local file like an example \ncurl https://www.facebook.com/ > fb.html\n\nYou will get the home page html.You can run it in your browser as well.\n']"
Scraping Real Time Visitors from Google Analytics,"
I have a lot of sites and want to build a dashboard showing the number of real time visitors on each of them on a single page. (would anyone else want this?) Right now the only way to view this information is to open a new tab for each site.
Google doesn't have a real-time API, so I'm wondering if it is possible to scrape this data. Eduardo Cereto found out that Google transfers the real-time data over the realtime/bind network request. Anyone more savvy have an idea of how I should start? Here's what I'm thinking:

Figure out how to authenticate programmatically

Inspect all of the realtime/bind requests to see how they change. Does each request have a unique key? Where does that come from? Below is my breakdown of the request:
https://www.google.com/analytics/realtime/bind?VER=8
&key= [What is this? Where does it come from? 21 character lowercase alphanumeric, stays the same each request]
&ds= [What is this? Where does it come from? 21 character lowercase alphanumeric, stays the same each request]
&pageId=rt-standard%2Frt-overview
&q=t%3A0%7C%3A1%3A0%3A%2Ct%3A11%7C%3A1%3A5%3A%2Cot%3A0%3A0%3A4%2Cot%3A0%3A0%3A3%2Ct%3A7%7C%3A1%3A10%3A6%3D%3DREFERRAL%3B%2Ct%3A10%7C%3A1%3A10%3A%2Ct%3A18%7C%3A1%3A10%3A%2Ct%3A4%7C5%7C2%7C%3A1%3A10%3A2!%3Dzz%3B%2C&f
The q variable URI decodes to this (what the?):
t:0|:1:0:,t:11|:1:5:,ot:0:0:4,ot:0:0:3,t:7|:1:10:6==REFERRAL;,t:10|:1:10:,t:18|:1:10:,t:4|5|2|:1:10:2!=zz;,&f
&RID=rpc
&SID= [What is this? Where does it come from? 16 character uppercase alphanumeric, stays the same each request]
&CI=0
&AID= [What is this? Where does it come from? integer, starts at 1, increments weirdly to 150 and then 298]
&TYPE=xmlhttp
&zx= [What is this? Where does it come from? 12 character lowercase alphanumeric, changes each request]
&t=1

Inspect all of the realtime/bind responses to see how they change. How does the data come in? It looks like some altered JSON. How many times do I need to connect to get the data? Where is the active visitors on site number in there? Here is a dump of sample data:


19
[[151,[""noop""]
]
]
388
[[152,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[49,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[0,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3,2,0],""name"":""Total""}]}}]]]
]
388
[[153,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[52,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[2,1,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3,2],""name"":""Total""}]}}]]]
]
388
[[154,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[53,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[0,3,1,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3],""name"":""Total""}]}}]]]
]

Let me know if you can help with any of the items above!

",18k,"
            27
        ","['\nTo get the same, Google has  launched new Real Time API. With this API you can easily retrieve real time online visitors as well as several Google Analytics with following dimensions and metrics. https://developers.google.com/analytics/devguides/reporting/realtime/dimsmets/\nThis is quite similar to Google Analytics API. To start development on this, \nhttps://developers.google.com/analytics/devguides/reporting/realtime/v3/devguide \n', '\nWith Google Chrome I can see the data on the Network Panel.\nThe request endpoint is https://www.google.com/analytics/realtime/bind\nSeems like the connection stays open for 2.5 minutes, and during this time it just keeps getting more and more data. \nAfter about 2.5 minutes the connection is closed and a new one is open.\nOn the Network panel you can only see the data for the connections that are terminated. So leave it open for 5 minutes or so and you can start to see the data.\nI hope that can give you a place to start.\n', '\nHaving google in the loop seems pretty redundant. Suggest you use a common element delivered on demand from the dashboard server and include this item by absolute URL on all pages to be monitored for a given site. The script outputting the item can read the IP of the browser asking and these can all be logged into a database and filtered for uniqueness giving a real time head count.\n<?php\n$user_ip = $_SERVER[""REMOTE_ADDR""];\n/// Some MySQL to insert $user_ip to the database table for website XXX  goes here\n\n\n$file = \'tracking_image.gif\';\n$type = \'image/gif\';\nheader(\'Content-Type:\'.$type);\nheader(\'Content-Length: \' . filesize($file));\nreadfile($file);\n?>\n\nAmmendum:\nA database can also add a timestamp to every row of data it stores. This can be used to further filter results and provide the number of visitors in the last hour or minute. \nClient side Javascript with AJAX for fine tuning or overkill\nThe onblur and onfocus javascript commands  can be used to tell if the the page is visible, pass the data back to the dashboard server via Ajax. http://www.thefutureoftheweb.com/demo/2007-05-16-detect-browser-window-focus/\nWhen a visitor closes a page this can also be detected by the javascript onunload function in the body tag and Ajax can be used to send data back to the server one last time before the browser finally closes the page.\nAs you may also wish to collect some information about the visitor like Google analytics does this page https://panopticlick.eff.org/ has a lot of javascript that can be examined and adapted.\n', '\nI needed/wanted realtime data for personal use so I reverse-engineered their system a little bit.\nInstead of binding to /bind I get data from /getData (no pun intended).\nAt /getData the minimum request is apparently: https://www.google.com/analytics/realtime/realtime/getData?pageId&key={{propertyID}}&q=t:0|:1\nHere\'s a short explanation of the possible query parameters and syntax, please remember that these are all guesses and I don\'t know all of them:\nQuery Syntax: pageId&key=propertyID&q=dataType:dimensions|:page|:limit:filters\nValues: \npageID: Required but seems to only be used for internal analytics.\n\npropertyID: a{{accountID}}w{{webPropertyID}}p{{profileID}}, as specified at the Documentation link below. You can also find this in the URL of all analytics pages in the UI.\n\n\ndataType:\n    t: Current data\n    ot: Overtime/Past\n    c: Unknown, returns only a ""count"" value\n\n\ndimensions (| separated or alone), most values are only applicable for t:\n    1:  Country\n    2:  City\n    3:  Location code?\n    4:  Latitude\n    5:  Longitude\n    6:  Traffic source type (Social, Referral, etc.)\n    7:  Source\n    8:  ?? Returns (not set)\n    9:  Another location code? longer.\n    10: Page URL\n    11: Visitor Type (new/returning)\n    12: ?? Returns (not set)\n    13: ?? Returns (not set)\n    14: Medium\n    15: ?? Returns ""1""\n\npage:\n    At first this seems to work for pagination but after further analysis it looks like it\'s also used to specify which of the 6 pages (Overview, Locations, Traffic Sources, Content, Events and Conversions) to return data for.\n\n    For some reason 0 returns an impossibly high metrictotal\n\nlimit: Result limit per page, maximum of 50\n\nfilters:\n    Syntax is as specified at the Documentation 2 link below except the OR is specified using | instead of a comma.6==CUSTOM;1==United%20States\n\n\nYou can also combine multiple queries in one request by comma separating them (i.e. q=t:1|2|:1|:10,t:6|:1|:10).\nFollowing the above ""documentation"", if you wanted to build a query that requests the page URL and city of the top 10 active visitors with a traffic source type of CUSTOM located in the US you would use this URL: https://www.google.com/analytics/realtime/realtime/getData?key={{propertyID}}&pageId&q=t:10|2|:1|:10:6==CUSTOM;1==United%20States\n\nDocumentation\nDocumentation 2\n\nI hope that my answer is readable and (although it\'s a little late) sufficiently answers your question and helps others in the future.\n']"
Is there a simple way in R to extract only the text elements of an HTML page?,"
Is there a simple way in R to extract only the text elements of an HTML page?
I think this is known as 'screen scraping' but I have no experience of it, I just need a simple way of extracting the text you'd normally see in a browser when visiting a url.
",27k,"
            26
        ","['\nI had to do this once upon time myself. \nOne way of doing it is to make use of XPath expressions. You will need these packages installed from the repository at http://www.omegahat.org/\nlibrary(RCurl)\nlibrary(RTidyHTML)\nlibrary(XML)\n\nWe use RCurl to connect to the website of interest. It has lots of options which allow you to access websites that the default functions in base R would have difficulty with I think it\'s fair to say. It is an R-interface to the libcurl library.\nWe use RTidyHTML to clean up malformed HTML web pages so that they are easier to parse. It is an R-interface to the libtidy library.\nWe use XML to parse the HTML code with our XPath expressions. It is an R-interface to the libxml2 library.\nAnyways, here\'s what you do (minimal code, but options are available, see help pages of corresponding functions):\nu <- ""http://stackoverflow.com/questions/tagged?tagnames=r"" \ndoc.raw <- getURL(u)\ndoc <- tidyHTML(doc.raw)\nhtml <- htmlTreeParse(doc, useInternal = TRUE)\ntxt <- xpathApply(html, ""//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]"", xmlValue)\ncat(unlist(txt))\n\nThere may be some problems with this approach, but I can\'t remember what they are off the top of my head (I don\'t think my xpath expression works with all web pages, sometimes it might not filter out script code or it may plain just not work with some other pages at all, best to experiment!)\nP.S. Another way, which works almost perfectly I think at web scraping all text from html is the following (basically getting Internet Explorer to do the conversion for you):\nlibrary(RDCOMClient) \nu <- ""http://stackoverflow.com/questions/tagged?tagnames=r""\nie <- COMCreate(""InternetExplorer.Application"") \nie$Navigate(u)\ntxt <- list()\ntxt[[u]] <- ie[[""document""]][[""body""]][[""innerText""]] \nie$Quit() \nprint(txt) \n\nHOWEVER, I\'ve never liked doing this because not only is it slow, but if you vectorise it and apply a vector of URLs, if internet explorer crashes on a bad page, then R might hang or crash itself (I don\'t think ?try helps that much in this case). Also it\'s prone to allowing pop-ups. I don\'t know, it\'s been a while since I\'ve done this, but thought I should point this out.\n', ""\nThe best solution is package htm2txt.\nlibrary(htm2txt)\nurl <- 'https://en.wikipedia.org/wiki/Alan_Turing'\ntext <- gettxt(url)\n\nFor details, see https://CRAN.R-project.org/package=htm2txt.\n"", '\nWell it´s not exactly a R way of doing it, but it´s as simple as they come: outwit plugin for firefox. The basic version is for free and helps to extract tables and stuff. \nah and if you really wanna do it the hard way in R, this link is for you:\n', ""\nI've had good luck with the readHTMLTable() function of the XML package. It returns a list of all tables on the page.\nlibrary(XML)\nurl <- 'http://en.wikipedia.org/wiki/World_population'\nallTables <- readHTMLTable(url)\n\nThere can be many tables on each page.\nlength(allTables)\n# [1] 17\n\nSo just select the one you want.\ntbl <- allTables[[3]]\n\nThe biggest hassle can be installing the XML package. It's big, and it needs the libxml2 library (and, under Linux, it needs the xml2-config Debian package, too). The second biggest hassle is that HTML tables often contain junk you don't want, besides the data you do want.\n"", ""\nYou can also use the rvest package and first, select all html nodes/tags containing text (e.g. p, h1, h2, h3) and then extract the text from those:\nrequire(rvest)\nurl = 'https://en.wikipedia.org/wiki/Alan_Turing'\nsite = read_html(url)\ntext = html_text(html_nodes(site, 'p,h1,h2,h3')) # comma separate\n\n"", '\nHere is another approach that can be used :\nlibrary(pagedown)\nlibrary(pdftools)\nchrome_print(input = ""http://stackoverflow.com/questions/tagged?tagnames=r"", \n             output = ""C:/.../test.pdf"")\ntext <- pdf_text(""C:/.../test.pdf"")\n\nIt is also possible to use RSelenium :\nlibrary(RSelenium)\nshell(\'docker run -d -p 4445:4444 selenium/standalone-firefox\')\nremDr <- remoteDriver(remoteServerAddr = ""localhost"", port = 4445L, browserName = ""firefox"")\nremDr$open()\nremDr$navigate(""http://stackoverflow.com/questions/tagged?tagnames=r"")\nremDr$getPageSource()[[1]]\n\n']"
Programmatic Python Browser with JavaScript,"
I want to screen-scrape a web-site that uses JavaScript. 
There is mechanize, the programmatic web browser for Python. However, it (understandably) doesn't interpret javascript. Is there any programmatic browser for Python which does? If not, is there any JavaScript implementation in Python that I could use to attempt to create one?
",18k,"
            14
        ","['\nYou might be better off using a tool like Selenium to automate the scraping using a web browser, so the JS executes and the page renders just like it would for a real user.\n', '\nThe PyV8 package nicely wraps Google\'s V8 Javascript engine for Python.  It\'s particularly nice because not only can you call from Python to Javascript code, but you can call back from Javascript to Python code.  This makes it quite straightforward to implement the usual browser-supplied objects (that is, everything in the Javascript global namespace: ""window"", ""document"", and so on), which you\'d need to do if you were going to make a Javascript-capable Python browser emulator thing, possibly by hooking this up with mechanize.\n', ""\nMy favorite is PyPhantomJS. It's written using Python and PyQt4. It's completely headless and you can control it completely from JavaScript.\nHowever, if you are looking to actually see the page, you can use QWebView from PyQt4 as well.\n"", '\nThere is also spynner "" a stateful programmatic web browser module for Python with Javascript/AJAX support based on the QtWebkit framework"" : http://code.google.com/p/spynner/\n', '\nYou could also try defining Chickenfoot page triggers on the pages in question, executing whatever operations you want on the page and saving the results of the operation to a local file, and calling Firefox from the command line inside your program, followed by reading the file.\n', '\ni recommend that you take a look at some of the options available to you at http://wiki.python.org/moin/WebBrowserProgramming - surprisingly this is coming up as a common question (i\'ve found three on stackoverflow today, by searching for the words ""python browser"" on google).  if you do the same you\'ll find the other answers i gave.\n', '\nyou may try zope browser\nhttp://pypi.python.org/pypi?:action=display&name=zope.testbrowser\n', ""\nPlaywright or pyppeteer are both reasonably good, and use headless Chromium to render pages and interpret JavaScript.\nI'd pick Playwright out of the two, simply because it's backed by a larger entity, and supports Chromium/Firefox/WebKit out of the box.\n""]"
Run multiple scrapy spiders at once using scrapyd,"
I'm using scrapy for a project where I want to scrape a number of sites - possibly hundreds - and I have to write a specific spider for each site. I can schedule one spider in a project deployed to scrapyd using:
curl http://localhost:6800/schedule.json -d project=myproject -d spider=spider2

But how do I schedule all spiders in a project at once?
All help much appreciated!
",8k,"
            12
        ","['\nMy solution for running 200+ spiders at once has been to create a custom command for the project.  See http://doc.scrapy.org/en/latest/topics/commands.html#custom-project-commands for more information about implementing custom commands.\nYOURPROJECTNAME/commands/allcrawl.py :\nfrom scrapy.command import ScrapyCommand\nimport urllib\nimport urllib2\nfrom scrapy import log\n\nclass AllCrawlCommand(ScrapyCommand):\n\n    requires_project = True\n    default_settings = {\'LOG_ENABLED\': False}\n\n    def short_desc(self):\n        return ""Schedule a run for all available spiders""\n\n    def run(self, args, opts):\n        url = \'http://localhost:6800/schedule.json\'\n        for s in self.crawler.spiders.list():\n            values = {\'project\' : \'YOUR_PROJECT_NAME\', \'spider\' : s}\n            data = urllib.urlencode(values)\n            req = urllib2.Request(url, data)\n            response = urllib2.urlopen(req)\n            log.msg(response)\n\nMake sure to include the following in your settings.py\nCOMMANDS_MODULE = \'YOURPROJECTNAME.commands\'\n\nThen from the command line (in your project directory) you can simply type\nscrapy allcrawl\n\n', ""\nSorry, I know this is an old topic, but I've started learning scrapy recently and stumbled here, and I don't have enough rep yet to post a comment, so posting an answer.\nFrom the common scrapy practices you'll see that if you need to run multiple spiders at once, you'll have to start multiple scrapyd service instances and then distribute your Spider runs among those.\n""]"
Python Scraping JavaScript using Selenium and Beautiful Soup,"
I'm trying to scrape a JavaScript enables page using BS and Selenium. 
I have the following code so far. It still doesn't somehow detect the JavaScript (and returns a null value). In this case I'm trying to scrape the Facebook comments in the bottom. (Inspect element shows the class as postText)
Thanks for the help!
from selenium import webdriver  
from selenium.common.exceptions import NoSuchElementException  
from selenium.webdriver.common.keys import Keys  
import BeautifulSoup

browser = webdriver.Firefox()  
browser.get('http://techcrunch.com/2012/05/15/facebook-lightbox/')  
html_source = browser.page_source  
browser.quit()

soup = BeautifulSoup.BeautifulSoup(html_source)  
comments = soup(""div"", {""class"":""postText""})  
print comments

",16k,"
            11
        ","['\nThere are some mistakes in your code that are fixed below. However, the class ""postText"" must exist elsewhere, since it is not defined in the original source code.\nMy revised version of your code was tested and is working on multiple websites.\nfrom selenium import webdriver  \nfrom selenium.common.exceptions import NoSuchElementException  \nfrom selenium.webdriver.common.keys import Keys  \nfrom bs4 import BeautifulSoup\n\nbrowser = webdriver.Firefox()  \nbrowser.get(\'http://techcrunch.com/2012/05/15/facebook-lightbox/\')  \nhtml_source = browser.page_source  \nbrowser.quit()\n\nsoup = BeautifulSoup(html_source,\'html.parser\')  \n#class ""postText"" is not defined in the source code\ncomments = soup.findAll(\'div\',{\'class\':\'postText\'})  \nprint comments\n\n']"
OpenGL/D3D: How do I get a screen grab of a game running full screen in Windows?,"
Suppose I have an OpenGL game running full screen (Left 4 Dead 2). I'd like to programmatically get a screen grab of it and then write it to a video file. 
I've tried GDI, D3D, and OpenGL methods (eg glReadPixels) and either receive a blank screen or flickering in the capture stream.
Any ideas?
For what it's worth, a canonical example of something similar to what I'm trying to achieve is Fraps.
",6k,"
            10
        ","['\nThere are a few approaches to this problem. Most of them are icky, and it totally depends on what kind of graphics API you want to target, and which functions the target application uses.\nMost DirectX, GDI+ and OpenGL applications are double or tripple-buffered, so they all call:\nvoid SwapBuffers(HDC hdc)\n\nat some point. They also generate WM_PAINT messages in their message queue whenever the window should be drawn. This gives you two options.\n\nYou can install a global hook or thread-local hook into the target process and capture WM_PAINT messages. This allows you to copy the contents from the device context just before the painting happens. The process can be found by enumerating all the processes on the system and look for a known window name, or a known module handle.\n\nYou can inject code into the target process\'s local copy of SwapBuffers. On Linux this would be easy to do via the LD_PRELOAD environmental variable, or by calling ld-linux.so.2 explicitly, but there is no equivalient on Windows. Luckily there is a framework from Microsoft Research which can do this for you called Detours. You can find this here: link.\n\n\nThe demoscene group Farbrausch made a demo-capturing tool named kkapture which makes use of the Detours library. Their tool targets applications that require no user input however, so they basically run the demos at a fixed framerate by hooking into all the possible time functions, like timeGetTime(), GetTickCount() and QueryPerformanceCounter(). It\'s totally rad. A presentation written by ryg (I think?) regarding kkapture\'s internals can be found here. I think that\'s of interest to you.\nFor more information about Windows hooks, see here and here.\nEDIT:\nThis idea intrigued me, so I used Detours to hook into OpenGL applications and mess with the graphics. Here is Quake 2 with green fog added:  \nSome more information about how Detours works, since I\'ve used it first hand now:\nDetours works on two levels. The actual hooking only works in the same process space as the target process. So Detours has a function for injecting a DLL into a process and force its DLLMain to run too, as well as functions that are supposed to be used in that DLL. When DLLMain is run, the DLL should call DetourAttach() to specify the functions to hook, as well as the ""detour"" function, which is the code you want to override with.\nSo it basically works like this:\n\nYou have a launcher application who\'s only task is to call DetourCreateProcessWithDll(). It works the same way as CreateProcessW, only with a few extra parameters. This injects a DLL into a process and calls its DllMain().\nYou implement a DLL that calls the Detour functions and sets up trampoline functions. That means calling DetourTransactionBegin(), DetourUpdateThread(), DetourAttach() followed by DetourTransactionEnd().\nUse the launcher to inject the DLL you implemented into a process.\n\nThere are some caveats though. When DllMain is run, libraries that are imported later with LoadLibrary() aren\'t visible yet. So you can\'t necessarily set up everything during the DLL attachment event. A workaround is to keep track of all the functions that are overridden so far, and try to initialize the others inside these functions that you can already call. This way you will discover new functions as soon as LoadLibrary have mapped them into the memory space of the process. I\'m not quite sure how well this would work for wglGetProcAddress though. (Perhaps someone else here has ideas regarding this?)\nSome LoadLibrary() calls seem to fail. I tested with Quake 2, and DirectSound and the waveOut API failed to initalize for some reason. I\'m still investigating this.\n', ""\nI found a sourceforge'd project called taksi:\nhttp://taksi.sourceforge.net/\nTaksi does not provide audio capture, though.\n"", ""\nI've written screen grabbers in the past (DirectX7-9 era).  I found good old DirectDraw worked remarkably well and would reliably grab bits of hardware-accelerated/video screen content which other methods (D3D, GDI, OpenGL) seemed to leave blank or scrambled.  It was very fast too.\n""]"
What's the best approach for parsing XML/'screen scraping' in iOS? UIWebview or NSXMLParser?,"
I am creating an iOS app that needs to get some data from a web page. My first though was to use NSXMLParser initWithContentsOfURL: and parse the HTML with the NSXMLParser delegate. However this approach seems like it could quickly become painful (if, for example, the HTML changed I would have to rewrite the parsing code which could be awkward). 
Seeing as I'm loading a web page I took take a look at UIWebView too. It looks like UIWebView may be the way to go. stringByEvaluatingJavaScriptFromString: seems like a very handy way to extract the data and would allow the javascript to be stored in a separate file that would be easy to edit if the HTML changed. However, using UIWebView seems a bit hacky (seeing as UIWebView is a UIView subclass it may block the main thread, and the docs say that the javascript has a limit of 10MB).
Does anyone have any advice regarding parsing XML/HTML before I get stuck in?
UPDATE:
I wrote a blog post about my solution:HTML parsing/screen scraping in iOS
",11k,"
            8
        ","['\nI\'ve done this a few times. The best approach I\'ve found is to use libxml2 which has a mode for HTML. Then you can use XPath to query the document. \nWorking with the libxml2 API is not the most enjoyable. So, I usually bring over the XPathQuery.h/.m files documented on this page:\nhttp://cocoawithlove.com/2008/10/using-libxml2-for-parsing-and-xpath.html\nThen I fetch the data using a NSConnection and query the data with something like this:\nNSArray *tdNodes = PerformHTMLXPathQuery(self.receivedData, @""//td[@class=\'col-name\']/a/span"");\n\nSummary:\n\nAdd libxml2 to your project, here are some quick instructions for XCode4: \nhttp://cmar.me/2011/04/20/adding-libxml2-to-an-xcode-4-project/\nGet the XPathQuery.h/.m\nUse an XPath statement to query the html document.\n\n', ""\nParsing HTML with an XML parser usually does not work anyway because many sites have incorrect HTML, which a web browser will deal with, but a strict XML parser like NSXMLParser will totally fail on.\nFor many scripting languages there are great scraping libraries that are more merciful. Like Python's Beautiful Soup module. Unfortunately I do not know of such modules for Objective-C.\nLoading stuff into a UIWebView might be the simplest way to go here. Note that you do not have to put the UIWebView on screen. You can create a separate UIWindow and add the UIWebView to it, so that you do full off-screen rendering. There was a WWDC2009 video about this I think. As you already mention, it will not be lightweight though.\nDepending on the data that you want and the complexity of the pages that you need to parse, you might also be able to parse it by using regular expressions or even a hand written parser. I have done this many times, and for simple data this works well.\n""]"
Webbrowser behaviour issues,"
I am trying to automate Webbrowser with .NET C#. The issue is that the control or should I say IE browser behaves strange on different computers. For example, I am clickin on link and fillup a Ajax popup form on 1st computer like this, without any error:
private void btn_Start_Click(object sender, RoutedEventArgs e)
{
    webbrowserIE.Navigate(""http://www.test.com/"");
    webbrowserIE.DocumentCompleted += fillup_LoadCompleted; 
}

void fillup_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{
    System.Windows.Forms.HtmlElement ele = web_BrowserIE.Document.GetElementById(""login"");
    if (ele != null)
        ele.InvokeMember(""Click"");

    if (this.web_BrowserIE.ReadyState == System.Windows.Forms.WebBrowserReadyState.Complete)
    {
        web_BrowserIE.Document.GetElementById(""login"").SetAttribute(""value"", myUserName);
        web_BrowserIE.Document.GetElementById(""password"").SetAttribute(""value"", myPassword);

        foreach (System.Windows.Forms.HtmlElement el in web_BrowserIE.Document.GetElementsByTagName(""button""))
        {
            if (el.InnerText == ""Login"")
            {
                el.InvokeMember(""click"");
            }
        }

        web_BrowserIE.DocumentCompleted -= fillup_LoadCompleted;        
    }
}

However, the above code wont work on 2nd pc and the only way to click is like this:
private void btn_Start_Click(object sender, RoutedEventArgs e)
{
    webbrowserIE.DocumentCompleted += click_LoadCompleted;
    webbrowserIE.Navigate(""http://www.test.com/""); 
}

void click_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{
    if (this.webbrowserIE.ReadyState == System.Windows.Forms.WebBrowserReadyState.Complete)
    {
        System.Windows.Forms.HtmlElement ele = webbrowserIE.Document.GetElementById(""login"");
        if (ele != null)
            ele.InvokeMember(""Click"");

        webbrowserIE.DocumentCompleted -= click_LoadCompleted;
        webbrowserIE.DocumentCompleted += fillup_LoadCompleted;
    }
}

void click_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{

        webbrowserIE.Document.GetElementById(""login_login"").SetAttribute(""value"", myUserName);
        webbrowserIE.Document.GetElementById(""login_password"").SetAttribute(""value"", myPassword);

        //If you know the ID of the form you would like to submit:
        foreach (System.Windows.Forms.HtmlElement el in webbrowserIE.Document.GetElementsByTagName(""button""))
        {
            if (el.InnerText == ""Login"")
            {
                el.InvokeMember(""click"");
            }
        }

        webbrowserIE.DocumentCompleted -= click_LoadCompleted;      
}

So, in second solution I have to call two Load Completed Chains. Could someone advise on how should I can handle this issue? Also, a proposal for more robust approach would be very helpfull. Thank you in advance 
",3k,"
            1
        ","['\nI could recommend two things:\n\nDon\'t execute your code upon DocumentComplete event, rather do upon DOM window.onload event.\nTo make sure your web page behaves in WebBrowser control the same way as it would in full Internet Explorer browser, consider implementing Feature Control.\n\n[EDITED] There\'s one more suggestion, based on the structure of your code. Apparently, you perform a series of navigation/handle DocumentComplete actions. It might be more natural and easy to use async/await for this. Here\'s an example of doing this, with or without async/await. It illustrates how to handle onload, too:\nasync Task DoNavigationAsync()\n{\n    bool documentComplete = false;\n    TaskCompletionSource<bool> onloadTcs = null;\n\n    WebBrowserDocumentCompletedEventHandler handler = delegate \n    {\n        if (documentComplete)\n            return; // attach to onload only once per each Document\n        documentComplete = true;\n\n        // now subscribe to DOM onload event\n        this.wb.Document.Window.AttachEventHandler(""onload"", delegate\n        {\n            // each navigation has its own TaskCompletionSource\n            if (onloadTcs.Task.IsCompleted)\n                return; // this should not be happening\n\n            // signal the completion of the page loading\n            onloadTcs.SetResult(true);\n        });\n    };\n\n    // register DocumentCompleted handler\n    this.wb.DocumentCompleted += handler;\n\n    // Navigate to http://www.example.com?i=1\n    documentComplete = false;\n    onloadTcs = new TaskCompletionSource<bool>();\n    this.wb.Navigate(""http://www.example.com?i=1"");\n    await onloadTcs.Task;\n    // the document has been fully loaded, you can access DOM here\n    MessageBox.Show(this.wb.Document.Url.ToString());\n\n    // Navigate to http://example.com?i=2\n    // could do the click() simulation instead\n\n    documentComplete = false;\n    onloadTcs = new TaskCompletionSource<bool>(); // new task for new navigation\n    this.wb.Navigate(""http://example.com?i=2"");\n    await onloadTcs.Task;\n    // the document has been fully loaded, you can access DOM here\n    MessageBox.Show(this.wb.Document.Url.ToString());\n\n    // no more navigation, de-register DocumentCompleted handler\n    this.wb.DocumentCompleted -= handler;\n}\n\nHere\'s the same code without async/await pattern (for .NET 4.0):\nTask DoNavigationAsync()\n{\n    // save the correct continuation context for Task.ContinueWith\n    var continueContext = TaskScheduler.FromCurrentSynchronizationContext(); \n\n    bool documentComplete = false;\n    TaskCompletionSource<bool> onloadTcs = null;\n\n    WebBrowserDocumentCompletedEventHandler handler = delegate \n    {\n        if (documentComplete)\n            return; // attach to onload only once per each Document\n        documentComplete = true;\n\n        // now subscribe to DOM onload event\n        this.wb.Document.Window.AttachEventHandler(""onload"", delegate\n        {\n            // each navigation has its own TaskCompletionSource\n            if (onloadTcs.Task.IsCompleted)\n                return; // this should not be happening\n\n            // signal the completion of the page loading\n            onloadTcs.SetResult(true);\n        });\n    };\n\n    // register DocumentCompleted handler\n    this.wb.DocumentCompleted += handler;\n\n    // Navigate to http://www.example.com?i=1\n    documentComplete = false;\n    onloadTcs = new TaskCompletionSource<bool>();\n    this.wb.Navigate(""http://www.example.com?i=1"");\n\n    return onloadTcs.Task.ContinueWith(delegate \n    {\n        // the document has been fully loaded, you can access DOM here\n        MessageBox.Show(this.wb.Document.Url.ToString());\n\n        // Navigate to http://example.com?i=2\n        // could do the \'click()\' simulation instead\n\n        documentComplete = false;\n        onloadTcs = new TaskCompletionSource<bool>(); // new task for new navigation\n        this.wb.Navigate(""http://example.com?i=2"");\n\n        onloadTcs.Task.ContinueWith(delegate \n        {\n            // the document has been fully loaded, you can access DOM here\n            MessageBox.Show(this.wb.Document.Url.ToString());\n\n            // no more navigation, de-register DocumentCompleted handler\n            this.wb.DocumentCompleted -= handler;\n        }, continueContext);\n\n    }, continueContext);\n}\n\nNote, it both cases it is still a piece of asynchronous code which returns a Task object. Here\'s an example of how to handle the completion of such task:\nprivate void Form1_Load(object sender, EventArgs e)\n{\n    DoNavigationAsync().ContinueWith(_ => {\n        MessageBox.Show(""Navigation complete!"");\n    }, TaskScheduler.FromCurrentSynchronizationContext());\n}\n\nThe benefit of using TAP pattern here is that DoNavigationAsync is a self-contained, independent method. It can be reused and it doesn\'t interfere with the state of parent object (in this case, the main form).\n']"
PHP CSS Selector Library? [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



Is there a PHP class/library that would allow me to query an XHTML document with CSS selectors? I need to scrape some pages for data that is very easily accessible if I could somehow use CSS selectors (jQuery has spoiled me!). Any ideas?
",22k,"
            33
        ","[""\nAfter Googling further (initial results weren't very helpful), it seems there is actually a Zend Framework library for this, along with some others:\n\nDOM-Query\nphpQuery\npQuery\nQueryPath\nSimple HTML DOM Parser\nUltimate Web Scraper Toolkit\nZend-Dom\n\n"", '\nXPath is a fairly standard way to access XML (and XHTML) nodes, and provides much more precision than CSS.\n', '\nAnother one:\nhttp://querypath.org/\n', '\nA great one is a component of symfony 2, CssSelector\\Parser\xadIntroduction. It converts CSS selectors into XPath expressions. Take a look =)\nSource code\n', '\nFor jQuery users most interesting may be port of jQuery to PHP, which is phpQuery. Almost all sections of the library are ported. Additionally it contains WebBrowser plugin, which can be used for Web Scraping whole site\'s path/processes (eg accessing data available after logging in). It simply simulates web browser on the server (events and cookies too). Latest versions has experimental support for XML namespaces and CSS3 ""|"" selector.\n', ""\nI ended up using PHP Query Lite, it's very simple and has all I need.\n"", '\nFor document parsing I use DOM.  This can quite easily solve your problem if you know the tag name (in this example ""div""):\n $doc = new DOMDocument();\n $doc->loadHTML($html);\n\n $elements = $doc->getElementsByTagName(""div"");\n foreach ($elements as $e){\n  if ($e->getAttribute(""class"")!=""someclass"") continue;\n\n  //its a div.classname\n }\n\nNot sure if DOM lets you get all elements of a document at once... you might have to do a tree traversal.\n', ""\nI wrote mine, based on Mootools CSS selector engine http://selectors.svn.exyks.org/. it rely on simplexml extension ability (so, it's read-only)\n""]"
Scraping websites with Javascript enabled?,"
I'm trying to scrape and submit information to websites that heavily rely on Javascript to do most of its actions. The website won't even work when i disable Javascript in my browser.
I've searched for some solutions on Google and SO and there was someone who suggested i should reverse engineer the Javascript, but i have no idea how to do that. 
So far i've been using Mechanize and it works on websites that don't require Javascript.
Is there any way to access websites that use Javascript by using urllib2 or something similar? 
I'm also willing to learn Javascript, if that's what it takes.
",25k,"
            18
        ","['\nI wrote a small tutorial on this subject, this might help:\nhttp://koaning.io.s3-website.eu-west-2.amazonaws.com/dynamic-scraping-with-python.html\nBasically what you do is you have the selenium library pretend that it is a firefox browser, the browser will wait until all javascript has loaded before it continues passing you the html string. Once you have this string, you can then parse it with beautifulsoup.\n', ""\nI've had exactly the same problem. It is not simple at all, but I finally found a great solution, using PyQt4.QtWebKit.\nYou will find the explanations on this webpage : http://blog.motane.lu/2009/07/07/downloading-a-pages-content-with-python-and-webkit/\nI've tested it, I currently use it, and that's great !\nIts great advantage is that it can run on a server, only using X, without a graphic environment.\n"", ""\nYou should look into using Ghost, a Python library that wraps the PyQt4 + WebKit hack.\nThis makes g the WebKit client:\nimport ghost\ng = ghost.Ghost()\n\nYou can grab a page with g.open(url) and then g.content will evaluate to the document in its current state.\nGhost has other cool features, like injecting JS and some form filling methods, and you can pass the resulting document to BeautifulSoup and so on: soup = bs4.BeautifulSoup(g.content).\nSo far, Ghost is the only thing I've found that makes this kind of thing easy in Python. The only limitation I've come across is that you can't easily create more than one instance of the client object, ghost.Ghost, but you could work around that.\n"", ""\nCheck out crowbar. I haven't had any experience with it, but I was curious about the answer to your question so I started googling around. I'd like to know if this works out for you.\nhttp://grep.codeconsult.ch/2007/02/24/crowbar-scrape-javascript-generated-pages-via-gecko-and-rest/\n"", ""\nMaybe you could use Selenium Webdriver, which has python bindings I believe. I think it's mainly used as a tool for testing websites, but I guess it should be usable for scraping too. \n"", '\nI would actually suggest using Selenium.  Its mainly designed for testing Web-Applications from a ""user perspective however it is basically a ""FireFox"" driver.  I\'ve actually used it for this purpose ... although I was scraping an dynamic AJAX webpage.  As long as the Javascript form has a recognizable ""Anchor Text"" that Selenium can ""click"" everything should sort itself out.\nHope that helps\n']"
How can I read and parse the contents of a webpage in R,"
I'd like to read the contents of a URL (e.q., http://www.haaretz.com/) in R. I am wondering how I can do it
",33k,"
            16
        ","['\nNot really sure how you want to process that page, because it\'s really messy.  As we re-learned in this famous stackoverflow question, it\'s not a good idea to do regex on html, so you will definitely want to parse this with the XML package.  \nHere\'s an example to get you started:\nrequire(RCurl)\nrequire(XML)\nwebpage <- getURL(""http://www.haaretz.com/"")\nwebpage <- readLines(tc <- textConnection(webpage)); close(tc)\npagetree <- htmlTreeParse(webpage, error=function(...){}, useInternalNodes = TRUE)\n# parse the tree by tables\nx <- xpathSApply(pagetree, ""//*/table"", xmlValue)  \n# do some clean up with regular expressions\nx <- unlist(strsplit(x, ""\\n""))\nx <- gsub(""\\t"","""",x)\nx <- sub(""^[[:space:]]*(.*?)[[:space:]]*$"", ""\\\\1"", x, perl=TRUE)\nx <- x[!(x %in% c("""", ""|""))]\n\nThis results in a character vector of mostly just webpage text (along with some javascript):\n> head(x)\n[1] ""Subscribe to Print Edition""              ""Fri., December 04, 2009 Kislev 17, 5770"" ""Israel Time:Â\xa016:48Â\xa0(EST+7)""           \n[4] ""Â\xa0Â\xa0Make Haaretz your homepage""          ""/*check the search form*/""               ""function chkSearch()"" \n\n', '\nYour best bet may be the XML package -- see for example this previous question.\n', '\nI know you asked for R. But maybe python+beautifullsoup is the way forward here? Then do your analysis with R you have scraped the screen with beautifullsoup?\n']"
Extracting table contents from html with python and BeautifulSoup,"
I want to extract certain information out of an html document. E.g. it contains a table 
(among other tables with other contents) like this:
    <table class=""details"">
            <tr>
                    <th>Advisory:</th>
                    <td>RHBA-2013:0947-1</td>
            </tr>
            <tr>    
                    <th>Type:</th>
                    <td>Bug Fix Advisory</td>
            </tr>
            <tr>
                    <th>Severity:</th>
                    <td>N/A</td>
            </tr>
            <tr>    
                    <th>Issued on:</th>
                    <td>2013-06-13</td>
            </tr>
            <tr>    
                    <th>Last updated on:</th>
                    <td>2013-06-13</td>
            </tr>

            <tr>
                    <th valign=""top"">Affected Products:</th>
                    <td><a href=""#Red Hat Enterprise Linux ELS (v. 4)"">Red Hat Enterprise Linux ELS (v. 4)</a></td>
            </tr>


    </table>

I want to extract Information like the date of ""Issued on:"". It looks like BeautifulSoup4
could do this easyly, but somehow I don't manage to get it right.
My code so far:
    from bs4 import BeautifulSoup
    soup=BeautifulSoup(unicodestring_containing_the_entire_htlm_doc)
    table_tag=soup.table
    if table_tag['class'] == ['details']:
            print table_tag.tr.th.get_text() + "" "" + table_tag.tr.td.get_text()
            a=table_tag.next_sibling
            print  unicode(a)
            print table_tag.contents

This gets me the contents of the first table row, and also a listing of the contents. 
But the next sibling thing is not working right, I guess I am just using it wrong.
Of course I could just parse the contents thingy, but it seems to me that beautiful soup
was designed to prevent us from doing exactly this (if I start parsing myself, I might as
well parse the whole doc ...). If someone could enlighten me on how to acomplish this, I 
would be gratefull. If there is a better way then BeautifulSoup, I would be interested to 
hear about it.
",41k,"
            11
        ","[""\n>>> from bs4 import BeautifulSoup\n>>> soup = BeautifulSoup(unicodestring_containing_the_entire_htlm_doc)\n>>> table = soup.find('table', {'class': 'details'})\n>>> th = table.find('th', text='Issued on:')\n>>> th\n<th>Issued on:</th>\n>>> td = th.findNext('td')\n>>> td\n<td>2013-06-13</td>\n>>> td.text\nu'2013-06-13'\n\n""]"
scrapy how to set referer url,"
I need to set the referer url, before scraping a site, the site uses refering url based Authentication, so it does not allow me to login if the referer is not valid.
Could someone tell how to do this in Scrapy?
",20k,"
            11
        ","[""\nIf you want to change the referer in your spider's request, you can change DEFAULT_REQUEST_HEADERS in the settings.py file:\nDEFAULT_REQUEST_HEADERS = {\n    'Referer': 'http://www.google.com' \n}\n\n"", '\nYou should do exactly as @warwaruk indicated, below is my example elaboration for a crawl spider:\nfrom scrapy.spiders import CrawlSpider\nfrom scrapy import Request\n\nclass MySpider(CrawlSpider):\n  name = ""myspider""\n  allowed_domains = [""example.com""]\n  start_urls = [\n      \'http://example.com/foo\'\n      \'http://example.com/bar\'\n      \'http://example.com/baz\'\n      ]\n  rules = [(...)]\n\n  def start_requests(self):\n    requests = []\n    for item in self.start_urls:\n      requests.append(Request(url=item, headers={\'Referer\':\'http://www.example.com/\'}))\n    return requests    \n\n  def parse_me(self, response):\n    (...)\n\nThis should generate following logs in your terminal:\n(...)\n[myspider] DEBUG: Crawled (200) <GET http://example.com/foo> (referer: http://www.example.com/)\n(...)\n[myspider] DEBUG: Crawled (200) <GET http://example.com/bar> (referer: http://www.example.com/)\n(...)\n[myspider] DEBUG: Crawled (200) <GET http://example.com/baz> (referer: http://www.example.com/)\n(...)\n\nWill work same with BaseSpider. In the end start_requests method is BaseSpider method, from which CrawlSpider inherits from.\nDocumentation explains more options to be set in Request apart from headers, such as: cookies , callback function, priority of the request etc.\n', ""\nJust set Referer url in the Request headers\n\nclass scrapy.http.Request(url[, method='GET', body, headers, ...\nheaders (dict) – the headers of this request. The dict values can be strings (for single valued headers) or lists (for multi-valued headers).\n\nExample:\nreturn Request(url=your_url,\n               headers={'Referer':'http://your_referer_url'})\n"", '\nOverride BaseSpider.start_requests and create there your custom Request passing it your referer header.\n']"
Scraping javascript-generated data using Python,"
I want to scrape some data of following url using Python.
http://www.hankyung.com/stockplus/main.php?module=stock&mode=stock_analysis_infomation&itemcode=078340
It's about a summary of company information. 
What I want to scrape is not shown on the first page. 
By clicking tab named ""재무제표"", you can access financial statement. And clicking tab named ""현금흐름표', you can access ""Cash Flow"". 
I want to scrape the ""Cash Flow"" data. 
However, Cash flow data is generated by javascript across the url.
The following link is that url which is hidden, http://stock.kisline.com/compinfo/financial/main.action?vhead=N&vfoot=N&vstay=&omit=&vwidth=
Cash flow data is generated by submitting some option value and cookie to this url.
As you perceived, itemcode=078340 in the first link means stock code and there are as many as 1680 stocks that I want gather cash flow data. I want make it a loop structure.
Is there good way to scrape cash flow data?
I tried scrapy but scrapy is difficult to cope with my another scraping code already I'm using.
",11k,"
            7
        ","[""\nThere's also dryscape (a library written by me, so the recommendation is a bit biased, obviously :) which uses a fast Webkit-based in-memory browser to navigate around. It understands Javascript, too, but is a lot more lightweight than Selenium.\n"", '\nIf you need to scape the page content which is updated with AJAX and you are not in the control of this AJAX interface I would use Selenium browser automator for the task:\nhttp://code.google.com/p/selenium/\n\nSelenium has Python bindings\nIt launches a real browser instance so it can do and scrape 100% the same thing as you see with your own eyes\nGet HTML document content after AJAX updates thru Selenium API\nUse lxml + xpath / CSS selectors to parse out the relevant parts out of the document\n\n']"
How to replace words with span tag using jsoup?,"
Assume I have the following html:
<html>
<head>
</head>
<body>
    <div id=""wrapper"" >
         <div class=""s2"">I am going <a title=""some title"" href="""">by flying</a>
           <p>mr tt</p>
         </div> 
    </div>
</body>    
</html>

Any words in the text nodes that are equal to or greater than 4 characters for example the word 'going' is replaced with html content (not text) <span>going<span> in the original html without changing anything else.
If I try do something like element.html(replacement), the problem is if lets the current element is <div class=""s2""> it will also wipe off <a title=""some title"" 
",6k,"
            7
        ","['\nIn this case you must traverse your document as suggested by this answer. Here\'s a way of doing it using Jsoup APIs:\n\nNodeTraversor and NodeVisitor allow you to traverse the DOM\nNode.replaceWith(...) allows for replacing a node in the DOM\n\nHere\'s the code:\npublic class JsoupReplacer {\n\n  public static void main(String[] args) {\n    so6527876();\n  }\n\n  public static void so6527876() {\n    String html = \n    ""<html>"" +\n    ""<head>"" +\n    ""</head>"" +\n    ""<body>"" +\n    ""    <div id=\\""wrapper\\"" >"" +\n    ""         <div class=\\""s2\\"">I am going <a title=\\""some title\\"" href=\\""\\"">by flying</a>"" +\n    ""           <p>mr tt</p>"" +\n    ""         </div> "" +\n    ""    </div>"" +\n    ""</body>    "" +\n    ""</html>"";\n    Document doc = Jsoup.parse(html);\n\n    final List<TextNode> nodesToChange = new ArrayList<TextNode>();\n\n    NodeTraversor nd  = new NodeTraversor(new NodeVisitor() {\n\n      @Override\n      public void tail(Node node, int depth) {\n        if (node instanceof TextNode) {\n          TextNode textNode = (TextNode) node;\n          String text = textNode.getWholeText();\n          String[] words = text.trim().split("" "");\n          for (String word : words) {\n            if (word.length() > 4) {\n              nodesToChange.add(textNode);\n              break;\n            }\n          }\n        }\n      }\n\n      @Override\n      public void head(Node node, int depth) {        \n      }\n    });\n\n    nd.traverse(doc.body());\n\n    for (TextNode textNode : nodesToChange) {\n      Node newNode = buildElementForText(textNode);\n      textNode.replaceWith(newNode);\n    }\n\n    System.out.println(""result: "");\n    System.out.println();\n    System.out.println(doc);\n  }\n\n  private static Node buildElementForText(TextNode textNode) {\n    String text = textNode.getWholeText();\n    String[] words = text.trim().split("" "");\n    Set<String> longWords = new HashSet<String>();\n    for (String word : words) {\n      if (word.length() > 4) {\n        longWords.add(word);\n      } \n    }\n    String newText = text;\n    for (String longWord : longWords) {\n      newText = newText.replaceAll(longWord, \n          ""<span>"" + longWord + ""</span>"");\n    }\n    return new DataNode(newText, textNode.baseUri());\n  }\n\n}\n\n', '\nI think you need to traverse the tree.  The result of text() on an Element will be all of the Element\'s text including text within child elements. Hopefully something like the following code will be helpful to you:\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.StringTokenizer;\nimport org.apache.commons.io.FileUtils;\nimport org.jsoup.Jsoup;\nimport org.jsoup.nodes.Document;\nimport org.jsoup.nodes.Element;\nimport org.jsoup.nodes.Node;\nimport org.jsoup.nodes.TextNode;\n\npublic class ScreenScrape {\n\n    public static void main(String[] args) throws IOException {\n        String content = FileUtils.readFileToString(new File(""test.html""));\n        Document doc = Jsoup.parse(content);\n        Element body = doc.body();\n        //System.out.println(body.toString());\n\n        StringBuilder sb = new StringBuilder();\n        traverse(body, sb);\n\n        System.out.println(sb.toString());\n    }\n\n    private static void traverse(Node n, StringBuilder sb) {\n        if (n instanceof Element) {\n            sb.append(\'<\');\n            sb.append(n.nodeName());            \n            if (n.attributes().size() > 0) {\n                sb.append(n.attributes().toString());\n            }\n            sb.append(\'>\');\n        }\n        if (n instanceof TextNode) {\n            TextNode tn = (TextNode) n;\n            if (!tn.isBlank()) {\n                sb.append(spanifyText(tn.text()));\n            }\n        }\n        for (Node c : n.childNodes()) {\n            traverse(c, sb);\n        }\n        if (n instanceof Element) {\n            sb.append(""</"");\n            sb.append(n.nodeName());\n            sb.append(\'>\');\n        }        \n    }\n\n    private static String spanifyText(String text){\n        StringBuilder sb = new StringBuilder();\n        StringTokenizer st = new StringTokenizer(text);\n        String token;\n        while (st.hasMoreTokens()) {\n             token = st.nextToken();\n             if(token.length() > 3){\n                 sb.append(""<span>"");\n                 sb.append(token);\n                 sb.append(""</span>"");\n             } else {\n                 sb.append(token);\n             }             \n             sb.append(\' \');\n        }\n        return sb.substring(0, sb.length() - 1).toString();\n    }\n\n}\n\n\nUPDATE\nUsing Jonathan\'s new Jsoup List element.textNode() method and combining it with MarcoS\'s suggested NodeTraversor/NodeVisitor technique I came up with (although I am modifying the tree whilst traversing it - probably a bad idea):\nDocument doc = Jsoup.parse(content);\nElement body = doc.body();\nNodeTraversor nd = new NodeTraversor(new NodeVisitor() {\n\n    @Override\n    public void tail(Node node, int depth) {\n        if (node instanceof Element) {\n            boolean foundLongWord;\n            Element elem = (Element) node;\n            Element span;\n            String token;\n            StringTokenizer st;\n            ArrayList<Node> changedNodes;\n            Node currentNode;\n            for (TextNode tn : elem.textNodes()) {\n                foundLongWord = Boolean.FALSE;\n                changedNodes = new ArrayList<Node>();\n                st = new StringTokenizer(tn.text());\n                while (st.hasMoreTokens()) {\n                    token = st.nextToken();\n                    if (token.length() > 3) {\n                        foundLongWord = Boolean.TRUE;\n                        span = new Element(Tag.valueOf(""span""), elem.baseUri());\n                        span.appendText(token);\n                        changedNodes.add(span);\n                    } else {\n                        changedNodes.add(new TextNode(token + "" "", elem.baseUri()));\n                    }\n                }\n                if (foundLongWord) {\n                    currentNode = changedNodes.remove(0);\n                    tn.replaceWith(currentNode);\n                    for (Node n : changedNodes) {\n                        currentNode.after(n);\n                        currentNode = n;\n                    }\n                }\n            }\n        }\n    }\n\n    @Override\n    public void head(Node node, int depth) {\n    }\n});    \nnd.traverse(body);\nSystem.out.println(body.toString());\n\n', '\nI am replacing word hello with hello(span tag)\nDocument doc = Jsoup.parse(content);\n    Element test =  doc.body();\n    Elements elemenets = test.getAllElements();\n    for(int i =0 ;i <elemenets .size();i++){\n        String elementText = elemenets .get(i).text();\n        if(elementText.contains(""hello""))\n            elemenets .get(i).html(l.get(i).text().replaceAll(""hello"",""<span style=\\""color:blue\\"">hello</span>""));\n    }\n\n']"
scrapy log handler,"
I seek your help in the following 2 questions - How do I set the handler for the different log levels like in python. Currently, I have 
STATS_ENABLED = True
STATS_DUMP = True 

LOG_FILE = 'crawl.log'

But the debug messages generated by Scrapy are also added into the log files. Those are very long and ideally, I would  like the DEBUG level messages to left on standard error and INFO messages to be dump to my LOG_FILE.
Secondly, in the docs, it says The logging service must be explicitly started through the scrapy.log.start() function. My question is, where do I run this scrapy.log.start()? Is it inside my spider?
",7k,"
            3
        ","['\n\nSecondly, in the docs, it says The logging service must be explicitly\n  started through the scrapy.log.start() function. My question is, where\n  do I run this scrapy.log.start()? Is it inside my spider?\n\nIf you run a spider using scrapy crawl my_spider -- the log is started automatically if STATS_ENABLED = True\nIf you start the crawler process manually, you can do scrapy.log.start() before starting the crawler process.\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.conf import settings\n\n\nsettings.overrides.update({}) # your settings\n\ncrawlerProcess = CrawlerProcess(settings)\ncrawlerProcess.install()\ncrawlerProcess.configure()\n\ncrawlerProcess.crawl(spider) # your spider here\n\nlog.start() # depends on LOG_ENABLED\n\nprint ""Starting crawler.""\ncrawlerProcess.start()\nprint ""Crawler stopped.""\n\nThe little knowledge I have about your first question:\nBecause you have to start the scrapy log manually, this allows you to use your own logger. \nI think you can copy module scrapy/scrapy/log.py in scrapy sources, modify it, import it instead of scrapy.log and run start() - scrapy will use your log. In it there is a line in function start() which says log.startLoggingWithObserver(sflo.emit, setStdout=logstdout). \nMake your own observer (http://docs.python.org/howto/logging-cookbook.html#logging-to-multiple-destinations) and use it there.\n', ""\n\nI would like the DEBUG level messages to left on standard error and INFO messages to be dump to my LOG_FILE.\n\nYou can set LOG_LEVEL = 'INFO' in settings.py, but it will completely disable DEBUG messages.\n"", '\nHmm, \nJust wanted to update that I am able to get the logging file handler to file by using\nfrom twisted.python import log\nimport logging\nlogging.basicConfig(level=logging.INFO, filemode=\'w\', filename=\'log.txt\'"""""")\nobserver = log.PythonLoggingObserver()\nobserver.start()\n\nhowever I am unable to get the log to display the spiders\' name like from twisted in standard error. I posted this question. \n', ""\nscrapy some-scrapy's-args -L 'INFO' -s LOG_FILE=log1.log\n\noutputs will be redirected to a logname file .\n""]"
Evaluate javascript on a local html file (without browser),"
This is part of a project I am working on for work.
I want to automate a Sharepoint site, specifically to pull data out of a database that I and my coworkers only have front-end access to.
I FINALLY managed to get mechanize (in python) to accomplish this using Python-NTLM, and by patching part of it's source code to fix a reoccurring error.
Now, I am at what I would hope is my final roadblock: Part of the form I need to submit seems to be output of a JavaScript function :| and lo and behold... Mechanize does not support javascript. I don't want to emulate the javascript functionality myself in python because I would ideally like a reusable solution...
So, does anyone know how I could evaluate the javascript on the local html I download from sharepoint? I just want to run the javascript somehow (to complete the loading of the page), but without a browser.
I have already looked into selenium, but it's pretty slow for the amount of work I need to get done... I am currently looking into PyV8 to try and evaluate the javascript myself... but surely there must be an app or library (or anything) that can do this??
",2k,"
            3
        ","['\nWell, in the end I came down to the following possible solutions:\n\nRun Chrome headless and collect the html output (thanks to koenp for the link!)\nRun PhantomJS, a headless browser with a javascript api\nRun HTMLUnit; same thing but for Java\nUse Ghost.py, a python-based headless browser (that I haven\'t seen suggested anyyyywhere for some reason!)\nWrite a DOM-based javascript interpreter based on Pyv8 (Google v8 javascript engine) and add this to my current ""half-solution"" with mechanize.\n\nFor now, I have decided to use either use Ghost.py or my own modification of the PySide/PyQT Webkit (how ghost works) to evaluate the javascript, as apparently they can run quite fast if you optimize them to not download images and disable the GUI.\nHopefully others will find this list useful!\n', ""\nWell you will need something that both understands the DOM and understand Javascript, so that comes down to a headless browser of some sort. Maybe you can take a look at the selenium webdriver, but I guess you already did that. I don't hink there is an easy way of doing this without running the stuff in an actually browser engine.\n""]"
CasperJS click event having AJAX call,"
I am trying to fetch data from a site by simulating events using CasperJS with phantomJS 1.7.0.
I am able to simulate normal click events and select events. But my code fails in following scenario:
When I click on button / anchor etc on remote page, the click on remote page  initiates an AJAX call / JS call(depending on how that page is implemented by programmer.). 
In case of JS call, my code works and I get changed data. But for clicks where is AJAX call is initiated, I do not get updated data.
For debugging, I tried to get the page source of the element container(before and after), but I see no change in code. 
I tried to set wait time from 10 sec to 1 ms range, but that to does not reflect any changes in behavior. 
Below is my piece of code for clicking. I am using an array of CSS Paths, which represents which element(s) to click.
/*Click on array of clickable elements using CSS Paths.*/
fn_click = function(){
casper.each(G_TAGS,function(casper, cssPath, count1) 
                    {
                            casper.then ( function() {
                            casper.click(cssPath);

                            this.echo('DEBUG AFTER CLICKING -START HTML ');
                            //this.echo(this.getHTML(""CONTAINER WHERE DETAILS CHANGE""));
                            this.echo('DEBUG AFTER CLICKING -START HTML');
                            casper.wait(5000, function() 
                                                    {   

                                                        casper.then(fn_getData);
                                                    } 
                                    );
                            });     
                    });
};

UPDATE:
I tried to use remote-debug option from phantomJS, to debug above script. 
It is not working. I am on windows. I will try to run remote debugging on Ubuntu as well. 
Please help me. I would appreciate any help on this. 
UPDATE:
Please have a look at following code as a sample. 
https://gist.github.com/4441570

Content before click and after click are same. 
I am clicking on sorting options provided under tag (votes / activity etc.). 
",5k,"
            2
        ","[""\nI had the same problem today. I found this post, which put me in the direction of jQuery.\nAfter some testing I found out that there was already a jQuery loaded on that webpage. (A pretty old version though)\nLoading another jQuery on top of that broke any js calls made, so also the link that does an Ajax call.\nTo solve this I found http://api.jquery.com/jQuery.noConflict/\nand I added the following to my code:\n    this.evaluate(function () { jq = $.noConflict(true) } ); \n\nAnything that was formerly assigned to $ will be restored that way. And the jQuery that you injected is now available under 'jq'.\nHope this helps you guys.\n""]"
C# WebClient - View source question,"
I'm using  a C# WebClient to post login details to a page and read the all the results.
The page I am trying to load includes flash (which, in the browser, translates into HTML). I'm guessing it's flash to avoid being picked up by search engines???
The flash I am interested in is just text (not an image/video) etc and when I ""View Selection Source"" in firefox I do actually see the text, within HTML, that I want to see.
(Interestingly when I view the source for the whole page I do not see the text, within HTML, that I want to see. Could this be related?)
Currently after I have posted my login details, and loaded the HTML back, I see the page which does NOT show the flash HTML (as if I had viewed source for the whole page).
Thanks in advance,
Jim
PS: I should point out that the POST is actually working, my log in is successful.
",3k,"
            2
        ","['\nFiddler (or similar tool) is invaluable to track down screen-scraping problems like this.  Using a normal browser and with fiddler active, look at all the requests being made as you go through the login and navigation process to get to the data you want.  In between, you will likely see one or more things that your code is doing differently which the server is responding to and hence showing you different HTML than a real client.\nThe list of stuff below (think of it as ""scraping 101"") is what you want to look for.  Most of the stuff below is probably stuff you\'re already doing, but I included everything for completeness. \nIn order to scrape effectively, you may need to deal with one or more of the following:\n\ncookies and/or hidden fields. when you show up at any page on a site, you\'ll typically get a session cookie and/or hidden form field which (in a normal browser) would be propagated back to the server on all subsequent requests. You will likely also get a persistent cookie.  On many sites, if a requests shows up without a proper cookie (or form field for sites using ""cookieless sessions""), the site will redirect the user to a ""no cookies"" UI, a login page, or another undesirable location (from the scraper app\'s perspective).  always make sure you capture the cookies set on the initial request and faithfully send them back to the server on subsequent requests, except if one of those subsequent requests changes a cookie (in which case propagate that new cookie instead).\nauthentication tokens a special case of above is forms-authentication cookies or hidden fields. make sure you\'re capturing the login token (usually a cookie) and sending it back.\nPOST vs. GET this is obvious, but make sure you\'re using the same HTTP method that a real browser does. \nform fields (esp. hidden ones!) I\'m sure you\'re doing this already, but make sure to send all form fields that a real browser does, not just the visible fields. make sure fields are HTML-encoded properly.\nHTTP headers. you already checked this, but it may make sense to check again just to make sure the (non-cookie) headers are identical. I always start with the exact same headers and then start pulling out headers one by one, and only keep the ones that cause the request to fail or return bogus data. this approach simplifies your scraping code. \nredirects. These can either come from the server, or from client script (e.g. ""if user doesn\'t have flash plug-in loaded, redirect to a non-flash page"").  See WebRequest: How to find a postal code using a WebRequest against this ContentType=""application/xhtml+xml, text/xml, text/html; charset=utf-8""? for a crazy example of how redirection can trip up a screen-scraper.  Note that if you\'re using .NET for scraping, you\'ll need to use HttpWebRequest (not WebClient) for redirect-dependent scraping, because by default WebClient doesn\'t provide a way for your code to attach cookies and headers to the second (post-redirect) request.  See the thread above for more details.\nsub-requests (frames, ajax, flash, etc.) - often, page elements (not the main HTTP requests) will end up fetching the data you want to scrape.  you\'ll be able to figure this out by looking which HTTP response contains the text you want, and then working backwards until you find what on the page is actually making the request for that content. A few sites do really crazy things in sub-requests, like requesting compressed or encrypted text via ajax, and then using client-side script to decrypt it.  if this is the case, you\'ll need to do a bit more work like reverse-engineering what the client script is doing.\nordering - this one is obvious: make HTTP requests in the same order that a browser client does. that doesn\'t mean you need to make every request (e.g. images). Typically you only need to make the requests which return text/html content type, unless the data you want is not in the HTML and is in an ajax/flash/etc. request. \n\n', '\n\n(Interestingly when I view the source for the whole page I do not see the text, within HTML, that I want to see. Could this be related?)\n\nThis usually means that the discrepancy is caused by some DOM manipulations via javascript after the page has loaded.  Try turning off javascript and see what it looks like.\n']"
"Why is contains(text(), ""string"" ) not working in XPath?","
I have written this expression //*[contains(text(), ""Brand:"" )] for the below HTML code.


<div class=""info-product mt-3"">
  <h3>Informazioni prodotto</h3>


  Brand: <span class=""brand_title font-weight-bold text-uppercase""><a href=""https://mammapack.com/brand/ava"">Ava</a></span><br> SKU: 8002910009960<br> Peso Lordo: 0.471 kg <br> Dimensioni: 44.00 × 145.00 × 153.00 mm<br>

  <p class=""mt-2"">
    AVA BUCATO A MANO E2 GR.380</p>
</div>


The xpath that I have written is not working I want to select Node that contains text Brand:. Can someone tell me my mistake?
",3k,"
            1
        ","['\nYour XPath,\n//*[contains(text(), ""Brand:"")]\n\nin XPath 1.0 will select all elements whose first text node child contains a ""Brand:"" substring.  In XPath 2.0 it is an error to call contains() with a sequence of more than one item as the first argument.\nThis XPath,\n//*[text()[contains(., ""Brand:"")]]\n\nwill select all elements with a text node child whose string value contains a ""Brand:"" substring.\nSee also\n\nXPath 1.0 vs 2.0+ different contains() behavior explanation\nTesting text() nodes vs string values in XPath\n\n']"
Scraping YouTube links from a webpage,"
I've been trying to scrape YouTube links from a webpage, but nothing has worked.
This is a picture of what I've been trying to scrape.:

This is the code I tried most recently:
youtube_link = soup.find(""a"", class_=""ytp-title-link yt-uix-sessionlink"")

And this is the link to the website the YouTube link is in: https://www.electronic-festivals.com/event/i-am-hardstyle-germany
",2k,"
            1
        ","['\nMost of the youtube links are within an iframe and javascript also needs to run. Try using selenium. The following extracts any src or href containing youtube. I only enter the key iframe hosting the youtube clip. You could loop all iframes checking.\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.by import By\n\ndef addItems(links, final):\n    for link in links:\n        ref = link.get_attribute(\'src\') if link.get_attribute(\'src\') is not None else link.get_attribute(\'href\')\n        final.append(ref)\n    return final\n\nurl = ""https://www.electronic-festivals.com/event/i-am-hardstyle-germany"" \ndriver = webdriver.Chrome()\ndriver.get(url)\ndriver.switch_to.frame(driver.find_element_by_css_selector(\'.media-youtube-player\'))\nfinal = []\n\ntry:\n    links = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ""[href*=youtube] , [src*=youtube]"")))\n    addItems(links, final)\nexcept:\n    pass\nfinally:\n    driver.switch_to.default_content()\n\nlinks = driver.find_elements_by_css_selector(\'[href*=youtube] , [src*=youtube]\')\naddItems(links, final)\n\nfor link in set(final):\n    print(link)\n\ndriver.quit()\n\n', '\nIf you mean by scraping downloading, try\npip install youtube-dl\n\nin your shell.\n']"
How does a site like kayak.com aggregate content? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



Greetings,
I've been toying with an idea for a new project and was wondering if anyone has any idea on how a service like Kayak.com is able to aggregate data from so many sources so quickly and accurately. More specifically, do you think Kayak.com is interacting with APIs or are they crawling/scraping airline and hotel websites in order to fulfill user requests? I know there isn't one right answer for this sort of thing but I'm curious to know what others think would be a good way to go about this. If it helps, pretend you are going to create kayak.com tomorrow ... where is your data coming from?
",79k,"
            84
        ","['\nI\'m working in travel industry as a software architect / project lead on the precisely kind of project you describe - in our region we work with suppliers directly, but for outgoing we connect to several aggregators.\nTo answer your question... some data you have, some you get in various ways, and some you have to torture and twist until it confesses.\nWhat\'s your angle?\nThe questions you have to ask are... Do you want to sell advertising like Kayak or do you take a cut like Expedia? Are you into search or into selling travel services? Do you target niche (for example, just air travel) or everything (accommodation, airlines, rent-a-car, additional services like transport/sightseeing/conferences etc)? Do you target region (US or part of US) or the world? How deep do you go - do you just show several sites on a single screen, or do you bundle different services together and package them dynamically?\nGetting the data\nIf you\'re going with Kayak business model, you technically don\'t need site\'s permission... but a lot of sites have affiliate programs with IFrames or other simple ways to direct the customer to their site. On the plus side, you don\'t have to deal with payments/complaints and travelers themselves. As for the cons... if you want to compare prices yourself and present the cheapest option to the user, you\'ll have to integrate on a deeper level, and that means APIs and web scraping.\nAs for web scraping... avoid it. It sucks. Really. Just don\'t do it. Trust me on this one. For example, some things like lowcosters you can\'t get without web scraping. Low cost airlines live from value added services. If the user doesn\'t see their website, they don\'t sell extra stuff, and they don\'t earn anything. Therefore, they don\'t have affiliates, they don\'t offer APIs, and they change their site layout almost constantly. However, there are companies which earn a living by web scraping lowcoster\'s sites and wrapping them into nice APIs. If you can afford them, you can give your users cost-comparison of low cost flights and that\'s huge.\nOn the other hand, there are ""normal"" carriers which offer APIs. It\'s not that big of a problem to get to airlines since they\'re all united under IATA; basically, you buy from IATA, and IATA distributes the money to carriers. However, you probably don\'t want to connect directly to carrier network. They have web services and SOAP these days, but believe me when I say that there are SOAP protocols which are just an insanely thin wrappers around a text prompt through which you can interact with a mainframe with an 80es-style protocol (think of a Unix prompt where you\'re billed per command; and it takes about 20 commands to do one search). That\'s why you probably want to connect to somebody a bit more down the food chain, with a better API.\nAirlines are thus on both extremes of Gaussian curve; on one side are individual suppliers, and on the other highly centralized systems where you implement one API and you\'re able to fly anywhere in the world. Accommodation and the rest of travel products are in between. There are several big players which aggregate hotels, and a ton of small suppliers with a lot of aggregators which cover only part of a spectrum. For example, you can rent a lighthouse and it\'s even not that expensive - but you won\'t be able to compare the prices of different lighthouses in one place.\nIf you\'re into Kayak business model, you\'ll probably end up scraping websites. If you\'re into integrating different providers, you\'ll often work with APIs, some of which are pretty good, and most of which are tolerable. I haven\'t worked with RSS but there\'s not a lot of difference between RSS and web scraping. There is also a fourth option not mentioned in Jeff\'s answer... the one where you get your data nightly, for example .CSV files through FTP and similar.\nLife sucks (mini-rant)\nAnd then there\'s complexity. The more value you want to add, the more complexity you\'ll have to handle. Can you search accommodations which allow pets? For a hostel which is located less than 5 km from the town center? Are you combining flights, and are you able to guarantee that the traveler will have enough time to get from one airport to another... can you sell the transport in advance? A famous cellist doesn\'t want to part from his precious 18th century cello; can you sell him another seat for the cello (yep, not making this one up)?\nWant to compare prices? Sure, the room is EUR 30 per night. But you can either get one double for 30 and one single for 20, or you can get one extra bed in a double and get 70% off for third person. But only if it\'s a child under 12 years of age; our extra beds are not for adults. And you don\'t get the price for extra bed in search results - only when you calculate the final price.\nAnd don\'t even get me started on dynamic packaging. Want to sell accommodation + rent-a-car? No problem; integrate with two different providers, and off you go... manually updating list of locations in the city (from rent-a-car provider) to match with hotels (from accommodation provider, who gives you only the city for each hotel). Of course, provided that you\'ve already matched the list of cities from the two, since there is no international standard for city codes.\nUnlike a lot of other industries which have many products, travel industry has many very complex products. Amazon has it easy; selling books and selling potatoes, it\'s the same thing; you can even ship them in the same box. They combine easily and aren\'t assembled from many parts. :)\nP.S. Linking to an interesting recent thread on Hacker News with some insider info regarding flights.\nP.P.S. Recently stumbled on a great albeit rather old blogpost on IATA\'s NDC protocol with overview of how travel industry is connected and a history lesson how this came to be.\n', '\nThey use a software package like ITA Software, which is one of the companies Google is in the process of picking up.\n', ""\nOnly 3 ways I know of to get data from websites.\nRSS Feeds - We use rss feeds a lot at my company to integrate existing site's data with our apps.  It's fast and most sites already have an RSS feed available.  The problem with this is not all sites implement the RSS standard properly so if you're pulling data from many RSS feeds across many sites then make sure you write your code so that you can add exceptions and filters easily. \nAPIs - These are nice if they are designed well and have all the information you need, however that's not always the case, plus if the sites are not using a standard api format then you'll have to support multiple API's.\nWeb Scraping - This method would be the most unreliable as well as the most expensive to maintain.  But if you're left with nothing else it can be done.  \n"", ""\nThis article says that Kayak was asked to stop scrapping a certain airlines page. That leads me to believe that they probably do scraping on sites that they don't have a relationship with (and a data feed that comes with that relationship).\n"", '\nTravelport offer a product called ""Universal API"" which connects to flights and hotels and car rental companies and copes with package deals and all the various complexities to do with taxes and exchange rates:\nhttps://developer.travelport.com/app/developer-network/resource-centre-uapi\nI\'ve just started using it and it seems fine so far.  The queries are a little slow, but then so is every query on every OTA (Online travel agent)\'s site.\n', ""\nThere's two good APIs I've found from flight comparison websites recently\nThere's one from Wego, and one from Skyscanner. Both seem to have a good range and breadth of data from a number of airlines and good documentation too.\nWego pays each time a user clicks from your app to a booking website and Skyscanner pay affiliates 50% of 'revenue' (I assume that means the commission they make from airlines)\n"", ""\nThis is an old post but I thought I'd just add. I'm a data architect who works for a company that feeds these travel sites with content. This company enters into contracts with many hotel brands, individual hotels and other content providers. We aggregate this information then pass it onto the different channels. They then aggregate again in to their system.\nThe Large GDS systems are also content providers.\nAggregation is done by many methods... matching algorithms(in-house) and keys. Being an aggregation service, we need to communicate on the client level.\nHope this helps! cheers!\n""]"
"Screen scraping: getting around ""HTTP Error 403: request disallowed by robots.txt""","
Is there a way to get around the following?
httperror_seek_wrapper: HTTP Error 403: request disallowed by robots.txt

Is the only way around this to contact the site-owner (barnesandnoble.com).. i'm building a site that would bring them more sales, not sure why they would deny access at a certain depth.
I'm using mechanize and BeautifulSoup on Python2.6.
hoping for a work-around
",48k,"
            54
        ","['\noh you need to ignore the robots.txt\nbr = mechanize.Browser()\nbr.set_handle_robots(False)\n\n', '\nYou can try lying about your user agent (e.g., by trying to make believe you\'re a human being and not a robot) if you want to get in possible legal trouble with Barnes & Noble.  Why not instead get in touch with their business development department and convince them to authorize you specifically?  They\'re no doubt just trying to avoid getting their site scraped by some classes of robots such as price comparison engines, and if you can convince them that you\'re not one, sign a contract, etc, they may well be willing to make an exception for you.\nA ""technical"" workaround that just breaks their policies as encoded in robots.txt is a high-legal-risk approach that I would never recommend.  BTW, how does their robots.txt read?\n', ""\nThe code to make a correct request:\nbr = mechanize.Browser()\nbr.set_handle_robots(False)\nbr.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\nresp = br.open(url)\nprint resp.info()  # headers\nprint resp.read()  # content\n\n"", ""\nMechanize automatically follows robots.txt, but it can be disabled assuming you have permission, or you have thought the ethics through ..\nSet a flag in your browser: \nbrowser.set_handle_equiv(False) \n\nThis ignores robots.txt.\nAlso, make sure you throttle your requests, so you don't put too much load on their site. (Note, this also makes it less likely that they will detect and ban you).\n"", ""\nThe error you're receiving is not related to the user agent.  mechanize by default checks robots.txt directives automatically when you use it to navigate to a site.  Use the .set_handle_robots(false) method of mechanize.browser to disable this behavior.\n"", ""\nSet your User-Agent header to match some real IE/FF User-Agent.\nHere's my IE8 useragent string:\nMozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; AskTB5.6)\n\n"", '\nWithout debating the ethics of this you could modify the headers to look like the googlebot for example, or is the googlebot blocked as well?\n', '\nAs it seems, you have to do less work to bypass robots.txt, at least says this article. So you might have to remove some code to ignore the filter.\n']"
BeautifulSoup: How do I extract all the <li>s from a list of <ul>s that contains some nested <ul>s?,"
I'm a newbie programmer trying to jump in to Python by building a script that scrapes http://en.wikipedia.org/wiki/2000s_in_film and extracts a list of ""Movie Title (Year)"".
My HTML source looks like:
<h3>Header3 (Start here)</h3>
<ul>
    <li>List items</li>
    <li>Etc...</li>
</ul>
<h3>Header 3</h3>
<ul>
    <li>List items</li>
    <ul>
        <li>Nested list items</li>
        <li>Nested list items</li></ul>
    <li>List items</li>
</ul>
<h2>Header 2 (end here)</h2>

I'd like all the li tags following the first h3 tag and stopping at the next h2 tag, including all nested li tags.
firstH3 = soup.find('h3')

...correctly finds the place I'd like to start.
firstH3 = soup.find('h3') # Start here
uls = []
for nextSibling in firstH3.findNextSiblings():
    if nextSibling.name == 'h2':
        break
    if nextSibling.name == 'ul':
        uls.append(nextSibling)

...gives me a list uls, each with li contents that I need.
Excerpt of the uls list:
<ul>
...
    <li><i><a href=""/wiki/Agent_Cody_Banks"" title=""Agent Cody Banks"">Agent Cody Banks</a></i> (2003)</li>
    <li><i><a href=""/wiki/Agent_Cody_Banks_2:_Destination_London"" title=""Agent Cody Banks 2: Destination London"">Agent Cody Banks 2: Destination London</a></i> (2004)</li>
    <li>Air Bud series:
        <ul>
            <li><i><a href=""/wiki/Air_Bud:_World_Pup"" title=""Air Bud: World Pup"">Air Bud: World Pup</a></i> (2000)</li>
            <li><i><a href=""/wiki/Air_Bud:_Seventh_Inning_Fetch"" title=""Air Bud: Seventh Inning Fetch"">Air Bud: Seventh Inning Fetch</a></i> (2002)</li>
            <li><i><a href=""/wiki/Air_Bud:_Spikes_Back"" title=""Air Bud: Spikes Back"">Air Bud: Spikes Back</a></i> (2003)</li>
            <li><i><a href=""/wiki/Air_Buddies"" title=""Air Buddies"">Air Buddies</a></i> (2006)</li>
        </ul>
    </li>
    <li><i><a href=""/wiki/Akeelah_and_the_Bee"" title=""Akeelah and the Bee"">Akeelah and the Bee</a></i> (2006)</li>
...
</ul>

But I'm unsure of where to go from here.

Update:
Final Code:
lis = []
    for ul in uls:
        for li in ul.findAll('li'):
            if li.find('ul'):
                break
            lis.append(li)

    for li in lis:
        print li.text.encode(""utf-8"")

The if...break throws out the LI's that contain UL's since the nested LI's are now duplicated.
Print output is now:


102 Dalmatians(2000)
10th & Wolf(2006)
11:14(2006)
12:08 East of Bucharest(2006)
13 Going on 30(2004)
1408(2007)
...


",79k,"
            35
        ","[""\n.findAll() works for nested li elements:\nfor ul in uls:\n    for li in ul.findAll('li'):\n        print(li)\n\nOutput:\n<li>List items</li>\n<li>Etc...</li>\n<li>List items</li>\n<li>Nested list items</li>\n<li>Nested list items</li>\n<li>List items</li>\n\n"", ""\nA list comprehension could work, too.\nlis = [li for ul in uls for li in ul.findAll('li')]\n\n"", '\nimport requests\nfrom bs4 import BeautifulSoup\nr = requests.get(""https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_list_test"")\nsoup =   BeautifulSoup(r.content,""lxml"")\nw3schollsList = soup.find_all(\'body\')\nfor w3scholl in w3schollsList:\n    ulList = w3scholl.find_all(\'li\')\n    for li in ulList:\n        print(li)\n\nNote: here is to get the ""li"" inside the div we made\n']"
"XPath to Parse ""SRC"" from IMG tag?","
Right now I successfully grabbed the full  element from an HTML page with this:
//img[@class='photo-large']

for example it would return this: 
<img src=""http://example.com/img.jpg"" class='photo-large' />

But I only need the SRC url (http://example.com/img.jpg). Any help?
",69k,"
            29
        ","[""\nYou are so close to answering this yourself that I am somewhat reluctant to answer it for you. However, the following XPath should provide what you want (provided the source is XHTML, of course).\n//img[@class='photo-large']/@src\n\nFor further tips, check out W3 Schools. They have excellent tutorials on such things and a great reference too.\n"", '\nUsing Hpricot this works:\ndoc.at(\'//img[@class=""photo-large""]\')[\'src\']\n\nIn case you have more than one image, the following gives an array:\ndoc.search(\'//img[@class=""photo-large""]\').map do |e| e[\'src\'] end\n\nHowever, Nokogiri is many times faster and it “can be used as a drop in replacement” for Hpricot.\nHere the version for Nokogiri, in which this XPath for selecting attributes works:\ndoc.at(\'//img[@class=""photo-large""]/@src\').to_s\n\nor for many images:\ndoc.search(\'//img[@class=""photo-large""]/@src\').to_a\n\n', '\n//img/@src\nyou can just go with this if you want a link of the image.\nexample:\n<img alt="""" class=""avatar width-full rounded-2"" height=""230"" src=""https://avatars3.githubusercontent.com/...;s=460"" width=""230"">\n\n']"
"Scrapy, scraping data inside a Javascript","
I am using scrapy to screen scrape data from a website. However, the data I wanted wasn't inside the html itself, instead, it is from a javascript. So, my question is:
How to get the values (text values) of such cases? 
This, is the site I'm trying to screen scrape:
https://www.mcdonalds.com.sg/locate-us/
Attributes I'm trying to get:
Address, Contact, Operating hours.
If you do a ""right click"", ""view source"" inside a chrome browser you will see that such values aren't available itself in the HTML.

Edit
Sry paul, i did what you told me to, found the admin-ajax.php and saw the body but, I'm really stuck now.
How do I retrieve the values from the json object and store it into a variable field of my own? It would be good, if you could share how to do just one attribute for the public and to those who just started scrapy as well.
Here's my code so far
Items.py 
class McDonaldsItem(Item):
name = Field()
address = Field()
postal = Field()
hours = Field()

McDonalds.py
from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector
import re

from fastfood.items import McDonaldsItem

class McDonaldSpider(BaseSpider):
name = ""mcdonalds""
allowed_domains = [""mcdonalds.com.sg""]
start_urls = [""https://www.mcdonalds.com.sg/locate-us/""]

def parse_json(self, response):

    js = json.loads(response.body)
    pprint.pprint(js)

Sry for long edit, so in short, how do i store the json value into my attribute? for eg
***item['address'] = * how to retrieve ****
P.S, not sure if this helps but, i run these scripts on the cmd line using
scrapy crawl mcdonalds -o McDonalds.json -t json ( to save all my data into a json file )
I cannot stress enough on how thankful i feel. I know it's kind of unreasonable to ask this of u, will totally be okay even if you dont have time for this.
",21k,"
            23
        ","['\n(I posted this to scrapy-users mailing list but by Paul\'s suggestion I\'m posting it here as it complements the answer with the shell command interaction.)\nGenerally, websites that use a third party service to render some data visualization (map, table, etc) have to send the data somehow, and in most cases this data is accessible from the browser.\nFor this case, an inspection (i.e. exploring the requests made by the browser) shows that the data is loaded from a POST request to https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php\nSo, basically you have there all the data you want in a nice json format ready for consuming. \nScrapy provides the shell command which is very convenient to thinker with the website before writing the spider:\n$ scrapy shell https://www.mcdonalds.com.sg/locate-us/\n2013-09-27 00:44:14-0400 [scrapy] INFO: Scrapy 0.16.5 started (bot: scrapybot)\n...\n\nIn [1]: from scrapy.http import FormRequest\n\nIn [2]: url = \'https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php\'\n\nIn [3]: payload = {\'action\': \'ws_search_store_location\', \'store_name\':\'0\', \'store_area\':\'0\', \'store_type\':\'0\'}\n\nIn [4]: req = FormRequest(url, formdata=payload)\n\nIn [5]: fetch(req)\n2013-09-27 00:45:13-0400 [default] DEBUG: Crawled (200) <POST https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php> (referer: None)\n...\n\nIn [6]: import json\n\nIn [7]: data = json.loads(response.body)\n\nIn [8]: len(data[\'stores\'][\'listing\'])\nOut[8]: 127\n\nIn [9]: data[\'stores\'][\'listing\'][0]\nOut[9]: \n{u\'address\': u\'678A Woodlands Avenue 6<br/>#01-05<br/>Singapore 731678\',\n u\'city\': u\'Singapore\',\n u\'id\': 78,\n u\'lat\': u\'1.440409\',\n u\'lon\': u\'103.801489\',\n u\'name\': u""McDonald\'s Admiralty"",\n u\'op_hours\': u\'24 hours<br>\\r\\nDessert Kiosk: 0900-0100\',\n u\'phone\': u\'68940513\',\n u\'region\': u\'north\',\n u\'type\': [u\'24hrs\', u\'dessert_kiosk\'],\n u\'zip\': u\'731678\'}\n\nIn short: in your spider you have to return the FormRequest(...) above, then in the callback load the json object from response.body and finally for each store\'s data in the list data[\'stores\'][\'listing\'] create an item with the wanted values.\nSomething like this:\nclass McDonaldSpider(BaseSpider):\n    name = ""mcdonalds""\n    allowed_domains = [""mcdonalds.com.sg""]\n    start_urls = [""https://www.mcdonalds.com.sg/locate-us/""]\n\n    def parse(self, response):\n        # This receives the response from the start url. But we don\'t do anything with it.\n        url = \'https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php\'\n        payload = {\'action\': \'ws_search_store_location\', \'store_name\':\'0\', \'store_area\':\'0\', \'store_type\':\'0\'}\n        return FormRequest(url, formdata=payload, callback=self.parse_stores)\n\n    def parse_stores(self, response):\n        data = json.loads(response.body)\n        for store in data[\'stores\'][\'listing\']:\n            yield McDonaldsItem(name=store[\'name\'], address=store[\'address\'])\n\n', '\nWhen you open https://www.mcdonalds.com.sg/locate-us/ in your browser of choice, open up the ""inspect"" tool (hopefully it has one, e.g. Chrome or Firefox), and look for the ""Network"" tab.\nYou can further filter for ""XHR"" (XMLHttpRequest) events, and you\'ll see a POST request to https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php with this body\naction=ws_search_store_location&store_name=0&store_area=0&store_type=0\n\nThe response to that POST request is a JSON object with all the information you want \nimport json\nimport pprint\n...\nclass MySpider(BaseSpider):\n...\n    def parse_json(self, response):\n\n        js = json.loads(response.body)\n        pprint.pprint(js)\n\nThis would output something like:\n{u\'flagicon\': u\'https://www.mcdonalds.com.sg/wp-content/themes/mcd/images/storeflag.png\',\n u\'stores\': {u\'listing\': [{u\'address\': u\'678A Woodlands Avenue 6<br/>#01-05<br/>Singapore 731678\',\n                           u\'city\': u\'Singapore\',\n                           u\'id\': 78,\n                           u\'lat\': u\'1.440409\',\n                           u\'lon\': u\'103.801489\',\n                           u\'name\': u""McDonald\'s Admiralty"",\n                           u\'op_hours\': u\'24 hours<br>\\r\\nDessert Kiosk: 0900-0100\',\n                           u\'phone\': u\'68940513\',\n                           u\'region\': u\'north\',\n                           u\'type\': [u\'24hrs\', u\'dessert_kiosk\'],\n                           u\'zip\': u\'731678\'},\n                          {u\'address\': u\'383 Bukit Timah Road<br/>#01-09B<br/>Alocassia Apartments<br/>Singapore 259727\',\n                           u\'city\': u\'Singapore\',\n                           u\'id\': 97,\n                           u\'lat\': u\'1.319752\',\n                           u\'lon\': u\'103.827398\',\n                           u\'name\': u""McDonald\'s Alocassia"",\n                           u\'op_hours\': u\'Daily: 0630-0100\',\n                           u\'phone\': u\'68874961\',\n                           u\'region\': u\'central\',\n                           u\'type\': [u\'24hrs_weekend\',\n                                     u\'drive_thru\',\n                                     u\'mccafe\'],\n                           u\'zip\': u\'259727\'},\n\n                        ...\n                          {u\'address\': u\'60 Yishuan Avenue 4 <br/>#01-11<br/><br/>Singapore 769027\',\n                           u\'city\': u\'Singapore\',\n                           u\'id\': 1036,\n                           u\'lat\': u\'1.423924\',\n                           u\'lon\': u\'103.840628\',\n                           u\'name\': u""McDonald\'s Yishun Safra"",\n                           u\'op_hours\': u\'24 hours\',\n                           u\'phone\': u\'67585632\',\n                           u\'region\': u\'north\',\n                           u\'type\': [u\'24hrs\',\n                                     u\'drive_thru\',\n                                     u\'live_screening\',\n                                     u\'mccafe\',\n                                     u\'bday_party\'],\n                           u\'zip\': u\'769027\'}],\n             u\'region\': u\'all\'}}\n\nI\'ll leave you to extract the fields you want.\nIn the FormRequest() you send with Scrapy you probably need to add a ""X-Requested-With: XMLHttpRequest"" header (your browser sends that if you look at the request headers in the inspect tool)\n']"
Puppeteer waitForSelector on multiple selectors,"
I have Puppeteer controlling a website with a lookup form that can either return a result or a ""No records found"" message. How can I tell which was returned? 
waitForSelector seems to wait for only one at a time, while waitForNavigation doesn't seem to work because it is returned using Ajax.
I am using a try catch, but it is tricky to get right and slows everything way down.
try {
    await page.waitForSelector(SELECTOR1,{timeout:1000}); 
}
catch(err) { 
    await page.waitForSelector(SELECTOR2);
}

",25k,"
            23
        ","[""\nMaking any of the elements exists\nYou can use querySelectorAll and waitForFunction together to solve this problem. Using all selectors with comma will return all nodes that matches any of the selector.\nawait page.waitForFunction(() => \n  document.querySelectorAll('Selector1, Selector2, Selector3').length\n);\n\nNow this will only return true if there is some element, it won't return which selector matched which elements.\n"", ""\nhow about using Promise.race() like something I did in the below code snippet, and don't forget the { visible: true } option in page.waitForSelector() method.\npublic async enterUsername(username:string) : Promise<void> {\n    const un = await Promise.race([\n        this.page.waitForSelector(selector_1, { timeout: 4000, visible: true })\n        .catch(),\n        this.page.waitForSelector(selector_2, { timeout: 4000, visible: true })\n        .catch(),\n    ]);\n\n    await un.focus();\n    await un.type(username);\n}\n\n"", ""\nAn alternative and simple solution would be to approach this from a more CSS perspective.  waitForSelector seems to follow the CSS selector list rules. So essentially you can select multiple CSS elements by just using a comma.\ntry {    \n    await page.waitForSelector('.selector1, .selector2',{timeout:1000})\n} catch (error) {\n    // handle error\n}\n\n"", '\nUsing Md. Abu Taher\'s suggestion, I ended up with this:\n// One of these SELECTORs should appear, we don\'t know which\nawait page.waitForFunction((sel) => { \n    return document.querySelectorAll(sel).length;\n},{timeout:10000},SELECTOR1 + "", "" + SELECTOR2); \n\n// Now see which one appeared:\ntry {\n    await page.waitForSelector(SELECTOR1,{timeout:10});\n}\ncatch(err) {\n    //check for ""not found"" \n    let ErrMsg = await page.evaluate((sel) => {\n        let element = document.querySelector(sel);\n        return element? element.innerHTML: null;\n    },SELECTOR2);\n    if(ErrMsg){\n        //SELECTOR2 found\n    }else{\n        //Neither found, try adjusting timeouts until you never get this...\n    }\n};\n//SELECTOR1 found\n\n', ""\nI had a similar issue and went for this simple solution:\nhelpers.waitForAnySelector = (page, selectors) => new Promise((resolve, reject) => {\n  let hasFound = false\n  selectors.forEach(selector => {\n    page.waitFor(selector)\n      .then(() => {\n        if (!hasFound) {\n          hasFound = true\n          resolve(selector)\n        }\n      })\n      .catch((error) => {\n        // console.log('Error while looking up selector ' + selector, error.message)\n      })\n  })\n})\n\nAnd then to use it:\nconst selector = await helpers.waitForAnySelector(page, [\n  '#inputSmsCode', \n  '#buttonLogOut'\n])\n\nif (selector === '#inputSmsCode') {\n  // We need to enter the 2FA sms code. \n} else if (selector === '#buttonLogOut') {\n  // We successfully logged in\n}\n\n"", '\nIn puppeteer you can simply use multiple selectors separated by coma like this:\nconst foundElement = await page.waitForSelector(\'.class_1, .class_2\');\n\nThe returned element will be an elementHandle of the first element found in the page.\nNext if you want to know which element was found you can get the class name like so:\nconst className = await page.evaluate(el => el.className, foundElement);\n\nin your case a code similar to this should work:\nconst foundElement = await page.waitForSelector([SELECTOR1,SELECTOR2].join(\',\'));\nconst responseMsg = await page.evaluate(el => el.innerText, foundElement);\nif (responseMsg == ""No records found""){ // Your code here }\n\n', ""\nOne step further using Promise.race() by wrapping it and just check index for further logic:\n// Typescript\nexport async function racePromises(promises: Promise<any>[]): Promise<number> {\n  const indexedPromises: Array<Promise<number>> = promises.map((promise, index) => new Promise<number>((resolve) => promise.then(() => resolve(index))));\n  return Promise.race(indexedPromises);\n}\n\n// Javascript\nexport async function racePromises(promises) {\n  const indexedPromises = promises.map((promise, index) => new Promise((resolve) => promise.then(() => resolve(index))));\n  return Promise.race(indexedPromises);\n}\n\nUsage:\nconst navOutcome = await racePromises([\n  page.waitForSelector('SELECTOR1'),\n  page.waitForSelector('SELECTOR2')\n]);\nif (navigationOutcome === 0) {\n  //logic for 'SELECTOR1'\n} else if (navigationOutcome === 1) {\n  //logic for 'SELECTOR2'\n}\n\n\n\n"", ""\nCombining some elements from above into a helper method, I've built a command that allows me to create multiple possible selector outcomes and have the first to resolve be handled.\n\n\n/**\r\n * @typedef {import('puppeteer').ElementHandle} PuppeteerElementHandle\r\n * @typedef {import('puppeteer').Page} PuppeteerPage\r\n */\r\n\r\n/** Description of the function\r\n  @callback OutcomeHandler\r\n  @async\r\n  @param {PuppeteerElementHandle} element matched element\r\n  @returns {Promise<*>} can return anything, will be sent to handlePossibleOutcomes\r\n*/\r\n\r\n/**\r\n * @typedef {Object} PossibleOutcome\r\n * @property {string} selector The selector to trigger this outcome\r\n * @property {OutcomeHandler} handler handler will be called if selector is present\r\n */\r\n\r\n/**\r\n * Waits for a number of selectors (Outcomes) on a Puppeteer page, and calls the handler on first to appear,\r\n * Outcome Handlers should be ordered by preference, as if multiple are present, only the first occuring handler\r\n * will be called.\r\n * @param {PuppeteerPage} page Puppeteer page object\r\n * @param {[PossibleOutcome]} outcomes each possible selector, and the handler you'd like called.\r\n * @returns {Promise<*>} returns the result from outcome handler\r\n */\r\nasync function handlePossibleOutcomes(page, outcomes)\r\n{\r\n  var outcomeSelectors = outcomes.map(outcome => {\r\n    return outcome.selector;\r\n  }).join(', ');\r\n  return page.waitFor(outcomeSelectors)\r\n  .then(_ => {\r\n    let awaitables = [];\r\n    outcomes.forEach(outcome => {\r\n      let await = page.$(outcome.selector)\r\n      .then(element => {\r\n        if (element) {\r\n          return [outcome, element];\r\n        }\r\n        return null;\r\n      });\r\n      awaitables.push(await);\r\n    });\r\n    return Promise.all(awaitables);\r\n  })\r\n  .then(checked => {\r\n    let found = null;\r\n    checked.forEach(check => {\r\n      if(!check) return;\r\n      if(found) return;\r\n      let outcome = check[0];\r\n      let element = check[1];\r\n      let p = outcome.handler(element);\r\n      found = p;\r\n    });\r\n    return found;\r\n  });\r\n}\n\n\nTo use it, you just have to call and provide an array of Possible Outcomes and their selectors / handlers:\n await handlePossibleOutcomes(page, [\n    {\n      selector: '#headerNavUserButton',\n      handler: element => {\n        console.log('Logged in',element);\n        loggedIn = true;\n        return true;\n      }\n    },\n    {\n      selector: '#email-login-password_error',\n      handler: element => {\n        console.log('password error',element);\n        return false;\n      }\n    }\n  ]).then(result => {\n    if (result) {\n      console.log('Logged in!',result);\n    } else {\n      console.log('Failed :(');\n    }\n  })\n\n"", ""\nI just started with Puppeteer, and have encountered the same issue, therefore I wanted to make a custom function which fulfills the same use-case.\nThe function goes as follows:\nasync function waitForMySelectors(selectors, page){\n    for (let i = 0; i < selectors.length; i++) {\n        await page.waitForSelector(selectors[i]);\n    }\n}\n\nThe first parameter in the function recieves an array of selectors, the second parameter is the page that we're inside to preform the waiting process with.\ncalling the function as the example below:\nvar SelectorsArray = ['#username', '#password'];\nawait waitForMySelectors(SelectorsArray, page);\n\nthough I have not preformed any tests on it yet, it seems functional.\n"", '\nIf you want to wait for the first of multiple selectors and get the matched element(s), you can start with waitForFunction:\nconst matches = await page.waitForFunction(() => {\n  const matches = [...document.querySelectorAll(YOUR_SELECTOR)];\n  return matches.length ? matches : null;\n});\n\nwaitForFunction will return an ElementHandle but not an array of them. If you only need native DOM methods, it\'s not necessary to get handles. For example, to get text from this array:\nconst contents = await matches.evaluate(els => els.map(e => e.textContent));\n\nIn other words, matches acts a lot like the array passed to $$eval by Puppeteer.\nOn the other hand, if you do need an array of handles, the following demonstration code makes the conversion and shows the handles being used as normal:\nconst puppeteer = require(""puppeteer""); // ^16.2.0\n\nconst html = `\n<!DOCTYPE html>\n<html>\n<head>\n<style>\nh1 {\n  display: none;\n}\n</style>\n</head>\n<body>\n<script>\nsetTimeout(() => {\n\n  // add initial batch of 3 elements\n  for (let i = 0; i < 3; i++) {\n    const h1 = document.createElement(""button"");\n    h1.textContent = \\`first batch #\\${i + 1}\\`;\n    h1.addEventListener(""click"", () => {\n      h1.textContent = \\`#\\${i + 1} clicked\\`;\n    });\n    document.body.appendChild(h1);\n  }\n\n  // add another element 1 second later to show it won\'t appear in the first batch\n  setTimeout(() => {\n    const h1 = document.createElement(""h1"");\n    h1.textContent = ""this won\'t be found in the first batch"";\n    document.body.appendChild(h1);\n  }, 1000);\n\n}, 3000); // delay before first batch of elements are added\n</script>\n</body>\n</html>\n`;\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch({headless: true});\n  const [page] = await browser.pages();\n  await page.setContent(html);\n\n  const matches = await page.waitForFunction(() => {\n    const matches = [...document.querySelectorAll(""button"")];\n    return matches.length ? matches : null;\n  });\n  const length = await matches.evaluate(e => e.length);\n  const handles = await Promise.all([...Array(length)].map((e, i) =>\n    page.evaluateHandle((m, i) => m[i], matches, i)\n  ));\n  await handles[1].click(); // show that the handles work\n  const contents = await matches.evaluate(els => els.map(e => e.textContent));\n  console.log(contents);\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close())\n;\n\nUnfortunately, it\'s a bit verbose, but this can be made into a helper.\nSee also Wait for first visible among multiple elements matching selector if you\'re interested in integrating the {visible: true} option.\n', ""\nPuppeteer methods might throw errors if they are unable to fufill a request. For example, page.waitForSelector(selector[, options]) might fail if the selector doesn't match any nodes during the given timeframe.\nFor certain types of errors Puppeteer uses specific error classes. These classes are available via require('puppeteer/Errors').\nList of supported classes:\nTimeoutError\nAn example of handling a timeout error:\nconst {TimeoutError} = require('puppeteer/Errors');\n\n// ...\n\ntry {\n  await page.waitForSelector('.foo');\n} catch (e) {\n  if (e instanceof TimeoutError) {\n    // Do something if this is a timeout.\n  }\n}\n\n""]"
Scraping in Python - Preventing IP ban,"
I am using Python to scrape pages. Until now I didn't have any complicated issues.
The site that I'm trying to scrape uses a lot of security checks and have some mechanism to prevent scraping. 
Using Requests and lxml I was able to scrape about 100-150 pages before getting banned by IP. Sometimes I even get ban on first request (new IP, not used before, different C block). I have tried with spoofing headers, randomize time between requests, still the same.
I have tried with Selenium and I got much better results. With Selenium I was able to scrape about 600-650 pages before getting banned. Here I have also tried to randomize requests (between 3-5 seconds, and make time.sleep(300) call on every 300th request). Despite that, Im getting banned.
From here I can conclude that site have some mechanism where they ban IP if it requested more than X pages in one open browser session or something like that.
Based on your experience what else should I try?
Will closing and opening browser in Selenium help (for example after every 100th requests close and open browser). I was thinking about trying with proxies but there are about million of pages and it will be very expansive.
",36k,"
            19
        ","['\nIf you would switch to the Scrapy web-scraping framework, you would be able to reuse a number of things that were made to prevent and tackle banning:\n\nthe built-in AutoThrottle extension:\n\n\nThis is an extension for automatically throttling crawling speed based on load of both the Scrapy server and the website you are crawling.\n\n\nrotating user agents with scrapy-fake-useragent middleware:\n\n\nUse a random User-Agent provided by fake-useragent every request\n\n\nrotating IP addresses:\n\nSetting Scrapy proxy middleware to rotate on each request\nscrapy-proxies\n\nyou can also run it via local proxy & TOR:\n\nScrapy: Run Using TOR and Multiple Agents\n\n\n', '\nI had this problem too. I used urllib with tor in python3.\n\ndownload and install tor browser\ntesting tor\n\nopen terminal and type:\ncurl --socks5-hostname localhost:9050 <http://site-that-blocked-you.com>\n\nif you see result it\'s worked.\n\nNow we should test in python. Now run this code\n\nimport socks\nimport socket\nfrom urllib.request import Request, urlopen\nfrom bs4 import BeautifulSoup\n\n#set socks5 proxy to use tor\n\nsocks.set_default_proxy(socks.SOCKS5, ""localhost"", 9050)\nsocket.socket = socks.socksocket\nreq = Request(\'http://check.torproject.org\', headers={\'User-Agent\': \'Mozilla/5.0\', })\nhtml = urlopen(req).read()\nsoup = BeautifulSoup(html, \'html.parser\')\nprint(soup(\'title\')[0].get_text())\n\nif you see \n\nCongratulations. This browser is configured to use Tor. \n\nit worked in python too and this means you are using tor for web scraping.\n', '\nYou could use proxies.\nYou can buy several hundred IPs for very cheap, and use selenium as you previously have done.\nFurthermore I suggest varying the browser your use and other user-agent parameters.\nYou could iterate over using a single IP address to load only x number of pages and stopping prior to getting banned.\ndef load_proxy(PROXY_HOST,PROXY_PORT):\n        fp = webdriver.FirefoxProfile()\n        fp.set_preference(""network.proxy.type"", 1)\n        fp.set_preference(""network.proxy.http"",PROXY_HOST)\n        fp.set_preference(""network.proxy.http_port"",int(PROXY_PORT))\n        fp.set_preference(""general.useragent.override"",""whater_useragent"")\n        fp.update_preferences()\n        return webdriver.Firefox(firefox_profile=fp)\n\n']"
Maintaining cookies between Mechanize requests,"
I'm trying to use the Ruby version of Mechanize to extract my employer's tickets from a ticket management system that we're moving away from that does not supply an API.
Problem is, it seems Mechanize isn't keeping the cookies between the post call and the get call shown below:
require 'rubygems'
require 'nokogiri'
require 'mechanize'

@agent = Mechanize.new

page = @agent.post('http://<url>.com/user_session', {
                                            'authenticity_token' => '<token>',
                                            'user_session[login]' => '<login>',
                                            'user_session[password]' => '<password>',
                                            'user_session[remember_me]' => '0',
                                            'commit' => 'Login'
})

page = @agent.get 'http://<url>.com/<organization>/<repo-name>/tickets/1'
puts page.title

user_session is the URL to which the site's login page POSTs, and I've verified that this indeed logs me in. But the page that returns from the get call is the 'Oops, you're not logged in!' page.
I've verified that clicking links on the page that returns from the post call works, but I can't actually get to where I need to go without JavaScript. And of course I've done this successfully on the browser with the same login.
What am I doing wrong?
",13k,"
            15
        ","[""\nOkay this might help you - first of all what version of mechanize are you using? You need to identify, if this problem is due to the cookies being overwritten/cleaned by mechanize between the requests or if the cookies are wrong/not being set in the first place. You can do that by adding a puts @agent.cookie_jar.jar inbetween the two requests, to see what is stored.\nIf its a overwriting issue, you might be able to solve it by collecting the cookies from the first request, and applying them to the second. There are many ways to do this:\nOne way is to just do a temp_jar = agent.cookie_jar.jar an then just going through each cookie and add it again using the .add method \nHOWEVER - the easiest way is by just installing the latest 2.1 pre release of mechanize (many fixes), because you will then be able to do it very simply.\nTo install the latest do a gem install mechanize --pre and make sure to get rid of the old version of mechanize gem uninstall mechanize 'some_version' after this, you can simply do as follows:\nrequire 'rubygems'\nrequire 'nokogiri'\nrequire 'mechanize'\n\n@agent = Mechanize.new\n\npage = @agent.post('http://<url>.com/user_session', {\n                                        'authenticity_token' => '<token>',\n                                        'user_session[login]' => '<login>',\n                                        'user_session[password]' => '<password>',\n                                        'user_session[remember_me]' => '0',\n                                        'commit' => 'Login'\n})\ntemp_jar = @agent.cookie_jar\n#Do whatever you need an use the cookies again in a new session after that\n@agent = Mechanize.new\n@agent.cookie_jar = temp_jar\n\npage = @agent.get 'http://<url>.com/<organization>/<repo-name>/tickets/1'\nputs page.title\n\nBTW the documentation is here http://mechanize.rubyforge.org/index.html\n"", ""\nMechanize would automatically send cookies obtained from the response in the consecutive request. You can use the same agent without re-new.\nrequire 'mechanize'\n\n@agent = Mechanize.new\n@agent.post(create_sessions_url, params, headers)\n@agent.get(ticket_url)\n\nTested with mechanize 2.7.6.\n""]"
Screen-scraping a windows application in c# [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 2 years ago.







                        Improve this question
                    



I need to scrape data from a windows application to run a query in another program. Does anyone know of a good starting point for me to do this in .NET?
",29k,"
            13
        ","[""\nYou may want to look into the WM_GETTEXT message. This can be used to read text from other windows -- it's an archaic part of the Windows API, and if you're in C#, you'll need to p/invoke for it. \nCheck out this page for an example of doing this in C#. \nBasically, you first FindControlEx() to get the handle of the window that you want (by caption).\nSecond, you recursively enumerate the controls on that window with EnumChildWindows() to find all of the window's child controls, and all of those children's children until you have a complete map of the target form. \nHere is a selected portion of Theta-ga's excellent explanation from Google Answers:\nTo get the contents of any textbox or listbox control, all we need is it's window handle.  If you have already obtained the window handle then move to part 2 of the explaination.\nPART 1: Obtaining the control handle\n\nTo obtain the handle of a control, we first obtain the handle of it?s parent window. We can do this by using the Win32 FindControlEx() method. This method takes in the window caption (such as 'Calculator') and/or its class name, and return its handle.\nOnce we have the parent window handle, we can call the Win32 EnumChildWindows method. This method takes in a callback method, which it calls with the handle of every child control it finds for the specified parent. For eg., if we call this method with the handle of the Calculator window, it will call the callback method with the handle of the textbox control, and then again with the handles of each of the buttons on the Calculator window, and so on.\nSince we are only interested in the handle of the textbox control, we can check the class of the window in the callback method. The Win32 method GetClassName() can be used for this. This method takes in a window handle and provides us with a string containing the class name. So a textbox belongs to the ?Edit? class, a listbox to the 'ListBox' class and so on. Once you have determined that you have the handle for the right control, you can read its contents.\n\nPART 2: Reading the contents of a control\n\nYou can read in the contents of a control by using the Win32 SendMessage() function, and using it to pass the WM_GETTEXT message to the target control. This will give you the text content of the control. This method will work for a textbox, button, or static control.\nHowever, the above approach will fail if you try to read the contents of a listbox. To get the contents of a listbox, we need to first use SendMessage() with the LB_GETCOUNT message to get the count of list items. Then we need to call SendMessage() with the LB_GETTEXT message for each item in the list.\n\n""]"
Selenium Scroll inside of popup div,"
I am using selenium and trying to scroll inside the popup div on instagram.
I get to a page like 'https://www.instagram.com/kimkardashian/', click followers, and then I can't get the followers list to scroll down.
I tried using hover, click_and_hold, and a few other tricks to select the div but none of them worked.
What would the best way be to get this selected?
This is what I tried so far:
driver.find_elements_by_xpath(""//*[contains(text(), 'followers')]"")[0].click()
element_to_hover_over = driver.find_elements_by_xpath(""//*[contains(text(), 'Follow')]"")[12]
hover = ActionChains(webdriver).move_to_element(element_to_hover_over)
hover.click_and_hold()
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")

",23k,"
            11
        ","['\nThe exact code is as follow. You first have to find the new iframe which contains the name of followers:\nscr1 = driver.find_element_by_xpath(\'/html/body/div[2]/div/div[2]/div/div[2]\')\ndriver.execute_script(""arguments[0].scrollTop = arguments[0].scrollHeight"", scr1)\n\nThis will automatically scroll down the page but you have make a for loop for it until it reaches to the end of page. You can see my Instagram crawler here.\n', '\nYou would need to use jQuery to execute a function on the div.  Here\'s the way I figured to do it.  It was easier to solve it with jQuery and execute a script than handle it with the api.\nheight = 2000\nquery = \'jQuery(""div"").filter((i, div) => jQuery(div).css(""overflow-y"") == ""scroll"")[0].scrollTop = %s\' %height\ndriver.execute_script(query)\n\n']"
Pass the user-agent through webdriver in Selenium,"
I am working on a website scraping project using Selenium in Python. When I open the homepage through a browser, it opens properly.
But, when I try to open the webpage through webdriver() in Selenium, it opens a completely different page.
I think, it is able to detect the user-agent( not sure what it is called) and is able to check the properties of the browser or something.
Is it possible to pass the properties though the webdriver() so that the right homepage is loaded.
Thanks
",13k,"
            10
        ","['\nChanging the user agent in the python version of webdriver is done by altering your browser\'s profile. I have only done this for webdriver.Firefox() by passing a profile parameter. You need to do the following:\nfrom selenium import webdriver\nprofile = webdriver.FirefoxProfile()\nprofile.set_preference(""general.useragent.override"",""your_user_agent_string"")\ndriver=webdriver.Firefox(profile)\n\nEvery time you wish to change the user agent you will need to restart your web browser (i.e. call driver=webdriver.Firefox(profile) again)\nIf you are unsure to what your user agent string is do a search for ""what is my user agent"" on a browser that displays the page properly and just copy and paste that one.\nHope that sorts it.\n', '\nAssuming the user-agent is the problem, in Java you can modify it like this:\nFirefoxProfile profile = new FirefoxProfile();\nprofile.addAdditionalPreference(""general.useragent.override"", ""some UA string"");\nWebDriver driver = new FirefoxDriver(profile);\n\nSee documentation here.\n']"
Save all image files from a website,"
I'm creating a small app for myself where I run a Ruby script and save all of the images off of my blog.
I can't figure out how to save the image files after I've identified them. Any help would be much appreciated.
require 'rubygems'
require 'nokogiri'
require 'open-uri'

url = '[my blog url]'
doc = Nokogiri::HTML(open(url))

doc.css(""img"").each do |item|
  #something
end

",11k,"
            8
        ","['\nURL = \'[my blog url]\'\n\nrequire \'nokogiri\' # gem install nokogiri\nrequire \'open-uri\' # already part of your ruby install\n\nNokogiri::HTML(open(URL)).xpath(""//img/@src"").each do |src|\n  uri = URI.join( URL, src ).to_s # make absolute uri\n  File.open(File.basename(uri),\'wb\'){ |f| f.write(open(uri).read) }\nend\n\nUsing the code to convert to absolute paths from here: How can I get the absolute URL when extracting links using Nokogiri?\n', ""\nassuming the src attribute is an absolute url, maybe something like:\nif item['src'] =~ /([^\\/]+)$/\n    File.open($1, 'wb') {|f| f.write(open(item['src']).read)}\nend\n\n"", ""\nTip: there's a simple way to get images from a page's head/body using the Scrapifier gem. The cool thing is that you can also define which type of image you want it to be returned (jpg, png, gif). \nGive it a try: https://github.com/tiagopog/scrapifier\nHope you enjoy.\n"", ""\nsystem %x{ wget #{item['src']} }\n\nEdit: This is assuming you're on a unix system with wget :)\nEdit 2: Updated code for grabbing the img src from nokogiri.\n""]"
web scraping to fill out (and retrieve) search forms?,"
I was wondering if it is possible to ""automate"" the task of typing in entries to search forms and extracting matches from the results. For instance, I have a list of journal articles for which I would like to get DOI's (digital object identifier); manually for this I would go to the journal articles search page (e.g., http://pubs.acs.org/search/advanced), type in the authors/title/volume (etc.) and then find the article out of its list of returned results, and pick out the DOI and paste that into my reference list. I use R and Python for data analysis regularly (I was inspired by a post on RCurl) but don't know much about web protocols... is such a thing possible (for instance using something like Python's BeautifulSoup?). Are there any good references for doing anything remotely similar to this task? I'm just as much interested in learning about web scraping and tools for web scraping in general as much as getting this particular task done... Thanks for your time!
",14k,"
            8
        ","['\nBeautiful Soup is great for parsing webpages- that\'s half of what you want to do.  Python, Perl, and Ruby all have a version of Mechanize, and that\'s the other half:\nhttp://wwwsearch.sourceforge.net/mechanize/\nMechanize let\'s you control a browser:\n# Follow a link\nbrowser.follow_link(link_node)\n\n# Submit a form\nbrowser.select_form(name=""search"")\nbrowser[""authors""] = [""author #1"", ""author #2""]\nbrowser[""volume""] = ""any""\nsearch_response = br.submit()\n\nWith Mechanize and Beautiful Soup you have a great start.  One extra tool I\'d consider is Firebug, as used in this quick ruby scraping guide:\nhttp://www.igvita.com/2007/02/04/ruby-screen-scraper-in-60-seconds/\nFirebug can speed your construction of xpaths for parsing documents, saving you some serious time.\nGood luck!\n', '\nPython Code: for search forms.\n# import \nfrom selenium import webdriver\n\nfrom selenium.common.exceptions import TimeoutException\n\nfrom selenium.webdriver.support.ui import WebDriverWait # available since 2.4.0\n\nfrom selenium.webdriver.support import expected_conditions as EC # available since 2.26.0\n\n# Create a new instance of the Firefox driver\ndriver = webdriver.Firefox()\n\n# go to the google home page\ndriver.get(""http://www.google.com"")\n\n# the page is ajaxy so the title is originally this:\nprint driver.title\n\n# find the element that\'s name attribute is q (the google search box)\ninputElement = driver.find_element_by_name(""q"")\n\n# type in the search\ninputElement.send_keys(""cheese!"")\n\n# submit the form (although google automatically searches now without submitting)\ninputElement.submit()\n\ntry:\n    # we have to wait for the page to refresh, the last thing that seems to be updated is the title\n    WebDriverWait(driver, 10).until(EC.title_contains(""cheese!""))\n\n    # You should see ""cheese! - Google Search""\n    print driver.title\n\nfinally:\n    driver.quit()\n\nSource: https://www.seleniumhq.org/docs/03_webdriver.jsp\n', '\nWebRequest req = WebRequest.Create(""http://www.URLacceptingPOSTparams.com"");\n\nreq.Proxy = null;\nreq.Method = ""POST"";\nreq.ContentType = ""application/x-www-form-urlencoded"";\n\n//\n// add POST data\nstring reqString = ""searchtextbox=webclient&searchmode=simple&OtherParam=???"";\nbyte[] reqData = Encoding.UTF8.GetBytes (reqString);\nreq.ContentLength = reqData.Length;\n//\n// send request\nusing (Stream reqStream = req.GetRequestStream())\n  reqStream.Write (reqData, 0, reqData.Length);\n\nstring response;\n//\n// retrieve response\nusing (WebResponse res = req.GetResponse())\nusing (Stream resSteam = res.GetResponseStream())\nusing (StreamReader sr = new StreamReader (resSteam))\n  response = sr.ReadToEnd();\n\n// use a regular expression to break apart response\n// OR you could load the HTML response page as a DOM \n\n(Adapted from Joe Albahri\'s ""C# in a nutshell"")\n', '\nThere are many tools for web scraping. There is a good firefox plugin called iMacros. It works great and needs no programming knowledge at all. The free version can be downloaded from here:\nhttps://addons.mozilla.org/en-US/firefox/addon/imacros-for-firefox/\nThe best thing about iMacros, is that it can get you started in minutes, and it can also be launched from the bash command line, and can also be called from within bash scripts.\nA more advanced step would be selenium webdrive. The reason I chose selenium is that it is documented in a great way suiting beginners. reading just the following page:\nwould get you upand running in no time.\nSelenium supports java, python, php , c so if you are familiar with any of these languages, you would be familiar with all the commands needed. I prefer webdrive variation of selenium, as it opens a browser, so that you can check the fields and outputs. After setting up the script using webdrive, you can easily migrate the script to IDE, thus running headless.\nTo install selenium you can do by typing the command\nsudo easy_install selenium\n\nThis will take care of the dependencies and everything needed for you.\nIn order to run your script interactively, just open a terminal, and type \npython\n\nyou will see the python prompt, >>> and you can type in the commands.\nHere is a sample code which you can paste in the terminal, it will search google for the word cheeses\npackage org.openqa.selenium.example;\n\nimport org.openqa.selenium.By;\nimport org.openqa.selenium.WebDriver;\nimport org.openqa.selenium.WebElement;\nimport org.openqa.selenium.firefox.FirefoxDriver;\nimport org.openqa.selenium.support.ui.ExpectedCondition;\nimport org.openqa.selenium.support.ui.WebDriverWait;\n\npublic class Selenium2Example  {\n    public static void main(String[] args) {\n        // Create a new instance of the Firefox driver\n        // Notice that the remainder of the code relies on the interface, \n        // not the implementation.\n        WebDriver driver = new FirefoxDriver();\n\n        // And now use this to visit Google\n        driver.get(""http://www.google.com"");\n        // Alternatively the same thing can be done like this\n        // driver.navigate().to(""http://www.google.com"");\n\n        // Find the text input element by its name\n        WebElement element = driver.findElement(By.name(""q""));\n\n        // Enter something to search for\n        element.sendKeys(""Cheese!"");\n\n        // Now submit the form. WebDriver will find the form for us from the element\n        element.submit();\n\n        // Check the title of the page\n        System.out.println(""Page title is: "" + driver.getTitle());\n\n        // Google\'s search is rendered dynamically with JavaScript.\n        // Wait for the page to load, timeout after 10 seconds\n        (new WebDriverWait(driver, 10)).until(new ExpectedCondition<Boolean>() {\n            public Boolean apply(WebDriver d) {\n                return d.getTitle().toLowerCase().startsWith(""cheese!"");\n            }\n        });\n\n        // Should see: ""cheese! - Google Search""\n        System.out.println(""Page title is: "" + driver.getTitle());\n\n        //Close the browser\n        driver.quit();\n    }}\n\nI hope that this can give you a head start.\nCheers :)\n']"
Python web scraping involving HTML tags with attributes,"
I'm trying to make a web scraper that will parse a web-page of publications and extract the authors. The skeletal structure of the web-page is the following:
<html>
<body>
<div id=""container"">
<div id=""contents"">
<table>
<tbody>
<tr>
<td class=""author"">####I want whatever is located here ###</td>
</tr>
</tbody>
</table>
</div>
</div>
</body>
</html>

I've been trying to use BeautifulSoup and lxml thus far to accomplish this task, but I'm not sure how to handle the two div tags and td tag because they have attributes. In addition to this, I'm not sure whether I should rely more on BeautifulSoup or lxml or a combination of both. What should I do?
At the moment, my code looks like what is below:
    import re
    import urllib2,sys
    import lxml
    from lxml import etree
    from lxml.html.soupparser import fromstring
    from lxml.etree import tostring
    from lxml.cssselect import CSSSelector
    from BeautifulSoup import BeautifulSoup, NavigableString

    address='http://www.example.com/'
    html = urllib2.urlopen(address).read()
    soup = BeautifulSoup(html)
    html=soup.prettify()
    html=html.replace('&nbsp', '&#160')
    html=html.replace('&iacute','&#237')
    root=fromstring(html)

I realize that a lot of the import statements may be redundant, but I just copied whatever I currently had in more source file.
EDIT: I suppose that I didn't make this quite clear, but I have multiple  tags in page that I want to scrape. 
",9k,"
            8
        ","[""\nIt's not clear to me from your question why you need to worry about the div tags -- what about doing just:\nsoup = BeautifulSoup(html)\nthetd = soup.find('td', attrs={'class': 'author'})\nprint thetd.string\n\nOn the HTML you give, running this emits exactly:\n####I want whatever is located here ###\n\nwhich appears to be what you want.  Maybe you can specify better exactly what it is you need and this super-simple snippet doesn't do -- multiple td tags all of class author of which you need to consider (all? just some? which ones?), possibly missing any such tag (what do you want to do in that case), and the like. It's hard to infer what exactly are your specs, just from this simple example and overabundant code;-).\nEdit: if, as per the OP's latest comment, there are multiple such td tags, one per author:\nthetds = soup.findAll('td', attrs={'class': 'author'})\nfor thetd in thetds:\n    print thetd.string\n\n...i.e., not much harder at all!-)\n"", ""\nor you could be using pyquery, since BeautifulSoup is not actively maintained anymore, see http://www.crummy.com/software/BeautifulSoup/3.1-problems.html\nfirst, install pyquery with \neasy_install pyquery\n\nthen your script could be as simple as\nfrom pyquery import PyQuery\nd = PyQuery('http://mywebpage/')\nallauthors = [ td.text() for td in d('td.author') ]\n\npyquery uses the css selector syntax familiar from jQuery which I find more intuitive than BeautifulSoup's. It uses lxml underneath, and is much faster than BeautifulSoup. But BeautifulSoup is pure python, and thus works on Google's app engine as well\n"", '\nThe lxml library is now the standard for parsing html in python.  The interface can seem awkward at first, but it is very serviceable for what it does.  \nYou should let the libary handle the xml specialism, such as those escaped &entities;\nimport lxml.html\n\nhtml = """"""<html><body><div id=""container""><div id=""contents""><table><tbody><tr>\n          <td class=""author"">####I want whatever is located here, eh? &iacute; ###</td>\n          </tr></tbody></table></div></div></body></html>""""""\n\nroot = lxml.html.fromstring(html)\ntds = root.cssselect(""div#contents td.author"")\n\nprint tds           # gives [<Element td at 84ee2cc>]\nprint tds[0].text   # what you want, including the \'í\'\n\n', '\nBeautifulSoup is certainly the canonical HTML parser/processor.  But if you have just this kind of snippet you need to match, instead of building a whole hierarchical object representing the HTML, pyparsing makes it easy to define leading and trailing HTML tags as part of creating a larger search expression:\nfrom pyparsing import makeHTMLTags, withAttribute, SkipTo\n\nauthor_td, end_td = makeHTMLTags(""td"")\n\n# only interested in <td>\'s where class=""author""\nauthor_td.setParseAction(withAttribute((""class"",""author"")))\n\nsearch = author_td + SkipTo(end_td)(""body"") + end_td\n\nfor match in search.searchString(html):\n    print match.body\n\nPyparsing\'s makeHTMLTags function does a lot more than just emit ""<tag>"" and ""</tag>"" expressions.  It also handles:\n\ncaseless matching of tags\n""<tag/>"" syntax\nzero or more attribute in the opening tag\nattributes defined in arbitrary order\nattribute names with namespaces\nattribute values in single, double, or no quotes\nintervening whitespace between tag and symbols, or attribute name, \'=\', and value\nattributes are accessible after parsing as named results\n\nThese are the common pitfalls when considering using a regex for HTML scraping.\n']"
How can I screen scrape with Perl?,"
I need to display some values that are stored in a website, for that I need to scrape the website and fetch the content from the table. Any ideas?
",16k,"
            7
        ","['\nIf you are familiar with jQuery you might want to check out pQuery, which makes this very easy:\n## print every <h2> tag in page\nuse pQuery;\n\npQuery(""http://google.com/search?q=pquery"")\n    ->find(""h2"")\n    ->each(sub {\n        my $i = shift;\n        print $i + 1, "") "", pQuery($_)->text, ""\\n"";\n    });\n\nThere\'s also HTML::DOM.\nWhatever you do, though, don\'t use regular expressions for this.\n', '\nI have used HTML Table Extract in the past.\nI personally find it a bit clumsy to use, but maybe I did not understand the object model well.\nI usually use this part of the manual to examine the data:\n use HTML::TableExtract;\n $te = HTML::TableExtract->new();\n $te->parse($html_string);\n\n     # Examine all matching tables\n     foreach $ts ($te->tables) {\n       print ""Table ("", join(\',\', $ts->coords), ""):\\n"";\n       foreach $row ($ts->rows) {\n          print join(\',\', @$row), ""\\n"";\n       }\n     }`\n\n', ""\nAlthough I've generally done this with LWP/LWP::Simple, the current 'preferred' module for any sort of webpage scraping in Perl is WWW::Mechanize.\n"", ""\nIf you're familiar with XPath, you can also use HTML::TreeBuilder::XPath. And if you're not... well you should be ;--)\n"", '\nYou could also use this simple perl module WEB::Scraper, this is simple to understand and make life easy for me. follow this example for more information.   \nhttp://teusje.wordpress.com/2010/05/02/web-scraping-with-perl/\n', '\nFor similar Stackoverflow questions have a look at....\n\nHow can I extract URLs from a web page in Perl\nHow can I extract XML of a website and save in a file using Perl’s LWP?\n\nI do like using pQuery for things like this however Web::Scraper does look interesting.\n', ""\nI don't mean to drag up a dead thread but anyone googling across this thread should also checkout WWW::Scripter - 'For scripting web sites that have scripts'\nhappy remote data aggregating ;)\n"", ""\nTake a look at the magical Web::Scraper, it's THE tool for web scraping.\n"", '\nI use LWP::UserAgent for most of my screen scraping needs. You can also Couple that with HTTP::Cookies if you need Cookies support.\nHere\'s a simple example on how to get source.\nuse LWP;\nuse HTTP::Cookies;\nmy $cookie_jar = HTTP::Cookies->new;\nmy $browser = LWP::UserAgent->new;\n$browser->cookie_jar($cookie_jar);\n\n$resp = $browser->get(""https://www.stackoverflow.com"");\nif($resp->is_success) {\n   # Play with your source here\n   $source = $resp->content;\n   $source =~ s/^.*<table>/<table>/i; # this is just an example \n   print $source;                     # not a solution to your problem.\n}\n\n', '\nCheck out this little example of web scraping with perl:\nlink text\n']"
BeautifulSoup get_text does not strip all tags and JavaScript,"
I am trying to use BeautifulSoup to get text from web pages.
Below is a script I've written to do so. It takes two arguments, first is the input HTML or XML file, the second output file.
import sys
from bs4 import BeautifulSoup

def stripTags(s): return BeautifulSoup(s).get_text()

def stripTagsFromFile(inFile, outFile):
    open(outFile, 'w').write(stripTags(open(inFile).read()).encode(""utf-8""))

def main(argv):
    if len(sys.argv) <> 3:
        print 'Usage:\t\t', sys.argv[0], 'input.html output.txt'
        return 1
    stripTagsFromFile(sys.argv[1], sys.argv[2])
    return 0

if __name__ == ""__main__"":
    sys.exit(main(sys.argv))

Unfortunately, for many web pages, for example: http://www.greatjobsinteaching.co.uk/career/134112/Education-Manager-Location
I get something like this (I'm showing only few first lines):
html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd""
    Education Manager  Job In London With  Caleeda | Great Jobs In Teaching

var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-15255540-21']);
_gaq.push(['_trackPageview']);
_gaq.push(['_trackPageLoadTime']);

Is there anything wrong with my script? I was trying to pass 'xml' as the second argument to BeautifulSoup's constructor, as well as 'html5lib' and 'lxml', but it doesn't help.
Is there an alternative to BeautifulSoup which would work better for this task? All I want is to extract the text which would be rendered in a browser for this web page.
Any help will be much appreciated. 
",15k,"
            7
        ","[""\nnltk's clean_html() is quite good at this!\nAssuming that your already have your html stored in a variable html like\nhtml = urllib.urlopen(address).read()\n\nthen just use\nimport nltk\nclean_text = nltk.clean_html(html)\n\nUPDATE\nSupport for clean_html and clean_url will be dropped for future versions of nltk. Please use BeautifulSoup for now...it's very unfortunate.\nAn example on how to achieve this is on this page:\nBeatifulSoup4 get_text still has javascript\n"", ""\nHere's an approach which is based on the answer here: BeautifulSoup Grab Visible Webpage Text by jbochi. This approach allows for comments embedded in elements containing page text, and does a bit to clean up the output by stripping newlines, consolidating space, etc.\nhtml = urllib.urlopen(address).read()\nsoup = BeautifulSoup.BeautifulSoup(html)\ntexts = soup.findAll(text=True)\n\ndef visible_text(element):\n    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n        return ''\n    result = re.sub('<!--.*-->|\\r|\\n', '', str(element), flags=re.DOTALL)\n    result = re.sub('\\s{2,}|&nbsp;', ' ', result)\n    return result\n\nvisible_elements = [visible_text(elem) for elem in texts]\nvisible_text = ''.join(visible_elements)\nprint(visible_text)\n\n"", ""\nThis was the problem I was having.  no solution seemed to be able to return the text (the text that would actually be rendered in the web broswer).  Other solutions mentioned that BS is not ideal for rendering and that html2text was a good approach.  I tried both html2text and nltk.clean_html and was surprised by the timing results so thought they warranted an answer for posterity.  Of course, the speed delta might highly depend on the contents of the data...\nOne answer here from @Helge was about using nltk of all things.  \nimport nltk\n\n%timeit nltk.clean_html(html)\nwas returning 153 us per loop\n\nIt worked really well to return a string with rendered html.  This nltk module was faster than even html2text, though perhaps html2text is more robust. \nbetterHTML = html.decode(errors='ignore')\n%timeit html2text.html2text(betterHTML)\n%3.09 ms per loop\n\n""]"
Using Nokogiri to Split Content on BR tags,"
I have a snippet of code im trying to parse with nokogiri that looks like this:
<td class=""j"">
    <a title=""title text1"" href=""http://link1.com"">Link 1</a> (info1), Blah 1,<br>
    <a title=""title text2"" href=""http://link2.com"">Link 2</a> (info1), Blah 1,<br>
    <a title=""title text2"" href=""http://link3.com"">Link 3</a> (info2), Blah 1 Foo 2,<br>
</td>

I have access to the source of the td.j using something like this:
data_items = doc.css(""td.j"")
My goal is to split each of those lines up into an array of hashes.  The only logical splitting point i can see is to split on the BRs and then use some regex on the string.  
I was wondering if there's a Better way to do this maybe using nokogiri only?  Even if i could use nokogiri to suck out the 3 line items it would make things easier for me as i could just do some regex parsing on the .content result. 
Not sure how to use Nokogiri to grab lines ending with br though -- should i be using xpaths? any direction is appreciated! thank you
",4k,"
            6
        ","['\nI\'m not sure about the point of using an array of hashes, and without an example I can\'t suggest something. However, for splitting the text on <br> tags, I\'d go about it this way:\nrequire \'nokogiri\'\n\ndoc = Nokogiri::HTML(\'<td class=""j"">\n    <a title=""title text1"" href=""http://link1.com"">Link 1</a> (info1), Blah 1,<br>\n    <a title=""title text2"" href=""http://link2.com"">Link 2</a> (info1), Blah 1,<br>\n    <a title=""title text2"" href=""http://link3.com"">Link 3</a> (info2), Blah 1 Foo 2,<br>\n</td>\')\n\ndoc.search(\'br\').each do |n|\n  n.replace(""\\n"")\nend\ndoc.at(\'tr.j\').text.split(""\\n"") # => ["""", ""    Link 1 (info1), Blah 1,"", ""Link 2 (info1), Blah 1,"", ""Link 3 (info2), Blah 1 Foo 2,""]\n\nThis will get you closer to a hash:\nHash[*doc.at(\'td.j\').text.split(""\\n"")[1 .. -1].map{ |t| t.strip.split(\',\')[0 .. 1] }.flatten] # => {""Link 1 (info1)""=>"" Blah 1"", ""Link 2 (info1)""=>"" Blah 1"", ""Link 3 (info2)""=>"" Blah 1 Foo 2""}\n\n', '\nIf your data really is that regular and you don\'t need the attributes from the <a> elements, then you could parse the text form of each table cell without having to worry about the <br> elements at all.\nGiven some HTML like this in html:\n<table>\n    <tbody>\n        <tr>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://link1.com"">Link 1</a> (info1), Blah 1,<br>\n                <a title=""title text2"" href=""http://link2.com"">Link 2</a> (info1), Blah 1,<br>\n                <a title=""title text2"" href=""http://link3.com"">Link 3</a> (info2), Blah 1 Foo 2,<br>\n            </td>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://link4.com"">Link 4</a> (info1), Blah 2,<br>\n                <a title=""title text2"" href=""http://link5.com"">Link 5</a> (info1), Blah 2,<br>\n                <a title=""title text2"" href=""http://link6.com"">Link 6</a> (info2), Blah 2 Foo 2,<br>\n            </td>\n        </tr>\n        <tr>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://link7.com"">Link 7</a> (info1), Blah 3,<br>\n                <a title=""title text2"" href=""http://link8.com"">Link 8</a> (info1), Blah 3,<br>\n                <a title=""title text2"" href=""http://link9.com"">Link 9</a> (info2), Blah 3 Foo 2,<br>\n            </td>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://linkA.com"">Link A</a> (info1), Blah 4,<br>\n                <a title=""title text2"" href=""http://linkB.com"">Link B</a> (info1), Blah 4,<br>\n                <a title=""title text2"" href=""http://linkC.com"">Link C</a> (info2), Blah 4 Foo 2,<br>\n            </td>\n        </tr>\n    </tbody>\n</table>\n\nYou could do this:\nchunks = doc.search(\'.j\').map { |td| td.text.strip.scan(/[^,]+,[^,]+/) }\n\nand have this:\n[\n    [ ""Link 1 (info1), Blah 1"", ""Link 2 (info1), Blah 1"", ""Link 3 (info2), Blah 1 Foo 2"" ],\n    [ ""Link 4 (info1), Blah 2"", ""Link 5 (info1), Blah 2"", ""Link 6 (info2), Blah 2 Foo 2"" ],\n    [ ""Link 7 (info1), Blah 3"", ""Link 8 (info1), Blah 3"", ""Link 9 (info2), Blah 3 Foo 2"" ],\n    [ ""Link A (info1), Blah 4"", ""Link B (info1), Blah 4"", ""Link C (info2), Blah 4 Foo 2"" ]\n]\n\nin chunks. Then you could convert that to whatever hash form you needed.\n']"
Why does scrapy throw an error for me when trying to spider and parse a site?,"
The following code
class SiteSpider(BaseSpider):
    name = ""some_site.com""
    allowed_domains = [""some_site.com""]
    start_urls = [
        ""some_site.com/something/another/PRODUCT-CATEGORY1_10652_-1__85667"",
    ]
    rules = (
        Rule(SgmlLinkExtractor(allow=('some_site.com/something/another/PRODUCT-CATEGORY_(.*)', ))),

        # Extract links matching 'item.php' and parse them with the spider's method parse_item
        Rule(SgmlLinkExtractor(allow=('some_site.com/something/another/PRODUCT-DETAIL(.*)', )), callback=""parse_item""),
    )
    def parse_item(self, response):
.... parse stuff

Throws the following error
Traceback (most recent call last):
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/base.py"", line 1174, in mainLoop
    self.runUntilCurrent()
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/base.py"", line 796, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 318, in callback
    self._startRunCallbacks(result)
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 424, in _startRunCallbacks
    self._runCallbacks()
--- <exception caught here> ---
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 441, in _runCallbacks
    self.result = callback(self.result, *args, **kw)
  File ""/usr/lib/pymodules/python2.6/scrapy/spider.py"", line 62, in parse
    raise NotImplementedError
exceptions.NotImplementedError: 

When I change the callback to ""parse"" and the function to ""parse"" i don't get any errors, but nothing is scraped. I changed it to ""parse_items"" thinking I might be overriding the parse method by accident. Perhaps I'm setting up the link extractor wrong?
What I want to do is parse each ITEM link on the CATEGORY page. Am I doing this totally wrong?
",9k,"
            6
        ","['\nI needed to change BaseSpider to CrawlSpider. Thanks srapy users!\nhttp://groups.google.com/group/scrapy-users/browse_thread/thread/4adaba51f7bcd0af#\n\nHi Bob,\nPerhaps it might work if you change\n  from BaseSpider to CrawlSpider? The\n  BaseSpider seems not implement Rule,\n  see:\nhttp://doc.scrapy.org/topics/spiders.html?highlight=rule#scrapy.contr...\n-M\n\n', '\nBy default scrapy searches for parse function in the class. Here in your spider, parse function is missing. Instead of parse you have given parse_item. The problem will be solved if parse_item is replace with parse.\nOr you can override the parse method in spider.py with that of parse_item.\n']"
PHP equivalent of PyQuery or Nokogiri? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



Basically, I want to do some HTML screen scraping, but figuring out if it is possible in PHP.
In Python, I would use
PyQuery.
In Ruby, I would use Nokogiri.
",4k,"
            6
        ","[""\nIn PHP you can use phpQuery\nP.S. it's kinda ironic, I came to this page looking for phpQuery equivalent in Python :)\n"", '\nIn PHP for screen scraping you can use Snoopy (http://sourceforge.net/projects/snoopy/) or Simple HTML DOM Parser (http://simplehtmldom.sourceforge.net/)\n', ""\nHere's a PHP port of Ruby nokogiri\n"", '\nThe closest thing to Nokogiri in PHP is this one.\n', '\nThere are also the built in PHP DOM libraries.  They include DomDocument with its loadHTML() method for parsing html from a string and DomElement and DomNode for building up HTML via an object. \n']"
"Issue scraping page with ""Load more"" button with rvest","
I want to obtain the links to the atms listed on this page: https://coinatmradar.com/city/345/bitcoin-atm-birmingham-uk/
Would I need to do something about the 'load more' button at the bottom of the page?
I have been using the selector tool you can download for chrome to select the CSS path. 
I've written the below code block and it only seems to retrieve the first ten links. 
library(rvest)

base <- ""https://coinatmradar.com/city/345/bitcoin-atm-birmingham-uk/""
base_read <- read_html(base)
atm_urls <- html_nodes(base_read, "".place > a"")
all_urls_final <- html_attr(atm_urls, ""href"" )
print(all_urls_final)

I expected to be able to retrieve all links to the atms listed in the area but my R code has not done so.
Any help would be great. Sorry if this is a really simple question.
",2k,"
            6
        ","['\nYou should give RSelenium a try. I\'m able to get the links with the following code:\n# install.packages(""RSelenium"")\nlibrary(RSelenium)\nlibrary(rvest)\n\n# Download binaries, start driver, and get client object.\nrd <- rsDriver(browser = ""firefox"", port = 4444L)\nffd <- rd$client\n\n# Navigate to page.\nffd$navigate(""https://coinatmradar.com/city/345/bitcoin-atm-birmingham-uk/"")\n\n# Find the load button and assign, then send click event.\nload_btn <- ffd$findElement(using = ""css selector"", "".load-more .btn"")\nload_btn$clickElement()\n\n# Wait for elements to load.\nSys.sleep(2)\n\n# Get HTML data and parse\nhtml_data <- ffd$getPageSource()[[1]]\nhtml_data %>% \n    read_html() %>% \n    html_nodes("".place a:not(.operator-link)"") %>% \n    html_attr(""href"")\n\n#### OUTPUT ####\n\n#  [1] ""/bitcoin_atm/5969/bitcoin-atm-shitcoins-club-birmingham-uk-bitcoin-embassy/""                   \n#  [2] ""/bitcoin_atm/7105/bitcoin-atm-general-bytes-northampton-costcutter/""                           \n#  [3] ""/bitcoin_atm/4759/bitcoin-atm-general-bytes-birmingham-uk-costcutter/""                         \n#  [4] ""/bitcoin_atm/2533/bitcoin-atm-general-bytes-birmingham-uk-londis-# convenience/""                 \n#  [5] ""/bitcoin_atm/5458/bitcoin-atm-general-bytes-coventry-agg-african-restaurant/""                  \n#  [6] ""/bitcoin_atm/711/bitcoin-atm-general-bytes-coventry-bigs-barbers/""                             \n#  [7] ""/bitcoin_atm/5830/bitcoin-atm-general-bytes-telford-bpred-lion-service-station/""               \n#  [8] ""/bitcoin_atm/5466/bitcoin-atm-general-bytes-nottingham-24-express-off-licence/""                \n#  [9] ""/bitcoin_atm/4615/bitcoin-atm-general-bytes-northampton-costcutter/""                           \n# [10] ""/bitcoin_atm/4841/bitcoin-atm-lamassu-worcester-computer-house/""                               \n# [11] ""/bitcoin_atm/3150/bitcoin-atm-bitxatm-leicester-keshs-wines-and-newsagents-braustone/""         \n# [12] ""/bitcoin_atm/2948/bitcoin-atm-bitxatm-coventry-nisa-local/""                                    \n# [13] ""/bitcoin_atm/4742/bitcoin-atm-bitxatm-birmingham-uk-custcutter-coventry-road-hay-mills/""       \n# [14] ""/bitcoin_atm/4741/bitcoin-atm-bitxatm-derby-michaels-drink-store-alvaston/""                    \n# [15] ""/bitcoin_atm/4740/bitcoin-atm-bitxatm-birmingham-uk-nisa-local-crabtree-# hockley/""              \n# [16] ""/bitcoin_atm/4739/bitcoin-atm-bitxatm-birmingham-uk-nisa-local-subway-boldmere/""               \n# [17] ""/bitcoin_atm/4738/bitcoin-atm-bitxatm-birmingham-uk-ashtree-convenience-store/""                \n# [18] ""/bitcoin_atm/4737/bitcoin-atm-bitxatm-birmingham-uk-nisa-local-finnemore-road-bordesley-green/""\n# [19] ""/bitcoin_atm/3160/bitcoin-atm-bitxatm-birmingham-uk-costcutter/"" \n\n', ""\nWhen you click show more the page does an XHR POST request for more results using an offset of 10 (suggesting results come in batches of 10) from current set. You can mimic this so long as you have the followings params in the post body (I suspect only the bottom 3 are essential)\n'direction' : 1\n'sort' : 1\n'offset' : 10\n'pagetype' : 'city'\n'pageid' : 345\n\nAnd the following request header is required (at least in Python implementations) \n'X-Requested-With' : 'XMLHttpRequest'\n\nYou send that correctly and you will get a response containing the additional content. Note: content is wrapped in  ![CDATA[]] as instruction that content should not be interpreted as xml - you will need to account for that by extracting content within for parsing.\nThe total number of atms is returned from original  page you have and with css selector\n.atm-number\n\nYou can split on  &nbsp; and take the upper bound value from the split and convert to int. You then can calculate each offset required to meet that total (being used in a loop as consecutive offset param until total achieved) e.g. 19 results will be 2 requests total with 1 request at offset 10 for additional content.\n""]"
Count number of results for a particular word on Twitter,"
To further a personal project of mine, I have been pondering how to count the number of results for a user specified word on Twitter.  I have used their API extensively, but have not been able to come up with an efficient or even halfway practical way to count the occurrences of a particular word.  The actual results are not critical, just the overall count.  I'll keep scratching my head.  Any ideas or direction pointing would be most appreciated.
e.g. http://search.twitter.com/search?q=tomatoes
",7k,"
            5
        ","[""\nI'm able to go back about a week. I start my search with the parameters that Adam posted and then key off of the smallest id in the set of search results, like so,\nhttp://search.twitter.com/search.atom?lang=en&q=iphone&rpp=100&max_id=\nwhere max_id = the min(id) of the 100 results I just pulled.\n"", '\nnet but I have made recursive function to call search query again and again until I don\'t find word ""page="" in result.\n']"
Can a cURL based HTTP request imitate a browser based request completely?,"
This is a two part question.
Q1: Can cURL based request 100% imitate a browser based request? 
Q2: If yes, what all options should be set. If not what extra does the browser do that cannot bee imitated by cURL?
I have a website and I see thousands of request being made from a single IP in a very short time. These requests harvest all my data. When looked at the log to identify the agent used, it looks like a request from browser. So was curious to know if its a bot and not a user.
Thanks in advance
",8k,"
            4
        ","['\nThis page has all the answers to your questions. You can imitate the things mostly.\n', ""\nR1 : I suppose, if you set all the correct headers, that, yes, a curl-based request can imitate a browser-based one : after all, both send an HTTP request, which is just a couple of lines of text following a specific convention (namely, the HTTP RFC)\n\nR2 : The best way to answer that question is to take a look at what your browser is sending ; with Firefox, for instance, you can use either Firebug or LiveHTTPHeaders to get that.\nFor instance, to get this page, Firefox sent those request headers :\nGET /questions/1926876/can-a-curl-based-http-request-imitate-a-browser-based-request-completely HTTP/1.1\nHost: stackoverflow.com\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; fr; rv:1.9.2b4) Gecko/20091124 Firefox/3.6b4\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: fr,fr-fr;q=0.8,en-us;q=0.5,en;q=0.3\nAccept-Encoding: gzip,deflate\nAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7\nKeep-Alive: 115\nConnection: keep-alive\nReferer: http://stackoverflow.com/questions/1926876/can-a-curl-based-http-request-imitate-a-browser-based-request-completely/1926889\nCookie: .......\nCache-Control: max-age=0\n\n(I Just removed a couple of informations -- but you get the idea ;-) )\nUsing curl, you can work with curl_setopt to set the HTTP headers ; here, you'd probably have to use a combination of CURLOPT_HTTPHEADER, CURLOPT_COOKIE, CURLOPT_USERAGENT, ...\n""]"
Convert a relative URL to an absolute URL with Simple HTML DOM?,"
When I'm scraping content from some pages, the script gives a relative URL. Is it possible to get a absolute URL with Simple HTML DOM?
",5k,"
            4
        ","[""\nI don’t think that the Simple HTML DOM Parser can do that.\nBut you can do that on your own. First you need to distinguish the base URI that is the URI of the document if not declared otherwise (see BASE element). Than get each URI reference and apply the algorithms to resolve a relative URI as described in RFC 3986 (there already are classes you can use for that like the PEAR package Net_URL2).\nSo, using these two classes, you could do something like this:\n$uri = new Net_URL2('http://example.com/foo/bar'); // URI of the resource\n$baseURI = $uri;\nforeach ($html->find('base[href]') as $elem) {\n    $baseURI = $uri->resolve($elem->href);\n}\n\nforeach ($html->find('*[src]') as $elem) {\n    $elem->src = $baseURI->resolve($elem->src)->__toString();\n}\nforeach ($html->find('*[href]') as $elem) {\n    if (strtoupper($elem->tag) === 'BASE') continue;\n    $elem->href = $baseURI->resolve($elem->href)->__toString();\n}\nforeach ($html->find('form[action]') as $elem) {\n    $elem->action = $baseURI->resolve($elem->action)->__toString();\n}\n\nRepeat the substitution for any other attribute containing a URI like background, cite, classid, codebase, data, longdesc, profile and usemap (see index of attributes in HTML 4.01).\n"", '\nIn addition to @Artefacto\'s answer, and if you are outputting the scraped HTML somewhere, you could simply add <base href=""http://example.com""> to the head of the document, which will establish the base URL for all relative URLs in the document as the specified href. Have a look at http://www.w3schools.com/tags/tag_base.asp\n', ""\nEDIT See Gumbo's answer for a formally correct answer. This is a simplified algorithm that will work in the vast majority of cases, but fail on some.\nSure. Do this:\n\nTake the relative URL (a URL that doesn't start with http://, https://, or any other protocol, and also doesn't start with /).\nTake the URL of the page.\nRemove the query string from it (if any). One simple way is to explode around ? and then take the first element of the resulting array (take element with index 0 or use reset).\n\n\nIf the URL of the page ends in /, append it the relative URL and you have the final URL.\nIf the URL doesn't end in /, take dirname of it, and append it the relative URL. You now have the final URL.\n\n\n""]"
"BeautifulSoup subpages of list with ""load more"" pagination","
Quite new here, so apologies in advance. I'm looking to get a list of all company descriptions from https://angel.co/companies to play around with. The web-based parsing tools I've tried aren't cutting it, so I'm looking to write a simple python script. Should I start by getting an array of all the company URLs then loop through them? Any resources or direction would be helpful--I've looked around BeautifulSoup's documentation and a few posts/video tutorials, but I'm getting hung up on simulating the json request, among other things (see here: Get all links with BeautifulSoup from a single page website ('Load More' feature))
I see a script that I believe is calling additional listings:
o.on(""company_filter_fetch_page_complete"", function(e) {
    return t.ajax({
        url: ""/companies/startups"",
        data: e,
        dataType: ""json"",
        success: function(t) {
            return t.html ? 
                (E().find("".more"").empty().replaceWith(t.html),
                 c()) : void 0
        }
    })
}),

Thanks!
",4k,"
            3
        ","['\nThe data you want to scrape is dynamically loaded using ajax, you need to do a lot of work to get to the html you actually want:  \nimport requests\nfrom bs4 import BeautifulSoup\n\nheader = {\n    ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36"",\n    ""X-Requested-With"": ""XMLHttpRequest"",\n    }\n\nwith requests.Session() as s:\n    r = s.get(""https://angel.co/companies"").content\n    csrf = BeautifulSoup(r).select_one(""meta[name=csrf-token]"")[""content""]\n    header[""X-CSRF-Token""] = csrf\n    ids = s.post(""https://angel.co/company_filters/search_data"", data={""sort"": ""signal""}, headers=header).json()\n    _ids = """".join([""ids%5B%5D={}&"".format(i)  for i in ids.pop(""ids"")])\n    rest = ""&"".join([""{}={}"".format(k,v) for k,v in ids.items()])\n    url = ""https://angel.co/companies/startups?{}{}"".format(_ids, rest)\n    rsp = s.get(url, headers=header)\n    print(rsp.json())\n\nWe first need to get a valid csrf-token which is what the initial request does, then we need to post to https://angel.co/company_filters/search_data:\n\nwhich gives us:\n{""ids"":[296769,297064,60,63,112,119,130,160,167,179,194,236,281,287,312,390,433,469,496,516],""total"":908164,""page"":1,""sort"":""signal"",""new"":false,""hexdigest"":""3f4980479bd6dca37e485c80d415e848a57c43ae""}\n\nThey are the params needed for our get to https://angel.co/companies/startups i.e our last request:\n\nThat request then gives us more json which holds the html and all the company info:\n{""html"":""<div class=\\"" dc59 frs86 _a _jm\\"" data-_tn=\\""companies/results ...........\n\nThere is way too much to post but that is what you will need to parse.\nSo putting it all together:\nIn [3]: header = {\n   ...:     ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36"",\n   ...:     ""X-Requested-With"": ""XMLHttpRequest"",\n   ...: }\n\nIn [4]: with requests.Session() as s:\n   ...:         r = s.get(""https://angel.co/companies"").content\n   ...:         csrf = BeautifulSoup(r, ""lxml"").select_one(""meta[name=csrf-token]"")[""content""]\n   ...:         header[""X-CSRF-Token""] = csrf\n   ...:         ids = s.post(""https://angel.co/company_filters/search_data"", data={""sort"": ""signal""}, headers=header).json()\n   ...:         _ids = """".join([""ids%5B%5D={}&"".format(i) for i in ids.pop(""ids"")])\n   ...:         rest = ""&"".join([""{}={}"".format(k, v) for k, v in ids.items()])\n   ...:         url = ""https://angel.co/companies/startups?{}{}"".format(_ids, rest)\n   ...:         rsp = s.get(url, headers=header)\n   ...:         soup = BeautifulSoup(rsp.json()[""html""], ""lxml"")\n   ...:         for comp in soup.select(""div.base.startup""):\n   ...:                 text = comp.select_one(""div.text"")\n   ...:                 print(text.select_one(""div.name"").text.strip())\n   ...:                 print(text.select_one(""div.pitch"").text.strip())\n   ...:         \nFrontback\nMe, now.\nOutbound\nOptimizely for messages\nAdaptly\nThe Easiest Way to Advertise Across The Social Web.\nDraft\nWords with Friends for Fantasy (w/ real money)\nGraphicly\nan automated ebook publishing and distribution platform\nAppstores\nApp Distribution Platform\neVenues\nOnline Marketplace & Booking Engine for Unique Meeting Spaces\nWePow\nVideo & Mobile Recruitment\nDoubleDutch\nEvent Marketing Automation Software\necomom\nIt\'s all good\nBackType\nAcquired by Twitter\nStipple\nNative advertising for the visual web\nPinterest\nA Universal Social Catalog\nSocialize\nIdentify and reward your most influential users with our drop-in social platform.\nStyleSeat\nLargest and fastest growing marketplace in the $400B beauty and wellness industry\nLawPivot\n99 Designs for legal\nOstrovok\nLeading hotel booking platform for Russian-speakers\nThumb\nLeading mobile social network that helps people get instant opinions\nAppFog\nMaking developing applications on the cloud easier than ever before\nArtsy\nMaking all the world’s art accessible to anyone with an Internet connection.\n\nAs far as the pagination goes, you are limited to 20 pages per day but to get all 20 pages is simply a case of adding page:page_no to our form data to get the new params needed, data={""sort"": ""signal"",""page"":page}, when you click load more you can see what is posted:\n\nSo the final code:\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef parse(soup):\n\n        for comp in soup.select(""div.base.startup""):\n            text = comp.select_one(""div.text"")\n            yield (text.select_one(""div.name"").text.strip()), text.select_one(""div.pitch"").text.strip()\n\ndef connect(page):\n    header = {\n        ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36"",\n        ""X-Requested-With"": ""XMLHttpRequest"",\n    }\n\n    with requests.Session() as s:\n        r = s.get(""https://angel.co/companies"").content\n        csrf = BeautifulSoup(r, ""lxml"").select_one(""meta[name=csrf-token]"")[""content""]\n        header[""X-CSRF-Token""] = csrf\n        ids = s.post(""https://angel.co/company_filters/search_data"", data={""sort"": ""signal"",""page"":page}, headers=header).json()\n        _ids = """".join([""ids%5B%5D={}&"".format(i) for i in ids.pop(""ids"")])\n        rest = ""&"".join([""{}={}"".format(k, v) for k, v in ids.items()])\n        url = ""https://angel.co/companies/startups?{}{}"".format(_ids, rest)\n        rsp = s.get(url, headers=header)\n        soup = BeautifulSoup(rsp.json()[""html""], ""lxml"")\n        for n, p in parse(soup):\n            yield n, p\nfor i in range(1, 21):\n    for name, pitch in connect(i):\n        print(name, pitch)\n\nObviously what you parse is up to you but everything you see in your browser in the results will be available.\n']"
Get instagram followers,"
I want to parse a website's followers count with BeautifulSoup. This is what I have so far:
username_extract = 'lazada_my'

url = 'https://www.instagram.com/'+ username_extract
r = requests.get(url)
soup = BeautifulSoup(r.content,'lxml')
f = soup.find('head', attrs={'class':'count'})

This is the part I want to parse:

Something within my soup.find() function is wrong, but I can't wrap my head around it. When returning f, it is empty. Any idea what I am doing wrong?
",5k,"
            3
        ","['\nI think you can use re module to search the correct count.\nimport requests\nimport re\n\nusername_extract = \'lazada_my\'\n\nurl = \'https://www.instagram.com/\'+ username_extract\nr = requests.get(url)\nm = re.search(r\'""followed_by"":\\{""count"":([0-9]+)\\}\', str(r.content))\nprint(m.group(1))\n\n', '\nsoup.find(\'head\', attrs={\'class\':\'count\'}) searches for something that looks like <head class=""count"">, which doesn\'t exist anywhere in the HTML. The data you\'re after is contained in the <script> tag that starts with window._sharedData:\nscript = soup.find(\'script\', text=lambda t: t.startswith(\'window._sharedData\'))\n\nFrom there, you can just strip off the variable assignment and the semicolon to get valid JSON:\n# <script>window._sharedData = ...;</script>\n#                              ^^^\n#                              JSON\n\npage_json = script.text.split(\' = \', 1)[1].rstrip(\';\')\n\nParse it and everything you need is contained in the object:\nimport json\n\ndata = json.loads(page_json)\nfollower_count = data[\'entry_data\'][\'ProfilePage\'][0][\'user\'][\'followed_by\'][\'count\']\n\n', '\nMost of the content is dynamically generated with JS. That\'s the reason you\'re getting empty results.\nBut, the followers count is present in the page source. Only thing is, it is not directly available in the form you want. You can see it here:\n<meta content=""407.4k Followers, 27 Following, 2,740 Posts - See Instagram photos and videos from Lazada Malaysia (@lazada_my)"" name=""description"" />\n\nIf you want to scrape the followers count without regex, you can use this:\n>>> followers = soup.find(\'meta\', {\'name\': \'description\'})[\'content\']\n>>> followers\n\'407.4k Followers, 27 Following, 2,740 Posts - See Instagram photos and videos from Lazada Malaysia (@lazada_my)\'\n>>> followers_count = followers.split(\'Followers\')[0]\n>>> followers_count\n\'407.4k \'\n\n', '\nYou have to look for the scripts, Then look for the \'window._sharedData\' exits in it. If exits then perform the regular expression operation. \nimport re\n\nusername_extract = \'lazada_my\'\nurl = \'https://www.instagram.com/\'+ username_extract\nr = requests.get(url)\nsoup = BeautifulSoup(r.content,\'lxml\')\ns = re.compile(r\'""followed_by"":{""count"":\\d*}\')\nfor i in soup.find_all(\'script\'):\n     if \'window._sharedData\' in str(i):\n         print s.search(str(i.contents)).group()\n\nResult,\n""followed_by"":{""count"":407426}\n\n', '\nThank you all, I ended up using William\'s solution. In case anybody will have future projects, here is my complete code for scraping a bunch of URL\'s for their follower count:\nimport requests\nimport csv \nimport pandas as pd\nimport re\n\ninsta = pd.read_csv(\'Instagram.csv\')\n\nusername = []\n\nbad_urls = [] \n\nfor lines in insta[\'Instagram\'][0:250]:\n    lines = lines.split(""/"")\n    username.append(lines[3])\n\nwith open(\'insta_output.csv\', \'w\') as csvfile:\nt = csv.writer(csvfile, delimiter=\',\')     #   ----> COMMA Seperated\nfor user in username:\n   try:\n       url = \'https://www.instagram.com/\'+ user\n       r = requests.get(url)\n       m = re.search(r\'""followed_by"":\\{""count"":([0-9]+)\\}\', str(r.content))\n       num_followers = m.group(1)\n       t.writerow([user,num_followers])    #  ----> Adding Rows\n   except:\n       bad_urls.append(url)\n\n']"
What prevents me from using $.ajax to load another domain's html?,"
My domain:
<!DOCTYPE html>  
<html>
<head>
<title>scrape</title>
<script src=""http://code.jquery.com/jquery-1.7.1.min.js""></script>
</head>
<body>
    <script>
        $.ajax({url:'http://their-domain.com/index.html',
        dataType:'html',
            success:function(data){console.log(data);}
        });
    </script>
</body>
</html>

What prevents me from being able to scrape their-domain? Any work around?
Addendum: thank you all for the suggestions to use a server side script, but I am for the moment interested in solving this problem exclusively using the client.
If I format the request using ""jsonp"" I do at least get a response, but with the following error:""Uncaught SyntaxError: Unexpected token <"". So I am getting a response from their-domain but the parser expects it to be json. (As well it should.) I am hacking through this trying to see if their is a way to trick the client into accepting this response. Please understand that I know this is atypical.
<!DOCTYPE html>  
<html>
<head>
<title>scrape</title>
<script src=""http://code.jquery.com/jquery-1.7.1.min.js""></script>
</head>
<body>
    <script>
        $.ajax({url:'http://their-domain.com/index.html',
        dataType:'jsonp',
            success:function(data){console.log(data);}
        });
    </script>
</body>
</html>

",2k,"
            2
        ","[""\nThere are Four ways to get around Same Origin Policy \n\nProxy - You request it from your server, your server requests it from other domain, your server returns it to the browser\nFlash cross domain policy - other domain must add a crossdomain.xml file to their site\nCross domain HTTP header - other domain must add an Access-Control-Allow-Origin header to their page \nJSONP - It's a json web service that provides a callback function.  Other domain must implement this.\n\nNote: The ONLY way to do it without the other domain's help is #1, routing it through your own server.\n"", '\nthe Same Origin Policy prevents client side scripts from getting data from domains that are not from the originator for the request. You would need a server side script to act as a proxy\n', ""\nIt's the Same Origin Policy, which prevents cross-domain requests.  If you want to scrape html, you are better off writing a server side process to get the content, then use ajax to make a request against your server, which contains the harvested data.  \n"", '\nOne workaround is to make a server-side script (eg. PHP) to get the page, and have $.ajax call that.\n']"
Screen Scraping of Image Links in PHP,"
I have a website that contains many different pages of products and each page has a certain amount of images in the same format across all pages. I want to be able to screen scrap each page's url so I can retrieve the url of each image from each page. The idea is to make a gallery for each page made up of hotlinked images.
I know this can be done in php, but I am not sure how to scrap the page for multiple links. Any ideas?
",5k,"
            2
        ","[""\nI would recommend using a DOM parser, such as PHP's very own DOMDocument. Example:\n$page = file_get_contents('http://example.com/images.php');\n$doc = new DOMDocument(); \n$doc->loadHTML($page);\n$images = $doc->getElementsByTagName('img'); \nforeach($images as $image) {\n    echo $image->getAttribute('src') . '<br />';\n}\n\n"", '\nYou can use a regular expression (regex) to go through the page source and parse all the IMG tags. \nThis regex will do the job quite nicely: <img[^>]+src=""(.*?)"" \nHow does this work? \n// <img[^>]+src=""(.*?)""\n// \n// Match the characters ""<img"" literally «<img»\n// Match any character that is not a "">"" «[^>]+»\n//    Between one and unlimited times, as many times as possible, giving back as needed (greedy) «+»\n// Match the characters ""src="""" literally «src=""»\n// Match the regular expression below and capture its match into backreference number 1 «(.*?)»\n//    Match any single character that is not a line break character «.*?»\n//       Between zero and unlimited times, as few times as possible, expanding as needed (lazy) «*?»\n// Match the character """""" literally «""»\n\nSample PHP code: \npreg_match_all(\'/<img[^>]+src=""(.*?)""/i\', $subject, $result, PREG_PATTERN_ORDER);\nfor ($i = 0; $i < count($result[0]); $i++) {\n    // image URL is in $result[0][$i];\n}\n\nYou\'ll have to do a bit more work to resolve things like relative URLs.\n', ""\nI really like PHP Simple HTML DOM Parser for things like this. An example of grabbing images is right there on the front page:\n// Create DOM from URL or file\n$html = file_get_html('http://www.google.com/');\n\n// Find all images\nforeach($html->find('img') as $element)\n       echo $element->src . '<br>';\n\n"", '\nYou can you this to scrap pages.\nhttp://simplehtmldom.sourceforge.net/\nbut it requires PHP 5+.\n']"
How to scrape charts from a website with python?,"
EDIT: 
So I have save the script codes below to a text file but using re to extract the data still doesn't return me anything. My code is: 
file_object = open('source_test_script.txt', mode=""r"")
soup = BeautifulSoup(file_object, ""html.parser"")
pattern = re.compile(r""^var (chart[0-9]+) = new Highcharts.Chart\(({.*?})\);$"", re.MULTILINE | re.DOTALL)
scripts = soup.find(""script"", text=pattern)
profile_text = pattern.search(scripts.text).group(1)
profile = json.loads(profile_text)

print profile[""data""], profile[""categories""]


I would like to extract the chart's data from a website. The following is the source code of the chart. 
  <script type=""text/javascript"">
    jQuery(function() {

    var chart1 = new Highcharts.Chart({

          chart: {
             renderTo: 'chart1',
              defaultSeriesType: 'column',
            borderWidth: 2
          },
          title: {
             text: 'Productions'
          },
          legend: {
            enabled: false
          },
          xAxis: [{
             categories: [1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016],

          }],
          yAxis: {
             min: 0,
             title: {
             text: 'Productions'
          }
          },

          series: [{
               name: 'Productions',
               data: [1,1,0,1,6,4,9,15,15,19,24,18,53,42,54,53,61,36]
               }]
       });
    });

    </script>

There are several charts like that from the website, called ""chart1"", ""chart2"", etc. I would like to extract the following data: the categories line and the data line, for each chart: 
categories: [1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016]

data: [1,1,0,1,6,4,9,15,15,19,24,18,53,42,54,53,61,36]

",5k,"
            2
        ","['\nAnother way is to use Highcharts\' JavaScript Library as one would in the console and pull that using Selenium. \nimport time\nfrom selenium import webdriver\n\nwebsite = """"\n\ndriver = webdriver.Firefox()\ndriver.get(website)\ntime.sleep(5)\n\ntemp = driver.execute_script(\'return window.Highcharts.charts[0]\'\n                             \'.series[0].options.data\')\ndata = [item[1] for item in temp]\nprint(data)\n\nDepending on what chart and series you are trying to pull your case might be slightly different.\n', '\nI\'d go a combination of regex and yaml parser.  Quick and dirty below - you may need to tweek the regex but it works with example:\nimport re\nimport sys\nimport yaml\n\nchart_matcher = re.compile(r\'^var (chart[0-9]+) = new Highcharts.Chart\\(({.*?})\\);$\',\n        re.MULTILINE | re.DOTALL)\n\nscript = sys.stdin.read()\n\nm = chart_matcher.findall(script)\n\nfor name, data in m:\n    print name\n    try:\n        chart = yaml.safe_load(data)\n        print ""categories:"", chart[\'xAxis\'][0][\'categories\']\n        print ""data:"", chart[\'series\'][0][\'data\']\n    except Exception, e:\n        print e\n\nRequires the yaml library (pip install PyYAML) and you should use BeautifulSoup to extract the correct <script> tag before passing it to the regex.\nEDIT - full example\nSorry I didn\'t make myself clear.  You use BeautifulSoup to parse the HTML and extract the <script> elements, and then use PyYAML to parse the javascript object declaration.  You can\'t use the built in json library because its not valid JSON but plain javascript object declarations (ie with no functions) are a subset of YAML.\nfrom bs4 import BeautifulSoup\nimport yaml\nimport re\n\nfile_object = open(\'source_test_script.txt\', mode=""r"")\nsoup = BeautifulSoup(file_object, ""html.parser"")\n\npattern = re.compile(r""var (chart[0-9]+) = new Highcharts.Chart\\(({.*?})\\);"", re.MULTILINE | re.DOTALL | re.UNICODE)\n\ncharts = {}\n\n# find every <script> tag in the source using beautifulsoup\nfor tag in soup.find_all(\'script\'):\n\n    # tabs are special in yaml so remove them first\n    script = tag.text.replace(\'\\t\', \'\')\n\n    # find each object declaration\n    for name, obj_declaration in pattern.findall(script):\n        try:\n            # parse the javascript declaration\n            charts[name] = yaml.safe_load(obj_declaration)\n        except Exception, e:\n            print ""Failed to parse {0}: {1}"".format(name, e)\n\n# extract the data you want\nfor name in charts:\n    print ""## {0} ##"".format(name);\n    print ""categories:"", charts[name][\'xAxis\'][0][\'categories\']\n    print ""data:"", charts[name][\'series\'][0][\'data\']\n    print\n\nOutput:\n## chart1 ##\ncategories: [1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]\ndata: [22, 1, 0, 1, 6, 4, 9, 15, 15, 19, 24, 18, 53, 42, 54, 53, 61, 36]\n\nNote I had to tweek the regex to make it handle the unicode output and whitespace from BeautifulSoup - in my original example I just piped your source directly to the regex.\nEDIT 2 - no yaml\nGiven that the javascript looks to be partially generated the best you can hope for is to grab the lines - not elegant but will probably work for you.\nfrom bs4 import BeautifulSoup\nimport json\nimport re\n\nfile_object = open(\'citec.repec.org_p_c_pcl20.html\', mode=""r"")\nsoup = BeautifulSoup(file_object, ""html.parser"")\n\npattern = re.compile(r""var (chart[0-9]+) = new Highcharts.Chart\\(({.*?})\\);"", re.MULTILINE | re.DOTALL | re.UNICODE)\n\ncharts = {}\n\nfor tag in soup.find_all(\'script\'):\n\n    # tabs are special in yaml so remove them first\n    script = tag.text\n\n    values = {}\n\n    # find each object declaration\n    for name, obj_declaration in pattern.findall(script):\n        for line in obj_declaration.split(\'\\n\'):\n            line = line.strip(\'\\t\\n ,;\')\n            for field in (\'data\', \'categories\'):\n                if line.startswith(field + "":""):\n                    data = line[len(field)+1:]\n                    try:\n                        values[field] = json.loads(data)\n                    except:\n                        print ""Failed to parse %r for %s"" % (data, name)\n\n        charts[name] = values\n\nprint charts\n\nNote that it fails for chart7 because that references another variable.\n']"
Close a scrapy spider when a condition is met and return the output object,"
I have made a spider to get reviews from a page like this here using scrapy. I want product reviews only till a certain date(2nd July 2016 in this case). I want to close my spider as soon as the review date goes earlier than the given date and return the items list.
Spider is working well but my problem is that i am not able to close my spider if the condition is met..if i raise an exception, spider closes without returning anything.
Please suggest the best way to close the spider manually. Here is my code:
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from scrapy import Selector
from tars.items import FlipkartProductReviewsItem
import re as r
import unicodedata
from datetime import datetime 

class Freviewspider(CrawlSpider):
    name = ""frs""
    allowed_domains = [""flipkart.com""]
    def __init__(self, *args, **kwargs):
        super(Freviewspider, self).__init__(*args, **kwargs)
        self.start_urls = [kwargs.get('start_url')]


    rules = (
        Rule(LinkExtractor(allow=(), restrict_xpaths=('//a[@class=""nav_bar_next_prev""]')), callback=""parse_start_url"", follow= True),
)


    def parse_start_url(self, response):

        hxs = Selector(response)
        titles = hxs.xpath('//div[@class=""fclear fk-review fk-position-relative line ""]')

        items = []

        for i in titles:

            item = FlipkartProductReviewsItem()

            #x-paths:

            title_xpath = ""div[2]/div[1]/strong/text()""
            review_xpath = ""div[2]/p/span/text()""
            date_xpath = ""div[1]/div[3]/text()""



            #field-values-extraction:

            item[""date""] = (''.join(i.xpath(date_xpath).extract())).replace('\n ', '')
            item[""title""] = (''.join(i.xpath(title_xpath).extract())).replace('\n ', '')

            review_list = i.xpath(review_xpath).extract()
            temp_list = []
            for element in review_list:
                temp_list.append(element.replace('\n ', '').replace('\n', ''))

            item[""review""] = ' '.join(temp_list)

            xxx = datetime.strptime(item[""date""], '%d %b %Y ')
            comp_date = datetime.strptime('02 Jul 2016 ', '%d %b %Y ')
            if xxx>comp_date:
                items.append(item)
            else:
                break

        return(items)

",6k,"
            2
        ",['\nTo force spider to close you can use raise CloseSpider exception as described here in scrapy docs. Just be sure to return/yield your items before you raise the exception.\n']
Screen scraping web page after delay,"
I'm trying to scrape a web page using C#, however after the page loads, it executes some JavaScript which loads more elements into the DOM which I need to scrape. A standard scraper simply grabs the html of the page on load and doesn't pick up the DOM changes made via JavaScript. How do I put in some sort of functionality to wait for a second or two and then grab the source?
Here is my current code:
private string ScrapeWebpage(string url, DateTime? updateDate)
{
    HttpWebRequest request = null;
    HttpWebResponse response = null;
    Stream responseStream = null;
    StreamReader reader = null;
    string html = null;
    try
    {
        //create request (which supports http compression)
        request = (HttpWebRequest)WebRequest.Create(url);
        request.Pipelined = true;
        request.Headers.Add(HttpRequestHeader.AcceptEncoding, ""gzip,deflate"");
        if (updateDate != null)
            request.IfModifiedSince = updateDate.Value;
        //get response.
        response = (HttpWebResponse)request.GetResponse();
        responseStream = response.GetResponseStream();
        if (response.ContentEncoding.ToLower().Contains(""gzip""))
            responseStream = new GZipStream(responseStream,
                CompressionMode.Decompress);
        else if (response.ContentEncoding.ToLower().Contains(""deflate""))
            responseStream = new DeflateStream(responseStream,
                CompressionMode.Decompress);
        //read html.
        reader = new StreamReader(responseStream, Encoding.Default);
        html = reader.ReadToEnd();
    }
    catch
    {
        throw;
    }
    finally
    {
        //dispose of objects.
        request = null;
        if (response != null)
        {
            response.Close();
            response = null;
        }
        if (responseStream != null)
        {
            responseStream.Close();
            responseStream.Dispose();
        }
        if (reader != null)
        {
            reader.Close();
            reader.Dispose();
        }
    }
    return html;
}

Here's a sample URL:
http://www.realtor.com/realestateandhomes-search/geneva_ny#listingType-any/pg-4
You'll see when the page first loads it says 134 listings found, then after a second it says 187 properties found.
",4k,"
            2
        ","[""\nTo execute the JavaScript I use webkit to render the page, which is the engine used by Chrome and Safari. Here is an example using its Python bindings.\nWebkit also has .NET bindings but I haven't used them.\n"", ""\nThe approach you have will not work regardless how long you wait, you need a browser to execute the javascript (or something that understands javascript).\nTry this question:\nWhat's a good tool to screen-scrape with Javascript support?\n"", '\nYou would need to execute the javascript yourself to get this functionality. Currently, your code only receives whatever the server replies with at the URL you request. The rest of the listings are ""showing up"" because the browser downloads, parses, and executes the accompanying javascript.\n', '\nThe answer to this similar question says to use a web browser control to read the page in and process it before scraping it. Perhaps with some kind of timer delay to give the javascript some time to execute and return results.\n']"
"Why is this HtmlAgilityPack operation invalid when there are, indeed, matching elements?","
I get ""InvalidOperationException > Message=Sequence contains no matching element"" with the following code:
private void buttonLoadHTML_Click(object sender, EventArgs e)
{
    GetParagraphsListFromHtml(@""C:\PlatypiRUs\fitt.html"");
}

// This code adapted from Kirk Woll's answer at 
   http://stackoverflow.com/questions/4752840/html-agility-pack-c-sharp-paragraph-
   parsing-problem
public List<string> GetParagraphsListFromHtml(string sourceHtml)
{
    var pars = new List<string>();
    HtmlAgilityPack.HtmlDocument doc = new HtmlAgilityPack.HtmlDocument();
    doc.LoadHtml(sourceHtml);
    foreach (var par in doc.DocumentNode
        .DescendantNodes()
        .Single(x => x.Id == ""body"")
        .DescendantNodes()
        .Where(x => x.Name == ""p""))
        //.Where(x => x.Name == ""h1"" || x.Name == ""h2"" || x.Name == ""h3"" || x.Name 
           == ""hp"" || )) <-- This is what I'd really like to do, but I don't know if   
           this is possible or, if it is, if the syntax is correct
    {
        pars.Add(par.InnerText);
    }
    // test
    foreach (string s in pars)
    {
        MessageBox.Show(s);
    }
    return pars;
}

Why is the code not finding the paragraphs?
I really want to find all the text (h1..3 or higher vals, too), but this is a start.
BTW: The html file I'm testing with does have some paragraph elements.
UPDATE
In response to Amy's implied request, and in the interest of full disclosure/ultimate illumination, here is the entire test html file:
<style>
body {
    background-color: orange;
    font-family: Verdana, sans-serif;
}

h1 {
    color: Blue;   
    font-family: 'Segoe UI', Verdana, sans-serif;
}

h2 {
    color: white;    
    font-family: 'Palatino Linotype', 'Palatino', sans-serif;
}

h3 {
    display: inline-block;
}
</style>

<h1>Found in the Translation</h1>
<h2>Bilingual Editions of Classic Literature</h2>
<div><label>Contact: </label><a href=""mailto:axx3andspace@gmail.com"">Found in the Translation</a></div>

<h2><cite>Around the World in 80 Days</cite> by Jules Verne (French &amp; English Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495308081"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51BCZUX2-dL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I0DOYRE"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51BCZUX2-dL._SL160_.jpg"" /></a>

<h2><cite>Gulliver's Travels</cite> by Jonathan Swift (English &amp; French Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495374688"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/517O76OyaWL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I5319ZO"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/517O76OyaWL._SL160_.jpg"" /></a>

<h2><cite>Journey to the Center of the Earth</cite> by Jules Verne (French &amp; English Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495409031"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/41hosXOIw8L._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I6LG25M"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/41qj8DfrihL._SL160_.jpg"" /></a>

<h2><cite>Treasure Island</cite> by Robert Louis Stevenson (English &amp; Finnish Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495418936"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51veMV3OiOL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00IA5V4KC"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51XNUWbA07L._SL160_.jpg"" /></a>

<h2><cite>Robinson Crusoe</cite> by Daniel Defoe (English &amp; French Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495448053"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51QQMRPrP9L._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I9IE8OY"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5128hqiw3DL._SL160_.jpg"" /></a>

<h2><cite>Don Quixote</cite> by Miguel de Cervantes Saavedra (Spanish &amp; English Side by Side)</h2>
<h3>Paperback</h3></br>
<h3>Volume I</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/149474967X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51HqjOPXLVL._SL160_.jpg"" /></a>
<h3>Volume II</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1494803445"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51NONygEMYL._SL160_.jpg"" /></a>
<h3>Volume III</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1494841983"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51G%2BW3ICHkL._SL160_.jpg"" /></a></br>
<h3>Kindle</h3></br>
<h3>Volume I</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HQMWPQ2"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51HqjOPXLVL._SL160_.jpg"" /></a>
<h3>Volume II</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HYN2QGM"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51NONygEMYL._SL160_.jpg"" /></a>
<h3>Volume III</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HLX519E"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51G%2BW3ICHkL._SL160_.jpg"" /></a></br>

<h2><cite>Alice's Adventures in Wonderland</cite> by Lewis Carroll (English &amp; German Side by Side)</h2>
<h3>Coming soon; for now, see:</h3></br/>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/193659420X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5143vIpQ2YL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00ESLTIYQ"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51%2BX0Dy7uNL._SL160_.jpg"" /></a>

<h2><cite>Alice's Adventures in Wonderland</cite> by Lewis Carroll (English &amp; Italian Side by Side)</h2>
<h3>Coming soon; for now, see:</h3></br/>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/193659420X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5143vIpQ2YL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00ESLTIYQ"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51%2BX0Dy7uNL._SL160_.jpg"" /></a>

<h2>Other Sites:</h2>
<p><a href=""http://usamaporama.azurewebsites.net/""  target=""_blank"">USA Map-O-Rama</a></p>
<p><a href=""http://www.awardwinnersonly.com/""  target=""_blank"">Award-winning Movies, Books, and Music</a></p>
<p><a href=""http://www.bigsurgarrapata.com/""  target=""_blank"">Garrapata State Park in Big Sur Throughout the Seasons</a></p>

UPDATE 2
This works (although it is with ""live"" web pages, and not html files saved to disk):
public List<string> GetParagraphsListFromHtml(string sourceHtml)
{
    var pars = new List<string>();
    HtmlAgilityPack.HtmlDocument doc = new HtmlAgilityPack.HtmlDocument();
    doc.LoadHtml(sourceHtml);

    var getHtmlWeb = new HtmlWeb();
    var document = getHtmlWeb.Load(""http://www.montereycountyweekly.com/opinion/letters/article_e333a222-942d-11e3-ba9c-001a4bcf6878.html""); 
    //http://www.bigsurgarrapata.com/ only returned one paragraph
    // http://usamaporama.azurewebsites.net/ <-- none
    // http://www.awardwinnersonly.com/ <- same as bigsurgarrapata
    var pTags = document.DocumentNode.SelectNodes(""//p"");
    int counter = 1;
    if (pTags != null)
    {
        foreach (var pTag in pTags)
        {
            pars.Add(pTag.InnerText);
            MessageBox.Show(pTag.InnerText);
            counter++;
        }
    }
    MessageBox.Show(""done!"");
    return pars;
}

",330,"
            0
        ","['\nIt turns out to be pretty easy; this is not complete, but this, inspired by this answer, is enough to get started:\nHtmlAgilityPack.HtmlDocument htmlDoc = new HtmlAgilityPack.HtmlDocument();\n\n// There are various options, set as needed\nhtmlDoc.OptionFixNestedTags = true;\n\nhtmlDoc.Load(@""C:\\Platypus\\dplatypus.htm"");\n\nif (htmlDoc.DocumentNode != null)\n{\n    IEnumerable<HtmlAgilityPack.HtmlNode> textNodes = htmlDoc.DocumentNode.SelectNodes(""//text()"");\n    foreach (HtmlNode node in textNodes)\n    {\n        if (!string.IsNullOrWhiteSpace(node.InnerText))\n        {\n            MessageBox.Show(node.InnerText);\n        }\n    }\n}\n\n']"
Parse a .Net Page with Postbacks,"
I need to read data from an online database that's displayed using an aspx page from the UN. I've done HTML parsing before, but it was always by manipulating query-string values. In this case, the site uses asp.net postbacks. So, you click on a value in box one, then box two shows, click on a value in box 2 and click a button to get your results.
Does anybody know how I could automate that process? 
Thanks,
Mike
",2k,"
            0
        ","[""\nYou may still only need to send one request, but that one request can be rather complicated.  ASP.Net is notoriously difficult (though not impossible) to screen scrape.  Between event validation and the ViewState, it's tricky to get your requests just right.  The simplest way to do it is often to use a sniffer tool like fiddler to see exactly what the http request looks like, and then just mimic that request.\nIf you do still need to send two requests, it's because the first request also places some state in a session somewhere, and that means whatever you use to send those requests needs to be able to send them with the same session.  This often means supporting cookies.\n"", '\nWatin would be my first choice.  You would code the selecting and clicking, then parse the HTML after.\n', ""\nI'd look at HtmlAgilityPack with the FormProcessor addon.\n""]"
Options for web scraping - C++ version only,"
I'm looking for a good C++ library for web scraping.
It has to be C/C++ and nothing else so please do not direct me to Options for HTML scraping or other SO questions/answers where C++ is not even mentioned.
",60k,"
            46
        ","['\n\nlibcurl to download the html file\nlibtidy to convert to valid xml\nlibxml to parse/navigate the xml\n\n', '\nUse myhtml C/C++ parser here; dead simple, very fast. No dependencies except C99. And has CSS selectors built in (example here)\n', '\nI recommend Qt5.6.2, this powerful library offer us\n\nHigh level, intuitive, asynchronous network api like QNetworkAccessManager, QNetworkReply, QNetworkProxy etc\nPowerful regex class like QRegularExpression\nDecent web engine like QtWebEngine\nRobust, mature gui like QWidgets\nMost of the Qt5 api are well designed, signal and slot make writing asynchronous codes become much easier too\nGreat unicode support\nFeature rich file system library. Whether create, remove, rename or find standard path to save files is piece of cake in Qt5\nAsynchronous api of QNetworkAccessManager make it easy to spawn many download request at once\nCross major desktop platforms, windows, mac os and linux, write once compiled anywhere, one code bases only.\nEasy to deploy on windows and mac(linux?maybe linuxdeployqt can save us tons of troubles)    \nEasy to install on windows, mac and linux\nAnd so on\n\nI already wrote an image scraper apps by Qt5, this app can scrape almost every image searched by Google, Bing and Yahoo. \nTo know more details about it, please visit my github project.\nI wrote down high level overview about how to scrape data by Qt5 on \nmy blogs(it is too long to post at stack overflow).\n\nDownload Bing images by Qt5\nCreate a better images downloader(Google, Bing and Yahoo) by Qt5\n\n', '\n// download winhttpclient.h\n// --------------------------------\n#include <winhttp\\WinHttpClient.h>\nusing namespace std;\ntypedef unsigned char byte;\n#define foreach         BOOST_FOREACH\n#define reverse_foreach BOOST_REVERSE_FOREACH\n\nbool substrexvealue(const std::wstring& html,const std::string& tg1,const std::string& tg2,std::string& value, long& next) {\n    long p1,p2;\n    std::wstring wtmp;\n    std::wstring wtg1(tg1.begin(),tg1.end());\n    std::wstring wtg2(tg2.begin(),tg2.end());\n\n    p1=html.find(wtg1,next);\n    if(p1!=std::wstring::npos) {\n        p2=html.find(wtg2,next);\n        if(p2!=std::wstring::npos) {\n            p1+=wtg1.size();\n            wtmp=html.substr(p1,p2-p1-1);\n            value=std::string(wtmp.begin(),wtmp.end());\n            boost::trim(value);\n            next=p1+1;\n        }\n    }\n    return p1!=std::wstring::npos;\n}\nbool extractvalue(const std::wstring& html,const std::string& tag,std::string& value, long& next) {\n    long p1,p2,p3;\n    std::wstring wtmp;\n    std::wstring wtag(tag.begin(),tag.end());\n\n    p1=html.find(wtag,next);\n    if(p1!=std::wstring::npos) {\n        p2=html.find(L"">"",p1+wtag.size()-1);\n        p3=html.find(L""<"",p2+1);\n        wtmp=html.substr(p2+1,p3-p2-1);\n        value=std::string(wtmp.begin(),wtmp.end());\n        boost::trim(value);\n        next=p1+1;\n    }\n    return p1!=std::wstring::npos;\n}\nbool GetHTML(const std::string& url,std::wstring& header,std::wstring& hmtl) {\n    std::wstring wurl = std::wstring(url.begin(),url.end());\n    bool ret=false;\n    try {\n        WinHttpClient client(wurl.c_str());\n        std::string url_protocol=url.substr(0,5);\n        std::transform(url_protocol.begin(), url_protocol.end(), url_protocol.begin(), (int (*)(int))std::toupper);\n        if(url_protocol==""HTTPS"")    client.SetRequireValidSslCertificates(false);\n        client.SetUserAgent(L""User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:19.0) Gecko/20100101 Firefox/19.0"");\n        if(client.SendHttpRequest()) {\n            header = client.GetResponseHeader();\n            hmtl = client.GetResponseContent();\n            ret=true;\n        }\n    }catch(...) {\n        header=L""Error"";\n        hmtl=L"""";\n    }\n    return ret;\n}\nint main() {\n    std::string url = ""http://www.google.fr"";\n    std::wstring header,html;\n    GetHTML(url,header,html));\n}\n\n']"
